filenr,sentence
0,PostgreSQL: Documentation: 16: Chapter 14. Performance Tips
0,Home
0,About
0,Download
0,Documentation
0,Community
0,Developers
0,Support
0,Donate
0,Your account
0,8th February 2024:
0,"PostgreSQL 16.2, 15.6, 14.11, 13.14, and 12.18 Released!"
0,Documentation → PostgreSQL 16
0,Supported Versions:
0,Current
0,(16)
0,Development Versions:
0,devel
0,Unsupported versions:
0,9.6
0,9.5
0,9.4
0,9.3
0,9.2
0,9.1
0,9.0
0,8.4
0,8.3
0,8.2
0,8.1
0,8.0
0,7.4
0,7.3
0,7.2
0,7.1
0,Chapter 14. Performance Tips
0,Prev
0,Part II. The SQL Language
0,Home
0,Next
0,Chapter 14. Performance Tips
0,Table of Contents
0,14.1. Using EXPLAIN
0,14.1.1. EXPLAIN Basics
0,14.1.2. EXPLAIN ANALYZE
0,14.1.3. Caveats
0,14.2. Statistics Used by the Planner
0,14.2.1. Single-Column Statistics
0,14.2.2. Extended Statistics
0,14.3. Controlling the Planner with Explicit JOIN Clauses
0,14.4. Populating a Database
0,14.4.1. Disable Autocommit
0,14.4.2. Use COPY
0,14.4.3. Remove Indexes
0,14.4.4. Remove Foreign Key Constraints
0,14.4.5. Increase maintenance_work_mem
0,14.4.6. Increase max_wal_size
0,14.4.7. Disable WAL Archival and Streaming Replication
0,14.4.8. Run ANALYZE Afterwards
0,14.4.9. Some Notes about pg_dump
0,14.5. Non-Durable Settings
0,"Query performance can be affected by many things. Some of these can be controlled by the user, while others are fundamental to the underlying design of the system. This chapter provides some hints about understanding and tuning PostgreSQL performance."
0,Prev
0,Next
0,13.7. Locking and Indexes
0,Home
0,14.1. Using EXPLAIN
0,Submit correction
0,"If you see anything in the documentation that is not correct, does not match"
0,"your experience with the particular feature or requires further clarification,"
0,please use
0,this form
0,to report a documentation issue.
0,Privacy Policy |
0,Code of Conduct |
0,About PostgreSQL |
0,Contact
0,Copyright © 1996-2024 The PostgreSQL Global Development Group
1,Tips for PostgreSQL Query Optimization: Improve Performance with EXPLAIN ANALYZE | EnterpriseDB
1,Skip to main content
1,Talk to an Expert
1,My Account
1,Dashboard
1,BigAnimal
1,Support Center
1,Training Portal
1,Migration Portal
1,Partner Portal
1,Account Settings
1,Sign Out
1,Sign In
1,EDB Login
1,BigAnimal Login
1,Products
1,EDB PRODUCTS
1,Postgres Cloud
1,Postgres Database-as-a-Service
1,BigAnimal
1,BigAnimal Performance Insights
1,(Coming soon)
1,BigAnimal Data Migration Service
1,(Coming soon)
1,EDB ENTERPRISE ADVANCED
1,Commercial Software with Enterprise Support
1,Postgres Advanced Server
1,Postgres Distributed
1,Postgres for Kubernetes
1,View EDB Software Portfolio
1,SUBSCRIPTION PLANS
1,BigAnimal Plans
1,Fully managed Postgres in the cloud
1,Enterprise Plan
1,Extending Postgres for enterprises
1,Standard Plan
1,PostgreSQL with enterprise tooling
1,Community 360 Plan
1,Leverage the power of PostgreSQL
1,Subscription Plan Comparison
1,Solutions
1,EDB SOLUTIONS
1,Your Power to Postgres Everywhere
1,Flexible deployment options to fit the needs of your enterprise
1,Migrate from Oracle to Postgres with EDB
1,"Oracle compatibility, enhanced migration tools, industry-leading support"
1,Public Sector Solutions
1,Postgres to accelerate mission outcomes
1,Fully Managed Solution for Oracle Migration
1,Multi-cloud database-as-a-service approach
1,Fully Managed Solution for Developers
1,The fastest way to develop apps with PostgreSQL
1,Oracle Migration Calculator
1,Estimate savings by migrating from Oracle to Postgres
1,Services & Support
1,SERVICES
1,"Professional Services, Support and Training Overview"
1,Increase your Postgres productivity and decrease your risk
1,Professional Services
1,Expert services and support
1,Training
1,Learn Postgres from EDB experts
1,SUPPORT
1,Technical Account Management
1,Proactive guidance and partnership
1,Remote DBA
1,Expert proactive support for your deployments
1,Developers
1,DOCUMENTATION AND RESOURCES
1,EDB Docs
1,EDB product documentation
1,Platform Compatibility
1,Supported versions and operating systems
1,Knowledge Base & Tech Alerts
1,Searchable support articles and alerts
1,Postgres Tutorial Blog
1,Read the latest about Postgres and more
1,Support
1,Support for PostgreSQL everywhere
1,DOWNLOADS AND INSTALLERS
1,Software Downloads
1,EDB integrated portfolio of software
1,Download PostgreSQL
1,Open source PostgreSQL package and installers
1,Resources
1,LEARN
1,Blog
1,Industry news and thought leadership
1,Customer Success Stories
1,Achieving more with Postgres
1,Events
1,Local events near you
1,Webinars
1,Upcoming and on-demand webinars
1,White Papers
1,PostgreSQL research and analysis
1,Resource Feature Callout 1
1,Connect
1,The Builders: A Postgres Podcast
1,"Discussions about Postgres, Data and AI featuring EDB Technical Staff."
1,Listen to Podcast
1,Resource Feature Callout 2
1,The Builders: A Postgres Podcast
1,Futures in Focus (CMO Michael Gale)
1,Spotlights on how visionaries and leaders are thinking about tomorrow's world.
1,Listen to Podcast
1,Company
1,About
1,We are Postgres experts
1,Why EDB
1,Real Enterprise Postgres by Real Postgres Experts
1,Press Room
1,Press Releases and Media Resources
1,Careers
1,Work at EDB
1,Contact Us
1,"Talk to sales, get support and more"
1,Become a Partner
1,Become a partner with EDB
1,Find a Partner
1,Search for global business partners
1,EDB Partners
1,Robust partner network
1,Why Partner With EDB
1,Benefits of being an EDB global business partner
1,Header CTA
1,Talk to an Expert
1,My Account
1,Dashboard
1,BigAnimal
1,Support Center
1,Training Portal
1,Migration Portal
1,Partner Portal
1,Account Settings
1,Sign Out
1,Sign In
1,EDB Login
1,BigAnimal Login
1,Search
1,Enter the terms you wish to search for.
1,SearchClear
1,How to Use EXPLAIN ANALYZE for Planning and Optimizing Query Performance in PostgreSQL
1,EDB Team
1,"March 30, 2023"
1,"With many people working from home these days because of the coronavirus pandemic, it can be a little challenging to get help from a colleague remotely. Sure, there’s Slack and all manner of collaboration tools, but it’s not quite the same as walking up to someone’s cubicle and getting a second pair of eyes to look at a problem, not to mention that our co-workers might be busy trying to juggle deadlines and unruly kids in the home. When it comes to dealing with poor database and query performance, it’s a daunting task to venture into the dark cavern of query planning and optimization, but fear not! EXPLAIN is our friend in those dark and lonely places."
1,"We recently received a request from one of our customers, concerned about a slow query on one of their JSON columns. They were seeing a slow performance in their development environments and were understandably worried about the impact that they’d see if they went to production with poor query performance. We got right to work to help them out, and our first stone to turn over was to have them send us their EXPLAIN ANALYZE output for the query, which yielded:"
1,postgres=# explain SELECT * FROM org where 'aa'::text IN (SELECT jsonb_array_elements(info -> 'dept') ->> 'name');
1,QUERY PLAN
1,-------------------------------------------------------------------------
1,Seq Scan on org
1,(cost=0.00..719572.55 rows=249996 width=1169)
1,Filter: (SubPlan 1)
1,SubPlan 1
1,Result
1,(cost=0.00..2.27 rows=100 width=32)
1,ProjectSet
1,(cost=0.00..0.52 rows=100 width=32)
1,Result
1,(cost=0.00..0.01 rows=1 width=0)
1,"They knew they had created an index, and were curious as to why the index was not being used. Our next data point to gather was information about the index itself, and it turned out that they had created their index like so:"
1,CREATE INDEX idx_org_dept ON org ((info -> 'dept'::text) ->> 'name'::text));
1,"Notice anything? Their query was wrapping info -> 'dept' in a function called jsonb_array_elements(), which led the query planner to think that it shouldn’t use the index. The fix was simple, and we were able to get the customer back on their way after a rather quick adjustment to their query. Once the customer changed their query to the following, the Index started getting scanned:"
1,postgres=# SELECT * FROM org where 'aa'::text IN (info -> 'dept' ->> 'name');
1,postgres=# explain SELECT * FROM organization where 'aa'::text IN (info -> 'dept' ->> 'name');
1,QUERY PLAN
1,----------------------------------------------------------------------------------------------
1,Index Scan using idx_org_dept on org
1,(cost=0.42..8.44 rows=1 width=1169)
1,Index Cond: ('aa'::text = ((info -> 'dept'::text) ->> 'name'::text))
1,(2 rows)
1,"As we can see, having and using EXPLAIN in your troubleshooting arsenal can be invaluable."
1,What is Explain?
1,"EXPLAIN is a keyword that gets prepended to a query to show a user how the query planner plans to execute the given query.  Depending on the complexity of the query, it will show the join strategy, method of extracting data from tables, estimated rows involved in executing the query, and a number of other bits of useful information.  Used with ANALYZE, EXPLAIN will also show the time spent on executing the query, sorts, and merges that couldn’t be done in-memory, and more.  This information is invaluable when it comes to identifying query performance bottlenecks and opportunities, and helps us understand what information the query planner is working with as it makes its decisions for us."
1,A Cost-Based Approach
1,"To the query planner, all the data on disk is basically the same.  To determine the fastest way to reach a particular piece of data requires some estimation of the amount of time it takes to do a full table scan, a merge of two tables, and other operations to get data back to the user.  PostgreSQL accomplishes this by assigning costs to each execution task, and these values are derived from the postgresql.conf file (see parameters ending in *_cost or beginning with enable_*).  When a query is sent to the database, the query planner calculates the cumulative costs for different execution strategies and selects the most optimal plan (which may not necessarily be the one with the lowest cost)."
1,bash $ pgbench -i && psql
1,<...>
1,postgres=# EXPLAIN SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;
1,QUERY PLAN
1,--------------------------------------------------------------------------------
1,Nested Loop
1,(cost=0.00..4141.00 rows=99999 width=461)
1,Join Filter: (a.bid = b.bid)
1,Seq Scan on pgbench_branches b
1,(cost=0.00..1.01 rows=1 width=364)
1,Seq Scan on pgbench_accounts a
1,(cost=0.00..2890.00 rows=99999 width=97)
1,Filter: (aid < 100000)
1,(5 rows)
1,"Here, we see that the Seq Scan on pgbench_accounts has cost 2890 to execute the task.  Where does this value come from?  If we look at some settings and do the calculations, we find:"
1,cost = ( #blocks * seq_page_cost ) + ( #records * cpu_tuple_cost ) + ( #records * cpu_filter_cost )
1,postgres=# select pg_relation_size('pgbench_accounts');
1,pg_relation_size
1,------------------
1,13434880
1,block_size
1,= 8192
1,"(8kB, typical OS)"
1,#blocks
1,= 1640
1,(relation_size / block_size)
1,#records
1,= 100000
1,seq_page_cost
1,= 1
1,(default)
1,cpu_tuple_cost
1,= 0.01
1,(default)
1,cpu_filter_cost = 0.0025 (default)
1,cost = ( 1640 * 1 ) + ( 100000 * 0.01 ) + ( 100000 * 0.0025 ) = 2890
1,"As we can see, the costs are directly based on some internal statistics that the query planner can work with."
1,A Note About Statistics
1,"The query planner calculates costs based on statistics stored in pg_statistic (don’t look there--there’s nothing human-readable in there.  If you want to get visibility into the table and row statistics, try looking at pg_stats).  If any of these internal statistics are off (i.e., a bloated table or too many joins that cause the Genetic Query Optimizer to kick in), a sub-optimal plan may be selected, leading to poor query performance.  Having bad statistics isn’t necessarily a problem--the statistics aren’t always updated in real-time, and much of it depends on PostgreSQL’s internal maintenance.  As such, it’s imperative that database maintenance is conducted regularly--this means frequent VACUUM-ing and ANALYZE-ing.  Without good statistics, you could end up with something like this:"
1,postgres=# EXPLAIN SELECT * FROM pgbench_history WHERE aid < 100;
1,QUERY PLAN
1,-----------------------------------------------------------------------
1,Seq Scan on pgbench_history
1,(cost=0.00..2346.00 rows=35360 width=50)
1,Filter: (aid < 100)
1,"In the example above, the database had gone through a fair amount of activity, and the statistics were inaccurate.  With an ANALYZE (not VACUUM ANALYZE or EXPLAIN ANALYZE, but just a plain ANALYZE), the statistics are fixed, and the query planner now chooses an Index Scan:"
1,postgres=# EXPLAIN SELECT * FROM pgbench_history WHERE aid < 100;
1,QUERY PLAN
1,----------------------------------------------------------------------
1,Index Scan using foo on pgbench_history
1,(cost=0.42..579.09 rows=153 width=50)
1,Index Cond: (aid < 100)
1,How Does EXPLAIN ANALYZE Help?
1,"When an EXPLAIN is prepended to a query, the query plan gets printed, but the query does not get run.  We won’t know whether the statistics stored in the database were correct or not, and we won’t know if some operations required expensive I/O instead of fully running in memory.  When used with ANALYZE, the query is actually run and the query plan, along with some under-the-hood activity is printed out."
1,"If we look at the first query above and run EXPLAIN ANALYZE instead of a plain EXPLAIN, we get:"
1,postgres=# EXPLAIN ANALYZE SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;
1,QUERY PLAN
1,-------------------------------------------------------------------------------------------------------------
1,Nested Loop
1,(cost=0.00..4141.00 rows=99999 width=461) (actual time=0.039..56.582 rows=99999 loops=1)
1,Join Filter: (a.bid = b.bid)
1,Seq Scan on pgbench_branches b
1,(cost=0.00..1.01 rows=1 width=364) (actual time=0.025..0.026 rows=1 loops=1)
1,Seq Scan on pgbench_accounts a
1,(cost=0.00..2890.00 rows=99999 width=97) (actual time=0.008..25.752 rows=99999 loops=1)
1,Filter: (aid < 100000)
1,Rows Removed by Filter: 1
1,Planning Time: 0.306 ms
1,Execution Time: 61.031 ms
1,(8 rows)
1,"You’ll notice here that there’s more information -- actual time and rows, as well as planning and execution times.  If we add BUFFERS, like EXPLAIN (ANALYZE, BUFFERS), we’ll even get cache hit/miss statistics in the output:"
1,"postgres=# EXPLAIN (BUFFERS, ANALYZE) SELECT * FROM pgbench_accounts a JOIN pgbench_branches b ON (a.bid=b.bid) WHERE a.aid < 100000;"
1,QUERY PLAN
1,-------------------------------------------------------------------------------------------------------------
1,Nested Loop
1,(cost=0.00..4141.00 rows=99999 width=461) (actual time=0.039..56.582 rows=99999 loops=1)
1,Join Filter: (a.bid = b.bid)
1,Buffers: shared hit=3 read=1638
1,Seq Scan on pgbench_branches b
1,(cost=0.00..1.01 rows=1 width=364) (actual time=0.025..0.026 rows=1 loops=1)
1,Buffers: shared hit=1
1,Seq Scan on pgbench_accounts a
1,(cost=0.00..2890.00 rows=99999 width=97) (actual time=0.008..25.752 rows=99999 loops=1)
1,Filter: (aid < 100000)
1,Rows Removed by Filter: 1
1,Buffers: shared hit=2 read=1638
1,Planning Time: 0.306 ms
1,Execution Time: 61.031 ms
1,(8 rows)
1,"Very quickly, you can see that EXPLAIN can be a useful tool for people looking to understand their database performance behaviors."
1,A Quick Review of Scan Types and Joins
1,"It’s important to know that every join type and scan type have their time and place.  Some people look for the word “Sequential” scan and immediately jump back in fear, not considering whether it would be worthwhile to access data another.  Take, for example, a table with 2 rows -- it would not make sense to the query planner to scan the index, then go back and retrieve data from the disk when it could just quickly scan the table and pull data out without touching the index.  In this case, and in the case of most other small-ish tables, it would be more efficient to do a sequential scan.  To quickly review the join and scan types that PostgreSQL works with:"
1,Scan Types
1,Sequential Scan
1,Basically a brute-force retrieval from disk
1,Scans the whole table
1,Fast for small tables
1,Index Scan
1,Scan all/some rows in an index; look up rows in heap
1,"Causes random seek, which can be costly for old-school spindle-based disks"
1,Faster than a Sequential Scan when extracting a small number of rows for large tables
1,Index Only Scan
1,Scan all/some rows in index
1,No need to lookup rows in the table because the values we want are already stored in the index itself
1,Bitmap Heap Scan
1,"Scan index, building a bitmap of pages to visit"
1,"Then, look up only relevant pages in the table for desired rows"
1,Join Types
1,Nested Loops
1,"For each row in the outer table, scan for matching rows in the inner table"
1,"Fast to start, best for small tables"
1,Merge Join
1,Zipper-operation on _sorted_ data sets
1,Good for large tables
1,High startup cost if an additional sort is required
1,Hash Join
1,"Build hash of inner table values, scan outer table for matches"
1,Only usable for equality conditions
1,"High startup cost, but fast execution"
1,"As we can see, every scan type and join type has its place.  What’s most important is that the query planner has good statistics to work with, as mentioned earlier."
1,"We’ve only talked about one instance where EXPLAIN helped identify a problem and give an idea of how to solve it.  At EDB Support, we’ve seen many situations where EXPLAIN could help identify things like:"
1,Inaccurate statistics leading to poor join/scan choices
1,Maintenance activity (VACUUM and ANALYZE) not aggressive enough
1,Corrupted indexes requiring a REINDEX
1,Index definition v. query mismatch
1,"work_mem being set too low, preventing in-memory sorts and joins"
1,Poor performance due to join order listing when writing a query
1,Improper ORM configuration
1,"EXPLAIN is certainly one of the most invaluable tools for anyone working with PostgreSQL, and using it well will save you lots of time!"
1,Join Postgres Pulse Live!
1,"We make use of the problems we solve and the conversations we have in helping people with Postgres, and this was another example of that effort in motion.  EXPLAIN and the query planner doesn’t start and stop with what we’ve outlined here, so if you have other questions, we’re here for you.  You can find all of our blog and YouTube series here, and you can always join us for our next session."
1,"Join us on Monday, May 4th, for our next Pulse Live Session!  We’ll dive into this week’s questions and quagmires around EXPLAIN use, as well as take questions from anyone who participates.  You can ask your questions via email at postgrespulse@enterprisedb.com, hashtag on Twitter, or live during the event right here."
1,Share this
1,Relevant Blogs
1,What is pgvector and How Can It Help You?
1,"There are a thousand ways (likely more) you can accelerate Postgres workloads. It all comes down to the way you store data, query data, how big your data is and..."
1,"November 03, 2023"
1,PostgreSQL 16 Update: Grouping Digits in SQL
1,PostgreSQL 16 was recently&nbsp;released. This is the story of a feature.
1,One of the minor new features in PostgreSQL 16 that I am excited about is the ability...
1,"October 17, 2023"
1,Deploying PostgreSQL Clusters in a Declarative Way
1,"How do you usually deploy your PostgreSQL clusters? Do you use in-house Ansible playbooks? Or are you using other orchestration tools, like Chef, Salt or Puppet? I still remember my..."
1,"September 26, 2023"
1,More Blogs
1,"PostgreSQL Logical Replication: Advantages, EDB's Contributions and PG 16 Enhancements"
1,Introduction
1,"PostgreSQL, known for its powerful features and robustness, offers various replication methods to ensure high availability, data consistency, and near-zero-downtime upgrades. As may be expected, logical replication is one..."
1,"September 01, 2023"
1,pgAdmin CI/CD
1,Almost exactly three years ago I wrote a blog on my personal page entitled Testing pgAdmin which went into great detail discussing how we test pgAdmin prior to releases. Back...
1,"August 24, 2023"
1,Customising your pgAdmin 4 workspace - Part 1: Browser
1,"If you are reading this then you are probably using pgAdmin as a tool to help in your day to day database management activities with PostgreSQL. But, did you know..."
1,"August 21, 2023"
1,Social Nav Footer
1,Footer Nav Main
1,Products and Solutions
1,Solutions
1,Software Portfolio
1,Subscription Plans Comparison
1,Topics
1,What is Cloud Computing?
1,What is Database-as-a-Service?
1,Press Room
1,Press Releases
1,Company
1,Leadership Team
1,Careers
1,Footer Copyright CTA
1,© 2024 EDB
1,Legal Notices
1,ESG
1,Privacy Policy
1,Marketing Preferences
1,Cookie Preferences
5,EDB Postgres Advanced Server on Linux on IBM zSystems - Performance Tuning Guide
5,Skip to main content (Press Enter).
5,Sign in
5,Skip auxiliary navigation (Press Enter).
5,Skip main navigation (Press Enter).
5,Toggle navigation
5,HomeGroupsEvents Upcoming Community EventsAll Community Events
5,Linux on IBM Z and LinuxONE - Group home
5,Back to Blog List
5,EDB Postgres Advanced Server on Linux on IBM zSystems - Performance Tuning Guide
5,Like
5,EDB Postgres Advanced Server on Linux on IBM zSystems - Performance Tuning Guide
5,"EDB Postgres offers an integrated, Open Source based SQL relational database solution built for enterprise-scale data needs. Combined with the industry leading reliability, isolation, and integrity of the IBM zSystems hardware, EDB Postgres on Linux on IBM zSystems and IBM LinuxONE represents an ideal deployment platform for your database consolidation and application modernization needs."
5,"The Systems Performance - Linux on IBM zSystems team has performed extensive research on the performance of EDB Postgres Advanced Server on the IBM zSystems platform. The outcome of this research is now available as a tuning guide, which summarizes the key findings and recommendations of this study. The recommendations given in this report can significantly boost the performance of EDB Postgres environments on IBM zSystems."
5,See the following chart for an overall summary of the provided tuning recommendations:
5,Related links:
5,EDB Postgres Advanced Server on IBM zSystems - Performance Tuning Guide
5,EDB Postgres Advanced Server
5,EDB Postgres Advanced Server documentation
5,IBM United States Software Announcement 222-173
5,PostgreSQL: Experiences and tuning recommendations on Linux on IBM Z
5,Copyright 2023 IBM Z and LinuxONE Community. All rights reserved.
5,Powered by Higher Logic
5,×Group Tags
5,Add a tag
5,User Tags may not contain the following characters: @ # $ & :
7,Tuning PostgreSQL performance [most important settings]
7,Open-source APMGolang HTTP routerGolang ClickHouseBlogBun Getting started
7,PostgreSQL
7,Reference open in new window GitHub open in new window Getting started
7,PostgreSQL
7,Reference open in new window GitHub open in new windowPostgreSQL
7,Overview
7,Supported data types
7,UUIDs
7,Arrays
7,LISTEN NOTIFY
7,COPY
7,Faceted navigation and search
7,Table Partitioning
7,Tuning PostgreSQL performance
7,max_connections
7,shared_buffers
7,work_mem
7,maintenance_work_mem
7,effective_cache_size
7,Autovacuum
7,WAL
7,SSD
7,Timeouts
7,Logging
7,Huge pages
7,Use indexes
7,Partitioning
7,Cursor pagination
7,Monitoring performance
7,Zero-downtime migrations
7,Optimizing PostgreSQL for ZFS and AWS EBS
7,"pgBackRest: PostgreSQL S3 backups # Tuning PostgreSQL settings for performancePostgreSQL has many configuration options that can be adjusted to improve performance. Here are some tips for tuning PostgreSQL performance.max_connectionsshared_bufferswork_memmaintenance_work_memeffective_cache_sizeAutovacuumWALSSDTimeoutsLoggingHuge pagesUse indexesPartitioningCursor paginationMonitoring performance# max_connectionsUse a reasonably low number of connections so you can give each connection more RAM, disk time, and CPU. To not get FATAL too many connections error, use a connection pool in front of PostgreSQL, for example, PgBounceropen in new window is a good option.max_connections = <4-8 * number_of_cpus>"
7,"On SSD, set max_connections to the number of concurrent I/O requests the disk(s) can handle * number_of_cpus.# shared_buffersshared_buffers controls how much memory PostgreSQL reserves for writing data to a disk. PostgreSQL picks a free page of RAM in shared buffers, writes the data into it, marks the page as dirty, and lets another process asynchronously write dirty pages to the disk in background.PostgreSQL also uses shared buffers as a cache if the data you are reading can be found there. For proper explanation, see thisopen in new window.WARNINGLowering shared buffers value too much may hurt write performance.shared_buffers = <20-40% of RAM>"
7,# work_memwork_mem specifies the max amount of memory each PostgreSQL query can use before falling back to temporary disk files. Every query may request the value defined by work_mem multiple times so be cautious with large values.work_mem = <1-5% of RAM>
7,"If your queries often use temp files, consider increasing work_mem value and lowering the max number of concurrent queries via max_connections.The optimal value for work_mem can vary depending on your specific workload, hardware resources, and available memory. Regular monitoring, benchmarking, and tuning are necessary to ensure optimal performance as your workload evolves over time.# maintenance_work_memmaintenance_work_mem limits the max amount of memory that can be used by maintenance operations, for example, CREATE INDEX or ALTER TABLE.maintenance_work_mem = <10-20% of RAM>"
7,# effective_cache_sizeeffective_cache_size gives PostgreSQL a hint about how much data it can expect to find in the system cache or ZFS ARC.effective_cache_size = <70-80% of RAM>
7,# AutovacuumAutovacuum is a background process responsible for removing dead tuples (deleted rows) and updating database statistics used by PostgreSQL query planner to optimize queries.Default autovacuum settings are rather conservative and can be increased to let autovacuum run more often and use more resources:# Allow vacuum to do more work before sleeping.
7,# 500-1000 should be enough.
7,vacuum_cost_limit = 500
7,# Use smaller nap time if you have many tables.
7,autovacuum_naptime = 10s
7,# Ran autovacuum when 5% of rows are inserted/updated/deleted.
7,autovacuum_vacuum_scale_factor = 0.05
7,autovacuum_vacuum_insert_scale_factor = 0.05
7,You can also run less autovacuum workers but give each of them more memory:# Run 2 autovacuum workers instead of 3.
7,autovacuum_max_workers = 2
7,# But give them more memory.
7,autovacuum_work_mem = <2-3% of RAM>
7,"# WALPostgreSQL WAL stands for Write-Ahead Logging. The Write-Ahead Log is a transaction log that records changes made to the database before they are written to the actual data files.When a transaction modifies the data in PostgreSQL, the changes are first written to the WAL before being applied to the actual database files. This process ensures that the changes are durably recorded on disk before considering the transaction committed.The following WAL settings work well most of the time and the only downside is increased recovery time when your database crashes:wal_compression = on"
7,min_wal_size = 1GB
7,max_wal_size = 8GB
7,wal_buffers = 16MB
7,checkpoint_timeout = 30min
7,checkpoint_completion_target = 0.9
7,"# SSDIf you are using solid-state drives, consider tweaking the following settings:# Cost of a randomaly fetched disk page."
7,# SSDs have low random reads cost relative to sequential reads.
7,random_page_cost = 1.1
7,# Number of simultaneous requests that can be handled efficiently by the disk subsystem.
7,# SSDs can handle more concurrent requests.
7,effective_io_concurrency = 200
7,# TimeoutsYou can tell PostgreSQL to cancel slow queries using the following settings:# Cancel queries slower than 5 seconds.
7,statement_timeout = 5000
7,# Max time to wait for a lock.
7,lock_timeout = 5000
7,# LoggingGood logging can tell you when queries are too slow or there are any other problems:# Log queries slower than 500ms.
7,log_min_duration_statement = 500
7,# Log queries that use temp files.
7,log_temp_files = 0
7,# Log queries that wait for locks.
7,log_lock_waits = on
7,"# Huge pagesHuge pages, also known as large pages, are a memory management feature in operating systems that allow applications to allocate and utilize larger page sizes than the standard small pages. In the context of databases like PostgreSQL, huge pages can offer performance benefits by reducing memory overhead and improving memory access efficiency.If your servers have 128+ GB of RAM, consider using huge pages to reduce the number of memory pages and to minimize the overheadopen in new window introduced by managing large amount of pages.# Use indexesIndexes can significantly speed up query performance by allowing PostgreSQL to quickly locate the data it needs. Ensure that your tables have appropriate indexes based on the queries being run.Use the EXPLAIN command to analyze queries and identify areas for optimization.EXPLAIN ANALYZE SELECT ...;"
7,"# PartitioningIf your tables are very large, consider partitioning them. Partitioning can improve query performance by allowing PostgreSQL to quickly access the relevant data.See PostgreSQL Table Partitioning.# Cursor paginationWhen dealing with large data sets, such as in a web application that needs to display a large number of records. consider using cursor pagination.# Monitoring performanceRegularly monitoring database activity can help identify performance issues. Use tables such as pg_stat_activity, pg_stat_database, and pg_stat_user_tables to monitor database activity and identify areas for optimization.To monitor PostgreSQLopen in new window, you can use OpenTelemetry PostgreSQLopen in new window receiver that comes with OpenTelemetry Collector.Uptrace is a OpenTelemetry APMopen in new window that supports distributed tracing, metrics, and logs. You can use it to monitor applications and troubleshoot issues.Uptrace comes with an intuitive query builder, rich dashboards, alerting rules with notifications, and integrations for most languages and frameworks.Uptrace can process billions of spans and metrics on a single server and allows you to monitor your applications at 10x lower cost.In just a few minutes, you can try Uptrace by visiting the cloud demoopen in new window (no login required) or running it locally with Dockeropen in new window. The source code is available on GitHubopen in new window.OpenTelemetry Backendopen in new windowDistributed tracing toolsopen in new window Edit this page open in new windowLast Updated:"
7,Table Partitioning
7,Zero-downtime migrations
8,PostgreSQL Tuning GuideProductsSolutionsDevelopersSupportResourcesCompanyContact SalesRegisterLoginEnglishChineseRegisterLoginEnglishChinesePostgreSQL Tuning Guidefor Ampere Altra Processors on Oracle Cloud InfrastructureOverviewSetup and TuningScaling of -c (clients) and -j(threads)OverviewSetup and TuningScaling of -c (clients) and -j(threads)OverviewThis document provides setup and tuning details for PostgreSQL running on Ampere Altra and Altra Max systems.
8,Note: This tuning guide will show you how to build/tune PostgreSQL for Ampere processors. Please note there is scope to stress the system more by using additional pg threads/more memory to get the best performance as needed.
8,"This test was done on OCI - A1 instance with 4 OCPUs and 40GB RAM. For this test, we used pgbench to stress PostgreSQL. We ran pgbench on the same system as PostgreSQL to avoid network latencies, however, if you find that pgbench consumes significant CPU resources, then it is recommended to run it on a different client system/VM."
8,Operating Sytem: Oracle Linux 8.6Kernel: 5.4.17-2136.308.9.el8uek.aarch64Storage: 1 TB – default balanced SSD (use iSCSI commands to attach the disk)THP is set to always.Setup and TuningFollow the steps below to setup the system:
8,Copysudo yum update
8,sudo yum groupinstall 'Development Tools'
8,sudo yum install readline-devel
8,The test used an external disk converted to xfs file system for storing PostgreSQL data.
8,The test downloaded the latest PostgreSQL source code to build it. GCC 10.2 and above is recommended to build PostgreSQL as it includes lse based optimizations.
8,Copywget https://ftp.postgresql.org/pub/source/v14.4/postgresql-14.4.tar.gz
8,This is an example to build PostgreSQL with recommended GCC flags.
8,"CopyCC=${path to GCC bin} CFLAGS=""-mcpu=neoverse-N1 -march=armv8.2-a+crypto+FP16+rcpc+dotprod+crc+lse -O3"" ./configure --enable-arm-crc32 --prefix=${path to postgresql build} &&"
8,make clean &&
8,make -j &&
8,make install
8,Initilize and start the newly built PostgreSQL.
8,Copypg_ctl -D /<postgres data dir> initdb
8,pg_ctl -D /<postgres data dir> -l logfile start
8,Here is an example of postgresql.conf used for this benchmarking exercise:
8,Copylisten_addresses = 'localhost'
8,port = 5432
8,# (change requires restart)
8,max_connections = 100 # (change requires restart)
8,shared_buffers = 25GB # min 128kB
8,"huge_pages = try # on, off, or try"
8,max_wal_senders = 0
8,max_wal_size=200GB
8,maintenance_work_mem = 512MB # min 1MB
8,"autovacuum_work_mem = -1 # min 1MB, or -1 to use maintenance_work_mem"
8,max_stack_depth = 7MB # min 100kB
8,dynamic_shared_memory_type = posix # the default is the first option
8,max_files_per_process = 4000 # min 25
8,effective_io_concurrency = 32 # 1-1000; 0 disables prefetching
8,"wal_level = minimal # minimal, archive, hot_standby, or logical"
8,synchronous_commit = off # synchronization level;
8,"wal_buffers = -1 # min 32kB, -1 sets based on shared_buffers"
8,checkpoint_timeout = 1h # range 30s-1h
8,"checkpoint_completion_target = 1 # checkpoint target duration, 0.0 - 1.0"
8,checkpoint_warning = 0 # 0 disables
8,log_min_messages = error # values in order of decreasing detail:
8,log_min_error_statement = error # values in order of decreasing detail:
8,log_timezone = 'GB'
8,log_checkpoints = on
8,log_autovacuum_min_duration = 0
8,autovacuum = off # Enable autovacuum subprocess? 'on'
8,"datestyle = 'iso, dmy'"
8,timezone = 'GB'
8,lc_messages = 'C.UTF-8' # locale for system error message
8,lc_monetary = 'C.UTF-8' # locale for monetary formatting
8,lc_numeric = 'C.UTF-8' # locale for number formatting
8,lc_time = 'C.UTF-8' # locale for time formatting
8,default_text_search_config = 'pg_catalog.english'
8,max_locks_per_transaction = 64 # min 10
8,"max_pred_locks_per_transaction = 64 # min 10 On Recoverable mode, we took out some of the flags that would impact recoverability."
8,"Adequately scale max_connections, shared_buffers and max_wal_size to avoid bottlenecks to sufficiently scale throughput delivered by PostgreSQL."
8,"Now, we create a database and populate it using pgbench. Next, we run a test with 8 clients and 8 threads to stress the database."
8,Copypgbench -i -s 50 pg50 // create a database of scale factor 50 <use the size you need>
8,"pgbench -c 8 -j 8 -T 60 pg50 // start the test, you may -c and -j as needed."
8,"Following the above steps, we measured a TPS of 6146, with a latency of 1.3ms on the above-mentioned OCI A1 system based on Ampere Altra."
8,The same steps on a comparable OCI E4 instance resulted in a TPS of 5547 with a latency of 1.4ms.
8,"There is more scope to push CPU/pgbench threads as needed on this system, as seen below.Scaling of -c (clients) and -j(threads)This table shows scaling of -c (clients) and -j(threads) within pgbench on a database of scale factor 50."
8,"-c, -j A1 - tps A1 - latency (ms) E4 – tps E4 – latency (ms) 86,0411.3 4,963 1.6166,7232.35,6642.8327,946 46,010 5.3 488,351 5.76,4627.4648,624 7.46,545 9.8"
8,"In general, it is good practice to do a manual checkpoint between each run. This is to avoid automatic checkpoints in between the run.Created At : April 6th 2023, 9:18:33 amLast Updated At : July 31st 2023, 4:27:15 pmAmpere Computing LLC"
8,4655 Great America Parkway Suite 601
8,"Santa Clara, CA 95054About | Contact Us | Privacy Policy | Cookie Use Policy | CCPA Policy Notice | Cookie Settings | Sitemap© 2023 Ampere Computing LLC. All rights reserved. Ampere, Altra and the A and Ampere logos are registered trademarks or trademarks of Ampere Computing.This site is running on Ampere Altra Processors."
10,PostgreSQL Tuning GuideProductsSolutionsDevelopersSupportResourcesCompanyContact SalesRegisterLoginEnglishChineseRegisterLoginEnglishChinesePostgreSQL Tuning Guidefor Ampere Altra Processors on Oracle Cloud InfrastructureOverviewSetup and TuningScaling of -c (clients) and -j(threads)OverviewSetup and TuningScaling of -c (clients) and -j(threads)OverviewThis document provides setup and tuning details for PostgreSQL running on Ampere Altra and Altra Max systems.
10,Note: This tuning guide will show you how to build/tune PostgreSQL for Ampere processors. Please note there is scope to stress the system more by using additional pg threads/more memory to get the best performance as needed.
10,"This test was done on OCI - A1 instance with 4 OCPUs and 40GB RAM. For this test, we used pgbench to stress PostgreSQL. We ran pgbench on the same system as PostgreSQL to avoid network latencies, however, if you find that pgbench consumes significant CPU resources, then it is recommended to run it on a different client system/VM."
10,Operating Sytem: Oracle Linux 8.6Kernel: 5.4.17-2136.308.9.el8uek.aarch64Storage: 1 TB – default balanced SSD (use iSCSI commands to attach the disk)THP is set to always.Setup and TuningFollow the steps below to setup the system:
10,Copysudo yum update
10,sudo yum groupinstall 'Development Tools'
10,sudo yum install readline-devel
10,The test used an external disk converted to xfs file system for storing PostgreSQL data.
10,The test downloaded the latest PostgreSQL source code to build it. GCC 10.2 and above is recommended to build PostgreSQL as it includes lse based optimizations.
10,Copywget https://ftp.postgresql.org/pub/source/v14.4/postgresql-14.4.tar.gz
10,This is an example to build PostgreSQL with recommended GCC flags.
10,"CopyCC=${path to GCC bin} CFLAGS=""-mcpu=neoverse-N1 -march=armv8.2-a+crypto+FP16+rcpc+dotprod+crc+lse -O3"" ./configure --enable-arm-crc32 --prefix=${path to postgresql build} &&"
10,make clean &&
10,make -j &&
10,make install
10,Initilize and start the newly built PostgreSQL.
10,Copypg_ctl -D /<postgres data dir> initdb
10,pg_ctl -D /<postgres data dir> -l logfile start
10,Here is an example of postgresql.conf used for this benchmarking exercise:
10,Copylisten_addresses = 'localhost'
10,port = 5432
10,# (change requires restart)
10,max_connections = 100 # (change requires restart)
10,shared_buffers = 25GB # min 128kB
10,"huge_pages = try # on, off, or try"
10,max_wal_senders = 0
10,max_wal_size=200GB
10,maintenance_work_mem = 512MB # min 1MB
10,"autovacuum_work_mem = -1 # min 1MB, or -1 to use maintenance_work_mem"
10,max_stack_depth = 7MB # min 100kB
10,dynamic_shared_memory_type = posix # the default is the first option
10,max_files_per_process = 4000 # min 25
10,effective_io_concurrency = 32 # 1-1000; 0 disables prefetching
10,"wal_level = minimal # minimal, archive, hot_standby, or logical"
10,synchronous_commit = off # synchronization level;
10,"wal_buffers = -1 # min 32kB, -1 sets based on shared_buffers"
10,checkpoint_timeout = 1h # range 30s-1h
10,"checkpoint_completion_target = 1 # checkpoint target duration, 0.0 - 1.0"
10,checkpoint_warning = 0 # 0 disables
10,log_min_messages = error # values in order of decreasing detail:
10,log_min_error_statement = error # values in order of decreasing detail:
10,log_timezone = 'GB'
10,log_checkpoints = on
10,log_autovacuum_min_duration = 0
10,autovacuum = off # Enable autovacuum subprocess? 'on'
10,"datestyle = 'iso, dmy'"
10,timezone = 'GB'
10,lc_messages = 'C.UTF-8' # locale for system error message
10,lc_monetary = 'C.UTF-8' # locale for monetary formatting
10,lc_numeric = 'C.UTF-8' # locale for number formatting
10,lc_time = 'C.UTF-8' # locale for time formatting
10,default_text_search_config = 'pg_catalog.english'
10,max_locks_per_transaction = 64 # min 10
10,"max_pred_locks_per_transaction = 64 # min 10 On Recoverable mode, we took out some of the flags that would impact recoverability."
10,"Adequately scale max_connections, shared_buffers and max_wal_size to avoid bottlenecks to sufficiently scale throughput delivered by PostgreSQL."
10,"Now, we create a database and populate it using pgbench. Next, we run a test with 8 clients and 8 threads to stress the database."
10,Copypgbench -i -s 50 pg50 // create a database of scale factor 50 <use the size you need>
10,"pgbench -c 8 -j 8 -T 60 pg50 // start the test, you may -c and -j as needed."
10,"Following the above steps, we measured a TPS of 6146, with a latency of 1.3ms on the above-mentioned OCI A1 system based on Ampere Altra."
10,The same steps on a comparable OCI E4 instance resulted in a TPS of 5547 with a latency of 1.4ms.
10,"There is more scope to push CPU/pgbench threads as needed on this system, as seen below.Scaling of -c (clients) and -j(threads)This table shows scaling of -c (clients) and -j(threads) within pgbench on a database of scale factor 50."
10,"-c, -j A1 - tps A1 - latency (ms) E4 – tps E4 – latency (ms) 86,0411.3 4,963 1.6166,7232.35,6642.8327,946 46,010 5.3 488,351 5.76,4627.4648,624 7.46,545 9.8"
10,"In general, it is good practice to do a manual checkpoint between each run. This is to avoid automatic checkpoints in between the run.Created At : April 6th 2023, 9:18:33 amLast Updated At : July 31st 2023, 4:27:15 pmAmpere Computing LLC"
10,4655 Great America Parkway Suite 601
10,"Santa Clara, CA 95054About | Contact Us | Privacy Policy | Cookie Use Policy | CCPA Policy Notice | Cookie Settings | Sitemap© 2023 Ampere Computing LLC. All rights reserved. Ampere, Altra and the A and Ampere logos are registered trademarks or trademarks of Ampere Computing.This site is running on Ampere Altra Processors."
11,Advanced Postgres Performance Tips
11,Skip to main content
11,thoughtbot
11,thoughtbot
11,Menu
11,Close Menu
11,Live on Twitch!
11,thoughtbot is livestreaming
11,"Work alongside the thoughtbot team as we collaborate with each other and our clients, live. Ask us anything, we're live right now!"
11,Services
11,Case Studies
11,Blog
11,Resources
11,Let’s Talk
11,Live on Twitch!
11,thoughtbot is livestreaming
11,"Work alongside the thoughtbot team as we collaborate with each other and our clients, live. Ask us anything, we're live right now!"
11,Let’s get started!
11,Back
11,View all Services
11,Web Development
11,Ruby on Rails
11,React
11,Maintenance
11,AI & Machine Learning
11,Mobile Development
11,React Native
11,iOS
11,Android
11,Design
11,"UX, UI, & Product Design"
11,Design Research
11,Design Systems
11,Product
11,Product Management
11,Product Design Sprint
11,Research & Strategic Insights
11,Accessibility
11,Team & Processes
11,Team Augmentation
11,Fractional Leadership
11,Team Level Up
11,Recruiting & Hiring
11,"DevOps, SRE, Platform"
11,View all Services
11,Back
11,View all Resources
11,Development
11,Open Source
11,Books
11,The Bike Shed Podcast
11,Upcase
11,Live Streaming on YouTube
11,The business of great software
11,Playbook
11,Startup Incubator
11,Giant Robots Smashing Into Other Giant Robots Podcast
11,Design Sprint Guide
11,Live Streaming on LinkedIN
11,View all Resources
11,Advanced Postgres Performance Tips
11,Caleb Hearth
11,"July 29, 2016"
11,updated on
11,"July 19, 2023"
11,postgres
11,databases
11,sql
11,performance
11,"You’ve added the INDEXes, both partial and covering. You’ve VACCUUM ANALYZEd."
11,"You JOINed and INNER JOINed everything to a single query. And yet, your"
11,report is still taking too long. What do you do when the low-hanging fruit has
11,been harvested?
11,It’s time to get down and dirty with some of the lesser known SQL constructs in
11,Rails land.
11,EXPLAIN what’s happening
11,"If you’ve gotten this far, you’re probably familiar with using EXPLAIN and"
11,EXPLAIN ANALYZE to get insight into what approach Postgres is taking to
11,execute queries and the actual performance of those approaches. You know that an
11,"Index Scan is preferable to a Seq Scan, but you’ll settle for a Heap Scan as a"
11,Join Cond.
11,"Query plans aren’t the easiest thing to read, however. They’re packed with"
11,information and it’s closer to being machine parsable than human readable.
11,Visualize the problem
11,Postgres Explain Viewer (PEV) is a tool to simplify reading query plans. It
11,provides a horizontal tree with each node representing a node in the query plan.
11,"It includes timing information, the error amount in the planned versus actual"
11,"times, and badges for interesting nodes like “costliest” or “bad estimate”."
11,It’s pretty useful to have at-a-glance feedback about your query. But knowing
11,how long queries take is just the beginning: what can we do to speed these up?
11,Materialize your desires
11,"Views are a tool for storing “partial queries”. That is, they allow the database"
11,to store a parsed query that you can treat as a table in most respects later.
11,You can SELECT (and sometimes UPDATE or DELETE) from a view with identical
11,syntax as if you were executing the statement against a table. They’re useful
11,when you’d like to perform a complex statement repeatedly and don’t want to deal
11,with cluttering your Ruby files with arcane ARel or strung-together scopes.
11,Making liberal use of views is a key aspect of good SQL database design. Views
11,"allow you to encapsulate the details of the structure of your tables, which"
11,"might change as your application evolves, behind consistent interfaces."
11,– PostgreSQL Documentation - Advanced Features - Views
11,"A view can be materialized, which means the results are stored by Postgres at"
11,CREATE MATERIALIZED VIEW and REFRESH MATERIALIZED VIEW time. The cost of the
11,"partial query is paid at these times, so we can benefit from that over and over,"
11,especially in read-heavy situations (most situations are read-heavy in my
11,experience).
11,Materialized views are especially helpful when your select list includes a
11,"subset of columns, you perform identical operations such as COUNT or SUM or"
11,"extracting part of a jsonb object on every query, and when JOINing additional"
11,"tables. When it comes time to actually retrieve rows, these rows can be queried"
11,against to return only relevant data. This is often cheaper to execute than a
11,full statement. These can be further beneficial by creating indices on the
11,"materialized views themselves (such as on the column you’re JOINed by, or on a"
11,data column for a reporting query).
11,"As an aside, Ruby on Rails does not have first-class support for views, despite"
11,that from a usage perspective they’re very similar to tables. We maintain a
11,"Rails extension, Scenic, which helps with versioning, migrating, and"
11,maintaining SQL views.
11,"The 37 minute query plan above can’t be improved on by a materialized view,"
11,"unfortunately, because there aren’t any good candidates for caching partial"
11,results. This was an exceptional case and I was surprised to find that this
11,feature wouldn’t be helpful to me.
11,Common Table Expressions and Subqueries
11,Common Table Expressions such as WITH expression_name AS (...) SELECT
11,... and Subqueries such as SELECT ... FROM (SELECT ...) AS subquery_name are
11,"tools for breaking up complex SQL queries, and sometimes the only way to achieve"
11,"a goal. While CTEs are arguably easier to read than subqueries, in Postgres they"
11,"are an “optimization fence”, preventing the query optimizer from rewriting"
11,queries by moving constraints into or out of the CTE. For example this query:
11,SELECT
11,"x, count"
11,"FROM (SELECT x, count(x) count FROM big_table GROUP BY x) aggregates"
11,WHERE x = 42;
11,The query could be optimized by “pushing” the WHERE clause into the subquery
11,to avoid performing the read and count operations for every row in big_table:
11,SELECT
11,"x, count"
11,"FROM (SELECT x, count(x) count FROM big_table GROUP BY x WHERE x = 42) aggregates;"
11,"However, a CTE such as this would prevent the optimization, causing the entire"
11,"table to be read, aggregated, and materialized to disk before being re-scanned"
11,from the materialized table for the x = 42 constraint:
11,"WITH aggregates AS (SELECT x, count(x) count FROM big_table GROUP BY x)"
11,SELECT
11,"x, count"
11,FROM aggregates
11,WHERE x = 42;
11,"There are also times when the optimization fence is useful, such as when using"
11,"data-modifying statements (INSERT, UPDATE, or DELETE) in WITH. As the CTE is"
11,"only executed once, the result is the same where a subquery is allowed to be"
11,called multiple times by the planner and would not return information from
11,deleted or updated rows.
11,"Both Common Table Expressions and subqueries are useful, and one or the other"
11,may be more performant in a specific case. This is one example where subqueries
11,"are the better option, but I usually find that a CTE is as faster or better than"
11,a subquery and lean on them most of the time. Experiment with both forms in
11,EXPLAIN ANALYZE to find the right tool for your query.
11,Control yourself; Take only what you need from it
11,"The common answer to why we should SELECT col1, ..., colN rather than SELECT"
11,"* is that we reduce network traffic. In most cases, for reasonably normalized"
11,"databases, and with today’s high-speed network connections, this is unlikely to"
11,"be a huge deal. In one example involving 326/89K rows, the difference in"
11,network traffic caused by selecting a single column versus all columns was about
11,10%.
11,A better reason to limit columns to only what’s needed is index lookups. We want
11,the planner to hit indexes instead of doing a sequential scan of the table
11,itself. We generally optimize this by creating indexes on important columns such
11,as foreign keys and of course primary keys come with an index by default. These
11,"are great, helping to identify the disk location of rows that match whatever"
11,constraints we have in a query for easier lookup or ordering.
11,"We can also use covering indexes, which include the specific columns and"
11,"expressions useful to a query, to store all of the relevant information. This"
11,prevents the second step of reading from the table itself and can provide a big
11,performance gain. Using * as the select list in a query would likely not make
11,use of a covering index unless you index the entire table and keep it up-to-date
11,with added columns.
11,Preparation is key
11,"Prepared statements split the work of parsing, analyzing, rewriting, planning,"
11,and executing a statement in the same way that materialized views split the work
11,of preparing parts of queries that don’t change by caching their results. When a
11,"statement is prepared, Postgres parses, analyzes, and rewrites it. It generally"
11,uses placeholders for values being provided at EXECUTE time.
11,PREPARE is an optimization for the very specific use-case of similar statements
11,being executed many times in a single session. Prepared statements are not
11,"persisted or shared between sessions, so they’re not something you can use to"
11,optimize a general use without setting up session connect events externally.
11,From the documentation:
11,Prepared statements have the largest performance advantage when a single
11,session is being used to execute a large number of similar statements. The
11,performance difference will be particularly significant if the statements are
11,"complex to plan or rewrite, for example, if the query involves a join of many"
11,tables or requires the application of several rules. If the statement is
11,"relatively simple to plan and rewrite but relatively expensive to execute, the"
11,performance advantage of prepared statements will be less noticeable.
11,"Since we previously mentioned that Rails does not support views, it’s only fair"
11,to point out that since version 3.1 it does make use of prepared statements.
11,Do more in the database
11,The ultimate Postgres performance tip is to do more in the database. Postgres is
11,"optimized to be very efficient at data storage, retrieval, and complex"
11,"operations such as aggregates, JOINs, etc. Let your web application deal with"
11,displaying data and your database with manipulating and converting data.
11,Becoming comfortable with using Postgres for even the most complex reports
11,"starts with familiarity with using it, from its syntax to its functions and"
11,"types. Read the Postgres documentation as you do the Ruby, Go, or Elixir docs."
11,"If you enjoyed this post, you might also like:"
11,ActiveRecord's where.not and nil
11,A Grand Piano for Your Violin
11,Reading a Postgres EXPLAIN ANALYZE Query Plan
11,Sign up to receive a weekly recap from thoughtbot
11,Email Address
11,Subscribe
11,Services
11,Case Studies
11,Resources
11,Hire Us
11,Our Company
11,Purpose
11,Blog
11,Sponsor
11,Mastodon
11,GitHub
11,Instagram
11,YouTube
11,twitch
11,© 2024
11,"thoughtbot, inc."
11,The design of a robot and thoughtbot are registered trademarks of
11,"thoughtbot, inc."
11,Privacy Policy
12,DB-BERT: making database tuning tools “read” the manual | The VLDB Journal
12,Skip to main content
12,Log in
12,Menu
12,Find a journal
12,Publish with us
12,Track your research
12,Search
12,Cart
12,Home
12,The VLDB Journal
12,Article
12,DB-BERT: making database tuning tools “read” the manual
12,Special Issue Paper
12,Open access
12,Published: 27 December 2023
12,(2023)
12,Cite this article
12,Download PDF
12,You have full access to this open access article
12,The VLDB Journal
12,Aims and scope
12,Submit manuscript
12,DB-BERT: making database tuning tools “read” the manual
12,Download PDF
12,Immanuel Trummer1
12,431 Accesses
12,Explore all metrics
12,"AbstractDB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specifically, the BERT model) for text analysis. During an initial training phase, it fine-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specific database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (PostgreSQL and MySQL). The experiments demonstrate clearly that DB-BERT benefits from combining general information about database tuning, mined from text documents, with scenario-specific insights, gained via trial runs. The full source code of DB-BERT is available online at https://itrummer.github.io/dbbert/."
12,Similar content being viewed by others
12,"Natural language processing: state of the art, current trends and challenges"
12,Article
12,14 July 2022
12,"Diksha Khurana, Aditya Koli, … Sukhdev Singh"
12,A survey on deep learning approaches for text-to-SQL
12,Article
12,Open access
12,23 January 2023
12,George Katsogiannis-Meimarakis & Georgia Koutrika
12,Trends and Future Perspective Challenges in Big Data
12,Chapter
12,© 2022
12,Use our pre-submission checklist
12,Avoid common mistakes on your manuscript.
12,1 Introduction
12,"Give me a user manual, and I’m happy for hours. \({--- \textrm{Lennon Parham}}\)"
12,"When all else fails, read the instructions. \({--- \textrm{Anonymous}}\)"
12,"Manuals are useful. For instance, before starting to tune a database management system (DBMS), it is recommended to read the associated manual. So far, those words of wisdom only seemed to apply to human database administrators. While it is widely acknowledged that database manuals contain useful information, this knowledge has long been considered inaccessible to machines due to barriers in natural language understanding. We believe that this has changed with recent advances in the field of natural language processing, namely by the introduction of powerful, pre-trained language models based on the Transformer architecture [51]. We present DB-BERT, a tuning tool, based on the BERT model [7], that “reads” (i.e., analyzes via natural language tools) the manual and hundreds of text documents with tuning hints in order to find promising settings for database system parameters faster.The problem of finding optimal values for DBMS parameters (also called “tuning knobs”) for specific workloads and performance metrics has received significant attention in recent years. DBMSs often have hundreds of parameters [34], making it very hard to find optimal settings manually. This motivates computational methods for automated parameter tuning. The dominant approach is currently machine learning [1], in particular reinforcement learning [24, 49, 57]. Here, a tuning tool selects value combinations for DBMS parameters to try in a principled manner, guided by the results of benchmark runs for specific settings. However, this approach is expensive (recent work uses hundreds of iterations per tuning session [49]) and works best if guided by input from database experts [17], pre-selecting a small set of parameters to tune and reasonable value ranges to consider. Our goal is to substitute such input by information that is gained automatically by analyzing text documents. We call the corresponding problem variant Natural Language Processing (NLP)-Enhanced Database Tuning.Table 1 Example tuning hints with extractionsFull size tableDB-BERT extracts, from text, tuning hints that recommend specific values for specific parameters. Instead of focusing on the database manual alone, typically containing recommendations to optimize performance for typical workloads, DB-BERT mines a large number of text documents on the Web. In doing so, DB-BERT is able to access the “long tail” of tuning recommendations, considering less common scenarios as well.Table 1 shows examples for tuning hints with sources and the associated, formal representation of each extracted hint. Some of the hints (second example) recommend an absolute value while others (first and third example) recommend relative values. For the latter, translating the hint into a concrete value recommendation requires knowledge of system properties such as the amount of RAM. Some of the hints (first two examples) mention the parameter explicitly while others (last example) refer to it only implicitly. DB-BERT can exploit all of the hints shown in Table 1.For a given text snippet, DB-BERT uses a fine-tuned version of the BERT Transformer model to solve four tasks. First, it decides whether a text snippet contains hints. Second, it translates hints into formulas such as the ones shown in Table 1. This may entail steps for resolving implicit parameter references as well as relative recommendations. Third, instead of relying on hints completely, DB-BERT may decide to deviate from proposed values within pre-defined ranges. Finally, given potentially conflicting hints from multiple sources, DB-BERT chooses weights for hints, representing their relative importance.DB-BERT does not rely on tuning hints alone. Instead, DB-BERT gains more information via trial runs, executing workloads with specific parameter settings while measuring performance. For instance, this enables DB-BERT to resolve conflicts between recommendations from multiple sources. Trying out recommended values reveals which recommendations are reliable. To decide which values to try, DB-BERT uses reinforcement learning, thereby balancing between exploration and exploitation in a principled manner.During a tuning session, DB-BERT iterates until a user-defined optimization time budget runs out. In each iteration, DB-BERT selects one or multiple DBMS configurations (i.e., parameter settings) to try out. DB-BERT translates the performance observed during those runs (on user-defined benchmarks) into a reward value. This reward value is used to guide the selection of configurations in future iterations, using the Double Deep Q-Networks [50] reinforcement learning algorithm. To apply this algorithm, we formulate database tuning as a Markov Decision Process (MDP) with discrete states and actions. We represent treatment for each hint as a sequence of decisions, determining the hint type (e.g., relative versus absolute values) as well as the hint weight. To leverage NLP for those decisions, we associate each decision option with a text label. This allows DB-BERT to compare hint text and decision label using the BERT Transformer.We train DB-BERT in a system and benchmark independent manner, before applying it for specific tuning tasks. In principle, we could use manually annotated tuning documents for training (assigning a high reward for hint translations that are consistent with annotations). However, generating such data requires expert knowledge and is hard to crowdsource (compared to cases where labeling requires only commonsense knowledge [11]). Instead, we exploit the database system itself for (noisy) feedback. We assume that tuning hints, if correctly translated, tend to recommend admissible values that do not to dramatically decrease performance. Hence, we train DB-BERT by assigning rewards for hint translations that result in admissible parameter settings (i.e., the DBMS accepts the setting). On the other side, we assign penalties for translations that result in inadmissible parameter settings (i.e., the DBMS rejects the setting) or settings that decrease performance significantly for a simple example workload. The result of training is a model (i.e., weights for around 110 million parameters of the fine-tuned BERT model) that can be used as starting point for tuning other database systems on other benchmarks.We also present an alternative version of DB-BERT which does not require any scenario-specific training (i.e., no specialized training for database tuning using text). Instead, this variant exploits out-of-the-box language analysis models, pre-trained on standard benchmarks from the NLP domain. More precisely, it maps the problem of extracting recommendations for specific parameters into a question answering problem, using tuning text as context. Also, it uses zero-shot classifiers to associate relative tuning hints with system resources such as RAM, CPU cores, or disk space.The idea of leveraging text documents for database tuning has been introduced in a recent vision paper [43], published by the same author as the current one. That paper proposes a simple approach based on supervised learning. The approach is trained via tuning hints that have been manually labeled with hint translations. In contrast to that, DB-BERT uses unlabeled text as input. No manual pre-processing is required on this input text. Choices associated with hint translation steps are annotated with manually provided text labels (15 labels in total). However, those labels are not scenario-dependent and we use the same labels across all experiments (Table 3 shows five out of the 15 labels). The same applies to all other tuning parameters introduced in the following sections. Besides the differences in manual labeling overheads, the prior approach is purely based on input text, does not integrate any performance measurements, and is therefore unable to adapt recommendations to specific benchmarks or metrics. Compared to the initial SIGMOD publication that this paper is based upon [45], this current version expands the original approach by a “zero-shot” variant which does not require task-specific training. This variant is evaluated in the experiments. We discuss differences to prior work in Sect. 2 in more detail.In our experiments, we compare against the latter work as well as against state-of-the-art methods for database tuning without input text. We exploit large document collections, mined by issuing Google queries with relevant keywords, as text input for DB-BERT. We consider different benchmarks (e.g., TPC-C and TPC-H), metrics (throughput and latency), and database systems (MySQL and PostgreSQL). The experiments demonstrate that DB-BERT benefits significantly from information gained via text analysis. In summary, our original, scientific contributions are the following:"
12,"We introduce multiple variants of DB-BERT, a system that combines natural language text documents and run time feedback of benchmark evaluations to guide database tuning."
12,"We describe the mechanisms used by DB-BERT to extract, prioritize, translate, aggregate, and evaluate tuning hints."
12,"We evaluate DB-BERT experimentally and compare against baselines, using multiple benchmarks, metrics, and database systems."
12,"The reminder of this paper is organized as follows. We cover required background in learning and NLP in Sect. 2. Then, in Sect. 3, we introduce our problem model and terminology. We give an overview of DB-BERT in Sect. 4. Then, in Sect. 5, we describe how DB-BERT extracts and prioritizes candidate hints from text documents. We show how DB-BERT translates single hints in Sect. 6 and how it aggregates and evaluates hints in Sect. 7. Next, we present a zero-shot variant of DB-BERT in Sect. 8 which does not require any task-specific training data. In Sect. 9, we report experimental results before we conclude with Sect. 10.2 Background and related workWe discuss technologies that DB-BERT is based upon. Also, we describe prior work addressing similar problems as DB-BERT.2.1 Pre-trained language modelsThe field of NLP has recently seen significant advances across a range of long-standing problems [53]. These advances have been enabled, in particular, by the emergence of large, pre-trained language models [16], based on the Transformer architecture [51]. Such models address two pain points of prior NLP approaches: lack of task-specific training data and bottlenecks in computational resources for training. Language models are trained, using significant computational resources, on tasks for which training data is readily available in large quantities. For instance, masked language modeling [7] (i.e., predicting masked words in a sentence) can use arbitrary Web text for training. Instead of training new models from scratch for other NLP-related tasks, pre-trained models can be used as a starting point. Pre-trained models can be used either via fine-tuning or via prompting. Using fine-tuning, pre-trained models are trained further on task-specific training data. However, due to the use of pre-training, the number of required training samples and computational overheads are reduced by many orders of magnitude [16]. The latest generation of language models [6, 25, 58], including also OpenAI’s GPT model series [10], can often be used without task-specific training via prompting [4]. Instead of training data, it is sufficient to describe a new task to solve as part of the prompt, the text input to the model.2.2 Applications of language models in data managementNatural language query interfaces [13, 14, 18, 26, 38] are the most popular application of pre-trained models in the context of databases. At the time of writing, corresponding approaches constitute the state of the art for text-to-SQL translation benchmarks (e.g., WikiSQL [59] or SPIDER [56]). The problem of translating text into queries shares certain characteristics with the problem of extracting tuning hints from text. In both cases, text is translated into a formal representation. However, whereas text-to-SQL methods typically translate a single sentence into one single SQL query, DB-BERT extracts multiple tuning hints from multi-sentence text passages. Also, DB-BERT must aggregate and prioritize conflicting hints obtained from multiple sources (a sub-problem that does not appear in the context of natural language query interfaces). Unlike most prior work on text-to-SQL translation, DB-BERT does not assume the presence of labeled training samples.Recent work explores a variety of novel use cases for large language models in data management [46]. These include applications for data preparation and integration problems [2, 39, 41], data profiling and discovery [5, 19, 47], as well as novel database engine designs that exploit language models for data processing directly [37, 39, 42] or to synthesize code for processing [2, 44].2.3 Reinforcement learningReinforcement learning [40] addresses scenarios such as the following. An agent explores an environment, selecting actions based on observations. Those actions may influence the environment (whose dynamics are initially unknown to the agent) and result in reward values. The goal of the agent is to maximize reward, accumulated over time. In order to do so, the agent needs to balance exploration (trying out action sequences about which little is known) with exploitation (exploiting action sequences that seem to work well, based on observations so far). The area of reinforcement learning has produced various algorithms that balance this tradeoff in a principled manner. Specifically, DB-BERT uses the Double Deep Q-Networks [50] algorithm. This algorithm learns to estimate action values in specific states via deep learning, using two separate models for selecting actions and evaluating them.Reinforcement learning has been used for various problems in the database domain [3, 15, 55, 57], including tuning problems (discussed in detail next). Different from prior work, we combine reinforcement learning with NLP to find promising parameter settings. More broadly, our work connects to prior work on leveraging text for reinforcement learning, in particular prior work on instruction following [27]. However, prior work does not consider performance tuning, specifically database tuning, as we do.2.4 Database tuningTable 2 Comparing DB-BERT to prior work on NLP-enhanced database tuningFull size tableTable 3 Labels associated with actions for decision \(d=0\). Placeholders are contained in square bracketsFull size tableA recent vision paper [43] on NLP-enhanced database tuning, written by the same author as the current publication, relates most to the current work. The prior work trains a Transformer model to recognize sentences containing tuning hints via supervised learning. For sentences classified as tuning hints, it extracts parameters and values according to a simple heuristic. This approach uses only text input but no run time feedback. It extracts a fixed set of recommendations from a document collection, without being able to adapt to specific workloads and performance metrics. DB-BERT, on the other hand, uses hints extracted from text merely as a starting point. It supports a broader range of tuning hints (e.g., implicit hints) and does not require annotated tuning hints during training. We summarize some of the differences in Table 2 and compare both approaches experimentally in Sect. 9.Machine learning is nowadays the method of choice for many database optimization problems, ranging from query optimization [9, 12, 20, 21, 28, 29, 32, 48] over physical design decisions [8, 15, 23, 55] up to database system parameter tuning [24, 34, 52, 57]. We address an extended version of the latter problem, expanding the input by natural language text documents.3 Problem modelWe tune configurations for database system parameters.Definition 1 (Configuration) Each DBMS is associated with a set \(\mathcal {P}\) of configuration parameters. Denote by \(\mathcal {V}\) the set of admissible parameter values. A configuration assigns each parameter to a valid value and is represented as a function \(\mathcal {P}\mapsto \mathcal {V}\). Equivalently, we represent this function as set \(\{\langle p_i,v_i\rangle \}\) for \(p_i\in \mathcal {P}\) and \(v_i\in \mathcal {V}\) of parameter-value pairs. Parameters not referenced in a configuration maintain their default values.Our goal is to find configurations that optimize performance. Traditionally, the following problem model is used.Definition 2 (Database Tuning) A database tuning problem is described by a tuple \(\langle b,\mathcal {P},\mathcal {V}\rangle \). Here, b is a benchmark defining a set of queries (or a transaction workload), together with a performance metric to optimize (e.g., run time or throughput). A solution assigns parameters \(\mathcal {P}\), selected for tuning, to values from \(\mathcal {V}\) and ideally optimizes performance according to benchmark b.In this work, we address a variant of this problem model.Definition 3 (NLP-Enhanced Tuning) An NLP-enhanced database tuning instance is described by a tuple \(\langle b,T,S\rangle \). Here, b is a benchmark to optimize and T a collection of text documents containing tuning hints. The goal is to find optimal configurations for b, considering all DBMS tuning knobs (more precisely, our current implementation considers all integer, numeric, and Boolean parameters for each system), using tuning hints extracted from T via natural language analysis. S is a vector of numerical system properties (such as the amount of RAM or the number of cores) needed to translate hints, potentially containing relative value suggestions, into concrete values.We do not expect users to specify parameters to tune nor to suggest value ranges for parameters. We rely on natural language analysis to identify relevant parameters and proposed values. However, the approach presented in this work assumes access to a DBMS instance. Via this interface, we verify whether extracted parameter names are valid and whether the parameter type falls within our scope. Our current implementation considers integer, Boolean, and numeric parameters. This scope covers a large share of performance-relevant parameters in database systems like PostgreSQL and MySQL. An extension to other value types, e.g., string-valued parameters, is, in principle, possible. However, integer, numerical, and Boolean parameters (which can be represented as integers) open up interesting possibilities for NLP-enhanced tuning. For instance, given multiple conflicting value recommendations for the same parameter, it is possible to select a value that minimizes distance to any of the recommendations (which requires a distance function). This is less convenient for non-numerical value types.The goal of text analysis is to extract tuning hints, described next.Definition 4 (Tuning Hint) A tuning hint suggests a value for one DBMS parameter. We model tuning hints as a triple \(\langle t,p,v\rangle \) where t is a text snippet containing the hint, p a specific parameter, and v a specific value mentioned in t. We call the hint explicit if p is mentioned explicitly in t and implicit otherwise. In pseudo-code, we use notation h.p or h.t to refer to parameter or text of hint h.Note that a text snippet t may contain suggestions for multiple parameters or multiple suggested values for the same parameter. This is why we need p and v to identify a specific hint within t. Value v may not always be the concrete value proposed for p. This is why we translate tuning hints into formulas, defined next.Definition 5 (Translated Hint) We translate tuning hints \(\langle t,p,v\rangle \) into a formula of the form \(p=f(v,S)\) where f is a formula and S a vector of numerical system properties (e.g., the amount of main memory). We consider formulas of type \(f(v,S)=v\cdot m\) as well as \(f(v,S)=v\cdot S_i\cdot m\) where \(S_i\) is the i-th component of S and \(m\in \mathbb {R}\) a multiplicator (picked from a discrete set M of multiplicators).We illustrate tuning hints and their translation.Example 1   Consider the text snippet \(t=\)“Properly configure shared_buffers - we recommend 25% of available RAM”.Footnote 1 Assume \(S=\langle 8GB, 4, 1TB\rangle \) describes the amount of RAM, the number of cores, and the amount of disk space on the target system. Then, the tuning hint \(\langle t,p,v\rangle \) for \(p=\)shared_buffers and \(v=0.25\) should translate into the formula \(f(v,S)=v\cdot S_0\cdot 1\) (where 1 represents the multiplicator), which evaluates to 2 GB.4 System overviewFigure 1 shows an overview of the DB-BERT system. DB-BERT searches settings for the tuning knobs of a DBMS that maximize performance according to a specific benchmark (specifying workload and performance metric). DB-BERT differs from prior tuning system in that it exploits text documents about the DBMS to tune, for instance the DBMS manual, as additional input.DB-BERT obtains as input the benchmark to tune, a collection of text documents containing suggested settings for tuning knobs, and numerical properties describing the hardware platform (namely, our implementation expects the amount of RAM, the number of cores, and the amount of disk space as inputs). The latter input is necessary to translate tuning hints in text documents that use relative recommendations (e.g., suggesting a buffer size as a percentage of the amount of RAM). Note that DB-BERT is not restricted to parameters that relate to the aforementioned hardware properties. DB-BERT can process hints for arbitrary parameters, as long as recommended values are specified as absolute values in text.DB-BERT does not use text input alone to determine parameter settings (separating it from prior work on NLP-enhanced database tuning [43]). Instead, it exploits run time feedback obtained by benchmarking specific configurations on the DBMS to tune. Hence, DB-BERT requires a connection to a DBMS instance.At the start of a tuning session, DB-BERT divides input text into text snippets and tries to extract tuning hints from each snippet (Step A in Fig. 1). A tuning hint corresponds to a recommendation of a specific value for a specific parameter. Extracting hints from text snippets is non-trivial, in particular as parameter references may be implicit (i.e., the text does not explicitly mention the name of the parameter to tune). Next, DB-BERT determines the order in which hints will be considered in the following stages (Step B in Fig. 1). Ideally, the most important hints are considered first. DB-BERT uses a heuristic to order hints, prioritizing hints about frequently mentioned parameters while limiting the number of hints considered consecutively for the same parameter.Next, DB-BERT iteratively creates configurations (i.e., value assignments for tuning knobs) from tuning hints. It evaluates those configurations on the input benchmark via trial runs. Iterations continue until the user interrupts optimization or a user-specified optimization time limit is reached.Fig. 1Overview of DB-BERT system: we exploit tuning hints, extracted from text documents, to find optimal DBMS knob settings for a given workloadFull size imageIn each iteration, DB-BERT considers a batch of tuning hints (not the entire set of tuning hints). It considers hints in the order established at the start of the tuning session, thereby considering the seemingly most important hints first. For each hint, DB-BERT takes three types of decisions. First, it translates the hint text into a simple equation, assigning a value to a parameter (Step C in Fig. 1). Second, in Step D, it decides whether to deviate from the recommended value (i.e., whether to multiply the recommended value by a constant). Third, it assigns a weight to the hint (Step E). These weights decide how to prioritize in case of conflicting recommendations about the same tuning knob. After treating all hints in the current batch, DB-BERT aggregates them into a small set of configurations (Step F), mediating between inconsistent recommendations using hint weights. It evaluates those configurations on the user-specified benchmark via trial runs (Step G in Fig. 1).DB-BERT learns to improve the way hints are translated, adapted, and weighted over the course of a tuning session. This allows DB-BERT to specialize a configuration to the current benchmark and platform. DB-BERT uses reinforcement learning to make all decisions associated with Steps C to E in Fig. 1. The learning process is therefore driven by a reward function that the system tries to maximize. In case of DB-BERT, that reward function is based on the performance results for specific configurations during trial runs. Configurations that are accepted by the DBMS (i.e., trying to set parameters to specific values does not result in an error) and achieve high performance generate high reward values. Based on rewards received, the system learns to improve its decision making in coming iterations (Step H in Fig. 1).DB-BERT uses deep reinforcement learning. This means that immediate and future reward values associated with specific choices are estimated using a neural network. Specifically, DB-BERT uses BERT, a pre-trained language model, as neural network. Due to pre-training, this model comes with powerful natural language analysis capabilities out of the box. To estimate the value of specific choices during Steps C to E, BERT is applied to pairs of text snippets. The first snippet is taken from the text of a tuning hint, the second snippet is a text label representing the semantics of that choice (see Table 3 in Sect. 6 for example labels). Based on reward values received, the initial weights of the BERT model are refined over the course of a tuning session (in Step H).Algorithm 1NLP-enhanced database performance tuning.Full size imageAlgorithm 1 represents the main function, executed by DB-BERT, in pseudo-code. The input integrates user-provided inputs, represented in Fig. 1, as well as other parameters, extracted automatically or kept constant across systems and benchmarks. These include the full set of integer, Boolean, and numeric tuning knobs, extracted from the DBMS, P, a set M of multiplicators (to deviate from values proposed in text), a set W of weights (to determine relative importance between conflicting hints from different sources), and parameters l, e, and n to choose the number of hints processed per parameter and iteration, the total number of hints considered per iteration, and the number of configurations evaluated per iteration, respectively. The semantics of those parameters will be described in more detail in the following sections.Line 8 in Algorithm 1 realizes Step A from Fig. 1, Line 10 realizes Step B. The main loop iterates until the tuning time budget is depleted. Function Batches(\(H_o,e\)) divides hints into batches of size at most e, following the previously established hint order. Each invocation of Run Episode realizes Steps C to H from Fig. 1. Finally, DB-BERT recommends the best observed configuration.Section 5 discusses hint extraction and ordering. Section 6 describes the learning process in more detail and Sect. 7 outlines how hints are aggregated into configurations.5 Extracting candidate hintsAlgorithm 2Extract candidate tuning hints from text documents.Full size imageFig. 2Given a text passage and DBMS parameter names, DB-BERT pairs extracted values with parameters that are explicitly mentioned or are similar to the textFull size imageIn a first step, DB-BERT extracts candidate tuning hints. Following Definition 4, a tuning hint consists of a text snippet, a parameter reference, and a value reference. Algorithm 2 describes the extraction process (illustrated in Fig. 2 as well). It extracts explicit as well as implicit parameter references. Implicit references are obtained by comparing the BERT encoding for the text (a vector) against BERT encodings of parameter names, selecting the parameter with minimal cosine distance. We consider all numbers that appear in text, potentially combined with size units, as potential value suggestions. By default, we add values 0 and 1, representing on and off values for Boolean flags, into the set of values (on and off values are often not explicitly mentioned in tuning hints). The set of candidate hints for a given text snippet is the Cartesian product between parameter references and values. This means that our candidates likely contain erroneous hints (i.e., parameter-value combinations that are not linked by the text). The task of separating actual from erroneous hints is solved during the translation phase, described in the next section.Algorithm 3Prioritize hints based on their parameters.Full size imageFig. 3DB-BERT prioritizes hints about frequently mentioned parameters while limiting the number of hints per parameters before switching to the next one. In the illustrated example, hints are considered in the order indicated by the red (numbered) arrowsFull size imageAfter extracting candidate hints, DB-BERT sorts them using Algorithm 3. Our goal is to increase chances of finding promising configurations when considering hints in sort order. We consider two rules of thumb. First, we expect important parameters to be mentioned in more documents. Second, we expect diminishing returns when considering more and more hints about the same parameter. As a result, we prioritize hints about parameters that appear in more documents. However, we consider at most a fixed number of hints about the same parameter, before switching to the next one. Algorithm 3 implements those high-level principles. After grouping hints by parameter, it iterates over hint index ranges. For each index range, it iterates over parameters in decreasing order of occurrences, adding up to l hints per parameter before switching to the next one (until no new hints are left to add for any parameter).Example 2   Fig. 3 illustrates hint ordering with three parameters. Blue rectangles represent hints for each parameter. The horizontal width is proportional to the number of hints. Starting with the most frequently mentioned parameter, we add a limited number of hints for each parameter. After treating the least frequently mentioned parameter (symbolized by the red arrow), Parameter 3, we start again with the first one until no more hints are left.6 Translating single hintsAlgorithm 4Transition function for translating single hints.Full size imageDB-BERT translates tuning hints into arithmetic formulas (see Definition 5 for details). Those formulas may depend on values, specified in text, as well as on system properties such as the amount of main memory. Evaluating a formula yields a value suggestion for a tuning knob.For each tuning hint, we model the translation as a sequence of decisions. We learn to translate tuning hints by using reinforcement learning. Reinforcement learning is generally applied to Markov Decision Processes (MDPs), specified by a set of states, actions, a transition function mapping state and action pairs to new states, and a reward function. A reinforcement learning agent learns to make decisions maximizing expected rewards, using observations as guidance. In our scenario, states represent (partially specified) arithmetic formulas. Actions specify parts of the formula. The transition functions links partially specified formulas and actions to states representing the formula, completed as specified in the action. The reward function is based on feedback from the DBMS, penalizing translations that result in inadmissible configurations while rewarding changes that improve performance. We describe the structure of the environment (i.e., states, actions, transitions, and rewards) in Sect. 6.1 and the structure of the learning agent in Sect. 6.2.6.1 Learning environmentAlgorithm 4 implements the transition function, used by DB-BERT to translate single hints (the pseudo-code is close to the implementation of the step function in the corresponding OpenAI Gym environmentFootnote 2). In Algorithm 4, and for a fixed tuning hint, the current state is characterized by a partially specified formula (f) and by variable d, the integer ID of the next decision to take. For each hint, we start with an empty formula f and \(d=0\). We represent actions (input a) as integer numbers from one to five. The semantics of actions depend on the value of d. For \(d=0\), the action decides whether the current hint is erroneous (constant NO_HINT) and, if not, whether the hint suggests a relative or absolute parameter value. Relative values are expressed as percentage of system properties such as main memory or the number of cores (stored in vector S with \(S_a\) representing a specific vector component). For relative values, we set f to the product between value v and the corresponding system property. We unify treatment of relative and absolute values by setting \(S_1=1\) (i.e., \(a=1\) represents an absolute value).For \(d=1\), the action picks a multiplicator from M that allows deviating from the proposed value. Unlike prior work using extracted hints without changes [43], such multiplicators allow DB-BERT to adapt to specific benchmarks. In the next section, we introduce an additional decision that weighs hints. Here, we have fully specified the formula after two decisions. Next, we try setting parameter p to the formula evaluation result. If the setting is rejected by the DBMS, we directly advance to an end state (constant END). This case yields negative reward (motivating our agent to learn translating hints into admissible formulas). Otherwise, we evaluate performance on the input benchmark b. The result is a reward value. Higher rewards are associated with better performance. We calculate reward by comparing performance with a configuration to evaluate to performance with default settings. For OLAP benchmarks (e.g., TPC-H), we use the delta of run times (scaled by a constant). For OLTP benchmarks (e.g., TPC-C), we use the throughput delta.Fig. 4Markov Decision Process for hint translation: parameter-value pairs are mapped to formulas by action sequences. Rectangles represent states (double lines mark end states). Arrows represent transitions (dashed arrows mark non-deterministic transitions)Full size imageWe reward configurations that are admissible and increase performance. Those two metrics are immediately relevant for tuning. We use them when applying DB-BERT for tuning a specific system for a specific benchmark. Before applying DB-BERT for specific tuning tasks, we perform a training phase to fine-tune DB-BERT’s language models for hint translation in general. To speed up convergence, only during training, we add an additional component to the reward function. This component rewards settings that seem more likely, e.g., since they are in the same order of magnitude as the default settings for a parameter. Such heuristics replace manually generated hint translations, used in prior work [43]. Figure 4 illustrates the MDP behind the translation process (some of the states in Fig. 4 are not explicitly represented in Algorithm 4).6.2 Learning agentAlgorithm 5Evaluating expected reward of actions.Full size imageDB-BERT introduces a learning agent to choose actions in order to maximize rewards. In each state, the agent selects among a discrete set of options. Each option can be expressed as a natural language statement. We can find out which option is correct by comparing that statement against the tuning hint text. Hence, we model action selection as a “multiple choice question answering problem”. Pre-trained language models can be used to solve this problem (in our implementation, we use the BertForMultipleChoice Transformer modelFootnote 3). We fine-tune model weights during training, based on rewards received.Algorithm 5 shows how the agent evaluates specific actions, based on observations. Besides the action to evaluate, the input includes a description of the current tuning hint (tuning text t, parameter p, and value v) as well as the current translation step (decision d). We associate each combination of an action and a decision with a label. The array containing those labels is represented via constant CHOICE_LABEL in the pseudo-code. The label is a natural language sentence, representing the semantics of the associated choice. It contains placeholders for the concrete parameter and value in the tuning hint. The Instantiate function replaces placeholders by concrete values.The BERT model uses three inputs: an input text, a type tag associating input tokens with one of two input types, and a mask indicating tokens to consider. Here, we concatenate hint text and instantiated label to form the input text. Types separate hint text from label. By default, all input text is considered for processing. An exception occurs during our generic training phase (see Sect. 6.1 for more details). Here, we want to avoid learning the names of specific parameters as they do not generalize across systems. Hence, we mask all occurrences of the current parameter name (Function Mask). On the other side, if learning system and benchmark-specific configurations for a concrete tuning problem, there are no reasons to hide information. Algorithm 5 uses a Boolean flag (MASKED_MODE) to switch between these two modes.Table 3 shows labels associated with different actions and the first decision level. At this level, we decide whether a candidate hint represents an actual hint and, if so, whether the value is relative or absolute. Finally, we illustrate the translation by an example.Example 3   Consider tuning hint \(\langle t,p,v\rangle \) with \(t=\)“Set shared_buffers to 25% of RAM”, \(v=25\%\), and finally \(p=\)shared_buffers. First, the agent decides whether the hint is valid and whether it recommends an absolute or relative value. Using the labels from Table 3, the agent evaluates alternative actions based on the hint text. For instance, for action 1, the agent generates the input text “Set shared_buffers to 25% of RAM. shared_buffers and 25% relate to main memory.”, separating the two sentences via the type specification. If masked mode is activated, the two occurrences of the shared_buffers parameter are masked. To make a choice, the agent internally compares values resulting from applying BERT to the input for each possible action.7 Aggregating hintsAlgorithm 6Transition function for interpreting multiple hints.Full size imageThe last section describes how to translate single tuning hints. However, we often need to integrate multiple hints, possibly from different sources, to obtain optimal performance. DB-BERT creates configurations based on groups of hints. This requires aggregating, possibly conflicting hints, from different sources. To support that, we expand the MDP presented in the last section. Instead of considering a single hint, we consider an entire batch of hints. For each single hint, we add an additional decision assigning the hint to a weight. This weight determines the priority when aggregating the hint with others into a configuration. Note that this approach gives DB-BERT the possibility to prioritize hints differently for each input workload. Different from static heuristics for hint priorization, this enables DB-BERT to specialize configurations to each input workload, even when using the same source text.Algorithm 6 shows complete pseudo-code executed during one iteration of DB-BERT’s main loop (Algorithm 6 is invoked by Algorithm 1). From the reinforcement learning perspective, each iteration corresponds to one episode of the associated MDP. Each episode starts from the same starting state, representing the default configuration. The number of hints considered per episode does therefore restrict the maximal number of changes, compared to the default configuration. However, as shown in recent work [17, 49], tuning a small number of tuning knobs is typically sufficient to achieve near-optimal performance.Algorithm 6 obtains a batch of candidate hints as input. It iterates over those hints and uses Algorithm 4 (Function Tstep) to translate single hints (respectively, to determine that a candidate hint is erroneous and should not be considered). We postpone benchmark evaluations by specifying “−” as benchmark parameter for Tstep. If successful at translating the current hint into a formula (i.e., \(f\ne -\)), Algorithm 6 assigns a weight (Line 18). Weights are chosen from a discrete set W of possibilities and are assigned by the learning agent (Function ChooseAction). Finally, the algorithm assembles a set \(H_w\) of weighted tuning hints.Algorithm 7Evaluate set of weighted tuning hints on benchmark.Full size imageNext, we assemble one or several configurations to evaluate, using weighted hints. Algorithm 7 chooses and evaluates configurations, using weighted hints as input. It iterates over parameters mentioned in hints (loop from Line 23–30) and selects a limited number of n values to try (n is a tuning parameter). Values are selected in order to cover the range of suggested values (in hints) as well as possible. We choose values iteratively (loop from Line 26–29). We want to cover values proposed in hints as closely as possible in the following sense. Given a distance function \(\delta \) comparing values for the same parameter, our goal is to minimize the maximal, weighted distance between a value proposed in a hint and the closest selected value. Function MaxDist calculates the latter metric, given a weighted set V of values and a set of selected configurations C. We select values greedily, minimizing the aforementioned cost function in each stepFootnote 4. Note that some tuning knobs can only be set to specific values within their value range (e.g., MySQL’s innodb_buffer_pool_size must be a multiple of the chunk size [31]). We cannot simply average proposed values.Example 4   Assume we collect hints recommending the following values for parameter shared_buffers: 1 GB with weight 1, 2 GB with weight 8, and 8 GB with weight 1. When selecting 1 GB, we obtain maximal weighted distance of \(8\cdot |2-1|=8\) GB from value 2 GB (only distance \(1\cdot |8-1|=7\) GB from 8 GB). Selecting 2 GB yields a maximal weighted distance of 6 GB from value 8 GB. Selecting 8 GB yields a maximal weighted distance of 48 GB from value 2 GB. Hence, we select value 2 GB first. Next, we select value 8 GB to minimize the maximal distance of the remaining values to 1 GB.Finally, we compose selected values for each parameter into n configurations (Line 32). Function Evaluate evaluates selected configurations on the given benchmark b. It assigns a penalty for configurations that are not accepted by the DBMS and, otherwise, calculates reward based on benchmark performance (we use the reward function introduced in Sect. 6.1). Function EvalWeighted returns the maximal reward obtained by any configuration.8 Zero-shot variantThe DB-BERT approach presented so far requires expensive training, specifically for the scenario of database tuning. As new variants of language models appear, this training would have to be repeated. Equally, if targeting different types of tuning text documents, re-training may be necessary for optimal performance. This motivates the “zero-shot” variant presented next. This variant differs from the main version by avoiding scenario-specific training. Instead, it maps the problem of extracting tuning hints into standard problems from the NLP domain such as question answering [36].8.1 Extracting hintsAlgorithm 8Extract tuning hints from text documents, using zero-shot methods.Full size imageAlgorithm 8 shows the algorithm used for analyzing tuning documents. Different from the prior variant of DB-BERT, this algorithm handles hint extraction and hint translation together. This means that reinforcement learning is not used anymore to translate text into tuning recommendations. Given a text passage, parameter, and system properties, Algorithm 8 first extracts parameters that relate to the input text. Again, parameter references may be either explicit (i.e., the name of the parameter is explicitly mentioned) or implicit (i.e., the text describes a desired effect while the name of the parameter has to be inferred).Next, Algorithm 8 iterates over all extracted parameter names. For each parameter, the algorithm generates a question (using a simple template). The purpose of that question is to extract recommended settings for the current parameter. To answer this question, given the tuning text as context, standard methods from the NLP area can be used. The call to sub-function QA represents the invocation of a model for question answering, pre-trained, for instance, on the SQUAD benchmark [36] (or pre-trained on tasks such as text completion which enable the latest generation of language models to solve question answering tasks with a high precision [4]).Given the lack of task-specific training for database tuning, the chance for spurious extraction may increase. Hence, Algorithm 8 filters answers using a confidence threshold \(\theta \), comparing \(\theta \) to the confidence score returned for the answer by the question answering model. Assuming that the answer confidence exceeds the threshold, a numerical value is extracted from the recommendation text. This value is either an absolute recommendation or a relative one, referring to system properties such as the amount of RAM, the amount of disk space, or the number of CPU cores. Given a collection of named system properties, the algorithm uses zero-shot classification to compare the recommendation text to a set of text labels (e.g., “RAM”, “disk”, “cores”). Here, language processing methods typically compare embedding vectors of text labels and a sample to find the class with minimal distance in the embedding vector space. Based on the results of zero-shot classification, the algorithm adapts the extracted value by multiplying with the system property value, associated with the result class.Finally, the resulting tuning hint is added to the result set, returned by Algorithm 8. Note that tuning hints do not refer to the source text anymore as no further text processing takes place after extraction (different from the prior variant which iteratively translates tuning hints into formulas).8.2 Learning configurationsIn this variant of DB-BERT, tuning hints are already translated as part of pre-processing. Hence, the number of decisions made via reinforcement learning reduces. Similarly to before, the algorithm iterates over tuning hints that are sorted using any of the simple heuristic discussed previously. For each of those hints, the reinforcement learning algorithm makes the following decisions:"
12,Select a multiplicative factor out of a given set of discrete alternatives. This factor adapts the recommended value by going either above or below the raw recommendation.
12,"Select a weight for the tuning hint. This weight represent the relative importance of the hint. In case of conflicting recommendations, hints with a higher weight are prioritized, as discussed previously."
12,"While the search space for reinforcement learning changes, the reward function (integrating rewards for appropriate value assignments as well as for performance improvements) remains the same. Similarly to before, weighted hints are aggregated into configurations to try, taking into account the weights associated with different hints for the same parameter.9 ExperimentsWe describe our experimental setup in Sect. 9.1, provide details on the text used for NLP-enhanced database tuning in Sect. 9.2, and details on the training process of all compared algorithms in Sect. 9.3. We compare DB-BERT against various baselines in Sect. 9.4 and study the impact of text document size, data size, and various DB-BERT features on performance in Sect. 9.5.9.1 Experimental setupWe compare approaches for tuning system configuration parameters for MySQL 8.0 and PostgreSQL 13.2. We consider all numerical and Boolean tuning parameters that those systems offer: 232 parameters for PostgreSQL and 266 parameters for MySQL. We use TPC-H with scaling factors one (Sect. 9.4) and ten (Sect. 9.5) and TPC-C with scaling factor 20 as benchmarks. For TPC-C, we use ten terminals, unlimited arrival rate, and 60 s for both, warmup and measurement time (for each trial run). Besides those parameters, we use the default TPC-C configurations for PostgreSQL and MySQL from the OLTP benchmark.Footnote 5 In addition to the two TPC benchmarks, we tune for the Join Order Benchmark (JOB) [22]. This benchmark is unusual in that it is designed to challenge the query optimizer in particular. Different from the TPC benchmarks, it has been proposed recently and encountering specialized tuning recommendations for this benchmark on the Web is unlikely. We use all queries of JOB on PostgreSQL and (as time for processing all queries with the default configuration exceeds the intended tuning time frame) the first 20 queries for MySQL. For the analytical benchmarks, each trial run executes all considered queries. For each tuning scenario and baseline, we execute five runs and allow for 25 min of tuning time per run (prior work uses the same time frame [57]). All experiments execute on a p3.2xlarge EC2 instance with 8 vCPUs, 61 GB of RAM, and a Tesla V100 GPU featuring 16 GB of memory. The EC2 instance uses the Amazon Deep Learning AMI with Ubuntu 18.04.We compare against the DDPG++ algorithm [49] as representative for tuning without NLP-enhancement. We consider different value ranges for tuning parameters, ranging from a factor of two around the default value (i.e., d/2 to \(2\cdot d\) where d is the default) to 100. In the following plots, we only report results for the factor leading to optimal performance at the end of the tuning period. Also, we compare to two baselines described in a recent vision paper on NLP-enhanced database tuning [43]. In the following, Prior-Main denotes the main method proposed by that prior work, based on supervised learning. Also, we compare against a simple baseline, denoted as Prior-Simple, described in the same paper [43]. Furthermore, we compare to several rule-based tuning tools, specialized for the database management systems we use in our evaluation. For PostgreSQL, we use PgTuner,Footnote 6 an online interface that allows users to tune PostgreSQL for specific workload types. In the interface, we provide the precise PostgreSQL version, as well as all relevant hardware properties of the target platform (RAM, disk, and CPUs) as input. For the analytical workloads (TPC-H and JOB), we use tuning recommendations for the data warehouse workload type. For TPC-C, we use recommendations for transactional processing. For MySQL, we use the MySQLTuner.Footnote 7 We obtain recommendations by executing the MySQLTuner tool locally on the target platform. In the following plots, legend entry “Specialized” denotes results for the tuning tool (PgTuner or MySQLTuner) matching the tuned database system.By default, we use the following configuration parameters for DB-BERT. DB-BERT uses reinforcement learning to select multiplicator values and weights for each hint from a fixed set of alternatives. For all experiments, DB-BERT selects the multiplicator from the set \(\{1/4,1/2,1,2,4\}\) and weights from \(\{0,2,4,8,16\}\). We use the same number of alternatives (five) in each case. This makes it easier to model the associated environment with OpenAI’s gym framework. We avoid using overly small or large multiplicators (if the optimal parameter value deviates by more than factor four from the proposed value in any direction, the associated hint should be disregarded). The set of weight alternatives allows DB-BERT to disregard hints (by using a weight of zero) as well as to make specific hints up to eight times more important, compared to other hints with non-zero weights. We set l to 10 in order to allow at most ten hints per episode and parameter. We consider at most 50 hints per episode in total (\(e=50\)) and evaluate two configurations per episode (\(n=2\)). DB-BERT splits text documents into segments of length at most 128 tokens.We also evaluate a DB-BERT variant, described in Sect. 8, that parses manuals via a zero-shot approach. This means that no task-specific training is used (unlike for the primary DB-BERT variant). In the following plots, this variant is denoted as DB-BERT0. We use the same settings for all tuning parameters that are shared between DB-BERT and DB-BERT0, except for the parameter determining the number of hints processed per episode. As DB-BERT0 tends to extract less hints than DB-BERT from the same documents, we set the number of hints processed per episode to ten (\(e=10\)). For zero-shot classification, we use a BART model, pre-trained on the MNLI benchmark.Footnote 8 For question answering, we use a Roberta model, pre-trained on the SQUAD benchmark.Footnote 9 For the confidence threshold, we use \(\theta =0.05\).All baselines (with the exception of specialized tuning tools) are implemented in Python 3.7, using Pytorch 1.8.1 and (for the NLP-enhanced tuning baselines) the Huggingface Transformers library [54]. DB-BERT uses Google’s programmable search engine APIFootnote 10 to retrieve text documents. Also, DB-BERT uses the Double Deep Q-Networks [50] implementation from the Autonomous Learning LibraryFootnote 11 as reinforcement learning algorithm.9.2 Tuning text documentsFig. 5Frequency distribution of hints and parameters in the collection of tuning documents for PostgreSQL and MySQLFull size imageDB-BERT comes with a script that retrieves text documents via Google search and transforms them into the input format required by DB-BERT. For most of the following experiments, we use two document collections retrieved via the queries “Postgresql performance tuning hints” (issued on April 11, 2021) and “MySQL performance tuning hints” (issued on April 15, 2021). We included the first 100 Google results for each of the two queries into the corresponding document collection (accounting for a total of 1.3 MB of text for PostgreSQL and 2.4 MB of text for MySQL). The results are diverse and cover blog entries, forum discussions (e.g., on Database Administrators Stack ExchangeFootnote 12), as well as the online manuals from both database systems. We call the document collection for PostgreSQL Pg100 and the one for MySQL Ms100 in the following.Figure 5 shows the distribution of parameter mentions and proposed value assignments in those document collections, generated via DB-BERT’s candidate hint extraction mechanism (see Sect. 5). Clearly, the distribution of hints over documents and parameters is non-uniform. For both database systems, few parameters are mentioned in multiple documents while most parameters are mentioned only in a single document. Similarly, there are a few assignments proposed by multiple sources. On the other side, most value assignments are proposed only once.Table 4 Tuning parameters mentioned in most documents for PostgreSQL and MySQLFull size tableTable 4 shows the most frequently mentioned parameters for both PostgreSQL and MySQL. Parameters related to buffer size (e.g., shared_buffers for PostgreSQL and innodb_buffer_pool_size for MySQL) feature prominently among them. Besides that, parameters related to the degree of parallelism (e.g., the parameter max_parallel_workers_per_gather) or logging (e.g., max_wal_size) are mentioned frequently as well.9.3 TrainingTwo of the compared algorithms, namely DB-BERT and Prior-Main, use training before run time. Prior-Main uses natural language tuning hints, annotated with associated formulas, as training data. We use the same training samples and training parameters as in the prior work [43]. Consistent with the experimental setup in the latter paper, we apply Prior-Main, trained on PostgreSQL samples, to tune MySQL and Prior-Main, trained on MySQL samples, to tune PostgreSQL. The goal is to demonstrate that NLP-enhanced database tuning does not require system-specific, annotated samples.Prior-Main does not support extracting benchmark-specific tuning hints from a fixed document collection, a disadvantage if the same document collection is used for tuning multiple benchmarks. To allow at least some degree of variability, we train the Prior-Main model separately for each of our five benchmark runs. This leads to slightly different extractions in each run. Training Prior-Main on the platform outlined in Sect. 9.1 took 417 s for MySQL samples and 393 for PostgreSQL samples.DB-BERT does not use annotated tuning hints for training. Instead, it uses the database system itself for run time feedback during the training phase. Similar to Prior-Main, we train DB-BERT on Pg100 to tune MySQL and on Ms100 to tune PostgreSQL. We activate the masked mode during training (see Sect. 6), meaning that parameter names are masked. This avoids learning system-specific parameter names (which are useless in our experimental setup) and focuses attention on the sentence structure of tuning hints instead. The reward signal of DB-BERT (see Sects. 6 and 7) combines reward for successfully changing parameter values according to tuning hints (meaning that the corresponding values are valid) and for performance obtained. To measure performance, we use a synthetic database containing two tables with two columns containing consecutive numbers from 1 to 1,000,000. We use a simple count aggregation query joining both tables with an equality predicate. Reward for performance is scaled down by a factor of 100 to avoid specialization to this artificial benchmark (it merely serves to penalize particularly bad configurations such as setting the buffer pool size to a minimal value). Finally, we add a small reward bonus for setting parameter values that are within the same order of magnitude as the default setting (assuming that extreme deviations from default values are possible but less likely). DB-BERT’s training starts from the BERT base model [7] with 110 million parameters. All model parameters are tuned during training.Fig. 6Reward and loss when training DB-BERT on Pg100Full size imageWe trained DB-BERT for 5000 iterations on Pg100 and for 10,000 iterations on Ms100 (due to the larger number of hints in this collection). Training took 43 min for Pg100 and 84 min for Ms100. Figure 6 shows progress for Pg100 as a function of the number of training steps.Note that, in contrast to the primary variant, DB-BERT0 does not require any training.9.4 Comparison with baselinesWe compare DB-BERT against baselines on TPC-H (see Fig. 7), JOB (see Fig. 8 with a logarithmic y-axis), and TPC-C (see Fig. 9). We tune PostgreSQL and MySQL for 25 min per run. We use throughput as optimization metric for TPC-C and execution time for TPC-H. We show performance of the best configuration found (y-axis) as a function of optimization time (x-axis). In these and the following plots, we report the arithmetic average as well as the 20th and 80th percentile of five runs (using error bars to show percentiles). The plots contain one data point per baseline for every 30 s of tuning time (not displaying any data points before results for the first trial run have become available for the corresponding baseline). This means that data points in the plots do not necessarily align with the start and end of trial runs.DDPG++ [49] is a database tuning approach, based on reinforcement learning. It was shown to be competitive with various other state-of-the-art tuning approaches [49]. However, the prior publication evaluates DDPG++ for a few tens of tuning parameters and allocates 150 iterations per tuning session. Here, we consider hundreds of parameters for tuning and aim at a tuning time frame that allows only few iterations. Clearly, within the allocated time frame, DDPG++ does not find solutions of comparable quality to DB-BERT. In particular for TPC-H, DDPG++ often tries parameter changes that decrease performance significantly (e.g., changes to optimizer cost constants triggering different join orders). Hence, performance of the best configuration found remains almost constant for DDPG++ (close to the one achieved via the default configuration, tried before the first iteration). DDPG++ could benefit from specifying parameter-specific value ranges to consider during tuning. For instance, increasing buffer pool size by an order of magnitude, compared to the default settings, is often beneficial. For optimizer cost constants (e.g., random_page_cost in PostgreSQL), doing so is however dangerous. Our goal is to show that such input can be partially substituted by information mined automatically from text.Specializing tuning tools to specific database systems and workload types is another option to avoid costly exploration. We compare to two tuning tools (MySQLTuner and PgTuner) that are specialized to the tuned database systems. Those tuning tools use hard-coded rules to map properties of the target platform and workload to recommended settings. A first advantage of those tools is that they do not require any trial runs with default configuration (hence, they minimize tuning time among all compared tuning tools). Overall, they achieve excellent performance on TPC-H and TPC-C. This is to be expected as those are two of the most popular benchmarks for database management systems. Hence, it can be assumed that the rules used by these tools are optimized to work well for such standard benchmarks. On the other hand, run times on JOB are higher than the optimum by a multiple. This shows that hard-coded tuning rules have limitations when applied to non-standard tuning scenarios.Prior-Simple and Prior-Main are the two most related baselines as both use tuning text as input, similar to DB-BERT. Prior-Simple uses a naive heuristic for translation. Applying this heuristic is fast and Prior-Simple is typically the first baseline to return results. However, it only extracts the recommendation to set checkpoint_completion_target to 0.9 in Pg100 and no recommendations in Ms100. Hence, it does not improve over the default configuration. Prior-Main performs significantly better. Due to small differences in training, extractions differ across different runs, leading to high variance. For instance, for Pg100, Prior-Main is able to extract a tuning hint that recommends setting shared_buffers to 25% of main memory in two out of five runs. This can lead to significant performance improvements, in particular for TPC-H. However, average performance is significantly below the optimum. As Prior-Main classifies all sentences in the document collection before aggregating tuning hints, its run time is significantly higher than the one of Prior-Simple.Both DB-BERT variants find attractive tradeoffs between tuning time and result quality, comparing to generic (i.e., non-specialized) tuning tools. For instance, when tuning for TPC-H, DB-BERT finds settings that lead to significant performance advantages after less than 200 (PostgreSQL), respectively, less than 400 s (MySQL). More precisely, at that point, both DB-BERT variants find settings that increase main memory allocation (e.g., PostgreSQL’s shared_buffers parameter) or increase the degree of parallellism (e.g., PostgreSQL’s max_parallel_workers_per_gather parameter), motivated by hints extracted from text, leading to a significant drop in execution time.Unlike DDPG++, DB-BERT uses tuning text as input that allows identifying the most relevant parameters and candidate values quickly. Compared to Prior-Simple and Prior-Main, it finds significantly better solutions in average. In particular for MySQL, Prior-Main typically fails to find solutions of comparable quality. Furthermore, the time taken by Prior-Main to analyze all documents is typically higher by a factor of two to three, compared to the time until DB-BERT produces a near-optimal solution (i.e., within one percent of DB-BERT’s final optimum). Tables 5 and 6 show configurations found by DB-BERT when tuning PostgreSQL. Despite extracting hints from the same document collection, DB-BERT is able to find benchmark-specific configurations.Compared to system-specific tuning tools, the DB-BERT variants, in particular DB-BERT0, shine on JOB. Hard-coded tuning rules are not flexible enough to optimize performance beyond standard benchmarks. For instance, in the default configuration, PostgreSQL is hampered by sub-optimal choices made by its query optimizer. This is to be expected as the data of JOB is skewed, invalidating assumptions made by the optimizer while estimating cost of candidate plans (e.g., assuming independent predicates). DB-BERT and DB-BERT0 both extract tuning recommendations about PostgreSQL’s effective_cache_size parameter from text. This parameter represents assumptions of the PostgreSQL planner on the size of the disk cache, available for each query. Both DB-BERT variants set this parameter to a value of 64 for trial runs, following recommendations in text. This value is significantly below the default of 524288. The new setting discourages index scans and leads to different query plans. Further analysis shows that changing the setting for this parameter alone reduces execution time approximately by factor two. Therefore, changing this parameter setting seems to have a similar effect as disabling nested loop joins, a change recommended in the original paper introducing JOB [22]. Unlike recommendations to, e.g., increase buffer pool size, hints concerning the effective_cache_size parameter are relatively rare. Enabling DB-BERT to access the “long tail” of tuning recommendations by parsing a large collection of text documents pays off in this case.In most cases, DB-BERT0 achieves similar performance to DB-BERT, despite the lack of a task-specific training phase. When tuning JOB on MySQL, DB-BERT0 even achieves significantly better results. An analysis of the logs shows that DB-BERT takes longer to find interesting parameter settings due to slightly more noisy extractions, causing DB-BERT to waste the initial trial runs with highly sub-optimal parameter settings that do not appear in text. As JOB requires most time per trial run, this delay prevents DB-BERT from trying efficient settings within the tuning time frame. A follow-up analysis shows that DB-BERT finds configurations with comparable performance to DB-BERT0 after around 2,000 s of tuning time. On the other hand, DB-BERT achieves better results when tuning MySQL for TPC-C. Here, DB-BERT0 consistently converges to a configuration that increases the amount of buffer space (innodb_buffer_pool_size) to 1 GB, while using default settings for other parameters. In contrast to that, DB-BERT changes settings for multiple parameters (e.g., parameter max_connections and parameter innodb_flush_log_at_trx_commit) for significantly higher throughput.Altogether, DB-BERT0 is the most robust optimization tool across different systems and benchmarks. More precisely, the relative performance degradation of DB-BERT0 (i.e., the relative increase of run time, compared to the optimum, or relative decrease in throughput, compared to the optimum) never exceeds 34% across all scenarios. The degradation reaches its maximum for TPC-C on MySQL where DB-BERT0 only achieves 66% of the optimal throughput. However, all other baselines experience performance degradations of at least up to 93%, compared to the optimum (e.g., when optimizing JOB on MySQL). System-specific tuning tools work well for standard benchmarks but lack the ability to adapt to less common tuning scenarios. Exploiting both, information gained from text documents as well as information gathered via trial runs, gives DB-BERT advantages over methods that exploit only one of those two sources of information. DB-BERT performs similarly to DB-BERT0 in most cases but is slowed down by noisy text extractions in one of the tuning scenarios.The best configurations for each benchmark and system, considering all runs and all DB-BERT variants, are reported online.Footnote 13Fig. 7Minimal execution time for TPC-H as a function of optimization time for different baselinesFull size imageFig. 8Minimal execution time for JOB as a function of optimization time for different baselinesFull size imageFig. 9Maximal throughput for TPC-C as a function of optimization time for different baselinesFull size image9.5 Further analysisTable 5 PostgreSQL configuration for TPC-H by DB-BERTFull size tableTable 6 PostgreSQL configuration for TPC-C by DB-BERTFull size tableFig. 10Comparison of different DB-BERT variants when optimizing PostgreSQL for TPC-HFull size imageWe study the impact of different factors on tuning performance. First, we compare DB-BERT against two simplified variants in Fig. 10. We compare against a variant of DB-BERT that processes hints in document order (instead of prioritizing them as described in Sect. 5). Also, we compare against a variant that does not consider implicit hints (i.e., only hints where parameter names are explicitly mentioned). Clearly, both simplifications degrade tuning performance on TPC-H. Considering hints in document order prevents DB-BERT from tuning the most relevant parameters first. Discarding implicit hints reduces the total set of available hints.Fig. 11NLP-enhanced database tuning for TPC-H on PostgreSQL with different input text (100 documents with generic hints versus one document with benchmark-specific hints)Full size imageNext, we study the impact of the input text. We replace Pg100, containing hundreds of generic tuning hints, by a single blog post.Footnote 14 This post describes how to tune PostgreSQL specifically for TPC-H. Figure 5 compares performance with different input documents for all NLP-enhanced tuning baselines. While the performance of Prior-Simple does not change with the input text, the performance of Prior-Main degrades as we switch to the smaller document. Prior-Main benefits from large document collections as redundant hints can partially make up for imprecise extractions. For the smaller input document, it does not extract any hints. DB-BERT, however, benefits from more specialized tuning hints. Using benchmark-specific input text, it converges to near-optimal solutions faster and ultimately finds a slightly better solution (using a higher value for the shared_buffers parameter, compared to Table 5, as proposed in the blog entry).Fig. 12Minimal execution time for TPC-H with scaling factor 10 as a function of optimization timeFull size imageFinally, we scale up the data size. Figure 12 reports results for TPC-H with scaling factor 10 (and using the TPC-H specific tuning text).Footnote 15 Compared to Fig. 11, showing results for scaling factor one, it takes longer for DB-BERT to find near-optimal solutions. This is expected, as longer run times per benchmark evaluation reduce the number of DB-BERT’s iterations per time unit. Compared to other baselines, DB-BERT finds significantly better solutions again.10 Conclusion and outlookWe presented DB-BERT, a database tuning system that extracts tuning hints from text documents. Our experiments demonstrate that such hints lead to significantly better tuning results.In future work, we will consider more diverse tuning objectives. Currently, DB-BERT is limited to optimizing metrics such as latency or throughput that can be easily measured. However, there are other, important metrics that are difficult to measure. For instance, many parameters (e.g., the fsync parameter in PostgreSQL) allow increasing performance if willing to accept a small risk of data loss. Database manuals typically contain warnings detailing such risks. We plan to extend DB-BERT to extract information on metrics that are difficult to measure from the manual. Thereby, it can support users in finding parameter settings that maximize performance while complying with constraints on other metrics.The current DB-BERT version only supports extraction for a common but limited class of tuning hints. More precisely, it support absolute value recommendations and relative recommendations that depend on one single system property. In some cases, recommendations link multiple parameters together. For instance, a recommendation such as “We can use the formula below to calculate the optimal work_mem value for the database server: Total RAM \(*\) 0.25 / max_connections”Footnote 16 is not supported. Expanding the scope to such hints requires extensions of the text extraction mechanism as well as of the reinforcement learning approach (since decisions for different parameters are not independent anymore). However, mining such hints seems particularly powerful as discovering dependencies between multiple parameters via trial runs is expensive.Sometimes, tuning hints come with valuable context, restricting their scope to specific scenarios. For instance, the hint “Note that on Windows, large values for shared_buffers aren’t as effective, and you may find better results keeping it relatively low and using the OS cache more instead”Footnote 17 refers to platforms with a Windows operating system. Beyond the operating system, hints may refer to specific hardware platforms (e.g., low parallelism versus high parallelism) or to specific workload types (e.g., analytical versus transactional workloads). For the moment, DB-BERT only exploits such context indirectly, by allowing users to retrieve documents that contain certain keywords (e.g., “analytical workload”). Exploiting context directly would require DB-BERT to recognize context and link it to properties of the current tuning scenario.In summary, while the current DB-BERT version already benefits significantly from parsed tuning hints, we see various avenues for future research and improvements."
12,"Noteshttps://blog.timescale.com/blog/13-tips-to-improve-postgresql-insert-performance/.https://gym.openai.com/.https://huggingface.co/transformers/model_doc/bert.html.While this heuristic may seem simplistic, it can be shown that it finds near-optimal solutions. Consider the reduction of MaxDist as a function of selected values in C (fixing V and assigning MaxDist(\(\emptyset ,V\)) to a large constant). The reduction is sub-modular in the set of selected values, meaning that adding more values shows diminishing returns. As it is also non-negative and monotone (adding values cannot increase the maximal distance), the greedy algorithm corresponds to the algorithm by Nemhauser [30] which guarantees solutions within factor \(1-e^{-1}\) of the optimum.https://github.com/oltpbenchmark/oltpbench.https://pgtune.leopard.in.ua/#.https://github.com/major/MySQLTuner-perl.https://huggingface.co/facebook/bart-large-mnli.https://huggingface.co/deepset/roberta-base-squad2.https://developers.google.com/custom-search.https://github.com/cpnota/autonomous-learning-library.https://dba.stackexchange.com/.https://drive.google.com/drive/folders/1A_1uvjXzCSrXIoyBh4u46LGmZzukyjjH?usp=sharing.http://rhaas.blogspot.com/2016/04/postgresql-96-with-parallel-query-vs.html.Note that execution time for the best configuration increases slightly at the beginning for DDPG10. This cannot happen as long as we consider a single run. However, we average over a smaller set of runs that finished their first evaluation fast for the first data point, while the second data point averages over all runs.https://www.enterprisedb.com/postgres-tutorials/how-tune-postgresql-memory.https://amonrait.freshdesk.com/support/solutions/articles/6000252619-how-to-increase-max-connections.ReferencesAken, D.V., Pavlo, A., Gordon, G.J.: Automatic database management system tuning through large-scale machine learning. In SIGMOD. pp. 1009–1024. (2017)Arora, S., Yang, B., Eyuboglu, S., Narayan, A., Hojel, A., Trummer, I., Re, C.: language models enable simple systems for generating structured views of heterogeneous data lakes. In: PVLDB. vol. 17(2), pp. 92–105. (2023)Basu, D., Lin, Q., Chen, W., Vo, H.T., Yuan, Z., Senellart, P., Bressan, S.: Cost-model oblivious database tuning with reinforcement learning. In: LNCS. vol. 9261, pp. 253–268. (2015)Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. Adv. Neural Inform. Process. Syst. 1877–1901 (2020)Chen, Z., Fan, J., Madden, S., Tang, N.: Symphony: towards natural language query answering over multi-modal data lakes. In CIDR. pp. 1–7. (2023)Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K. Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N. Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: PaLM: Scaling Language Modeling with Pathways. In CoRR. pp. 1–87. (2022) arxiv:2204.02311Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL. pp. 4171–4186. (2019)Ding, B., Das, S., Marcus, R., Wu, W., Chaudhuri, S., Narasayya, V.R.: AI meets AI: Leveraging query executions to improve index recommendations. In SIGMOD. pp. 1241–1258. (2019)Doshi, L., Zhuang, V., Jain, G., Marcus, R., Huang, H., Altinbüken, D., Brevdo, E., Fraser, C.: Kepler: Robust Learning for Parametric Query Optimization. In: Proceedings of the ACM on Management of Data. vol. 1(1), pp. 1–25. (2023)Floridi, L., Chiriatti, M.: GPT-3: its nature, scope, limits, and consequences. Minds Mach. 30(4), 681–694 (2020)Article"
12,Google Scholar
12,"Gatt, A., Krahmer, E.: Survey of the state of the art in natural language generation: core tasks, applications and evaluation. pp. 1–111. (2017) arXiv preprint arXiv:1703.09902Giannakouris, V., Trummer, I.: Building Learned Federated Query Optimizers. In VLDB PhD Workshop (2022)Guo, J., Zhan, Z., Gao, Y., Xiao, Y., Lou, J.-G., Liu, T., Zhang, D.: Towards complex text-to-SQL in cross-domain database with intermediate representation. In ACL. pp. 4524–4535. (2019)Herzig, J., Nowak, P.K., Müller, T., Piccinno, F., Eisenschlos, J.M.: TAPAS: weakly supervised table parsing via pre-training. (2019)Hilprecht, B., Binnig, C., Röhm, U.: Learning a partitioning advisor for cloud databases. In SIGMOD. pp. 143–157. (2020)Howard, J., Ruder, S.: Universal Language Model Fine-tuning for Text Classification. In ACL. pp. 328–339. (2018)Kanellis, K., Alagappan, R., Venkataraman, S.: Too many knobs to tune? Towards faster database tuning by pre-selecting important knobs. In HotStorage 2020 - 12th USENIX Workshop on Hot Topics in Storage and File Systems, co-located with USENIX ATC 2020, pp. 1–8. (2020)Karagiannis, G., Saeed, M., Papotti, P., Trummer, I.: Scrutinizer: a mixed-initiative approach to large-scale. Data-driven claim verification. In: PVLDB. vol. 13(12), pp. 2508–2521. (2020)Kayali, M., Lykov, A., Fountalis, I., Vasiloglou, N., Olteanu, D., Suciu, D.: CHORUS: foundation models for unified data discovery and exploration. In: CoRR. (2023) arxiv:2306.09610Kraska, T., Li, T., Madden, S., Markakis, M., Ngom, A., Wu, Z., Yu, G.X.: Check out the big brain on BRAD: simplifying cloud data processing with learned automated data meshes. In: PVLDB. vol. 16(11), pp. 3293–3301. (2023)Krishnan, S., Yang, Z., Goldberg, K., Hellerstein, J., Stoica, I.: Learning to optimize join queries with deep reinforcement learning. In aiDM. pp. 1–6. (2020)Leis, V., Gubichev, A., Boncz, P., Kemper, A., Neumann, T.: How good are query optimizers, really?. In: PVLDB. vol. 9(3), pp. 204–215. (2015)Li, G., Zhou, X., Cao, L.: AI meets database: AI4DB and DB4AI. In: Proceedings of the ACM SIGMOD International Conference on Management of Data. pp. 2859–2866. (2021)Li, G., Zhou, X., Li, S., Gao, B.: QTune: a QueryAware database tuning system with deep reinforcement learning. In: PVLDB. vol. 12(12), pp. 2118–2130. (2018)Lieber, O., Sharir, O., Lenz, B., Shoham, Y.: Jurassic-1: technical details and evaluation. Technical report (2021)Lin, X.V., Socher, R., Xiong, C.: Bridging textual and tabular data for cross-domain Text-to-SQL semantic parsing. In EMNLP. pp. 4870–4888. (2020)Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson, S., Rocktäschel, T.: A survey of reinforcement learning informed by natural language. In IJCAI. pp. 6309–6317. (2019)Marcus, R., Negi, P., Mao, H., Zhang, C., Alizadeh, M., Kraska, T., Papaemmanouil, O., Tatbul, N.: Neo: a Learned query optimizer. In: PVLDB. vol. 12(11), pp. 1705–1718. (2018)Marcus, T.R., Negi, P., Mao, H., Tatbul, N., Alizadeh, M., Kraska, B.: Making Learned Query Optimization Practical: In ACM SIGMOD Record. vol. 51, pp. 6–13. (2022)Nemhauser, G., Wolsey, L.: Best algorithms for approximating the maximum of a submodular set function. Math. Oper. Res. 3(3), 177–188 (1978)Article"
12,MathSciNet
12,Google Scholar
12,"Oracle. MySQL 8. 0 Reference Manual. 2021Ortiz, J., Balazinska, M., Gehrke, J., Keerthi, S.S.: Learning state representations for query optimization with deep reinforcement learning. In DEEM. (2018)Patibandla, P.: https://amplitude.engineering/how-a-single-postgresql-config-change-improved-slow-query-performance-by-50x-85593b8991b0 (2017)Pavlo, A., Angulo, G., Arulraj, J., Lin, H., Lin, J., Ma, L., Menon, P., Mowry, T. C., Perron, M., Quah, I., Santurkar, S., Tomasic, A., Toor, S., Aken, D. V., Wang, Z., Wu, Y., Xian, R., Zhang, T.: Self-driving database management systems. In CIDR. pp. 1–6. (2017)PERCONA. Tuning PostgreSQL parameters to optimize performance. (2018) https://www.percona.com/blog/2018/08/31/tuning-postgresql-database-parameters-to-optimize-performance/Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuad: 100,000+ questions for machine comprehension of text. In EMNLP. pp. 2383–2392. (2016)Saeed, M., De Cao, N., Papotti, P.: Querying large language models with SQL. In: CoRR. (2023) arxiv:2304.00472Saha, D., Floratou, A., Sankaranarayanan, K., Minhas, U.F., Mittal, A.R., Ozcan, F.: ATHENA: An ontology-driven system for natural language querying over relational data stores. In: VLDB. vol. 9(12), pp. 1209–1220 (2016)Suri, S., Ilyas, I., Re, C., Rekatsinas, T.: Ember: no-code context enrichment via similarity-based keyless joins. In: PVLDB. vol. 15(3), pp. 699–712. (2021)Sutton, R.S., Barto, A.G.: Reinforcement learning, second edition: An introduction. (2018)Tang, N., Fan, J., Li, F., Tu, J., Du, X., Li, G., Madden, S., Ouzzani, M.: Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation. In: PVLDB. vol. 14(8), pp. 1254–1261 (2021)Thorne, J., Yazdani, M., Saeidi, M., Silvestri, F., Riedel, S., Halevy, A.: From natural language processing to neural databases. In: Proceedings of the VLDB Endowment. vol.14(6), pp. 1033–1039. (2021)Trummer, I.: The case for NLP-enhanced database tuning: towards tuning tools that “read the manual”. In PVLDB. (2021)Trummer, I.: CodexDB: synthesizing code for query processing from natural language instructions using GPT-3 codex. In: PVLDB. vol. 15(11), pp. 2921–2928. (2022)Trummer, I.: DB-BERT: a Database Tuning Tool that “Reads the Manual”. In: SIGMOD. pp. 190–203. (2022)Trummer, I.: From BERT to GPT-3 codex: harnessing the potential of very large language models for data management. In: PVLDB. vol. 15(12), pp. 3770–3773. (2022)Trummer, I.: Can large language models predict data correlations from column names?. In: PVLDB. vol. 16(13), pp. 4310–4323. (2023)Trummer, I., Wang, J., Maram, D., Moseley, S., Jo, S., Antonakakis, J.: SkinnerDB: regret-bounded query evaluation via reinforcement learning. In: SIGMOD. pp. 1039–1050. (2019)Van Aken, D., Yang, D., Brillard, S., Fiorino, A., Zhang, B., Bilien, C., Pavlo, A.: An inquiry into machine learning-based automatic configuration tuning services on real-world database management systems. In: PVLDB. vol. 14(7), pp. 1241–1253. (2021)Van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double Q-Learning. In AAAI. pp. 2094–2100. (2016)Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inf. Process. Syst. 5999–6009 (2017)Wang, J., Trummer, I., Basu, D.: UDO: universal database optimization using reinforcement learning. In: PVLDB. vol. 14(13), pp. 3402–3414. (2021)Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Funtowicz, M., Davison, J., Shleifer, S., Platen, P.V., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush, A.M.: Transformers: state-of-the-art natural language processing. pp. 38–45. (2020)Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A.: Transformers: state-of-the-art natural language processing. In EMNLP. pp. 38–45. (2020)Yang, Z., Chandramouli, B., Wang, C., Gehrke, J., Li, Y., Minhas, U.F., Larson, P.Å., Kossmann, D., Acharya, R.: Qd-tree: learning data layouts for big data analytics. In SIGMOD. pp. 193–208. (2020)Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., Zhang, Z., Radev, D.R.: Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018. pp. 3911–3921. (2020)Zhang, J., Liu, Y., Zhou, K., Li, G., Xiao, Z., Cheng, B., Xing, J., Wang, Y., Cheng, T., Liu, L., Ran, M., Li, Z.: An end-to-end automatic cloud database tuning system using deep reinforcement learning. In SIGMOD. pp. 415–432. (2019)Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M.T., Li, X., Lin, X.V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P.S., Sridhar, A., Wang, T., Zettlemoyer, L.: OPT: open pre-trained transformer language models. In: CoRR. (2022) arxiv:2205.01068Zhong, V., Xiong, C., Socher, R.: Seq2SQL: generating structured queries from natural language using reinforcement learning. In: CoRR. pp. 1–12. (2017) arxiv:1709.00103Download referencesAcknowledgementsThis project is supported by NSF CAREER grant IIS-2239326 (“Mining Hints from Text Documents to Guide Automated Database Performance Tuning”).Author informationAuthors and AffiliationsCornell University, Ithaca, NY, USAImmanuel TrummerAuthorsImmanuel TrummerView author publicationsYou can also search for this author in"
12,PubMed Google ScholarCorresponding authorCorrespondence to
12,Immanuel Trummer.Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Rights and permissions
12,Open Access
12,"This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/."
12,"Reprints and permissionsAbout this articleCite this articleTrummer, I. DB-BERT: making database tuning tools “read” the manual."
12,The VLDB Journal
12,"(2023). https://doi.org/10.1007/s00778-023-00831-yDownload citationReceived: 31 January 2023Revised: 19 August 2023Accepted: 12 November 2023Published: 27 December 2023DOI: https://doi.org/10.1007/s00778-023-00831-yShare this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard"
12,Provided by the Springer Nature SharedIt content-sharing initiative
12,KeywordsAutomated database tuningText miningLanguage modelsReinforcement learning
12,Use our pre-submission checklist
12,Avoid common mistakes on your manuscript.
12,Advertisement
12,Search
12,Search by keyword or author
12,Search
12,Navigation
12,Find a journal
12,Publish with us
12,Track your research
12,Discover content
12,Journals A-Z
12,Books A-Z
12,Publish with us
12,Publish your research
12,Open access publishing
12,Products and services
12,Our products
12,Librarians
12,Societies
12,Partners and advertisers
12,Our imprints
12,Springer
12,Nature Portfolio
12,BMC
12,Palgrave Macmillan
12,Apress
12,Your privacy choices/Manage cookies
12,Your US state privacy rights
12,Accessibility statement
12,Terms and conditions
12,Privacy policy
12,Help and support
12,Cancel contracts here
12,134.96.246.2
12,GER Saarland (3000123700)
12,- Nationallizenz Ebooks Medicine (3000137282)
12,- Max Planck Gesellschaft zur Förderung der Wissenschaften e.V. (3902408896)
12,- Nationallizenz Ebooks Chemistry and Materials Science 2005-2008 (3000262839)
12,- Nationallizenz Zeitschriften-Archiv (3000137278)
12,- DEAL DE / Springer Compact Clearingstelle Uni Freiburg _ (3003169293)
12,- MPDL Services gGmbH (3902044458)
12,- Nationallizenz Lecture Notes-Archiv (3000262825)
12,- Universitätsklinikum des Saarlandes HNO-Klinik 6 (3000669453)
12,"- Universität des Saarlandes, Saarländische Universitäts-und Landesbibliothek (3000070524)"
12,© 2024 Springer Nature
14,Improving query performance for RDS for PostgreSQL with Amazon RDS Optimized Reads - Amazon Relational Database ServiceImproving query performance for RDS for PostgreSQL with Amazon RDS Optimized Reads - Amazon Relational Database ServiceAWSDocumentationAmazon RDSUser GuideOverview of RDS Optimized Reads in PostgreSQLUse casesBest practicesUsingMonitoringLimitationsImproving query performance for RDS for PostgreSQL with Amazon RDS Optimized ReadsYou can achieve faster query processing for RDS for PostgreSQL with Amazon RDS Optimized Reads. An
14,RDS for PostgreSQL DB instance or Multi-AZ DB cluster that uses RDS Optimized Reads can achieve up to 50% faster
14,query processing compared to one that doesn't use it.TopicsOverview of RDS Optimized Reads in PostgreSQLUse cases for RDS Optimized ReadsBest practices for RDS Optimized ReadsUsing RDS Optimized ReadsMonitoring DB instances that use RDS Optimized ReadsLimitations for RDS Optimized Reads in PostgreSQL
14,Overview of RDS Optimized Reads in PostgreSQL
14,"Optimized Reads is available by default on RDS for PostgreSQL versions 15.2 and higher, 14.7 and higher, and 13.10 and higher."
14,"When you use an RDS for PostgreSQL DB instance or Multi-AZ DB cluster that has RDS Optimized Reads turned on,"
14,it achieves up to 50% faster query performance using the local Non-Volatile Memory
14,Express (NVMe) based solid state drive (SSD) block-level storage. You can achieve faster
14,query processing by placing the temporary tables that are generated by PostgreSQL on the
14,"local storage, which reduces the traffic to Elastic Block Storage (EBS) over the"
14,network.
14,"In PostgreSQL, temporary objects are assigned to a temporary namespace that drops"
14,automatically at the end of the session.
14,"The temporary namespace while dropping removes any objects that are session-dependent, including schema-qualified objects, such"
14,"as tables, functions, operators, or even extensions."
14,"In RDS for PostgreSQL, the temp_tablespaces parameter is configured for this"
14,temporary work area where the temporary objects are stored.
14,The following queries return the name of the tablespace and its location.
14,postgres=> show temp_tablespaces;
14,temp_tablespaces
14,---------------------
14,rds_temp_tablespace
14,(1 row)
14,The rds_temp_tablespace is a tablespace configured by RDS that points to the NVMe local storage.
14,You can always switch back to Amazon EBS storage by modifying this parameter in the
14,Parameter group using the AWS Management Console to point to any tablespace other
14,"than rds_temp_tablespace. For more information, see"
14,Modifying parameters in a DB parameter group. You can also use the SET
14,command to modify the value of the temp_tablespaces parameter to
14,pg_default at the session level using SET command. Modifying the
14,parameter redirects the temporary work area to Amazon EBS. Switching back to Amazon EBS helps when
14,the local storage for your RDS instance or cluster isn't sufficient to perform a
14,specific SQL operation.
14,postgres=> SET temp_tablespaces TO 'pg_default';
14,SET
14,postgres=> show temp_tablespaces;
14,temp_tablespaces
14,------------------
14,pg_default
14,Use cases for RDS Optimized Reads
14,The following are some use cases that can benefit from Optimized Reads:
14,"Analytical queries that include Common Table Expressions (CTEs), derived"
14,"tables, and grouping operations."
14,Read replicas that handle the unoptimized queries for an application.
14,On-demand or dynamic reporting queries with complex operations such as GROUP
14,BY and ORDER BY that can't always use appropriate indexes.
14,Other workloads that use internal temporary tables.
14,CREATE INDEX or REINDEX operations for sorting.
14,Best practices for RDS Optimized Reads
14,Use the following best practices for RDS Optimized Reads:
14,Add retry logic for read-only queries in case they fail because the instance
14,store is full during the execution.
14,Monitor the storage space available on the instance store with the CloudWatch metric
14,FreeLocalStorage. If the instance store is reaching its limit
14,"because of the workload on the DB instance or Multi-AZ DB cluster, modify it to use a larger DB instance"
14,class.
14,Using RDS Optimized Reads
14,When you provision an RDS for PostgreSQL DB instance with one of the NVMe based DB instance classes in a
14,"Single-AZ DB instance deployment, Multi-AZ DB instance deployment, or Multi-AZ DB cluster deployment,"
14,the DB instance automatically uses RDS Optimized Reads.
14,"For more information about Multi-AZ deployment, see"
14,Configuring and managing a
14,Multi-AZ deployment.
14,"To turn on RDS Optimized Reads, do one of the following:"
14,Create an RDS for PostgreSQL DB instance or Multi-AZ DB cluster using one of the NVMe based DB instance classes. For
14,"more information, see Creating an Amazon RDS DB instance."
14,Modify an existing RDS for PostgreSQL DB instance or Multi-AZ DB cluster to use one of the NVMe based DB instance
14,"classes. For more information, see Modifying an Amazon RDS DB instance."
14,RDS Optimized Reads is available in all AWS Regions where one or more of the DB instance classes with local NVMe SSD storage are supported. For more
14,"information, see DB instance classes."
14,"To switch back to a non-optimized reads RDS instance, modify the DB instance class of your"
14,RDS instance or cluster to the similar instance class that only supports EBS storage for
14,"your database workloads. For example, if the current DB instance class is db.r6gd.4xlarge,"
14,"choose db.r6g.4xlarge to switch back. For more information, see Modifying an"
14,Amazon RDS DB instance.
14,Monitoring DB instances that use RDS Optimized Reads
14,You can monitor DB instances that use RDS Optimized Reads using the following CloudWatch
14,metrics:
14,FreeLocalStorage
14,ReadIOPSLocalStorage
14,ReadLatencyLocalStorage
14,ReadThroughputLocalStorage
14,WriteIOPSLocalStorage
14,WriteLatencyLocalStorage
14,WriteThroughputLocalStorage
14,"These metrics provide data about available instance store storage, IOPS, and throughput. For more information"
14,"about these metrics, see Amazon CloudWatch instance-level metrics for Amazon RDS."
14,"To monitor current usage of your local storage, log in to your database using the"
14,following query:
14,SELECT
14,"spcname AS ""Name"","
14,"pg_catalog.pg_size_pretty(pg_catalog.pg_tablespace_size(oid)) AS ""size"""
14,FROM
14,pg_catalog.pg_tablespace
14,WHERE
14,spcname IN ('rds_temp_tablespace');
14,"For more information about the temporary files and their usage, see Managing"
14,temporary files with PostgreSQL.
14,Limitations for RDS Optimized Reads in PostgreSQL
14,The following limitation apply to RDS Optimized Reads in PostgreSQL:
14,Transactions can fail when the instance store is full.
14,"Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsWorking with read"
14,"replicas for RDS for PostgreSQLImporting data into PostgreSQLDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better."
15,🛩️ Shane Borden's Technology Blog | My Experiences Navigating Technology – All Views Are My Own
15,🛩️ Shane Borden's Technology Blog
15,My Experiences Navigating Technology – All Views Are My Own
15,Menu
15,Skip to content
15,HomeAbout
15,“Row Movement” in PostgreSQL… Is it bad?
15,Leave a reply
15,"In Oracle, right or wrong, I was always taught to try to avoid “row movement” between partitions due to the general thought that the extra workload of a “delete” + “insert” (rewrite of the row) should be avoided due to the extra I/O, index fragmentation and the associated risks of a migrating ROWID in the cases where the app developers might have used it in their code (now that’s a whole other problem)."
15,Oracle didn’t even let you do it by default.
15,"Table by table, you had to explicitly set:"
15,alter table [table name] enable row movement;
15,"Now, you also had to set this to do table reorganizations such as “alter table…. shrink space / shrink space compact” so it wasn’t something unheard of."
15,"However, when a customer recently explained to me that they were going to partition a PostgreSQL table and update the partition key column from null to the date when the row got processed, my mind immediately went to the space of that’s probably bad……. RIGHT??"
15,"Well, once I thought about it, maybe it’s not all that bad due to the way MVCC and the subsequent VACUUM operations occur in PostgreSQL."
15,"The only thing I could think of that might be a factor is that you would lose any potential benefit of HOT (Heap-Only-Tuple) updates since the row will no longer be part of the original partition, seeing that partitions in PostgreSQL are just another table."
15,The benefit though is that I could limit my vacuum operations to one single partition and SMALLER table.
15,A plus for this customer.
15,**** Note: An implementation like this does not necessarily follow best practices with regards to partitioning.
15,"That being said, I was attempting to validate the idea with regards to how PostgreSQL MVCC behaves."
15,"That being said, I wanted to at least be able to prove / disprove my thoughts with a demonstration, so off to PostgreSQL we go."
15,First let’s create a simple partitioned table and use pg_partman to help:
15,CREATE TABLE partman_test.partman_partitioned (
15,"id integer not null,"
15,"val varchar(20) not null,"
15,"created_tmstp timestamp not null,"
15,event_tmstp timestamp null)
15,PARTITION BY RANGE (event_tmstp);
15,CREATE INDEX partman_partitioned_ix1 ON partman_test.partman_partitioned (id);
15,"SELECT partman.create_parent( p_parent_table => 'partman_test.partman_partitioned',"
15,"p_control => 'event_tmstp',"
15,"p_type => 'native',"
15,"p_interval=> 'daily',"
15,p_premake => 3);
15,"Now, lets insert some random data using a date randomizer function to spread the data across new partitions:"
15,CREATE OR REPLACE FUNCTION partman_test.random_date(out random_date_entry timestamp) AS $$
15,select current_timestamp(3) + random() * interval '2 days'
15,$$ LANGUAGE SQL;
15,INSERT INTO partman_test.partman_partitioned VALUES (
15,"generate_series(0,10000),"
15,"substr(md5(random()::text), 0,10),"
15,"partman_test.random_date(),"
15,NULL);
15,"And then for demonstration purposes, I will set autovacuum to “off” for all the partitions” and run 100 updates to move the data into random partitions using the following statement:"
15,ALTER TABLE partman_test.partman_partitioned_default SET (autovacuum_enabled = false);
15,ALTER TABLE partman_test.partman_partitioned_p2023_09_05 SET (autovacuum_enabled = false);
15,ALTER TABLE partman_test.partman_partitioned_p2023_09_06 SET (autovacuum_enabled = false);
15,ALTER TABLE partman_test.partman_partitioned_p2023_09_07 SET (autovacuum_enabled = false);
15,do $$
15,declare
15,v_id integer;
15,begin
15,for cnt in 1..100 loop
15,select id
15,FROM partman_test.partman_partitioned
15,WHERE event_tmstp is null
15,LIMIT 1 FOR UPDATE SKIP LOCKED
15,INTO v_id;
15,UPDATE partman_test.partman_partitioned
15,SET event_tmstp = partman_test.random_date()
15,WHERE id = v_id and event_tmstp is null;
15,commit;
15,end loop;
15,end; $$;
15,"Once the updates finish, let’s look at the vacuum stats:"
15,relname
15,|autovac_enabled|live_tup|dead_dup|hot_upd|mod_since_stats|ins_since_vac|
15,--------------------------------------------+---------------+--------+--------+-------+---------------+-------------+
15,partman_test.partman_partitioned
15,|true
15,partman_test.partman_partitioned_default
15,|false
15,100|
15,100|
15,partman_test.partman_partitioned_p2023_09_05|false
15,10|
15,10|
15,10|
15,partman_test.partman_partitioned_p2023_09_06|false
15,52|
15,52|
15,52|
15,partman_test.partman_partitioned_p2023_09_07|false
15,38|
15,38|
15,38|
15,Extension “pg_stattuple” confirms that dead tuples only exist in the “default” partition.
15,The reason as to why the numbers don’t match pg_stat_all_tables is a discussion for another day:
15,table_len|tuple_count|tuple_len|tuple_percent|dead_tuple_count|dead_tuple_len|dead_tuple_percent|free_space|free_percent|
15,---------+-----------+---------+-------------+----------------+--------------+------------------+----------+------------+
15,524288|
15,9901|
15,475248|
15,90.65|
15,82|
15,3936|
15,0.75|
15,3308|
15,0.63|
15,8192|
15,10|
15,560|
15,6.84|
15,0.0|
15,7564|
15,92.33|
15,8192|
15,52|
15,2912|
15,35.55|
15,0.0|
15,5044|
15,61.57|
15,8192|
15,38|
15,2128|
15,25.98|
15,0.0|
15,5884|
15,71.83|
15,"So, we definitely proved that we didn’t get the benefit of HOT updates, but due to the MVCC model of PostgreSQL, the update becomes just like any other non-HOT update."
15,This is due to the fact that the updated row is behaving as if it had an index on the row (primary cause of a non-HOT update and sometimes common) and the rest of the MVCC model is just behaving as it would anyway.
15,"I did want to validate with one more tool, but unfortunately the extension, “pg_walinspect” was not installed on this CloudSQL for Postgres instance so I was unable to use it."
15,What about locks?
15,We do get additional locks to manage because we are effecting two partitions instead of one (but they are all fastpath locks):
15,"(postgres@10.3.0.31:5432) [tpcc] > select locktype, database, relation::regclass, page, tuple, pid, mode,granted,fastpath, waitstart from pg_locks;"
15,locktype
15,| database |
15,relation
15,| page | tuple |
15,pid
15,mode
15,| granted | fastpath | waitstart
15,---------------+----------+------------------------------------+------+-------+--------+------------------+---------+----------+-----------
15,relation
15,69323 | partman_partitioned_p2023_09_05
15,| NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,relation
15,69323 | partman_partitioned_default_id_idx | NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,relation
15,69323 | partman_partitioned_default
15,| NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,relation
15,69323 | partman_partitioned
15,| NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,If we were to have no row movement between partitions there is a slightly lesser amount of locks to manage:
15,"(postgres@10.3.0.31:5432) [tpcc] > select locktype, database, relation::regclass, page, tuple, pid, mode,granted,fastpath, waitstart from pg_locks;"
15,locktype
15,| database |
15,relation
15,| page | tuple |
15,pid
15,mode
15,| granted | fastpath | waitstart
15,---------------+----------+----------------------------------------+------+-------+--------+------------------+---------+----------+-----------
15,relation
15,69323 | partman_partitioned_p2023_09_05_id_idx | NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,relation
15,69323 | partman_partitioned_p2023_09_05
15,| NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,relation
15,69323 | partman_partitioned
15,| NULL |
15,NULL | 129825 | RowExclusiveLock | t
15,| t
15,| NULL
15,"Also, be aware that you may need to pay special attention to the vacuum operations and settings of the default partition as this type of operation may cause some significant bloat over time."
15,"However, one positive is that the bloat will be contained to one and only one partition."
15,One last caveat that comes to mind.
15,Be sure that either you specify the partition key or explicitly update the “default” partition in your query because otherwise you would get a multiple partition scan which could cause other performance and locking issues.
15,Enjoy!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, partitioning, postgres, PostgreSQL, SQL and tagged GoogleCloud, partitioning, postgres, PostgreSQL on September 5, 2023 by sborden76."
15,Using the “hint_plan” Table Provided by the PostgreSQL Extension “pg_hint_plan”
15,Leave a reply
15,Introduction
15,"For those who have worked with Oracle, the pg_hint_plan extension is one that will allow you to hint plans in patterns that you are likely very familiar with:"
15,sql_patch
15,sql_profile
15,sql_plan_baselines
15,"While currently, the functionality provided by pg_hint_plan is not nearly as robust (hints list), it does provide most of what you would encounter day to day as a DBA. That being said, one thing that is currently missing is the ability to easily add hints without changing code via stored_procedures / functions like in Oracle. The only way to currently do this in Open Source PostgreSQL is to manually manipulate a table named “hints” typically located in the “hint_plan” schema."
15,The “hints” table which is provided by the extension is highly dependent (just like Oracle) on a normalized SQL statement.
15,"A normalized SQL statement in PostgreSQL is one that has all carriage returns removed, all spaces converted to single spaces and all literals and parameters replaced with a “?”."
15,"Typically you have to do this manually, but in this blog post, I am going to show how I have leveraged entries in “pg_stat_statements” along with custom written functions to normalize the statement and place it into the “hints” table."
15,"To use this “hints” table feature, the following setting must be enabled at either the session or system level:"
15,set session pg_hint_plan.enable_hint_table to on;or in the postgresql.conf:pg_hint_plan.enable_hint_table to on;
15,What Does a Normalized Statement Look Like?
15,"Typically, when you receive code from a developer or even code that you work on yourself, you format it in order to to make it human readable and easier to interpret."
15,"For example, you might want your statement to look like this (notice the parameters / literals in the statement:"
15,SELECT
15,"b.bid,"
15,sum(abalance)
15,FROM
15,pgbench_branches b
15,JOIN pgbench_accounts a ON (b.bid = a.bid)
15,WHERE
15,b.bid = 12345
15,AND a.aid BETWEEN 100 AND 200
15,GROUP BY
15,b.bid
15,ORDER BY
15,Now to normalize the statement for use with the “hints” table it needs to look like this:
15,"select b.bid, sum(abalance) from pgbench_branches b join pgbench_accounts a on (b.bid = a.bid) where b.bid = ? and a.aid between ? and ? group by b.bid order by 1;"
15,You can either manually manipulate the statement to get it in this format do this or we can attempt to do it programmatically.
15,I prefer as much as possible to let the system format it for me so I have written a few helper scripts to do this:
15,Helper Queries:
15,"**** Feel free to utilize these functions, however they may contain errors or may not normalize all statements."
15,"They depend on the pg_stat_statements table and if the entire statement will not fit within the query field of that table, then these functions will not produce the correct output."
15,I will also place them on my public github.
15,"If you find any errors or omissions, please let me know. ****"
15,hint_plan.display_candidate_pg_hint_plan_queries
15,"While you can easily select from the “hints” table on your own, this query will show what a normalized statement will look like before loading it to the table."
15,You can leave the “p_query_id” parameter null to return all queries present in the pg_stat_statements in a normalized form or you can populate it with a valid “query_id” and it will return a single normalized statement:
15,CREATE OR REPLACE FUNCTION hint_plan.display_candidate_pg_hint_plan_queries(
15,p_query_id bigint default null
15,"RETURNS TABLE(queryid bigint, norm_query_string text)"
15,LANGUAGE 'plpgsql'
15,COST 100
15,VOLATILE PARALLEL UNSAFE
15,AS $BODY$
15,DECLARE
15,pg_stat_statements_exists boolean := false;
15,BEGIN
15,SELECT EXISTS (
15,SELECT FROM
15,information_schema.tables
15,WHERE
15,table_schema LIKE 'public' AND
15,table_type LIKE 'VIEW' AND
15,table_name = 'pg_stat_statements'
15,) INTO pg_stat_statements_exists;
15,IF pg_stat_statements_exists AND p_query_id is not null THEN
15,RETURN QUERY
15,"SELECT pss.queryid,"
15,substr(regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,"regexp_replace(pss.query, '\$\d+', '?', 'g'),"
15,"E'\r', ' ', 'g'),"
15,"E'\t', ' ', 'g'),"
15,"E'\n', ' ', 'g'),"
15,"'\s+', ' ', 'g') || ';',1,100)"
15,FROM pg_stat_statements pss where pss.queryid = p_query_id;
15,ELSE
15,RETURN QUERY
15,"SELECT pss.queryid,"
15,substr(regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,"regexp_replace(pss.query, '\$\d+', '?', 'g'),"
15,"E'\r', ' ', 'g'),"
15,"E'\t', ' ', 'g'),"
15,"E'\n', ' ', 'g'),"
15,"'\s+', ' ', 'g') || ';',1,100)"
15,FROM pg_stat_statements pss;
15,END IF;
15,END;
15,$BODY$;
15,If our candidate query was this:
15,"select queryid, query from pg_stat_statements where queryid ="
15,-8949523101378282526;
15,queryid
15,query
15,----------------------+-----------------------------
15,"-8949523101378282526 | select b.bid, sum(abalance)+"
15,| from pgbench_branches b
15,| join pgbench_accounts a
15,| on (b.bid = a.bid)
15,| where b.bid = $1
15,| group by b.bid
15,| order by 1
15,(1 row)
15,The display function would return the following normalized query:
15,SELECT hint_plan.display_candidate_pg_hint_plan_queries(p_query_id => -8949523101378282526);
15,-[ RECORD 1 ]--------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,"display_candidate_pg_hint_plan_queries | (-8949523101378282526,""select b.bid, sum(abalance) from pgbench_branches b join pgbench_accounts a on (b.bid = a.bid) where b.bid = ? group by b.bid order by 1;"")"
15,You can then verify that the query is normalized properly and then move on toward using the next function to add the normalized query to the “hints” table.
15,hint_plan.add_stored_pg_hint_plan
15,"Using the same query in the previous section, we will now add it to the “hints” table."
15,This is where it is important to understand what hint you want to add.
15,CREATE OR REPLACE FUNCTION hint_plan.add_stored_pg_hint_plan(
15,"p_query_id bigint,"
15,"p_hint_text text,"
15,p_application_name text default ''
15,RETURNS varchar
15,LANGUAGE 'plpgsql'
15,COST 100
15,VOLATILE PARALLEL UNSAFE
15,AS $BODY$
15,-- p_hint_text can contain one or more hints either separated by a space or
15,-- a carriage return character.
15,Examples include:
15,-- Space Separated: SeqScan(a) Parallel(a 0 hard)
15,-- ASCII CRLF Separated: SeqScan(a)'||chr(10)||'Parallel(a 0 hard)
15,-- Single Hint: SeqScan(a)
15,-- Escaped text does not work: /* E'SeqScan(a)\nParallel(a 0 hard)'
15,DECLARE
15,hint_id hint_plan.hints.id%TYPE;
15,normalized_query_text hint_plan.hints.norm_query_string%TYPE;
15,pg_stat_statements_exists boolean := false;
15,BEGIN
15,SELECT EXISTS (
15,SELECT FROM
15,information_schema.tables
15,WHERE
15,table_schema LIKE 'public' AND
15,table_type LIKE 'VIEW' AND
15,table_name = 'pg_stat_statements'
15,) INTO pg_stat_statements_exists;
15,IF NOT pg_stat_statements_exists THEN
15,"RAISE NOTICE 'pg_stat_statements extension has not been loaded, exiting';"
15,RETURN 'error';
15,ELSE
15,SELECT regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,regexp_replace(
15,"regexp_replace(query, '\$\d+', '?', 'g'),"
15,"E'\r', ' ', 'g'),"
15,"E'\t', ' ', 'g'),"
15,"E'\n', ' ', 'g'),"
15,"'\s+', ' ', 'g') || ';'"
15,INTO normalized_query_text
15,FROM pg_stat_statements where queryid = p_query_id;
15,IF normalized_query_text IS NOT NULL THEN
15,"INSERT INTO hint_plan.hints(norm_query_string, application_name, hints)"
15,"VALUES (normalized_query_text,"
15,"p_application_name,"
15,p_hint_text
15,SELECT id into hint_id
15,FROM hint_plan.hints
15,WHERE norm_query_string = normalized_query_text;
15,RETURN cast(hint_id as text);
15,ELSE
15,"RAISE NOTICE 'Query ID %q does not exist in pg_stat_statements', cast(p_query_id as text);"
15,RETURN 'error';
15,END IF;
15,END IF;
15,END;
15,$BODY$;
15,Hint text contain one or more hints either separated by a space or a carriage return character. Examples include:
15,Space Separated: SeqScan(a) Parallel(a 0 hard)
15,ASCII CRLF Separated: SeqScan(a)’||chr(10)||’Parallel(a 0 hard)
15,Single Hint: SeqScan(a)
15,Escaped text does not work in the context of this function although this can be used if you are inserting manually to the “hints” table: E’SeqScan(a)\nParallel(a 0 hard)’
15,"SELECT hint_plan.add_stored_pg_hint_plan(p_query_id => -8949523101378282526,"
15,"p_hint_text => 'SeqScan(a) Parallel(a 0 hard)',"
15,p_application_name => '');
15,-[ RECORD 1 ]-----------+---
15,add_stored_pg_hint_plan | 28
15,Time: 40.889 ms
15,select * from hint_plan.hints where id = 28;
15,-[ RECORD 1 ]-----+------------------------------------------------------------------------------------------------------------------------------------------
15,| 28
15,"norm_query_string | select b.bid, sum(abalance) from pgbench_branches b join pgbench_accounts a on (b.bid = a.bid) where b.bid = ? group by b.bid order by 1;"
15,application_name
15,hints
15,| SeqScan(a) Parallel(a 0 hard)
15,"In the above example, we are forcing a serial sequential scan of the “pgbench_accounts”."
15,We left the “application name” parameter empty so that the hint applies to any calling application.
15,hint_plan.delete_stored_pg_hint_plan
15,"You could easily just issue a delete against the “hints” table, but in keeping with utilizing a “function” approach to utilizing this functionality, a delete helper has also been developed:"
15,CREATE OR REPLACE FUNCTION hint_plan.delete_stored_pg_hint_plan(
15,p_hint_id bigint
15,"RETURNS TABLE(id integer, norm_query_string text, application_name text, hints text)"
15,LANGUAGE 'plpgsql'
15,COST 100
15,VOLATILE PARALLEL UNSAFE
15,AS $BODY$
15,BEGIN
15,RETURN QUERY
15,DELETE FROM hint_plan.hints h WHERE h.id = p_hint_id RETURNING *;
15,END;
15,$BODY$;
15,To delete a plan you can call the procedure as follows:
15,SELECT hint_plan.delete_stored_pg_hint_plan(p_hint_id => 28);
15,-[ RECORD 1 ]--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,"delete_stored_pg_hint_plan | (28,""select b.bid, sum(abalance) from pgbench_branches b join pgbench_accounts a on (b.bid = a.bid) where b.bid = ? group by b.bid order by 1;"","""",""SeqScan(a) Parallel(a 0 hard)"")"
15,Time: 33.685 ms
15,select * from hint_plan.hints where id = 28;
15,(0 rows)
15,Time: 24.868 ms
15,As you can see the “hints” table is very useful and can help you emulate many parts of SQL Plan Management just like in Oracle. Enjoy and all feedback is welcomed!!!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, Performance Tuning, pg_hint_plan, postgres, PostgreSQL, SQL, SQL Plan Management and tagged AlloyDB, CloudSQL, dailyprompt, dailyprompt-1899, GoogleCloud, Performance Tuning, pg_hint_plan, postgres, PostgreSQL, SQL Plan Management on April 6, 2023 by sborden76."
15,Leverage Google Cloud Logging + Monitoring for Custom Cloud SQL for Postgres or AlloyDB Alerts
15,1 Reply
15,"As migrations to CloudSQL and AlloyDB pick up speed, inevitably you will run into a condition where the cloud tooling has not quite caught up with exposing custom alerts and incidents that you may be exposing on-premises with tools such as Nagios or Oracle Enterprise Manager."
15,One such example is monitoring of replication tools such as the GoldenGate Heartbeat table.
15,"While there are many ways that you may be able to implement this, I wanted to demonstrate a way to leverage Google Cloud Logging + Google Cloud Monitoring."
15,Using this method will allow us to keep a long term log of certain parameters like lag or anything else you have built into the heartbeat mechanism.
15,"To demonstrate, lets use Python to query the database and create a Cloud Logging Entry:"
15,import argparse
15,"from datetime import datetime, timedelta"
15,"from sqlalchemy import create_engine, text"
15,from google.cloud import logging
15,def retrievePgAlert(
15,"username: str,"
15,"password: str,"
15,"hostname: str,"
15,"portNumber: int,"
15,"databaseName: str,"
15,"alertType: str,"
15,) -> None:
15,alertList: list = []
15,"conn_string = f""postgresql+psycopg2://{username}:{password}@{hostname}:{portNumber}/{databaseName}?client_encoding=utf8"""
15,engine = create_engine(conn_string)
15,with engine.connect() as con:
15,"if alertType == ""ogg-lag"":"
15,sqlQuery = text(
15,"f""select replicat, effective_date, lag from ogg.heartbeat where lag >=:lagAmt and effective_date >= now() - interval ':intervalAmt min'"""
15,result = con.execute(
15,"sqlQuery, {""lagAmt"": oggLagAmt, ""intervalAmt"": checkIntervalMinutes}"
15,).fetchall()
15,for row in result:
15,alertList.append(row)
15,if not alertList:
15,"print(f""No alerts as of {datetime.now().strftime('%m/%d/%Y %H:%M:%S')}"")"
15,else:
15,for alertText in alertList:
15,print(
15,"f""Replicat: {alertText[0]} at date {alertText[1]} has a total lag of: {alertText[2]} seconds"""
15,writeGcpCloudLoggingAlert(
15,"logger_alert_type=alertType,"
15,"loggerName=args.loggerName,"
15,"logger_message=alertList,"
15,con.close()
15,engine.dispose()
15,def writeGcpCloudLoggingAlert(
15,"logger_alert_type: str,"
15,"loggerName: str,"
15,"logger_message: list,"
15,) -> None:
15,# Writes log entries to the given logger.
15,logging_client = logging.Client()
15,# This log can be found in the Cloud Logging console under 'Custom Logs'.
15,logger = logging_client.logger(loggerName)
15,# Struct log. The struct can be any JSON-serializable dictionary.
15,"if logger_alert_type == ""ogg-lag"":"
15,replicatName: str
15,effectiveDate: datetime
15,lagAmount: str
15,for alertFields in logger_message:
15,replicatName = alertFields[0]
15,effectiveDate = alertFields[1]
15,lagAmount = int(alertFields[2])
15,logger.log_struct(
15,"""alertType"": logger_alert_type,"
15,"""replicat"": str(alertFields[0]),"
15,"""alertDate"": alertFields[1].strftime(""%m/%d/%Y, %H:%M:%S""),"
15,"""alertRetrievalDate"": datetime.now().strftime(""%m/%d/%Y, %H:%M:%S""),"
15,"""lagInSeconds"": int(alertFields[2]),"
15,"severity=""ERROR"","
15,"print(""Wrote logs to {}."".format(logger.name))"
15,def delete_logger(loggerName):
15,"""""""Deletes a logger and all its entries."
15,Note that a deletion can take several minutes to take effect.
15,""""""""
15,logging_client = logging.Client()
15,logger = logging_client.logger(loggerName)
15,logger.delete()
15,"print(""Deleted all logging entries for {}"".format(logger.name))"
15,"if __name__ == ""__main__"":"
15,"cloudSQLHost: str = ""127.0.0.1"""
15,hostname: str
15,portNumber: str
15,database: str
15,username: str
15,password: str
15,oggLagAmt: int = 15
15,checkIntervalMinutes: int = 20
15,"with open(""~/.pgpass"", ""r"") as pgpassfile:"
15,for line in pgpassfile:
15,"if line.strip().split("":"")[0] == cloudSQLHost:"
15,"hostname, portNumber, database, username, password = line.strip().split("
15,""":"""
15,parser = argparse.ArgumentParser(
15,"description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter"
15,parser.add_argument(
15,"""-loggerName"","
15,"""--loggerName"","
15,"type=str,"
15,"help=""GCP Cloud Log Namespace"","
15,"default=""postgres-alert"","
15,parser.add_argument(
15,"""-alertType"","
15,"""--alertType"","
15,"type=str,"
15,"help=""Type of alert to log"","
15,"default=""ogg-lag"","
15,args = parser.parse_args()
15,"if args.alertType == ""ogg-lag"":"
15,retrievePgAlert(
15,"hostname=hostname,"
15,"username=username,"
15,"password=password,"
15,"portNumber=portNumber,"
15,"databaseName=database,"
15,"alertType=args.alertType,"
15,"In this script we utilize the Google Cloud Logging APIs, SQLAlchemy and some other basic python imports to query the database based on a lag amount we are looking for from the heartbeat table."
15,"***Note: The query within the python code could check for any condition by changing the query, by leveraging “gcloud” commands or REST API calls."
15,"If the condition is met, the script creates a JSON message which is then written to the appropriate Google Cloud Logging Namespace."
15,An example of the JSON message is below (sensitive information like the project id and instance id have been redacted):
15,"""insertId"": ""1b6fb35g18b606n"","
15,"""jsonPayload"": {"
15,"""alertRetrievalDate"": ""01/20/2023, 18:47:20"","
15,"""lagInSeconds"": 15,"
15,"""alertType"": ""ogg-lag"","
15,"""alertDate"": ""01/20/2023, 18:34:55"","
15,"""replicat"": ""r_hr"""
15,"""resource"": {"
15,"""type"": ""gce_instance"","
15,"""labels"": {"
15,"""project_id"": ""[project id]"","
15,"""instance_id"": ""****************"","
15,"""zone"": ""projects/[project id]/zones/us-central1-c"""
15,"""timestamp"": ""2023-01-20T18:47:20.103058301Z"","
15,"""severity"": ""ERROR"","
15,"""logName"": ""projects/[project id]/logs/postgres-alert"","
15,"""receiveTimestamp"": ""2023-01-20T18:47:20.103058301Z"""
15,Create a Cloud Logging Alert
15,"Now that we have published a message to Cloud Logging, what can we do with it?"
15,"Generally there are two paths, either a Cloud Metric or a Cloud Alert."
15,"For this demonstration, we will use the “Cloud Alert”."
15,So to start the setup navigate to the console page “Operations Logging” —> “Logs Explorer”.
15,From there click the “Create alert” function.
15,The following dialog will show.
15,"You will need to double check the query to retrieve the appropriate logs in step 2, and in step 3, you can choose the time between notifications (this is to mute alerts that happen in between the interval) and how long past the last alert an incident will stay open."
15,"In this case, we will mute duplicate alerts that happen for 5 minutes after the first alert (if an alert occurs at 6 minutes another notification will fire) and incidents will remain open for 30 minutes past the last alert (no new incidents will be logged unless an alert occurs after that time frame)."
15,The query to be used within the alert is as follows:
15,"logName=""projects/[project id]/logs/postgres-alert"""
15,"AND severity=""ERROR"""
15,"AND (jsonPayload.alertType = ""ogg-lag"")"
15,AND (jsonPayload.lagInSeconds >= 15)
15,AND resource.labels.instance_id = [instance id]
15,The following dialogues outline the screens used to setup the alert.
15,"The last step will be to choose your notification method, which is managed by different notification channels."
15,The different types of notification channels include:
15,Mobile Devices
15,PagerDuty Services
15,PagerDuty Sync
15,Slack
15,Webhooks
15,E-Mail
15,SMS
15,Pub/Sub
15,"Once all of this is defined, your alert is now set to notify once you place the python script on an appropriate schedule such as linux cron, Google Cloud Scheduler, etc."
15,In this case we will now wait for an issue to occur that conforms to the alert.
15,When it does an email like the following will result to the notification channel:
15,"As your migration to cloud continues, keep an open mind and look for alternative ways to handle all of the operational “things” you are accustomed to in your on-premises environment."
15,Most of the time there is a way in cloud to handle it!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, Cloud Operations, CloudSQL PostgreSQL, Goldengate, Google Cloud, postgres, PostgreSQL, Python and tagged AlloyDB, CloudSQL, GoogleCloud, postgres, PostgreSQL, Python on February 3, 2023 by sborden76."
15,Why is My App Table Scanning in PostgreSQL but not Oracle?
15,Leave a reply
15,My team and I have been working on a lot of migrations off of Oracle and onto Google CloudSQL for Postgres / AlloyDB lately.
15,One of the common things that I have been seeing in my performance tuning / migration activities is that Oracle handles certain application datatypes differently than PostgreSQL.
15,"For my most recent client, they used the “float” datatype extensively in their Java code and unfortunately, the PostgreSQL JDBC driver doesn’t convert that datatype like the Oracle JDBC driver does."
15,The result of the datatype mismatch ends up as a full table scan in PostgreSQL whereas in Oracle it was using an index.
15,*** Note:
15,This client did not use an ORM within their code.
15,"While I still need to test it, I am hopeful that this same issue will not manifest itself when using an ORM like SQLAlchemy (Python) or Hibernate (Java)."
15,"While in Oracle numbers are usually stored in the numeric datatype, you have many options within PostgreSQL to do the same thing:"
15,numeric (x)
15,"numeric (x,y)"
15,numeric
15,smallint
15,bigint
15,int
15,Each serve a purpose and should be used with careful analysis.
15,"That said, don’t forget about the code!"
15,"To demonstrate why, I modified a simple Java test harness I usually use to test connectivity"
15,/ check SSL Encryption functionality during migrations to show what happens if the datatypes are not looked at carefully.
15,"As you can see, in Oracle, the JDBC driver will convert any of the following types to “numeric“, the same query plan is also achieved using index range scans and no table scans."
15,The output is below and Oracle Code is at the bottom of the post:
15,java -classpath /Users/shaneborden/Documents/java/ojdbc11.jar:/Users/shaneborden/Documents/java OracleJdbcTest
15,Password: *******
15,=====
15,Database info =====
15,DatabaseProductName: Oracle
15,DatabaseProductVersion: Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
15,Version 19.15.0.0.0
15,DatabaseMajorVersion: 19
15,DatabaseMinorVersion: 0
15,=====
15,Driver info =====
15,DriverName: Oracle JDBC driver
15,DriverVersion: 21.8.0.0.0
15,DriverMajorVersion: 21
15,DriverMinorVersion: 8
15,=====
15,JDBC/DB attributes =====
15,Supports getGeneratedKeys(): true
15,===== Database info =====
15,===== Query Plan - Cast Int to Numeric =====
15,Plan hash value: 423740054
15,-----------------------------------------------------------------------------------------------------------
15,| Id
15,| Operation
15,| Name
15,| Rows
15,| Bytes | Cost (%CPU)| Time
15,-----------------------------------------------------------------------------------------------------------
15,0 | SELECT STATEMENT
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,1 |
15,TABLE ACCESS BY INDEX ROWID BATCHED| DATATYPE_TEST
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,2 |
15,INDEX RANGE SCAN
15,| DATATYPE_NUMBER_VAL |
15,2 |
15,(0)| 00:00:01 |
15,-----------------------------------------------------------------------------------------------------------
15,Predicate Information (identified by operation id):
15,---------------------------------------------------
15,"2 - access(""NUMBER_VAL""=:1)"
15,===== Query Plan - Cast Long to Numeric =====
15,Plan hash value: 423740054
15,-----------------------------------------------------------------------------------------------------------
15,| Id
15,| Operation
15,| Name
15,| Rows
15,| Bytes | Cost (%CPU)| Time
15,-----------------------------------------------------------------------------------------------------------
15,0 | SELECT STATEMENT
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,1 |
15,TABLE ACCESS BY INDEX ROWID BATCHED| DATATYPE_TEST
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,2 |
15,INDEX RANGE SCAN
15,| DATATYPE_NUMBER_VAL |
15,2 |
15,(0)| 00:00:01 |
15,-----------------------------------------------------------------------------------------------------------
15,Predicate Information (identified by operation id):
15,---------------------------------------------------
15,"2 - access(""NUMBER_VAL""=:1)"
15,===== Query Plan - Cast Float to Numeric =====
15,Plan hash value: 423740054
15,-----------------------------------------------------------------------------------------------------------
15,| Id
15,| Operation
15,| Name
15,| Rows
15,| Bytes | Cost (%CPU)| Time
15,-----------------------------------------------------------------------------------------------------------
15,0 | SELECT STATEMENT
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,1 |
15,TABLE ACCESS BY INDEX ROWID BATCHED| DATATYPE_TEST
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,2 |
15,INDEX RANGE SCAN
15,| DATATYPE_NUMBER_VAL |
15,2 |
15,(0)| 00:00:01 |
15,-----------------------------------------------------------------------------------------------------------
15,Predicate Information (identified by operation id):
15,---------------------------------------------------
15,"2 - access(""NUMBER_VAL""=:1)"
15,===== Query Plan - Cast Double to Numeric =====
15,Plan hash value: 423740054
15,-----------------------------------------------------------------------------------------------------------
15,| Id
15,| Operation
15,| Name
15,| Rows
15,| Bytes | Cost (%CPU)| Time
15,-----------------------------------------------------------------------------------------------------------
15,0 | SELECT STATEMENT
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,1 |
15,TABLE ACCESS BY INDEX ROWID BATCHED| DATATYPE_TEST
15,2 |
15,28 |
15,(0)| 00:00:01 |
15,2 |
15,INDEX RANGE SCAN
15,| DATATYPE_NUMBER_VAL |
15,2 |
15,(0)| 00:00:01 |
15,-----------------------------------------------------------------------------------------------------------
15,Predicate Information (identified by operation id):
15,---------------------------------------------------
15,"2 - access(""NUMBER_VAL""=:1)"
15,=========================
15,Command successfully executed
15,"However in PostgreSQL, the same statement where we convert “int” to “numeric” and “long” to “Numeric” an index scan is executed, however when casting “float” and “double” datatypes to numeric a table scan results."
15,"Similar behavior is seen when using the other PostgreSQL datatypes such as “smallint”, “bigint” and “int”."
15,The output is shown below and the PostgreSQL Code is located at the bottom of the post:
15,java -classpath /Users/shaneborden/Documents/java/postgresql-42.5.0.jar:/Users/shaneborden/Documents/java PostgresJdbcTest
15,Password: *******
15,=====
15,Database info =====
15,DatabaseProductName: PostgreSQL
15,DatabaseProductVersion: 14.4
15,DatabaseMajorVersion: 14
15,DatabaseMinorVersion: 4
15,=====
15,Driver info =====
15,DriverName: PostgreSQL JDBC Driver
15,DriverVersion: 42.5.0
15,DriverMajorVersion: 42
15,DriverMinorVersion: 5
15,=====
15,JDBC/DB attributes =====
15,Supports getGeneratedKeys(): true
15,===== Database info =====
15,Current Date from Postgres : 2022-12-27 16:25:41.493047-05
15,Client connected pid from Postgres : 17727
15,Postgres DB Unique Name from Postgres : mytpchdb
15,Client connected hostname from Postgres : null
15,Client connected application_name from Postgres : PostgreSQL JDBC Driver
15,===== Query Plan - Cast Int to Numeric =====
15,Index Scan using datatype_test_numeric_decimal on datatype_test
15,(cost=0.43..6.47 rows=2 width=28) (actual time=0.057..0.060 rows=2 loops=1)
15,Index Cond: (numeric_val = '10001'::numeric)
15,Planning Time: 0.389 ms
15,Execution Time: 0.085 ms
15,===== Query Plan - Cast Long to Numeric =====
15,Index Scan using datatype_test_numeric_decimal on datatype_test
15,(cost=0.43..6.47 rows=2 width=28) (actual time=0.011..0.013 rows=2 loops=1)
15,Index Cond: (numeric_val = '10001'::numeric)
15,Planning Time: 0.126 ms
15,Execution Time: 0.027 ms
15,===== Query Plan - Cast Float to Numeric =====
15,Seq Scan on datatype_test
15,(cost=0.00..233334.01 rows=50000 width=28) (actual time=1050.733..2622.224 rows=2 loops=1)
15,Filter: ((numeric_val)::double precision = '10001'::real)
15,Rows Removed by Filter: 9999999
15,Planning Time: 0.094 ms
15,Execution Time: 2622.273 ms
15,===== Query Plan - Cast Double to Numeric =====
15,Seq Scan on datatype_test
15,(cost=0.00..233334.01 rows=50000 width=28) (actual time=1055.081..2629.634 rows=2 loops=1)
15,Filter: ((numeric_val)::double precision = '10001'::double precision)
15,Rows Removed by Filter: 9999999
15,Planning Time: 0.096 ms
15,Execution Time: 2629.660 ms
15,"As you can see, its very important to also ensure that your datatypes within your code are fully compliant with the destination RDBMS."
15,The “double” and the “float” types within the Java code cause a table scan!
15,"While Oracle has become very forgiving with that over the years, PostgreSQL just isn’t there yet and you need to make sure that you adjust your code accordingly!"
15,Code Samples:
15,Jar / Java Requirements:
15,openjdk 11
15,postgresql-42.5.0.jar
15,ojdbc11.jar
15,Compile:
15,To compile the code:Oracle: javac OracleJdbcTest.javaPostgres: javac PostgresJdbcTest.java
15,Oracle Java Test Harness:
15,Note: To execute the code follow the instructions in the comment block at the top of the code.
15,The instructions to create the sample table objects are also contained within the same comment block.
15,import java.sql.Connection;
15,import java.sql.DriverManager;
15,import java.sql.PreparedStatement;
15,import java.sql.ResultSet;
15,import java.sql.SQLException;
15,import java.util.Properties;
15,import java.sql.*;
15,* Simple Java Program to connect Oracle database by using Oracle JDBC thin driver
15,* Make sure you have Oracle JDBC thin driver in your classpath before running this program
15,* @author
15,javac OracleJdbcTest.java
15,java -classpath /Users/shaneborden/Documents/java/ojdbc11.jar:/Users/shaneborden/Documents/java OracleJdbcTest
15,Setup:
15,CREATE TABLE tc.datatype_test (
15,"number_decimal_val number(12,2),"
15,"number_val number(12),"
15,random_val number(4))
15,TABLESPACE USERS;
15,CREATE SEQUENCE tc.datatype_test_seq
15,START WITH
15,INCREMENT BY
15,NOCACHE
15,NOCYCLE;
15,BEGIN
15,FOR i in 1 .. 1000000
15,LOOP
15,INSERT INTO tc.datatype_test VALUES (
15,"tc.datatype_test_seq.nextval,"
15,"floor(dbms_random.value(1, 1000000)),"
15,"floor(dbms_random.value(1, 1000)));"
15,END LOOP;
15,END;
15,CREATE INDEX tc.datatype_number_decimal_val on tc.datatype_test(number_decimal_val);
15,CREATE INDEX tc.datatype_number_val on tc.datatype_test(number_val);
15,public class OracleJdbcTest
15,"public static void main(String args[]) throws SQLException, ClassNotFoundException"
15,try
15,java.io.Console console = System.console();
15,Boolean dataTypeCheck = true;
15,"String sourceDatatType = ""Numeric"";"
15,"String inputPassword = new String(console.readPassword(""Password: ""));"
15,Integer intQueryParam = 10001;
15,Long longQueryParam = 10001L;
15,Float floatQueryParam = 10001f;
15,Double doubleQueryParam = 10001.0;
15,/**Set URL of Oracle database server*/
15,"String url = ""jdbc:oracle:thin:@//192.168.1.105:1521/vboxncdb.localdomain.com"";"
15,"String xPlanSql = ""select * from table(dbms_xplan.display)"";"
15,/** properties for creating connection to Oracle database */
15,Properties props = new Properties();
15,"props.setProperty(""user"", ""datatypeTestUser"");"
15,"props.setProperty(""password"","
15,inputPassword);
15,/** creating connection to Oracle database using JDBC*/
15,"Connection conn = DriverManager.getConnection(url,props);"
15,DatabaseMetaData dbmd = conn.getMetaData();
15,"System.out.println(""====="
15,"Database info ====="");"
15,"System.out.println("""
15,"DatabaseProductName: "" + dbmd.getDatabaseProductName() );"
15,"System.out.println("""
15,"DatabaseProductVersion: "" + dbmd.getDatabaseProductVersion() );"
15,"System.out.println("""
15,"DatabaseMajorVersion: "" + dbmd.getDatabaseMajorVersion() );"
15,"System.out.println("""
15,"DatabaseMinorVersion: "" + dbmd.getDatabaseMinorVersion() );"
15,"System.out.println(""====="
15,"Driver info ====="");"
15,"System.out.println("""
15,"DriverName: "" + dbmd.getDriverName() );"
15,"System.out.println("""
15,"DriverVersion: "" + dbmd.getDriverVersion() );"
15,"System.out.println("""
15,"DriverMajorVersion: "" + dbmd.getDriverMajorVersion() );"
15,"System.out.println("""
15,"DriverMinorVersion: "" + dbmd.getDriverMinorVersion() );"
15,"System.out.println(""====="
15,"JDBC/DB attributes ====="");"
15,if (dbmd.supportsGetGeneratedKeys() )
15,"System.out.println("""
15,"Supports getGeneratedKeys(): true"");"
15,else
15,"System.out.println("""
15,"Supports getGeneratedKeys(): false"");"
15,"System.out.println(""===== Database info ====="");"
15,"String sql = ""with session_data as ("";"
15,"sql = sql + ""select sysdate as current_day,SYS_CONTEXT ('USERENV', 'DB_UNIQUE_NAME') as db_name,SYS_CONTEXT ('USERENV', 'SERVICE_NAME') as service_name, "";"
15,"sql = sql + ""SYS_CONTEXT ('USERENV', 'HOST') as host, SYS_CONTEXT ('USERENV', 'IP_ADDRESS') as ip_address,"
15,"SYS_CONTEXT('USERENV','SID') sid from dual) "";"
15,"sql = sql + ""select sd.current_day, sd.db_name, sd.service_name, sd.host, sd.ip_address, "";"
15,"sql = sql + ""sd.sid, nvl(sci.network_service_banner, 'Traffic Not Encrypted') network_service_banner "";"
15,"sql = sql + ""from session_data sd "";"
15,"sql = sql + ""left join v$session_connect_info sci on (sd.sid = sci.sid) "";"
15,"sql = sql + ""where sci.network_service_banner like '%Crypto-checksumming service adapter%'"";"
15,/** creating PreparedStatement object to execute query*/
15,PreparedStatement preStatement = conn.prepareStatement(sql);
15,ResultSet result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println(""Current Date from Oracle : "" +"
15,"result.getString(""current_day""));"
15,"System.out.println(""Oracle DB Unique Name from Oracle : "" +"
15,"result.getString(""db_name""));"
15,"System.out.println(""Oracle Connected Listener Service Name from Oracle : "" +"
15,"result.getString(""service_name""));"
15,"System.out.println(""Client connected hostname from Oracle : "" +"
15,"result.getString(""host""));"
15,"System.out.println(""Client connected ip_address from Oracle : "" +"
15,"result.getString(""ip_address""));"
15,"System.out.println(""Client connected encryption info from Oracle : "" +"
15,"result.getString(""network_service_banner""));"
15,if (dataTypeCheck)
15,"if (sourceDatatType == ""Numeric122"") {"
15,"sql = ""EXPLAIN PLAN FOR "";"
15,"sql = sql + ""select * from tc.datatype_test where number_decimal_val = ?"";"
15,"} else if (sourceDatatType == ""Numeric"") {"
15,"sql = ""EXPLAIN PLAN FOR "";"
15,"sql = sql + ""select * from tc.datatype_test where number_val = ?"";"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Int to ""+ sourceDatatType +"" ====="");"
15,/** creating PreparedStatement object to execute query*/
15,preStatement = conn.prepareStatement(sql);
15,"preStatement.setInt(1, intQueryParam);"
15,result = preStatement.executeQuery();
15,PreparedStatement xPlanStatement = conn.prepareStatement(xPlanSql);
15,ResultSet xPlanResult = xPlanStatement.executeQuery();
15,while(xPlanResult.next())
15,"System.out.println("""
15,""" + xPlanResult.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Long to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setLong(1, longQueryParam);"
15,result = preStatement.executeQuery();
15,xPlanStatement = conn.prepareStatement(xPlanSql);
15,xPlanResult = xPlanStatement.executeQuery();
15,while(xPlanResult.next())
15,"System.out.println("""
15,""" + xPlanResult.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Float to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setFloat(1, floatQueryParam);"
15,result = preStatement.executeQuery();
15,xPlanStatement = conn.prepareStatement(xPlanSql);
15,xPlanResult = xPlanStatement.executeQuery();
15,while(xPlanResult.next())
15,"System.out.println("""
15,""" + xPlanResult.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Double to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setDouble(1, doubleQueryParam);"
15,result = preStatement.executeQuery();
15,xPlanStatement = conn.prepareStatement(xPlanSql);
15,xPlanResult = xPlanStatement.executeQuery();
15,while(xPlanResult.next())
15,"System.out.println("""
15,""" + xPlanResult.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,conn.close();
15,"System.out.println(""========================="");"
15,"System.out.println(""Command successfully executed"");"
15,catch(SQLException exp) {
15,"System.out.println(""Exception: "" + exp.getMessage());"
15,"System.out.println(""SQL State: "" + exp.getSQLState());"
15,"System.out.println(""Vendor Error: "" + exp.getErrorCode());"
15,PostgreSQL Java Test Harness:
15,Note: To execute the code follow the instructions in the comment block at the top of the code. The instructions to create the sample table objects are also contained within the same comment block.
15,import java.sql.Connection;
15,import java.sql.DriverManager;
15,import java.sql.PreparedStatement;
15,import java.sql.ResultSet;
15,import java.sql.SQLException;
15,import java.util.Properties;
15,import java.sql.*;
15,* Simple Java Program to connect Postgres database by using Postgres JDBC thin driver
15,* Make sure you have Postgres JDBC thin driver in your classpath before running this program
15,* @author
15,java -classpath /Users/shaneborden/Documents/java/postgresql-42.5.0.jar:/Users/shaneborden/Documents/java PostgresJdbcTest
15,Setup:
15,CREATE TABLE datatype_test (
15,"int_val int,"
15,"bigint_val bigint,"
15,"numeric_val numeric(12),"
15,"numeric_decimal_val numeric(12,2),"
15,smallint_val smallint);
15,INSERT INTO datatype_test VALUES (
15,"generate_series(0,10000000),"
15,"generate_series(0,10000000),"
15,"floor(random()*10000000),"
15,"random()*10000000,"
15,floor(random()* (32765-1 + 1) + 1) );
15,SET SESSION max_parallel_maintenance_workers TO 4;
15,SET SESSION maintenance_work_mem TO '2 GB';
15,CREATE INDEX datatype_test_int on datatype_test(int_val);
15,CREATE INDEX datatype_test_bigint on datatype_test(bigint_val);
15,CREATE INDEX datatype_test_numeric on datatype_test(numeric_val);
15,CREATE INDEX datatype_test_numeric_decimal on datatype_test(numeric_val);
15,CREATE INDEX datatype_test_smallint on datatype_test(smallint_val);
15,public class PostgresJdbcTest
15,"public static void main(String args[]) throws SQLException, ClassNotFoundException"
15,try
15,java.io.Console console = System.console();
15,Boolean dataTypeCheck = true;
15,"String sourceDatatType = ""Numeric"";"
15,"String inputPassword = new String(console.readPassword(""Password: ""));"
15,Integer intQueryParam = 10001;
15,Long longQueryParam = 10001L;
15,Float floatQueryParam = 10001f;
15,Double doubleQueryParam = 10001.0;
15,/**Set URL of Postgres database server*/
15,"String url = ""jdbc:postgresql://localhost:6000/mytpchdb"";"
15,/** properties for creating connection to Postgres database */
15,Properties props = new Properties();
15,"props.setProperty(""user"", ""postgres"");"
15,"props.setProperty(""password"","
15,inputPassword);
15,/** creating connection to Postgres database using JDBC*/
15,"Connection conn = DriverManager.getConnection(url,props);"
15,DatabaseMetaData dbmd = conn.getMetaData();
15,"System.out.println(""====="
15,"Database info ====="");"
15,"System.out.println("""
15,"DatabaseProductName: "" + dbmd.getDatabaseProductName() );"
15,"System.out.println("""
15,"DatabaseProductVersion: "" + dbmd.getDatabaseProductVersion() );"
15,"System.out.println("""
15,"DatabaseMajorVersion: "" + dbmd.getDatabaseMajorVersion() );"
15,"System.out.println("""
15,"DatabaseMinorVersion: "" + dbmd.getDatabaseMinorVersion() );"
15,"System.out.println(""====="
15,"Driver info ====="");"
15,"System.out.println("""
15,"DriverName: "" + dbmd.getDriverName() );"
15,"System.out.println("""
15,"DriverVersion: "" + dbmd.getDriverVersion() );"
15,"System.out.println("""
15,"DriverMajorVersion: "" + dbmd.getDriverMajorVersion() );"
15,"System.out.println("""
15,"DriverMinorVersion: "" + dbmd.getDriverMinorVersion() );"
15,"System.out.println(""====="
15,"JDBC/DB attributes ====="");"
15,if (dbmd.supportsGetGeneratedKeys() )
15,"System.out.println("""
15,"Supports getGeneratedKeys(): true"");"
15,else
15,"System.out.println("""
15,"Supports getGeneratedKeys(): false"");"
15,"System.out.println(""===== Database info ====="");"
15,"String sql = ""select now() as current_day,current_database() as db_name, "";"
15,"sql = sql + ""client_hostname as host, application_name, pid from pg_stat_activity "";"
15,"sql = sql + "" where pid = pg_backend_pid() "";"
15,/** creating PreparedStatement object to execute query*/
15,PreparedStatement preStatement = conn.prepareStatement(sql);
15,ResultSet result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println("""
15,"Current Date from Postgres : "" +"
15,"result.getString(""current_day""));"
15,"System.out.println("""
15,"Client connected pid from Postgres : "" +"
15,"result.getString(""pid""));"
15,"System.out.println("""
15,"Postgres DB Unique Name from Postgres : "" +"
15,"result.getString(""db_name""));"
15,"System.out.println("""
15,"Client connected hostname from Postgres : "" +"
15,"result.getString(""host""));"
15,"System.out.println("""
15,"Client connected application_name from Postgres : "" +"
15,"result.getString(""application_name""));"
15,if (dataTypeCheck)
15,"if (sourceDatatType == ""Int"") {"
15,"sql = ""EXPLAIN (ANALYZE, COSTS)"";"
15,"sql = sql + ""select * from datatype_test where int_val = ?"";"
15,"} else if (sourceDatatType == ""Bigint"") {"
15,"sql = ""EXPLAIN (ANALYZE, COSTS)"";"
15,"sql = sql + ""select * from datatype_test where bigint_val = ?"";"
15,"} else if (sourceDatatType == ""Numeric"") {"
15,"sql = ""EXPLAIN (ANALYZE, COSTS)"";"
15,"sql = sql + ""select * from datatype_test where numeric_val = ?"";"
15,"} else if (sourceDatatType == ""Numeric122"") {"
15,"sql = ""EXPLAIN (ANALYZE, COSTS)"";"
15,"sql = sql + ""select * from datatype_test where numeric_val = ?"";"
15,"} else if (sourceDatatType == ""Smallint"") {"
15,"sql = ""EXPLAIN (ANALYZE, COSTS)"";"
15,"sql = sql + ""select * from datatype_test where smallint_val = ?"";"
15,Statement stmt = conn.createStatement();
15,"stmt.execute(""SET max_parallel_workers_per_gather = 0"");"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Int to ""+ sourceDatatType +"" ====="");"
15,/** creating PreparedStatement object to execute query*/
15,preStatement = conn.prepareStatement(sql);
15,"preStatement.setInt(1, intQueryParam);"
15,result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println("""
15,""" + result.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Long to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setLong(1, longQueryParam);"
15,result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println("""
15,""" + result.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Float to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setFloat(1, floatQueryParam);"
15,result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println("""
15,""" + result.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,"System.out.println(""===== Query Plan - Cast Double to ""+ sourceDatatType +"" ====="");"
15,"preStatement.setDouble(1, doubleQueryParam);"
15,result = preStatement.executeQuery();
15,while(result.next())
15,"System.out.println("""
15,""" + result.getString(1));"
15,"System.out.println("""");"
15,"System.out.println("""");"
15,conn.close();
15,"System.out.println(""========================="");"
15,"System.out.println(""Command successfully executed"");"
15,catch(SQLException exp) {
15,"System.out.println(""Exception: "" + exp.getMessage());"
15,"System.out.println(""SQL State: "" + exp.getSQLState());"
15,"System.out.println(""Vendor Error: "" + exp.getErrorCode());"
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, PostgreSQL, Uncategorized and tagged Java, postgres, PostgreSQL on January 5, 2023 by sborden76."
15,Tuning the PostgreSQL “random_page_cost” Parameter
15,Leave a reply
15,"In my previous post “Three Configuration Parameters for PostgreSQL That Are Worth Further Investigation!“, I introduced three PostgreSQL parameters that I feel are the most “neglected” and present the most opportunity for performance tuning in a PostgreSQL instance. I’ve previously posted parts 1 and 2 which cover “work_mem” and “effective_io_concurrency“, so in the final part of this series, I would like to demonstrate tuning the “random_page_cost” parameter."
15,"Because PostgreSQL has the ability to be installed on many different types of systems, the default for this parameter represents a system that is likely the least performant, one that has low CPU and a disk subsystem that is less than ideal."
15,"This setting can be overridden at the individual object level as well, however that may represent a management nightmare so I would recommend against that."
15,"A good explanation of the parameter exists here, and for most CloudSQL instances, should likely be set lower than the default because random page costs are expected to be less expensive on the types of I/O subsystems are present within today’s cloud environments."
15,"For those of you that come from Oracle backgrounds, this parameter is very much like the “OPTIMIZER_INDEX_COST_ADJ” parameter that we used to manipulate in older Oracle versions."
15,To refresh your mind on this parameter you can see the 19c explanation here.
15,"As a simple example of how the query plan can change for a simple SQL, I will first show the query plan with the default setting of 4."
15,"While it is using an index, the access path could be better:"
15,set max_parallel_workers_per_gather = 0;
15,set session random_page_cost to 4;
15,"explain (analyze, verbose, costs, settings, buffers, wal, timing, summary, format text)"
15,"SELECT c.c_name, c.c_acctbal, sum(o.o_totalprice)"
15,FROM orders o
15,JOIN customer c ON (c.c_custkey = o.o_custkey)
15,WHERE c.c_custkey = 30003 and o.o_orderstatus = 'O'
15,"GROUP BY c.c_name, c.c_acctbal;"
15,QUERY PLAN
15,--------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,GroupAggregate
15,(cost=76.28..76.37 rows=1 width=57) (actual time=0.034..0.035 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, sum(o.o_totalprice)"
15,"Group Key: c.c_name, c.c_acctbal"
15,Buffers: shared hit=7
15,Sort
15,(cost=76.28..76.30 rows=8 width=33) (actual time=0.033..0.034 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, o.o_totalprice"
15,"Sort Key: c.c_name, c.c_acctbal"
15,Sort Method: quicksort
15,Memory: 25kB
15,Buffers: shared hit=7
15,Nested Loop
15,(cost=4.97..76.16 rows=8 width=33) (actual time=0.027..0.028 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, o.o_totalprice"
15,Buffers: shared hit=7
15,Index Scan using customer_pk on public.customer c
15,(cost=0.42..8.44 rows=1 width=31) (actual time=0.014..0.015 rows=1 loops=1)
15,"Output: c.c_custkey, c.c_mktsegment, c.c_nationkey, c.c_name, c.c_address, c.c_phone, c.c_acctbal, c.c_comment"
15,Index Cond: (c.c_custkey = '30003'::numeric)
15,Buffers: shared hit=4
15,Bitmap Heap Scan on public.orders o
15,(cost=4.55..67.64 rows=8 width=14) (actual time=0.009..0.009 rows=0 loops=1)
15,"Output: o.o_orderdate, o.o_orderkey, o.o_custkey, o.o_orderpriority, o.o_shippriority, o.o_clerk, o.o_orderstatus, o.o_totalprice, o.o_comment"
15,Recheck Cond: (o.o_custkey = '30003'::numeric)
15,Filter: (o.o_orderstatus = 'O'::bpchar)
15,Buffers: shared hit=3
15,Bitmap Index Scan on order_customer_fkidx
15,(cost=0.00..4.55 rows=16 width=0) (actual time=0.008..0.008 rows=0 loops=1)
15,Index Cond: (o.o_custkey = '30003'::numeric)
15,Buffers: shared hit=3
15,"Settings: effective_cache_size = '3053008kB', effective_io_concurrency = '10', max_parallel_workers_per_gather = '0', work_mem = '512MB'"
15,Query Identifier: 7272380376793434809
15,Planning:
15,Buffers: shared hit=2
15,Planning Time: 0.234 ms
15,Execution Time: 0.076 ms
15,"And now with a change to a setting of 2, we get a different access path:"
15,set max_parallel_workers_per_gather = 0;
15,set session random_page_cost to 2;
15,"explain (analyze, verbose, costs, settings, buffers, wal, timing, summary, format text)"
15,"SELECT c.c_name, c.c_acctbal, sum(o.o_totalprice)"
15,FROM orders o
15,JOIN customer c ON (c.c_custkey = o.o_custkey)
15,WHERE c.c_custkey = 30003 and o.o_orderstatus = 'O'
15,"GROUP BY c.c_name, c.c_acctbal;"
15,QUERY PLAN
15,--------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,GroupAggregate
15,(cost=39.38..39.48 rows=1 width=57) (actual time=0.027..0.028 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, sum(o.o_totalprice)"
15,"Group Key: c.c_name, c.c_acctbal"
15,Buffers: shared hit=7
15,Sort
15,(cost=39.38..39.40 rows=8 width=33) (actual time=0.026..0.027 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, o.o_totalprice"
15,"Sort Key: c.c_name, c.c_acctbal"
15,Sort Method: quicksort
15,Memory: 25kB
15,Buffers: shared hit=7
15,Nested Loop
15,(cost=0.85..39.26 rows=8 width=33) (actual time=0.021..0.022 rows=0 loops=1)
15,"Output: c.c_name, c.c_acctbal, o.o_totalprice"
15,Buffers: shared hit=7
15,Index Scan using customer_pk on public.customer c
15,(cost=0.42..4.44 rows=1 width=31) (actual time=0.012..0.012 rows=1 loops=1)
15,"Output: c.c_custkey, c.c_mktsegment, c.c_nationkey, c.c_name, c.c_address, c.c_phone, c.c_acctbal, c.c_comment"
15,Index Cond: (c.c_custkey = '30003'::numeric)
15,Buffers: shared hit=4
15,Index Scan using order_customer_fkidx on public.orders o
15,(cost=0.43..34.75 rows=8 width=14) (actual time=0.008..0.008 rows=0 loops=1)
15,"Output: o.o_orderdate, o.o_orderkey, o.o_custkey, o.o_orderpriority, o.o_shippriority, o.o_clerk, o.o_orderstatus, o.o_totalprice, o.o_comment"
15,Index Cond: (o.o_custkey = '30003'::numeric)
15,Filter: (o.o_orderstatus = 'O'::bpchar)
15,Buffers: shared hit=3
15,"Settings: effective_cache_size = '3053008kB', effective_io_concurrency = '10', max_parallel_workers_per_gather = '0', random_page_cost = '2', work_mem = '512MB'"
15,Query Identifier: 7272380376793434809
15,Planning:
15,Buffers: shared hit=2
15,Planning Time: 0.199 ms
15,Execution Time: 0.064 ms
15,"Now, for this simple example, the execution time isn’t vastly different, because in both cases an index is being used, however, in cases where the parameter adjustment allows an index to be used over a sequential scan, you will really see the benefit."
15,"Ultimately, there are some other parameters that may benefit from adjustment such as the “cpu_*” parameters, however, those will require much more testing and experimentation over the adjustment of “random_page_cost” especially if your system is running SSDs as in most Google CloudSQL for Postgres instances or even Google AlloyDB where the I/O subsystem is built specifically for the implementation."
15,And if you use either of these
15,"implementations, I would highly consider updating this parameter from the default of 4 to at least 2, maybe even 1.1 depending on the shape that you have chosen and the I/O limits served by each Shape."
15,Enjoy!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, Performance Tuning, PostgreSQL and tagged AlloyDB, CloudSQL, GoogleCloud, Performance Tuning, PostgreSQL on January 3, 2023 by sborden76."
15,Tuning the PostgreSQL “effective_io_concurrency” Parameter
15,5 Replies
15,"In my previous post “Three Configuration Parameters for PostgreSQL That Are Worth Further Investigation!“, I introduced three PostgreSQL parameters that I feel are the most “neglected” and present the most opportunity for performance tuning in a PostgreSQL instance. I’ve previously discussed “work_mem” in a previous post, so in Part 2 of this series, I would like to demonstrate tuning the “effective_io_concurrency” parameter. While this parameter has been discussed in other blogs, I will attempt to make this discussion relevant to how it might affect a CloudSQL instance."
15,The parameter “effective_io_concurrency” reflects the number of simultaneous requests that can be handled efficiently by the disk subsystem.
15,One thing to keep in mind is that currently this parameter only effects “bitmap_heap_scans” where the data is not already present in the shared buffer.
15,"In general, if using spinning HDD devices, this should be set to reflect the number of drives that participate in the RAID stripe."
15,"In cases where SSDs are used, you can set this value much higher, although you must take into account any Quality of Service I/O ops limits which are usually present in a cloud implementation. A full explanation of the parameter can be found here."
15,"To do a simple demonstration of how this parameter can effect queries, I set up a small Google CloudSQL for Postgres instance (2 vCPU X 8GB memory) and loaded up some tables, then executed a query that ensured a “bitmap heap scan” changing “effective_io_concurrency” parameter between each test."
15,"In addition, the instance was bounced before each test to ensure that the shared buffers were cleared."
15,Setup:
15,CREATE TABLE public.effective_io_concurrency_test (
15,"id int PRIMARY KEY,"
15,"value numeric,"
15,"product_id int,"
15,effective_date timestamp(3)
15,INSERT INTO public.effective_io_concurrency_test VALUES (
15,"generate_series(0,100000000),"
15,"random()*1000,"
15,"random()*100,"
15,current_timestamp(3));
15,CREATE INDEX prod_id_idx ON public.effective_io_concurrency_test (product_id);
15,VACUUM ANALYZE public.effective_io_concurrency_test;
15,Execution:
15,"The resulting query plans did not show any variation in execution path or cost, but the timings did vary across the tests."
15,"EXPLAIN (analyze, verbose, costs, settings, buffers, wal, timing, summary, format text)"
15,SELECT * FROM public.effective_io_concurrency_test
15,WHERE id BETWEEN 10000 AND 100000;
15,-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Bitmap Heap Scan on public.effective_io_concurrency_test
15,(cost=7035.15..522009.95 rows=547023 width=27) (actual time=293.542..33257.631 rows=588784 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((effective_io_concurrency_test.id >= 10000) AND (effective_io_concurrency_test.id = 100) AND (effective_io_concurrency_test.product_id
15,BitmapOr
15,(cost=7035.15..7035.15 rows=547450 width=0) (actual time=156.951..156.954 rows=0 loops=1)
15,Buffers: shared hit=6 read=668
15,I/O Timings: read=24.501
15,Bitmap Index Scan on effective_io_concurrency_test_pkey
15,(cost=0.00..1459.74 rows=94117 width=0) (actual time=14.908..14.908 rows=90001 loops=1)
15,Index Cond: ((effective_io_concurrency_test.id >= 10000) AND (effective_io_concurrency_test.id
15,Bitmap Index Scan on prod_id_idx
15,(cost=0.00..5301.90 rows=453333 width=0) (actual time=142.040..142.040 rows=499255 loops=1)
15,Index Cond: ((effective_io_concurrency_test.product_id >= 100) AND (effective_io_concurrency_test.product_id <= 200))
15,Buffers: shared hit=3 read=421
15,I/O Timings: read=14.154
15,"Settings: effective_cache_size = '3259448kB', effective_io_concurrency = '8', random_page_cost = '2', work_mem = '512MB'"
15,Query Identifier: -8974663893066369302
15,Planning:
15,Buffers: shared hit=103 read=17
15,I/O Timings: read=26.880
15,Planning Time: 28.350 ms
15,Execution Time: 33322.389 ms
15,Summary:
15,effective_io_concurrencyQuery Time1
15,/* CloudSQL Default */194708.918 ms2
15,/* Equal number of CPU */107953.205 ms4
15,/* 2x number of CPU */58161.010 ms8
15,/* 4x number of CPU */33322.389 ms10
15,/* 5x number of CPU */30118.593 ms20
15,/* 6x number of CPU */28758.106 ms
15,"As you can see, there is a diminishing return as we increased the parameter, but why?"
15,Upon looking at Google Cloud Console “System Insights” the reason was clear.
15,"**** One thing to note, is that the CPU Utilization spike is a result of the shutdown and restart of the instance between each test."
15,The utilization following the spike represents the utilization found during the test itself.
15,The Conclusion:
15,"While CPU utilization didn’t hit any limit, the IOPS limits for that CloudSQL shape did."
15,"You can add IOPS by changing the shape, but the point of this was to show that the optimal setting always depends on your workload and instance shape."
15,"In this case and for this CloudSQL shape, you might actually want to choose a setting of “4” which represents a setting of 2x the number of CPU and one that doesn’t quite max out the guaranteed IOPS."
15,"The setting doesn’t get you the fastest query time, but does leave resources left over for other queries to execute at the same time."
15,"As always, be sure to test any changed in your own system and balance accordingly because your “mileage may vary” depending on your individual situation."
15,"That being said, in almost no cases is the default setting acceptable unless you are running HDD or on an OS which lacks “posix_fadvise” function (like MacOS or Solaris)."
15,Enjoy!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, Performance Tuning, PostgreSQL and tagged effective_io_concurrency, GoogleCloud, Performance Tuning, PostgreSQL on December 27, 2022 by sborden76."
15,Tuning the PostgreSQL “work_mem” Parameter
15,2 Replies
15,"In my previous post “Three Configuration Parameters for PostgreSQL That Are Worth Further Investigation!“, I introduced three PostgreSQL parameters that I feel are the most “neglected” and present the most opportunity for performance tuning in a PostgreSQL instance. In the first of what will become a three part series, I would like to demonstrate tuning the “work_mem” parameter."
15,The “work_mem” parameter optimizes database operations such as:
15,sorts
15,bitmap heap scans
15,hash joins
15,materialized common table expressions (WITH statements)
15,"To get started, lets create a test table with 100M rows of data:"
15,CREATE TABLE public.work_mem_test (
15,"id int PRIMARY KEY,"
15,"value numeric,"
15,"product_id int,"
15,effective_date timestamp(3)
15,INSERT INTO public.work_mem_test VALUES (
15,"generate_series(0,100000000),"
15,"random()*1000,"
15,"random()*100,"
15,current_timestamp(3));
15,CREATE INDEX prod_value_idx ON public.work_mem_test (product_id);
15,VACUUM ANALYZE public.work_mem_test;
15,"We will then run an explain analyze with the “COSTS, BUFFERS, VERBOSE” options so that we can fully see what is going on with the query."
15,"For demonstration purposes, I have set the “work_mem” to the lowest possible setting of 64kB."
15,"In addition, so that we don’t get the variability of parallel processing I have set the “max_parallel_workers_per_gather” to zero to disable parallel processing."
15,Most systems may also experience better gains than this test case as this was a very small 2 vCPU / 8GB Google CloudSQL PostgreSQL instance:
15,set session work_mem to '64kB';
15,set max_parallel_workers_per_gather = 0;
15,set effective_io_concurrency = 20;
15,"EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS)"
15,SELECT * FROM public.work_mem_test
15,WHERE id BETWEEN 10000 AND 100000
15,OR product_id BETWEEN 100 AND 200
15,ORDER BY value NULLS FIRST;
15,---------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Sort
15,(cost=2640424.97..2641959.63 rows=613866 width=27) (actual time=16593.228..16778.132 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Sort Key: work_mem_test.value NULLS FIRST
15,Sort Method: external merge
15,Disk: 24248kB
15,"Buffers: shared hit=6 read=363702, temp read=15340 written=16486"
15,I/O Timings: read=4120.104
15,Bitmap Heap Scan on public.work_mem_test
15,(cost=7805.12..2539439.23 rows=613866 width=27) (actual time=82.454..15413.822 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((work_mem_test.id >= 10000) AND (work_mem_test.id <= 100000)) OR ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200)))
15,Rows Removed by Index Recheck: 48506004
15,Heap Blocks: exact=2058 lossy=360975
15,Buffers: shared hit=6 read=363702
15,I/O Timings: read=4120.104
15,BitmapOr
15,(cost=7805.12..7805.12 rows=614306 width=0) (actual time=80.340..80.342 rows=0 loops=1)
15,Buffers: shared hit=6 read=669
15,I/O Timings: read=17.683
15,Bitmap Index Scan on work_mem_test_pkey
15,(cost=0.00..1280.95 rows=82638 width=0) (actual time=12.680..12.680 rows=90001 loops=1)
15,Index Cond: ((work_mem_test.id >= 10000) AND (work_mem_test.id <= 100000))
15,Buffers: shared hit=3 read=247
15,I/O Timings: read=7.831
15,Bitmap Index Scan on prod_value_idx
15,(cost=0.00..6217.24 rows=531667 width=0) (actual time=67.657..67.657 rows=499851 loops=1)
15,Index Cond: ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200))
15,Buffers: shared hit=3 read=422
15,I/O Timings: read=9.852
15,Query Identifier: 731698457235411789
15,Planning:
15,Buffers: shared hit=39 read=2
15,I/O Timings: read=2.588
15,Planning Time: 3.136 ms
15,Execution Time: 16811.970 ms
15,(30 rows)
15,Time: 16903.784 ms (00:16.904)
15,"EXPLAIN, via the BUFFERS keyword gives us the following data points:"
15,Rows Removed by Index Recheck: 48506004Heap Blocks: exact=2058 lossy=360975…Execution Time: 16811.970 ms
15,"This essentially means that the 64kB of work_mem can hold 2058 blocks in the bitmap structure within that work_mem size. To get the remainder of the results, everything that falls out of that bitmapare lossy blocks, meaning that they don’t point to an exact tuple, but to rather a block with many tuples. The recheck condition then checks that block for the tuples the query is looking for."
15,"The following formula is a starting point, but may or may not give you the exact setting needed based on various factors."
15,"Since we used the lowest possible work_mem, the setting becomes a multiple of that:"
15,new_mem_in_mbytes =
15,((exact heap blocks + lossy heap blocks) / exact heap blocks) * work_mem_in_bytes / 1048576
15,"= ceil(round(((2058 + 360975) / 2058) * 65536 / 1048576,1))"
15,= 11MB
15,Note:
15,"In most cases, I have found that this formula has worked well on the first pass, however as you will see in the subsequent tests, this estimated work_mem setting wasn’t quite close to the actual amount needed and this is likely due to a mis-estimate by the planner"
15,Reducing the Lossy Block Access
15,So for the next test I will increase the “work_mem” to 11MB and re-execute the test.
15,set session work_mem to '11MB';
15,"EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS)"
15,SELECT * FROM public.work_mem_test
15,WHERE id BETWEEN 10000 AND 100000
15,OR product_id BETWEEN 100 AND 200
15,ORDER BY value NULLS FIRST;
15,---------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Sort
15,(cost=2160340.77..2161875.43 rows=613866 width=27) (actual time=11382.002..11574.572 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Sort Key: work_mem_test.value NULLS FIRST
15,Sort Method: external merge
15,Disk: 24232kB
15,"Buffers: shared hit=23329 read=340379, temp read=3029 written=3034"
15,I/O Timings: read=3618.302
15,Bitmap Heap Scan on public.work_mem_test
15,(cost=7805.12..2090832.53 rows=613866 width=27) (actual time=185.251..10923.764 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((work_mem_test.id >= 10000) AND (work_mem_test.id = 100) AND (work_mem_test.product_id
15,BitmapOr
15,(cost=7805.12..7805.12 rows=614306 width=0) (actual time=132.954..132.957 rows=0 loops=1)
15,Buffers: shared hit=675
15,Bitmap Index Scan on work_mem_test_pkey
15,(cost=0.00..1280.95 rows=82638 width=0) (actual time=4.449..4.450 rows=90001 loops=1)
15,Index Cond: ((work_mem_test.id >= 10000) AND (work_mem_test.id
15,Bitmap Index Scan on prod_value_idx
15,(cost=0.00..6217.24 rows=531667 width=0) (actual time=128.503..128.503 rows=499851 loops=1)
15,Index Cond: ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200))
15,Buffers: shared hit=425
15,Query Identifier: 731698457235411789
15,Planning:
15,Buffers: shared hit=30
15,Planning Time: 0.179 ms
15,Execution Time: 11611.071 ms
15,(26 rows)
15,Time: 11695.952 ms (00:11.696)
15,"With 11MB, we got more exact heap blocks, but still not enough memory to process."
15,Applying the formula based on the execution plan of the query….
15,new_mem_in_mbytes =
15,((exact heap blocks + lossy heap blocks) / exact heap blocks) * work_mem_in_bytes / 1048576
15,"= ceil(round(((164090 + 198943) / 164090) * 12582912 / 1048576,1));"
15,= 24MB
15,Let’s increase just a little bit more to 24MB as the latest iteration of the formula has suggested.
15,set session work_mem to '24MB';
15,"EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS)"
15,SELECT * FROM public.work_mem_test
15,WHERE id BETWEEN 10000 AND 100000
15,OR product_id BETWEEN 100 AND 200
15,ORDER BY value NULLS FIRST;
15,---------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Sort
15,(cost=1709385.33..1710919.99 rows=613866 width=27) (actual time=3651.589..3791.250 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Sort Key: work_mem_test.value NULLS FIRST
15,Sort Method: external merge
15,Disk: 24232kB
15,"Buffers: shared hit=23329 read=340379, temp read=3029 written=3031"
15,I/O Timings: read=1493.162
15,Bitmap Heap Scan on public.work_mem_test
15,(cost=7805.12..1639877.09 rows=613866 width=27) (actual time=348.261..3201.421 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((work_mem_test.id >= 10000) AND (work_mem_test.id = 100) AND (work_mem_test.product_id
15,BitmapOr
15,(cost=7805.12..7805.12 rows=614306 width=0) (actual time=188.309..188.311 rows=0 loops=1)
15,Buffers: shared hit=675
15,Bitmap Index Scan on work_mem_test_pkey
15,(cost=0.00..1280.95 rows=82638 width=0) (actual time=5.090..5.091 rows=90001 loops=1)
15,Index Cond: ((work_mem_test.id >= 10000) AND (work_mem_test.id
15,Bitmap Index Scan on prod_value_idx
15,(cost=0.00..6217.24 rows=531667 width=0) (actual time=183.215..183.215 rows=499851 loops=1)
15,Index Cond: ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200))
15,Buffers: shared hit=425
15,Query Identifier: 731698457235411789
15,Planning:
15,Buffers: shared hit=30
15,Planning Time: 0.242 ms
15,Execution Time: 3828.215 ms
15,(25 rows)
15,Time: 3912.670 ms (00:03.913)
15,No more lossy block scans! Time has also reduced quite significantly from the first execution.
15,Handling the Sort Method
15,"Now, we need to pay attention to the explain plan line:"
15,"""Bitmap Heap Scan on public.work_mem_test….rows=613866 width=27"""
15,"""Sort Method: external merge Disk: 24232kB."""
15,"Some of the sort is in memory and some is spilled to disk. So in order to fit the entire rowset in memory, we must multiply the input rows by the width, which is 16MB. In addition, the planner spilled another 24MB to disk, so let’s add that also to “work_mem”."
15,"16Mb + 24Mb which is being spilled = 40Mb more ""work_mem"""
15,"So with the current “work_mem” of 24MB plus the additional computed to remove the sort (rounded up), the total needed is 64MB."
15,Lets run one more test:
15,set session work_mem to '64MB';
15,"EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS)"
15,SELECT * FROM public.work_mem_test
15,WHERE id BETWEEN 10000 AND 100000
15,OR product_id BETWEEN 100 AND 200
15,ORDER BY value NULLS FIRST;
15,---------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Sort
15,(cost=613087.41..614622.07 rows=613866 width=27) (actual time=3186.918..3309.896 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Sort Key: work_mem_test.value NULLS FIRST
15,Sort Method: quicksort
15,Memory: 61169kB
15,Buffers: shared hit=6 read=363702
15,I/O Timings: read=1306.892
15,Bitmap Heap Scan on public.work_mem_test
15,(cost=7805.12..554071.67 rows=613866 width=27) (actual time=245.348..2908.344 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((work_mem_test.id >= 10000) AND (work_mem_test.id = 100) AND (work_mem_test.product_id
15,BitmapOr
15,(cost=7805.12..7805.12 rows=614306 width=0) (actual time=115.051..115.053 rows=0 loops=1)
15,Buffers: shared hit=6 read=669
15,I/O Timings: read=3.561
15,Bitmap Index Scan on work_mem_test_pkey
15,(cost=0.00..1280.95 rows=82638 width=0) (actual time=6.160..6.161 rows=90001 loops=1)
15,Index Cond: ((work_mem_test.id >= 10000) AND (work_mem_test.id
15,Bitmap Index Scan on prod_value_idx
15,(cost=0.00..6217.24 rows=531667 width=0) (actual time=108.889..108.889 rows=499851 loops=1)
15,Index Cond: ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200))
15,Buffers: shared hit=3 read=422
15,I/O Timings: read=2.231
15,Query Identifier: 731698457235411789
15,Planning:
15,Buffers: shared hit=30
15,Planning Time: 0.180 ms
15,Execution Time: 3347.271 ms
15,(28 rows)
15,Time: 3431.188 ms (00:03.431)
15,"With that adjustment, we have significantly increased the efficiency and performance of the query. From the beginning, just by tuning “work_mem”, we have shaved approximately 13.5 seconds of processing time!"
15,What about a top-N Heapsort??
15,"Now if we want to demonstrate a top-N Heapsort, we can change the query just a little bit more:"
15,set session work_mem to '64MB';
15,"EXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS)"
15,SELECT * FROM public.work_mem_test
15,WHERE id BETWEEN 10000 AND 100000
15,OR product_id BETWEEN 100 AND 200
15,ORDER BY value NULLS FIRST LIMIT 10;
15,---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
15,Limit
15,(cost=567337.09..567337.12 rows=10 width=27) (actual time=3021.185..3021.190 rows=10 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Buffers: shared hit=6 read=363702
15,I/O Timings: read=1313.044
15,Sort
15,(cost=567337.09..568871.76 rows=613866 width=27) (actual time=3021.183..3021.186 rows=10 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Sort Key: work_mem_test.value NULLS FIRST
15,Sort Method: top-N heapsort
15,Memory: 26kB
15,Buffers: shared hit=6 read=363702
15,I/O Timings: read=1313.044
15,Bitmap Heap Scan on public.work_mem_test
15,(cost=7805.12..554071.67 rows=613866 width=27) (actual time=235.909..2911.978 rows=589379 loops=1)
15,"Output: id, value, product_id, effective_date"
15,Recheck Cond: (((work_mem_test.id >= 10000) AND (work_mem_test.id = 100) AND (work_mem_test.product_id
15,BitmapOr
15,(cost=7805.12..7805.12 rows=614306 width=0) (actual time=108.429..108.431 rows=0 loops=1)
15,Buffers: shared hit=6 read=669
15,I/O Timings: read=3.114
15,Bitmap Index Scan on work_mem_test_pkey
15,(cost=0.00..1280.95 rows=82638 width=0) (actual time=5.582..5.582 rows=90001 loops=1)
15,Index Cond: ((work_mem_test.id >= 10000) AND (work_mem_test.id
15,Bitmap Index Scan on prod_value_idx
15,(cost=0.00..6217.24 rows=531667 width=0) (actual time=102.845..102.845 rows=499851 loops=1)
15,Index Cond: ((work_mem_test.product_id >= 100) AND (work_mem_test.product_id <= 200))
15,Buffers: shared hit=3 read=422
15,I/O Timings: read=2.037
15,Query Identifier: 4969544646514690020
15,Planning:
15,Buffers: shared hit=30
15,Planning Time: 0.177 ms
15,Execution Time: 3023.304 ms
15,(32 rows)
15,Time: 3107.421 ms (00:03.107)
15,"Because we are only returning the top N rows, the memory used is not as high because a different sort methodology can be used."
15,"In addition, the time is further reduced."
15,"As you can see with, a little tuning of the “work_mem” parameter, lots of performance can be gained in the system. In this example, we have increased “work_mem” a fairly small amount from 64kb to 64MB. In my mind you never want to increase “work_mem” to a setting where if all workers were being worked by CPU, you could overrun the free memory on the system."
15,"Also, remember that there is some overhead to maintaining that memory so it’s really important to find a good balance for your workload. Keep in mind that you can set this parameter at the server level, as an alter in the query text or at the user level as a profile."
15,Enjoy!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Database Migration, Google Cloud, PostgreSQL and tagged CloudSQL, Google Cloud, Performance Tuning, PostgreSQL, work_mem on December 5, 2022 by sborden76."
15,Three Configuration Parameters for PostgreSQL That Are Worth Further Investigation!
15,3 Replies
15,"In my new role at Google, not only am I still working with lots of Oracle and replication tools, I am also expanding more into moving Oracle systems to Google Cloud on either CloudSQL for PostgreSQL or AlloyDB for PostgreSQL."
15,"After you have been looking at the systems for a little bit of time, there seem to be a few things worth tweaking from the out of the box values."
15,It is my goal to discuss some of those things now and in future blog posts.
15,Let me start off by saying managed PostgreSQL CloudSQL products such as Google’s CloudSQL for PostgreSQL and AlloyDB for PostgreSQL (in Preview as of this post) are designed to be low maintenance and fit many different types of workloads.
15,"That being said, there are a few configuration parameters that you should really look at tuning as the defaults (as of PostgreSQL version 14) in most cases are just not set to the most efficient value if your workload is anything more than a VERY light workload."
15,work_mem
15,Sets the base maximum amount of memory to be used by a query operation (such as a sort or hash table) before writing to temporary disk files and the default value is four megabytes (4MB).
15,"People coming from the Oracle world will equate this setting with PGA, however you must keep in mind that the implementation is “private” memory in PostgreSQL while it is “shared” memory in Oracle."
15,You must take care not to over configure this setting in PostgreSQL.
15,A full description of the parameter can be found here.
15,random_page_cost
15,Sets the planner’s estimate of the cost of a non-sequentially-fetched disk page and the default is 4.0. In reality this setting is good for a system in which disk performance is a concern (i.e a system with HDD vs SSDs) as it is assumed that random disk access is 40x slower than sequential access.
15,"Essentially if you want your system to prefer index and cache reads, lower this number from the default, but to no lower than the setting for seq_page_cost."
15,"For normal CloudSQL for PostgreSQL deployments that use SSD, I like to set this to 2."
15,"In deployments which utilize AlloyDB for PostgreSQL an even lower setting of 1.1 can be used due to the efficient Colossus Storage implementation.For those that have been around Oracle for a while, this parameter behaves much like the “optimizer_index_cost_adj” parameter."
15,A full description of the parameter can be found here.
15,effective_io_concurrency
15,Sets the number of concurrent disk I/O operations that PostgreSQL expects can be executed simultaneously. Raising this value will increase the number of I/O operations that any individual PostgreSQL session attempts to initiate in parallel. The default is 1 and at this point this setting only effects bitmap heap scans.
15,"That being said, bitmap heap scans, while efficient, by nature have to look at the index as well as a corresponding heap block and if that data has to be read from disk and if your system can handle the parallelism like when you use SSD storage, you should increase this to a more meaningful value."
15,"I will do a separate blog post to show the effects of this, but in general as this number is increased beyond 1/2 the number of CPUs available, greater diminishing returns are observed."
15,A full description of the parameter can be found here.
15,"In closing, just like Oracle and other RDBMSs, there are numerous configuration parameters all which can have effects on the workload."
15,"However, the above three parameters are the ones I most often find that have opportunities for optimization, especially on more modern platforms."
15,In future posts I will detail how each one of these can change a workload.
15,Enjoy!
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in AlloyDB for PostgreSQL, CloudSQL PostgreSQL, Google Cloud, Performance Tuning, PostgreSQL and tagged AlloyDB, CloudSQL, effective_io_concurrency, GoogleCloud, PostgreSQL, random_page_cost, work_mem on December 2, 2022 by sborden76."
15,"Indexes Can Check-Out Anytime, But Can They Leave?"
15,Leave a reply
15,"When I think about indexes in any DBMS, I immediately think about the lyrics in the Eagles song “Hotel California”."
15,"“You can check-out any time you like, but you can never leave…..”"
15,Almost every system that has been around for a while is over indexed.
15,Every index costs something to maintain and some more than others.
15,"Also, systems and code change."
15,"There could be an index that was once needed, but over time the code shifts to use another one thats more efficient instead."
15,"Guess what though, you are still maintaining that old index."
15,"That costs CPU, I/O which translates into TIME."
15,"Thankfully in Oracle 12.2, they have introduced a feature called INDEX USAGE TRACKING."
15,In older versions this was called INDEX MONITORING and it was much harder and invasive to use.
15,"In the newer versions, it just works and you can read more about both options here: https://oracle-base.com/articles/12c/index-usage-tracking-12cr2"
15,"By default, indexes in the views “DBA_INDEX_USAGE” and “V$INDEX_USAGE_INFO”."
15,The “V$” table is flushed to the “DBA” table every 15 minutes.
15,"For most purposes, using the “DBA” table is just fine."
15,"The thing to remember is that querying only these tables tell you the indexes that are being USED, not the ones that are NOT USED."
15,"To get the ones that are not used, we just need to expand the query a bit:"
15,SELECT
15,"di.owner,"
15,"di.index_name,"
15,"di.index_type,"
15,di.table_name
15,FROM
15,dba_indexes di
15,WHERE
15,di.owner in (select username from dba_users where oracle_maintained = 'N')
15,AND di.uniqueness = 'NONUNIQUE'
15,AND NOT EXISTS (
15,SELECT
15,FROM
15,dba_index_usage iu
15,WHERE
15,iu.owner = di.owner
15,AND iu.name = di.index_name
15,GROUP BY
15,"di.owner,"
15,"di.index_name,"
15,"di.index_type,"
15,di.table_name
15,ORDER BY
15,"di.owner,"
15,"di.table_name,"
15,di.index_name;
15,"Now that we have the indexes that are not used, which ones have the biggest impact on database operations?"
15,We just expand the query again to leverage the “DBA_TAB_MODIFICATIONS” table.
15,This will allow us to see how many operations occur on each table that the index sits on:
15,SELECT
15,"di.owner,"
15,"di.index_name,"
15,"di.index_type,"
15,"di.table_name,"
15,SUM(tm.inserts + tm.updates + tm.deletes) operations
15,FROM
15,"dba_indexes di,"
15,dba_tab_modifications tm
15,WHERE
15,di.owner in (select username from dba_users where oracle_maintained = 'N')
15,AND di.owner = tm.table_owner
15,AND di.table_name = tm.table_name
15,AND di.uniqueness = 'NONUNIQUE'
15,AND NOT EXISTS (
15,SELECT
15,FROM
15,dba_index_usage iu
15,WHERE
15,iu.owner = di.owner
15,AND iu.name = di.index_name
15,GROUP BY
15,"di.owner,"
15,"di.index_name,"
15,"di.index_type,"
15,di.table_name
15,ORDER BY
15,"SUM(tm.inserts + tm.updates + tm.deletes) ASC NULLS FIRST,"
15,"di.owner,"
15,"di.table_name,"
15,di.index_name;
15,"Now, I have done an equi-join condition against the DBA_TAB_MODIFICATIONS so this is only showing us unused indexes against tables that the system has recorded operations against."
15,"If you wanted to show all indexes regardless if there were operations or not, then just change the join condition against DBA_TAB_MODIFICATIONS to be an outer join."
15,"In my case, there were some very large indexes that were unused sitting on tables that had LOTS of operations (names changed to protect the innocent):"
15,OWNERINDEX NAMEINDEX TYPETABLE NAMEOPERATIONSHRFK_1_IDXNORMALEMPLOYEES295931545HR1EMP_TOTFUNCTION-BASED NORMALEMP_HOURS1374288673HR2EMP_TOTFUNCTION-BASED NORMALEMP_HOURS1425114200HR3EMP_TOTFUNCTION-BASED NORMALEMP_HOURS2487284020
15,Adding “OPERATIONS” to the query now gives us some sense of the “BENEFIT” of dropping the index.
15,Based on the results you can present your case for dropping the index.
15,People always love to have the “BENEFIT” quantified and this is the closest thing you can do without performing a session trace which could prove to be invasive and fairly difficult to get the exact statement you are looking for.
15,Hopefully this will give you the additional information you need to advocate for dropping that index that is no longer being used.
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in Oracle, Performance Tuning, RAC, SQL and tagged Indexes, Oracle, Performance Tuning, Statistics on June 2, 2022 by sborden76."
15,SQLPlus is Expensive!
15,Leave a reply
15,I was recently tasked to work on a server that had severe performance issues.
15,"The first thing I noticed was that the server was, at times, running at a load average 3-4x and under steady state about 1.5x the number of cores on the host!"
15,"Further investigation, yielded that there was a large quantity of SQPlus invocations from a set of ETL scripts."
15,"Digging even more, I found that the script performed ETL across more than 100 schemas 75% of which were done via parallel background processes with a random “sleep” built in."
15,This is where I noticed something interesting.
15,"Instead of coding the script to invoke SQLPlus once, run a bunch of statements and exit, the coder constructed the script to invoke SQLPlus, run a statement, exit, invoke SQLPlus, run a statement, and on and on and on."
15,"My first thought was that this has to be EXPENSIVE right? When a SQLPlus login is made, a lot of things happen."
15,"Simply stated, the OS has to start a database process, and the database has to do a bunch of things to get your session ready (password authentication, memory allocation, update some tables with login details, setup permissions), etc, etc."
15,Managing processes is one of the most expensive things an Operating System can do.
15,"In terms of the database itself, no less than 19 individual statements are executed in order to prepare your session."
15,"Digging into those statements, probably one of the most expensive is:"
15,sql_id:
15,9zg9qd9bm4spu
15,"update user$ set spare6=DECODE(to_char(:2, 'YYYY-MM-DD'), '0000-00-00', to_date(NULL), :2) where user#=:1"
15,"It can be so expensive, Oracle has actually logged a bug (Bug 33121934  Library cache lock / load lock / mutex x during connection storm due to update user$) where you can discontinue updating the last login time for every login and do it every X number of seconds."
15,The explain plan for this statement is actually quite lengthy and there is a lot more work going on than you think!
15,"So based on all of that, I headed back to my scripts only to discover for every invocation of the script (the script is invoked for each of the 114 schemas), there were a minimum of 11 SQLPlus invocations and a max of of 27 invocations depending on the schema."
15,So every minute that this script was executed there was a total of almost 1400 SQLPlus Invocations!!!
15,Definitely a big contributor to the very high load average on the server.
15,"As with anything in IT, change is tough and there’s hardly ever much appetite to change something that “works”."
15,That is where it is important to always quantify the benefits. So to do this I wrote a quick script to show the difference in CPU Utilization for a script that does many SQLPlus invocations vs one that does one invocation and then calls many sql scripts.
15,"The resultant stats are as follows. For the original script in the test system I used had one SQLPlus invocation for each sql file that was called, the following resources were used."
15,The original script executed SQLPlus 61 times and yielded the following system usage:
15,real
15,0m0.876s
15,user
15,0m0.480s
15,sys
15,0m0.463s
15,The modified script that reduced the number of calls from 61 to 14 (77%) yielded the following system usage:
15,real
15,0m0.188s
15,user
15,0m0.117s
15,sys
15,0m0.085s
15,*** NOTE: The test system I was working on had no data to manipulate for ETL so most of the work was the SQLPlus call.
15,Which is what I wanted to demonstrate anyway!
15,"As you can see, reducing the number of SQLPlus calls by 77%, reduced system usage by:"
15,real: 79%user: 75%sys: 82%
15,"So with a small change to group sql statements into one SQLPlus call, we are able to drastically improve system stability and reduce resource utilization."
15,A WIN in my book!
15,"Once this gets through the test cycles, I will follow up with a post on the improvement seen in production."
15,Share this:LinkedInTwitterFacebookEmailPrintLike Loading...
15,"This entry was posted in Linux, Oracle, Scripting, SQL and tagged Linux, Oracle, Scripting, SQLPlus on May 19, 2022 by sborden76."
15,Post navigation
15,← Older posts
15,Search for:
15,Follow Me
15,Twitter
15,GitHub
15,LinkedIn
15,Subscribe to Blog via Email
15,Enter your email address to subscribe to this blog and receive notifications of new posts by email.
15,Email Address:
15,Subscribe
15,"Join 1,213 other subscribers"
15,Recent Posts
15,“Row Movement” in PostgreSQL… Is it bad?
15,Using the “hint_plan” Table Provided by the PostgreSQL Extension “pg_hint_plan”
15,Leverage Google Cloud Logging + Monitoring for Custom Cloud SQL for Postgres or AlloyDB Alerts
15,Why is My App Table Scanning in PostgreSQL but not Oracle?
15,Tuning the PostgreSQL “random_page_cost” Parameter
15,Blog Archive
15,September 2023 (1)
15,April 2023 (1)
15,February 2023 (1)
15,January 2023 (2)
15,December 2022 (3)
15,June 2022 (1)
15,May 2022 (1)
15,September 2021 (1)
15,August 2021 (2)
15,October 2020 (2)
15,September 2020 (1)
15,August 2020 (1)
15,June 2020 (1)
15,October 2019 (2)
15,March 2019 (1)
15,October 2018 (2)
15,July 2018 (2)
15,September 2017 (1)
15,April 2017 (2)
15,March 2017 (1)
15,December 2016 (1)
15,September 2016 (1)
15,May 2016 (1)
15,January 2016 (2)
15,December 2015 (1)
15,September 2015 (1)
15,August 2015 (2)
15,Categories
15,AlloyDB for PostgreSQL (8)
15,ASM (1)
15,Cloud Operations (1)
15,CloudSQL PostgreSQL (8)
15,Database Migration (4)
15,Datapump (2)
15,Encryption (1)
15,Engineered Systems (2)
15,Exadata (1)
15,FTP (1)
15,GaOUG (1)
15,Goldengate (9)
15,Google Cloud (8)
15,GRID (1)
15,Java (2)
15,Linux (7)
15,Metrics (1)
15,Network (1)
15,Oracle (20)
15,Oracle Enterprise Manager (1)
15,partitioning (1)
15,Performance Tuning (5)
15,pg_hint_plan (1)
15,postgres (3)
15,PostgreSQL (8)
15,Python (1)
15,RAC (4)
15,RMAN (1)
15,SCP (1)
15,Scripting (1)
15,Smart Scan (1)
15,Solaris (3)
15,SQL (6)
15,SQL Plan Management (1)
15,SQLDeveloper (1)
15,SuperCluster (2)
15,Training (2)
15,Tuxedo (1)
15,Uncategorized (3)
15,upgrade (1)
15,WebLogic (1)
15,"Blogs I FollowJon Adams, OCP, OCI Architect and DBA, MCTS, OCI Cloud DBA, AWS Cloud PractitionerToo Meh For A MemoirCarlos Sierra's Tools and TipsMauro Pagano's BlogDBA spot"
15,Website Powered by WordPress.com.
15,"Jon Adams, OCP, OCI Architect and DBA, MCTS, OCI Cloud DBA, AWS Cloud PractitionerOracle, SQL Server, Cloud, ObservationsToo Meh For A MemoirA blog: because I'm not interesting enough yet to write an actual book.Carlos Sierra's Tools and TipsTools and Tips for Oracle Performance and SQL TuningMauro Pagano's BlogDBA spotOracle topics"
15,🛩️ Shane Borden's Technology Blog
15,Website Powered by WordPress.com.
15,Subscribe
15,Subscribed
15,🛩️ Shane Borden's Technology Blog
15,Join 29 other subscribers
15,Sign me up
15,Already have a WordPress.com account? Log in now.
15,Privacy
15,🛩️ Shane Borden's Technology Blog
15,Customize
15,Subscribe
15,Subscribed
15,Sign up
15,Log in
15,Report this content
15,View site in Reader
15,Manage subscriptions
15,Collapse this bar
15,Loading Comments...
15,Write a Comment...
15,Email (Required)
15,Name (Required)
15,Website
17,Community Guide to PostgreSQL GUI Tools - PostgreSQL wiki
17,"Want to edit, but don't see an edit button when logged in?"
17,Click here.
17,Community Guide to PostgreSQL GUI ToolsFrom PostgreSQL wikiJump to navigationJump to search
17,"This page is a list of miscellaneous utilities that work with Postgres (ex: data loaders, comparators etc.)."
17,"Things that don't do queries' ""enter SQL and get it back out again."""
17,"If you'd like to find clients that allows you to ""enter SQL and get it back out again"" see PostgreSQL Clients."
17,If you'd like to find DB visualization or design tools see Design Tools.
17,Contents
17,1 Open Source / Free Software
17,1.1 Libre Office
17,1.2 PASH-Viewer: PostgreSQL Active Session History Viewer
17,"1.3 pgrights: GUI for PostgreSQL roles, privileges and policies"
17,1.4 Sohag Developer
17,1.5 Beekeeper Studio
17,1.6 Execsql.py
17,2 Proprietary
17,2.1 Access
17,2.2 Five
17,2.3 dbForge Studio for PostgreSQL
17,2.4 dbForge Data Compare for PostgreSQL
17,2.5 dbForge Schema Compare for PostgreSQL
17,2.6 TablePlus
17,2.7 Ultorg
17,2.8 WaveMaker Ajax GUI Design Tool
17,2.9 Postgres Compare
17,2.10 Full Convert
17,2.11 Abris Platform
17,2.12 Replicator Pro
17,2.13 DBTools Manager
17,2.14 PostgreSQL PHP Generator
17,2.15 ConvertDB for PosttgreSQL
17,2.16 dotConnect for PostgreSQL
17,2.17 Devart PostgreSQL Data Access Components
17,2.18 Devart ODBC Driver for PostgreSQL
17,2.19 Devart Excel Add-in for PostgreSQL
17,2.20 EMS Database Management Tools for PostgreSQL
17,2.21 SQL Maestro Group products for PostgreSQL
17,2.22 Datanamic DataDiff for PostgreSQL
17,2.23 Datanamic SchemaDiff for PostgreSQL
17,2.24 DB MultiRun PostgreSQL Edition
17,2.25 DB Doc for PostgreSQL
17,2.26 SQL Blob Export
17,2.27 SQL File Import
17,2.28 SQL Image Viewer
17,2.29 SQL Multi Select
17,2.30 Devart SSIS Data Flow Components for PostgreSQL
17,2.31 EDB Postgres Enterprise Manager
17,2.32 ClusterControl by Severalnines
17,2.33 Reportizer
17,2.34 Exportizer Enterprise
17,2.35 TiCodeX SQL Schema Compare
17,2.36 pgMustard
17,2.37 ODBC Driver for PostgreSQL by CData
17,2.38 JDBC Driver for PostgreSQL by CData
17,2.39 ADO.NET Provider for PostgreSQL by CData
17,2.40 Excel Add-In
17,for PostgreSQL by CData
17,2.41 SSIS Components for PostgreSQL by CData
17,2.42 Power BI Connector for PostgreSQL by CData
17,3 Other Resources
17,Open Source / Free Software
17,"This is the list of ""open source and free"" miscellaneous utilities:"
17,Libre Office
17,http://www.libreoffice.org/discover/base/
17,"Supports MySQL/MariaDB, Adabas D, MS Access and PostgreSQL, as well as other JDBC/ODBC databases."
17,PASH-Viewer: PostgreSQL Active Session History Viewer
17,https://github.com/dbacvetkov/PASH-Viewer
17,Java (multi-platform).
17,"Open-source software which provides graphical view of active session history and help you to answer questions like ""What wait events were taking most time?"", ""Which sessions were taking most time?"", ""Which queries were taking most time and what were they doing?"". It also supports Active Session History extension by pgsentinel."
17,"Does not do DB inserts, modifications, etc."
17,"pgrights: GUI for PostgreSQL roles, privileges and policies"
17,https://github.com/apsavin/pgrights
17,"MacOS (based on Electron, so versions for other OS can be build from source code)."
17,"Open-source software which allows you to easily understand what can do (and what can't) a PostgreSQL user with a table's data. In other words, it's a viewer of results of GRANT commands and row-level security rules applied for a particular table and for a particular role."
17,"Only modifies user rights, no other capability."
17,Sohag Developer
17,Sohag Developer
17,Gnu/Linux Windows (Other OS can compile from source code).
17,Build a powerful database applications following few steps using Sohag Developer .
17,Sohag Developer currently supports PostgreSQL database and has a set of CRUD generators that generates (Qt/C++ code and ui forms - PHP web applications uses web forms and bootstrap framework ) to manipulate the data in the database tables or views.
17,Beekeeper Studio
17,https://beekeeperstudio.io/
17,"Beekeeper Studio is a modern cross-platform SQL editor and database manager available for Linux, Mac, and Windows. Some of its features include:"
17,"Clean, smooth, usable UI with dark and light themes"
17,Tabbed Interface
17,Multiple connections at the same time
17,Saved queries and run history
17,Auto-complete
17,Execsql.py
17,https://pypi.org/project/execsql/
17,Execsql.py is a SQL scripting client that reads SQL statements from a text file and sends them to the PostgreSQL server.
17,"The script file can also contain special metacommands that are interpreted by execsql.py and that can be used to import and export data, copy data between databases, conditionally execute script blocks, display data in GUI dialogs, and perform other functions."
17,Data can be exported in 19 different tabular formats or using any of three different template processors.
17,Default and custom logging features can be used to document script actions.
17,Proprietary
17,"This is a list of ""closed source"" projects, some might have some manner of free version."
17,Access
17,https://products.office.com/en-us/access
17,Windows
17,"Yes, you can use MS Access as a PostgreSQL database interface. Supports data access to PostgreSQL tables and views; many ODBC-based limitations and errors."
17,Five
17,https://five.co/
17,Rapidly develop and deliver modern business software to internal or external users.
17,Five is free to use. Develop and test applications locally free of charge. Only subscribe when you build something production worthy.
17,Features:
17,"Store, retrieve and process data from any data source (such as a PostgreSQL DB)"
17,"Rapidly implement business logic using SQL, JavaScript or TypeScript"
17,Cut down the time you spent on front-end development
17,"Scalable, secure and affordable: deploy apps in one click"
17,Keep your data secure on Five’s managed infrastructure
17,Build password-protected multi-user applications
17,Useful developer tools to accelerate your development
17,Free download available for Windows & MacOS
17,dbForge Studio for PostgreSQL
17,http://www.devart.com/dbforge/postgresql/studio/
17,"Microsoft Windows, Linux, macOS"
17,"dbForge Studio for PostgreSQL is a feature-rich IDE designed for database development and management. The tasks that can be handled with its help include completion-aided SQL coding, comparison and synchronization of databases, data management, data analysis and reporting, query optimization, and customizable generation of realistic test data."
17,Key features:
17,SQL Development: Smart code completion and formatting
17,Database Explorer: Object tree navigation and operations from the context menu
17,Database Comparison: Detection of differences in schemas and table data and generation of synchronization scripts
17,Data Import & Export: 10+ most widely used data formats
17,Query Profiler: SQL query performance tuning
17,Data Generator: Creation of meaningful dummy data for testing
17,Pivot Tables: Grouping and summarization of data for analysis
17,Master-Detail Browser: Review and analysis of data in related tables
17,Reporting: Visual data reports in 9 different formats
17,Command-Line Interface: CLI-powered automation of recurring operations
17,dbForge Data Compare for PostgreSQL
17,http://www.devart.com/dbforge/postgresql/datacompare/
17,"Microsoft Windows, Linux, macOS"
17,"dbForge Data Compare for PostgreSQL is an effective tool for table data comparison, which makes it easy to detect differences in data, analyze them, and generate SQL scripts for data synchronization."
17,Key features:
17,Identify the differences between two databases
17,Compare separate tables or table groups by table name mask
17,Compare tables with different structures
17,Compare custom query execution results
17,Generate detailed comparison reports
17,Synchronize data in tables and views fully or partially using scripts
17,Schedule data synchronization with Windows Task Scheduler
17,dbForge Schema Compare for PostgreSQL
17,http://www.devart.com/dbforge/postgresql/schemacompare/
17,"Microsoft Windows, Linux, macOS"
17,"dbForge Schema Compare for PostgreSQL and Amazon Redshift is a free tool for easy and effective comparison of database structure differences. Additionally, it generates and runs SQL scripts for synchronization of source and target schemas."
17,Key features:
17,Find differences in Redshift and PostgreSQL database schemas
17,Generate SQL scripts to update one database with the contents of another
17,Migrate PostgreSQL schemas to Amazon Redshift
17,Apply updates from development databases to staging or production
17,Compare and synchronize pre-object security permissions
17,Compare PL/pgSQL and Python code
17,TablePlus
17,https://tableplus.com/
17,"macOS, Windows, iOS"
17,"TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more."
17,True native built.
17,"Workspace supports multiple tabs, multiple windows"
17,"Powerful SQL editor with full features: auto syntax highlight, auto-suggestion, split pane, favorite and history."
17,"Data Filter & Sorting, import & export"
17,Full-dark theme & modern shortcut
17,"With plugin system, you can be able to write your own new features to work with database per your needs (export charts, pretty json…)."
17,Ultorg
17,https://www.ultorg.com
17,Ultorg is a general-purpose user interface and visual query system that works with any existing PostgreSQL database.
17,"Windows, MacOS, Linux"
17,Key features:
17,Quickly show data across many tables and one-to-many relationships.
17,"See data in continuously auto-formatted table, form, or crosstab layouts."
17,"Build the equivalent of any SQL-92 SELECT query by interacting directly with the data (choose fields, joins, filters, calculations etc.)."
17,Edit data and commit changes back to the database. Insert/delete/dropdown rows from FK relationships etc.
17,"Join PostgreSQL tables with tables from other data sources (CSV files, Google Sheets, and others)."
17,Avoid typing long SQL queries and associated formatting code.
17,WaveMaker Ajax GUI Design Tool
17,http://www.wavemaker.com/
17,"Windows, Macintosh, Linux"
17,"WaveMaker is an Ajax-based GUI design tool for Postgres. WaveMaker is built using itself! WaveMaker generates a standard Java WAR file based on Spring, Hibernate and Dojo. WaveMaker supports Postgres schema creation and import and includes a visual query editor."
17,Postgres Compare
17,frameless
17,https://www.postgrescompare.com/
17,"Windows, Mac & Linux"
17,"A comprehensive tool for identifying the differences between databases and generating an update script to synchronize them. Postgres Compare reads the system catalogs to determine the structure of the database and compares it to another to find the changes. Generate SQL and deploy the alterations, save snapshots for later. Automate the process via the command line."
17,Full Convert
17,https://www.spectralcore.com/fullconvert
17,"Database conversion and synchronization between PostgreSQL and Microsoft Access, dBase, FoxPro, Microsoft Excel, Firebird, Interbase, MySQL, Oracle, Paradox, Microsoft SQL Server, SQL Server, SQL Server Azure, SQL Server Compact(SQLCE), SQLite, Delimited text files (CSV), XML and many more via ODBC."
17,Abris Platform
17,https://abrisplatform.com
17,"Web Application for Linux/Windows, requires Apache+PHP or Docker"
17,Abris Platform is an application development platform for creating Web-based front-ends for PostgreSQL databases. Can be used to quickly create applications with convenient forms via SQL declarative description.
17,Key features:
17,"Quick setup - Abris Platform provides built-in means for flexible data structures configuration (tables, fields and relations). This can be done during system initialization as well as during system expluatation."
17,Single page application - Related entities and tables are instantly accessible for view and edit on the same screen.
17,Search - Use general in-table search and complex column filters.
17,"Charts - Data can be represented via charts of different types: bar charts, pie charts, lines any many others."
17,Maps - Build-in support for geo data. Abris platform allow to vizualize geo-data event in real-time using OpenStreet Map package.
17,"Reporting - Filter settings can be saved as a report description and data can be exported in the following formats: HTML, PDF, Excel."
17,Data import - Insert data in the current open list view from the computer clipboard.
17,"Notifications - Notification pool, that can be filled in PostgreSQL functions."
17,"Administrative tool - Built in administrative tools take care of user management, activity monitoring and auditing and allow to configure user/group access policy on the table and field database level."
17,Replicator Pro
17,https://www.spectralcore.com/replicator
17,"Replicator allows table data comparison and sync - even with heterogeneous databases. It is unique in the fact it can replicate changes only even if source is non-relational (CSV, DBF, Excel documents, Paradox...). Replicator has a built-in scheduler for easy periodic change replication."
17,DBTools Manager
17,http://www.dbtools.com.br
17,Windows
17,Admin
17,"Freeware, available for PostgreSQL and MySQL, allows managing all aspects of the database: db, table, triggers, functions, etc. Includes import/export wizards to migrate data and structure to/from other database engines. Developed by DBTools Software."
17,PostgreSQL PHP Generator
17,http://www.sqlmaestro.com/products/postgresql/phpgenerator/
17,Windows
17,"PostgreSQL PHP Generator is a freeware but powerful PostgreSQL GUI frontend that allows you to generate high-quality PHP scripts for the selected tables, views and queries for the further working with these objects through the web."
17,ConvertDB for PosttgreSQL
17,http://convertdb.com/postgresql has PostgreSQL export/ import tools
17,Windows
17,"The software is able to connect to remote PostgreSQL 9.x/7.4 located on Linux, Solaris, Mac OS X, and Windows."
17,"ConvertDB cross-database migration tools assist in data conversion and synchronization among PostgreSQL, MySQL, MS SQL Server, MS Windows SQL Azure,"
17,and MS Access databases
17,1 Million of records can be transferred in 5-10 minutes.
17,"Bi-directional synchronization between PostgreSQL, MS SQL, MySQL and Oracle"
17,Scheduling migration and synchronization jobs.
17,dotConnect for PostgreSQL
17,https://www.devart.com/dotconnect/postgresql/
17,Windows
17,"dotConnect for PostgreSQL, formerly known as PostgreSQLDirect .NET, is an enhanced ORM enabled data provider for PostgreSQL that builds on ADO.NET technology to present a complete solution for developing PostgreSQL-based database applications. It introduces new approaches for designing application architecture, boosts productivity, and leverages database applications."
17,Key features:
17,Direct Mode
17,Database Application Development Extension
17,PostgreSQL Advanced Features Support
17,Optimized Code
17,ORM Support
17,BIS Support
17,Devart PostgreSQL Data Access Components
17,Windows
17,https://www.devart.com/pgdac/
17,"PostgreSQL Data Access Components (PgDAC) is a library of components that provides native connectivity to PostgreSQL from Delphi, C++Builder, Lazarus (and Free Pascal) on Windows, Mac OS X, iOS, Android, Linux, and FreeBSD for both 32-bit and 64-bit platforms. PgDAC is designed to help programmers develop really lightweight, faster and cleaner PostgreSQL database applications without deploying any additional libraries."
17,Native Connectivity to PostgreSQL
17,PgDAC is a complete replacement for standard PostgreSQL connectivity solutions and presents an efficient alternative to the Borland Database Engine (BDE) and standard dbExpress driver for access to PostgreSQL. It provides direct access to PostgreSQL without PostgreSQL Client.
17,Devart ODBC Driver for PostgreSQL
17,https://www.devart.com/odbc/postgresql/
17,Windows
17,"Devart ODBC Driver for PostgreSQL provides high-performance and feature-rich connectivity solution for ODBC-based applications to access PostgreSQL databases from Windows, both 32-bit and 64-bit. Full support for standard ODBC API functions and data types implemented in our driver makes the interaction of your database applications with PostgreSQL fast, easy and extremely handy."
17,Devart Excel Add-in for PostgreSQL
17,https://www.devart.com/excel-addins/postgresql.html
17,Windows
17,"Devart Excel Add-in for PostgreSQL allows you to quickly and easily connect Microsoft Excel to PostgreSQL, load data from PostgreSQL to Excel, instantly refresh data in an Excel workbook from the database, edit these data, and save them back to PostgreSQL. It enables you to work with PostgreSQL data like with usual Excel worksheets, easily perform data cleansing and de-duplication, and apply all the Excel's powerful data processing and analysis capabilities to these data."
17,EMS Database Management Tools for PostgreSQL
17,http://www.sqlmanager.net/en/products/postgresql
17,Windows
17,PostgreSQL Tools Products Family:
17,EMS SQL Manager for PostgreSQL see PostgreSQL Clients.
17,"SQL Management Studio for PostgreSQL - a single workbench for administering PostgreSQL databases, managing database schema and objects as well as for database design, migration, extraction, query building, data import, export, and database comparison."
17,"SQL Manager for PostgreSQL - high performance graphical tool for PostgreSQL database administration and development. It makes creating and editing PostgreSQL database objects easy and fast, and allows you to run SQL scripts, visually design databases, build SQL queries, extract, print and search metadata, import and export PostgreSQL database data and much more."
17,"Data Export for PostgreSQL - tool to export PostgreSQL database data quickly to any of 19 available formats, including MS Access, MS Excel, MS Word, RTF, HTML, TXT, ODF and more. Data Export for PostgreSQL has a kata kata lucu friendly wizard, which allows you to set various options of PostgreSQL export process visually and a command-line utility to automate your PostgreSQL export jobs using the configuration file."
17,"Data Import for PostgreSQL - tool to import data to PostgreSQL tables from MS Excel 97-2007, MS Access, DBF, TXT, CSV, MS Word 2007, RTF, ODF and HTML files. This utility allows you to quickly import data to one or several PostgreSQL tables or views at once, save all PostgreSQL import parameters set on current wizard session, use special batch insert mode to import PostgreSQL data at the maximum possible speed and much more."
17,"Data Pump for PostgreSQL - migration tool for converting databases and importing table data from an ADO-compatible source (e.g. MS Access, MS SQL database or any other database with ADO support) to PostgreSQL databases."
17,"Data Generator for PostgreSQL - tool for generating test data to PostgreSQL database tables. The utility can help you to simulate the database production environment and allows you to populate several PostgreSQL database tables with test data simultaneously, define tables for generating data, set value ranges, control a wide variety of generation parameters for each field type and much more."
17,DB Comparer for PostgreSQL - a tool for comparing PostgreSQL database schemas and discovering differences in their structures. You can view all the differences in compared database objects and execute an automatically generated script to synchronize structure of PostgreSQL databases and eliminate these differences.
17,DB Extract for PostgreSQL - easy-to-use tool for creating PostgreSQL database backups in a form of SQL scripts. This database script utility allows you to save metadata of all PostgreSQL database objects as well as PostgreSQL table data as database snapshots.
17,"SQL Query for PostgreSQL - a useful tool that lets you quickly and simply build SQL queries to PostgreSQL databases. Visual PostgreSQL query building, as well as direct editing of a query text, is available."
17,Data Comparer for PostgreSQL - tool for PostgreSQL data comparison and synchronization. Using this utility you can view all the differences in compared PostgreSQL tables and execute an automatically generated script to eliminate these differences.
17,SQL Maestro Group products for PostgreSQL
17,http://www.sqlmaestro.com/products/postgresql/
17,Windows
17,SQL Maestro Group offers a number of tools for PostgreSQL.
17,Maestro for PostgreSQL see PostgreSQL Clients.
17,"PostgreSQL Data Wizard provides you with a number of easy-to-use wizards to transfer any database to PostgreSQL, export data from PostgreSQL tables, views and queries to most popular formats, and import data from various sources into PostgreSQL tables."
17,PostgreSQL Code Factory is a
17,GUI tool aimed at the SQL queries and scripts development.
17,PostgreSQL Data Sync is a powerful and easy-to-use tool for database contents comparison and synchronization.
17,PostgreSQL PHP Generator Professional is a frontend that allows you to generate high-quality PHP applications for your database in a few mouse clicks.
17,"SQL Maestro Group also produces similar tools for MySQL, Oracle, MS SQL Server, SQLite, Firebird, DB2, SQL Anywhere, and MaxDB."
17,Datanamic DataDiff for PostgreSQL
17,http://www.datanamic.com/datadiff-for-postgresql/
17,Windows
17,"Datanamic DataDiff for PostgreSQL is a utility for data comparison and synchronization. Compare data for selected tables in two databases, view differences and publish changes quickly and safely. Flexible comparison and synchronization settings will enable you to set up a customized comparison key and to select tables and fields for comparison and for synchronization."
17,"DB Data Difftective can be used for data migrations, verification of (corrupt) data, data auditing etc."
17,Datanamic SchemaDiff for PostgreSQL
17,http://www.datanamic.com/schemadiff-for-postgresql/index.html
17,Windows
17,"Datanamic SchemaDiff for PostgreSQL is a tool for comparison and synchronization of database schemas. It allows you to compare and synchronize tables, views, functions, sequences (generators), stored procedures, triggers and constraints between two databases."
17,DB MultiRun PostgreSQL Edition
17,http://www.datanamic.com/multirun/index.html
17,Windows
17,DB MultiRun is a simple tool to execute multiple SQL scripts on multiple databases quickly.
17,"Define a list of databases, add SQL scripts to execute on these databases and click ""execute"" to run those scripts on the databases in the list. The multi-threaded execution of the SQL scripts makes it complete the task fast. After execution of the scripts, you can examine the results of the executed scripts on each database."
17,DB Doc for PostgreSQL
17,https://www.yohz.com/dbdoc_details.htm
17,Windows
17,"DB Doc helps you document your database schema and generate shareable PDF, HTML, XML, and Microsoft Word document in only 5 steps."
17,"Furthermore, the layout of the generated documents are customizable."
17,SQL Blob Export
17,http://www.yohz.com/sbe_details.htm
17,Windows
17,SQL Blob Export exports unlimited images and files from your tables or queries in 5 simple steps.
17,SQL File Import
17,http://www.yohz.com/sfi_overview.htm
17,Windows
17,"SQL File Import allows you to upload files, images, and other data into your database, without having to write any SQL statements."
17,"SQL File Import supports PostgreSQL, Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL)."
17,A scripting engine allows you to transform data before importing them into your database.
17,A command line version is also included to allow you to perform unattended upload/import tasks.
17,SQL Image Viewer
17,http://www.yohz.com/siv_details.htm
17,Windows
17,"SQL Image Viewer allows you to retrieve, view, convert and export images stored in Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL). It supports the following image formats: BMP, GIF, JPG, PNG, PSD, and TIFF."
17,"It also allows you to export binary data and recognises the following binary file types: PDF, MP3, WAV, 7Z, BZ2, GZ, RAR, ZIP, and has experimental support for DOC, PPT and XLS file types."
17,A command line version is also included to allow you to perform unattended scheduled exports of binary data.
17,SQL Multi Select
17,http://www.yohz.com/sms_details.htm
17,Windows
17,SQL Multi Select is a query tool that allows you to run multiple scripts on multiple servers with a single click.
17,"Result sets from different servers are consolidated into a single view, allowing for easy comparison and analysis."
17,Devart SSIS Data Flow Components for PostgreSQL
17,https://www.devart.com/ssis/
17,Windows
17,Devart SSIS Data Flow Components for PostgreSQL allow you to integrate database and cloud data via SQL Server Integration Services (SSIS).
17,"Devart SSIS Data Flow Components provide easy to set up cost-effective data integration using SSIS ETL engine. They provide high performance data loading, convenient component editors, SQL support for cloud data sources and lots of data source specific features."
17,EDB Postgres Enterprise Manager
17,http://www.enterprisedb.com/products/postgres-enterprise-manager
17,"Windows, Mac OS X, Linux"
17,"Postgres Enterprise Manager is the only solution available today that allows you to intelligently manage, monitor, and tune large scale Postgres installations from a single GUI console."
17,"Monitoring features include: server auto-discovery, over 225 pre-configured ready to run probes, custom probes, alert management, personalized alerts, remote monitoring, versatile charting, custom dashboards and web client."
17,"DBA tools include: database objects management, Postgres Expert (best practice configuration settings), Audit Manager, Log Manager, Log Analysis Expert, Capacity Manager and Team Support."
17,"Developer tools include: Query Tool, Data Grid, SQL Profiler, SQL Debugger and Import tools."
17,"Tuning tools include: At-A-Glance performance dashboards, Tuning Wizard, Performance Diagnostics and Index Advisor."
17,ClusterControl by Severalnines
17,https://severalnines.com/product/clustercontrol/for_postgresql
17,"ClusterControl is an all-inclusive open source database management system that allows you to deplore, monitor, manage and scale your database environments. ClusterControl provides the basic functionality you need to get PostgreSQL up-and-running using our deployment wizard, monitoring and basic management abilities like automatic failover, backups, and restores."
17,Point and Click Replication Deployments - ClusterControl allows you to easily deploy and configure master/slave replication PostgreSQL instances.
17,Advanced Performance Monitoring - ClusterControl monitors queries and detects anomalies with built-in alerts.
17,Automated Failover Handling - ClusterControl detects master failures and automatically promotes a new master
17,"Database Automation - ClusterControl lets you manage configurations, schedule, and restore backups."
17,Reportizer
17,https://www.reportizer.net
17,"Reportizer is a database reporting tool, which allows easy creating, modifying, and printing database reports from different types of databases, including PostgreSQL. Reports can be edited in convenient visual report builder or in text mode. It supports calculating fields, multi-column reports, expressions, grouping, displaying images etc. Reportizer can export reports to HTML, XLSX, image, or internal format. There is an ability to load and print reports from command line. Reportizer allows to manage report collections, which can be held either in files or in database tables."
17,Exportizer Enterprise
17,https://www.vlsoftware.net/exportizer/
17,"Exportizer Enterprise is a database export tool, which can work with PostgreSQL database either as source or destination. It allows to export data to database, file, clipboard, or printer."
17,"Possible sources: ODBC data sources, files of DB (Paradox), DBF (dBase, FoxPro), MDB, ACCDB, XLS, XLSX, GDB, IB, FDB, HTML, UDL, DBC, TXT, CSV types, databases specified by ADO connection strings, and databases like Oracle, SQL Server, PostgreSql, DB2, Informix, SQLite, Interbase etc."
17,"Possible destinations: file formats like text, CSV, XLS, XLSX, RTF, XML, HTML, PDF, DBF, SLK, SQL script, and relational database of any supported type including PostgreSQL."
17,It is possible to export all or selected tables from an open database at once.
17,The data migration can be done in super-fast batch mode.
17,"Exportizer Enterprise can automatically detect the most known image types (JPEG, PNG, GIF, BMP, ICO) in BLOB fields and export them, for example, to HTML or XLSX."
17,Images and other BLOB data can be exported to multiple separate files during a single export operation.
17,There is an ability to specify the source-to-target field mappings.
17,Export operations can be performed either via the program interface or via command line.
17,TiCodeX SQL Schema Compare
17,https://www.ticodex.com/
17,TiCodeX SQL Schema Compare is a tools that allows database administrators to compare multiple database schema in order to manage versioning.
17,"The software runs on Windows, Linux and Mac and supports Microsoft SQL (MS-SQL), MySQL, PostgreSQL, Azure SQL and MS-SQL on Amazon RDS."
17,Key Features:
17,"Runs on Windows, Linux and MacOS"
17,"Localized in English, German and Italian"
17,Compare changes between two SQL Database schemas (as example from development to test to production)
17,View database differences and explore schema changes to see what's going on
17,Automatically create full database migration scripts
17,Securely save database and server login details
17,pgMustard
17,https://www.pgmustard.com/
17,"pgMustard is a performance tool for PostgreSQL that provides a user interface for your explain analyze output, as well as tips on what to do to speed up your query."
17,Features:
17,Performance advice – scored by estimated time-saving potential
17,Per-operation timings¹
17,The number of rows¹ returned by each operation
17,A timing bar – that pins to keep it in context
17,Collapsible subtrees – with fast ones collapsed by default
17,Descriptions of operations
17,Links to blog posts for further reading
17,"¹ Taking loops, threads, subplans, and CTEs into account."
17,Requirements:
17,"TEXT or JSON format query plans, from PostgreSQL 9.6 or newer"
17,The interface and advice are in English
17,"A GitHub or Google account, for signing in"
17,"Web application, no installation required"
17,ODBC Driver for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/odbc/
17,"The CData ODBC Driver for PostgreSQL enables real-time access to PostgreSQL data, directly from any applications that support ODBC connectivity, the most widely supported interface for connecting applications with data. The driver wraps the complexity of accessing PostgreSQL data in a standard ODBC driver compliant with ODBC 3.8. The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
17,JDBC Driver for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/jdbc/
17,"The CData JDBC Driver for PostgreSQL offers the most natural way to connect to PostgreSQL data from Java-based applications and developer technologies. The driver wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, 100%-Java JDBC driver."
17,"The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
17,ADO.NET Provider for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/ado/
17,"The CData ADO.NET Provider for PostgreSQL offers the most natural way to access PostgreSQL data from .NET applications. The provider wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, fully managed ADO.NET Data Provider."
17,"The provider hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
17,Excel Add-In
17,for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/excel/
17,"The CData Excel Add-In provides the easiest way to connect to PostgreSQL data from Excel. From the CData ribbon, you can select PostgreSQL data as tables and columns into the spreadsheet. The spreadsheet is then linked with the remote data. To update the data, edit the spreadsheet."
17,SSIS Components for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/ssis/
17,The CData SSIS Components for PostgreSQL enable you to connect SQL Server with PostgreSQL data through SSIS Workflows. The components wrap the complexity of accessing PostgreSQL data in standard SSIS data flow components. You can then connect and synchronize PostgreSQL tables with SQL Server tables.
17,"The components hide the complexity of accessing data and provide additional security features, smart caching, batching, socket management, and more."
17,Power BI Connector for PostgreSQL by CData
17,https://www.cdata.com/drivers/postgresql/powerbi/
17,The CData Power BI Connector for PostgreSQL offers self-service integration with Microsoft Power BI. The connector facilitates live access to PostgreSQL data in Power BI from the Get Data window. The connector also provides direct querying to visualize and analyze PostgreSQL data.
17,Other Resources
17,"PostgreSQL Clients - list of UI's for accessing your db contents (enter sql, get back the results)"
17,Design Tools - tools for designing and visualizing database schemas
17,"Old possibly abandoned projects, see Community_Guide_to_PostgreSQL_Tools_Abandoned"
17,"Retrieved from ""https://wiki.postgresql.org/index.php?title=Community_Guide_to_PostgreSQL_GUI_Tools&oldid=38452"""
17,Navigation menuPage actionsPageDiscussionView sourceHistoryPage actionsPageDiscussionMoreToolsIn other languagesPersonal toolsLog inNavigationMain PageRandom pageRecent changesHelpToolsWhat links hereRelated changesSpecial pagesPrintable versionPermanent linkPage informationSearch
17,"This page was last edited on 1 December 2023, at 19:59.Privacy policyAbout PostgreSQL wikiDisclaimers"
19,SAS Help CenterYou need to enable JavaScript to run this app.
20,Performance tuning - Azure Cosmos DB for PostgreSQL | Microsoft Learn
20,Skip to main content
20,This browser is no longer supported.
20,"Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
20,Download Microsoft Edge
20,More info about Internet Explorer and Microsoft Edge
20,Table of contents
20,Exit focus mode
20,Read in English
20,Save
20,Table of contents
20,Read in English
20,Save
20,Edit
20,Print
20,Twitter
20,LinkedIn
20,Facebook
20,Email
20,Table of contents
20,Performance tuning in Azure Cosmos DB for PostgreSQL
20,Article
20,05/09/2023
20,2 contributors
20,Feedback
20,In this article
20,APPLIES TO:
20,Azure Cosmos DB for PostgreSQL (powered by the Citus database
20,extension to PostgreSQL)
20,Running a distributed database at its full potential offers high performance.
20,"However, reaching that performance can take some adjustments in application"
20,code and data modeling. This article covers some of the most common--and
20,effective--techniques to improve performance.
20,Client-side connection pooling
20,A connection pool holds open database connections for reuse. An application
20,"requests a connection from the pool when needed, and the pool returns one that"
20,"is already established if possible, or establishes a new one. When done, the"
20,application releases the connection back to the pool rather than closing it.
20,Adding a client-side connection pool is an easy way to boost application
20,"performance with minimal code changes. In our measurements, running single-row"
20,insert statements goes about 24x faster on a cluster with pooling enabled.
20,"For language-specific examples of adding pooling in application code, see the"
20,app stacks guide.
20,Note
20,Azure Cosmos DB for PostgreSQL also provides server-side connection
20,"pooling using pgbouncer, but it mainly serves"
20,to increase the client connection limit. An individual application's
20,performance benefits more from client- rather than server-side pooling.
20,(Although both forms of pooling can be used at once without harm.)
20,Scoping distributed queries
20,Updates
20,"When updating a distributed table, try to filter queries on the distribution"
20,"column--at least when it makes sense, when the new filters don't change the"
20,meaning of the query.
20,"In some workloads, it's easy."
20,Transactional/operational workloads like
20,multi-tenant SaaS apps or the Internet of Things distribute tables by tenant or
20,device. Queries are scoped to a tenant- or device-ID.
20,"For instance, in our multi-tenant"
20,tutorial
20,we have an ads table distributed by company_id. The naive way to update an
20,ad is to single it out like this:
20,-- slow
20,UPDATE ads
20,SET impressions_count = impressions_count+1
20,WHERE id = 42; -- missing filter on distribution column
20,"Although the query uniquely identifies a row and updates it, Azure Cosmos DB for PostgreSQL"
20,"doesn't know, at planning time, which shard the query will update. The Citus extension takes a"
20,"ShareUpdateExclusiveLock on all shards to be safe, which blocks other queries"
20,trying to update the table.
20,"Even though the id was sufficient to identify a row, we can include an"
20,extra filter to make the query faster:
20,-- fast
20,UPDATE ads
20,SET impressions_count = impressions_count+1
20,WHERE id = 42
20,AND company_id = 1; -- the distribution column
20,The Azure Cosmos DB for PostgreSQL query planner sees a direct filter on the distribution
20,"column and knows exactly which single shard to lock. In our tests, adding"
20,filters for the distribution column increased parallel update performance by
20,100x.
20,Joins and CTEs
20,We've seen how UPDATE statements should scope by the distribution column to
20,"avoid unnecessary shard locks. Other queries benefit from scoping too, usually"
20,to avoid the network overhead of unnecessarily shuffling data between worker
20,nodes.
20,"-- logically correct, but slow"
20,WITH single_ad AS (
20,SELECT *
20,FROM ads
20,WHERE id=1
20,SELECT *
20,FROM single_ad s
20,JOIN campaigns c ON (s.campaign_id=c.id);
20,"We can speed up the query up by filtering on the distribution column,"
20,"company_id, in the CTE and main SELECT statement."
20,"-- faster, joining on distribution column"
20,WITH single_ad AS (
20,SELECT *
20,FROM ads
20,WHERE id=1 and company_id=1
20,SELECT *
20,FROM single_ad s
20,JOIN campaigns c ON (s.campaign_id=c.id)
20,WHERE s.company_id=1 AND c.company_id = 1;
20,"In general, when joining distributed tables, try to include the distribution"
20,"column in the join conditions. However, when joining between a distributed and"
20,"reference table it's not required, because reference table contents are"
20,replicated across all worker nodes.
20,"If it seems inconvenient to add the extra filters to all your queries, keep in"
20,mind there are helper libraries for several popular application frameworks that
20,make it easier. Here are instructions:
20,"Ruby on Rails,"
20,"Django,"
20,"ASP.NET,"
20,Java Hibernate.
20,Efficient database logging
20,"Logging all SQL statements all the time adds overhead. In our measurements,"
20,using more a judicious log level improved the transactions per second by
20,10x vs full logging.
20,"For efficient everyday operation, you can disable logging except for errors and"
20,abnormally long-running queries:
20,setting
20,value
20,reason
20,log_statement_stats
20,OFF
20,Avoid profiling overhead
20,log_duration
20,OFF
20,Don't need to know the duration of normal queries
20,log_statement
20,NONE
20,Don't log queries without a more specific reason
20,log_min_duration_statement
20,A value longer than what you think normal queries should take
20,Shows the abnormally long queries
20,Note
20,The log-related settings in our managed service take the above
20,"recommendations into account. You can leave them as they are. However, we've"
20,"sometimes seen customers change the settings to make logging aggressive,"
20,which has led to performance issues.
20,Lock contention
20,The database uses locks to keep data consistent under concurrent access.
20,"However, some query patterns require an excessive amount of locking, and faster"
20,alternatives exist.
20,System health and locks
20,"Before diving into common locking inefficiencies, let's see how to view locks"
20,and activity throughout the database cluster. The
20,citus_stat_activity view
20,gives a detailed view.
20,"The view shows, among other things, how queries are blocked by ""wait events,"""
20,including locks.
20,Grouping by
20,wait_event_type
20,paints a picture of system health:
20,-- general system health
20,"SELECT wait_event_type, count(*)"
20,FROM citus_stat_activity
20,WHERE state != 'idle'
20,GROUP BY 1
20,ORDER BY 2 DESC;
20,A NULL wait_event_type means the query isn't waiting on anything.
20,"If you do see locks in the stat activity output, you can view the specific"
20,blocked queries using citus_lock_waits:
20,SELECT * FROM citus_lock_waits;
20,"For example, if one query is blocked on another trying to update the same row,"
20,you'll see the blocked and blocking statements appear:
20,-[ RECORD 1 ]-------------------------+--------------------------------------
20,waiting_gpid
20,| 10000011981
20,blocking_gpid
20,| 10000011979
20,blocked_statement
20,| UPDATE numbers SET j = 3 WHERE i = 1;
20,current_statement_in_blocking_process | UPDATE numbers SET j = 2 WHERE i = 1;
20,waiting_nodeid
20,| 1
20,blocking_nodeid
20,| 1
20,"To see not only the locks happening at the moment, but historical patterns, you"
20,"can capture locks in the PostgreSQL logs. To learn more, see the"
20,log_lock_waits
20,server setting in the PostgreSQL documentation. Another great resource is
20,seven tips for dealing with
20,locks
20,on the Citus Data Blog.
20,Common problems and solutions
20,DDL commands
20,"DDL Commands like truncate, drop, and create index all take write locks,"
20,and block writes on the entire table. Minimizing such operations reduces
20,locking issues.
20,Tips:
20,"Try to consolidate DDL into maintenance windows, or use them less often."
20,PostgreSQL supports building indices
20,"concurrently,"
20,to avoid taking a write lock on the table.
20,Consider setting
20,lock_timeout
20,"in a SQL session prior to running a heavy DDL command. With lock_timeout,"
20,PostgreSQL will abort the DDL command if the command waits too long for a write
20,lock. A DDL command waiting for a lock can cause later queries to queue behind
20,itself.
20,Idle in transaction connections
20,Idle (uncommitted) transactions sometimes block other queries unnecessarily.
20,For example:
20,BEGIN;
20,UPDATE ... ;
20,-- Suppose the client waits now and doesn't COMMIT right away.
20,-- Other queries that want to update the same rows will be blocked.
20,COMMIT; -- finally!
20,"To manually clean up any long-idle queries on the coordinator node, you can run"
20,a command like this:
20,SELECT pg_terminate_backend(pid)
20,FROM pg_stat_activity
20,WHERE datname = 'citus'
20,AND pid <> pg_backend_pid()
20,AND state in ('idle in transaction')
20,AND state_change < current_timestamp - INTERVAL '15' MINUTE;
20,PostgreSQL also offers an
20,idle_in_transaction_session_timeout
20,setting to automate idle session termination.
20,Deadlocks
20,"Azure Cosmos DB for PostgreSQL detects distributed deadlocks and cancels their queries, but the"
20,situation is less performant than avoiding deadlocks in the first place. A
20,common source of deadlocks comes from updating the same set of rows in a
20,different order from multiple transactions at once.
20,"For instance, running these transactions in parallel:"
20,Session A:
20,BEGIN;
20,UPDATE ads SET updated_at = now() WHERE id = 1 AND company_id = 1;
20,UPDATE ads SET updated_at = now() WHERE id = 2 AND company_id = 1;
20,Session B:
20,BEGIN;
20,UPDATE ads SET updated_at = now() WHERE id = 2 AND company_id = 1;
20,UPDATE ads SET updated_at = now() WHERE id = 1 AND company_id = 1;
20,-- ERROR:
20,canceling the transaction since it was involved in a distributed deadlock
20,"Session A updated ID 1 then 2, whereas the session B updated 2 then 1. Write"
20,SQL code for transactions carefully to update rows in the same order. (The
20,"update order is sometimes called a ""locking hierarchy."")"
20,"In our measurement, bulk updating a set of rows with many transactions went"
20,3x faster when avoiding deadlock.
20,I/O during ingestion
20,I/O bottlenecking is typically less of a problem for Azure Cosmos DB for PostgreSQL than
20,for single-node PostgreSQL because of sharding. The shards are individually
20,"smaller tables, with better index and cache hit rates, yielding better"
20,performance.
20,"However, even with Azure Cosmos DB for PostgreSQL, as tables and indices grow larger, disk"
20,I/O can become a problem for data ingestion.
20,Things to look out for are an
20,increasing number of 'IO' wait_event_type entries appearing in
20,citus_stat_activity:
20,"SELECT wait_event_type, wait_event count(*)"
20,FROM citus_stat_activity
20,WHERE state='active'
20,"GROUP BY 1,2;"
20,Run the above query repeatedly to capture wait event related information. Note
20,how the counts of different wait event types change.
20,"Also look at metrics in the Azure portal,"
20,particularly the IOPS metric maxing out.
20,Tips:
20,"If your data is naturally ordered, such as in a time series, use PostgreSQL"
20,table partitioning. See this
20,guide to learn
20,how to partition distributed tables.
20,Remove unused indices. Index maintenance causes I/O amplification during
20,ingestion.
20,"To find which indices are unused, use this"
20,query.
20,"If possible, avoid indexing randomized data. For instance, some UUID"
20,generation algorithms follow no order. Indexing such a value causes a lot
20,"overhead. Try a bigint sequence instead, or monotonically increasing UUIDs."
20,Summary of results
20,"In benchmarks of simple ingestion with INSERTs, UPDATEs, transaction blocks, we"
20,observed the following query speedups for the techniques in this article.
20,Technique
20,Query speedup
20,Scoping queries
20,100x
20,Connection pooling
20,24x
20,Efficient logging
20,10x
20,Avoiding deadlock
20,Next steps
20,Advanced query performance tuning
20,Useful diagnostic queries
20,Build fast app stacks
20,Feedback
20,Coming soon: Throughout 2024 we will be phasing out GitHub Issues as the feedback mechanism for content and replacing it with a new feedback system. For more information see: https://aka.ms/ContentUserFeedback.
20,Submit and view feedback for
20,This product
20,This page
20,View all page feedback
20,Additional resources
20,California Consumer Privacy Act (CCPA) Opt-Out Icon
20,Your Privacy Choices
20,Theme
20,Light
20,Dark
20,High contrast
20,Previous Versions
20,Blog
20,Contribute
20,Privacy
20,Terms of Use
20,Trademarks
20,© Microsoft 2024
20,Additional resources
20,In this article
20,California Consumer Privacy Act (CCPA) Opt-Out Icon
20,Your Privacy Choices
20,Theme
20,Light
20,Dark
20,High contrast
20,Previous Versions
20,Blog
20,Contribute
20,Privacy
20,Terms of Use
20,Trademarks
20,© Microsoft 2024
21,Reddit - Dive into anything
21,Skip to main content
21,Reddit and its partners use cookies and similar technologies to provide you with a better experience.
21,"By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising."
21,"By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform."
21,"For more information, please see our"
21,Cookie Notice
21,and our
21,Privacy Policy.
21,Open menu
21,Open navigation
21,Go to Reddit Home
21,r/freebsd
21,A chip
21,A close button
21,Get app
21,Get the Reddit app
21,Log In
21,Log in to Reddit
21,Expand user menu
21,Open settings menu
21,Log In / Sign Up
21,Advertise on Reddit
21,Shop Collectible Avatars
21,Get the Reddit app
21,Scan this QR code to download the app now
21,Or check it out in the app stores
21,Go to freebsd
21,r/freebsd
21,r/freebsd
21,Unofficial subreddit for The FreeBSD Project
21,Members
21,Online
21,Dense_Care8224
21,ADMIN
21,MOD
21,mysql zfs performance tuning tips?
21,trying to get a bit more performance out of mysql running on FreeBSD and wondering if there are any dataset settings to optimize for performance - even if a mysql re-install is required with a brand new dataset / mountpoint ? thanks
21,Read more
21,&nbsp;
21,TOPICS
21,Gaming
21,Valheim
21,Genshin Impact
21,Minecraft
21,Pokimane
21,Halo Infinite
21,Call of Duty: Warzone
21,Path of Exile
21,Hollow Knight: Silksong
21,Escape from Tarkov
21,Watch Dogs: Legion
21,Sports
21,NFL
21,NBA
21,Megan Anderson
21,Atlanta Hawks
21,Los Angeles Lakers
21,Boston Celtics
21,Arsenal F.C.
21,Philadelphia 76ers
21,Premier League
21,UFC
21,Business
21,GameStop
21,Moderna
21,Pfizer
21,Johnson & Johnson
21,AstraZeneca
21,Walgreens
21,Best Buy
21,Novavax
21,SpaceX
21,Tesla
21,Crypto
21,Cardano
21,Dogecoin
21,Algorand
21,Bitcoin
21,Litecoin
21,Basic Attention Token
21,Bitcoin Cash
21,Television
21,The Real Housewives of Atlanta
21,The Bachelor
21,Sister Wives
21,90 Day Fiance
21,Wife Swap
21,The Amazing Race Australia
21,Married at First Sight
21,The Real Housewives of Dallas
21,My 600-lb Life
21,Last Week Tonight with John Oliver
21,Celebrity
21,Kim Kardashian
21,Doja Cat
21,Iggy Azalea
21,Anya Taylor-Joy
21,Jamie Lee Curtis
21,Natalie Portman
21,Henry Cavill
21,Millie Bobby Brown
21,Tom Hiddleston
21,Keanu Reeves
21,RESOURCES
21,About Reddit
21,Advertise
21,Help
21,Blog
21,Careers
21,Press
21,Communities
21,Best of Reddit
21,Topics
21,Impressum
21,Content Policy
21,Privacy Policy
21,User Agreement
21,"Reddit, Inc. © 2024. All rights reserved."
21,Top 4%
21,Rank by size
21,Top Posts
21,Reddit
21,"reReddit: Top posts of March 30, 2023"
21,Reddit
21,reReddit: Top posts of March 2023
21,Reddit
21,reReddit: Top posts of 2023
22,How To Master PostgreSQL Performance Like Never Before | Metis
22,"ProductPreventionOptimize queries, prevent slowdownsMonitoringElevate database monitoring and performanceTroubleshootingInstant query fixes, database optimization simplifiedPricingResources"
22,"ResourcesFind all the information, learning and support you want"
22,BlogDiscover best-in-class database and infrastructure knowledge
22,"Events and WebinarsExplore upcoming events, love and on-demand webinars"
22,DocumentationLearn how to use Metis
22,The Database CommunityJoin other developers and devops with a passion for databases
22,VideosLearn how to use Metis and what is new in the infrastructure world
22,"Knowledge BaseEverything you need to know about databases and their management systemsCompanyAboutDev confidence, prevent glitches with MetisCareersFind your future with usContactConnect with our teamJoin our CommunityGitHubLoginBook a DemoStart for Free"
22,"All PostsCategory5 min readHow To Master PostgreSQL Performance Like Never BeforePostgreSQL has become super popular thanks to its flexibility and strength - it's this really powerful open-source database that can handle all sorts of tasks, from small projects to huge enterprise needs. But as data keeps getting bigger and bigger, it's super important to tweak things in PostgreSQL to make it run faster and smoother. Let's see how to do that.Published onJanuary 22, 2024Share this postContributorsAdam Furmanek"
22,Dev RelItay Braun
22,"CTOTable of ContentsThis is also a headingThis is a headingSee how Metis can make your database 3x faster and 50% cheaper!Book a DemoPostgreSQL has become super popular thanks to its flexibility and strength - it's this really powerful open-source database that can handle all sorts of tasks, from small projects to huge enterprise needs. But as data keeps getting bigger and bigger, it's super important to tweak things in PostgreSQL to make it run faster and smoother. Doing stuff like fine-tuning queries, setting up indexes smartly, and tweaking settings can make a huge difference. These tweaks aren't just about speed - they're about making PostgreSQL even more reliable and adaptable, keeping it at the top of the game in our data-heavy world.Database performance enhancement is crucial for our business. Slow databases can affect all the aspects of our application, no matter in which domain we work. It’s crucial to include query plan optimization as a natural step during the development or maintenance. Apart from that, we should also focus on other aspects of PostgreSQL query tuning like indexing strategies or join optimizations. In this article, we’re going to understand how to address various aspects of PostgreSQL performance optimizations to make sure our applications never slow down.Understanding PostgreSQL Performance OptimizationEnhancing PostgreSQL performance involves following this workflow:Gather metrics and metadata to establish a baseline.Investigate the underlying causes of each performance issue.If the existing data is insufficient to pinpoint the root cause, gather additional information.Examine the collected baseline data for any anomalies.Performance problems usually happen for the following reasons:IO Inefficiency: This occurs when the query reads numerous rows or when a small number of rows contain substantial data, typically in the form of JSON, images, or XML.CPU Inefficiency: The query consumes a significant amount of CPU resources.Insufficient Memory: In cases where multiple queries contend for limited cache space, the server continually reads data from the hard drive to cache and flushes the cache back to the hard drive.Concurrency Issues: Many concurrent queries attempt to access a resource (read or write), leading to blocking situations where SQL commands wait for others to complete.Slow Network: While the database server swiftly executes the query, the time taken to send the result set back to the application server is prolonged due to a slow network.Suboptimal Configuration: Poorly configured settings contribute to the suboptimal performance of the query engine.Suboptimal execution plans: When essential statistics are absent, the Query Planner suggests suboptimal plans, resulting in the generation of inefficient queries.The Query PlannerSQL is declarative. The SQL command states what data to retrieve: which tables, which column, how to filter the data… However, the SQL command doesn’t state how exactly to run the query.Let’s look at this example:SELECT"
22,"orders.order_id,"
22,"orders.order_date,"
22,"orders.order_amount,"
22,"customers.customer_id,"
22,customers.customer_name
22,FROM
22,orders
22,JOIN
22,customers ON orders.customer_id = customers.customer_id
22,WHERE
22,orders.order_date > ‘2022-01-01’
22,AND orders.order_amount > 1000
22,ORDER BY
22,orders.order_date DESC;
22,"Just by looking at the SQL, we don’t know anything about the actual execution of the query: Which indexes exist on the relevant tables? Which indexes are the most efficient? How many rows are read to return the data?How exactly are the tables joined together? Are there any implicit conversions? How sorting the data affects the order of retrieving the data and joining it? The PostgreSQL Query Planner optimizes SQL queries through a cost-based approach, considering factors such as available indexes, table statistics, and join strategies. It determines parallel execution, may rewrite queries, and adapts based on actual performance feedback. Key responsibilities include:Cost-Based Optimization: Evaluate execution plan costs.Statistics: Use up-to-date table and index statistics.Available Indexes: Consider index usage for efficient data retrieval.Join Strategies: Choose optimal join methods for multiple tables.Parallel Execution: Determine when to execute queries in parallel.Query Rewriting: Transform queries for improved efficiency.Feedback Mechanism: Adapt based on feedback about actual performance.User Hints: Allow advanced users to provide optimization hints.Indexing Strategies for PostgreSQLCreating indexes for a database can be a complex task. Here are some guidelines to simplify the process. Start with educated guessesInitially, feel free to adopt a simple or best-effort approach and rely on informed speculation. Once you have concrete data on usage patterns and identify queries causing bottlenecks in your database, leverage this information to determine which indices to establish and which to remove. Keep in mind that indices come with a cost, so choose judiciously based on the observed usage patterns.Detect the slow queries and their access methodThe best strategy is a blend of rigorous performance tests to prevent inefficient queries from being deployed to production. There are tools that can make this task pretty straightforward. Additionally, actively monitor slow queries in the production environment to adapt and fine-tune index choices based on real-world usage. This iterative approach ensures optimal performance without sacrificing production stability.It's important to know how each query is accessing data (Execution Plan). It might already be using an index, and if there's a performance problem, it could be due to other factors. Sometimes, even if an index exists, the system might decide not to use it. Understanding these aspects helps solve performance issues more effectively.Recommended reading: Reading Postgres Execution Plans Made EasyVerify the index is usedEven if there's an index, the query planner might choose not to use it, for example, when the WHERE clause involves a function.-- Good"
22,WHERE start_date BETWEEN ‘2024-01-01’ AND ‘2024-12’31
22,-- Bad. There is a function on the start_date column
22,WHERE YEAR(start_date) = 2024
22,"Multi-column indexesWhen you have a complicated condition in your WHERE clause, it's helpful to build an index on multiple columns instead of having many separate ones. This way, the database can efficiently handle queries with intricate conditions, improving performance by using a single index structure instead of multiple ones.Covering IndexesWe use the term ""covering index"" when an index can satisfy an entire query on its own, without needing to refer back to the original table or another index for additional data. Prioritize creating covering indexes for your most resource-demanding queries, weighing the benefits against the additional space required for dedicated indices. This works when the query needs only 3-5 columns, as creating an index with 6+ columns is rare.Advanced Indexes and Data StorageDatabases have evolved beyond a simple table with a B-tree index. New types of indexes, each with a specific purpose, have been introduced, making them a practical choice for optimizing certain types of queries.Column Store Index for Analytics: A column store index organizes data by columns rather than rows, making it efficient for analytical queries that involve aggregations and reporting. It's optimized for scenarios where a large number of rows are scanned and grouped.Hyper Tables for Time Series: Hyper tables are specifically designed for efficient storage and retrieval of time series data. They provide features like automatic partitioning and retention policies, time functions, and filling gaps, making them well-suited for applications dealing with time-based data, such as IoT sensors, log files, or financial time series.BRIN (Block Range Index) Index: BRIN indexes are suitable for large tables where data is sorted in chronological or numerical order. They divide the table into blocks and store the minimum and maximum values for each block, minimizing the storage overhead. BRIN indexes are useful for scenarios where data exhibits a natural ordering, such as time-series data.GIN (Generalized INdex) Index for JSON: GIN indexes are designed for efficient querying of JSON data. They allow for indexing individual elements within JSON documents, enabling fast searches and retrievals. GIN indexes are valuable when dealing with complex and nested JSON structures commonly found in modern applications.Bloom Filter Index: A Bloom filter is a probabilistic data structure used to test whether an element is a member of a set. It's particularly useful when the goal is to quickly eliminate non-members without the need for a full database lookup. Bloom filters are memory-efficient and find applications in scenarios like caching, spell-checking, and distributed systems.[.tip-box]Plan optimization tip: Regularly analyze and tweak execution plans, focusing on indexing strategies and query structuring to reduce execution time and resource usage. Utilize EXPLAIN and EXPLAIN ANALYZE commands to deeply understand query execution paths and identify performance bottlenecks.[.tip-box]Optimizing Query Plans in PostgreSQLAn execution plan in a database outlines the steps the database engine will take to retrieve data for a specific query. For example, which indexes to use, how exactly to join the data, and when to sort the results.There are many differences between Actual and Estimated Execution Plans:Estimated Execution Plan:Is created by the query planner BEFORE the query is executed.The query is not executed. It is calculated based on statistics and assumptions, providing a prediction of how the database will process the query.Since the query is not executed, the plan is generated in a few milliseconds.Actual Execution Plan:Is generated after query execution, reflecting the real steps taken by the database engine.Provides accurate information about resource usage such as the number of rows scanned and execution times of each step.How to optimize a plan:Before generating execution plans, it's essential to prepare data and queries that closely resemble the conditions of the real-world production database. Simulating a realistic environment ensures that the execution plans obtained accurately reflect the challenges and nuances present in actual usage. Without this preparation, the generated plans may not provide meaningful insights into the performance characteristics of the queries in a production setting. Once it’s done, you can perform the following steps:Generate the Actual Execution Plan: Obtain the execution plan for the query using the EXPLAIN ANALYZE command.Review the Execution Plan: Examine the execution plan to understand the sequence of operations, access methods, and join strategies employed by the database engine. Since the plan can be a very long JSON text, use tools, such as Metis to identify the slowest steps.Identify Table Scans and Index Usage: usually, this is the first place to look for slow steps. Look for instances of full table scans or missing index usage. These can be performance bottlenecks, and optimizing them can significantly improve query speed.Evaluate Filter Conditions: Examine filter conditions and WHERE clauses to ensure they are selective. Indexes should be utilized to narrow down the dataset early in the query execution.Check Join Operations: Analyze join operations to ensure efficient join methods are being employed. Consider the use of appropriate indexes to enhance join performance. Verify the join columns have the exact data types and no data conversions are needed.Limit the Number of Returned Rows: Evaluate whether the query needs to return the entire result set or if limiting the number of rows can improve performance. Consider using the LIMIT or FETCH FIRST clauses, especially when dealing with large datasets. Update Statistics:Ensure that statistics on tables and indexes are up-to-date. The last update date is not saved inside the execution plan. For PostgreSQL, you can use a query like this:SELECT schemaname, relname, last_autoanalyze, last_analyze FROM pg_stat_all_tables WHERE relname = 'table_name';Monitor and Iterate:Execute the optimized query in a real-world scenario and monitor its performance. Iterate the optimization process based on actual execution results and continue refining the query as needed. By systematically analyzing the execution plan and addressing identified issues, you can fine-tune the query for optimal performance in your specific database environment. Regular monitoring and iteration ensure ongoing optimization as data distributions and usage patterns evolve.Advanced Performance Tuning TechniquesNow let’s delve into more advanced performance-tuning techniques.Table PartitioningPartitioning is a database design technique focusing on dividing large tables into smaller ones called partitions. The partitions are created based on multiple criteria like ranges of values or hashing algorithms. See our guide to efficient partitioning to see how this technique works.By partitioning tables, queries can access only a subset of data which results in faster query execution times. The database engine doesn’t need to read all the data but can read the appropriate partitions only. This way we bring fewer rows and effectively execute queries faster.Partitioning is especially useful for large databases which we can logically split into smaller ones based on business criteria, like dates, customer identifiers, tenant identifiers, or geographic locations. However, this requires the particular queries to work only within a subset of the partitions. If the query needs to read all the data, then partitioning won’t be helpful.Concurrency ControlConcurrency control in databases is about managing simultaneous access to the same data by multiple users or transactions while ensuring consistency and correctness. Since we want to achieve the maximum possible performance, we want multiple transactions to run at the same time and not wait for each other. However, several transactions might attempt to access or modify data concurrently, which can lead to various issues like lost updates, inconsistent reads, or conflicts. See our article about transaction isolation levels to learn more.We can achieve higher performance in many ways. We can let some transactions proceed without waiting for the latest data by using lower isolation levels (like Read Committed or Read Uncommitted). We can configure transactions to work with snapshots by using Multi-Version Concurrency Control (MVCC). We can also modify particular statements within transactions to use different isolation levels. All these techniques can improve performance by unblocking transactions from waiting.We need to understand that these techniques should be evaluated based on business requirements. While it’s attractive to claim that our application always works with the “latest data” and never shows outdated records, this is effectively impossible in distributed systems. The distance between New York and Sydney is 53 light-milliseconds. We can’t exchange information faster than light can travel. This effectively means that any information exchange between New York and Sydney to achieve consensus takes at least 212 milliseconds (two full round trips). Internet signals take longer to process (we typically assume around two-thirds of the speed of light) so we should expect at least 318 milliseconds. If we want to achieve global consistency, we need to limit ourselves to at most 3 transactions per second in theory (effectively it’s even worse).Taking the limits of physics into account, we need to evaluate how our business can deal with propagation delays. Maybe we don’t need to lock the records in order to process them, maybe we can run compensating transactions, or maybe we should accept the cost of making wrong decisions very infrequently.Advanced Configuration TweaksWe can tune various database parameters to increase the performance.The first aspect is shared memory. This comes down to shared_buffers, work_mem, and max_connections. These parameters control the amount of memory used for caching data, sorting, hashing, or for simultaneous connections. If we have a high amount of memory, we can increase these parameters to improve performance. See our article about PostgreSQL parameters to learn more.The second aspect is about disk I/O. This comes down to selecting proper drives (like high-performance NVMe drives or SSDs) or using proper RAID setups. We can also tune parameters to adjust the Write-Ahead Logging settings or the frequency of storing the data on the disk. These parameters are wal_level, effective_io_concurrency, or checkpoint_timeout. We can tune them to achieve the highest possible throughput.Last but not least, we can configure background maintenance tasks. One of them is vacuuming which we can configure with autovacuum_vacuum_scale_factor and autovacuum_max_workers. We can also trigger vacuuming manually. Another aspect is bloated indexes. We can rebuild them with the REINDEX command and we can restructure tables using an index with the CLUSTER command.Finally, we can also scale the database vertically and horizontally. Vertical scaling is when we replace the hardware with a more powerful one. Horizontal scaling is when we add more hardware to build a cluster of nodes that can share the load.Leveraging Monitoring Tools For Performance InsightsQuery optimization is hard. However, many steps can be automated and we can get good support from optimization tools. Monitoring tools provide real-time insights into the performance of your database. They allow you to identify and address performance bottlenecks, slow queries, and resource-intensive operations. By analyzing the data collected by monitoring tools, you can fine-tune your database configuration, indexes, and queries to improve overall performance. Let’s see some examples.MetisMetis can analyze queries and provide insights about each execution:With Metis, you don’t need to manually capture plans and look for common issues. Metis does that for you. It integrates with your application and your database, extracts the activity, and provides an explanation of what happened. You can use it to learn facts about the interaction, immediately get alerts about obvious issues (like lack of index), and get actionable ideas on how to rewrite the query to get it faster.Metis can also visualize the plans for you, analyze schema migrations, monitor the live performance, and integrate with your CI/CD pipeline to analyze pull requests.pgMustardAnother tool is pgMustard.It can visualize queries and suggest improvements:Reading query plans is hard. pgMustard can make it much easier by providing graphical explanations of each operation. While it doesn’t tell you immediately how to fix the query, it can be a good starting point for understanding why the query is slow.PostgreSQL Workload AnalyzerPostgreSQL Workload Analyzer (PoWA) can gather data from various extensions that can explain how queries work.PoWA can integrate with your database and correlate data from multiple sources. This gives a great understanding of what was happening around the time when the slow query was executed. Sometimes the problem with the query is not in the query itself but with things happening around it. PoWA can shed some light on the whole context.ConclusionSQL performance is crucial for our business. Slow databases affect our whole tech stack and in turn whole ecosystems. There are many techniques to tune the performance: it all starts with the query plan that explains what happened. We can tune data access patterns to use indexes or read less data. We can improve filtering and joins, denormalize the database, partition tables, play with concurrency, or do many more things specific to each particular database engine.There are many tools that can help with database performance. Some of them focus on bringing data from multiple sources, and some of them specialize in one particular aspect of the process. Metis is the only tool that takes a holistic approach and optimizes everything, from the very first code change to the production database.FAQsHow can I optimize a slow query in PostgreSQL?You can use various optimization techniques, including indexes, query restructuring, join optimizations, query plan analysis, scaling (both vertical and horizontal), sharding the data, denormalizing tables, or using specialized extensions.What are the best practices for index tuning in PostgreSQL?You need to pick the right indexes for your data types, include the columns that cover the whole query, modify filters to utilize indexes, and use specialized solutions for your business domain like JSON indexes or vector databases.How do I configure PostgreSQL for high performance?You start by tweaking shared memory and I/O parameters to reduce the number of reads from the drive. You then tune the number of connections and level of parallelism that your database supports. Next, you adjust the background tasks and make sure that statistics and fragmentation are under control. Finally, you focus on queries one by one to understand why they are slow.What tools are available for monitoring PostgreSQL performance?Metis is the only tool that takes a holistic approach to database tuning. It starts with analyzing developers’ environments to find slow queries and inefficient schema migrations. It integrates with CI/CD to stop dangerous changes from being deployed. Finally, it monitors the live performance of the production database to identify anomalies and configuration differences. There are other tools that help with monitoring like pgMustard that can visualize the queries and suggest improvements, or PoWA that can capture details from multiple extensions and present them together to build a bigger picture.What is the difference between vertical and horizontal scaling in PostgreSQL?Vertical scaling is when we replace the hardware with a more powerful one. Horizontal scaling is when we add more hardware to build a cluster of nodes that can share the load."
22,"Please enable JavaScript to view the comments powered by Disqus.This is some text inside of a div block. This is some text inside of a div block. This is some text inside of a div block. This is some text inside of a div block. This is some text inside of a div block.Related ContentMarch 11, 2024Database Chaos: Is Your Bottom Line Hanging By a Thread?In this article, we’re going to see how database bugs can negatively affect our business and how we can protect ourselves from dire consequences.Read more"
22,"March 4, 2024How Enabling Slow Query Log Enhances Postgres ObservabilityIn PostgreSQL, the slow query log is a feature that allows you to log queries that take longer than a specified threshold to execute. This log helps you identify and optimize queries that may be causing performance issues in your database. Let’s see how we can use it.Read more"
22,"February 26, 2024Why You Should Leverage Database Integration with OpenTelemetryDatabase observability is crucial for maintaining optimal performance and reliability in modern software systems. It enables organizations to monitor key metrics such as query execution time, resource utilization, and transaction throughput, facilitating the early detection and resolution of issues like slow queries or resource contention. Let's see how to build it.Read more"
22,"Ready to take your database to the next level?Start using Metis and get your database observability set up in minutesI'm Ready, Let's StartContact UsFirst nameLast nameEmailMessageThank you! Your submission has been received!Oops! Something went wrong while submitting the form.Link OneLink TwoLink ThreeLink FourLink FiveProductPreventionMonitoringTroubleshootingResourcesBlogDocumentationLive DemoPricingPricing PlansFAQCompanyAboutCareers"
22,Join Our Community
22,GitHub©2023 MetisPrivacy PolicyTerms of Service
25,Maximizing Efficiency with Parallel Queries in PostgreSQL: A Comprehensive Guide
25,"LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings."
25,Accept
25,Reject
25,Agree & Join LinkedIn
25,"By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy."
25,Sign in to view more content
25,Create your free account or sign in to continue your search
25,Sign in
25,Welcome back
25,Email or phone
25,Password
25,Show
25,Forgot password?
25,Sign in
25,"By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy."
25,New to LinkedIn? Join now
25,"By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy."
25,New to LinkedIn? Join now
25,Skip to main content
25,LinkedIn
25,Articles
25,People
25,Learning
25,Jobs
25,Join now
25,Sign in
25,Maximizing Efficiency with Parallel Queries in PostgreSQL: A Comprehensive Guide
25,Report this article
25,Shiv Iyer
25,Shiv Iyer
25,Founder CEO of #MinervaDB #ChistaDATA #MySQL #PostgreSQL #RocksDB #InnoDB #MariaDB #ClickHouse #Linux #Entreprenuer #Investor #Startup #OpenSource #BigData #Analytics #SRE #AI #ML #DevOps #OLTP #OLAP #SQLServer #SQL
25,"Published Jan 16, 2024"
25,+ Follow
25,"Making the best use of parallel queries in PostgreSQL can significantly improve the performance of your database, especially when dealing with large datasets. Here are some strategies and considerations to effectively utilize parallel queries:1. Understand When Parallelism is Effective:"
25,"- Parallel queries are most beneficial for CPU-bound operations and large I/O operations. They are particularly effective for large sequential scans, aggregates, and joins.2. Ensure Your Queries are Parallel-Aware:"
25,"- Not all queries can be executed in parallel. Check if your query is parallel-aware by explaining the query plan. Operations like sequential scans, aggregations, and some joins can benefit from parallel execution.3. Configure Parallel Settings Appropriately:"
25,- Adjust parallel-related settings in PostgreSQL:
25,- : Sets the maximum number of parallel workers that can be used by the database.
25,- : Determines the maximum number of parallel workers that can be started by a single Gather or Gather Merge node.
25,and : Control when a parallel scan is initiated.4. Optimize Your Data Model:
25,- Ensure your data model and indexes support parallel processing. Proper indexing can significantly impact the performance of parallel queries.5. Consider the Workload and Resources:
25,"- Parallel queries consume more CPU and memory. If your system is already resource-constrained, increasing parallelism might not yield the expected performance gains and could even degrade overall performance.6. Use Parallel-Aware Extensions:"
25,- Some PostgreSQL extensions are designed to improve parallel query performance. Be aware of and consider using these extensions if they fit your use case.7. Analyze and Optimize Query Plans:
25,- Use
25,to understand how your queries are executed. Look for bottlenecks or steps that don't parallelize as expected.8. Partition Your Data:
25,- Data partitioning can help in parallel processing by allowing queries to run on different portions of the data concurrently.9. Balance Load Across Nodes:
25,"- In a distributed PostgreSQL setup, ensure that the data and query load are balanced across different nodes to maximize parallel processing benefits.10. Monitor Performance:"
25,- Continuously monitor the performance of your parallel queries. Tools like
25,can be used to track and analyze query performance.11. Test and Iterate:
25,- Parallel query performance can vary based on the specific query and data. Test different configurations and iterate based on the results to find the optimal settings for your workload.12. Upgrade PostgreSQL Version:
25,"- Newer versions of PostgreSQL often come with improvements and optimizations in parallel processing. Ensure you are on a version that supports robust parallel query execution.By following these guidelines, you can leverage the power of parallel queries in PostgreSQL to achieve faster query response times, especially for data-intensive operations. However, it's important to remember that parallelism is not a one-size-fits-all solution and should be calibrated based on the specific needs and constraints of your database environment.Read more: https://minervadb.xyz/cardinality-in-postgresql-the-key-to-query-performance-optimization/https://minervadb.xyz/mastering-data-type-conversion-practical-applications-of-the-postgresql-cast-operator/https://minervadb.xyz/troubleshooting-out-of-memory-oom-errors-in-postgresql/https://minervadb.xyz/boosting-postgresql-performance-configuring-efficient-caching-for-optimal-query-response/https://minervadb.xyz/how-is-process-memory-protection-implemented-in-postgresql/https://minervadb.xyz/demystifying-postgresql-mvcc-unlocking-concurrency-control-in-your-database/"
25,Like
25,Comment
25,Copy
25,LinkedIn
25,Facebook
25,Twitter
25,Share
25,"To view or add a comment, sign in"
25,More articles by this author
25,No more previous content
25,Creating Deep Neural Network Models with BigQuery ML for Predictive Analytics in GA4 Datasets
25,"Mar 13, 2024"
25,Implementing Sharding in PostgreSQL with PL/Proxy: A Step-by-Step Guide
25,"Mar 11, 2024"
25,Effective Strategies for Troubleshooting and Resolving 'Failure to Find WAL Records' Errors in PostgreSQL
25,"Mar 11, 2024"
25,Managing Performance in Azure SQL: Strategies for Handling Skewed Indexes
25,"Mar 11, 2024"
25,Optimizing SQL Server: Mitigating the Impact of Expensive Query Operations
25,"Mar 11, 2024"
25,Optimizing SQL Server Performance: Managing TRACEWRITE Waits and Minimizing Tracing Impact
25,"Mar 9, 2024"
25,Optimizing PostgreSQL Queries with Partial Indexes: A Step-by-Step Guide
25,"Mar 8, 2024"
25,Optimizing SQL Server Query Performance with Unique Subquery Comparisons
25,"Feb 27, 2024"
25,Seamlessly Integrating Threads and Coroutines in Python: A Guide to Transitioning to asyncio
25,"Feb 25, 2024"
25,Optimizing Sort Indexes in InnoDB for Enhanced Database Performance
25,"Feb 24, 2024"
25,No more next content
25,See all
25,Others also viewed
25,Effective Strategies for Reclaiming Disk Space in
25,MySQL 8
25,Shiv Iyer
25,2mo
25,Analyzing Scans in PostgreSQL
25,Devi .
25,SQL Parser Implementation in PostgreSQL
25,Shiv Iyer
25,1mo
25,Async Communication With Postgres Database Triggers
25,Eric Masi
25,Use pg_profile Generating PostgreSQL Statistical Reports
25,Jinqiang Tian
25,The Data Geek
25,Matillion
25,The Future of the MERN Stack: Postgres is the New King.
25,Aondohemba Akighir
25,4mo
25,Looking for a proficient DBA
25,GYTWorkz
25,What happens to uncommitted transactions in MySQL if server crashes after the update?
25,Shiv Iyer
25,1mo
25,What are conversion locks in InnoDB? How do they influence the overall performance of MySQL?
25,Shiv Iyer
25,1mo
25,Show more
25,Show less
25,Explore topics
25,Sales
25,Marketing
25,Business Administration
25,HR Management
25,Content Management
25,Engineering
25,Soft Skills
25,See All
25,LinkedIn
25,© 2024
25,About
25,Accessibility
25,User Agreement
25,Privacy Policy
25,Cookie Policy
25,Copyright Policy
25,Brand Policy
25,Guest Controls
25,Community Guidelines
25,العربية (Arabic)
25,Čeština (Czech)
25,Dansk (Danish)
25,Deutsch (German)
25,English (English)
25,Español (Spanish)
25,Français (French)
25,हिंदी (Hindi)
25,Bahasa Indonesia (Indonesian)
25,Italiano (Italian)
25,日本語 (Japanese)
25,한국어 (Korean)
25,Bahasa Malaysia (Malay)
25,Nederlands (Dutch)
25,Norsk (Norwegian)
25,Polski (Polish)
25,Português (Portuguese)
25,Română (Romanian)
25,Русский (Russian)
25,Svenska (Swedish)
25,ภาษาไทย (Thai)
25,Tagalog (Tagalog)
25,Türkçe (Turkish)
25,Українська (Ukrainian)
25,简体中文 (Chinese (Simplified))
25,正體中文 (Chinese (Traditional))
25,Language
27,Skew join optimization using skew hints | Databricks on AWS
27,Help Center
27,Documentation
27,Knowledge Base
27,Community
27,Support
27,Feedback
27,Try Databricks
27,English
27,English
27,日本語
27,Português
27,Amazon
27,Web Services
27,Microsoft Azure
27,Google Cloud Platform
27,Databricks on AWS
27,Get started
27,Get started
27,What is Databricks?
27,DatabricksIQ
27,Release notes
27,Load & manage data
27,Connect to data sources
27,Discover data
27,Query data
27,Ingest data
27,Prepare data
27,Monitor data and AI assets
27,Share data (Delta sharing)
27,Databricks Marketplace
27,Work with data
27,Data engineering
27,Delta Live Tables
27,Structured Streaming
27,Apache Spark
27,Compute
27,Notebooks
27,Workflows
27,Libraries
27,Init scripts
27,Repos
27,DBFS
27,Files
27,Migration
27,Optimization & performance
27,Predictive optimization for Delta Lake
27,Optimize performance with caching on Databricks
27,Archival support in Databricks
27,Dynamic file pruning
27,Low shuffle merge on Databricks
27,Adaptive query execution
27,What is predictive I/O?
27,Cost-based optimizer
27,Query semi-structured data in Databricks
27,Higher-order functions
27,Transform complex data types
27,Range join optimization
27,Skew join optimization using skew hints
27,Isolation levels and write conflicts on Databricks
27,Bloom filter indexes
27,Generative AI & LLMs
27,Machine learning
27,Model serving
27,Data warehousing
27,Delta Lake
27,Developer tools
27,Technology partners
27,Administration
27,Account and workspace administration
27,Security and compliance
27,Data governance (Unity Catalog)
27,Lakehouse architecture
27,Reference & resources
27,Reference
27,Resources
27,What’s coming?
27,Documentation archive
27,"Updated Mar 13, 2024"
27,Send us feedback
27,Documentation
27,Databricks data engineering
27,Optimization recommendations on Databricks
27,Skew join optimization using skew hints
27,Skew join optimization using skew hints
27,"This article describes how to use skew hints to ameliorate data skew in a table, a condition that can downgrade query performance."
27,Note
27,Skew join hints are not required. Skew is automatically taken care of if adaptive query execution (AQE) and spark.sql.adaptive.skewJoin.enabled are both enabled. See Adaptive query execution.
27,What is data skew?
27,"Data skew is a condition in which a table’s data is unevenly distributed among partitions in the cluster. Data skew can severely downgrade performance of queries, especially those with joins. Joins between big tables require shuffling data and the skew can lead to an extreme imbalance of work in the cluster. It’s likely that data skew is affecting a query if a query appears to be stuck finishing very few tasks (for example, the last 3 tasks out of 200). To verify that data skew is affecting a query:"
27,Click the stage that is stuck and verify that it is doing a join.
27,"After the query finishes, find the stage that does a join and check the task duration distribution."
27,"Sort the tasks by decreasing duration and check the first few tasks. If one task took much longer to complete than the other tasks, there is skew."
27,"To ameliorate skew, Delta Lake on Databricks SQL accepts skew hints in queries. With the information from a skew hint, Databricks Runtime can construct a better query plan, one that does not suffer from data skew."
27,Configure skew hint with relation name
27,"A skew hint must contain at least the name of the relation with skew. A relation is a table, view, or a subquery."
27,All joins with this relation then use skew join optimization.
27,-- table with skew
27,SELECT /*+ SKEW('orders') */
27,"FROM orders, customers"
27,WHERE c_custId = o_custId
27,-- subquery with skew
27,SELECT /*+ SKEW('C1') */
27,"FROM (SELECT * FROM customers WHERE c_custId < 100) C1, orders"
27,WHERE C1.c_custId = o_custId
27,Configure skew hint with relation name and column names
27,There might be multiple joins on a relation and only some of them will suffer from skew.
27,Skew join optimization has some overhead so it is better to use it only when needed.
27,"For this purpose, the skew hint accepts column names. Only joins with these columns use skew join optimization."
27,-- single column
27,"SELECT /*+ SKEW('orders', 'o_custId') */"
27,"FROM orders, customers"
27,WHERE o_custId = c_custId
27,-- multiple columns
27,"SELECT /*+ SKEW('orders', ('o_custId', 'o_storeRegionId')) */"
27,"FROM orders, customers"
27,WHERE o_custId = c_custId AND o_storeRegionId = c_regionId
27,"Configure skew hint with relation name, column names, and skew values"
27,"You can also specify skew values in the hint. Depending on the query and data, the skew values might be known (for example, because they never change) or might be easy to find out. Doing this reduces the overhead of skew join optimization. Otherwise, Delta Lake detects them automatically."
27,"-- single column, single skew value"
27,"SELECT /*+ SKEW('orders', 'o_custId', 0) */"
27,"FROM orders, customers"
27,WHERE o_custId = c_custId
27,"-- single column, multiple skew values"
27,"SELECT /*+ SKEW('orders', 'o_custId', (0, 1, 2)) */"
27,"FROM orders, customers"
27,WHERE o_custId = c_custId
27,"-- multiple columns, multiple skew values"
27,"SELECT /*+ SKEW('orders', ('o_custId', 'o_storeRegionId'), ((0, 1001), (1, 1002))) */"
27,"FROM orders, customers"
27,WHERE o_custId = c_custId AND o_storeRegionId = c_regionId
27,"© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation."
27,Send us feedback
27,| Privacy Policy | Terms of Use
29,PostgreSQL Performance Tuning Tips
29,Basil Sisson
29,PostgreSQL Performance Tuning Tips
29,3 min read
29,PostgreSQL is a powerful open-source database system that can be optimized to achieve high performance.
29,"this article, we will discuss some PostgreSQL performance tuning tricks that can help you improve your"
29,database's performance.
29,Optimize Configuration Parameters
29,One of the simplest ways to improve PostgreSQL performance is by tweaking configuration parameters.
29,"Parameters like shared_buffers, work_mem, and effective_cache_size can"
29,significantly impact the
29,database's
29,performance. Increasing the size of shared_buffers and effective_cache_size can reduce
29,the number of
29,disk
29,"reads, while increasing work_mem can help with query performance."
29,Indexing Strategies
29,Proper indexing is essential for improving database performance. Indexes speed up queries by allowing the
29,"database to quickly locate the required data. However, creating too many indexes can slow down database"
29,"writes, so it's important to find the right balance. Analyze query patterns to determine which indexes"
29,are
29,necessary and remove any unused indexes.
29,Table Partitioning
29,Partitioning large tables into smaller chunks can improve query performance by limiting the amount of
29,data
29,"that needs to be scanned. PostgreSQL supports table partitioning, allowing you to split a table into"
29,"smaller, more manageable pieces. This technique can significantly improve query performance and reduce"
29,the
29,risk of table bloat.
29,Parallel Query Execution
29,"PostgreSQL supports parallel query execution, which can speed up query performance significantly."
29,Parallel
29,query execution splits a single query into multiple parallel processes that can work on different parts
29,"the data simultaneously. However, not all queries are suitable for parallel execution, so you need to"
29,evaluate the query patterns to determine the appropriate settings.
29,Connection Pooling
29,Connection pooling is an effective way to improve database performance by reducing the overhead of
29,establishing new database connections. A connection pool maintains a pool of pre-established database
29,"connections that can be reused, eliminating the need to establish a new connection for each query. This"
29,can
29,significantly reduce the overhead of database connections and improve overall performance.
29,These PostgreSQL performance tuning tricks can help you optimize your database for maximum performance.
29,"However, it's essential to understand your database's specific needs and requirements before"
29,implementing
29,"any changes. By analyzing query patterns, understanding your workload, and optimizing your configuration"
29,"parameters, indexing strategies, table partitioning, parallel query execution, and connection pooling,"
29,you
29,can achieve significant improvements in your database's performance.
29,© 2024 Basil Sisson
30,Reddit - Dive into anything
30,Skip to main content
30,Reddit and its partners use cookies and similar technologies to provide you with a better experience.
30,"By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising."
30,"By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform."
30,"For more information, please see our"
30,Cookie Notice
30,and our
30,Privacy Policy.
30,Open menu
30,Open navigation
30,Go to Reddit Home
30,r/freebsd
30,A chip
30,A close button
30,Get app
30,Get the Reddit app
30,Log In
30,Log in to Reddit
30,Expand user menu
30,Open settings menu
30,Log In / Sign Up
30,Advertise on Reddit
30,Shop Collectible Avatars
30,Get the Reddit app
30,Scan this QR code to download the app now
30,Or check it out in the app stores
30,Go to freebsd
30,r/freebsd
30,r/freebsd
30,Unofficial subreddit for The FreeBSD Project
30,Members
30,Online
30,Dense_Care8224
30,ADMIN
30,MOD
30,mysql zfs performance tuning tips?
30,trying to get a bit more performance out of mysql running on FreeBSD and wondering if there are any dataset settings to optimize for performance - even if a mysql re-install is required with a brand new dataset / mountpoint ? thanks
30,Read more
30,Top 4%
30,Rank by size
30,Top Posts
30,Reddit
30,"reReddit: Top posts of March 30, 2023"
30,Reddit
30,reReddit: Top posts of March 2023
30,Reddit
30,reReddit: Top posts of 2023
30,&nbsp;
30,TOPICS
30,Gaming
30,Valheim
30,Genshin Impact
30,Minecraft
30,Pokimane
30,Halo Infinite
30,Call of Duty: Warzone
30,Path of Exile
30,Hollow Knight: Silksong
30,Escape from Tarkov
30,Watch Dogs: Legion
30,Sports
30,NFL
30,NBA
30,Megan Anderson
30,Atlanta Hawks
30,Los Angeles Lakers
30,Boston Celtics
30,Arsenal F.C.
30,Philadelphia 76ers
30,Premier League
30,UFC
30,Business
30,GameStop
30,Moderna
30,Pfizer
30,Johnson & Johnson
30,AstraZeneca
30,Walgreens
30,Best Buy
30,Novavax
30,SpaceX
30,Tesla
30,Crypto
30,Cardano
30,Dogecoin
30,Algorand
30,Bitcoin
30,Litecoin
30,Basic Attention Token
30,Bitcoin Cash
30,Television
30,The Real Housewives of Atlanta
30,The Bachelor
30,Sister Wives
30,90 Day Fiance
30,Wife Swap
30,The Amazing Race Australia
30,Married at First Sight
30,The Real Housewives of Dallas
30,My 600-lb Life
30,Last Week Tonight with John Oliver
30,Celebrity
30,Kim Kardashian
30,Doja Cat
30,Iggy Azalea
30,Anya Taylor-Joy
30,Jamie Lee Curtis
30,Natalie Portman
30,Henry Cavill
30,Millie Bobby Brown
30,Tom Hiddleston
30,Keanu Reeves
30,RESOURCES
30,About Reddit
30,Advertise
30,Help
30,Blog
30,Careers
30,Press
30,Communities
30,Best of Reddit
30,Topics
30,Impressum
30,Content Policy
30,Privacy Policy
30,User Agreement
30,"Reddit, Inc. © 2024. All rights reserved."
32,PostgreSQL Performance Tuning Tips
32,Basil Sisson
32,PostgreSQL Performance Tuning Tips
32,3 min read
32,PostgreSQL is a powerful open-source database system that can be optimized to achieve high performance.
32,"this article, we will discuss some PostgreSQL performance tuning tricks that can help you improve your"
32,database's performance.
32,Optimize Configuration Parameters
32,One of the simplest ways to improve PostgreSQL performance is by tweaking configuration parameters.
32,"Parameters like shared_buffers, work_mem, and effective_cache_size can"
32,significantly impact the
32,database's
32,performance. Increasing the size of shared_buffers and effective_cache_size can reduce
32,the number of
32,disk
32,"reads, while increasing work_mem can help with query performance."
32,Indexing Strategies
32,Proper indexing is essential for improving database performance. Indexes speed up queries by allowing the
32,"database to quickly locate the required data. However, creating too many indexes can slow down database"
32,"writes, so it's important to find the right balance. Analyze query patterns to determine which indexes"
32,are
32,necessary and remove any unused indexes.
32,Table Partitioning
32,Partitioning large tables into smaller chunks can improve query performance by limiting the amount of
32,data
32,"that needs to be scanned. PostgreSQL supports table partitioning, allowing you to split a table into"
32,"smaller, more manageable pieces. This technique can significantly improve query performance and reduce"
32,the
32,risk of table bloat.
32,Parallel Query Execution
32,"PostgreSQL supports parallel query execution, which can speed up query performance significantly."
32,Parallel
32,query execution splits a single query into multiple parallel processes that can work on different parts
32,"the data simultaneously. However, not all queries are suitable for parallel execution, so you need to"
32,evaluate the query patterns to determine the appropriate settings.
32,Connection Pooling
32,Connection pooling is an effective way to improve database performance by reducing the overhead of
32,establishing new database connections. A connection pool maintains a pool of pre-established database
32,"connections that can be reused, eliminating the need to establish a new connection for each query. This"
32,can
32,significantly reduce the overhead of database connections and improve overall performance.
32,These PostgreSQL performance tuning tricks can help you optimize your database for maximum performance.
32,"However, it's essential to understand your database's specific needs and requirements before"
32,implementing
32,"any changes. By analyzing query patterns, understanding your workload, and optimizing your configuration"
32,"parameters, indexing strategies, table partitioning, parallel query execution, and connection pooling,"
32,you
32,can achieve significant improvements in your database's performance.
32,© 2024 Basil Sisson
33,How to improve timings of query executed on a DB link? - Ask TOM
33,Skip to Main Content
33,Ask TOM
33,Site Feedback
33,Sign In
33,QuestionsOffice HoursVideosResourcesClasses
33,QuestionsHow to improve timings of query executed on a DB link?
33,Breadcrumb
33,Question and Answer
33,Thanks for the question.
33,"Asked: September 29, 2021 - 10:23 am UTC"
33,Last updated: February
33,"06, 2024 - 11:23 am UTC"
33,Version: Oracle 19 C
33,Viewed 10K+ times! This question is
33,You Asked
33,"Hello Team,First of all, thanks for all the good work you are doing.It would be great if you can help us understand how we can improve timings of query executed on a DB link.Consider an Oracle instance (DB1) that can talk with another oracle instance (DB2) via DB Link. A select statements fetches ~1,00,000 values from DB1 and pass them to a function that is defined on DB2. The function do some operations on the values and returns the results. All the data is in DB1 but the function has to be in DB2.We have tried multiple things and using DRIVING_SITE hint helped to reduce timings. However, it comes with its own issues( Code re-write, hint can be ignored, issue in techniques used in plan stabilization, query transformation, non-applicability to DML"
33,"). Things like switching to bulk processing did not help much.Any thing else that we can try?We are looking for techniques that can be applied to a large number of statements.Thanks,AB"
33,and Chris said...
33,I discussed options for tuning distributed queries in an Office Hours session a while ago: https://asktom.oracle.com/pls/apex/asktom.search?oh=5684 All the data is in DB1 but the function has to be in DB2.Why does the function have to be in DB2?The best way to speed this query up is to stop transferring the data around and process it all locally. Creating the function on DB1 and avoiding DB links is the single biggest gain you can make performance-wise.Or - even better - moving the logic out of the function and into pure SQL should also help; this isn't always practical however.
33,Rating
33,(2 ratings)
33,"Is this answer out of date? If it is, please let us know via a Comment"
33,Comments
33,Comment
33,Reason for moving the function outside DB1
33,"A reader, October"
33,"01, 2021 - 5:12 am UTC"
33,Thank you for the response Chris.The function invokes a C lib via External Procedure call. The lib do some processing and returns the result back to the function.The function has to be moved to DB2 (a non-ExaCC DB) because
33,"DB1 is in ExaCC and we are not allowed to put the C lib in ExaCC. Also, due to some security restrictions, we can not move logic from the C lib in DB."
33,October
33,"04, 2021 - 4:47 am UTC"
33,"I spoke to some of our Exa CC people - they repliedWhile we recommend to not install additional software on servers (as especially additional RPMs will affect OS patching/upgrade), it never says anywhere you are not allowed to install additional software….One option you could consider is using a docker container for additional software, as this does not interfere with the OS updates then… See: Decoupling RPM Dependencies, running Oracle R Distribution (ORD) on Exadata Database Nodes in Docker (Doc ID 2257680.1)If this is the case, you may want to put the software onto an ACFS folder, to have a common location across the cluster (and not to worry to have to update each node individually)."
33,Any tips or tricks when the remote DB is not Oracle?
33,"Justin, January"
33,"29, 2024 - 11:25 pm UTC"
33,"Greetings!We have some scenarios where we need to extract data from a remote MSSQL environment, and we need to limit the rows extracted based on an update flag in a secondary table. When running this natively on MSSQL, you can easily facilitate this through an Inner Join, and the performance is adequate."
33,"When we have appropriate access on the MSSQL side, we'll create a MSSQL view to perform this work, and then issue a select against the remote view."
33,This performs as expected.We now have a scenario where we cannot create a remote view and have to perform the row limitation on our side.
33,e.g.Select
33,a.*
33,From
33,Table_A@Db_Link a
33,Inner Join Table_B@Db_Link b
33,on a.Id = b.Id
33,Where
33,"b.Update_Date >= Trunc(Sysdate)I've tried various /*+Driving_Site (a) */ and /*+ No_Merge */ optimization hints, but the result is the same."
33,"Both datasets are brought back to the local environment, in their entirety, and then the join is performed. Any guidance or quick lessons to be learned here?Thanks!-Justin"
33,February
33,"06, 2024 - 11:23 am UTC"
33,"I don't know of a way to do this. Your performance tuning options querying database links across different database systems will always be limited. In this specific query, it's possible that the whole of table B is returned, then joined and filtered in Oracle Database. You could try using a subquery to ensure table B is filtered first, e.g.:Select"
33,a.*
33,From
33,Table_A@Db_Link a
33,Inner Join ( select * from Table_B@Db_Link b where b.Update_Date >= Trunc(Sysdate) ) b
33,on a.Id = b.Id
33,"Connor and Chris don't just spend all day on AskTOM. You can also catch regular content via Connor's blog and Chris's blog. Or if video is more your thing, check out Connor's latest video and Chris's latest video from their Youtube channels. And of course, keep up to date with AskTOM via the official twitter account."
33,More to Explore
33,Administration
33,Need more information on Administration? Check out the Administrators guide for the Oracle Database
33,Oracle | Integrated Cloud Applications & Platform Services
33,Facebook
33,Twitter
33,LinkedIn
33,YouTube
33,Oracle RSS Feed
33,© Oracle
33,Site Map
33,Terms of Use and Privacy
33,About AskTOM
33,AskTOM 5.0.7 Built with love using Oracle APEX
33,5.0.7
33,Built with
33,using Oracle APEX(opens in new window)
34,How to Scale PostgreSQL: A Comprehensive Guide for Rapid Growth - SQL Knowledge Center
34,SQL
34,SQL Server
34,SQLite
34,PostgreSQL
34,MySQL
34,T-SQL
34,Tools
34,About
34,Learn SQL for Free
34,How to Scale PostgreSQL: A Comprehensive Guide for Rapid Growth
34,By Cristian G. Guasch • Updated: 09/22/23 • 10 min read
34,"I’ve spent years delving into the ins and outs of PostgreSQL, and I’m here to share my insights on how to scale PostgreSQL effectively. Scaling your PostgreSQL database might seem like a daunting task at first, but with the right strategy, it’s definitely achievable. It all starts with understanding your workload and identifying what exactly needs scaling in your set-up."
34,Chat with SQL Databases using AI
34,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
34,Start Free Trial
34,"Now, you’re probably wondering: why is scaling important? Well, as your application grows, so does the amount of data you need to handle. Your database must be able to keep up with this growth or else it’ll lead to slow response times and potentially crash under heavy loads. That’s where scaling comes in – it boosts your database’s capacity to handle larger volumes of data without sacrificing performance."
34,"In this guide, we’ll explore various techniques for scaling PostgreSQL. From vertical and horizontal scaling to partitioning data and implementing read replicas – there are many ways for you to optimize your system. All these methods will ensure that as traffic increases or data expands, your application remains robust and responsive."
34,Understanding PostgreSQL and Its Scalability
34,"Let’s dive in. I’ve found that one of the many reasons why tech-savvy folks love working with PostgreSQL is its remarkable scalability. Now, you’re probably wondering what exactly does this mean? Well, scalability, in simple terms, refers to a system’s ability to handle an increasing amount of workload without compromising performance."
34,"It’s important to note that PostgreSQL offers both vertical and horizontal scalability. Vertical scaling involves adding more resources such as CPU or memory to your existing database server. This is usually straightforward; however, there may be hardware limitations or cost constraints which restrict how much you can scale up vertically."
34,"On the other hand, horizontal scaling (also known as sharding) requires distributing your database across multiple servers. It can provide greater flexibility and potentially unlimited growth since it isn’t restricted by the limits of a single machine."
34,Let me show you some code examples:
34,# Example for vertical scaling
34,"connection = psycopg2.connect(database=""mydb"", user=""myuser"", password=""mypassword"")"
34,cursor = connection.cursor()
34,"cursor.execute(""SET work_mem TO '256MB';"")"
34,# Increasing the working memory
34,# Example for horizontal scaling
34,"# Assuming we have two servers ""server1"" and ""server2"""
34,"connection1 = psycopg2.connect(host=""server1"", database=""mydb"", user=""myuser"", password=""mypassword"")"
34,"connection2 = psycopg2.connect(host=""server2"", database=""mydb"", user=""myuser"", password=""mypassword"")"
34,A common mistake you might make when trying to scale PostgreSQL is not considering the trade-offs between vertical and horizontal scaling. Each method has its pros and cons that should be carefully evaluated based on your specific needs before making a decision.
34,"Remember, while PostgreSQL’s native capabilities offer a great deal of control over how data is stored and retrieved – enabling impressive levels of fine-tuning – it’s not a silver bullet for all scalability challenges. In some cases, you may need to look at other solutions such as caching or read replicas to complement your scaling strategy."
34,"I’ll be covering these points in more detail in the subsequent sections of this article, so stay tuned!"
34,Identifying Factors That Impact PostgreSQL Performance
34,"First off, I’d like to discuss one of the most common factors that can affect the performance of your PostgreSQL database: hardware limitations. If your server is lacking in resources – think insufficient memory, underpowered CPUs, or slow disk drives – then you’re going to struggle to get good performance out of your database. It’s a bit like trying to run a marathon with one leg; it’s possible, but it’s going to be hard work."
34,"Let me share an example. Let’s say you’ve got a query that needs 1GB of RAM to run efficiently. But your server only has 512MB available. What happens? Well, PostgreSQL will start using disk space as ‘virtual’ memory – and since disk operations are much slower than memory operations, this can significantly slow down your query."
34,-- A simple SELECT statement that might require significant memory
34,SELECT * FROM large_table ORDER BY some_column;
34,"Next up on our list is inefficient queries or suboptimal schema design. You might have all the hardware resources in the world, but if your queries are not written efficiently or if your tables aren’t properly indexed for those queries, then you’ll still suffer from poor performance."
34,Here’s an example of a poorly designed table:
34,CREATE TABLE orders (
34,"order_id serial PRIMARY KEY,"
34,"customer_id integer NOT NULL,"
34,-- Missing index on `customer_id`
34,"In this case, every time we want to find all orders for a particular customer (a very common operation), PostgreSQL would need to scan the entire table – which could be very slow if there are millions of rows."
34,"Lastly, let’s talk about configuration settings. Like any software package, PostgreSQL comes with tons of settings that control its behavior. Some default settings may not be optimal for high-performance scenarios and therefore could become potential bottlenecks."
34,"For instance, the shared_buffers parameter controls how much memory PostgreSQL can use for caching data. If this value is set too low, PostgreSQL may not be making full use of the available RAM, resulting in unnecessary disk I/O."
34,# Example of a potentially suboptimal PostgreSQL configuration option
34,shared_buffers = 128MB # Might be too low for servers with lots of RAM
34,"In conclusion, there are many factors that can affect PostgreSQL performance. Through careful consideration and tuning of these aspects – hardware resources, query efficiency and schema design, and configuration settings – you can significantly improve the performance of your PostgreSQL database."
34,Chat with SQL Databases using AI
34,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
34,Start Free Trial
34,Best Practices for Scaling PostgreSQL Databases
34,"Scaling PostgreSQL databases is no small feat. It’s an art that requires a deep understanding of the technology and plenty of hands-on experience. However, I’ve found a few best practices over the years that can make this process smoother."
34,"Firstly, it’s crucial to understand your workload. That means identifying which queries are most frequent and which ones consume the most resources. You’ll want to optimize these first, as they’re likely causing the biggest bottlenecks in your system."
34,SELECT
34,"query,"
34,"calls,"
34,"total_time,"
34,"rows,"
34,100.0 * shared_blks_hit /
34,"nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent"
34,FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;
34,This SQL command will help you identify top consuming queries in terms of time spent on them.
34,"Next up is indexing properly. Indexes are essential for improving database performance but remember, not all indexes bring benefits. Over-indexing can slow down write operations significantly and consume more space unnecessarily."
34,CREATE INDEX idx_orders_on_created_at ON orders (created_at);
34,"In this example code snippet, we’re creating an index on created_at column in orders table which could speed up retrieval times based on date of order creation."
34,Connection pooling is another practice worth considering when scaling PostgreSQL databases. It reduces overhead by reusing database connections instead of opening new ones every time a client connects to the database.
34,Here’s an example how you may set connection pool using PgBouncer:
34,databases:
34,postgres_db = host=localhost dbname=postgres_db
34,pool_mode = session
34,listen_addr = *
34,listen_port = 6432
34,auth_type = md5
34,auth_file = /etc/pgbouncer/userlist.txt
34,admin_users = postgres
34,"stats_users = stats, postgres"
34,This configuration file sets PgBouncer to listen on all addresses (listen_addr = *) and port 6432 (listen_port = 6432). The pool mode is set as ‘session’ which means a client keeps a server connection for the entire duration of its session.
34,"Lastly, don’t forget about regular monitoring. It helps you understand how your database behaves over time and whether your optimizations are actually improving performance."
34,pg_stat_activity is one such view that gives information about current queries being executed in PostgreSQL:
34,SELECT * FROM pg_stat_activity;
34,"Running this command will provide details like query start time, backend start, application name running the query etc."
34,Remember each scenario can be unique so it’s vital to test these practices carefully before implementing them in your production environment.
34,Case Study: Successful PostgreSql Scaling Scenarios
34,Let me share with you some success stories of PostgreSQL scaling. They serve as a testament to the scalability and reliability of this fantastic open-source relational database.
34,"One such example is Instagram, which has incredibly leveraged PostgreSQL’s capabilities. As they grew from zero to over one billion users, their primary datastore was none other than PostgreSQL. They utilized it for storing all their metadata including users, photos, comments, likes and more! It’s mind-boggling how PostgreSQL scaled alongside Instagram’s exponential growth."
34,-- Here's an oversimplified snapshot of what part of their architecture might have looked like:
34,CREATE TABLE Users (
34,"ID SERIAL PRIMARY KEY,"
34,"Username VARCHAR(50) UNIQUE NOT NULL,"
34,Email VARCHAR(50) UNIQUE NOT NULL
34,CREATE TABLE Photos (
34,"ID SERIAL PRIMARY KEY,"
34,"UserID INT REFERENCES Users(ID),"
34,"PhotoURL VARCHAR(255) NOT NULL,"
34,Caption TEXT
34,Using partitioning techniques like sharding was one way Instagram managed to scale effectively with PostgreSQL. But be careful while sharding! Common mistakes include not considering future growth or evenly distributing data among shards.
34,"Another stand out case is Apple Inc., who turned to Postgres when they required a robust solution for managing large datasets across multiple servers in iCloud services. By relying on horizontal scaling (adding more machines), they were able to handle massive workloads efficiently."
34,-- Below represents a simplified version of how Apple may have used tables
34,-- in their iCloud service.
34,CREATE TABLE DeviceBackups (
34,"ID SERIAL PRIMARY KEY,"
34,"UserID INT REFERENCES Users(ID),"
34,"BackupDate TIMESTAMP DEFAULT NOW(),"
34,BackupData BYTEA
34,Remember that every use case is unique and there isn’t always a one-size-fits-all solution when it comes to database scaling strategies.
34,"Going forward, let’s keep these successful examples in mind as we delve into the various strategies for scaling PostgreSQL. Whether you’re a small start-up or a tech giant like Instagram or Apple, PostgreSQL proves to be a highly scalable and reliable solution."
34,Conclusion: Key Takeaways on How to Scale PostgreSQL
34,"Wrapping up, I’ve got some crucial points for you. When it comes to scaling PostgreSQL, key elements can’t be overlooked."
34,"First off, remember that partitioning your data is a must-do. It’s one way of dividing and conquering the data load in an effective manner. This approach aids in managing large tables by breaking them down into smaller, more manageable pieces."
34,CREATE TABLE order_items (
34,"product_no integer,"
34,"order_id integer not null,"
34,"quantity integer,"
34,) PARTITION BY RANGE (order_id);
34,"Secondly, don’t forget about connection pooling – it’s your best friend here! By reducing the overhead of creating new connections each time a client request is made, you save valuable resources."
34,pgbouncer -d -v pgbouncer.ini
34,Third on my list is replication methods like Master-Slave and Multi-Master Replication. They are great tools for scaling read operations and providing redundancy.
34,A common pitfall? Overlooking the importance of regular database tuning and maintenance tasks such as vacuuming and analyzing databases.
34,"Finally, please keep in mind that there’s no one-size-fits-all solution when it comes to scaling PostgreSQL. The optimal method largely depends on specific use cases or applications; so always consider this when making a choice!"
34,Partition Data
34,Use Connection Pooling
34,Implement Replication methods
34,Regular Database Tuning
34,I hope these takeaways offer guidance as you venture into the realm of scaling PostgreSQL!
34,Chat with SQL Databases using AI
34,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
34,Start Free Trial
34,Related articles
34,How to Divide one Column by Another in SQL – Quick Tricks for PostgreSQL and SQLite
34,How to Connect pgAdmin with PostgreSQL: Your Easy Guide to Database Integration
34,How to Get Last 7 Days Record in PostgreSQL: Your Quick Guide
34,How to Import Data into PostgreSQL: Your Comprehensive Guide to Smooth Data Transfer
34,How to Drop Database in PostgreSQL: Your Comprehensive Guide
34,How to Check PostgreSQL Version: Your Quick and Easy Guide
34,How to Check Database Size in PostgreSQL: Your Quick Guide
34,How to Delete Table in PostgreSQL: Your Comprehensive Guide
34,How to Create Index in PostgreSQL: Your Simplified Guide to Database Optimization
34,How to Login to PostgreSQL: Your Ultimate Step-by-Step Guide
34,How to Import Database in PostgreSQL: A Step-by-Step Guide for Beginners
34,How to Backup PostgreSQL Database: Step-by-Step Guide for Secure Data Storage
34,"How to Import CSV into PostgreSQL: A Clear, Step-by-Step Guide"
34,How to Pivot in PostgreSQL: A Comprehensive Guide for Data Wrangling
34,How to Call a Function in PostgreSQL: Your Easy Step-by-Step Guide
34,How to Connect PostgreSQL Database: Your Comprehensive Guide for Seamless Integration
34,How to Check if PostgreSQL is Running: Your Quick Guide
34,How to Upgrade PostgreSQL: A Comprehensive Guide for a Seamless Transition
34,How to Comment in PostgreSQL: An Essential Guide for Beginners
34,How to Rename a Column in PostgreSQL: Your Quick and Easy Guide
34,How to Concatenate in PostgreSQL: Your Ultimate Guide for String Combining
34,"How to Query a JSON Column in PostgreSQL: Your Clear, Step-by-Step Guide"
34,How to Install PostgreSQL: Your Easy Guide for a Smooth Installation
34,How to Restart PostgreSQL: A Quick and Simple Guide for Database Management
34,How to Change PostgreSQL Password: A Quick and Easy Guide for Users
34,How to Create a User in PostgreSQL: Your Ultimate Guide for Success
34,How to Create a Database in PostgreSQL: Your Simple Step-by-Step Guide
34,How to Start PostgreSQL: A Beginner’s Step-by-Step Guide
34,How to Delete a Column in PostgreSQL: Your Quick Guide
34,How to Connect PostgreSQL Database in Python: A Step-By-Step Guide for Beginners
34,How to Use PostgreSQL: Your Simple Guide to Navigating the Database World
34,How to Get Current Date in PostgreSQL: Your Comprehensive Guide
34,Cristian G. Guasch
34,"Hey! I'm Cristian Gonzalez, I created SQL Easy while I was working at StubHub (an eBay company) to help me and my workmates learn SQL easily and fast."
34,SQL from zero to Data Analyst level
34,✅ Access 348 lessons and exercises
34,👥 Join 7789 students
34,📈 Learn Data Analysis for Product Management and Marketing
34,📊 Learn SQL from basics to advanced
34,💼 Practice for upcoming interviews
34,💬 Get support from Community Forum
34,Free Sign Up!
34,How to sponsor?
34,Support us with a yearly donation and help SQL-Easy.com thrive!
34,Apply
34,"Languages: English, Español, Portoghese, Italiano, Français, 日本語, Deutsch, اللغة العربية."
34,Easy to Learn 2017-2024
34,Learn SQL · Learn HTML · Learn CSS · Learn JS · Learn Python · Learn PHP
34,About · Sponsor SQL-Easy.com · Cookie Policy
42,Intel® Xeon® Performance Solutions and Tuning Guides for Xeon CPUs
42,Skip To Main Content
42,Toggle Navigation
42,Sign In
42,My Intel
42,My Tools
42,Sign Out
42,English
42,Select Your Language
42,Bahasa Indonesia
42,Deutsch
42,English
42,Español
42,Français
42,Português
42,Tiếng Việt
42,ไทย
42,한국어
42,日本語
42,简体中文
42,繁體中文
42,Toggle Search
42,Search
42,Close Search Panel
42,Advanced Search
42,close
42,Sign In to access restricted content
42,Using Intel.com Search
42,You can easily search the entire Intel.com site in several ways.
42,Brand Name:
42,Core i9
42,Document Number:
42,123456
42,Code Name:
42,Alder Lake
42,Special Operators:
42,"“Ice Lake”, Ice AND Lake, Ice OR Lake, Ice*"
42,Quick Links
42,You can also try the quick links below to see results for most popular searches.
42,Product Information
42,Support
42,Drivers & Software
42,Recent Searches
42,Sign In to access restricted content
42,Advanced Search
42,All of these terms
42,Any of these terms
42,Exact term only
42,Find results with
42,All Results
42,Product Information
42,Support
42,Drivers & Software
42,Documentation & Resources
42,Partners
42,Communities
42,Corporate
42,Show results from
42,Only search in
42,Title
42,Description
42,Content ID
42,Search
42,Sign in to access
42,restricted content.
42,The browser version you are using is not recommended for this site.Please consider upgrading to the latest version of your browser by clicking one of the following links.
42,Safari
42,Chrome
42,Edge
42,Firefox
42,Tuning Guides for Intel® Xeon® Scalable Processor-Based Systems
42,767939
42,Updated
42,4/23/2023
42,Version
42,Latest
42,Public
42,"These tuning guides were designed for people who are already familiar with the workload and are looking for system settings that will improve performance. They provide BIOS, operating system, and workload settings to get the most out of Intel platforms with server workloads. Developers and systems integrators can use them as a launching point for further tuning."
42,4th Gen Intel® Xeon® Scalable Processor and Intel® Xeon® CPU Max Series
42,Tuning Guides
42,PDF
42,BERT-based AI inference with Intel® AMX
42,Deep Learning Optimization
42,Download Chinese
42,Databases
42,ClickHouse with Intel® IAA and Intel® AVX512
42,RocksDB Compression and Decompression with Intel® IAA
42,SQL Server OLAP
42,SQL Server OLTP
42,HPC
42,Intel® Xeon® CPU Max Series Tuning Guide
42,Download Chinese
42,Media
42,Media Processing Basics
42,Download Chinese
42,Networking
42,Open vSwitch with DPDK
42,User Space Network Stack Acceleration with Intel ® DSA
42,3rd Gen Intel® Xeon® Processors
42,Tuning Guides
42,PDF
42,Deep Learning Optimization
42,Download English
42,Download Chinese
42,OpenVINO™
42,Download
42,Data Analytics
42,Apache Spark*
42,Download
42,Databases
42,MongoDB*
42,Download
42,"Open Source Database Tuning (MySQL, PostgreSQL)"
42,Download
42,Redis*
42,Download English
42,Download Chinese
42,RocksDB*
42,Download English
42,Download Chinese
42,RocksDB* db_bench
42,Download
42,Microsoft* SQL Server
42,Download
42,HPC
42,HPC Cluster Tuning
42,Download
42,Genomics Analytics
42,Download
42,NAMD
42,Download
42,LAMMPS
42,Download
42,Relion*
42,Download
42,Media
42,Scalable Video / HEVC Transcode
42,Download
42,Data Compression
42,Download
42,Web-Tier
42,NGINX* - Crypto
42,Download English
42,Download Chinese
42,NGINX* - QAT
42,Download English
42,Download Chinese
42,WordPress*
42,Download
42,Java* Developer Guide
42,Download
42,Virtualization
42,Linux KVM*
42,Download
42,AI workloads
42,TensorFlow*
42,Maximize TensorFlow* Performance on CPU: Considerations and Recommendations for Inference Workloads
42,Guide to TensorFlow* Runtime optimizations for CPU
42,Getting Started with Intel® Optimization for MXNet*
42,Code Sample: Intel® Deep Learning Boost New Deep Learning Instruction bfloat16 - Intrinsic Functions
42,General Intel® Architecture tuning guides
42,Intel Architecture Reference Manuals
42,Automated SKU Selection for Intel® Xeon® Processors through Machine Learning
42,Additional resources
42,Technical Overview of the 4th Gen Intel® Xeon® Scalable processor family
42,Build an Intel®-Based Cluster with OpenHPC* 2.0 on CentOS* 8
42,Product and Performance Information
42,"1Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex."
42,Get Help
42,Company Overview
42,Contact Intel
42,Newsroom
42,Investors
42,Careers
42,Corporate Responsibility
42,Diversity & Inclusion
42,Public Policy
42,© Intel Corporation
42,Terms of Use
42,*Trademarks
42,Cookies
42,Privacy
42,Supply Chain Transparency
42,Site Map
42,Your Privacy Choices
42,California Consumer Privacy Act (CCPA) Opt-Out Icon
42,Notice at Collection
42,Recycling
42,"Intel technologies may require enabled hardware, software or service activation. // No product or component can be absolutely secure. // Your costs and results may vary. // Performance varies by use, configuration and other factors. // See our complete legal Notices and Disclaimers. // Intel is committed to respecting human rights and avoiding causing or contributing to adverse impacts on human rights. See Intel’s Global Human Rights Principles. Intel’s products and software are intended only to be used in applications that do not cause or contribute to adverse impacts on human rights."
44,Blogs on MySQL and PostgreSQL Database Optimization | OtterTune
44,"Product TourBlogPlans and PricingContactResourcesLoginGet StartedProduct TourBlogBlogBlogBlogSearch blog hereJan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Subscribe to blog updates.Subscribe to blog updates.ProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy Policy"
45,NGINX Performance Tuning Tips | OpenLogic by Perforce
45,Skip to main content
45,Created with Avocode.Secondary NavigationDownloadsCustomer PortalCompanyAbout OpenLogic by PerforceCareers at PerforceCustomersNewsletterPressContactContact UsBecome an OpenLogic PartnerSubscribeOpenLogicMain Navigation - Mega MenuSolutionsMain Navigation - Mega MenuExplore SolutionsSolutions OverviewSupported TechnologyKey OfferingsMain Navigation - Mega MenuBy TechnologyBy NeedMain Navigation - Mega MenuOpenJDKDatabasesContainersVirtualizationEnterprise LinuxTomcatCI/CD ToolsMessaging MiddlewareAnalyticsMonitoring ToolsAngularJSAll Supported TechnologiesMain Navigation - Mega MenuConsulting ServicesEnterprise SupportMigration ServicesInnovation Guidance Technical Account ManagementImplementations and UpgradesServicesMain Navigation - Mega MenuProfessional ServicesTrainingMain Navigation - Mega MenuConsulting ServicesImplementations and UpgradesMigration ServicesTechnical Account ManagementMain Navigation - Mega MenuTraining OverviewApache CamelApache ActiveMQApache KafkaDevOps and CI/CDKubernetes Boot CampMonitoring and ObservabilityNGINXMySQL and PostgreSQL One-DayMySQL and PostgreSQL 2.5 DayBlogResourcesMain Navigation - Mega MenuExplore ResourcesPapers & VideosEvents & WebinarsNewsletterRecorded WebinarsStack BuilderSubscribeCentOS Guide for EnterpriseRead NowCustomersTalk to an ExpertDownloadsCustomer PortalCompanyMain Navigation - Mega MenuAbout OpenLogic by PerforceCareers at PerforceNewsletterPressContactMain Navigation - Mega MenuContact UsSubscribeBreadcrumbHomeResourcesBlog
45,NGINX Performance Tuning: Top 5 Tips
45,"February 27, 2024"
45,NGINX Performance Tuning: Top 5 TipsWeb InfrastructureBy
45,"Aleksander PahnNGINX performance tuning is an important exercise to ensure high availability on your website. If you are using NGINX as a web server or reverse proxy, for load balancing and/or HTTP caching, read this blog to get tips on how to tune NGINX to keep it fast and performant.Table of ContentsWhy NGINX Performance Tuning MattersBefore You Tune NGINXTip #1: Modify NGINX Worker ProcessesTip #2: Manage NGINX Worker ConnectionsTip #3: Configure NGINX as a Load BalancerTip #4: Cache and Compress Static ContentTip #5: Pay Attention to NGINX Logging and BufferingFinal ThoughtsTable of Contents1 - Why NGINX Performance Tuning Matters2 - Before You Tune NGINX3 - Tip #1: Modify NGINX Worker Processes4 - Tip #2: Manage NGINX Worker Connections5 - Tip #3: Configure NGINX as a Load Balancer6 - Tip #4: Cache and Compress Static Content7 - Tip #5: Pay Attention to NGINX Logging and Buffering8 - Final ThoughtsBack to topWhy NGINX Performance Tuning MattersIn general, performance tuning is important to maximize the return on investment from a business asset and maintain high levels of availability of the service. For NGINX, tuning will help your site meet and exceed performance benchmarks for speed and latency. It is important to remember that NGINX performance tuning is not a one-time exercise; as website or application loads may change from your initial estimates or planning, it may be necessary to reevaluate and make adjustments to maximize performance. Back to topBefore You Tune NGINXBefore you tune any application, you must understand the current baselines for performance and application use case. You can start by profiling the system resources and looking at the types of content returned. Browser developer tools, system performance tools like dstat (piped to a CSV file which can be visualized), and Apache Bench, can be helpful for this, as well as reviewing NGINX logs. One benefit of profiling is the ability to establish key performance and risk indicators. You can see at what level of requests the application is likely to underperform and break SLAs.It is recommended to use multiple NGINX instances to test tuning results on different hardware (more CPUs, more RAM and disk for caching) and different versions of NGINX. Be aware that while SELinux or AppArmor are recommended, these may cause an additional performance impact. This is something that should be checked during performance testing.Determine if your clients will use the HTTP 1.1, HTTP/2 or HTTP/3 protocols. Depending on the type of client application, using HTTP/2 or even HTTP/3 will cut down on round trips and thus improve latency.Back to topTip #1: Modify NGINX Worker ProcessesBy default, NGINX uses the number of hardware CPU cores for the number of worker processes. This is done to pin each worker to a CPU core and minimize context switching during low level connection management via the Linux Kernel.However, the following configuration directive can be used to tweak the number of worker processes:worker_processes: Defines the number of worker processes.The ideal ratio of CPU to worker processes takes advantage of the way the connections and events are fetched from the Kernel network stack queues. That said, in some workloads, and with large amounts of modern processors that have very high throughput, the number of worker processes can exceed the number of CPU cores.In other words, the larger numbers of worker processes can be a benefit regardless of the penalty from context switching from active process to process on the CPU. The way to determine that is with benchmarking, but likely the smaller the traffic per request, the shorter the connection time and the smaller the impact from context switching becomes.Back to topTip #2: Manage NGINX Worker ConnectionsHow many connections per worker process (and CPU core) to use should be determined based on content and workload testing.The following configuration directive sets the number of connections per worker process; the default value is 512:worker_connections: Sets the maximum number of simultaneous connections that can be opened by a worker process.Keep in mind here that TLS processing will increase CPU load. Large numbers of connections with mutual TLS are more likely to load a single core. Similarly, worker process CPU usage will increase when using content compression. To make sure that connections are closed right after all the requested data is received, don’t disable the lingering_close directive.Back to topTip #3: Configure NGINX as a Load BalancerUsing NGINX as a frontend load balancer allows efficient load distribution among a group of backend servers. This ensures high availability by sending requests only to backend servers found to be healthy.The connections are distributed in a weighted round-robin fashion. For example, more connections can be routed to a server with faster hardware. Or connections may be routed to backend servers with the least number of connections.The open source version of NGINX uses a passive health check to monitor if the backend service is up and ready to receive connections. Using the passive health check mechanism, NGINX will monitor connections as they happen, and try to resume failed connections. If the connections start failing, a backend service will be marked unavailable.An upstream service is marked unavailable based on the following configurable settings:fail_timeout: Sets the time during which a number of failed attempts must happen for the server to be marked unavailable, and also the time for which the server is marked unavailable (default is 10 seconds).max_fails: Sets the number of failed attempts that must occur during the fail_timeout period for the server to be marked unavailable (default is 1 attempt).The load balancing is performed using the ngx_http_upstream_module and is configured with its directives. The next issue to solve in a multi-backend setup is session persistence. This is achieved by using the sticky session affinity configuration directive.Back to topTip #4: Cache and Compress Static ContentCaching can be categorized as server-side and client-side. The less round trips are needed to get data, the faster it can be loaded or reused.Proxy_Cache: Defines the shared memory and storage to use for proxy caching. The directives in the same tree, such as proxy_cache_path and proxy_cache_key, define the location and patterns to cache.FastCGI Caching: Micro caching is most useful for dynamic content that does not change very often, like calendar data, RSS feeds, daily statistics, status pages, etc.Client-Side Cache: Web servers have only partial control over client-side caching. But they can provide HTTP response headers to the HTTP client with caching recommendations. Cache-control Header values:“Cache-Control public;” – Allows the resources to be cached by the HTTP client and any intermediate proxy. This is the option I recommend.“Cache-Control private;” – Only cache on HTTP client side and not on intermediate proxy.“Max-age” – Value in seconds which indicates the amount of time to cache the resources.Example: Cache-Control:public, max-age=864000“Expires” – Sets a date from which a cached resource should be no longer considered as valid. For example, “expires” 6M for 6 months or 2Y for 2 years. If not specified, the Cache-Control: max-age will be calculated automatically from the Expires directive and returned in the HTTP headers.Each content type can be mapped with its own expiry. Example:map $sent_http_content_type $expires { default            off; text/html          epoch; text/css            max; application/javascript     max; ~image/    max; ~font/    max; } server { listen 80 _; listen [::]:80 _; expires $expires;If “expires” and “max-age” are defined, then max-age will be used.Gzip response compression: By compressing the response, the time used up for content transmission is reduced. This improves the user experience and potentially reduces bandwidth costs on metered connections. Some content types maybe already be compressed, such as JPEGs.Example: gzip_types text/plain text/xml text/css text/javascript application/json application/x-javascript application/xml application/xml+rss application/javascript;Note: When using the SSL/TLS protocol, compressed responses may be subject to BREACH attacks.Back to topTip #5: Pay Attention to NGINX Logging and BufferingAccess and error logs are the first stop for troubleshooting on the server side. These logs show configuration errors, resources being accessed, HTTP status codes, and payload size.Use the buffer or gzip parameter to enable log buffering. After buffering is enabled, the data will be written to the log file:if the next log line does not fit into the buffer; if the buffered data is older than specified by the flush parameter (1.3.10, 1.2.7); when a worker process is re-opening log files or is shutting down. The ngx_http_log_module includes:access_log: access_log logs/access.log combined;log_format: log_format combined ""..."";The ngx_core_module includes:error_log: error_log logs/error.log error;The ngx_http_rewrite_module includes:rewrite_log: rewrite_log off;The ngx_http_session_log_module includes:session_log: session_log off;Other Logging TipsFor logging and application performance testing, set up a location block with minimal logging as it would be in production, and another location block with maximum logging for debug purposes. This way some requests can be dissected in detail and others processed with minimal overhead. This will conserve server resources. Use remote logging to aggregate information across multiple endpoints in the architecture. Create observability by combining OS and hardware metrics with NGINX logs and request tracing using OpenTelemetry.Back to topFinal ThoughtsAccording to the most recent State of Open Source Report, NGINX was the most used open source web server in 2023, surpassing Apache HTTP Server for the first time. Hopefully these tips will help you tune NGINX for optimal performance. If you need assistance getting started with NGINX, check out my blog on NGINX setup and configuration, or consider NGINX training for your team. Get Support For NGINX and All Your OSS OpenLogic provides end-to-end enterprise technical support and services for NGINX and more than 400 open source technologies. Our support is backed by SLAs and delivered by experts with 15+ years of open source experience.Explore SolutionsAdditional ResourcesGuide - Overview of Open Source Web InfrastructureBlog - Tomcat vs. NGINXBlog - Apache vs. NGINXBlog - Best Practices for Web Server Security On-Demand Webinar - Open Source Web Infrastructure: Current Ecosystem and TrendsBack to topAleksander Pahn Enterprise Architect, OpenLogic by Perforce"
45,"With a background in multiple security software domains, Aleksander has over 17 years of industry experience. He specializes in patch management, email security and open source software.  Footer menuSolutionsBy NeedEnterprise SupportConsulting ServicesMigration ServicesBy TechnologyOpenJDKDatabasesContainersEnterprise LinuxVirtualizationCI/CD ToolsMessaging MiddlewareAnalyticsAll Supported TechnologiesDownloadsBlogGuidesHistory of Open Source AdoptionCentOSOpenJDKOpen Source for EnterpriseTomcatAngularJS AlternativesApache KafkaOpen Source DatabasesWeb InfrastructureResourcesPapers & VideosRecorded WebinarsEventsNewsletterStack BuilderCustomersCustomer PortalCompanyAbout OpenLogic by PerforceCareers at PerforceCustomersNewsletterPressContactContact UsBecome an OpenLogic PartnerSubscribeServicesProfessional ServicesConsulting ServicesImplementations and UpgradesMigration ServicesTechnical Account ManagementTrainingTraining OverviewApache CamelApache ActiveMQ (Instructor-Led)Apache KafkaDevOps and CI/CDKubernetes Boot CampMonitoring and ObservabilityNGINXMySQL and PostgreSQL 2.5 DayMySQL and PostgreSQL One-DayOpenLogic by Perforce ©  Perforce Software, Inc. Terms of Use  |  Privacy Policy | SitemapSocial MenuTwitterLinkedInRSSSend Feedback"
46,Optimize Looker performance  |  Google Cloud
46,Technology areas
46,close
46,"AI solutions, generative AI, and ML"
46,Application development
46,Application hosting
46,Compute
46,Data analytics and pipelines
46,Databases
46,"Distributed, hybrid, and multi-cloud"
46,Industry solutions
46,Networking
46,Observability and monitoring
46,Security
46,Storage
46,Cross-product tools
46,close
46,Access and resources management
46,"Cloud SDK, languages, frameworks, and tools"
46,Costs and usage management
46,Infrastructure as code
46,Migration
46,Related sites
46,close
46,Google Cloud Documentation Home
46,Google Cloud Home
46,Free Trial and Free Tier
46,Architecture Center
46,Blog
46,Contact Sales
46,Google Cloud Developer Center
46,Google Developer Center
46,Google Cloud Marketplace (in console)
46,Google Cloud Marketplace Documentation
46,Google Cloud Skills Boost
46,Google Cloud Solution Center
46,Google Cloud Support
46,Google Cloud Tech Youtube Channel
46,English
46,Deutsch
46,Español – América Latina
46,Français
46,Indonesia
46,Italiano
46,Português – Brasil
46,中文 – 简体
46,日本語
46,한국어
46,Sign in
46,Looker
46,Overview
46,Guides
46,Best Practices
46,LookML Reference
46,API Reference
46,Community
46,Historical Releases
46,Contact Us
46,Start free
46,Technology areas
46,More
46,Overview
46,Guides
46,Best Practices
46,LookML Reference
46,API Reference
46,Community
46,Historical Releases
46,Cross-product tools
46,More
46,Related sites
46,More
46,Console
46,Contact Us
46,Start free
46,Administration
46,Authentication
46,Enabling the Alternate Login Option
46,Best practices
46,Best practice: Secure your folders! A content access walk-throughBest practice: Keeping Looker secure
46,Database connections
46,Connecting an MS-SQL named instance
46,Looker-hosted administration
46,What happens if the URL changes for my Looker instance?Looker-Hosted Infrastructure Migration Information
46,Self-hosted installation and administration
46,Customer-hosted architecture solutions: Component walkthroughsCustomer-hosted architecture solutions: overviewCustomer-hosted infrastructure architecture patternsInstalling Chromium for Amazon Linux
46,User management
46,Best practice: Manage users with groupsCreating a dashboard-only user
46,API and Embed
46,API
46,I mastered Looker's API Explorer. What now?
46,Embed
46,"Embed SDK versus the create_sso_embed_url endpointTroubleshooting SSO embed 404s, permissions, and content accessTroubleshooting SSO embed authentication errors"
46,Browsing Data
46,Dashboards
46,Upgrading the default dashboard experience for all Looker usersMoving a dashboard to another Looker instance
46,General
46,What are all the row limits in Looker?
46,Cookbooks
46,Getting the most out of derived tables in Looker
46,Getting the most out of Looker visualizations
46,Cookbook overview: Getting the most out of Looker visualizationsSingle value visualization customizationTooltip customizationConditional formatting customization
46,Maximizing code reusability with DRY LookML
46,"Cookbook overview: Maximizing code reusability with DRY LookMLDefine fields once, use substitution operators everywhereDefining reusable lists of fields with the set parameterDefining reusable measures for complex calculationsDefining a string once to use throughout your LookML projectCustomizing a single base view for multiple use cases"
46,Development
46,Best practices
46,Best practice: LookML dos and don'tsBest practice: Create a positive experience for Looker users
46,How to
46,"Aggregate awareness tutorialBest practice: Writing sustainable, maintainable LookMLBucketing in LookerConditional formatting using value_formatCreating custom map regionsGetting the relationship parameter rightHow to ""dimensionalize"" a measure in Looker"
46,LookML projects and version control
46,Converting .strings files to .strings.json filesMigrating to a new Git repository
46,Modeling Data
46,Live spreadsheets in databasesNested data in BigQuery (repeated records)Understanding symmetric aggregates
46,Parameters and templated filters use cases
46,Create a dynamic dashboard image with Liquid and HTMLCreating dynamic tiersEasy Date Formatting with LiquidInteresting ways to use Liquid in labelsMore powerful data drillingTimeframe versus timeframe analysis using templated filtersUsing date_start and date_end with date filtersUsing user attributes for dynamic schema and table name injection
46,Troubleshooting
46,"Changing the model or Explore of a Look or dashboardError: Column < name > must appear in the GROUP BY clause or be used in an aggregate functionError: Could not find the model or view requestedError: Measures with Looker aggregations (sum, average, min, max, list types) may not reference other measuresError: Non-Unique value/primary key (or sql_distinct_key), value overflow or collision when computing sumError: Redefinition of fieldError: Unknown or inaccessible fieldHow to fix model configuration errorsTroubleshooting an example of an advanced extends use caseWhy are my fields with division showing up as 0?Why aren't my measures coming through a join?"
46,Exploring Data
46,How to
46,Aggregating across rows (row totals) in table calculationsCalculating percent of previous and percent change with table calculationsConditional formatting using table calculationsCreating a running total down columns with table calculationsHow to calculate percent of total
46,Troubleshooting
46,Display potentially confusing table calculation totals as nullsError: view_name.field_name no longer exists on explore_name and will be ignoredTroubleshooting common filter suggestion issuesWhy are there nulls in my secondary merged results query?Why don't my totals match the values in my table?
46,General information and announcements
46,Legacy features and deprecation notices
46,Chrome is deprecating third-party cookiesAllowing XHTML-style empty tags in custom visualizationsRemoving the legacy dashboard experience - a timeline from Looker 21.20 (November 2021) through Looker 23.6 (April 2023)Looker Internet Explorer 11 supportNew LookML - How to convert a YAML LookML project
46,Looker Support
46,About Looker SupportLooker Support detailsLooker Support integrations with Google CloudWhy can't I access in-app support?
46,Security updates
46,Log4j 2 Looker updates
46,Performance Considerations
46,Best Practices
46,Considerations when building performant Looker dashboardsOptimize Looker performance
46,Troubleshooting
46,Performance overview
46,Scheduling and sending data
46,Actions
46,Looker actions - Google DriveLooker actions - Google SheetsLooker integration: Google Ads Customer MatchSending Looker content to Slack with the Slack Attachment (API Token) integrationSetting up a local action hub for actions that use OAuth and streamingUsing Lookerbot for Slack
46,Alerts
46,Setting alert conditions based on percentage changes of data valuesSetting alerts based on time series data
46,General
46,Downloading or delivering dashboards in rendered formats
46,Visualizing Data
46,Creating reference lines for visualizations with visualization menu settingsHow to create vertical reference linesHow to plot dimensions on a Y-axis
46,"AI solutions, generative AI, and ML"
46,Application development
46,Application hosting
46,Compute
46,Data analytics and pipelines
46,Databases
46,"Distributed, hybrid, and multi-cloud"
46,Industry solutions
46,Networking
46,Observability and monitoring
46,Security
46,Storage
46,Access and resources management
46,"Cloud SDK, languages, frameworks, and tools"
46,Costs and usage management
46,Infrastructure as code
46,Migration
46,Google Cloud Documentation Home
46,Google Cloud Home
46,Free Trial and Free Tier
46,Architecture Center
46,Blog
46,Contact Sales
46,Google Cloud Developer Center
46,Google Developer Center
46,Google Cloud Marketplace (in console)
46,Google Cloud Marketplace Documentation
46,Google Cloud Skills Boost
46,Google Cloud Solution Center
46,Google Cloud Support
46,Google Cloud Tech Youtube Channel
46,"Note that you are viewing Looker documentation. For Looker Studio documentation, visit https://support.google.com/looker-studio."
46,Home
46,Technology areas
46,Looker
46,Documentation
46,Best Practices
46,Send feedback
46,Optimize Looker performance
46,Stay organized with collections
46,Save and categorize content based on your preferences.
46,"These best practices reflect recommendations shared by a cross-functional team of seasoned Lookers. These insights come from years of experience working with Looker customers from implementation to long-term success. The practices are written to work for most users and situations, but you should use your best judgment when implementing."
46,Optimize query performance
46,You can ensure that queries are built and executed optimally against your database with the following frontend and backend tips:
46,Build Explores using many_to_one joins whenever possible. Joining views from the most granular level to the highest level of detail (many_to_one) typically provides the best query performance.
46,"Maximize caching to sync with your ETL policies wherever possible to reduce database query traffic. By default, Looker caches queries for one hour. You can control the caching policy and sync Looker data refreshes with your ETL process by applying datagroups"
46,"within Explores, using the persist_with parameter. This enables Looker to integrate more closely with the backend data pipeline, so cache usage can be maximized without the risk of analyzing stale data. Named caching policies can be applied to an entire model and/or to individual Explores and persistent derived tables (PDTs)."
46,"Use Looker's aggregate awareness functionality to create roll-ups or summary tables that Looker can use for queries whenever possible, especially for common queries of large databases. You can also leverage aggregate awareness to drastically improve the performance of entire dashboards. See the Aggregate awareness tutorial for additional information."
46,"Use PDTs for faster queries. Convert Explores with many complex or unperformant joins, or dimensions with subqueries or subselects, into PDTs so that the views are pre-joined and ready prior to runtime."
46,"If your database dialect supports incremental PDTs, configure incremental PDTs to reduce the time Looker spends rebuilding PDT tables."
46,"Avoid joining views into Explores on concatenated primary keys that are defined in Looker. Instead, join on the base fields that make up the concatenated primary key from the view. Alternatively, recreate the view as a PDT with the concatenated primary key predefined in the table's SQL definition, rather than in a view's LookML."
46,"Leverage the Explain in SQL Runner tool for benchmarking. EXPLAIN produces an overview of your database's query execution plan for a given SQL query, letting you detect query components that can be optimized. Learn more in the How to optimize SQL with EXPLAIN Community post."
46,Declare indexes. You can look at the indexes of each table directly in Looker from
46,SQL Runner by clicking the gear icon in a table and then selecting Show Indexes.
46,"The most common columns that can benefit from indexes are important dates and foreign keys. Adding indexes to these columns will increase performance for almost all queries. This also applies for PDTs. LookML parameters, such as indexes, sort keys, and distribution, can be applied appropriately."
46,"Increase memory, cores, and I/O (input/output) of databases with insufficient hardware or necessary provisioned resources (such as AWS) for processing large datasets, to increase query performance."
46,Optimize Looker server performance
46,You can also take measures to ensure that the Looker server and application are performing optimally:
46,"Limit the number of elements within an individual dashboard. There is no precise rule for defining the number, because the design of each element impacts memory consumption based on a variety of factors; however, dashboards with 25 or more tiles tend to be problematic when it comes to performance."
46,"Use the dashboard auto refresh feature strategically. If a dashboard uses auto refresh, make sure it refreshes no faster than the ETL processes running behind the scenes."
46,"Use pivots strategically, and avoid over-using pivots within dashboard tiles and Looks. Queries with pivoted dimensions will consume more memory. The more dimensions that are pivoted, the more memory is consumed when content (an Explore, a Look, or a dashboard) is loaded."
46,"Use post-query processing features, such as merge results, custom fields,"
46,"and table calculations, sparingly."
46,These features are intended to be used as proofs of concept to help design your model.
46,"It is best practice to hardcode any frequently used calculations and functions in LookML, which will generate SQL to be processed on your database."
46,"Excessive calculations can compete for Java memory on the Looker instance, causing the Looker instance to respond more slowly."
46,"Limit the number of views included within a model when a large number of view files are present. Including all views in a single model can slow performance. When a large number of views are present within a project, consider including only the view files needed within each model. Consider using strategic naming conventions for view file names, to enable easy inclusion of groups of views within a model. An example is outlined in the includes parameter documentation."
46,Avoid returning a large number of data points by default within dashboard tiles and Looks. Queries that return thousands of data points will consume more memory. Ensure that data is limited wherever possible by applying frontend
46,"filters to dashboards, Looks and Explores, and on the LookML level with required filters, conditionally_filter and sql_always_where parameters."
46,"Download or deliver queries using the All Results option sparingly, as some queries can be very large and overwhelm the Looker server when processed."
46,"For more help identifying the source of performance issues, check out the Performance overview Best Practices page."
46,Send feedback
46,"Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates."
46,Last updated 2023-10-11 UTC.
46,Why Google
46,Choosing Google Cloud
46,Trust and security
46,Open cloud
46,Multicloud
46,Global infrastructure
46,Customers and case studies
46,Analyst reports
46,Whitepapers
46,Blog
46,Products and pricing
46,Google Cloud pricing
46,Google Workspace pricing
46,See all products
46,Solutions
46,Infrastructure modernization
46,Databases
46,Application modernization
46,Smart analytics
46,Artificial Intelligence
46,Security
46,Productivity & work transformation
46,Industry solutions
46,DevOps solutions
46,Small business solutions
46,See all solutions
46,Resources
46,Google Cloud documentation
46,Google Cloud quickstarts
46,Google Cloud Marketplace
46,Learn about cloud computing
46,Support
46,Code samples
46,Cloud Architecture Center
46,Training
46,Certifications
46,Google for Developers
46,Google Cloud for Startups
46,System status
46,Release Notes
46,Engage
46,Contact sales
46,Find a Partner
46,Become a Partner
46,Events
46,Podcasts
46,Developer Center
46,Press Corner
46,Google Cloud on YouTube
46,Google Cloud Tech on YouTube
46,Follow on X
46,Join User Research
46,We're hiring. Join Google Cloud!
46,Google Cloud Community
46,About Google
46,Privacy
46,Site terms
46,Google Cloud terms
46,Manage cookies
46,Our third decade of climate action: join us
46,Sign up for the Google Cloud newsletter
46,Subscribe
46,English
46,Deutsch
46,Español – América Latina
46,Français
46,Indonesia
46,Italiano
46,Português – Brasil
46,中文 – 简体
46,日本語
46,한국어
47,PostgreSQL performance tips you have never seen before | Citus Con: An Event for Postgres 2023 - YouTubeInfoPresseUrheberrechtKontaktCreatorWerbenEntwicklerImpressumVerträge hier kündigenNutzungsbedingungenDatenschutzRichtlinien & SicherheitWie funktioniert YouTube?Neue Funktionen testen© 2024 Google LLC
49,Grant Fritchey - Intimidating Databases and Code
49,Skip to content
49,Home
49,About
49,Copyright
49,Resources
49,Book – “Execution Plans” code
49,Azure Data Platform Instructors
49,Upcoming Speaking Engagements
49,01Dec 2023 by Grant Fritchey 2 Comments
49,PostgreSQL Events – A Newbies Perspective: #PGSQLPhriday 014
49,PostgreSQL
49,"For this month's #PGSQLPhriday 014 blogging event, Pavlo Golub has asked a pretty simple question: What do you think about PostgreSQL events? Prior to this year, I'd never attended an event focused on PostgreSQL. Heck, I'd never attended an event that had an intentional track or learning pathway, or whatever, focused on PostgreSQL. In the past year though, I've now attended four. As a complete newbie, let me tell you a little bit about the PostgreSQL community as I see it. Welcoming The very first word that comes to mind when I think about the PostgreSQL community is that it is welcoming. Like the SQL Server community that I've been practically living with for the last almost 20 years, the people who make up the PostgreSQL community are very kind,…"
49,Read More
49,13Nov 2023 by Grant Fritchey No Comments
49,Query Store Reports Time Intervals
49,SQL Server
49,"A great question came up over at DBA.StackExchange regarding the query store reports time intervals: How can SQL Server's Query Store Reports show data for minute-length intervals, when ""Statistics Collection Interval"" is set to 1 hour? I was intrigued because it's not something I had thought about at all. How Does the Report Decided on Time? The first thing you need to know is that all performance information inside Query Store is aggregated. By default, the aggregation interval is 60 minutes. You can adjust that up or down (although, I wouldn't recommend making it too granular, you'll see a massive increase in storage size). It's aggregated because trying to capture every execution of every query, as anyone who has done it using Extended Events knows, is expensive and has a…"
49,Read More
49,15Sep 2023 by Grant Fritchey 2 Comments
49,T-SQL Tuesday #166: Wrap-up
49,SQL Server
49,"Once more, my apologies for being late on getting the T-SQL Tuesday announcement out. I have no excuse. However, our extended event on Extended Events (yes, I'm the third person to make this joke, yes, I'm blatantly stealing) still has several entries, so let's talk about them. Let's get mine out of the way. I was simply curious what the search engines revealed when I asked a pretty common question: how do you identify slow queries? What I found was, the answers on most search engines to this question are old, very old. Not to say wrong, but since many of them were created before a working version of Extended Events (let alone Query Store) was released, how could they tell you. On to actually good posts. One of my…"
49,Read More
49,14Sep 2023 by Grant Fritchey 3 Comments
49,T-SQL Tuesday #166: Why Not Extended Events?
49,SQL Server
49,"With 165 T-SQL Tuesday events, two, just two, this one, T-SQL Tuesday #166, and another one back in 2018 or 2019 (I forget and I'm far too lazy to go look) have been on Extended Events. At conferences I'm frequently the only one doing sessions on Extended Events (although, sometimes, Erin Stellato is there, presenting a better session than mine). I did a session at SQL Konferenz in Germany earlier this week on Extended Events. Hanging out in the hallway at the event (which was great by the way), I was talking with some consultants. Here's their paraphrased (probably badly) story: ""I was working with an organization just a few weeks back. They found that Trace was truncating the text on some queries they were trying to track. I asked…"
49,Read More
49,11Sep 2023 by Grant Fritchey 4 Comments
49,T-SQL Tuesday #166: Extended Events
49,T-SQL
49,"When I was put on the list to host September's T-SQL Tuesday, well, I forgot to put it in my calendar. So I'm late (and in the doghouse with Steve). Because of this, I'm going to bend the rules a little (sorry Steve) and give you a few days to get your posts together. In theory, they're all due tomorrow, Tuesday, September 12. However, let's say they're all due by the end of the day on Thursday, September 14th. My apologies for being tardy. I'll still post a roundup on Friday. So, what's the topic for T-SQL Tuesday. Well, it's in the title, Extended Events. Let's talk about it. Why Extended Events? As anyone who has read my blog or books, or seen me speak, you'll know that I've got…"
49,Read More
49,23Aug 2023 by Grant Fritchey No Comments
49,State of the Database Landscape Survey 2023
49,Professional Development
49,"As data professionals, of any stripe, we should, as much as we can, where we can, base our decisions on data. After all, in theory anyway, we're the experts at making that possible for others. We should lead the way on it. However, how do you know how others are implementing, oh, I don't know, cloud migrations, or multi-platform database management? What kind of success are they having? Where are they facing challenges? Well, one mechanism for answering these questions this is to simply ask. State of the Database Landscape Survey 2023 Yep. That's exactly what we're doing. We're asking you, and your peers, how you're doing. More, we're asking you how you're doing it. Please, help us out. Follow this link and fill out your information. Whether you're literally…"
49,Read More
49,26Jun 2023 by Grant Fritchey 2 Comments
49,Battle of the Query Hints in Query Store
49,SQL Server
49,"I recently presented a session on the Query Store at Data Saturday Rhineland and the question came up: If there's already a query hint on a query, what happens when you try to force a similar query hint? Yeah, OK, that is a weird one. I don't know the answer, but I'm about to find out. Setting up the Battle I've got this simple procedure I use a lot to illustrate bad parameter sniffing. In AdventureWorks, this query can produce up to five different plans, depending on the values called. Most of the time, it's one of two plans, which I'll get to in a minute. Here's the query: CREATE OR ALTER PROC dbo.ProductTransactionHistoryByReference (@ReferenceOrderID INT) AS BEGIN SELECT p.Name, p.ProductNumber, th.ReferenceOrderID FROM Production.Product AS p JOIN Production.TransactionHistory AS th…"
49,Read More
49,02Jun 2023 by Grant Fritchey No Comments
49,PGSQL Phriday #009: On Rollback
49,PostgreSQL
49,"The invitation this month for #PGSqlPhriday comes from Dian Fay. The topic is pretty simple, database change management. Now, I may have, once or twice, spoken about database change management, database DevOps, automating deployments, and all that sort of thing. Maybe. Once or twice. OK. This is my topic. I've got some great examples on taking changes from the schema on your PostgreSQL databases and then deploying them. All the technical stuff you could want. However, I don't want to talk about that today. Instead, I want to talk about something really important, the concept of rollbacks when it comes to database deployments. Why Are Rollbacks Difficult? The entire purpose of your PostgreSQL database is to persist, that is to store, the data. Then, of course, offer up a way…"
49,Read More
49,08May 2023 by Grant Fritchey 2 Comments
49,Why Put PostgreSQL in Azure
49,PostgreSQL
49,"I've had people come up to me and say ""PostgreSQL is open source and therefore license free. Why on earth would I put PostgreSQL in Azure?"" Honestly, I think that's a very fair question. The shortest possible answer is, of course, you don't have to. You can host your own PostgreSQL instances on local hardware, or build out VMs in Azure and put PostgreSQL out there, some other VM host, or maybe in Kubernetes containers, I mean, yeah, you have tons of options. So why PostgreSQL in Azure, and specifically, I mean the Platform as a Service offering? Let's talk about it. PostgreSQL in Azure Let's assume for a moment that you're PostgreSQL expert (I'm not, but I do play one on TV). You know exactly how to set up…"
49,Read More
49,10Apr 2023 by Grant Fritchey 12 Comments
49,ChatGPT Lied To Me
49,Data Science
49,"I've been seeing more and more noise about how ChatGPT can help with technical stuff, writing code, all that. Well, I wanted to know how best to set up Query Store in SQL Server 2022, so I asked it. I'm not going to say this any other way, ChatGPT lied to me. Here's the conversation. I'm in italics. The Conversation With ChatGPT What are the best settings for Query Store in SQL Server 2022 Query Store is a powerful feature in SQL Server 2022 that helps in monitoring query performance and identifying query regressions over time. However, setting up Query Store requires careful consideration of several factors, including the size of the database, the frequency of queries, and the query workload. Here are some best practices for configuring Query Store…"
49,Read More
49,Posts navigation
49,131
49,Next
49,Subscribe for Updates
49,Email Address
49,Subscribe
49,Join 871 other subscribers
49,Search ScaryDBA.com
49,Search for:
49,Social
49,Facebook
49,Google Plus
49,Youtube
49,CategoriesCategories
49,Select Category
49,AWS  (14)
49,Deployment Pipelines  (2)
49,RDS  (10)
49,Azure  (128)
49,Containers  (9)
49,Data Science  (6)
49,Database Fundamentals  (34)
49,DevOps  (53)
49,Database Lifecycle Management  (10)
49,Misc  (145)
49,NoSQL (Not Only SQL)  (5)
49,DocumentDB  (2)
49,HDInsight  (3)
49,Object Relational Mapping  (21)
49,nHibernate  (16)
49,Oracle  (1)
49,PASS  (244)
49,PostgreSQL  (16)
49,PowerShell  (33)
49,Professional Development  (154)
49,Redgate Software  (46)
49,Spatial Data  (9)
49,SQL Server  (313)
49,SQL Server 2016  (85)
49,SQL Server 2017  (41)
49,Surface  (11)
49,T-SQL  (260)
49,Tools  (81)
49,Uncategorized  (90)
49,Visual Studio  (33)
49,You Can’t Do That In Profiler  (13)
49,MVP
49,RSS Feed  Subscribe in a reader
49,RSS Feed  Subscribe in a reader
49,Subscribe for Updates
49,Email Address
49,Subscribe
49,Join 871 other subscribers
49,MVP
49,Follow Us
49,Twitter
49,YouTube
49,Tumblr
49,GitHub
49,LinkedIn
49,Pinterest
49,Facebook
49,WordPress Theme - Total by HashThemes
51,How to Increase Database Performance – 6 Easy Tips - DNSstuff
51,Skip to content
51,Menu
51,Networking
51,Network Monitoring Software
51,Bandwidth Monitoring
51,Scan Network for IP addresses
51,Network Traffic Monitoring Tools
51,IP Address Conflict
51,Network Mapping Tools
51,IPAM Software
51,Observability
51,What is Observability?
51,Server Monitoring Best Practices
51,Systems
51,IT Inventory Software
51,SSL Certificate Monitoring
51,Windows Server Performance Monitoring
51,IIS Performance Monitoring
51,Databases
51,SQL Server Queries Monitoring
51,SQL Server Performance
51,Oracle Database Monitoring
51,How to Increase Database Performance
51,Security
51,Help Desk
51,ITSM
51,IT Service Management (ITSM) Tools
51,ITIL Event Management
51,Incident Management Tools
51,Help Desk vs Service Desk
51,IT Asset Management Software
51,Help Desk Ticketing System
51,Free Help Desk Software
51,Free Tools
51,Compare
51,"How to Increase Database Performance – 6 Easy TipsBy Staff Contributor on July 28, 2023"
51,"Database administrators are all too familiar with the frustration of receiving an endless stream of calls about slow online performance. Instead of trying to resolve each individual issue as it arises, a better solution is to undertake database performance tuning activities that will improve online performance for all your end users. These activities can help you identify any bottlenecks in your system and ensure your infrastructure is able to handle increased loads."
51,"There are several steps you can take to increase database performance. The following six easy tips can help you prevent or rectify possible issues with database performance. Even with these tips, it’s important to remember the best way to increase database performance is always by using the right tools. Based on my experiences and tests I can recommend SolarWinds® Database Performance Analyzer (DPA) and Database Performance Monitor (DPM)."
51,By using even the free trial versions of these tools you’ll be able to:
51,"Monitor and analyze the performance of database servers running individually, in clusters, and as cloud infrastructure."
51,Receive and implement database performance advice on specific databases and related queries.
51,"Monitor database performance in real time and analyze past periods. In addition, the software tested by us enables the detection of anomalies in the load time and the location of bottlenecks in database performance."
51,"After going through the best tips for manually improving performance, I’ll take a closer look at some of the best tools to help you improve performance even further."
51,Why Is Increasing Database Performance Important?Tips To Increase Database PerformanceTools That Can Help YouSummary
51,Why Is Increasing Database Performance Important?
51,"People often wonder whether it’s important to increase database performance. The truth is your business can only ever be as successful as your IT operations allow it to be. In fact, a high-functioning database can have a huge impact on corporate profitability. When data retrieval is slowed down by anything from a poorly written query to an indexing issue, a bottleneck that slows down performance and lowers productivity for the entire organization can emerge. When you learn how to increase database performance, you’re better able to avoid unnecessary financial loss as a result of server inefficiencies."
51,"There are also many financial gains that come with improving the end-user experience. Since your customer-facing websites and applications retrieve data from your centralized database, inefficient indexes and suboptimal queries can have just as big an impact on customers as on your internal end users. As a result, your customer satisfaction is directly linked to database performance. That means knowing how to increase database performance can be one of the most important customer service tools in your toolbox."
51,Tips to Increase Database Performance
51,"While there are many ways you can go about learning how to increase database performance, these six have proven to be some of the most effective and impactful when it comes to avoiding performance degradation."
51,Tip 1: Optimize Queries
51,"In many cases database performance issues are caused by inefficient SQL queries. Optimizing your SQL queries is one of the best ways to increase database performance. When you try to do that manually, you’ll encounter several dilemmas around choosing how best to improve query efficiency. These include understanding whether to write a join or a subquery, whether to use EXISTS or IN, and more. When you know the best path forward, you can write queries that improve efficiency and thus database performance as a whole. That means fewer bottlenecks and fewer unhappy end users."
51,The best way to optimize queries is to use a database performance analysis solution that can guide your optimization efforts by directing you to the most inefficient queries and offering expert advice on how best to improve them.
51,Tip 2: Improve Indexes
51,"In addition to queries, the other essential element of the database is the index. When done right, indexing can increase your database performance and help optimize the duration of your query execution. Indexing creates a data structure that helps keep all your data organized and makes it easier to locate information. Because it’s easier to find data, indexing increases the efficiency of data retrieval and speeds up the entire process, saving both you and the system time and effort."
51,Tip 3: Defragment Data
51,"Data defragmentation is one of the best approaches to increasing database performance. Over time, with so much data constantly being written to and deleted from your database, your data can become fragmented. That fragmentation can slow down the data retrieval process as it interferes with a query’s ability to quickly locate the information it’s looking for. When you defragment data, you allow for relevant data to be grouped together and you erase index page issues. That means your I/O related operations will run faster."
51,Tip 4: Increase Memory
51,"The efficiency of your database can suffer significantly when you don’t have enough memory available for the database to work correctly. Even if it seems like you have a lot of memory in total, you might not be meeting the demands of your database. A good way to figure out if you need more memory is to check how many page faults your system has. When the number of faults is high, it means your hosts are either running low on or completely out of available memory. Increasing your memory allocation will help boost efficiency and overall performance."
51,Tip 5: Strengthen CPU
51,"A better CPU translates directly into a more efficient database. That’s why you should consider upgrading to a higher-class CPU unit if you’re experiencing issues with your database performance. The more powerful your CPU is, the less strain it’ll have when dealing with multiple requests and applications. When assessing your CPU, you should keep track of all the elements of CPU performance, including CPU ready times, which tell you about the times your system tried to use the CPU, but couldn’t because the resources were otherwise occupied."
51,Tip 6: Review Access
51,"Once you know your database hardware is working well, you need to review your database access, including which applications are actually accessing your database. If one of your services or applications is suffering from poor database performance, it’s important not to jump to conclusions about which service or application is responsible for the issue. It’s possible a single client is experiencing the bad performance, but it’s also possible the database as a whole is having issues. Dig into who and what is accessing the database and if it’s only one service that’s having an issue, drill down into its metrics to try and find the root cause."
51,"While these tips can help you increase database performance, manual efforts can only do so much. If you want to save time and energy while strengthening your performance optimization efforts, you should invest in a database performance analysis and monitoring solution. The following tools are the best ones on the market when it comes to increasing database performance."
51,Tools That Can Help You
51,1. SolarWinds Database Performance Analyzer (DPA)
51,"SolarWinds DPA is a powerful multidimensional database performance solution that supports PostgreSQL, MySQL, IBM DB2, Amazon Aurora, SAP ASE, Oracle, Microsoft SQL Server, MariaDB, and Azure SQL databases. It focuses on bottleneck identification and offers automated index and query optimization advisors to help you target your database performance optimization efforts."
51,The solution uses response time analysis — which measures the actual amount of time it takes to complete an operation — to tune queries and improve overall database performance. DPA gives users the insights they need to align their resource provisioning with database performance to help ensure hardware isn’t interfering with performance.
51,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
51,"Working in a database environment often requires database administrators or developers (DevOp) to observe the behavior of the entire environment in which their databases and applications run. DPA easily integrates the process of monitoring databases, hardware, and other entities used by applications and databases."
51,DPA can also be integrated with SolarWinds Storage Resource Monitor (SRM) to get a deeper understanding of your database performance. When DPA and SRM are integrated you can get contextually relevant information on the storage objects related to the databases being monitored by DPA. You can then correlate storage performance with the relevant databases.
51,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
51,"You can try a 14-day free trial, during which DPA is fully functional."
51,Learn more about DPA
51,Download free trial
51,2. SolarWinds® SQL Sentry
51,"SQL Sentry is built to help you quickly identify long-running and high-impact queries, so you can resolve performance problems and quickly increase database performance."
51,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
51,Of note is how SQL Sentry can help you more easily perform the following:
51,Identifying top resource-consuming queries over time with the ability to tune them in the same interface
51,Defining alerting and automated remediation for known performance problems
51,Knowing when a new release or data churn over time has changed the performance profile for one or more queries
51,You can try SQL Sentry free for 14 days.
51,3. SolarWinds Database Performance Monitor (DPM)
51,"Database Performance Monitor is another great solution from SolarWinds. This software puts a bigger focus on monitoring database performance as compared to DPA. DPM offers a convenient and efficient means of monitoring your database performance in real time, around the clock. Check-ups on performance are simple and DPM gives you access to expert guidance that can help you improve database performance. The tool sends you instant recommendations around underperforming queries or server configuration changes. What is important is the way DPM works—it’s most often used to monitor no-SQL databases. This monitoring tool for databases such as Redis, MongoDB, and Azure can work locally and in cloud and hybrid environments. As it works in SaaS model, it allows access to the user dashboard in a web-based user interface."
51,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
51,"DPM helps you understand how your queries are impacting performance and gets to the bottom of any problems the tool detects, thanks to extensive correlated data. Try DPM free for 14 days."
51,4. SolarWinds Server Configuration Monitor (SCM)
51,"Another good solution to consider if your business runs on SQL databases is SolarWinds SCM, which runs SQL queries to connect and monitor any relational databases in your system including MySQL, Microsoft SQL Server, PostgreSQL, or Oracle. SCM collects data through queries then saves it and monitors it for changes. This can help you stay on top of any database configuration changes — including changes to user permissions and schemas — that could negatively impact database performance."
51,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
51,SCM comes with a free 30-day trial.
51,5. Paessler PRTG Network Monitor
51,"Paessler PRTG Network Monitor is another great tool if you’re looking to increase your database performance. Like DPM, it puts an emphasis on database monitoring. In fact, one of the benefits of the tool is it’s a one-stop shop for infrastructure monitoring covering databases and applications, bandwidth, packets, traffic, cloud services, uptime, ports, IPs, virtual environments, hardware, web services, security, physical environments, disk usage, and IoT devices. The tool is easy to set up and highly customizable, making it a good fit for anyone looking for a simple solution to their database and infrastructure monitoring needs."
51,© 2023 PAESSLER AG. All rights reserved.
51,A free version of PRTG is available for 30 days.
51,Increasing Database Performance Through Monitoring and Analysis
51,"If you’re looking for the best way to increase your database performance, there’s no better option than using professional software."
51,"I compared ease of use, integration with other programs, technical support during installation, and during use of the tools. This allowed me to select and recommend SolarWinds products to database administrators looking for a scalable solution for their daily work in database analysis and monitoring. Although there many monitoring and analysis products on the market, SolarWinds DPA, SQL Sentry, and SCM are a good place to start. The tools cost starts at USD$1,111.* You can check how it works with a 14 days trial version (available on official SolarWinds website: click here)."
51,Related Posts
51,10 MySQL Database Performance Tuning Tips
51,Oracle 12c Performance Tuning — Top Tips for Database Admins
51,Top Db2 Performance Tuning Tips
51,Categories Systems
51,Post navigation
51,MySQL vs. MSSQL – Performance and Main DifferencesCommon Database Problems and Performance Issues
51,Most Popular Posts
51,Ultimate Guide to Windows Event Logs for 2024
51,What Is Syslog? Syslog Server vs. Event Log Explained + Recommended Syslog Management Tool
51,5 Best Tripwire Alternatives 2024
51,"Jitter, Packet Loss, and Latency in Network Performance"
51,Best Syslog Servers in 2024
51,13 Best Service Request Management Software of 2024
51,Top 8 Observability Tools
51,10 Best IT Self-Service Software in 2024
51,8 Best Service-Level Management Tools for 2024
51,Dameware Products Review 2024
51,Languages
51,English
51,Deutsch
51,Français
51,"© 2024 SolarWinds Worldwide, LLC. All rights reserved."
51,About Us |
51,Trademarks |
51,Privacy Policy |
51,Terms Of Use
51,Scroll back to top
52,"Performance | GORM - The fantastic ORM library for Golang, aims to be developer friendly."
52,DocsGenCommunityAPIContribute
52,English
52,English
52,简体中文
52,Deutsch
52,bahasa Indonesia
52,日本語
52,Русский
52,한국어
52,हिन्दी
52,French
52,Italiano
52,Español
52,Performance
52,"GORM optimizes many things to improve the performance, the default performance should be good for most applications, but there are still some tips for how to improve it for your application."
52,"Disable Default TransactionGORM performs write (create/update/delete) operations inside a transaction to ensure data consistency, which is bad for performance, you can disable it during initialization"
52,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
52,"SkipDefaultTransaction: true,})"
52,Caches Prepared StatementCreates a prepared statement when executing any SQL and caches them to speed up future calls
52,"// Globally modedb, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
52,"PrepareStmt: true,})// Session modetx := db.Session(&Session{PrepareStmt: true})tx.First(&user, 1)tx.Find(&users)tx.Model(&user).Update(""Age"", 18)"
52,NOTE Also refer how to enable interpolateparams for MySQL to reduce roundtrip https://github.com/go-sql-driver/mysql#interpolateparams
52,"SQL Builder with PreparedStmtPrepared Statement works with RAW SQL also, for example:"
52,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
52,"PrepareStmt: true,})db.Raw(""select sum(age) from users where role = ?"", ""admin"").Scan(&age)"
52,"You can also use GORM API to prepare SQL with DryRun Mode, and execute it with prepared statement later, checkout Session Mode for details"
52,"Select FieldsBy default GORM select all fields when querying, you can use Select to specify fields you want"
52,"db.Select(""Name"", ""Age"").Find(&Users{})"
52,Or define a smaller API struct to use the smart select fields feature
52,type User struct {
52,uint
52,Name
52,string
52,Age
52,int
52,Gender string
52,// hundreds of fields}type APIUser struct {
52,uint
52,"Name string}// Select `id`, `name` automatically when querydb.Model(&User{}).Limit(10).Find(&APIUser{})// SELECT `id`, `name` FROM `users` LIMIT 10"
52,Iteration / FindInBatchesQuery and process records with iteration or in batches
52,"Index HintsIndex is used to speed up data search and SQL query performance. Index Hints gives the optimizer information about how to choose indexes during query processing, which gives the flexibility to choose a more efficient execution plan than the optimizer"
52,"import ""gorm.io/hints""db.Clauses(hints.UseIndex(""idx_user_name"")).Find(&User{})// SELECT * FROM `users` USE INDEX (`idx_user_name`)db.Clauses(hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForJoin()).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR JOIN (`idx_user_name`,`idx_user_id`)""db.Clauses("
52,"hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForOrderBy(),"
52,"hints.IgnoreIndex(""idx_user_name"").ForGroupBy(),).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR ORDER BY (`idx_user_name`,`idx_user_id`) IGNORE INDEX FOR GROUP BY (`idx_user_name`)"""
52,"Read/Write SplittingIncrease data throughput through read/write splitting, check out Database Resolver"
52,Last updated: 2024-03-09
52,PrevNext
52,Platinum Sponsors
52,Become a Sponsor!
52,Gold Sponsors
52,Become a Sponsor!
52,Platinum Sponsors
52,Become a Sponsor!
52,Gold Sponsors
52,Become a Sponsor!
52,Contents
52,Disable Default TransactionCaches Prepared StatementSQL Builder with PreparedStmtSelect FieldsIteration / FindInBatchesIndex HintsRead/Write Splitting
52,Improve this page
52,Back to Top
52,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
52,© 2013~2024 Jinzhu
52,Documentation licensed under CC BY 4.0.
52,感谢 无闻 对域名 gorm.cn 的捐赠
52,浙ICP备2020033190号-1
52,Home
52,DocsGenCommunityAPIContribute
52,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
52,English
52,English
52,简体中文
52,Deutsch
52,bahasa Indonesia
52,日本語
52,Русский
52,한국어
52,हिन्दी
52,French
52,Italiano
52,Español
53,Azure Database for PostgreSQL Blog - Microsoft Community Hub
53,Microsoft
53,Tech Community
53,Home
53,Community Hubs
53,Community Hubs
53,Community Hubs Home
53,Products
53,Special Topics
53,Video Hub
53,Close
53,Products (49)
53,Special Topics
53,(27)
53,Video Hub
53,(459)
53,Most Active Hubs
53,"Microsoft 365 Microsoft Teams Windows Outlook Security, Compliance and Identity SharePoint Azure Windows Server Exchange Intune and Configuration Manager Microsoft Edge Insider OneDrive"
53,Microsoft Viva
53,Connect and learn from experts and peers
53,Microsoft FastTrack
53,Best practices and the latest news on Microsoft FastTrack
53,Microsoft Sales Copilot
53,A role-based copilot designed for sellers
53,Most Active Hubs
53,Education Sector ITOps Talk AI and Machine Learning Microsoft Mechanics Microsoft Partner Community Healthcare and Life Sciences Internet of Things (IoT) Public Sector Mixed Reality Small and Medium Business Startups at Microsoft
53,Azure Partner Community
53,Expand your Azure partner-to-partner network
53,Microsoft Tech Talks
53,Bringing IT Pros together through In-Person & Virtual events
53,MVP Award Program
53,Find out more about the Microsoft MVP Award Program.
53,Video Hub
53,Azure Exchange Microsoft 365 Microsoft 365 Business Microsoft 365 Enterprise Microsoft Edge Microsoft Outlook Microsoft Teams Security SharePoint Windows
53,Browse All Community Hubs
53,Blogs
53,Blogs
53,Events
53,Events
53,Events Home
53,Microsoft Ignite
53,Microsoft Build
53,Community Events
53,Microsoft Learn
53,Microsoft Learn
53,Home
53,Community
53,Blog
53,Azure
53,Dynamics 365
53,Microsoft 365
53,"Security, Compliance & Identity"
53,Power Platform
53,Github
53,Teams
53,.NET
53,Lounge
53,Lounge
53,1.3M
53,Members
53,14.8K
53,Online
53,323K
53,Discussions
53,Search
53,cancel
53,Turn on suggestions
53,Auto-suggest helps you quickly narrow down your search results by suggesting possible matches as you type.
53,Showing results for
53,Show  only
53,Search instead for
53,Did you mean:
53,Sign In
53,Sign In
53,cancel
53,Turn on suggestions
53,Auto-suggest helps you quickly narrow down your search results by suggesting possible matches as you type.
53,Showing results for
53,Show  only
53,Search instead for
53,Did you mean:
53,Azure Database for PostgreSQL Blog
53,Filter by label
53,Filter by label
53,@microsoft
53,Active Directory
53,ADF
53,Analytics
53,Announcements
53,Authentication
53,Auto Failover
53,availability
53,Azure
53,Azure Active Directory
53,Azure Active Directory (AAD)
53,Azure CLI
53,Azure Cosmos DB for PostgreSQL
53,Azure Database
53,Azure Database for PostgreSQL
53,Azure Database for PostgreSQL Flexible Server
53,Azure Database Migration Service
53,Azure Databases
53,Azure Data Studio
53,Azure Monitor
53,Azure Open Source
53,Azure Postgres
53,Azure PostgresSQL Flexible Server
53,bloat
53,Caching
53,ChatGPT
53,Citus
53,Citus Con
53,CloudComputing
53,Cloud Security
53,Community
53,Community Events
53,conferences
53,conference talks
53,Connection handling best practice with PostgreSQL
53,connection pooling
53,Connections
53,Connectivity
53,copy
53,Database
53,DatabaseMigration
53,Database Monitoring
53,Database Performance
53,databases
53,DatabaseScalability
53,Data Management
53,datatype
53,deployment options
53,developers
53,Disaster Recovery
53,etl
53,Flexible
53,Flexible Server
53,Flexible Server networking
53,Geospatial
53,gis
53,GraphQL
53,logging
53,Logs
53,Metrics
53,Microsoft Ignite 2023
53,Migration
53,Migration Tool
53,monitoring
53,Observability
53,opensource
53,Open Source
53,Oracle to Postgres
53,Performance
53,Performance Issues
53,pg_dump
53,pgBadger
53,pgbouncer
53,PGSQLPhriday
53,Podcasts
53,postgis
53,Postgres
53,Postgres contributions
53,Postgres extensions
53,Postgres indexes
53,PostgreSQL
53,PostgreSQL15
53,PostgreSQL Enhancements
53,PostgreSQL Flexible Server
53,PostgresUpdate
53,private
53,privatelink
53,Python
53,query caching
53,read replica
53,Replication
53,retirement
53,scalability
53,Scaling out Azure Database for PostgreSQL using Foreign Data Wrappers
53,secure
53,Security
53,Single Server
53,sql
53,Troubleshooting
53,Follow
53,RSS
53,URL
53,Copy
53,Options
53,Author
53,Add author
53,Searching
53,invalid author
53,# of articles
53,100
53,200
53,Labels
53,Select Label (0)
53,Clear selected
53,@microsoft
53,Active Directory
53,ADF
53,Alerting
53,Alerts
53,alter
53,AMDComputeSKUs
53,Analytics
53,Announcements
53,API
53,Application performance
53,april 1st
53,April Fools' joke
53,Artifical Intelligent
53,Audit
53,Authentication
53,Auto Failover
53,Automation
53,AutomationTasks
53,Autoscaling
53,Autovacuum
53,Autovacuum Monitoring
53,availability
53,Azure
53,Azure Active Directory
53,Azure Active Directory (AAD)
53,Azure Active Directory Audit
53,Azure AI
53,AzureAMDOptions
53,AzureAutomationTasks
53,Azure Blog
53,Azure CLI
53,Azure Cosmos DB for PostgreSQL
53,Azure Data
53,AzureDatabase
53,Azure Database
53,Azure Database for PostgreSQL
53,Azure Database for PostgreSQL Flexible Server
53,Azure Database Migration Service
53,Azure Databases
53,Azure Data Factory
53,Azure Data Studio
53,Azure Defender for Cloud
53,Azure Developer
53,Azure Diagnostics
53,AzureInnovation
53,Azure Integration
53,Azure Log Analytics
53,Azure Monitor
53,Azure monitor alert
53,azure networking
53,Azure Open AI
53,Azure Open Source
53,Azure Postgres
53,Azure-Postgres-Flexible-Server
53,AzurePostgreSQLUpdate
53,Azure PostgresSQL Flexible Server
53,Azure Private Link
53,Azure private link – connecting Azure Database for PostgreSQL - Single server
53,AzureServices
53,Azure Storage
53,Azure Synapse Analtyics
53,AzureUpdate
53,Azure Updates
53,Backup and Recovery
53,backup and restore
53,Benchmarking
53,bloat
53,Blob
53,Caching
53,CDC
53,ChatGPT
53,Citus
53,Citus Con
53,CloudComputing
53,Cloud Computing
53,Cloud Database
53,CloudDatabases
53,CloudMigration
53,Cloud Security
53,Cloud Services
53,Cloud Storage
53,CloudTechNews
53,Community
53,Community Events
53,conferences
53,conference talks
53,Connection handling best practice with PostgreSQL
53,connection pooling
53,Connections
53,Connectivity
53,connnections
53,copy
53,Cost Management
53,Cost Optimization
53,cross region replication
53,customer feedback
53,customer managed key
53,DAB
53,Dashboards
53,Data API Builder
53,Database
53,Database development
53,Database Features
53,DatabaseManagement
53,Database Management
53,DatabaseMigration
53,Database Monitoring
53,DatabaseOptimization
53,DatabasePerformance
53,Database Performance
53,databases
53,DatabaseScalability
53,DatabaseSecurity
53,Database Security
53,Database Upgrades
53,data loading
53,Data Management
53,Data Migration
53,data protection
53,datatype
53,Data Visualisation
53,demo
53,deployment options
53,developers
53,Diagnostic Settings
53,different subscription
53,Disaster Recovery
53,Encryption
53,endpoint
53,Endpoint Management
53,Endpoints
53,etl
53,Events and Conferences
53,Extensions
53,failover
53,failover cluster
53,Fast Restore
53,Flexible
53,FlexibleDNS
53,FlexibleServer
53,Flexible Server
53,Flexible Server networking
53,flexible working
53,Function App
53,Functions
53,gateway
53,geo
53,geo disaster recovery
53,Geospatial
53,gis
53,Grafana
53,GraphQL
53,High Availability
53,high cpu
53,Hyperscale (Citus)
53,logging
53,Logs
53,Metrics
53,Microsoft Ignite 2023
53,Migration
53,Migration Tool
53,monitoring
53,Observability
53,opensource
53,Open Source
53,Oracle to Postgres
53,Performance
53,Performance Issues
53,pg_dump
53,pgBadger
53,pgbouncer
53,PGSQLPhriday
53,Podcasts
53,postgis
53,Postgres
53,Postgres contributions
53,Postgres extensions
53,Postgres indexes
53,PostgreSQL
53,PostgreSQL15
53,PostgreSQL Enhancements
53,PostgreSQL Flexible Server
53,PostgresUpdate
53,private
53,privatelink
53,Python
53,query caching
53,read replica
53,Replication
53,retirement
53,scalability
53,Scaling out Azure Database for PostgreSQL using Foreign Data Wrappers
53,secure
53,Security
53,Single Server
53,sql
53,Troubleshooting
53,Clear
53,Home
53,Home
53,Azure Data
53,Azure Database for PostgreSQL Blog
53,Options
53,Mark all as New
53,Mark all as Read
53,Pin this item to the top
53,Subscribe
53,Bookmark
53,Subscribe to RSS Feed
53,951
53,PostgreSQL Making Index creation faster
53,Gayathri_Paderla
53,Mar 05 2024 08:38 AM
53,Best practices for creating PostgreSQL indexes to make the creation
53,process faster.
53,"1,136"
53,Tips for tuning Postgres LIKE & ILIKE queries via pg_trgm on Azure
53,Sarat_Balijepalli
53,Mar 04 2024 09:54 AM
53,Tips to improve query performance when using pattern search with LIKE or
53,ILIKE operators by using apt Indexing.
53,"2,669"
53,What’s in a name? Hello POSETTE: An Event for Postgres 2024
53,clairegiordano
53,Mar 01 2024 01:27 PM
53,What does POSETTE stand for? And what exactly is POSETTE: An Event for
53,Postgres? (Hint: it’s free & virtual.)
53,"1,044"
53,"Podcast about transitioning from developer to PostgreSQL specialist, with Derk van Veen"
53,techieari
53,Feb 28 2024 09:32 AM
53,Fave bits from this podcast episode Postgres and Derk’s transition from
53,Java developer to Postgres specialist.
53,"2,429"
53,"The azure_ai extension to Azure Database for PostgreSQL, definition and demo"
53,clairegiordano
53,Feb 27 2024 10:43 AM
53,What you need to know about this demo of the new azure_ai extension to
53,Azure Database for PostgreSQL – Flexible Server.
53,"7,137"
53,What’s new in the Postgres 16 query planner / optimizer
53,David_Rowley
53,Feb 08 2024 10:07 AM
53,Postgres 16 improves the query planner to make many SQL queries run
53,faster than on previous versions of Postgres.
53,"9,671"
53,January 2024 Recap: Azure PostgreSQL Flexible Server
53,varun-dhawan
53,Feb 05 2024 07:21 AM
53,January 2024 Feature Recap: Azure PostgreSQL Flexible Server Updates:
53,New Features and Enhancements.
53,"1,213"
53,"Podcast highlights on benchmarking Postgres performance, from Path To Citus Con Episode 11"
53,Aaron Wislang
53,Feb 02 2024 01:43 PM
53,"Highlights from Episode 11 about Postgres benchmarking, with engineers"
53,Jelte Fennema-Nio and Marco Slot.
53,"1,875"
53,Harness the Power of the Migration Service in Azure Database for PostgreSQL (Preview)
53,adityaduvuri
53,Jan 30 2024 01:03 AM
53,"Modernize with ease, streamline PostgreSQL migrations to Azure."
53,"4,364"
53,Advanced Monitoring for PostgreSQL Using Log Analytics
53,varun-dhawan
53,Jan 22 2024 03:15 PM
53,This blog discusses how to use Azure Log Analytics for monitoring and
53,alerting on a large fleet Azure Postgres servers.
53,"1,992"
53,Seamless Shift: Migrating from Amazon RDS for PostgreSQL to Azure Database for PostgreSQL
53,gyanisinha
53,Jan 15 2024 05:59 AM
53,A step-by-step game plan for seamless migration from Amazon RDS for
53,PostgreSQL to Azure Database for PostgreSQL
53,"1,444"
53,Highlights from podcast episode about Postgres monitoring with Lukas Fittl and Rob Treat
53,techieari
53,Dec 15 2023 12:50 PM
53,"Highlights from the latest Path To Citus Con episode with Lukas, Rob,"
53,"Claire, and Pino all about PostgreSQL monitoring."
53,"2,079"
53,Enhance your Azure PostgreSQL Flexible Server security posture with Azure Defender
53,Gennadyk
53,Dec 13 2023 08:01 AM
53,Announcing Azure Defender for Cloud support with Microsoft Azure
53,Database for PostgreSQL - Flexible Server
53,"8,593"
53,November 2023 Recap: Azure PostgreSQL Flexible Server
53,varun-dhawan
53,Dec 06 2023 08:27 AM
53,November 2023 Azure Database for PostgreSQL Flexible Server Updates: New
53,Features and Enhancements.
53,"2,005"
53,My Illustrated Guide to Postgres at PASS Data Summit 2023
53,clairegiordano
53,Dec 01 2023 12:06 PM
53,This illustrated guide to my first-timer Postgres experience at PASS
53,Data Community Summit 2023 is a show and tell of wh...
53,"2,322"
53,Mastering the Move: Migrating from Azure Database for PostgreSQL Single to Flexible Server
53,hitakagi
53,Dec 01 2023 06:57 AM
53,Migrate from Azure PostgreSQL - Single to Flexible Server: Key steps for
53,a seamless transition.
53,"4,679"
53,Enhance Monitoring with Azure Postgres Grafana Dashboard
53,varun-dhawan
53,Nov 30 2023 10:31 AM
53,Enhancing Monitoring: Introducing the New Grafana Dashboard Integration.
53,"5,921"
53,Azure Database for PostgreSQL: AI-Ready for Enterprise Applications in Flexible Server
53,charlesfeddersenMS
53,Nov 15 2023 08:00 AM
53,Your guide to the 11 new features announced at Microsoft Ignite 2023 for
53,Azure Database for PostgreSQL – Flexible Server...
53,"2,215"
53,Multi-Region Disaster Recovery: Streamlined with New Features
53,AlicjaKucharczyk
53,Nov 15 2023 08:00 AM
53,Multi region disaster recovery in Azure PostgreSQL Elevate your disaster
53,recovery strategy with new public preview featu...
53,"4,542"
53,General Availability of PostgreSQL 16 on Azure Database for PostgreSQL - Flexible Server
53,AlicjaKucharczyk
53,Nov 15 2023 08:00 AM
53,PostgreSQL16-new-features PostgreSQL 16 is now generally available on
53,Azure Database for PostgreSQL - Flexible Server.
53,"6,221"
53,Introducing the azure_ai extension to Azure Database for PostgreSQL
53,Denzil Ribeiro
53,Nov 15 2023 08:00 AM
53,"With a single line of SQL, you can use the new azure_ai extension to"
53,call Azure OpenAI and Azure AI Language service. Al...
53,"2,823"
53,Introducing Premium SSD v2 for Azure Database for PostgreSQL Flexible Server
53,kabharati
53,Nov 15 2023 08:00 AM
53,Introducing Premium SSD v2 for Azure Database for PostgreSQL Flexible
53,Server
53,"3,205"
53,Introducing Private Link based networking with Azure Database for PostgreSQL – Flexible Server
53,Gennadyk
53,Nov 15 2023 08:00 AM
53,Today we are proud to announce preview of Azure Private Link support for
53,private networking with Azure Database for Post...
53,"2,220"
53,Near-zero Downtime Scaling in Azure Database for PostgreSQL Flexible Server
53,kabharati
53,Nov 15 2023 08:00 AM
53,Blazing Fast Scaling using Near Zero Downtime Scaling on Flexible Server
53,"3,031"
53,Highlights of Path To Citus Con podcast ep09 with Postgres experts Dimitri Fontaine and Vik Fearing
53,techieari
53,Nov 11 2023 12:40 PM
53,"Fave bits from this podcast episode about SQL, Postgres, & whether you"
53,can (or should) solve every data problem in SQL.
53,"1,840"
53,Introducing Pre-Migration Validations for Single Server to Flexible Server Migration tool
53,shriramm
53,Oct 26 2023 10:22 AM
53,Plan your migrations better by performing pre-migration validations in
53,advance to know the potential migration issues al...
53,"2,754"
53,September 2023 Recap: Azure Database for PostgreSQL Flexible Server
53,kabharati
53,Oct 12 2023 01:12 PM
53,September 2023 Updates for Azure Database PostgreSQL Flexible Server:
53,Major Developments & Features
53,"4,773"
53,The Story Behind the Activity Book for Postgres
53,Teresa_Giacomini
53,Sep 28 2023 05:11 PM
53,Read the story behind “An Activity Book for Postgres”. What inspired its
53,creation? How can you get your own copy?
53,"3,154"
53,Creating and Monitoring Read Replicas in Azure Postgres Flexible Server
53,TyBecker
53,Sep 25 2023 03:17 PM
53,How using a Postgres Read Replica can help with the performance and
53,resiliency of your data solution.
53,"1,773"
53,Why giving talks at Postgres conferences matters? Highlights from a podcast
53,clairegiordano
53,Sep 20 2023 09:59 AM
53,Highlights from this conference-focused episode of the Path To Citus Con
53,podcast for developers who love Postgres.
53,Previous
53,Next
53,Latest Comments
53,Gayathri_Paderla
53,PostgreSQL Making Index creation faster
53,Mar 11 2024 12:50 PM
53,Thank you for reviewing the article. It's important to note that there
53,"are multiple methods available for calculating bloat, and in this"
53,"article, I have opted for this specific formula."
53,0 Likes
53,Srinuvas
53,PostgreSQL Making Index creation faster
53,Mar 06 2024 08:30 AM
53,"Hi Gayathri, Thank you for sharing the info. The calculation for"
53,dead_pct should be (n_dead_tup::float / (n_live_tup + n_dead_tup)) *
53,100.
53,0 Likes
53,theuberger
53,General Availability of PostgreSQL 16 on Azure Database for PostgreSQL - Flexible Server
53,Mar 05 2024 04:30 AM
53,Hey @AlicjaKucharczyk Any updates on the timeframe when existing fs will
53,be able to upgrade to PG16?
53,0 Likes
53,vovadmin
53,Major Version Upgrade in Azure PostgreSQL Flexible Server - Generally Available
53,Feb 23 2024 02:54 AM
53,"I want to upgrade from 15.4 to 16 but I got: ""You are already running"
53,"the latest major PostgreSQL version available.""How can I upgrade?"
53,0 Likes
53,swadhwa82
53,Azure Database for PostgreSQL : Logical Replication
53,Feb 13 2024 04:49 AM
53,"Hello @bmckerr-MSFT - Thanks so much for sharing the details. However, I"
53,am curious to understand what the advantages could be of using Logical
53,"replication for DR scenarios across multi region. On the other hand,"
53,"Microsoft also recommends, read replica approach and promote Read"
53,replica as Read/Write...
53,0 Likes
53,Browse
53,What's new
53,Surface Pro 9
53,Surface Laptop 5
53,Surface Studio 2+
53,Surface Laptop Go 2
53,Surface Laptop Studio
53,Surface Duo 2
53,Microsoft 365
53,Windows 11 apps
53,Microsoft Store
53,Account profile
53,Download Center
53,Microsoft Store support
53,Returns
53,Order tracking
53,Virtual workshops and training
53,Microsoft Store Promise
53,Flexible Payments
53,Education
53,Microsoft in education
53,Devices for education
53,Microsoft Teams for Education
53,Microsoft 365 Education
53,Education consultation appointment
53,Educator training and development
53,Deals for students and parents
53,Azure for students
53,Business
53,Microsoft Cloud
53,Microsoft Security
53,Dynamics 365
53,Microsoft 365
53,Microsoft Power Platform
53,Microsoft Teams
53,Microsoft Industry
53,Small Business
53,Developer & IT
53,Azure
53,Developer Center
53,Documentation
53,Microsoft Learn
53,Microsoft Tech Community
53,Azure Marketplace
53,AppSource
53,Visual Studio
53,Company
53,Careers
53,About Microsoft
53,Company news
53,Privacy at Microsoft
53,Investors
53,Diversity and inclusion
53,Accessibility
53,Sustainability
53,California Consumer Privacy Act (CCPA) Opt-Out Icon
53,Your Privacy Choices
53,Sitemap
53,Contact Microsoft
53,Privacy
53,Manage cookies
53,Terms of use
53,Trademarks
53,Safety & eco
53,About our ads
53,© Microsoft 2024
54,"PostgreSQL High CPU Usage: Causes, Consequences, and Solutions"
54,Skip to content
54,Menu
54,Menu WindowsWindows 11Windows 10Windows 8Windows 7Windows VistaWindows XPDevelopmentJavaPythonPHPRuby/RailsDatabasesPostgreSQLMySQL (MariaDB)CassandraTipsLinux TipsAndroid TipsiPhone / iPad TipsMacOS TipsWeb TipsSoftware TipsOtherGadgetsCool Websites
54,"PostgreSQLPostgreSQL High CPU Usage: Causes, Consequences, and Solutions"
54,"soodMay 30, 2023"
54,Table of Contents
54,"Part I: Understanding PostgreSQL and CPU UsageIntroduction to PostgreSQL and High CPU UsageWhy High CPU Usage is a Concern in PostgreSQLCommon Causes of High CPU Usage in PostgreSQLPart II: Troubleshooting and Resolving High CPU Usage in PostgreSQLDetecting High CPU Usage in PostgreSQLResolving High CPU Usage: Strategies and ToolsPostgreSQL Configuration Best PracticesTips for Preventing High CPU UsageConclusionPart I: Understanding PostgreSQL and CPU UsageIntroduction to PostgreSQL and High CPU UsagePostgreSQL, an open-source relational database system, is known for its robustness and advanced features. Yet, just like any other technology, it can encounter issues such as high CPU usage. High CPU usage occurs when processes and applications use more CPU power than necessary. This can cause significant challenges for a database like PostgreSQL, especially if it’s not swiftly identified and managed.Why High CPU Usage is a Concern in PostgreSQLHigh CPU usage in PostgreSQL can have various negative impacts on system performance. Tasks may become slower, causing delays in data retrieval and modifications. As per PostgreSQL Performance Tips, efficiently managing CPU usage is essential to maintaining optimal database performance.Risks of system instability can also rise. A consistently overworked CPU can fail, causing system crashes and potential data loss. These technical issues might hinder the smooth operations of your applications, leading to a poor user experience.Furthermore, high CPU usage increases costs and inefficiency. Systems with consistently high CPU usage can use more power, leading to increased operational expenses. Also, hardware subjected to prolonged high CPU usage may have a reduced lifespan, requiring replacements or upgrades sooner than expected.Common Causes of High CPU Usage in PostgreSQLThe causes of high CPU usage in PostgreSQL can be various and complex, ranging from inefficient queries to configuration missteps:Inefficient Queries: Complex or poorly optimized queries can consume significant CPU resources. For instance, a query that leads to a full table scan instead of using an index can lead to high CPU usage.High Traffic Volume: High numbers of simultaneous connections can lead to increased CPU usage. Each connection requires certain CPU resources to manage, and more connections mean more CPU usage.Database Design Issues: Incorrectly structured databases can cause excessive CPU usage. Issues could include poorly distributed data, improper use of indexes, sub-optimal data types, or lack of partitioning.Configuration Missteps: PostgreSQL is highly configurable, but incorrect settings can lead to high CPU usage. This might include inappropriate memory settings or lack of connection limits.See also  Postgres Pgpool-II Ubuntu: Step-by-Step ConfigurationExample: Consider a table ‘users’ with a million rows. An inefficient query might look like this:SELECT * FROM users WHERE email LIKE '%@example.com';This query leads to a full table scan. A more optimized query, assuming there’s an index on the ’email’ field, could be:SELECT * FROM users WHERE email = 'user@example.com';Part II: Troubleshooting and Resolving High CPU Usage in PostgreSQLDetecting High CPU Usage in PostgreSQLHigh CPU usage can be detected by understanding PostgreSQL’s system statistics views and using tools such as ‘pg_stat_activity’ and ‘pg_stat_user_tables’. External monitoring tools can also be used to help identify high CPU usage issues.Example: To find currently running queries, use the pg_stat_activity view:SELECT * FROM pg_stat_activity;Resolving High CPU Usage: Strategies and ToolsThere are several strategies and tools available for resolving high CPU usage in PostgreSQL. These include:Query Optimization Techniques: Improving the efficiency of queries can significantly reduce CPU usage. This may include rewriting queries, using prepared statements, or using proper indexing.Database Tuning: PostgreSQL is highly configurable, and adjusting settings appropriately can reduce CPU usage.Using Indexes: Properly utilizing indexes can make data retrieval more efficient, reducing the need for expensive full table scans.Partitioning: Partitioning larger tables into smaller, more manageable pieces can improve query performance and reduce CPU usage.Hardware Upgrade: In some cases, upgrading hardware or adding additional resources may be the best way to handle high CPU usage.Example: Creating an index on the ’email’ field of the ‘users’ table can enhance query performance:CREATE INDEX idx_users_email ON users(email);PostgreSQL Configuration Best PracticesPostgreSQL offers a wealth of configuration parameters that can be adjusted to optimize CPU usage. However, it’s critical to understand these parameters, monitor their impact, and adjust as necessary.Example: Adjusting the ‘work_mem’ configuration parameter:SET work_mem TO '16MB';Tips for Preventing High CPU UsagePreventing high CPU usage in PostgreSQL largely depends on regular monitoring and optimization, correct database design principles, and regular updates and maintenance. Regular monitoring helps identify issues early, allowing for proactive optimization. Correct database design principles, such as appropriate use of indexes and partitioning, can prevent inefficient operations that lead to high CPU usage. Finally, regular updates and maintenance ensure the system runs on the latest and most optimized version of PostgreSQL.ConclusionManaging CPU usage in PostgreSQL is crucial for maintaining optimal performance and stability. By understanding the causes of high CPU usage, monitoring system statistics, and using proper optimization strategies, you can ensure a smoothly running PostgreSQL system.Support us & keep this site free of annoying ads. Shop Amazon.com or Donate with PaypalLeave a CommentCommentName"
54,Email
54,"Website Save my name, email, and website in this browser for the next time I comment."
54,ΔSupport us & keep this site free of annoying ads. Shop Amazon.com or Donate with PaypalcPanel / WHM
54,Django Tips
54,Facebook Tips
54,Google Tips
54,iPhone Tips
54,Laravel Tips
54,Samsung Tips
54,Software Testing (QA)
54,Video Games
54,"VirtualizationFind us on socialFacebookTwitterAmazonRSS FeedAbout UsWelcome to HeatWare.net - A technology blog for geeks and non-geeks alike! We programming tips, code examples, Cloud technology help, Windows & application troubleshooting, and more! The owner of this blog also run a feedback and rating database for online transactions."
54,© 2024 HeatWare.net
54,Search for:
55,How to fix transaction wraparound in PostgreSQL?
55,Fujitsu Enterprise Postgres
55,Enterprise data security
55,Key features
55,Product overview
55,Product features
55,Compare versions
55,Product specifications
55,What is PostgreSQL?
55,5 key enterprise capabilities
55,Fujitsu Enterprise Postgres on IBM Power®
55,Fujitsu Enterprise Postgres on IBM LinuxONE
55,Fujitsu Enterprise Postgres for Kubernetes
55,Fujitsu Enterprise Postgres documentation
55,Frequently asked questions
55,Services
55,Support
55,Support overview
55,Support resources
55,Training
55,Resources
55,Resource Center
55,Resource Center main page
55,• White papers
55,• Brochures
55,• Videos
55,• Tip sheets
55,• eBooks
55,• Fujitsu Enterprise Postgres Operator FAQ
55,Knowledge Portal
55,Knowledge Portal main page
55,• Documentation
55,• PostgreSQL Insider
55,• FAQ
55,• Blog
55,• Knowledge articles
55,• Application development
55,• Backup/Recovery
55,• Client tools
55,• Configuration
55,• High availability
55,• Installation/Setup
55,• Operation/Maintenance
55,• Performance/Tuning
55,• Product structure
55,• Security
55,• Quick start guides
55,• IBM Redbooks
55,• Search
55,PostgreSQL Insider
55,Fujitsu Enterprise Postgres manuals
55,Fujitsu Enterprise Postgres/PostgreSQL
55,• Fujitsu Enterprise Postgres FAQ
55,• Fujitsu Enterprise Postgres trial version
55,• PostgreSQL documentation
55,PartnerWeb (Fujitsu internal)
55,News
55,Blog
55,Migration Portal
55,Trial
55,Get Fujitsu Enterprise Postgres trial
55,• Fujitsu Enterprise Postgres
55,• Fujitsu Enterprise Postgres for Kubernetes
55,• Fujitsu Enterprise Postgres on IBM LinuxONE™
55,• Fujitsu Enterprise Postgres on IBM Power®
55,Contact
55,Fujitsu Enterprise Postgres
55,Enterprise data security
55,Key features
55,Product overview
55,Product features
55,Compare versions
55,Product specifications
55,What is PostgreSQL?
55,5 key enterprise capabilities
55,Fujitsu Enterprise Postgres on IBM Power®
55,Fujitsu Enterprise Postgres on IBM LinuxONE
55,Fujitsu Enterprise Postgres for Kubernetes
55,Fujitsu Enterprise Postgres documentation
55,Frequently asked questions
55,Services
55,Support
55,Support overview
55,Support resources
55,Training
55,Resources
55,Resource Center
55,Resource Center main page
55,• White papers
55,"• Brochures, Flyers & Datasheets"
55,• Videos
55,• Tip sheets
55,• eBooks
55,• Fujitsu Enterprise Postgres Operator FAQ
55,Knowledge Portal
55,Knowledge Portal main page
55,• Documentation
55,• PostgreSQL Insider
55,• FAQ
55,• Blog
55,• Knowledge articles
55,• Application development
55,• Backup/Recovery
55,• Client tools
55,• Configuration
55,• High availability
55,• Installation/Setup
55,• Operation/Maintenance
55,• Performance/Tuning
55,• Product structure
55,• Security
55,• Quick start guides
55,• IBM Redbooks
55,• Search
55,PostgreSQL Insider
55,Fujitsu Enterprise Postgres manuals
55,Fujitsu Enterprise Postgres/PostgreSQL
55,• Fujitsu Enterprise Postgres FAQ
55,• Fujitsu Enterprise Postgres trial version
55,• PostgreSQL documentation
55,PartnerWeb (Fujitsu internal)
55,News
55,Blog
55,Migration Portal
55,Trial
55,Get Fujitsu Enterprise Postgres trial
55,• Fujitsu Enterprise Postgres
55,• Fujitsu Enterprise Postgres for Kubernetes
55,• Fujitsu Enterprise Postgres on IBM LinuxONE™
55,• Fujitsu Enterprise Postgres on IBM Power®
55,Contact
55,Fujitsu PostgreSQL blog
55,< Back to blog home
55,Fujitsu PostgreSQL blog
55,How to fix transaction wraparound in PostgreSQL?
55,Nishchay Kothari |
55,"July 13, 2023"
55,"In this blog, Nishchay Kothari from Fujitsu's Center of Excellence team discusses what steps to take in case your system becomes read-only due to transaction wraparound."
55,"As a PostgreSQL DBA, we commonly hear and use the term ""dead rows"" and ""bloat."" In this blog post, I'd like to talk about these concepts as well as ""transaction wraparound,"" one of the most dangerous situations in PostgreSQL. I'll also outline how to handle this problem with an example."
55,"Enterprise-grade database systems with high traffic are bound to the risk of transaction wraparound. I will walk you through the steps to rectify, to help you manage PostgreSQL with confidence."
55,Contents
55,> Dead rows and bloat
55,> Types of vacuuming
55,> Transaction wraparound
55,> How to fix transaction wraparound
55,> How to avoid transaction wraparound
55,> Conclusion
55,Dead rows and bloat
55,"The old version of any row that is affected by an operation like UPDATE or DELETE in PostgreSQL is marked internally so that queries don't return that row. Therefore, these rows are referred to as ""dead rows"", and the space occupied by dead rows is called ""bloat."" A database program that often executes a lot of UPDATE/DELETE operations can quickly become very large and require sporadic maintenance work."
55,Types of vacuuming
55,"Typically, vacuuming, a type of periodic database cleaning, is carried out in PostgreSQL. It comes in two different forms; one of them is plain VACUUM (without FULL), which merely frees up space for other uses. Plain vacuum does not incur exclusive locks, allowing it to run concurrently with regular reading and writing of the table. This type of vacuum operation will keep the additional space accessible for reuse inside the same table, rather than returning it to the operating system. The operating system receives the excess space back when using the VACUUM FULL form, which also rewrites the whole contents of the table onto a new disk file with no extra space. This form must be processed with an ACCESS EXCLUSIVE lock on each table, which makes it slower."
55,How it works
55,Vacuum
55,Vacuum full
55,Method
55,Free up dead rows for reuse
55,Rewrite the table with no dead rows
55,Access Exclusive Lock
55,Yes
55,Free space is made available for
55,Inside the same table
55,The Operating System
55,Transaction wraparound
55,"Now, let’s learn about the transaction and transaction wraparound. Every row that is updated in the database in PostgreSQL receives a transaction ID (Txid) from the transaction control mechanism. These IDs regulate which rows are shown to other active transactions."
55,"The issue with transaction wraparound is Multi-Version Concurrency Control (MVCC). The ability to distinguish between two transactions based on their Txids is essential for MVCC. Txids in Postgres are just 32-bit integers. Accordingly, there are only roughly four billion (232) possible Txids."
55,"Four billion may seem excessive and difficult to achieve, but I must admit that for database systems with really high write-intensive workloads, four billion transactions are achievable in a matter of weeks."
55,"Therefore, PostgreSQL stops allowing WRITE operations and switches the database to READONLY mode if there are 2,000,000,000 unvacuumed transactions."
55,Any one or more of the following conditions could contribute to transaction ID wraparound.
55,Autovacuum is set to turned off
55,Long-running transactions
55,Heavy DML operations forcing the cancellation of autovacuum worker processes
55,Many sessions or connections holding locks for very long durations.
55,"I was able to successfully simulate a transaction wraparound issue on my test machine. I'm getting the following entries in my PostgreSQL log files, which means that PostgreSQL has stopped accepting DML statements and has switched to READONLY mode."
55,"2022-09-16 08:43:38.265 +08 [74098] WARNING: database ""postgres"" must be vacuumed within 3000000 transactions (10184)2022-09-16 08:43:38.265 +08 HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 08:43:48.640 +08 [5764] ERROR: database is not accepting commands to avoid wraparound data loss in database ""postgres"" (10182)2022-09-16 08:43:48.640 +08 [5764] HINT: Stop the postmaster and vacuum that database in single-user mode.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"Therefore, I have established a connection to the issue database ""postgres"" and run some SELECT & CREATE commands to make sure."
55,"[fsepuser@primary log]$ psqlpsql (14.0)Type ""help"" for help.postgres=# select datname from pg_database;  datname----------postgrestemplateltemplate0(3 rows)postgres=# create table test (id numeric);ERROR: database is not accepting commands to avoid wraparound data loss in database ""postgres"" (10182)HINT: Stop the postmaster and vacuum that database in single-user mode.You might also need to commit or roll back old prepared transactions, or drop stale replication slots.postgres=#postgres=# select usename from pg_user;usename----------fsepuser(1 row)postgres=#"
55,"The database ""Postgres"" is currently confirmed to be in READONLY mode because of transaction wraparound."
55,How to fix transaction wraparound
55,"Let me now walk you through the methods to remedy this scenario. We must first take the database offline, connect in single-user mode, and then execute VACUUM FULL on each database. However, we must first complete the steps listed below."
55,"First of all, we need to find the name of table which is responsible for wraparound by using the query below."
55,"SELECT c.relnamespace::regnamespace as schema_name, c.relname as table_name,greatest(age(c.relfrozenxid),age(t.relfrozenxid)) as age,2^31-1000000-greatest(age(c.relfrozenxid),age(t.relfrozenxid)) as remainingFROM pg_class c LEFT JOIN pg_class t ON c.reltoastrelid = t.oidWHERE c.relkind IN ('r', 'm') ORDER BY 4;"
55,"Dropping the table is the only option to fix the issue if you discover any tables in the output of the above query that are part of the 'pg_temp_' schema, as PostgreSQL does not allow you to VACUUM temporary tables generated in other sessions."
55,"Another scenario is that PostgreSQL is unable to freeze any transaction IDs that were established after the oldest presently active transaction began. This is due to the way the MVCC operates. Transactions may occasionally get so old that VACUUM is unable to clean them up for the entire 2 billion transaction ID wraparound limit, which results in the system no longer accepting new DML."
55,"2022-09-16 05:49:07.514 +08 [64931] WARNING: oldest xmin is far in the past (11833)2022-09-16 05:49:07.514 +08 [64931] HINT: Close open transactions soon to avoid wraparound problems.You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"You have probably noticed that all the screenshots' HINTs tell you to COMMIT or ROLLBACK previously created prepared transactions. Therefore, using the query below, we must check for orphan or outdated prepared transactions."
55,"SELECT age(transaction),* FROM pg_prepared_xacts ;"
55,"Using the gid from the above query, rollback any prepared transactions you find. The rollback query is below."
55,ROLLBACK PREPARED gid;
55,"Once the above processes have been completed, we must bring the databases offline, connect in single-user mode, and conduct the VACUUM FULL option on each database."
55,[fsepuser@primary log]$ pg_ctl -D /database/instl stop -mfwaiting for server to shut down...... doneserver stopped[fsepuser@primary log]$
55,"Now, you need to start the database in single-user mode as below."
55,"[fsepuser@primary ~]$ /opt/fsepv14server64/bin/postgres --single -D /database/instl postgres2022-09-16 12:06:45.628 +08 [84585] WARNING: database with OID 14728 must be vacuumed within 3000000 transactions (10185)2022-09-16 12:06:45.628 +08 [84585] HINT: to avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.PostgreSQL stand-alone backend 14.0backend>"
55,"After successfully entering into single-user mode, we need to execute VACUUM FULL for each database."
55,"backend> Vacuum Full;2022-09-16 12:09:02.711 +08 [84585] WARNING: database ""postgres"" must be vacuumed within 3000000 transactions (10184)2022-09-16 12:09:02.711 +08 [84585] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:09:03.860 +08 [84585] WARNING: database ""postgres"" must be vacuumed within 2999999 transactions (10184)2022-09-16 12:09:03.860 +08 [84585] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"2022-09-16 12:09:08.052 +08 [84585] WARNING: database ""postgres"" must be vacuumed within 3000000 transactions (10184)2022-09-16 12:09:08.052 +08 [84585] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:09:08.058 +08 [84585] WARNING: database ""postgres"" must be vacuumed within 3000000 transactions (10184)2022-09-16 12:09:08.058 +08 [84585] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:09:08.065 +08 [84585] WARNING: database ""template1"" must be vacuumed within 3000000 transactions (10184)2022-09-16 12:09:08.065 +08 [84585] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"Now, if you can see that PostgreSQL asking to do the same VACUUM FULL for the template1 database as well. Let’s do the same for template1."
55,"[fsepuser@primary ~]$ /opt/fsepv14server64/bin/postgres --single -D /database/instl template12022-09-16 12:14:14.897 +08 [84775] WARNING: database with OID 1 must be vacuumed within 2999860 transactions (10185)2022-09-16 12:14:14.897 +08 [84775] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.PostgreSQL stand-alone backend 14.0backend> vacuum full; 2022-09-16 12:14:24.965 +08 [84775] WARNING: database ""template1"" must be vacuumed within 2999860 transactions (10184)2022-09-16 12:14:24.965 +08 [84775] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:14:25.026 +08 [84775] WARNING: database ""template1"" must be vacuumed within 2999859 transactions (10184)2022-09-16 12:14:25.026 +08 [84775] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,Now it’s asking to execute same operation on template0.
55,"2022-09-16 12:14:25.696 +08 [84775] WARNING: database ""template1"" must be vacuumed within 2999795 transactions (10184)2022-09-16 12:14:25.696 +08 [84775] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:14:25.703 +08 [84775] WARNING: database ""template0"" must be vacuumed within 2999794 transactions (10184)2022-09-16 12:14:25.703 +08 [84775] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,Let’s do the same for template0.
55,"[fsepuser@primary ~]$ /opt/fsepv14server64/bin/postgres --single -D /database/instl template02022-09-16 12:16:18.247 +08 [84863] WARNING: database with OID 14727 must be vacuumed within 2999794 transactions (10185)2022-09-16 12:16:18.247 +08 [84863] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.PostgreSQL stand-alone backend 14.0backend> Vacuum Full; 2022-09-16 12:16:36.346 +08 [84863] WARNING: database ""template0"" must be vacuumed within 2999794 transactions (10184)2022-09-16 12:16:36.346 +08 [84863] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"2022-09-16 12:16:37.036 +08 [84863] WARNING: database ""template0"" must be vacuumed within 2999730 transactions (10184)2022-09-16 12:16:37.036 +08 [84863] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots.2022-09-16 12:16:37.042 +08 [84863] WARNING: database ""template0"" must be vacuumed within 2999729 transactions (10184)2022-09-16 12:16:37.042 +08 [84863] HINT: To avoid a database shutdown, execute a database-wide VACUUM in that database.        You might also need to commit or roll back old prepared transactions, or drop stale replication slots."
55,"If you noticed, PostgreSQL is no longer requesting to perform VACUUM FULL on any additional databases because I only have three databases (postgres, template1, and template0), and I have successfully finished VACUUM FULL on each of them.After that we can start the PostgreSQL as below."
55,"[fsepuser@primary ~]$ pg_ctl -D /database/instl startwaiting for server to start....2022-09-16 04:19:11.580 GMT [84987] WARNING: The license will expire in 66 days.2022-09-16 12:19:11.594 +08 [84987] LOG: redirecting log output to logging collector process2022-09-16 12:19:11.594 +08 [84987] HINT: Allure log output will appear in directory 'log'.doneserver started[fsepuser@primary ~]$ psqlpsql (14.0)Type ""help"" for help. postgres=#"
55,"Now, let’s confirm if the problem is fixed or not by executing DML statements."
55,"[fsepuser@primary ~]$ psqlpsql (14.0)Type ""help"" for help.postgres=# select usename from pg_user; useename---------- fsepuser(1 row)postgres=# create table test (id numeric);CREATE TABLEpostgres=#postgres=# insert into test values (1);INSERT 0 1postgres=# select * from test ;id ----   1 (1 row) postgres=#"
55,As you can see that the issue is fixed now and PostgreSQL able to perform DML operations effectively like before the issue.
55,How to avoid transaction wraparound
55,"In PostgreSQL, there are various important methods to use in order to prevent transaction wraparound. First and foremost, you must keep track of the age of your database's oldest transaction ID (XID). This is possible using the pg_class catalog table. You can determine if the XID is approaching the wraparound limit by keeping track of its age."
55,"Regular maintenance procedures, such as vacuuming and database analyzing, should be carried out in order to prevent transaction wraparound. Vacuuming helps to recover space that had been occupied by obsolete or deleted tuples, minimizing bloat and preventing transaction IDs from reaching the wraparound threshold. Analyzing updates the optimizer statistics, ensuring that query plans are correct."
55,"Additionally, consider modifying the autovacuum parameters to some optimal settings in your PostgreSQL configuration. The autovacuum processes are controlled by those settings and helping in controlling transaction IDs."
55,Another preventive method is to configure a monitoring system to deliver notifications when the XID age reaches a certain limit. This allows you to respond quickly and prevents you from approaching the wraparound limit.
55,Conclusion
55,"Although it is ideal to prevent transaction wraparound from occurring by setting up regular maintenance and analysis of your system, unexpected events or new requirements may arise as your business grow and evolve. With the example above, I illustrated how you can clean up your PostgreSQL system in case transaction wraparound occurs. I hope now you will not be afraid of transaction wraparound, knowing that you are able to fix it effectively."
55,Fujitsu Enterprise Postgres leverages and extends the strength and reliability of PostgreSQL with additional enterprise features.
55,Compare the list of features.
55,Compare >
55,"We also have a series of technical articles for PostgreSQL enthusiasts of all stripes, with tips and how-to's."
55,Subscribe to be notified of future blog posts
55,"If you would like to be notified of my next blog posts and other PostgreSQL-related articles, fill the form here."
55,Topics:
55,"PostgreSQL,"
55,"PostgreSQL support,"
55,Fujitsu Enterprise Postgres
55,Receive our blog
55,Fill the form to receive notifications of future posts
55,Search by topic
55,PostgreSQL (81)
55,Fujitsu Enterprise Postgres (56)
55,PostgreSQL community (52)
55,Database security (23)
55,PostgreSQL development (23)
55,PostgreSQL event (16)
55,Data governance (15)
55,Security (15)
55,Open source (14)
55,Logical replication (13)
55,Data Masking (11)
55,"""Database security"" blog series (10)"
55,Announcement (10)
55,Enhanced enterprise open source database (8)
55,PostgreSQL performance (7)
55,PostgreSQL support (7)
55,Transparent Data Encryption (7)
55,PGCon (6)
55,Digital Transformation (5)
55,Community support (4)
55,Customer data (4)
55,Data Redaction (4)
55,Database Migration (4)
55,Dedicated Audit Log (4)
55,EU General Data Protection Regulation (GDPR) (4)
55,Event (4)
55,PGDay Australia (4)
55,Business 4.0 (3)
55,CIS Benchmark (3)
55,Database performance (3)
55,Encryption (3)
55,Financial services (3)
55,High Availability (3)
55,High-Speed Data Load (3)
55,How-to (3)
55,NIST Cybersecurity Framework (3)
55,PGConf (3)
55,Sustainability (3)
55,Vertical Clustered Index (3)
55,pgDay Asia (3)
55,Australian Privacy Amendment (NDB) Act 2017 (2)
55,Backup (2)
55,Cloud journey (2)
55,Data management (2)
55,Data monetization (2)
55,DevOps (2)
55,DevOps approach (2)
55,Hybrid Transactional Analytical Processing (HTAP) (2)
55,Industry 4.0 (2)
55,Legacy applications (2)
55,Open Banking (2)
55,PGDay Down Under (2)
55,PostGIS (2)
55,Regulatory compliance (2)
55,TCO (2)
55,Agribusiness (1)
55,Big Data (1)
55,Business continuity (1)
55,CIO (1)
55,CTO (1)
55,Centralizing authorization (1)
55,Convergence (1)
55,Data portability (1)
55,Demo (1)
55,Elasticsearch (1)
55,FIPS (1)
55,Federal Information Processing Standards (1)
55,Fujitsu World Tour (1)
55,IBM LinuxONE (1)
55,Index (1)
55,Internet of Things (1)
55,K8s (1)
55,Linux (1)
55,Memory components (1)
55,Micro-services (1)
55,Mirroring Controller (1)
55,NoSQL (1)
55,Singapore PDPC (1)
55,Society 5.0 (1)
55,Spectre and Meltdown (1)
55,Strategy (1)
55,see all >
55,Read our latest blogs
55,Read our most recent articles regarding all aspects of PostgreSQL and Fujitsu Enterprise Postgres.
55,Receive our blog
55,Fill the form to receive notifications of future posts
55,Search by topic
55,PostgreSQL (81)
55,Fujitsu Enterprise Postgres (56)
55,PostgreSQL community (52)
55,Database security (23)
55,PostgreSQL development (23)
55,PostgreSQL event (16)
55,Data governance (15)
55,Security (15)
55,Open source (14)
55,Logical replication (13)
55,Data Masking (11)
55,"""Database security"" blog series (10)"
55,Announcement (10)
55,Enhanced enterprise open source database (8)
55,PostgreSQL performance (7)
55,PostgreSQL support (7)
55,Transparent Data Encryption (7)
55,PGCon (6)
55,Digital Transformation (5)
55,Community support (4)
55,Customer data (4)
55,Data Redaction (4)
55,Database Migration (4)
55,Dedicated Audit Log (4)
55,EU General Data Protection Regulation (GDPR) (4)
55,Event (4)
55,PGDay Australia (4)
55,Business 4.0 (3)
55,CIS Benchmark (3)
55,Database performance (3)
55,Encryption (3)
55,Financial services (3)
55,High Availability (3)
55,High-Speed Data Load (3)
55,How-to (3)
55,NIST Cybersecurity Framework (3)
55,PGConf (3)
55,Sustainability (3)
55,Vertical Clustered Index (3)
55,pgDay Asia (3)
55,Australian Privacy Amendment (NDB) Act 2017 (2)
55,Backup (2)
55,Cloud journey (2)
55,Data management (2)
55,Data monetization (2)
55,DevOps (2)
55,DevOps approach (2)
55,Hybrid Transactional Analytical Processing (HTAP) (2)
55,Industry 4.0 (2)
55,Legacy applications (2)
55,Open Banking (2)
55,PGDay Down Under (2)
55,PostGIS (2)
55,Regulatory compliance (2)
55,TCO (2)
55,Agribusiness (1)
55,Big Data (1)
55,Business continuity (1)
55,CIO (1)
55,CTO (1)
55,Centralizing authorization (1)
55,Convergence (1)
55,Data portability (1)
55,Demo (1)
55,Elasticsearch (1)
55,FIPS (1)
55,Federal Information Processing Standards (1)
55,Fujitsu World Tour (1)
55,IBM LinuxONE (1)
55,Index (1)
55,Internet of Things (1)
55,K8s (1)
55,Linux (1)
55,Memory components (1)
55,Micro-services (1)
55,Mirroring Controller (1)
55,NoSQL (1)
55,Singapore PDPC (1)
55,Society 5.0 (1)
55,Spectre and Meltdown (1)
55,Strategy (1)
55,see all >
55,Terms of use
55,Privacy policy
55,About
55,White papers
55,Brochures
55,FAQ
55,email: enterprisepostgresql@fujitsu.com
55,All Rights Reserved. Copyright 2023 Fujitsu Australia Ltd
55,Privacy Statement
57,Introduction to PostgreSQL Performance Tuning and Optimization
57,Skip to content
57,Philip McClarence
57,Menu
57,Menu
57,PostgreSQL
57,Aurora
57,Errors
57,How To – Postgres
57,MongoDB
57,MySQL
57,Cassandra
57,Hadoop
57,Oracle
57,Oracle Basics
57,Oracle News
57,Errors
57,Oracle Tasks
57,Oracle RAC
57,Oracle Tuning
57,Introduction to PostgreSQL Performance Tuning and Optimization
57,19/02/2024 by Philip
57,"Optimising the performance of a PostgreSQL database is a crucial task for any database administrator. The complexities in fine-tuning a database can be vast, but a few adjustments can lead to significant improvements. Configuring your system to handle large pages, for example, can greatly enhance memory management efficiency. This involves a process that requires changing system settings and restarting your server, ensuring that changes are effectively applied."
57,"In the realm of database optimisation, it’s important to understand the variety of parameters that contribute to the overall performance. From initial setup to query tuning and regular maintenance tasks like autovacuum, all play a vital role. Taking a strategic approach to tune these parameters can help in reducing latency, speeding up query times, and ultimately providing a more stable and faster database environment."
57,Key Takeaways
57,Changing system settings can improve PostgreSQL memory management efficiency.
57,Various parameters from setup to maintenance impact database performance.
57,Strategic tuning of a PostgreSQL server can enhance query speed and stability.
57,Enhancing File System Efficiency
57,"To improve your disk’s performance in a Linux environment, consider altering the way your system handles file access timestamps. By default, every read operation updates the file’s last-accessed record, consuming unnecessary CPU resources. You can optimise this aspect by tweaking your file system’s mount options."
57,Adjust the /etc/fstab file to include the noatime option alongside the default settings for the partitions that house your PostgreSQL data and Write-Ahead Logging (WAL) files. An example mount entry is shown below:
57,"/dev/mapper/pgdata-01-data /pgdata xfs defaults,noatime 1 1"
57,"To implement these changes without restarting, execute:"
57,"mount -o remount,noatime /pgdata"
57,"This adjustment is particularly beneficial for spinning disks, as it reduces the disk’s workload for read and write operations. Remember, this is the starting point of optimising your system; ongoing monitoring of both the operating system and PostgreSQL performance is imperative to identify opportunities for further enhancements. Frequent hardware reviews, including network cards and disk spaces, may also lead to considerable performance uplifts."
57,Large Memory Pages in PostgreSQL
57,"When using PostgreSQL on a Linux system, performance can be improved by utilising large memory pages, commonly referred to as ‘huge pages’. Linux generally uses a default page size of 4kB, which when using memory-intensive applications like PostgreSQL could result in inefficiencies due to fragmentation. To mitigate this, huge pages are used to allocate memory in larger blocks, which can significantly boost the application’s performance."
57,"To begin configuring huge pages, you will need to determine the exact number required for your PostgreSQL instances. This can be done by checking the ‘ postmaster.pid’ file for the main process ID, then retrieving the ‘VmPeak’ value which indicates the peak virtual memory size."
57,"For a quick check of the process ID, use the command:"
57,head -n 1 $PGDATA/postmaster.pid
57,"To locate the ‘VmPeak’, run:"
57,grep -i vmpeak /proc/{process_id}/status
57,"Should you operate multiple PostgreSQL instances, sum up the VmPeak values. Next, verify the huge page size on your system:"
57,grep -i hugepagesize /proc/meminfo
57,"Divide the VmPeak sum by the huge page size to find out the number of huge pages needed. It’s prudent to allocate a slightly larger number than the calculated result to ensure adequate memory, but be cautious not to overestimate excessively as this can cause system boot issues or affect other processes that require standard page sizes."
57,Proceed to adjust the ‘tuned.conf’ by adding your calculated huge page number to the [sysctl] section. For instance:
57,vm.nr_hugepages=4500
57,Apply the new settings with:
57,tuned-adm profile edbpostgres
57,Now set ‘huge_pages’ to ‘on’ within the ‘postgresql.conf’ file and restart PostgreSQL to activate the changes.
57,"Furthermore, modify the PostgreSQL service unit file to establish that the tuned service starts before PostgreSQL after a system reboot. Edit with:"
57,systemctl edit postgresql-13.service
57,Then include:
57,[Unit]
57,After=tuned.service
57,"After saving the changes, reload the daemon to ensure they are adopted:"
57,systemctl daemon-reload
57,"By following these steps, you should have your PostgreSQL instance configured to use huge pages, potentially enhancing the performance of your database server."
57,Initial Steps in Improving PostgreSQL Efficiency
57,Tweaking Settings & Access Controls
57,"In tailoring your PostgreSQL to achieve better operational results, precise adjustment of settings and access control mechanisms is crucial. This involves modifying specific configurations in the postgresql.conf file which serves as the database’s main configuration repository."
57,Optimal Connection Numbers
57,"Determining the appropriate max_connections setting is pivotal for balancing resource availability and system workload. Aim for the highest of either 100 or a count quadruple that of your CPU cores. Exceeding this could lead to unnecessary strain on the system, while suboptimal figures might not adequately sustain your needs. For more extensive demands, consider implementing a connection pooler, such as pgbouncer, to maintain efficient resource management."
57,"Remember, the max_connections parameter’s influence is quite significant as it directly affects PostgreSQL’s internal data structure sizes, which could lead to CPU inefficiency if not configured properly. Adjust carefully to suit your performance requirements."
57,Memory Allocation and Management
57,Buffer Distribution
57,"When considering buffer allocation, various tasks may demand differing sizes. An initial guide may be to allocate the lesser of half your system’s Random Access Memory (RAM) or 10GB. Although there is no rigid science behind this guidance, it stems from the PostgreSQL community’s accumulated experience. Kernel cache and buffer interactions present complexities not easily outlined, so this approach tends to yield acceptable outcomes."
57,Working Memory
57,"A suggested baseline calculation for your working memory setup is to subtract your buffer allocation from your total RAM, then divide this figure by sixteen times the quantity of CPU cores your system has. Should multiple queries saturate your memory, CPU capacity would frequently be your limiting factor. Within each query plan, every operation node could potentially employ memory volumes designated here, making prudence advisable to avoid excessive allocation."
57,Maintenance Memory
57,"Your system’s maximum memory usage for database maintenance — inclusive of tasks such as vacuuming and index creation — often determines the speediness of these operations. Commencing with 1GB can be judicious, facilitating these tasks to complete more swiftly and potentially enhancing I/O activity during their execution."
57,Vacuum Working Memory
57,"Adjusting your maintenance memory higher empowers each autovacuum worker to utilise equivalent memory. For each unneeded tuple the vacuum clears, it uses 6 bytes. Hence, an assignment of 8MB can manage approximately 1.4 million such tuples, influencing the efficiency of cleanup processes."
57,I/O Request Management
57,"Adapting the effective_io_concurrency value is meant to align with your storage’s disk number, enhancing read-ahead capabilities during database operations. With traditional spinning disks arranged in a striped RAID, this setting assists PostgreSQL in parallel read predictions. However, solid-state drives (SSDs) exhibit different properties, advocating for a 200 setting to capitalise on their distinct performance characteristics."
57,Write-Ahead Log
57,WAL Data Compression
57,"Enabling data compression for Write-Ahead Logging (WAL) can significantly improve the efficiency of your database. Switch it on to allow the PostgreSQL server to condense full-page images in the WAL, when full-page write operations are activated or during a base backup."
57,Hints in WAL Files
57,"For facilitating the use of tools like pg_rewind, you should ensure the hints logging feature within WAL is switched on. This aids in the seamless tracking of necessary changes."
57,Memory Allocation for WAL
57,"Allocate sufficient space for the WAL data to reside in memory, which allows for background writing to disk. A setting of 64MB can improve performance as it offers substantial space to buffer the WAL segments, each typically sized at 16MB."
57,Timing of Checkpoints
57,"Strike a balance with your checkpoint intervals as they affect WAL volume and recovery time. The recommendation is a minimum duration of 15 minutes, subject to your recovery point objectives (RPO)."
57,Checkpoint Completion
57,You’re advised to aim for nearly complete checkpoints within 90% of the designated checkpoint interval. This strategy smoothens the input/output load by dispersing checkpoint-related write operations.
57,Upper Limit on WAL Size
57,"To ensure efficient checkpointing initiated by timeouts, set the max_wal_size cautiously. It’s advised to use between one-half and two-thirds of the total disk space allocated for WAL to prevent out-of-space issues."
57,Archive Mode
57,"Keep the archive mode activated if there is any possibility that you might require WAL archiving in the future, as altering this setting will necessitate a system restart."
57,Archiving WAL Files
57,"A placeholder archive command, ‘: to be configured’, is recommended until actual archiving is set up. This returns a success message, allowing PostgreSQL to understand that the WAL segment can be recycled or deleted."
57,"By adhering to these suggestions and adjustments, you can enhance the reliability and performance of your PostgreSQL database’s WAL operations."
57,Query Tuning
57,Disc Access Cost
57,"To optimise your queries, adjusting the random_page_cost parameter is crucial. This setting informs the PostgreSQL query planner about the expense of retrieving a random disk page. If your system is equipped with SSDs, consider setting this to 1.1 to promote the use of index scans. With standard spinning disks, the default setting generally suffices. Adjust this value not just globally but for individual tablespaces based on their storage mediums."
57,Cache Size Estimation
57,"The effective_cache_size parameter helps with the estimation of memory available for caching data. It’s sensible to set this to either three-quarters of your total RAM or to the aggregated value of buffer/cache, available memory, and shared buffers—whichever is smaller. For instance, given a system with 64GB RAM and 8GB allocated for shared buffers, the effective_cache_size would be the minimum of 64GB * 0.75 and the sum 58GB + 5.8GB + 8GB (assuming these are the free command outputs), resulting in 48.2GB."
57,Row Processing Cost
57,"Last but not least, review the cpu_tuple_cost. This is the projected cost for processing each individual row. The default might be set too low at 0.01, and it’s often beneficial to increase this to 0.03 to reflect a more accurate cost, thereby improving the query planner’s decisions and overall query performance."
57,Reporting and Logging
57,Gathering Logs
57,"To effectively capture log output, particularly when stderr or csvlog are included as destinations, enable the log collection feature."
57,Log File Location
57,"Once you enable the log collector, specify a directory location separate from the data path. Logs maintained outside the data directory will not be included in foundational backups, keeping them isolated and manageable."
57,Checkpoint Logging
57,Activate checkpoint logging to ensure that checkpoints occur as scheduled by the checkpoint_timeout setting rather than being driven by the max_wal_size threshold. This information is crucial for future diagnostics.
57,Log Line Identifier
57,"The log line prefix must be structured to assist in quick diagnosis. It should include at a minimum the timestamp, process identifier, line number, user, database, and application name. For example, ‘%m [%p-%l] %u@%d app=%a ‘—make note of the space at the end."
57,Lock Wait Logging
57,"For identifying slow queries due to lock waits, turn this setting on."
57,Statement Logging
57,"Record data definition language (DDL) statements to generate a basic audit trail and assist in pinpointing the exact moment of critical errors, like an unintended table deletion."
57,Temporary File Recording
57,"Log all temporary files by setting this to zero, which can indicate whether your work_mem is configured properly."
57,Collecting Timed Statistics (EPAS)
57,"For the Dynamic Runtime Instrumentation Tools Architecture (DRITA) functionality, the timing data feature should be enabled. Activate it to collect timing information, bringing insight into your query execution."
57,Autovacuum Settings
57,Monitoring Vacuum Activity
57,"To optimise the autovacuum function, it’s integral to keep track of its operations. By setting the monitoring value to zero, every instance of the autovacuum process will be logged, allowing for a detailed view of its performance."
57,Worker Limit for Autovacuum
57,"Autovacuum is supported by a set number of workers, with a default quantity of three. Only a single worker can operate on a table at any given time, so having more workers enhances the ability for simultaneous and regular maintenance across diverse tables. To improve efficiency, consider setting this limit to five workers. Remember, modifying the number of workers necessitates a restart of the database server."
57,I/O Quota for Autovacuum
57,"There’s a cap on the I/O resources the autovacuum can use, preventing it from monopolising the server’s bandwidth. The vacuum’s I/O consumption is metered by a quota, which, once reached, causes the autovacuum to pause. The starting quota is modest, so to maintain a steady cleaning pace, a rise to a quota limit of 3000 might be beneficial. This adjustment allows the vacuum to proceed with more intensive operations without frequent interruptions."
57,Client Connection Defaults
57,Timeout for Idle Transactions
57,"If your transactions remain idle for an extended period, they might block other operations. To avoid this, configure your system to terminate these sessions after a certain duration, such as 10 minutes. This ensures a stalled transaction does not indefinitely hold resources. However, your application should be capable of handling such terminations gracefully."
57,Localisation Settings for Messages
57,"For effective log analysis, it’s best to set your logging messages to a non-localised format (‘C’). Doing so ensures compatibility with log analysers that might not interpret translated messages correctly."
57,Essential Libraries to Preload
57,"Consider preloading libraries like pg_stat_statements as it provides significant insights with minimal performance overhead. Although it’s not compulsory, integrating this can optimise tracking and analysing database performance metrics."
57,Remember to adjust your listen_addresses or incorporate a connection pooler like PgBouncer to manage your database connections efficiently and ensure that your superuser settings are secure.
57,Identifying Inefficient SQL Queries
57,"To uncover inefficient SQL commands that may be hindering your database performance, two primary strategies can help:"
57,Using log_min_duration_statement
57,"This parameter acts like a stopwatch, recording any SQL query that surpasses a specific duration threshold, allowing you to single out longer-running queries. For example, even setting this to one millisecond (1ms) will catch every query, but be aware this might overwhelm your system with excessive logging details."
57,"A common practice is to begin with a boundary of one second (1s), and step by step reduce this value as you refine your queries. Persist with this iterative process until you arrive at a time limit that balances detailed scrutiny with manageable logging."
57,"Do keep in mind: Not all frequently executed, shorter-duration queries will get caught in this net, yet their cumulative effect can be more taxing on the system than less frequent, longer-running ones."
57,Implementing the pg_stat_statements Module
57,"For a comprehensive overview, including those elusive, often-run yet brief queries, enabling the pg_stat_statements module is advised. Essential to note, this requires adding it to shared_preload_libraries and restarting the server, but its marginal footprint and significant insight typically justify the initial setup."
57,"What the pg_stat_statements delivers is a digest of all queries processed by the server: it normalizes, combines similar patterns, and collates them into one record brimming with metrics like execution count, total time, and min/max durations, giving you a precise benchmark to identify and tackle inefficient queries."
57,"Keep in mind, ‘similar’ in this context means syntax structure, not the literal content. For example, SELECT * FROM table WHERE id = 10; would be aggregated with SELECT * FROM table WHERE id = 20; due to their matching formats."
57,"To start gathering data, you’ll need to install the pg_stat_statements extension using CREATE EXTENSION pg_stat_statements;. This creates a view where you can observe the collected statistics."
57,"Be conscious of security: this module indiscriminately gathers stats across all user and database interactions on the server. The extension can be added to one or more databases and, by default, restricts users to only see their queries. Superusers and those with pg_read_all_stats or pg_monitor roles are exempt from this restriction, gaining visibility of every query."
57,"Remember: While dealing with datasets that exhibit signs of bloat, such as excessive, unused space or redundant data, the insights from pg_stat_statements may provide indications for cleanup that could further streamline your query performance."
57,"Lastly, for a more intuitive experience, consider using tools like EDB’s PostgreSQL Enterprise Manager (PEM), which features an SQL Profiler for a more user-friendly analysis of your SQL queries’ performance metrics."
57,Refining Query Structure
57,Utilising Exposed Column References
57,"When crafting SQL queries, aim for a construct where the column of interest remains unencumbered on one side of the condition. For instance, rather than adding intervals directly to a timestamp column in the WHERE clause, compare the column to a manipulated version of the current timestamp. This allows the effective use of indexes and enhances query speed significantly. Here’s a structure to follow:"
57,Instead of:
57,SELECT * FROM t
57,WHERE t.a_timestamp + interval '3 days' < CURRENT_TIMESTAMP
57,Adopt this format:
57,SELECT * FROM t
57,WHERE t.a_timestamp < CURRENT_TIMESTAMP - interval '3 days'
57,Avoiding NOT IN with Subqueries
57,"To ensure your queries return accurate results, use NOT EXISTS rather than NOT IN when dealing with subqueries that may include NULL values. The presence of NULL can result in the IN clause only returning true or null, but never false. This applies inversely to NOT IN, where only false or null returns are possible, potentially impacting the query’s logic. By opting for NOT EXISTS, you enable the database to execute an Anti Join, leading to more efficient relations processing and more predictable performance."
57,For positive checks:
57,"SELECT 1 IN (1, 2);"
57,"For negative checks, instead of:"
57,SELECT 1 NOT IN (SELECT value FROM t);
57,Use:
57,SELECT 1 WHERE NOT EXISTS (SELECT 1 FROM t WHERE t.value = 1);
57,"By adjusting your query formulation, you’ll leverage the structured nature of the database schema, boost sorting, and querying through efficient index utilization like CREATE INDEX, and minimise sequential scans and unnecessary aggregates."
57,"Utilising EXPLAIN (ANALYZE, BUFFERS) for Query Diagnostic"
57,Inaccurate Predictions
57,"When your database returns unexpected query times, outdated table statistics could be the culprit. The planner’s misjudgment, such as expecting a handful rather than hundreds of rows, could lead to an inefficient choice of query plan, such as opting for a nested loop over a more appropriate hash or merge join. By refreshing these statistics using the ANALYZE command, or improving planner insight with CREATE STATISTICS for correlated data, you can rectify inaccurate estimates and enhance performance."
57,Insufficient Memory for Sorting
57,"Feel your queries slowing down unexpectedly? An operation using disk space due to a lack of work_mem could be the reason. Memory is faster, so when sorting tasks overflow into disk space, response times suffer. Consider allocating more memory to work_mem to counteract this. Beware, though, an overemphasis on this parameter might be unnecessary if a well-designed index can obviate the need for sorting in the first place. To prevent extreme cases of large temporary file generation, adjust the temp_file_limit setting."
57,Partitioned Hash Operations
57,"If you encounter hashing in numerous batches, it’s a sign that work_mem is not sufficient for the task at hand. Reducing the batch count by increasing this memory allocation can sharply reduce execution time. A side-by-side comparison of plans before and after adjusting work_mem will highlight the stark improvements in performance."
57,Fetches from the Table Heap
57,"Whenever there is a mismatch between the Visibility Map and table heap concerning row visibility, more I/O operations occur than necessary, particularly with Index Scans. Index-Only Scans aim to avoid this but if a Visibility Map is outdated, Index-Only Scans will default back to standard practices, impacting performance. Running VACUUM ensures the Visibility Map reflects the latest state, potentially reducing unnecessary heap accesses."
57,Approximated Bitmap Scans
57,"For scattered data, Postgres uses Bitmap Index Scans, efficiently fetching each required page only once. However, when work_mem can’t store the complete map, the scan mode switches to ‘lossy’, requiring a full perusal of all rows on the identified pages, instead of directly accessing the relevant rows. Your query’s efficiency depends greatly on the allocation of enough work_mem to maintain a precise bitmap."
57,Misguided Query Plans
57,"Finally, keep a sharp lookout for cases where the database’s execution plan might not align with what you’d expect for an efficient query. This misalignment indicates that the database’s understanding of how best to execute the query is flawed, often because it lacks all the necessary information or parameters it needs to make an informed decision. Addressing this may involve tweaking various database settings or restructuring how the data is accessed and stored."
57,Partitioning
57,Partitioning your database can enhance its maintenance and bolster its performance through parallel processing.
57,Maintenance Advantages
57,"Autovacuum Efficiency: When you have an extensively large table, the vacuuming process to remove outdated or deleted rows can become unwieldy. By segmenting your tables, each piece and associated index undergo vacuuming individually, easing and hastening the task for autovacuum workers."
57,"Memory Management: If you’re unable to allocate sufficient autovacuum work memory, partitioning may reduce the required space for dead row lists during cleanup."
57,"Conflict Prevention: As partitioned tables are smaller, the autovacuum process needs fewer interruptions. This comes in handy especially when it’s crucial for avoiding database wraparound issues."
57,"Indexes: By splitting the table, you optimise index usage. Insertions and updates more readily fill spaces in the main table’s data pages, but the same isn’t true for indexes. Regular vacuuming is a necessity to keep index performance up – a smaller, partitioned index has better opportunities for reuse and does not need frequent reindexing."
57,"Frozen Rows and Partition Detachment: To address the challenge of permanent historical data and avoid vacuuming, implementing partitioning can simplify things. Once data is no longer altered and is ‘frozen’, older partitions storing this data can be detached and removed, obviating the need for autovacuuming."
57,Parallelisation Benefits
57,"Improved I/O: By partitioning your data (for example, by tenant in a multi-tenant system), you can allocate different workloads to separate tablespaces. This enhances I/O performance due to reduced contention."
57,Streamlined Workers: Concurrently running workers can make light work of smaller data partitions. Frequent and quicker maintenance tasks keep the system in optimum health.
57,"It’s worth noting that it’s a misconception to presume partitioning enhances query performance merely by dividing a large table into smaller ones. This strategy could potentially lead to inferior performance. Instead, consider partitioning as a method to manage and scale your database effectively."
57,Optimising Database Management for OLTP Systems
57,"To enhance your PostgreSQL performance, it’s essential to engage in regular monitoring and fine-tuning. This tailored approach is crucial to ensure that the system aligns with the unique demands of online transaction processing (OLTP)."
57,Monitor diligently: Keep a close watch on system performance.
57,Adjust settings: Modify configurations to meet evolving workload requirements.
57,Uphold data integrity: Maintain reliable data storage and retrieval processes.
57,Adhering to these best practices will significantly contribute to a robust and efficient database environment.
57,Frequently Asked Questions
57,What Techniques Can Enhance PostgreSQL Database Efficiency?
57,"There are several techniques to enhance the performance of a PostgreSQL database. These include adjusting the configuration parameters to suit your system’s specifics, utilising appropriate indexing strategies, and streamlining queries by rewriting or splitting complex ones. Additionally, hardware improvements should be considered, using tools for monitoring PostgreSQL performance, and regularly running the vacuum and analyze commands."
57,How Can One Optimise Queries Within PostgreSQL?
57,"To optimise queries in PostgreSQL, you should:"
57,Identify slow-running queries with EXPLAIN and EXPLAIN ANALYZE commands.
57,Make use of appropriate indexes.
57,Rewrite queries to remove unnecessary complexity.
57,Use subqueries and common table expressions wisely.
57,Reduce the number of rows to be processed as early as possible in the query.
57,Distinguishing Between Database Tuning and Optimisation
57,"Database tuning generally refers to system-level adjustments to enhance performance, such as changing configuration settings or upgrading hardware to reduce bottlenecks. On the other hand, optimisation often deals with improving the efficiency of individual queries and ensuring that the database schema supports the workload."
57,The Role of Indexing in PostgreSQL Performance
57,Indexing can significantly improve query performance in PostgreSQL by allowing the database engine to locate data more quickly rather than scanning entire tables. Types of indexes and factors for their use:
57,"B-Tree indexes, effective for equality and range queries."
57,"GIN and GiST indexes, suitable for complex data types like JSON or full-text search."
57,"Partial indexes, which include only a subset of records."
57,Considerations for Tuning PostgreSQL with Large Datasets
57,"With large datasets, consider the following:"
57,Hardware: Ensure adequate memory and SSDs for swifter I/O.
57,Maintenance: Regularly vacuum and analyze the database.
57,"Partitioning: Splitting large tables into smaller, more manageable pieces."
57,Resource Management: Use work_mem and shared_buffers to manage memory allocation effectively.
57,Best Practices for SQLITE Joins
57,When using joins:
57,Selectivity: Choose the table with the smallest result set to lead the join.
57,Indexing: Create indexes on the columns used in join conditions.
57,EXPLAIN Analyze: Use this command to understand how your joins are being executed and to identify inefficiencies.
57,"Join Types: Understand the differences between INNER, OUTER, LEFT JOIN, and others, to select the most appropriate for the query."
57,"By adhering to these best practices, you can enhance the performance of your joins and, by extension, your PostgreSQL database operations."
57,Categories Postgres Performance Tuning Tags Performance Tuning
57,PostgreSQL Query Optimization Performance Tuning with EXPLAIN ANALYZEPostgres Performance Tuning Query Plans
57,Leave a Comment Cancel replyCommentName
57,Email
57,Website
57,"Save my name, email, and website in this browser for the next time I comment."
57,Search for:
57,Categories
57,Cloud (6)
57,AWS (6)
57,AWS Glue (2)
57,Lake Formation (2)
57,Data Warehousing (4)
57,Databases (715)
57,Big Data (3)
57,Cassandra (2)
57,Hadoop (1)
57,MongoDB (5)
57,MySQL (327)
57,Errors (326)
57,Oracle (43)
57,Errors (2)
57,Oracle Basics (35)
57,Oracle RAC (12)
57,Oracle Tasks (4)
57,Oracle Tuning (3)
57,PostgreSQL (337)
57,Aurora (12)
57,Errors (256)
57,How To – Postgres (44)
57,Postgres Performance Tuning (7)
57,RedShift (4)
57,Featured (11)
57,Self Improvement (3)
57,Tutorial (7)
57,Uncategorized (12)
57,Video (10)
57,Recent Posts
57,PostgreSQL Performance Tuning
57,Postgres Performance Tuning Query Plans
57,Introduction to PostgreSQL Performance Tuning and Optimization
57,PostgreSQL Query Optimization Performance Tuning with EXPLAIN ANALYZE
57,Understanding PostgreSQL EXPLAIN: A Guide to Query Performance Optimisation
57,Get Started with EXPLAIN ANALYZE: Optimising SQL Query Performance
57,PostgreSQL Performance Tuning and Optimization
57,Solving MySQL Error 2044 – CR_SHARED_MEMORY_EVENT_ERROR: A Guide to Shared Memory Issues
57,Navigating MySQL Error 2032 – CR_DATA_TRUNCATED: Solutions for Data Truncation Issues
57,Addressing MySQL Error 2023 – CR_PROBE_SLAVE_HOSTS: Resolving Issues with SHOW SLAVE HOSTS
57,Troubleshooting MySQL Error 2015 – CR_NAMEDPIPE_CONNECTION: Issues with Named Pipe Connections
57,Resolving MySQL Error 2003 – CR_CONN_HOST_ERROR: Can’t Connect to MySQL Server
57,How to Diagnose and Resolve MySQL Error 1351 – SQLSTATE: HY000 (ER_VIEW_SELECT_TMPTABLE)
57,Solving MySQL Error 2045: Shared Memory Connection Troubles
57,Addressing MySQL Error 2033: Resolving Parameter Issues in Statements
57,Navigating MySQL Error 2024: Resolving Slave Connection Issues
57,Tackling MySQL Error 2013: Solutions for Restoring Lost Server Connections
57,Resolving MySQL Error 2004: Can’t Create TCP/IP Socket
57,Diagnosing and Fixing MySQL Error 1352: Ensuring Column Count Consistency in Views
57,Resolving MySQL Error 2042 – (CR_SHARED_MEMORY_FILE_MAP_ERROR) Shared Memory Allocation Issue
57,© 2024 Philip McClarence • Built with GeneratePress
59,Top 10 PostgreSQL Extensions You Should Know About | Airbyte
59,Complete the State of Data & AI Survey for a chance to win a Steam Deck!
59,View Press KitProduct
59,"ProductAirbyte CloudFully-managed, get started in minutesAirbyte Self-Managed EnterpriseSecure data movement for your entire orgAirbyte Open SourceUsed by 40k+ companiesPowered by AirbyteEmbed 100s integrations at once in your appcapabilitiesExtract & LoadReliable database and API replication at any scalePyAirbyteThe power of Airbyte to every Python developerConnector builderBuild a new connector  in 10 minAI / LLMs with Proprietary DataEmbeddings from unstructured sourcesTry our demo appExplore our public demoSolutions"
59,"Use CasesDatabase replicationHigh-volume DBs with low latencyArtificial intelligenceMake sense of unstructured data with LLMsAnalyticsMarketing, sales, product, finance, eng & moreEmbed ConnectorsEasily collect credentials from your end-usersResourcesSuccess storiesLearn from other members’ successCompare Airbyte vs. alternativesChoose the right solutions for youBuild vs. BuyEvaluate your costs in both scenariosResource centerOur guides to help you in your journeyPartnersBecome a technology or consulting partnerTHE LARGEST DATA ENGINEERING SURVEYCheck out State of DataNo items found.Developers"
59,"LearnDocsHow to use and contribute to AirbyteBlogData engineering thought leadershipTutorialsImprove your data replication gameQuickstartsDeploy your use case in minutesPublic RoadmapGet a glimpse in the futureData Engineering ResourcesPlace for all data knowledgeCommunityMonthly NewsletterStay up to date. 20k+ subscribersSupport centerAccess our knowledge baseCommunityJoin our 15,000+ data  communityCommunity Reward ProgramLeave your mark in the OSS communityEvents & community callsLive events by the Airbyte teamOur Social PlatformCommunity forumGitHub Discussions for questions, ideasSlack15,000+  share tips and get supportYoutubeLearn more about Airbyte and data engineeringDiscourse (read-only)Previous forum (still lots of knowledge)ConnectorsPricing"
59,"StarTalk to SalesTry it  freeData Engineering ResourcesTop 10 PostgreSQL Extensions You Should Know AboutAditi Prakash••July 27, 2023•TL;DRPostgreSQL is a powerful relational database management system (RDBMS). It’s become a mainstay in most modern data stacks because of its extensibility, with the platform’s diverse extensions giving users tons of added functionalities.Postgres extensions revolutionize the way developers and data teams interact with databases. They enable data engineers to build robust databases that precisely match their application’s requirements.In this article, we’ve listed the 10 best PostgreSQL extensions that every data professional must know about, along with tips to install and use them effectively.Understanding PostgreSQL ExtensionsPostgreSQL extensions are add-on modules that enhance the functionality of the database solution. They provide additional features, data types, functions, and operators that are not present in the core Postgres system.Extensions are created by the PostgreSQL community or third-party developers to address specific use cases or to provide specialized functionalities for different application scenarios.The extension architecture in PostgreSQL allows developers to create and package their features as self-contained units, making it easy to install, update, and manage extensions independently of the main PostgreSQL installation. This modular approach ensures that the core database remains lean and efficient while allowing users to extend its capabilities to match their needs.How to Install and Manage PostgreSQL ExtensionsTo install a PostgreSQL extension, follow these steps:Check for Extension Availability: Before installing an extension, check if your current PostgreSQL version supports it. You can browse the official PostgreSQL documentation or search for the extension’s name on the PostgreSQL Extension Network (PGXN) website.Install the Extension: Once you’ve identified the extension you want to install, use the CREATE EXTENSION SQL command to install it. Replace extension_name with the actual name of the extension.CREATE EXTENSION extension_name;           If the extension has any specific configuration options, you can set them using the WITH clause:CREATE EXTENSION extension_name WITH option = value;Verify Installation: After executing the CREATE EXTENSION command, check for error messages. If the command runs successfully, the extension is installed and ready for use.Managing PostgreSQL extensionsOver time, you’ll need to manage and update Postgres extensions so that they are relevant to your current use cases. Here’s how you can do that: Listing Installed Extensions: To see a list of all the extensions installed in the database, you can use the \dx command in the psql tool or execute the following SQL query:SELECT * FROM pg_extension;           This will display information about each installed extension, such as its name, version, and default schema.Updating Extensions: If you want to install a newer version of an extension, you can use the ALTER EXTENSION command. It will update the Postgres extension to the latest version. Use this SQL query:ALTER EXTENSION extension_name UPDATE;          Note that some extensions require specific steps for updating, as mentioned in their documentation.Uninstalling Extensions: To remove an installed extension from the database, use the DROP EXTENSION command. This will remove all objects associated with the extension, so be cautious when using this command.DROP EXTENSION extension_name;Managing Extension Versions: PostgreSQL allows you to have multiple versions of an extension installed simultaneously. You can control which version is active for a particular database using the ALTER EXTENSION command with the VERSION option:ALTER EXTENSION extension_name SET SCHEMA new_schema;           This will set the active version of the extension to the one installed in the new_schema schema.Viewing Extension Details: You can get detailed information about a specific Postgres extension using the \dx extension_name command in the psql tool or by querying the pg_extension catalog table:SELECT * FROM pg_extension WHERE extname = 'extension_name';The Top 10 PostgreSQL ExtensionsLet’s take a closer look at each of the 10 best Postgres extensions available today:1. PostGISPostGIS is a powerful open-source extension that enables PostgreSQL to handle geographic objects and spatial data. This eliminates the need for separate specialized systems and allows location queries to be run in SQL.PostGIS extends PostgreSQL by introducing new data types and functions for storing, querying, and analyzing spatial data, such as points, lines, polygons, and more complex geometries.Here are three examples of how to use the PostGIS extension: Creating a spatially-enabled table:CREATE TABLE spatial_data ("
59,"id SERIAL PRIMARY KEY,"
59,"name VARCHAR,"
59,"location GEOMETRY(Point, 4326) -- 4326 is the SRID (Spatial Reference Identifier) for WGS 84"
59,"Querying spatial data. For example, to find all points within 1000 meters of a given point:SELECT * FROM spatial_data"
59,"WHERE ST_DWithin(location, ST_GeomFromText('POINT(-73.975972 40.782865)', 4326), 1000);"
59,"Spatial joins. If you want to find points within a polygon:SELECT p.name, pg.name AS polygon_name"
59,FROM points p
59,JOIN polygons pg
59,"ON ST_Within(p.location, pg.location);"
59,"These examples demonstrate just a fraction of PostGIS’ capabilities. It is a versatile extension that unlocks the potential for building spatially-aware applications, handling geospatial data in GIS projects, and performing complex analyses within the PostgreSQL database.2. hstoreThe hstore module is a Postgres extension that allows you to store and manipulate sets of key-value pairs as a single value in a PostgreSQL table.The hstore extension is designed to be lightweight and efficient. It allows data teams to store semi-structured data within a relational database.Here are three examples of how to use this extension: Creating a table with an hstore column:CREATE TABLE products ("
59,"product_id SERIAL PRIMARY KEY,"
59,"product_name VARCHAR,"
59,properties hstore
59,Querying data from the hstore column. Let’s say you want to retrieve all products with the “color” property as “red”:SELECT * FROM products
59,WHERE properties -> 'color' = 'red';
59,"Deleting a key-value pair from the hstore column. So, to remove the “weight” property from a specific productUPDATE products"
59,"SET properties = delete(properties, 'weight')"
59,WHERE product_id = 1;
59,"3. pg_stat_statementspg_stat_statements is a built-in PostgreSQL extension that provides a way to collect and track statistics about SQL statements executed in the database. It records information like the total execution time, number of calls, and number of rows returned for each SQL statement. This extension is a valuable tool for developers to analyze and optimize queries. It makes optimizing the database system and improving the overall application performance easier.Here are three examples of how to use this extension:To view query statistics:SELECT query, total_time, calls, rows"
59,FROM pg_stat_statements
59,ORDER BY total_time DESC
59,LIMIT 10;
59,"This query will show the top 10 most time-consuming SQL statements in terms of total execution time, and the number of calls and rows returned.If you want to reset the collected statistics:SELECT pg_stat_statements_reset();         This will reset all the gathered statistics to zero.Configuration options:           pg_stat_statements provides some configuration options that you can set in the postgresql.conf file or in a session. For example, you can adjust the number of            queries to track or specify whether to include or exclude query text.          To track all statements:ALTER SYSTEM SET pg_stat_statements.max ON;          To include query text in the statistics:ALTER SYSTEM SET pg_stat_statements.track = all;          To exclude query text in the statistics:ALTER SYSTEM SET pg_stat_statements.track = none;4. pgcryptopgcrypto is a Postgres extension that enables cryptographic functions and data encryption capabilities within the database.This extension enhances the data security and privacy of PostgreSQL databases by allowing data engineers to store hashed passwords, encrypt sensitive information, and perform cryptographic operations on the server side, reducing the risk of exposing sensitive data in transit or at rest.Developers can perform operations like hashing, encryption, and decryption directly within SQL queries or PL/pgSQL functions. Here are three examples of how to use this extension:Hashing a password and storing it in the “Users” table:INSERT INTO users (username, password_hash)"
59,"VALUES ('user123', crypt('password123', gen_salt('bf')));Encrypting sensitive data: INSERT INTO sensitive_data (id, encrypted_info)"
59,"VALUES (1, pgp_sym_encrypt('sensitive_info', 'passphrase'));Generating cryptographic hashes, like an SHA-256 hash of a string:SELECT digest('Hello, world!', 'sha256');5. citextcitext stands for “case-insensitive text.” It allows database users to store and compare textual data in a case-insensitive manner. Using the citext data type, they can compare text without considering the letter case.This is especially useful for user authentication (username and password comparisons), searching for records based on names or titles, and performing case-insensitive queries.Without this extension, developers typically need to use the LOWER() or UPPER() functions to convert text to a common case for comparisons, which can be time-consuming and also hinder query performance. Here are three examples of how to use this extension:Creating a table with a citext column:CREATE TABLE users ("
59,"user_id SERIAL PRIMARY KEY,"
59,"username CITEXT,"
59,email CITEXT
59,");To search for a user by username without worrying about the case:SELECT * FROM users WHERE username = 'jOhNDoE';Inserting case-insensitive data:INSERT INTO users (username, email)"
59,"VALUES ('JohnDoe', 'john.doe@example.com');6. pg_trgmpg_trgm adds support for trigram-based text search and similarity ranking. Trigrams are three-character sequences extracted from words. They serve as the basis for similarity comparisons between strings.The extension enhances PostgreSQL’s full-text search functionality. It provides a mechanism for applications to handle complex queries more intelligently, even when the query terms have typographical errors or are not an exact match to the data stored in the system.Traditional exact text search methods might not be sufficient when dealing with user input, misspellings, or variations in word forms. pg_trgm offers a solution in these situations.Here are three examples of how to use this Postgres extension:Creating an index on a text column for trigram-based similarity search:CREATE INDEX trgm_index ON your_table USING gin (your_text_column gin_trgm_ops);Find similar strings based on trigram similarity:SELECT * FROM your_table"
59,"WHERE your_text_column % 'search_term';           Here, the % operator performs a similarity search based on trigrams. It returns rows where the your_text_column is similar to the given ‘search_term.’Ranking similarity using trigram similarity score:SELECT *, similarity(your_text_column, 'search_term') AS trigram_similarity"
59,FROM your_table
59,WHERE your_text_column % 'search_term'
59,"ORDER BY trigram_similarity DESC;          In this example, the similarity() function calculates the trigram similarity score between the “your_text_column” and the given ‘search_term’, and the results are           ordered by the similarity score in descending order.7. tablefunctablefunc provides additional table functions. These can be applied to queries to generate crosstab results, pivot tables, and perform data transformations.The extension enables users to convert row data into columns, which helps present data in a pivot table-like format. It can also fill in missing data in the crosstab results, helping to improve data visualization and reporting.Data analysts can use tablefunc to pivot data based on specific criteria, making it easier to analyze and summarize information. They can also run dynamic crosstab queries, where the number of resulting columns may vary based on the data.Here are two examples of how to use this extension:Crosstab queries:          Let’s say you have a table named “Sales” with columns for month, product, and revenue. To convert the data into a crosstab format with months as rows and           products as columns, you can use the crosstab function from tablefunc:SELECT *"
59,FROM crosstab(
59,"'SELECT month, product, revenue FROM sales ORDER BY 1, 2',"
59,"'VALUES (''January''), (''February''), (''March''), (''April''), (''May''), (''June'')'"
59,") AS ct(month text, product1 revenue1 numeric, product2 revenue2 numeric, product3 revenue3 numeric);"
59,You can use the crosstab function with NULL filling to handle missing data. To replace NULL values with 0:SELECT *
59,FROM crosstab(
59,"'SELECT month, product, revenue FROM sales ORDER BY 1, 2',"
59,"'VALUES (''January''), (''February''), (''March''), (''April''), (''May''), (''June'')'"
59,") AS ct(month text, product1 revenue1 numeric, product2 revenue2 numeric, product3 revenue3 numeric)"
59,WITH NULL AS 0;
59,"8. intarrayintarray adds support for one-dimensional arrays of integers to the Postgres database. It enables data engineers to store and manipulate arrays of integers in a single database column. With intarray, users can store and manipulate integer arrays directly within PostgreSQL, simplifying data handling and enabling better query performance for array-related operations.Here are three examples of how to use this extension:Creating a table with an integer array column:CREATE TABLE scores ("
59,"id SERIAL PRIMARY KEY,"
59,"player_name VARCHAR,"
59,scores INT[]
59,Querying data from the integer array column. If you want to find all players with a score greater than 90:SELECT * FROM scores
59,"WHERE 90 = ANY(scores);Aggregating data from the integer array column. So, you can calculate the average score for each player using:SELECT player_name, AVG(score) AS average_score"
59,"FROM scores, unnest(scores) score"
59,"GROUP BY player_name;9. earthdistanceThe earthdistance module adds support for geolocation-based calculations and queries in PostgreSQL databases. It lets users calculate the distance between two points specified by latitude and longitude coordinates. The extension provides two approaches - cube-based and point-based - for calculating distances accurately on the Earth’s surface. This enables developers to perform proximity searches, location-based queries, and other geospatial analyses.Here are three examples of how to use this extension:Creating a table with latitude and longitude columns:CREATE TABLE locations ("
59,"location_id SERIAL PRIMARY KEY,"
59,"name VARCHAR,"
59,"latitude DOUBLE PRECISION,"
59,longitude DOUBLE PRECISION
59,");Querying locations within a certain distance. Let’s say you want to find locations within 100 kilometers from a given point:SELECT name, latitude, longitude"
59,FROM locations
59,"WHERE earth_box(ll_to_earth(40.7128, -74.0060), 100000) @> ll_to_earth(latitude, longitude);Calculating distances between locations:SELECT earth_distance(ll_to_earth(40.7128, -74.0060), ll_to_earth(34.0522, -118.2437)) AS distance_km;10. cubecube introduces a new data type called “cube,” so users can efficiently store and manipulate multidimensional points. The extension also offers indexing support, enabling fast queries and operations on multidimensional data.By using the cube module, users can store and process n-dimensional points, perform range queries, calculate distances between points, and use indexing for faster data retrieval in higher-dimensional spaces.Here are three examples of how to use this extension:Creating a table with a cube column:CREATE TABLE points ("
59,"point_id SERIAL PRIMARY KEY,"
59,position CUBE
59,");Querying data using cube operators. For example, to find points within a specific range in a 2-dimensional space:SELECT *"
59,FROM points
59,"WHERE position @ cube(array[1, 1], array[2, 4]);           To find the distance between two 3-dimensional points:SELECT cube_distance(position, CUBE(array[1.0, 2.0, 3.0]))"
59,FROM points
59,WHERE point_id = 1;Using cube indexing. To create an index on the cube column:CREATE INDEX idx_points_position ON points USING gist (position);          To perform a range query using the index: SELECT *
59,FROM points
59,"WHERE position @ cube(array[0, 0], array[2, 5]);How to Choose the Right PostgreSQL ExtensionThere are seven main factors to consider when choosing a PostgreSQL extension:Functionality: Consider the specific functionality you need for your application or database and choose an extension that addresses these requirements.Compatibility: Ensure that the extension is compatible with your PostgreSQL version. Some extensions might not be available or supported in older or newer versions of PostgreSQL.Performance: Evaluate the performance impact of extensions loaded on your database. Some extensions may introduce overhead or have specific hardware requirements, so consider the potential impact on query performance.Support and Maintenance: Check the level of support and maintenance for the extension. Active and well-maintained extensions are more likely to receive updates and bug fixes, ensuring the extension remains compatible with future PostgreSQL versions.Documentation: Look for extensions with clear and comprehensive documentation. Good documentation helps you understand how to use the extension effectively and troubleshoot any issues.Community and Adoption: Consider the popularity and adoption of the extension within the PostgreSQL community. Widely-used and well-established extensions can generally be more reliable and trustworthy.Security: Evaluate the security implications of using the extension. Extensions should follow best practices for data security and should not introduce vulnerabilities into your system.Potential issues while using PostgreSQL extensionsData engineers and developers might face the following issues while using PostgreSQL extensions:Dependency Conflicts: Systems using multiple extensions might have conflicting dependencies or requirements. All extensions used must be compatible with each other.Performance Overhead: Some extensions may introduce performance overhead, especially if they involve complex queries or large data sets. Test the performance impact of the extension on your workload.Bugs and Compatibility Issues: Extensions might not always be bug-free, and they might not work correctly with all versions of PostgreSQL. Stay updated with the latest versions and bug fixes for the extensions.Upgrade Compatibility: PostgreSQL upgrades may cause issues with current extensions. Some extensions might require updates to work with the latest Postgres version.Security Risks: Poorly maintained or insecure extensions could introduce vulnerabilities into the database. Always review the reputation and trustworthiness of the extension and its developer.Feature Overlap: Be aware of any feature overlap between PostgreSQL core functionality and the extension. Ensure that there are no duplicate functionalities or unnecessary complexity.Licensing and Legal Considerations: Check the licensing terms of the extension to ensure it aligns with the project’s requirements and complies with your organization’s policies.Real-World Use Cases of Postgres ExtensionsTo illustrate how extensions can elevate PostgreSQL databases, let’s look at two example case studies:Case study 1: Using PostGIS for geospatial data analysisScenario: A logistics and transportation company needs to optimize its delivery routes based on real-time traffic conditions and customer locations. They have an extensive database of geospatial data, including information about customers, delivery points, road networks, and traffic patterns.Solution: The company uses the PostGIS extension to gain geospatial capabilities in PostgreSQL. They create tables to store customer locations, delivery points, and road network data, utilizing PostGIS data types like ‘GEOMETRY’ and ‘GEOGRAPHY’.As a result, they can perform:Geospatial Analysis: They can use PostGIS functions to calculate the distance between delivery points and customers, identify the closest warehouse for each delivery point, and determine the most efficient routes based on real-time traffic data.Proximity Search: Proximity searches can find customers within a certain radius of a specific location, allowing them to target marketing campaigns effectively.Spatial Indexing: PostGIS provides efficient spatial indexing, enabling fast spatial queries on large datasets. This can produce insights to streamline the entire logistics operation.Geospatial Visualization: By using PostGIS in conjunction with visualization tools, they can create interactive maps displaying delivery routes, traffic conditions, and customer clusters.Using PostGIS, the logistics company boosts its delivery operations, reduces travel times, and optimizes its resource usage, improving customer satisfaction and cost savings.Case study 2: Using hstore for handling key-value pairs in e-commerce applicationsScenario: An e-commerce platform wants to allow sellers to add custom product attributes and specifications. However, these specifications can vary greatly between different product categories and sellers.Solution: The company adopts the hstore extension in PostgreSQL to handle dynamic and flexible product attributes. They create a table to store product information, using the hstore data type for the custom attributes.By implementing hstore, they can improve the user experience for sellers and customers. Here’s how:Custom Product Attributes: Sellers can add custom product attributes, such as color options, size variations, and technical specifications. The hstore key-value storage allows them to efficiently add and manage these attributes without altering the table structure.Search and Filtering: Customers can search for products based on custom attributes. For example, customers can filter products by color, size, or other specifications.Indexing and Performance: The hstore extension supports indexing on the key-value pairs, improving the performance of attribute-based searches, even with a large number of products.Scalability: As the platform grows and the number of products and sellers increases, hstore’s flexible nature allows the database schema to adapt easily to new attributes without requiring extensive schema changes.By leveraging hstore in their PostgreSQL database, the e-commerce platform provides sellers with a flexible and customizable product listing experience while customers benefit from enhanced search options. All of these factors can lead to increased sales.ConclusionWhile PostgreSQL already offers a rich set of features, extensions provide additional specialized functionality that caters to diverse use cases and industries. It allows data teams to extend the capabilities of PostgreSQL without modifying the core database code.By incorporating extensions into their PostgreSQL databases, organizations can tailor their database environments to suit their specific needs, making PostgreSQL a versatile and powerful solution.By adopting the right extensions, data engineers can streamline development, enhance database performance, and unlock new capabilities within their PostgreSQL environment.For more in-depth guides to PostgreSQL and data management, check out our Content Hub.Limitless data movement with free Alpha and Beta connectorsIntroducing: our Free Connector ProgramThe data movement infrastructure for the modern data teams.Try a 14-day free trialAbout the AuthorAditi Prakash is an experienced B2B SaaS writer who has specialized in data engineering, data integration, ELT and ETL best practices for industry-leading companies since 2021.About the AuthorTable of contentsExample H2Example H3Example H4Example H5Example H6Example H2Example H3Example H4Example H5Example H6Get your data syncing in minutesTry Airbyte freeJoin our newsletter to get all the insights on the data stack."
59,"Related postsData Pipeline vs. ETL: Optimize Data Flow (Beginner's Guide)•March 14, 2024•15 min readOn-Premise vs. Cloud Data Warehouses: The Comparison Guide•March 12, 2024•15 min readData Mart vs. Data Lake: Making the Best Choice•March 12, 2024•15 min readWhat is Big Data Integration: Examples and Use Cases•March 12, 2024•15 min readAirbyte is an open-source data integration engine that helps you consolidate your data in your data warehouses, lakes and databases.© 2024 Airbyte, Inc.ProductFeaturesDemo AppConnectorsConnector Development Kit (CDK)Airbyte Open SourceAirbyte CloudAirbyte Self-ManagedCompare Airbyte offersPricingChangelogRoadmapCompare top ELT solutionsRESOURCESDocumentationBlogAirbyte API DocsTerraform Provider DocsCommunityData Engineering ResourcesTutorialsQuickstartsNewsletterResource centerCommunity CallState of Data surveyTop ETL Tools""How to Sync"" TutorialsCOMPANYCompany HandbookAbout UsCareersOpen employee referral programAirbyte YC Startup ProgramPartnersPressData protection - Trust reportTerms of ServicePrivacy PolicyCookie PreferencesDo Not Sell/Share My Personal InformationContact SalesGet answers quick on Airbyte SlackHi there! Did you know our Slack is the most active Slack community on data integration? It’s also the easiest way to get help from our vibrant community.Join Airbyte SlackI’m not interested in joining"
60,Issue analysis methods - PolarDB - Alibaba Cloud Documentation Center
60,Document Center
60,All Products
60,Search
60,Document Center
60,PolarDB
60,PolarDB for PostgreSQL
60,Performance Tuning Guide
60,Instance issue analysis
60,Issue analysis methods
60,all-products-head
60,This Product
60,This Product
60,All Products
60,PolarDB:Issue analysis methods
60,Document Center
60,PolarDB:Issue analysis methods
60,"Last Updated:Mar 17, 2023"
60,"The basic method of issue analysis is to identify performance bottlenecks. To identify performance bottlenecks, you need to identify the resource type that ranks the first in terms of the consumption amount and find the reason for the high resource consumption. Then, you can perform a more detailed analysis of the issue. The following two types of basic resources are used in"
60,"PolarDB for PostgreSQL databases:System resources: CPU, I/O, network, memory, and disk resources. Database resources: locks and caches. The locks include lightweight locks (lwlocks) and other locks. The caches include the cache stored in buffer pools and the Simple Least Recently Used (SLRU) cache. The common optimization strategy is to analyze the wait events of a database and identify the insufficient resources that cause bottlenecks. Then, identify the issue causes based on the analysis on the resources."
60,Thank you! We've received your
60,feedback.
61,Ways to shoot yourself in the foot with Postgres | Hacker News
61,Hacker News
61,new | past | comments | ask | show | jobs | submit
61,login
61,Ways to shoot yourself in the foot with Postgres (philbooth.me)
61,743 points by philbo 10 months ago
61,| hide | past | favorite | 268 comments
61,h1fra 10 months ago
61,| next [–]
61,"Few tips I gathered along the years:- Configure Vacuum and maintenance_work_mem regularly if your DB size increases, if you allocate too much or too often it can clog up your memory.- If you plan on deleting more than a 10000 rows regularly, maybe you should look at partition, it's surprisingly very slow to delete that ""much"" data. And even more with foreign key.- Index on Boolean is useless, it's an easy mistake that will take memory and space disk for nothing.- Broad indices are easier to maintain but if you can have multiple smaller indices with WHERE condition it will be much faster- You can speed up, by a huge margin, big string indices with md5/hash index (only relevant for exact match)- Postgres as a queue is definitely working and scales pretty far- Related: be sure to understand the difference between transaction vs explicit locking, a lot of people assume too much from transaction and it will eventually breaks in prod."
61,masklinn 10 months ago
61,| parent | next [–]
61,"> Index on Boolean is useless, it's an easy mistake that will take memory and space disk for nothing.However if the field is highly biased (e.g. 90 or 99% one value) it can be useful to create a partial index on the rarer value. Though even better is to create a partial index on the other stuff filtered by that value, especially if the smaller set is the commonly queried one (e.g. soft-deletes)."
61,Waterluvian 10 months ago
61,| root | parent | next [–]
61,"Yeah. Finding ”open” tickets, for example. There’s actually some really good cases to index on a Boolean."
61,WirelessGigabit 10 months ago
61,| root | parent | next [–]
61,"We work in different places. Here the index in closed tickets would be smaller. But you know, some sales guy called and they want this little feature NOW."
61,j45 10 months ago
61,| root | parent | prev | next [–]
61,Nice example.
61,ellisv 10 months ago
61,| root | parent | prev | next [–]
61,"Also good to remember that booleans can have 3 values: true, false, or null. Creating a partial index on `WHERE NOT NULL` can be helpful too."
61,masklinn 10 months ago
61,| root | parent | next [–]
61,"Postgres uses distinct nulls. I've not checked, but I'd assume postgres simply does not index nulls, as it can't find them again anyway (unless you use the new ""NULLS NOT DISTINCT"" anyway). I think you need a separate index on the boolean IS NULL (which should probably be a partial index on whichever of IS NULL and IS NOT NULL is better)."
61,marcosdumay 10 months ago
61,| root | parent | next [–]
61,"Postgres absolutely adds nulls to its indexes. You can even control how they are ordered, and on the last few versions if nulls are equal or not.A complete index over a column will have entries for all records, and can be used on ""x is null"" and ""x is not null"" filters."
61,ellisv 10 months ago
61,| root | parent | prev | next [–]
61,"You're correct, thanks for noting this – I had it backwards."
61,WirelessGigabit 10 months ago
61,| root | parent | prev | next [–]
61,Even when the column is made with NOT NULL?
61,exac 10 months ago
61,| root | parent | next [–]
61,No.
61,h1fra 10 months ago
61,| root | parent | prev | next [–]
61,"If it's highly biased indeed, in combinaison of a condition it's useful.I was referring of indexing the column without distinction, the last time I checked (years ago) Postgres didn't do any statistical distribution so the query planner was always discarding the index anyway."
61,nvilcins 10 months ago
61,| parent | prev | next [–]
61,"> - Related: be sure to understand the difference between transaction vs explicit locking, a lot of people assume too much from transaction and it will eventually breaks in prod.I recently went from:"
61,* somewhat understanding the concept of transactions and combining that with a bunch of manual locking to ensure data integrity in our web-app;
61,to:
61,"* realizing how powerful modern Postgres actually is and delegating integrity concerns to it via the right configs (e.g., applying ""serializable"" isolation level), and removing the manual locks."
61,So I'm curious what situations are there that should make me reconsider controlling locks manually instead of blindly trusting Postgres capabilities.
61,azurelake 10 months ago
61,| root | parent | next [–]
61,Just FYI if you didn’t already know this:
61,Any transaction which is run at a transaction isolation
61,"level other than SERIALIZABLE will not be affected by SSI. If you want to enforce business rules through SSI, all transactions should be run at the SERIALIZABLE transaction isolation level, and that should probably be set as the default."
61,"Given that running everything at SERIALIZABLE probably isn’t practical for you, I think it’s more clear code wise to use explicit locks. That way, you can grep for what queries are related synchronization wise, vs. SERIALIZABLE being implicit."
61,wongarsu 10 months ago
61,| root | parent | next [–]
61,Continuing with the FYIs:Explicit locks can mean just calling LOCK TABLE account_balances IN SHARE ROW EXCLUSIVE MODE; early in the transaction and then doing SELECT ... FOR UPDATE; or similar configurations to enforce business rules where it matters.https://www.postgresql.org/docs/current/sql-lock.html
61,h1fra 10 months ago
61,| root | parent | prev | next [–]
61,"I think, in the using Postgres as a queue scenario, it's not fixing the problem that two processes can read the same row at the same time thus both executing the process.If you manually SELECT FOR UPDATE SKIP LOCKED LIMIT 1, then the second process will be forced to select the next task without waiting for the lock."
61,nextaccountic 10 months ago
61,| parent | prev | next [–]
61,"> - You can speed up, by a huge margin, big string indices with md5/hash index (only relevant for exact match)Do you mean a https://www.postgresql.org/docs/current/indexes-types.html#I... index? It's a 32-bit hash (but which hash is it, is it CRC32?). How to do a MD5 index?Anyway, MD5 is slow, does Postgres offer fast hashes like SipHash (DoS resistant) or FNV (not DoS resistant)?"
61,h1fra 10 months ago
61,| root | parent | next [–]
61,You can store the md5 (or any hash) in a new column and use it in the index instead of the string column. It will still be a string index but much shorter. You have to be aware of hash collision but in my case it was a multi column index so the risk was close to zero.
61,MD5 was maybe not the best choice but it's builtin so available everywhere.What I did to not maintain a second column is to use the function directly in the index:```
61,"CREATE UNIQUE INDEX CONCURRENTLY ""groupid_md5_uniq"" ON ""crawls"" (""group_id"", md5(""url""));"
61,``````
61,SELECT * FROM crawls WHERE group_id= $0 AND md5(url) = md5($1)
61,"```This simple trick, that did not required an extensive refactor, speed up the query time by a factor of thousand."
61,ellisv 10 months ago
61,| root | parent | next [–]
61,We do this by making the md5 a char(32) generated column of the text column.
61,somehnguy 10 months ago
61,| parent | prev | next [–]
61,"> - Index on Boolean is useless, it's an easy mistake that will take memory and space disk for nothing.I’ve seen this advice elsewhere as well, but recently tried it and found it wasn’t the case on my data set. I have about 5m rows, with an extremely heavy bias on one column being ‘false’. Adding a plain index on this column cut query time in about half. We’re in the millisecond ranges here, but still."
61,dist-epoch 10 months ago
61,| root | parent | next [–]
61,Just index the less common value:
61,CREATE INDEX ON session(is_active) WHERE is_active;
61,giovannibonetti 10 months ago
61,| root | parent | next [–]
61,"There is no need for adding the boolean value to the index in this case, since it is constant (true). You can add a more useful column instead, like id or whatever your queries use:CREATE INDEX ON session(id) WHERE is_active;"
61,somehnguy 10 months ago
61,| root | parent | prev | next [–]
61,I tested that and it seemed to make 0 difference between a basic 'create index on table(column)'.
61,dist-epoch 10 months ago
61,| root | parent | next [–]
61,"Have you measured the disk size of the index? That's where you should see a difference, not in speed."
61,somehnguy 10 months ago
61,| root | parent | next [–]
61,"It does appear smaller, but single digit megabytes on a table with millions of rows. Not a major difference for most use cases I think. But good to know for the few that it would make a difference."
61,klysm 10 months ago
61,| root | parent | prev | next [–]
61,"I know nothing about partial indices in Postgres, but it seems like for indexing a Boolean, you either index the true or false values right? I feel like Postgres could intelligently choose to pick the less frequent value"
61,Someone 10 months ago
61,| root | parent | next [–]
61,"Is that correct? I would think that, even with NOT NULL Boolean field, the physical table has three kinds of rows: those with a true value, those with a false value, and those no longer in the table (with either true or false, but that doesn’t matter)If so, you can’t, in general, efficiently find the false rows if you know which rows have true or vice versa.You also can only use an index on rows with true values to efficiently find those with other values if the index can return the true rows in order (so that you can use the logic “there’s a gap in the index ⇒ there are non-true values in that gap)"
61,klysm 10 months ago
61,| root | parent | next [–]
61,"Yeah that seems more like how it would work, I’m curious about the internals there"
61,smilliken 10 months ago
61,| root | parent | prev | next [–]
61,The benefit is a proportionally smaller index.
61,moring 10 months ago
61,| parent | prev | next [–]
61,"> Index on Boolean is useless, it's an easy mistake that will take memory and space disk for nothing.Why? Is it because an index on the bool alone, with symmetric distribution, will still leave you with half the table to scan? In other words, does that statement apply to biased distribution (as mentioned by another response) or indices on multiple fields of which one is a boolean?"
61,jeltz 10 months ago
61,| root | parent | next [–]
61,"Yes, it is because it leaves you with half the table the scan while adding the overhead of doing an index scan. And of you have a biased distribution you probably want a partial index since those are smaller."
61,xwdv 10 months ago
61,| root | parent | next [–]
61,"No, it’s rarely half the table, most bool columns are biased to one value."
61,code_biologist 10 months ago
61,| root | parent | prev | next [–]
61,Half the rows to scan in 99% of cases means you’ll still hit every page and incur exactly the same amount of IO (the expensive part) as a full table scan.
61,webstrand 10 months ago
61,| root | parent | next [–]
61,Would periodically clustering the table on the boolean index help here? Since then the true rows would be in different pages than the false rows. Unless I misunderstand what clustering does.
61,marcosdumay 10 months ago
61,| root | parent | next [–]
61,"The thing is that, since you can only cluster around a single ordering, a boolean column is very rarely the most useful one to use.But then, given the number of things that very rarely happen in a database, you are prone to have 1 or 2 of them happening every time you do something. Just not that specific thing; but if you keep all of those rules in mind, you will always be surprised."
61,valenterry 10 months ago
61,| root | parent | prev | next [–]
61,"Yes, that would indeed help."
61,alberth 10 months ago
61,| parent | prev | next [–]
61,"> You can speed up, by a huge margin, big string indices with md5/hash indexDumb question: what's the use case for having a md5/hash field in your database?"
61,__s 10 months ago
61,| root | parent | next [–]
61,Postgres offers hash indexes as opposed to b-tree indexes: https://www.postgresql.org/docs/current/hash-intro.htmlFor equality comparisons of large types it's quite beneficial
61,h1fra 10 months ago
61,| root | parent | prev | next [–]
61,"I have answered here: https://news.ycombinator.com/item?id=35701126In my case, I had to index big tables with URLs, with no need for partial match."
61,I did it naively but did help a lot.
61,pier25 10 months ago
61,| parent | prev | next [–]
61,> Postgres as a queue is definitely working and scales pretty farYou mean with triggers and listen/notify ?
61,ellisv 10 months ago
61,| root | parent | next [–]
61,Probably meant as something like this: https://www.crunchydata.com/blog/message-queuing-using-nativ...But I find that listen/notify seem to be drastically underused.
61,baq 10 months ago
61,| prev | next [–]
61,I’d add ’not reading the table of contents of the manual’ to the list.I’ve probably worked with hundreds of people now who use a database daily either in code or just to explore data and can count on two hands (optimistically…) the number of folks who actually read the fine manual in any other way than googling something specific. Pro tip: read it so you know what to google for!
61,hannofcart 10 months ago
61,| parent | next [–]
61,Googling? That's so passe.I just enter my vague question into this AI chat thingy and I try the first thing that it tells me on my production server.Has worked fine for me so far. What could possibly go wrong?
61,throw99998888 10 months ago
61,| root | parent | next [–]
61,"""I'll copy paste this snippet from a 2013 StackOverflow thread instead.""""Let me just install this 2MB js dependency real quick. I don't know anything about its author nor if its maintained at all, but it will prevent me from writing 10 lines of vainilla js.""ChatGPT is a vast improvement from this."
61,bhawks 10 months ago
61,| root | parent | prev | next [–]
61,Wow that's old school.Here we have an agent integrated into langchain that executes the command directly on the server. If there are any errors it uses ai to debug and fix them too.!/s see https://python.langchain.com/en/latest/modules/agents/toolki...
61,dror 10 months ago
61,| root | parent | next [–]
61,"Agent, shmagent.CREATE EXTENSION pggpt;and you're done. The AI watches your db and adjusts the params on the fly as needed."
61,"We're joking, but I give it less than 12 months before there'll be something like this."
61,duckmysick 10 months ago
61,| root | parent | prev | next [–]
61,You can ask this AI chat thingy if it's a good idea to experiment on your production server. Maybe it will suggest something else.
61,xupybd 10 months ago
61,| parent | prev | next [–]
61,It took me too long to understand this. I always felt pressure to get things done so skipped reading the manual. Turns out I would have gotten more done had I just read the manual.
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"There's the hoary old cliche about ""if I was given three hours to cut down a tree with an axe, I would spend the first hour sharpening the axe"" but in many cases it's really true. You can look like a superhero just by pointing out some small feature that makes life easier.One time I pointed out that, rather than reordering the tables to make loading work with foreign key constraints, we could pause trigger execution, load all the data, and then resume the triggers. pg_dump can even do it for you if you pick the options right (might be default), in addition to natively ordering the data correctly, but if you're messing around with large SQL files anyway, it's helpful.."
61,couchand 10 months ago
61,| root | parent | next [–]
61,"Here to point out that Lincoln said ""six hours to cut down a tree... four hours sharpening"", and I think that ratio is better.He wasn't the first to observe the general principle, it's at least as old as the Bible[0].[0]: https://biblehub.com/ecclesiastes/10-10.htm"
61,j45 10 months ago
61,| root | parent | prev | next [–]
61,Clever architecture and approach that borders on a series or layers of simple decisions can often put perform clever coding and maintain a greater degree of flexibility.
61,j45 10 months ago
61,| root | parent | next [–]
61,iOS Typo*put perform = outperform
61,nelsonic 10 months ago
61,| parent | prev | next [–]
61,This. https://www.postgresql.org/docs/current/
61,nerdponx 10 months ago
61,| parent | prev | next [–]
61,"FWIW I think this is good advice for any tool. You don't have to (and shouldn't) read the manual front to back, but you absolutely should look at the table of contents and at least start reading the introductory material."
61,Already__Taken 10 months ago
61,| parent | prev | next [–]
61,I do find SQL not easy in this regard. alter table add constraint is a totally different command than alter table. gotchas like that.
61,j45 10 months ago
61,| root | parent | next [–]
61,It’s not that bad to learn sql by example.Project based learning is best for sql and excel formulas.
61,Start at the start and it builds up quickly.Don’t worry it’s way less work than trying to make a nosql database into a sql database.
61,arichard123 10 months ago
61,| prev | next [–]
61,"Here's one, postgres default has this:"
61,seq_page_cost = 1.0
61,random_page_cost = 4.0
61,"Which is fine if you are using spinning disks to store your data. It makes postgresql prefer sequential scans over index usageI think it's time the default were changed to suit SSDs, where a random page cost is the same as a sequential one."
61,seq_page_cost = 1.0
61,random_page_cost = 1.0
61,jashmatthews 10 months ago
61,| parent | next [–]
61,The defaults do suck but common storage options like SSDs or Elastic Block Storage still do sequential IO substantially faster than random.
61,masklinn 10 months ago
61,| root | parent | next [–]
61,"Yes but nowhere near the extent rotating rust did.You may want to set random page costs higher than 1.0, in part because DB/FS-level pages and SSD blocks are completely different (and going through a block will be more efficient than having to hit multiple blocks), but probably 1.5 to 2.5.Interestingly enough according to some folks “seek” on EBS is highly concurrent, whereas “scan” is slow and more erratic, so you may want to set random_page_cost lower than on SSDs in order to favour seeks."
61,gshulegaard 10 months ago
61,| root | parent | next [–]
61,I wouldn't set random_page_cost lower than seq_page_cost.
61,It can cause the query planner to do wacky things (I learned the hard way).
61,"The documentation mentions it, but not as strongly as I think is warranted given how erratic my PostgreSQL cluster started behaving after I made that configuration change.> Although the system will let you set random_page_cost to less than seq_page_cost, it is not physically sensible to do so. However, setting them equal makes sense if the database is entirely cached in RAM, since in that case there is no penalty for touching pages out of sequence. Also, in a heavily-cached database you should lower both values relative to the CPU parameters, since the cost of fetching a page already in RAM is much smaller than it would normally be.https://www.postgresql.org/docs/current/runtime-config-query...Curious though, lowering both values is something I haven't done before but now I am curious about."
61,wongarsu 10 months ago
61,| root | parent | prev | next [–]
61,"Looking up some random SSD benchmarks, 2.0 seems about right for high-quality SSDs. Though you might as well benchmark your specific setup."
61,it_citizen 10 months ago
61,| prev | next [–]
61,"First way to shoot myself in the foot: not using it.Too often, I ruled out Postgres as a solution to a certain problem before even trying and jumped to more specialized solutions or moved the problem to the application layer.It took me years to stop underestimating what this awesome software can do."
61,fastball 10 months ago
61,| parent | next [–]
61,I am of the firm opinion that Postgres + Redis are basically the only DBs you ever need.
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"Agreed, and I'm over in the corner sharpening an axe, looking at Redis with highly malicious intent, too. I think many of the things people use Redis for could be accomplished with a small postgres server and a decent schema e.g. `create table redislike (id bigserial primary key, contents jsonb);`"
61,code_biologist 10 months ago
61,| root | parent | next [–]
61,"You can stuff some pretty insane stuff in Redis keys beside ints, giant multi MB strings with globby query patterns. Your general point is dead on."
61,electroly 10 months ago
61,| root | parent | prev | next [–]
61,"I definitely think they're great picks but I don't think the statement is making a particularly strong or interesting claim. I think it's equally true with basically any RDBMS in place of PostgreSQL. MySQL + Redis? Absolutely you'd be fine, tons of high-performance sites do this, probably more than use PostgreSQL. SQL Server + Redis? Still fine; you're Stack Overflow. Oracle + Redis? Weird choice but you'll still be fine. Also most companies probably don't need the Redis.So... what are we saying here, exactly? Are we saying that you don't need a no-SQL database, and that just SQL is enough with some Redis frosting on the cake to taste? I agree with that, but now we're not really talking about PostgreSQL any more, we're just debating SQL vs. no-SQL."
61,fastball 10 months ago
61,| root | parent | next [–]
61,"Well what I'm saying here is that SQL is great and sometimes it's genuinely helpful to have a much faster key-value store in a variety of scenarios.That's basically all you need.From there, it's PostgreSQL and Redis specifically because Postgres is the best SQL database and Redis is the best high-performance KV store."
61,wiredfool 10 months ago
61,| root | parent | prev | next [–]
61,"I think a columnar database can be super useful.I use Postgres a lot, but with a largish database, I managed to get the whole database footprint in Clickhouse smaller than the primary key index in Postgresql. (~80GB vs 160GB, and 1.2 TB for the whole unencrypted PG database) Now, it wasn't a great index key and the original schema wasn't good, but optimizing PG got me about a factor of 3 or 4, and Clickhouse compression got me another order of magnitude."
61,kevincox 10 months ago
61,| root | parent | prev | next [–]
61,And 95% of the time you don't even need Redis.
61,klysm 10 months ago
61,| root | parent | prev | next [–]
61,What do you use redis for?
61,white_dragon88 10 months ago
61,| root | parent | next [–]
61,Caching frequently fetched complex objects to mitigate load on the Postgres DB.
61,throwawaaarrgh 10 months ago
61,| root | parent | next [–]
61,"You could also cache those in the VFS or VMS. A cache is a cache, but one that is built into everything and doesn't require networking can be quite a bit less maintenance, risk, etc"
61,klysm 10 months ago
61,| root | parent | prev | next [–]
61,So are you caching the query results keyed by query or something higher up?
61,bigfatfrock 10 months ago
61,| root | parent | prev | next [–]
61,amen. I've run both in furious production conditions for over a decade with failures only caused by myself or other inept coders!
61,ak39 10 months ago
61,| prev | next [–]
61,"""2. Push all your application logic into Postgres functions and procedures""Why are functions and procedures (an abstraction layer at db layer) considered harmful to performance when the same abstraction layer will be required at the application layer (introducing out of process overhead and possibly network traffic)? I don't agree with this advice. (Or I don't understand it.)"
61,RowanH 10 months ago
61,| parent | next [–]
61,"Worst mistake I've ever made was implementing logic in the db - made for horrible debugging. It was only a few small bits of logic, but man, the amount of gotchas years later not realising something was there.."
61,certainly I think you either have to all/most of your logic in the DB or none. Definitely not a sprinkling..
61,chasd00 10 months ago
61,| root | parent | next [–]
61,"That's right, you either do all of it in the DB or none of it. Mixing the two makes long term maintenance complicated unless your overall solution is very well documented and the documentation is very well maintained. That's two rare ""very well""'s."
61,beckler 10 months ago
61,| root | parent | prev | next [–]
61,"I once worked for a place years ago that had these crazy data integrity bugs. There were about 35 different bugs, and we suspected that most of them were pieces of legacy code not cleaned up from acquisitions (it was a healthcare company).Anyways, if you were able to solve one, you got a $5k bonus and a two week vacation. While I was there, someone figured one of them out, and it was due to logic in the DB. ...but I have a feeling that all of them were due to logic in the DB."
61,orthoxerox 10 months ago
61,| parent | prev | next [–]
61,"1. It's much easier to debug concurrency issues when you use SPs, but much harder to debug anything else."
61,"2. At some point you will want to move some of your data into another system, and will have to pull the logic out into the application layer."
61,3. PL/pgSQL (or any other imperative SQL extension) isn't something you can find lots of devs on the market for.
61,"4. Upgrades and rollbacks are much more painful and require downtime.My team wrote a few critically important pieces of software that are running on Oracle, and here's why we did this:1. Concurrency issues were the biggest pain point that we tried to avoid. We still had to fix a lot of bugs in the logic itself, and debugging them without unit tests was painful"
61,"2. We were tightly integrated with another system written in PL/SQL. When we started on v2, an independent solution, I moved almost all logic out of the database except for the critical synchronization logic."
61,"3. We had a veteran team of PL/SQL devs in house. We still needed to get a subcontractor that wrote the API layer in Java, something PL/SQL isn't suited for at all."
61,"4. Upgrades and rollbacks were a pain, especially after we had to move to a 24x7 SLA that left us with no upgrade window. Oracle has edition-based redefinition, but Postgres doesn't."
61,j45 10 months ago
61,| root | parent | next [–]
61,"A middle ground that has had some success is managing a queue in Postgres that falls out business or application logic in the app, whether it’s micro service or monolith."
61,orthoxerox 10 months ago
61,| root | parent | next [–]
61,"Yes, that's basically what we ended up doing: a queue of tasks in Postgres that a variable number of workers could access via a clever SP that encapsulated all inter-task conflicts and spat out the next task you were allowed to process."
61,j45 10 months ago
61,| root | parent | next [–]
61,"There are some good business process management tools that could manage that mapping instead of the clever SP, but you have me intrigued.Are there any links that could be helpful to work through the programs and caveats of such a SP?Part of it for me is creating a table that can store the process in the database to be able to traverse it reasonably."
61,orthoxerox 10 months ago
61,| root | parent | next [–]
61,It was basically a more complicated version of this: https://stackoverflow.com/questions/68809885/how-to-select-f...
61,j45 10 months ago
61,| root | parent | next [–]
61,"Nice, thanks!"
61,vbilopav 10 months ago
61,| parent | prev | next [–]
61,"I don't understand it either.Author seems to be arguing against long functions/procedures. But if you move that to the client, presumably with ORM support - you're going to be executing more or less the same sequence of SQL queries and commands. Only difference is that when doing it on client you will have a lot of latency.Yes, you can cache some data in between those commands to avoid same multiple queries, but if you use temp tables to do so, they will use memory only if it is available, otherwise you are limited with the actual storage size.Only time I had memory issues with PostgreSQL when I used too much data in arrays and json's. Those are memory only variables. For example, I'd return a huge json to client and I'd run out of memory on PostgreSQL. I started streaming row by row and problem solved."
61,philbo 10 months ago
61,| root | parent | next [–]
61,"Fwiw the specific case which motivated that section in the post was a set of recursive functions we used to denormalise an irregular graph structure (so not suitable for CTE) into a single blob of JSON to be sent to another data store. 99% of the time there were no issues with this but at times of heavier load and on complex subgraphs, those recursive call stacks contributed to severe replication lag on the replicas they were running on.Moving the traversal logic into the application and just sending SQL queries to Postgres (we don't use an ORM) eliminated the lag. RTT between the application and the db was a few ms and this wasn't user-facing logic anyway, so extra latency wasn't an issue in this case.Probably the fundamental problem here was a sub-optimal schema, but sometimes you're just working with what you've got. Plus a commenter on Reddit pointed out that if we used pure SQL functions instead of PL/pgSQL, we'd also have seen better performance then."
61,ak39 10 months ago
61,| root | parent | next [–]
61,"""Probably the fundamental problem here was a sub-optimal schema, but sometimes you're just working with what you've got. Plus a commenter on Reddit pointed out that if we used pure SQL functions instead of PL/pgSQL, we'd also have seen better performance then.""So, would the better advice not have been to use simpler SQL instead of complex recursive statements, instead of taking a drastic approach to abandon ship (move logic to a completely new layer)?Also, if you're doing string concats manually for your Json, this might cause some overhead for larger objects. ??"
61,philbo 10 months ago
61,| root | parent | next [–]
61,"> So, would the better advice not have been to use simpler SQL instead of complex recursive statements, instead of taking a drastic approach to abandon ship (move logic to a completely new layer)?Probably, yep! But I didn't know that when I wrote it.I didn't want to give any concrete advice at all tbh. The entire rationale for the post was that I'm not an expert and I've broken prod in some surprising ways and if I share those ways maybe it will stop other people making similar mistakes in future. But I guess I over-stepped in my discussion for this mistake, sorry about that."
61,philbo 10 months ago
61,| root | parent | prev | next [–]
61,"> Also, if you're doing string concats manually for your Json, this might cause some overhead for larger objects. ??Good point, I hadn't considered that part of it. It wasn't string concats, we were building it with `jsonb_set`, but I can definitely see the JSON structure in memory as being part of the problem now you mention it (although maybe that reinforces the argument for doing it in the application layer)."
61,zeroimpl 10 months ago
61,| root | parent | prev | next [–]
61,"I’m fully against using triggers to implement business logic, but find stored procedures can be great for encapsulating some elements of"
61,"application/business logic. I’ve got several applications that access the same databases, and putting logic in the database avoids needing to duplicate it in clients.Most comments about debugability are nonsense. It’s just different, with some pros and cons. One simple example - if you have a bug in production, you’re not going to attach a debugger to your production application. But you can absolutely open a readonly connection to the database and start running queries, invoking functions, etc. It helps if you can architect your functions to distinguish pure readonly functions from those with side effects, but you can still debug even if that’s not the case."
61,senttoschool 10 months ago
61,| root | parent | prev | next [–]
61,Probably because you can't do proper testing as easy as application code. And debugging is much harder.
61,jci 10 months ago
61,| root | parent | next [–]
61,"I think I’m this is a commonly stated fact, but I don’t find it particularly true. Like any other technology, you just need to put in some initial effort to set up your test framework. In the case of PostgreSQL, pgTAP does a great job."
61,noisy_boy 10 months ago
61,| root | parent | next [–]
61,"It is commonly stated and I found it to be very true. PostgreSQL is quite advanced in its procedural aspects (Oracle isn't too far behind either) but they were not made with particular focus on debugging. I'll need to have hacks like creating temp tables to dump records at a given stage vs simply setting a breakpoint. I can unit test the shit out of bog standard Java code; PL/SQL for all its capabilities doesn't even come close. The one area this tilts to the other side is when you need to do heavy processing with high volume of data on database side; a well written stored proc would handily outperform application side logic simply due to the network latency involved. But for typical use cases, putting complex business logic in stored procs just isn't worth it."
61,vbilopav 10 months ago
61,| root | parent | prev | next [–]
61,"I disagree on both points.Edit: but I was referencing specific performances claims, that you will somehow take some load of database server. I just don't see it."
61,JoshuaRogers 10 months ago
61,| root | parent | next [–]
61,"The context here was that it’s not free, as I understood it. So, moving logic to the database, might make it faster, but that doesn’t mean that it’s instantaneous or that I no longer have to think about the scaling concerns of it.So, personally, I read that section as “logic in the database is not a zero cost abstraction."
61,spacebanana7 10 months ago
61,| parent | prev | next [–]
61,"In theory it's attractive to perform business logic in the DB with functions and procedures but in practice the ""devops experience"" is painful.Functions are restricted in their available languages, ability to A/B test and scale. There's also complexity entailed by having two sources of business logic because people can forget which one does what, needing to constantly switch back and forth between procedures and app code when debugging.Additionally the networking, resiliency and update patterns of databases are often not well suited to functions. You may want your functions to have public internet access but not you DB or rollbacks of your data but not function versions.All of these issues can be overcome by people who're confident DBAs and sysadmin types in addition to being application developers but that's a small group of people.I wish there were more startups in this area working to improve the developer experience of DB functions because you're correct about the superior performance and I'm pretty sure most of the issues I raised could be solved with well thought out tooling. However, at the moment such tools don't exist so these functions are painful to use."
61,maxloh 10 months ago
61,| parent | prev | next [–]
61,It is always more easier to scale horizontally at application layer (just adding more servers) than at database layer (which involves syncing data between multiple database instances).
61,Zanfa 10 months ago
61,| root | parent | next [–]
61,"In my experience more often than not, Postgres performance problems aren't really caused by the database, but either badly designed schemas or queries. For a lot of developers, the thinking goes that 10s of millions of rows sounds like a lot like big data, so they must start building microservices, distributed systems, use K/V stores and horizontally scale a la Google, whereas their entire dataset could actually fit in RAM of a single server and they could avoid the majority of the complexity if they just scaled vertically, which is usually much, much easier."
61,Merad 10 months ago
61,| root | parent | next [–]
61,I think many people underestimate the capabilities of SQL databases by a couple orders of magnitude.
61,I once worked on a feature that integrated tightly with a third party service.
61,"Their api didn't have any support for aggregate queries, and my company was smaller without real BI or data team, so I ended up writing a tool to dump our account data into a local Postgres db in order to do a some data analysis."
61,"By the time I left the company that db was approaching 50 GB, the table holding the primary data had about 40 million rows, and a couple of the supporting tables were over 100 million rows."
61,"This was all on a 2018-era Dell dev laptop - a fairly powerful machine (6 core/12 thread, 32 GB RAM, SSD), but certainly no server."
61,It took about 90 seconds to update the materialized views that summarized the data I looked at most frequently.
61,"More than acceptable for my use case, and there was a lot of room for improvement in that schema (it was pretty much a straight dump of the api data)."
61,smcleod 10 months ago
61,| root | parent | prev | next [–]
61,Came here to say exactly this. Over the last 12~ years working with PostgreSQL I've dealt with quite a few performance related issues - almost all were poorly written queries.
61,bakuninsbart 10 months ago
61,| root | parent | next [–]
61,Can you point to some good resources on how to write better postgres queries? Or give examples of common pitfalls?
61,bbojan 10 months ago
61,| root | parent | next [–]
61,Start with EXPLAIN ANALYZE then work from there. You can use tools where you paste the output of it and it shows you the data in a more easy to understand format.
61,vberg 10 months ago
61,| root | parent | next [–]
61,I am using https://explain.dalibo.com/ for that exact purpose and it does a great job highlighting the perfs issues.
61,Zanfa 10 months ago
61,| root | parent | prev | next [–]
61,I don't have any other good recommendations other than learning how to read and use EXPLAIN [1]. This should typically be the first tool when you have a slow query.[1] https://www.postgresql.org/docs/current/sql-explain.html
61,Ialdaboth 10 months ago
61,| root | parent | prev | next [–]
61,An old classic but too many indices can be harmful too.
61,branko_d 10 months ago
61,| root | parent | prev | next [–]
61,https://use-the-index-luke.com/
61,lordnacho 10 months ago
61,| root | parent | prev | next [–]
61,I would guess the most common pitfall is either not having indices or having the wrong kind of index for your query.
61,eastern 10 months ago
61,| root | parent | prev | next [–]
61,100%. Exaggerating the bigness of their own data is a common phenomena. Sometimes one is talking to a group of developers who are all so impressed with this bigness but every one of them has a phone in their pocket which could fit their entire dataset.
61,ak39 10 months ago
61,| root | parent | prev | next [–]
61,You can horizontally scale the database directly.
61,beebmam 10 months ago
61,| root | parent | next [–]
61,Depends on the scenario
61,postdb 10 months ago
61,| parent | prev | next [–]
61,"This comes a lot from people who want to ""horizontal"" scaling. The camp that thinks everything should be in the middle tier (Java/C#/). Also cost on AWS is cheap for those, and expensive for RDS. In the end db will be bottle neck. Of course DevOp ppl will can also create cache layer etc to lessen the stress to the db."
61,remus 10 months ago
61,| parent | prev | next [–]
61,"I think the article is kinda mixing two points here.One the one hand, it is sensible to try and keep all your business logic in one place (could be the database, could be the application) as spreading it across multiple places can make it hard to maintain.The current trend is to do your business logic in the application and treat the db as a data storage layer. The point in the article is that if you're using this model, but then stored procs and functions start creeping in to your db and it turns out they're actually doing some heavy lifting, then this can negatively impact the performance of your 'data storage layer' (which is actually not a data storage layer any more)."
61,citrin_ru 10 months ago
61,| root | parent | next [–]
61,Why splitting logic between Postgres and an application considered worse than splitting it between multiple micro-services? A DB is a storage service with INSERT/SELECT/e.t.c. as an API. Why we cannot extend this API to include stored procedures too? Indexes are commonly used to enforce data integrity. Why we cannot use triggers to do this even better?
61,KyeRussell 10 months ago
61,| root | parent | next [–]
61,"Comparing to microservices isn’t really apt, because of how you’re (meant to) slice service responsibilities."
61,postdb 10 months ago
61,| root | parent | prev | next [–]
61,"For sure, it doesn't matter where you put it if it is in one place. If one look at company like superbase, and their product like PostgresREST. It is just way faster way to develop API, and it will scale too. Often it is about how one 'horizontally' scale."
61,re-thc 10 months ago
61,| root | parent | next [–]
61,PostgresREST was invented way before Superbase was around. Just a FYI.
61,ciberado 10 months ago
61,| parent | prev | next [–]
61,"I've been out of the trenches for some time, but when I participated in projects that relied on heavily in store procedures, we felt constrained in terms of flexibility (the language options were very restricted, and we were not able to use common libraries), the tooling (the support in the IDE was not great, neither it was straightforward to debug the code) and the scalability (vertical, instead of horizontal). Also, this approach introduced a heavy coupling.It is true that we were much more familiar with application layer technologies, but the lack of expertise can also be considered a restriction, I think."
61,elp 10 months ago
61,| parent | prev | next [–]
61,While I agree with all the other commenters about debugging and scaling issues at least some of the time the stored procedure route can be very powerful.Stored procedures will eliminate insane levels of latency if there are many records to be updated in ways that are hard to do in application layer code without repeated calls the the db. I use them a lot for DB maintenance. Often for that kind of work its also a lot simpler and easier to reason with than app layer code.
61,dur-randir 10 months ago
61,| parent | prev | next [–]
61,"Pain to upgrade, double pain to a/b rollout, triple pain to debug."
61,marcosdumay 10 months ago
61,| parent | prev | next [–]
61,"One is better keeping heavy processing away from the database. Your application layer can scale almost indefinitely, and the main bottleneck for a random system is usually the database.As a rule, processing cost should give you a default bias into moving anything away from the database. Multiple sources, the need for temporary storage, and the existence of concerns that don't deal directly with your data should bias you more towards moving your code away from the database.On the other hand, data consolidation and enforcing non-local (to a record) rules should bias you towards moving your code into the database. If a lot of those happen, moving it there may even reduce the load of your database.Any one sided advice here is guaranteed to be wrong."
61,mamcx 10 months ago
61,| parent | prev | next [–]
61,"I work with all major RDBMS on the market (I integrate with ERPs/Accounting packages so even RDBMS that are niche and things that are a insult to call DBMS).ANYONE that have a problem with RDBMS ""functions and procedures & views (!)"" are invariably mishandling the RDBMS: Bad schemas, null refactoring in the DB after years/decades(!) of cruft, re-implementation, poorly, of things the RDBMS already have (like for example, date types), procedural doing stuff that SQL already do easier and in short time, too big SQL that never, ever, use VIEWS to abstract away, the RDBMS was never upgraded or is assumed never will so nothing of the new things inventing like 10 years ago is used.And that is a short list.---If you consider the RDBMS like the BEST programming language environment (sans SQL but still better than most languages for data!) and use the most BASIC ideas around it: like think a little about how do your schemas considering the queries you will do, some refactoring at least once every 5 years, pls!, use the CORRECT data types, pls pls!, use VIEWS pls pls pls!, etc your logic in triggers/functions MUST BE short and EASY."
61,matwood 10 months ago
61,| parent | prev | next [–]
61,"Like most things, it depends.Having application logic in the db is harder to debug and test (and possibly scale, but that also depends). But as you mention, it can be much faster if the logic is working on a lot of records.Also, IME, the data store often far outlives the original application. Having the logic tightly coupled to the data model means future applications are less like to break the assumptions made in the original data model."
61,hrdwdmrbl 10 months ago
61,| parent | prev | next [–]
61,"Think about it this way: you have to implement the same amount of business logic in any case. The only question being discussed here is where the work will be performed. The author is talking about scaling. If you have 3 server insurance but 1 database instance, it’s better (generally speaking) to put the logic in the server because there are 3 of them. That will scale better. In the case of Postgres, even if you have replicas, they’ll be read replicas. If you put everything in Postgres you are putting everything in the bottleneck. You can add more server instances but you can’t add more database writers."
61,icedchai 10 months ago
61,| parent | prev | next [–]
61,"I'd say because it is a much more specialized skill than programming in python / ruby / JS / or whatever your app language. Ideally, I would say ""use the best tool for the job"", which may very well be a stored procedure for data locality reasons, but practically speaking, with a larger team, you may be asking for trouble."
61,jaggederest 10 months ago
61,| prev | next [–]
61,"Sometimes you must `EXPLAIN ANALYZE` expensive queries in production, sadly. The behavior of postgres (even on non-bitwise copies) can be different under load.The biggest way I have seen this be true is with fragmented tables/indexes - same data but organized more compactly can change planner behavior.Article actually touches on another way that can be true - if your `work_mem` is drastically different the planner may prefer not to use an index, for example, and there are similar settings. Even with identical settings PG may choose different plans, it can be a bit tempermental in my experience, so sometimes you have to run the nonperformant query in offpeak production to understand performance."
61,Whitespace 10 months ago
61,| parent | next [–]
61,The article and the comments here don't make it clear why running it in production shouldn't be done.
61,"If slow_query is already running in production, why would running EXPLAIN ANALYZE slow_query be bad?Is the overhead of running EXPLAIN ANALYZE so much worse than running slow_query itself?"
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"No, it's really not, and that's why I say it must sometimes be done. Certainly if you're running tens or hundreds of copies of the query per minute, one more won't hurt (much).The real problem you run into is when the query in question is doing something pathologically bad - locking a bunch of tables and then grinding away for an hour, which effectively is a denial of service attack."
61,chasd00 10 months ago
61,| root | parent | prev | next [–]
61,"one thing to consider is the person who needs to run explain analyze may not have any access whatsoever to the production database. Also, there may be no process in place to get someone to run it on prod on their behalf. Finally, if there is a DBA on production they may just say no."
61,lswainemoore 10 months ago
61,| root | parent | prev | next [–]
61,One reason you might not want to run it in production is if it's not a read-only query.
61,williamdclt 10 months ago
61,| parent | prev | next [–]
61,"I don't _think_ the query planner takes ""current load"" into account.If you have:- Same resources (CPU/memory)"
61,"- Same settings (eg work_mem, amongst others)"
61,- Same dataset
61,"- Same/similar statistics (gathered with ANALYZE or the autovacuum)you should get the same results. If I'm wrong, please somebody correct me!"
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"You might be right, definitely if you run e.g. a fully cloned server on the same disk image and hardware it will behave the same, I'm thinking of times when memory pressure on the database from other queries caused caches to get evicted, things like that. It's not really the planner, it's the measured performance from lock contention and the data in memory etc."
61,samokhvalov 10 months ago
61,| parent | prev | next [–]
61,"What do you do if you need to check index ideas, or new table design?"
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"Generally indexes are cheap, if built concurrently, so I often build all the possible indexes (for relatively modest sized data types, load, and tables) and look at the planner statistics in production to validate which indexes are useful for the query load. That only works if you have a read-heavy usage pattern, for write-heavy usage patterns it can sometimes be better not to have an index at all (one of the things he alludes to in the article about missing foreign key indexes - indexes come with a write cost, so on a write-only table, foreign key indexes are a bad idea) but as with everything real timing data is the key.For new table design in Serious Business Databases I do two or three step: offline replica with the same data and settings (but not 100% identical), usually in a transaction on a logical replica or main production (postgres supports DDL transactions, which is a big help, but if you alter a table in a transaction it can block tons of other transactions).It's important to carefully construct your alter table / create table / create index statements to ensure that everything is concurrent and won't block, of course, and there are plenty of pitfalls there, but definitely doable."
61,glasshug 10 months ago
61,| root | parent | next [–]
61,Indexes can multiply your storage cost though.
61,sa46 10 months ago
61,| prev | next [–]
61,Some additional techniques for triggers I've found helpful:- Triggers for validation are awesome. Avoid triggers for logic if you can help it -- harder to debug and update than a server sending SQL and easier than you might think to cause performance problems and cascading triggers.- Use custom error codes in validation triggers and add as much context as possible to the message when raising the exception. Future you will thank you.
61,RAISE EXCEPTION USING
61,"ERRCODE = 'SR010',"
61,MESSAGE = 'cannot add a draft invoice ' || new.invoice_id || ' to route ' || new.route_id;
61,"- Postgres exceptions abort transactions, so if using explicit transactions, make sure you have a defer Rollback() so you don't return an aborted transaction to the server connection pool.- For better trigger performance, prefer statement-level triggers [1] or conditional before-row-based triggers.[1]: https://www.postgresql.org/docs/current/trigger-definition.h..."
61,keitmo 10 months ago
61,| prev | next [–]
61,"Here's one that bit us a few years ago:SEQUENCEs, used to implement SERIAL and BIGSERIAL primary keys, are not transacted. ""BEGIN; {insert 1,000,000 rows}; ROLLBACK"" always adds 1,000,000 to the table's primary key SEQUENCE, despite the ROLLBACK. Likewise for upsert (via INSERT ON CONFLICT).The end result: A table's SERIAL (32-bit signed integer) primary key can overflow even when it contains far fewer than 2^31 rows."
61,andorov 10 months ago
61,| parent | next [–]
61,this also matters if you do a lot of upserts on a table that are predominantly updates.
61,postgres requests an id from the sequence for each row of the incoming data ahead of time since it doesn't know which rows are updates and which are inserts.
61,the sequence doesn't reset down for the unused so this can eat through it unexpectedly quickly.if you hit the max integer for the sequence and need space to implement a fundamental fix you can quickly change the sequence to start at -1 and go down.
61,there's no issue with negative ids since they're also integers.
61,skeletal88 10 months ago
61,| parent | prev | next [–]
61,"But that is the point of serials, that they ignore transactions and are monotonically increasing."
61,hot_gril 10 months ago
61,| parent | prev | next [–]
61,Yes. Another reason I blindly use `bigserial` as the PK for everything.
61,natmaka 10 months ago
61,| prev | next [–]
61,Pertinent:
61,https://wiki.postgresql.org/wiki/Don%27t_Do_This
61,raspasov 10 months ago
61,| parent | next [–]
61,If only every piece of software had a page like that.
61,damagednoob 10 months ago
61,| parent | prev | next [–]
61,There's something ironic about having so many features that you have a dedicated page telling users which ones not to use.
61,ainar-g 10 months ago
61,| root | parent | next [–]
61,"PostgreSQL has been around for almost 27 years (and even longer, if you include the Ingress and Post-Ingress eras)."
61,"And things, well things, they tend to accumulate, to quote Trent Reznor, heh.On a related note, one could say that the C++ Core Guidelines[1] at least partially represent such a list."
61,"55 matches for “don't use” and 247 for “avoid”, although not all of them are about language features, obviously.[1]: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines"
61,PeterisP 10 months ago
61,| root | parent | prev | next [–]
61,"I think that every platform that's old and has backwards compatibility has to have such a page - because there inevitably will be features for which we (now!) know that there are better ways to achieve the same goal, but they have to stay there for compatibility reasons."
61,mbork_pl 10 months ago
61,| root | parent | prev | next [–]
61,"To be fair, some of them exist in PostgreSQL because it tries rather hard to conform to the SQL standard."
61,orthoxerox 10 months ago
61,| parent | prev | next [–]
61,I honestly expected the list to be longer.
61,moring 10 months ago
61,| prev | next [–]
61,"> 9: Compare indexed columns with IS NOT DISTINCT FROMDoes anybody know why this is the case? Usually, an index is not used if the semantics of the index do not match the semantics of the query, so ""using"" it cannot ever produce correct results. But the workaround presented seems to have identical semantics to IS DISTINCT FROM and still uses the index, so why isn't IS DISTINCT FROM using the index then?"
61,Diggsey 10 months ago
61,| parent | next [–]
61,"I asked this specific question before on the PostgreSQL IRC, and was told that it simply wasn't implemented. There's no huge technical blocker to it being done, it's just a bit awkward to make it work the way the code is structured AIUI."
61,singlow 10 months ago
61,| parent | prev | next [–]
61,"My first guess would be that IS NOT DISTINCT FROM considers nulls to match. Indexes might not contain null values. I don't know of postgres indexes are sparse or can be non-sparse? It would be hard to do use a sparse index and find null matches. I would expect this could be optimized away if the column is also not null, but then why would you use the operator in that case so maybe no one has handled it."
61,whakim 10 months ago
61,| root | parent | next [–]
61,"Indices in Postgres do contain NULL values. My guess is that it's possible to make IS DISTINCT FROM an indexable operator, but it wouldn't be straightforward given that NULLs are a bit of a special case for indexing comparisons in the sense that they totally ignore data types. IS DISTINCT FROM would probably have to redefine a bunch of existing opclasses to account for the possibility of NULL."
61,singlow 10 months ago
61,| root | parent | prev | next [–]
61,"Hmm - actually read some docs and I think that the implementation of IS NOT DISTINCT FROM was originally implemented one way when nulls were not indexed, but then hasn't been fixed since null indexing was implemented."
61,hans_castorp 10 months ago
61,| parent | prev | next [–]
61,> Does anybody know why this is the case?Most of the time the answer to that is: because nobody cared enough or had time enough to implement it
61,moring 10 months ago
61,| root | parent | next [–]
61,"That might be the case, but my experience with databases (and especially PostgreSQL) is that most of the time I actually misunderstood the exact semantics of either the operation or the index. That would be a good chance to learn something :)"
61,albertopv 10 months ago
61,| prev | next [–]
61,"Two years ago I moved to a new company using Postgres as THE relational db, coming from years of Sql Server I found poor query plan issues troubleshooting tools.Anyway, I don't know if it's the same in Postgres, but in Sql Server an OR condition like that could kill your performance quite easily in a relatively complex query, often I had to refactor my query to a UNION (usually with ALL to avoid a distinct step, but it depends on the specific case)."
61,baq 10 months ago
61,| parent | next [–]
61,"Similar in postgres, depending on version.SQL Server is a damn fine DB if you can afford it. Highly recommended."
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"SQL Server is one of the descendants of Ingres, and PostgreSQL is, as the name might suggest, the successor project for database research after Ingres. They're both great databases really, it's a fun connection in their mutual history."
61,albertopv 10 months ago
61,| root | parent | next [–]
61,"Wow, I didn't know!"
61,thomasjudge 10 months ago
61,| root | parent | prev | next [–]
61,"Sybase, actually"
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"The first Sybase grew out of the Ingres project at UCB, so transitively, SQL Server is also a descendant."
61,johnthescott 10 months ago
61,| root | parent | next [–]
61,bob epstein was vp at brittion-lee when he left to form sybase.
61,"BLI built a relational database machine (IDM), which was influenced by ingres but not much was inherited, code wise."
61,"sybase used a VM/pcode architecture, very much not like ingres.https://www.google.com/search?q=britton+lee+inc+wikipedia"
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"Neat, thanks for elaborating, I knew BLI grew out of the ingres project but not how closely they were related."
61,dx034 10 months ago
61,| root | parent | prev | next [–]
61,"I second that. I found it by far the most pleasant database to work with, including its tooling. Postgres is probably second. Too bad SQL Server is so expensive."
61,sohnakukkar 10 months ago
61,| root | parent | next [–]
61,How much SQL Server costs?
61,vikingerik 10 months ago
61,| root | parent | next [–]
61,"You can google it, but the short answer is Enterprise Edition is on the order of tens of thousands of dollars per CPU."
61,dx034 10 months ago
61,| root | parent | next [–]
61,"Maybe they'll change it at some point. I believe if SQL Server was free, it would dominate the business. I last used it 7+ years ago and back then it was (from my subjective experience) better than Postgres is now. And I love Postgres. But SQL Server was a dream to work with. Feature rich with amazing tooling. You can get most features in Postgres with plugins and manual work, but SQL Server does it all for you."
61,SigmundA 10 months ago
61,| parent | prev | next [–]
61,"Compared to Sql server, PG's lack of true clustered indexes and no query plan re-use was very surprising, also no hints to wrangle a bad query plan!"
61,olavgg 10 months ago
61,| root | parent | next [–]
61,And SQL server's lack of backup with SSH piping makes it basically pain to administer.
61,joelrwilliams1 10 months ago
61,| parent | prev | next [–]
61,I've run across this in Oracle and MySQL as well...and--same solution as you--have split out the or conditions using union all.
61,hot_gril 10 months ago
61,| parent | prev | next [–]
61,"Postgres can optimize OR pretty well, but not always."
61,rwmj 10 months ago
61,| prev | next [–]
61,The main tip I learned from using PostgreSQL (or relational databases in general) is never use an ORM.
61,They cause far more trouble than they are worth and it's far easier to see what is going on when you're writing SQL queries directly.
61,sodapopcan 10 months ago
61,| parent | next [–]
61,ORMs can be great in many situations. Any ORM worth its salt has a way to spit out exactly what SQL it is generating.
61,"I have worked on apps, though, where we end with a disproportionate amount of raw SQL so obviously in those scenarios they become useless."
61,"It's almost always been writing some sort of reporting system in Postgres instead of using a column store, though."
61,emaginniss 10 months ago
61,| parent | prev | next [–]
61,"I am strongly biased against ORMs, but I still recognize their use in CRUD operations."
61,"In any case where you are doing bulk operations, true upserts, or more complex queries, then I 100% agree."
61,hot_gril 10 months ago
61,| parent | prev | next [–]
61,"It's not just the fact that it hides the queries. ORMs are all-around cancerous. I've been on several teams that's tried to use one, and there were regrets every time."
61,hot_gril 10 months ago
61,| root | parent | next [–]
61,"And by regrets I don't mean ""this ORM wasn't worth,"" I mean ""the ORM ruined everything to the point where we have to rewrite it."""
61,aeyes 10 months ago
61,| parent | prev | next [–]
61,"This is ancient knowledge and I would have agreed with you 15 years ago, today the only reason to not use an ORM is analytical queries.Since the Postgres planner doesn't really allow you to tune your query there aren't many ways to construct your query in a way which would to a much worse execution plan. Over the years we have migrated most raw SQL back to using the ORM without taking performance hits, pretty much the only remaining raw queries are CTEs where we force a certain order of query execution.Usually these ORM problems are caused by schema design anyways. If you need 10+ joins you are going to have a hard time with or without an ORM."
61,klysm 10 months ago
61,| root | parent | next [–]
61,Do CTEs actually force an order of execution? I thought the point was that they are declarative
61,aeyes 10 months ago
61,| root | parent | next [–]
61,"Before Postgres 12 yes - always, starting with Postgres 12 you have to add ""AS MATERIALIZED"" to force the order."
61,eitland 10 months ago
61,| parent | prev | next [–]
61,"Might be a good idea if you arr the only one on the team or if everyone is like you.If not, be prepared to check for sql injection vulnerabilities the first n PRs from any new team member.Also to explain how to do it and your reasoning for it."
61,rwmj 10 months ago
61,| root | parent | next [–]
61,"Kind of goes without saying that any framework/library you use must not allow you to write SQL injection vulnerabilities, and if it does you should stop using it right now."
61,steve-chavez 10 months ago
61,| root | parent | next [–]
61,"SQL injection is always possible with an ORM, since they always allow executing raw SQL as an escape hatch."
61,eitland 10 months ago
61,| root | parent | next [–]
61,"True. But unlike with the alternative that many end on, raw sql, it doesn't funnel anyone into a place where writing"
61,"""select * from sometable where id="" + id"
61,feels like a logical next step unless you actually have studied the subject or read the manual ;-)
61,tough 10 months ago
61,| parent | prev | next [–]
61,"I dunno, sounds like a great way to get rekt to deal with SQL queries directly, there's some ORM's that let you do this when needed but default to the ORM DSL when needed.Im a happy user of prisma, I value It supports several databases and not only pg. using for example sqlite for localhost dev has its perks, and it's easy to move later to other stuff if you have already planned for it and not using types that are incompatible between your two targets"
61,bagol 10 months ago
61,| parent | prev | next [–]
61,"Anyone not using ORM, will eventually build his own ORM (or at least query builder). I think the argument is not about ""not using ORM"", but more about ""not using ORM made by someone else""."
61,alberth 10 months ago
61,| prev | next [–]
61,This configuration tuner will help address some of the articles points.https://pgtune.leopard.in.ua/
61,re-thc 10 months ago
61,| parent | next [–]
61,If only there were profiles for popular cloud providers.
61,oofnik 10 months ago
61,| prev | next [–]
61,"At my first DevOps job we had defined a function which deleted old partitions from time to time. If you invoked it manually because of some automation failure with the wrong arguments, it had a habit of nuking production data, which, needless to say, happened at least once. Naturally, the function was called footgun.We can pretend that the proliferation of managed databases, newfangled NoSQL datastores, and other abstractions preclude the need for accumulated empirical observation across a range of tech stacks and time, but sometimes there's really no substitute for greybeard wisdom."
61,matthijs 10 months ago
61,| prev | next [–]
61,"Using `truncate` in combination with `cascade` is another that I found unexpected:""Automatically truncate all tables that have foreign-key references to any of the named tables, or to any tables added to the group due to CASCADE.""So it will simply clear out other tables that reference table to truncate, even if you have `on delete set null` and the foreign key column is null.https://www.postgresql.org/docs/current/sql-truncate.html"
61,jaggederest 10 months ago
61,| parent | next [–]
61,"Another footgun is that, while `DELETE FROM table_name` is transactional, TRUNCATE is not transactional. Once you push the truncate button, that data is gone in every transaction everywhere all at once.You should be very hesitant about using TRUNCATE on a production database unless that table (and all related foreign keyed tables) are truly ephemeral. Even if the data is cleared every night at midnight, for example, is there going to be a 10 second analysis transaction running across the midnight boundary that will fail hard with suddenly missing rows?Running a full delete on the rows and vacuum will still result in a tiny file on storage and doesn't wake me up in a cold sweat when I have a flashback. Even renaming the table is in many ways safer."
61,hans_castorp 10 months ago
61,| root | parent | next [–]
61,"TRUNCATE is absolutely transactional. You can rollback a TRUNCATE statement if you run it in a transaction.https://dbfiddle.uk/xkgzxMUUThe only difference to other DML statements is, that it will put an exclusive lock on the table. So until the TRUNCATE is committed, no other transaction can read from the table."
61,jaggederest 10 months ago
61,| root | parent | next [–]
61,"> TRUNCATE is not MVCC-safe. After truncation, the table will appear empty to concurrent transactions, if they are using a snapshot taken before the truncation occurred.Sorry, that's what I mean. It's safe in the sense that you can roll it back, but it's not safe in the sense that other concurrent transactions will see the table as empty if they are long-running."
61,petepete 10 months ago
61,| root | parent | prev | next [–]
61,> The only difference to other DML statements isTruncate is DDL. It's like dropping and recreating the table in a single operation.
61,CodeCompost 10 months ago
61,| prev | next [–]
61,"Postgres doesn't automatically create indexes for foreign keys. This may come as a surprise if you're more familiar with other databases, so pay attention to the implications as it can hurt you in a few ways.I don't know of any database system that does this. In the case for SQL Server, the foreign keys usually get added by the ORM layer (Entity Framework migrations if you're using dotnet)."
61,berkut 10 months ago
61,| parent | next [–]
61,MySQL InnoDB does I believe...
61,singlow 10 months ago
61,| root | parent | next [–]
61,It will create an index if no existing index meets the requirements of the foreign key.
61,ahachete 10 months ago
61,| prev | next [–]
61,"The recommendation for work_mem doesn't account for all the possible cases. It is already noted elsewhere on this thread [1] that the use of memory per connection could be higher than work_mem, and this is true even if you don't use stored procedures, as the memory incurred can be on a per-query node. So it can be a multiple of work_mem per connection.But there's a factor that even worsens this: parallel query, which is typically enabled by default, and will add another multiple to work_mem.Tuning work_mem is a hard art, and requires a delicate balance between trying to optimize some query's performance (that could avoid touching disk or using some indexes) vs the risk of causing db-wide errors like OOMs (very dangerous) or running out of SHM (errors only on queries being run, but still not desirable). So I normally lean on being quite conservative (db stability first!) so I divide the available memory (server memory - OS memory - shared_buffers and other PG buffers memory) by the number of connections, also divided by the parallelism and by another multiple factor --and then leave some additional room.In any case I'd recommend reviewing the detailed information, suggestions and links on the topic on postgresqlCONF [2] (disclaimer: a free project built by my company)[1]: https://news.ycombinator.com/item?id=35697986[2]: https://postgresqlco.nf/doc/en/param/work_mem/(edit: formatting)"
61,hot_gril 10 months ago
61,| prev | next [–]
61,"The two biggest ones imo were not mentioned:1. Contrary to popular belief, Postgres isn't fully ACID (specifically the ""I"") with the default isolation mode. For example, selecting the sum of a column then inserting conditionally on that creates a race condition. Serializable mode is fully isolated, but it has many caveats and shouldn't be used often, so you should instead become familiar with what's isolated and what isn't. See https://www.postgresql.org/docs/current/transaction-iso.html2. timestamp (without time zone) is bad; always use timestamptz, no exceptions. Unintuitively, timestamp (without time zone) is the one that makes your DB's time zone affect your selected data. Neither one actually stores a time zone, it's just a difference in output formatting. This is a moot point if your DB's locale is set to UTC, but that's not the default."
61,klysm 10 months ago
61,| parent | next [–]
61,> it has many caveats and shouldn't be used oftenI’d argue it isn’t used enough given its isolation advantages
61,hot_gril 10 months ago
61,| root | parent | next [–]
61,"Serializable mode is only useful in low throughput cases. You take a big performance hit and can easily have a bunch of queries waiting one each other until the queue fills up. I haven't used it in forever, I just make sure my stuff works with the default isolation mode. You also can't slap it on like a band-aid for unsure users. The DB client has to implement retry logic, which most Postgres libs don't seem to have, and the code has to careful not to cause side effects inside the retriable area.Spanner and some other DBMSs are fully serializable always. They do a better job of that than Postgres, I guess cause they focus on it, but they're still a lot slower in the end.What would be cool is some extension to watch your non-serializable transactions for concurrency anomalies like 1% of the time and use that tiny sample to alert you of race conditions. Like a thread sanitizer does for code. Does that exist??"
61,klysm 10 months ago
61,| root | parent | next [–]
61,"Yes you take the performance hit, but many workloads are low enough throughput for it to be completely fine.I’m not sure how you could detect such anomalies though without running the same serialization checking logic within postgres"
61,hot_gril 10 months ago
61,| root | parent | next [–]
61,"Actually I agree. There are probably lots of low-throughput, high-consequence things like that.For the sampling, you'd have to run the same logic. The idea is just to do it infrequently as to not totally ruin your overall performance. Idk if this would work."
61,klysm 10 months ago
61,| root | parent | next [–]
61,"It’s definitely an interesting idea, I wonder if it’s easy to implement such a thing."
61,Mavvie 10 months ago
61,| prev | next [–]
61,"> Setting acquired_at on read guarantees that each event is handled only once. After they've been handled, you can then delete the acquired events in batches too (there are better options than Postgres for permanently storing historical event data of unbounded size).This bothers me. It's technically true, but ignores a lot of nuance/complexity around real-world event processing needs. This approach means you will never be able to retry event processing in case it fails (or your server is shut down/crashes). So you either have to update the logic to also process events where ""acquired_at is older than some timeout"", which breaks your ""handled only once"" guarantee, or you can change to a SELECT FOR UPDATE SKIP LOCKED approach which has its own problems like higher database resource usage (but at least it won't process a slow job twice at the same time)."
61,philbo 10 months ago
61,| parent | next [–]
61,"Yep, a few people have mentioned this to me here and on Reddit. I didn't know about the issues with the approach I proposed, so was pleased to read the comments. Will add a correction to the post as soon as I have a sec, thanks."
61,Mavvie 10 months ago
61,| root | parent | next [–]
61,"Thank you! It was a great article, and definitely pointed out a few things I'm doing wrong in my Postgres setups."
61,Calamitous 10 months ago
61,| prev | next [–]
61,"Most common one I’ve seen in the last 5-10 years: using a JSON column instead of putting in a lookup table, or instead of properly analyzing and normalizing your data.That’s a mistake that you’ll be paying for for a while."
61,lopatin 10 months ago
61,| parent | next [–]
61,How much slower is it in your experience?
61,pramsey 10 months ago
61,| root | parent | next [–]
61,"If you let the JSON blobs grow past the page size the cost of recovering TOAST tuples can be 10x reading main storage. Also JSON is fundamentally de-normalized so you can incur scan and read costs just from hauling out duplicate values where a nice normalized lookup would be snappy. And finally JSON recovery is going to pull the whole object every time, even though you are probably only interested in one element of the object, so again, higher recovery times compared to an equivalent normalized model."
61,Calamitous 10 months ago
61,| root | parent | prev | next [–]
61,"It’s not even the speed, it’s writing queries. The syntax is clunkier, and since you don’t have a nice clean schema, you end up doing a lot of jumping through hoops to check for the presence of keys/proper data types, etc."
61,Or doing your basic data validation in your app instead of letting the database do its job.
61,afhammad 10 months ago
61,| prev | next [–]
61,"From my understanding, `work_mem` is the maximum available memory per operation and not just per connection. If you have a stored procedure with loops and/or many nested operations, that can quickly get quite big.One trick worth noting, is that you can override the working memory at the transaction level. If you have a query you know needs more memory (e.g doing a distinct or plain sorting on a large table), within a transaction you can do:`set local work_mem = '50MB'`That will override the setting for operations inside this transaction only."
61,philbo 10 months ago
61,| parent | next [–]
61,"This is a great tip, I had no idea there was `set local work_mem`. Thanks!"
61,jasfi 10 months ago
61,| prev | next [–]
61,"PL/pgSQL needs some styling improvements:1. Get a better name, PL/pgSQL doesn't exactly roll off the tongue.2. Get rid of those $$ at the start and end of any PL/pgSQL, it's just verbose and ugly."
61,hans_castorp 10 months ago
61,| parent | next [–]
61,"Get rid of those $$ at the start and end of any PL/pgSQL, it's just verbose and ugly.The Postgres parser itself doesn't know the language rules and thus can not parse the source code. The body of the function/procedure is passed to the language handler as a string without Postgres looking at it. The procedural code (PL/pgSQL, PL/python, PL/perl, plv8, ...) will not be parsed until the function/procedure is actually executed.Today there might be better ways to implement such a dynamic system to register new languages, but I guess it will be huge effort to change this to a way that would understand the old and the new syntax.Since Postgres 14, at least ""LANGUAGE SQL"" functions/procedures can be written without using dollar quoting if the SQL/PSM syntax is used."
61,jaggederest 10 months ago
61,| parent | prev | next [–]
61,"Dollar quoting is just the easiest way to write strings with lots of escape characters, it's not required (I think it's the best way to do pl/pgsql though). You can also tag dollar-quoted strings:https://www.postgresql.org/docs/current/sql-syntax-lexical.h..."
61,Topgamer7 10 months ago
61,| prev | next [–]
61,I found one yesterday. Names can only be like 62 characters long. Then it silently truncates then. At least when creating triggers that is.
61,claytongulick 10 months ago
61,| prev | next [–]
61,"My rule of thumb is to use DB functions for writes that require good data consistency, and for reads that are join heavy.For things like formatting and transforming data, I prefer to have that work done on the client (web browser) if it's for a presentation concern, or middle tier if it's for an integration. My theory there is ""don't make the database work hard"". Use it for what it's great at: fast reads and data consistency.DB functions and sprocs are a great way to reduce latency and load, and boost performance if used for the right things.An easy rule of thumb is that if my function isn't using SQL as the language, I need to really think about whether it belongs in the DB.That all being said, I do use udfs to construct JSON results for some reports also, when it makes sense based on the params and number of round-trips to the server it would cause in the middle or UI tiers. Even though it violates some of the things above, it's really damn convenient and usually limited to low-traffic queries."
61,philsnow 10 months ago
61,| prev | next [–]
61,A writing tip:
61,"even in lists of ""don't""s like this, find a way to write directives/imperatives in a positive sense.Asking readers to keep mentally flipping the sense of the thing you're telling them to do just adds cognitive load and makes it harder for them to pay attention to what you want them to pay attention to.Write ""do""s, not ""don't""s."
61,philbo 10 months ago
61,| parent | next [–]
61,"Yeah, a lot of people have said this. I was wary of asserting ""do's"" because I'm not a Postgres expert. It felt more honest to phrase stuff in terms of my own mistakes and ""don't do what I did"", but of course that's confusing. And then I ruined it by still asserting ""do's"" in the bodies of some sections."
61,hot_gril 10 months ago
61,| root | parent | next [–]
61,"I think the ""don't""s are fine, but each bullet should say ""don't"" in that case."
61,philsnow 10 months ago
61,| root | parent | next [–]
61,"That would also work well, yeah"
61,blocko 10 months ago
61,| parent | prev | next [–]
61,"I mostly agree, but there is one slight benefit: whenever I read one of these articles, each topic acts as a little quiz where I get to test whether I think it's a do or a don't before seeing the explanation"
61,hot_gril 10 months ago
61,| parent | prev | next [–]
61,Glad I wasn't the only one confused by this.
61,reese_john 10 months ago
61,| prev | next [–]
61,> With that in place you could acquire events from the queue like so:
61,UPDATE event_queue
61,SET acquired_at = now()
61,WHERE id IN (
61,SELECT id
61,FROM event_queue
61,WHERE acquired_at IS NULL
61,ORDER BY occurred_at
61,LIMIT 1000 -- Set this limit according to your usage
61,RETURNING *;Would you need a FOR UPDATE in that subquery?
61,Mavvie 10 months ago
61,| parent | next [–]
61,"No, because they're setting acquired_at which marks it as ""handled"". You only need FOR UPDATE (SKIP LOCKED) if you want to process the event inside a transaction; but the approach in the article is to just bulk ""grab the events and mark them as done, atomically"""
61,deusex_ 10 months ago
61,| prev | next [–]
61,Another distinct set of similar tips that I found useful https://medium.com/productboard-engineering/eight-tips-to-re...
61,lbriner 10 months ago
61,| prev | next [–]
61,"One thing that caught me out is that if you are doing an operation on a string like ""lower(email)"" then the query planner will not use an index on the email column, instead you would need an index on ""lower(email)"", which is fine if you always access it in the same way but otherwise requires multiple indexes to get coverage in all your scenarios.There are also plenty of other weird planner choices which I can't work out but which were much easier to understand in SQL Server. Sometimes, the smallest change will stop using an index e.g. using LIMIT on a query can completely bypass an index."
61,dathinab 10 months ago
61,| prev | next [–]
61,one bad surprised I had had was people but understanding transactions.Or more specifically transaction isolation levels.
61,fbn79 10 months ago
61,| prev | next [–]
61,I'm using last stable postgres but I can confirm you that point 7 still exists. here my case https://dba.stackexchange.com/questions/323960/query-using-v...
61,xnickb 10 months ago
61,| prev | next [–]
61,"The ""solution"" for the number 4 is a pretty bad footgun in itself, as it is obviously prone to race conditions.Didn't read further."
61,philbo 10 months ago
61,| parent | next [–]
61,"Yep, a few people have made the same comment. I'll make a correction to the post soon, sorry that it upset you. As obvious as it seemed, I had no idea.Thanks for pointing it out though, feedback from experts is the big payoff to posting from my p.o.v. and now I know a thing which I didn't know before."
61,dpedu 10 months ago
61,| parent | prev | next [–]
61,Even within a transaction?
61,topmax 10 months ago
61,| prev | next [–]
61,What about `pg_notify`? I just want to use it to replace my kafka server which is lite overload but costs much.
61,MrPowerGamerBR 10 months ago
61,| parent | next [–]
61,"If you are using NOTIFY/LISTEN, keep track to check if your database does not have any long running queries. If you end up getting PostgreSQL to vacuum freeze your tables while the long running query is active, PostgreSQL will delete files from the pg_xact folder and that will bork out any LISTEN query, until you fully restart the database."
61,philsnow 10 months ago
61,| root | parent | next [–]
61,"> PostgreSQL will delete files from the pg_xact folder and that will bork out any LISTEN query, until you fully restart the databasethat sounds like a bug; are you aware of an issue/ticket tracking that?"
61,MrPowerGamerBR 10 months ago
61,| root | parent | next [–]
61,"While I haven't found an ""issue"" talking about this (sorry, I don't know how PostgreSQL tracks open bugs), they do know about it since I already seen that issue being talked about on PostgreSQL's mailing list.Here's my issue on StackExchange, for anyone that wants to delve deeper into my issue: https://dba.stackexchange.com/questions/325104/error-could-n...Here's an thread talking about the issue, while OP's issue doesn't seem to match exactly what I was experiencing, one of the replies describes my exact issue: https://postgrespro.com/list/thread-id/2546853"
61,klysm 10 months ago
61,| parent | prev | next [–]
61,Take a look into logical replication. You can even send messages over it
61,NegativeLatency 10 months ago
61,| prev | next [–]
61,Any good guides like this for MySQL? Unfortunately having to use it instead of Postgres at my current job.
61,grafelic 10 months ago
61,| parent | next [–]
61,"Check out https://www.percona.com/blog (filter for MySQL)I have been a heavy user of the Percona distribution of MySQL for many years and highly recommend it. I think that most of the MySQL articles on percona.com are relevant for other MySQL distributions.As a side note, depending on the software stack, ProxySQL can have a major positive impact on performance and scalability."
61,Scarbutt 10 months ago
61,| parent | prev | next [–]
61,"They seem pretty generic, the same ideas/concepts can probably be taken over to mysql."
61,lbriner 10 months ago
61,| root | parent | next [–]
61,This is definitely not true. They are not generic at all.* work_mem is postgres specific* stored procs and functions don't perform as badly in SQL Server* Triggers behave differently* NOTIFY is postgres specificetc.
61,peter_retief 10 months ago
61,| prev | next [–]
61,Handed down to me by the grey beards!
61,I like that.
61,"Postgres default settings are not the best, I remember moving from mysql to postgres some time back and discovered this."
61,I always wondered why the default would not work best for most deployments?
61,Anyone know why this is so?
61,eqefqe 10 months ago
61,| prev | next [–]
61,Things that Postgresql should address:- Start compiling queries and functions to native code and hash them just like other DBs- Improve connection scalability by using tasks instead of threads (each thread needs a lot of RAM!)- Automatically maintained clustered index (like in SQL Server) would be nice to have
61,xupybd 10 months ago
61,| prev | next [–]
61,These are some good tips but I've not hit these performance issues. I work on smaller scale applications. One has been in production since 2012 the database performs very well. I guess I need to get out and work for bigger companies to experience this.
61,murkt 10 months ago
61,| parent | next [–]
61,"One doesn’t need to work for bigger companies to have lots of data these days. Has been true for many years already.In our Postgres DB we have more than 4 TB of data, which I don’t think is too big. We didn’t need any special sauce, no vendors chiming in, only a better understanding of the technology than average Joe.On the big company part - I have yet to employ more than five developers."
61,winrid 10 months ago
61,| parent | prev | next [–]
61,"Yes, there's nothing quite like the query planner deciding to try something new and suddenly 100 application servers are DDOSing your primary :)"
61,turtles3 10 months ago
61,| root | parent | next [–]
61,"This may be irrational but it's something that worries me about using postgres in production. Sure as a developer I love all the features, but the fact that the query planner can suddenly decide on a radically different (and incredibly inefficient) query plan makes it hard to trust. In some ways, a dumber query planner that needs coercing into the right query plan is more reassuring, in that you know it'll keep using that query plan unless explicitly told otherwise."
61,funcDropShadow 10 months ago
61,| root | parent | next [–]
61,But that dumber query planner will bite you when your data changes. If the relative size of multiple tables change the query might have to change to be still efficient. Postgres query planner handles that just fine. I've used Postgres for years multi TB databases and I've experienced a problem with Postgres suddenly changing plans.
61,klysm 10 months ago
61,| root | parent | next [–]
61,Is there a “never” missing from your last sentence?
61,funcDropShadow 10 months ago
61,| root | parent | next [–]
61,Yes :( And it is too late for editing it.
61,winrid 10 months ago
61,| root | parent | prev | next [–]
61,"haha, this example is actually from Mongo, but it's pretty rare."
61,jwsteigerwalt 10 months ago
61,| prev | next [–]
61,"This list conceptually applies to just about any RDBMS. NOTIFY is incredibly powerful and when used prudently, can be a fantastic solution to some business problems."
61,18 more comments...
61,Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
61,Search:
62,"PostgreSQL High Performance Tuning Guide - SkillmapperCourse245220Save to CollectionPostgreSQL High Performance Tuning GuideCourse ComparisonOverviewSyllabusTopicReviewRelated ContentSkills RelatedmysqlpostgresqlnewsqlFeedback RecapPractical and focused on Evaluations9.49%Good and logic Introduction9.23%Helpful Guide, Beginner Friendly8.88%Applied fundamentals and worthy instructor7.97%Knowledge and Experience Teaching7.71%OverviewThis Udemy course is designed to help users build a dynamic database solution"
62,using the latest PostgreSQL releases. and will discuss how best to use
62,"indexes, bitmap scans, and other postgreSQL optimization techniques. The"
62,"course includes learnings from users about how best to use indexes, bitmap"
62,"scans, and other postgreSQL optimization techniques, and how to use these"
62,"techniques effectively.SyllabusUnderstanding PostgreSQL Server ArchitectureConfiguring Vacuum for PerformanceHow to use an Index efficientlySee MoreReviewsMost FavorableLeast FavorableWorst review Some students found it difficult to focus on the course. The sound quality is bad, and the transcript is far from accurate, which affected their learning experience. Improvement is needed in these areas.Some students found it difficult to focus on the course. The sound quality is bad, and the transcript is far from accurate, which affected their learning experience. Improvement is needed in these areas.17.99 USDGo to courseThis Course Includes:SkillMapper rating:78%Start date:Self-PacedAmount of students:4.6Kduration:0 hoursDownloadable resources:51Certificate of completion:Share this course© SkillMapper SASPrivacy PolicyTerms and use"
63,"Tuning Autovacuum for best Postgres performance · pganalyzeTuning Autovacuum for best Postgres performanceLearn the most important concepts of why and when Postgres has to vacuum, why that matters, and when it’s best to tune the default settings that control autovacuum scheduling, vacuum overhead, and more.Download this eBookHundreds Of Companies Monitor Their Production PostgreSQL Databases With pganalyzeContactFAQPlans & PricingResourcesEnterpriseDocumentationCustomer StoriesCareersIndex AdvisorSecurityStatusCompare pganalyzeTerms of ServicePrivacy PolicyNewsletterSOC 2 Type 2Do Not Sell My Personal InformationGet in touch Email us anytime for questions:  support@pganalyze.comSOC 2 certification pganalyze is SOC 2 Type 2 certified© 2023 Duboce Labs, Inc."
64,Performance Tuning (Linux) | EMQX Documentation
64,"Skip to content EMQX DocsSearchKMain Navigationemqx.ioEnglish简体中文GitHubEnglish简体中文AppearanceGitHubContact UsGet StartedMenuReturn to top Sidebar Navigation Quick StartWhat's NewFeature ComparisonGet StartedOperating LimitationsFAQTechnical SupportProduct RoadmapInstallation and MigrationDeployDebianUbuntuCentOS/RHELmacOSKubernetesInstall from Source CodeUpgrade GuideRolling UpgradeUpgrade EMQX Cluster from 4.4 to 5.1Upgrade EMQX on KubernetesDeveloper GuidePublish/SubscribeMQTT Core ConceptsTest with MQTT ClientsMQTT Shared SubscriptionMQTT Retained MessageMQTT Will MessageExclusive SubscriptionDelayed PublishAuto SubscribeTopic RewriteWildcard SubscriptionClient SDKConnect via C SDKConnect via Java SDKConnect via Go SDKConnect via Python SDKConnect via JavaScript SDKREST APIAPI DocsEMQX ClusteringArchitectureCreate and Manage ClusterCluster SecurityConfigure Load BalancerLoad Balance EMQX Cluster with NginxLoad Balance EMQX Cluster with HAProxyPerformance and TuningPerformance Tuning (Linux)Performance Test with eMQTT-BenchPerformance Test with XMeter CloudTest Scenarios and Results for ReferenceSecurity GuideNetwork and TLSEnable SSL/TLS ConnectionClient TLS Connection Code ExamplesObtain SSL/TLS CertificatesCRL CheckOCSP StaplingAuthenticationX.509 Certificate AuthenticationJWT AuthenticationPassword basedUse Built-in DatabaseIntegrate with MySQLIntegrate with MongoDBIntegrate with PostgreSQLIntegrate with RedisIntegrate with LDAPUse HTTP ServiceMQTT 5.0 Enhanced AuthenticationPSK AuthenticationUse HTTP API to Manage User DataAuthorizationUse ACL fileUse Built-in DatabaseIntegrate with MySQLIntegrate with MongoDBIntegrate with PostgreSQLIntegrate with RedisIntegrate with LDAPUse HTTP ServiceBanned ClientsFlapping DetectRule EngineCreate RulesRule SQL ReferenceData Sources and FieldsBuilt-in SQL Functionsjq FunctionsFlow DesignerData IntegrationConnectorWebhookApache KafkaApache IoTDBApache PulsarAWS KinesisAzure Event HubsCassandraClickHouseConfluentDynamoDBElasticsearchGCP PubSubGreptimeDBHStreamDBHTTP ServerInfluxDBMicrosoft SQL ServerMongoDBMQTTMySQLOpenTSDBOracle DatabasePostgreSQLRabbitMQRedisRocketMQSysKeeperTDengineTimescaleDBAdministration GuideConfigurationClusterListenerMQTTFlappingLimiterLogsPrometheusDashboardConfiguration ManualCommand Line InterfaceEMQX DashboardDashboard Home PageConnectionsTopics and subscriptionTopicsSubscriptionsRetained MessagesAccess ControlAuthenticationAuthorizationBanned ClientsData IntegrationFlowsRulesData BridgeManagementExtensionsDiagnoseSystemSingle Sign-On (SSO)Configure OpenLDAP and Microsoft Entra ID SSOConfigure SAML-Based SSOAudit LogRate LimitLogs and ObservabilityStatistics and MetricsAlarmLogsTopic MetricsSlow SubscriptionsLog TraceSystem TopicIntegrate with PrometheusIntegrate with OpenTlelemetryMetricsLogsTracesBackup and RestorePlugin and ExtensionHooksPluginsgRPC Hook ExtensionTelemetryAdvanced FeaturesMQTT over QUICFeatures and BenefitsUse MQTT over QUICMulti-Protocol GatewaySTOMP GatewayMQTT-SN GatewayCoAP GatewayLwM2M GatewayExProto GatewayTutorialsMQTT ProgrammingEMQX EssentialsMQTT GuideDesign and ImplementationClusteringIn-flight and QueueMessage RetransmissionReferenceMQTT 5.0 SpecificationMQTT 3.1.1 SpecificationMQTT GlossaryMQTT 5.0 FeaturesMQTT Reason CodeRelease NotesVersion 5Version 4Version 0.1 to 3.xIncompatible ChangesIncompatible Changes in EMQX 5.5Incompatible Changes in EMQX 5.4Incompatible Changes between EMQX 4.4 and EMQX 5.1Authentication / Authorization Incompatibility Between EMQX 4.4 and EMQX 5.1Data Integration Incompatibility Between EMQX 5.1 and EMQX 4.4Gateway Incompatibility Between EMQX 4.4 and EMQX 5.1On this page Table of Contents for current page Performance Tuning (Linux) ​Due to the typically large number of devices and data in IoT applications, EMQX, as an MQTT server, is responsible for handling and delivering messages generated by a massive number of devices. In this scenario, optimizing EMQX system performance becomes particularly crucial.Optimization aims to maximize the following aspects of performance:Message Processing Capability: Enhancing EMQX's ability to process messages quickly and efficiently, ensuring it can rapidly receive, process, and forward device-generated messages.Throughput: Increasing throughput to ensure the system can handle and deliver messages from devices in a timely manner.Stability: Reducing latency under high loads, improving system responsiveness, and lowering the risk of system crashes or failures.This page provides general tuning suggestions for benchmarking and deployment.Turn Off Swap ​Linux swap partitions may cause nondeterministic memory latency to an Erlang virtual machine, significantly affecting the system stability. It is recommended to turn off the swap permanently.To turn off swap immediately, execute the command sudo swapoff -a.To turn off swap permanently, comment out the swap line in /etc/fstab and reboot the host.Linux Kernel Tuning ​The system-wide limit on max opened file handles:bash# 2 millions system-wide"
64,sysctl -w fs.file-max=2097152
64,sysctl -w fs.nr_open=2097152
64,echo 2097152 > /proc/sys/fs/nr_openThe limit on opened file handles for the current session:bashulimit -n 2097152/etc/sysctl.conf ​Persist fs.file-max configuration to /etc/sysctl.conf:bashfs.file-max = 2097152Set the maximum number of file handles for the service in /etc/systemd/system.conf:bashDefaultLimitNOFILE=2097152emqx.service ​Set the maximum number of file handles for emqx service in one of the below paths depending on which Linux distribution is used./usr/lib/systemd/system/emqx.service/lib/systemd/system/emqx.servicebashLimitNOFILE=2097152/etc/security/limits.conf ​Persist the maximum number of opened file handles for users in /etc/security/limits.conf:bash*
64,soft
64,nofile
64,2097152
64,hard
64,nofile
64,2097152TCP Network Tuning ​Increase the number of incoming connections backlog:bashsysctl -w net.core.somaxconn=32768
64,sysctl -w net.ipv4.tcp_max_syn_backlog=16384
64,sysctl -w net.core.netdev_max_backlog=16384Local port rangebashsysctl -w net.ipv4.ip_local_port_range='1024 65535'TCP Socket read/write buffer:bashsysctl -w net.core.rmem_default=262144
64,sysctl -w net.core.wmem_default=262144
64,sysctl -w net.core.rmem_max=16777216
64,sysctl -w net.core.wmem_max=16777216
64,sysctl -w net.core.optmem_max=16777216
64,#sysctl -w net.ipv4.tcp_mem='16777216 16777216 16777216'
64,sysctl -w net.ipv4.tcp_rmem='1024 4096 16777216'
64,sysctl -w net.ipv4.tcp_wmem='1024 4096 16777216'TCP connection tracking:bashsysctl -w net.nf_conntrack_max=1000000
64,sysctl -w net.netfilter.nf_conntrack_max=1000000
64,"sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=30TIME-WAIT Bucket Pool, Recycling, and Reuse:bashsysctl -w net.ipv4.tcp_max_tw_buckets=1048576"
64,# Enabling following option is not recommended. It could cause connection reset under NAT
64,# sysctl -w net.ipv4.tcp_tw_recycle=1
64,# sysctl -w net.ipv4.tcp_tw_reuse=1Timeout for FIN-WAIT-2 Sockets:bashsysctl -w net.ipv4.tcp_fin_timeout=15Erlang VM Tuning ​Tune and optimize the Erlang VM in etc/emqx.conf file:bash## Sets the maximum number of simultaneously existing ports for this system
64,"node.max_ports = 2097152EMQX Tuning ​Tune the acceptor pool size and max_connections limit in etc/emqx.conf.For example, for TCP listeners:bash## TCP Listener"
64,listeners.tcp.$name.acceptors = 64
64,"listeners.tcp.$name.max_connections = 1024000Client Machine Tuning ​Tune the client machine to benchmark EMQX:sysctl -w net.ipv4.ip_local_port_range=""500 65535"""
64,echo 1000000 > /proc/sys/fs/nr_open
64,ulimit -n 100000MQTT Benchmarking ​Test tools for concurrent connections: emqtt_bench.Previous pagePerformance and TuningNext pagePerformance Test with eMQTT-Bench© 2013-2023 EMQ Technologies Inc. All rights reserved
65,postgresql database optimization course
65,+91-951-376-2021
65,+91-704-259-3729
65,info@koenig-solutions.com
65,All Courses
65,All Courses
65,Microsoft
65,Cisco
65,AWS
65,VMware
65,Oracle
65,AXELOS
65,PECB
65,ISACA
65,CompTIA
65,EC-Council
65,Red Hat
65,PMI
65,ISC2
65,SAP
65,BCS
65,PL-300T00: Microsoft Power BI Data Analyst
65,AZ-104T00-A: Microsoft Azure Administrator
65,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
65,AZ-400T00-A: Designing and Implementing Microsoft DevOps solutions
65,AZ-900T01: Microsoft Azure Fundamentals (1 day)
65,DP-203T00: Data Engineering on Microsoft Azure
65,ITIL® 4 Foundation
65,AWS Certified Solutions Architect - Associate (Architecting on AWS)
65,MS-900T01-A: Microsoft 365 Fundamentals
65,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
65,Microsoft Power Platform Fundamentals
65,"Microsoft Security, Compliance, and Identity Fundamentals SC-900T00-A"
65,AI-050: Develop Generative AI Solutions with Azure OpenAI Service
65,MD-102: Endpoint Administrator
65,PL-200T00: Microsoft Power Platform Functional Consultant
65,Show All Courses
65,PL-300T00: Microsoft Power BI Data Analyst
65,AZ-104T00-A: Microsoft Azure Administrator
65,AZ-900T01: Microsoft Azure Fundamentals (1 day)
65,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
65,DP-203T00: Data Engineering on Microsoft Azure
65,AZ-400T00-A: Designing and Implementing Microsoft DevOps solutions
65,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
65,Microsoft Power Platform Fundamentals
65,MS-900T01-A: Microsoft 365 Fundamentals
65,MD-102: Endpoint Administrator
65,AZ-500: Microsoft Azure Security Technologies
65,AZ-204T00: Developing Solutions for Microsoft Azure
65,PL-400T00: Microsoft Power Platform Developer
65,PL-200T00: Microsoft Power Platform Functional Consultant
65,"Microsoft Security, Compliance, and Identity Fundamentals SC-900T00-A"
65,Show All Courses
65,Implementing Cisco Enterprise Network Core Technologies (ENCOR) v1.3
65,Implementing and Administering Cisco Solutions (CCNA) v2.0
65,Implementing and Configuring Cisco Identity Services Engine (SISE) v4.0
65,Implementing Cisco SD-WAN Solutions (ENSDWI) v3.0
65,Implementing Cisco Application Centric Infrastructure (DCACI) v1.2
65,Cisco Certified Network Associate v2.0 (200-301 CCNA)
65,Implementing Cisco Data Center Core Technologies (DCCOR) v1.2
65,Implementing Cisco Application Centric Infrastructure - Advanced (DCACIA) v 1.2
65,Securing Email with Cisco Email Security Appliance (SESA) v3.1
65,Cisco SD-WAN Operation and Deployment (SDWFND) v2.0
65,Implementing Cisco Enterprise Advanced Routing and Services (ENARSI)
65,Implementing and Operating Cisco Security Core Technologies (SCOR)
65,Developing Applications using Cisco Core Platforms and APIs (DEVCOR)
65,Securing Networks with Cisco Firepower Next Generation Firewall (SSNGFW) v1.0
65,Cisco Certified DevNet Associate (DEVASC)
65,Show All Courses
65,AWS Certified Solutions Architect - Associate (Architecting on AWS)
65,AWS Certified Cloud Practitioner ( AWS Cloud Practitioner )
65,AWS Certified Solutions Architect – Professional ( Advanced Architecting on AWS )
65,AWS Certified Developer – Associate (Developing on AWS)
65,AWS Certified Security – Specialty ( Security Engineering on AWS )
65,AWS Certified DevOps Engineer – Professional ( DevOps Engineering on AWS )
65,AWS Certified Sysops Administrator - Associate (Cloud Operations on AWS)
65,Developing Serverless Solutions on AWS
65,AWS Security Essentials
65,Data Warehousing on AWS
65,AWS Cloud Financial Management for Builders
65,AWS Technical Essentials
65,AWS Certified Database - Specialty (Planning and Designing Databases on AWS)
65,The Machine Learning Pipeline on AWS
65,Authoring Visual Analytics Using Amazon QuickSight
65,Show All Courses
65,"VMware vSphere: Install, Configure, Manage [V8]"
65,"VMware NSX: Install, Configure, Manage [V4.0]"
65,"VMware Cloud Foundation: Deploy, Configure, Manage [V5.0]"
65,VMware Horizon 8: Deploy and Manage plus App Volumes Fast Track
65,"VMware NSX-T Data Center: Install, Configure, Manage [V3.2]"
65,VMware vRealize Automation: Orchestration and Extensibility [V8.6]
65,VMware Horizon: Deploy and Manage [V8.8]
65,VMware vSphere with Tanzu: Deploy and Manage [V7]
65,"VMware Cloud Foundation: Planning, Management, Operations [V4.3]"
65,"VMware Aria Automation: Install, Configure, Manage [V8.10]"
65,"VMware Site Recovery Manager: Install, Configure, Manage [V8.6]"
65,"VMware Site Recovery Manager: Install, Configure, Manage [V8.2]"
65,VMware NSX Advanced Load Balancer: Web Application Firewall Security
65,VMware Horizon 8: Skills for Virtual Desktop Management
65,VMware Spring: Core
65,Show All Courses
65,Oracle Database 19c: Administration Workshop
65,Java Spring Boot
65,Oracle Exadata Database Machine: Implementation and Administration
65,Oracle Database 19c: Advanced PL/SQL
65,Java SE 8 Fundamentals Ed 1
65,Oracle Database 19c: Backup and Recovery
65,Oracle Database 19c: RAC Administration Workshop
65,Oracle Database 19c: PL/SQL Workshop
65,Oracle WebLogic Server 12c: Administration I Ed 3
65,Primavera P6 Professional Fundamentals Rel 19 Ed 1
65,R12.2.9 Oracle Payables Management Fundamentals
65,Oracle WebLogic Server 14c: Administration I
65,Oracle WebLogic Server 14c: Administration II
65,MySQL High Availability with InnoDB Cluster
65,Oracle WebLogic Server 12c: Administration II Ed 3
65,Show All Courses
65,ITIL® 4 Foundation
65,"ITIL® 4 Specialist Create, Deliver and Support"
65,"ITIL® 4 Strategist: Direct, Plan, and Improve"
65,PRINCE2® 6th Edition Foundation & Practitioner
65,ITIL® 4 Specialist Drive Stakeholder Value
65,ITIL® 4 Specialist High-velocity IT
65,PRINCE2® 6th Edition Practitioner
65,PRINCE2® Agile Foundation
65,ITIL® 4 Digital and IT Strategy
65,"ITIL® 4 Specialist: Monitor, Support and Fulfil"
65,P3O Foundation
65,PRINCE2® agile practitioner
65,PRINCE2® 6th Edition
65,Foundation
65,PRINCE2® Agile Foundation and Practitioner
65,Show All Courses
65,ISO 27001 (ISMS) Lead Implementer
65,ISO 22301 (BCMS) Lead Implementer
65,Certified Data Protection Officer : CDPO (includes GDPR)
65,ISO/IEC 27001:2022 Transition
65,ISO 31000 Lead Risk Manager
65,ISO/IEC 20000 (ITSM) Lead Implementer
65,ISO 20000 (ITSM) Lead Auditor
65,ISO 27001 (ISMS) Lead Auditor
65,ISO 31000 Risk Manager
65,ISO 37301 Lead Implementer
65,ISO 22301 (BCMS) Lead Auditor
65,ISO 9001 (QMS) Lead Auditor
65,ISO 55001 Lead Auditor
65,ISO/IEC 27005 Lead Risk Manager
65,ISO/IEC 27001 Foundation
65,Show All Courses
65,Certified Information Security Manager (CISM)
65,Certified Information Systems Auditor-CISA
65,Cobit 2019 Foundation
65,CRISC
65,Certified in the Governance of Enterprise IT (CGEIT)
65,Certified Data Privacy Solutions Engineer-CDPSE
65,COBIT 2019 Design and Implementation
65,Certificate of Cloud Auditing Knowledge (CCAK)
65,Cloud Fundamentals Certificate
65,IT Audit Fundamental Certificate
65,IT Risk Fundamentals
65,COBIT-5-Assessor
65,Implementing NIST Cyber Security Framework using COBIT 2019
65,Cyber Security Audit
65,CSX Fundamentals
65,Show All Courses
65,CompTIA Security+ SY0-701
65,CompTIA-SY0-601-Security+
65,CompTIA Network+
65,(N10-008)
65,CompTIA Cybersecurity Analyst (CySA+)
65,CompTIA A+ 1101-1102
65,CompTIA Advanced Security Practitioner (CASP+)(CAS-004)
65,CompTIA Cloud+ CV0-003
65,CompTIA Pentest+ ( PT0-002)
65,CompTIA Server+ (SK0-005)
65,CompTIA Cloud Essentials+
65,CompTIA Data+ DA0-001
65,CompTIA Cloud+
65,CompTIA Network+ (N10-007)
65,CompTIA A+ 1001-1002
65,PROJECT+
65,Show All Courses
65,Certified Ethical Hacker v12 - CEHv12
65,Certified SOC Analyst-CSA
65,Certified Chief Information Security Officer( CCISO )
65,Certified Penetration Testing Professional - CPENT
65,Certified Application Security Engineer .NET
65,Certified DevSecOps Engineer (E|CDE)
65,EC-Council Disaster Recovery Professional v3
65,Certified Network Defender (CNDv2)
65,CHFI V10
65,Advanced Network Defense
65,Certified Blockchain Professional
65,Certified Application Security Engineer JAVA
65,Certified advanced Penetration Tester
65,EC-Council Certified Incident Handler (ECIH V2)
65,Certified Secure Computer User (CSCU)
65,Show All Courses
65,Red Hat System Administration I (RH124) – RHEL 9
65,RHCSA Rapid Track (RH199) – RHEL 9
65,Red Hat OpenShift Administration I: Operating a Production Cluster
65,Red Hat System Administration II (RH134) – RHEL 9
65,Red Hat System Administration III: Linux Automation (RH294) – RHEL 9
65,Red Hat OpenShift Administration II: Configuring a Production Cluster (DO280)
65,Red Hat OpenShift Developer II: Building Kubernetes Applications (DO288)
65,Apache and Secure Web Server Administration
65,Red Hat High Availability Clustering
65,Network Automation with Red Hat Ansible Automation Platform
65,Red Hat OpenShift Administration III: Scaling Kubernetes Deployments in the Enterprise
65,Red Hat Ceph Storage for OpenStack - CL260
65,Red Hat OpenShift Development I: Introduction to Containers with Podman
65,Red Hat OpenStack Administration II: Day 2 Operations for Cloud Operators (CL210)
65,Red Hat OpenStack Administration I: Core Operations for Domain Operators (CL110)
65,Show All Courses
65,Project Management Professional (PMP)® Certification Prep
65,PMI Agile Certified Practitioner (PMI-ACP)®
65,Certified Associate in Project Management (CAPM)® Certification Prep
65,PMI Professional in Business Analysis (PMI-PBA)®
65,Portfolio Management Professional (PfMP)
65,Program Management Professional (PgMP)®
65,Project Management for Software Development
65,Understanding Project Budget and Accounting
65,PMI-RMP Exam Prep
65,Disciplined Agile® Senior Scrum Master (DASSM)
65,Disciplined Agile® Value Stream Consultant (DAVSC)
65,Disciplined Agile® Coach (DAC)
65,Disciplined Agile® Scrum Master (DASM)
65,Project Portfolio Management
65,PMI Scheduling Professional (PMI-SP)
65,Show All Courses
65,Certified Information Systems Security Professional (CISSP)
65,Certified Cloud Security Professional
65,(CCSP)
65,"Certified in Governance, Risk and Compliance (CGRC)"
65,Systems Security Certified Practitioner (SSCP )
65,Certified in Cybersecurity
65,CISSP-ISSEP
65,CISSP-ISSMP
65,Certified Secure Software Lifecycle Professional (CSSLP)
65,CISSP-ISSAP
65,Show All Courses
65,ADM100 - System Administration I of SAP S/4HANA and SAP Business Suite
65,SAP S/4HANA Overview - S4H00
65,SAP HANA 2.0 SPS06 Modeling - HA300
65,GRC300 - SAP Access Control Implementation and Configuration
65,Configuring SAP Global Trade Services - GTS200
65,TS450 - Sourcing and Procurement in SAP S/4HANA - Academy Part I
65,TS452 - Sourcing and Procurement in SAP S/4HANA - Academy Part II
65,ADMCLD - Introduction to SAP Business Technology Platform (BTP) Administration
65,PLM300 - Business Processes in Plant Maintenance
65,TS421 - SAP S/4HANA Production Planning & Manufacturing Academy Part I
65,SAP FICO
65,TS410
65,Integrated Business Processes in SAP S/4HANA
65,SAP Success Factors
65,S43100 Managing Technical Objects in SAP S/4HANA
65,SACS21: SAP Analytics Cloud: Story Design
65,Show All Courses
65,Certificate in Information Security Management Principles (CISMP)
65,BCS Practitioner Certificate in Modelling Business Processes
65,BCS Foundation Level Certificate in DevOps
65,BCS Foundation Certificate in Business Analysis
65,BCS Practitioner Certificate in Business Analysis Practice
65,BCS Practitioner Certificate in Enterprise and Solution Architecture
65,BCS Foundation Certificate in Architecture Concepts and Domains
65,BCS Practitioner Certificate in Digital Product Management
65,BCS Foundation Certificate in Artificial Intelligence
65,BCS Foundation Certificate in Agile
65,BCS Foundation Certificate in Organisational Behaviour
65,BCS Foundation Certificate in Business Change
65,BCS Practitioner Certificate in Requirements Engineering
65,BCS Practitioner Certificate in Data Protection
65,BCS Foundation Certificate in Data Protection
65,Show All Courses
65,About Koenig
65,About us
65,Our Clientele
65,Leadership
65,Our Partners
65,Student Feedback
65,Our Offerings
65,Live Online Training
65,Classroom Training
65,Fly-Me-a-Trainer
65,Flexi
65,Upcoming Webinars
65,Contact Us
65,Login
65,+91-704-259-3729
65,+91-951-376-2021
65,About Koenig
65,About us
65,Our Clientele
65,Leadership
65,Our Partners
65,Student Feedback
65,All Course
65,All Course
65,All Courses
65,PL-300T00: Microsoft Power BI Data Analyst
65,AZ-104T00-A: Microsoft Azure Administrator
65,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
65,AZ-400T00-A: Designing and Implementing Microsoft DevOps solutions
65,AZ-900T01: Microsoft Azure Fundamentals (1 day)
65,DP-203T00: Data Engineering on Microsoft Azure
65,ITIL® 4 Foundation
65,AWS Certified Solutions Architect - Associate (Architecting on AWS)
65,MS-900T01-A: Microsoft 365 Fundamentals
65,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
65,Microsoft Power Platform Fundamentals
65,"Microsoft Security, Compliance, and Identity Fundamentals SC-900T00-A"
65,AI-050: Develop Generative AI Solutions with Azure OpenAI Service
65,MD-102: Endpoint Administrator
65,PL-200T00: Microsoft Power Platform Functional Consultant
65,Microsoft
65,PL-300T00: Microsoft Power BI Data Analyst
65,AZ-104T00-A: Microsoft Azure Administrator
65,AZ-900T01: Microsoft Azure Fundamentals (1 day)
65,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
65,DP-203T00: Data Engineering on Microsoft Azure
65,AZ-400T00-A: Designing and Implementing Microsoft DevOps solutions
65,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
65,Microsoft Power Platform Fundamentals
65,MS-900T01-A: Microsoft 365 Fundamentals
65,MD-102: Endpoint Administrator
65,AZ-500: Microsoft Azure Security Technologies
65,AZ-204T00: Developing Solutions for Microsoft Azure
65,PL-400T00: Microsoft Power Platform Developer
65,PL-200T00: Microsoft Power Platform Functional Consultant
65,"Microsoft Security, Compliance, and Identity Fundamentals SC-900T00-A"
65,Cisco
65,Implementing Cisco Enterprise Network Core Technologies (ENCOR) v1.3
65,Implementing and Administering Cisco Solutions (CCNA) v2.0
65,Implementing and Configuring Cisco Identity Services Engine (SISE) v4.0
65,Implementing Cisco SD-WAN Solutions (ENSDWI) v3.0
65,Implementing Cisco Application Centric Infrastructure (DCACI) v1.2
65,Cisco Certified Network Associate v2.0 (200-301 CCNA)
65,Implementing Cisco Data Center Core Technologies (DCCOR) v1.2
65,Implementing Cisco Application Centric Infrastructure - Advanced (DCACIA) v 1.2
65,Securing Email with Cisco Email Security Appliance (SESA) v3.1
65,Cisco SD-WAN Operation and Deployment (SDWFND) v2.0
65,Implementing Cisco Enterprise Advanced Routing and Services (ENARSI)
65,Implementing and Operating Cisco Security Core Technologies (SCOR)
65,Developing Applications using Cisco Core Platforms and APIs (DEVCOR)
65,Securing Networks with Cisco Firepower Next Generation Firewall (SSNGFW) v1.0
65,Cisco Certified DevNet Associate (DEVASC)
65,AWS
65,AWS Certified Solutions Architect - Associate (Architecting on AWS)
65,AWS Certified Cloud Practitioner ( AWS Cloud Practitioner )
65,AWS Certified Solutions Architect – Professional ( Advanced Architecting on AWS )
65,AWS Certified Developer – Associate (Developing on AWS)
65,AWS Certified Security – Specialty ( Security Engineering on AWS )
65,AWS Certified DevOps Engineer – Professional ( DevOps Engineering on AWS )
65,AWS Certified Sysops Administrator - Associate (Cloud Operations on AWS)
65,Developing Serverless Solutions on AWS
65,AWS Security Essentials
65,Data Warehousing on AWS
65,AWS Cloud Financial Management for Builders
65,AWS Technical Essentials
65,AWS Certified Database - Specialty (Planning and Designing Databases on AWS)
65,The Machine Learning Pipeline on AWS
65,Authoring Visual Analytics Using Amazon QuickSight
65,VMware
65,"VMware vSphere: Install, Configure, Manage [V8]"
65,"VMware NSX: Install, Configure, Manage [V4.0]"
65,"VMware Cloud Foundation: Deploy, Configure, Manage [V5.0]"
65,VMware Horizon 8: Deploy and Manage plus App Volumes Fast Track
65,"VMware NSX-T Data Center: Install, Configure, Manage [V3.2]"
65,VMware vRealize Automation: Orchestration and Extensibility [V8.6]
65,VMware Horizon: Deploy and Manage [V8.8]
65,VMware vSphere with Tanzu: Deploy and Manage [V7]
65,"VMware Cloud Foundation: Planning, Management, Operations [V4.3]"
65,"VMware Aria Automation: Install, Configure, Manage [V8.10]"
65,"VMware Site Recovery Manager: Install, Configure, Manage [V8.6]"
65,"VMware Site Recovery Manager: Install, Configure, Manage [V8.2]"
65,VMware NSX Advanced Load Balancer: Web Application Firewall Security
65,VMware Horizon 8: Skills for Virtual Desktop Management
65,VMware Spring: Core
65,Oracle
65,Oracle Database 19c: Administration Workshop
65,Java Spring Boot
65,Oracle Exadata Database Machine: Implementation and Administration
65,Oracle Database 19c: Advanced PL/SQL
65,Java SE 8 Fundamentals Ed 1
65,Oracle Database 19c: Backup and Recovery
65,Oracle Database 19c: RAC Administration Workshop
65,Oracle Database 19c: PL/SQL Workshop
65,Oracle WebLogic Server 12c: Administration I Ed 3
65,Primavera P6 Professional Fundamentals Rel 19 Ed 1
65,R12.2.9 Oracle Payables Management Fundamentals
65,Oracle WebLogic Server 14c: Administration I
65,Oracle WebLogic Server 14c: Administration II
65,MySQL High Availability with InnoDB Cluster
65,Oracle WebLogic Server 12c: Administration II Ed 3
65,AXELOS
65,ITIL® 4 Foundation
65,"ITIL® 4 Specialist Create, Deliver and Support"
65,"ITIL® 4 Strategist: Direct, Plan, and Improve"
65,PRINCE2® 6th Edition Foundation & Practitioner
65,ITIL® 4 Specialist Drive Stakeholder Value
65,ITIL® 4 Specialist High-velocity IT
65,PRINCE2® 6th Edition Practitioner
65,PRINCE2® Agile Foundation
65,ITIL® 4 Digital and IT Strategy
65,"ITIL® 4 Specialist: Monitor, Support and Fulfil"
65,P3O Foundation
65,PRINCE2® agile practitioner
65,PRINCE2® 6th Edition
65,Foundation
65,PRINCE2® Agile Foundation and Practitioner
65,PECB
65,ISO 27001 (ISMS) Lead Implementer
65,ISO 22301 (BCMS) Lead Implementer
65,Certified Data Protection Officer : CDPO (includes GDPR)
65,ISO/IEC 27001:2022 Transition
65,ISO 31000 Lead Risk Manager
65,ISO/IEC 20000 (ITSM) Lead Implementer
65,ISO 20000 (ITSM) Lead Auditor
65,ISO 27001 (ISMS) Lead Auditor
65,ISO 31000 Risk Manager
65,ISO 37301 Lead Implementer
65,ISO 22301 (BCMS) Lead Auditor
65,ISO 9001 (QMS) Lead Auditor
65,ISO 55001 Lead Auditor
65,ISO/IEC 27005 Lead Risk Manager
65,ISO/IEC 27001 Foundation
65,ISACA
65,Certified Information Security Manager (CISM)
65,Certified Information Systems Auditor-CISA
65,Cobit 2019 Foundation
65,CRISC
65,Certified in the Governance of Enterprise IT (CGEIT)
65,Certified Data Privacy Solutions Engineer-CDPSE
65,COBIT 2019 Design and Implementation
65,Certificate of Cloud Auditing Knowledge (CCAK)
65,Cloud Fundamentals Certificate
65,IT Audit Fundamental Certificate
65,IT Risk Fundamentals
65,COBIT-5-Assessor
65,Implementing NIST Cyber Security Framework using COBIT 2019
65,Cyber Security Audit
65,CSX Fundamentals
65,CompTIA
65,CompTIA Security+ SY0-701
65,CompTIA-SY0-601-Security+
65,CompTIA Network+
65,(N10-008)
65,CompTIA Cybersecurity Analyst (CySA+)
65,CompTIA A+ 1101-1102
65,CompTIA Advanced Security Practitioner (CASP+)(CAS-004)
65,CompTIA Cloud+ CV0-003
65,CompTIA Pentest+ ( PT0-002)
65,CompTIA Server+ (SK0-005)
65,CompTIA Cloud Essentials+
65,CompTIA Data+ DA0-001
65,CompTIA Cloud+
65,CompTIA Network+ (N10-007)
65,CompTIA A+ 1001-1002
65,PROJECT+
65,EC-Council
65,Certified Ethical Hacker v12 - CEHv12
65,Certified SOC Analyst-CSA
65,Certified Chief Information Security Officer( CCISO )
65,Certified Penetration Testing Professional - CPENT
65,Certified Application Security Engineer .NET
65,Certified DevSecOps Engineer (E|CDE)
65,EC-Council Disaster Recovery Professional v3
65,Certified Network Defender (CNDv2)
65,CHFI V10
65,Advanced Network Defense
65,Certified Blockchain Professional
65,Certified Application Security Engineer JAVA
65,Certified advanced Penetration Tester
65,EC-Council Certified Incident Handler (ECIH V2)
65,Certified Secure Computer User (CSCU)
65,Red Hat
65,Red Hat System Administration I (RH124) – RHEL 9
65,RHCSA Rapid Track (RH199) – RHEL 9
65,Red Hat OpenShift Administration I: Operating a Production Cluster
65,Red Hat System Administration II (RH134) – RHEL 9
65,Red Hat System Administration III: Linux Automation (RH294) – RHEL 9
65,Red Hat OpenShift Administration II: Configuring a Production Cluster (DO280)
65,Red Hat OpenShift Developer II: Building Kubernetes Applications (DO288)
65,Apache and Secure Web Server Administration
65,Red Hat High Availability Clustering
65,Network Automation with Red Hat Ansible Automation Platform
65,Red Hat OpenShift Administration III: Scaling Kubernetes Deployments in the Enterprise
65,Red Hat Ceph Storage for OpenStack - CL260
65,Red Hat OpenShift Development I: Introduction to Containers with Podman
65,Red Hat OpenStack Administration II: Day 2 Operations for Cloud Operators (CL210)
65,Red Hat OpenStack Administration I: Core Operations for Domain Operators (CL110)
65,PMI
65,Project Management Professional (PMP)® Certification Prep
65,PMI Agile Certified Practitioner (PMI-ACP)®
65,Certified Associate in Project Management (CAPM)® Certification Prep
65,PMI Professional in Business Analysis (PMI-PBA)®
65,Portfolio Management Professional (PfMP)
65,Program Management Professional (PgMP)®
65,Project Management for Software Development
65,Understanding Project Budget and Accounting
65,PMI-RMP Exam Prep
65,Disciplined Agile® Senior Scrum Master (DASSM)
65,Disciplined Agile® Value Stream Consultant (DAVSC)
65,Disciplined Agile® Coach (DAC)
65,Disciplined Agile® Scrum Master (DASM)
65,Project Portfolio Management
65,PMI Scheduling Professional (PMI-SP)
65,ISC2
65,Certified Information Systems Security Professional (CISSP)
65,Certified Cloud Security Professional
65,(CCSP)
65,"Certified in Governance, Risk and Compliance (CGRC)"
65,Systems Security Certified Practitioner (SSCP )
65,Certified in Cybersecurity
65,CISSP-ISSEP
65,CISSP-ISSMP
65,Certified Secure Software Lifecycle Professional (CSSLP)
65,CISSP-ISSAP
65,SAP
65,ADM100 - System Administration I of SAP S/4HANA and SAP Business Suite
65,SAP S/4HANA Overview - S4H00
65,SAP HANA 2.0 SPS06 Modeling - HA300
65,GRC300 - SAP Access Control Implementation and Configuration
65,Configuring SAP Global Trade Services - GTS200
65,TS450 - Sourcing and Procurement in SAP S/4HANA - Academy Part I
65,TS452 - Sourcing and Procurement in SAP S/4HANA - Academy Part II
65,ADMCLD - Introduction to SAP Business Technology Platform (BTP) Administration
65,PLM300 - Business Processes in Plant Maintenance
65,TS421 - SAP S/4HANA Production Planning & Manufacturing Academy Part I
65,SAP FICO
65,TS410
65,Integrated Business Processes in SAP S/4HANA
65,SAP Success Factors
65,S43100 Managing Technical Objects in SAP S/4HANA
65,SACS21: SAP Analytics Cloud: Story Design
65,BCS
65,Certificate in Information Security Management Principles (CISMP)
65,BCS Practitioner Certificate in Modelling Business Processes
65,BCS Foundation Level Certificate in DevOps
65,BCS Foundation Certificate in Business Analysis
65,BCS Practitioner Certificate in Business Analysis Practice
65,BCS Practitioner Certificate in Enterprise and Solution Architecture
65,BCS Foundation Certificate in Architecture Concepts and Domains
65,BCS Practitioner Certificate in Digital Product Management
65,BCS Foundation Certificate in Artificial Intelligence
65,BCS Foundation Certificate in Agile
65,BCS Foundation Certificate in Organisational Behaviour
65,BCS Foundation Certificate in Business Change
65,BCS Practitioner Certificate in Requirements Engineering
65,BCS Practitioner Certificate in Data Protection
65,BCS Foundation Certificate in Data Protection
65,Koenig Originals Courses
65,Rare Courses
65,Our Offerings
65,Live Online Training
65,Classroom Training
65,Fly-Me-a-Trainer
65,Flexi
65,Upcoming Webinars
65,Contact Us
65,Unable to find what you're searching for?
65,We're here to help you find it
65,+91-951-376-2021
65,+91-704-259-3729
65,info@koenig-solutions.com
65,Login
65,PostgreSQL Performance Tuning Course Overview
65,Overview
65,Schedule & Fee
65,Request More Info
65,Course Information
65,Download Course Contents
65,PostgreSQL
65,PostgreSQL Performance Tuning
65,PostgreSQL Performance Tuning Course Overview
65,"The PostgreSQL Performance Tuning course is an in-depth program designed to equip learners with the skills necessary to optimize the performance of PostgreSQL databases. This comprehensive course covers various aspects of tuning and monitoring PostgreSQL instances to ensure they operate efficiently and handle high loads effectively. Starting with an introduction to PostgreSQL and its architecture, the course proceeds to cover installation, configuration, security, and the basics of performance tuning.Learners will delve into advanced topics such as indexing strategies, query optimization, configuration tweaks, and the use of performance monitoring tools. Understanding and implementing database partitioning, replication, and devising robust backup and recovery strategies are also key components of the curriculum. By the end of the course, participants will be well-versed in PostgreSQL performance tuning, enabling them to identify bottlenecks, optimize queries, configure databases for high performance, and ensure reliability and high availability of PostgreSQL systems."
65,Show More
65,4.6 Ratings
65,Download Course Contents
65,Koenig's Unique Offerings
65,1-on-1 Training
65,Schedule personalized sessions based upon your availability.
65,Customized Training
65,Tailor your learning experience. Dive deeper in topics of greater interest to you.
65,4-Hour Sessions
65,"Optimize learning with Koenig's 4-hour sessions, balancing knowledge retention and time constraints."
65,Free Demo Class
65,Join our training with confidence. Attend a free demo class to experience our expert
65,trainers and get all your queries answered.
65,Purchase This Course
65,INR
65,"202,500 ♱"
65,INR
65,"202,500♱"
65,INR
65,Fee On Request
65,1-on-1
65,1-on-1
65,Public
65,Live Online Training (Duration : 24 Hours)
65,Per Participant
65,Guaranteed-to-Run (GTR)
65,4 Hours
65,Week Days
65,8 Hours
65,Weekends
65,Day
65,Time
65,Training Schedule:
65,♱ Excluding VAT/GST Classroom Training price is on request
65,Enroll Now
65,Enroll Now
65,Live Online Training (Duration : 24 Hours)
65,Per Participant
65,♱ Excluding VAT/GSTClassroom Training price is on request
65,Enroll Now
65,Request Price
65,Individual
65,Corporate
65,Request
65,More Information
65,Email:
65,WhatsApp:
65,Course Information
65,Student Feedback
65,FAQ
65,Suggested Courses
65,Download Course Contents
65,Course Prerequisites
65,"To ensure a successful learning experience in the PostgreSQL Performance Tuning course offered by Koenig Solutions, it is recommended that participants have the following minimum prerequisites:"
65,Basic understanding of relational database management systems (RDBMS) and database concepts.
65,Familiarity with SQL and experience in writing SQL queries.
65,Some hands-on experience with PostgreSQL or another SQL-based database.
65,"Knowledge of command-line operations in Linux/Unix or Windows, as PostgreSQL can be operated on these platforms."
65,An understanding of basic computer network concepts and client-server architecture.
65,Willingness to learn and apply new techniques for optimizing database performance.
65,These prerequisites are intended to provide a foundation upon which the course content can build. They are not meant to be barriers but rather to set the stage for a productive and enlightening educational experience.
65,Target Audience for PostgreSQL Performance Tuning
65,Koenig Solutions' PostgreSQL Performance Tuning course is designed for database professionals seeking to optimize PostgreSQL databases.
65,Target audience for the course includes:
65,Database Administrators (DBAs)
65,Data Architects
65,System Administrators managing database servers
65,Database Analysts
65,IT Professionals responsible for maintaining PostgreSQL databases
65,DevOps Engineers involved in the deployment and scaling of database systems
65,Software Developers who need to understand database performance for better application integration
65,Data Scientists requiring knowledge of database optimization for large datasets
65,Technical Team Leads overseeing database-centric projects
65,Database Consultants providing performance tuning services
65,Cloud Database Engineers working with PostgreSQL on cloud platforms
65,Learning Objectives - What you will Learn in this PostgreSQL Performance Tuning?Introduction to Learning Outcomes:
65,"Gain in-depth skills in optimizing PostgreSQL databases with this comprehensive course covering performance tuning, indexing strategies, query optimization, monitoring, replication, and backup strategies."
65,Learning Objectives and Outcomes:
65,Understand the internal architecture of PostgreSQL for better performance tuning.
65,"Install, configure, and secure PostgreSQL for optimal operation."
65,Apply best practices in indexing and query optimization to enhance database efficiency.
65,Monitor database performance using PostgreSQL's built-in statistics and diagnostic tools.
65,Implement effective backup and recovery strategies to ensure data integrity.
65,Configure PostgreSQL settings for high availability and read/write performance optimization.
65,Troubleshoot and resolve common performance issues in PostgreSQL.
65,Use partitioning and replication to scale PostgreSQL and improve load management.
65,Automate and manage PostgreSQL configuration for consistent performance across environments.
65,Leverage advanced performance tuning techniques for complex scenarios and cloud deployments.
65,FAQ's
65,Do you also offer career/ course guidance?
65,"Yes, we do."
65,What does the fee include?
65,The Fee includes:
65,Testing Via Qubits
65,What is the difference between Group Training and 1-on-1?
65,Schedule for Group Training is decided by Koenig. Schedule for 1-on-1 is decided by you.
65,What is the advantage of 1-on-1 if there can be other students?
65,"In 1 on 1 Public you can select your own schedule, other students can be merged. Choose 1-on-1 if published schedule doesn't meet your requirement. If you want a private session, opt for 1-on-1 Private."
65,What is the difference between Ultra-fast Track and normally 24 hours class. is it the same course content?
65,Duration of Ultra-Fast Track is 50% of the duration of the Standard Track. Yes(course content is same).
65,What is the difference both 1-on-1 Public and 1-on-1 Private.?
65,1-on-1 Public - Select your start date. Other students can be merged. 1-on-1 Private - Select your start date. You will be the only student in the class.
65,Is there Hands-on training?
65,"Yes, course requiring practical include hands-on labs."
65,How can we pay? Do you have a payment link?
65,"You can buy online from the page by clicking on ""Buy Now"". You can view alternate payment method on payment options page."
65,Can I pay from website?
65,"Yes, you can pay from the course page and flexi page."
65,Is this website Secure?
65,"Yes, the site is secure by utilizing Secure Sockets Layer (SSL) Technology. SSL technology enables the encryption of sensitive"
65,"information during online transactions. We use the highest assurance SSL/TLS certificate, which ensures that no unauthorized"
65,person can get to your sensitive payment data over the web.
65,Is my information secure?
65,We use the best standards in Internet security. Any data retained is not shared with third parties.
65,"Once I made my payment online, can I cancel it?"
65,You can request a refund if you do not wish to enroll in the course.
65,How do I get a copy of my payment?
65,"To receive an acknowledgment of your online payment, you should have a valid email address. At the point when you enter your name,"
65,"Visa, and other data, you have the option of entering your email address. Would it be a good idea for you to decide to enter your"
65,"email address, confirmation of your payment will be emailed to you."
65,How will I know that my payment has been accepted?
65,"After you submit your payment, you will land on the payment confirmation screen."
65,It contains your payment confirmation message. You will likewise get a confirmation email after your transaction is submitted.
65,What types of credit cards are accepted?
65,"We do accept all major credit cards from Visa, Mastercard, American Express, and Discover."
65,How long does it take for a credit card transaction to process if I pay online?
65,"Credit card transactions normally take 48 hours to settle. Approval is given right away; however,"
65,it takes 48 hours for the money to be moved.
65,Can I use more than one payment method per transaction?
65,"Yes, we do accept partial payments, you may use one payment method for part of the transaction and another payment method"
65,for other parts of the transaction.
65,Can I still send in a paper check?
65,"Yes, if we have an office in your city."
65,Do you offer corporate training?
65,"Yes, we do offer corporate training"
65,More details
65,Do you accept purchase orders?
65,"Yes, we do."
65,Are weekend classes available?
65,"Yes, we also offer weekend classes."
65,Do I have to bring my laptop?
65,"Yes, Koenig follows a BYOL(Bring Your Own Laptop) policy."
65,Do I have to go through the course material before I come to class?
65,It is recommended but not mandatory. Being acquainted with the basic course material will enable you and the trainer to
65,move at a desired pace during classes.
65,You can access courseware for most vendors.
65,"I received an email from ""koenigindia@gmail.com"". Is this Koenig's official email id?"
65,"Yes, this is our official email address which we use if a recipient is not able to receive emails from our @koenig-solutions.com email address."
65,Is there any Installments/EMI option available regarding the course payment?
65,Buy-Now. Pay-Later option is available using credit card in USA and India only.
65,How will I receive the certificate after attending the course?
65,You will receive the letter of course attendance post training completion via learning enhancement tool after registration.
65,Can I see trainers profile before the training?
65,Yes you can.
65,Do you provide self-paced videos?
65,"Yes, we do. For details go to flexi"
65,What payment options are available?
65,You can pay through debit/credit card or bank wire transfer.
65,Where are your Training centers?
65,"Dubai, London, Sydney, Singapore, New York, Delhi, Goa, Bangalore, Chennai and Gurugram."
65,Can I request for a demo class before Registering?
65,Yes you can request your customer experience manager for the same.
65,Is there a Money Back Guarantee?
65,Yes of course. 100% refund if training not upto your satisfaction.
65,Prices & Payments
65,I am an entitled to claim tax rebate for training expenses. Can I get an invoice for Associated expenses including travel related costs?
65,Yes of course.
65,"Are you open during Christmas, New Year, Ramadan, Other holidays?"
65,"Yes, We are"
65,Travel and Visa
65,Do you provide visa assistance?
65,Yes we do after your registration for course.
65,Food and Beverages
65,Is Halal food available in India?
65,Yes.
65,Others
65,What is the genesis of the name of your company - Koenig?
65,Says our CEO-
65,“It is an interesting story and dates back half a century.
65,My father started a manufacturing business in India in the 1960's
65,for import substitute electromechanical components such as microswitches.
65,German and Japanese goods were held in high esteem so he named his company
65,Essen Deinki (Essen is a well known industrial town in Germany and Deinki
65,is Japanese for electric company). His products were very good quality and
65,the fact that they sounded German and Japanese also helped. He did quite well.
65,In 1970s he branched out into electronic products and again looked for a German name.
65,"This time he chose Koenig, and Koenig Electronics was born. In 1990s after graduating from college I"
65,was looking for a name for my company and Koenig Solutions sounded just right. Initially
65,we had marketed under the brand of Digital Equipment Corporation but DEC went out of business and we switched
65,to the Koenig name. Koenig is difficult to pronounce and marketeers said it is not a good choice for a B2C brand.
65,But it has proven lucky for us.” – Says Rohit Aggarwal (Founder and CEO - Koenig Solutions)
65,I am worried about the communication skills of the trainers. Do they speak good English?
65,All our trainers are fluent in English . Majority of our customers are from outside India and our trainers speak in
65,a neutral accent which is easily understandable by students from all nationalities.
65,Our money back guarantee also stands for accent of the trainer.
65,What other benefits can I avail when visiting Koenig?
65,Medical services in India are at par with the world and are a fraction of costs in Europe and USA. A number of our
65,"students have scheduled cosmetic, dental and ocular procedures during their stay in India. We can provide advice about this, on request."
65,We want to train 4 people and would like them in a class to themselves. Is it possible?
65,"Yes, if you send 4 participants, we can offer an exclusive training for them which can be started from Any Date™ suitable for you."
65,Show all 16 frequently asked questions
65,Suggested Courses
65,Learn EDB Postgres Advanced Server v14: Comprehensive Course Guide
65,Learn Comprehensive Postgresql Plus Advanced Server Course Online
65,"Learn to Master the Art of Backup, Recovery and Replication with our Comprehensive PostgreSQL Course"
65,PostgreSQL Database Administration Certification Training Course
65,PostgreSQL Database Developer Training Certification Course
65,Mastering PostgreSQL: Expert Techniques for Efficient Database Querying
65,Subscribe to our Newsletter
65,Subscribe
65,Company
65,About us
65,Leadership
65,Contact Us
65,Webinars
65,Our Clientele
65,All Courses
65,Our Partners
65,Our Story
65,Learning Options
65,Explore All Learning Options
65,Live Online Training
65,1-on-1TM Training
65,Classroom Training
65,Fly-me-a-Trainer (FMAT)
65,Flexi
65,Resources
65,Technical Questions & Answers
65,Blog
65,Sitemap
65,Employment Exchange for Digital Skills (EEDS)
65,Qubits
65,Others
65,Environment Policy
65,Payment Methods
65,Terms of Service
65,Career
65,Top Technologies
65,Cloud Computing
65,Artificial Intelligence
65,Microsoft Office
65,Security
65,Microsoft Dynamics
65,Top Partners
65,Microsoft
65,AWS
65,Cisco
65,PECB
65,VMware
65,Top Courses
65,PL-300T00: Microsoft Power BI Data Analyst
65,AZ-104T00-A: Microsoft Azure Administrator
65,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
65,AZ-900T01: Microsoft Azure Fundamentals (1 day)
65,DP-203T00: Data Engineering on Microsoft Azure
65,AZ-400T00-A: Designing and Implementing Microsoft DevOps Solutions
65,AWS Certified Solutions Architect - Associate (Architecting on AWS)
65,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
65,ITIL® 4 Foundation
65,Microsoft Power Platform Fundamentals
65,"All rights reserved. ©1997 - 2023, Koenig Solutions Pvt. Ltd."
65,PMP ® is a registered trademark of the Project Management Institute.
65,"ITIL ® and PRINCE2 ® are registered Trademarks of AXELOS Limited, used under the permission of AXELOS Limited. All rights reserved."
65,Koenig Solutions Pvt. Ltd. is rated 4.7 stars by https://www.trustpilot.com/review/www.koenig-solutions.com based on
65,244 reviews
65,TOGAF ® is a registered trademark of The Open Group.
65,"The APMG International and swirl device logo is a trademark of the APM Group Limited, used under permission of The APM Group Limited. All rights reserved."
65,AgilePM® is a registered trademark of Agile Business Consortium Limited. All rights reserved.
65,"We believe in the philosophy To Err is Human, to Admit Divine! We are not perfect but we are trying. Keep visiting our website, you will see improvements and occasional blunders, Feel free to tell us how we can improve by writing to"
65,webmaster@koenig-solutions.com
66,High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Oracle Cloud Infrastructure Documentation
66,All Pages
66,Skip to main content
66,High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,This article describes the key performance tuning features in Autonomous Database on Dedicated Exadata Infrastructure.
66,"Note that throughout this section the term ""you"" is broadly used to mean any user in your organization who has the responsibility for performing certain tasks. In some cases, that's the application DBA, in others it's the application developer."
66,"Autonomous Database includes several features that automatically monitor, analyze and optimize the performance of your database. For a complete list of the SQL tuning and performance management features of Oracle Autonomous Database, and instructions on how to use them, see Oracle Database SQL Tuning Guide."
66,You can see a broad categorization for the key performance tuning features of Autonomous Database depicted below.
66,"Tip:In the following image, you can click the feature you want to explore further."
66,Predefined Database Services
66,Connection Pools
66,Special-Purpose Connection Features
66,SQL Performance Tuning Features
66,SQL Tracing
66,Optimizer Statistics
66,Optimizer Hints
66,Automatic IndexingAutomatic indexing automates the index management tasks in Autonomous Database. Auto indexing is disabled by default in Autonomous Database.
66,"Fast IngestFast ingest optimizes the processing of high-frequency, single-row data inserts into a database. Fast ingest uses the large pool for buffering the inserts before writing them to disk, so as to improve data insert performance."
66,Predefined Job Classes with Oracle SchedulerAutonomous Database includes predefined job_class values to use with Oracle Scheduler. These job classes let you group jobs that share common characteristics and behavior into larger entities so that you can prioritize among these classes by controlling the resources allocated to each class.
66,Performance Monitoring and Tuning Tools
66,Parent topic: High Performance
66,Predefined Database Services
66,How your application connects to your database and how you code SQL calls to the database determine the overall performance of your application's transaction processing and reporting operations.
66,"When making connections to your Autonomous Database, the performance of your application's interaction with the database depends on which database service you connect to. Autonomous Database provides multiple sets of database services to use when connecting to your database. These connection services are designed to support different kinds of database operations as described in Predefined Database Service Names for Autonomous Databases."
66,Tip:Ensure to review the key characteristics of the predefined database services and the table that compares the different sets of database services based on these characteristics to decide which database service is more appropriate for your application's performance requirements.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Connection Pools
66,"When making connections to your Autonomous Database, you can use connection pools to reduce the performance overhead of repeatedly creating and destroying individual connections. This is another factor that has great impact on the performance of your application's interaction with the database."
66,"Quite often, the use of connection pools is considered only when designing or enhancing an application to provide continuous availability. However, the use of connection pools instead of individual connections can benefit almost every transaction processing application. A connection pool provides the following benefits:"
66,Reduces the number of times new connection objects are created.
66,Promotes connection object reuse.
66,Quickens the process of getting a connection.
66,Controls the amount of resources spent on maintaining connections.
66,Reduces the amount of coding effort required to manually manage connection objects.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Special-Purpose Connection Features
66,Oracle Net Services (previous called SQL*Net) provides a variety of connection features that improve performance in specific connection scenarios. These features are described in Oracle Database Net Services Administrator's Guide.
66,"Colocation tagging is one such feature that is useful in certain transaction processing applications. If your application repeatedly makes connections to the same database service, colocation tagging permits all such connections to be directed to the same database instance, bypassing the load-balancing processing normally done on the database side of connections. For more information, see COLOCATION_TAG of Client Connections."
66,Shared Server Configuration is another feature Autonomous Database supports for maintaining legacy applications designed without connection pooling. The shared server architecture enables the database server to allow many client processes to share very few server processes. This increases the number of users that the application can support. Using the shared server architecture for such legacy applications enables them to scale up without making any changes to the application itself.
66,You can enable shared server connections while provisioning an Autonomous Container Database (ACD) and this setting applies to all the databases created in it. See Create an Autonomous Container Database for instructions.
66,"See also Oracle Database Net Services Administrator's Guide for more detailed information about shared servers, including features such as session multiplexing."
66,"Once the Shared Server connection is enabled for your Autonomous Container Database, changing the connect string is not necessary. The default configuration is set to Dedicated."
66,Note
66,"You can not disable a Shared Server for a specific Autonomous Database created under a Shared Server-enabled Autonomous Container Database, and you can not use a Dedicated connection for Autonomous Databases created under a Shared Server-enabled Autonomous Container Database."
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,SQL Performance Tuning Features
66,Great applications begin with well written SQL. Oracle Autonomous Database provides numerous features that enable you to build high performance applications and validate your SQL and PL/SQL code. Some of these features are listed below:
66,Automatic Indexing
66,Optimizer Statistics and Hints
66,Automatic resolution of SQL plan regressions
66,Automatic quarantine of runaway SQL statements
66,SQL Plan Management
66,SQL Tuning sets
66,SQL Trace
66,"As you develop your application, you can quickly learn how these features are affecting the SQL code you write and so improve your code by using the SQL Worksheet provided by both Oracle Database Actions (which is built into your Autonomous Database) and Oracle SQL Developer (a free application you install on your development system)."
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,SQL Tracing
66,"When an application operation takes longer than expected, getting a trace of all the SQL statements executed as part of this operation with details such as time spent by that SQL statement in the parse, execution, and fetch phases will help you identify and resolve the cause of the performance issue. You can use SQL tracing on an Autonomous Database to achieve this."
66,SQL tracing is disabled by default in Autonomous Database. You must enable it to start collecting the SQL tracing data. Refer to Use SQL Tracing on Autonomous Database for detailed instructions to enable and use SQL Tracing.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Optimizer Statistics
66,Autonomous Database gathers optimizer statistics automatically so that you do not need to perform this task manually and this helps to ensure your statistics are current. Automatic statistics gathering is enabled in Autonomous Database and runs in a standard maintenance window.
66,Note
66,"For more information on maintenance window times and automatic optimizer statistics collection, see Oracle Database Administrator’s Guide."
66,For more information on optimizer statistics see Oracle Database SQL Tuning Guide.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Optimizer Hints
66,Optimizer hints are special comments in a SQL statement that pass instructions to the optimizer. The optimizer uses hints to choose an execution plan for the statement unless prevented by some condition.
66,Autonomous Database defaults for the optimizer and PARALLEL hints differ depending on your workload:
66,Autonomous Data Warehouse: Autonomous Database with Data Warehouse ignores optimizer hints and PARALLEL hints in SQL statements by default.
66,"If your application relies on hints, you can enable optimizer hints by setting the parameter OPTIMIZER_IGNORE_HINTS to FALSE at the session or system level using ALTER SESSION or ALTER SYSTEM."
66,"For example, the following command enables hints in your session: ALTER SESSION"
66,SET OPTIMIZER_IGNORE_HINTS=FALSE;
66,You can also enable PARALLEL hints in your SQL statements by setting OPTIMIZER_IGNORE_PARALLEL_HINTS to FALSE at the session or system level using ALTER SESSION or ALTER SYSTEM.
66,"For example, the following command enables PARALLEL hints in your session: ALTER SESSION"
66,SET OPTIMIZER_IGNORE_PARALLEL_HINTS=FALSE;
66,Autonomous Transaction Processing: Autonomous Database honors optimizer hints and PARALLEL hints in SQL statements by default.
66,You can disable optimizer hints by setting the parameter OPTIMIZER_IGNORE_HINTS to TRUE at the session or system level using ALTER SESSION or ALTER SYSTEM.
66,"For example, the following command disables hints in your session:ALTER SESSION"
66,SET OPTIMIZER_IGNORE_HINTS=TRUE;You can also disable PARALLEL hints in your SQL statements by setting OPTIMIZER_IGNORE_PARALLEL_HINTS to TRUE at the session or system level using ALTER SESSION or ALTER SYSTEM.
66,"For example, the following command enables PARALLEL hints in your session:"
66,ALTER SESSION
66,SET OPTIMIZER_IGNORE_PARALLEL_HINTS=TRUE;
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Automatic Indexing
66,Automatic indexing automates the index management tasks in Autonomous Database. Auto indexing is disabled by default in Autonomous Database.
66,"Creating indexes manually requires deep knowledge of the data model, application, and"
66,"data distribution. In the past, DBAs were responsible for making choices about which"
66,"indexes to create, and then sometimes the DBAs did not revise their choices or"
66,"maintain indexes as the conditions changed. As a result, opportunities for"
66,"improvement were lost, and use of unnecessary indexes could become a performance"
66,liability.
66,The automatic indexing feature in Autonomous Database monitors the application workload and creates and maintains
66,indexes automatically.
66,"Tip:For a ""try it out"" alternative that demonstrates these instructions, run Lab 14: Automatic Indexing in the Oracle Autonomous Database Dedicated for Developers and Database Users Workshop."
66,Enable Automatic Indexing
66,"Use the DBMS_AUTO_INDEX.CONFIGURE procedure to enable automatic indexing. For example, executing the below statement enables automatic indexing in a database and creates any new auto indexes as visible indexes, so that they can be used in SQL statements.EXEC DBMS_AUTO_INDEX.CONFIGURE('AUTO_INDEX_MODE','IMPLEMENT');"
66,Use the DBMS_AUTO_INDEX package to report on the automatic task and to set automatic indexing preferences.
66,Note
66,"Note:When automatic indexing is enabled, index compression for auto indexes is enabled by default."
66,"Disable Automatic IndexingUse the DBMS_AUTO_INDEX.CONFIGURE procedure to disable automatic indexing. For example, executing the below statement disables automatic indexing in a database so that no new auto indexes are created. However, the existing auto indexes remain enabled.EXEC DBMS_AUTO_INDEX.CONFIGURE('AUTO_INDEX_MODE','OFF');"
66,For more information see Managing Auto Indexes in Oracle Database Administrator’s Guide.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Fast Ingest
66,"Fast ingest optimizes the processing of high-frequency, single-row data inserts into a database. Fast ingest uses the large pool for buffering the inserts before writing them to disk, so as to improve data insert performance."
66,"The intent of fast-ingest is to support applications that generate lots of informational data that has important value in the aggregate but that doesn't necessarily require full ACID guarantees. Many applications in the Internet of Things (IoT) have a rapid ""fire and forget"" type workload, such as sensor data, smart meter data or even traffic cameras. For these applications, data might be collected and written to the database in high volumes for later analysis."
66,"Fast ingest is very different from normal Oracle Database transaction processing where data is logged and never lost once ""written"" to the database (that is, committed). In order to achieve the maximum ingest throughput, the normal Oracle transaction mechanisms are bypassed, and it is the responsibility of the application to check to see that all data was indeed written to the database. Special APIs have been added that can be called to check if the data has been written to the database."
66,"For an overview of fast ingest and the steps involved in using this feature, refer to Using Fast Ingest in Database Performance Tuning Guide."
66,"To use fast ingest with your Autonomous Database, you must:"
66,"Enable the Optimizer to Use Hints: Set the optimizer_ignore_hints parameter to FALSE at the session or system level, as appropriate."
66,"Depending on your Autonomous Database workload type, by default optimizer_ignore_hints may be set to FALSE at the system level. See Optimizer Statistics for more information."
66,Create a Table for Fast Ingest: Refer to Database Performance Tuning Guide for the limitations for tables to be eligible for Fast Ingest (tables with the specified characteristics cannot use fast ingest).
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Predefined Job Classes with Oracle
66,Scheduler
66,Autonomous Database includes predefined job_class values to use with Oracle Scheduler. These job classes let you group jobs that share common characteristics and behavior into larger entities so that you can prioritize among these classes by controlling the resources allocated to each class.
66,"With predefined job classes, you can ensure that your critical jobs have priority and enough resources to complete. For example, for a critical project to load a data warehouse, you can combine all the data warehousing jobs into one class and prioritize it over other jobs by allocating a high percentage of the available resources. You can also assign relative priorities to the jobs within a job class."
66,"The predefined job_class values, TPURGENT, TP, HIGH, MEDIUM and LOW map to the corresponding consumer groups. These job classes allow you to specify the consumer group a job runs in with DBMS_SCHEDULER.CREATE_JOB."
66,The DBMS_SCHEDULER.CREATE_JOB procedure supports
66,PLSQL_BLOCK and STORED_PROCEDURE
66,job types for the job_type parameter in Autonomous Database.
66,For example: use the following to create a single regular job to run
66,in HIGH consumer group:
66,BEGIN
66,DBMS_SCHEDULER.CREATE_JOB (
66,"job_name => 'update_sales',"
66,"job_type => 'STORED_PROCEDURE',"
66,"job_action => 'OPS.SALES_PKG.UPDATE_SALES_SUMMARY',"
66,"start_date => '28-APR-19 07.00.00 PM Australia/Sydney',"
66,"repeat_interval => 'FREQ=DAILY;INTERVAL=2',"
66,"end_date => '20-NOV-19 07.00.00 PM Australia/Sydney',"
66,"auto_drop => FALSE,"
66,"job_class => 'HIGH',"
66,comments => 'My new job');
66,END;
66,Notes for Oracle Scheduler:
66,To use DBMS_SCHEDULER.CREATE_JOB additional
66,grants for specific roles or privileges might be required.
66,The ADMIN user and users with
66,DWROLE have the required
66,CREATE SESSION and CREATE
66,JOB privileges. If a user does not have
66,DWROLE then grants are required for
66,CREATE SESSION and CREATE
66,JOB privileges.
66,The instance_id job attribute is
66,ignored for Oracle Scheduler jobs running on Autonomous Database.
66,See Scheduling Jobs with
66,Oracle Scheduler for more information on Oracle Scheduler and
66,DBMS_SCHEDULER.CREATE_JOB.
66,See SET_ATTRIBUTE
66,Procedure for information on job attributes.
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Performance Monitoring and Tuning Tools
66,"Several situations such as changing workloads, resource limitations on application and database servers, or simply network bottlenecks can give rise to application performance issues. Oracle provides a wide range of tools to help you monitor performance, diagnose performance issues, and tune your application or the database to resolve the issue. Some of them are listed below:"
66,Tool
66,Details
66,Performance Hub
66,"A readily available feature-rich tool that is available in the Oracle Cloud Infrastructure (OCI) console. Performance Hub also comes in-built with Oracle Database Actions, Oracle Enterprise Manager and Oracle Management Cloud."
66,See Monitor Database Performance with Performance Hub for more details.
66,Autonomous Database Metrics
66,"The Autonomous Database Metrics help you measure useful quantitative data, such as CPU and storage utilization, the number of successful and failed database log in and connection attempts, database operations, SQL queries, and transactions, and so on. You can use metrics data to diagnose and troubleshoot problems with your Autonomous Database resources."
66,"See Monitor Databases with Autonomous Database Metrics for more information such as it prerequisites, usage, and the list of metrics available for Autonomous Database on Dedicated Exadata Infrastructure."
66,Automatic Workload Repository (AWR) and Automatic Database Diagnostic Monitor (ADDM)
66,Two other commonly used tools are the Automatic Workload Repository (AWR) and the Automatic Database Diagnostic Monitor (ADDM).
66,"AWR stores performance related statistics for an Oracle database, and ADDM is a diagnostic tool that analyzes the AWR data on a regular basis, locates root causes of any performance problems, provides recommendations for correcting the problems, and identifies non-problem areas of the system. Because AWR is a repository of historical performance data, ADDM can analyze performance issues after the event, often saving time and resources in reproducing a problem."
66,"For instructions on using these tools, as well as detailed information about database performance monitoring and tuning, see Oracle Database Performance Tuning Guide."
66,AWR and ADDM are also available from Performance Hub. See Monitor Database Performance with Performance Hub for more details.
66,"For a quick introduction to database performance monitoring and tuning, see Oracle Database 2 Day + Performance Tuning Guide."
66,Parent topic: High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,High Performance Features in Autonomous Database on Dedicated Exadata Infrastructure
66,Predefined Database Services
66,Connection Pools
66,Special-Purpose Connection Features
66,SQL Performance Tuning Features
66,SQL Tracing
66,Optimizer Statistics
66,Optimizer Hints
66,Automatic Indexing
66,Fast Ingest
66,Predefined Job Classes with Oracle
66,Scheduler
66,Performance Monitoring and Tuning Tools
66,"Copyright © 2024, Oracle and/or its affiliates."
66,About Oracle
66,Contact Us
66,Legal Notices
66,Terms of Use & Privacy
66,Document Conventions
67,How to Improve PHP Performance: Best Practices for PHP Apps | Zend by Perforce
67,Skip to main content
67,"Created with Avocode.Secondary NavigationPHP Security CenterBlogStoreDownloadsDownloadsPluginsMyZend AccountCompanyAbout Zend by PerforceCareers at PerforceCustomersPartnersPressContactContact UsRequest SupportSubscribeZendMain Navigation - Mega MenuProductsMain Navigation - Mega MenuZendPHPPHP Runtime and SupportPHP LTSPatches for EOL PHPZend ServerPHP Application ServerLaminas Enterprise SupportFormerly Zend FrameworkServicesMain Navigation - Mega MenuService OverviewMigration ServicesAuditsCI/CDCustom ConsultingInnovate faster and cut risk with PHP experts from Zend Services.Explore ServicesSolutionsMain Navigation - Mega MenuPHP Cloud SolutionsPHP Container SolutionsPHP Security SolutionsWindows SolutionsHosting Provider SolutionsSee How Zend Helps Leading Hosting Providers Keep Their Managed Sites on Secure PHPRead MoreTrainingMain Navigation - Mega MenuTraining OverviewLearn PHP from PHP experts with free, on-demand, and instructor led courses.Explore TrainingResourcesMain Navigation - Mega MenuExplore ResourcesEvents & WebinarsPapers & VideosRecorded WebinarsBlogOrchestrating Your PHP ApplicationsWatch NowSupportMain Navigation - Mega MenuExplore SupportPHP Long-Term SupportKnowledgebaseDocumentationDownload SoftwareDownload PluginsRequest SupportSubmit support requests and browse self-service resources.Explore SupportTry FreePHP Security CenterBlogStoreDownloadsMain Navigation - Mega MenuDownloadsPluginsMyZend AccountDownloadsPluginsMyZend AccountCompanyMain Navigation - Mega MenuAbout Zend by PerforceCareers at PerforceCustomersPartnersPressAbout Zend by PerforceCareers at PerforceCustomersPartnersPressContactMain Navigation - Mega MenuContact UsRequest SupportSubscribeBreadcrumbHomeResourcesBlog"
67,How To Improve PHP Performance: Best Practices For PHP Apps
67,"August 17, 2023"
67,How to Improve PHP Performance: Best Practices for PHP AppsPHP DevelopmentBy
67,"Massimiliano CavicchioliPHP performance, or PHP web app performance, is one of the most important parts of modern PHP development. From user experience and satisfaction, to scalability and server costs, ensuring your PHP application is performing its best is critical.In this blog we discuss PHP performance basics, including the basics of web app performance, why it's important, and the various metrics used in assessing performance. Then we look at performance considerations when building new applications and performance tuning tips for teams working on existing applications. Let's dive in.Table of ContentsWhat Is PHP Performance?Best Practices for PHP App Performance When Building New Applications8 PHP Performance Tuning Best PracticesFinal ThoughtsTable of Contents1 - What Is PHP Performance?2 - Best Practices for PHP App Performance When Building New Applications3 - 8 PHP Performance Tuning Best Practices4 - Final ThoughtsBack to topWhat Is PHP Performance?PHP performance refers to the efficiency in which a web application built using PHP functions in terms of speed, responsiveness, and overall resource utilization.Why Is PHP Performance Important?PHP app performance, and its associated best practices, are vital for your users to have a good experience. In a sense, web performance can be considered a subset of web accessibility.  When looking at PHP performance, as with accessibility, you consider what device a site visitor is using to access the site and the device connection speed. Reducing the download and render time of a site improves conversion rates (rate at which site visitors perform a measured or desired action) and user retention. Ultimately, performance directly impacts this journey to a desired outcome — with improvements to performance improving conversion.  For web applications, users often expect a site to load in two seconds or less. These same site visitors begin abandoning slow sites at 3 seconds — and site speed is just one factor that can impact your outcomes. If the site is slow to react to user interaction, or appears broken, this can cause users to lose interest and trust, ultimately leaving the site or web app for good.8 Key Web Application Performance MetricsSo how can teams measure PHP web app performance? There are a variety of well established metrics that teams can use to measure overall web app performance, including:MetricDescriptionUser SatisfactionUses a mathematical formula to determine overall user satisfaction. Also commonly called an ""Apdex"" score.Time to First ByteMeasures the time it takes to render the application on end-user devices.Time to First PaintMeasures how long it takes for the initial pixels to show on the screen.Speed IndexChecks how readily the above-the-fold content appears on a screen.Time to InteractMeasures how much time passes before a web page is fully interactive.DNS Lookup TimeMeasures the time it takes for a domain lookup to occur while the browser loads the page.Error RateTracks the percentage of request issues you incur in relation to the overall number of requests.Peak Response TimeMeasures the longest response time for a total number of requests that travel across the server.Back to topBest Practices for PHP App Performance When Building New ApplicationsWith the above factors in mind, it may seem daunting to develop a functional and highly performant PHP app. The good news is that this is something that is top of mind for nearly every web app developer working today, so you'll have plenty of information to go off of. For teams just getting started on building a new application, these 5 PHP performance best practices are worth considering:1. Optimize Your CodeOne of the most-commonly overlooked ways to improve your PHP performance is in optimizing your web application code.Here are some suggestions to help you with the task:Choose a performant PHP framework natively supporting highly performant architectures, possibly based on the Middleware pattern instead of the MVC one (Laminas, Mezzio, Laravel 10.x, Symfony 6.x, etc...)Code your application to be stateless (no session mechanism active)Prevent the application from running out of memoryAvoid running queries in a loop; use native, optimized, SQL queries insteadUnderstand how the Swoole library works (OpenSwoole, Swoole), integrate it properly in your Web application (all the frameworks cited above support it) and the gain can be 10 fold2. Transition to HTTP/2 for supporting HTTP requestsOne of the easiest ways to architect for performance when developing a new PHP application is to use the updated Hypertext Transfer Protocol (HTTP) HTTP/2.Compared to HTTP/1.x, HTTP/2 is a more powerful and offers better overall web application performance. HTTP/2 offers the following key advantages over HTTP/1.x:It reduces latency by enabling multiplexing of requests and responsesHTTP/2 compresses the HTTP header fields effectively, which reduces overheadsIt allows request prioritizationHTTP/2 includes more protocol enhancements for better performance 3. Cache, Cache, CacheCaching is an important aspect of improving PHP app performance.  At a basic level, caching involves hosting a version of files in a temporary storage location to reduce latency and bandwidth consumption. This in turn can improve overall app performance and user experience.When developing your caching strategy, be sure to evaluate the following caching techniques:Network caching (usually applicable on network devices like LBs, Routers, Switches, etc...)HTTP caching (the HTTP cache stores a response associated with a request and reuses the stored response for subsequent request)Reverse proxy server caching (acting as a surrogate, the proxy server intercepts user requests arriving from the Internet, forwards them to the appropriate content host, caches the returned data for re-use, and delivers that data to the requesting user)Database caching (improves scalability by distributing query workload from backend to multiple cheap front-end systems - possibly in-memory - enanching the flexibility in the processing of the  proprietary data)The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance — in terms of both both speed and scalability — can be dramatically improved.4. Focus on Mobile Devices First, Desktop SecondDo you try to optimize your website for desktop first, and then look at the mobile user experience? Optimizing the user experience including the performance for mobile devices is harder.Focus on mobile devices first. Remove common performance bottlenecks and test the site on mobile devices. You can then easily optimize it for the desktop afterward.5. Ensure Your Front-End Is OptimizedWeb browsers use HTTPS (secure hypertext transfer protocol) requests to fetch parts of the webpage from the web server — including things like images, stylesheets, scripts, etc.. Depending on the needs of the site, you may have a lot of large graphic files you need to load. But incorrectly handling those large graphic files can make your website load slowly.Regardless of the file type, how and when it's loaded can impact the overall page loading speed. For images, that may mean choosing the appropriate resolution for different devises, or, or even changing the file size or type being served in given situations. A few examples to consider when optimizing your web front-end include:“Minifying” the size of files (e.g. JavaScript and CSS files) by removing or shortening symbols.Combine different JavaScript and CSS files into a single bundle.If necessary, create different bundles by following the above process.Implement the asynchronous loading of JavaScript. This involves the browser parsing the webpage even when the JavaScript is being loaded.Use a Content Delivery Network  for static content: when users access your site from around the world, a CDN can decrease the time it takes to serve your page to those users.Back to top8 PHP Performance Tuning Best PracticesYou can make key software engineering decisions for better web application performance, and using the right tools is the most important of them.  As briefly stated above, choosing the right development framework is a great beginning to create the necessary architectural convergence centered on performance.The next fundamental step is the choice of a PHP monitoring tool capable of delivering the best actionable data helping engineers to identify the problematic part of the code and web interactions, in order to satisfy the important metrics we specified above. It's also important to note that web application performance tuning isn’t a one-time activity. You need to monitor the key metrics routinely and take appropriate steps in every release.For teams looking to improve performance on existing applications, here are a few best practices to consider:1. Profile Your CodeIn software engineering, profiling (""program profiling"", ""software profiling"") is a form of dynamic program analysis that measures, for example, the space (memory) or time complexity of a program, the usage of particular instructions, or the frequency and duration of function calls.  Most commonly, profiling information serves to aid program optimization, and more specifically, performance engineering. Profiling is achieved by instrumenting either the program source code or its binary executable form using a tool called a profiler (or code profiler).  Profilers may use a number of different techniques, such as event-based, statistical, instrumented, and simulation methods.Schedule a Custom ZendHQ Demo for Your TeamReady to see how ZendHQ can help your team improve PHP app performance? Schedule a custom demo via the link below.SCHEDULE YOUR ZENDHQ DEMO 2. Refactor Your CodeOnce code bottlenecks are identified via monitoring and/or profiling tools, developers must refactor the code to eliminate those issues..  One common use case for code refactoring is in the optimization of DB queries. TTFB and overall DB queries can always be optimized, improving the TTFB and the overall scalability of the system.While writing the code (before it is analyzed by APM tools) make sure to avoid as much as possible the use of loops especially when associated with DB queries. When code loops are necessary, make good use of features like generators, coroutines or async processing in order to minimize memory footprint, speed, and, ultimately, user experience.3. Evaluate and Optimize Your Caching StrategiesA good code monitoring tool will help engineers understand cache ‘hit and miss’ and performance gains for the caching strategies applied at the code level like DB queries caching, data caching, and page caching.Iteratively, misses must be reduced to zero and more code sections must be added to the various caching buckets in a continuous effort to deliver the best performances at first commit.4. Inspect and Optimize Your Server ConfigurationAnother aspect a valuable monitoring tool can help with is allowing engineers to understand how server configurations can be enhanced for improved performances.Usage of memory, access to the filesystem, DB queries connections and overall timings, etc., once properly presented, can give all the necessary hints on how to tweak the servers' configurations both at the PHP level and the web server level.A good APM will deliver sufficient data for engineers to effectively optimize connection pools configurations, memory limits, persistent connections, TCP configurations, SSL terminations and more.5. Write Clean CodeWhen coding, it is good practice to write it so that you can understand it when you simply read it on a printed page or on a screen, without having to refer to anything else.  If the code is simple and concise, both the programmer and the compiler can understand it easily. Code that is easy for the compiler to understand is also easy for it to optimize and make it performant.6. Use the Right Data TypesUse the right data types; data types are the key to optimizing code performance.  By using the right data types, you can minimize the number of conversions that take place and make your code more efficient. In general, you should use the smallest data type that can accurately represent the data you are working with.7. Avoid Unnecessary I/OAvoid unnecessary I/O, making sure that your code only reads from and writes to the filesystem when it absolutely needs to (possibly in async/deferred jobs).8. Use the Latest PHP Version Whenever PossibleIf possible, always use the latest PHP versions. Taking this approach usually guarantees the best overall performance for PHP applications. As an example, PHP 8 introduced the JIT compiler, then followed up with additional performance improvements in PHP 8.1 and PHP 8.2.Back to topFinal ThoughtsFrom hosting costs to user experience, performance can ultimately mean success or failure for many web applications. While we've outlined a number of ways to improve PHP performance here, it's ultimately just a starting point. Constantly assessing and improving your PHP performance — including finding ways to improve your optimization efforts — needs to be a priority.If you're not sure where to start, or you've inherited a nightmare application and need help, Zend offers a variety of products and professional services that can get you on the right track.Need Help With Your PHP Performance?Zend can help. Contact us today to get started.Contact UsAdditional ResourcesBlog - Changes to Watch in PHP 8.3Blog - Why Good PHP Monitoring MattersBlog - Cloud Orchestration Basics for PHP AppsBlog - Exploring ZendHQ Role-Based Authentication FeaturesResource Collection - PHP Versions: Performance, Security, and Feature ComparisonsOn-Demand Webinar - How Queueing Enables PHP at ScaleWhite Paper - Planning Your Next PHP MigrationBack to topMassimiliano Cavicchioli Software Architect Massimiliano Cavicchioli is a Software Architect at Zend. He has over 20 years of experience in the tech world, 16 of which he spent contributing to the PHP ecosystem at Zend. During his many years of providing consultation to enterprise corporations, Massi has developed a wealth of tangible knowledge — making him highly capable in helping companies achieve best practices in both PHP and OSS.Do you want to write for the Zend PHP blog?Contact UsFooter menuProductsZendPHPPHP Long-Term SupportZend ServerLaminas Enterprise SupportPHP Development ToolsResourcesPHP Security CenterPapers & VideosEvents & WebinarsRecorded WebinarsBlogCase StudiesServicesPHP Long-Term SupportMigration ServicesAuditsCI/CD ServicesCustom ConsultingDownloadsMyZend AccountPluginsContainer RegistryTrainingSupportPHP Long-term SupportKnowledgebaseDocumentationStoreHubsPHP Versions GuideDeveloping Web Apps With PHPGuide to PHP and IBM iFREE TRIALSZendPHPZend ServerCompanyAbout Zend by PerforceCareers at PerforceCustomersPartnersPressContactRequest SupportSubscribeZend by Perforce ©"
67,"Perforce Software, Inc.Terms of Use  |  Privacy Policy | SitemapSocial MenuFacebookLinkedInTwitterYouTubeRSSSend Feedback"
68,PostgreSQL vs. Oracle Database: Choosing the Ideal Database Solution for Your Business | In-Depth Comparison and Analysis
68,"Definitive Guide on ""How to Select the Right Data Warehouse?"" Click here ->UsecasesSales Analytics Marketing AnalyticsOperations AnalyticsData ManagementIntegrationsPricingCustomersResourcesBlogsData SecurityDocumentationFree TrialData EngineeringDecember 4, 2023Postgres vs. Oracle: An In-Depth Comparison for Database ManagementBlogsData EngineeringBlog PostIndex of ContentsRelated articlesThis is another article on the siteThis is also a headingThis is a headingIn today's digital age, effective database management is crucial for businesses to thrive. Two of the most prominent database management systems, PostgreSQL (Postgres) and Oracle DB, have gained widespread recognition for their robustness, performance, and scalability. Both Postgres and Oracle offer a plethora of features and functionalities, making them the preferred choices for enterprises and organizations across the globe. In this comprehensive comparison, we'll delve into the strengths and weaknesses of each system, helping you make an informed decision on which database solution best suits your specific needs.Overview of PostgresPostgreSQL, commonly referred to as Postgres, is an open-source object-relational database management system known for its extensibility, reliability, and SQL compliance. Developed in the early 1980s at the University of California, Berkeley, Postgres has evolved over the years, gaining a loyal community and commercial backing. As an open-source solution, it offers a cost-effective alternative for organizations looking to minimize licensing costs.1.1 Key Features of Postgresa) Extensibility: Postgres allows users to define custom data types, operators, and functions, enabling seamless integration with unique business requirements.b) ACID Compliance: Postgres adheres to the ACID (Atomicity, Consistency, Isolation, Durability) principles, ensuring data integrity, scalability operations and transactional reliability.c) Replication: The built-in replication capabilities of Postgres enable easy data redundancy and high availability for critical applications.d) JSON Support: Postgres offers native support for JSON data types, making it an ideal choice for applications dealing with semi-structured data.e) Full-Text Search: Postgres provides robust full-text search capabilities, allowing users to efficiently search large volumes of textual data.Overview of Oracle Database Management SystemsOracle DB, developed by Oracle Corporation, is one of the most widely used relational database management systems (RDBMS). It has a long-standing presence in the industry, catering to the needs of large enterprises and mission-critical applications. As a commercial solution, Oracle DB provides extensive support and a wide range of advanced features, making it a top choice for businesses with high-performance demands.2.1 Key Features of Oracle Database Management Systemsa) Scalability: Oracle DB excels in handling large-scale enterprise applications with high concurrency and heavy workloads.b) High Availability: Oracle offers various features like Real Application Clusters (RAC) and Data Guard, providing fault tolerance and data redundancy.c) Security: Oracle DB provides robust security mechanisms, including fine-grained access controls, encryption, and auditing features, ensuring data protection and compliance.d) In-Memory Option: Oracle's In-Memory option enhances performance by storing critical data in memory, significantly reducing query response times.e) Partitioning: Oracle offers advanced data partitioning techniques, which optimize data retrieval and storage efficiency.Performance ComparisonPerformance is a critical aspect when choosing a database management system, as it directly impacts the overall efficiency of an application or business operation.3.1 Postgres PerformancePostgres is known for its stability and reliability, making it suitable for various workloads. It performs exceptionally well for read-intensive applications and small to medium-sized databases. However, in scenarios with extremely complex queries or heavy write loads, Postgres might experience performance bottlenecks, especially when dealing with large-scale datasets.To optimize performance, Postgres allows users to fine-tune configurations and leverage extensions for specific use cases. Its ability to handle advanced indexing, such as B-tree, hash, and generalized search trees, significantly enhances query performance.3.2 Oracle DB PerformanceOracle DB is renowned for its exceptional performance, especially in high-throughput, write-intensive scenarios. With its advanced indexing techniques, efficient query optimizer, and parallel processing capabilities, Oracle can handle massive workloads with ease. The In-Memory option further enhances its performance by speeding up query and spatial data processing, and reducing response times for analytical and reporting tasks.Moreover, Oracle's ability to scale horizontally with Real Application Clusters (RAC) and distribute data across multiple nodes ensures optimal performance for applications demanding high availability and scalability. This feature makes Oracle the preferred choice for mission-critical systems, large enterprises, and data-intensive applications.In summary, while Postgres is reliable and performs well for medium-sized workloads and read-intensive applications, Oracle outshines in high-throughput scenarios, especially with its In-Memory option and advanced scalability features.Database SecurityDatabase security is paramount in today's data-driven world, where sensitive information needs protection from unauthorized access, manipulation, or breaches.4.1 Postgres SecurityAs an open-source database, Postgres benefits from its large community of developers, which ensures that vulnerabilities are promptly addressed through regular updates and patches. Postgres provides robust security features, including role-based access control (RBAC), SSL encryption, and data masking. While these features are adequate for many use cases, some enterprises may require additional security options that are more readily available in commercial databases.4.2 Oracle DB SecurityOracle Database takes security to another level with its comprehensive set of security features. It offers advanced access controls through fine-grained privileges, Virtual Private Database (VPD), and transparent data encryption to secure data at rest. Additionally, Oracle Advanced Security provides network encryption and strong authentication mechanisms, ensuring secure data transmission over networks.Oracle Database's security features are designed to meet the stringent requirements of highly regulated industries, making it a preferred choice for organizations dealing with sensitive data and compliance mandates.Licensing and Cost ConsiderationsWhen selecting a database management system, understanding the licensing and cost implications is crucial, as it directly impacts the overall budget and ROI.5.1 Postgres Licensing and CostPostgres stands out as an open-source database, which means it is free to use and distribute. The absence of licensing fees can significantly reduce the overall cost of ownership, especially for small to medium-sized businesses or projects with limited budgets. However, it is essential to consider other expenses, such as support, maintenance, and the potential need for third-party tools or extensions.5.2 Oracle DB Licensing and CostOracle Database is a commercial product, and its licensing model can be complex. It typically involves various editions, options, and user-based or processor-based licensing models. While Oracle Database's advanced features and robust support justify its cost for enterprises with demanding requirements, the licensing fees can be substantial, especially for large-scale deployments.It is crucial for organizations to carefully evaluate their needs and compare the features and costs of both databases before making a decision.Community and SupportThe strength of the community and the availability of support resources are essential considerations when selecting a database management system.6.1 Postgresql Supports and CommunityPostgres has a vibrant and active open-source community that continuously contributes to its development and improvement. The community provides extensive documentation, forums, and mailing lists, making it easy to find help and solutions to common issues.Additionally, various companies offer commercial support packages for Postgres, providing professional assistance and ensuring smooth operation for critical systems.6.2 Oracle DB Community and SupportOracle Corporation offers robust commercial support for Oracle DB, with various support tiers tailored to different needs. Enterprises can benefit from 24/7 technical assistance, software updates, and access to My Oracle Support, a comprehensive knowledge base.While the Oracle community is not as extensive as Postgres due to its proprietary nature, the available resources and support from the vendor itself make up for it.7. Cross-Platform CompatibilityCross-platform compatibility is an important consideration for businesses that operate across multiple operating systems and environments.7.1 Postgres Cross-Platform CompatibilityPostgres is highly portable and can run on various operating systems, including Linux, Windows, macOS, and Unix-based systems. Its full open source software all-source nature allows it to be easily adapted and compiled for different platforms, ensuring flexibility in deployment.7.2 Oracle DB Cross-Platform CompatibilityOracle DB is also designed to be compatible with various operating systems, making it suitable for diverse IT infrastructures. However, as a commercial product, certain features and optimizations may be more tailored to specific platforms. Organizations with a heterogeneous environment should ensure that Oracle supports their desired platforms.Data Replication and High AvailabilityData replication and high availability are crucial for ensuring business continuity and minimizing downtime in case of hardware failures or disasters.8.1 Postgres Data Replication and High AvailabilityPostgres offers several built-in replication options, such as Streaming Replication, Logical Replication, and Replication Slots. These features provide data redundancy and failover capabilities, enabling organizations to achieve high availability with ease.8.2 Oracle DB Data Integrity and High AvailabilityOracle DBprovides robust data replication and high availability solutions, such as Oracle Data Guard and Real Application Clusters (RAC). Oracle Data Guard facilitates synchronous and asynchronous replication for disaster recovery, while RAC allows load balancing and fault tolerance through the distribution of data across multiple nodes.Both Postgres and Oracle DB offer efficient replication and high availability mechanisms, allowing organizations to choose the one that best aligns with their specific requirements and budget.Ecosystem and Third-Party IntegrationThe ecosystem and availability of third-party integrations can significantly impact the overall development and integration process.9.1 Postgres Ecosystem and Third-Party IntegrationPostgres boasts a vast ecosystem with numerous extensions, plugins, and tools developed by its active community. Many popular tools, frameworks, and libraries seamlessly integrate with Postgres, making it a versatile choice for developers. Additionally, Postgres supports various data formats, including JSON and Geospatial data, further expanding its application potential.9.2 Oracle DB Ecosystem and Third-Party IntegrationOracle DB benefits from its extensive ecosystem, with a wide array of tools, libraries, and technologies that integrate smoothly with the database. Oracle's robust Application Development Framework (ADF) and support for Java, .NET, and other programming languages make it a favored choice for enterprises with diverse application needs.ConclusionIn the battle of Postgres vs. Oracle, both databases have their unique strengths and weaknesses. Postgres, being an open-source solution, provides cost-effectiveness, flexibility, and a thriving community. It is well-suited for small to medium-sized businesses and applications that demand read-intensive workloads.On the other hand, Oracle DB excels in performance, scalability, and security, making it an ideal choice for large enterprises, mission-critical applications, and data-intensive workloads. Its extensive feature set and advanced support options cater to the needs of organizations with high-performance demands and stringent security requirements.Frequently Asked Questions (FAQ)1. What is PostgreSQL (Postgres) and how does it differ from Oracle DB? PostgreSQL, commonly known as Postgres, is an open-source object-relational database management system celebrated for its extensibility, reliability, and adherence to SQL standards. It differs from Oracle DB as it is open-source, allowing cost-effective implementation and customization through a thriving community.2. What are the key features of PostgreSQL? Postgres offers features such as extensibility for custom data types, ACID compliance ensuring data integrity, built-in replication for redundancy, native support for JSON data types, and robust full-text search capabilities.3. What is Oracle DB, and what are its standout features? Oracle DB, developed by Oracle Corporation, is a widely used commercial relational database management system known for its scalability, high availability, and robust security mechanisms. Its standout features include scalability for large enterprise applications, high availability through Real Application Clusters (RAC) and Data Guard, advanced security with encryption and fine-grained access controls, an In-Memory option for improved performance, and advanced data partitioning techniques.4. How do PostgreSQL and Oracle Database compare in terms of performance? PostgreSQL is renowned for stability and reliability, performing well for read-intensive applications and medium-sized databases. However, in scenarios with complex queries or heavy write loads, it might experience performance limitations, especially with large datasets. Oracle Database excels in high-throughput scenarios, especially with its In-Memory option, efficient query optimization, and scalability features, making it ideal for data-intensive applications and large enterprises.5. What are the security features offered by PostgreSQL and Oracle Database? PostgreSQL provides robust security features like role-based access control (RBAC), SSL encryption, and data masking. Oracle Database takes security to another level with fine-grained privileges, Virtual Private Database (VPD), transparent data encryption, network encryption, and strong authentication mechanisms, designed to meet the stringent requirements of highly regulated industries.Written by Soham DuttaDatabasesRelated PostsMarch 24, 2023PostgreSQL FunctionMay 18, 2023PostgreSQL vs DynamoDBMarch 3, 2023MongoDB vs PostgreSQL: Choosing the Right Database for Your ProjectCreate Your Free AccountIngest, Transform and Analyze data without writing a single line of codeFree TrialJoin our CommunityGet help, network with fellow data engineers, and access product updates.Sign UpBlogsMetabase vs Power BI: A Comprehensive Comparison Between Two Most Used Business Intelligence ToolsMetabase vs Redash: Their Features and Functions, Origin and Key DifferencesMySQL vs SQL Server: A Comprehensive Comparison of Two Leading Relational Database Management SystemsThe Role of Analytics in Marketing: Types Of Analytics, Marketing Analytics Process and ToolsRole of Analytics in Operations Management: Its Uses, Key Components, And Real-World ExamplesWhat is Data Visualization: Unlocking Insights through Visual RepresentationSales Analysis Demystified: How to Transform Data into Actionable InsightsPower of Effective Data CleaningExploring the Realm of Business Analytics: What is Business Analytics, Its Process and ToolsUnveiling the Power of Ad Hoc Analysis: A Comprehensive GuideMastering Trend Analysis: A Comprehensive Guide to Uncover InsightsData Analyst vs Business Analyst: A Comprehensive Guide to Help You Choose the Right Career PathA Comprehensive Guide to Help You Understand Types of Data AnalysisUsing Agile Analytics to Deliver Business-Focused Solutions7 Best Tableau Alternatives in the Market9 Best Data Exploration Tools: Their Advantages, Disadvantages & PricingThe List of Metabase AlternativesThe Power of Advanced AnalyticsData Democratization: Unlocking Access to Empower Decision-MakingWhat is Embedded Analytics? Its Benefits & ToolsUncovering The Power Of Data GovernanceBusiness Dashboards Decoded: Visualizing Success in Data-Driven Decision-MakingMariaDB vs MySQL: A head-to-head comparison for data enthusiasts Tableau vs Excel: Deciphering Data Analysis - Choosing the Right"
68,Data Analysis Tool for InsightsUnlocking Insights: A Guide to Self-Service Analytics PostgreSQL vs. SQL Server: A Comprehensive Comparison of Database SystemsWhat is Indexing in MongoDB: A Comprehensive Guide
68,Marketing Analytics Tools: The Ultimate Guide to Help You Choose the Right Marketing Analytics Tool What is a Datamart?A Beginner's Guide to Creating Databases in MongoDBDatabricks vs. Snowflake: A Comprehensive Comparison for Data-Driven Decision-MakingPostgreSQL Guide: Getting Started with a Powerful Open Source DatabaseOracle Data WarehouseGoogle Data Lake: A Deep Dive into Google's Powerful Data Storage SolutionDelta Lake vs. Data Lake: Unraveling the Differences and BenefitsBest ETL CertificationsUnveiling the Power of Cortex Data Lake: A Comprehensive GuideThe Essential Need of Data Warehouses: Unleashing the Power of DataPostgreSQL TEXT vs. VARCHAR: Choosing the Right Data Type for Your DatabaseData Warehouse as a Service (DWaaS): Transforming Analytics with the CloudUnlocking Valuable Insights with BigQuery: A Comprehensive GuideUnmasking the Mystery: Why MongoDB is Not Recognized in Your Application?Navigating Data Excellence: A Comprehensive Study of Amazon Data Warehouse SolutionsPostgreSQL Schema vs. Database: Unveiling the Distinctions for Effective Data ManagementData warehouse Modernization - Revolutionizing Data Management for the Digital AgeMongoDB vs. SQLite: Choosing the Right Database for Your ApplicationData Pipeline vs. ETL: Navigating the Modern Data Integration LandscapeMongoDB vs. Mongoose: Understanding the Differences and Use CasesHealthcare Data Warehouse Solutions: Extracting Insights for Better CareHadoop vs. MongoDB: A Comprehensive Comparison for Big Data and NoSQLBigtable vs. BigQuery: A Comprehensive Comparison for Data Management and AnalyticsMongoDB vs. Lucene: A Comparative Analysis for Data ManagementWhat is a Data Science Pipeline: Unraveling the Path to Data-Driven InsightsTypes of Data Warehouses: Understanding the Building Blocks of Modern Data ManagementGoogle Big Query v/s Azure Synapse - A comparative study of two prominent data warehouse solutionsBuilding a Data Warehouse: A Comprehensive Guide and the Buy vs. Build DilemmaDemystifying the ETL Process in Data Warehousing: A Comprehensive Guide Modern Data WarehouseMongoDB vs Oracle: A Comparative Analysis of Two Leading Database SystemsData Mesh vs Data Lake: Understanding Two Data Management ApproachesAzure Data WarehousePostgres vs. Oracle: An In-Depth Comparison for Database ManagementAthena vs. Redshift: Unraveling the Battle of Cloud Data WarehousesUnderstanding Enterprise Data WarehouseRedis vs PostgreSQL: Comparing Two Powerful Database TechnologiesData Mart vs. Data Lake: Unveiling the Differences and BenefitsAWS DATA WAREHOUSE Aurora vs. Postgres: A Comprehensive ComparisonCockroachDB vs. PostgreSQL: A Comprehensive ComparisonChoosing the Right Data Lake Tools in 2024: A Comprehensive GuideMongoDB vs. DocumentDB: A Comprehensive Comparison for Choosing the Right NoSQL DatabaseMongoDB vs MariaDB: A Comprehensive Comparison for Modern Database SolutionsWhat is OLAP Cube?Couchbase vs MongoDB: A Comprehensive Comparison of Leading NoSQL DatabasesMongoDB vs Redis: A Comprehensive Comparison for Modern Database SolutionsBuilding an Efficient Data Warehouse Architecture: Optimising Performance and AccessibilityETL Data Pipeline: Simplifying Data Integration and TransformationData Lake vs. Lakehouse: Unraveling the DifferencesWhat is ETLData Fabric vs. Data Lake: Unraveling the Differences and BenefitsWhat is Data Warehouse? - A Detailed ExplainationData pipeline Architecture How to Build Efficient Data Pipeline with SprinkleWhat is Databricks? Functionalities and ImportanceSnowflake vs AthenaSnowflake vs Bigquery: A Comprehensive Cloud Data Warehouses ComparisonRedshift vs BigQueryPostgreSQL vs DynamoDBMongoDB vs DynamoDB - 11 Major DifferencesMongoDB vs Elasticsearch : 2024 ComparisonWhy is digital marketing analytics useful?Data Lake vs Data Warehouse vs Data Mart - A Comprehensive Comparison10 Best Fivetran Alternatives for 2024 | A Comprehensive Comparison of the Best ETL Tools in Market 2024Cassandra vs. MongoDB: A Detailed Comparison8 Best Data Warehouse Software You Should Know in 2024Best Data Pipeline Tools for 20249 Best Hevodata Alternatives for 2024Azure Data Warehouse: A Comprehensive GuideWhat is Reverse ETL? 10 Advantages of Modern Marketing Data StackPostgres vs. Oracle: An In-Depth Comparison for Database ManagementProductIntegrationsPricingData SecurityUsecasesSales AnalyticsMarketing AnalyticsOperations AnalyticsData ManagementPipelineComparisonHevoFivetranStitchAWS GlueAzure Data FactoryCompareTableauPowerBIGoogle Data StudioMetabaseRedashSupersetResourcesRequest DemoGDPRPrivacyTerms of ServiceCopyright © 2024 Sprinkle Data. All rights reserved.
70,Optimize PostgreSQL Performance with Datasentinel
70,Home
70,"FeaturesSession HistoryOur active session history feature allows for precise information on the workload and quick identification of bottlenecks and associated consuming queries.Top QueriesExplore your most consuming queries from various dimensions and gain valuable insights into their performanceLock ExplorerLock explorer makes it easy to identify blocking sessions during blocked session issues, along with the associated queries and locks.AlertingThe alerting feature offers a user-friendly, customizable monitoring solution for PostgreSQL databases, enabling quick setup, extensive monitoring of critical metrics, and flexible notification options.Live 360Gain real-time access and control over PostgreSQL instances for instant visibility of your clustersReportingThe reporting feature allows you to define different types of activity reports for analysis purposesTop Tables & IndexesEasily identify the most accessed tables, indexes, partitions, and materialized views through customizable grouping dimensionsCluster & System MetricsOur tool is designed to comprehensively gather a wide array of internal cluster metrics, ensuring no detail is overlookedExplore theOther Featuresas wellTips & Hintsin our detailed documentation"
70,"AvailabilityOn-premisesUsing Datasentinel in an on-premises mode gives you full control over your data, ensuring maximum privacy and compliance with local data regulations. It allows for deep customization to fit into existing workflows, and offers low-latency, real-time monitoring and performance tuning of your PostgreSQL databases.SaaSUsing Datasentinel in SaaS mode offers hassle-free setup, with no need for on-premises infrastructure. It provides PostgreSQL monitoring and performance tuning from anywhere, ensuring data security with encrypted connections.For in-depth technical details, please refer toour detailed architecture documentationDocsChangelogBlogAboutGet StartedENFRGet StartedFeaturesExplore our product's key features: a carefully curated list designed to meet your monitoring and optimization needs.Session HistoryComprehensive Active Session History Timeline for Enhanced Database Performance InsightsLearn More"
70,Top QueriesPinpoint and Optimize Your Top QueriesLearn More
70,Lock ExplorerContinuously monitor lock activity and quickly identify any blocking session issuesLearn More
70,AlertingComprehensive monitoring and notification capabilitiesLearn More
70,Live 360Instant Access to your clusters for Real-Time MonitoringLearn More
70,ReportingComprehensive Reporting for Customizable AnalysisLearn More
70,Top Tables & IndexesExplore your most accessed tables and indexes in detailLearn More
70,Cluster & System MetricsComprehensive Collection of Cluster and System Metrics for In-depth AnalysisLearn More
70,"Datasentinel also features Changed parameter detection, Agentless monitoring for easy deployment, fine-grained User management with Role-based access, as well as a comprehensive API for complete automation, among many other advanced functionalities.To optimize your experience, discover tips, and explore more possibilities, visit our dedicated Documentation! ‍Optimize the performance of your projectsBoost Your Projects with Datasentinel and unleash PostgreSQL ExcellenceGet StartedDatasentinel14 Avenue Opera75001 PARISFRANCE‍contact@datasentinel.io‍Access Live Demo‍About‍ENFRFeaturesSession HistoryTop QueriesLock ExplorerAlertingLive 360ReportingTop Tables & IndexesCluster & System MetricsProductHomeDocsChangelogBlog2024 © Datasentinel"
70,Privacy Policy
72,�      ���msܸ�7����S ��o���ѣ��8G��]%����ur�u
72,Cb8�H�
72,�3����I�G;����g�3 ��J�����5
72,�F��?:�� ���7���w����
72,p����h
72,"2o�""����;��{�+���_�l��+ʨ�)&�"
72,L	W�袱M
72,"4��V��1DĘ#;��qť1������a��#(.C	%""˩��(.��>c�怋���4�6����AΧ�$1 	�!�����y�@�y��"
72,"HQz�_��9����lJ8""bO��"
72,�lY��}
72,:�� 9Gbߦ
72,����}���KgΩ-����}U�?�f�'�<��DV�3�B� /�
72,�?�
72,�-�N����_)��ߞEO�~���������!_<��
72,�J�������;$&.�������'���Ӄ��|����/�T�H��dU��{�~�>z6��|����x*�l��@1ٳ��N&��G/�'�m��7�����F����E�j��F����N!F�?#h�A��G�~�}���
72,�DD|/ɾ�������
72,���!5�B��`S���(jWp��诣��������/IK�� n*�*� E52�3�3{�M
72,���0ሉ�hNz�_����ɋ
72,����_�]��ᴢɪFZ�����
72,z!���%y
72,��b:��q��w�D$�uH��&{�`�l���d����O_��^�&+����������G�qm?G{ب�l�m��j�������= A���_y�E	C�YG=�
72,H\4͎o��|OR�E��Ȧ~�!�F��W��G������#��ȧ$z4%i[����hDQc굍
72,ˏ���3����M��d>��������`v|tp��/���㳣�٫�Ù}���F������Bxr6;;�
72,��������
72,B��c�ʙ
72,�͏�W���Ć�����˃���ā/��
72,�9zu�L�ǎ
72,gp�+j��b=
72,�<�
72,�?|�qM�&���ޢ?_�l�/�>�*�����/��|��>NfX������:t�Nf����~k������ˣ�ׇe��n_}\�НN;-%Z�8ct�{��c�9y��(�a�)$��}r�9bf�D�!3d
72,����������
72,�<>x�����d6��'
72,N���d5���h��f'�'��C�}輚�ىs4�
72,�OO샃��
72,�F�~zrxxt��Չ=?�
72,"���G'g�p�2�""�^�:<��"
72,���խ�y�;���q�������_��O�'��r���v~)�^
72,ޞN�?\\
72,����������]������W��������?.l�����d~n�Ur1%��<::==p��:6:9}y�
72,���:=q�����˗ǧG� �^�N^
72,�49��S�rm'�˓�W��W��}��C����`�O��K�����Kxzzt��9�;�=9p^�:�A���dr=:<�����~<�'�O����[�;�}�7�����?y�-��g���tq���?]������%��������������͑�����p���m����>�
72,�8�P��/߼�tL��7
72,9����H�
72,"0dSB�-F`��|SѪ�ǲ±���h��^NI��0M� �霒�����Wt>?, *�|D��?J�z_�l|�݅�Sܕ��ׯ�!Ϩ�4Xx��M���W�U����U@� 6%1"
72,"��#S-����ǋhA��szh:e���az�""To92EAO F�@��x��K�s�^�l����֒��ڒe�D�c�K����q(7`x�M]��""�Eن�6%�[a�h? n��p��a�fJ��<�@H��Ó��ՏǗ���x�iy���~�����|�Ͽ�����"
72,"��AϮ�>�l���8ʫ���S���;�����#xv6yf��|��9:;r�W���������+����9{5A����D�}pz|rbۇ��{6�q=b����`��M���v4�.R�VX,""�>B5*�:HMs��� b������"
72,"�Sl���,��"
72,�0q�F�q'VXľ�!s6E7O���e�~��a��O�����cİ���c
72,�LM�_i[T˩�[�����H�x4%���=gv�
72,�ĢC]U��n��/`
72,d�!��P%iȼQ�`�3fe�R��R���L���3�ODC��zE����䂅��8�<rp�O
72,����6��]2u�q�2^�YԳ�x���r����H�S�N�|xpx4z���*�w���	Te��46v4��EБ�/*H{�sp�������_�G_�6]�����7s��
72,G�\�;MſއK_jbr��Q��ĳ�ٔp��bJB����@���Q�.%�	9�����z������^��/����&��ţ6��e�E{
72,"�K��g����>}��""x�~F�?�=�O!��2�����ϷW{>�����j��WԤ��=j���"
72,�-�#������Qs�h:���
72,"C""d�}�3�|O,y��o��WNɳ��1}���=��D��"
72,�l�9z�^�
72,�D'��gK�o�?�]ɍ�ϞO��&�:�2�գ�c������*
72,"���3��/x�l�>��^�,�cI""���^���g���~��J~�"
72,�}�G��7���d���_��
72,�0����){������A|�
72,"ğ���}�,"
72,(�[ٍ���/ĞX�J�W2����Q������>��v9~�����I�|��eڊ8�����8��_ě)��;�zʿ��Yڱ~	[�!ق�g����|GO��������%i^���|A�a[9U�� G������J}��� Q��}�ɘ��8���޼&���f�rԚP����xP�%jY���\��z
72,/w�+�X��[�mX%C��q�B�d
72,m7x�� s1�\�
72,�����j<)}J
72,"QD����]�H9qB[���e,����ʳH\����G4�f��������ԏ��!��(_u	M�<.��V�M��'���r��ZQXD Vc"
72,��x2ؙ�.Wjs6^@o>�(�7���T>���}�A�pz�zF�Ɇ
72,��@�X
72,�������A��
72,"�*�2�d�g���	�WI�F*H����߼NzG��/(co^��挎+�z?��;4�Ċ��,��Q�؜2�	rL-T��"
72,��c�Xb�ϡ}'z��`
72,"�R	�""���A�G��"
72,"�6D�]��h�H}�V}�@W.,�ʩ""R%��"
72,��N�l�O~�fp
72,i����g��ͷ�C�v
72,�8ʃ�Af�+���=��
72,"-�QWV�uwn���N""ua���_��Rg)�u'��1�4��;Em"
72,",ƂA�A�WQw�XΣ��>�4	�΢�"
72,g��b=p��2xL�Bb��e7�;�nL8����h�� ���Z�����ϐ�%b
72,z��6հ�\!�����؞��z�q��
72,{h��|��JLBrp#_�
72,"��S�X_?)ɡ�.��S��iL0���;""+S� ��zף�u*"
72,"��J�""�:�/�>u����5�>�ǘD�t���aV��j2Ug�"
72,�̿30&�a&��@7����\� C�4��C�5ǥ��a�P�����Ŧ p
72,`t���é
72,�A��A�����ڏZ��S�����y��־�Y���k#��.��� 1���� c��.x�~w�Q���!o�i�PU�[��n��g��^��26FXJ��ں�|@�$�
72,"&_�@�M;�&�,$?��9�8W������}������L�C�Qme쇊�ʿ�P�G��"
72,"�|db�LVy�n:�""��E���=�m<��&kV��e�U?c����d���2-z��@��]D�X�ca�v��L��G\dЂ���ȷ��� ��i[��MH`5������Z�T���Am"
72,"��)[3�?T��w����x�oW8���4'�@�G�7F�""[ڠ2���"
72,P|M�U���(��|\Dk�\P1�N�[Ц��9ߥ�u
72,-;o.��kP���6&��'cںި��+
72,"�s���á��������ͷԁ��0wz��*�Ӧ��8��o{ �\���""^�Z]4�e�	$6�X�"
72,�`��
72,HG��C��7�Ķ��A:#�!	��a�.�=�b
72,"��t�]g��w�*)�,���Xؐ!]`?n8��"
72,�;[
72,"�bH@썑M}1[ݭ��?����r�1��Z�~H9�""ANh'����	��n�"
72,��UI@ܩj�#V�
72,"x�e6�N {A�Gݵ.����pρ~i�h�5��� ������r�5YÕ��w��_)_�,��s6/�M"
72,����_�nQF{�
72,"�O,��P������#H误O9zED�ϵg�[�� �D�""F�x��z�tc�	�F�h���!C"
72,KT5�fp�M�ZH��o�zN�s�g������P
72,P�8ޖ�V���	��<��M	�
72,v:�۴���E�%
72,�'�wh�)0���g졹�l�z��Bs������1d�aW>S�m�Nψ�|��Qz~|�p��l��|�_���xrL癳�.�`�ﱏ�[Ë�t�C>�oѠ���&��a=ꦎ>cH��M������]Q7��߁�M	M�H٬��@(A��]eެ����ъ)O~XBh�:���W>��K�]�pS�t�){��~R�R�����]�@�]���I
72,�I����V{���w
72,+b��uv�xv����b�E'yws+��α���Q�
72,CA���U�߫�ف�c1=;E��H�:��O���
72,0�W�s6=��# =1}|G�7�
72,w��ԗ���nrܡ�3
72,tG��N�k�yypֱ�zD�ߌ=��i��=_@���6d�(_�ev��B����
72,�6K�
72,������=L���7ԿW�^����1�t7W��
72,��]{�t��A4:<
72,GI7�-1Z�����8 �g`r<K�8�d:��MF�������t0$/�@���տo����c#}���ԁ
72, ��$N�t y��y
72,"Po��t��`�������ޫ��cp�wp�;9�;yq�wr:s�y�џ����7���a�ϗn�x�[׶�j�Fчh���IȊ�I�_��5-iN�O^f�2Fo�IC`C昨+�#�1U9�А���8���d��v,5�L��X!��"
72,�![˿i(�[�/Ƈq;�!㔍e�EM�.Y��e�\��/�0�k�;��U
72,'{�6{�69F+��e
72,�h(�=
72,"����""�#��[����"
72,ñg��3[��׈�oC!(�JER��h�E�y�zmB%
72,"��d큩""3�)�[ J/�V|_��H"
72,A���
72,W�|D+�06
72,��)E��ڐ!�4��
72,"ElN6�`��""�;"
72,p+#�l�]L��
72,�+�eLnc5ݤ�����Zҕr:��k��qd��o���!�њ]H~ӡ?q�<�&��ɥ����f�Oe5�/R���z��B����^6S�c�
72,�`l��(�@e��0 ���gC�g�#)5�]�Y�3���@���`T��&��
72,r)[O�K� �
72,����;�0�4 ���.e�I��Q�\�����[�<@����Q�lK]\e�V�� �cA]�C���gΦ�OGw�(�8�ӟ2
72,R�ļ����
72,"*Bc�w��O""@f���IY�=�G��)G�/�jw����"
72,e�����A�t&��H���z���p�gp<>���k3;����hr�w�����k�crpv8l::ۛ
72,ONG@0Hx�3��P�g�ӽ��g/A�
72,b��ѷ}@�џNNN�_�D����?��
72,��d:�
72,�:8=
72,�u��d�����<
72,NG�{G'�XG?��^
72,��):ֹg�
72,U��J�u��GZe�î�X���
72,:m�lǂ��������j@l��W
72,Nb�*���v��Ge �]K
72,0*����x}���T�Z��ځ��~ua�/
72,�.tǶ���՗�Q�C�v���Kè
72,�֕`J]���S�Bwhۮ�[}iu�ߺ�V�Z�*ԅ�����
72,����U�8�7
72,�0���H}��J��F������F����S*�L��R7���z����Q=�c*�o5d���BQFR]$
72,"4""V<\q�""_��8�F�Kx|z����I|���"
72,Y8�D�`���/ hS���X�(']$�t(�ѳ|>>
72,"�y�9""C�玬lȐP�������^|"
72,>I�G�
72,*�'}M�Ax��k���tr���L�&��e�׫=�?��r��)x|���X�?9����ȟ�@0��utU�;��4w�+H �bZx���
72,�#����
72,�DhFo
72,���D�p�N�x�S�e��6��%+�
72,"�dH�]�E�ON��7�	r���.�9�	���""�����%G6"
72,x�M+`��C�� j�V���u�R��dĐN��_���g4/ t�=XFGԂ}����b.X�\C-���aC�
72,"9��J[��V�-����1e�]�)���	y�[���;)�m��҃����[e���=s�.[kIV���˲,��[\��E�d�_-���t�O1W��/��i�K�t%h���"
72,��2p#ބFm���V���8��zU�#���2�`ĘDs�T��q�&�	�
72,��*;U*tݶJU$��
72,�a��D �9vI�F@�Inbh��
72,���Kk��1r�8~��k�F�9�������B���G9y�+�J��
72,e~M��JM{Z�ٵ����zA� �eە!��{�f
72,"�M �6���,4��R��oOʰ�J���[IT�U,O��?�ż7�i΍�6H�4�.u�P�GF,`Y�Z��5ƶl�>l*L3�J��e�6I�t5�K0��Y2�zjl����]��\���ň���$8����h(���k4���#SY��LԣƭY�\����o��ڣa�9�����Sˤ9ywK�e@<��P�N�r��pn�ְ��A��u��H��%�i�>+���J�yu���ZWn�<��g�$Ӌ�I� ����9dv�����,��h|ɓj�ab&p!�zX��@�fhZ��f�R�F;�G9�,�w���z���ó"
72,"T. q s�P�g�@١]�vk�wq��<-є(��,7�9`2""l��Z�c`��z��x"
72,"!��{���+�3ȱmɿ��C��)�hj��Ҁ�ۓH�D���S������a�ސk�Wy �""�����uP���=��"
72,"�g��HH5�+��*��O�k�,�\"
72,��5/м?����u�L�cf�����
72,9�A
72,ȊR�I��b�!�A�(�P�(i�CM�֔M��T}
72,m�����a���RK�i����f�Y@-ԊRu����m����ú�E�COp�
72,�)��
72,"z�.""��m�Z�m[�puwqa�dU�#�Z"
72,HD�������������ZQj
72,�J��P�0q�Jk@Ƥ��E�c��W>�:��
72,���3�;�`҄��Af/�@�Cx���6��0UјV�)�Ꞙ�{S@b�r�$�UE'� �d&C�
72,gƺj�	l�T��
72,"\P�Cvj2��V Cg	��dB �N?_)G3qk����%s속)���&`�q]�=�J�g�рOC��Q}�X��n_���""7?e��6*�x��j�a�s� �gc��>$N�(m�֕OQ��6Ո2��s�!mQJ�[��1�)�G�.b���l�""pE��A�!��As	�"
72,�&1($���?&��1&�gp>�v?�����: �3WO]g�>֒�H���^�܁M
72,"�~DqB��A����X�dB��,����͐��j/=]/J�W�bj%gȧY"
72,pI���������a1�%��
72,�j�֞q�GPh��@�3��
72,��1� jQl?
72,d	g�^�D�i�P�@�� tyf��+��v�t[QN{e��
72,!#�1��j�l
72,"�ue}�J���J��jt���3��E��J�]R�i�AV�cA� З�l�-Pm&}�v���Ǹ���U���r�,t-�:Jo=��!a�k��X��8""�����sMԚ�9z>D"
72,"}�����@���v킔y��s���K*""�v1�4� �7������%@�ǜ#�knGR��Ũ���N��_���W�b8�z*M�=��r�~��(��:}��ćG�U�A�:ͣ'�Y ��fK�D�7�ކD`��-VI�����Oٺ����E��"
72,pq�P�%w$!� /�)uR�˖����'β��|�ks�@���x��A�[&+�nh��;z�����웓��3�o�� ��<
72,�C>��?�bǺ� r���Q�j�l$�l�b̶]o
72,",ȳ*�J��h�P%)$�h�F�����6��YY�^`�w�X�C]"
72,=�?�_݀kU$����B�15ez�G����ʴ�[�O6�����Fl�A�G�E�u6��yBoz�U��[ݻ�e��ڠpu
72,l=U��Uc��
72,"�G*�I1�A3��.Kr��t�p8�J�%Y>,�kud�=c�����e�:�j"
72,��[�Z��t�W���[P.�
72,"�����9�\�P���H�TL}�QBi-���%�%�LO6����k���H=\a��*t��)T���=��]\T��Ag؋���+C�Ru�qEU�����`KrJ=R5���%�*풷,��F�i����""�&���Q�o2}@zV�h ݒnӝ[�\vt�""��"
72,p�)�G��
72,�[�DSc�8*�妩�P�:������e�w�azV�ˆ|I��#
72,"� [X,�-H"
72,KʙӬ���Uu�
72,�����k�vɎa��
72,-i��+�
72,uyV�Z��Nu��+���j0��5�x7n�m|ɊV�M�x����+A���\P�A�Z (|4�m�4=d�s�kԤr	�1)����qa��
72,"_aa/�9�~�z؈s�,A�xL�K�n	��=YNT	�&�H�*�2��^���<-���2جq5qi��:�"
72,��.* �g
72,��AU3��s���ȋt�\g�<�њ�.-h SrF�e�ג�PG�}:�ayYh��0Rcc
72,��\��o$�!�K
72,"�G���""WʴE3��	�ɵ<�ء.Pp�F����U��w�����D""W2�@��""M%�Ωe�>�4QF��YH�ũ��D�%R�hϏ�"
72,�!&�N��$�W��ù����/i����3����q2��6d4ODf�����us���nZ�
72,"����ʲbX�� �HG�2��Y�a�C� �<�.i�����H,P��t�-�� ���"
72,��ue5$[��
72,"�%���d��<��̤9 �{�g�#d��h��#pg!�,�E��"
72,F4��V��a�t���w
72,\Q�Ĭ�	����#�H����y�sΑ��6)�?�|�
72,�<h#���p�
72,���oC�9 ��M	�S\B��(��-�=���c��5ﱉ�~��Ɛ�3��i�*m�ZJП]�=7�}5��
72,���q�3����ڲ�{����$�|�2����
72,"#�$w��,L��VM��]�8V�6�sdH��5��*+?��71j�d��j/#�Bk��S������X-ύ�tpO�ד>b"
72,D�W9�IZ
72,��}�b�E��FM��<���?I�4�ư �R��-
72,��{VZ��Y
72,"�qo[���9b��(h�Z�Aw����$�=.w�J�mZ.����F�A��f�@�9)�{_�ԇ������IZ=����[1����{�+VՒۅk��Ԛ��w���-�F@�D��""t0Ֆ""�����I��,���CR8խz���f�L�{x� õ��"
72,J=��򞚠�#R|�T��
72,�'+�Mv�Z��f�jؔ�
72,���<�����
72,pR�JQ黔ʰ{��Cn
72,m�#���][!-h�v��_�'��*e+Ri�P���2��mӐ���\������h�
72,",�,� ""�"
72,*�ת��ץ��{p!��B-D0n ���W#�k0�c]�m���
72,!�N
72,�ɭ��I]��
72,SE8��Ԁ��_�p}8��l�%�w���f�
72,`�t���x���E2M9�܁x>3��)���
72,"�&r9m�}�(`E��Z""�3���<��qQ3XE�{%���"
72,y���꽦�K�&جjcw
72,"�""�4����7cA]�C@�G1� 1r�1G>�Q�)�3"
72,���F=4��BP �p��<�AL
72,�z�
72,��G����HFo.	�_ p
72,"x�HbZ2$޼�K,1Z�����8 �����`r�w2�v�b:�"
72,��7 �{
72,Po-��� ��Pn����t��!��M����W�_ɳ�/��z�/�7��#�o�4��[����������<�Q��3�ޝJ=���%��uv��������-��U�]��4&J����;Cf6���lMC��Ź�$
72,���Ϳ�8�����a艵���`����u�@�����& DU�Iƥ��=o
72,K��z�!-h��
72,Y� �Z��{�
72,"JLn""xU"
72,"b�K*I���C!6��d �""�.���_��)�6�p"
72,�|zS�^s�%�C��
72,~蹔a��[Eɐ�t	}Oue�H`�
72,{J��=��	��
72,"��V�,͙""4a�"
72,"m/,"
72,"#7R�~���\F�Wv[m����LYz�A�Њ=""�ԝ 1�P"
72,"Bԭ{y""]��캗�\�ʞͤ��S���S�"
72,�vܒL���	/�)�����R�c#!�	��>�
72,p���F�I6����8��GbZ6p��hV�>f�Sʇp�Aq�š��P`/��E^�2�$�{ZH�
72,|���JGNj�jb
72,=An1~�I��fr�Ą��M)��u���z �	�M�v;
72,��O.[Hu�q��P\C6kI8:���1��D+�ӥq��
72,�Od�$�}F��юZbϫw�0!��P/Q�B�1T��lx��� Y>u�
72,�1-=1�|2�NXw_�j�h�
72,���o�
72,&M?�K3.�qn{�0�<��N֑Us���5����>�H�ľ����� �
72,1�q����I�3sIЃqQ�.��n�#�^�H;S���JSD
72,�K��$�p��Dƚ��g<a��|5م�r%� �&�ή�
72,"�����@�,�9��~�"
72,e��pn��w�	�5+_tk��K=����q�
72,�t�DIl�ѓK�9�D)�O=��SGr��C�gy��x�ł�b�a��#TSv�_/���������$c���~�3B�Q���>�q��1)g�]�F�b=W�����A��?۱���
72,"t�o�)j3�UStt�L""%�X�^\����б��A�!�b��c��;��\�u4��}Ln"
72,"l�r9ʦ��]A��?���ʌڛ��L��yh�""O���xd"
72,��^�/
72,"���k""]	9P 0�"
72,ِ��`C��CLn
72,kM��z*}7kc��
72,`*�m�)��^�ezfi
72,k�H��V#u�c#H�({h�S�a�����i��)�!���
72,�N��͆Ѹ(4
72,.X�*)4�����V��D��G���P
72,Pe��z�U�^��9b}�ѹ��
72,"b��tO21@��h\O3R���'`��uC�E�=����=Xvx^��T�4g����,""�t`�%ғ��g�`�;tE�"
72,�J��[M�>!��7��#�/<�F�~8��{�v?����+MK����z��;G�ًb
72,݉Z��`�L�(��^���fƼ�<�Ѫ�U�aM��v�|�N�
72,�n�B�U�v�cE�\��t�$���9R-���Yx��G�%��~E�1������$��c��{I&K�v�h�[A�nBȲ@��y8ɵ�J����N�=2���ԛL���ta�U�e2MY�c�>�]�v�
72,i�V>�`�
72,O�� �N�m
72,"�""�U"
72,��ٌQO����by�_K�V�n*i���t�Tխ�v�-�÷M*�:p�6��
72,"��7�#p��t��)���0�P�2�J���CQmǪ��,ś�Q�9���2�k򴧄����׀��L^y�с��x��3���r"
72,Y���k^��F��i(
72,�V˳�_f�����%r�%F��R�[���A�yH�
72,y�/��TO��҅�%C
72,c���#�
72,"\���U���zG]�B^{bU�X��6�e�5si��Խ��""4�!(�%ͤ�d*1�j�@5� �E��<"
72,�����W��꘍�J6V�v)�Γ��us�4�j��ݳ�+�
72,�U@��Ǝ�7�;�Vp�8�^�%>�[U�3	�����_�FWU)�W/����X�P��Ƚ��*��J�����j�UHc&��p �t�����u�E������ڴtE���u�
72,9/�5ff.�b�({�{tզ��t;
72,�%.����E���7��B�9��+j���4���A����y
72,r͝~(���7�*�`�u[m�%z]EH�(���:���2@�J-�_�M�`��*j�YO�t�����OZ0#K0H�� P���m��
72,��1$v��YΈIO���o�-�%D�lp�F��$�ul��M�r�|�B�@v��'�M	A
72,e��h;6
72,��Ħ��:|��Բ�v/=�%����.C�뵌  oUIO�xe��Q�J�]����D֭T0�BD�ߌޕ��r�h�
72,�����O�{ f鯁��	�?��7�.�N�ս�:�]}����GiAL�Y
72,j?���KM�V�F�
72,�R��=��@����:��f�J=�W�6�<����U���
72,Z@�G�`oX�W��Aw�
72,���|�����-�7tڧ�1}��8��iF����S�HxL�k��ޤI
72,j�ˈ�	���x�����=*��.��6u��]��
72,e8k5���/
72,�<}��j\��j��iV�
72,���5O��+lN�*�'z�67u���5ԺM��;���AٺW�v]��w2SX6_d?��`Գ�.N�\�h(����7]�fj�Q6j�;(
72,vY��V��#;dX�K���L�8��IA���
72,"�,�v���s4��G���CiJyYd�0"
72,��D��H��<��>$N�
72,"UM�;�c�h�2�O��_2���0Y�n�L���S�$T� O�G�ؚ��]$��Y9�]��P�Ψ�&��""��iV�>��"
72,�um�!��.������&����y��i��
72,���.��\C��)��zr
72,=�GKc���Mc�}��v���
72,"2�p㼎F����Y�X�I�`��d������s^pG��1Ɯ��""�9���DւrQc��bjm��	H�^�)^~���M�"
72,{��g�(I㬒�-]]O�7�.�'TJ��us����)mo]b�ą�t��7.*}mr�s~C�b1�l�������z���ه;�R
72,"�^�>�W��?s���k���W�~x	�dt��=��Fg�nE��kU�U3�VCJOT�#����u��>�A��>i,�H�f�� H�}ԅ�}T5���=m��!�Co�l�`��"
72,{:=�@�c[�$��\���3ͥ�L��k�CbW���gi��*>�!��mУ��kQ�(�4��xZ}�h0�j5���ƚ�OL?����
72,#n5$2Рm��^�
72,��l`R���m��FGo�((����l��E�����N�����D@�
72,p�.���%A�y}�ds�3 �ݜ�~��SW��� ��9�]�����B�p����c
72,"U����,��j��:���"
72,`ם�OL�rt4��\�{�|.��/(�Q`�
72,"����E���P�""�&�LϿ$\��GDl�9}�AI2�f�-���#s=�fm$�u�OSƙ��������x@9�@�g�âr�5st�Z��,k�B)����\?��ʳt˨�h-�8Z��,�v��,�Vңd�)"
72,V�h ]�W˭�B[���Gn6�F��+��k����ܳ�f
72,"0�B^� ��k�d.֡��->�$~a�G�ؔ,)��l���z�R{]p�|H�XXk�F���a��"
72,�M�%�UDo�
72,����WU=��]+R��m�K
72,}�W��Q�A<��åV�.ӛ1�]U�´�J�T~���0סTOΒ�g���K�'��J�0J|*�JG�)�|d/ �ܯl-
72,6]W�\�����
72,����n�%L]%�o��'��bP4��M�R�e~�(�S?jq����e��&L����.�������'�+[y(�U��+���
72,C^ԑd
72,"p��b>�P��IQ��ѯ�L��1���,,g����H7�,]k���kizR5a�"
72,"�.��Q�K���U�IM֮�^Q��!�Y!G,w4�A1��OQ�Я�w��B�~6ۼ|M�Iw�B�̿��.�6"
72,7>�cim�Z��
72,f�С�*Zr�X)�i
72,1�^;�']���1π:�&:��^���!$m�	4�E��~	[[Ѹ�����zR�qȡ{�bm$t�a������%�!��
72,��2m�?�<�
72,�k���K�Vrl��
72,"�1Z��?�ܸz�o��""�O��7A���f���?MMs��ʏ&�an�H��U���f|O|�E|0sp�wv���H��%����V�Ƿ��x3>�%@"
72,q2
72,"�9Jݎ�X�_�Q�o+0F]��u�O%a�^d�,���}�!(оX��"
72,�q&�GgM���BPC��S�
72,u2!g��>z��� ����ux
72,�T(=7L��f)�ZJ�U��Ǣ���q��Sh��x��R_=� @~�)i��&I~W�SP��1.�a��fʿ
72,8���u��!�6.y�la���q��a�	�����5�S��Bq�[�J��F2X��P��u����h�1��\}
72,�$���\Z�
72,�jH;�Gߕ�f�� Ytν��Cx��ZmL��흱����e��Jfq���3�ֆ��;�(-���S��A���(Y�GS�w[�RmR�[rRґ��G�.fr��Q*Dے'�
72,.�)
72,"yZ5|��V��`��J5B���:��I�^|�g�uӭy���v��-�l��1q�->hq��D0&T����0q�a0�d,h,'�1v�$�����}2��<��:�"
72,�ˆ��v�3�>c��7��C( ��5�H\�:�	���%�
72,"l[�R�nblr���{��g""�B""�s"
72,"���:Ɣ�2\ͷ�v>��8o���Oe��o�%֑ʓ��L'@,P>""�pR�0(ȱ���b�K"
72,zM�cU6G�
72,�e�	 E�.a�L���K��`-y�c
72,"���,i�@��f�.���>бp�n�Q�:��"
72,l��u�זQ�h�p~������Q�f3TE����2|A��Y�^��񦗊t�g-)i4����?�y�6�¤�*��a�s�*�9
72,"ͪU,�>b1X�	�e�E�u��|Ӟ O�vI��"
72,",�L�yt�8�R�t�f��ȐUţ��}�%���"
72,ì�c+q����6�vAkէF�'Q�j5�W-zJ4y��
72,(.˫���ڢH��s nb��� %G�|Ay��YS�_���|�i=��n�
72,"���;��5 ��Q�	��i��""�k�_�n�dX:e����j��)����2P�"
72,Pq�D�.�e�J�S�F
72,;����{���(������I'I;�e��d�
72,��)�:�b�L�����_�'^��+W���-�I�*�K
72,"V\�ګU��ɗ����T{��""�0o�BU�~���ųT�4=�6����������D�UF����U�ou�� d���9��M�aHr�,ȷ4yf`���0�貖A��$Y��"
72,ϔ�A��쏊��
72,"�I�=����uf,�}S�b�N{F���a���C[��� ��#�h:��hz$*�D�����4�VB�I�"
72,~ݍW������jv���&g�a㙩+0�5
72,"g�,	>�j��"""
72,:�I
72,".�,Z)Z��<���x�<0=���L��*�*~RE��V�Ҩ�KƂP�������"
72,�v��)�~��8�})s����k���>$x��^����о�n����I� �*2�1~c
72,�⿷Sw8�߽�5�)��v�t��`���+yn�I��E
72,�(�
72,W����6�+��['zh�i��S�.%���r�ѹXA��;�Eq�߬_՝�N��Yx��=��)v���g��T>��
72,"5b�	����,>�ܟ��)%�+nSU�)�ʫ�u�r.�����}	n�Y4��q"
72,j6o�4
72,��<`�M��KO��	� ٘�
72,�q'y&M#����
72,"B�9C|�M��K�h� ���""�u�F�hj�X����m��8v�ǃ��\�� r|E��NKS£y�G �@з"
72,1���4r�
72,"#���ǲ�,gS߇���@^��@�����Q�8��� Y7=���NB��85�u�mr�U#B9�}#��B�JQ 1w�UElG-J=�d�n,-H�*��߉�S����߆�"
72,�_���0��˔���p�Iy����:�
72,"�D1�v'�L�/�K��p��Z,æ�l�L`t�Kc<��y�|T~+ç�S�f�rhí!"
72,�:���3���p����J�
72,�����!9/
72,jRb���n
72,w=_i�Bs��
72,n3W��_/(��ܖ	3P�6�j�mC���N�K��u��O��ˑ�o5͑�b
72,d�_�P|��e$@��|��--_j��pO
72,֣��vĄ@�~&X�[$��ρG�c6����K�ʳ~��>=Z+��}��Uq)
72,"Y����]��:�����C��Z""��Y+�"
72,"]�Rg��iO$3�/	�)�6u�e�%b��uou���""f1�r����\���m{D�$x����"
72,"d������&�h""��]z��\ z�\��d���Q_n"
72,"�_E����h�<,fG-���UC��^�rSjjo!�Ľ����i$�V�=�ҔwB:�C�T�Jty�|��"
72,��sѡv/ӌ���� t_��S���mv��M�
72,����Jב#K�'ɏt.���
72,"��u�VMM�'_G3�'v���Z&�w�{�d�a������Y�2e�N""MI9�sX�}�\j;�r�Z���K��;��JP6]b�MV�j�\]�����a�5]�J���7RH�ƍ�o�����"
72,�U�o��
72,/{��.L�gSI�k�x��� ��`
72,��S+��e�������|s/.��}�8Ȃzk�
72,����*�A�<.ke +P`� KY�j��~%�2�=�X�m�pOp��O�+lQ6�n�*.]7c��͌LlM�����L�
72,��؆��mD�bضllU+l�1�
72,�8A�[w|ɲ�P�e���i���u�%W�.6E<��$Jۄ���4�O#:�a۹�'
72,��x;
72,�-���x~b�X�ϒ�M�QK ���h y|q�
72,"Bϳ�%,���k�ћ���m�i��~	����r����^쨑���?De"
72,"���*Μ�o���/,�_?��""��H�D�M�'��&��6I� +#4�2C��e�B�ؘ��?���u��#""�{c���Gl	թ�Uy���d�e�S�����"
72,"`��%��F�oū�Y��VW�<�N�2������Za��d�e>����*w��o�y���A[�,yk��&u�:l�N(7���|̢KĢ)Wg�5���p��F[Q��I���B�Nm&=y�(6�!���Y��:�X;u'g�^GS���&ˎ��bh�""6�%%ּ8��?�j�Kh��}8t"
72,������C���>������
72,"���TD��R���t��6����x���f��K&q꠯�S���Tǩ�����,Z�g���7?���t��J��[��O!J"
72,"���d$"""
72,�C>l�O򬢦YC�k%@�>a6sέ�&f�k
72,�ʊ�Im]5mfkP���s�*}���u 
72,7+K�r�� ܜ
72,�9@�zJ���X�l����2^8�h��D����
72,7��mX
72,"���ҭ�;�՛DP�g�@�""��D��F#���Ȯ��JK���s�^�0zl	ZuI����.�\��t�8�1璫C�"
72,�E��z���B�F���D������W�Z
72,GB��f�c�3�X;��9S�.q��X1 0g��K3�l�#g
72,��A\`RV��*@���e�Կ���#;dX���[
72,��C�&�
72,"~�X��\\�+4��@%�ha��zjm��$���L!q,!�c��+uc�>Z���`4+��R��k��u���A���=�D��ث��!�/���3]�堇U�;�x\""�ö7�=�cz"
72,Oz�T�נA�}ʤD0��
72,"N����>�Z#�[�*]�L�{,��5�B V��6����u���Ǘ"
72,u�g�u=�6
72,��Xb
72,B�Z>�(���9!7>�aY$�a��诅w�A�&�1������
72,RS9�VP�
72,��&
72,"�����D����˄mNY?�au��Z""ͥ��\"
72,"c�I��<���M"""
72,�v�~G}�w�7�)�I̅`���$ҮL�Dg�C͍Qˠ1��ih^ySqKq���Xh? ⴉ��Ҕ#f1��(�&���Aވ0j��Q�#5�f�y�~���i_:)����Nm�41��.���o��c��xȃh�lS�ǹ��:���A��kp��e�6l���������鰎�y+K�lY~��	�b>C
72,�>x�k17���&
72,���+���>�<�я3�z�)�C7�+g*�����U��*�4��;aʄ2@�9
72,=�(K�B������b<�ݧ��ϱ]��WS�eX����
72,�-PA��1c$$N
72,�e4i��@���������� `����%��bf8
72,Vx�Ɍ>VB��h7
72,���e�c�X0��	�m�*̵d���d�F��a�`����E
72,u�u��5b�v���
72,����h�X
72,#��o^��^D�����C.��<�Z�+j���
72,:�Īu�x�q��Y6e
72,"yP[�f�ց/��ņ۸���&�%��P�t_f��nk�vV��X�N�""R�-մ�]f8��_���m��7?-'}�O�5��#�u�N�֠��څ+�������t���ܷ�"
72,�Jeh�Zm�w��}�c~D6�
72,�VΔUG�f�zM'x�fEكG�x[i�ө5ȿ�TX
72,"�jJg�^��w]t��v�	�R%�+[Z,ۘr̶����L���#У����2�z,l��%�!n"
72,47&.���e*¾��|d�LҊ��
72,x����>P�C�()6�� F�T��YE�a恲L����@�K�o�5k��U U��]d+qh
72,��e�3�#��N����>L۵	��޽o(O��&3&&���9j��̠zP�%Q�YU��
72,غXrp4ov�6Ts�_��F!q�m����$�~(��9��[��:���K��$}`�*!��rڏ&�s���������ܱ
72,6Eh�-���?�!�ݚY�3���5c����.E�݂]׾����m(�Ԇ�ݹ���z�XM&�Υt7
72,���hG��*X&��m����
72,"86ފ�'��;�_�o�|�S��j���]��\��K�Hc���F���^	8�ꦴF�mfB}~w��M��iS?�""�/�]�<��[�1���x��֮�xa"
72,���3BL�CAb
72,y��D{��&Q>�6��2h�R1�W6]F�4a���s��b$D�&�r �N���fb��Vt)^'F�F2
72,OGl����6m�a��E%���~+�.�j
72,"��#{,"
72,��b9�d������ƍ��(���;��ڰ���{����n!Ϧiš2x�E�o��61t�k��o�ngK�)��>���tU�����
72,*4�0t�q�c2�HV�����#6y��f��Qa�r�
72,��yuw��8�V�����4�
72,"���.�WS��-1�A`��|��,���4>���ðm���3���KxrO�F�2��%S�&7º�\Q��"
72,���e
72,Lp��8{�i
72,�˅|��40W%�ܪ�No.���
72,cŖ�Jo�M����d��ż}7k�ye=�
72,UV�*��;
72,������|�GF�o
72,"��n���H�����V�����A4���*%�#,�I���"
72,"�$�R!���db� GW��敂L d �T#��,q��U�k�p�9�z�w��"
72,"��� �9e~1x�eh:�m���""p�)`8��4d��UPUN�j��"
72,�*� ]ĵ��p2׮��t͚	{b�Th�F�I�/EP
72,"�;���L���ֹ���p�,���^yޕWO�Fs;�J�Ձ[%�f�t�(�쳯���]����F�T�(U��e���Qwڕ1,e����T�������/�ؼ[�\"
72,"����Z	���-*�նk�/�X�o��\��$�dZ��ɨ΅p�[H^SRk�k4�Q��^Ɗ�a����2��w�|nQ@��O���HR""�R���;g�"
72,k?�I8�U�9tUtU\t
72,�65��Q'��UHVQ/n
72,��ye��Y.�YJ��9$h�Y�
72,_{`{I�C�A���\�8	A^�s�.��$�AO&M�
72,��p���h���
72,/�A��`
72,a�>z1u�ӑŜ2�U�
72,]��S?��y߁|1��9�F�
72,]�8�߽�x��X��
72,"�h�w�,�5��8�g>�g}[Tӷ��&yBc$c�B+1&��X�2��K�ĥ����ue���&�	Y"
72,\���<��UA�D�������
72,Lz�ɞ:fW�V�-�6������\��f>��K��>����4j��%�<+�|-Ӥ
72,?�rN���N
72,�6�R� ��6њ��['�Le�l�'�
72,�#�5戴o����4Y��0�����X_E�9-
72,2Dʹu�'6#�[Fku�=m&]3EE��~jL�ؑU�h\�
72,���u�K�
72,"A2d�=�j���Z�/�|�;����q|���,f��-Ϣ�Ċߊ;A>�b�0�+]��?�p�H�iY�?U����Ąզ��C}=հ~�N.5a�e����i{/���D,P1���鎾�wIA��Zg����F��{��%���"
72,D��Z+I�������D��*(�{z�
72,����UO�
72,��[��G�?���ЌS�	=ܕ��w��'��ٷ
72,����E_=;t4[�
72,1�}M�E�V5��/�.
72,l=�E�V���梧��=��@A5�����	y~�1�6q�oZ&s4ݮ!
72,�\�eɹ�7����#�ٔ9��v'P�gkD���W7 ����)rnG���ζ4�o�J
72,m��O1ל�b;
72,��K(���\>ݢl5d��[�
72,\�AR�6�ݯ�n��I��h<�&x�ᘒ��#O����Y��ֲ]�-�D
72,;Hf ��	K/;
72,�A�ġ��;����ja
72,"��6%���jgSg�m �]?��3���,MB�{Z;u��'W׷P�"
72,Ƨvp
72,�5Y�
72,"u�����|����$��誏5Y�tn��ZRɴ�1�ꉭ�DK��Y�.�EW�""�"
72,]=XO2�Ɇ�M���a����i�����]6�}� �4}�u�eb���8z�0�k������7� �(dZ;�HDs_�
72,"""y�;�+j�cG���ܔ��%�Ϙ�t�hv:}2P��Y�w�"
72,�e���X�T��۽�2�P$Ռ
72,"��.���;��^��D��Q�o��jժek�]jCOn!�t�:{�)�)%Uv��C""K}�+�t��H�똳\�5�,���ǵ5�"
72,"���Yˀt���YW�/X,�MT�"
72,�!.��
72,�|��kc۱�f�m��5�ֶ���w�5}�N]^ki���{���qdM�~��=�v�LE�~����+uV���ReJ%)�fmǌ� 
72,�H��!�l.�I���I��
72,"��I0`d��U�pw~�_�p?�y�G+��ׂ� "";����ܴ��"
72,"���܎�����t��$Fu�-�]����τ$�LM�h����Bn˅F����dW�(䶴P�zS��,�:�"
72,�h��3��+�N|
72,�w4$AE7�ͬӖ��B�l�@��<�Iڹߟ�6���]�
72,".m>R�����Q�XZ�aH�@�,��%KO�J���H0/;S�wL33�t�̈́��^��B��R��F�������sG8M&:ZIb9��o��[t��z�~�4Iq�O[um���zI�>h>/}Ká�;QN�~�2K�i�dBok.�r���"
72,%�%���#�_
72,"N� ��,�K/�c���"
72,"����R�?�	2�b�R���!��n�O�;�!b�� /�,m��*N��oHF&��"
72,R[��
72,"���.�f�""����	&�!M�T	�����Շ��R�}�"
72,"S�%�֌=����K_�9]�<_=���ɲMj��s����ŏ-	N�TP4��N�@(w����QP�&$��$��8-�Ŕ��J ���*.""*�1���(m-��J�P2,==���4�b���UP�	HƟ�z�"
72,�g+H���}��y�����ԅ��I�%��`'8S�'��m�z!2���4�j�N[wC!�[�᥻����\��v�^���z^��&ɴ%ZQY>%���ֲ��N�m��Ü��������Ds�;��L�y���9�N��>��ujW)
72,Ŗ6B��:Nr
72,"VI�S5	�V� ��-*!�_R%,"
72,"yCU�Z�:����x,քgDvrN��L������8�"
72,"b�-�<�A��<��l�ct�F���lx\#LІ�S�͂d�J��}�����k�8r����""=ok�KD�4뫳���H��־Ko9itW�8��6�َIc,I�#�BV�Mʰ�$ދA���Rb�U'��0%�9�2�"
72,X��uuW��]�>�iq��ݒ��
72,��������gt�ԥ��-�!�$����-ʿ�&�v�՗_�\��z�a��.ce+�l�I�U��XU�Fb�[�(Z%�j��g�d�������A�F�$��?+���
72,PI�Ǘ���%���5��ڕs�q�rNS�K���Ta��d�)35b1z��wy��eC��)�'���v�.Y��}��b��
72,M��R�Y��h�Zw�h��t���*�4{ծ�?�_����;!H&�Jܛ����
72,�*25�����꺒嫄�HR�����U�~;Rױ�o	��\��l�{T��	�35 �*$�$�UkH�����HC����yU�<$�
72,�#/��������S/�\
72,�q��UCU�
72,R#x�}��&փ�^��F�w�>�����+��a�{*�l�	��K��$�ݓ�&~j��
72,�	�^+�T ��=��qt��=���:�^@v�F j��� ���p����
72,d���
72,�B��z���%_rb�W�5�����_α�:z�
72,(�P��0<9B�fE
72,�M{* �eCp�����ET��c/�V1/����
72,;Z��K�k���%��*h�.a7��.iN�
72,S�����<4K�kwcM �g����ji*p]�����wVE͊�	�p9g�tϝ  �mU��p
72,OU�&�*ǧ��k�Xge��uכ9u����5�+�I��n(����F�
72,�qT�@�O���=
72,4�_������&�F�c��|��M'�:�핷�R��!�8a���G:�i*;ĥ���>N	B�f��dS���G��~<�m�=���z6^�-�>r�
72,"4��rL��o�u��k�i,n�W/ъ_������,�`b"
72,"�`x݌�,���[���/�����xK�Ƃ �	-�Gv�%%0{���VD�M�e;�L��]�7�;�ۤBGIj�%�Q���?'��#��#��	��?���[��u��-D��m��D�rTW��p�""�a6�����x�E��yJ|b�q��� ���77r@��+Jt�P�9!��ݣ�r=%�B&�TT~��""���H�.�Km:�1�	�7���g��]�]pV2��t[)B�,��!�x"
72,`�`~�
72,�b��
72,"�y�5��U�33�h&�����=��E�7�0,!k"
72,�Pz��I��31k
72,���/h��
72,_X N��8|���a���8��e�_�ax��q@�//ݜ���.�LC���~��m�.29sz�<Х�*���f��
72,'��4]�c���s���Vo�Rt�(
72,"�\j8��[*I(�1�����""qX���!"
72,#�����Y!��<�r�x/
72,"D4~�H""t[S:��e��"
72,"{��K�$$�""{��+�M<���A؀h���bz�3;f"
72,����yͫ�iv�[�&�g7�
72,���$9����{
72,����h�˽�HPV3���i��S�P�� ӿ�B��8�F��1>PA]찿�Q�~[m����
72,"E""l"
72,��	�^��ÔjG&�%�B���I��
72,-Vh�RU�oOjdD>1>�_12 �?V�
72,o64�[����'�}NX
72,:͆�b5S�>'d�^��w�4xR$*p�[ڎ�l�E�
72,:�+p��+o
72,"r""�j�c�Ő��p�]��r�I�$	4�vC��P���66	!r�_��h&��̵��F�#|��P0��SU���S�9+���U[�ݟƺ�tƠ)�d�5��O"
72,j� ����Z�o}*�֙|*T)���hd
72,p�4P�:��V�CɊL
72,"��L4w�D �nza�[}�[&d�S5yv_�{�;lyWq'~�x�[""����'""�""R��0|EW���\�}�dQ��M��,�m>��B"
72,��%�G�x�$|
72,�^`�C�Qe.KB�7
72,u�d>G�LB��}��m ��V��9�G��[��¿��
72,VZ�
72,��_�V
72,��
72,�4�/���<��5v����Kum3X�F
72,[�%�/z��ko~f�4�g��]�I���:
72,o�����t
72,"�>3��4ޢ��O�q�A����""͍ڹ�"
72,�zc0&��������T�ztԊ�;]Q
72,'[���~�����������c��v��TŠ
72,"K��#�.�,/�#!)�stX'��]�m��&'�C�q�0k���k?J�(;�)�"
72,�I���n�!h���p��GC��%N�Jl4J��
72,6�]�k	`K�r�<\ߢZz(
72,"/&��.���T�x�-�-�W�U˽��h�I�""�'�>˞! ���"
72,ƔP���0�Nq]���ᔼQ#����
72,��WHGܜ���
72,�w�U
72,�m��@0G�ڗ8K�
72,�ҭ��4<'*�^����X�������Ϊ ��.�/IT=T6�>���98�0Y=��#�o�����W˞8F��$��G��	[�K�Cڅzm9��u>
72,��
72,�ąf
72,A��u?]���
72,"�'.�뉲T���b}pr����mKp""�j;:ev%8���M��?(�+��yD�w%Y���K(��j"
72,"��f�ᖤ8$�Y?,�8%t����Zy_�.�hU�-��55�'��"
72,#��|�*�����` �NG)ړ�YoVT�^�p�u�j
72,"�Y��XH^�z5]V�r�:X?uI>nԃ���_c""t�"
72,"��	�����m`k�ZsZ��E����+gx��/�i�K�6N9b�w�J}�!9�""s�@=�M֭�,�[��F��������U�/Hn	r��o�-���^��I�,���"
72,"��#�7:Iq�""�Jp��3�/���i��R��=��*"
72,!YЬ�S�I��:���x\���d.*	�K�~8�IP�r_��W?�:
72,"Y��P�HU�U��8�]g�:a�z�����U:$��>}��7��i?ar�bZ�m}�0a�hW������Q""'����b�\.y#^~����"
72,�G/U��&��ƯV�-��
72,���R�8.jf 4���'�ײ����C�)�NUq{k.�De�
72,6����!$
72,"�ø��Ӳ���b,�@��H)'��Mw�RI$$Is�q���m�?��YT��oK���""F��G\�G%GiB��m��M\�Gd��k�������[�"
72,�eܐ~��]��4
72,�]�
72,���ץ%\1�;�=?dG��hf�V
72,"�#4�dQ�h&�	ٚݦѻ���B�B�XH,Et��"
72,�z�����v�=�D-tbK��e��'
72,"P3���WI��3�:��cDSd�9,|$� m��4"
72,"��-K1�н""���rF�[Ίx��ۆs̉�aOU6�?mJ� �1'���M'k���^��<�R�M���βb��������,V���C	�d"
72,��=�zG�� T�SY
72,"#�&[g�:��T��0�To=$q���s�R""�zc�h��MN���߬���%�H�qf8��>��&[j�"
72,t�`I�^0��X����uG���*v����ZL���ANs��
72,���P	]��V�^�я�8�N��\��
72,"x�B""��y�j�b�Hd���l�!"
72,��#r+h!�>�Y�M��
72,U�	p )?؊u�S>PG���Ѭp�
72,"ŪI:�n�/vdziw�&��	*����)�""�� �U~ �w{�:W�""5�R	tV��tկ�W��{��U2%��}ʞ�ttWx"
72,4�$�0
72,M���0&�|V��C��sm���kNh
72,pm'Ie�w�z-=0��3n
72,�gI�~�q����z�иn�&��V�K]
72,�9+O��i�-�(�����N6O
72,"T{���mb������""m��x�YՀՓ؜������:=V?_��꧲&r��2d�u��H`���As��w�"
72,��淯
72,�H�5oz�2	 T�;��sL3��޾s����D�
72, 3_-�G�G��B���P�Ձ�ED�q�%�ގ�{E�����i����L堃HU��e;�k�9�ؕx ���r�>�闍��xy��r�Ϲ7f�\��V��Z���R��L���l^-�\Uѐ���%�Wċy4�=�:z:r�r�t�2�x33+ܞo����V
72,ؐ/ׄ�*�B��F��TЄ�!�A
72,CVdr&��vH�1hB�)�[PT�ڌdR� �]ߙ1:x
72,����.�G���^�2
72,����	�x`�|
72,�?�ɘ��U�����	��Z�F��� �6� ����WNgi*�Hw�h5S�Gu��Ī�O:���j` �ᅴ��
72,U���md����e���jw���T�rX!lжu
72,��G�
72,"gm�)d�wT�E��,��4�RD""�;�"
72,"�Q�7[�""���g��E"
72,"��'%q�n""ù�2�N5��c4����-&�銳��r�qJ$�GN"
72,c����oD��E)Iq�J�h/zt�p��_�+�O�
72,nhQ�������t6�\M
72,"�s�$�)����""���f�fq]z�^ծ��j��Q=�RtB��������X=��"
72,"��c�b�	T���:����,?��9��d���Z"
72,"�Q��,�Q%��q�oy]�?""I���s�C,�<��,�^�_e�aK�E�w���Sm�\�>an՛�9\5���VCfs��sE���V�:N�]ƒ�N:9�n>H�"
72,�2�N^�\R@4�瑢B�[�<�Fh<����./
72,"�T&h�57[���x�����""�/�>u��0�j�"
72,�MQ
72,h�z$ۖ�������
72,��L�`��\��}
72,ԑ���6�r���*w����m�Z}XU) �g:0�\@�� ��l�P
72,���*v�jSd�й�P
72,ĺ|	�4l�ČS�M!��q&pA�	�U8V ��s�7L��ru�5t���0��
72,A%�aR�8���;���l�0g�ػ�I>p�o��S��
72,z��C�
72,"�D0�Z��8���as2�&b�	<��	<""gIR i%G Ī�Z�2&���Q"
72,辠.�[Ҕ�-�̙�
72, �C]���@�Ar�$���
72,�N޶du��S�M.v{�u*�-���D'9��;���lM��4�k���y�%	v�S�N�xH�׊�T��
72,nq���O�
72,��T ڛ��yG�b�3�R��u�kH���2�lq��~�G}nF�N
72,����	�Jqn���-y��}�2J�)��@�k
72,"�UP�*?^�*�����= e5��,�A��ӈ�T�Y�z)�M0��"
72,�%O
72,l�a5
72,���%�iT�+{\�7-��5Pw��h�
72,��EW����
72,"e<�!9���$�~��#""��`���h=��X�	ψ$""�UD蠐4�a=Ty+�j����	R�>��"
72,"۵=�U<�Y�� ���RC]fF���,�"
72,R�8�$�OPf��̀��@w5��vk�W�%NX<Ҁ}R�4��{��l��
72,�S�X5 �
72,M�*PN
72,�P���n���17�$d
72,;SSYY������Z�*�!3��jC
72,��|Y�e�
72,R�
72,���L*�c��
72,�|��CM�d�P�
72,��8鵑�RCvh�6I���9>��ar�bS�ff
72,���>�� [�|��uL���cu�i��'�U�o�����cZ[
72,"W��pD �K""d�����h�k�!�U�t"
72,MȐO2g�
72,BM�H��}h�����>�ǂˁ]2���w|F~/	��
72,�q�
72,"W���1p""*t������ ���M�^��5!���v���o�Q(�E�d��?�4���(�w�����N�:m�:��jCI���0�Q�1x_:E[��z�r�"
72,�JhJ�q̘)'Z�gM��������N���:PO~m�1I֌=���5l����j����Fn�}���%�i�*|��Mv�5�ˮb[��J�i�Ϯ5���u�N��ʎ��ж�@P���w�j�[& g
72,"������Գ���hl���g,�	��L"
72,����
72,�<�����^����&v ��В=�> ��
72,g.�
72,"�L]�JX��@ޓ��)_z�* ����""D"
72,��U��e�
72,�Nl��m�*�zErf4w�W	}mu�@���H��}�|&=7QB8w����>�a��X��N����*��t2�L^�M�н�
72,;~�{Mwͭ��	�AtH!d�d�O��fZ �G]�d��a<�Yh�4�����=�EX!
72,�)4�B�������7�1�<��*�
72,������	�$��wr:X)�� oZ�@��N�
72,V}�
72,>��}w���$��]�]wt
72,\�Ș���Y�.�f��������;�C��'ԁ=�
72,�sĞ2�>�X�M)d���K��
72,2y�i�t��:�z�6=F�Ռ.�U�:����utL�
72,"M�s�+��,��v�owh��X�9g�H,�ܴfډ�{E���nٽ����a�v�j=���"
72,ug�Cۛ���p�j+2��;x��-9�Ec��N�s�.z�WM�����*�1�z
72,��o4�ղ��Ith��
72,V�NzV��4�!i��8F�ḠM�Q�^p
72,"""뗷F��rS ��L��?�!���D�#哆K��C2����S��GdC3Z�A7��"
72,x'S
72,h�C�-Bu���1#��h�BOC�����nwNdE9|;�#�f�`�Ul��4K*��a�i�qb)�9���^�^-������A+璎yр�����Wn�D{�΢@��#��w!h��Xo�s��
72,9Z�m�q��[��+!�k4��Eb�֛z�K���=�y�_��P���*���Kz\9_�;�<�rF3DT�lG�>��ЬP��
72,ᆳ�r�h�j/��6́+����nG��X��_߸ݳYl�a�Wo�3��I��@$4$�*�� ��4%�^�DE=9�P�*ٖ1�
72,"�C����s]�,h�B��g# :n-x����i88���9\"
72,��*���W$-fʐ�DV�r'4d>��SUUo�'ھC
72,� �B���
72,\1=!���a/�
72,�F�������m��}��Ų�S�@�Ѹ����9�q��喸
72,�d�M�u�1���� �AA��<)��aDމ�m�������P��K��B�G]
72,�bc���r���{�ym�
72,'/�6��v��4<ЪʗRR�
72,%O.�
72,.1%�
72,���C
72,�W����) ��r�XĮ���v�Owh��J�k���MX�%��Emc�_��R��Eޯ���r�e�@��29իb_=�G��
72,"�A�,=�CӦN�Y��R�����Nm{���dd"
72,":0S�""�n��f]�C[�&|��(Z��v��|��0l��4R���M���m�aw�qUQ&S��"
72,"��""""1M&Ɓ�SPT]o5���1"
72,�n����{0���s�������&tS&4׹�f+�0�#�X�.k�姺��i23� (fbB���b�jҋ���js&���k�{�&9�ՍK#v��U
72,�;�A!d���r�grK8��b�<�
72,"��7���7�N��×Ix�,F!Km�xB*�5R��;"
72,"#�,9����"
72,����݋B��ْ'�9g)�[��/2+O�����~$������]�D�wFƌ{���' ���-;˵��LH�$}����f�`8�ߓ��O-$PZ���%_p�B�O
72,"j�dH(2W���$1c�Y[��˭���ڬ�]򓬳-6km��$�mg꘱�|���""�=�"
72,���X��\��1���?e�<�~2��%A��
72,��j���jM����	L��
72,�s��A��=�w
72,N����PK@����8�|M��`
72,"�BH�����:4P��4:�ᲖSW�j�+x`�""w=��Y��,����l�x����ɋу����V��;��T�BϺ��-�A����3�:0�"
72,"���X�C&P��Y"" +�&E�"
72,��)
72,"��W�$��|{�""@�.�Z�m,�.�rx�	��GC��$6x"
72,"C+Z�摰Q~������y�etlf����""�`"
72,�C4P���
72,���Ͼ*�@��� ��.#�@xS�����滣����|}E�B;�`˄�s�b%i�P5RT�۩ZIW-
72,� �
72,r�-q��eѪ6{Vm��85!�
72,�}|�
72,�]�=t����`�l����h�����ƴ[�M���ŝ�R�'5��m!�	b�/M�6 ?��]چ���z��O��'$8]G�(�@��`�V
72,���A�8��U;�-�I'Q�\5����VB.O�0���wÈ#������� �.�
72,q��Zs5�إ]�ݥ�1@�3԰�(lҫ��)4��9������c7��Ӭ`��6Դ~�SF�Ggq�)��'�R+H���U��5����	{II&{!Ug����
72,"�P���i�e��U���+<	]s�)����J�1�I�&F�G�q�����	I����`�1B������ �<�(����a�CC��ZRV�$+I��M��h�:|ְ�""ڀ�����l�=��Z��"
72,yP�uC�3@7�Z��<7�
72,Z`w4�i�)HhV<�Y��J5��e�
72,]�h�64��;��ֻ'yD���4�D��
72,�~G���eU
72,4���\k����Ĭ���M��}{R����	+�����jx����eI��
72,��r�0!�4����!���9(S��v�)
72,u��L
72,"Z�(b�,R�J����T߹db�`:�:�Ob��4&��͙���jb�D��2O-"
72,�'�I�C�љ�#l
72,~vp�
72,"J�q���шA`,ԏ�ݴOjx����ͯ;��ANs�ЌL�7P"
72,7nk�=�Y�U	N���c�f��x��}�2
72,��%##���`�
72,��կ�2!y��L�S}ʲ %�gT����
72,\դ�CC�aB3@�^��Y�>�W�dޫV1���4��u�3C'�����I�����Y��xGG�]M�S�C�d�)>���~g8K>��&�4��H���$);��s���ܢ��/0ٕ�Z�̪��*M�S����e�u���H+���2��~�(|��QQ���:�E�����T9|� ����Q�4K�=�����'��ˮL�iՆsT�5�O5�N�����߰%3tX�ɐ��3xb�Y����o����f]������x��e͓�VQ�hzP�e.sR�5���(�tg/ 8���wx���i@n6��~����<sY�A/��?��
72,"�F��{M�Nн6R�,""�"">Z��H��%�ݪ��rjQs�a�RL"
72,R��_�T��@]�&�����Hn	TN�;u�C�/*fT8O
72,�c��6��VnzR/W��f�^��i}��&aP+�g�>�� �.c�U9)�2&Wb˞h��|E��ܒ��aO��-�Nf�\�4ǜ���0��\�
72,<9��$
72,-f9����3ʻ�����&���@$�V��BV��ª�]��QX3܋7po�4v�X�2~�j�1�&�Of
72,�غ%����
72,�D�7*�Ng
72,&jj���V�4o+�SD~6���Jʂw�t�:�P�/Z�n�
72,�>ș�1��0|e�|��mEy�gY=�I6[&�D���ꬖ�0�j��d��K5#��Kq�5���� p��.�jK4DC+
72,�R.�/�K�Z�3���x�X
72,�s�8j�E��� ���P^��U
72,��Pwio-��7�a���w�q�^
72,"�L_�/��z�H��/����J�ur�L�E�p�Ԁ�,�=Y'X"
72,5gK�P�i�E��բ8�v���fG��@�I��
72,���pZ��
72,j�G.�F!���'~I
72,B�n��O���?^#
72,i��aK��6�-�BN�
72,�P��>��>��ׂ�f�#�8��ق�֩#T��ֹ�#gDX�g6-2��d��M;=
72,�(��N��Z�O�'�+DL{ΓM}5
72,����e��g\�
72,�@�
72,(�N�.�.$�X�
72,"Դ*e�n�jJT߆,���Y�g3�Áܜ�o��y��G�6��:�.�&1/��	¢~+���k�.58%��o6ׇ?8K�lB��@�OOj��G�"
72,�	c8͓������ڛ#�Y�NJ��<@�S:
72,�.�6�ix�����	��B�e��T5T�w ��vt��ZY(��(�A/��>�oK $'8�ZjN
72,o��r��ö�-��ʉ�[��1·���g�
72,���`�C�d���/?qd�
72,�P�m���%I9J_��R
72,�u�$�y�iN��eL�PH�^�]�����=ʀZ��vwl�)Z�
72,��`P�'���
72,�v�s�@[���%�H]N�il�I�
72,��S�I�&�#������|_�/�NHy���
72,�M�m�U���B���l8@0e{~���*_F���m�>P�
72,Y�T�P��E���
72,��M�G����e���
72,)�\ۖ���tE$�i��N8G��
72,�g	N/]ޚ
72,�:]��gЍ�\)�
72,"�#�]VT�*(q�""%X"
72,�;}��q�p�]�CE�����	�?��MP|g�� H��
72,"�0O��\.9���vG���1�N#�O�K`�;��*��z�l<�=����6@�s�8�$ώ߁y�xG""����j	����P���Ƃ�6Q�������X�ͤ��ӿ�$"
72,��H;luJi�	ׄ��3
72,"�ް��,��$Q7|sN���P�v�ИdrUүx�e�[�"
72,�	��B�*w ��m� 8�zH w�ڹ�E8JA�
72,ڦ��G���9�-�lO;��^(7����:�=0aH�F�
72,���&
72,H�O�:�
72,"���x��6Vd�x.����""�2xN�R#)��>eT�;��Oy��y.4�\�0��q_���u��ܩ��i���m�����Ҝ���l��6�_Q7hT�4,'�����u]�q�8���9`|-���"
72,t�F7|�jr
72,ä��
72,�:
72,��I�2�ɣ�HF8
72,��f
72,� UA�C�5)�j�v�V����9T�ڜ�p�k8�'U�{��Dc
72,:/��\n�Et숞-��{
72,���~�'>��ߝ�^tS�XSO��p�����dy�$'��KH[b�_�I��*R�G~�8��%��$3M����G-�����ۭ�ք9Ts����މ\��YN��g����7f\�ɍ����w�� ���f�2K��@ui<��� �Ob���_��q)ɤ�w�l-�a}��
72,�ϙ�!s���f��i�y���dU��;b�����JT��a3ڰ��v%C�E�M�%+
72,���@%��5w�.5~ �[{�]��
72,"T�ȓ""{t�v[�S�B�[O�YT�}Y��e"
72,؎�r���&ob�6��4��W���9�Ѐ	��gM���[W�5!T�(b�:�E
72,k{�
72,"?#��,:j��"
72,��
72,���zW��p�� �R��B��
72,�ٚ��)�N��
72,"��*I���f8U)���JH�3�a�o�<�.Oq�G�Y���E��Z�Lc�B;�U���%*��9ʷ�/^��WS��,EK(�a�	�H�4:�AD��٘'�A����096�ԍ� a��|9�C�^%�܎��Z�%/����J���4n��q"
72,��8+
72,�`oL׹~դ��(�*o'����bM�S �-@
72,iBw=`�x��Nh�Q�6�k�N�s�]�0�Cv������jW
72,8鏳�WIZ
72,"��Rq�(�Y����^=e8y�]r�fk�&�*��6�R>�@�p�/]��rY����>�	h��""�Ŏ�'�ױ�!1������TG#�8#V+Ǟzd��A�%��c�lL� N��k�_}�2�F,h&h����L �+:tU�m��N��ɡ�;��9g\َ���b�[�Dʢ�{!y��z��&�ZbL��V���"
72,U��_G-W)�ؚ&djٴ�7M��c�\=��䀳�3q�H�h�Vm&H��f.���=զ7��i��%�W�q�j
72,"�pǦS�����""����v���1|:������Uǉ~=X�y�EY����r���(ax�L��"
72,"���*�%���1��^��_��Vw.�ݳ��ɤ�e�R5�}E�c_����uc�,�����7u?�{��VHǃ�&[�	��S+�ZwtS=h]̥����H=��X�WC��G��fL�u�<�#��"
72,"���r��d�0�B��c䜊��lw=Kw���q����!r�B��aȊLNGj8P���g��(��a��asQ͌]�URD_��_-9C�86�s�W���s�p�4Ҍ�㖳����9s4"" I��P(�5z>�ίi�,����d@�~�8Ht/���0"
72,�=p@�	�mz˄��z�r�K��
72,"?s\�A'����|��Iyθ$���2~��ʂe�^�X]y[64�h1�:��/��r��,�;�]j�"
72,>�~&� �����kTfe�{�yEL
72,�����#
72,"��9,�T<���.ɨ�|Aw%��eqW$�:85�u�$C��.�R�������v�p��Ɏ�����D�\�f"
72,���
72,�U�
72,�E�&�<�k�
72,���$��65�>��
72,IЕ�8UP�	��=y���h���dig��s�q�����}M�B���K
72,�+x?�����x�]�{;Y�}HR�
72,��:k2t���bo$��|n�pvo?�d�W�{�
72,�h��ٟ�/��=
72,"ș�n,+_A��W-y�T�������rL�&\rp��f����^���c�:IzIn,ȡ���J��e!�]�# ��u�~�"
72,�ֳ.�����t7�]��
72,"K�㖷c#�E""�*dY%���ɐ-]s�_��`C�d��![g �s��J�dF����D �����aI}�p��쐩�&�%"
72,8��
72,ж�UVK(`8^{~@Wz-���@�%P�aXd۪
72,C�O\�2l��GY��xм��Z[[U;l�I
72,����V������ִ79ٮ�R0ц�1<��yfT��
72,D��r�Ђ��v;Oo���
72,��U!_�ls��hh�I��i��>��3�0\�
72,ض��1J�8�h7�O
72,G1�M��^
72,d՜�
72,i��:��$a���v�1�NPϞ\��X3U�Ƴ	�k:����
72,"Z�H��� �#g �n�$nrJ��2�=@;� h~6������ �1	r�7��4/��J��-�	��s,�<'�<tX&�ӕpWq-[u�,꠲*6&��8Lx�V�7j"
72,�ZG���a����E�0�j��p]}�B��\�&Ε��~O�t�h�g��<C?� �� 
72,�������y�ӭY�LE
72,�Z'��u��	� Fp�U�U*��>
72,Իg�A�`��qwj�IC˙���%�6Y�N��2�A�U����;sx
72,"y@U>M""?)�T�Y���H��+��0�Y�T"
72,�����l	3�Ĉ8�#?[tGrƍˢ���s
72,�(�7��x�
72,"5)��IOP;�y�f>[#*�,*B�ri�*"
72,�����p-i.
72," �	�=��#�z\�!7�*|Kq�q걫qm�,#I�Gǡ�F��=��r��c	���&����w��IjbK�Lq�^��ࡧ[U�ȌV'��R�C7����M[���"",�k�y$�]�"
72,"""A��&p��}��R'��*`�>�	3S��J��8V�@D򄽤6����1I��>�X���m�g�M!p�hqh�b48��xn�W���k��l��"
72,8%?n>� ��r��M�� ҂q����*���� %�7���	��s�P�ա�>hj���y�ҁ��5�o������>}�&��:�^�I����S�.9xܩ��
72,}�]DBu���@��`D&p���4Pg̪7u�Y����C*f\���-c��
72,_�ۅt��/��0
72,(*t���U�� �yp��h��}P
72,��{�<�}7�2'{�HkN�x���ᐫ�sC�j´�r���0A
72,"73,1%DdG�"
72,"H�7Z�e����m>]Q����3�Ǎ���`/�W�cϼ�Ś��t�*�""�\]M�0W�ʓ���v�T�>�=�<�����r��*,��A�=�>v�%���p�*��+��RI�QF1B;�l�;*"
72,��_�8k�%~��P��>|����:F�A�u�Xp>c��`�ȡ�w$Rn5B�
72,"�>ܑH�K,�"
72,"OX�ۈ�����������CW5݆�rf^b��,"
72,N�M�cS���L�kOM��'8^��e�+��Q��'����x
72,f�x��1�Z����VY�����t���
72,O*�i�F7W�W���z{�����&��
72,/�W�|5�z�����I���B�$�A�Ҕv6A��oQF	@p��
72,����R�/1�^��T����qh����Α+0�t�����;�8�K&�D�y�
72,���
72,���l�<Ҽ��O�H�U8��$���P����z b�l&�%}��n�k5P�������Q�8��H� ��
72,"Z�����G�8z�{87|t����'�,|$��2L"
72,���ʟ22<�����&˥��_D�@�q=_�z�N���}��-r��#�������C[����u��=e�
72,Ir3�^�!9E]n\�\ ������
72,��+�S��]:Oz�z����8d<*
72,�{I
72,"�NU)�p""�dV�b�0Dw�įF��Ν�!�d?�n?�a��圍yu-+I��p��k��|+	���"
72,\�g
72,���drhs
72,RDS�	ʲ��
72,����{���uu�܂չ��c����YTEa]v4U�7V�1Ϻ�8�
72,"B�I���[""�6|�"
72,"\W�e:1��,c��݃O��=��~�e�ut6"
72,"(�,Œ�B���Qp"
72,"Hc������""��˴d^|*hML��"
72,َ�q�zYѸ^���� �}�u�f[;c� ���Q�1��a(at#�QաE����
72,��P�yD�k��)x��׼����
72,sA�T�!KH�3�.v��zX�v��Q��}_W]cwC\G��0�l���=a{)�
72,R���
72,n��4��n��7�{�U�`scxٰ����b��&k)�Qβt�y�U�a�
72,�Dm�%��
72,y�b:Ѯ��@)�		H�k�KP��T�A&�H�CE
72,V�S#�
72,2���%�PN�p��$����ܢے�
72,��b�1�~���C
72,�~m��� �� ����m{�?Qo�$�6���(���_
72,"N���X.�,�I�S��>14Kb���nr������GHy����������aP$�W膯��8�r[���}��:�D�3*(�N��ڂK��6n�4�y�l���:nЎ́�ץ����"
72,�����$��������`MS�:�`����Ol�*_l��U��Nz��&�P=���he �P׋<_̭�왐��
72,jm[���#@g�&6p�
72,'Gk����
72,�W����ܼS
72,K�$��R|@Z��(��xϋ�Ð�
72,�&�)~�?y������/�hS�X�3��V��De����S�P�iʿ'�
72,�pG�
72,��p�0A
72,`��1��U�
72,C�_�Y
72,"A:)_�	�� z�\�,�.'a1͂��-����).�6���i�["
72,r[Ν�6�8��ܖ��2V���E]'���4I�^�u��u�q��Y`
72,"� |GCTt���,װ�*2T�˴9��$쩜�\�;�9|Rp�.jǦ�"
72,_�pbm(I #.
72,"��e���4��� �8+�ˎT"":��`q��!�"
72,�j��)��𔪰@G+7e
72,"C\S����D,I����=Q�E��O��)�s0��Ә!�"
72,l���WS`p(�D������R�
72,38]�T�mM�W�Bn���E�a/�����@x�Q��K)��c��d&b�M���+�
72,�caoqw� ��-�q7y� �Q�y�KȀ~�\s�V�
72,�ii�	���|t�HB`��В��y����4�ZbC
72,�;�_^}8��Z����L���e�A���
72,}��'z��t��|�H^�N�ڤ��+�I־��w��͈��S�
72,xL�*����:���FQP!$�U�L@
72,���{Ł.:
72,"���EDe 9�S�~�4�""*�CIq�9���X,D+V@I��'`l��Mw�>B%@������ԕ'8S�U��>�%�v�	���5�۲\O�i2!��HL����d�.B����W��R�?V��p�]�8���n��I2}�TQ��ץ^W���Ξm���:�x��Pw���v��n�n�Γ�Y��"
72,։�ؚl�����B��8��]l�:p����u1d/Į��$�a��H�<���f{x��Pqyx%��x�}TY
72,_%��
72,�~��X�I�O2 ��
72,����l4��;�yH���yBqo��p.��eC��`Z6�rF$T�]=xC�e��
72,Ki�19GWL+MW1Y���Y���5�r+�U�t��HH�iׂ
72,��4
72,�+i�����lG��1�$�P!
72,+�&e@W�{>�k�+��SqNbuJ�s^%$�D�S�����]C��0����QxK�|�8.9&F-�6l��Ib5P I~а�9V���cA#R�+��Zd�iv�2I2�z�8�	G�G�����L�՗(d�ꫯ�I��*�^���V��� �i�oSs��AQ������wۯJ�
72,�DR���[&d�ˮ��j��f׿��͚
72,��Hעl��=/K�Ϙ�U��#�W�=��-
72,E�$����Z�=�����Q9�-��pΩ���E1�yU�g�:Ǧ.�j>ΞPKȊ�N�x�Mq�J�����z���wBr���O��s�%�VL�J=���_���wg�������]NC�ܠb�X���;�
72,$x(�l�^�J�T
72,"�4�ϯ3""�h�c""ΘX%4+�_�Y�o����4����������q��uC�?�"
72,��Q�������/���z�y����}��
72,"�$��ͫIi�z*�������/��}������K���/�5��^\�?��[����ߎ+��?��?���~�|W�,��	��[�c��o'��7�~��S�i#�˛������[�Y�Z�^�r�@��M�}A8�o�KA�ߝU��ye��Y��h�'�Z?y^�����u�Ph��#>�y�>��I��y^��Up�*�ۯ����C��^ێ�����_���#1�B�����R��	ϫ�a����j��~�7R��T��J�NlKE�8�I�Fd8[&��	S��O3�N^_�ѕ#����<�	����/��u�t�[MQ+�����E~~_�_�����j>�u����aKʕ�2!�k""�U��0�׮���jSv�y��)\e�����++/*B�hj�_ A�#Hb"
72,"�&X'8{""UW�#�I�=�b^�VV�(�rK��-���lɄ�⋁�8�͞��p�ڋ1)G�<����4�k=;�W��V��Յ��G&Isa��R�Z��1[�x��uٌ		��a˪@�,)����Bx�T�z6�	(ba��LV:�s���"
72,W���n9
72,"I��6,Iؓ�Mr�aR���˒D�]��"
72,����e���?(�Nr�ɪQ�V��Z
72,P����H݅����G�驚�q9�G+���E伪���ԿP�j�9g9�
72,J ٠�
72,�w��F��?ki��%�E�l�R�[]��;ݣ^��W_�Ώ�lG�@������SԖ�ۤ��ο�9A�i��7_֞[�R|�0Ց�p8&+�%I������6*� X��&�U�u��r
72,��T=�Em�����͟���D������盻����_Տm3�����ڜ|�̀�+�8�(��P�Sݢ��߿�~w��n>�_*��Y��~O����rN��Y�
72,XSg�� �o�V6ݼhC�ӣ��B�9	�j�ތ�4�Yt��+'&�ڢ��8)W
72,"��Դc=��U�mEr���c�,=).�u�C���jE�oh�Y:l��W��տ�UWr��՞Q����l}N�-��V�T�_�����|�6FbȲ��U��Ϳ��������X��J/q�1YkU��ZӁfV���-���"
72,"��R��p3�|��Ǩh,S3RT�Nʅ�]����y��ϯ�՛��'Әnm�p��߿��tw������_}|wW� �){C���B�z�"
72,�^���_�|���{w��������5�6@;��_���
72,>�}�G��
72,nޗ\!�8a�� tq?sr�t������7��~?i@�4��<���ǿ}���;TN�?�\_<\]�C���b��I^5�OW�~
72,4dE��~_���n>ض��5�
72,����vd��J�h{}gx��7;^
72,"�k��=���U�v�ޏ*���~�x��qF�G��,!%k�S�������#����b��r�R:a��[,'��}��f��d'���چ��e�N2��Zm+�����z~�[2�nEn� �����$�F�q�'�/M�UY�J������w�ǋ��[-��z�C�꼠��}5ۡ��$^���������ŏH�ŵ��LmL�i/eY{�Yr�����b6O	�FW�j�U(�xx��|��fVm䵅�����W��"
72,Y�h U
72,|��廳�Ͽc��w	U��m�R���H=k|���f[����jsF3��K5�V���Y���\��_�'������ktO��B�O
72,��
72,�W-�:�����&�/�
72,�2�_�&N�w�&�ഛ9�
72,��r5}i��a_�s��%�iFe���ѿ~}Nq��L72
72,LYu����gQ
72,i���-<������c�S�77HHug���
72,�D��p����B~}4o
72,"���ە��F���,��Q����lտ�Pt�C���1N��e��lه�۳��/P�%D}��V_> �v8)Hs�.X��"
72,���U���+��z!�
72,�&��y�>��	��)�\�J�̜0֓�Z�
72,A�Si���Y��;��x������?���@9������7_�ֻ�:[]v�1�����j��G
72,"���[��>��]�=�o�ӪH��K��#�"","
72,)���=Z�^�stּ)���Z�ns�`���Mu�h���V֛H������K�#=I��Xd�f�������=�i 2I���6��TJ�mCUyH�I��-��
72,K{Q�'���9
72,�o�W�k���،E*�:!�:�a q\..k��3`h� !'�?j{�vky�IX��)c�
72,"��1¢��DE�L�eI�UH��銆�>�c>�����pl_śi�bՑ)�D;���#�v�*gA�#�%,V^���M�T�4�(�Z����lG8�.$K�,���C��p�n+ℊҞ �yBC*���ֱs��J(�tx`�'�D���(9��x�:[��"
72,ZRւ^�Q=�
72,�_Z1j>�3���V�D~����X�t���
72,"P��c-��@7���M�8��W}�C�0)""�6�$j�.rҍ��@�.�iM�{�QI8Q3,͚Y��q��PU�I�'0�%��K�~��N����3 ssԫ��	X!-T'1y>zi�W�˩2:.Ix�%ݑr&(Xi��,xʦ�[""�m��V�Ѭ������Q��U���@W_��ʏ�Y�O�?�ɭ�����ɧw'��z��� q)eȳZb�@�$!+B��;�:mj@���ݷ��+����շ�k����}:)A�(�""���+r 3��Tu[����""?S3�"
72,��k	���?�\��P�ը�
72,���I���ņ��������
72,�u��/
72,Ͽ}��wg$=�#>�T���8.��*P�!�����_4�9P�P�dsӠ���c���t%��w��Ii?�:
72,֯ZT�k=j��;
72,"��Y2���9����Õ�*��e��{zzz�[���X���W_��""�4-�N��񪪌�[�&��8g"
72,ݑr��_�^k���F��>���m�iG��
72,kR?:#2l�W:+W�����_�|��>��d�]\H��Y����p<0`�7�ϻI�i��<��5[�����5+��#�5i��/�?_��}>c�g�����}>�g�&����c&�1ۃ���TS���ʛ�(;d!0X��c�dݮ��P���L�I1.�� H�k򬞞ṵ����I6�1��5g�j>��0��E��¸*P�.�����
72,"L��S����z>�89��}sͱ&g�����jϒ�^��q���x֗�J�(y1�A!��@2Ӛop�-�Z����B()dV+��O��:��Y�̛_AY�O�~��=u*B~Pڟl����F	è,�iWk"
72,I9�0$.MРNW�Ք�k:u�q� �e�
72,"C��EQ���Y��JI˳�)�EY,V���,J�(��ɧU9k����w�����!E��v��"
72,��x�
72,0n{�u��f��aڭ�*��2� X�t��l�gMK�W��y� sW'k�r;���#�����d0��f#^�� �#
72,|>J9OЅ���Y�*  }�����dF�Ds*0�
72,��X�r#��&.ϛ�� )!�޻��]~��|yw�$P�8$۳�:#G��~�������O~>|�t�w�EU��L�Aq�0��=���ӟ^~?y���A���೟+v���y��y[��]](��S����|o�؛�0	z�р��X<
72,�˺��MS���E�uq_������	�G���e��_�?��j����/�?�Q��?��G!՟���G���G�?
72,"���*F����""�������|��0��,k�zf/&gYy��,�~+ox$S���wFfJ�r�S���Ly^�.�\`�俯R<�"
72,"�P!lPWf��U^'��II�o�CWH��Џ�/&5��6+ʼ�UE),i~�bZ�cZ�ze-,�Ev��e"
72,"�?��~�K,���5�@g��������c��C�<{���gT�fZ-F��f"
72,�Qs
72,��m�?�����Ɵ���R-檅��N������V�¸ז��������?�>�<�&JD��+�h��v��iF�I
72,ӕ��K�o��KΎ-.��`����7��?pci0�NI��u�
72,"��ƍ�$�@2A҉\�D@����B �cɵ��������ۑ=@��{�,���̈��b���o"
72,j�������k>�#���L=
72,}  ;^��/��m�lC�h+v����@��	JUI�:{���L��']���!��0
72,�gy6;x������;���~{���͎Z���~�Ḛ��~3b��������?����y�+�п�-��W���C
72,"��=���7Ϟ~{a8rV]���8z���ɳ��z��w�O�h?���0��6�&[d�V�Q>m`�'�?����TL�ɠC4��ɦ��-�f��d��^Ng,�|1[�R/��+""B�1nҗ�$:"
72,���?
72,����Q�i��2U�	7X��>d��
72,�u��y��F�/}�N+
72,�9
72,�V;
72,Fx/���K���vJ�1���^�RM��IS�r�z��������%(zX�
72,��ֶ��E��N/�%M�ث�|�n㚶��*�ݚ>��Z����K �z (��s�����������g�n�D�7]�l�
72,H�|տ�
72,�����^�mD�P�˹�����+u��G2���k�m��qh�`���o�b�߿
72,";�ǆ�}���N�ۂ�}B�e�,��e����f���x�c1��{"
72,�	��}�3���5Z=�d�yy
72,�:��u���h�|��
72,"��n���#9�{���{"""
72,>�.q�-ץz��ROG���IG��
72,�IG�����tt��
72,����Z���F���4{
72,g�>��{�tB��0�{�z?���a���րQ
72,�h��}��?���H�ѭ�5wd�*�3�
72,��C	6�o8��������;��EU6y��70�#���ɂ�d�����L�����N�J��i� ���ԣߠz�i`JY������ħe�ic�Tu�c^��̝ۅ��4�
72,ak)��
72,�:����f?IД�����ʱJ�-*�/򖽀�~
72,"��\��O�	z{:y�E��������m�۞N^|����q,>""r�6�����:��K��	�o��Vͻ�0�c�����[/��E�./�K*]sh_�}�~�Z,�ӻ.�kd�������3����}Z���I]�"
72,���~��I�?~oӅ��[ E�O����m���������Ok�
72,J�Q�/'�4J����
72,�� i�����\�k/n:��4�f=Dw�c˃�݇|^
72,"��Ϸ�f����g�ʽx:y��`,��R�I���T\yz�9��?��)�v�"
72,n���'��W�����������nG�MO�l����wy�y��i|:QlY��6g���e�Y�7\Z���i��y1C�a��?��{(�B����U��YF�~Z���?��_V�?6�^f��8
72,��w^U�$[�u��wU�r*�:AwgW���?SX���$������|�h�L�6����wi�W��@lv��kl����������]�V���#��\�
72,�EKu}zW�
72,"��y��:���{���Mc�\�)�`�RyITk�Ѓ�����lu�Ջ����T���K]�:�f������'��o���6�Ko�갤}vz�Eg`y�]�lv���1��Q�8xqV�)���L��/��ˀ\,�v �:+��"
72,��Y�:g�y^2��������
72,����94uE�l�
72,"vZ6}�� �ͫ�:�f[@}�""'"
72,"�=����W''y}tV��СX������XzlU�U���4���lYc�o��`��>ـ �t���0��o�xo��:oWu�oC*�Ǻ~����""w��՛-���ӳ�W��}��e���5��>�B1oxR��"
72,��Iq	�����r�=�i�
72,"�,��a Lc8�Y������]���Y�i�jh�w���m��bA1\������?�qE�/�XE���,�v5Xp�u=�,oQ��;��;��\�@����_{�/�G�(�#<[ (�l0\ҷdP��˪��������p�11����F�X����]2R���S��ݼ��R���#P�f[0W��"
72,�h�wQ�Z�5X����~M�#�պ�>��.R��a�-�:�\=[ՠe�Vx�����B��� 9�Q3Nl��E�6����HM�t�i��2������i��b��*�U�Yy�AwK\��y�
72,�] +~
72,k~h�X����n�_��f�y5��:��|w@��Is^^?}O�
72,"�,C�*�OWmΊ�a�"
72,�M��
72,4�̭䡺+_�?(ڮ]n[��|t
72,"�""�,�����z[��u�أ��܂’Z?"
72,}VWe��CA1�ʓy1m��2T>��㇞c�u2^pe�4��l�H��$1�	�t�7�|Ϊ�tU�7gy�2<���1!usV�a��*}�)cM�����Og ��6h#e�AF�Y)y�j߅*؉o�y�O��u^w��Z����j$ޔJœ
72,��ɞL&O_�`O����^|��)�H�5�X��4}53�u~$�`
72,j�uŵ
72,��U����΍7m��V���9��M[�٢(O�B��Y�����lSf�欺Z��;u���uS5��H�x�dW�|2���
72,���<���:_T��
72,�%�����K8��7�a6�J��je��8\%�Ғn��!���|�bn�e:$�7�%��rv��ϰ���-�h=	�����66SwX�PY9c�
72,����w)��k
72,q@�r`�b�?
72,"v��C�	�;MnR�,[.��rҽ�d�""��D�"
72,�-^���q�3P^������.��g)h�nm]���u>�o��B�ț��
72,�U��1Ջ��J�処��
72,"�[��ȑ�n��+�7��w�o�/�,�"
72,���2�E[v���I��N�l�K��M��#0W�
72,��e[ds4�`Gm����Bewf^��5�/�
72,�on����v�6��o^l���n�}�ˇ�/}��ݥ!�H`�����	u�:���{@����	�^i�w��p+j�
72,�;��Ǖ�Y�>��h
72,i�]�H^��bM���ҁ��W؛��{�k�����~?(ț�X�6����0P�
72,"0����:��V�y�o���{���,kH�^d�*���Y7�����l�V��n��=>��Qm�����~"
72,vS?�?h݌�i�h�����+�g�1�b���eoΊ������D�͊B����?<{�՝�-=���{��޷���Hlt�Լ�^�7��L����
72,�HS#�4��$��8ʎ���F!����8���@7�Ҙ!�?�'Ew�B����?�?������t{m
72,"Խ$�,�z���Ѫ�N�?Z��;����Ԕ�ѧe] ��r4�%1����-y�H��ׂ�.~٨'���"
72,".�,A���w���iY�%B���$�h[��7�7�"
72,���+ �V%���d�#�\�g�W�y>ϖ(���
72,ؾ'�Us�΁{ٿ>y���4Y3F7��vl��x�EE�LW�����y�Fu�V����U�
72,�y��\ [�û\�Q<�_��'�V
72,�Oi4�j��
72,D�V
72,�r��<��1� �1�
72,�I	bx؜��Ӫ�О���N�4�+\Cd)��B�$3���#3��v��w&�����7��u��(F��T���J�GT�_�.��Z�	&3��I����a0;���תd_����b��
72,"� �����t��L���݌c;��,�f�ɳ<["
72,Q.��G.����'�g���1�ץ���WsV��N}/`n�(J6�{'*�j�uw�W�[�x
72,��'򃁽���w<����M�!��Ѣ+|p�f�~:Y�B	���4�-��>&o�- ~���5������&�m>��^��}
72,���z���0 ��.
72,���uO�����������yy[هA�E{~��pS�W/��U��P��˰��eV�e���J��ᣘ����@̊�-��t%
72,i�J���(�-�o+o�E��G�CF`l�AFp|��1�6�c
72,"��;F��fs��A��ûOA��R �!��J��9�@�Ae%aT5�+L���p��p��V�@t��>8������6��yU����}H2�:�k\n_�<[.~��jq�қբ[z�""���Vv�{Y�7"
72,��7`��q����n�
72,��.�2*�`�Z*�&mѴŴA'/���
72,�˯��
72,"X�fI���]�T�8&ʾ�{�>�������#��7U�Eˎ�>��Թ{*��c=��Az���S��?>#�,���"
72,�U]���`���
72,��eM���3v�*�o�
72,�u��6#og�o�]r���
72,pH�hE��$�;./�
72,B�^����1�u}����M��ۄ9:b���H�m�H��fo
72,g嬺P0���.��
72,"�C��6�EI�{�i�j����X!e(��ZA�uq#�	�I�f����4,��޾Z�����~^��묭jV�m^�� ��b�8�ds^�Ӫ�]�!�؀���K��i^��/+[E�l����|~��N�#�����"
72,�h���w�3
72,�P�-K^ @{5��+����������c6�uzI�^�l�%�#��;�Zx{����;���ZR�]��
72,"ٟ�""�����(�?�w������9��ヤq��ִUM��� %��wC�9cߤ]�"
72,�nɑ�}��Q'
72,N�����5��E3z��Q}����2ֿ�ƾ�
72,vY�n�X�y���2$D7��l��a�
72,������E���hR��w��M�l�1��f�leT�����%m���ڢ�ֹ�*
72,�*-�m��@ *�
72,H**X ����U�gy��hr�k^W_ݓ����N � 
72,�k�o
72,0�.��Jj
72,�J2�
72,��`��g�
72,�P��	H
72,x�u�j�
72,}w�W�Y]�N�
72,"�ͼ^qt.��7>���t���pܼ������~���)��q���� ~r�i������""r^m�=v���pE�/p��n�E�-)w�F%���X7]ʫ�Լ�j���3\w[�)	��'S��LvV���;`��+��J~�w��m0.}j3��>�#`q����"
72,'�48
72,�����&���)ߩ�m�W�온]�M�Ű���y���dXd͈~�j5��
72,�}N3��:]T�����8[4�μ��C�'�F�?/��v�Ƥ������|5o
72,8�~<9�{�0_J�LK��;a���ԚD���B2n�>|��JݒO{;�u�
72,���y�'��\c����0JC�i��o���Z7E썱�֫r�]g��������:Ҁ�^G�p�Һ�'u�k^~�-[�����ye������:��L�*
72,^�������y_�\c�\]������
72,$��E+�e�k�����G�M�
72,"RK�e5a���n�0���]�hx���7Y��&�M�3��;��]�.�����?>crT���̦qb�.K/�e�3��sk�gUy5U~� sw� �t�}Ԑ�Ե�i��W�d��C����]�Z�o [P�S,z��7"
72,;}.��O����?!`
72,�'�������3�
72,��vO�͢�s��R?PWB0E5L��jQv~�doP����2�]����d�%
72,��
72,"R�Ӝ2�����������Z��7���}m��,������T����O;z�d�^���Q᛬�o����Yu"
72,�4+��?.f�|��a�
72,We��>_��Y��+�!mG_��B�7�_��
72,�X������1���t��٨1;�-W����0�j��΢tG׼*�3�E{V�ZV�͒
72,��62��X�!�
72,"�>xd�iq��u^�T�8��򿬲9, �˽����Є"
72,�[���4nF�ZW�
72,"lU�n��묜���C���t�L����FT>���2%O�""�e5� n�A�;�盤�b�mU�YQ�����{�I4����9r���-���d52�<�Osr�bD�!��H��Ϥ�aTٓ0���Xsz>���E���pB��!"
72,cZ��t��?�lm������
72,�Ù9|w� _dE��%p�YV5A�ȯv`]��((W�;� %3�3����<k�K�} 0RM}�d�\�q̮�(�a��1�>=+泣���}q�e0����1�������&D��߶y	�
72,}Gpn�f�h��>|u�Y7Ė�2u�aqACP��H�
72,S����-���r7v�MI�M��<�Y^�JM;�f_��;��U����X�J
72,�[��$�o��&�˺f�;��]�#��>ʩ��$���u�/HwG�^z
72,"}#������Y���>�v������/5��,��Eb�����m"
72,"��""*�m�Da���^�Ƕw���*�K�l>�sv�w��IFeV.Y�M繮r�`��e�_0"
72,��Gݨy�-��׫T}����Sv�5������W��*h�5��(r��.
72,��Y���~�H�������0)�/�ק����
72,"���Eb�:�m䍭e,�ts~�MP����F��9�4��3#���30���)��cLC7����xh�#F�&$t��A?λh����C��ڌ�E�4� �2�u{�Qyk<����Q��[�FK��y�d���I�Ŕ��c"
72,",x~C�����C��,oP"
72,��{�
72,t[����ܓ��X@Z
72,"݈�w.ɚ��""10fN����`>���"
72,�Z��7;�A�)�S�E6�bY��|��}��@���]
72,"��6K0tW%}�5�>��[��� .��n^����7����_�0���K�h��d��'��S��[Vs\O��?�2Ǵ�|F�6CRj��͊2i�˳�f�OTa��:,����YV泋�~YVlZ��j{�"
72,�.(?��X��a�5�����j��Α�.����xI�XI�-�]j�e���k拃e^7)fy����Sa�
72,�L�[})l=��t��G�=Qw���		�}Օ��Vm��ڂ��S�����4Ģ6lE�zE��S�E���O)
72,�U�/	oů9[fu��[8
72,�ڳ�ֻH�Am(���4y����2l
72,��Z�s@�E>K_�چ��_}�?�4�6L-���%5
72,��/=Ҿh�藔0��
72,�-�{^�^�ǯ��~t�o��|~Ծ��4��`�=(���ԤE������
72,�f���n#���VF)�:˚�u�Y� ���
72,"��W��+����4Ǽ�t1�2,������r��x�0E����½O߶u�W����Q &"
72,!�~'�_��07�~\�#G*i�#�/*����U�H��������_&�:��Tͳ�Y*%�^
72,��Iʞ�h�l��d	B�xX�b5o�e*4t)������D��u�ˇ�(�y�/��?<i���P��7��7E �{R���
72,"H""����G����RE��8� m��^�a��"
72,�q�X�m
72,���m�g�m����ˍ����=pê]�Z���N�2��㡱�L�8B~���'�O$�^⿓�$�oF�-��s�L��F:_�ʝ���xtC�Z����=K����м�LE�p{
72,=�^�}?���~�%/g�
72,"?�����O����%�堀]]�h�kAx�l�hTD�,ã��S�*����¬]��w���[�8R�E���3TӪl�jλ"
72,"�������#�_r[N68�n���v��=W\��C��ŗ��m,�E*"
72,"��_�K�&��T+��c��6�bȦ������aHΖ�,�Jv6}M���ª�=��ɳ����m?��"
72,U'7=�N��b��)����{�@*�6�8�_�5%8u�	��
72,T�e{��?�ܑw��vh��������
72,�9[d���m����
72,���h6�n>6λY5��q3�
72,�����	]�]d��HWuq�Jv7�W�dk��<��r#(���Yqzv��5�
72,�G�.\v)�nz��^g�
72,�py���1�]a��A���QC��H
72,�Z�Ĳ
72,����#�/G�蕤�ίÔ�-�v�3�������rz�ŌohoQ�c:6݅� %:a���*S�m�{pQq�����z��(˖����qF62x>n
72,�����
72,��i
72,"-�������y��)�xϵ�""E?���6�wYq�Ϋ]����?�ʢ�������L4ֺ"
72,��w�� �z�͏�߲�r���x��۪��.v�����љf��14O�G�gp &��h`��(�#gYyNY��x�&�_�|;5/��&��&��H>�ʣ��5�)oWu�$&?}����'�:�R(&�
72,"�,D5֚��:}A��gc��N��g��_�����$�"
72,���&o�fի]6�
72,���_
72,`�������t�ذQ0�>L2��퀸��	���E�����	[T�m���{�n���ys0It)O��7
72,"UC�}n��D���R�=�k/�pX�!��""c���ˡ�ƴ�6�hV����5�,g�y�4�w��㬆���{j��Ѭx�=�g��*;�������q�)?��l�d��e��N�I�����{�h�^i���j���i���Rܶ����j9u���ڦ���^�Sچ;��Wm[�]��얘͋�r�h�E�7����VM[�����K!7;�=_��HH��f��NO���~Y]d{gY������^��S�v���"
72,Dͼ�~��<�����Y��Ə	�{0�NG�+�[-�l��ʼy�9�գd��+Z�<;���|v|�?}���oR�l�' �x�2�c��z-�s^�
72,t��g���D'��s��;��Y���Y��i�ˮ��߲����?�O���������?�i�W�����v��������������;���n��w��5�
72,����
72,�/����&��i�⇛S��~S��~�ƋfO��^Q�ɀS�
72,<*����à4�
72,"^,N�����}�h�~1����7�WŬ���Sv��g�c؛b֞�ټ}�"
72,G� �Iv�ƺ^��k�e��(Z
72,5ˬ[�q[�%z~�R�|���)5�W��^ȫ6�����/�߶xao�v�:x����-�����tէ�k�qﻧ߾cK_�ο�LG��Zk��m��I=�[
72,"�)�F�gE����nv�8˺Xd�9[?�*۽&_��|�s�hYL�U�),X�az�V�4���H��@����y9-� F�iz��_A~|��������<)�ǋ��?d��z�zn�o"
72,"�|����d�0[֏Sow�""���/E����k&�˷_�|\8/�.�W�""F��Pa�{�0�]�"
72,o�'T*�����B������=�
72,"�����O� T$� ]�4he�R�vھ�Sv?�R����ٯ�ԃ^q�h?)��hG��rM��	l�����,�_�ٺz������<�""l�"
72,��Rj~�9#��5+f�Ύ���'��
72,>�Ϫ�y����
72,�����;�Y�����
72,�/��9
72,z{�t�t���q�pk�;��[�wx�͋����s�q��	uؾ8
72,"�l�5/#���w٘��eé9��t�_��k=ܗ�x�W�h{�e�Ï{q��W�]g,��b�Ӣ	,|������3��7h�����z���W������o�^y��F�`q�y2������"
72,ݏ� �T�o���x篿c��2/_���k�E���n���ߘ�ͫ�Z~��L
72,~��l��X��7ů��_�aH�_q¾|S���
72,G�;x
72,z���?�<��
72,"�T�����X>o��{����w��m���+ʢ}�	���/�Vޞ�U��s��X4���vO�e����i�O�,["
72,"ï�ݯ��t���Τ�z�q]��""=��y�'U�U�;x�3�oΊ6���2���=�!mZ�����e'E9�:�gEy�O�Vh�q��t5�jv\ճ����Rlq ��W5;������w٫�z3�g�9ݨW�����iQ��A�:gg�)�"
72,}O�?<1.�X��iu�4�s~�������;��z'ۚ��)��G���m��
72,����\�O��rOlZ��A�Zfx��m�������9(�7Wb$�d@�g#	3m���PZ<�O���OX ��{m]��������y�0 ��
72,q�U�Z�u]G��u� ۛfm~Z��_��j9~D!؏w^���IFP��1T��gkà���_��V�����������y�~�
72,���#��'o���>��&f�UoJP�����@y��r����0�p����1W�w��~�1�����r�`�G�l�-��¹_̤�R�e��<���
72,",��(_%\��d>vA�w��gb�Ӏw��;�gyǈ"
72,��7���;�	&�e��I1�?�����b^6�
72,���7�W��B�7�Ow~��Χ-�A��a���~J3L����>�1�M��I�Ú��^�w>�
72,����G�
72,��;ϕ�A��d�ZH=�6���
72,���ܙ��i��L:
72,"��x�AC[Fg����r�aZp%4��j�""�&2i�F���Jä��k��[�mcp��d��R�)�c��B:m�gR���(í���j�A+"
72,"��PVv퉒\Za��2�""�/���"
72,�Dzn����G چ����	 e�A�0��
72,"�3\E�`k<��8����;''R���eHi,<6*(�"
72,�:xOP{��D:���H!�
72," ��WZ!E��%`Tj����Nk����-Uל( ���SÍӶ�)�a?a�)	t2��~���q8��""���hZy��QJ"
72,%���G��f	�Dh{�i�~�QA[k�k���W
72,Y�i@����I	x ��
72,# ��Pl���
72,�L�{�r�aRr%ao ؊I����р��c0�SI/��^	����`�?(�Y�� �(`3aY�(�&Jq�b��c �T��^�F�
72,�W��!�%��;$RG�~���
72,"�-N���;���6w���[�o���$w�8����i""7"
72,(���Ý7���j���<4�yf
72,"6���yh�""�gVq'��߷�['̳��yn,��!K�("
72,"U|f`<�p�����~���U�8�(��袝tM�}P�9A,"
72,�l�V9ٵ'��hn
72,��g�s퉥M�Yǃ��є�qb
72,"�Π؍�f-�7жRh;�-����VN�Ԋ�(���;�_*��Q�[��K�*_""N��!*dc��ehiqG	-5.K��oC#�	n@� X""�P�*x �DIm���	�뚬C����:Jy��1�pܵ5��Yf#7ڒG"
72,H �agS�f=ȏ��N��d�i����ɳ��B���Ԟ�����
72,2J������+�E��U3�ۉ�< .���
72,�=���
72,"�J���\;;1�2q�s\[oznr�M���䀛H�""7A[F�s���[{nr�[}�MЌf�,����`V|�L�*�}�L�h�B�L ��=/9�8L�y	P ��z�%n�@�/�=���pܵ��"
72,s@Nb~d�?�} k�X�
72,"�""}�N};�߷;���]�0>�'���y���N�?L�+�*�(i����ȅ"
72,�J����
72,"Z�w��C��kO���	;t�I\�l�{�.""�4ʚط�k����-������,2��8�MG�4�>"
72,"�9�MPf,7v��6Gp��R�7-�ʄI�"
72,\� ��C/4��t]�C��s���'�<㐹�
72,f��/��6�9�!	sa�:$�����KP��Փ�����Y<*�HŢ�E6'��P��a�&Cܬ����>�h1Ɨ���ݢux[e-�*�
72,"m�Tү}�P:���,��i��zh{.��=p�s"
72,��O�LH逿��Չ?�ۊK%%��
72,%�6
72,���)@��s)$'��w�4z��<(�~��ES����S{�{5z�-.��X�/�a@��h��3�:#��tpJR{
72,�B�����P�
72,�[]ہ@w�]?~�����y���c���}���XR��:�@	��>L�_
72,�$�BzmG����a�8r���;���d�p�
72,"�^h4�Klj��Tֽ$�h�{�N�+I,"
72,w�A�8��1���2)
72,"<跉�,�~I)�$	=��d�퇶�B*�5����`�A����D��\I7z�HGW\X����Ke���"
72,°:hh[c#.S�4�<�(qZMʰ�h�
72,nIU�Z8|�I�n
72,�W���@)a�B[{�`^Z����[�� q�Ba�=�
72,��6�4IP^����f��-��)5W��VU4���{@.B^�>�B�mѥ#
72,"��4+�UO�gL��sB�V�M8�Z⸑|��`x�""Y@/����5���]�چ���KG�k�DV �"
72,�RѦ2�
72,t^��I�n�=)V� �Bp�
72,AV#�&f�
72,"o+n���%f�(�H""ICE�t�(��U }/""�I.\��[���-q������`.a���XI�S:1�$̪�A{"
72,�L�^� ��Ƚ�:L
72,�m�-�Q�X���*0� 5%�!����η�E.
72,0�q�{Y��Y���\�0�5Qى�\K���B��V�l�����t@?�5�
72,�=������GhK���m�&��S�����cV�~
72,ਠXmjŽ��e��0=��A�
72,�1�5
72,;R��/9:x��P���8��@dF��)뤓�
72,��G淋
72,�Zyl+
72,�J�/@��
72,���1bۃ �
72,�o�� �N*�6'
72,"|�c0��6>��,%�xp���[ţ��Ǐ�p��"
72,������~�?>w2�k鱲��48�
72,�����&�&�
72,צ�1��kWx� ���
72,�b��3�
72,"/""L��Pc�u'�� ꤆�W*2D�Cვ�=!Ҩ�s"
72,��e�0�~z�
72,���!�hٯ
72,9K�
72,:�����'
72,v���l�2�_\9��ȅs��#�/|��+���\8���-���J��
72,U����Fn�fNq��3�Gho
72,�#�}O1v�S`�
72,�)���;����dP�E��{�\��4(�4�0n����8�
72,�3g����1@�$Ai�����.�����+�ϣ2�D9+��:���������Q1��Y|?hT}�
72,�/���*M�/%�q��6�?�
72,"��9A� V��ۀ��H�Gi-wɍt��&6=ě,	�8���rM�=��=���"
72,�{��;㤒+���J����5Y���9���Vx�`Ӈ1r�B�aO�L8�`���
72,�<@R`���͈W���;�ĸ�
72,(Q�iБf0��p��=�4i��B�
72,"Ŵa��K�1��&=S���+���Њ���2D�Đ���@����+��K������,no���8�4u�v ��4��U�����0uD�:�M簏�WHq��ܐx�Om�D�8�N��T�QC�LR��0.�a�`�a3��2����$t�#Uz]���DB��\E\�I�[أ/"
72,"HM��C|��0�Nw"")b*"
72,���-
72,"�P����x�m""��k���?��(�	��[l�ƣ>(�T��"
72,"X�ҥ��v���\��oI�ȓ�ȣ�,""F�@[��7�P����Wh#[��� ��L��"
72,����
72,��$s 5�$��)�b�3���Ab�&
72,"i.l�Q�t�{K݅��ޥ��u""���I�vN���/H;��I�F�WG>��"
72,VO>
72,���2���H�
72,"����N�X���=:""b(nc�mR����藴L�%m���m�{v�"
72,(�s������(�Rnd{*�෭�=��3��Øu-��(�;�ܦHl`*��ð|t蝈V��R�w4�4&HO^b蓒� �6N豠�sk �J�^x¤�t'Y��k�&n����8�1�.
72,�<7VL
72,��[=72�S�y0�i�R*\�ɡ�Zw�
72,"�J��#w��4�:���&�&��g��*�,��j+�<�,8�y4�[P�����f�+��\���DP��pM��2j�GQ@�_ �WL�-�F�E�#�\I�(��k71�*#e%F�"
72,�>-@�d�Z�D8
72,�����I�SʈO4
72,"�J퉴)��{��γ�x�{414�YG+t�+ɝ����S�]p:P8���� 4�XITh��i�hSAѺ&/h�A?Q�G�1A,%���KP��=i�8�S�E��Sh�"
72,��l��Tg<��Lɴ�j��5�`�5�8��b���	`�`Ud�ˠ�En�X2
72,$�'g���0ŵv2S<A���k��L;�G'��� |R��r��
72,"�as\II�'�9���NP?q���;2\�P��""�*��1��"
72,"�p�X�Ԁ^.& � 5��F��[G�u�o?���""M�yv�O��4��7mU�\�?�۬�7�T�"
72,O�EWV�g���ZV�?co��yv������z�#�L�EG�#2���Z��\�H2G:oK
72,"u��	� �*E,��2��h�xE��ʬ��(Е��w��4�("
72,�g�@��4-c������ѭ�T�挦V��h�Rƾ��ʓ�
72,bT�z?�7�0���`O�ZA[ٴt51D(���H�Vt�mKN9%`��3
72,"(�l,�o�T��#+c�"
72,��C#����HcX���1���A�Ctf��{���A3\
72,D+tK��;
72,{Ykx�\9<��	i�6������&MԸ�E��a�
72,4=�
72,"@���K��;.�p�\��0�U�?&~9dM�	1�g�q)"",_S"
72,"�,�>oqxA҄(%E傑�����vFJ����6 L�	�-a�p	��v0��x8�	�ƀ+�M�G��Q�sA�� ��I����j=jV��9����U��h�B��A��=���R� zC�ץ"
72,"�&����v0�3��=2,�hiq1��=����%Ƥ¹D�ܢ�Cq��"
72,�h
72,.N�@�뀈�J����Rl������5�a�ag
72,"�B�~��ii���"">:|��7V"
72,&z���
72,�(p����:
72,FC|�p�F��BZ�a�(�%F�����
72,*b}�N
72,e��
72,"��n���,��Gm��� �zx���!"
72,"��A�)8�<C�Y<L��h��UD�C�""�dpe�� �������N�|"
72,��t)
72,]�J+?�Af��L`)<��`N0��	h���a�B_F���������cz����u���������GA�
72,��幋`�P�*�
72,p�L�N@�!���B/�o
72,��ӆ6�D�
72,�q�a3P��$Ն��p�i5��Sy
72,�p��1�����\���=�Ntʚ�<7x�J���p��bU@�9G>|�U��'I�-��wt�T�Q��:�ЈL�A�N^K
72,*��
72,"_7""�������0��#�!��!B��j"
72,\8м=f
72,aݟG4 ��V���q�
72,��Ά��v^ER�
72,j���mD���
72,�#ɡ�@&�\���M�a����G�w���Z	{�E�(��(ʔ�x�\�4*�
72,[4r�G䂥 ��ȇvOhX�bB�����
72,P�L)�)k�A�����s�Xn���)�P�g>r�i�:���� @5���#�Eǵ��J��Ç4��
72,ҁ˔{�M�繊ޥJ�9�
72,�dB{G�����˕o��
72,���n��S��
72,�\e������
72,�AKx*��L�T��8D��
72,d����Ѭ�Ko�����]BE��FQֳ���%�\(*�)�Zp����Z��i��R��K�q� c�u42�)�<�XK��
72,v>�N1��t�����
72,BFR�?ZE��[4.0�4
72,L��ЭAH*��c
72,���
72,��vcf*�	Iq�Js��)=U����x�o
72,W�H�����\���)hq���@��C�p��
72,�4�')ËH>b��H�(#Q
72,"��K5ȼ�l�o~ynj$�,��H""YOo�D��;�H�roG	,+9�H��G	�3�K$"
72,"��H""Y�� ���I$]�$��<��D�թ�D�Շ�D��`�D��H""�H�K�D�I�$��\��Drn�I$�9�Dr�3�HNsG	�#F��H""a��H""9�\F�)n�H""9�\��$v$���Ǝ$�<��D`���I$"
72,"�h$�6�f�HI""Y���zJ#M�z�E�d"
72,q$���V�$�I$�`��	�r$��� ��#~�$
72,�d
72,"0{/�,$��ܺ�D.r#�d�D��'Gɂa7�H�"
72,7�H�?�HtƑDr�S�$��
72,"I$h�1#�A9�HNrF	��#���F	�c��X""	��X""	��LI""YP�F�Fn� �,��#���7�<ǒ�*"
72,�_��1
72,Z`�m�
72,��x�1��a�P�>�
72,�!��)��!kYX��P
72,*Bj���j�(�1�*o�
72,�����
72,C����fh��DW���̀�r�����N'`t
72,"�)""_"
72,�Q<�hC������ \\d}w��S��W�by�ZbL+�
72,qi2��KKKOmӑAʀ��XmUE�[�V
72,/u�Ϭ�ʧ�0
72,����
72,�������ii8�d�`�aj8��sb�b��'F�(�� �ڒ�)D
72,5�\b.��R+
72,�x'�
72,"�H�T<���""���Ј�LB��G$�1(�,U��x�a��N�ɉ��P�m�gF��։��o��j�k�.�"
72,�l��r�qe4^���
72,G���S������'
72,pp�h�Y���J@fB��X���
72,�|&R%
72,���c��
72,ږX�F��V��.��u��0�^<��
72,�)��xՍ.4�[G��X���M#�u�l��]Y���%�Mǋ�QҶ��ޏ��m����`?���I���8L���<�4��0�d��
72,�p1��A[��
72,�}����Tj����i#ɸ
72,1�I
72,�7tj�e�J�zh����B
72,cե�j:
72,"3A�3xf;48nS""`ؽ�T|�X�T���=���X�RG���5$\���S�Ac"
72,"4�l""h"
72,"*����B��MB��""Ih����<����t6YhAR�늋@���{C���K�����;�"
72,Ib��y\`G�u��C�����Y�]�O�H�����&�٥!��������c��s�?Àa�����0
72,"���""�lj4=-a}������̈���ʈ�2~��"
72,ڋ`1�_��	n`�l��+<�MD�l�A$��
72,�<�r
72,>�\J��A�
72,"<��;RBd�P�R�/����$x1�Ū$��+�""P�lcRNA�IhI�S�N)0p��~t�wa\ȁzF6 ���`�e�v>PyH�ϟz�:�z�-����B���Oy���x��$R*?X"
72,���uW�b�k
72,"T �V5�2,# �#�E��%i1F�W��jE0�4�X"
72,|��Z
72,(E����cU lzN��r�q�$�I���1���0��:�ρ6�
72,����$�� ]ϟz���>7~�un�?�@O	_��ߥ��F+$s
72,ːK�p/sx`�c�TB��	�s
72,",i�`;� A�Sv��'��@wu����[���4�Fw�J�2��P<)��ƣ{]rʗ"
72,���Ŝ�m�����dM�]Aa '
72,"r��a���� ����{A�yMe�D�ܡ 񊰓�.<""�sA��r.��"
72,���ud:&���eB��_%%p��D���ȃ2��+���G0�TA]i�#B��R`RHe�c�B|.���Wa~P[��aM�1��:�T�b��O s���4;gr.�a
72,�	�EaJ.�Y
72,S�\��G���<ߖ�[x��ш�Z�o�H����s�
72,"t�=��D4,���s��"
72,�HE�
72,Ӗ�#�����.���Z��e�¾lrz��}�;aݮ�K��t;:�����4;�kGh�I!В�ХԒZ+d�ѯ�r#���Қ��`�
72,�R�l4�pOc�@�� 0�����0
72, 7� G��ȀU �
72,�oR�A�ɹ1
72,�[�шQ&`
72,b�I^M���j�^�@�Lh�q
72,C�����j���X: O��%�Jc{%
72,"����^${�9�p�OYy��]c{7�Њ�C�2R""3ΐQ�1�ӂd�u�"
72,���c=s
72,.W��E�Ga��|*�̕F����\�ޑL�Od\�h����Au�
72,-�L|$�f��
72,�:��i
72,G!C�����c4#S
72,W����$�xEO��f)ǡ.��S�e;f[��Ʋ����e�=�d�Ĵ
72,�7�;��Z���
72,O��z��υŜ�;X_xN3��!!��
72,�n晥@��
72,��P�+d�W-&宔ts
72,�<�$�GǯS���h��
72,��A2��N�
72,�.ȅ$��&v�))(~_ ���a�ʕ�d�
72,"c�%*�\�ĩ�t-� �d� s�&%,%��2�h�b�7��4��!Fo����B�"
72,� ��|�>y�0�� 寥�J���0�-zrpX���
72,E�)u=��*�*	���E
72,R ��^�I-�^�
72,4�R`O��<$����<�g�\���ϜK�
72,"F�""-(2��I�""d�XN�p5�u�PZU�9�(1���T�7�C74Q܆�EZ^��q�M��\2�)ӭ���AIZ_tсa��S"
72,"ʐ�`���A�L˱��B�\x\��C�$�[��`�|�1Ř��/���1�s�,,���}���,���CM��%E���=S&"
72,"lm_D��T��(����x""A�|��(48r""�SRn�Ԕ��R^�$ѿT�ŠI��BҐ�""l/"
72,"��a~�T�6�++�P�pHȖ""7�\�P�}\%υ�H����5G��U�;s.�����{Uak��s��}����ym�,0 �\n4�V�f��ã�V�q�ke�L"
72,^ς�G��B_��B3+P�6N�(W�J+��A��P֖�sL�Ke�\�s
72,C����7����iG��d�`m��ǿZe�)��eYŢ��[W��^������v��RX�
72,"�)��ip7U�����e\Y(��R�:�/D��K�:�H )mn�$�}#�9JQ�~""�T0�nXS�"
72,"S�Rj�TJ���@Vx,czi��"
72,�^�qY��/
72,��CvX6�|o
72,G&/��K%d��Y�s����s
72,"�,��)5��cgBJ����.Ji�8^���3 EF��\z�B>�o4�v�/���`>Ǩ�'�=��Ŭ�n�!����[���(#�H�6��;�(`�>�%���Cy�q���s0j��}�����k*o�"
72,���}�
72,������Cy:�=�͟ޗ{�������Ų��8�H�0I� 1*
72,�!��x`d��e�Y �����}8.�ȟ�O��S�.�}�O�u��ğ$b�7Py8�����>lJ�N
72,%mYz���	t���(+
72,"��a���'�5������*�2]A""�-�"
72,�JK�
72,�V�uchO�tݓsVAs�9�
72,��d.��I
72,은y�I
72,���C��� �}�`��%�+@aqa;/��#l��.�
72,*(g�܄ֵ��E<sөǣ���v�_=�:֯����E6�C�]
72,��d6��9F����N���*<����RD;��e)H�G�=����Sb8�m�y����ѡ��W�l��E.G�F��|�w#�J��L�A
72,"�y6��x��""��mUv��h���ٺ����m�h��i��k�Y����ѯ8�?������"
72,",6�[M����j���Ǜ�۽>�Ǜ�۽>z|���.?W��xկ��@ �ʽ�pO�$!?�Vs��'��Ǐ?��]����g��7u�VӇ�X��]�ij��h��D��>�$<�\�0�0�����|G��Z�40S%��o	�@�,.��&-��{��'x@	7!�#�ŻA��C���"
72,����
72,��M��
72,3}����-���9�&��1w�G0:N��N2
72,T�;� ���d.��H:�`�9���D��s����s���y�$/.FBc
72,0���
72,P�҉1���A���=�+�N�6wh��S�El�8?�����B
72,"(�8W�,㹖�p;�x��1J���\I�@�<ÀC�0U\�R�;�󠔙bk.��#�(��G#������h�"
72,"R����D�K8�4�V;Lae����bĿ2�؄�! �,#ӳ�Onū;̾�ro�L��4��QE8��a�"
72,���J.�C!�$��%��S&L�H�����&��u
72,"D��_r�E���u��m���̵t�=��\Y)���VP��!y��H\e����F	FR�k/�T`��:0�9,�3�Jh&�TX��"
72,"W�yw�1����^c9%�~B��Q5�Za����k�u N��\ϓ|U!7B��ǎN<xf%�̾�ԅ���k EG sI����>ʚ�G�Ĵ�=-cO�ǋ���v���eM��)���^8�S�r�u�Q�;z,�\ii�"
72,"K��͸��������|٫9u3�MvQw]�b��h���2����޽�ì^fB2�g��4�.d�w���hr�f�f�,g/�F�:V��+늝���릞of�����CvY,��	���~��'�~]|�eh�����^�Vm���̊6��X�c�VM��N��7D�P�5�"
72,&^e�U}���u�tE՝��}���?�x�˃r���آXg��o2a�o��`z�u��&[�jö�S�U���YU�:3|?��^��iqXM�W0L��Ҕ���L�f=�<�E�ibK�����.����
72,��VuUvuSV�����#�t��^&6���(+X����}�}O���uݼN��Z��nVE5��߿����=�y|Y
72,"����̪����,c��"
72,�l[}?T��7�lV�]�*�bW��Ɠ)T�i�v����'�CϚ��gu����^�A|^/>
72,�?FU[v�˘��U �T�
72,"]��Q��L�X�Ɠ���/c��M�5�����Pfߤ�=1�y��M}Y.�쳬6��e_b�=��kb��gD��Y:�^m�5!��wqWf����{¾,��bߕmyQ.���^&�Ŷ�^n��ʪ��e��U��fC[���v�ɱ_�'��"
72,�~����
72,"""�ݬ�e��M9�lJ�CU7U�*gYY��̶ �t�u6�MW^&�f<��w80�""�C`�q"
72,ݑ53��wC�v��'_�Ż���xQVEӎ'�K�ާ�^;
72,"�~��L)_�ܵ�c@t��勪�^��""f3�k<����z�-3�${+t�b/+.�M���z��æo#��w�t���G�����$"
72,E߾f]��5�ط�kI�tUl��F�ɗT�aG%X�z�xݎ�Ml۬��X���l>�w�4$���\�O��p�Y����R0�T���J˸(���E�|sZV/c�շ�{�
72,����m�[*��t�	�e:�un�^=�v�5��ϟ�6u�>=��ռn���\�'�a�}��E�x}��~����o#>��;K���)�s��_���o
72,�\m���s�h�Z�U���|W��p4��z��f�e�i�x��fͦTx�
72,�+�������Kϋ��!��[��A�ټh^��'6M�b�e��my���0ΰ����zI
72,��;�4�Q�U�g�b�z�Y�v��c�yt�J�<�e������6����Mە�7L0VݏU/Y���
72,h����E�Ⱥz�X�4��+�f�����J����|
72,p�iں��u�Oڸ��.�q�[�t�Hu���] ��B!�gڏ��شe]��D.�$��|�jYV/�5!�1>
72,M>b��ӿ�2��XŦ���S��E�fW��ܲ���A|�26�袾����f���dc�;Y����n'Ofu5�����tL5�t
72,��d�Ģ�sv]vWi��t����b���@���\	��f��g��2V�|��îv���D�]���΅�EpR{=��
72,�{Ͻ��D�\/]���Z�H�
72,�Mf�V�ۢ�)�wq���97B���0�Z*�OdȽ��;��8��s�T�=��k�A��j��)�T:�7�(�Ko�`������
72,�J�B.�/J�K��:����Չ��
72,�{-��~X=6��i.ԉҹq�x}�����@����
72,����{��{���{��
72,r~��������	�I���
72,"Eϵ�J)d5�:o,��M��Ұ{���{J"
72,-�S*��7���+� {�+%=1La��ʜ(�{�
72,�׮s
72,"���ern�Ut��3ZK�ϓln�1�{�ǊC.���g�&""t����H����k�ɯ�@��ɕ�0�2�S0�����fS�g�;�̝�<����Dن�x�q4 Y`Z{#y���k�:�"
72,�zmqLI�s�����HW�r���{��4H;m�^5���P�vP(C��r�udȃ���p�ɐ
72,���ߓ�R~z���i�:ާ�?�mh�a�c�]HY�rn�#*tJ��܆$�`���=�[��p�a[v�ҹ��
72,X��nt����* I!�
72,i4ꍁ:m^����8t��SM�\��	.����:цI
72,�xU�䠠���i�l�]So+�$�CŴ4�џ�X�%��g2Tu����`+���
72,��	�uuѮ����u����bY�Wo�X�Y�4���}�[�;S�7�A{U_?`��
72,K>0+o] 
72,��O|=^Ch�c&�B�pnr�%p�\8�yȵ�^����}�����l %��y���r
72,lyt1l�xD>g�d��N^��r�Ł�iT�0j���\�g��j�#�~�q
72,�b5X��i���^6�~/���qST��?�'ߺ��a����_���˿����_��{���
72,���
72,M������?���������얏���u�]b��Lz��\d�u
72,"�;�&��'��""N.����_i�YY,3t���#���b�X��x�~�o�#��i�F+gu����W��q9�1/�}�죬ڮX4�j��"
72,HO�FG��v���t�
72,����f�����=��@͎��xG����z�O*:�S��Eu�/����-��će�U������a��������ۮh�c{~V�o���_�Ͽci���R��ˇ�.6�v<��!V���*��=��^�ux �4�Y]�(c;�L�Ǐ���_��!�=��U�5�Ml�lUϋ%���rQm���6�ʠ=�/
72,�r~FU���+.�j
72,_�e�5�2���bY/m�7�WΡr80�G}�s��[%�ރ���ﴷ��`rze����n��י`�L<�|M(`[Ǧ��bY�)��{:���ӋM��Րs̖uYw��g�!�
72,"^�ټlWe?ٴ��ūr>��Y�l��׿���h����N\���aPZw09mgM��X�������P+jǗu�j��W�*����_��B=,��EW?���%��Y�L����̗u1���n�8:yg�o��Cf��@W'�l�O��j;�mW5�a��g�y�n>"
72,�jo��'���n�����%ħH�2٣�C����pH���+�WM_�~;�
72,"�r�I(1�����ِ�����ʈ䎒ِ2���54�#��iy���c���*W�*�״�,�]F�����r�3l@q�8kgM�\��⬪�|6zV.*��]�M.��4�O�/;OV�""�~�?̒�{n�"
72,j^�;mG\�.����/��
72,}���vEW�ƫ����<��ͣ
72,2p���V[����Hs
72,�G{!��ޙ�
72,s7�K..
72,!��\zg|���b6+
72,gfR�X�Y1�0�|&\:�����BznBqY�8s��ƘK����-��̓�^F�L��᫯��W_�xt�j��˯Ԫ��|2�����������^���
72,mڛ�'������u׿_}k�/��f�x�r
72,"_ٿ_~w��O���l�cI���5&"""
72,_u����  ��}-��:	 
74,"Nucamp Requires Javascript to Run, sorry."
74,Nucamp | Affordable Coding Bootcamp
75,Comparison between equivalent Intel & Graviton instances for MariaDB & PostgreSQL on Amazon RDS
75,Search
75,Browse
75,Community
75,About Community
75,Private Forums
75,Private Forums
75,Intel oneAPI Toolkits Private Forums
75,All other private forums and groups
75,Intel AI Software - Private Forums
75,GEH Pilot Community Sandbox
75,Intel® Connectivity Research Program (Private)
75,Intel-Habana Gaudi Technology Forum
75,Developer Software Forums
75,Developer Software Forums
75,Toolkits & SDKs
75,Software Development Tools
75,Software Development Topics
75,Software Development Technologies
75,Intel® DevCloud
75,Intel® Developer Cloud
75,"oneAPI Registration, Download, Licensing and Installation"
75,GPU Compute Software
75,Software Archive
75,Edge Developer Toolbox
75,Product Support Forums
75,Product Support Forums
75,Memory & Storage
75,Embedded Products
75,Visual Computing
75,FPGA
75,Graphics
75,Processors
75,Wireless
75,Ethernet Products
75,Server Products
75,Intel® Enpirion® Power Solutions
75,Intel Unite® App
75,Intel vPro® Platform
75,Intel® Trusted Execution Technology (Intel® TXT)
75,Intel® Unison™ App
75,Intel® QuickAssist Technology (Intel® QAT)
75,Gaming Forums
75,Gaming Forums
75,Intel® ARC™ Graphics
75,Gaming on Intel® Processors with Intel® Graphics
75,Developing Games on Intel Graphics
75,Blogs
75,Blogs
75,@Intel
75,Products and Solutions
75,Tech Innovation
75,Thought Leadership
75,Cloud
75,Examine critical components of Cloud computing with Intel® software experts
75,Success!
75,Subscription added.
75,Success!
75,Subscription removed.
75,"Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your"
75,profile.
75,Intel Community
75,Blogs
75,Tech Innovation
75,Cloud
75,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
75,107 Discussions
75,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
75,Subscribe
75,Article Options
75,Subscribe to RSS Feed
75,Mark as New
75,Mark as Read
75,Bookmark
75,Subscribe
75,Printer Friendly Page
75,Report Inappropriate Content
75,Mohan_Potheri
75,Employee
75,‎03-31-2023
75,08:00 AM
75,"9,803"
75,Introduction:
75,"Databases are typically the crown jewel of enterprise applications. All workloads including web-based e-commerce, social media, cloud services are typically backed by a database. Open-source databases[i] have become completely mainstream over the past decade and are the primary leaders in innovation in the database space. Open-source software has many attributes that make them successful in this cloud era. One of the major benefits is that developers can use open-source software and databases, without any licensing fees. Open-source software as developers code in features that they need quickly and contribute it back to the community. Open-source projects are therefore more agile and have out-evolved closed source alternatives since the early 2000s."
75,"AWS is positioning Graviton (an ARM based processor) aggressively from a price perspective compared to Intel 3rd generation Xeon Scalable Processors based instances.[ii] They are using cost savings as the primary mechanism to lure customers away from Intel based instances. Re-platforming is needed for customers moving to Graviton, which requires enterprise re-certification of the software with associated porting cost.  Intel Xeon leads across most popular database, web, and throughput related workloads. The cloud ecosystem for Intel has developed over the past 15 years, whereas ARM is relatively new and untested. Customers can potentially experience cloud vendor lock-in as Graviton is unique to AWS."
75,"The critical nature of open-source databases in the cloud makes them a good workload to compare Amazon EC2 Intel 3rd generation Xeon Scalable and Graviton instances. MariaDB[iii]  is an open-source variant of MySQL that offers a consistent set of advanced features and functionality across all major cloud platforms. PostgreSQL is one of the most powerful open-source databases known for its proven architecture, reliability, data integrity, robust feature set and extensibility. We will use MariaDB and PostgreSQL as the two open-source relational databases used in comparison testing on AWS EC2 between Intel 3rd generation Xeon Scalable processors and Amazon Graviton."
75,Amazon RDS:
75,"The Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.  Amazon RDS is used in modern applications for data storage in web and mobile applications. Customers move to managed databases from RDS to avoid having to manage their own databases. Many customers want to leverage open-source databases in the public cloud and break free from legacy databases"
75,MariaDB and PostgreSQL are popular open-source relational databases used for cloud-based applications. We will be deploying identically sized RDS instances for these two databases on Intel 3rd generation Xeon Scalable and Graviton based instances and running the commonly used Sysbench workload to compare their relative performance.
75,Instance Configuration:
75,The details about the Amazon EC2 instance choices that were made with Intel and Graviton instances are shown in Table 1.
75,Category
75,Attribute
75,Config1
75,Config2
75,Run Info
75,Testing Date
75,"Nov 3-11, 2022"
75,"Nov 3-11, 2022"
75,Cloud
75,AWS
75,AWS
75,Instance Type and CPU
75,Instance Type
75,db.r6g.4xlarge or
75,db.r6i.4xlarge
75,db.r6g.8xlarge or db.r6i.8xlarge
75,CPU(s)
75,Memory
75,128GB
75,256GB
75,Network BW / Instance
75,12.5 Gbps
75,25 Gbps
75,Storage: Direct attached
75,SSD GP2
75,SSD GP2
75,Drive Summary
75,1 volume 75GB
75,1 volume 75GB
75,Table 1: Instance configuration details for the testing
75,Workload Configuration:
75,Details about the workload and its attributes are shown in Table 2. Sysbench 1.0.18 was run 4 times per configuration and the results were then averaged for both MariaDB and PostgreSQL.
75,Category
75,Attribute
75,Config1
75,Config2
75,Run Info
75,Benchmark
75,sysbench 1.0.18
75,sysbench 1.0.18
75,Dates
75,"Nov 3-11, 2022"
75,"Nov 3-11, 2022"
75,CPUs
75,Thread(s) per Core
75,"1,2,4"
75,"0.5,1,2"
75,Core(s)
75,CPU Models
75,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
75,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
75,BIOS
75,Workload Specific Details
75,Workload
75,MariaDB 10.6.10
75,PostgreSQL 14.4-R1
75,Command Line
75,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --mysql-user=sbtest --mysql-password=password --mysql-host=mariadb-r6g-4xl-v1.couqinukves2.us-east-1.rds.amazonaws.com  --db-driver=mysql --mysql-db=sbtest run
75,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --pgsql-user=sbtest --pgsql-password=password --pgsql-host=pg-r6i-16xl-v1.couqinukves2.us-east-1.rds.amazonaws.com --pgsql-port=5432 --db-driver=pgsql --pgsql-db=sbtest run
75,Table 2: Workload configuration details for the testing
75,MariaDB Results:
75,The results from the sysbench testing for MariaDB are shown in Table 3. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
75,Queries per second
75,Threads
75,r6g.4xlarge (Graviton)
75,r6i.4xlarge (Intel)
75,Abs Diff
75,Percentage
75,Difference (qps)
75,45420.95
75,55450.305
75,10029.355
75,22%
75,82310.385
75,103531.905
75,21221.52
75,26%
75,138068.393
75,161312.673
75,23244.28
75,17%
75,Threads
75,r6g.8xlarge (Graviton)
75,r6i.8xlarge (Intel)
75,Abs Diff
75,Percentage
75,Difference (qps)
75,102649.813
75,127593.153
75,24943.34
75,24%
75,169209.105
75,216709.785
75,47500.68
75,28%
75,250122.328
75,302356.915
75,52234.5875
75,21%
75,Table 3: MariaDB QPS comparison between Intel and Graviton Instances.
75,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 20-30% for MariaDB.
75,Figure 1: Graphical comparison of sysbench performance for MariaDB between Intel and Graviton based instances (Higher is better)
75,PostgreSQL Results:
75,The results from the sysbench testing for PostgreSQL are shown in Table 4. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
75,Queries per second
75,Threads
75,r6g.4xlarge (Graviton)
75,r6i.4xlarge (Intel)
75,Abs Diff
75,Percentage
75,Difference (qps)
75,58117.2825
75,65466.75
75,7349.4675
75,13%
75,99423.5025
75,114673.365
75,15249.8625
75,15%
75,140116.51
75,152913.408
75,12796.8975
75,Queries per second - 32 vCPU (8xlarge)
75,Threads
75,r6g.8xlarge (Graviton)
75,r6i.8xlarge
75,(Intel)
75,Abs Diff
75,Percentage
75,Difference (qps)
75,81133.195
75,125247.73
75,44114.535
75,54%
75,141914.253
75,208751.038
75,66836.785
75,47%
75,219109.908
75,288519.455
75,69409.5475
75,32%
75,Table 4: PostgreSQL QPS comparison between Intel and Graviton Instances.
75,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 30-50% for PostgreSQL.
75,Figure 2: Graphical comparison of sysbench performance for PostgreSQL between Intel and Graviton based instances (Higher is better)
75,Conclusion:
75,"Customers need to be careful with their choice of instances for their workloads in the cloud. Our results show that not all instances are created equal. Intel 3rd generation Xeon Scalable processors-based instances outperform Amazon similar Graviton based instances for open-source relational databases by 20-50% as the results have shown. Intel’s active participation in the open-source community and its innovative HW and SW optimizations work to boost performance of Database workloads as we have shown. By choosing Intel instances and right sizing them based on their performance characteristics in Amazon RDS, lower TCO with optimal performance can be attained."
75,Disclosure text:
75,"Tests were performed in October-November 2022 on AWS in region us-east-1. All configurations used general Purpose SSD gp2 storage. Baseline I/O performance for gp2 storage is 3 IOPS for each GiB, with a minimum of 100 IOPS. This relationship means that larger volumes have better performance. For our experiments we used. 550GiB storage with a baseline performance of 1500 IOPS. We ran the following Database Engines: - MariaDB 10.6.10, - PostgreSQL 14.4-R1. These were run on each of 4 DB server Instances described below. Database server used AWS RDS servers with 4 DB Instance types."
75,db.r6g – memory-optimized instance classes powered by AWS Graviton2 processors
75,"db.r6g.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
75,"db.r6g.4xlarge, 16 vCPU, 128             GB Memory  & Up to 10 Gbps Network interface"
75,db.r6i – memory-optimized instance classes powered by 3rd Generation Intel Xeon Scalable processors
75,"db.r6i.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
75,"db.r6i.4xlarge, 16 vCPU, 128               GB Memory & Up to 10 Gbps Network interface"
75,Pricing URL for MariaDB: https://aws.amazon.com/rds/mariadb/pricing/
75,For PostgreSQL:   https://aws.amazon.com/rds/postgresql/pricing/
75,"DB Client machine details:  For Database client machine, we used the EC2 instance type: c6i.4xlarge with 16vCPU (8 core), with 32 GB Memory, 75 GB GP2 Storage volume  with 12.5GB Network bandwidth powered by 3rd Generation Intel Xeon Scalable processors. The client machines use the following Software Image (AMI) with Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2022-09-14 & ami-0149b2da6ceec4bb0. All DB Instances, as well as the client Instances were run in US-EAST-1 region. Benchmarking Software: We used sysbench tool to load data and to run oltp_read tests on all these configurations. We used sysbench version"
75,1.0.18 (using system LuaJIT 2.1.0-beta3) for all the DB testing.
75,Disclaimer text:
75,"Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation."
75,Bibliography
75,[i] https://www.dbta.com/BigDataQuarterly/Articles/The-Past-Present-and-Future-of-Open-Source-Databases-150954.aspx discusses the present and future of open source databases
75,"[ii] https://www.percona.com/blog/comparing-graviton-arm-performance-to-intel-and-amd-for-mysql-part-3/ compares DB Engines (and clients on the same instances) on M6i.* (Intel) , M6a.* (AMD),  M6g.*(Graviton) EC2 instances."
75,[iii] https://mariadb.com/database-topics/mariadb-vs-mysql/ provides a good comparison of MySQL and MariaDB.
75,Appendix A: DB Configuration Tuning:
75,------------------------------------------------------------------------------------------------------
75,Tuning for MariaDB:
75,We followed this article for performance tuning mariadb:
75,https://mariadb.com/resources/blog/10-database-tuning-tips-for-peak-workloads/
75,The following parameters were tuned.
75,1. InnoDB Buffer Pool Size
75,Making the InnoDB buffer pool size as large as possible ensures you use memory rather than disks for most read operations (because the buffer pool is where data and indexes are cached).
75,LEFT IT UNCHANGED from the RDS default which is {DBInstanceClassMemory*3/4}
75,where
75,DBInstanceClassMemory is a Formula variable with this description:
75,(from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ParamValuesRef.html
75,2. InnoDB Log File Size
75,"The redo logs make sure writes are fast and durable, and  the InnoDB redo space size is important for write-intensive workloads. The logs’ size is determined by innodb_log-file-size. For best results, generally you’ll want to set a combined total size to be at least 1/4 (or even 1/2) of the InnoDB buffer pool size, or equal to one hour’s worth of log entries during peak load. For MariaDB, we set innodb_log_file_size as {DBInstanceClassMemory*(3/4)*(1/4)}:"
75,- We computed and entered the number in a custom parameter group.
75,innodb_log_file_size
75,For 4xl instance this is   25769803776 (24GB of log file size for 128GB of RAM in that instance)
75,For 8xl instance this is   51539607552 (48GB of log file size for 256GB of RAM in that instance)
75,Tuning for PostgreSQL:
75,+-----------------------------------------------------------------------------+
75,CHANGED THIS FOR EVERY DB Instance class before DB Instance creation:
75,+-----------------------------------------------------------------------------+
75,max_wal_size = '96GB'
75,-->Default is 2048 (specified in MB)
75,16xl - 393216
75,8xl - 196608
75,4xl = 98304
75,2xl - 49152
75,+-----------------------------------------------------------------------------+
75,Refer to the blog:
75,https://www.percona.com/blog/2021/01/22/postgresql-on-arm-based-aws-ec2-instances-is-it-any-good/
75,Tags (4)
75,Tags:MariaDBopen sourcePostgreSQLRelational Databases
75,Kudo
75,"You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in."
75,Comment
75,About the Author
75,"Mohan Potheri is a Cloud Solutions Architect with more than 20 years in IT infrastructure, with in depth experience on Cloud architecture. He currently focuses on educating customers and partners on Intel capabilities and optimizations available on Amazon AWS. He is actively engaged with the Intel and AWS Partner communities to develop compelling solutions with Intel and AWS. He is a VMware vExpert (VCDX#98) with extensive knowledge on premises and hybrid cloud. He also has extensive experience with business critical applications such as SAP, Oracle, SQL and Java across UNIX, Linux and Windows environments. Mohan Potheri is an expert on AI/ML, HPC and has been a speaker in multiple conferences such as VMWorld, GTC, ISC and"
75,other Partner events.
75,Community support is provided during standard business hours (Monday to Friday 7AM - 5PM PST). Other contact methods are available here.
75,"Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade."
75,"For more complete information about compiler optimizations, see our Optimization Notice."
75,©Intel Corporation
75,Terms of Use
75,*Trademarks
75,Cookies
75,Privacy
75,Supply Chain Transparency
75,Site Map
77,An Introduction to Hibernate 6
77,An Introduction to Hibernate 6
77,version 6.3.2.Final
77,Table of Contents
77,Preface
77,1. Introduction
77,1.1. Hibernate and JPA
77,1.2. Writing Java code with Hibernate
77,"1.3. Hello, Hibernate"
77,"1.4. Hello, JPA"
77,1.5. Organizing persistence logic
77,1.6. Testing persistence logic
77,1.7. Architecture and the persistence layer
77,1.8. Overview
77,2. Configuration and bootstrap
77,2.1. Including Hibernate in your project build
77,2.2. Optional dependencies
77,2.3. Configuration using JPA XML
77,2.4. Configuration using Hibernate API
77,2.5. Configuration using Hibernate properties file
77,2.6. Basic configuration settings
77,2.7. Automatic schema export
77,2.8. Logging the generated SQL
77,2.9. Minimizing repetitive mapping information
77,2.10. Nationalized character data in SQL Server
77,3. Entities
77,3.1. Entity classes
77,3.2. Access types
77,3.3. Entity class inheritance
77,3.4. Identifier attributes
77,3.5. Generated identifiers
77,3.6. Natural keys as identifiers
77,3.7. Composite identifiers
77,3.8. Version attributes
77,3.9. Natural id attributes
77,3.10. Basic attributes
77,3.11. Enumerated types
77,3.12. Converters
77,3.13. Compositional basic types
77,3.14. Embeddable objects
77,3.15. Associations
77,3.16. Many-to-one
77,3.17. One-to-one (first way)
77,3.18. One-to-one (second way)
77,3.19. Many-to-many
77,3.20. Collections of basic values and embeddable objects
77,3.21. Collections mapped to SQL arrays
77,3.22. Collections mapped to a separate table
77,3.23. Summary of annotations
77,3.24. equals() and hashCode()
77,4. Object/relational mapping
77,4.1. Mapping entity inheritance hierarchies
77,4.2. Mapping to tables
77,4.3. Mapping entities to tables
77,4.4. Mapping associations to tables
77,4.5. Mapping to columns
77,4.6. Mapping basic attributes to columns
77,4.7. Mapping associations to foreign key columns
77,4.8. Mapping primary key joins between tables
77,4.9. Column lengths and adaptive column types
77,4.10. LOBs
77,4.11. Mapping embeddable types to UDTs or to JSON
77,4.12. Summary of SQL column type mappings
77,4.13. Mapping to formulas
77,4.14. Derived Identity
77,4.15. Adding constraints
77,5. Interacting with the database
77,5.1. Persistence Contexts
77,5.2. Creating a session
77,5.3. Managing transactions
77,5.4. Operations on the persistence context
77,5.5. Cascading persistence operations
77,5.6. Proxies and lazy fetching
77,5.7. Entity graphs and eager fetching
77,5.8. Flushing the session
77,5.9. Queries
77,5.10. HQL queries
77,5.11. Criteria queries
77,5.12. A more comfortable way to write criteria queries
77,5.13. Native SQL queries
77,"5.14. Limits, pagination, and ordering"
77,5.15. Representing projection lists
77,5.16. Named queries
77,5.17. Controlling lookup by id
77,5.18. Interacting directly with JDBC
77,5.19. What to do when things go wrong
77,6. Compile-time tooling
77,6.1. Named queries and the Metamodel Generator
77,6.2. Generated query methods
77,6.3. Generating query methods as instance methods
77,6.4. Generated finder methods
77,6.5. Paging and ordering
77,6.6. Query and finder method return types
77,6.7. An alternative approach
77,7. Tuning and performance
77,7.1. Tuning the connection pool
77,7.2. Enabling statement batching
77,7.3. Association fetching
77,7.4. Batch fetching and subselect fetching
77,7.5. Join fetching
77,7.6. The second-level cache
77,7.7. Specifying which data is cached
77,7.8. Caching by natural id
77,7.9. Caching and association fetching
77,7.10. Configuring the second-level cache provider
77,7.11. Caching query result sets
77,7.12. Second-level cache management
77,7.13. Session cache management
77,7.14. Stateless sessions
77,7.15. Optimistic and pessimistic locking
77,7.16. Collecting statistics
77,7.17. Tracking down slow queries
77,7.18. Adding indexes
77,7.19. Dealing with denormalized data
77,7.20. Reactive programming with Hibernate
77,8. Advanced Topics
77,8.1. Filters
77,8.2. Multi-tenancy
77,8.3. Using custom-written SQL
77,8.4. Handling database-generated columns
77,8.5. User-defined generators
77,8.6. Naming strategies
77,8.7. Spatial datatypes
77,8.8. Ordered and sorted collections and map keys
77,8.9. Any mappings
77,8.10. Selective column lists in inserts and updates
77,8.11. Using the bytecode enhancer
77,8.12. Named fetch profiles
77,9. Credits
77,Preface
77,Hibernate 6 is a major redesign of the world’s most popular and feature-rich ORM solution.
77,"The redesign has touched almost every subsystem of Hibernate, including the APIs, mapping annotations, and the query language."
77,"This new Hibernate is more powerful, more robust, and more typesafe."
77,"With so many improvements, it’s very difficult to summarize the significance of this work."
77,But the following general themes stand out.
77,Hibernate 6:
77,"finally takes advantage of the advances in relational databases over the past decade, updating the query language to support a raft of new constructs in modern dialects of SQL,"
77,"exhibits much more consistent behavior across different databases, greatly improving portability, and generates much higher-quality DDL from dialect-independent code,"
77,"improves error reporting by more scrupulous validation of queries before access to the database,"
77,"improves the type-safety of O/R mapping annotations, clarifies the separation of API, SPI, and internal implementation, and fixes some long-standing architectural flaws,"
77,"removes or deprecates legacy APIs, laying the foundation for future evolution, and"
77,"makes far better use of Javadoc, putting much more information at the fingertips of developers."
77,"Hibernate 6 and Hibernate Reactive are now core components of Quarkus 3, the most exciting new environment for cloud-native development in Java, and Hibernate remains the persistence solution of choice for almost every major Java framework or server."
77,"Unfortunately, the changes in Hibernate 6 have obsoleted much of the information about Hibernate that’s available in books, in blog posts, and on stackoverflow."
77,"This guide is an up-to-date, high-level discussion of the current feature set and recommended usage."
77,It does not attempt to cover every feature and should be used in conjunction with other documentation:
77,"Hibernate’s extensive Javadoc,"
77,"the Guide to Hibernate Query Language, and"
77,the Hibernate User Guide.
77,The Hibernate User Guide includes detailed discussions of most aspects of Hibernate.
77,"But with so much information to cover, readability is difficult to achieve, and so it’s most useful as a reference."
77,"Where necessary, we’ll provide links to relevant sections of the User Guide."
77,1. Introduction
77,Hibernate is usually described as a library that makes it easy to map Java classes to relational database tables.
77,But this formulation does no justice to the central role played by the relational data itself.
77,So a better description might be:
77,"Hibernate makes relational data visible to a program written in Java, in a natural and typesafe form,"
77,"making it easy to write complex queries and work with their results,"
77,"letting the program easily synchronize changes made in memory with the database, respecting the ACID properties of transactions, and"
77,allowing performance optimizations to be made after the basic persistence logic has already been written.
77,"Here the relational data is the focus, along with the importance of typesafety."
77,"The goal of object/relational mapping (ORM) is to eliminate fragile and untypesafe code, and make large programs easier to maintain in the long run."
77,"ORM takes the pain out of persistence by relieving the developer of the need to hand-write tedious, repetitive, and fragile code for flattening graphs of objects to database tables and rebuilding graphs of objects from flat SQL query result sets."
77,"Even better, ORM makes it much easier to tune performance later, after the basic persistence logic has already been written."
77,"A perennial question is: should I use ORM, or plain SQL?"
77,The answer is usually: use both.
77,JPA and Hibernate were designed to work in conjunction with handwritten SQL.
77,"You see, most programs with nontrivial data access logic will benefit from the use of ORM at least somewhere."
77,"But if Hibernate is making things more difficult, for some particularly tricky piece of data access logic, the only sensible thing to do is to use something better suited to the problem!"
77,Just because you’re using Hibernate for persistence doesn’t mean you have to use it for everything.
77,"Developers often ask about the relationship between Hibernate and JPA, so let’s take a short detour into some history."
77,1.1. Hibernate and JPA
77,"Hibernate was the inspiration behind the Java (now Jakarta) Persistence API, or JPA, and includes a complete implementation of the latest revision of this specification."
77,The early history of Hibernate and JPA
77,"The Hibernate project began in 2001, when Gavin King’s frustration with Entity Beans in EJB 2 boiled over."
77,"It quickly overtook other open source and commercial contenders to become the most popular persistence solution for Java, and the book Hibernate in Action, written with Christian Bauer, was an influential bestseller."
77,"In 2004, Gavin and Christian joined a tiny startup called JBoss, and other early Hibernate contributors soon followed: Max Rydahl Andersen, Emmanuel Bernard, Steve Ebersole, and Sanne Grinovero."
77,"Soon after, Gavin joined the EJB 3 expert group and convinced the group to deprecate Entity Beans in favor of a brand-new persistence API modelled after Hibernate."
77,"Later, members of the TopLink team got involved, and the Java Persistence API evolved as a collaboration between—primarily—Sun, JBoss, Oracle, and Sybase, under the leadership of Linda Demichiel."
77,"Over the intervening two decades, many talented people have contributed to the development of Hibernate."
77,"We’re all especially grateful to Steve, who has led the project for many years, since Gavin stepped back to focus in other work."
77,We can think of the API of Hibernate in terms of three basic elements:
77,"an implementation of the JPA-defined APIs, most importantly, of the interfaces EntityManagerFactory and EntityManager, and of the JPA-defined O/R mapping annotations,"
77,"a native API exposing the full set of available functionality, centered around the interfaces SessionFactory, which extends EntityManagerFactory, and Session, which extends EntityManager, and"
77,"a set of mapping annotations which augment the O/R mapping annotations defined by JPA, and which may be used with the JPA-defined interfaces, or with the native API."
77,"Hibernate also offers a range of SPIs for frameworks and libraries which extend or integrate with Hibernate, but we’re not interested in any of that stuff here."
77,"As an application developer, you must decide whether to:"
77,"write your program in terms of Session and SessionFactory, or"
77,"maximize portability to other implementations of JPA by, wherever reasonable, writing code in terms of"
77,"EntityManager and EntityManagerFactory, falling back to the native APIs only where necessary."
77,"Whichever path you take, you will use the JPA-defined mapping annotations most of the time, and the Hibernate-defined annotations for more advanced mapping problems."
77,"You might wonder if it’s possible to develop an application using only JPA-defined APIs, and, indeed, that’s possible in principle."
77,JPA is a great baseline that really nails the basics of the object/relational mapping problem.
77,"But without the native APIs, and extended mapping annotations, you miss out on much of the power of Hibernate."
77,"Since Hibernate existed before JPA, and since JPA was modelled on Hibernate, we unfortunately have some competition and duplication in naming between the standard and native APIs."
77,For example:
77,Table 1. Examples of competing APIs with similar naming
77,Hibernate
77,JPA
77,org.hibernate.annotations.CascadeType
77,javax.persistence.CascadeType
77,org.hibernate.FlushMode
77,javax.persistence.FlushModeType
77,org.hibernate.annotations.FetchMode
77,javax.persistence.FetchType
77,org.hibernate.query.Query
77,javax.persistence.Query
77,org.hibernate.Cache
77,javax.persistence.Cache
77,@org.hibernate.annotations.NamedQuery
77,@javax.persistence.NamedQuery
77,@org.hibernate.annotations.Cache
77,@javax.persistence.Cacheable
77,"Typically, the Hibernate-native APIs offer something a little extra that’s missing in JPA, so this isn’t exactly a flaw."
77,But it’s something to watch out for.
77,1.2. Writing Java code with Hibernate
77,"If you’re completely new to Hibernate and JPA, you might already be wondering how the persistence-related code is structured."
77,"Well, typically, our persistence-related code comes in two layers:"
77,"a representation of our data model in Java, which takes the form of a set of annotated entity classes, and"
77,a larger number of functions which interact with Hibernate’s APIs to perform the persistence operations associated with your various transactions.
77,"The first part, the data or ""domain"" model, is usually easier to write, but doing a great and very clean job of it will strongly affect your success in the second part."
77,"Most people implement the domain model as a set of what we used to call ""Plain Old Java Objects"", that is, as simple Java classes with no direct dependencies on technical infrastructure, nor on application logic which deals with request processing, transaction management, communications, or interaction with the database."
77,"Take your time with this code, and try to produce a Java model that’s as close as reasonable to the relational data model. Avoid using exotic or advanced mapping features when they’re not really needed."
77,"When in the slightest doubt, map a foreign key relationship using @ManyToOne with @OneToMany(mappedBy=…​) in preference to more complicated association mappings."
77,The second part of the code is much trickier to get right. This code must:
77,"manage transactions and sessions,"
77,"interact with the database via the Hibernate session,"
77,"fetch and prepare data needed by the UI, and"
77,handle failures.
77,"Responsibility for transaction and session management, and for recovery from certain kinds of failure, is best handled in some sort of framework code."
77,"We’re going to come back soon to the thorny question of how this persistence logic should be organized, and how it should fit into the rest of the system."
77,"1.3. Hello, Hibernate"
77,"Before we get deeper into the weeds, we’ll quickly present a basic example program that will help you get started if you don’t already have Hibernate integrated into your project."
77,We begin with a simple gradle build file:
77,build.gradle
77,plugins {
77,id 'java'
77,group = 'org.example'
77,version = '1.0-SNAPSHOT'
77,repositories {
77,mavenCentral()
77,dependencies {
77,// the GOAT ORM
77,implementation 'org.hibernate.orm:hibernate-core:6.3.0.Final'
77,// Hibernate Validator
77,implementation 'org.hibernate.validator:hibernate-validator:8.0.0.Final'
77,implementation 'org.glassfish:jakarta.el:4.0.2'
77,// Agroal connection pool
77,implementation 'org.hibernate.orm:hibernate-agroal:6.3.0.Final'
77,implementation 'io.agroal:agroal-pool:2.1'
77,// logging via Log4j
77,implementation 'org.apache.logging.log4j:log4j-core:2.20.0'
77,// JPA Metamodel Generator
77,annotationProcessor 'org.hibernate.orm:hibernate-jpamodelgen:6.3.0.Final'
77,// Compile-time checking for HQL
77,//implementation 'org.hibernate:query-validator:2.0-SNAPSHOT'
77,//annotationProcessor 'org.hibernate:query-validator:2.0-SNAPSHOT'
77,// H2 database
77,runtimeOnly 'com.h2database:h2:2.1.214'
77,Only the first of these dependencies is absolutely required to run Hibernate.
77,"Next, we’ll add a logging configuration file for log4j:"
77,log4j2.properties
77,rootLogger.level = info
77,rootLogger.appenderRefs = console
77,rootLogger.appenderRef.console.ref = console
77,logger.hibernate.name = org.hibernate.SQL
77,logger.hibernate.level = info
77,appender.console.name = console
77,appender.console.type = Console
77,appender.console.layout.type = PatternLayout
77,appender.console.layout.pattern = %highlight{[%p]} %m%n
77,Now we need some Java code.
77,We begin with our entity class:
77,Book.java
77,package org.hibernate.example;
77,import jakarta.persistence.Entity;
77,import jakarta.persistence.Id;
77,import jakarta.validation.constraints.NotNull;
77,@Entity
77,class Book {
77,@Id
77,String isbn;
77,@NotNull
77,String title;
77,Book() {}
77,"Book(String isbn, String title) {"
77,this.isbn = isbn;
77,this.title = title;
77,"Finally, let’s see code which configures and instantiates Hibernate and asks it to persist and query the entity."
77,Don’t worry if this makes no sense at all right now.
77,It’s the job of this Introduction to make all this crystal clear.
77,Main.java
77,package org.hibernate.example;
77,import org.hibernate.cfg.Configuration;
77,import static java.lang.Boolean.TRUE;
77,import static java.lang.System.out;
77,import static org.hibernate.cfg.AvailableSettings.*;
77,public class Main {
77,public static void main(String[] args) {
77,var sessionFactory = new Configuration()
77,.addAnnotatedClass(Book.class)
77,// use H2 in-memory database
77,".setProperty(URL, ""jdbc:h2:mem:db1"")"
77,".setProperty(USER, ""sa"")"
77,".setProperty(PASS, """")"
77,// use Agroal connection pool
77,".setProperty(""hibernate.agroal.maxSize"", ""20"")"
77,// display SQL in console
77,".setProperty(SHOW_SQL, TRUE.toString())"
77,".setProperty(FORMAT_SQL, TRUE.toString())"
77,".setProperty(HIGHLIGHT_SQL, TRUE.toString())"
77,.buildSessionFactory();
77,// export the inferred database schema
77,sessionFactory.getSchemaManager().exportMappedObjects(true);
77,// persist an entity
77,sessionFactory.inTransaction(session -> {
77,"session.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
77,});
77,// query data using HQL
77,sessionFactory.inSession(session -> {
77,"out.println(session.createSelectionQuery(""select isbn||': '||title from Book"").getSingleResult());"
77,});
77,// query data using criteria API
77,sessionFactory.inSession(session -> {
77,var builder = sessionFactory.getCriteriaBuilder();
77,var query = builder.createQuery(String.class);
77,var book = query.from(Book.class);
77,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
77,book.get(Book_.title)));
77,out.println(session.createSelectionQuery(query).getSingleResult());
77,});
77,Here we’ve used Hibernate’s native APIs.
77,We could have used JPA-standard APIs to achieve the same thing.
77,"1.4. Hello, JPA"
77,"If we limit ourselves to the use of JPA-standard APIs, we need to use XML to configure Hibernate."
77,META-INF/persistence.xml
77,"<persistence xmlns=""https://jakarta.ee/xml/ns/persistence"""
77,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
77,"xsi:schemaLocation=""https://jakarta.ee/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
77,"version=""3.0"">"
77,"<persistence-unit name=""example"">"
77,<class>org.hibernate.example.Book</class>
77,<properties>
77,<!-- H2 in-memory database -->
77,"<property name=""jakarta.persistence.jdbc.url"""
77,"value=""jdbc:h2:mem:db1""/>"
77,<!-- Credentials -->
77,"<property name=""jakarta.persistence.jdbc.user"""
77,"value=""sa""/>"
77,"<property name=""jakarta.persistence.jdbc.password"""
77,"value=""""/>"
77,<!-- Agroal connection pool -->
77,"<property name=""hibernate.agroal.maxSize"""
77,"value=""20""/>"
77,<!-- display SQL in console -->
77,"<property name=""hibernate.show_sql"" value=""true""/>"
77,"<property name=""hibernate.format_sql"" value=""true""/>"
77,"<property name=""hibernate.highlight_sql"" value=""true""/>"
77,</properties>
77,</persistence-unit>
77,</persistence>
77,Note that our build.gradle and log4j2.properties files are unchanged.
77,Our entity class is also unchanged from what we had before.
77,"Unfortunately, JPA doesn’t offer an inSession() method, so we’ll have to implement session and transaction management ourselves."
77,"We can put that logic in our own inSession() function, so that we don’t have to repeat it for every transaction."
77,"Again, you don’t need to understand any of this code right now."
77,Main.java (JPA version)
77,package org.hibernate.example;
77,import jakarta.persistence.EntityManager;
77,import jakarta.persistence.EntityManagerFactory;
77,import java.util.Map;
77,import java.util.function.Consumer;
77,import static jakarta.persistence.Persistence.createEntityManagerFactory;
77,import static java.lang.System.out;
77,import static org.hibernate.cfg.AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION;
77,import static org.hibernate.tool.schema.Action.CREATE;
77,public class Main {
77,public static void main(String[] args) {
77,"var factory = createEntityManagerFactory(""example"","
77,// export the inferred database schema
77,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE));"
77,// persist an entity
77,"inSession(factory, entityManager -> {"
77,"entityManager.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
77,});
77,// query data using HQL
77,"inSession(factory, entityManager -> {"
77,"out.println(entityManager.createQuery(""select isbn||': '||title from Book"").getSingleResult());"
77,});
77,// query data using criteria API
77,"inSession(factory, entityManager -> {"
77,var builder = factory.getCriteriaBuilder();
77,var query = builder.createQuery(String.class);
77,var book = query.from(Book.class);
77,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
77,book.get(Book_.title)));
77,out.println(entityManager.createQuery(query).getSingleResult());
77,});
77,"// do some work in a session, performing correct transaction management"
77,"static void inSession(EntityManagerFactory factory, Consumer<EntityManager> work) {"
77,var entityManager = factory.createEntityManager();
77,var transaction = entityManager.getTransaction();
77,try {
77,transaction.begin();
77,work.accept(entityManager);
77,transaction.commit();
77,catch (Exception e) {
77,if (transaction.isActive()) transaction.rollback();
77,throw e;
77,finally {
77,entityManager.close();
77,"In practice, we never access the database directly from a main() method."
77,So now let’s talk about how to organize persistence logic in a real system.
77,The rest of this chapter is not compulsory.
77,"If you’re itching for more details about Hibernate itself, you’re quite welcome to skip straight to the next chapter, and come back later."
77,1.5. Organizing persistence logic
77,"In a real program, persistence logic like the code shown above is usually interleaved with other sorts of code, including logic:"
77,"implementing the rules of the business domain, or"
77,for interacting with the user.
77,"Therefore, many developers quickly—even too quickly, in our opinion—reach for ways to isolate the persistence logic into some sort of separate architectural layer."
77,We’re going to ask you to suppress this urge for now.
77,The easiest way to use Hibernate is to call the Session or EntityManager directly.
77,"If you’re new to Hibernate, frameworks which wrap JPA are only going to make your life more difficult."
77,We prefer a bottom-up approach to organizing our code.
77,"We like to start thinking about methods and functions, not about architectural layers and container-managed objects."
77,"To illustrate the sort of approach to code organization that we advocate, let’s consider a service which queries the database using HQL or SQL."
77,"We might start with something like this, a mix of UI and persistence logic:"
77,"@Path(""/"") @Produces(""application/json"")"
77,public class BookResource {
77,"@GET @Path(""book/{isbn}"")"
77,public Book getBook(String isbn) {
77,"var book = sessionFactory.fromTransaction(session -> session.find(Book.class, isbn));"
77,return book == null ? Response.status(404).build() : book;
77,"Indeed, we might also finish with something like that—it’s quite hard to identify anything concretely wrong with the code above, and for such a simple case it seems really difficult to justify making this code more complicated by introducing additional objects."
77,"One very nice aspect of this code, which we wish to draw your attention to, is that session and transaction management is handled by generic ""framework"" code, just as we already recommended above."
77,"In this case, we’re using the fromTransaction() method, which happens to come built in to Hibernate."
77,"But you might prefer to use something else, for example:"
77,"in a container environment like Jakarta EE or Quarkus, container-managed transactions and container-managed persistence contexts, or"
77,something you write yourself.
77,"The important thing is that calls like createEntityManager() and getTransaction().begin() don’t belong in regular program logic, because it’s tricky and tedious to get the error handling correct."
77,Let’s now consider a slightly more complicated case.
77,"@Path(""/"") @Produces(""application/json"")"
77,public class BookResource {
77,private static final RESULTS_PER_PAGE = 20;
77,"@GET @Path(""books/{titlePattern}/{page:\\d+}"")"
77,"public List<Book> findBooks(String titlePattern, int page) {"
77,var books = sessionFactory.fromTransaction(session -> {
77,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
77,".setParameter(1, titlePattern)"
77,".setPage(Page.page(RESULTS_PER_PAGE, page))"
77,.getResultList();
77,});
77,return books.isEmpty() ? Response.status(404).build() : books;
77,"This is fine, and we won’t complain if you prefer to leave the code exactly as it appears above."
77,But there’s one thing we could perhaps improve.
77,"We love super-short methods with single responsibilities, and there looks to be an opportunity to introduce one here."
77,"Let’s hit the code with our favorite thing, the Extract Method refactoring. We obtain:"
77,"static List<Book> findBooksByTitleWithPagination(Session session,"
77,"String titlePattern, Page page) {"
77,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
77,".setParameter(1, titlePattern)"
77,.setPage(page)
77,.getResultList();
77,"This is an example of a query method, a function which accepts arguments to the parameters of a HQL or SQL query, and executes the query, returning its results to the caller."
77,"And that’s all it does; it doesn’t orchestrate additional program logic, and it doesn’t perform transaction or session management."
77,"It’s even better to specify the query string using the @NamedQuery annotation, so that Hibernate can validate the query it at startup time, that is, when the SessionFactory is created, instead of when the query is first executed."
77,"Indeed, since we included the Metamodel Generator in our Gradle build, the query can even be validated at compile time."
77,"We need a place to put the annotation, so lets move our query method to a new class:"
77,@CheckHQL // validate named queries at compile time
77,"@NamedQuery(name=""findBooksByTitle"","
77,"query=""from Book where title like :title order by title"")"
77,class Queries {
77,"static List<Book> findBooksByTitleWithPagination(Session session,"
77,"String titlePattern, Page page) {"
77,"return session.createNamedQuery(""findBooksByTitle"", Book.class)"
77,".setParameter(""title"", titlePattern)"
77,.setPage(page)
77,.getResultList();
77,Notice that our query method doesn’t attempt to hide the EntityManager from its clients.
77,"Indeed, the client code is responsible for providing the EntityManager or Session to the query method."
77,This is a quite distinctive feature of our whole approach.
77,The client code may:
77,"obtain an EntityManager or Session by calling inTransaction() or fromTransaction(), as we saw above, or,"
77,"in an environment with container-managed transactions, it might obtain it via dependency injection."
77,"Whatever the case, the code which orchestrates a unit of work usually just calls the Session or EntityManager directly, passing it along to helper methods like our query method if necessary."
77,@GET
77,"@Path(""books/{titlePattern}"")"
77,public List<Book> findBooks(String titlePattern) {
77,var books = sessionFactory.fromTransaction(session ->
77,"Queries.findBooksByTitleWithPagination(session, titlePattern,"
77,"Page.page(RESULTS_PER_PAGE, page));"
77,return books.isEmpty() ? Response.status(404).build() : books;
77,You might be thinking that our query method looks a bit boilerplatey.
77,"That’s true, perhaps, but we’re much more concerned that it’s not very typesafe."
77,"Indeed, for many years, the lack of compile-time checking for HQL queries and code which binds arguments to query parameters was our number one source of discomfort with Hibernate."
77,"Fortunately, there’s now a solution to both problems: as an incubating feature of Hibernate 6.3, we now offer the possibility to have the Metamodel Generator fill in the implementation of such query methods for you."
77,"This facility is the topic of a whole chapter of this introduction, so for now we’ll just leave you with one simple example."
77,Suppose we simplify Queries to just the following:
77,interface Queries {
77,"@HQL(""where title like :title order by title"")"
77,"List<Book> findBooksByTitleWithPagination(String title, Page page);"
77,Then the Metamodel Generator automatically produces an implementation of the method annotated @HQL in a class named Queries_.
77,We can call it just like we called our handwritten version:
77,@GET
77,"@Path(""books/{titlePattern}"")"
77,public List<Book> findBooks(String titlePattern) {
77,var books = sessionFactory.fromTransaction(session ->
77,"Queries_.findBooksByTitleWithPagination(session, titlePattern,"
77,"Page.page(RESULTS_PER_PAGE, page));"
77,return books.isEmpty() ? Response.status(404).build() : books;
77,"In this case, the quantity of code eliminated is pretty trivial."
77,The real value is in improved type safety.
77,We now find out about errors in assignments of arguments to query parameters at compile time.
77,"At this point, we’re certain you’re full of doubts about this idea."
77,And quite rightly so.
77,"We would love to answer your objections right here, but that will take us much too far off track."
77,So we ask you to file away these thoughts for now.
77,We promise to make it make sense when we properly address this topic later.
77,"And, after that, if you still don’t like this approach, please understand that it’s completely optional."
77,Nobody’s going to come around to your house to force it down your throat.
77,"Now that we have a rough picture of what our persistence logic might look like, it’s natural to ask how we should test our code."
77,1.6. Testing persistence logic
77,"When we write tests for our persistence logic, we’re going to need:"
77,"a database, with"
77,"an instance of the schema mapped by our persistent entities, and"
77,"a set of test data, in a well-defined state at the beginning of each test."
77,"It might seem obvious that we should test against the same database system that we’re going to use in production, and, indeed, we should certainly have at least some tests for this configuration."
77,"But on the other hand, tests which perform I/O are much slower than tests which don’t, and most databases can’t be set up to run in-process."
77,"So, since most persistence logic written using Hibernate 6 is extremely portable between databases, it often makes good sense to test against an in-memory Java database."
77,(H2 is the one we recommend.)
77,"We do need to be careful here if our persistence code uses native SQL, or if it uses concurrency-management features like pessimistic locks."
77,"Whether we’re testing against our real database, or against an in-memory Java database, we’ll need to export the schema at the beginning of a test suite."
77,"We usually do this when we create the Hibernate SessionFactory or JPA EntityManager, and so traditionally we’ve used a configuration property for this."
77,The JPA-standard property is jakarta.persistence.schema-generation.database.action.
77,"For example, if we’re using Configuration to configure Hibernate, we could write:"
77,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
77,Action.SPEC_ACTION_DROP_AND_CREATE);
77,"Alternatively, in Hibernate 6, we may use the new SchemaManager API to export the schema, just as we did above."
77,sessionFactory.getSchemaManager().exportMappedObjects(true);
77,"Since executing DDL statements is very slow on many databases, we don’t want to do this before every test."
77,"Instead, to ensure that each test begins with the test data in a well-defined state, we need to do two things before each test:"
77,"clean up any mess left behind by the previous test, and then"
77,reinitialize the test data.
77,"We may truncate all the tables, leaving an empty database schema, using the SchemaManager."
77,sessionFactory.getSchemaManager().truncateMappedObjects();
77,"After truncating tables, we might need to initialize our test data."
77,"We may specify test data in a SQL script, for example:"
77,/import.sql
77,"insert into Books (isbn, title) values ('9781932394153', 'Hibernate in Action')"
77,"insert into Books (isbn, title) values ('9781932394887', 'Java Persistence with Hibernate')"
77,"insert into Books (isbn, title) values ('9781617290459', 'Java Persistence with Hibernate, Second Edition')"
77,"If we name this file import.sql, and place it in the root classpath, that’s all we need to do."
77,"Otherwise, we need to specify the file in the configuration property jakarta.persistence.sql-load-script-source."
77,"If we’re using Configuration to configure Hibernate, we could write:"
77,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_LOAD_SCRIPT_SOURCE,"
77,"""/org/example/test-data.sql"");"
77,The SQL script will be executed every time exportMappedObjects() or truncateMappedObjects() is called.
77,There’s another sort of mess a test can leave behind: cached data in the second-level cache.
77,We recommend disabling Hibernate’s second-level cache for most sorts of testing.
77,"Alternatively, if the second-level cache is not disabled, then before each test we should call:"
77,sessionFactory.getCache().evictAllRegions();
77,"Now, suppose you’ve followed our advice, and written your entities and query methods to minimize dependencies on ""infrastructure"", that is, on libraries other than JPA and Hibernate, on frameworks,"
77,"on container-managed objects, and even on bits of your own system which are hard to instantiate from scratch."
77,Then testing persistence logic is now straightforward!
77,You’ll need to:
77,"bootstrap Hibernate and create a SessionFactory or EntityManagerFactory and the beginning of your test suite (we’ve already seen how to do that), and"
77,"create a new Session or EntityManager inside each @Test method, using inTransaction(), for example."
77,"Actually, some tests might require multiple sessions."
77,But be careful not to leak a session between different tests.
77,Another important test we’ll need is one which validates our O/R mapping annotations against the actual database schema.
77,"This is again the job of the schema management tooling, either:"
77,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
77,Action.ACTION_VALIDATE);
77,Or:
77,sessionFactory.getSchemaManager().validateMappedObjects();
77,"This ""test"" is one which many people like to run even in production, when the system starts up."
77,1.7. Architecture and the persistence layer
77,"Let’s now consider a different approach to code organization, one we treat with suspicion."
77,"In this section, we’re going to give you our opinion."
77,"If you’re only interested in facts, or if you prefer not to read things that might undermine the opinion you currently hold, please feel free to skip straight to the next chapter."
77,"Hibernate is an architecture-agnostic library, not a framework, and therefore integrates comfortably with a wide range of Java frameworks and containers."
77,"Consistent with our place within the ecosystem, we’ve historically avoided giving out much advice on architecture."
77,"This is a practice we’re now perhaps inclined to regret, since the resulting vacuum has come to be filled with advice from people advocating architectures, design patterns, and extra frameworks which we suspect make Hibernate a bit less pleasant to use than it should be."
77,"In particular, frameworks which wrap JPA seem to add bloat while subtracting some of the fine-grained control over data access that Hibernate works so hard to provide."
77,"These frameworks don’t expose the full feature set of Hibernate, and so the program is forced to work with a less powerful abstraction."
77,"The stodgy, dogmatic, conventional wisdom, which we hesitate to challenge for simple fear of pricking ourselves on the erect hackles that inevitably accompany such dogma-baiting is:"
77,Code which interacts with the database belongs in a separate persistence layer.
77,We lack the courage—perhaps even the conviction—to tell you categorically to not follow this recommendation.
77,"But we do ask you to consider the cost in boilerplate of any architectural layer, and whether the benefits this cost buys are really worth it in the context of your system."
77,"To add a little background texture to this discussion, and at the risk of our Introduction degenerating into a rant at such an early stage, we’re going ask you to humor us while talk a little more about ancient history."
77,An epic tale of DAOs and Repositories
77,"Back in the dark days of Java EE 4, before the standardization of Hibernate, and subsequent ascendance of JPA in Java enterprise development, it was common to hand-code the messy JDBC interactions that Hibernate takes care of today."
77,"In those terrible times, a pattern arose that we used to call Data Access Objects (DAOs)."
77,"A DAO gave you a place to put all that nasty JDBC code, leaving the important program logic cleaner."
77,"When Hibernate arrived suddenly on the scene in 2001, developers loved it."
77,"But Hibernate implemented no specification, and many wished to reduce or at least localize the dependence of their project logic on Hibernate."
77,"An obvious solution was to keep the DAOs around, but to replace the JDBC code inside them with calls to the Hibernate Session."
77,We partly blame ourselves for what happened next.
77,Back in 2002 and 2003 this really seemed like a pretty reasonable thing to do.
77,"In fact, we contributed to the popularity of this approach by recommending—or at least not discouraging—the use of DAOs in Hibernate in Action."
77,"We hereby apologize for this mistake, and for taking much too long to recognize it."
77,"Eventually, some folks came to believe that their DAOs shielded their program from depending in a hard way on ORM, allowing them to ""swap out"" Hibernate, and replace it with JDBC, or with something else."
77,"In fact, this was never really true—there’s quite a deep difference between the programming model of JDBC, where every interaction with the database is explicit and synchronous, and the programming model of stateful sessions in Hibernate, where updates are implicit, and SQL statements are executed asynchronously."
77,"But then the whole landscape for persistence in Java changed in April 2006, when the final draft of JPA 1.0 was approved."
77,"Java now had a standard way to do ORM, with multiple high-quality implementations of the standard API."
77,"This was the end of the line for the DAOs, right?"
77,"Well, no."
77,It wasn’t.
77,"DAOs were rebranded ""repositories"", and continue to enjoy a sort-of zombie afterlife as a front-end to JPA."
77,"But are they really pulling their weight, or are they just unnecessary extra complexity and bloat? An extra layer of indirection that makes stack traces harder to read and code harder to debug?"
77,Our considered view is that they’re mostly just bloat.
77,"The JPA EntityManager is a ""repository"", and it’s a standard repository with a well-defined specification written by people who spend all day thinking about persistence."
77,"If these repository frameworks offered anything actually useful—and not obviously foot-shooty—over and above what EntityManager provides, we would have already added it to EntityManager decades ago."
77,"Ultimately, we’re not sure you need a separate persistence layer at all."
77,At least consider the possibility that it might be OK to call the EntityManager directly from your business logic.
77,We can already hear you hissing at our heresy.
77,"But before slamming shut the lid of your laptop and heading off to fetch garlic and a pitchfork, take a couple of hours to weigh what we’re proposing."
77,"OK, so, look, if it makes you feel better, one way to view EntityManager is to think of it as a single generic ""repository"" that works for every entity in your system."
77,"From this point of view, JPA is your persistence layer."
77,And there’s few good reasons to wrap this abstraction in a second abstraction that’s less generic.
77,"Even where a distinct persistence layer is appropriate, DAO-style repositories aren’t the unambiguously most-correct way to factorize the equation:"
77,"most nontrivial queries touch multiple entities, and so it’s often quite ambiguous which repository such a query belongs to,"
77,"most queries are extremely specific to a particular fragment of program logic, and aren’t reused in different places across the system, and"
77,the various operations of a repository rarely interact or share common internal implementation details.
77,"Indeed, repositories, by nature, exhibit very low cohesion."
77,"A layer of repository objects might make sense if you have multiple implementations of each repository, but in practice almost nobody ever does."
77,"That’s because they’re also extremely highly coupled to their clients, with a very large API surface."
77,"And, on the contrary, a layer is only easily replaceable if it has a very narrow API."
77,"Some people do indeed use mock repositories for testing, but we really struggle to see any value in this."
77,"If we don’t want to run our tests against our real database, it’s usually very easy to ""mock"" the database itself by running tests against an in-memory Java database like H2."
77,"This works even better in Hibernate 6 than in older versions of Hibernate, since HQL is now much more portable between platforms."
77,"Phew, let’s move on."
77,1.8. Overview
77,It’s now time to begin our journey toward actually understanding the code we saw earlier.
77,This introduction will guide you through the basic tasks involved in developing a program that uses Hibernate for persistence:
77,"configuring and bootstrapping Hibernate, and obtaining an instance of SessionFactory or EntityManagerFactory,"
77,"writing a domain model, that is, a set of entity classes which represent the persistent types in your program, and which map to tables of your database,"
77,"customizing these mappings when the model maps to a pre-existing relational schema,"
77,"using the Session or EntityManager to perform operations which query the database and return entity instances, or which update the data held in the database,"
77,"using the Hibernate Metamodel Generator to improve compile-time type-safety,"
77,"writing complex queries using the Hibernate Query Language (HQL) or native SQL, and, finally"
77,tuning performance of the data access logic.
77,"Naturally, we’ll start at the top of this list, with the least-interesting topic: configuration."
77,2. Configuration and bootstrap
77,We would love to make this section short.
77,"Unfortunately, there’s several distinct ways to configure and bootstrap Hibernate, and we’re going to have to describe at least two of them in detail."
77,The four basic ways to obtain an instance of Hibernate are shown in the following table:
77,"Using the standard JPA-defined XML, and the operation Persistence.createEntityManagerFactory()"
77,Usually chosen when portability between JPA implementations is important.
77,Using the Configuration class to construct a SessionFactory
77,"When portability between JPA implementations is not important, this option is quicker, adds some flexibility and saves a typecast."
77,Using the more complex APIs defined in org.hibernate.boot
77,"Used primarily by framework integrators, this option is outside the scope of this document."
77,By letting the container take care of the bootstrap process and of injecting the SessionFactory or EntityManagerFactory
77,Used in a container environment like WildFly or Quarkus.
77,Here we’ll focus on the first two options.
77,Hibernate in containers
77,"Actually, the last option is extremely popular, since every major Java application server and microservice framework comes with built-in support for Hibernate."
77,Such container environments typically also feature facilities to automatically manage the lifecycle of an EntityManager or Session and its association with container-managed transactions.
77,"To learn how to configure Hibernate in such a container environment, you’ll need to refer to the documentation of your chosen container."
77,"For Quarkus, here’s the relevant documentation."
77,"If you’re using Hibernate outside of a container environment,"
77,you’ll need to:
77,"include Hibernate ORM itself, along with the appropriate JDBC driver, as dependencies of your project, and"
77,"configure Hibernate with information about your database,"
77,by specifying configuration properties.
77,2.1. Including Hibernate in your project build
77,"First, add the following dependency to your project:"
77,org.hibernate.orm:hibernate-core:{version}
77,Where {version} is the version of Hibernate you’re using.
77,You’ll also need to add a dependency for the JDBC
77,driver for your database.
77,Table 2. JDBC driver dependencies
77,Database
77,Driver dependency
77,PostgreSQL or CockroachDB
77,org.postgresql:postgresql:{version}
77,MySQL or TiDB
77,com.mysql:mysql-connector-j:{version}
77,MariaDB
77,org.mariadb.jdbc:mariadb-java-client:{version}
77,DB2
77,com.ibm.db2:jcc:{version}
77,SQL Server
77,com.microsoft.sqlserver:mssql-jdbc:${version}
77,Oracle
77,com.oracle.database.jdbc:ojdbc11:${version}
77,com.h2database:h2:{version}
77,HSQLDB
77,org.hsqldb:hsqldb:{version}
77,Where {version} is the latest version of the JDBC driver for your databse.
77,2.2. Optional dependencies
77,"Optionally, you might also add any of the following additional features:"
77,Table 3. Optional dependencies
77,Optional feature
77,Dependencies
77,An SLF4J logging implementation
77,org.apache.logging.log4j:log4j-core
77,or org.slf4j:slf4j-jdk14
77,"A JDBC connection pool, for example, Agroal"
77,org.hibernate.orm:hibernate-agroal
77,and io.agroal:agroal-pool
77,"The Hibernate Metamodel Generator, especially if you’re using the JPA criteria query API"
77,org.hibernate.orm:hibernate-jpamodelgen
77,"The Query Validator, for compile-time checking of HQL"
77,org.hibernate:query-validator
77,"Hibernate Validator, an implementation of Bean Validation"
77,org.hibernate.validator:hibernate-validator
77,and org.glassfish:jakarta.el
77,Local second-level cache support via JCache and EHCache
77,org.hibernate.orm:hibernate-jcache
77,and org.ehcache:ehcache
77,Local second-level cache support via JCache and Caffeine
77,org.hibernate.orm:hibernate-jcache
77,and com.github.ben-manes.caffeine:jcache
77,Distributed second-level cache support via Infinispan
77,org.infinispan:infinispan-hibernate-cache-v60
77,"A JSON serialization library for working with JSON datatypes, for example, Jackson or Yasson"
77,com.fasterxml.jackson.core:jackson-databind
77,or org.eclipse:yasson
77,Hibernate Spatial
77,org.hibernate.orm:hibernate-spatial
77,"Envers, for auditing historical data"
77,org.hibernate.orm:hibernate-envers
77,You might also add the Hibernate bytecode enhancer to your
77,Gradle build if you want to use field-level lazy fetching.
77,2.3. Configuration using JPA XML
77,"Sticking to the JPA-standard approach, we would provide a file named persistence.xml, which we usually place in the META-INF directory of a persistence archive, that is, of the .jar file or directory which contains our entity classes."
77,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
77,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
77,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
77,"version=""2.0"">"
77,"<persistence-unit name=""org.hibernate.example"">"
77,<class>org.hibernate.example.Book</class>
77,<class>org.hibernate.example.Author</class>
77,<properties>
77,<!-- PostgreSQL -->
77,"<property name=""jakarta.persistence.jdbc.url"""
77,"value=""jdbc:postgresql://localhost/example""/>"
77,<!-- Credentials -->
77,"<property name=""jakarta.persistence.jdbc.user"""
77,"value=""gavin""/>"
77,"<property name=""jakarta.persistence.jdbc.password"""
77,"value=""hibernate""/>"
77,<!-- Automatic schema export -->
77,"<property name=""jakarta.persistence.schema-generation.database.action"""
77,"value=""drop-and-create""/>"
77,<!-- SQL statement logging -->
77,"<property name=""hibernate.show_sql"" value=""true""/>"
77,"<property name=""hibernate.format_sql"" value=""true""/>"
77,"<property name=""hibernate.highlight_sql"" value=""true""/>"
77,</properties>
77,</persistence-unit>
77,</persistence>
77,"The <persistence-unit> element defines a named persistence unit, that is:"
77,"a collection of associated entity types, along with"
77,"a set of default configuration settings, which may be augmented or overridden at runtime."
77,Each <class> element specifies the fully-qualified name of an entity class.
77,Scanning for entity classes
77,"In some container environments, for example, in any EE container, the <class> elements are unnecessary, since the container will scan the archive for annotated classes, and automatically recognize any class annotated @Entity."
77,Each <property> element specifies a configuration property and its value.
77,Note that:
77,"the configuration properties in the jakarta.persistence namespace are standard properties defined by the JPA spec, and"
77,properties in the hibernate namespace are specific to Hibernate.
77,We may obtain an EntityManagerFactory by calling Persistence.createEntityManagerFactory():
77,EntityManagerFactory entityManagerFactory =
77,"Persistence.createEntityManagerFactory(""org.hibernate.example"");"
77,"If necessary, we may override configuration properties specified in persistence.xml:"
77,EntityManagerFactory entityManagerFactory =
77,"Persistence.createEntityManagerFactory(""org.hibernate.example"","
77,"Map.of(AvailableSettings.JAKARTA_JDBC_PASSWORD, password));"
77,2.4. Configuration using Hibernate API
77,"Alternatively, the venerable class Configuration allows an instance of Hibernate to be configured in Java code."
77,SessionFactory sessionFactory =
77,new Configuration()
77,.addAnnotatedClass(Book.class)
77,.addAnnotatedClass(Author.class)
77,// PostgreSQL
77,".setProperty(AvailableSettings.JAKARTA_JDBC_URL, ""jdbc:postgresql://localhost/example"")"
77,// Credentials
77,".setProperty(AvailableSettings.JAKARTA_JDBC_USER, user)"
77,".setProperty(AvailableSettings.JAKARTA_JDBC_PASSWORD, password)"
77,// Automatic schema export
77,".setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
77,Action.SPEC_ACTION_DROP_AND_CREATE)
77,// SQL statement logging
77,".setProperty(AvailableSettings.SHOW_SQL, TRUE.toString())"
77,".setProperty(AvailableSettings.FORMAT_SQL, TRUE.toString())"
77,".setProperty(AvailableSettings.HIGHLIGHT_SQL, TRUE.toString())"
77,// Create a new SessionFactory
77,.buildSessionFactory();
77,"The Configuration class has survived almost unchanged since the very earliest (pre-1.0) versions of Hibernate, and so it doesn’t look particularly modern."
77,"On the other hand, it’s very easy to use, and exposes some options that persistence.xml doesn’t support."
77,Advanced configuration options
77,"Actually, the Configuration class is just a very simple facade for the more modern, much more powerful—but more complex—API defined in the package org.hibernate.boot."
77,"This API is useful if you have very advanced requirements, for example, if you’re writing a framework or implementing a container."
77,"You’ll find more information in the User Guide, and in the package-level documentation of org.hibernate.boot."
77,2.5. Configuration using Hibernate properties file
77,"If we’re using the Hibernate Configuration API, but we don’t want to put certain configuration properties directly in the Java code, we can specify them in a file named hibernate.properties, and place the file in the root classpath."
77,# PostgreSQL
77,jakarta.persistence.jdbc.url=jdbc:postgresql://localhost/example
77,# Credentials
77,jakarta.persistence.jdbc.user=hibernate
77,jakarta.persistence.jdbc.password=zAh7mY$2MNshzAQ5
77,# SQL statement logging
77,hibernate.show_sql=true
77,hibernate.format_sql=true
77,hibernate.highlight_sql=true
77,2.6. Basic configuration settings
77,The class AvailableSettings enumerates all the configuration properties understood by Hibernate.
77,"Of course, we’re not going to cover every useful configuration setting in this chapter."
77,"Instead, we’ll mention the ones you need to get started, and come back to some other important settings later, especially when we talk about performance tuning."
77,Hibernate has many—too many—switches and toggles.
77,"Please don’t go crazy messing about with these settings; most of them are rarely needed, and many only exist to provide backward compatibility with older versions of Hibernate."
77,"With rare exception, the default behavior of every one of these settings was carefully chosen to be the behavior we recommend."
77,The properties you really do need to get started are these three:
77,Table 4. JDBC connection settings
77,Configuration property name
77,Purpose
77,jakarta.persistence.jdbc.url
77,JDBC URL of your database
77,jakarta.persistence.jdbc.user and jakarta.persistence.jdbc.password
77,Your database credentials
77,"In Hibernate 6, you don’t need to specify hibernate.dialect."
77,The correct Hibernate SQL Dialect will be determined for you automatically.
77,The only reason to specify this property is if you’re using a custom user-written Dialect class.
77,"Similarly, neither hibernate.connection.driver_class nor jakarta.persistence.jdbc.driver is needed when working with one of the supported databases."
77,Pooling JDBC connections is an extremely important performance optimization.
77,You can set the size of Hibernate’s built-in connection pool using this property:
77,Table 5. Built-in connection pool size
77,Configuration property name
77,Purpose
77,hibernate.connection.pool_size
77,The size of the built-in connection pool
77,"By default, Hibernate uses a simplistic built-in connection pool."
77,"This pool is not meant for use in production, and later, when we discuss performance, we’ll see how to select a more robust implementation."
77,"Alternatively, in a container environment, you’ll need at least one of these properties:"
77,Table 6. Transaction management settings
77,Configuration property name
77,Purpose
77,jakarta.persistence.transactionType
77,"(Optional, defaults to JTA)"
77,Determines if transaction management is via JTA or resource-local transactions.
77,Specify RESOURCE_LOCAL if JTA should not be used.
77,jakarta.persistence.jtaDataSource
77,JNDI name of a JTA datasource
77,jakarta.persistence.nonJtaDataSource
77,JNDI name of a non-JTA datasource
77,"In this case, Hibernate obtains pooled JDBC database connections from a container-managed DataSource."
77,2.7. Automatic schema export
77,You can have Hibernate infer your database schema from the mapping
77,"annotations you’ve specified in your Java code, and export the schema at"
77,initialization time by specifying one or more of the following configuration
77,properties:
77,Table 7. Schema management settings
77,Configuration property name
77,Purpose
77,jakarta.persistence.schema-generation.database.action
77,"If drop-and-create, first drop the schema and then export tables, sequences, and constraints"
77,"If create, export tables, sequences, and constraints, without attempting to drop them first"
77,"If create-drop, drop the schema and recreate it on SessionFactory startup"
77,"Additionally, drop the schema on SessionFactory shutdown"
77,"If drop, drop the schema on SessionFactory shutdown"
77,"If validate, validate the database schema without changing it"
77,"If update, only export what’s missing in the schema"
77,jakarta.persistence.create-database-schemas
77,"(Optional) If true, automatically create schemas and catalogs"
77,jakarta.persistence.schema-generation.create-source
77,"(Optional) If metadata-then-script or script-then-metadata, execute an additional SQL script when exported tables and sequences"
77,jakarta.persistence.schema-generation.create-script-source
77,(Optional) The name of a SQL DDL script to be executed
77,jakarta.persistence.sql-load-script-source
77,(Optional) The name of a SQL DML script to be executed
77,This feature is extremely useful for testing.
77,"The easiest way to pre-initialize a database with test or ""reference"" data is to place a list of SQL insert statements in a file named, for example, import.sql, and specify the path to this file using the property jakarta.persistence.sql-load-script-source."
77,"We’ve already seen an example of this approach, which is cleaner than writing Java code to instantiate entity instances and calling persist() on each of them."
77,"As we mentioned earlier, it can also be useful to control schema export programmatically."
77,The SchemaManager API allows programmatic control over schema export:
77,sessionFactory.getSchemaManager().exportMappedObjects(true);
77,JPA has a more limited and less ergonomic API:
77,"Persistence.generateSchema(""org.hibernate.example"","
77,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE))"
77,2.8. Logging the generated SQL
77,"To see the generated SQL as it’s sent to the database, you have two options."
77,"One way is to set the property hibernate.show_sql to true, and Hibernate will log SQL direct to the console."
77,You can make the output much more readable by enabling formatting or highlighting.
77,These settings really help when troubleshooting the generated SQL statements.
77,Table 8. Settings for SQL logging to the console
77,Configuration property name
77,Purpose
77,hibernate.show_sql
77,"If true, log SQL directly to the console"
77,hibernate.format_sql
77,"If true, log SQL in a multiline, indented format"
77,hibernate.highlight_sql
77,"If true, log SQL with syntax highlighting via ANSI escape codes"
77,"Alternatively, you can enable debug-level logging for the category org.hibernate.SQL using your preferred SLF4J logging implementation."
77,"For example, if you’re using Log4J 2 (as above in Optional dependencies), add these lines to your log4j2.properties file:"
77,# SQL execution
77,logger.hibernate.name = org.hibernate.SQL
77,logger.hibernate.level = debug
77,# JDBC parameter binding
77,logger.jdbc-bind.name=org.hibernate.orm.jdbc.bind
77,logger.jdbc-bind.level=trace
77,# JDBC result set extraction
77,logger.jdbc-extract.name=org.hibernate.orm.jdbc.extract
77,logger.jdbc-extract.level=trace
77,But with this approach we miss out on the pretty highlighting.
77,2.9. Minimizing repetitive mapping information
77,"The following properties are very useful for minimizing the amount of information you’ll need to explicitly specify in @Table and @Column annotations, which we’ll discuss below in Object/relational mapping:"
77,Table 9. Settings for minimizing explicit mapping information
77,Configuration property name
77,Purpose
77,hibernate.default_schema
77,A default schema name for entities which do not explicitly declare one
77,hibernate.default_catalog
77,A default catalog name for entities which do not explicitly declare one
77,hibernate.physical_naming_strategy
77,A PhysicalNamingStrategy implementing your database naming standards
77,hibernate.implicit_naming_strategy
77,"An ImplicitNamingStrategy which specifies how ""logical"" names of relational objects should be inferred when no name is specified in annotations"
77,"Writing your own PhysicalNamingStrategy and/or ImplicitNamingStrategy is an especially good way to reduce the clutter of annotations on your entity classes, and to implement your database naming conventions, and so we think you should do it for any nontrivial data model."
77,We’ll have more to say about them in Naming strategies.
77,2.10. Nationalized character data in SQL Server
77,"By default, SQL Server’s char and varchar types don’t accommodate Unicode data."
77,But a Java string may contain any Unicode character.
77,"So, if you’re working with SQL Server, you might need to force Hibernate to use the nchar and nvarchar column types."
77,Table 10. Setting the use of nationalized character data
77,Configuration property name
77,Purpose
77,hibernate.use_nationalized_character_data
77,Use nchar and nvarchar instead of char and varchar
77,"On the other hand, if only some columns store nationalized data, use the @Nationalized annotation to indicate fields of your entities which map these columns."
77,"Alternatively, you can configure SQL Server to use the UTF-8 enabled collation _UTF8."
77,3. Entities
77,An entity is a Java class which represents data in a relational database table.
77,We say that the entity maps or maps to the table.
77,"Much less commonly, an entity might aggregate data from multiple tables, but we’ll get to that later."
77,An entity has attributes—properties or fields—which map to columns of the table.
77,"In particular, every entity must have an identifier or id, which maps to the primary key of the table."
77,"The id allows us to uniquely associate a row of the table with an instance of the Java class, at least within a given persistence context."
77,"We’ll explore the idea of a persistence context later. For now, think of it as a one-to-one mapping between ids and entity instances."
77,An instance of a Java class cannot outlive the virtual machine to which it belongs.
77,But we may think of an entity instance having a lifecycle which transcends a particular instantiation in memory.
77,"By providing its id to Hibernate, we may re-materialize the instance in a new persistence context, as long as the associated row is present in the database."
77,"Therefore, the operations persist() and remove() may be thought of as demarcating the beginning and end of the lifecycle of an entity, at least with respect to persistence."
77,"Thus, an id represents the persistent identity of an entity, an identity that outlives a particular instantiation in memory."
77,"And this is an important difference between entity class itself and the values of its attributes—the entity has a persistent identity, and a well-defined lifecycle with respect to persistence, whereas a String or List representing one of its attribute values doesn’t."
77,An entity usually has associations to other entities.
77,"Typically, an association between two entities maps to a foreign key in one of the database tables."
77,"A group of mutually associated entities is often called a domain model, though data model is also a perfectly good term."
77,3.1. Entity classes
77,An entity must:
77,"be a non-final class,"
77,with a non-private constructor with no parameters.
77,"On the other hand, the entity class may be either concrete or abstract, and it may have any number of additional constructors."
77,An entity class may be a static inner class.
77,Every entity class must be annotated @Entity.
77,@Entity
77,class Book {
77,Book() {}
77,...
77,"Alternatively, the class may be identified as an entity type by providing an XML-based mapping for the class."
77,Mapping entities using XML
77,"When XML-based mappings are used, the <entity> element is used to declare an entity class:"
77,<entity-mappings>
77,<package>org.hibernate.example</package>
77,"<entity class=""Book"">"
77,<attributes> ... </attributes>
77,</entity>
77,...
77,</entity-mappings>
77,"Since the orm.xml mapping file format defined by the JPA specification was modelled closely on the annotation-based mappings, it’s usually easy to go back and forth between the two options."
77,"We won’t have much more to say about XML-based mappings in this Introduction, since it’s not our preferred way to do things."
77,"""Dynamic"" models"
77,We love representing entities as classes because the classes give us a type-safe model of our data.
77,But Hibernate also has the ability to represent entities as detyped instances of java.util.Map.
77,"There’s information in the User Guide, if you’re curious."
77,This must sound like a weird feature for a project that places importance on type-safety.
77,"Actually, it’s a useful capability for a very particular sort of generic code."
77,"For example, Hibernate Envers is a great auditing/versioning system for Hibernate entities."
77,Envers makes use of maps to represent its versioned model of the data.
77,3.2. Access types
77,"Each entity class has a default access type, either:"
77,"direct field access, or"
77,property access.
77,Hibernate automatically determines the access type from the location of attribute-level annotations.
77,Concretely:
77,"if a field is annotated @Id, field access is used, or"
77,"if a getter method is annotated @Id, property access is used."
77,"Back when Hibernate was just a baby, property access was quite popular in the Hibernate community."
77,"Today, however, field access is much more common."
77,"The default access type may be specified explicitly using the @Access annotation, but we strongly discourage this, since it’s ugly and never necessary."
77,Mapping annotations should be placed consistently:
77,"if @Id annotates a field, the other mapping annotations should also be applied to fields, or,"
77,"if @Id annotates a getter, the other mapping annotations should be applied to getters."
77,It is in principle possible to mix field and property access using explicit @Access annotations at the attribute level.
77,We don’t recommend doing this.
77,"An entity class like Book, which does not extend any other entity class, is called a root entity."
77,Every root entity must declare an identifier attribute.
77,3.3. Entity class inheritance
77,An entity class may extend another entity class.
77,@Entity
77,class AudioBook extends Book {
77,AudioBook() {}
77,...
77,A subclass entity inherits every persistent attribute of every entity it extends.
77,A root entity may also extend another class and inherit mapped attributes from the other class.
77,"But in this case, the class which declares the mapped attributes must be annotated @MappedSuperclass."
77,@MappedSuperclass
77,class Versioned {
77,...
77,@Entity
77,class Book extends Versioned {
77,...
77,"A root entity class must declare an attribute annotated @Id, or inherit one from a @MappedSuperclass."
77,A subclass entity always inherits the identifier attribute of the root entity.
77,It may not declare its own @Id attribute.
77,3.4. Identifier attributes
77,An identifier attribute is usually a field:
77,@Entity
77,class Book {
77,Book() {}
77,@Id
77,Long id;
77,...
77,But it may be a property:
77,@Entity
77,class Book {
77,Book() {}
77,private Long id;
77,@Id
77,Long getId() { return id; }
77,void setId(Long id) { this.id = id; }
77,...
77,An identifier attribute must be annotated @Id or @EmbeddedId.
77,Identifier values may be:
77,"assigned by the application, that is, by your Java code, or"
77,generated and assigned by Hibernate.
77,We’ll discuss the second option first.
77,3.5. Generated identifiers
77,"An identifier is often system-generated, in which case it should be annotated @GeneratedValue:"
77,@Id @GeneratedValue
77,Long id;
77,"System-generated identifiers, or surrogate keys make it easier to evolve or refactor the relational data model."
77,"If you have the freedom to define the relational schema, we recommend the use of surrogate keys."
77,"On the other hand, if, as is more common, you’re working with a pre-existing database schema, you might not have the option."
77,"JPA defines the following strategies for generating ids, which are enumerated by GenerationType:"
77,Table 11. Standard id generation strategies
77,Strategy
77,Java type
77,Implementation
77,GenerationType.UUID
77,UUID or String
77,A Java UUID
77,GenerationType.IDENTITY
77,Long or Integer
77,An identity or autoincrement column
77,GenerationType.SEQUENCE
77,Long or Integer
77,A database sequence
77,GenerationType.TABLE
77,Long or Integer
77,A database table
77,GenerationType.AUTO
77,Long or Integer
77,"Selects SEQUENCE, TABLE, or UUID based on the identifier type and capabilities of the database"
77,"For example, this UUID is generated in Java code:"
77,@Id @GeneratedValue UUID id;
77,// AUTO strategy selects UUID based on the field type
77,"This id maps to a SQL identity, auto_increment, or bigserial column:"
77,@Id @GeneratedValue(strategy=IDENTITY) Long id;
77,The @SequenceGenerator and @TableGenerator annotations allow further control over SEQUENCE and TABLE generation respectively.
77,Consider this sequence generator:
77,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
77,Values are generated using a database sequence defined as follows:
77,create sequence seq_book start with 5 increment by 10
77,Notice that Hibernate doesn’t have to go to the database every time a new identifier is needed.
77,"Instead, a given process obtains a block of ids, of size allocationSize, and only needs to hit the database each time the block is exhausted."
77,"Of course, the downside is that generated identifiers are not contiguous."
77,"If you let Hibernate export your database schema, the sequence definition will have the right start with and increment values."
77,"But if you’re working with a database schema managed outside Hibernate, make sure the initialValue and allocationSize members of @SequenceGenerator match the start with and increment specified in the DDL."
77,Any identifier attribute may now make use of the generator named bookSeq:
77,@Id
77,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
77,// reference to generator defined elsewhere
77,Long id;
77,"Actually, it’s extremely common to place the @SequenceGenerator annotation on the @Id attribute that makes use of it:"
77,@Id
77,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
77,// reference to generator defined below
77,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
77,Long id;
77,JPA id generators may be shared between entities.
77,"A @SequenceGenerator or @TableGenerator must have a name, and may be shared between multiple id attributes."
77,This fits somewhat uncomfortably with the common practice of annotating the @Id attribute which makes use of the generator!
77,"As you can see, JPA provides quite adequate support for the most common strategies for system-generated ids."
77,"However, the annotations themselves are a bit more intrusive than they should be, and there’s no well-defined way to extend this framework to support custom strategies for id generation."
77,Nor may @GeneratedValue be used on a property not annotated @Id.
77,"Since custom id generation is a rather common requirement, Hibernate provides a very carefully-designed framework for user-defined Generators, which we’ll discuss in User-defined generators."
77,3.6. Natural keys as identifiers
77,Not every identifier attribute maps to a (system-generated) surrogate key.
77,Primary keys which are meaningful to the user of the system are called natural keys.
77,"When the primary key of a table is a natural key, we don’t annotate the identifier attribute @GeneratedValue, and it’s the responsibility of the application code to assign a value to the identifier attribute."
77,@Entity
77,class Book {
77,@Id
77,String isbn;
77,...
77,"Of particular interest are natural keys which comprise more than one database column, and such natural keys are called composite keys."
77,3.7. Composite identifiers
77,"If your database uses composite keys, you’ll need more than one identifier attribute."
77,There are two ways to map composite keys in JPA:
77,"using an @IdClass, or"
77,using an @EmbeddedId.
77,"Perhaps the most immediately-natural way to represent this in an entity class is with multiple fields annotated @Id, for example:"
77,@Entity
77,@IdClass(BookId.class)
77,class Book {
77,Book() {}
77,@Id
77,String isbn;
77,@Id
77,int printing;
77,...
77,But this approach comes with a problem: what object can we use to identify a Book and pass to methods like find() which accept an identifier?
77,The solution is to write a separate class with fields that match the identifier attributes of the entity.
77,The @IdClass annotation of the Book entity identifies the id class to use for that entity:
77,class BookId {
77,String isbn;
77,int printing;
77,BookId() {}
77,"BookId(String isbn, int printing) {"
77,this.isbn = isbn;
77,this.printing = printing;
77,@Override
77,public boolean equals(Object other) {
77,if (other instanceof BookId) {
77,BookId bookId = (BookId) other;
77,return bookId.isbn.equals(isbn)
77,&& bookId.printing == printing;
77,else {
77,return false;
77,@Override
77,public int hashCode() {
77,return isbn.hashCode();
77,Every id class should override equals() and hashCode().
77,This is not our preferred approach.
77,"Instead, we recommend that the BookId class be declared as an @Embeddable type:"
77,@Embeddable
77,class BookId {
77,String isbn;
77,int printing;
77,BookId() {}
77,"BookId(String isbn, int printing) {"
77,this.isbn = isbn;
77,this.printing = printing;
77,...
77,We’ll learn more about Embeddable objects below.
77,"Now the entity class may reuse this definition using @EmbeddedId, and the @IdClass annotation is no longer required:"
77,@Entity
77,class Book {
77,Book() {}
77,@EmbeddedId
77,BookId bookId;
77,...
77,This second approach eliminates some duplicated code.
77,"Either way, we may now use BookId to obtain instances of Book:"
77,"Book book = session.find(Book.class, new BookId(isbn, printing));"
77,3.8. Version attributes
77,An entity may have an attribute which is used by Hibernate for optimistic lock checking.
77,"A version attribute is usually of type Integer, Short, Long, LocalDateTime, OffsetDateTime, ZonedDateTime, or Instant."
77,@Version
77,LocalDateTime lastUpdated;
77,"Version attributes are automatically assigned by Hibernate when an entity is made persistent, and automatically incremented or updated each time the entity is updated."
77,"If an entity doesn’t have a version number, which often happens when mapping legacy data, we can still do optimistic locking."
77,"The @OptimisticLocking annotation lets us specify that optimistic locks should be checked by validating the values of ALL fields, or only the DIRTY fields of the entity."
77,And the @OptimisticLock annotation lets us selectively exclude certain fields from optimistic locking.
77,The @Id and @Version attributes we’ve already seen are just specialized examples of basic attributes.
77,3.9. Natural id attributes
77,"Even when an entity has a surrogate key, it should always be possible to write down a combination of fields which uniquely identifies an instance of the entity, from the point of view of the user of the system."
77,This combination of fields is its natural key.
77,"Above, we considered the case where the natural key coincides with the primary key."
77,"Here, the natural key is a second unique key of the entity, distinct from its surrogate primary key."
77,"If you can’t identify a natural key, it might be a sign that you need to think more carefully about some aspect of your data model."
77,"If an entity doesn’t have a meaningful unique key, then it’s impossible to say what event or object it represents in the ""real world"" outside your program."
77,"Since it’s extremely common to retrieve an entity based on its natural key, Hibernate has a way to mark the attributes of the entity which make up its natural key."
77,Each attribute must be annotated @NaturalId.
77,@Entity
77,class Book {
77,Book() {}
77,@Id @GeneratedValue
77,Long id; // the system-generated surrogate key
77,@NaturalId
77,String isbn; // belongs to the natural key
77,@NaturalId
77,int printing; // also belongs to the natural key
77,...
77,Hibernate automatically generates a UNIQUE constraint on the columns mapped by the annotated fields.
77,Consider using the natural id attributes to implement equals() and hashCode().
77,"The payoff for doing this extra work, as we will see much later, is that we can take advantage of optimized natural id lookups that make use of the second-level cache."
77,"Note that even when you’ve identified a natural key, we still recommend the use of a generated surrogate key in foreign keys, since this makes your data model much easier to change."
77,3.10. Basic attributes
77,A basic attribute of an entity is a field or property which maps to a single column of the associated database table.
77,The JPA specification defines a quite limited set of basic types:
77,Table 12. JPA-standard basic attribute types
77,Classification
77,Package
77,Types
77,Primitive types
77,"boolean, int, double, etc"
77,Primitive wrappers
77,java.lang
77,"Boolean, Integer, Double, etc"
77,Strings
77,java.lang
77,String
77,Arbitrary-precision numeric types
77,java.math
77,"BigInteger, BigDecimal"
77,Date/time types
77,java.time
77,"LocalDate, LocalTime, LocalDateTime, OffsetDateTime, Instant"
77,Deprecated date/time types 💀
77,java.util
77,"Date, Calendar"
77,Deprecated JDBC date/time types 💀
77,java.sql
77,"Date, Time, Timestamp"
77,Binary and character arrays
77,"byte[], char[]"
77,UUIDs
77,java.util
77,UUID
77,Enumerated types
77,Any enum
77,Serializable types
77,Any type which implements java.io.Serializable
77,We’re begging you to use types from the java.time package instead of anything which inherits java.util.Date.
77,Serializing a Java object and storing its binary representation in the database is usually wrong.
77,"As we’ll soon see in Embeddable objects, Hibernate has much better ways to handle complex Java objects."
77,Hibernate slightly extends this list with the following types:
77,Table 13. Additional basic attribute types in Hibernate
77,Classification
77,Package
77,Types
77,Additional date/time types
77,java.time
77,"Duration, ZoneId, ZoneOffset, Year, and even ZonedDateTime"
77,JDBC LOB types
77,java.sql
77,"Blob, Clob, NClob"
77,Java class object
77,java.lang
77,Class
77,Miscellaneous types
77,java.util
77,"Currency, URL, TimeZone"
77,"The @Basic annotation explicitly specifies that an attribute is basic, but it’s often not needed, since attributes are assumed basic by default."
77,"On the other hand, if a non-primitively-typed attribute cannot be null, use of @Basic(optional=false) is highly recommended."
77,@Basic(optional=false) String firstName;
77,@Basic(optional=false) String lastName;
77,String middleName; // may be null
77,Note that primitively-typed attributes are inferred NOT NULL by default.
77,How to make a column not null in JPA
77,There are two standard ways to add a NOT NULL constraint to a mapped column in JPA:
77,"using @Basic(optional=false), or"
77,using @Column(nullable=false).
77,You might wonder what the difference is.
77,"Well, it’s perhaps not obvious to a casual user of the JPA annotations, but they actually come in two ""layers"":"
77,"annotations like @Entity, @Id, and @Basic belong to the logical layer, the subject of the current chapter—they specify the semantics of your Java domain model, whereas"
77,"annotations like @Table and @Column belong to the mapping layer, the topic of the next chapter—they specify how elements of the domain model map to objects in the relational database."
77,"Information may be inferred from the logical layer down to the mapping layer, but is never inferred in the opposite direction."
77,"Now, the @Column annotation, to whom we’ll be properly introduced a bit later, belongs to the mapping layer, and so its nullable member only affects schema generation (resulting in a not null constraint in the generated DDL)."
77,"On the other hand, the @Basic annotation belongs to the logical layer, and so an attribute marked optional=false is checked by Hibernate before it even writes an entity to the database."
77,Note that:
77,"optional=false implies nullable=false, but"
77,nullable=false does not imply optional=false.
77,"Therefore, we prefer @Basic(optional=false) to @Column(nullable=false)."
77,But wait!
77,An even better solution is to use the @NotNull annotation from Bean Validation.
77,"Just add Hibernate Validator to your project build, as described in Optional dependencies."
77,3.11. Enumerated types
77,We included Java enums on the list above.
77,"An enumerated type is considered a sort of basic type, but since most databases don’t have a native ENUM type, JPA provides a special @Enumerated annotation to specify how the enumerated values should be represented in the database:"
77,"by default, an enum is stored as an integer, the value of its ordinal() member, but"
77,"if the attribute is annotated @Enumerated(STRING), it will be stored as a string, the value of its name() member."
77,"//here, an ORDINAL encoding makes sense"
77,@Enumerated
77,@Basic(optional=false)
77,DayOfWeek dayOfWeek;
77,"//but usually, a STRING encoding is better"
77,@Enumerated(EnumType.STRING)
77,@Basic(optional=false)
77,Status status;
77,"In Hibernate 6, an enum annotated @Enumerated(STRING) is mapped to:"
77,"a VARCHAR column type with a CHECK constraint on most databases, or"
77,an ENUM column type on MySQL.
77,Any other enum is mapped to a TINYINT column with a CHECK constraint.
77,JPA picks the wrong default here.
77,"In most cases, storing an integer encoding of the enum value makes the relational data harder to interpret."
77,"Even considering DayOfWeek, the encoding to integers is ambiguous."
77,"If you check java.time.DayOfWeek, you’ll notice that SUNDAY is encoded as 6."
77,"But in the country I was born, SUNDAY is the first day of the week!"
77,So we prefer @Enumerated(STRING) for most enum attributes.
77,An interesting special case is PostgreSQL.
77,"Postgres supports named ENUM types, which must be declared using a DDL CREATE TYPE statement."
77,"Sadly, these ENUM types aren’t well-integrated with the language nor well-supported by the Postgres JDBC driver, so Hibernate doesn’t use them by default."
77,"But if you would like to use a named enumerated type on Postgres, just annotate your enum attribute like this:"
77,@JdbcTypeCode(SqlTypes.NAMED_ENUM)
77,@Basic(optional=false)
77,Status status;
77,The limited set of pre-defined basic attribute types can be stretched a bit further by supplying a converter.
77,3.12. Converters
77,A JPA AttributeConverter is responsible for:
77,"converting a given Java type to one of the types listed above, and/or"
77,perform any other sort of pre- and post-processing you might need to perform on a basic attribute value before writing and reading it to or from the database.
77,Converters substantially widen the set of attribute types that can be handled by JPA.
77,There are two ways to apply a converter:
77,"the @Convert annotation applies an AttributeConverter to a particular entity attribute, or"
77,"the @Converter annotation (or, alternatively, the @ConverterRegistration annotation) registers an AttributeConverter for automatic application to all attributes of a given type."
77,"For example, the following converter will be automatically applied to any attribute of type BitSet, and takes care of persisting the BitSet to a column of type varbinary:"
77,@Converter(autoApply = true)
77,"public static class EnumSetConverter implements AttributeConverter<EnumSet<DayOfWeek>,Integer> {"
77,@Override
77,public Integer convertToDatabaseColumn(EnumSet<DayOfWeek> enumSet) {
77,int encoded = 0;
77,var values = DayOfWeek.values();
77,for (int i = 0; i<values.length; i++) {
77,if (enumSet.contains(values[i])) {
77,encoded |= 1<<i;
77,return encoded;
77,@Override
77,public EnumSet<DayOfWeek> convertToEntityAttribute(Integer encoded) {
77,var set = EnumSet.noneOf(DayOfWeek.class);
77,var values = DayOfWeek.values();
77,for (int i = 0; i<values.length; i++) {
77,if (((1<<i) & encoded) != 0) {
77,set.add(values[i]);
77,return set;
77,"On the other hand, if we don’t set autoapply=true, then we must explicitly apply the converter using the @Convert annotation:"
77,@Convert(converter = BitSetConverter.class)
77,@Basic(optional = false)
77,BitSet bitset;
77,"All this is nice, but it probably won’t surprise you that Hibernate goes beyond what is required by JPA."
77,3.13. Compositional basic types
77,"Hibernate considers a ""basic type"" to be formed by the marriage of two objects:"
77,"a JavaType, which models the semantics of a certain Java class, and"
77,"a JdbcType, representing a SQL type which is understood by JDBC."
77,"When mapping a basic attribute, we may explicitly specify a JavaType, a JdbcType, or both."
77,JavaType
77,An instance of org.hibernate.type.descriptor.java.JavaType represents a particular Java class.
77,It’s able to:
77,"compare instances of the class to determine if an attribute of that class type is dirty (modified),"
77,"produce a useful hash code for an instance of the class,"
77,"coerce values to other types, and, in particular,"
77,convert an instance of the class to one of several other equivalent Java representations at the request of its partner JdbcType.
77,"For example, IntegerJavaType knows how to convert an Integer or int value to the types Long, BigInteger, and String, among others."
77,"We may explicitly specify a Java type using the @JavaType annotation, but for the built-in JavaTypes this is never necessary."
77,@JavaType(LongJavaType.class)
77,"// not needed, this is the default JavaType for long"
77,long currentTimeMillis;
77,"For a user-written JavaType, the annotation is more useful:"
77,@JavaType(BitSetJavaType.class)
77,BitSet bitSet;
77,"Alternatively, the @JavaTypeRegistration annotation may be used to register BitSetJavaType as the default JavaType for BitSet."
77,JdbcType
77,A org.hibernate.type.descriptor.jdbc.JdbcType is able to read and write a single Java type from and to JDBC.
77,"For example, VarcharJdbcType takes care of:"
77,"writing Java strings to JDBC PreparedStatements by calling setString(), and"
77,reading Java strings from JDBC ResultSets using getString().
77,"By pairing LongJavaType with VarcharJdbcType in holy matrimony, we produce a basic type which maps Longs and primitive longss to the SQL type VARCHAR."
77,We may explicitly specify a JDBC type using the @JdbcType annotation.
77,@JdbcType(VarcharJdbcType.class)
77,long currentTimeMillis;
77,"Alternatively, we may specify a JDBC type code:"
77,@JdbcTypeCode(Types.VARCHAR)
77,long currentTimeMillis;
77,The @JdbcTypeRegistration annotation may be used to register a user-written JdbcType as the default for a given SQL type code.
77,JDBC types and JDBC type codes
77,The types defined by the JDBC specification are enumerated by the integer type codes in the class java.sql.Types.
77,Each JDBC type is an abstraction of a commonly-available type in SQL.
77,"For example, Types.VARCHAR represents the SQL type VARCHAR (or VARCHAR2 on Oracle)."
77,"Since Hibernate understand more SQL types than JDBC, there’s an extended list of integer type codes in the class org.hibernate.type.SqlTypes."
77,"For example, SqlTypes.GEOMETRY represents the spatial data type GEOMETRY."
77,AttributeConverter
77,"If a given JavaType doesn’t know how to convert its instances to the type required by its partner JdbcType, we must help it out by providing a JPA AttributeConverter to perform the conversion."
77,"For example, to form a basic type using LongJavaType and TimestampJdbcType, we would provide an AttributeConverter<Long,Timestamp>."
77,@JdbcType(TimestampJdbcType.class)
77,@Convert(converter = LongToTimestampConverter.class)
77,long currentTimeMillis;
77,"Let’s abandon our analogy right here, before we start calling this basic type a ""throuple""."
77,3.14. Embeddable objects
77,"An embeddable object is a Java class whose state maps to multiple columns of a table, but which doesn’t have its own persistent identity."
77,"That is, it’s a class with mapped attributes, but no @Id attribute."
77,An embeddable object can only be made persistent by assigning it to the attribute of an entity.
77,"Since the embeddable object does not have its own persistent identity, its lifecycle with respect to persistence is completely determined by the lifecycle of the entity to which it belongs."
77,An embeddable class must be annotated @Embeddable instead of @Entity.
77,@Embeddable
77,class Name {
77,@Basic(optional=false)
77,String firstName;
77,@Basic(optional=false)
77,String lastName;
77,String middleName;
77,Name() {}
77,"Name(String firstName, String middleName, String lastName) {"
77,this.firstName = firstName;
77,this.middleName = middleName;
77,this.lastName = lastName;
77,...
77,"An embeddable class must satisfy the same requirements that entity classes satisfy, with the exception that an embeddable class has no @Id attribute."
77,"In particular, it must have a constructor with no parameters."
77,"Alternatively, an embeddable type may be defined as a Java record type:"
77,@Embeddable
77,"record Name(String firstName, String middleName, String lastName) {}"
77,"In this case, the requirement for a constructor with no parameters is relaxed."
77,"Unfortunately, as of May 2023, Java record types still cannot be used as @EmbeddedIds."
77,We may now use our Name class (or record) as the type of an entity attribute:
77,@Entity
77,class Author {
77,@Id @GeneratedValue
77,Long id;
77,Name name;
77,...
77,Embeddable types can be nested.
77,"That is, an @Embeddable class may have an attribute whose type is itself a different @Embeddable class."
77,JPA provides an @Embedded annotation to identify an attribute of an entity that refers to an embeddable type.
77,"This annotation is completely optional, and so we don’t usually use it."
77,On the other hand a reference to an embeddable type is never polymorphic.
77,"One @Embeddable class F may inherit a second @Embeddable class E, but an attribute of type E will always refer to an instance of that concrete class E, never to an instance of F."
77,"Usually, embeddable types are stored in a ""flattened"" format."
77,Their attributes map columns of the table of their parent entity.
77,"Later, in Mapping embeddable types to UDTs or to JSON, we’ll see a couple of different options."
77,"An attribute of embeddable type represents a relationship between a Java object with a persistent identity, and a Java object with no persistent identity."
77,We can think of it as a whole/part relationship.
77,"The embeddable object belongs to the entity, and can’t be shared with other entity instances."
77,And it exits for only as long as its parent entity exists.
77,Next we’ll discuss a different kind of relationship: a relationship between Java objects which each have their own distinct persistent identity and persistence lifecycle.
77,3.15. Associations
77,An association is a relationship between entities.
77,We usually classify associations based on their multiplicity.
77,"If E and F are both entity classes, then:"
77,"a one-to-one association relates at most one unique instance E with at most one unique instance of F,"
77,"a many-to-one association relates zero or more instances of E with a unique instance of F, and"
77,a many-to-many association relates zero or more instances of E with zero or more instance of F.
77,An association between entity classes may be either:
77,"unidirectional, navigable from E to F but not from F to E, or"
77,"bidirectional, and navigable in either direction."
77,"In this example data model, we can see the sorts of associations which are possible:"
77,An astute observer of the diagram above might notice that the relationship we’ve presented as a unidirectional one-to-one association could reasonably be represented in Java using subtyping.
77,This is quite normal.
77,A one-to-one association is the usual way we implement subtyping in a fully-normalized relational model.
77,It’s related to the JOINED inheritance mapping strategy.
77,"There are three annotations for mapping associations: @ManyToOne, @OneToMany, and @ManyToMany."
77,They share some common annotation members:
77,Table 14. Association-defining annotation members
77,Member
77,Interpretation
77,Default value
77,cascade
77,Persistence operations which should cascade to the associated entity; a list of CascadeTypes
77,fetch
77,Whether the association is eagerly fetched or may be proxied
77,LAZY for @OneToMany and @ManyToMany
77,EAGER for @ManyToOne 💀💀💀
77,targetEntity
77,The associated entity class
77,Determined from the attribute type declaration
77,optional
77,"For a @ManyToOne or @OneToOne association, whether the association can be null"
77,true
77,mappedBy
77,"For a bidirectional association, an attribute of the associated entity which maps the association"
77,"By default, the association is assumed unidirectional"
77,We’ll explain the effect of these members as we consider the various types of association mapping.
77,Let’s begin with the most common association multiplicity.
77,3.16. Many-to-one
77,A many-to-one association is the most basic sort of association we can imagine.
77,It maps completely naturally to a foreign key in the database.
77,Almost all the associations in your domain model are going to be of this form.
77,"Later, we’ll see how to map a many-to-one association to an association table."
77,"The @ManyToOne annotation marks the ""to one"" side of the association, so a unidirectional many-to-one association looks like this:"
77,class Book {
77,@Id @GeneratedValue
77,Long id;
77,@ManyToOne(fetch=LAZY)
77,Publisher publisher;
77,...
77,"Here, the Book table has a foreign key column holding the identifier of the associated Publisher."
77,A very unfortunate misfeature of JPA is that @ManyToOne associations are fetched eagerly by default.
77,This is almost never what we want.
77,Almost all associations should be lazy.
77,The only scenario in which fetch=EAGER makes sense is if we think there’s always a very high probability that the associated object will be found in the second-level cache.
77,"Whenever this isn’t the case, remember to explicitly specify fetch=LAZY."
77,"Most of the time, we would like to be able to easily navigate our associations in both directions."
77,"We do need a way to get the Publisher of a given Book, but we would also like to be able to obtain all the Books belonging to a given publisher."
77,"To make this association bidirectional, we need to add a collection-valued attribute to the Publisher class, and annotate it @OneToMany."
77,Hibernate needs to proxy unfetched associations at runtime.
77,"Therefore, the many-valued side must be declared using an interface type like Set or List, and never using a concrete type like HashSet or ArrayList."
77,"To indicate clearly that this is a bidirectional association, and to reuse any mapping information already specified in the Book entity, we must use the mappedBy annotation member to refer back to Book.publisher."
77,@Entity
77,class Publisher {
77,@Id @GeneratedValue
77,Long id;
77,"@OneToMany(mappedBy=""publisher"")"
77,Set<Book> books;
77,...
77,The Publisher.books field is called the unowned side of the association.
77,"Now, we passionately hate the stringly-typed mappedBy reference to the owning side of the association."
77,"Thankfully, the Metamodel Generator gives us a way to make it a"
77,bit more typesafe:
77,@OneToMany(mappedBy=Book_.PUBLISHER)
77,// get used to doing it this way!
77,Set<Book> books;
77,We’re going to use this approach for the rest of the Introduction.
77,"To modify a bidirectional association, we must change the owning side."
77,Changes made to the unowned side of an association are never synchronized to the database.
77,"If we desire to change an association in the database, we must change it from the owning side."
77,"Here, we must set Book.publisher."
77,"In fact, it’s often necessary to change both sides of a bidirectional association."
77,"For example, if the collection Publisher.books was stored in the second-level cache, we must also modify the collection, to ensure that the second-level cache remains synchronized with the database."
77,"That said, it’s not a hard requirement to update the unowned side, at least if you’re sure you know what you’re doing."
77,"In principle Hibernate does allow you to have a unidirectional one-to-many, that is, a @OneToMany with no matching @ManyToOne on the other side."
77,"In practice, this mapping is unnatural, and just doesn’t work very well."
77,Avoid it.
77,"Here we’ve used Set as the type of the collection, but Hibernate also allows the use of List or Collection here, with almost no difference in semantics."
77,"In particular, the List may not contain duplicate elements, and its order will not be persistent."
77,@OneToMany(mappedBy=Book_.PUBLISHER)
77,Collection<Book> books;
77,We’ll see how to map a collection with a persistent order much later.
77,"Set, List, or Collection?"
77,"A one-to-many association mapped to a foreign key can never contain duplicate elements, so Set seems like the most semantically correct Java collection type to use here, and so that’s the conventional practice in the Hibernate community."
77,The catch associated with using a set is that we must carefully ensure that Book has a high-quality implementation of equals() and hashCode().
77,"Now, that’s not necessarily a bad thing, since a quality equals() is independently useful."
77,But what if we used Collection or List instead?
77,Then our code would be much less sensitive to how equals() and hashCode() were implemented.
77,"In the past, we were perhaps too dogmatic in recommending the use of Set."
77,Now? I guess we’re happy to let you guys decide.
77,"In hindsight, we could have done more to make clear that this was always a viable option."
77,3.17. One-to-one (first way)
77,"The simplest sort of one-to-one association is almost exactly like a @ManyToOne association, except that it maps to a foreign key column with a UNIQUE constraint."
77,"Later, we’ll see how to map a one-to-one association to an association table."
77,A one-to-one association must be annotated @OneToOne:
77,@Entity
77,class Author {
77,@Id @GeneratedValue
77,Long id;
77,"@OneToOne(optional=false, fetch=LAZY)"
77,Person author;
77,...
77,"Here, the Author table has a foreign key column holding the identifier of the associated Publisher."
77,"A one-to-one association often models a ""type of"" relationship."
77,"In our example, an Author is a type of Person."
77,"An alternative—and often more natural—way to represent ""type of"" relationships in Java is via entity class inheritance."
77,We can make this association bidirectional by adding a reference back to the Author in the Person entity:
77,@Entity
77,class Person {
77,@Id @GeneratedValue
77,Long id;
77,@OneToOne(mappedBy = Author_.PERSON)
77,Author author;
77,...
77,"Person.author is the unowned side, because it’s the side marked mappedBy."
77,Lazy fetching for one-to-one associations
77,Notice that we did not declare the unowned end of the association fetch=LAZY.
77,That’s because:
77,"not every Person has an associated Author, and"
77,"the foreign key is held in the table mapped by Author, not in the table mapped by Person."
77,"Therefore, Hibernate can’t tell if the reference from Person to Author is null without fetching the associated Author."
77,"On the other hand, if every Person was an Author, that is, if the association were non-optional, we would not have to consider the possibility of null references, and we would map it like this:"
77,"@OneToOne(optional=false, mappedBy = Author_.PERSON, fetch=LAZY)"
77,Author author;
77,This is not the only sort of one-to-one association.
77,3.18. One-to-one (second way)
77,An arguably more elegant way to represent such a relationship is to share a primary key between the two tables.
77,"To use this approach, the Author class must be annotated like this:"
77,@Entity
77,class Author {
77,@Id
77,Long id;
77,"@OneToOne(optional=false, fetch=LAZY)"
77,@MapsId
77,Person author;
77,...
77,"Notice that, compared with the previous mapping:"
77,"the @Id attribute is no longer a @GeneratedValue and,"
77,"instead, the author association is annotated @MapsId."
77,This lets Hibernate know that the association to Person is the source of primary key values for Author.
77,"Here, there’s no extra foreign key column in the Author table, since the id column holds the identifier of Person."
77,"That is, the primary key of the Author table does double duty as the foreign key referring to the Person table."
77,The Person class doesn’t change.
77,"If the association is bidirectional, we annotate the unowned side @OneToOne(mappedBy = Author_.PERSON) just as before."
77,3.19. Many-to-many
77,A unidirectional many-to-many association is represented as a collection-valued attribute.
77,It always maps to a separate association table in the database.
77,It tends to happen that a many-to-many association eventually turns out to be an entity in disguise.
77,Suppose we start with a nice clean many-to-many association between Author and Book.
77,"Later on, it’s quite likely that we’ll discover some additional information which comes attached to the association, so that the association table needs some extra columns."
77,"For example, imagine that we needed to report the percentage contribution of each author to a book."
77,That information naturally belongs to the association table.
77,"We can’t easily store it as an attribute of Book, nor as an attribute of Author."
77,"When this happens, we need to change our Java model, usually introducing a new entity class which maps the association table directly."
77,"In our example, we might call this entity something like BookAuthorship, and it would have @OneToMany associations to both Author and Book, along with the contribution attribute."
77,"We can evade the disruption occasioned by such ""discoveries"" by simply avoiding the use of @ManyToMany right from the start."
77,There’s little downside to representing every—or at least almost every—logical many-to-many association using an intermediate entity.
77,A many-to-many association must be annotated @ManyToMany:
77,@Entity
77,class Book {
77,@Id @GeneratedValue
77,Long id;
77,@ManyToMany
77,Set<Author> authors;
77,...
77,"If the association is bidirectional, we add a very similar-looking attribute to Book, but this time we must specify mappedBy to indicate that this is the unowned side of the association:"
77,@Entity
77,class Book {
77,@Id @GeneratedValue
77,Long id;
77,@ManyToMany(mappedBy=Author_.BOOKS)
77,Set<Author> authors;
77,...
77,"Remember, if we wish to the modify the collection we must change the owning side."
77,We’ve again used Sets to represent the association.
77,"As before, we have the option to use Collection or List."
77,But in this case it does make a difference to the semantics of the association.
77,A many-to-many association represented as a Collection or List may contain duplicate elements.
77,"However, as before, the order of the elements is not persistent."
77,"That is, the collection is a bag, not a set."
77,3.20. Collections of basic values and embeddable objects
77,We’ve now seen the following kinds of entity attribute:
77,Kind of entity attribute
77,Kind of reference
77,Multiplicity
77,Examples
77,Single-valued attribute of basic type
77,Non-entity
77,At most one
77,@Basic String name
77,Single-valued attribute of embeddable type
77,Non-entity
77,At most one
77,@Embedded Name name
77,Single-valued association
77,Entity
77,At most one
77,@ManyToOne Publisher publisher
77,@OneToOne Person person
77,Many-valued association
77,Entity
77,Zero or more
77,@OneToMany Set<Book> books
77,@ManyToMany Set<Author> authors
77,"Scanning this taxonomy, you might ask: does Hibernate have multivalued attributes of basic or embeddable type?"
77,"Well, actually, we’ve already seen that it does, at least in two special cases."
77,"So first, lets recall that JPA treats byte[] and char[] arrays as basic types."
77,"Hibernate persists a byte[] or char[] array to a VARBINARY or VARCHAR column, respectively."
77,But in this section we’re really concerned with cases other than these two special cases.
77,"So then, apart from byte[] and char[], does Hibernate have multivalued attributes of basic or embeddable type?"
77,"And the answer again is that it does. Indeed, there are two different ways to handle such a collection, by mapping it:"
77,"to a column of SQL ARRAY type (assuming the database has an ARRAY type), or"
77,to a separate table.
77,So we may expand our taxonomy with:
77,Kind of entity attribute
77,Kind of reference
77,Multiplicity
77,Examples
77,byte[] and char[] arrays
77,Non-entity
77,Zero or more
77,byte[] image
77,char[] text
77,Collection of basic-typed elements
77,Non-entity
77,Zero or more
77,@Array String[] names
77,@ElementCollection Set<String> names
77,Collection of embeddable elements
77,Non-entity
77,Zero or more
77,@ElementCollection Set<Name> names
77,"There’s actually two new kinds of mapping here: @Array mappings, and @ElementCollection mappings."
77,These sorts of mappings are overused.
77,There are situations where we think it’s appropriate to use a collection of basic-typed values in our entity class.
77,But such situations are rare.
77,Almost every many-valued relationship should map to a foreign key association between separate tables.
77,And almost every table should be mapped by an entity class.
77,The features we’re about to meet in the next two subsections are used much more often by beginners than they’re used by experts.
77,"So if you’re a beginner, you’ll save yourself same hassle by staying away from these features for now."
77,We’ll talk about @Array mappings first.
77,3.21. Collections mapped to SQL arrays
77,Let’s consider a calendar event which repeats on certain days of the week.
77,We might represent this in our Event entity as an attribute of type DayOfWeek[] or List<DayOfWeek>.
77,"Since the number of elements of this array or list is upper bounded by 7, this is a reasonable case for the use of an ARRAY-typed column."
77,It’s hard to see much value in storing this collection in a separate table.
77,Learning to not hate SQL arrays
77,"For a long time, we thought arrays were a kind of weird and warty thing to add to the relational model, but recently we’ve come to realize that this view was overly closed-minded."
77,"Indeed, we might choose to view SQL ARRAY types as a generalization of VARCHAR and VARBINARY to generic ""element"" types."
77,"And from this point of view, SQL arrays look quite attractive, at least for certain problems."
77,"If we’re comfortable mapping byte[] to VARBINARY(255), why would we shy away from mapping DayOfWeek[] to TINYINT ARRAY[7]?"
77,"Unfortunately, JPA doesn’t define a standard way to map SQL arrays, but here’s how we can do it in Hibernate:"
77,@Entity
77,class Event {
77,@Id @GeneratedValue
77,Long id;
77,...
77,@Array(length=7)
77,DayOfWeek[] daysOfWeek;
77,// stored as a SQL ARRAY type
77,...
77,"The @Array annotation is optional, but it’s important to limit the amount of storage space the database allocates to the ARRAY column."
77,"Now for the gotcha: not every database has a SQL ARRAY type, and some that do have an ARRAY type don’t allow it to be used as a column type."
77,"In particular, neither DB2 nor SQL Server have array-typed columns."
77,"On these databases, Hibernate falls back to something much worse: it uses Java serialization to encode the array to a binary representation, and stores the binary stream in a VARBINARY column."
77,"Quite clearly, this is terrible."
77,"You can ask Hibernate to do something slightly less terrible by annotating the attribute @JdbcTypeCode(SqlTypes.JSON), so that the array is serialized to JSON instead of binary format."
77,But at this point it’s better to just admit defeat and use an @ElementCollection instead.
77,"Alternatively, we could store this array or list in a separate table."
77,3.22. Collections mapped to a separate table
77,JPA does define a standard way to map a collection to an auxiliary table: the @ElementCollection annotation.
77,@Entity
77,class Event {
77,@Id @GeneratedValue
77,Long id;
77,...
77,@ElementCollection
77,DayOfWeek[] daysOfWeek;
77,// stored in a dedicated table
77,...
77,"Actually, we shouldn’t use an array here, since array types can’t be proxied, and so the JPA specification doesn’t even say they’re supported."
77,"Instead, we should use Set, List, or Map."
77,@Entity
77,class Event {
77,@Id @GeneratedValue
77,Long id;
77,...
77,@ElementCollection
77,List<DayOfWeek> daysOfWeek;
77,// stored in a dedicated table
77,...
77,"Here, each collection element is stored as a separate row of the auxiliary table."
77,"By default, this table has the following definition:"
77,create table Event_daysOfWeek (
77,"Event_id bigint not null,"
77,"daysOfWeek tinyint check (daysOfWeek between 0 and 6),"
77,"daysOfWeek_ORDER integer not null,"
77,"primary key (Event_id, daysOfWeek_ORDER)"
77,"Which is fine, but it’s still a mapping we prefer to avoid."
77,@ElementCollection is one of our least-favorite features of JPA.
77,Even the name of the annotation is bad.
77,The code above results in a table with three columns:
77,"a foreign key of the Event table,"
77,"a TINYINT encoding the enum, and"
77,an INTEGER encoding the ordering of elements in the array.
77,"Instead of a surrogate primary key, it has a composite key comprising the foreign key of Event and the order column."
77,"When—inevitably—we find that we need to add a fourth column to that table, our Java code must change completely."
77,"Most likely, we’ll realize that we need to add a separate entity after all."
77,So this mapping isn’t very robust in the face of minor changes to our data model.
77,"There’s much more we could say about ""element collections"", but we won’t say it, because we don’t want to hand you the gun you’ll shoot your foot with."
77,3.23. Summary of annotations
77,Let’s pause to remember the annotations we’ve met so far.
77,Table 15. Declaring entities and embeddable types
77,Annotation
77,Purpose
77,JPA-standard
77,@Entity
77,Declare an entity class
77,@MappedSuperclass
77,Declare a non-entity class with mapped attributes inherited by an entity
77,@Embeddable
77,Declare an embeddable type
77,@IdClass
77,Declare the identifier class for an entity with multiple @Id attributes
77,Table 16. Declaring basic and embedded attributes
77,Annotation
77,Purpose
77,JPA-standard
77,@Id
77,Declare a basic-typed identifier attribute
77,@Version
77,Declare a version attribute
77,@Basic
77,Declare a basic attribute
77,Default
77,@EmbeddedId
77,Declare an embeddable-typed identifier attribute
77,@Embedded
77,Declare an embeddable-typed attribute
77,Inferred
77,@Enumerated
77,Declare an enum-typed attribute and specify how it is encoded
77,Inferred
77,@Array
77,"Declare that an attribute maps to a SQL ARRAY, and specify the length"
77,Inferred
77,@ElementCollection
77,Declare that a collection is mapped to a dedicated table
77,Table 17. Converters and compositional basic types
77,Annotation
77,Purpose
77,JPA-standard
77,@Converter
77,Register an AttributeConverter
77,@Convert
77,Apply a converter to an attribute
77,@JavaType
77,Explicitly specify an implementation of JavaType for a basic attribute
77,@JdbcType
77,Explicitly specify an implementation of JdbcType for a basic attribute
77,@JdbcTypeCode
77,Explicitly specify a JDBC type code used to determine the JdbcType for a basic attribute
77,@JavaTypeRegistration
77,Register a JavaType for a given Java type
77,@JdbcTypeRegistration
77,Register a JdbcType for a given JDBC type code
77,Table 18. System-generated identifiers
77,Annotation
77,Purpose
77,JPA-standard
77,@GeneratedValue
77,Specify that an identifier is system-generated
77,@SequenceGenerator
77,Define an id generated backed by on a database sequence
77,@TableGenerator
77,Define an id generated backed by a database table
77,@IdGeneratorType
77,Declare an annotation that associates a custom Generator with each @Id attribute it annotates
77,@ValueGenerationType
77,Declare an annotation that associates a custom Generator with each @Basic attribute it annotates
77,Table 19. Declaring entity associations
77,Annotation
77,Purpose
77,JPA-standard
77,@ManyToOne
77,Declare the single-valued side of a many-to-one association (the owning side)
77,@OneToMany
77,Declare the many-valued side of a many-to-one association (the unowned side)
77,@ManyToMany
77,Declare either side of a one-to-one association
77,@OneToOne
77,Declare either side of a one-to-one association
77,@MapsId
77,Declare that the owning side of a @OneToOne association maps the primary key column
77,Phew!
77,"That’s already a lot of annotations, and we have not even started with the annotations for O/R mapping!"
77,3.24. equals() and hashCode()
77,"Entity classes should override equals() and hashCode(), especially when associations are represented as sets."
77,People new to Hibernate or JPA are often confused by exactly which fields should be included in the hashCode().
77,And people with more experience often argue quite religiously that one or another approach is the only right way.
77,"The truth is, there’s no unique right way to do it, but there are some constraints."
77,So please keep the following principles in mind:
77,"You should not include a mutable field in the hashcode, since that would require rehashing every collection containing the entity whenever the field is mutated."
77,"It’s not completely wrong to include a generated identifier (surrogate key) in the hashcode, but since the identifier is not generated until the entity instance is made persistent, you must take great care to not add it to any hashed collection before the identifier is generated. We therefore advise against including any database-generated field in the hashcode."
77,"It’s OK to include any immutable, non-generated field in the hashcode."
77,"We therefore recommend identifying a natural key for each entity, that is, a combination of fields that uniquely identifies an instance of the entity, from the perspective of the data model of the program. The natural key should correspond to a unique constraint on the database, and to the fields which are included in equals() and hashCode()."
77,"In this example, the equals() and hashCode() methods agree with the @NaturalId annotation:"
77,@Entity
77,class Book {
77,@Id @GeneratedValue
77,Long id;
77,@NaturalId
77,@Basic(optional=false)
77,String isbn;
77,...
77,@Override
77,public boolean equals(Object other) {
77,return other instanceof Book
77,&& ((Book) other).isbn.equals(isbn);
77,@Override
77,public int hashCode() {
77,return isbn.hashCode();
77,"That said, an implementation of equals() and hashCode() based on the generated identifier of the entity can work if you’re careful."
77,4. Object/relational mapping
77,"Given a domain model—that is, a collection of entity classes decorated with all the fancy annotations we just met in the previous chapter—Hibernate will happily go away and infer a complete relational schema, and even export it to your database if you ask politely."
77,"The resulting schema will be entirely sane and reasonable, though if you look closely, you’ll find some flaws."
77,"For example, every VARCHAR column will have the same length, VARCHAR(255)."
77,But the process I just described—which we call top down mapping—simply doesn’t fit the most common scenario for the use of O/R mapping.
77,It’s only rarely that the Java classes precede the relational schema.
77,"Usually, we already have a relational schema, and we’re constructing our domain model around the schema."
77,This is called bottom up mapping.
77,"Developers often refer to a pre-existing relational database as ""legacy"" data."
77,"This tends to conjure images of bad old ""legacy apps"" written in COBOL or something."
77,"But legacy data is valuable, and learning to work with it is important."
77,"Especially when mapping bottom up, we often need to customize the inferred object/relational mappings."
77,"This is a somewhat tedious topic, and so we don’t want to spend too many words on it."
77,"Instead, we’ll quickly skim the most important mapping annotations."
77,Hibernate SQL case convention
77,Computers have had lowercase letters for rather a long time now.
77,"Most developers learned long ago that text written in MixedCase, camelCase, or even snake_case is easier to read than text written in SHOUTYCASE."
77,This is just as true of SQL as it is of any other language.
77,"Therefore, for over twenty years, the convention on the Hibernate project has been that:"
77,"query language identifiers are written in lowercase,"
77,"table names are written in MixedCase, and"
77,column names are written in camelCase.
77,"That is to say, we simply adopted Java’s excellent conventions and applied them to SQL."
77,"Now, there’s no way we can force you to follow this convention, even if we wished to."
77,"Hell, you can easily write a PhysicalNamingStrategy which makes table and column names ALL UGLY AND SHOUTY LIKE THIS IF YOU PREFER."
77,"But, by default, it’s the convention Hibernate follows, and it’s frankly a pretty reasonable one."
77,4.1. Mapping entity inheritance hierarchies
77,In Entity class inheritance we saw that entity classes may exist within an inheritance hierarchy.
77,There’s three basic strategies for mapping an entity hierarchy to relational tables.
77,"Let’s put them in a table, so we can more easily compare the points of difference between them."
77,Table 20. Entity inheritance mapping strategies
77,Strategy
77,Mapping
77,Polymorphic queries
77,Constraints
77,Normalization
77,When to use it
77,SINGLE_TABLE
77,"Map every class in the hierarchy to the same table, and uses the value of a discriminator column to determine which concrete class each row represents."
77,"To retrieve instances of a given class, we only need to query the one table."
77,Attributes declared by subclasses map to columns without NOT NULL constraints. 💀
77,Any association may have a FOREIGN KEY constraint. 🤓
77,Subclass data is denormalized. 🧐
77,Works well when subclasses declare few or no additional attributes.
77,JOINED
77,"Map every class in the hierarchy to a separate table, but each table only maps the attributes declared by the class itself."
77,"Optionally, a discriminator column may be used."
77,"To retrieve instances of a given class, we must JOIN the table mapped by the class with:"
77,all tables mapped by its superclasses and
77,all tables mapped by its subclasses.
77,Any attribute may map to a column with a NOT NULL constraint. 🤓
77,Any association may have a FOREIGN KEY constraint. 🤓
77,The tables are normalized. 🤓
77,The best option when we care a lot about constraints and normalization.
77,TABLE_PER_CLASS
77,"Map every concrete class in the hierarchy to a separate table, but denormalize all inherited attributes into the table."
77,"To retrieve instances of a given class, we must take a UNION over the table mapped by the class and the tables mapped by its subclasses."
77,Associations targeting a superclass cannot have a corresponding FOREIGN KEY constraint in the database. 💀💀
77,Any attribute may map to a column with a NOT NULL constraint. 🤓
77,Superclass data is denormalized. 🧐
77,Not very popular.
77,"From a certain point of view, competes with @MappedSuperclass."
77,The three mapping strategies are enumerated by InheritanceType.
77,We specify an inheritance mapping strategy using the @Inheritance annotation.
77,"For mappings with a discriminator column, we should:"
77,"specify the discriminator column name and type by annotating the root entity @DiscriminatorColumn, and"
77,specify the values of this discriminator by annotating each entity in the hierarchy @DiscriminatorValue.
77,For single table inheritance we always need a discriminator:
77,@Entity
77,"@DiscriminatorColumn(discriminatorType=CHAR, name=""kind"")"
77,@DiscriminatorValue('P')
77,class Person { ... }
77,@Entity
77,@DiscriminatorValue('A')
77,class Author { ... }
77,"We don’t need to explicitly specify @Inheritance(strategy=SINGLE_TABLE), since that’s the default."
77,For JOINED inheritance we don’t need a discriminator:
77,@Entity
77,@Inheritance(strategy=JOINED)
77,class Person { ... }
77,@Entity
77,class Author { ... }
77,"However, we can add a discriminator column if we like, and in that case the generated SQL for polymorphic queries will be slightly simpler."
77,"Similarly, for TABLE_PER_CLASS inheritance we have:"
77,@Entity
77,@Inheritance(strategy=TABLE_PER_CLASS)
77,class Person { ... }
77,@Entity
77,class Author { ... }
77,"Hibernate doesn’t allow discriminator columns for TABLE_PER_CLASS inheritance mappings, since they would make no sense, and offer no advantage."
77,"Notice that in this last case, a polymorphic association like:"
77,@ManyToOne Person person;
77,"is a bad idea, since it’s impossible to create a foreign key constraint that targets both mapped tables."
77,4.2. Mapping to tables
77,The following annotations specify exactly how elements of the domain model map to tables of the relational model:
77,Table 21. Annotations for mapping tables
77,Annotation
77,Purpose
77,@Table
77,Map an entity class to its primary table
77,@SecondaryTable
77,Define a secondary table for an entity class
77,@JoinTable
77,Map a many-to-many or many-to-one association to its association table
77,@CollectionTable
77,Map an @ElementCollection to its table
77,"The first two annotations are used to map an entity to its primary table and, optionally, one or more secondary tables."
77,4.3. Mapping entities to tables
77,"By default, an entity maps to a single table, which may be specified using @Table:"
77,@Entity
77,"@Table(name=""People"")"
77,class Person { ... }
77,"However, the @SecondaryTable annotation allows us to spread its attributes across multiple secondary tables."
77,@Entity
77,"@Table(name=""Books"")"
77,"@SecondaryTable(name=""Editions"")"
77,class Book { ... }
77,The @Table annotation can do more than just specify a name:
77,Table 22. @Table annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped table
77,schema 💀
77,The schema to which the table belongs
77,catalog 💀
77,The catalog to which the table belongs
77,uniqueConstraints
77,One or more @UniqueConstraint annotations declaring multi-column unique constraints
77,indexes
77,One or more @Index annotations each declaring an index
77,It only makes sense to explicitly specify the schema in annotations if the domain model is spread across multiple schemas.
77,"Otherwise, it’s a bad idea to hardcode the schema (or catalog) in a @Table annotation."
77,Instead:
77,"set the configuration property hibernate.default_schema (or hibernate.default_catalog), or"
77,simply specify the schema in the JDBC connection URL.
77,The @SecondaryTable annotation is even more interesting:
77,Table 23. @SecondaryTable annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped table
77,schema 💀
77,The schema to which the table belongs
77,catalog 💀
77,The catalog to which the table belongs
77,uniqueConstraints
77,One or more @UniqueConstraint annotations declaring multi-column unique constraints
77,indexes
77,One or more @Index annotations each declaring an index
77,pkJoinColumns
77,"One or more @PrimaryKeyJoinColumn annotations, specifying primary key column mappings"
77,foreignKey
77,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the @PrimaryKeyJoinColumns
77,Using @SecondaryTable on a subclass in a SINGLE_TABLE entity inheritance hierarchy gives us a sort of mix of SINGLE_TABLE with JOINED inheritance.
77,4.4. Mapping associations to tables
77,"The @JoinTable annotation specifies an association table, that is, a table holding foreign keys of both associated entities."
77,This annotation is usually used with @ManyToMany associations:
77,@Entity
77,class Book {
77,...
77,@ManyToMany
77,"@JoinTable(name=""BooksAuthors"")"
77,Set<Author> authors;
77,...
77,But it’s even possible to use it to map a @ManyToOne or @OneToOne association to an association table.
77,@Entity
77,class Book {
77,...
77,@ManyToOne(fetch=LAZY)
77,"@JoinTable(name=""BookPublisher"")"
77,Publisher publisher;
77,...
77,"Here, there should be a UNIQUE constraint on one of the columns of the association table."
77,@Entity
77,class Author {
77,...
77,"@OneToOne(optional=false, fetch=LAZY)"
77,"@JoinTable(name=""AuthorPerson"")"
77,Person author;
77,...
77,"Here, there should be a UNIQUE constraint on both columns of the association table."
77,Table 24. @JoinTable annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped association table
77,schema 💀
77,The schema to which the table belongs
77,catalog 💀
77,The catalog to which the table belongs
77,uniqueConstraints
77,One or more @UniqueConstraint annotations declaring multi-column unique constraints
77,indexes
77,One or more @Index annotations each declaring an index
77,joinColumns
77,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the owning side"
77,inverseJoinColumns
77,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the unowned side"
77,foreignKey
77,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the joinColumnss
77,inverseForeignKey
77,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the inverseJoinColumnss
77,"To better understand these annotations, we must first discuss column mappings in general."
77,4.5. Mapping to columns
77,These annotations specify how elements of the domain model map to columns of tables in the relational model:
77,Table 25. Annotations for mapping columns
77,Annotation
77,Purpose
77,@Column
77,Map an attribute to a column
77,@JoinColumn
77,Map an association to a foreign key column
77,@PrimaryKeyJoinColumn
77,"Map the primary key used to join a secondary table with its primary, or a subclass table in JOINED inheritance with its root class table"
77,@OrderColumn
77,Specifies a column that should be used to maintain the order of a List.
77,@MapKeyColumn
77,Specified a column that should be used to persist the keys of a Map.
77,We use the @Column annotation to map basic attributes.
77,4.6. Mapping basic attributes to columns
77,The @Column annotation is not only useful for specifying the column name.
77,Table 26. @Column annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped column
77,table
77,The name of the table to which this column belongs
77,length
77,"The length of a VARCHAR, CHAR, or VARBINARY column type"
77,precision
77,"The decimal digits of precision of a FLOAT, DECIMAL, NUMERIC, or TIME, or TIMESTAMP column type"
77,scale
77,"The scale of a DECIMAL or NUMERIC column type, the digits of precision that occur to the right of the decimal point"
77,unique
77,Whether the column has a UNIQUE constraint
77,nullable
77,Whether the column has a NOT NULL constraint
77,insertable
77,Whether the column should appear in generated SQL INSERT statements
77,updatable
77,Whether the column should appear in generated SQL UPDATE statements
77,columnDefinition 💀
77,A DDL fragment that should be used to declare the column
77,We no longer recommend the use of columnDefinition since it results in unportable DDL.
77,Hibernate has much better ways to customize the generated DDL using techniques that result in portable behavior across different databases.
77,Here we see four different ways to use the @Column annotation:
77,@Entity
77,"@Table(name=""Books"")"
77,"@SecondaryTable(name=""Editions"")"
77,class Book {
77,@Id @GeneratedValue
77,"@Column(name=""bookId"") // customize column name"
77,Long id;
77,"@Column(length=100, nullable=false) // declare column as VARCHAR(100) NOT NULL"
77,String title;
77,"@Column(length=17, unique=true, nullable=false) // declare column as VARCHAR(17) NOT NULL UNIQUE"
77,String isbn;
77,"@Column(table=""Editions"", updatable=false) // column belongs to the secondary table, and is never updated"
77,int edition;
77,We don’t use @Column to map associations.
77,4.7. Mapping associations to foreign key columns
77,The @JoinColumn annotation is used to customize a foreign key column.
77,Table 27. @JoinColumn annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped foreign key column
77,table
77,The name of the table to which this column belongs
77,referencedColumnName
77,The name of the column to which the mapped foreign key column refers
77,unique
77,Whether the column has a UNIQUE constraint
77,nullable
77,Whether the column has a NOT NULL constraint
77,insertable
77,Whether the column should appear in generated SQL INSERT statements
77,updatable
77,Whether the column should appear in generated SQL UPDATE statements
77,columnDefinition 💀
77,A DDL fragment that should be used to declare the column
77,foreignKey
77,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
77,A foreign key column doesn’t necessarily have to refer to the primary key of the referenced table.
77,"It’s quite acceptable for the foreign key to refer to any other unique key of the referenced entity, even to a unique key of a secondary table."
77,Here we see how to use @JoinColumn to define a @ManyToOne association mapping a foreign key column which refers to the @NaturalId of Book:
77,@Entity
77,"@Table(name=""Items"")"
77,class Item {
77,...
77,@ManyToOne(optional=false)
77,// implies nullable=false
77,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"","
77,// a reference to a non-PK column
77,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn"")) // supply a name for the FK constraint"
77,Book book;
77,...
77,In case this is confusing:
77,"bookIsbn is the name of the foreign key column in the Items table,"
77,"it refers to a unique key isbn in the Books table, and"
77,it has a foreign key constraint named ItemsToBooksBySsn.
77,Note that the foreignKey member is completely optional and only affects DDL generation.
77,"If you don’t supply an explicit name using @ForeignKey, Hibernate will generate a quite ugly name."
77,"The reason for this is that the maximum length of foreign key names on some databases is extremely constrained, and we need to avoid collisions."
77,"To be fair, this is perfectly fine if you’re only using the generated DDL for testing."
77,For composite foreign keys we might have multiple @JoinColumn annotations:
77,@Entity
77,"@Table(name=""Items"")"
77,class Item {
77,...
77,@ManyToOne(optional=false)
77,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"")"
77,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")"
77,Book book;
77,...
77,"If we need to specify the @ForeignKey, this starts to get a bit messy:"
77,@Entity
77,"@Table(name=""Items"")"
77,class Item {
77,...
77,@ManyToOne(optional=false)
77,"@JoinColumns(value = {@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn""),"
77,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")},"
77,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn""))"
77,Book book;
77,...
77,"For associations mapped to a @JoinTable, fetching the association requires two joins, and so we must declare the @JoinColumns inside the @JoinTable annotation:"
77,@Entity
77,class Book {
77,@Id @GeneratedValue
77,Long id;
77,@ManyToMany
77,"@JoinTable(joinColumns=@JoinColumn(name=""bookId""),"
77,"inverseJoinColumns=@joinColumn(name=""authorId""),"
77,"foreignKey=@ForeignKey(name=""BooksToAuthors""))"
77,Set<Author> authors;
77,...
77,"Again, the foreignKey member is optional."
77,4.8. Mapping primary key joins between tables
77,The @PrimaryKeyJoinColumn is a special-purpose annotation for mapping:
77,"the primary key column of a @SecondaryTable—which is also a foreign key referencing the primary table, or"
77,the primary key column of the primary table mapped by a subclass in a JOINED inheritance hierarchy—which is also a foreign key referencing the primary table mapped by the root entity.
77,Table 28. @PrimaryKeyJoinColumn annotation members
77,Annotation member
77,Purpose
77,name
77,The name of the mapped foreign key column
77,referencedColumnName
77,The name of the column to which the mapped foreign key column refers
77,columnDefinition 💀
77,A DDL fragment that should be used to declare the column
77,foreignKey
77,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
77,"When mapping a subclass table primary key, we place the @PrimaryKeyJoinColumn annotation on the entity class:"
77,@Entity
77,"@Table(name=""People"")"
77,@Inheritance(strategy=JOINED)
77,class Person { ... }
77,@Entity
77,"@Table(name=""Authors"")"
77,"@PrimaryKeyJoinColumn(name=""personId"") // the primary key of the Authors table"
77,class Author { ... }
77,"But to map a secondary table primary key, the @PrimaryKeyJoinColumn annotation must occur inside the @SecondaryTable annotation:"
77,@Entity
77,"@Table(name=""Books"")"
77,"@SecondaryTable(name=""Editions"","
77,"pkJoinColumns = @PrimaryKeyJoinColumn(name=""bookId"")) // the primary key of the Editions table"
77,class Book {
77,@Id @GeneratedValue
77,"@Column(name=""bookId"") // the name of the primary key of the Books table"
77,Long id;
77,...
77,4.9. Column lengths and adaptive column types
77,Hibernate automatically adjusts the column type used in generated DDL based on the column length specified by the @Column annotation.
77,"So we don’t usually need to explicitly specify that a column should be of type TEXT or CLOB—or worry about the parade of TINYTEXT, MEDIUMTEXT, TEXT, LONGTEXT types on MySQL—because Hibernate will automatically select one of those types if required to accommodate a string of the length we specify."
77,The constant values defined in the class Length are very helpful here:
77,Table 29. Predefined column lengths
77,Constant
77,Value
77,Description
77,DEFAULT
77,255
77,The default length of a VARCHAR or VARBINARY column when none is explicitly specified
77,LONG
77,32600
77,The largest column length for a VARCHAR or VARBINARY that is allowed on every database Hibernate supports
77,LONG16
77,32767
77,The maximum length that can be represented using 16 bits (but this length is too large for a VARCHAR or VARBINARY column on for some database)
77,LONG32
77,2147483647
77,The maximum length for a Java string
77,We can use these constants in the @Column annotation:
77,@Column(length=LONG)
77,String text;
77,@Column(length=LONG32)
77,byte[] binaryData;
77,This is usually all you need to do to make use of large object types in Hibernate.
77,4.10. LOBs
77,JPA provides a @Lob annotation which specifies that a field should be persisted as a BLOB or CLOB.
77,Semantics of the @Lob annotation
77,What the spec actually says is that the field should be persisted
77,…​as a large object to a database-supported large object type.
77,"It’s quite unclear what this means, and the spec goes on to say that"
77,…​the treatment of the Lob annotation is provider-dependent…​
77,which doesn’t help much.
77,Hibernate interprets this annotation in what we think is the most reasonable way.
77,"In Hibernate, an attribute annotated @Lob will be written to JDBC using the setClob() or setBlob() method of PreparedStatement, and will be read from JDBC using the getClob() or getBlob() method of ResultSet."
77,"Now, the use of these JDBC methods is usually unnecessary!"
77,JDBC drivers are perfectly capable of converting between String and CLOB or between byte[] and BLOB.
77,"So unless you specifically need to use these JDBC LOB APIs, you don’t need the @Lob annotation."
77,"Instead, as we just saw in Column lengths and adaptive column types, all you need is to specify a large enough column length to accommodate the data you plan to write to that column."
77,"Unfortunately, the driver for PostgreSQL doesn’t allow BYTEA or TEXT columns to be read via the JDBC LOB APIs."
77,This limitation of the Postgres driver has resulted in a whole cottage industry of bloggers and stackoverflow question-answerers recommending convoluted ways to hack the Hibernate Dialect for Postgres to allow an attribute annotated @Lob to be written using setString() and read using getString().
77,But simply removing the @Lob annotation has exactly the same effect.
77,Conclusion:
77,"on PostgreSQL, @Lob always means the OID type,"
77,"@Lob should never be used to map columns of type BYTEA or TEXT, and"
77,please don’t believe everything you read on stackoverflow.
77,"Finally, as an alternative, Hibernate lets you declare an attribute of type java.sql.Blob or java.sql.Clob."
77,@Entity
77,class Book {
77,...
77,Clob text;
77,Blob coverArt;
77,....
77,"The advantage is that a java.sql.Clob or java.sql.Blob can in principle index up to 263 characters or bytes, much more data than you can fit in a Java String or byte[] array (or in your computer)."
77,"To assign a value to these fields, we’ll need to use a LobHelper."
77,We can get one from the Session:
77,LobHelper helper = session.getLobHelper();
77,book.text = helper.createClob(text);
77,book.coverArt = helper.createBlob(image);
77,"In principle, the Blob and Clob objects provide efficient ways to read or stream LOB data from the server."
77,"Book book = session.find(Book.class, bookId);"
77,"String text = book.text.getSubString(1, textLength);"
77,InputStream bytes = book.images.getBinaryStream();
77,"Of course, the behavior here depends very much on the JDBC driver, and so we really can’t promise that this is a sensible thing to do on your database."
77,4.11. Mapping embeddable types to UDTs or to JSON
77,There’s a couple of alternative ways to represent an embeddable type on the database side.
77,Embeddables as UDTs
77,"First, a really nice option, at least in the case of Java record types, and for databases which support user-defined types (UDTs), is to define a UDT which represents the record type."
77,Hibernate 6 makes this really easy.
77,"Just annotate the record type, or the attribute which holds a reference to it, with the new @Struct annotation:"
77,@Embeddable
77,"@Struct(name=""PersonName"")"
77,"record Name(String firstName, String middleName, String lastName) {}"
77,@Entity
77,class Person {
77,...
77,Name name;
77,...
77,This results in the following UDT:
77,"create type PersonName as (firstName varchar(255), middleName varchar(255), lastName varchar(255))"
77,And the name column of the Author table will have the type PersonName.
77,Embeddables to JSON
77,A second option that’s available is to map the embeddable type to a JSON (or JSONB) column.
77,"Now, this isn’t something we would exactly recommend if you’re defining a data model from scratch, but it’s at least useful for mapping pre-existing tables with JSON-typed columns."
77,"Since embeddable types are nestable, we can map some JSON formats this way, and even query JSON properties using HQL."
77,"At this time, JSON arrays are not supported!"
77,"To map an attribute of embeddable type to JSON, we must annotate the attribute @JdbcTypeCode(SqlTypes.JSON), instead of annotating the embeddable type."
77,But the embeddable type Name should still be annotated @Embeddable if we want to query its attributes using HQL.
77,@Embeddable
77,"record Name(String firstName, String middleName, String lastName) {}"
77,@Entity
77,class Person {
77,...
77,@JdbcTypeCode(SqlTypes.JSON)
77,Name name;
77,...
77,"We also need to add Jackson or an implementation of JSONB—for example, Yasson—to our runtime classpath."
77,To use Jackson we could add this line to our Gradle build:
77,runtimeOnly 'com.fasterxml.jackson.core:jackson-databind:{jacksonVersion}'
77,"Now the name column of the Author table will have the type jsonb, and Hibernate will automatically use Jackson to serialize a Name to and from JSON format."
77,4.12. Summary of SQL column type mappings
77,"So, as we’ve seen, there are quite a few annotations that affect the mapping of Java types to SQL column types in DDL."
77,"Here we summarize the ones we’ve just seen in the second half of this chapter, along with some we already mentioned in earlier chapters."
77,Table 30. Annotations for mapping SQL column types
77,Annotation
77,Interpretation
77,@Enumerated
77,Specify how an enum type should be persisted
77,@Nationalized
77,"Use a nationalized character type: NCHAR, NVARCHAR, or NCLOB"
77,@Lob 💀
77,Use JDBC LOB APIs to read and write the annotated attribute
77,@Array
77,Map a collection to a SQL ARRAY type of the specified length
77,@Struct
77,Map an embeddable to a SQL UDT with the given name
77,@TimeZoneStorage
77,Specify how the time zone information should be persisted
77,@JdbcType or @JdbcTypeCode
77,Use an implementation of JdbcType to map an arbitrary SQL type
77,"In addition, there are some configuration properties which have a global affect on how basic types map to SQL column types:"
77,Table 31. Type mapping settings
77,Configuration property name
77,Purpose
77,hibernate.use_nationalized_character_data
77,Enable use of nationalized character types by default
77,hibernate.type.preferred_boolean_jdbc_type
77,Specify the default SQL column type for mapping boolean
77,hibernate.type.preferred_uuid_jdbc_type
77,Specify the default SQL column type for mapping UUID
77,hibernate.type.preferred_duration_jdbc_type
77,Specify the default SQL column type for mapping Duration
77,hibernate.type.preferred_instant_jdbc_type
77,Specify the default SQL column type for mapping Instant
77,hibernate.timezone.default_storage
77,Specify the default strategy for storing time zone information
77,These are global settings and thus quite clumsy.
77,We recommend against messing with any of these settings unless you have a really good reason for it.
77,There’s one more topic we would like to cover in this chapter.
77,4.13. Mapping to formulas
77,Hibernate lets us map an attribute of an entity to a SQL formula involving columns of the mapped table.
77,"Thus, the attribute is a sort of ""derived"" value."
77,Table 32. Annotations for mapping formulas
77,Annotation
77,Purpose
77,@Formula
77,Map an attribute to a SQL formula
77,@JoinFormula
77,Map an association to a SQL formula
77,@DiscriminatorFormula
77,Use a SQL formula as the discriminator in single table inheritance.
77,For example:
77,@Entity
77,class Order {
77,...
77,"@Column(name = ""sub_total"", scale=2, precision=8)"
77,BigDecimal subTotal;
77,"@Column(name = ""tax"", scale=4, precision=4)"
77,BigDecimal taxRate;
77,"@Formula(""sub_total * (1.0 + tax)"")"
77,BigDecimal totalWithTax;
77,...
77,4.14. Derived Identity
77,"An entity has a derived identity if it inherits part of its primary key from an associated ""parent"" entity."
77,We’ve already met a kind of degenerate case of derived identity when we talked about one-to-one associations with a shared primary key.
77,But a @ManyToOne association may also form part of a derived identity.
77,"That is to say, there could be a foreign key column or columns included as part of the composite primary key."
77,There’s three different ways to represent this situation on the Java side of things:
77,"using @IdClass without @MapsId,"
77,"using @IdClass with @MapsId, or"
77,using @EmbeddedId with @MapsId.
77,Let’s suppose we have a Parent entity class defined as follows:
77,@Entity
77,class Parent {
77,@Id
77,Long parentId;
77,...
77,"The parentId field holds the primary key of the Parent table, which will also form part of the composite primary key of every Child belonging to the Parent."
77,First way
77,"In the first, slightly simpler approach, we define an @IdClass to represent the primary key of Child:"
77,class DerivedId {
77,Long parent;
77,String childId;
77,"// constructors, equals, hashcode, etc"
77,...
77,And a Child entity class with a @ManyToOne association annotated @Id:
77,@Entity
77,@IdClass(DerivedId.class)
77,class Child {
77,@Id
77,String childId;
77,@Id @ManyToOne
77,"@JoinColumn(name=""parentId"")"
77,Parent parent;
77,...
77,"Then the primary key of the Child table comprises the columns (childId,parentId)."
77,Second way
77,"This is fine, but sometimes it’s nice to have a field for each element of the primary key."
77,We may use the @MapsId annotation we met earlier:
77,@Entity
77,@IdClass(DerivedId.class)
77,class Child {
77,@Id
77,Long parentId;
77,@Id
77,String childId;
77,@ManyToOne
77,@MapsId(Child_.PARENT_ID) // typesafe reference to Child.parentId
77,"@JoinColumn(name=""parentId"")"
77,Parent parent;
77,...
77,We’re using the approach we saw previously to refer to the parentId property of Child in a typesafe way.
77,"Note that we must place column mapping information on the association annotated @MapsId, not on the @Id field."
77,We must slightly modify our @IdClass so that field names align:
77,class DerivedId {
77,Long parentId;
77,String childId;
77,"// constructors, equals, hashcode, etc"
77,...
77,Third way
77,The third alternative is to redefine our @IdClass as an @Embeddable.
77,"We don’t actually need to change the DerivedId class, but we do need to add the annotation."
77,@Embeddable
77,class DerivedId {
77,Long parentId;
77,String childId;
77,"// constructors, equals, hashcode, etc"
77,...
77,Then we may use @EmbeddedId in Child:
77,@Entity
77,class Child {
77,@EmbeddedId
77,DerivedId id;
77,@ManyToOne
77,@MapsId(DerivedId_.PARENT_ID) // typesafe reference to DerivedId.parentId
77,"@JoinColumn(name=""parentId"")"
77,Parent parent;
77,...
77,The choice between @IdClass and @EmbeddedId boils down to taste.
77,The @EmbeddedId is perhaps a little DRYer.
77,4.15. Adding constraints
77,Database constraints are important.
77,"Even if you’re sure that your program has no bugs 🧐, it’s probably not the only program with access to the database."
77,Constraints help ensure that different programs (and human administrators) play nicely with each other.
77,"Hibernate adds certain constraints to generated DDL automatically: primary key constraints, foreign key constraints, and some unique constraints."
77,But it’s common to need to:
77,"add additional unique constraints,"
77,"add check constraints, or"
77,customize the name of a foreign key constraint.
77,We’ve already seen how to use @ForeignKey to specify the name of a foreign key constraint.
77,There’s two ways to add a unique constraint to a table:
77,"using @Column(unique=true) to indicate a single-column unique key, or"
77,using the @UniqueConstraint annotation to define a uniqueness constraint on a combination of columns.
77,@Entity
77,"@Table(uniqueConstraints=@UniqueConstraint(columnNames={""title"", ""year"", ""publisher_id""}))"
77,class Book { ... }
77,"This annotation looks a bit ugly perhaps, but it’s actually useful even as documentation."
77,The @Check annotation adds a check constraint to the table.
77,@Entity
77,"@Check(name=""ValidISBN"", constraints=""length(isbn)=13"")"
77,class Book { ... }
77,The @Check annotation is commonly used at the field level:
77,"@Id @Check(constraints=""length(isbn)=13"")"
77,String isbn;
77,5. Interacting with the database
77,"To interact with the database, that is, to execute queries, or to insert, update, or delete data, we need an instance of one of the following objects:"
77,"a JPA EntityManager,"
77,"a Hibernate Session, or"
77,a Hibernate StatelessSession.
77,"The Session interface extends EntityManager, and so the only difference between the two interfaces is that Session offers a few more operations."
77,"Actually, in Hibernate, every EntityManager is a Session, and you can narrow it like this:"
77,Session session = entityManager.unwrap(Session.class);
77,An instance of Session (or of EntityManager) is a stateful session.
77,It mediates the interaction between your program and the database via a operations on a persistence context.
77,"In this chapter, we’re not going to talk much about StatelessSession."
77,We’ll come back to this very useful API when we talk about performance.
77,What you need to know for now is that a stateless session doesn’t have a persistence context.
77,"Still, we should let you know that some people prefer to use StatelessSession everywhere."
77,"It’s a simpler programming model, and lets the developer interact with the database more directly."
77,"Stateful sessions certainly have their advantages, but they’re more difficult to reason about, and when something goes wrong, the error messages can be more difficult to understand."
77,5.1. Persistence Contexts
77,"A persistence context is a sort of cache; we sometimes call it the ""first-level cache"", to distinguish it from the second-level cache."
77,"For every entity instance read from the database within the scope of a persistence context, and for every new entity made persistent within the scope of the persistence context, the context holds a unique mapping from the identifier of the entity instance to the instance itself."
77,"Thus, an entity instance may be in one of three states with respect to a given persistence context:"
77,"transient — never persistent, and not associated with the persistence context,"
77,"persistent — currently associated with the persistence context, or"
77,"detached — previously persistent in another session, but not currently associated with this persistence context."
77,"At any given moment, an instance may be associated with at most one persistence context."
77,"The lifetime of a persistence context usually corresponds to the lifetime of a transaction, though it’s possible to have a persistence context that spans several database-level transactions that form a single logical unit of work."
77,"A persistence context—that is, a Session or EntityManager—absolutely positively must not be shared between multiple threads or between concurrent transactions."
77,"If you accidentally leak a session across threads, you will suffer."
77,Container-managed persistence contexts
77,"In a container environment, the lifecycle of a persistence context scoped to the transaction will usually be managed for you."
77,There are several reasons we like persistence contexts.
77,"They help avoid data aliasing: if we modify an entity in one section of code, then other code executing within the same persistence context will see our modification."
77,"They enable automatic dirty checking: after modifying an entity, we don’t need to perform any explicit operation to ask Hibernate to propagate that change back to the database."
77,"Instead, the change will be automatically synchronized with the database when the session is flushed."
77,They can improve performance by avoiding a trip to the database when a given entity instance is requested repeatedly in a given unit of work.
77,They make it possible to transparently batch together multiple database operations.
77,A persistence context also allows us to detect circularities when performing operations on graphs of entities.
77,"(Even in a stateless session, we need some sort of temporary cache of the entity instances we’ve visited while executing a query.)"
77,"On the other hand, stateful sessions come with some very important restrictions, since:"
77,"persistence contexts aren’t threadsafe, and can’t be shared across threads, and"
77,"a persistence context can’t be reused across unrelated transactions, since that would break the isolation and atomicity of the transactions."
77,"Furthermore, a persistence context holds a hard references to all its entities, preventing them from being garbage collected."
77,"Thus, the session must be discarded once a unit of work is complete."
77,"If you don’t completely understand the previous passage, go back and re-read it until you do."
77,A great deal of human suffering has resulted from users mismanaging the lifecycle of the Hibernate Session or JPA EntityManager.
77,We’ll conclude by noting that whether a persistence context helps or harms the performance of a given unit of work depends greatly on the nature of the unit of work.
77,For this reason Hibernate provides both stateful and stateless sessions.
77,5.2. Creating a session
77,"Sticking with standard JPA-defined APIs, we saw how to obtain an EntityManagerFactory in Configuration using JPA XML."
77,It’s quite unsurprising that we may use this object to create an EntityManager:
77,EntityManager entityManager = entityManagerFactory.createEntityManager();
77,"When we’re finished with the EntityManager, we should explicitly clean it up:"
77,entityManager.close();
77,"On the other hand, if we’re starting from a SessionFactory, as described in Configuration using Hibernate API, we may use:"
77,Session session = sessionFactory.openSession();
77,But we still need to clean up:
77,session.close();
77,Injecting the EntityManager
77,"If you’re writing code for some sort of container environment, you’ll probably obtain the EntityManager by some sort of dependency injection."
77,"For example, in Java (or Jakarta) EE you would write:"
77,@PersistenceContext EntityManager entityManager;
77,"In Quarkus, injection is handled by CDI:"
77,@Inject EntityManager entityManager;
77,"Outside a container environment, we’ll also have to write code to manage database transactions."
77,5.3. Managing transactions
77,"Using JPA-standard APIs, the EntityTransaction interface allows us to control database transactions."
77,The idiom we recommend is the following:
77,EntityManager entityManager = entityManagerFactory.createEntityManager();
77,EntityTransaction tx = entityManager.getTransaction();
77,try {
77,tx.begin();
77,//do some work
77,...
77,tx.commit();
77,catch (Exception e) {
77,if (tx.isActive()) tx.rollback();
77,throw e;
77,finally {
77,entityManager.close();
77,"Using Hibernate’s native APIs we might write something really similar,"
77,"but since this sort of code is extremely tedious, we have a much nicer option:"
77,sessionFactory.inTransaction(session -> {
77,//do the work
77,...
77,});
77,Container-managed transactions
77,"In a container environment, the container itself is usually responsible for managing transactions."
77,"In Java EE or Quarkus, you’ll probably indicate the boundaries of the transaction using the @Transactional annotation."
77,5.4. Operations on the persistence context
77,"Of course, the main reason we need an EntityManager is to do stuff to the database."
77,The following important operations let us interact with the persistence context and schedule modifications to the data:
77,Table 33. Methods for modifying data and managing the persistence context
77,Method name and parameters
77,Effect
77,persist(Object)
77,Make a transient object persistent and schedule a SQL insert statement for later execution
77,remove(Object)
77,Make a persistent object transient and schedule a SQL delete statement for later execution
77,merge(Object)
77,Copy the state of a given detached object to a corresponding managed persistent instance and return
77,the persistent object
77,detach(Object)
77,Disassociate a persistent object from a session without
77,affecting the database
77,clear()
77,Empty the persistence context and detach all its entities
77,flush()
77,"Detect changes made to persistent objects association with the session and synchronize the database state with the state of the session by executing SQL insert, update, and delete statements"
77,"Notice that persist() and remove() have no immediate effect on the database, and instead simply schedule a command for later execution."
77,Also notice that there’s no update() operation for a stateful session.
77,Modifications are automatically detected when the session is flushed.
77,"On the other hand, except for getReference(), the following operations all result in immediate access to the database:"
77,Table 34. Methods for reading and locking data
77,Method name and parameters
77,Effect
77,"find(Class,Object)"
77,Obtain a persistent object given its type and its id
77,"find(Class,Object,LockModeType)"
77,"Obtain a persistent object given its type and its id, requesting the given optimistic or pessimistic lock mode"
77,"getReference(Class,id)"
77,"Obtain a reference to a persistent object given its type and its id, without actually loading its state from the database"
77,getReference(Object)
77,"Obtain a reference to a persistent object with the same identity as the given detached instance, without actually loading its state from the database"
77,refresh(Object)
77,Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database
77,"refresh(Object,LockModeType)"
77,"Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database, requesting the given optimistic or pessimistic lock mode"
77,"lock(Object, LockModeType)"
77,Obtain an optimistic or pessimistic lock on a persistent object
77,Any of these operations might throw an exception.
77,"Now, if an exception occurs while interacting with the database, there’s no good way to resynchronize the state of the current persistence context with the state held in database tables."
77,"Therefore, a session is considered to be unusable after any of its methods throws an exception."
77,The persistence context is fragile.
77,"If you receive an exception from Hibernate, you should immediately close and discard the current session. Open a new session if you need to, but throw the bad one away first."
77,Each of the operations we’ve seen so far affects a single entity instance passed as an argument.
77,But there’s a way to set things up so that an operation will propagate to associated entities.
77,5.5. Cascading persistence operations
77,It’s quite often the case that the lifecycle of a child entity is completely dependent on the lifecycle of some parent.
77,"This is especially common for many-to-one and one-to-one associations, though it’s very rare for many-to-many associations."
77,"For example, it’s quite common to make an Order and all its Items persistent in the same transaction, or to delete a Project and its Filess at once."
77,This sort of relationship is sometimes called a whole/part-type relationship.
77,Cascading is a convenience which allows us to propagate one of the operations listed in Operations on the persistence context from a parent to its children.
77,"To set up cascading, we specify the cascade member of one of the association mapping annotations, usually @OneToMany or @OneToOne."
77,@Entity
77,class Order {
77,...
77,"@OneToMany(mappedby=Item_.ORDER,"
77,"// cascade persist(), remove(), and refresh() from Order to Item"
77,"cascade={PERSIST,REMOVE,REFRESH},"
77,// also remove() orphaned Items
77,orphanRemoval=true)
77,private Set<Item> items;
77,...
77,Orphan removal indicates that an Item should be automatically deleted if it is removed from the set of items belonging to its parent Order.
77,5.6. Proxies and lazy fetching
77,"Our data model is a set of interconnected entities, and in Java our whole dataset would be represented as an enormous interconnected graph of objects."
77,"It’s possible that this graph is disconnected, but more likely it’s connected, or composed of a relatively small number of connected subgraphs."
77,"Therefore, when we retrieve on object belonging to this graph from the database and instantiate it in memory, we simply can’t recursively retrieve and instantiate all its associated entities."
77,"Quite aside from the waste of memory on the VM side, this process would involve a huge number of round trips to the database server, or a massive multidimensional cartesian product of tables, or both."
77,"Instead, we’re forced to cut the graph somewhere."
77,Hibernate solves this problem using proxies and lazy fetching.
77,"A proxy is an object that masquerades as a real entity or collection, but doesn’t actually hold any state, because that state has not yet been fetched from the database."
77,"When you call a method of the proxy, Hibernate will detect the call and fetch the state from the database before allowing the invocation to proceed to the real entity object or collection."
77,Now for the gotchas:
77,Hibernate will only do this for an entity which is currently associated with a persistence context.
77,"Once the session ends, and the persistence context is cleaned up, the proxy is no longer fetchable, and instead its methods throw the hated LazyInitializationException."
77,A round trip to the database to fetch the state of a single entity instance is just about the least efficient way to access data.
77,It almost inevitably leads to the infamous N+1 selects problem we’ll discuss later when we talk about how to optimize association fetching.
77,"We’re getting a bit ahead of ourselves here, but let’s quickly mention the general strategy we recommend to navigate past these gotchas:"
77,All associations should be set fetch=LAZY to avoid fetching extra data when it’s not needed.
77,"As we mentioned earlier, this setting is not the default for @ManyToOne associations, and must be specified explicitly."
77,But strive to avoid writing code which triggers lazy fetching.
77,"Instead, fetch all the data you’ll need upfront at the beginning of a unit of work, using one of the techniques described in Association fetching, usually, using join fetch in HQL or an EntityGraph."
77,It’s important to know that some operations which may be performed with an unfetched proxy don’t require fetching its state from the database.
77,"First, we’re always allowed to obtain its identifier:"
77,"var pubId = entityManager.find(Book.class, bookId).getPublisher().getId(); // does not fetch publisher"
77,"Second, we may create an association to a proxy:"
77,"book.setPublisher(entityManager.getReference(Publisher.class, pubId)); // does not fetch publisher"
77,Sometimes it’s useful to test whether a proxy or collection has been fetched from the database.
77,JPA lets us do this using the PersistenceUnitUtil:
77,boolean authorsFetched = entityManagerFactory.getPersistenceUnitUtil().isLoaded(book.getAuthors());
77,Hibernate has a slightly easier way to do it:
77,boolean authorsFetched = Hibernate.isInitialized(book.getAuthors());
77,"But the static methods of the Hibernate class let us do a lot more, and it’s worth getting a bit familiar them."
77,Of particular interest are the operations which let us work with unfetched collections without fetching their state from the database.
77,"For example, consider this code:"
77,"Book book = session.find(Book.class, bookId);"
77,"// fetch just the Book, leaving authors unfetched"
77,"Author authorRef = session.getReference(Author.class, authorId);"
77,// obtain an unfetched proxy
77,"boolean isByAuthor = Hibernate.contains(book.getAuthors(), authorRef); // no fetching"
77,This code fragment leaves both the set book.authors and the proxy authorRef unfetched.
77,"Finally, Hibernate.initialize() is a convenience method that force-fetches a proxy or collection:"
77,"Book book = session.find(Book.class, bookId);"
77,"// fetch just the Book, leaving authors unfetched"
77,Hibernate.initialize(book.getAuthors());
77,// fetch the Authors
77,"But of course, this code is very inefficient, requiring two trips to the database to obtain data that could in principle be retrieved with just one query."
77,"It’s clear from the discussion above that we need a way to request that an association be eagerly fetched using a database join, thus protecting ourselves from the infamous N+1 selects."
77,One way to do this is by passing an EntityGraph to find().
77,5.7. Entity graphs and eager fetching
77,"When an association is mapped fetch=LAZY, it won’t, by default, be fetched when we call the find() method."
77,We may request that an association be fetched eagerly (immediately) by passing an EntityGraph to find().
77,The JPA-standard API for this is a bit unwieldy:
77,var graph = entityManager.createEntityGraph(Book.class);
77,graph.addSubgraph(Book_.publisher);
77,"Book book = entityManager.find(Book.class, bookId, Map.of(SpecHints.HINT_SPEC_FETCH_GRAPH, graph));"
77,This is untypesafe and unnecessarily verbose.
77,Hibernate has a better way:
77,var graph = session.createEntityGraph(Book.class);
77,graph.addSubgraph(Book_.publisher);
77,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
77,"This code adds a left outer join to our SQL query, fetching the associated Publisher along with the Book."
77,We may even attach additional nodes to our EntityGraph:
77,var graph = session.createEntityGraph(Book.class);
77,graph.addSubgraph(Book_.publisher);
77,graph.addPluralSubgraph(Book_.authors).addSubgraph(Author_.person);
77,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
77,This results in a SQL query with four left outer joins.
77,"In the code examples above, The classes Book_ and Author_ are generated by the JPA Metamodel Generator we saw earlier."
77,They let us refer to attributes of our model in a completely type-safe way.
77,"We’ll use them again, below, when we talk about Criteria queries."
77,JPA specifies that any given EntityGraph may be interpreted in two different ways.
77,A fetch graph specifies exactly the associations that should be eagerly loaded.
77,Any association not belonging to the entity graph is proxied and loaded lazily only if required.
77,A load graph specifies that the associations in the entity graph are to be fetched in addition to the associations mapped fetch=EAGER.
77,"You’re right, the names make no sense."
77,"But don’t worry, if you take our advice, and map your associations fetch=LAZY, there’s no difference between a ""fetch"" graph and a ""load"" graph, so the names don’t matter."
77,JPA even specifies a way to define named entity graphs using annotations.
77,But the annotation-based API is so verbose that it’s just not worth using.
77,5.8. Flushing the session
77,"From time to time, a flush operation is triggered, and the session synchronizes dirty state held in memory—that is, modifications to the state of entities associated with the persistence context—with persistent state held in the database. Of course, it does this by executing SQL INSERT, UPDATE, and DELETE statements."
77,"By default, a flush is triggered:"
77,"when the current transaction commits, for example, when Transaction.commit() is called,"
77,"before execution of a query whose result would be affected by the synchronization of dirty state held in memory, or"
77,when the program directly calls flush().
77,"Notice that SQL statements are not usually executed synchronously by methods of the Session interface like persist() and remove(). If synchronous execution of SQL is desired, the StatelessSession allows this."
77,This behavior can be controlled by explicitly setting the flush mode.
77,"For example, to disable flushes that occur before query execution, call:"
77,entityManager.setFlushMode(FlushModeType.COMMIT);
77,Hibernate allows greater control over the flush mode than JPA:
77,session.setHibernateFlushMode(FlushMode.MANUAL);
77,"Since flushing is a somewhat expensive operation (the session must dirty-check every entity in the persistence context), setting the flush mode to COMMIT can occasionally be a useful optimization."
77,Table 35. Flush modes
77,Hibernate FlushMode
77,JPA FlushModeType
77,Interpretation
77,MANUAL
77,Never flush automatically
77,COMMIT
77,COMMIT
77,Flush before transaction commit
77,AUTO
77,AUTO
77,"Flush before transaction commit, and before execution of a query whose results might be affected by modifications held in memory"
77,ALWAYS
77,"Flush before transaction commit, and before execution of every query"
77,A second way to reduce the cost of flushing is to load entities in read-only mode:
77,"Session.setDefaultReadOnly(false) specifies that all entities loaded by a given session should be loaded in read-only mode by default,"
77,"SelectionQuery.setReadOnly(false) specifies that every entity returned by a given query should be loaded in read-only mode, and"
77,"Session.setReadOnly(Object, false) specifies that a given entity already loaded by the session should be switched to read-only mode."
77,It’s not necessary to dirty-check an entity instance in read-only mode.
77,5.9. Queries
77,Hibernate features three complementary ways to write queries:
77,"the Hibernate Query Language, an extremely powerful superset of JPQL, which abstracts most of the features of modern dialects of SQL,"
77,"the JPA criteria query API, along with extensions, allowing almost any HQL query to be constructed programmatically via a typesafe API, and, of course"
77,"for when all else fails, native SQL queries."
77,5.10. HQL queries
77,A full discussion of the query language would require almost as much text as the rest of this Introduction.
77,"Fortunately, HQL is already described in exhaustive (and exhausting) detail in A Guide to Hibernate Query Language."
77,It doesn’t make sense to repeat that information here.
77,Here we want to see how to execute a query via the Session or EntityManager API.
77,The method we call depends on what kind of query it is:
77,"selection queries return a result list, but do not modify the data, but"
77,"mutation queries modify data, and return the number of modified rows."
77,"Selection queries usually start with the keyword select or from, whereas mutation queries begin with the keyword insert, update, or delete."
77,Table 36. Executing HQL
77,Kind
77,Session method
77,EntityManager method
77,Query execution method
77,Selection
77,"createSelectionQuery(String,Class)"
77,"createQuery(String,Class)"
77,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
77,Mutation
77,createMutationQuery(String)
77,createQuery(String)
77,executeUpdate()
77,So for the Session API we would write:
77,List<Book> matchingBooks =
77,"session.createSelectionQuery(""from Book where title like :titleSearchPattern"", Book.class)"
77,".setParameter(""titleSearchPattern"", titleSearchPattern)"
77,.getResultList();
77,"Or, if we’re sticking to the JPA-standard APIs:"
77,List<Book> matchingBooks =
77,"entityManager.createQuery(""select b from Book b where b.title like :titleSearchPattern"", Book.class)"
77,".setParameter(""titleSearchPattern"", titleSearchPattern)"
77,.getResultList();
77,"The only difference between createSelectionQuery() and createQuery() is that createSelectionQuery() throws an exception if passed an insert, delete, or update."
77,"In the query above, :titleSearchPattern is called a named parameter."
77,We may also identify parameters by a number.
77,These are called ordinal parameters.
77,List<Book> matchingBooks =
77,"session.createSelectionQuery(""from Book where title like ?1"", Book.class)"
77,".setParameter(1, titleSearchPattern)"
77,.getResultList();
77,"When a query has multiple parameters, named parameters tend to be easier to read, even if slightly more verbose."
77,Never concatenate user input with HQL and pass the concatenated string to createSelectionQuery().
77,This would open up the possibility for an attacker to execute arbitrary code on your database server.
77,"If we’re expecting a query to return a single result, we can use getSingleResult()."
77,Book book =
77,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
77,".setParameter(1, isbn)"
77,.getSingleResult();
77,"Or, if we’re expecting it to return at most one result, we can use getSingleResultOrNull()."
77,Book bookOrNull =
77,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
77,".setParameter(1, isbn)"
77,.getSingleResultOrNull();
77,"The difference, of course, is that getSingleResult() throws an exception if there’s no matching row in the database, whereas getSingleResultOrNull() just returns null."
77,"By default, Hibernate dirty checks entities in the persistence context before executing a query, in order to determine if the session should be flushed."
77,"If there are many entities association with the persistence context, then this can be an expensive operation."
77,"To disable this behavior, set the flush mode to COMMIT or MANUAL:"
77,Book bookOrNull =
77,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
77,".setParameter(1, isbn)"
77,.setHibernateFlushMode(MANUAL)
77,.getSingleResult();
77,Setting the flush mode to COMMIT or MANUAL might cause the query to return stale results.
77,"Occasionally we need to build a query at runtime, from a set of optional conditions."
77,"For this, JPA offers an API which allows programmatic construction of a query."
77,5.11. Criteria queries
77,"Imagine we’re implementing some sort of search screen, where the user of our system is offered several different ways to constrain the query result set."
77,"For example, we might let them search for books by title and/or the author name."
77,"Of course, we could construct a HQL query by string concatenation, but this is a bit fragile, so it’s quite nice to have an alternative."
77,HQL is implemented in terms of criteria objects
77,"Actually, in Hibernate 6, every HQL query is compiled to a criteria query before being translated to SQL."
77,This ensures that the semantics of HQL and criteria queries are identical.
77,First we need an object for building criteria queries.
77,"Using the JPA-standard APIs, this would be a CriteriaBuilder, and we get it from the EntityManagerFactory:"
77,CriteriaBuilder builder = entityManagerFactory.getCriteriaBuilder();
77,"But if we have a SessionFactory, we get something much better, a HibernateCriteriaBuilder:"
77,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
77,The HibernateCriteriaBuilder extends CriteriaBuilder and adds many operations that JPQL doesn’t have.
77,"If you’re using EntityManagerFactory, don’t despair, you have two perfectly good ways to obtain the HibernateCriteriaBuilder associated with that factory."
77,Either:
77,HibernateCriteriaBuilder builder =
77,entityManagerFactory.unwrap(SessionFactory.class).getCriteriaBuilder();
77,Or simply:
77,HibernateCriteriaBuilder builder =
77,(HibernateCriteriaBuilder) entityManagerFactory.getCriteriaBuilder();
77,We’re ready to create a criteria query.
77,CriteriaQuery<Book> query = builder.createQuery(Book.class);
77,Root<Book> book = query.from(Book.class);
77,Predicate where = builder.conjunction();
77,if (titlePattern != null) {
77,"where = builder.and(where, builder.like(book.get(Book_.title), titlePattern));"
77,if (namePattern != null) {
77,"Join<Book,Author> author = book.join(Book_.author);"
77,"where = builder.and(where, builder.like(author.get(Author_.name), namePattern));"
77,query.select(book).where(where)
77,.orderBy(builder.asc(book.get(Book_.title)));
77,"Here, as before, the classes Book_ and Author_ are generated by Hibernate’s JPA Metamodel Generator."
77,Notice that we didn’t bother treating titlePattern and namePattern as parameters.
77,"That’s safe because, by default, Hibernate automatically and transparently treats strings passed to the CriteriaBuilder as JDBC parameters."
77,Execution of a criteria query works almost exactly like execution of HQL.
77,Table 37. Executing criteria queries
77,Kind
77,Session method
77,EntityManager method
77,Query execution method
77,Selection
77,createSelectionQuery(CriteriaQuery)
77,createQuery(CriteriaQuery)
77,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
77,Mutation
77,createMutationQuery(CriteriaUpdate) or createQuery(CriteriaDelete)
77,createQuery(CriteriaUpdate) or createQuery(CriteriaDelte)
77,executeUpdate()
77,For example:
77,List<Book> matchingBooks =
77,session.createSelectionQuery(query)
77,.getResultList();
77,"Update, insert, and delete queries work similarly:"
77,CriteriaDelete<Book> delete = builder.createCriteriaDelete(Book.class);
77,Root<Book> book = delete.from(Book.class);
77,"delete.where(builder.lt(builder.year(book.get(Book_.publicationDate)), 2000));"
77,session.createMutationQuery(delete).executeUpdate();
77,"It’s even possible to transform a HQL query string to a criteria query, and modify the query programmatically before execution:"
77,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
77,"var query = builder.createQuery(""from Book where year(publicationDate) > 2000"", Book.class);"
77,var root = (Root<Book>) query.getRootList().get(0);
77,"query.where(builder.like(root.get(Book_.title), builder.literal(""Hibernate%"")));"
77,"query.orderBy(builder.asc(root.get(Book_.title)), builder.desc(root.get(Book_.isbn)));"
77,List<Book> matchingBooks = session.createSelectionQuery(query).getResultList();
77,Do you find some of the code above a bit too verbose?
77,We do.
77,5.12. A more comfortable way to write criteria queries
77,"Actually, what makes the JPA criteria API less ergonomic than it should be is the need to call all operations of the CriteriaBuilder as instance methods, instead of having them as static functions."
77,The reason it works this way is that each JPA provider has its own implementation of CriteriaBuilder.
77,Hibernate 6.3 introduces the helper class CriteriaDefinition to reduce the verbosity of criteria queries.
77,Our example looks like this:
77,CriteriaQuery<Book> query =
77,"new CriteriaDefinition(entityManagerFactory, Book.class) {{"
77,select(book);
77,if (titlePattern != null) {
77,"restrict(like(book.get(Book_.title), titlePattern));"
77,if (namePattern != null) {
77,var author = book.join(Book_.author);
77,"restrict(like(author.get(Author_.name), namePattern));"
77,orderBy(asc(book.get(Book_.title)));
77,}};
77,"When all else fails, and sometimes even before that, we’re left with the option of writing a query in SQL."
77,5.13. Native SQL queries
77,"HQL is a powerful language which helps reduce the verbosity of SQL, and significantly increases portability of queries between databases."
77,"But ultimately, the true value of ORM is not in avoiding SQL, but in alleviating the pain involved in dealing with SQL result sets once we get them back to our Java program."
77,"As we said right up front, Hibernate’s generated SQL is meant to be used in conjunction with handwritten SQL, and native SQL queries are one of the facilities we provide to make that easy."
77,Table 38. Executing SQL
77,Kind
77,Session method
77,EntityManager method
77,Query execution method
77,Selection
77,"createNativeQuery(String,Class)"
77,"createNativeQuery(String,Class)"
77,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
77,Mutation
77,createNativeMutationQuery(String)
77,createNativeQuery(String)
77,executeUpdate()
77,Stored procedure
77,createStoredProcedureCall(String)
77,createStoredProcedureQuery(String)
77,execute()
77,"For the most simple cases, Hibernate can infer the shape of the result set:"
77,Book book =
77,"session.createNativeQuery(""select * from Books where isbn = ?1"", Book.class)"
77,.getSingleResult();
77,String title =
77,"session.createNativeQuery(""select title from Books where isbn = ?1"", String.class)"
77,.getSingleResult();
77,"However, in general, there isn’t enough information in the JDBC ResultSetMetaData to infer the mapping of columns to entity objects."
77,"So for more complicated cases, you’ll need to use the @SqlResultSetMapping annotation to define a named mapping, and pass the name to createNativeQuery(). This gets fairly messy, so we don’t want to hurt your eyes by showing you an example of it."
77,"By default, Hibernate doesn’t flush the session before execution of a native query."
77,That’s because the session is unaware of which modifications held in memory would affect the results of the query.
77,"So if there are any unflushed changes to Books, this query might return stale data:"
77,List<Book> books =
77,"session.createNativeQuery(""select * from Books"")"
77,.getResultList()
77,There’s two ways to ensure the persistence context is flushed before this query is executed.
77,"Either, we could simply force a flush by calling flush() or by setting the flush mode to ALWAYS:"
77,List<Book> books =
77,"session.createNativeQuery(""select * from Books"")"
77,.setHibernateFlushMode(ALWAYS)
77,.getResultList()
77,"Or, alternatively, we could tell Hibernate which modified state affects the results of the query:"
77,List<Book> books =
77,"session.createNativeQuery(""select * from Books"")"
77,.addSynchronizedEntityClass(Book.class)
77,.getResultList()
77,You can call stored procedures using createStoredProcedureQuery() or createStoredProcedureCall().
77,"5.14. Limits, pagination, and ordering"
77,"If a query might return more results than we can handle at one time, we may specify:"
77,"a limit on the maximum number of rows returned, and,"
77,"optionally, an offset, the first row of an ordered result set to return."
77,The offset is used to paginate query results.
77,There’s two ways to add a limit or offset to a HQL or native SQL query:
77,"using the syntax of the query language itself, for example, offset 10 rows fetch next 20 rows only, or"
77,using the methods setFirstResult() and setMaxResults() of the SelectionQuery interface.
77,"If the limit or offset is parameterized, the second option is simpler."
77,"For example, this:"
77,List<Book> books =
77,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
77,".setParameter(1, titlePattern)"
77,.setMaxResults(MAX_RESULTS)
77,.getResultList();
77,is simpler than:
77,List<Book> books =
77,"session.createSelectionQuery(""from Book where title like ?1 order by title fetch first ?2 rows only"")"
77,".setParameter(1, titlePattern)"
77,".setParameter(2, MAX_RESULTS)"
77,.getResultList();
77,Hibernate’s SelectionQuery has a slightly different way to paginate the query results:
77,List<Book> books =
77,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
77,".setParameter(1, titlePattern)"
77,.setPage(Page.first(MAX_RESULTS))
77,.getResultList();
77,A closely-related issue is ordering.
77,It’s quite common for pagination to be combined with the need to order query results by a field that’s determined at runtime.
77,"So, as an alternative to the HQL order by clause, SelectionQuery offers the ability to specify that the query results should be ordered by one or more fields of the entity type returned by the query:"
77,List<Book> books =
77,"session.createSelectionQuery(""from Book where title like ?1"")"
77,".setParameter(1, titlePattern)"
77,".setOrder(List.of(Order.asc(Book._title), Order.asc(Book._isbn)))"
77,.setMaxResults(MAX_RESULTS)
77,.getResultList();
77,"Unfortunately, there’s no way to do this using JPA’s TypedQuery interface."
77,"Table 39. Methods for query limits, pagination, and ordering"
77,Method name
77,Purpose
77,JPA-standard
77,setMaxResults()
77,Set a limit on the number of results returned by a query
77,setFirstResult()
77,Set an offset on the results returned by a query
77,setPage()
77,Set the limit and offset by specifying a Page object
77,setOrder()
77,Specify how the query results should be ordered
77,5.15. Representing projection lists
77,"A projection list is the list of things that a query returns, that is, the list of expressions in the select clause."
77,"Since Java has no tuple types, representing query projection lists in Java has always been a problem for JPA and Hibernate."
77,"Traditionally, we’ve just used Object[] most of the time:"
77,var results =
77,"session.createSelectionQuery(""select isbn, title from Book"", Object[].class)"
77,.getResultList();
77,for (var result : results) {
77,var isbn = (String) result[0];
77,var title = (String) result[1];
77,...
77,This is really a bit ugly.
77,Java’s record types now offer an interesting alternative:
77,"record IsbnTitle(String isbn, String title) {}"
77,var results =
77,"session.createSelectionQuery(""select isbn, title from Book"", IsbnTitle.class)"
77,.getResultList();
77,for (var result : results) {
77,var isbn = result.isbn();
77,var title = result.title();
77,...
77,Notice that we’re able to declare the record right before the line which executes the query.
77,"Now, this is only superficially more typesafe, since the query itself is not checked statically, and so we can’t say it’s objectively better."
77,But perhaps you find it more aesthetically pleasing.
77,"And if we’re going to be passing query results around the system, the use of a record type is much better."
77,The criteria query API offers a much more satisfying solution to the problem.
77,Consider the following code:
77,var builder = sessionFactory.getCriteriaBuilder();
77,var query = builder.createTupleQuery();
77,var book = query.from(Book.class);
77,var bookTitle = book.get(Book_.title);
77,var bookIsbn = book.get(Book_.isbn);
77,var bookPrice = book.get(Book_.price);
77,"query.select(builder.tuple(bookTitle, bookIsbn, bookPrice));"
77,var resultList = session.createSelectionQuery(query).getResultList();
77,for (var result: resultList) {
77,String title = result.get(bookTitle);
77,String isbn = result.get(bookIsbn);
77,BigDecimal price = result.get(bookPrice);
77,...
77,"This code is manifestly completely typesafe, and much better than we can hope to do with HQL."
77,5.16. Named queries
77,The @NamedQuery annotation lets us define a HQL query that is compiled and checked as part of the bootstrap process.
77,"This means we find out about errors in our queries earlier, instead of waiting until the query is actually executed."
77,"We can place the @NamedQuery annotation on any class, even on an entity class."
77,"@NamedQuery(name=""10BooksByTitle"","
77,"query=""from Book where title like :titlePattern order by title fetch first 10 rows only"")"
77,class BookQueries {}
77,"We have to make sure that the class with the @NamedQuery annotation will be scanned by Hibernate, either:"
77,"by adding <class>org.hibernate.example.BookQueries</class> to persistence.xml, or"
77,by calling configuration.addClass(BookQueries.class).
77,"Unfortunately, JPA’s @NamedQuery annotation can’t be placed on a package descriptor."
77,"Therefore, Hibernate provides a very similar annotation, @org.hibernate.annotations.NamedQuery which can be specified at the package level."
77,"If we declare a named query at the package level, we must call:"
77,"configuration.addPackage(""org.hibernate.example"")"
77,so that Hibernate knows where to find it.
77,The @NamedNativeQuery annotation lets us do the same for native SQL queries.
77,"There’s much less advantage to using @NamedNativeQuery, because there is very little that Hibernate can do to validate the correctness of a query written in the native SQL dialect of your database."
77,Table 40. Executing named queries
77,Kind
77,Session method
77,EntityManager method
77,Query execution method
77,Selection
77,"createNamedSelectionQuery(String,Class)"
77,"createNamedQuery(String,Class)"
77,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
77,Mutation
77,createNamedMutationQuery(String)
77,createNamedQuery(String)
77,executeUpdate()
77,We execute our named query like this:
77,List<Book> books =
77,entityManager.createNamedQuery(BookQueries_.QUERY_10_BOOKS_BY_TITLE)
77,".setParameter(""titlePattern"", titlePattern)"
77,.getResultList()
77,"Here, BookQueries_.QUERY_10_BOOKS_BY_TITLE is a constant with value ""10BooksByTitle"", generated by the Metamodel Generator."
77,"Note that the code which executes the named query is not aware of whether the query was written in HQL or in native SQL, making it slightly easier to change and optimize the query later."
77,It’s nice to have our queries checked at startup time.
77,It’s even better to have them checked at compile time.
77,"In Organizing persistence logic, we mentioned that the Metamodel Generator can do that for us, with the help of the @CheckHQL annotation, and we presented that as a reason to use @NamedQuery."
77,"But actually, Hibernate has a separate Query Validator capable of performing compile-time validation of HQL query strings that occur as arguments to createQuery() and friends."
77,"If we use the Query Validator, there’s not much advantage to the use of named queries."
77,5.17. Controlling lookup by id
77,"We can do almost anything via HQL, criteria, or native SQL queries."
77,"But when we already know the identifier of the entity we need, a query can feel like overkill."
77,And queries don’t make efficient use of the second level cache.
77,We met the find() method earlier.
77,It’s the most basic way to perform a lookup by id.
77,"But as we also already saw, it can’t quite do everything."
77,"Therefore, Hibernate has some APIs that streamline certain more complicated lookups:"
77,Table 41. Operations for lookup by id
77,Method name
77,Purpose
77,byId()
77,"Lets us specify association fetching via an EntityGraph, as we saw; also lets us specify some additional options, including how the lookup interacts with the second level cache, and whether the entity should be loaded in read-only mode"
77,byMultipleIds()
77,Lets us load a batch of ids at the same time
77,Batch loading is very useful when we need to retrieve multiple instances of the same entity class by id:
77,var graph = session.createEntityGraph(Book.class);
77,graph.addSubgraph(Book_.publisher);
77,List<Book> books =
77,session.byMultipleIds(Book.class)
77,.withFetchGraph(graph)
77,// control association fetching
77,.withBatchSize(20)
77,// specify an explicit batch size
77,.with(CacheMode.GET)
77,// control interaction with the cache
77,.multiLoad(bookIds);
77,"The given list of bookIds will be broken into batches, and each batch will be fetched from the database in a single select."
77,"If we don’t specify the batch size explicitly, a batch size will be chosen automatically."
77,We also have some operations for working with lookups by natural id:
77,Method name
77,Purpose
77,bySimpleNaturalId()
77,For an entity with just one attribute is annotated @NaturalId
77,byNaturalId()
77,For an entity with multiple attributes are annotated @NaturalId
77,byMultipleNaturalId()
77,Lets us load a batch of natural ids at the same time
77,Here’s how we can retrieve an entity by its composite natural id:
77,Book book =
77,session.byNaturalId(Book.class)
77,".using(Book_.isbn, isbn)"
77,".using(Book_.printing, printing)"
77,.load();
77,"Notice that this code fragment is completely typesafe, again thanks to the Metamodel Generator."
77,5.18. Interacting directly with JDBC
77,From time to time we run into the need to write some code that calls JDBC directly.
77,"Unfortunately, JPA offers no good way to do this, but the Hibernate Session does."
77,session.doWork(connection -> {
77,"try (var callable = connection.prepareCall(""{call myproc(?)}"")) {"
77,"callable.setLong(1, argument);"
77,callable.execute();
77,});
77,"The Connection passed to the work is the same connection being used by the session, and so any work performed using that connection occurs in the same transaction context."
77,"If the work returns a value, use doReturningWork() instead of doWork()."
77,"In a container environment where transactions and database connections are managed by the container, this might not be the easiest way to obtain the JDBC connection."
77,5.19. What to do when things go wrong
77,"Object/relational mapping has been called the ""Vietnam of computer science""."
77,"The person who made this analogy is American, and so one supposes that he meant to imply some kind of unwinnable war."
77,"This is quite ironic, since at the very moment he made this comment, Hibernate was already on the brink of winning the war."
77,"Today, Vietnam is a peaceful country with exploding per-capita GDP, and ORM is a solved problem."
77,"That said, Hibernate is complex, and ORM still presents many pitfalls for the inexperienced, even occasionally for the experienced."
77,Sometimes things go wrong.
77,"In this section we’ll quickly sketch some general strategies for avoiding ""quagmires""."
77,Understand SQL and the relational model.
77,Know the capabilities of your RDBMS.
77,Work closely with the DBA if you’re lucky enough to have one.
77,"Hibernate is not about ""transparent persistence"" for Java objects."
77,It’s about making two excellent technologies work smoothly together.
77,Log the SQL executed by Hibernate.
77,You cannot know that your persistence logic is correct until you’ve actually inspected the SQL that’s being executed.
77,"Even when everything seems to be ""working"", there might be a lurking N+1 selects monster."
77,Be careful when modifying bidirectional associations.
77,"In principle, you should update both ends of the association."
77,"But Hibernate doesn’t strictly enforce that, since there are some situations where such a rule would be too heavy-handed."
77,"Whatever the case, it’s up to you to maintain consistency across your model."
77,Never leak a persistence context across threads or concurrent transactions.
77,Have a strategy or framework to guarantee this never happens.
77,"When running queries that return large result sets, take care to consider the size of the session cache."
77,Consider using a stateless session.
77,"Think carefully about the semantics of the second-level cache, and how the caching policies impact transaction isolation."
77,Avoid fancy bells and whistles you don’t need.
77,"Hibernate is incredibly feature-rich, and that’s a good thing, because it serves the needs of a huge number of users, many of whom have one or two very specialized needs."
77,But nobody has all those specialized needs.
77,"In all probability, you have none of them."
77,"Write your domain model in the simplest way that’s reasonable, using the simplest mapping strategies that make sense."
77,"When something isn’t behaving as you expect, simplify."
77,Isolate the problem.
77,"Find the absolute minimum test case which reproduces the behavior, before asking for help online."
77,"Most of the time, the mere act of isolating the problem will suggest an obvious solution."
77,"Avoid frameworks and libraries that ""wrap"" JPA."
77,"If there’s any one criticism of Hibernate and ORM that sometimes does ring true, it’s that it takes you too far from direct control over JDBC."
77,An additional layer just takes you even further.
77,Avoid copy/pasting code from random bloggers or stackoverflow reply guys.
77,"Many of the suggestions you’ll find online just aren’t the simplest solution, and many aren’t correct for Hibernate 6."
77,"Instead, understand what you’re doing; study the Javadoc of the APIs you’re using; read the JPA specification; follow the advice we give in this document; go direct to the Hibernate team on Zulip."
77,"(Sure, we can be a bit cantankerous at times, but we do always want you to be successful.)"
77,Always consider other options.
77,You don’t have to use Hibernate for everything.
77,6. Compile-time tooling
77,The Metamodel Generator is a standard part of JPA.
77,"We’ve actually already seen its handiwork in the code examples earlier: it’s the author of the class Book_, which contains the static metamodel of the entity class Book."
77,The Metamodel Generator
77,Hibernate’s Metamodel Generator is an annotation processor that produces what JPA calls a static metamodel.
77,"That is, it produces a typed model of the persistent classes in our program, giving us a type-safe way to refer to their attributes in Java code."
77,"In particular, it lets us specify entity graphs and criteria queries in a completely type-safe way."
77,The history behind this thing is quite interesting.
77,"Back when Java’s annotation processing API was brand spankin' new, the static metamodel for JPA was proposed by Gavin King for inclusion in JPA 2.0, as a way to achieve type safety in the nascent criteria query API."
77,"It’s fair to say that, back in 2010, this API was not a runaway success."
77,"Tools did not, at the time, feature robust support for annotation processors."
77,And all the explicit generic types made user code quite verbose and difficult to read.
77,(The need for an explicit reference to a CriteriaBuilder instance also contributed verbosity to the criteria API.)
77,"For years, Gavin counted this as one of his more embarrassing missteps."
77,But time has been kind to the static metamodel.
77,"In 2023, all Java compilers, build tools, and IDEs have robust support for annotation processing, and Java’s local type inference (the var keyword) eliminates the verbose generic types."
77,"JPA’s CriteriaBuilder and EntityGraph APIs are still not quite perfect, but the imperfections aren’t related to static type safety or annotation processing."
77,The static metamodel itself is undeniably useful and elegant.
77,"And so now, in Hibernate 6.3, we’re finally ready to go new places with the Metamodel Generator."
77,And it turns out that there’s quite a lot of unlocked potential there.
77,"Now, you still don’t have to use the Metamodel Generator with Hibernate—the APIs we just mentioned still also accept plain strings—but we find that it works well with Gradle and integrates smoothly with our IDE, and the advantage in type-safety is compelling."
77,We’ve already seen how to set up the annotation processor in the Gradle build we saw earlier.
77,"For more details on how to integrate the Metamodel Generator, check out the Static Metamodel Generator section in the User Guide."
77,"Here’s an example of the sort of code that’s generated for an entity class, as mandated by the JPA specification:"
77,Generated Code
77,@StaticMetamodel(Book.class)
77,public abstract class Book_ {
77,/**
77,* @see org.example.Book#isbn
77,**/
77,"public static volatile SingularAttribute<Book, String> isbn;"
77,/**
77,* @see org.example.Book#text
77,**/
77,"public static volatile SingularAttribute<Book, String> text;"
77,/**
77,* @see org.example.Book#title
77,**/
77,"public static volatile SingularAttribute<Book, String> title;"
77,/**
77,* @see org.example.Book#type
77,**/
77,"public static volatile SingularAttribute<Book, Type> type;"
77,/**
77,* @see org.example.Book#publicationDate
77,**/
77,"public static volatile SingularAttribute<Book, LocalDate> publicationDate;"
77,/**
77,* @see org.example.Book#publisher
77,**/
77,"public static volatile SingularAttribute<Book, Publisher> publisher;"
77,/**
77,* @see org.example.Book#authors
77,**/
77,"public static volatile SetAttribute<Book, Author> authors;"
77,"public static final String ISBN = ""isbn"";"
77,"public static final String TEXT = ""text"";"
77,"public static final String TITLE = ""title"";"
77,"public static final String TYPE = ""type"";"
77,"public static final String PUBLICATION_DATE = ""publicationDate"";"
77,"public static final String PUBLISHER = ""publisher"";"
77,"public static final String AUTHORS = ""authors"";"
77,"For each attribute of the entity, the Book_ class has:"
77,"a String-valued constant like TITLE , and"
77,a typesafe reference like title to a metamodel object of type Attribute.
77,We’ve already been using metamodel references like Book_.authors and Book.AUTHORS in the previous chapters.
77,So now let’s see what else the Metamodel Generator can do for us.
77,"The Metamodel Generator provides statically-typed access to elements of the JPA Metamodel. But the Metamodel is also accessible in a ""reflective"" way, via the EntityManagerFactory."
77,EntityType<Book> book = entityManagerFactory.getMetamodel().entity(Book.class);
77,"SingularAttribute<Book,Long> id = book.getDeclaredId(Long.class)"
77,This is very useful for writing generic code in frameworks or libraries.
77,"For example, you could use it to create your own criteria query API."
77,"Automatic generation of finder methods and query methods is a new feature of Hibernate’s implementation of the Metamodel Generator, and an extension to the functionality defined by the JPA specification."
77,"In this chapter, we’re going to explore these features."
77,The functionality described in the rest of this chapter depends on the use of the annotations described in Entities.
77,"The Metamodel Generator is not currently able to generate finder methods and query methods for entities declared completely in XML, and it’s not able to validate HQL which queries such entities."
77,"(On the other hand, the O/R mappings may be specified in XML, since they’re not needed by the Metamodel Generator.)"
77,We’re going to meet three different kinds of generated method:
77,"a named query method has its signature and implementation generated directly from a @NamedQuery annotation,"
77,"a query method has a signature that’s explicitly declared, and a generated implementation which executes a HQL or SQL query specified via a @HQL or @SQL annotation, and"
77,"a finder method annotated @Find has a signature that’s explicitly declared, and a generated implementation inferred from the parameter list."
77,"To whet our appetites, let’s see how this works for a @NamedQuery."
77,6.1. Named queries and the Metamodel Generator
77,"The very simplest way to generate a query method is to put a @NamedQuery annotation anywhere we like, with a name beginning with the magical character #."
77,Let’s just stick it on the Book class:
77,@CheckHQL // validate the query at compile time
77,"@NamedQuery(name = ""#findByTitleAndType"","
77,"query = ""select book from Book book where book.title like :titlen and book.type = :type"")"
77,@Entity
77,public class Book { ... }
77,Now the Metamodel Generator adds the following method declaration to the metamodel class Book_.
77,Generated Code
77,/**
77,* Execute named query {@value #QUERY_FIND_BY_TITLE_AND_TYPE} defined by annotation of {@link Book}.
77,**/
77,"public static List<Book> findByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
77,return entityManager.createNamedQuery(QUERY_FIND_BY_TITLE_AND_TYPE)
77,".setParameter(""titlePattern"", title)"
77,".setParameter(""type"", type)"
77,.getResultList();
77,"We can easily call this method from wherever we like, as long as we have access to an EntityManager:"
77,List<Book> books =
77,"Book_.findByTitleAndType(entityManager, titlePattern, Type.BOOK);"
77,"Now, this is quite nice, but it’s a bit inflexible in various ways, and so this probably isn’t the best way to generate a query method."
77,6.2. Generated query methods
77,The principal problem with generating the query method straight from the @NamedQuery annotation is that it doesn’t let us explicitly specify the return type or parameter list.
77,"In the case we just saw, the Metamodel Generator does a reasonable job of inferring the query return type and parameter types, but we’re often going to need a bit more control."
77,"The solution is to write down the signature of the query method explicitly, as an abstract method in Java."
77,"We’ll need a place to put this method, and since our Book entity isn’t an abstract class, we’ll just introduce a new interface for this purpose:"
77,interface Queries {
77,"@HQL(""where title like :title and type = :type"")"
77,"List<Book> findBooksByTitleAndType(String title, String type);"
77,"Instead of @NamedQuery, which is a type-level annotation, we specify the HQL query using the new @HQL annotation, which we place directly on the query method."
77,This results in the following generated code in the Queries_ class:
77,Generated Code
77,@StaticMetamodel(Queries.class)
77,public abstract class Queries_ {
77,/**
77,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
77,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
77,**/
77,"public static List<Book> findBooksByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
77,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
77,".setParameter(""title"", title)"
77,".setParameter(""type"", type)"
77,.getResultList();
77,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
77,"""where title like :title and type = :type"";"
77,Notice that the signature differs just slightly from the one we wrote down in the Queries interface: the Metamodel Generator has prepended a parameter accepting EntityManager to the parameter list.
77,"If we want to explicitly specify the name and type of this parameter, we may declare it explicitly:"
77,interface Queries {
77,"@HQL(""where title like :title and type = :type"")"
77,"List<Book> findBooksByTitleAndType(StatelessSession session, String title, String type);"
77,"The Metamodel Generator defaults to using EntityManager as the session type, but other types are allowed:"
77,"Session,"
77,"StatelessSession, or"
77,Mutiny.Session from Hibernate Reactive.
77,The real value of all this is in the checks which can now be done at compile time.
77,"The Metamodel Generator verifies that the parameters of our abstract method declaration match the parameters of the HQL query, for example:"
77,"for a named parameter :alice, there must be a method parameter named alice with exactly the same type, or"
77,"for an ordinal parameter ?2, the second method parameter must have exactly the same type."
77,"The query must also be syntactically legal and semantically well-typed, that is, the entities, attributes, and functions referenced in the query must actually exist and have compatible types."
77,The Metamodel Generator determines this by inspecting the annotations of the entity classes at compile time.
77,The @CheckHQL annotation which instructs Hibernate to validate named queries is not necessary for query methods annotated @HQL.
77,The @HQL annotation has a friend named @SQL which lets us specify a query written in native SQL instead of in HQL.
77,In this case there’s a lot less the Metamodel Generator can do to check that the query is legal and well-typed.
77,We imagine you’re wondering whether a static method is really the right thing to use here.
77,6.3. Generating query methods as instance methods
77,One thing not to like about what we’ve just seen is that we can’t transparently replace a generated static function of the Queries_ class with an improved handwritten implementation without impacting clients.
77,"Now, if our query is only called in one place, which is quite common, this isn’t going to be a big issue, and so we’re inclined to think the static function is fine."
77,"But if this function is called from many places, it’s probably better to promote it to an instance method of some class or interface."
77,"Fortunately, this is straightforward."
77,All we need to do is add an abstract getter method for the session object to our Queries interface.
77,(And remove the session from the method parameter list.)
77,We may call this method anything we like:
77,interface Queries {
77,EntityManager entityManager();
77,"@HQL(""where title like :title and type = :type"")"
77,"List<Book> findBooksByTitleAndType(String title, String type);"
77,"Here we’ve used EntityManager as the session type, but other types are allowed, as we saw above."
77,Now the Metamodel Generator does something a bit different:
77,Generated Code
77,@StaticMetamodel(Queries.class)
77,public class Queries_ implements Queries {
77,private final @Nonnull EntityManager entityManager;
77,public Queries_(@Nonnull EntityManager entityManager) {
77,this.entityManager = entityManager;
77,public @Nonnull EntityManager entityManager() {
77,return entityManager;
77,/**
77,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
77,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
77,**/
77,@Override
77,"public List<Book> findBooksByTitleAndType(String title, Type type) {"
77,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
77,".setParameter(""title"", title)"
77,".setParameter(""type"", type)"
77,.getResultList();
77,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
77,"""where title like :title and type = :type"";"
77,"The generated class Queries_ now implements the Queries interface, and the generated query method implements our abstract method directly."
77,"Of course, the protocol for calling the query method has to change:"
77,Queries queries = new Queries_(entityManager);
77,"List<Book> books = queries.findByTitleAndType(titlePattern, Type.BOOK);"
77,"If we ever need to swap out the generated query method with one we write by hand, without impacting clients, all we need to do is replace the abstract method with a default method of the Queries interface."
77,For example:
77,interface Queries {
77,EntityManager entityManager();
77,// handwritten method replacing previous generated implementation
77,"default List<Book> findBooksByTitleAndType(String title, String type) {"
77,entityManager()
77,".createQuery(""where title like :title and type = :type"", Book.class)"
77,".setParameter(""title"", title)"
77,".setParameter(""type"", type)"
77,.setFlushMode(COMMIT)
77,.setMaxResults(100)
77,.getResultList();
77,What if we would like to inject a Queries object instead of calling its constructor directly?
77,"As you recall, we don’t think these things really need to be container-managed objects."
77,"But if you want them to be—if you’re allergic to calling constructors, for some reason—then:"
77,"placing jakarta.inject on the build path will cause an @Inject annotation to be added to the constructor of Queries_, and"
77,placing jakarta.enterprise.context on the build path will cause a @Dependent annotation to be added to the Queries_ class.
77,"Thus, the generated implementation of Queries will be a perfectly functional CDI bean with no extra work to be done."
77,Is the Queries interface starting to look a lot like a DAO-style repository object?
77,"Well, perhaps."
77,You can certainly decide to use this facility to create a BookRepository if that’s what you prefer.
77,"But unlike a repository, our Queries interface:"
77,"doesn’t attempt to hide the EntityManager from its clients,"
77,"doesn’t implement or extend any framework-provided interface or abstract class, at least not unless you want to create such a framework yourself, and"
77,isn’t restricted to service a particular entity class.
77,We can have as many or as few interfaces with query methods as we like.
77,There’s no one-one-correspondence between these interfaces and entity types.
77,"This approach is so flexible that we don’t even really know what to call these ""interfaces with query methods""."
77,6.4. Generated finder methods
77,"At this point, one usually begins to question whether it’s even necessary to write a query at all."
77,Would it be possible to just infer the query from the method signature?
77,"In some simple cases it’s indeed possible, and this is the purpose of finder methods."
77,A finder method is a method annotated @Find.
77,For example:
77,@Find
77,Book getBook(String isbn);
77,A finder method may have multiple parameters:
77,@Find
77,"List<Book> getBooksByTitle(String title, Type type);"
77,The name of the finder method is arbitrary and carries no semantics.
77,But:
77,"the return type determines the entity class to be queried, and"
77,"the parameters of the method must match the fields of the entity class exactly, by both name and type."
77,"Considering our first example, Book has a persistent field String isbn, so this finder method is legal."
77,"If there were no field named isbn in Book, or if it had a different type, this method declaration would be rejected with a meaningful error at compile time."
77,"Similarly, the second example is legal, since Book has fields String title and Type type."
77,You might notice that our solution to this problem is very different from the approach taken by others.
77,"In DAO-style repository frameworks, you’re asked to encode the semantics of the finder method into the name of the method."
77,"This idea came to Java from Ruby, and we think it doesn’t belong here."
77,"It’s completely unnatural in Java, and by almost any measure other than counting characters it’s objectively worse than just writing the query in a string literal."
77,At least string literals accommodate whitespace and punctuation characters.
77,"Oh and, you know, it’s pretty useful to be able to rename a finder method without changing its semantics. 🙄"
77,The code generated for this finder method depends on what kind of fields match the method parameters:
77,@Id field
77,Uses EntityManager.find()
77,All @NaturalId fields
77,Uses Session.byNaturalId()
77,"Other persistent fields, or a mix of field types"
77,Uses a criteria query
77,"The generated code also depends on what kind of session we have, since the capabilities of stateless sessions, and of reactive sessions, differ slightly from the capabilities of regular stateful sessions."
77,"With EntityManager as the session type, we obtain:"
77,/**
77,* Find {@link Book} by {@link Book#isbn isbn}.
77,* @see org.example.Dao#getBook(String)
77,**/
77,@Override
77,public Book getBook(@Nonnull String isbn) {
77,"return entityManager.find(Book.class, isbn);"
77,/**
77,* Find {@link Book} by {@link Book#title title} and {@link Book#type type}.
77,"* @see org.example.Dao#getBooksByTitle(String,Type)"
77,**/
77,@Override
77,"public List<Book> getBooksByTitle(String title, Type type) {"
77,var builder = entityManager.getEntityManagerFactory().getCriteriaBuilder();
77,var query = builder.createQuery(Book.class);
77,var entity = query.from(Book.class);
77,query.where(
77,title==null
77,? entity.get(Book_.title).isNull()
77,": builder.equal(entity.get(Book_.title), title),"
77,type==null
77,? entity.get(Book_.type).isNull()
77,": builder.equal(entity.get(Book_.type), type)"
77,return entityManager.createQuery(query).getResultList();
77,It’s even possible to match a parameter of a finder method against a property of an associated entity or embeddable.
77,"The natural syntax would be a parameter declaration like String publisher.name, but because that’s not legal Java, we can write it as String publisher$name, taking advantage of a legal Java identifier character that nobody ever uses for anything else:"
77,@Find
77,List<Book> getBooksByPublisherName(String publisher$name);
77,"A finder method may specify fetch profiles, for example:"
77,@Find(namedFetchProfiles=Book_.FETCH_WITH_AUTHORS)
77,Book getBookWithAuthors(String isbn);
77,This lets us declare which associations of Book should be pre-fetched by annotating the Book class.
77,6.5. Paging and ordering
77,"Optionally, a query method may have additional ""magic"" parameters which do not map to query parameters:"
77,Parameter type
77,Purpose
77,Example argument
77,Page
77,Specifies a page of query results
77,Page.first(20)
77,Order<? super E>
77,"Specifies an entity attribute to order by, if E is the entity type returned by the query"
77,Order.asc(Book_.title)
77,List<Order? super E>
77,(or varargs)
77,"Specifies entity attributes to order by, if E is the entity type returned by the query"
77,"List.of(Order.asc(Book_.title), Order.asc(Book_.isbn))"
77,Order<Object[]>
77,"Specifies a column to order by, if the query returns a projection list"
77,Order.asc(1)
77,List<Object[]>
77,(or varargs)
77,"Specifies columns to order by, if the query returns a projection list"
77,"List.of(Order.asc(1), Order.desc(2))"
77,"Thus, if we redefine our earlier query method as follows:"
77,interface Queries {
77,"@HQL(""from Book where title like :title and type = :type"")"
77,"List<Book> findBooksByTitleAndType(String title, Page page, Order<? super Book>... order);"
77,Then we can call it like this:
77,List<Book> books =
77,"Queries_.findBooksByTitleAndType(entityManager, titlePattern, Type.BOOK,"
77,"Page.page(RESULTS_PER_PAGE, page), Order.asc(Book_.isbn));"
77,6.6. Query and finder method return types
77,A query method doesn’t need to return List.
77,It might return a single Book.
77,"@HQL(""where isbn = :isbn"")"
77,Book findBookByIsbn(String isbn);
77,"For a query with a projection list, Object[] or List<Object[]> is permitted:"
77,"@HQL(""select isbn, title from Book where isbn = :isbn"")"
77,Object[] findBookAttributesByIsbn(String isbn);
77,"But when there’s just one item in the select list, the type of that item should be used:"
77,"@HQL(""select title from Book where isbn = :isbn"")"
77,String getBookTitleByIsbn(String isbn);
77,"@HQL(""select local datetime"")"
77,LocalDateTime getServerDateTime();
77,"A query which returns a selection list may have a query method which repackages the result as a record, as we saw in Representing projection lists."
77,"record IsbnTitle(String isbn, String title) {}"
77,"@HQL(""select isbn, title from Book"")"
77,List<IsbnTitle> listIsbnAndTitleForEachBook(Page page);
77,A query method might even return TypedQuery or SelectionQuery:
77,"@HQL(""where title like :title"")"
77,SelectionQuery<Book> findBooksByTitle(String title);
77,"This is extremely useful at times, since it allows the client to further manipulate the query:"
77,List<Book> books =
77,"Queries_.findBooksByTitle(entityManager, titlePattern)"
77,.setOrder(Order.asc(Book_.title))
77,// order the results
77,".setPage(Page.page(RESULTS_PER_PAGE, page))"
77,// return the given page of results
77,.setFlushMode(FlushModeType.COMMIT)
77,// don't flush session before query execution
77,.setReadOnly(true)
77,// load the entities in read-only mode
77,.setCacheStoreMode(CacheStoreMode.BYPASS)
77,// don't cache the results
77,".setComment(""Hello world!"")"
77,// add a comment to the generated SQL
77,.getResultList();
77,"An insert, update, or delete query must return int or void."
77,"@HQL(""delete from Book"")"
77,int deleteAllBooks();
77,"@HQL(""update Book set discontinued = true where isbn = :isbn"")"
77,void discontinueBook(String isbn);
77,"On the other hand, finder methods are currently much more limited."
77,"A finder method must return an entity type like Book, or a list of the entity type, List<Book>, for example."
77,"As you might expect, for a reactive session, all query methods and finder methods must return Uni."
77,6.7. An alternative approach
77,"What if you just don’t like the ideas we’ve presented in this chapter, preferring to call the Session or EntityManager directly, but you still want compile-time validation for HQL?"
77,"Or what if you do like the ideas, but you’re working on a huge existing codebase full of code you don’t want to change?"
77,"Well, there’s a solution for you, too."
77,"The Query Validator is a separate annotation processor that’s capable of type-checking HQL strings, not only in annotations, but even when they occur as arguments to createQuery(), createSelectionQuery(), or createMutationQuery(). It’s even able to check calls to setParameter(), with some restrictions."
77,"The Query Validator works in javac, Gradle, Maven, and the Eclipse Java Compiler."
77,"Unlike the Metamodel Generator, which is a completely bog-standard Java annotation processor based on only standard Java APIs, the Query Validator makes use of internal compiler APIs in javac and ecj. This means it can’t be guaranteed to work in every Java compiler. The current release is known to work in JDK 11 and above, though JDK 15 or above is preferred."
77,7. Tuning and performance
77,Once you have a program up and running using Hibernate to access
77,"the database, it’s inevitable that you’ll find places where performance is"
77,disappointing or unacceptable.
77,"Fortunately, most performance problems are relatively easy to solve with"
77,"the tools that Hibernate makes available to you, as long as you keep a"
77,couple of simple principles in mind.
77,First and most important: the reason you’re using Hibernate is
77,"that it makes things easier. If, for a certain problem, it’s making"
77,"things harder, stop using it. Solve this problem with a different tool"
77,instead.
77,Just because you’re using Hibernate in your program doesn’t mean
77,you have to use it everywhere.
77,Second: there are two main potential sources of performance bottlenecks in
77,a program that uses Hibernate:
77,"too many round trips to the database, and"
77,memory consumption associated with the first-level (session) cache.
77,So performance tuning primarily involves reducing the number of accesses
77,"to the database, and/or controlling the size of the session cache."
77,"But before we get to those more advanced topics, we should start by tuning"
77,the connection pool.
77,7.1. Tuning the connection pool
77,"The connection pool built in to Hibernate is suitable for testing, but isn’t intended for use in production."
77,"Instead, Hibernate supports a range of different connection pools, including our favorite, Agroal."
77,"To select and configure Agroal, you’ll need to set some extra configuration properties, in addition to the settings we already saw in Basic configuration settings."
77,Properties with the prefix hibernate.agroal are passed through to Agroal:
77,# configure Agroal connection pool
77,hibernate.agroal.maxSize 20
77,hibernate.agroal.minSize 10
77,hibernate.agroal.acquisitionTimeout PT1s
77,hibernate.agroal.reapTimeout PT10s
77,"As long as you set at least one property with the prefix hibernate.agroal, the AgroalConnectionProvider will be selected automatically."
77,There’s many to choose from:
77,Table 42. Settings for configuring Agroal
77,Configuration property name
77,Purpose
77,hibernate.agroal.maxSize
77,The maximum number of connections present on the pool
77,hibernate.agroal.minSize
77,The minimum number of connections present on the pool
77,hibernate.agroal.initialSize
77,The number of connections added to the pool when it is started
77,hibernate.agroal.maxLifetime
77,"The maximum amount of time a connection can live, after which it is removed from the pool"
77,hibernate.agroal.acquisitionTimeout
77,"The maximum amount of time a thread can wait for a connection, after which an exception is thrown instead"
77,hibernate.agroal.reapTimeout
77,The duration for eviction of idle connections
77,hibernate.agroal.leakTimeout
77,The duration of time a connection can be held without causing a leak to be reported
77,hibernate.agroal.idleValidationTimeout
77,A foreground validation is executed if a connection has been idle on the pool for longer than this duration
77,hibernate.agroal.validationTimeout
77,The interval between background validation checks
77,hibernate.agroal.initialSql
77,A SQL command to be executed when a connection is created
77,The following settings are common to all connection pools supported by Hibernate:
77,Table 43. Common settings for connection pools
77,hibernate.connection.autocommit
77,The default autocommit mode
77,hibernate.connection.isolation
77,The default transaction isolation level
77,Container-managed datasources
77,"In a container environment, you usually don’t need to configure a connection pool through Hibernate."
77,"Instead, you’ll use a container-managed datasource, as we saw in Basic configuration settings."
77,7.2. Enabling statement batching
77,"An easy way to improve performance of some transactions, with almost no work at all, is to turn on automatic DML statement batching."
77,"Batching only helps in cases where a program executes many inserts, updates, or deletes against the same table in a single transaction."
77,All we need to do is set a single property:
77,Table 44. Enabling JDBC batching
77,Configuration property name
77,Purpose
77,Alternative
77,hibernate.jdbc.batch_size
77,Maximum batch size for SQL statement batching
77,setJdbcBatchSize()
77,"Even better than DML statement batching is the use of HQL update or delete queries, or even native SQL that calls a stored procedure!"
77,7.3. Association fetching
77,Achieving high performance in ORM means minimizing the number of round trips to the database. This goal should be uppermost in your mind whenever you’re writing data access code with Hibernate. The most fundamental rule of thumb in ORM is:
77,"explicitly specify all the data you’re going to need right at the start of a session/transaction, and fetch it immediately in one or two queries,"
77,and only then start navigating associations between persistent entities.
77,"Without question, the most common cause of poorly-performing data access code in Java programs is the problem of N+1 selects."
77,"Here, a list of N rows is retrieved from the database in an initial query, and then associated instances of a related entity are fetched using N subsequent queries."
77,This isn’t a bug or limitation of Hibernate; this problem even affects typical handwritten JDBC code behind DAOs.
77,"Only you, the developer, can solve this problem, because only you know ahead of time what data you’re going to need in a given unit of work."
77,But that’s OK.
77,Hibernate gives you all the tools you need.
77,"In this section we’re going to discuss different ways to avoid such ""chatty"" interaction with the database."
77,Hibernate provides several strategies for efficiently fetching associations and avoiding N+1 selects:
77,"outer join fetching—where an association is fetched using a left outer join,"
77,"batch fetching—where an association is fetched using a subsequent select with a batch of primary keys, and"
77,subselect fetching—where an association is fetched using a subsequent select with keys re-queried in a subselect.
77,"Of these, you should almost always use outer join fetching."
77,But let’s consider the alternatives first.
77,7.4. Batch fetching and subselect fetching
77,Consider the following code:
77,List<Book> books =
77,"session.createSelectionQuery(""from Book order by isbn"", Book.class)"
77,.getResultList();
77,"books.forEach(book -> book.getAuthors().forEach(author -> out.println(book.title + "" by "" + author.name)));"
77,"This code is very inefficient, resulting, by default, in the execution of N+1 select statements, where n is the number of Books."
77,Let’s see how we can improve on that.
77,SQL for batch fetching
77,"With batch fetching enabled, Hibernate might execute the following SQL on PostgreSQL:"
77,/* initial query for Books */
77,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
77,from Book b1_0
77,order by b1_0.isbn
77,/* first batch of associated Authors */
77,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
77,from Book_Author a1_0
77,join Author a1_1 on a1_1.id=a1_0.authors_id
77,where a1_0.books_isbn = any (?)
77,/* second batch of associated Authors */
77,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
77,from Book_Author a1_0
77,join Author a1_1 on a1_1.id=a1_0.authors_id
77,where a1_0.books_isbn = any (?)
77,The first select statement queries and retrieves Books.
77,The second and third queries fetch the associated Authors in batches.
77,The number of batches required depends on the configured batch size.
77,"Here, two batches were required, so two SQL statements were executed."
77,The SQL for batch fetching looks slightly different depending on the database.
77,"Here, on PostgreSQL, Hibernate passes a batch of primary key values as a SQL ARRAY."
77,SQL for subselect fetching
77,"On the other hand, with subselect fetching, Hibernate would execute this SQL:"
77,/* initial query for Books */
77,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
77,from Book b1_0
77,order by b1_0.isbn
77,/* fetch all associated Authors */
77,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
77,from Book_Author a1_0
77,join Author a1_1 on a1_1.id=a1_0.authors_id
77,where a1_0.books_isbn in (select b1_0.isbn from Book b1_0)
77,Notice that the first query is re-executed in a subselect in the second query.
77,"The execution of the subselect is likely to be relatively inexpensive, since the data should already be cached by the database."
77,"Clever, huh?"
77,Enabling the use of batch or subselect fetching
77,"Both batch fetching and subselect fetching are disabled by default, but we may enable one or the other globally using properties."
77,Table 45. Configuration settings to enable batch and subselect fetching
77,Configuration property name
77,Property value
77,Alternatives
77,hibernate.default_batch_fetch_size
77,A sensible batch size >1 to enable batch fetching
77,"@BatchSize(), setFetchBatchSize()"
77,hibernate.use_subselect_fetch
77,true to enable subselect fetching
77,"@Fetch(SUBSELECT), setSubselectFetchingEnabled()"
77,"Alternatively, we can enable one or the other in a given session:"
77,session.setFetchBatchSize(5);
77,session.setSubselectFetchingEnabled(true);
77,We may request subselect fetching more selectively by annotating a collection or many-valued association with the @Fetch annotation.
77,@ManyToMany @Fetch(SUBSELECT)
77,Set<Author> authors;
77,"Note that @Fetch(SUBSELECT) has the same effect as @Fetch(SELECT), except after execution of a HQL or criteria query."
77,"But after query execution, @Fetch(SUBSELECT) is able to much more efficiently fetch associations."
77,"Later, we’ll see how we can use fetch profiles to do this even more selectively."
77,That’s all there is to it.
77,"Too easy, right?"
77,"Sadly, that’s not the end of the story."
77,"While batch fetching might mitigate problems involving N+1 selects, it won’t solve them."
77,The truly correct solution is to fetch associations using joins.
77,Batch fetching (or subselect fetching) can only be the best solution in rare cases where outer join fetching would result in a cartesian product and a huge result set.
77,But batch fetching and subselect fetching have one important characteristic in common: they can be performed lazily.
77,"This is, in principle, pretty convenient."
77,"When we query data, and then navigate an object graph, lazy fetching saves us the effort of planning ahead."
77,It turns out that this is a convenience we’re going to have to surrender.
77,7.5. Join fetching
77,"Outer join fetching is usually the best way to fetch associations, and it’s what we use most of the time."
77,"Unfortunately, by its very nature, join fetching simply can’t be lazy."
77,"So to make use of join fetching, we must plan ahead."
77,Our general advice is:
77,"Avoid the use of lazy fetching, which is often the source of N+1 selects."
77,"Now, we’re not saying that associations should be mapped for eager fetching by default!"
77,"That would be a terrible idea, resulting in simple session operations that fetch almost the entire database."
77,Therefore:
77,Most associations should be mapped for lazy fetching by default.
77,"It sounds as if this tip is in contradiction to the previous one, but it’s not."
77,It’s saying that you must explicitly specify eager fetching for associations precisely when and where they are needed.
77,"If we need eager join fetching in some particular transaction, we have four different ways to specify that."
77,Passing a JPA EntityGraph
77,We’ve already seen this in Entity graphs and eager fetching
77,Specifying a named fetch profile
77,We’ll discuss this approach later in Named fetch profiles
77,Using left join fetch in HQL/JPQL
77,See A Guide to Hibernate Query Language for details
77,Using From.fetch() in a criteria query
77,Same semantics as join fetch in HQL
77,"Typically, a query is the most convenient option."
77,Here’s how we can ask for join fetching in HQL:
77,List<Book> booksWithJoinFetchedAuthors =
77,"session.createSelectionQuery(""from Book join fetch authors order by isbn"")"
77,.getResultList();
77,"And this is the same query, written using the criteria API:"
77,var builder = sessionFactory.getCriteriaBuilder();
77,var query = builder.createQuery(Book.class);
77,var book = query.from(Book.class);
77,book.fetch(Book_.authors);
77,query.select(book);
77,query.orderBy(builder.asc(book.get(Book_.isbn)));
77,List<Book> booksWithJoinFetchedAuthors =
77,session.createSelectionQuery(query).getResultList();
77,"Either way, a single SQL select statement is executed:"
77,"select b1_0.isbn,a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
77,from Book b1_0
77,join (Book_Author a1_0 join Author a1_1 on a1_1.id=a1_0.authors_id)
77,on b1_0.isbn=a1_0.books_isbn
77,order by b1_0.isbn
77,Much better!
77,"Join fetching, despite its non-lazy nature, is clearly more efficient than either batch or subselect fetching, and this is the source of our recommendation to avoid the use of lazy fetching."
77,There’s one interesting case where join fetching becomes inefficient: when we fetch two many-valued associations in parallel.
77,Imagine we wanted to fetch both Author.books and Author.royaltyStatements in some unit of work.
77,"Joining both collections in a single query would result in a cartesian product of tables, and a large SQL result set."
77,"Subselect fetching comes to the rescue here, allowing us to fetch books using a join, and royaltyStatements using a single subsequent select."
77,"Of course, an alternative way to avoid many round trips to the database is to cache the data we need in the Java client."
77,"If we’re expecting to find the associated data in a local cache, we probably don’t need join fetching at all."
77,But what if we can’t be certain that all associated data will be in the cache?
77,"In that case, we might be able to reduce the cost of cache misses by enabling batch fetching."
77,7.6. The second-level cache
77,"A classic way to reduce the number of accesses to the database is to use a second-level cache, allowing"
77,data cached in memory to be shared between sessions.
77,"By nature, a second-level cache tends to undermine the ACID properties of transaction processing in a relational database."
77,We don’t use a distributed transaction with two-phase commit to ensure that changes to the cache and database happen atomically.
77,"So a second-level cache is often by far the easiest way to improve the performance of a system, but only at the cost of making it much more difficult to reason about concurrency."
77,And so the cache is a potential source of bugs which are difficult to isolate and reproduce.
77,"Therefore, by default, an entity is not eligible for storage in the second-level cache."
77,We must explicitly mark each entity that will
77,be stored in the second-level cache with the @Cache annotation from org.hibernate.annotations.
77,But that’s still not enough.
77,"Hibernate does not itself contain an implementation of a second-level cache, so it’s necessary to configure an external cache provider."
77,Caching is disabled by default.
77,"To minimize the risk of data loss, we force you to stop and think before any entity goes into the cache."
77,"Hibernate segments the second-level cache into named regions, one for each:"
77,mapped entity hierarchy or
77,collection role.
77,"For example, there might be separate cache regions for Author, Book, Author.books, and Book.authors."
77,"Each region is permitted its own policies for expiry, persistence, and replication. These policies must be configured externally to Hibernate."
77,"The appropriate policies depend on the kind of data an entity represents. For example, a program might have different caching policies for ""reference"" data, for transactional data, and for data used for analytics. Ordinarily, the implementation of those policies is the responsibility of the underlying cache implementation."
77,7.7. Specifying which data is cached
77,"By default, no data is eligible for storage in the second-level cache."
77,An entity hierarchy or collection role may be assigned a region using the @Cache annotation.
77,"If no region name is explicitly specified, the region name is just the name of the entity class or collection role."
77,@Entity
77,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
77,class Publisher {
77,...
77,"@Cache(usage=READ_WRITE, region=""PublishedBooks"")"
77,@OneToMany(mappedBy=Book_.PUBLISHER)
77,Set<Book> books;
77,...
77,The cache defined by a @Cache annotation is automatically utilized by Hibernate to:
77,"retrieve an entity by id when find() is called, or"
77,to resolve an association by id.
77,The @Cache annotation must be specified on the root class of an entity inheritance hierarchy.
77,It’s an error to place it on a subclass entity.
77,"The @Cache annotation always specifies a CacheConcurrencyStrategy, a policy governing access to the second-level cache by concurrent transactions."
77,Table 46. Cache concurrency
77,Concurrency policy
77,Interpretation
77,Explanation
77,READ_ONLY
77,Immutable data
77,Read-only access
77,"Indicates that the cached object is immutable, and is never updated. If an entity with this cache concurrency is updated, an exception is thrown."
77,"This is the simplest, safest, and best-performing cache concurrency strategy. It’s particularly suitable for so-called ""reference"" data."
77,NONSTRICT_READ_WRITE
77,Concurrent updates are extremely improbable
77,Read/write access with no locking
77,"Indicates that the cached object is sometimes updated, but that it’s extremely unlikely that two transactions will attempt to update the same item of data at the same time."
77,"This strategy does not use locks. When an item is updated, the cache is invalidated both before and after completion of the updating transaction. But without locking, it’s impossible to completely rule out the possibility of a second transaction storing or retrieving stale data in or from the cache during the completion process of the first transaction."
77,READ_WRITE
77,Concurrent updates are possible but not common
77,Read/write access using soft locks
77,Indicates a non-vanishing likelihood that two concurrent transactions attempt to update the same item of data simultaneously.
77,"This strategy uses ""soft"" locks to prevent concurrent transactions from retrieving or storing a stale item from or in the cache during the transaction completion process. A soft lock is simply a marker entry placed in the cache while the updating transaction completes."
77,"A second transaction may not read the item from the cache while the soft lock is present, and instead simply proceeds to read the item directly from the database, exactly as if a regular cache miss had occurred."
77,"Similarly, the soft lock also prevents this second transaction from storing a stale item to the cache when it returns from its round trip to the database with something that might not quite be the latest version."
77,TRANSACTIONAL
77,Concurrent updates are frequent
77,Transactional access
77,"Indicates that concurrent writes are common, and the only way to maintain synchronization between the second-level cache and the database is via the use of a fully transactional cache provider. In this case, the cache and the database must cooperate via JTA or the XA protocol, and Hibernate itself takes on little responsibility for maintaining the integrity of the cache."
77,Which policies make sense may also depend on the underlying second-level cache implementation.
77,"JPA has a similar annotation, named @Cacheable."
77,"Unfortunately, it’s almost useless to us, since:"
77,"it provides no way to specify any information about the nature of the cached entity and how its cache should be managed, and"
77,"it may not be used to annotate associations, and so we can’t even use it to mark collection roles as eligible for storage in the second-level cache."
77,7.8. Caching by natural id
77,"If our entity has a natural id, we can enable an additional cache, which holds cross-references from natural id to primary id, by annotating the entity @NaturalIdCache."
77,"By default, the natural id cache is stored in a dedicated region of the second-level cache, separate from the cached entity data."
77,@Entity
77,"@Cache(usage=READ_WRITE, region=""Book"")"
77,"@NaturalIdCache(region=""BookIsbn"")"
77,class Book {
77,...
77,@NaturalId
77,String isbn;
77,@NaturalId
77,int printing;
77,...
77,This cache is utilized when the entity is retrieved using one of the operations of Session which performs lookup by natural id.
77,"Since the natural id cache doesn’t contain the actual state of the entity, it doesn’t make sense to annotate an entity @NaturalIdCache unless it’s already eligible for storage in the second-level cache, that is, unless it’s also annotated @Cache."
77,"It’s worth noticing that, unlike the primary identifier of an entity, a natural id might be mutable."
77,"We must now consider a subtlety that often arises when we have to deal with so-called ""reference data"", that is, data which fits easily in memory, and doesn’t change much."
77,7.9. Caching and association fetching
77,Let’s consider again our Publisher class:
77,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
77,@Entity
77,class Publisher { ... }
77,"Data about publishers doesn’t change very often, and there aren’t so many of them."
77,Suppose we’ve set everything up so that the publishers are almost always available in the second-level cache.
77,Then in this case we need to think carefully about associations of type Publisher.
77,@ManyToOne
77,Publisher publisher;
77,"There’s no need for this association to be lazily fetched, since we’re expecting it to be available in memory, so we won’t set it fetch=LAZY."
77,"But on the other hand, if we leave it marked for eager fetching then, by default, Hibernate will often fetch it using a join."
77,This places completely unnecessary load on the database.
77,The solution is the @Fetch annotation:
77,@ManyToOne @Fetch(SELECT)
77,Publisher publisher;
77,"By annotating the association @Fetch(SELECT), we suppress join fetching, giving Hibernate a chance to find the associated Publisher in the cache."
77,"Therefore, we arrive at this rule of thumb:"
77,"Many-to-one associations to ""reference data"", or to any other data that will almost always be available in the cache, should be mapped EAGER,SELECT."
77,"Other associations, as we’ve already made clear, should be LAZY."
77,"Once we’ve marked an entity or collection as eligible for storage in the second-level cache, we still need to set up an actual cache."
77,7.10. Configuring the second-level cache provider
77,"Configuring a second-level cache provider is a rather involved topic, and quite outside the scope of this document."
77,"But in case it helps, we often test Hibernate with the following configuration, which uses EHCache as the cache implementation, as above in Optional dependencies:"
77,Table 47. EHCache configuration
77,Configuration property name
77,Property value
77,hibernate.cache.region.factory_class
77,jcache
77,hibernate.javax.cache.uri
77,/ehcache.xml
77,"If you’re using EHCache, you’ll also need to include an ehcache.xml file"
77,that explicitly configures the behavior of each cache region belonging to
77,your entities and collections.
77,You’ll find more information about configuring EHCache here.
77,"We may use any other implementation of JCache, such as Caffeine."
77,JCache automatically selects whichever implementation it finds on the classpath.
77,"If there are multiple implementations on the classpath, we must disambiguate using:"
77,Table 48. Disambiguating the JCache implementation
77,Configuration property name
77,Property value
77,hibernate.javax.cache.provider
77,"The implementation of javax.cache.spiCachingProvider, for example:"
77,org.ehcache.jsr107.EhcacheCachingProvider
77,for EHCache
77,com.github.benmanes.caffeine.jcache.spi.CaffeineCachingProvider
77,for Caffeine
77,"Alternatively, to use Infinispan as the cache implementation, the following settings are required:"
77,Table 49. Infinispan provider configuration
77,Configuration property name
77,Property value
77,hibernate.cache.region.factory_class
77,infinispan
77,hibernate.cache.infinispan.cfg
77,"Path to infinispan configuration file, for example:"
77,org/infinispan/hibernate/cache/commons/builder/infinispan-configs.xml
77,for a distributed cache
77,org/infinispan/hibernate/cache/commons/builder/infinispan-configs-local.xml
77,to test with local cache
77,Infinispan is usually used when distributed caching is required.
77,There’s more about using Infinispan with Hibernate here.
77,"Finally, there’s a way to globally disable the second-level cache:"
77,Table 50. Setting to disable caching
77,Configuration property name
77,Property value
77,hibernate.cache.use_second_level_cache
77,"true to enable caching, or false to disable it"
77,"When hibernate.cache.region.factory_class is set, this property defaults to true."
77,This setting lets us easily disable the second-level cache completely when troubleshooting or profiling performance.
77,You can find much more information about the second-level cache in the User Guide.
77,7.11. Caching query result sets
77,The caches we’ve described above are only used to optimize lookups by id or by natural id.
77,"Hibernate also has a way to cache the result sets of queries, though this is only rarely an efficient thing to do."
77,The query cache must be enabled explicitly:
77,Table 51. Setting to enable the query cache
77,Configuration property name
77,Property value
77,hibernate.cache.use_query_cache
77,true to enable the query cache
77,"To cache the results of a query, call SelectionQuery.setCacheable(true):"
77,"session.createQuery(""from Product where discontinued = false"")"
77,.setCacheable(true)
77,.getResultList();
77,"By default, the query result set is stored in a cache region named default-query-results-region."
77,"Since different queries should have different caching policies, it’s common to explicitly specify a region name:"
77,"session.createQuery(""from Product where discontinued = false"")"
77,.setCacheable(true)
77,".setCacheRegion(""ProductCatalog"")"
77,.getResultList();
77,A result set is cached together with a logical timestamp.
77,"By ""logical"", we mean that it doesn’t actually increase linearly with time, and in particular it’s not the system time."
77,"When a Product is updated, Hibernate does not go through the query cache and invalidate every cached result set that’s affected by the change."
77,"Instead, there’s a special region of the cache which holds a logical timestamp of the most-recent update to each table."
77,"This is called the update timestamps cache, and it’s kept in the region default-update-timestamps-region."
77,It’s your responsibility to ensure that this cache region is configured with appropriate policies.
77,"In particular, update timestamps should never expire or be evicted."
77,"When a query result set is read from the cache, Hibernate compares its timestamp with the timestamp of each of the tables that affect the results of the query, and only returns the result set if the result set isn’t stale."
77,"If the result set is stale, Hibernate goes ahead and re-executes the query against the database and updates the cached result set."
77,"As is generally the case with any second-level cache, the query cache can break the ACID properties of transactions."
77,7.12. Second-level cache management
77,"For the most part, the second-level cache is transparent."
77,"Program logic which interacts with the Hibernate session is unaware of the cache, and is not impacted by changes to caching policies."
77,"At worst, interaction with the cache may be controlled by specifying of an explicit CacheMode:"
77,session.setCacheMode(CacheMode.IGNORE);
77,"Or, using JPA-standard APIs:"
77,entityManager.setCacheRetrieveMode(CacheRetrieveMode.BYPASS);
77,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
77,The JPA-defined cache modes come in two flavors: CacheRetrieveMode and CacheStoreMode.
77,Table 52. JPA-defined cache retrieval modes
77,Mode
77,Interpretation
77,CacheRetrieveMode.USE
77,Read data from the cache if available
77,CacheRetrieveMode.BYPASS
77,Don’t read data from the cache; go direct to the database
77,We might select CacheRetrieveMode.BYPASS if we’re concerned about the possibility of reading stale data from the cache.
77,Table 53. JPA-defined cache storage modes
77,Mode
77,Interpretation
77,CacheStoreMode.USE
77,Write data to the cache when read from the database or when modified; do not update already-cached items when reading
77,CacheStoreMode.REFRESH
77,Write data to the cache when read from the database or when modified; always update cached items when reading
77,CacheStoreMode.BYPASS
77,Don’t write data to the cache
77,We should select CacheStoreMode.BYPASS if we’re querying data that doesn’t need to be cached.
77,It’s a good idea to set the CacheStoreMode to BYPASS just before running a query which returns a large result set full of data that we don’t expect to need again soon.
77,"This saves work, and prevents the newly-read data from pushing out the previously cached data."
77,In JPA we would use this idiom:
77,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
77,List<Publisher> allpubs =
77,"entityManager.createQuery(""from Publisher"", Publisher.class)"
77,.getResultList();
77,entityManager.setCacheStoreMode(CacheStoreMode.USE);
77,But Hibernate has a better way:
77,List<Publisher> allpubs =
77,"session.createSelectionQuery(""from Publisher"", Publisher.class)"
77,.setCacheStoreMode(CacheStoreMode.BYPASS)
77,.getResultList();
77,A Hibernate CacheMode packages a CacheRetrieveMode with a CacheStoreMode.
77,Table 54. Hibernate cache modes and JPA equivalents
77,Hibernate CacheMode
77,Equivalent JPA modes
77,NORMAL
77,"CacheRetrieveMode.USE, CacheStoreMode.USE"
77,IGNORE
77,"CacheRetrieveMode.BYPASS, CacheStoreMode.BYPASS"
77,GET
77,"CacheRetrieveMode.USE, CacheStoreMode.BYPASS"
77,PUT
77,"CacheRetrieveMode.BYPASS, CacheStoreMode.USE"
77,REFRESH
77,"CacheRetrieveMode.REFRESH, CacheStoreMode.BYPASS"
77,There’s no particular reason to prefer Hibernate’s CacheMode over the JPA equivalents.
77,This enumeration only exists because Hibernate had cache modes long before they were added to JPA.
77,"For ""reference"" data, that is, for data which is expected to always be found in the second-level cache, it’s a good idea to prime the cache at startup."
77,There’s a really easy way to do this: just execute a query immediately after obtaining the
77,EntityManager or SessionFactory.
77,SessionFactory sessionFactory =
77,setupHibernate(new Configuration())
77,.buildSessionFactory();
77,// prime the second-level cache
77,sessionFactory.inSession(session -> {
77,"session.createSelectionQuery(""from Country""))"
77,.setReadOnly(true)
77,.getResultList();
77,"session.createSelectionQuery(""from Product where discontinued = false""))"
77,.setReadOnly(true)
77,.getResultList();
77,});
77,"Very occasionally, it’s necessary or advantageous to control the cache explicitly, for example, to evict some data that we know to be stale."
77,The Cache interface allows programmatic eviction of cached items.
77,"sessionFactory.getCache().evictEntityData(Book.class, bookId);"
77,Second-level cache management via the Cache interface is not transaction-aware.
77,"None of the operations of Cache respect any isolation or transactional semantics associated with the underlying caches. In particular, eviction via the methods of this interface causes an immediate ""hard"" removal outside any current transaction and/or locking scheme."
77,"Ordinarily, however, Hibernate automatically evicts or updates cached data after modifications, and, in addition, cached data which is unused will eventually be expired according to the configured policies."
77,This is quite different to what happens with the first-level cache.
77,7.13. Session cache management
77,Entity instances aren’t automatically evicted from the session cache when they’re no longer needed.
77,"Instead, they stay pinned in memory until the session they belong to is discarded by your program."
77,"The methods detach() and clear() allow you to remove entities from the session cache, making them available for garbage collection."
77,"Since most sessions are rather short-lived, you won’t need these operations very often."
77,"And if you find yourself thinking you do need them in a certain situation, you should strongly consider an alternative solution: a stateless session."
77,7.14. Stateless sessions
77,"An arguably-underappreciated feature of Hibernate is the StatelessSession interface, which provides a command-oriented, more bare-metal approach to interacting with the database."
77,You may obtain a stateless session from the SessionFactory:
77,StatelessSession ss = getSessionFactory().openStatelessSession();
77,A stateless session:
77,"doesn’t have a first-level cache (persistence context), nor does it interact with any second-level caches, and"
77,"doesn’t implement transactional write-behind or automatic dirty checking, so all operations are executed immediately when they’re explicitly called."
77,"For a stateless session, we’re always working with detached objects."
77,"Thus, the programming model is a bit different:"
77,Table 55. Important methods of the StatelessSession
77,Method name and parameters
77,Effect
77,"get(Class, Object)"
77,"Obtain a detached object, given its type and its id, by executing a select"
77,fetch(Object)
77,Fetch an association of a detached object
77,refresh(Object)
77,Refresh the state of a detached object by executing
77,a select
77,insert(Object)
77,Immediately insert the state of the given transient object into the database
77,update(Object)
77,Immediately update the state of the given detached object in the database
77,delete(Object)
77,Immediately delete the state of the given detached object from the database
77,upsert(Object)
77,Immediately insert or update the state of the given detached object using a SQL merge into statement
77,"There’s no flush() operation, and so update() is always explicit."
77,"In certain circumstances, this makes stateless sessions easier to work with, but with the caveat that a stateless session is much more vulnerable to data aliasing effects, since it’s easy to get two non-identical Java objects which both represent the same row of a database table."
77,"If we use fetch() in a stateless session, we can very easily obtain two objects representing the same database row!"
77,"In particular, the absence of a persistence context means that we can safely perform bulk-processing tasks without allocating huge quantities of memory."
77,Use of a StatelessSession alleviates the need to call:
77,"clear() or detach() to perform first-level cache management, and"
77,setCacheMode() to bypass interaction with the second-level cache.
77,"Stateless sessions can be useful, but for bulk operations on huge datasets, Hibernate can’t possibly compete with stored procedures!"
77,"When using a stateless session, you should be aware of the following additional limitations:"
77,"persistence operations never cascade to associated instances,"
77,"changes to @ManyToMany associations and @ElementCollections cannot be made persistent, and"
77,operations performed via a stateless session bypass callbacks.
77,7.15. Optimistic and pessimistic locking
77,"Finally, an aspect of behavior under load that we didn’t mention above is row-level data contention."
77,"When many transactions try to read and update the same data, the program might become unresponsive with lock escalation, deadlocks, and lock acquisition timeout errors."
77,There’s two basic approaches to data concurrency in Hibernate:
77,"optimistic locking using @Version columns, and"
77,database-level pessimistic locking using the SQL for update syntax (or equivalent).
77,"In the Hibernate community it’s much more common to use optimistic locking, and Hibernate makes that incredibly easy."
77,"Where possible, in a multiuser system, avoid holding a pessimistic lock across a user interaction."
77,"Indeed, the usual practice is to avoid having transactions that span user interactions. For multiuser systems, optimistic locking is king."
77,"That said, there is also a place for pessimistic locks, which can sometimes reduce the probability of transaction rollbacks."
77,"Therefore, the find(), lock(), and refresh() methods of the reactive session accept an optional LockMode."
77,We can also specify a LockMode for a query.
77,"The lock mode can be used to request a pessimistic lock, or to customize the behavior of optimistic locking:"
77,Table 56. Optimistic and pessimistic lock modes
77,LockMode type
77,Meaning
77,READ
77,An optimistic lock obtained implicitly whenever
77,an entity is read from the database using select
77,OPTIMISTIC
77,An optimistic lock obtained when an entity is
77,"read from the database, and verified using a"
77,select to check the version when the
77,transaction completes
77,OPTIMISTIC_FORCE_INCREMENT
77,An optimistic lock obtained when an entity is
77,"read from the database, and enforced using an"
77,update to increment the version when the
77,transaction completes
77,WRITE
77,A pessimistic lock obtained implicitly whenever
77,an entity is written to the database using
77,update or insert
77,PESSIMISTIC_READ
77,A pessimistic for share lock
77,PESSIMISTIC_WRITE
77,A pessimistic for update lock
77,PESSIMISTIC_FORCE_INCREMENT
77,A pessimistic lock enforced using an immediate
77,update to increment the version
77,7.16. Collecting statistics
77,We may ask Hibernate to collect statistics about its activity by setting this configuration property:
77,Configuration property name
77,Property value
77,hibernate.generate_statistics
77,true to enable collection of statistics
77,The statistics are exposed by the Statistics object:
77,long failedVersionChecks =
77,sessionFactory.getStatistics()
77,.getOptimisticFailureCount();
77,long publisherCacheMissCount =
77,sessionFactory.getStatistics()
77,.getEntityStatistics(Publisher.class.getName())
77,.getCacheMissCount()
77,Hibernate’s statistics enable observability.
77,Both Micrometer and SmallRye Metrics are capable of exposing these metrics.
77,7.17. Tracking down slow queries
77,"When a poorly-performing SQL query is discovered in production, it can sometimes be hard to track down exactly where in the Java code the query originates."
77,Hibernate offers two configuration properties that can make it easier to identify a slow query and find its source.
77,Table 57. Settings for tracking slow queries
77,Configuration property name
77,Purpose
77,Property value
77,hibernate.log_slow_query
77,Log slow queries at the INFO level
77,"The minimum execution time, in milliseconds, which characterizes a ""slow"" query"
77,hibernate.use_sql_comments
77,Prepend comments to the executed SQL
77,true or false
77,"When hibernate.use_sql_comments is enabled, the text of the HQL query is prepended as a comment to the generated SQL, which usually makes it easy to find the HQL in the Java code."
77,The comment text may be customized:
77,"by calling Query.setComment(comment) or Query.setHint(AvailableHints.HINT_COMMENT,comment), or"
77,via the @NamedQuery annotation.
77,"Once you’ve identified a slow query, one of the best ways to make it faster is to actually go and talk to someone who is an expert at making queries go fast."
77,"These people are called ""database administrators"", and if you’re reading this document you probably aren’t one."
77,Database administrators know lots of stuff that Java developers don’t.
77,"So if you’re lucky enough to have a DBA about, you don’t need to Dunning-Kruger your way out of a slow query."
77,An expertly-defined index might be all you need to fix a slow query.
77,7.18. Adding indexes
77,The @Index annotation may be used to add an index to a table:
77,@Entity
77,"@Table(indexes=@Index(columnList=""title, year, publisher_id""))"
77,class Book { ... }
77,"It’s even possible to specify an ordering for an indexed column, or that the index should be case-insensitive:"
77,@Entity
77,"@Table(indexes=@Index(columnList=""(lower(title)), year desc, publisher_id""))"
77,class Book { ... }
77,This lets us create a customized index for a particular query.
77,Note that SQL expressions like lower(title) must be enclosed in parentheses in the columnList of the index definition.
77,It’s not clear that information about indexes belongs in annotations of Java code.
77,"Indexes are usually maintained and modified by a database administrator, ideally by an expert in tuning the performance of one particular RDBMS."
77,So it might be better to keep the definition of indexes in a SQL DDL script that your DBA can easily read and modify.
77,"Remember, we can ask Hibernate to execute a DDL script using the property javax.persistence.schema-generation.create-script-source."
77,7.19. Dealing with denormalized data
77,"A typical relational database table in a well-normalized schema has a relatively small number of columns, and so there’s little to be gained by selectively querying columns and populating only certain fields of an entity class."
77,"But occasionally, we hear from someone asking how to map a table with a hundred columns or more!"
77,This situation can arise when:
77,"data is intentionally denormalized for performance,"
77,"the results of a complicated analytic query are exposed via a view, or"
77,someone has done something crazy and wrong.
77,Let’s suppose that we’re not dealing with the last possibility.
77,Then we would like to be able to query the monster table without returning all of its columns.
77,"At first glance, Hibernate doesn’t offer a perfect bottled solution to this problem."
77,This first impression is misleading.
77,"Actually, Hibernate features more than one way to deal with this situation, and the real problem is deciding between the ways."
77,We could:
77,"map multiple entity classes to the same table or view, being careful about ""overlaps"" where a mutable column is mapped to more than one of the entities,"
77,"use HQL or native SQL queries returning results into record types instead of retrieving entity instances, or"
77,use the bytecode enhancer and @LazyGroup for attribute-level lazy fetching.
77,"Some other ORM solutions push the third option as the recommended way to handle huge tables, but this has never been the preference of the Hibernate team or Hibernate community."
77,It’s much more typesafe to use one of the first two options.
77,7.20. Reactive programming with Hibernate
77,"Finally, many systems which require high scalability now make use of reactive programming and reactive streams."
77,Hibernate Reactive brings O/R mapping to the world of reactive programming.
77,You can learn much more about Hibernate Reactive from its Reference Documentation.
77,"Hibernate Reactive may be used alongside vanilla Hibernate in the same program, and can reuse the same entity classes."
77,This means you can use the reactive programming model exactly where you need it—perhaps only in one or two places in your system.
77,You don’t need to rewrite your whole program using reactive streams.
77,8. Advanced Topics
77,"In the last chapter of this Introduction, we turn to some topics that don’t really belong in an introduction."
77,"Here we consider some problems, and solutions, that you’re probably not going to run into immediately if you’re new to Hibernate."
77,"But we do want you to know about them, so that when the time comes, you’ll know what tool to reach for."
77,8.1. Filters
77,"Filters are one of the nicest and under-usedest features of Hibernate, and we’re quite proud of them."
77,"A filter is a named, globally-defined, parameterized restriction on the data that is visible in a given session."
77,Examples of well-defined filters might include:
77,"a filter that restricts the data visible to a given user according to row-level permissions,"
77,"a filter which hides data which has been soft-deleted,"
77,"in a versioned database, a filter that displays versions which were current at a given instant in the past, or"
77,a filter that restricts to data associated with a certain geographical region.
77,A filter must be declared somewhere.
77,A package descriptor is as good a place as any for a @FilterDef:
77,"@FilterDef(name = ""ByRegion"","
77,"parameters = @ParamDef(name = ""region"", type = String.class))"
77,package org.hibernate.example;
77,This filter has one parameter.
77,"Fancier filters might in principle have multiple parameters, though we admit this must be quite rare."
77,"If you add annotations to a package descriptor, and you’re using Configuration to configure Hibernate, make sure you call Configuration.addPackage() to let Hibernate know that the package descriptor is annotated."
77,"Typically, but not necessarily, a @FilterDef specifies a default restriction:"
77,"@FilterDef(name = ""ByRegion"","
77,"parameters = @ParamDef(name = ""region"", type = String.class),"
77,"defaultCondition = ""region = :region"")"
77,package org.hibernate.example;
77,"The restriction must contain a reference to the parameter of the filter, specified using the usual syntax for named parameters."
77,Any entity or collection which is affected by a filter must be annotated @Filter:
77,@Entity
77,@Filter(name = example_.BY_REGION)
77,class User {
77,@Id String username;
77,String region;
77,...
77,"Here, as usual, example_.BY_REGION is generated by the Metamodel Generator, and is just a constant with the value ""ByRegion""."
77,"If the @Filter annotation does not explicitly specify a restriction, the default restriction given by the @FilterDef will be applied to the entity."
77,But an entity is free to override the default condition.
77,@Entity
77,"@Filter(name = example_.FILTER_BY_REGION, condition = ""name = :region"")"
77,class Region {
77,@Id String name;
77,...
77,Note that the restriction specified by the condition or defaultCondition is a native SQL expression.
77,Table 58. Annotations for defining filters
77,Annotation
77,Purpose
77,@FilterDef
77,Defines a filter and declares its name (exactly one per filter)
77,@Filter
77,Specifies how a filter applies to a given entity or collection (many per filter)
77,"By default, a new session comes with every filter disabled."
77,A filter may be explicitly enabled in a given session by calling enableFilter() and assigning arguments to the parameters of the filter.
77,You should do this right at the start of the session.
77,sessionFactory.inTransaction(session -> {
77,session.enableFilter(example_.FILTER_BY_REGION)
77,".setParameter(""region"", ""es"")"
77,.validate();
77,...
77,});
77,"Now, any queries executed within the session will have the filter restriction applied."
77,Collections annotated @Filter will also have their members correctly filtered.
77,"On the other hand, filters are not applied to @ManyToOne associations, nor to find()."
77,This is completely by design and is not in any way a bug.
77,More than one filter may be enabled in a given session.
77,"When we only need to filter rows by a static condition with no parameters, we don’t need a filter, since @SQLRestriction provides a much simpler way to do that."
77,"We’ve mentioned that a filter can be used to implement versioning, and to provide historical views of the data."
77,"Being such a general-purpose construct, filters provide a lot of flexibility here."
77,"But if you’re after a more focused/opinionated solution to this problem, you should definitely check out Envers."
77,Using Envers for auditing historical data
77,"Envers is an add-on to Hibernate ORM which keeps a historical record of each versioned entity in a separate audit table, and allows past revisions of the data to be viewed and queried."
77,"A full introduction to Envers would require a whole chapter, so we’ll just give you a quick taste here."
77,"First, we must mark an entity as versioned, using the @Audited annotation:"
77,@Audited @Entity
77,"@Table(name=""CurrentDocument"")"
77,"@AuditTable(""DocumentRevision"")"
77,class Document { ... }
77,"The @AuditTable annotation is optional, and it’s better to set either org.hibernate.envers.audit_table_prefix or org.hibernate.envers.audit_table_suffix and let the audit table name be inferred."
77,The AuditReader interface exposes operations for retrieving and querying historical revisions.
77,It’s really easy to get hold of one of these:
77,AuditReader reader = AuditReaderFactory.get(entityManager);
77,Envers tracks revisions of the data via a global revision number.
77,We may easily find the revision number which was current at a given instant:
77,Number revision = reader.getRevisionNumberForDate(datetime);
77,We can use the revision number to ask for the version of our entity associated with the given revision number:
77,"Document doc = reader.find(Document.class, id, revision);"
77,"Alternatively, we can directly ask for the version which was current at a given instant:"
77,"Document doc = reader.find(Document.class, id, datetime);"
77,We can even execute queries to obtain lists of entities current at the given revision number:
77,List documents =
77,reader.createQuery()
77,".forEntitiesAtRevision(Document.class, revision)"
77,.getResultList();
77,"For much more information, see the User Guide."
77,Another closely-related problem is multi-tenancy.
77,8.2. Multi-tenancy
77,A multi-tenant database is one where the data is segregated by tenant.
77,"We don’t need to actually define what a ""tenant"" really represents here; all we care about at this level of abstraction is that each tenant may be distinguished by a unique identifier."
77,And that there’s a well-defined current tenant in each session.
77,We may specify the current tenant when we open a session:
77,var session =
77,sessionFactory.withOptions()
77,.tenantIdentifier(tenantId)
77,.openSession();
77,"Or, when using JPA-standard APIs:"
77,var entityManager =
77,"entityManagerFactory.createEntityManager(Map.of(HibernateHints.HINT_TENANT_ID, tenantId));"
77,"However, since we often don’t have this level of control over creation of the session, it’s more common to supply an implementation of CurrentTenantIdentifierResolver to Hibernate."
77,There are three common ways to implement multi-tenancy:
77,"each tenant has its own database,"
77,"each tenant has its own schema, or"
77,"tenants share tables in a single schema, and rows are tagged with the tenant id."
77,"From the point of view of Hibernate, there’s little difference between the first two options."
77,Hibernate will need to obtain a JDBC connection with permissions on the database and schema owned by the current tenant.
77,"Therefore, we must implement a MultiTenantConnectionProvider which takes on this responsibility:"
77,"from time to time, Hibernate will ask for a connection, passing the id of the current tenant, and then we must create an appropriate connection or obtain one from a pool, and return it to Hibernate, and"
77,"later, Hibernate will release the connection and ask us to destroy it or return it to the appropriate pool."
77,Check out DataSourceBasedMultiTenantConnectionProviderImpl for inspiration.
77,The third option is quite different.
77,"In this case we don’t need a MultiTenantConnectionProvider, but we will need a dedicated column holding the tenant id mapped by each of our entities."
77,@Entity
77,class Account {
77,@Id String id;
77,@TenantId String tenantId;
77,...
77,The @TenantId annotation is used to indicate an attribute of an entity which holds the tenant id.
77,"Within a given session, our data is automatically filtered so that only rows tagged with the tenant id of the current tenant are visible in that session."
77,Native SQL queries are not automatically filtered by tenant id; you’ll have to do that part yourself.
77,"To make use of multi-tenancy, we’ll usually need to set at least one of these configuration properties:"
77,Table 59. Multi-tenancy configuration
77,Configuration property name
77,Purpose
77,hibernate.tenant_identifier_resolver
77,Specifies the CurrentTenantIdentifierResolver
77,hibernate.multi_tenant_connection_provider
77,Specifies the MultiTenantConnectionProvider
77,8.3. Using custom-written SQL
77,"We’ve already discussed how to run queries written in SQL, but occasionally that’s not enough."
77,Sometimes—but much less often than you might expect—we would like to customize the SQL used by Hibernate to perform basic CRUD operations for an entity or collection.
77,For this we can use @SQLInsert and friends:
77,@Entity
77,"@SQLInsert(sql = ""insert into person (name, id, valid) values (?, ?, true)"", check = COUNT)"
77,"@SQLUpdate(sql = ""update person set name = ? where id = ?"")"
77,"@SQLDelete(sql = ""update person set valid = false where id = ?"")"
77,"@SQLSelect(sql = ""select id, name from person where id = ? and valid = true"")"
77,public static class Person { ... }
77,"If the custom SQL should be executed via a CallableStatement, just specify callable=true."
77,"Any SQL statement specified by one of these annotations must have exactly the number of JDBC parameters that Hibernate expects, that is, one for each column mapped by the entity, in the exact order Hibernate expects. In particular, the primary key columns must come last."
77,"However, the @Column annotation does lend some flexibility here:"
77,"if a column should not be written as part of the custom insert statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(insertable=false), or"
77,"if a column should not be written as part of the custom update statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(updatable=false)."
77,"If you need custom SQL, but are targeting multiple dialects of SQL, you can use the annotations defined in DialectOverrides."
77,"For example, this annotation lets us override the custom insert statement just for PostgreSQL:"
77,"@DialectOverride.SQLInsert(dialect = PostgreSQLDialect.class,"
77,"override = @SQLInsert(sql=""insert into person (name,id) values (?,gen_random_uuid())""))"
77,It’s even possible to override the custom SQL for specific versions of a database.
77,Sometimes a custom insert or update statement assigns a value to a mapped column which is calculated when the statement is executed on the database.
77,"For example, the value might be obtained by calling a SQL function:"
77,"@SQLInsert(sql = ""insert into person (name, id) values (?, gen_random_uuid())"")"
77,But the entity instance which represents the row being inserted or updated won’t be automatically populated with that value.
77,And so our persistence context loses synchronization with the database.
77,"In situations like this, we may use the @Generated annotation to tell Hibernate to reread the state of the entity after each insert or update."
77,8.4. Handling database-generated columns
77,"Sometimes, a column value is assigned or mutated by events that happen in the database, and aren’t visible to Hibernate."
77,For example:
77,"a table might have a column value populated by a trigger,"
77,"a mapped column might have a default value defined in DDL, or"
77,"a custom SQL insert or update statement might assign a value to a mapped column, as we saw in the previous subsection."
77,"One way to deal with this situation is to explicitly call refresh() at appropriate moments, forcing the session to reread the state of the entity."
77,But this is annoying.
77,The @Generated annotation relieves us of the burden of explicitly calling refresh().
77,"It specifies that the value of the annotated entity attribute is generated by the database, and that the generated value should be automatically retrieved using a SQL returning clause, or separate select after it is generated."
77,A useful example is the following mapping:
77,@Entity
77,class Entity {
77,@Generated @Id
77,"@ColumnDefault(""gen_random_uuid()"")"
77,UUID id;
77,The generated DDL is:
77,create table Entity (
77,"id uuid default gen_random_uuid() not null,"
77,primary key (uuid)
77,"So here the value of id is defined by the column default clause, by calling the PostgreSQL function gen_random_uuid()."
77,"When a column value is generated during updates, use @Generated(event=UPDATE)."
77,"When a value is generated by both inserts and updates, use @Generated(event={INSERT,UPDATE})."
77,"For columns which should be generated using a SQL generated always as clause, prefer the @GeneratedColumn annotation, so that Hibernate automatically generates the correct DDL."
77,"Actually, the @Generated and @GeneratedColumn annotations are defined in terms of a more generic and user-extensible framework for handling attribute values generated in Java, or by the database."
77,"So let’s drop down a layer, and see how that works."
77,8.5. User-defined generators
77,"JPA doesn’t define a standard way to extend the set of id generation strategies, but Hibernate does:"
77,"the Generator hierarchy of interfaces in the package org.hibernate.generator lets you define new generators, and"
77,the @IdGeneratorType meta-annotation from the package org.hibernate.annotations lets you write an annotation which associates a Generator type with identifier attributes.
77,"Furthermore, the @ValueGenerationType meta-annotation lets you write an annotation which associates a Generator type with a non-@Id attribute."
77,"These APIs are new in Hibernate 6, and supersede the classic IdentifierGenerator interface and @GenericGenerator annotation from older versions of Hibernate."
77,"However, the older APIs are still available and custom IdentifierGenerators written for older versions of Hibernate continue to work in Hibernate 6."
77,Hibernate has a range of built-in generators which are defined in terms of this new framework.
77,Table 60. Built-in generators
77,Annotation
77,Implementation
77,Purpose
77,@Generated
77,GeneratedGeneration
77,Generically handles database-generated values
77,@GeneratedColumn
77,GeneratedAlwaysGeneration
77,Handles values generated using generated always
77,@CurrentTimestamp
77,CurrentTimestampGeneration
77,Generic support for database or in-memory generation of creation or update timestamps
77,@CreationTimestamp
77,CurrentTimestampGeneration
77,A timestamp generated when an entity is first made persistent
77,@UpdateTimestamp
77,CurrentTimestampGeneration
77,"A timestamp generated when an entity is made persistent, and regenerated every time the entity is modified"
77,@UuidGenerator
77,UuidGenerator
77,A more flexible generator for RFC 4122 UUIDs
77,"Furthermore, support for JPA’s standard id generation strategies is also defined in terms of this framework."
77,"As an example, let’s look at how @UuidGenerator is defined:"
77,@IdGeneratorType(org.hibernate.id.uuid.UuidGenerator.class)
77,@ValueGenerationType(generatedBy = org.hibernate.id.uuid.UuidGenerator.class)
77,@Retention(RUNTIME)
77,"@Target({ FIELD, METHOD })"
77,public @interface UuidGenerator { ... }
77,@UuidGenerator is meta-annotated both @IdGeneratorType and @ValueGenerationType because it may be used to generate both ids and values of regular attributes.
77,"Either way, this Generator class does the hard work:"
77,public class UuidGenerator
77,// this generator produced values before SQL is executed
77,implements BeforeExecutionGenerator {
77,// constructors accept an instance of the @UuidGenerator
77,"// annotation, allowing the generator to be ""configured"""
77,// called to create an id generator
77,public UuidGenerator(
77,"org.hibernate.annotations.UuidGenerator config,"
77,"Member idMember,"
77,CustomIdGeneratorCreationContext creationContext) {
77,"this(config, idMember);"
77,// called to create a generator for a regular attribute
77,public UuidGenerator(
77,"org.hibernate.annotations.UuidGenerator config,"
77,"Member member,"
77,GeneratorCreationContext creationContext) {
77,"this(config, idMember);"
77,...
77,@Override
77,public EnumSet<EventType> getEventTypes() {
77,"// UUIDs are only assigned on insert, and never regenerated"
77,return INSERT_ONLY;
77,@Override
77,"public Object generate(SharedSessionContractImplementor session, Object owner, Object currentValue, EventType eventType) {"
77,// actually generate a UUID and transform it to the required type
77,return valueTransformer.transform( generator.generateUuid( session ) );
77,You can find out more about custom generators from the Javadoc for @IdGeneratorType and for org.hibernate.generator.
77,8.6. Naming strategies
77,"When working with a pre-existing relational schema, it’s usual to find that the column and table naming conventions used in the schema don’t match Java’s naming conventions."
77,"Of course, the @Table and @Column annotations let us explicitly specify a mapped table or column name."
77,But we would prefer to avoid scattering these annotations across our whole domain model.
77,"Therefore, Hibernate lets us define a mapping between Java naming conventions, and the naming conventions of the relational schema."
77,Such a mapping is called a naming strategy.
77,"First, we need to understand how Hibernate assigns and processes names."
77,Logical naming is the process of applying naming rules to determine the logical names of objects which were not explicitly assigned names in the O/R mapping.
77,"That is, when there’s no @Table or @Column annotation."
77,"Physical naming is the process of applying additional rules to transform a logical name into an actual ""physical"" name that will be used in the database."
77,"For example, the rules might include things like using standardized abbreviations, or trimming the length of identifiers."
77,"Thus, there’s two flavors of naming strategy, with slightly different responsibilities."
77,Hibernate comes with default implementations of these interfaces:
77,Flavor
77,Default implementation
77,An ImplicitNamingStrategy is responsible for assigning a logical name when none is specified by an annotation
77,A default strategy which implements the rules defined by JPA
77,A PhysicalNamingStrategy is responsible for transforming a logical name and producing the name used in the database
77,A trivial implementation which does no processing
77,"We happen to not much like the naming rules defined by JPA, which specify that mixed case and camel case identifiers should be concatenated using underscores."
77,We bet you could easily come up with a much better ImplicitNamingStrategy than that!
77,(Hint: it should always produce legit mixed case identifiers.)
77,A popular PhysicalNamingStrategy produces snake case identifiers.
77,Custom naming strategies may be enabled using the configuration properties we already mentioned without much explanation back in Minimizing repetitive mapping information.
77,Table 61. Naming strategy configuration
77,Configuration property name
77,Purpose
77,hibernate.implicit_naming_strategy
77,Specifies the ImplicitNamingStrategy
77,hibernate.physical_naming_strategy
77,Specifies the PhysicalNamingStrategy
77,8.7. Spatial datatypes
77,Hibernate Spatial augments the built-in basic types with a set of Java mappings for OGC spatial types.
77,"Geolatte-geom defines a set of Java types implementing the OGC spatial types, and codecs for translating to and from database-native spatial datatypes."
77,Hibernate Spatial itself supplies integration with Hibernate.
77,"To use Hibernate Spatial, we must add it as a dependency, as described in Optional dependencies."
77,Then we may immediately use Geolatte-geom and JTS types in our entities.
77,No special annotations are needed:
77,import org.locationtech.jts.geom.Point;
77,import jakarta.persistence.*;
77,@Entity
77,class Event {
77,Event() {}
77,"Event(String name, Point location) {"
77,this.name = name;
77,this.location = location;
77,@Id @GeneratedValue
77,Long id;
77,String name;
77,Point location;
77,The generated DDL uses geometry as the type of the column mapped by location:
77,create table Event (
77,"id bigint not null,"
77,"location geometry,"
77,"name varchar(255),"
77,primary key (id)
77,Hibernate Spatial lets us work with spatial types just as we would with any of the built-in basic attribute types.
77,var geometryFactory = new GeometryFactory();
77,...
77,"Point point = geometryFactory.createPoint(new Coordinate(10, 5));"
77,"session.persist(new Event(""Hibernate ORM presentation"", point));"
77,But what makes this powerful is that we may write some very fancy queries involving functions of spatial types:
77,Polygon triangle =
77,geometryFactory.createPolygon(
77,new Coordinate[] {
77,"new Coordinate(9, 4),"
77,"new Coordinate(11, 4),"
77,"new Coordinate(11, 20),"
77,"new Coordinate(9, 4)"
77,Point event =
77,"session.createQuery(""select location from Event where within(location, :zone) = true"", Point.class)"
77,".setParameter(""zone"", triangle)"
77,.getSingleResult();
77,"Here, within() is one of the functions for testing spatial relations defined by the OpenGIS specification."
77,"Other such functions include touches(), intersects(), distance(), boundary(), etc."
77,Not every spatial relation function is supported on every database.
77,A matrix of support for spatial relation functions may be found in the User Guide.
77,"If you want to play with spatial functions on H2, run the following code first:"
77,sessionFactory.inTransaction(session -> {
77,session.doWork(connection -> {
77,try (var statement = connection.createStatement()) {
77,"statement.execute(""create alias if not exists h2gis_spatial for \""org.h2gis.functions.factory.H2GISFunctions.load\"""");"
77,"statement.execute(""call h2gis_spatial()"");"
77,});
77,} );
77,8.8. Ordered and sorted collections and map keys
77,"Java lists and maps don’t map very naturally to foreign key relationships between tables, and so we tend to avoid using them to represent associations between our entity classes."
77,"But if you feel like you really need a collection with a fancier structure than Set, Hibernate does have options."
77,"The first three options let us map the index of a List or key of a Map to a column, and are usually used with a @ElementCollection, or on the owning side of an association:"
77,Table 62. Annotations for mapping lists and maps
77,Annotation
77,Purpose
77,JPA-standard
77,@OrderColumn
77,Specifies the column used to maintain the order of a list
77,@ListIndexBase
77,The column value for the first element of the list (zero by default)
77,@MapKeyColumn
77,Specifies the column used to persist the keys of a map
77,(used when the key is of basic type)
77,@MapKeyJoinColumn
77,Specifies the column used to persist the keys of a map
77,(used when the key is an entity)
77,@ManyToMany
77,@OrderColumn // order of list is persistent
77,List<Author> authors = new ArrayList<>();
77,@ElementCollection
77,"@OrderColumn(name=""tag_order"") @ListIndexBase(1) // order column and base value"
77,List<String> tags;
77,@ElementCollection
77,"@CollectionTable(name = ""author_bios"","
77,// table name
77,"joinColumns = @JoinColumn(name = ""book_isbn"")) // column holding foreign key of owner"
77,"@Column(name=""bio"")"
77,// column holding map values
77,"@MapKeyJoinColumn(name=""author_ssn"")"
77,// column holding map keys
77,"Map<Author,String> biographies;"
77,"For a Map representing an unowned @OneToMany association, the column must also be mapped on the owning side, usually by an attribute of the target entity."
77,In this case we usually use a different annotation:
77,Table 63. Annotation for mapping an entity attribute to a map key
77,Annotation
77,Purpose
77,JPA-standard
77,@MapKey
77,Specifies an attribute of the target entity which acts as the key of the map
77,@OneToMany(mappedBy = Book_.PUBLISHER)
77,@MapKey(name = Book_.TITLE) // the key of the map is the title of the book
77,"Map<String,Book> booksByTitle = new HashMap<>();"
77,"Now, let’s introduce a little distinction:"
77,"an ordered collection is one with an ordering maintained in the database, and"
77,a sorted collection is one which is sorted in Java code.
77,These annotations allow us to specify how the elements of a collection should be ordered as they are read from the database:
77,Table 64. Annotations for ordered collections
77,Annotation
77,Purpose
77,JPA-standard
77,@OrderBy
77,Specifies a fragment of JPQL used to order the collection
77,@SQLOrder
77,Specifies a fragment of SQL used to order the collection
77,"On the other hand, the following annotations specify how a collection should be sorted in memory, and are used for collections of type SortedSet or SortedMap:"
77,Table 65. Annotations for sorted collections
77,Annotation
77,Purpose
77,JPA-standard
77,@SortNatural
77,Specifies that the elements of a collection are Comparable
77,@SortComparator
77,Specifies a Comparator used to sort the collection
77,"Under the covers, Hibernate uses a TreeSet or TreeMap to maintain the collection in sorted order."
77,8.9. Any mappings
77,An @Any mapping is a sort of polymorphic many-to-one association where the target entity types are not related by the usual entity inheritance.
77,The target type is distinguished using a discriminator value stored on the referring side of the relationship.
77,This is quite different to discriminated inheritance where the discriminator is held in the tables mapped by the referenced entity hierarchy.
77,"For example, consider an Order entity containing Payment information, where a Payment might be a CashPayment or a CreditCardPayment:"
77,interface Payment { ... }
77,@Entity
77,class CashPayment { ... }
77,@Entity
77,class CreditCardPayment { ... }
77,"In this example, Payment is not be declared as an entity type, and is not annotated @Entity. It might even be an interface, or at most just a mapped superclass, of CashPayment and CreditCardPayment. So in terms of the object/relational mappings, CashPayment and CreditCardPayment would not be considered to participate in the same entity inheritance hierarchy."
77,"On the other hand, CashPayment and CreditCardPayment do have the same identifier type."
77,This is important.
77,"An @Any mapping would store the discriminator value identifying the concrete type of Payment along with the state of the associated Order, instead of storing it in the table mapped by Payment."
77,@Entity
77,class Order {
77,...
77,@Any
77,@AnyKeyJavaClass(UUID.class)
77,//the foreign key type
77,"@JoinColumn(name=""payment_id"") // the foreign key column"
77,"@Column(name=""payment_type"")"
77,// the discriminator column
77,// map from discriminator values to target entity types
77,"@AnyDiscriminatorValue(discriminator=""CASH"", entity=CashPayment.class)"
77,"@AnyDiscriminatorValue(discriminator=""CREDIT"", entity=CreditCardPayment.class)"
77,Payment payment;
77,...
77,"It’s reasonable to think of the ""foreign key"" in an @Any mapping as a composite value made up of the foreign key and discriminator taken together. Note, however, that this composite foreign key is only conceptual and cannot be declared as a physical constraint on the relational database table."
77,There are a number of annotations which are useful to express this sort of complicated and unnatural mapping:
77,Table 66. Annotations for @Any mappings
77,Annotations
77,Purpose
77,@Any
77,Declares that an attribute is a discriminated polymorphic association mapping
77,@AnyDiscriminator
77,Specify the Java type of the discriminator
77,@JdbcType or @JdbcTypeCode
77,Specify the JDBC type of the discriminator
77,@AnyDiscriminatorValue
77,Specifies how discriminator values map to entity types
77,@Column or @Formula
77,Specify the column or formula in which the discriminator value is stored
77,@AnyKeyJavaType or @AnyKeyJavaClass
77,"Specify the Java type of the foreign key (that is, of the ids of the target entities)"
77,@AnyKeyJdbcType or @AnyKeyJdbcTypeCode
77,Specify the JDBC type of the foreign key
77,@JoinColumn
77,Specifies the foreign key column
77,"Of course, @Any mappings are disfavored, except in extremely special cases, since it’s much more difficult to enforce referential integrity at the database level."
77,There’s also currently some limitations around querying @Any associations in HQL.
77,This is allowed:
77,from Order ord
77,join CashPayment cash
77,on id(ord.payment) = cash.id
77,Polymorphic association joins for @Any mappings are not currently implemented.
77,8.10. Selective column lists in inserts and updates
77,"By default, Hibernate generates insert and update statements for each entity during boostrap, and reuses the same insert statement every time an instance of the entity is made persistent, and the same update statement every time an instance of the entity is modified."
77,This means that:
77,"if an attribute is null when the entity is made persistent, its mapped column is redundantly included in the SQL insert, and"
77,"worse, if a certain attribute is unmodified when other attributes are changed, the column mapped by that attribute is redundantly included in the SQL update."
77,"Most of the time, this just isn’t an issue worth worrying about."
77,"The cost of interacting with the database is usually dominated by the cost of a round trip, not by the number of columns in the insert or update."
77,"But in cases where it does become important, there are two ways to be more selective about which columns are included in the SQL."
77,The JPA-standard way is to indicate statically which columns are eligible for inclusion via the @Column annotation.
77,"For example, if an entity is always created with an immutable creationDate, and with no completionDate, then we would write:"
77,@Column(updatable=false) LocalDate creationDate;
77,@Column(insertable=false) LocalDate completionDate;
77,"This approach works quite well in many cases, but often breaks down for entities with more than a handful of updatable columns."
77,An alternative solution is to ask Hibernate to generate SQL dynamically each time an insert or update is executed.
77,We do this by annotating the entity class.
77,Table 67. Annotations for dynamic SQL generation
77,Annotation
77,Purpose
77,@DynamicInsert
77,Specifies that an insert statement should be generated each time an entity is made persistent
77,@DynamicUpdate
77,Specifies that an update statement should be generated each time an entity is modified
77,"It’s important to realize that, while @DynamicInsert has no impact on semantics, the more useful @DynamicUpdate annotation does have a subtle side effect."
77,"The wrinkle is that if an entity has no version property, @DynamicUpdate opens the possibility of two optimistic transactions concurrently reading and selectively updating a given instance of the entity."
77,"In principle, this might lead to a row with inconsistent column values after both optimistic transactions commit successfully."
77,"Of course, this consideration doesn’t arise for entities with a @Version attribute."
77,But there’s a solution!
77,Well-designed relational schemas should have constraints to ensure data integrity.
77,That’s true no matter what measures we take to preserve integrity in our program logic.
77,We may ask Hibernate to add a check constraint to our table using the @Check annotation.
77,Check constraints and foreign key constraints can help ensure that a row never contains inconsistent column values.
77,8.11. Using the bytecode enhancer
77,Hibernate’s bytecode enhancer enables the following features:
77,"attribute-level lazy fetching for basic attributes annotated @Basic(fetch=LAZY) and for lazy non-polymorphic associations,"
77,interception-based—instead of the usual snapshot-based—detection of modifications.
77,"To use the bytecode enhancer, we must add the Hibernate plugin to our gradle build:"
77,plugins {
77,"id ""org.hibernate.orm"" version ""6.3.0.Final"""
77,hibernate { enhancement }
77,Consider this field:
77,@Entity
77,class Book {
77,...
77,"@Basic(optional = false, fetch = LAZY)"
77,@Column(length = LONG32)
77,String fullText;
77,...
77,"The fullText field maps to a clob or text column, depending on the SQL dialect."
77,"Since it’s expensive to retrieve the full book-length text, we’ve mapped the field fetch=LAZY, telling Hibernate not to read the field until it’s actually used."
77,"Without the bytecode enhancer, this instruction is ignored, and the field is always fetched immediately, as part of the initial select that retrieves the Book entity."
77,"With bytecode enhancement, Hibernate is able to detect access to the field, and lazy fetching is possible."
77,"By default, Hibernate fetches all lazy fields of a given entity at once, in a single select, when any one of them is accessed."
77,"Using the @LazyGroup annotation, it’s possible to assign fields to distinct ""fetch groups"", so that different lazy fields may be fetched independently."
77,"Similarly, interception lets us implement lazy fetching for non-polymorphic associations without the need for a separate proxy object."
77,"However, if an association is polymorphic, that is, if the target entity type has subclasses, then a proxy is still required."
77,Interception-based change detection is a nice performance optimization with a slight cost in terms of correctness.
77,"Without the bytecode enhancer, Hibernate keeps a snapshot of the state of each entity after reading from or writing to the database."
77,"When the session flushes, the snapshot state is compared to the current state of the entity to determine if the entity has been modified."
77,Maintaining these snapshots does have an impact on performance.
77,"With bytecode enhancement, we may avoid this cost by intercepting writes to the field and recording these modifications as they happen."
77,"This optimization isn’t completely transparent, however."
77,Interception-based change detection is less accurate than snapshot-based dirty checking.
77,"For example, consider this attribute:"
77,byte[] image;
77,"Interception is able to detect writes to the image field, that is, replacement of the whole array."
77,"It’s not able to detect modifications made directly to the elements of the array, and so such modifications may be lost."
77,8.12. Named fetch profiles
77,We’ve already seen two different ways to override the default fetching strategy for an association:
77,"JPA entity graphs, and"
77,"the join fetch clause in HQL, or, equivalently, the method From.fetch() in the criteria query API."
77,A third way is to define a named fetch profile.
77,"First, we must declare the profile, by annotating a class or package:"
77,"@FetchProfile(name = ""EagerBook"")"
77,@Entity
77,class Book { ... }
77,"Note that even though we’ve placed this annotation on the Book entity, a fetch profile—unlike an entity graph—isn’t ""rooted"" at any particular entity."
77,"We may specify association fetching strategies using the fetchOverrides member of the @FetchProfile annotation, but frankly it looks so messy that we’re embarrassed to show it to you here."
77,"Similarly, a JPA entity graph may be defined using @NamedEntityGraph."
77,"But the format of this annotation is even worse than @FetchProfile(fetchOverrides=…​), so we can’t recommend it. 💀"
77,A better way is to annotate an association with the fetch profiles it should be fetched in:
77,"@FetchProfile(name = ""EagerBook"")"
77,@Entity
77,class Book {
77,...
77,@ManyToOne(fetch = LAZY)
77,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
77,Publisher publisher;
77,@ManyToMany
77,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
77,Set<Author> authors;
77,...
77,@Entity
77,class Author {
77,...
77,@OneToOne
77,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
77,Person person;
77,...
77,"Here, once again, Book_.PROFILE_EAGER_BOOK is generated by the Metamodel Generator, and is just a constant with the value ""EagerBook""."
77,"For collections, we may even request subselect fetching:"
77,"@FetchProfile(name = ""EagerBook"")"
77,"@FetchProfile(name = ""BookWithAuthorsBySubselect"")"
77,@Entity
77,class Book {
77,...
77,@OneToOne
77,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
77,Person person;
77,@ManyToMany
77,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
77,"@FetchProfileOverride(profile = Book_.BOOK_WITH_AUTHORS_BY_SUBSELECT,"
77,mode = SUBSELECT)
77,Set<Author> authors;
77,...
77,We may define as many different fetch profiles as we like.
77,Table 68. Annotations for defining fetch profiles
77,Annotation
77,Purpose
77,@FetchProfile
77,"Declares a named fetch profile, optionally including a list of @FetchOverrides"
77,@FetchProfile.FetchOverride
77,Declares a fetch strategy override as part of the @FetchProfile declaration
77,@FetchProfileOverride
77,"Specifies the fetch strategy for the annotated association, in a given fetch profile"
77,A fetch profile must be explicitly enabled for a given session:
77,session.enableFetchProfile(Book_.PROFILE_EAGER_BOOK);
77,"Book eagerBook = session.find(Book.class, bookId);"
77,So why or when might we prefer named fetch profiles to entity graphs?
77,"Well, it’s really hard to say."
77,"It’s nice that this feature exists, and if you love it, that’s great."
77,But Hibernate offers alternatives that we think are more compelling most of the time.
77,The one and only advantage unique to fetch profiles is that they let us very selectively request subselect fetching.
77,"We can’t do that with entity graphs, and we can’t do it with HQL."
77,There’s a special built-in fetch profile named org.hibernate.defaultProfile which is defined as the profile with @FetchProfileOverride(mode=JOIN) applied to every eager @ManyToOne or @OneToOne association.
77,If you enable this profile:
77,"session.enableFetchProfile(""org.hibernate.defaultProfile"");"
77,Then outer joins for such associations will automatically be added to every HQL or criteria query.
77,This is nice if you can’t be bothered typing out those join fetches explicitly.
77,And in principle it even helps partially mitigate the problem of JPA having specified the wrong default for the fetch member of @ManyToOne.
77,9. Credits
77,The full list of contributors to Hibernate ORM can be found on the
77,GitHub repository.
77,The following contributors were involved in this documentation:
77,Gavin King
77,Version 6.3.2.Final
77,Last updated 2023-11-23 14:49:43 UTC
78,Performance recommendations - MoodleDocs
78,Forums
78,Documentation
78,Downloads
78,Demo
78,Tracker
78,Development
78,Translation
78,Search
78,Search
78,Moodle Sites
78,What are you looking for?
78,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
78,Moodle.com
78,Our social network to share and curate open educational resources.
78,MoodleNet
78,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
78,Moodle Academy
78,Moodle.com
78,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
78,MoodleNet
78,Our social network to share and curate open educational resources.
78,Moodle Academy
78,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
78,Documentation
78,Menu
78,Main pageTable of contentsDocs overviewRecent changes
78,Log in
78,4.3 docs4.2 docs
78,4.1 docs
78,Article
78,Page Comments
78,View source
78,View history
78,Performance recommendations
78,"From MoodleDocsJump to:navigation, search"
78,Main page ► Managing a Moodle site ► Performance ► Performance recommendations
78,Performance
78,Performance recommendations
78,Performance settings
78,Performance overview
78,Caching
78,Performance FAQ
78,MUC FAQ
78,"Moodle can be made to perform very well, at small usage levels or scaling up to many thousands of users. The factors involved in performance are basically the same as for any PHP-based database-driven system. When trying to optimize your server, try to focus on the factor which will make the most difference to the user. For example, if you have relatively more users browsing than accessing the database, look to improve the webserver performance."
78,Contents
78,1 Obtain a baseline benchmark
78,2 Scalability
78,2.1 Server cluster
78,3 Hardware configuration
78,4 Operating System
78,5 Caching Performance
78,6 Web Server Performance
78,6.1 PHP Performance
78,6.1.1 APC
78,6.2 Apache Performance
78,6.3 IIS Performance
78,6.4 OpenLiteSpeed
78,"6.5 Lighttpd, NginX and Cherokee Performance"
78,6.6 X-Sendfile
78,7 Cron Performance
78,8 Database Performance
78,8.1 MariaDB Performance
78,8.2 MySQL Performance
78,8.3 PostgreSQL Performance
78,8.4 Read replicas
78,8.5 Other database performance links
78,9 Performance of different Moodle modules
78,10 See also
78,Obtain a baseline benchmark
78,"Before attempting any optimization, you should obtain a baseline benchmark of the component of the system you are trying to improve. For Linux try LBS (Note: Last updated May 2002) and for Windows use the Performance Monitor. Once you have quantitative data about how your system is performing currently, you'll be able to determine if the change you have made has had any real impact."
78,"The overall aim of adjustments to improve performance is to use RAM (cacheing) and to reduce disk-based activity. It is especially important to try to eliminate swap file usage as much as you can. If your system starts swapping, this is a sign that you need more RAM."
78,"The optimization order preference is usually: primary storage (more RAM), secondary storage (faster hard disks/improved hard disk configuration), processor (more and faster)."
78,It can be interesting to install and use the Benchmark plugin in order to find the bottlenecks of your system that specifically affect Moodle or do a load test / stress test with tool like JMeter. See moodledev JMeter documentation
78,Scalability
78,Moodle's design (with clear separation of application layers) allows for strongly scalable setups. (Please check the list of large Moodle installations.)
78,"Large sites usually separate the web server and database onto separate servers, although for smaller installations this is typically not necessary."
78,"It is possible to load-balance a Moodle installation, for example by using more than one webserver. The separate webservers should query the same database and refer to the same filestore and cache areas (see Caching), but otherwise the separation of the application layers is complete enough to make this kind of clustering feasible. Similarly, the database could be a cluster of servers (e.g. a MySQL cluster), but this is not an easy task and you should seek expert support, e.g. from a Moodle Partner."
78,"On very large, load-balanced, systems the performance of the shared components become critical. It's important that your shared file areas are properly tuned and that you use an effective cache (Redis is highly recommended). A good understanding of these areas of system administration should be considered a minimum requirement."
78,Server cluster
78,Using Moodle forum discussions:
78,Moodle clustering
78,Software load balancing
78,TCP load balancing
78,Installation for 3000 simultaneous users
78,Hardware configuration
78,Note: The fastest and most effective change that you can make to improve performance is to increase the amount of RAM on your web server - get as much as possible (e.g. 4GB or more). Increasing primary memory will reduce the need for processes to swap to disk and will enable your server to handle more users.
78,"Better performance is gained by obtaining the best processor capability you can, i.e. dual or dual core processors. A modern BIOS should allow you to enable hyperthreading, but check if this makes a difference to the overall performance of the processors by using a CPU benchmarking tool."
78,"If you can afford them, use SCSI hard disks instead of SATA drives. SATA drives will increase your system's CPU utilization, whereas SCSI drives have their own integrated processors and come into their own when you have multiple drives. If you must have SATA drives, check that your motherboard and the drives themselves support NCQ (Native Command Queuing)."
78,"Purchase hard disks with a low seek time. This will improve the overall speed of your system, especially when accessing Moodle's reports. Naturally these days Solid State Drives outperform rotating media immensely, especially Enterprise-Grade SSD's."
78,Size your swap file correctly. The general advice is to set it to 4 x physical RAM.
78,"Use a RAID disk system. Although there are many different RAID configurations you can create, the following generally works best:"
78,install a hardware RAID controller (if you can)
78,the operating system and swap drive on one set of disks configured as RAID-1.
78,"Moodle, Web server and Database server on another set of disks configured as RAID-5."
78,"If your 'moodleData' area is going to be on relatively slow storage (e.g. NFS mount on to a NAS device) you will have performance issues with the default cache configuration (which writes to this storage). See the page on Caching and choose an alternative. Redis is recommended. Using GlusterFS / OCFS2 / GFS2 on a SAN device and Fiber Channel could improve performance (See more info on the Moodle forum thread, NFS performance tuing )"
78,Use gigabit ethernet for improved latency and throughput. This is especially important when you have your webserver and database server separated out on different hosts.
78,Check the settings on your network card. You may get an improvement in performance by increasing the use of buffers and transmit/receive descriptors (balance this with processor and memory overheads) and off-loading TCP checksum calculation onto the card instead of the OS.
78,Read this Case Study on a server stress test with 300 users.
78,See this accompanying report on network traffic and server loads.
78,Also see this SFSU presentation at Educause (using VMWare): [1]
78,Operating System
78,"You can use Linux(recommended), Unix-based, Windows or Mac OS X for the server operating system. *nix operating systems generally require less memory than Mac OS X or Windows servers for doing the same task as the server is configured with just a shell interface. Additionally Linux does not have licensing fees attached, but can have a big learning curve if you're used to another operating system. If you have a large number of processors running SMP, you may also want to consider using a highly tuned OS such as Solaris."
78,Check your own OS and vendor specific instructions for optimization steps.
78,For Linux look at the Linux Performance Team site.
78,"For Linux investigate the hdparm command, e.g. hdparm -m16 -d1 can be used to enable read/write on multiple sectors and DMA. Mount disks with the ""async"" and ""noatime"" options."
78,"For Windows set the sever to be optimized for network applications (Control Panel, Network Connections, LAN connection, Properties, File & Printer Sharing for Microsoft Networks, Properties, Optimization). You can also search the Microsoft TechNet site for optimization documents."
78,Caching Performance
78,"Caching in Moodle can default to disk for a lot of the different caches which is rather slow overall, and so pretty solid gains can be made by moving this to RAM, by use of a Memory Caching Application such as Redis or Memcached. In fact I'd go as far to say the single biggest improvement we made to our (relatively small) Moodle site was installing Redis, and this is amplified when you're using classic Hard Drives rather than SSD's, and especially when they slowly but surely begin to fail (the classic slow to write, but no SMART errors or write errors - just reeeeaaallly slow)."
78,"These will also cache some database queries, meaning that they don't have to be re-run, again improving performance there. Personally, I would recommend Redis over Memcached due to better security features and being more up to date/developed. For more information/how to install Redis in particular, visit the Redis cache store page."
78,Web Server Performance
78,"Most web browsers these days feature Inspector elements which will allow you to watch the time it takes for each page component to load, typically found under the ""Network"" tab. Also, the Yslow extension will evaluate your page against Yahoo's 14 rules, full text Best Practices for Speeding Up Your Web Site, (video) for fast loading websites."
78,PHP Performance
78,"PHP contains a built-in accelerator (for more recent versions of PHP, this is OpCache). Make sure it is enabled."
78,Improvements in read/write performance can be improved by putting the cached PHP pages on a TMPFS filesystem - but remember that you'll lose the cache contents when there is a power failure or the server is rebooted.
78,Performance of PHP is better when installed as an Apache/IIS6 ISAPI module (rather than a CGI). IIS 7.0/7.5 (Windows Server 2008/R2) users should choose a FastCGI installation for best performance.
78,"Also check the memory_limit in php.ini. The default value for the memory_limit directive is 128M. On some sites, it may need to be larger - especially for some backup operations."
78,Also see PHP settings by Moodle version
78,Use PHP-FPM (with apache).
78,APC
78,APC on CentOS 5.x (linux)
78,APC on Debian (linux)
78,Apache Performance
78,"If you are using Apache on a Windows server, use the build from Apache Lounge which is reported to have performance and stability improvements compared to the official Apache download. Note that this is an unofficial build, so may not keep up with official releases."
78,Set the MaxRequestWorkers directive correctly (MaxClients before Apache 2.4). Use this formula to help (which uses 80% of available memory to leave room for spare):
78,MaxRequestWorkers = Total available memory * 80% / Max memory usage of apache process
78,"Memory usage of apache process is usually 10MB but Moodle can easily use up to 100MB per process, so a general rule of thumb is to divide your available memory in megabytes by 100 to get a conservative setting for MaxClients. You are quite likely to find yourself lowering the MaxRequestWorkers from its default of 150 on a Moodle server. To get a more accurate estimate read the value from the shell command:"
78,#ps -ylC httpd --sort:rss
78,"If you need to increase the value of MaxRequestWorkers beyond 256, you will also need to set the ServerLimit directive."
78,Warning: Do not be tempted to set the value of MaxRequestWorkers higher than your available memory as your server will consume more RAM than available and start to swap to disk.
78,Consider reducing the number of modules that Apache loads in the httpd.conf file to the minimum necessary to reduce the memory needed.
78,Use the latest version of Apache - Apache 2 has an improved memory model which reduces memory usage further.
78,"For Unix/Linux systems, consider lowering MaxConnectionsPerChild (MaxRequestsPerChild before Apache 2.4) in httpd.conf to as low as 20-30 (if you set it any lower the overhead of forking begins to outweigh the benefits)."
78,"For a heavily loaded server, consider setting KeepAlive Off (do this only if your Moodle pages do not contain links to resources or uploaded images) or lowering the KeepAliveTimeout to between 2 and 5. The default is 15 (seconds) - the higher the value the more server processes will be kept waiting for possibly idle connections. A more accurate value for KeepAliveTimeout is obtained by observing how long it takes your users to download a page. After altering any of the KeepAlive variables, monitor your CPU utilization as there may be an additional overhead in initiating more worker processes/threads."
78,"As an alternative to using KeepAlive Off, consider setting-up a Reverse Proxy server in front of the Moodle server to cache HTML files with images. You can then return Apache to using keep-alives on the Moodle server."
78,"If you do not use a .htaccess file, set the AllowOverride variable to AllowOverride None to prevent .htaccess lookups."
78,Set DirectoryIndex correctly so as to avoid content-negotiation. Here's an example from a production server:
78,DirectoryIndex index.php index.html index.htm
78,"Unless you are doing development work on the server, set ExtendedStatus Off and disable mod_info as well as mod_status."
78,Leave HostnameLookups Off (as default) to reduce DNS latency.
78,Consider reducing the value of TimeOut to between 30 and 60 (seconds).
78,"For the Options directive, avoid Options Multiviews as this performs a directory scan. To reduce disk I/O further use"
78,Options -Indexes FollowSymLinks
78,Compression reduces response times by reducing the size of the HTTP response
78,Install and enable mod_deflate - refer to documentation or man pages
78,Add this code to the virtual server config file within the <directory> section for the root directory (or within the .htaccess file if AllowOverrides is On):
78,<ifModule mod_deflate.c>
78,AddOutputFilterByType DEFLATE text/html text/plain text/xml text/x-js text/javascript text/css application/javascript
78,</ifmodule>
78,Use Apache event MPM (and not the default Prefork or Worker)
78,IIS Performance
78,All alter this location in the registry:
78,HKLM\SYSTEM\CurrentControlSet\Services\Inetinfo\Parameters\
78,The equivalent to KeepAliveTimeout is ListenBackLog (IIS - registry location is HKLM\ SYSTEM\ CurrentControlSet\ Services\ Inetinfo\ Parameters). Set this to between 2 and 5.
78,Change the MemCacheSize value to adjust the amount of memory (Mb) that IIS will use for its file cache (50% of available memory by default).
78,"Change the MaxCachedFileSize to adjust the maximum size of a file cached in the file cache in bytes. Default is 262,144 (256K)."
78,"Create a new DWORD called ObjectCacheTTL to change the length of time (in milliseconds) that objects in the cache are held in memory. Default is 30,000 milliseconds (30 seconds)."
78,OpenLiteSpeed
78,"OpenLiteSpeed has its own built in cache called LSCache, which is controlled through the Web GUI, and also is compatible with PHP OpCache. More info on optimizing OpenLiteSpeed can be found on the OpenLiteSpeed page."
78,"Lighttpd, NginX and Cherokee Performance"
78,"You can increase server performance by using a light-weight webserver like lighttpd, nginx or cherokee in combination with PHP in FastCGI-mode. Lighttpd was originally created as a proof-of-concept [2] to address the C10k problem and while primarily recommended for memory-limited servers, its design origins and asynchronous-IO model make it a suitable and proven [3] alternative HTTP server for high-load websites and web apps, including Moodle. See the MoodleDocs Lighttpd page for additional information, configuration example and links."
78,"Alternatively, both lighttpd and nginx are capable of performing as a load-balancer and/or reverse-proxy to alleviate load on back-end servers [4], providing benefit without requiring an actual software change on existing servers."
78,Do note that these are likely to be the least tested server environments of all particularly if you are using advanced features such as web services and/or Moodle Networking. They are probably best considered for heavily used Moodle sites with relatively simple configurations.
78,X-Sendfile
78,X-Sendfile modules improve performance when sending large files from Moodle. It is recommended to configure your web server and Moodle to use this feature if available.
78,Configure web server:
78,Apache - https://tn123.org/mod_xsendfile/
78,Lighttpd - http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file
78,Nginx - http://wiki.nginx.org/XSendfile
78,OpenLiteSpeed - https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:config:internal-redirect
78,Enable support in config.php (see config-dist.php):
78,$CFG->xsendfile = 'X-Sendfile';
78,// Apache {@see https://tn123.org/mod_xsendfile/}
78,$CFG->xsendfile = 'X-LIGHTTPD-send-file'; // Lighttpd {@see http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file}
78,$CFG->xsendfile = 'X-Accel-Redirect';
78,// Nginx {@see http://wiki.nginx.org/XSendfile}
78,Configure file location prefixes if your server implementation requires it:
78,$CFG->xsendfilealiases = array(
78,"'/dataroot/' => $CFG->dataroot,"
78,"'/cachedir/' => '/var/www/moodle/cache',"
78,// for custom $CFG->cachedir locations
78,"'/localcachedir/' => '/var/local/cache',"
78,// for custom $CFG->localcachedir locations
78,'/tempdir/'
78,"=> '/var/www/moodle/temp',"
78,// for custom $CFG->tempdir locations
78,'/filedir'
78,"=> '/var/www/moodle/filedir',"
78,// for custom $CFG->filedir locations
78,Cron Performance
78,"Cron is a very important part of the overall performance of Moodle as many asynchronous processes are offloaded to Cron, so it needs to be running and have enough through put to handle the work being given to it by the front ends."
78,See Cron with Unix or Linux#High performance cron tasks
78,Database Performance
78,MariaDB Performance
78,"MariaDB Optimizations are similar to MySQL, but at the same time different due to the way MariaDB operates. Performance as a whole is typically better than MySQL using MariaDB, so if you're looking for Database Optimization, potentially switching from MySQL to MariaDB may help with performance, otherwise if you're already using MariaDB and looking to Optimize it, Performance Recommendations can be found on the MariaDB Page."
78,MySQL Performance
78,"The number one thing you can do to improve MySQL performance is to read, understand and implement the recommendations in the Innodb Buffer Pool article."
78,"The buffer pool size can safely be changed while your server is running, as long as your server has enough memory (RAM) to accommodate the value you set. On a machine that is dedicated to MySQL, you can safely set this value to 80% of available memory."
78,"Consider setting innodb_buffer_pool_instances to the number of cores, vCPUs, or chips you have available. Adjust this value in accordance with the recommendations in the MySQL documentation."
78,The following are MySQL specific settings which can be adjusted for better performance in your my.cnf (my.ini in Windows). The file contains a list of settings and their values. To see the current values use these commands
78,SHOW STATUS;
78,SHOW VARIABLES;
78,"Important: You must make backups of your database before attempting to change any MySQL server configuration. After any change to the my.cnf, restart mysqld."
78,"If you are able, the MySQLTuner tool can be run against your MySQL server and will calculate appropriate configuration values for most of the following settings based on your current load, status and variables automatically."
78,Enable the query cache with
78,query_cache_type = 1.
78,"For most Moodle installs, set the following:"
78,query_cache_size = 36M
78,query_cache_min_res_unit = 2K.
78,The query cache will improve performance if you are doing few updates on the database.
78,Set the table cache correctly. For Moodle 1.6 set
78,table_cache = 256 #(table_open_cache in MySQL > 5.1.2)
78,"(min), and for Moodle 1.7 set"
78,table_cache = 512 #(table_open_cache in MySQL > 5.1.2)
78,"(min). The table cache is used by all threads (connections), so monitor the value of opened_tables to further adjust - if opened_tables > 3 * table_cache(table_open_cache in MySQL > 5.1.2) then increase table_cache up to your OS limit. Note also that the figure for table_cache will also change depending on the number of modules and plugins you have installed. Find the number for your server by executing the mysql statement below. Look at the number returned and set table_cache to this value."
78,mysql>SELECT COUNT(table_name) FROM information_schema.tables WHERE table_schema='yourmoodledbname';
78,Set the thread cache correctly. Adjust the value so that your thread cache utilization is as close to 100% as possible by this formula:
78,thread cache utilization (%) = (threads_created / connections) * 100
78,"The key buffer can improve the access speed to Moodle's SELECT queries. The correct size depends on the size of the index files (.myi) and in Moodle 1.6 or later (without any additional modules and plugins), the recommendation for this value is key_buffer_size = 32M. Ideally you want the database to be reading once from the disk for every 100 requests so monitor that the value is suitable for your install by adjusting the value of key_buffer_size so that the following formulas are true:"
78,key_read / key_read_requests < 0.01
78,key_write / key_write_requests <= 1.0
78,"Set the maximum number of connections so that your users will not see a ""Too many connections"" message. Be careful that this may have an impact on the total memory used. MySQL connections usually last for milliseconds, so it is unusual even for a heavily loaded server for this value to be over 200."
78,Manage high burst activity. If your Moodle install uses a lot of quizzes and you are experiencing performance problems (check by monitoring the value of threads_connected - it should not be rising) consider increasing the value of back_log.
78,"Optimize your tables weekly and after upgrading Moodle. It is good practice to also optimize your tables after performing a large data deletion exercise, e.g. at the end of your semester or academic year. This will ensure that index files are up to date. Backup your database first and then use:"
78,mysql>CHECK TABLE mdl_tablename;
78,mysql>OPTIMIZE TABLE mdl_tablename;
78,"The common tables in Moodle to check are mdl_course_sections, mdl_forum_posts, mdl_log and mdl_sessions (if using dbsessions). Any errors need to be corrected using REPAIR TABLE (see the MySQL manual and this forum script)."
78,Maintain the key distribution. Every month or so it is a good idea to stop the mysql server and run these myisamchk commands.
78,#myisamchk -a -S /pathtomysql/data/moodledir/*.MYI
78,"Warning: You must stop the mysql database process (mysqld) before running any myisamchk command. If you do not, you risk data loss."
78,Reduce the number of temporary tables saved to disk. Check this with the created_tmp_disk_tables value. If this is relatively large (>5%) increase tmp_table_size until you see a reduction. Note that this will have an impact on RAM usage.
78,PostgreSQL Performance
78,"There are some good papers around on tuning PostgreSQL (like this one), and Moodle's case does not seem to be different to the general case."
78,The first thing to recognise is that if you really need to worry about tuning you should be using a separate machine for the database server. If you are not using a separate machine then the answers to many performance questions are substantially muddied by the memory requirements of the rest of the application.
78,"You should probably enable autovacuum, unless you know what you are doing. Many e-learning sites have predictable periods of low use, so disabling autovacuum and running a specific vacuum at those times can be a good option. Or perhaps leave autovacuum running but do a full vacuum weekly in a quiet period."
78,"Set shared_buffers to something reasonable. For versions up to 8.1 my testing has shown that peak performance is almost always obtained with buffers < 10000, so if you are using such a version, and have more than 512M of RAM just set shared_buffers to 10,000 (8MB)."
78,"The buffer management had a big overhaul in 8.2 and ""reasonable"" is now a much larger number. I have not conducted performance tests with 8.2, but the recommendations from others are generally that you should now scale shared_buffers much more with memory and may continue to reap benefits even up to values like 100,000 (80MB). Consider using 1-2% of system RAM."
78,"PostgreSQL will also assume that the operating system is caching its files, so setting effective_cache_size to a reasonable value is also a good idea. A reasonable value will usually be (total RAM - RAM in use by programs). If you are running Linux and leave the system running for a day or two you can look at 'free' and under the 'cached' column you will see what it currently is. Consider taking that number (which is kB) and dividing it by 10 (i.e. allow 20% for other programs cache needs and then divide by 8 to get pages). If you are not using a dedicated database server you will need to decrease that value to account for usage by other programs."
78,"Some other useful parameters that can have positive effects, and the values I would typically set them to on a machine with 4G RAM, are:"
78,work_mem = 10240
78,"That's 10M of RAM to use instead of on-disk sorting and so forth. That can give a big speed increase, but it is per connection and 200 connections * 10M is 2G, so it can theoretically chew up a lot of RAM."
78,maintenance_work_mem = 163840
78,"That's 160M of RAM which will be used by (e.g.) VACUUM, index rebuild, cluster and so forth. This should only be used periodically and should be freed when those processes exit, so I believe it is well worth while."
78,wal_buffers = 64
78,"These buffers are used for the write-ahead log, and there have been a number of reports on the PostgreSQL mailing lists of improvement from this level of increase."
78,This is a little out of date now (version 8.0) but still worth a read: http://www.powerpostgresql.com/Docs
78,And there is lots of good stuff here as well: http://www.varlena.com/GeneralBits/Tidbits/index.php
78,Based on Andrew McMillan's post at Tuning PostgreSQL forum thread.
78,Splitting mdl_log to several tables and using a VIEW with UNION to read them as one. (See Tim Hunt explanation on the Moodle forums)
78,Read replicas
78,Since Moodle 3.9 you can configure read replica's to be used where possible. For very large systems as much as 80-90% of the DB load can be moved away from the primary. For configuration see config-dist:
78,https://github.com/moodle/moodle/blob/master/config-dist.php#L84-L117
78,Other database performance links
78,Consider using a distributed caching system like memcached but note that memcached does not have any security features so it should be used behind a firewall.
78,Consider using PostgreSQL. See how to migrate from MySQL to PostgreSQL (forum discussion).
78,General advice on tuning MySQL parameters (advice from the MySQL manual)
78,InnoDB performance optimization taken from the MySQL performance blog site.
78,Performance of different Moodle modules
78,"Moodle's activity modules, filters, and other plugins can be activated/deactivated. If necessary, you may wish to deactivate some features (such as chat) if not required - but this isn't necessary. Some notes on the performance of certain modules:"
78,"The Chat module is said to be a hog in terms of frequent HTTP requests to the main server. This can be reduced by setting the module to use Streamed updates, or, if you're using a Unix-based webserver, by running the chat in daemon mode. When using the Chat module use the configuration settings to tune for your expected load. Pay particular attention to the chat_old_ping and chat_refresh parameters as these can have greatest impact on server load."
78,The Moodle Cron task is triggered by calling the script cron.php. If this is called over HTTP (e.g. using wget or curl) it can take a large amount of memory on large installations. If it is called by directly invoking the php command (e.g. php -f /path/to/moodle/directory/admin/cli/cron.php) efficiency can be much improved.
78,The Recent activities block is consuming too many resources if you have huge number of records mdl_log. This is being tested to optimize the SQL query.
78,"The Quiz module is known to stretch database performance. However, it has been getting better in recent versions, and we don't know of any good, up-to-date performance measurements. (Here is a case study from 2007 with 300 quiz users.). The following suggestions were described by Al Rachels in this forum thread:"
78,"make sure both Moodle, and the operating system, are installed on a solid state drive"
78,upgrade to and use PHP 7
78,run MySQLTuner and implement its recommendations
78,See Performance settings for more information on performance-related Moodle settings.
78,See also
78,Using Moodle: Hardware and Performance forum
78,Why Your Moodle Site is Slow: Five Simple Settings blog post from Jonathan Moore
78,I teach with Moodle performance testing: http://www.iteachwithmoodle.com/2012/11/17/moodle-2-4-beta-performance-test-comparison-with-moodle-2-3/
78,Moodle 2.4.5 vs 2.5.2 performance and MUC APC cahe store
78,Moodle performance testing 2.4.6 vs 2.5.2 vs 2.6dev
78,Moodle performance analysis revisited (now with MariaDB)
78,"Tim Hunt's blog (May 2, 2013) on performance testing Moodle"
78,"New Relic, Application Performance Monitoring"
78,Performance enhancements for Apache and PHP (Apache Event MPM and PHP-FPM)
78,Performance recommendations
78,Moodle performance investigation – using performance info
78,Moodle Caching at Scale
78,"There have been a lot of discussions on moodle.org about performance, here are some of the more interesting and (potentially) useful ones:"
78,Performance woes!
78,Performance perspectives - a little script
78,Comments on planned server hardware
78,Moodle performance in a pil by Martin Langhoff
78,Advice on optimising php/db code in moodle2+
78,Moodle 2.5 performance testing at the OU
78,100 active users limit with 4vCPU
78,Performance Tip ... shared...
78,"Retrieved from ""https://docs.moodle.org/403/en/index.php?title=Performance_recommendations&oldid=147626"""
78,Category: Performance
78,Tools
78,What links here
78,Related changes
78,Special pages
78,Printable version
78,Permanent link
78,Page information
78,In other languages
78,Español
78,Français
78,日本語
78,Deutsch
78,"This page was last edited on 11 January 2024, at 15:57."
78,Content is available under GNU General Public License unless otherwise noted.
78,Privacy
78,About Moodle Docs
78,Disclaimers
80,Configuring Infinispan caches
80,Configuring Infinispan caches
80,Table of Contents
80,1. Infinispan caches
80,1.1. Cache API
80,1.2. Cache Managers
80,1.3. Cache modes
80,1.3.1. Comparison of cache modes
80,1.4. Local caches
80,1.4.1. Simple caches
80,2. Clustered caches
80,2.1. Replicated caches
80,2.2. Distributed caches
80,2.2.1. Read consistency
80,2.2.2. Key ownership
80,2.2.3. Capacity factors
80,Zero capacity nodes
80,2.2.4. Level one (L1) caches
80,2.2.5. Server hinting
80,2.2.6. Key affinity service
80,2.2.7. Grouping API
80,2.3. Invalidation caches
80,2.4. Scattered caches
80,2.5. Asynchronous replication
80,2.5.1. Return values with asynchronous replication
80,2.6. Configuring initial cluster size
80,3. Infinispan cache configuration
80,3.1. Declarative cache configuration
80,3.1.1. Cache configuration
80,3.2. Adding cache templates
80,3.2.1. Creating caches from templates
80,3.2.2. Cache template inheritance
80,3.2.3. Cache template wildcards
80,3.2.4. Cache templates from multiple XML files
80,3.3. Creating remote caches
80,3.3.1. Default Cache Manager
80,3.3.2. Creating caches with Infinispan Console
80,3.3.3. Creating remote caches with the Infinispan CLI
80,3.3.4. Creating remote caches from Hot Rod clients
80,3.3.5. Creating remote caches with the REST API
80,3.4. Creating embedded caches
80,3.4.1. Adding Infinispan to your project
80,3.4.2. Creating and using embedded caches
80,3.4.3. Cache API
80,AdvancedCache API
80,Flags
80,Asynchronous API
80,Why use such an API?
80,Which processes actually happen asynchronously?
80,4. Enabling and configuring Infinispan statistics and JMX monitoring
80,4.1. Enabling statistics in embedded caches
80,4.2. Enabling statistics in remote caches
80,4.3. Enabling Hot Rod client statistics
80,4.4. Configuring Infinispan metrics
80,4.5. Registering JMX MBeans
80,4.5.1. Enabling JMX remote ports
80,4.5.2. Infinispan MBeans
80,4.5.3. Registering MBeans in custom MBean servers
80,4.6. Exporting metrics during a state transfer operation
80,4.7. Monitoring the status of cross-site replication
80,5. Configuring JVM memory usage
80,5.1. Default memory configuration
80,5.2. Eviction and expiration
80,5.3. Eviction with Infinispan caches
80,5.3.1. Eviction strategies
80,5.3.2. Configuring maximum count eviction
80,5.3.3. Configuring maximum size eviction
80,5.3.4. Manual eviction
80,5.3.5. Passivation with eviction
80,5.4. Expiration with lifespan and maximum idle
80,5.4.1. How expiration works
80,5.4.2. Expiration reaper
80,5.4.3. Maximum idle and clustered caches
80,5.4.4. Configuring lifespan and maximum idle times for caches
80,5.4.5. Configuring lifespan and maximum idle times per entry
80,5.5. JVM heap and off-heap memory
80,5.5.1. Off-heap data storage
80,5.5.2. Configuring off-heap memory
80,6. Configuring persistent storage
80,6.1. Passivation
80,6.1.1. How passivation works
80,6.2. Write-through cache stores
80,6.3. Write-behind cache stores
80,6.4. Segmented cache stores
80,6.5. Shared cache stores
80,6.6. Transactions with persistent cache stores
80,6.7. Global persistent location
80,6.7.1. Configuring the global persistent location
80,6.8. File-based cache stores
80,6.8.1. Configuring file-based cache stores
80,6.8.2. Configuring single file cache stores
80,6.9. JDBC connection factories
80,6.9.1. Configuring managed datasources
80,Configuring caches with JNDI names
80,Connection pool tuning properties
80,6.9.2. Configuring JDBC connection pools with Agroal properties
80,6.10. SQL cache stores
80,6.10.1. Data types for keys and values
80,Composite keys and values
80,Embedded keys
80,SQL types to Protobuf types
80,6.10.2. Loading Infinispan caches from database tables
80,6.10.3. Using SQL queries to load data and perform operations
80,SQL query store configuration
80,6.10.4. SQL cache store troubleshooting
80,6.11. JDBC string-based cache stores
80,6.11.1. Configuring JDBC string-based cache stores
80,6.12. RocksDB cache stores
80,6.13. Remote cache stores
80,6.14. Cluster cache loaders
80,6.15. Creating custom cache store implementations
80,6.15.1. Infinispan Persistence SPI
80,6.15.2. Creating cache stores
80,6.15.3. Examples of custom cache store configuration
80,6.15.4. Deploying custom cache stores
80,6.16. Migrating data between cache stores
80,6.16.1. Cache store migrator
80,6.16.2. Getting the cache store migrator
80,6.16.3. Configuring the cache store migrator
80,Configuration properties for the cache store migrator
80,6.16.4. Migrating Infinispan cache stores
80,7. Configuring Infinispan to handle network partitions
80,7.1. Split clusters and network partitions
80,7.1.1. Data consistency in a split cluster
80,7.2. Cache availability and degraded mode
80,7.2.1. Degraded cache recovery example
80,7.2.2. Verifying cache availability during network partitions
80,7.2.3. Making caches available
80,7.3. Configuring partition handling
80,7.4. Partition handling strategies
80,7.5. Merge policies
80,7.6. Configuring custom merge policies
80,7.7. Manually merging partitions in embedded caches
80,8. Security authorization with role-based access control
80,8.1. Infinispan user roles and permissions
80,8.1.1. Permissions
80,8.1.2. Role and permission mappers
80,Mapping users to roles and permissions in Infinispan
80,8.1.3. Configuring role mappers
80,8.2. Configuring caches with security authorization
80,9. Configuring transactions
80,9.1. Transactions
80,9.1.1. Configuring transactions
80,9.1.2. Isolation levels
80,9.1.3. Transaction locking
80,Pessimistic transactional cache
80,Optimistic transactional cache
80,What do I need - pessimistic or optimistic transactions?
80,9.1.4. Write Skews
80,Forcing write locks on keys in pessimitic transactions
80,9.1.5. Dealing with exceptions
80,9.1.6. Enlisting Synchronizations
80,9.1.7. Batching
80,API
80,Batching and JTA
80,9.1.8. Transaction recovery
80,When to use recovery
80,How does it work
80,Configuring recovery
80,Enable JMX support
80,Recovery cache
80,Integration with the transaction manager
80,Reconciliation
80,Force commit/rollback based on XID
80,10. Configuring locking and concurrency
80,10.1. Locking and concurrency
80,10.1.1. Clustered caches and locks
80,10.1.2. The LockManager
80,10.1.3. Lock striping
80,10.1.4. Concurrency levels
80,10.1.5. Lock timeout
80,10.1.6. Consistency
80,10.1.7. Data Versioning
80,11. Using clustered counters
80,11.1. Clustered Counters
80,11.1.1. Installation and Configuration
80,List counter names
80,11.1.2. CounterManager interface
80,Remove a counter via CounterManager
80,11.1.3. The Counter
80,The StrongCounter interface: when the consistency or bounds matters.
80,Bounded StrongCounter
80,Uses cases
80,Usage Examples
80,The WeakCounter interface: when speed is needed
80,Weak Counter Interface
80,Uses cases
80,Examples
80,11.1.4. Notifications and Events
80,12. Listeners and notifications
80,12.1. Listeners and notifications
80,12.2. Cache-level notifications
80,12.3. Cache Manager notifications
80,12.4. Synchronicity of events
80,Create and configure Infinispan caches with the mode and capabilities that suit your application requirements.
80,You can configure caches with expiration to remove stale entries or use eviction to control cache size.
80,"You can also add persistent storage to caches, enable partition handling for clustered caches, set up transactions, and more."
80,1. Infinispan caches
80,"Infinispan caches provide flexible, in-memory data stores that you can configure to suit use cases such as:"
80,Boosting application performance with high-speed local caches.
80,Optimizing databases by decreasing the volume of write operations.
80,Providing resiliency and durability for consistent data across clusters.
80,1.1. Cache API
80,"Cache<K,V> is the central interface for Infinispan and extends java.util.concurrent.ConcurrentMap."
80,"Cache entries are highly concurrent data structures in key:value format that support a wide and configurable range of data types, from simple strings to much more complex objects."
80,1.2. Cache Managers
80,The CacheManager API is the entry point for interacting with Infinispan.
80,"Cache Managers control cache lifecycle; creating, modifying, and deleting cache instances."
80,Cache Managers also provide cluster management and monitoring along with the ability to execute code across nodes.
80,Infinispan provides two CacheManager implementations:
80,EmbeddedCacheManager
80,Entry point for caches when running Infinispan inside the same Java Virtual Machine (JVM) as the client application.
80,RemoteCacheManager
80,Entry point for caches when running Infinispan Server in its own JVM. When you instantiate a RemoteCacheManager it establishes a persistent TCP connection to Infinispan Server through the Hot Rod endpoint.
80,Both embedded and remote CacheManager implementations share some methods and properties.
80,"However, semantic differences do exist between EmbeddedCacheManager and RemoteCacheManager."
80,1.3. Cache modes
80,Infinispan Cache Managers can create and control multiple caches that use
80,"different modes. For example, you can use the same Cache Manager for local"
80,"caches, distributed caches, and caches with invalidation mode."
80,Local
80,Infinispan runs as a single node and never replicates read or write operations on cache entries.
80,Replicated
80,Infinispan replicates all cache entries on all nodes in a cluster and performs local read operations only.
80,Distributed
80,Infinispan replicates cache entries on a subset of nodes in a cluster and assigns entries to fixed owner nodes.
80,Infinispan requests read operations from owner nodes to ensure it returns the correct value.
80,Invalidation
80,Infinispan evicts stale data from all nodes whenever operations modify entries in the cache. Infinispan performs local read operations only.
80,Scattered
80,Infinispan stores cache entries across a subset of nodes.
80,By default Infinispan assigns a primary owner and a backup owner to each cache entry in scattered caches.
80,"Infinispan assigns primary owners in the same way as with distributed caches, while backup owners are always the nodes that initiate the write operations."
80,Infinispan requests read operations from at least one owner node to ensure it returns the correct value.
80,1.3.1. Comparison of cache modes
80,The cache mode that you should choose depends on the qualities and guarantees you need for your data.
80,The following table summarizes the primary differences between cache modes:
80,Simple
80,Local
80,Invalidation
80,Replicated
80,Distributed
80,Scattered
80,Clustered
80,Yes
80,Yes
80,Yes
80,Yes
80,Read performance
80,Highest
80,(local)
80,High
80,(local)
80,High
80,(local)
80,High
80,(local)
80,Medium
80,(owners)
80,Medium
80,(primary)
80,Write performance
80,Highest
80,(local)
80,High
80,(local)
80,Low
80,"(all nodes, no data)"
80,Lowest
80,(all nodes)
80,Medium
80,(owner nodes)
80,Higher
80,(single RPC)
80,Capacity
80,Single node
80,Single node
80,Single node
80,Smallest node
80,Cluster
80,"\$(sum_(i=1)^""nodes""""node_capacity"")/""owners""\$"
80,Cluster
80,"\$(sum_(i=1)^""nodes""""node_capacity"")/""2""\$"
80,Availability
80,Single node
80,Single node
80,Single node
80,All nodes
80,Owner nodes
80,Owner nodes
80,Features
80,"No TX, persistence, indexing"
80,All
80,No indexing
80,All
80,All
80,No TX
80,1.4. Local caches
80,Infinispan offers a local cache mode that is similar to a ConcurrentHashMap.
80,"Caches offer more capabilities than simple maps, including write-through and"
80,write-behind to persistent storage as well as management capabilities such as eviction and expiration.
80,"The Infinispan Cache API extends the ConcurrentMap API in Java, making it easy to migrate from a map to a Infinispan cache."
80,Local cache configuration
80,XML
80,"<local-cache name=""mycache"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,</local-cache>
80,JSON
80,"""local-cache"": {"
80,"""name"": ""mycache"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,YAML
80,localCache:
80,"name: ""mycache"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,1.4.1. Simple caches
80,A simple cache is a type of local cache that disables support for the following capabilities:
80,Transactions and invocation batching
80,Persistent storage
80,Custom interceptors
80,Indexing
80,Transcoding
80,"However, you can use other Infinispan capabilities with simple caches such as expiration, eviction, statistics, and security features."
80,"If you configure a capability that is not compatible with a simple cache, Infinispan throws an exception."
80,Simple cache configuration
80,XML
80,"<local-cache simple-cache=""true"" />"
80,JSON
80,"""local-cache"" : {"
80,"""simple-cache"" : ""true"""
80,YAML
80,localCache:
80,"simpleCache: ""true"""
80,2. Clustered caches
80,You can create embedded and remote caches on Infinispan clusters that replicate data across nodes.
80,2.1. Replicated caches
80,Infinispan replicates all entries in the cache to all nodes in the cluster.
80,Each node can perform read operations locally.
80,"Replicated caches provide a quick and easy way to share state across a cluster, but is suitable for clusters of less than ten nodes."
80,"Because the number of replication requests scales linearly with the number of nodes in the cluster, using replicated caches with larger clusters reduces performance."
80,However you can use UDP multicasting for replication requests to improve performance.
80,"Each key has a primary owner, which serializes data container updates in order to provide consistency."
80,Figure 1. Replicated cache
80,Synchronous or asynchronous replication
80,"Synchronous replication blocks the caller (e.g. on a cache.put(key, value)) until the modifications have been replicated successfully to all the nodes in the cluster."
80,"Asynchronous replication performs replication in the background, and write operations return immediately."
80,"Asynchronous replication is not recommended, because communication errors, or errors that happen on remote nodes are not reported to the caller."
80,Transactions
80,"If transactions are enabled, write operations are not replicated through the primary owner."
80,"With pessimistic locking, each write triggers a lock message, which is broadcast to all the nodes."
80,"During transaction commit, the originator broadcasts a one-phase prepare message and an unlock message (optional)."
80,Either the one-phase prepare or the unlock message is fire-and-forget.
80,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
80,"Again, either the one-phase prepare or the unlock message is fire-and-forget."
80,2.2. Distributed caches
80,"Infinispan attempts to keep a fixed number of copies of any entry in the cache,"
80,configured as numOwners.
80,"This allows distributed caches to scale linearly, storing more data as nodes are added to the cluster."
80,"As nodes join and leave the cluster, there will be times when a key has more or less than numOwners copies."
80,"In particular, if numOwners nodes leave in quick succession, some entries will be lost, so we say that a distributed cache tolerates numOwners - 1 node failures."
80,The number of copies represents a trade-off between performance and durability of data.
80,"The more copies you maintain, the lower performance will be, but also the lower the risk of losing data due to server or network failures."
80,"Infinispan splits the owners of a key into one primary owner, which coordinates writes to the key, and zero or more backup owners."
80,The following diagram shows a write operation that a client sends to a backup owner.
80,"In this case the backup node forwards the write to the primary owner, which then replicates the write to the backup."
80,Figure 2. Cluster replication
80,Figure 3. Distributed cache
80,Read operations
80,Read operations request the value from the primary owner.
80,"If the primary owner does not respond in a reasonable amount of time, Infinispan requests the value from the backup owners as well."
80,"A read operation may require 0 messages if the key is present in the local cache, or up to 2 * numOwners messages if all the owners are slow."
80,Write operations
80,Write operations result in at most 2 * numOwners messages.
80,One message from the originator to the primary owner and numOwners - 1 messages from the primary to the backup nodes along with the corresponding acknowledgment messages.
80,Cache topology changes may cause retries and additional messages for both read and write operations.
80,Synchronous or asynchronous replication
80,Asynchronous replication is not recommended because it can lose updates.
80,"In addition to losing updates, asynchronous distributed caches can also see a stale value when a thread writes to a key and then immediately reads the same key."
80,Transactions
80,"Transactional distributed caches send lock/prepare/commit/unlock messages to the affected nodes only, meaning all nodes that own at least one key affected by the transaction."
80,"As an optimization, if the transaction writes to a single key and the originator is the primary owner of the key, lock messages are not replicated."
80,2.2.1. Read consistency
80,"Even with synchronous replication, distributed caches are not linearizable."
80,"For transactional caches, they do not support serialization/snapshot isolation."
80,"For example, a thread is carrying out a single put request:"
80,cache.get(k) -> v1
80,"cache.put(k, v2)"
80,cache.get(k) -> v2
80,But another thread might see the values in a different order:
80,cache.get(k) -> v2
80,cache.get(k) -> v1
80,"The reason is that read can return the value from any owner, depending on how fast the primary owner replies."
80,The write is not atomic across all the owners.
80,"In fact, the primary commits the update only after it receives a confirmation from the backup."
80,"While the primary is waiting for the confirmation message from the backup, reads from the backup will see the new value, but reads from the primary will see the old one."
80,2.2.2. Key ownership
80,Distributed caches split entries into a fixed number of segments and assign
80,each segment to a list of owner nodes.
80,"Replicated caches do the same, with the exception that every node is an owner."
80,The first node in the list of owners is the primary owner.
80,The other nodes in the list are backup owners.
80,"When the cache topology changes, because a node joins or leaves the cluster, the segment ownership table is broadcast to every node."
80,This allows nodes to locate keys without making multicast requests or maintaining metadata for each key.
80,The numSegments property configures the number of segments available.
80,"However, the number of segments cannot change unless the cluster is restarted."
80,Likewise the key-to-segment mapping cannot change.
80,Keys must always map to the same segments regardless of cluster topology changes.
80,It is important that the key-to-segment mapping evenly distributes the number of segments allocated to each node while minimizing the number of segments that must move when the cluster topology changes.
80,Consistent hash factory implementation
80,Description
80,SyncConsistentHashFactory
80,Uses an algorithm based on consistent hashing. Selected by default when server hinting is disabled.
80,This implementation always assigns keys to the same nodes in every cache as
80,"long as the cluster is symmetric. In other words, all caches run on all nodes."
80,This implementation does have some negative points in that the load distribution is slightly uneven. It also moves more segments than strictly necessary on a join or leave.
80,TopologyAwareSyncConsistentHashFactory
80,Equivalent to SyncConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners. This is the default consistent hashing implementation with server hinting.
80,DefaultConsistentHashFactory
80,"Achieves a more even distribution than SyncConsistentHashFactory, but with one disadvantage. The order in which nodes join the cluster determines which nodes own which segments. As a result, keys might be assigned to different nodes in different caches."
80,TopologyAwareConsistentHashFactory
80,Equivalent to DefaultConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners.
80,ReplicatedConsistentHashFactory
80,Used internally to implement replicated caches. You should never explicitly
80,select this algorithm in a distributed cache.
80,Hashing configuration
80,"You can configure ConsistentHashFactory implementations, including custom ones, with embedded caches only."
80,XML
80,"<distributed-cache name=""distributedCache"""
80,"owners=""2"""
80,"segments=""100"""
80,"capacity-factor=""2"" />"
80,ConfigurationBuilder
80,Configuration c = new ConfigurationBuilder()
80,.clustering()
80,.cacheMode(CacheMode.DIST_SYNC)
80,.hash()
80,.numOwners(2)
80,.numSegments(100)
80,.capacityFactor(2)
80,.build();
80,Additional resources
80,KeyPartitioner
80,2.2.3. Capacity factors
80,Capacity factors allocate the number of segments based on resources available to each node in the cluster.
80,The capacity factor for a node applies to segments for which that node is both the primary owner and backup owner.
80,"In other words, the capacity factor specifies is the total capacity that a node has in comparison to other nodes in the cluster."
80,The default value is 1 which means that all nodes in the cluster have an equal capacity and Infinispan allocates the same number of segments to all nodes in the cluster.
80,"However, if nodes have different amounts of memory available to them, you can configure the capacity factor so that the Infinispan hashing algorithm assigns each node a number of segments weighted by its capacity."
80,The value for the capacity factor configuration must be a positive number and can be a fraction such as 1.5.
80,You can also configure a capacity factor of 0 but is recommended only for nodes that join the cluster temporarily and should use the zero capacity configuration instead.
80,Zero capacity nodes
80,"You can configure nodes where the capacity factor is 0 for every cache, user defined caches, and internal caches."
80,"When defining a zero capacity node, the node does not hold any data."
80,Zero capacity node configuration
80,XML
80,<infinispan>
80,"<cache-container zero-capacity-node=""true"" />"
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""zero-capacity-node"" : ""true"""
80,YAML
80,infinispan:
80,cacheContainer:
80,"zeroCapacityNode: ""true"""
80,ConfigurationBuilder
80,new GlobalConfigurationBuilder().zeroCapacityNode(true);
80,2.2.4. Level one (L1) caches
80,Infinispan nodes create local replicas when they retrieve entries from another node in the cluster.
80,L1 caches avoid repeatedly looking up entries on primary owner nodes and adds performance.
80,The following diagram illustrates how L1 caches work:
80,Figure 4. L1 cache
80,"In the ""L1 cache"" diagram:"
80,A client invokes cache.get() to read an entry for which another node in the cluster is the primary owner.
80,The originator node forwards the read operation to the primary owner.
80,The primary owner returns the key/value entry.
80,The originator node creates a local copy.
80,Subsequent cache.get() invocations return the local entry instead of forwarding to the primary owner.
80,L1 caching performance
80,Enabling L1 improves performance for read operations but requires primary owner nodes to broadcast invalidation messages when entries are modified.
80,This ensures that Infinispan removes any out of date replicas across the cluster.
80,"However this also decreases performance of write operations and increases memory usage, reducing overall capacity of caches."
80,"Infinispan evicts and expires local replicas, or L1 entries, like any other cache entry."
80,L1 cache configuration
80,XML
80,"<distributed-cache l1-lifespan=""5000"""
80,"l1-cleanup-interval=""60000"">"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""l1-lifespan"": ""5000"","
80,"""l1-cleanup-interval"": ""60000"""
80,YAML
80,distributedCache:
80,"l1Lifespan: ""5000"""
80,"l1-cleanup-interval: ""60000"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
80,.l1()
80,".lifespan(5000, TimeUnit.MILLISECONDS)"
80,".cleanupTaskFrequency(60000, TimeUnit.MILLISECONDS);"
80,2.2.5. Server hinting
80,"Server hinting increases availability of data in distributed caches by replicating entries across as many servers, racks, and data centers as possible."
80,Server hinting applies only to distributed caches.
80,"When Infinispan distributes the copies of your data, it follows the order of precedence: site, rack, machine, and node."
80,All of the configuration attributes are optional.
80,"For example, when you specify only the rack IDs, then Infinispan distributes the copies across different racks and nodes."
80,Server hinting can impact cluster rebalancing operations by moving more segments than necessary if the number of segments for the cache is too low.
80,An alternative for clusters in multiple data centers is cross-site replication.
80,Server hinting configuration
80,XML
80,<cache-container>
80,"<transport cluster=""MyCluster"""
80,"machine=""LinuxServer01"""
80,"rack=""Rack01"""
80,"site=""US-WestCoast""/>"
80,</cache-container>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""transport"" : {"
80,"""cluster"" : ""MyCluster"","
80,"""machine"" : ""LinuxServer01"","
80,"""rack"" : ""Rack01"","
80,"""site"" : ""US-WestCoast"""
80,YAML
80,cacheContainer:
80,transport:
80,"cluster: ""MyCluster"""
80,"machine: ""LinuxServer01"""
80,"rack: ""Rack01"""
80,"site: ""US-WestCoast"""
80,GlobalConfigurationBuilder
80,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder()
80,.transport()
80,".clusterName(""MyCluster"")"
80,".machineId(""LinuxServer01"")"
80,".rackId(""Rack01"")"
80,".siteId(""US-WestCoast"");"
80,Additional resources
80,org.infinispan.configuration.global.TransportConfigurationBuilder
80,2.2.6. Key affinity service
80,"In a distributed cache, a key is allocated to a list of nodes with an opaque algorithm."
80,There is no easy way to reverse the computation and generate a key that maps to a particular node.
80,"However, Infinispan can generate a sequence of (pseudo-)random keys, see what their primary owner is, and hand them out to the application when it needs a key mapping to a particular node."
80,Following code snippet depicts how a reference to this service can be obtained and used.
80,// 1. Obtain a reference to a cache
80,Cache cache = ...
80,Address address = cache.getCacheManager().getAddress();
80,// 2. Create the affinity service
80,KeyAffinityService keyAffinityService = KeyAffinityServiceFactory.newLocalKeyAffinityService(
80,"cache,"
80,"new RndKeyGenerator(),"
80,"Executors.newSingleThreadExecutor(),"
80,100);
80,// 3. Obtain a key for which the local node is the primary owner
80,Object localKey = keyAffinityService.getKeyForAddress(address);
80,// 4. Insert the key in the cache
80,"cache.put(localKey, ""yourValue"");"
80,The service is started at step 2: after this point it uses the supplied Executor to generate and queue keys.
80,"At step 3, we obtain a key from the service, and at step 4 we use it."
80,Lifecycle
80,"KeyAffinityService extends Lifecycle, which allows stopping and (re)starting it:"
80,public interface Lifecycle {
80,void start();
80,void stop();
80,The service is instantiated through KeyAffinityServiceFactory.
80,"All the factory methods have an Executor parameter, that is used for asynchronous key generation (so that it"
80,won’t happen in the caller’s thread).
80,It is the user’s responsibility to handle the shutdown of this Executor.
80,"The KeyAffinityService, once started, needs to be explicitly stopped."
80,This stops the background key generation and releases other held resources.
80,The only situation in which KeyAffinityService stops by itself is when the Cache Manager with which it was registered is shutdown.
80,Topology changes
80,"When the cache topology changes, the ownership of the keys generated by the KeyAffinityService might change."
80,"The key affinity service keep tracks of these topology changes and doesn’t return keys that would currently map to a different node, but it won’t do anything about keys generated earlier."
80,"As such, applications should treat KeyAffinityService purely as an optimization, and they should not rely on the location of a generated key for correctness."
80,"In particular, applications should not rely on keys generated by KeyAffinityService for the same address to always be located together."
80,Collocation of keys is only provided by the Grouping API.
80,2.2.7. Grouping API
80,"Complementary to the Key affinity service, the Grouping API allows you to co-locate a group of entries on the same nodes, but without being able to select the actual nodes."
80,"By default, the segment of a key is computed using the key’s hashCode()."
80,"If you use the Grouping API, Infinispan will compute the segment of the group and use that as the segment of the key."
80,"When the Grouping API is in use, it is important that every node can still compute the owners of every key without contacting other nodes."
80,"For this reason, the group cannot be specified manually."
80,The group can either be intrinsic to the entry (generated by the key class) or extrinsic (generated by an external function).
80,"To use the Grouping API, you must enable groups."
80,Configuration c = new ConfigurationBuilder()
80,.clustering().hash().groups().enabled()
80,.build();
80,<distributed-cache>
80,"<groups enabled=""true""/>"
80,</distributed-cache>
80,"If you have control of the key class (you can alter the class definition, it’s not part of an unmodifiable library), then we recommend using an intrinsic group."
80,"The intrinsic group is specified by adding the @Group annotation to a method, for example:"
80,class User {
80,...
80,String office;
80,...
80,public int hashCode() {
80,"// Defines the hash for the key, normally used to determine location"
80,...
80,// Override the location by specifying a group
80,// All keys in the same group end up with the same owners
80,@Group
80,public String getOffice() {
80,return office;
80,The group method must return a String
80,"If you don’t have control over the key class, or the determination of the group is an orthogonal concern to the key class, we recommend using an extrinsic group."
80,An extrinsic group is specified by implementing the Grouper interface.
80,public interface Grouper<T> {
80,"String computeGroup(T key, String group);"
80,Class<T> getKeyType();
80,"If multiple Grouper classes are configured for the same key type, all of them will be called, receiving the value computed by the previous one."
80,"If the key class also has a @Group annotation, the first Grouper will receive the group computed by the annotated method."
80,This allows you even greater control over the group when using an intrinsic group.
80,Example Grouper implementation
80,public class KXGrouper implements Grouper<String> {
80,"// The pattern requires a String key, of length 2, where the first character is"
80,"// ""k"" and the second character is a digit. We take that digit, and perform"
80,"// modular arithmetic on it to assign it to group ""0"" or group ""1""."
80,"private static Pattern kPattern = Pattern.compile(""(^k)(<a>\\d</a>)$"");"
80,"public String computeGroup(String key, String group) {"
80,Matcher matcher = kPattern.matcher(key);
80,if (matcher.matches()) {
80,"String g = Integer.parseInt(matcher.group(2)) % 2 + """";"
80,return g;
80,} else {
80,return null;
80,public Class<String> getKeyType() {
80,return String.class;
80,Grouper implementations must be registered explicitly in the cache configuration.
80,If you are configuring Infinispan programmatically:
80,Configuration c = new ConfigurationBuilder()
80,.clustering().hash().groups().enabled().addGrouper(new KXGrouper())
80,.build();
80,"Or, if you are using XML:"
80,<distributed-cache>
80,"<groups enabled=""true"">"
80,"<grouper class=""com.example.KXGrouper"" />"
80,</groups>
80,</distributed-cache>
80,Advanced API
80,AdvancedCache has two group-specific methods:
80,getGroup(groupName) retrieves all keys in the cache that belong to a group.
80,removeGroup(groupName) removes all the keys in the cache that belong to a group.
80,"Both methods iterate over the entire data container and store (if present), so they can be slow when a cache contains lots of small groups."
80,2.3. Invalidation caches
80,Invalidation cache mode in Infinispan is designed to optimize systems that perform high volumes of read operations to a shared permanent data store.
80,You can use invalidation mode to reduce the number of database writes when state changes occur.
80,Invalidation cache mode is deprecated for Infinispan remote deployments.
80,Use invalidation cache mode with embedded caches that are stored in shared cache stores only.
80,"Invalidation cache mode is effective only when you have a permanent data store, such as a database, and are only using Infinispan as an optimization in a read-heavy system to prevent hitting the database for every read."
80,"When a cache is configured for invalidation, each data change in a cache triggers a message to other caches in the cluster, informing them that their data is now stale and should be removed from memory."
80,Invalidation messages remove stale values from other nodes' memory.
80,"The messages are very small compared to replicating the entire value, and also other caches in the cluster look up modified data in a lazy manner, only when needed."
80,The update to the shared store is typically handled by user application code or Hibernate.
80,Figure 5. Invalidation cache
80,"Sometimes the application reads a value from the external store and wants to write it to the local cache, without removing it from the other nodes."
80,"To do this, it must call Cache.putForExternalRead(key, value) instead of Cache.put(key, value)."
80,Invalidation mode is suitable only for shared stores where all nodes can access the same data.
80,"Using invalidation mode without a persistent store is impractical, as updated values need to be read from a shared store for consistency across nodes."
80,"Never use invalidation mode with a local, non-shared, cache store."
80,"The invalidation message will not remove entries in the local store, and some nodes will keep seeing the stale value."
80,"An invalidation cache can also be configured with a special cache loader, ClusterLoader."
80,"When ClusterLoader is enabled, read operations that do not find the key on the local node will request it from all the other nodes first, and store it in memory locally."
80,"This can lead to storing stale values, so only use it if you have a high tolerance for stale values."
80,Synchronous or asynchronous replication
80,"When synchronous, a write operation blocks until all nodes in the cluster have evicted the stale value."
80,"When asynchronous, the originator broadcasts invalidation messages but does not wait for responses."
80,That means other nodes still see the stale value for a while after the write completed on the originator.
80,Transactions
80,Transactions can be used to batch the invalidation messages.
80,Transactions acquire the key lock on the primary owner.
80,"With pessimistic locking, each write triggers a lock message, which is"
80,broadcast to all the nodes.
80,"During transaction commit, the originator broadcasts a one-phase prepare message (optionally fire-and-forget) which invalidates all affected keys and releases the locks."
80,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
80,"Either the one-phase prepare or the unlock message is fire-and-forget, and the last message always releases the locks."
80,2.4. Scattered caches
80,Scattered caches are very similar to distributed caches as they allow linear scaling of the cluster.
80,Scattered caches allow single node failure by maintaining two copies of the data (numOwners=2).
80,"Unlike distributed caches, the location of data is not fixed; while we use the same Consistent Hash algorithm to locate the primary owner, the backup copy is stored on the node that wrote the data last time."
80,"When the write originates on the primary owner, backup copy is stored on any other node (the exact location of this copy is not important)."
80,"This has the advantage of single Remote Procedure Call (RPC) for any write (distributed caches require one or two RPCs), but reads have to always target the primary owner."
80,"That results in faster writes but possibly slower reads, and therefore this mode is more suitable for write-intensive applications."
80,Storing multiple backup copies also results in slightly higher memory consumption.
80,"In order to remove out-of-date backup copies, invalidation messages are broadcast in the cluster, which generates some overhead."
80,This lowers the performance of scattered caches in clusters with a large number of nodes.
80,"When a node crashes, the primary copy may be lost."
80,"Therefore, the cluster has to reconcile the backups and find out the last written backup copy."
80,This process results in more network traffic during state transfer.
80,"Since the writer of data is also a backup, even if we specify machine/rack/site IDs on the transport level the cluster cannot be resilient to more than one failure on the same machine/rack/site."
80,You cannot use scattered caches with transactions or asynchronous replication.
80,"The cache is configured in a similar way as the other cache modes, here is an example of declarative configuration:"
80,"<scattered-cache name=""scatteredCache"" />"
80,Configuration c = new ConfigurationBuilder()
80,.clustering().cacheMode(CacheMode.SCATTERED_SYNC)
80,.build();
80,Scattered mode is not exposed in the server configuration as the server is usually accessed through the Hot Rod
80,protocol. The protocol automatically selects primary owner for the writes and therefore the write (in distributed
80,"mode with two owner) requires single RPC inside the cluster, too. Therefore, scattered cache would not bring"
80,the performance benefit.
80,2.5. Asynchronous replication
80,All clustered cache modes can be configured to use asynchronous communications with the
80,"mode=""ASYNC"""
80,"attribute on the <replicated-cache/>, <distributed-cache>, or <invalidation-cache/>"
80,element.
80,"With asynchronous communications, the originator node does not receive any"
80,"acknowledgement from the other nodes about the status of the operation, so there is no"
80,way to check if it succeeded on other nodes.
80,"We do not recommend asynchronous communications in general, as they can cause"
80,"inconsistencies in the data, and the results are hard to reason about."
80,"Nevertheless, sometimes speed is more important than consistency, and the option is"
80,available for those cases.
80,Asynchronous API
80,"The Asynchronous API allows you to use synchronous communications,"
80,but without blocking the user thread.
80,There is one caveat:
80,The asynchronous operations do NOT preserve the program order.
80,"If a thread calls cache.putAsync(k, v1); cache.putAsync(k, v2), the final value of k"
80,may be either v1 or v2.
80,The advantage over using asynchronous communications is that the final value can’t be
80,v1 on one node and v2 on another.
80,2.5.1. Return values with asynchronous replication
80,"Because the Cache interface extends java.util.Map, write methods like"
80,"put(key, value) and remove(key) return the previous value by default."
80,"In some cases, the return value may not be correct:"
80,"When using AdvancedCache.withFlags() with Flag.IGNORE_RETURN_VALUE,"
80,"Flag.SKIP_REMOTE_LOOKUP, or Flag.SKIP_CACHE_LOAD."
80,"When the cache is configured with unreliable-return-values=""true""."
80,When using asynchronous communications.
80,"When there are multiple concurrent writes to the same key, and the cache topology"
80,changes.
80,"The topology change will make Infinispan retry the write operations, and a retried"
80,operation’s return value is not reliable.
80,Transactional caches return the correct previous value in cases 3 and 4.
80,"However, transactional caches also have a gotcha: in distributed mode, the"
80,read-committed isolation level is implemented as repeatable-read.
80,"That means this example of ""double-checked locking"" won’t work:"
80,Cache cache = ...
80,TransactionManager tm = ...
80,tm.begin();
80,try {
80,Integer v1 = cache.get(k);
80,// Increment the value
80,"Integer v2 = cache.put(k, v1 + 1);"
80,"if (Objects.equals(v1, v2) {"
80,// success
80,} else {
80,// retry
80,} finally {
80,tm.commit();
80,The correct way to implement this is to use
80,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(k).
80,"In caches with optimistic locking, writes can also return stale previous values. Write skew checks can avoid stale previous values."
80,2.6. Configuring initial cluster size
80,Infinispan handles cluster topology changes dynamically.
80,This means that nodes do not need to wait for other nodes to join the cluster before Infinispan initializes the caches.
80,"If your applications require a specific number of nodes in the cluster before caches start, you can configure the initial cluster size as part of the transport."
80,Procedure
80,Open your Infinispan configuration for editing.
80,Set the minimum number of nodes required before caches start with the initial-cluster-size attribute or initialClusterSize() method.
80,"Set the timeout, in milliseconds, after which the Cache Manager does not start with the initial-cluster-timeout attribute or initialClusterTimeout() method."
80,Save and close your Infinispan configuration.
80,Initial cluster size configuration
80,XML
80,<infinispan>
80,<cache-container>
80,"<transport initial-cluster-size=""4"""
80,"initial-cluster-timeout=""30000"" />"
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""transport"" : {"
80,"""initial-cluster-size"" : ""4"","
80,"""initial-cluster-timeout"" : ""30000"""
80,YAML
80,infinispan:
80,cacheContainer:
80,transport:
80,"initialClusterSize: ""4"""
80,"initialClusterTimeout: ""30000"""
80,ConfigurationBuilder
80,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
80,.transport()
80,.initialClusterSize(4)
80,".initialClusterTimeout(30000, TimeUnit.MILLISECONDS);"
80,3. Infinispan cache configuration
80,Cache configuration controls how Infinispan stores your data.
80,"As part of your cache configuration, you declare the cache mode you want to use."
80,"For instance, you can configure Infinispan clusters to use replicated caches or distributed caches."
80,Your configuration also defines the characteristics of your caches and enables the Infinispan capabilities that you want to use when handling data.
80,"For instance, you can configure how Infinispan encodes entries in your caches, whether replication requests happen synchronously or asynchronously between nodes, if entries are mortal or immortal, and so on."
80,3.1. Declarative cache configuration
80,"You can configure caches declaratively, in XML, JSON, and YAML format, according to the Infinispan schema."
80,Declarative cache configuration has the following advantages over programmatic configuration:
80,Portability
80,Define each configuration in a standalone file that you can use to create embedded and remote caches.
80,You can also use declarative configuration to create caches with Infinispan Operator for clusters running on Kubernetes.
80,Simplicity
80,Keep markup languages separate to programming languages.
80,"For example, to create remote caches it is generally better to not add complex XML directly to Java code."
80,"Infinispan Server configuration extends infinispan.xml to include cluster transport mechanisms, security realms, and endpoint configuration."
80,"If you declare caches as part of your Infinispan Server configuration you should use management tooling, such as Ansible or Chef, to keep it synchronized across the cluster."
80,"To dynamically synchronize remote caches across Infinispan clusters, create them at runtime."
80,3.1.1. Cache configuration
80,"You can create declarative cache configuration in XML, JSON, and YAML format."
80,All declarative caches must conform to the Infinispan schema.
80,"Configuration in JSON format must follow the structure of an XML configuration, elements correspond to objects and attributes correspond to fields."
80,Infinispan restricts characters to a maximum of 255 for a cache name or a cache template name.
80,"If you exceed this character limit, Infinispan throws an exception."
80,Write succinct cache names and cache template names.
80,"A file system might set a limitation for the length of a file name, so ensure that a cache’s name does not exceed this limitation."
80,"If a cache name exceeds a file system’s naming limitation, general operations or initialing operations towards that cache might fail."
80,Write succinct file names.
80,Distributed caches
80,XML
80,"<distributed-cache owners=""2"""
80,"segments=""256"""
80,"capacity-factor=""1.0"""
80,"l1-lifespan=""5000"""
80,"mode=""SYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<locking isolation=""REPEATABLE_READ""/>"
80,"<transaction mode=""FULL_XA"""
80,"locking=""OPTIMISTIC""/>"
80,"<expiration lifespan=""5000"""
80,"max-idle=""1000"" />"
80,"<memory max-count=""1000000"""
80,"when-full=""REMOVE""/>"
80,"<indexing enabled=""true"""
80,"storage=""local-heap"">"
80,"<index-reader refresh-interval=""1000""/>"
80,<indexed-entities>
80,<indexed-entity>org.infinispan.Person</indexed-entity>
80,</indexed-entities>
80,</indexing>
80,"<partition-handling when-split=""ALLOW_READ_WRITES"""
80,"merge-policy=""PREFERRED_NON_NULL""/>"
80,"<persistence passivation=""false"">"
80,<!-- Persistent storage configuration. -->
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""mode"": ""SYNC"","
80,"""owners"": ""2"","
80,"""segments"": ""256"","
80,"""capacity-factor"": ""1.0"","
80,"""l1-lifespan"": ""5000"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""locking"": {"
80,"""isolation"": ""REPEATABLE_READ"""
80,"""transaction"": {"
80,"""mode"": ""FULL_XA"","
80,"""locking"": ""OPTIMISTIC"""
80,"""expiration"" : {"
80,"""lifespan"" : ""5000"","
80,"""max-idle"" : ""1000"""
80,"""memory"": {"
80,"""max-count"": ""1000000"","
80,"""when-full"": ""REMOVE"""
80,"""indexing"" : {"
80,"""enabled"" : true,"
80,"""storage"" : ""local-heap"","
80,"""index-reader"" : {"
80,"""refresh-interval"" : ""1000"""
80,"""indexed-entities"": ["
80,"""org.infinispan.Person"""
80,"""partition-handling"" : {"
80,"""when-split"" : ""ALLOW_READ_WRITES"","
80,"""merge-policy"" : ""PREFERRED_NON_NULL"""
80,"""persistence"" : {"
80,"""passivation"" : false"
80,YAML
80,distributedCache:
80,"mode: ""SYNC"""
80,"owners: ""2"""
80,"segments: ""256"""
80,"capacityFactor: ""1.0"""
80,"l1Lifespan: ""5000"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,locking:
80,"isolation: ""REPEATABLE_READ"""
80,transaction:
80,"mode: ""FULL_XA"""
80,"locking: ""OPTIMISTIC"""
80,expiration:
80,"lifespan: ""5000"""
80,"maxIdle: ""1000"""
80,memory:
80,"maxCount: ""1000000"""
80,"whenFull: ""REMOVE"""
80,indexing:
80,"enabled: ""true"""
80,"storage: ""local-heap"""
80,indexReader:
80,"refreshInterval: ""1000"""
80,indexedEntities:
80,"- ""org.infinispan.Person"""
80,partitionHandling:
80,"whenSplit: ""ALLOW_READ_WRITES"""
80,"mergePolicy: ""PREFERRED_NON_NULL"""
80,persistence:
80,"passivation: ""false"""
80,# Persistent storage configuration.
80,Replicated caches
80,XML
80,"<replicated-cache segments=""256"""
80,"mode=""SYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<locking isolation=""REPEATABLE_READ""/>"
80,"<transaction mode=""FULL_XA"""
80,"locking=""OPTIMISTIC""/>"
80,"<expiration lifespan=""5000"""
80,"max-idle=""1000"" />"
80,"<memory max-count=""1000000"""
80,"when-full=""REMOVE""/>"
80,"<indexing enabled=""true"""
80,"storage=""local-heap"">"
80,"<index-reader refresh-interval=""1000""/>"
80,<indexed-entities>
80,<indexed-entity>org.infinispan.Person</indexed-entity>
80,</indexed-entities>
80,</indexing>
80,"<partition-handling when-split=""ALLOW_READ_WRITES"""
80,"merge-policy=""PREFERRED_NON_NULL""/>"
80,"<persistence passivation=""false"">"
80,<!-- Persistent storage configuration. -->
80,</persistence>
80,</replicated-cache>
80,JSON
80,"""replicated-cache"": {"
80,"""mode"": ""SYNC"","
80,"""segments"": ""256"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""locking"": {"
80,"""isolation"": ""REPEATABLE_READ"""
80,"""transaction"": {"
80,"""mode"": ""FULL_XA"","
80,"""locking"": ""OPTIMISTIC"""
80,"""expiration"" : {"
80,"""lifespan"" : ""5000"","
80,"""max-idle"" : ""1000"""
80,"""memory"": {"
80,"""max-count"": ""1000000"","
80,"""when-full"": ""REMOVE"""
80,"""indexing"" : {"
80,"""enabled"" : true,"
80,"""storage"" : ""local-heap"","
80,"""index-reader"" : {"
80,"""refresh-interval"" : ""1000"""
80,"""indexed-entities"": ["
80,"""org.infinispan.Person"""
80,"""partition-handling"" : {"
80,"""when-split"" : ""ALLOW_READ_WRITES"","
80,"""merge-policy"" : ""PREFERRED_NON_NULL"""
80,"""persistence"" : {"
80,"""passivation"" : false"
80,YAML
80,replicatedCache:
80,"mode: ""SYNC"""
80,"segments: ""256"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,locking:
80,"isolation: ""REPEATABLE_READ"""
80,transaction:
80,"mode: ""FULL_XA"""
80,"locking: ""OPTIMISTIC"""
80,expiration:
80,"lifespan: ""5000"""
80,"maxIdle: ""1000"""
80,memory:
80,"maxCount: ""1000000"""
80,"whenFull: ""REMOVE"""
80,indexing:
80,"enabled: ""true"""
80,"storage: ""local-heap"""
80,indexReader:
80,"refreshInterval: ""1000"""
80,indexedEntities:
80,"- ""org.infinispan.Person"""
80,partitionHandling:
80,"whenSplit: ""ALLOW_READ_WRITES"""
80,"mergePolicy: ""PREFERRED_NON_NULL"""
80,persistence:
80,"passivation: ""false"""
80,# Persistent storage configuration.
80,Multiple caches
80,XML
80,<infinispan
80,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
80,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-14.0.xsd"
80,"urn:infinispan:server:14.0 https://infinispan.org/schemas/infinispan-server-14.0.xsd"""
80,"xmlns=""urn:infinispan:config:14.0"""
80,"xmlns:server=""urn:infinispan:server:14.0"">"
80,"<cache-container name=""default"""
80,"statistics=""true"">"
80,"<distributed-cache name=""mycacheone"""
80,"mode=""ASYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<expiration lifespan=""300000""/>"
80,"<memory max-size=""400MB"""
80,"when-full=""REMOVE""/>"
80,</distributed-cache>
80,"<distributed-cache name=""mycachetwo"""
80,"mode=""SYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<expiration lifespan=""300000""/>"
80,"<memory max-size=""400MB"""
80,"when-full=""REMOVE""/>"
80,</distributed-cache>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""name"" : ""default"","
80,"""statistics"" : ""true"","
80,"""caches"" : {"
80,"""mycacheone"" : {"
80,"""distributed-cache"" : {"
80,"""mode"": ""ASYNC"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""expiration"" : {"
80,"""lifespan"" : ""300000"""
80,"""memory"": {"
80,"""max-size"": ""400MB"","
80,"""when-full"": ""REMOVE"""
80,"""mycachetwo"" : {"
80,"""distributed-cache"" : {"
80,"""mode"": ""SYNC"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""expiration"" : {"
80,"""lifespan"" : ""300000"""
80,"""memory"": {"
80,"""max-size"": ""400MB"","
80,"""when-full"": ""REMOVE"""
80,YAML
80,infinispan:
80,cacheContainer:
80,"name: ""default"""
80,"statistics: ""true"""
80,caches:
80,mycacheone:
80,distributedCache:
80,"mode: ""ASYNC"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,expiration:
80,"lifespan: ""300000"""
80,memory:
80,"maxSize: ""400MB"""
80,"whenFull: ""REMOVE"""
80,mycachetwo:
80,distributedCache:
80,"mode: ""SYNC"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,expiration:
80,"lifespan: ""300000"""
80,memory:
80,"maxSize: ""400MB"""
80,"whenFull: ""REMOVE"""
80,Additional resources
80,Infinispan configuration schema reference
80,infinispan-config-14.0.xsd
80,3.2. Adding cache templates
80,The Infinispan schema includes *-cache-configuration elements that you can use to create templates.
80,"You can then create caches on demand, using the same configuration multiple times."
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add the cache configuration with the appropriate *-cache-configuration element or object to the Cache Manager.
80,Save and close your Infinispan configuration.
80,Cache template example
80,XML
80,<infinispan>
80,<cache-container>
80,"<distributed-cache-configuration name=""my-dist-template"""
80,"mode=""SYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<memory max-count=""1000000"""
80,"when-full=""REMOVE""/>"
80,"<expiration lifespan=""5000"""
80,"max-idle=""1000""/>"
80,</distributed-cache-configuration>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""distributed-cache-configuration"" : {"
80,"""name"" : ""my-dist-template"","
80,"""mode"": ""SYNC"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""expiration"" : {"
80,"""lifespan"" : ""5000"","
80,"""max-idle"" : ""1000"""
80,"""memory"": {"
80,"""max-count"": ""1000000"","
80,"""when-full"": ""REMOVE"""
80,YAML
80,infinispan:
80,cacheContainer:
80,distributedCacheConfiguration:
80,"name: ""my-dist-template"""
80,"mode: ""SYNC"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,expiration:
80,"lifespan: ""5000"""
80,"maxIdle: ""1000"""
80,memory:
80,"maxCount: ""1000000"""
80,"whenFull: ""REMOVE"""
80,3.2.1. Creating caches from templates
80,Create caches from configuration templates.
80,Templates for remote caches are available from the Cache templates menu in Infinispan Console.
80,Prerequisites
80,Add at least one cache template to the Cache Manager.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Specify the template from which the cache inherits with the configuration attribute or field.
80,Save and close your Infinispan configuration.
80,Cache configuration inherited from a template
80,XML
80,"<distributed-cache configuration=""my-dist-template"" />"
80,JSON
80,"""distributed-cache"": {"
80,"""configuration"": ""my-dist-template"""
80,YAML
80,distributedCache:
80,"configuration: ""my-dist-template"""
80,3.2.2. Cache template inheritance
80,Cache configuration templates can inherit from other templates to extend and override settings.
80,Cache template inheritance is hierarchical.
80,"For a child configuration template to inherit from a parent, you must include it after the parent template."
80,"Additionally, template inheritance is additive for elements that have multiple values."
80,"A cache that inherits from another template merges the values from that template, which can override properties."
80,Template inheritance example
80,XML
80,<infinispan>
80,<cache-container>
80,"<distributed-cache-configuration name=""base-template"">"
80,"<expiration lifespan=""5000""/>"
80,</distributed-cache-configuration>
80,"<distributed-cache-configuration name=""extended-template"""
80,"configuration=""base-template"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,"<expiration lifespan=""10000"""
80,"max-idle=""1000""/>"
80,</distributed-cache-configuration>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""caches"" : {"
80,"""base-template"" : {"
80,"""distributed-cache-configuration"" : {"
80,"""expiration"" : {"
80,"""lifespan"" : ""5000"""
80,"""extended-template"" : {"
80,"""distributed-cache-configuration"" : {"
80,"""configuration"" : ""base-template"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,"""expiration"" : {"
80,"""lifespan"" : ""10000"","
80,"""max-idle"" : ""1000"""
80,YAML
80,infinispan:
80,cacheContainer:
80,caches:
80,base-template:
80,distributedCacheConfiguration:
80,expiration:
80,"lifespan: ""5000"""
80,extended-template:
80,distributedCacheConfiguration:
80,"configuration: ""base-template"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,expiration:
80,"lifespan: ""10000"""
80,"maxIdle: ""1000"""
80,3.2.3. Cache template wildcards
80,You can add wildcards to cache configuration template names.
80,"If you then create caches where the name matches the wildcard, Infinispan applies the configuration template."
80,Infinispan throws exceptions if cache names match more than one wildcard.
80,Template wildcard example
80,XML
80,<infinispan>
80,<cache-container>
80,"<distributed-cache-configuration name=""async-dist-cache-*"""
80,"mode=""ASYNC"""
80,"statistics=""true"">"
80,"<encoding media-type=""application/x-protostream""/>"
80,</distributed-cache-configuration>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""distributed-cache-configuration"" : {"
80,"""name"" : ""async-dist-cache-*"","
80,"""mode"": ""ASYNC"","
80,"""statistics"": ""true"","
80,"""encoding"": {"
80,"""media-type"": ""application/x-protostream"""
80,YAML
80,infinispan:
80,cacheContainer:
80,distributedCacheConfiguration:
80,"name: ""async-dist-cache-*"""
80,"mode: ""ASYNC"""
80,"statistics: ""true"""
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,"Using the preceding example, if you create a cache named ""async-dist-cache-prod"" then Infinispan uses the configuration from the async-dist-cache-* template."
80,3.2.4. Cache templates from multiple XML files
80,Split cache configuration templates into multiple XML files for granular flexibility and reference them with XML inclusions (XInclude).
80,Infinispan provides minimal support for the XInclude specification.
80,"This means you cannot use the xpointer attribute, the xi:fallback element, text processing, or content negotiation."
80,"You must also add the xmlns:xi=""http://www.w3.org/2001/XInclude"" namespace to infinispan.xml to use XInclude."
80,Xinclude cache template
80,"<infinispan xmlns:xi=""http://www.w3.org/2001/XInclude"">"
80,"<cache-container default-cache=""cache-1"">"
80,<!-- References files that contain cache configuration templates. -->
80,"<xi:include href=""distributed-cache-template.xml"" />"
80,"<xi:include href=""replicated-cache-template.xml"" />"
80,</cache-container>
80,</infinispan>
80,Infinispan also provides an infinispan-config-fragment-14.0.xsd schema that you can use with configuration fragments.
80,Configuration fragment schema
80,"<local-cache xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
80,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-fragment-14.0.xsd"""
80,"xmlns=""urn:infinispan:config:14.0"""
80,"name=""mycache""/>"
80,Additional resources
80,XInclude specification
80,3.3. Creating remote caches
80,"When you create remote caches at runtime, Infinispan Server synchronizes your configuration across the cluster so that all nodes have a copy."
80,For this reason you should always create remote caches dynamically with the following mechanisms:
80,Infinispan Console
80,Infinispan Command Line Interface (CLI)
80,Hot Rod or HTTP clients
80,3.3.1. Default Cache Manager
80,Infinispan Server provides a default Cache Manager that controls the lifecycle of remote caches.
80,Starting Infinispan Server automatically instantiates the Cache Manager so you can create and delete remote caches and other resources like Protobuf schema.
80,"After you start Infinispan Server and add user credentials, you can view details about the Cache Manager and get cluster information from Infinispan Console."
80,Open 127.0.0.1:11222 in any browser.
80,You can also get information about the Cache Manager through the Command Line Interface (CLI) or REST API:
80,CLI
80,Run the describe command in the default container.
80,[//containers/default]> describe
80,REST
80,Open 127.0.0.1:11222/rest/v2/cache-managers/default/ in any browser.
80,Default Cache Manager configuration
80,XML
80,<infinispan>
80,"<!-- Creates a Cache Manager named ""default"" and enables metrics. -->"
80,"<cache-container name=""default"""
80,"statistics=""true"">"
80,<!-- Adds cluster transport that uses the default JGroups TCP stack. -->
80,"<transport cluster=""${infinispan.cluster.name:cluster}"""
80,"stack=""${infinispan.cluster.stack:tcp}"""
80,"node-name=""${infinispan.node.name:}""/>"
80,<!-- Requires user permission to access caches and perform operations. -->
80,<security>
80,<authorization/>
80,</security>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""jgroups"" : {"
80,"""transport"" : ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
80,"""cache-container"" : {"
80,"""name"" : ""default"","
80,"""statistics"" : ""true"","
80,"""transport"" : {"
80,"""cluster"" : ""cluster"","
80,"""node-name"" : """","
80,"""stack"" : ""tcp"""
80,"""security"" : {"
80,"""authorization"" : {}"
80,YAML
80,infinispan:
80,jgroups:
80,"transport: ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
80,cacheContainer:
80,"name: ""default"""
80,"statistics: ""true"""
80,transport:
80,"cluster: ""cluster"""
80,"nodeName: """""
80,"stack: ""tcp"""
80,security:
80,authorization: ~
80,3.3.2. Creating caches with Infinispan Console
80,Use Infinispan Console to create remote caches in an intuitive visual interface from any web browser.
80,Prerequisites
80,Create a Infinispan user with admin permissions.
80,Start at least one Infinispan Server instance.
80,Have a Infinispan cache configuration.
80,Procedure
80,Open 127.0.0.1:11222/console/ in any browser.
80,Select Create Cache and follow the steps as Infinispan Console guides you through the process.
80,3.3.3. Creating remote caches with the Infinispan CLI
80,Use the Infinispan Command Line Interface (CLI) to add remote caches on Infinispan Server.
80,Prerequisites
80,Create a Infinispan user with admin permissions.
80,Start at least one Infinispan Server instance.
80,Have a Infinispan cache configuration.
80,Procedure
80,Start the CLI.
80,bin/cli.sh
80,Run the connect command and enter your username and password when prompted.
80,Use the create cache command to create remote caches.
80,"For example, create a cache named ""mycache"" from a file named mycache.xml as follows:"
80,create cache --file=mycache.xml mycache
80,Verification
80,List all remote caches with the ls command.
80,ls caches
80,mycache
80,View cache configuration with the describe command.
80,describe caches/mycache
80,3.3.4. Creating remote caches from Hot Rod clients
80,"Use the Infinispan Hot Rod API to create remote caches on Infinispan Server from Java, C++, .NET/C#, JS clients and more."
80,This procedure shows you how to use Hot Rod Java clients that create remote caches on first access.
80,You can find code examples for other Hot Rod clients in the Infinispan Tutorials.
80,Prerequisites
80,Create a Infinispan user with admin permissions.
80,Start at least one Infinispan Server instance.
80,Have a Infinispan cache configuration.
80,Procedure
80,Invoke the remoteCache() method as part of your the ConfigurationBuilder.
80,Set the configuration or configuration_uri properties in the hotrod-client.properties file on your classpath.
80,ConfigurationBuilder
80,"File file = new File(""path/to/infinispan.xml"")"
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,"builder.remoteCache(""another-cache"")"
80,".configuration(""<distributed-cache name=\""another-cache\""/>"");"
80,"builder.remoteCache(""my.other.cache"")"
80,.configurationURI(file.toURI());
80,hotrod-client.properties
80,"infinispan.client.hotrod.cache.another-cache.configuration=<distributed-cache name=\""another-cache\""/>"
80,infinispan.client.hotrod.cache.[my.other.cache].configuration_uri=file:///path/to/infinispan.xml
80,"If the name of your remote cache contains the . character, you must enclose it in square brackets when using hotrod-client.properties files."
80,Additional resources
80,Hot Rod Client Configuration
80,org.infinispan.client.hotrod.configuration.RemoteCacheConfigurationBuilder
80,3.3.5. Creating remote caches with the REST API
80,Use the Infinispan REST API to create remote caches on Infinispan Server from any suitable HTTP client.
80,Prerequisites
80,Create a Infinispan user with admin permissions.
80,Start at least one Infinispan Server instance.
80,Have a Infinispan cache configuration.
80,Procedure
80,Invoke POST requests to /rest/v2/caches/<cache_name> with cache configuration in the payload.
80,Additional resources
80,Creating and Managing Caches with the REST API
80,3.4. Creating embedded caches
80,Infinispan provides an EmbeddedCacheManager API that lets you control both the Cache Manager and embedded cache lifecycles programmatically.
80,3.4.1. Adding Infinispan to your project
80,Add Infinispan to your project to create embedded caches in your applications.
80,Prerequisites
80,Configure your project to get Infinispan artifacts from the Maven repository.
80,Procedure
80,Add the infinispan-core artifact as a dependency in your pom.xml as
80,follows:
80,<dependencies>
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-core</artifactId>
80,</dependency>
80,</dependencies>
80,3.4.2. Creating and using embedded caches
80,Infinispan provides a GlobalConfigurationBuilder API that controls the Cache Manager and a ConfigurationBuilder API that configures caches.
80,Prerequisites
80,Add the infinispan-core artifact as a dependency in your pom.xml.
80,Procedure
80,Initialize a CacheManager.
80,You must always call the cacheManager.start() method to initialize a CacheManager before you can create caches.
80,Default constructors do this for you but there are overloaded versions of the constructors that do not.
80,Cache Managers are also heavyweight objects and Infinispan recommends instantiating only one instance per JVM.
80,Use the ConfigurationBuilder API to define cache configuration.
80,"Obtain caches with getCache(), createCache(), or getOrCreateCache() methods."
80,Infinispan recommends using the getOrCreateCache() method because it either creates a cache on all nodes or returns an existing cache.
80,If necessary use the PERMANENT flag for caches to survive restarts.
80,Stop the CacheManager by calling the cacheManager.stop() method to release JVM resources and gracefully shutdown any caches.
80,// Set up a clustered Cache Manager.
80,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder();
80,// Initialize the default Cache Manager.
80,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
80,// Create a distributed cache with synchronous replication.
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.clustering().cacheMode(CacheMode.DIST_SYNC);
80,// Obtain a volatile cache.
80,"Cache<String, String> cache = cacheManager.administration().withFlags(CacheContainerAdmin.AdminFlag.VOLATILE).getOrCreateCache(""myCache"", builder.build());"
80,// Stop the Cache Manager.
80,cacheManager.stop();
80,getCache() method
80,"Invoke the getCache(String) method to obtain caches, as follows:"
80,"Cache<String, String> myCache = manager.getCache(""myCache"");"
80,"The preceding operation creates a cache named myCache, if it does not already exist, and returns it."
80,"Using the getCache() method creates the cache only on the node where you invoke the method. In other words, it performs a local operation that must be invoked on each node across the cluster. Typically, applications deployed across multiple nodes obtain caches during initialization to ensure that caches are symmetric and exist on each node."
80,createCache() method
80,Invoke the createCache() method to create caches dynamically across the entire cluster.
80,"Cache<String, String> myCache = manager.administration().createCache(""myCache"", ""myTemplate"");"
80,The preceding operation also automatically creates caches on any nodes that subsequently join the cluster.
80,"Caches that you create with the createCache() method are ephemeral by default. If the entire cluster shuts down, the cache is not automatically created again when it restarts."
80,PERMANENT flag
80,Use the PERMANENT flag to ensure that caches can survive restarts.
80,"Cache<String, String> myCache = manager.administration().withFlags(AdminFlag.PERMANENT).createCache(""myCache"", ""myTemplate"");"
80,"For the PERMANENT flag to take effect, you must enable global state and set a configuration storage provider."
80,"For more information about configuration storage providers, see GlobalStateConfigurationBuilder#configurationStorage()."
80,Additional resources
80,EmbeddedCacheManager
80,EmbeddedCacheManager Configuration
80,org.infinispan.configuration.global.GlobalConfiguration
80,org.infinispan.configuration.cache.ConfigurationBuilder
80,3.4.3. Cache API
80,"Infinispan provides a Cache interface that exposes simple methods for adding, retrieving and removing entries, including atomic mechanisms exposed by the JDK’s ConcurrentMap interface."
80,"Based on the cache mode used, invoking these methods will trigger a number of things to happen, potentially even including replicating an entry to a remote node or looking up an entry from a remote node, or potentially a cache store."
80,"For simple usage, using the Cache API should be no different from using the JDK Map API, and hence migrating from simple in-memory caches based on a Map to Infinispan’s Cache should be trivial."
80,Performance Concerns of Certain Map Methods
80,"Certain methods exposed in Map have certain performance consequences when used with Infinispan, such as"
80,"size() ,"
80,"values() ,"
80,keySet() and
80,entrySet() .
80,"Specific methods on the keySet, values and entrySet are fine for use please see their Javadoc for further details."
80,Attempting to perform these operations globally would have large performance impact as well as become a scalability bottleneck.
80,"As such, these methods should only be used for informational or debugging purposes only."
80,"It should be noted that using certain flags with the withFlags() method can mitigate some of these concerns, please check each method’s documentation for more details."
80,Mortal and Immortal Data
80,"Further to simply storing entries, Infinispan’s cache API allows you to attach mortality information to data."
80,"For example, simply using put(key, value) would create an immortal entry, i.e., an entry that lives in the cache forever, until it is removed (or evicted from memory to prevent running out of memory)."
80,"If, however, you put data in the cache using put(key, value, lifespan, timeunit) , this creates a mortal entry, i.e., an entry that has a fixed lifespan and expires after that lifespan."
80,"In addition to lifespan , Infinispan also supports maxIdle as an additional metric with which to determine expiration."
80,Any combination of lifespans or maxIdles can be used.
80,putForExternalRead operation
80,Infinispan’s Cache class contains a different 'put' operation called putForExternalRead . This operation is particularly useful when Infinispan is used as a temporary cache for data that is persisted elsewhere.
80,"Under heavy read scenarios, contention in the cache should not delay the real transactions at hand, since caching should just be an optimization and not something that gets in the way."
80,"To achieve this, putForExternalRead() acts as a put call that only operates if the key is not present in the cache, and fails fast and silently if another thread is trying to store the same key at the same time. In this particular scenario, caching data is a way to optimise the system and it’s not desirable that a failure in caching affects the on-going transaction, hence why failure is handled differently. putForExternalRead() is considered to be a fast operation because regardless of whether it’s successful or not, it doesn’t wait for any locks, and so returns to the caller promptly."
80,"To understand how to use this operation, let’s look at basic example. Imagine a cache of Person instances, each keyed by a PersonId , whose data originates in a separate data store. The following code shows the most common pattern of using putForExternalRead within the context of this example:"
80,"// Id of the person to look up, provided by the application"
80,PersonId id = ...;
80,// Get a reference to the cache where person instances will be stored
80,"Cache<PersonId, Person> cache = ...;"
80,"// First, check whether the cache contains the person instance"
80,// associated with with the given id
80,Person cachedPerson = cache.get(id);
80,if (cachedPerson == null) {
80,"// The person is not cached yet, so query the data store with the id"
80,Person person = dataStore.lookup(id);
80,// Cache the person along with the id so that future requests can
80,// retrieve it from memory rather than going to the data store
80,"cache.putForExternalRead(id, person);"
80,} else {
80,"// The person was found in the cache, so return it to the application"
80,return cachedPerson;
80,"Note that putForExternalRead should never be used as a mechanism to update the cache with a new Person instance originating from application execution (i.e. from a transaction that modifies a Person’s address). When updating cached values, please use the standard put operation, otherwise the possibility of caching corrupt data is likely."
80,AdvancedCache API
80,"In addition to the simple Cache interface, Infinispan offers an AdvancedCache interface, geared towards extension authors."
80,The AdvancedCache offers the ability to access certain internal components and to apply flags to alter the default behavior of certain cache methods.
80,The following code snippet depicts how an AdvancedCache can be obtained:
80,AdvancedCache advancedCache = cache.getAdvancedCache();
80,Flags
80,Flags are applied to regular cache methods to alter the behavior of certain methods.
80,"For a list of all available flags, and their effects, see the Flag enumeration."
80,Flags are applied using AdvancedCache.withFlags() .
80,"This builder method can be used to apply any number of flags to a cache invocation, for example:"
80,"advancedCache.withFlags(Flag.CACHE_MODE_LOCAL, Flag.SKIP_LOCKING)"
80,.withFlags(Flag.FORCE_SYNCHRONOUS)
80,".put(""hello"", ""world"");"
80,Asynchronous API
80,"In addition to synchronous API methods like Cache.put() , Cache.remove() , etc., Infinispan also has an asynchronous, non-blocking API where you can achieve the same results in a non-blocking fashion."
80,"These methods are named in a similar fashion to their blocking counterparts, with ""Async"" appended.  E.g., Cache.putAsync() , Cache.removeAsync() , etc.  These asynchronous counterparts return a CompletableFuture that contains the actual result of the operation."
80,"For example, in a cache parameterized as Cache<String, String>, Cache.put(String key, String value) returns String while Cache.putAsync(String key, String value) returns CompletableFuture<String>."
80,Why use such an API?
80,Non-blocking APIs are powerful in that they provide all of the guarantees of synchronous communications - with the ability to handle communication failures and exceptions - with the ease of not having to block until a call completes.  This allows you to better harness parallelism in your system.  For example:
80,Set<CompletableFuture<?>> futures = new HashSet<>();
80,"futures.add(cache.putAsync(key1, value1)); // does not block"
80,"futures.add(cache.putAsync(key2, value2)); // does not block"
80,"futures.add(cache.putAsync(key3, value3)); // does not block"
80,// the remote calls for the 3 puts will effectively be executed
80,"// in parallel, particularly useful if running in distributed mode"
80,// and the 3 keys would typically be pushed to 3 different nodes
80,// in the cluster
80,// check that the puts completed successfully
80,for (CompletableFuture<?> f: futures) f.get();
80,Which processes actually happen asynchronously?
80,There are 4 things in Infinispan that can be considered to be on the critical path of a typical write operation.
80,"These are, in order of cost:"
80,network calls
80,marshalling
80,writing to a cache store (optional)
80,locking
80,"Using the async methods will take the network calls and marshalling off the critical path.  For various technical reasons, writing to a cache store and acquiring locks, however, still happens in the caller’s thread."
80,4. Enabling and configuring Infinispan statistics and JMX monitoring
80,Infinispan can provide Cache Manager and cache statistics as well as export JMX MBeans.
80,4.1. Enabling statistics in embedded caches
80,Configure Infinispan to export statistics for the Cache Manager and embedded caches.
80,Procedure
80,Open your Infinispan configuration for editing.
80,"Add the statistics=""true"" attribute or the .statistics(true) method."
80,Save and close your Infinispan configuration.
80,Embedded cache statistics
80,XML
80,<infinispan>
80,"<cache-container statistics=""true"">"
80,"<distributed-cache statistics=""true""/>"
80,"<replicated-cache statistics=""true""/>"
80,</cache-container>
80,</infinispan>
80,GlobalConfigurationBuilder
80,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder().cacheContainer().statistics(true);
80,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
80,Configuration builder = new ConfigurationBuilder();
80,builder.statistics().enable();
80,4.2. Enabling statistics in remote caches
80,Infinispan Server automatically enables statistics for the default Cache Manager.
80,"However, you must explicitly enable statistics for your caches."
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add the statistics attribute or field and specify true as the value.
80,Save and close your Infinispan configuration.
80,Remote cache statistics
80,XML
80,"<distributed-cache statistics=""true"" />"
80,JSON
80,"""distributed-cache"": {"
80,"""statistics"": ""true"""
80,YAML
80,distributedCache:
80,statistics: true
80,4.3. Enabling Hot Rod client statistics
80,Hot Rod Java clients can provide statistics that include remote cache and near-cache hits and misses as well as connection pool usage.
80,Procedure
80,Open your Hot Rod Java client configuration for editing.
80,Set true as the value for the statistics property or invoke the statistics().enable() methods.
80,Export JMX MBeans for your Hot Rod client with the jmx and jmx_domain properties or invoke the jmxEnable() and jmxDomain() methods.
80,Save and close your client configuration.
80,Hot Rod Java client statistics
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.statistics().enable()
80,.jmxEnable()
80,".jmxDomain(""my.domain.org"")"
80,.addServer()
80,".host(""127.0.0.1"")"
80,.port(11222);
80,RemoteCacheManager remoteCacheManager = new RemoteCacheManager(builder.build());
80,hotrod-client.properties
80,infinispan.client.hotrod.statistics = true
80,infinispan.client.hotrod.jmx = true
80,infinispan.client.hotrod.jmx_domain = my.domain.org
80,4.4. Configuring Infinispan metrics
80,Infinispan generates metrics that are compatible with any monitoring system.
80,Gauges provide values such as the average number of nanoseconds for write operations or JVM uptime.
80,"Histograms provide details about operation execution times such as read,"
80,"write, and remove times."
80,"By default, Infinispan generates gauges when you enable statistics but you can also configure it to generate histograms."
80,Infinispan metrics are provided at the vendor scope.
80,Metrics related to the JVM are provided in the base scope.
80,Prerequisites
80,You must add Micrometer Core and Micrometer Registry Prometheus JARs to your classpath to export Infinispan metrics for embedded caches.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add the metrics element or object to the cache container.
80,Enable or disable gauges with the gauges attribute or field.
80,Enable or disable histograms with the histograms attribute or field.
80,Save and close your client configuration.
80,Metrics configuration
80,XML
80,<infinispan>
80,"<cache-container statistics=""true"">"
80,"<metrics gauges=""true"""
80,"histograms=""true"" />"
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""statistics"" : ""true"","
80,"""metrics"" : {"
80,"""gauges"" : ""true"","
80,"""histograms"" : ""true"""
80,YAML
80,infinispan:
80,cacheContainer:
80,"statistics: ""true"""
80,metrics:
80,"gauges: ""true"""
80,"histograms: ""true"""
80,GlobalConfigurationBuilder
80,GlobalConfiguration globalConfig = new GlobalConfigurationBuilder()
80,//Computes and collects statistics for the Cache Manager.
80,.statistics().enable()
80,//Exports collected statistics as gauge and histogram metrics.
80,.metrics().gauges(true).histograms(true)
80,.build();
80,Verification
80,Infinispan Server exposes statistics through the metrics endpoint that you can collect with monitoring tools such as Prometheus.
80,"To verify that statistics are exported to the metrics endpoint, you can do the following:"
80,Prometheus format
80,curl -v http://localhost:11222/metrics \
80,--digest -u username:password
80,OpenMetrics format
80,curl -v http://localhost:11222/metrics \
80,--digest -u username:password \
80,"-H ""Accept: application/openmetrics-text"""
80,Infinispan no longer provides metrics in MicroProfile JSON format.
80,Additional resources
80,Micrometer Prometheus
80,4.5. Registering JMX MBeans
80,Infinispan can register JMX MBeans that you can use to collect statistics and
80,perform administrative operations.
80,You must also enable statistics otherwise Infinispan provides 0 values for all statistic attributes in JMX MBeans.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add the jmx element or object to the cache container and specify true as the value for the enabled attribute or field.
80,"Add the domain attribute or field and specify the domain where JMX MBeans are exposed, if required."
80,Save and close your client configuration.
80,JMX configuration
80,XML
80,<infinispan>
80,"<cache-container statistics=""true"">"
80,"<jmx enabled=""true"""
80,"domain=""example.com""/>"
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""statistics"" : ""true"","
80,"""jmx"" : {"
80,"""enabled"" : ""true"","
80,"""domain"" : ""example.com"""
80,YAML
80,infinispan:
80,cacheContainer:
80,"statistics: ""true"""
80,jmx:
80,"enabled: ""true"""
80,"domain: ""example.com"""
80,GlobalConfigurationBuilder
80,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
80,.jmx().enable()
80,".domain(""org.mydomain"");"
80,4.5.1. Enabling JMX remote ports
80,Provide unique remote JMX ports to expose Infinispan MBeans through connections in JMXServiceURL format.
80,Infinispan Server does not expose JMX remotely via the single port endpoint.
80,If you want to remotely access Infinispan Server via JMX you must enable a remote port.
80,You can enable remote JMX ports using one of the following approaches:
80,Enable remote JMX ports that require authentication to one of the Infinispan Server security realms.
80,Enable remote JMX ports manually using the standard Java management configuration options.
80,Prerequisites
80,"For remote JMX with authentication, define JMX specific user roles using the default security realm."
80,Users must have controlRole with read/write access or the monitorRole with read-only access to access any JMX resources.
80,Procedure
80,Start Infinispan Server with a remote JMX port enabled using one of the following ways:
80,Enable remote JMX through port 9999.
80,bin/server.sh --jmx 9999
80,Using remote JMX with SSL disabled is not intended for production environments.
80,Pass the following system properties to Infinispan Server at startup.
80,bin/server.sh -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
80,Enabling remote JMX with no authentication or SSL is not secure and not recommended in any environment.
80,Disabling authentication and SSL allows unauthorized users to connect to your server and access the data hosted there.
80,Additional resources
80,Creating security realms
80,4.5.2. Infinispan MBeans
80,Infinispan exposes JMX MBeans that represent manageable resources.
80,org.infinispan:type=Cache
80,Attributes and operations available for cache instances.
80,org.infinispan:type=CacheManager
80,"Attributes and operations available for Cache Managers, including Infinispan cache and cluster health statistics."
80,For a complete list of available JMX MBeans along with descriptions and
80,"available operations and attributes, see the Infinispan JMX Components"
80,documentation.
80,Additional resources
80,Infinispan JMX Components
80,4.5.3. Registering MBeans in custom MBean servers
80,Infinispan includes an MBeanServerLookup interface that you can use to
80,register MBeans in custom MBeanServer instances.
80,Prerequisites
80,Create an implementation of MBeanServerLookup so that the getMBeanServer() method returns the custom MBeanServer instance.
80,Configure Infinispan to register JMX MBeans.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add the mbean-server-lookup attribute or field to the JMX configuration for the Cache Manager.
80,Specify fully qualified name (FQN) of your MBeanServerLookup implementation.
80,Save and close your client configuration.
80,JMX MBean server lookup configuration
80,XML
80,<infinispan>
80,"<cache-container statistics=""true"">"
80,"<jmx enabled=""true"""
80,"domain=""example.com"""
80,"mbean-server-lookup=""com.example.MyMBeanServerLookup""/>"
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""statistics"" : ""true"","
80,"""jmx"" : {"
80,"""enabled"" : ""true"","
80,"""domain"" : ""example.com"","
80,"""mbean-server-lookup"" : ""com.example.MyMBeanServerLookup"""
80,YAML
80,infinispan:
80,cacheContainer:
80,"statistics: ""true"""
80,jmx:
80,"enabled: ""true"""
80,"domain: ""example.com"""
80,"mbeanServerLookup: ""com.example.MyMBeanServerLookup"""
80,GlobalConfigurationBuilder
80,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
80,.jmx().enable()
80,".domain(""org.mydomain"")"
80,.mBeanServerLookup(new com.acme.MyMBeanServerLookup());
80,4.6. Exporting metrics during a state transfer operation
80,You can export time metrics for clustered caches that Infinispan redistributes across nodes.
80,"A state transfer operation occurs when a clustered cache topology changes, such as a node joining or leaving a cluster."
80,"During a state transfer operation, Infinispan exports metrics from each cache, so that you can determine a cache’s status."
80,"A state transfer exposes attributes as properties, so that Infinispan can export metrics from each cache."
80,You cannot perform a state transfer operation in invalidation mode.
80,Infinispan generates time metrics that are compatible with the REST API and the JMX API.
80,Prerequisites
80,Configure Infinispan metrics.
80,"Enable metrics for your cache type, such as embedded cache or remote cache."
80,Initiate a state transfer operation by changing your clustered cache topology.
80,Procedure
80,Choose one of the following methods:
80,Configure Infinispan to use the REST API to collect metrics.
80,Configure Infinispan to use the JMX API to collect metrics.
80,Additional resources
80,Enabling and configuring Infinispan statistics and JMX monitoring (Infinispan caches)
80,StateTransferManager (Infinispan 14.0 API)
80,4.7. Monitoring the status of cross-site replication
80,Monitor the site status of your backup locations to detect interruptions in the communication between the sites.
80,"When a remote site status changes to offline, Infinispan stops replicating your data to the backup location."
80,Your data become out of sync and you must fix the inconsistencies before bringing the clusters back online.
80,Monitoring cross-site events is necessary for early problem detection.
80,Use one of the following monitoring strategies:
80,Monitoring cross-site replication with the REST API
80,Monitoring cross-site replication with the Prometheus metrics or any other monitoring system
80,Monitoring cross-site replication with the REST API
80,Monitor the status of cross-site replication for all caches using the REST endpoint.
80,You can implement a custom script to poll the REST endpoint or use the following example.
80,Prerequisites
80,Enable cross-site replication.
80,Procedure
80,Implement a script to poll the REST endpoint.
80,The following example demonstrates how you can use a Python script to poll the site status every five seconds.
80,#!/usr/bin/python3
80,import time
80,import requests
80,from requests.auth import HTTPDigestAuth
80,class InfinispanConnection:
80,"def __init__(self, server: str = 'http://localhost:11222', cache_manager: str = 'default',"
80,"auth: tuple = ('admin', 'change_me')) -> None:"
80,super().__init__()
80,self.__url = f'{server}/rest/v2/cache-managers/{cache_manager}/x-site/backups/'
80,self.__auth = auth
80,self.__headers = {
80,'accept': 'application/json'
80,def get_sites_status(self):
80,try:
80,"rsp = requests.get(self.__url, headers=self.__headers, auth=HTTPDigestAuth(self.__auth[0], self.__auth[1]))"
80,if rsp.status_code != 200:
80,return None
80,return rsp.json()
80,except:
80,return None
80,# Specify credentials for Infinispan user with permission to access the REST endpoint
80,USERNAME = 'admin'
80,PASSWORD = 'change_me'
80,# Set an interval between cross-site status checks
80,POLL_INTERVAL_SEC = 5
80,# Provide a list of servers
80,SERVERS = [
80,"InfinispanConnection('http://127.0.0.1:11222', auth=(USERNAME, PASSWORD)),"
80,"InfinispanConnection('http://127.0.0.1:12222', auth=(USERNAME, PASSWORD))"
80,#Specify the names of remote sites
80,REMOTE_SITES = [
80,'nyc'
80,#Provide a list of caches to monitor
80,CACHES = [
80,"'work',"
80,'sessions'
80,"def on_event(site: str, cache: str, old_status: str, new_status: str):"
80,# TODO implement your handling code here
80,print(f'site={site} cache={cache} Status changed {old_status} -> {new_status}')
80,"def __handle_mixed_state(state: dict, site: str, site_status: dict):"
80,if site not in state:
80,state[site] = {c: 'online' if c in site_status['online'] else 'offline' for c in CACHES}
80,return
80,for cache in CACHES:
80,"__update_cache_state(state, site, cache, 'online' if cache in site_status['online'] else 'offline')"
80,"def __handle_online_or_offline_state(state: dict, site: str, new_status: str):"
80,if site not in state:
80,state[site] = {c: new_status for c in CACHES}
80,return
80,for cache in CACHES:
80,"__update_cache_state(state, site, cache, new_status)"
80,"def __update_cache_state(state: dict, site: str, cache: str, new_status: str):"
80,old_status = state[site].get(cache)
80,if old_status != new_status:
80,"on_event(site, cache, old_status, new_status)"
80,state[site][cache] = new_status
80,def update_state(state: dict):
80,rsp = None
80,for conn in SERVERS:
80,rsp = conn.get_sites_status()
80,if rsp:
80,break
80,if rsp is None:
80,print('Unable to fetch site status from any server')
80,return
80,for site in REMOTE_SITES:
80,"site_status = rsp.get(site, {})"
80,new_status = site_status.get('status')
80,if new_status == 'mixed':
80,"__handle_mixed_state(state, site, site_status)"
80,else:
80,"__handle_online_or_offline_state(state, site, new_status)"
80,if __name__ == '__main__':
80,_state = {}
80,while True:
80,update_state(_state)
80,time.sleep(POLL_INTERVAL_SEC)
80,"When a site status changes from online to offline or vice-versa, the function on_event is invoked."
80,"If you want to use this script, you must specify the following variables:"
80,USERNAME and PASSWORD: The username and password of Infinispan user with permission to access the REST endpoint.
80,POLL_INTERVAL_SEC: The number of seconds between polls.
80,SERVERS: The list of Infinispan Servers at this site.
80,The script only requires a single valid response but the list is provided to allow fail over.
80,REMOTE_SITES: The list of remote sites to monitor on these servers.
80,CACHES: The list of cache names to monitor.
80,Additional resources
80,REST API: Getting status of backup locations
80,Monitoring cross-site replication with the Prometheus metrics
80,"Prometheus, and other monitoring systems, let you configure alerts to detect when a site status changes to offline."
80,Monitoring cross-site latency metrics can help you to discover potential issues.
80,Prerequisites
80,Enable cross-site replication.
80,Procedure
80,Configure Infinispan metrics.
80,Configure alerting rules using the Prometheus metrics format.
80,"For the site status, use 1 for online and 0 for offline."
80,"For the expr filed, use the following format:"
80,vendor_cache_manager_default_cache_<cache name>_x_site_admin_<site name>_status.
80,"In the following example, Prometheus alerts you when the NYC site gets offline for cache named work or sessions."
80,groups:
80,- name: Cross Site Rules
80,rules:
80,- alert: Cache Work and Site NYC
80,expr: vendor_cache_manager_default_cache_work_x_site_admin_nyc_status == 0
80,- alert: Cache Sessions and Site NYC
80,expr: vendor_cache_manager_default_cache_sessions_x_site_admin_nyc_status == 0
80,The following image shows an alert that the NYC site is offline for cache work.
80,Figure 6. Prometheus Alert
80,Additional resources
80,Configuring Infinispan metrics
80,Prometheus Alerting Overview
80,Grafana Alerting Documentation
80,Openshift Managing Alerts
80,5. Configuring JVM memory usage
80,Control how Infinispan stores data in JVM memory by:
80,Managing JVM memory usage with eviction that automatically removes data from caches.
80,Adding lifespan and maximum idle times to expire entries and prevent stale data.
80,"Configuring Infinispan to store data in off-heap, native memory."
80,5.1. Default memory configuration
80,By default Infinispan stores cache entries as objects in the JVM heap.
80,"Over time, as applications add entries, the size of caches can exceed the amount of memory that is available to the JVM."
80,"Likewise, if Infinispan is not the primary data store, then entries become out of date which means your caches contain stale data."
80,XML
80,<distributed-cache>
80,"<memory storage=""HEAP""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""memory"" : {"
80,"""storage"": ""HEAP"""
80,YAML
80,distributedCache:
80,memory:
80,"storage: ""HEAP"""
80,5.2. Eviction and expiration
80,Eviction and expiration are two strategies for cleaning the data container by
80,"removing old, unused entries."
80,"Although eviction and expiration are similar, they have some important differences."
80,Eviction lets Infinispan control the size of the data container by removing entries when the container becomes larger than a configured threshold.
80,Expiration limits the amount of time entries can exist. Infinispan uses
80,a scheduler to periodically remove expired entries. Entries that are expired
80,but not yet removed are immediately removed on access; in this case get()
80,"calls for expired entries return ""null"" values."
80,Eviction is local to Infinispan nodes.
80,Expiration takes place across Infinispan clusters.
80,You can use eviction and expiration together or independently of each other.
80,You can configure eviction and expiration declaratively in infinispan.xml to apply cache-wide defaults for entries.
80,You can explicitly define expiration settings for specific entries but you cannot define eviction on a per-entry basis.
80,You can manually evict entries and manually trigger expiration.
80,5.3. Eviction with Infinispan caches
80,Eviction lets you control the size of the data container by removing entries from memory in one of two ways:
80,Total number of entries (max-count).
80,Maximum amount of memory (max-size).
80,Eviction drops one entry from the data container at a time and is local to the node on which it occurs.
80,Eviction removes entries from memory but not from persistent cache stores.
80,"To ensure that entries remain available after Infinispan evicts them, and to prevent inconsistencies with your data, you should configure persistent storage."
80,"When you configure memory, Infinispan approximates the current memory usage of the data container."
80,"When entries are added or modified, Infinispan compares the current memory usage of the data container to the maximum size."
80,"If the size exceeds the maximum, Infinispan performs eviction."
80,Eviction happens immediately in the thread that adds an entry that exceeds the maximum size.
80,5.3.1. Eviction strategies
80,When you configure Infinispan eviction you specify:
80,The maximum size of the data container.
80,A strategy for removing entries when the cache reaches the threshold.
80,You can either perform eviction manually or configure Infinispan to do one of the following:
80,Remove old entries to make space for new ones.
80,Throw ContainerFullException and prevent new entries from being created.
80,The exception eviction strategy works only with transactional caches that use 2 phase commits; not with 1 phase commits or synchronization optimizations.
80,Refer to the schema reference for more details about the eviction strategies.
80,Infinispan includes the Caffeine caching library that implements a variation
80,of the Least Frequently Used (LFU) cache replacement algorithm known as
80,"TinyLFU. For off-heap storage, Infinispan uses a custom implementation of the"
80,Least Recently Used (LRU) algorithm.
80,Additional resources
80,Caffeine
80,Infinispan configuration schema reference
80,5.3.2. Configuring maximum count eviction
80,Limit the size of Infinispan caches to a total number of entries.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Specify the total number of entries that caches can contain before
80,Infinispan performs eviction with either the max-count attribute or maxCount() method.
80,Set one of the following as the eviction strategy to control how Infinispan removes entries with the when-full attribute or whenFull() method.
80,REMOVE Infinispan performs eviction. This is the default strategy.
80,MANUAL You perform eviction manually for embedded caches.
80,EXCEPTION Infinispan throws an exception instead of evicting entries.
80,Save and close your Infinispan configuration.
80,Maximum count eviction
80,"In the following example, Infinispan removes an entry when the cache contains a total of 500 entries and a new entry is created:"
80,XML
80,<distributed-cache>
80,"<memory max-count=""500"" when-full=""REMOVE""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"" : {"
80,"""memory"" : {"
80,"""max-count"" : ""500"","
80,"""when-full"" : ""REMOVE"""
80,YAML
80,distributedCache:
80,memory:
80,"maxCount: ""500"""
80,"whenFull: ""REMOVE"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.memory().maxCount(500).whenFull(EvictionStrategy.REMOVE);
80,Additional resources
80,Infinispan configuration schema reference
80,org.infinispan.configuration.cache.MemoryConfigurationBuilder
80,5.3.3. Configuring maximum size eviction
80,Limit the size of Infinispan caches to a maximum amount of memory.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Specify application/x-protostream as the media type for cache encoding.
80,You must specify a binary media type to use maximum size eviction.
80,"Configure the maximum amount of memory, in bytes, that caches can use before"
80,Infinispan performs eviction with the max-size attribute or maxSize() method.
80,Optionally specify a byte unit of measurement.
80,The default is B (bytes). Refer to the configuration schema for supported units.
80,Set one of the following as the eviction strategy to control how Infinispan removes entries with either the when-full attribute or whenFull() method.
80,REMOVE Infinispan performs eviction. This is the default strategy.
80,MANUAL You perform eviction manually for embedded caches.
80,EXCEPTION Infinispan throws an exception instead of evicting entries.
80,Save and close your Infinispan configuration.
80,Maximum size eviction
80,"In the following example, Infinispan removes an entry when the size of the cache reaches 1.5 GB (gigabytes) and a new entry is created:"
80,XML
80,<distributed-cache>
80,"<encoding media-type=""application/x-protostream""/>"
80,"<memory max-size=""1.5GB"" when-full=""REMOVE""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"" : {"
80,"""encoding"" : {"
80,"""media-type"" : ""application/x-protostream"""
80,"""memory"" : {"
80,"""max-size"" : ""1.5GB"","
80,"""when-full"" : ""REMOVE"""
80,YAML
80,distributedCache:
80,encoding:
80,"mediaType: ""application/x-protostream"""
80,memory:
80,"maxSize: ""1.5GB"""
80,"whenFull: ""REMOVE"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,"builder.encoding().mediaType(""application/x-protostream"")"
80,.memory()
80,".maxSize(""1.5GB"")"
80,.whenFull(EvictionStrategy.REMOVE);
80,Additional resources
80,Infinispan configuration schema reference
80,org.infinispan.configuration.cache.EncodingConfiguration
80,org.infinispan.configuration.cache.MemoryConfigurationBuilder
80,Cache Encoding and Marshalling
80,5.3.4. Manual eviction
80,"If you choose the manual eviction strategy, Infinispan does not perform eviction."
80,You must do so manually with the evict() method.
80,You should use manual eviction with embedded caches only.
80,"For remote caches, you should always configure Infinispan with the REMOVE or EXCEPTION eviction strategy."
80,This configuration prevents a warning message when you enable passivation but do not configure eviction.
80,XML
80,<distributed-cache>
80,"<memory max-count=""500"" when-full=""MANUAL""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"" : {"
80,"""memory"" : {"
80,"""max-count"" : ""500"","
80,"""when-full"" : ""MANUAL"""
80,YAML
80,distributedCache:
80,memory:
80,"maxCount: ""500"""
80,"whenFull: ""MANUAL"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,"builder.encoding().mediaType(""application/x-protostream"")"
80,.memory()
80,".maxSize(""1.5GB"")"
80,.whenFull(EvictionStrategy.REMOVE);
80,5.3.5. Passivation with eviction
80,Passivation persists data to cache stores when Infinispan evicts entries.
80,"You should always enable eviction if you enable passivation, as in the following examples:"
80,XML
80,<distributed-cache>
80,"<persistence passivation=""true"">"
80,<!-- Persistent storage configuration. -->
80,</persistence>
80,"<memory max-count=""100""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""memory"" : {"
80,"""max-count"" : ""100"""
80,"""persistence"" : {"
80,"""passivation"" : true"
80,YAML
80,distributedCache:
80,memory:
80,"maxCount: ""100"""
80,persistence:
80,"passivation: ""true"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.memory().maxCount(100);
80,builder.persistence().passivation(true); //Persistent storage configuration
80,5.4. Expiration with lifespan and maximum idle
80,Expiration configures Infinispan to remove entries from caches when they reach one of the following time limits:
80,Lifespan
80,Sets the maximum amount of time that entries can exist.
80,Maximum idle
80,Specifies how long entries can remain idle. If operations do not occur for
80,"entries, they become idle."
80,Maximum idle expiration does not currently support caches with persistent storage.
80,"If you use expiration and eviction with the EXCEPTION eviction strategy, entries that are expired, but not yet removed from the cache, count towards the size of the data container."
80,5.4.1. How expiration works
80,"When you configure expiration, Infinispan stores keys with metadata that"
80,determines when entries expire.
80,Lifespan uses a creation timestamp and the value for the lifespan configuration property.
80,Maximum idle uses a last used timestamp and the value for the max-idle configuration property.
80,Infinispan checks if lifespan or maximum idle metadata is set and then
80,compares the values with the current time.
80,If (creation + lifespan < currentTime) or (lastUsed + maxIdle < currentTime) then Infinispan detects that the entry is expired.
80,Expiration occurs whenever entries are accessed or found by the expiration
80,reaper.
80,"For example, k1 reaches the maximum idle time and a client makes a"
80,"Cache.get(k1) request. In this case, Infinispan detects that the entry is"
80,expired and removes it from the data container. The Cache.get(k1) request returns null.
80,"Infinispan also expires entries from cache stores, but only with lifespan"
80,expiration. Maximum idle expiration does not work with cache stores. In the
80,"case of cache loaders, Infinispan cannot expire entries because loaders can"
80,only read from external storage.
80,Infinispan adds expiration metadata as long primitive data types to cache
80,entries. This can increase the size of keys by as much as 32 bytes.
80,5.4.2. Expiration reaper
80,Infinispan uses a reaper thread that runs periodically to detect and remove
80,expired entries. The expiration reaper ensures that expired entries that are no
80,longer accessed are removed.
80,The Infinispan ExpirationManager interface handles the expiration reaper and
80,exposes the processExpiration() method.
80,"In some cases, you can disable the expiration reaper and manually expire"
80,"entries by calling processExpiration(); for instance, if you are using local"
80,cache mode with a custom application where a maintenance thread runs
80,periodically.
80,"If you use clustered cache modes, you should never disable the expiration"
80,reaper.
80,Infinispan always uses the expiration reaper when using cache stores. In this
80,case you cannot disable it.
80,Additional resources
80,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
80,org.infinispan.expiration.ExpirationManager
80,5.4.3. Maximum idle and clustered caches
80,Because maximum idle expiration relies on the last access time for cache
80,"entries, it has some limitations with clustered cache modes."
80,"With lifespan expiration, the creation time for cache entries provides a value"
80,"that is consistent across clustered caches. For example, the creation time for"
80,k1 is always the same on all nodes.
80,"For maximum idle expiration with clustered caches, last access time for entries"
80,is not always the same on all nodes. To ensure that entries have the same
80,"relative access times across clusters, Infinispan sends touch commands to all"
80,owners when keys are accessed.
80,The touch commands that Infinispan send have the following considerations:
80,Cache.get() requests do not return until all touch commands complete. This synchronous behavior increases latency of client requests.
80,"The touch command also updates the ""recently accessed"" metadata for cache entries on all owners, which Infinispan uses for eviction."
80,"With scattered cache mode, Infinispan sends touch commands to all nodes, not just primary and backup owners."
80,Additional information
80,Maximum idle expiration does not work with invalidation mode.
80,Iteration across a clustered cache can return expired entries that have
80,exceeded the maximum idle time limit. This behavior ensures performance because
80,no remote invocations are performed during the iteration. Also note that
80,iteration does not refresh any expired entries.
80,5.4.4. Configuring lifespan and maximum idle times for caches
80,Set lifespan and maximum idle times for all entries in a cache.
80,Procedure
80,Open your Infinispan configuration for editing.
80,"Specify the amount of time, in milliseconds, that entries can stay in the cache with the lifespan attribute or lifespan() method."
80,"Specify the amount of time, in milliseconds, that entries can remain idle after last access with the max-idle attribute or maxIdle() method."
80,Save and close your Infinispan configuration.
80,Expiration for Infinispan caches
80,"In the following example, Infinispan expires all cache entries after 5 seconds or 1 second after the last access time, whichever happens first:"
80,XML
80,<replicated-cache>
80,"<expiration lifespan=""5000"" max-idle=""1000"" />"
80,</replicated-cache>
80,JSON
80,"""replicated-cache"" : {"
80,"""expiration"" : {"
80,"""lifespan"" : ""5000"","
80,"""max-idle"" : ""1000"""
80,YAML
80,replicatedCache:
80,expiration:
80,"lifespan: ""5000"""
80,"maxIdle: ""1000"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,"builder.expiration().lifespan(5000, TimeUnit.MILLISECONDS)"
80,".maxIdle(1000, TimeUnit.MILLISECONDS);"
80,5.4.5. Configuring lifespan and maximum idle times per entry
80,Specify lifespan and maximum idle times for individual entries.
80,"When you add lifespan and maximum idle times to entries, those values take priority over expiration configuration for caches."
80,"When you explicitly define lifespan and maximum idle time values for cache entries, Infinispan replicates those values across the cluster along with the cache entries."
80,"Likewise, Infinispan writes expiration values along with the entries to persistent storage."
80,Procedure
80,"For remote caches, you can add lifespan and maximum idle times to entries interactively with the Infinispan Console."
80,"With the Infinispan Command Line Interface (CLI), use the --max-idle= and --ttl= arguments with the put command."
80,"For both remote and embedded caches, you can add lifespan and maximum idle times with cache.put() invocations."
80,//Lifespan of 5 seconds.
80,//Maximum idle time of 1 second.
80,"cache.put(""hello"", ""world"", 5, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
80,//Lifespan is disabled with a value of -1.
80,//Maximum idle time of 1 second.
80,"cache.put(""hello"", ""world"", -1, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
80,Additional resources
80,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
80,org.infinispan.expiration.ExpirationManager
80,5.5. JVM heap and off-heap memory
80,Infinispan stores cache entries in JVM heap memory by default.
80,"You can configure Infinispan to use off-heap storage, which means that your data occupies native memory outside the managed JVM memory space."
80,The following diagram is a simplified illustration of the memory space for a JVM process where Infinispan is running:
80,Figure 7. JVM memory space
80,JVM heap memory
80,The heap is divided into young and old generations that help keep referenced Java objects and other application data in memory.
80,"The GC process reclaims space from unreachable objects, running more frequently on the young generation memory pool."
80,"When Infinispan stores cache entries in JVM heap memory, GC runs can take longer to complete as you start adding data to your caches."
80,"Because GC is an intensive process, longer and more frequent runs can degrade application performance."
80,Off-heap memory
80,Off-heap memory is native available system memory outside JVM memory management.
80,The JVM memory space diagram shows the Metaspace memory pool that holds class metadata and is allocated from native memory.
80,The diagram also represents a section of native memory that holds Infinispan cache entries.
80,Off-heap memory:
80,Uses less memory per entry.
80,Improves overall JVM performance by avoiding Garbage Collector (GC) runs.
80,"One disadvantage, however, is that JVM heap dumps do not show entries stored in off-heap memory."
80,5.5.1. Off-heap data storage
80,"When you add entries to off-heap caches, Infinispan dynamically allocates native memory to your data."
80,Infinispan hashes the serialized byte[] for each key into buckets that are similar to a standard Java HashMap.
80,Buckets include address pointers that Infinispan uses to locate entries that you store in off-heap memory.
80,"Even though Infinispan stores cache entries in native memory, run-time operations require JVM heap representations of those objects."
80,"For instance, cache.get() operations read objects into heap memory before returning."
80,"Likewise, state transfer operations hold subsets of objects in heap memory while they take place."
80,Object equality
80,Infinispan determines equality of Java objects in off-heap storage using the serialized byte[] representation of each object instead of the object instance.
80,Data consistency
80,Infinispan uses an array of locks to protect off-heap address spaces.
80,The number of locks is twice the number of cores and then rounded to the nearest power of two.
80,This ensures that there is an even distribution of ReadWriteLock instances to prevent write operations from blocking read operations.
80,5.5.2. Configuring off-heap memory
80,Configure Infinispan to store cache entries in native memory outside the JVM
80,heap space.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Set OFF_HEAP as the value for the storage attribute or storage() method.
80,Set a boundary for the size of the cache by configuring eviction.
80,Save and close your Infinispan configuration.
80,Off-heap storage
80,Infinispan stores cache entries as bytes in native memory.
80,Eviction happens when there are 100 entries in the data container and Infinispan gets a request to create a new entry:
80,XML
80,<replicated-cache>
80,"<memory storage=""OFF_HEAP"" max-count=""500""/>"
80,</replicated-cache>
80,JSON
80,"""replicated-cache"" : {"
80,"""memory"" : {"
80,"""storage"" : ""OFF_HEAP"","
80,"""max-count"" : ""500"""
80,YAML
80,replicatedCache:
80,memory:
80,"storage: ""OFF_HEAP"""
80,"maxCount: ""500"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.memory().storage(StorageType.OFF_HEAP).maxCount(500);
80,Additional resources
80,Infinispan configuration schema reference
80,org.infinispan.configuration.cache.MemoryConfigurationBuilder
80,6. Configuring persistent storage
80,Infinispan uses cache stores and loaders to interact with persistent storage.
80,Durability
80,Adding cache stores allows you to persist data to non-volatile storage so it
80,survives restarts.
80,Write-through caching
80,Configuring Infinispan as a caching layer in front of persistent storage
80,simplifies data access for applications because Infinispan handles all
80,interactions with the external storage.
80,Data overflow
80,Using eviction and passivation techniques ensures that Infinispan keeps only
80,frequently used data in-memory and writes older entries to persistent storage.
80,6.1. Passivation
80,Passivation configures Infinispan to write entries to cache stores when it
80,"evicts those entries from memory. In this way, passivation prevents unnecessary"
80,and potentially expensive writes to persistent storage.
80,Activation is the process of restoring entries to memory from the cache store
80,"when there is an attempt to access passivated entries. For this reason, when you"
80,"enable passivation, you must configure cache stores that implement both"
80,CacheWriter and CacheLoader interfaces so they can write and load entries
80,from persistent storage.
80,"When Infinispan evicts an entry from the cache, it notifies cache listeners"
80,that the entry is passivated then stores the entry in the cache store. When
80,"Infinispan gets an access request for an evicted entry, it lazily loads the"
80,entry from the cache store into memory and then notifies cache listeners that
80,the entry is activated while keeping the value still in the store.
80,Passivation uses the first cache loader in the Infinispan configuration and
80,ignores all others.
80,Passivation is not supported with:
80,Transactional stores. Passivation writes and removes entries from the store
80,outside the scope of the actual Infinispan commit boundaries.
80,Shared stores. Shared cache stores require entries to always exist in the
80,"store for other owners. For this reason, passivation is not supported because"
80,entries cannot be removed.
80,"If you enable passivation with transactional stores or shared stores,"
80,Infinispan throws an exception.
80,6.1.1. How passivation works
80,Passivation disabled
80,Writes to data in memory result in writes to persistent storage.
80,"If Infinispan evicts data from memory, then data in persistent storage"
80,includes entries that are evicted from memory. In this way persistent storage
80,is a superset of the in-memory cache.
80,This is recommended when you require highest consistency as the store will be able to be read again after a crash.
80,"If you do not configure eviction, then data in persistent storage provides a"
80,copy of data in memory.
80,Passivation enabled
80,Infinispan adds data to persistent storage only when it evicts data from
80,"memory, an entry is removed or upon shutting down the node."
80,"When Infinispan activates entries, it restores data in memory but keeps the data in the store still."
80,"This allows for writes to be just as fast as without a store, and still maintains consistency."
80,When an entry is created or updated only the in memory will be updated and thus
80,the store will be outdated for the time being.
80,Passivation is not supported when a store is also configured as shared.
80,This is due to entries can become out of sync between nodes depending on when a write is evicted versus read.
80,To gurantee data consistency any store that is not shared should always have purgeOnStartup enabled.
80,This is true for both passivation enabled or disabled since a store could hold an outdated entry while down and resurrect it at a later point.
80,The following table shows data in memory and in persistent storage after a
80,series of operations:
80,Operation
80,Passivation disabled
80,Passivation enabled
80,Insert k1.
80,Memory: k1
80,Disk: k1
80,Memory: k1
80,Disk: -
80,Insert k2.
80,"Memory: k1, k2"
80,"Disk: k1, k2"
80,"Memory: k1, k2"
80,Disk: -
80,Eviction thread runs and evicts k1.
80,Memory: k2
80,"Disk: k1, k2"
80,Memory: k2
80,Disk: k1
80,Read k1.
80,"Memory: k1, k2"
80,"Disk: k1, k2"
80,"Memory: k1, k2"
80,Disk: k1
80,Eviction thread runs and evicts k2.
80,Memory: k1
80,"Disk: k1, k2"
80,Memory: k1
80,"Disk: k1, k2"
80,Remove k2.
80,Memory: k1
80,Disk: k1
80,Memory: k1
80,Disk: k1
80,6.2. Write-through cache stores
80,Write-through is a cache writing mode where writes to memory and writes to
80,"cache stores are synchronous. When a client application updates a cache entry,"
80,"in most cases by invoking Cache.put(), Infinispan does not return the call"
80,until it updates the cache store. This cache writing mode results in updates to
80,the cache store concluding within the boundaries of the client thread.
80,The primary advantage of write-through mode is that the cache and cache store
80,"are updated simultaneously, which ensures that the cache store is always"
80,consistent with the cache.
80,"However, write-through mode can potentially decrease performance because the"
80,need to access and update cache stores directly adds latency to cache
80,operations.
80,Write-through configuration
80,Infinispan uses write-through mode unless you explicitly add write-behind configuration to your caches.
80,There is no separate element or method for configuring write-through mode.
80,"For example, the following configuration adds a file-based store to the cache that implicitly uses write-through mode:"
80,<distributed-cache>
80,"<persistence passivation=""false"">"
80,<file-store>
80,"<index path=""path/to/index"" />"
80,"<data path=""path/to/data"" />"
80,</file-store>
80,</persistence>
80,</distributed-cache>
80,6.3. Write-behind cache stores
80,Write-behind is a cache writing mode where writes to memory are synchronous
80,and writes to cache stores are asynchronous.
80,"When clients send write requests, Infinispan adds those operations to a"
80,modification queue. Infinispan processes operations as they join the queue so
80,that the calling thread is not blocked and the operation completes immediately.
80,If the number of write operations in the modification queue increases beyond
80,"the size of the queue, Infinispan adds those additional operations to the"
80,"queue. However, those operations do not complete until Infinispan processes"
80,operations that are already in the queue.
80,"For example, calling Cache.putAsync returns immediately and the Stage also"
80,completes immediately if the modification queue is not full. If the
80,"modification queue is full, or if Infinispan is currently processing a batch"
80,"of write operations, then Cache.putAsync returns immediately and the Stage"
80,completes later.
80,Write-behind mode provides a performance advantage over write-through mode
80,because cache operations do not need to wait for updates to the underlying cache
80,"store to complete. However, data in the cache store remains inconsistent with"
80,"data in the cache until the modification queue is processed. For this reason,"
80,"write-behind mode is suitable for cache stores with low latency, such as"
80,"unshared and local file-based cache stores, where the time between the"
80,write to the cache and the write to the cache store is as small as possible.
80,Write-behind configuration
80,XML
80,<distributed-cache>
80,<persistence>
80,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
80,"dialect=""H2"""
80,"shared=""true"""
80,"table-name=""books"">"
80,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
80,"username=""sa"""
80,"password=""changeme"""
80,"driver=""org.h2.Driver""/>"
80,"<write-behind modification-queue-size=""2048"""
80,"fail-silently=""true""/>"
80,</table-jdbc-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"" : {"
80,"""table-jdbc-store"": {"
80,"""dialect"": ""H2"","
80,"""shared"": ""true"","
80,"""table-name"": ""books"","
80,"""connection-pool"": {"
80,"""connection-url"": ""jdbc:h2:mem:infinispan"","
80,"""driver"": ""org.h2.Driver"","
80,"""username"": ""sa"","
80,"""password"": ""changeme"""
80,"""write-behind"" : {"
80,"""modification-queue-size"" : ""2048"","
80,"""fail-silently"" : true"
80,YAML
80,distributedCache:
80,persistence:
80,tableJdbcStore:
80,"dialect: ""H2"""
80,"shared: ""true"""
80,"tableName: ""books"""
80,connectionPool:
80,"connectionUrl: ""jdbc:h2:mem:infinispan"""
80,"driver: ""org.h2.Driver"""
80,"username: ""sa"""
80,"password: ""changeme"""
80,writeBehind:
80,"modificationQueueSize: ""2048"""
80,"failSilently: ""true"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence()
80,.async()
80,.modificationQueueSize(2048)
80,.failSilently(true);
80,Failing silently
80,Write-behind configuration includes a fail-silently parameter that controls what happens when either the cache store is unavailable or the modification queue is full.
80,"If fail-silently=""true"" then Infinispan logs WARN messages and rejects"
80,write operations.
80,"If fail-silently=""false"" then Infinispan throws exceptions if it detects"
80,the cache store is unavailable during a write operation. Likewise if the
80,"modification queue becomes full, Infinispan throws an exception."
80,"In some cases, data loss can occur if Infinispan restarts and write operations"
80,"exist in the modification queue. For example the cache store goes offline but,"
80,"during the time it takes to detect that the cache store is unavailable, write"
80,operations are added to the modification queue because it is not full. If
80,Infinispan restarts or otherwise becomes unavailable before the cache store
80,"comes back online, then the write operations in the modification queue are lost"
80,because they were not persisted.
80,6.4. Segmented cache stores
80,Cache stores can organize data into hash space segments to which keys map.
80,"Segmented stores increase read performance for bulk operations; for example,"
80,"streaming over data (Cache.size, Cache.entrySet.stream), pre-loading the"
80,"cache, and doing state transfer operations."
80,"However, segmented stores can also result in loss of performance for write"
80,operations. This performance loss applies particularly to batch write
80,operations that can take place with transactions or write-behind stores. For
80,"this reason, you should evaluate the overhead for write operations before you"
80,enable segmented stores. The performance gain for bulk read operations might
80,not be acceptable if there is a significant performance loss for write
80,operations.
80,The number of segments you configure for cache stores must match the number of
80,segments you define in the Infinispan configuration with the
80,clustering.hash.numSegments parameter.
80,If you change the numSegments parameter in the configuration after you add a
80,"segmented cache store, Infinispan cannot read data from that cache store."
80,6.5. Shared cache stores
80,Infinispan cache stores can be local to a given node or shared across all nodes in the cluster.
80,"By default, cache stores are local (shared=""false"")."
80,"Local cache stores are unique to each node; for example, a file-based cache store that persists data to the host filesystem."
80,"Local cache stores should use ""purge on startup"" to avoid loading stale entries from persistent storage."
80,"Shared cache stores allow multiple nodes to use the same persistent storage; for example, a JDBC cache store that allows multiple nodes to access the same database."
80,"Shared cache stores ensure that only the primary owner write to persistent storage, instead of backup nodes performing write operations for every modification."
80,"Purging deletes data, which is not typically the desired behavior for persistent storage."
80,Local cache store
80,<persistence>
80,"<store shared=""false"""
80,"purge=""true""/>"
80,</persistence>
80,Shared cache store
80,<persistence>
80,"<store shared=""true"""
80,"purge=""false""/>"
80,</persistence>
80,Additional resources
80,Infinispan Configuration Schema
80,6.6. Transactions with persistent cache stores
80,Infinispan supports transactional operations with JDBC-based cache stores only.
80,"To configure caches as transactional, you set transactional=true to keep data in persistent storage synchronized with data in memory."
80,"For all other cache stores, Infinispan does not enlist cache loaders in transactional operations."
80,This can result in data inconsistency if transactions succeed in modifying data in memory but do not completely apply changes to data in the cache store.
80,In these cases manual recovery is not possible with cache stores.
80,6.7. Global persistent location
80,Infinispan preserves global state so that it can restore cluster topology and cached data after restart.
80,Remote caches
80,Infinispan Server saves cluster state to the $ISPN_HOME/server/data directory.
80,You should never delete or modify the server/data directory or its content.
80,Infinispan restores cluster state from this directory when you restart your server instances.
80,Changing the default configuration or directly modifying the server/data directory can cause unexpected behavior and lead to data loss.
80,Embedded caches
80,Infinispan defaults to the user.dir system property as the global persistent location.
80,In most cases this is the directory where your application starts.
80,"For clustered embedded caches, such as replicated or distributed, you should always enable and configure a global persistent location to restore cluster topology."
80,You should never configure an absolute path for a file-based cache store that is outside the global persistent location.
80,"If you do, Infinispan writes the following exception to logs:"
80,"ISPN000558: ""The store location 'foo' is not a child of the global persistent location 'bar'"""
80,6.7.1. Configuring the global persistent location
80,Enable and configure the location where Infinispan stores global state for clustered embedded caches.
80,Infinispan Server enables global persistence and configures a default location.
80,You should not disable global persistence or change the default configuration for remote caches.
80,Prerequisites
80,Add Infinispan to your project.
80,Procedure
80,Enable global state in one of the following ways:
80,Add the global-state element to your Infinispan configuration.
80,Call the globalState().enable() methods in the GlobalConfigurationBuilder API.
80,Define whether the global persistent location is unique to each node or shared between the cluster.
80,Location type
80,Configuration
80,Unique to each node
80,persistent-location element or persistentLocation() method
80,Shared between the cluster
80,shared-persistent-location element or sharedPersistentLocation(String) method
80,Set the path where Infinispan stores cluster state.
80,"For example, file-based cache stores the path is a directory on the host filesystem."
80,Values can be:
80,Absolute and contain the full location including the root.
80,Relative to a root location.
80,"If you specify a relative value for the path, you must also specify a system property that resolves to a root location."
80,"For example, on a Linux host system you set global/state as the path."
80,You also set the my.data property that resolves to the /opt/data root location.
80,In this case Infinispan uses /opt/data/global/state as the global persistent location.
80,Global persistent location configuration
80,XML
80,<infinispan>
80,<cache-container>
80,<global-state>
80,"<persistent-location path=""global/state"" relative-to=""my.data""/>"
80,</global-state>
80,</cache-container>
80,</infinispan>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""global-state"": {"
80,"""persistent-location"" : {"
80,"""path"" : ""global/state"","
80,"""relative-to"" : ""my.data"""
80,YAML
80,cacheContainer:
80,globalState:
80,persistentLocation:
80,"path: ""global/state"""
80,"relativeTo : ""my.data"""
80,GlobalConfigurationBuilder
80,new GlobalConfigurationBuilder().globalState()
80,.enable()
80,".persistentLocation(""global/state"", ""my.data"");"
80,Additional resources
80,Infinispan configuration schema
80,org.infinispan.configuration.global.GlobalStateConfiguration
80,6.8. File-based cache stores
80,File-based cache stores provide persistent storage on the local host filesystem where Infinispan is running.
80,"For clustered caches, file-based cache stores are unique to each Infinispan node."
80,"Never use filesystem-based cache stores on shared file systems, such as an NFS or Samba share, because they do not provide file locking capabilities and data corruption can occur."
80,"Additionally if you attempt to use transactional caches with shared file systems, unrecoverable failures can happen when writing to files during the commit phase."
80,Soft-Index File Stores
80,SoftIndexFileStore is the default implementation for file-based cache stores and stores data in a set of append-only files.
80,When append-only files:
80,"Reach their maximum size, Infinispan creates a new file and starts writing to it."
80,"Reach the compaction threshold of less than 50% usage, Infinispan overwrites the entries to a new file and then deletes the old file."
80,SoftIndexFileStore should use purge on startup to ensure stale entries are not resurrected.
80,B+ trees
80,"To improve performance, append-only files in a SoftIndexFileStore are indexed using a B+ Tree that can be stored both on disk and in memory."
80,The in-memory index uses Java soft references to ensure it can be rebuilt if removed by Garbage Collection (GC) then requested again.
80,"Because SoftIndexFileStore uses Java soft references to keep indexes in memory, it helps prevent out-of-memory exceptions."
80,GC removes indexes before they consume too much memory while still falling back to disk.
80,SoftIndexFileStore creates a B+ tree per configured cache segment.
80,"This provides an additional ""index"" as it only has so many elements and provides additional parallelism for index updates."
80,Currently we allow for a parallel amount based on one sixteenth of the number of cache segments.
80,Each entry in the B+ tree is a node.
80,"By default, the size of each node is limited to 4096 bytes."
80,SoftIndexFileStore throws an exception if keys are longer after serialization occurs.
80,File limits
80,SoftIndexFileStore will use two plus the configured openFilesLimit amount of files at a given time.
80,The two additional file pointers are reserved for the log appender for newly updated data and another
80,for the compactor which writes compacted entries into a new file.
80,The amount of open allocated files allocated for indexing is one tenth of the total number of the configured openFilesLimit.
80,This number has a minimum of 1 or the number of cache segments.
80,Any number remaning from configured limit is allocated for open data files themselves.
80,Segmentation
80,Soft-index file stores are always segmented. The append log(s) are not directly segmented and segmentation is handled directly by the index.
80,Expiration
80,The SoftIndexFileStore has full support for expired entries and their requirements.
80,Single File Cache Stores
80,Single file cache stores are now deprecated and planned for removal.
80,"Single File cache stores, SingleFileStore, persist data to file."
80,Infinispan also maintains an in-memory index of keys while keys and values are stored in the file.
80,"Because SingleFileStore keeps an in-memory index of keys and the location of values, it requires additional memory, depending on the key size and the number of keys."
80,"For this reason, SingleFileStore is not recommended for use cases where the keys are larger or there can be a larger number of them."
80,"In some cases, SingleFileStore can also become fragmented."
80,"If the size of values continually increases, available space in the single file is not used but the entry is appended to the end of the file."
80,Available space in the file is used only if an entry can fit within it.
80,"Likewise, if you remove all entries from memory, the single file store does not decrease in size or become defragmented."
80,Segmentation
80,"Single file cache stores are segmented by default with a separate instance per segment, which results in multiple directories."
80,Each directory is a number that represents the segment to which the data maps.
80,6.8.1. Configuring file-based cache stores
80,Add file-based cache stores to Infinispan to persist data on the host filesystem.
80,Prerequisites
80,Enable global state and configure a global persistent location if you are configuring embedded caches.
80,Procedure
80,Add the persistence element to your cache configuration.
80,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
80,Include the file-store element and configure attributes as appropriate.
80,Specify false as the value for the shared attribute.
80,File-based cache stores should always be unique to each Infinispan instance.
80,"If you want to use the same persistent across a cluster, configure shared storage such as a JDBC string-based cache store ."
80,Configure the index and data elements to specify the location where Infinispan creates indexes and stores data.
80,Include the write-behind element if you want to configure the cache store with write-behind mode.
80,File-based cache store configuration
80,XML
80,<distributed-cache>
80,"<persistence passivation=""true"">"
80,"<file-store shared=""false"">"
80,"<data path=""data""/>"
80,"<index path=""index""/>"
80,"<write-behind modification-queue-size=""2048"" />"
80,</file-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""passivation"": true,"
80,"""file-store"" : {"
80,"""shared"": false,"
80,"""data"": {"
80,"""path"": ""data"""
80,"""index"": {"
80,"""path"": ""index"""
80,"""write-behind"": {"
80,"""modification-queue-size"": ""2048"""
80,YAML
80,distributedCache:
80,persistence:
80,"passivation: ""true"""
80,fileStore:
80,"shared: ""false"""
80,data:
80,"path: ""data"""
80,index:
80,"path: ""index"""
80,writeBehind:
80,"modificationQueueSize: ""2048"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence().passivation(true)
80,.addSoftIndexFileStore()
80,.shared(false)
80,".dataLocation(""data"")"
80,".indexLocation(""index"")"
80,.modificationQueueSize(2048);
80,6.8.2. Configuring single file cache stores
80,"If required, you can configure Infinispan to create single file stores."
80,Single file stores are deprecated.
80,You should use soft-index file stores for better performance and data consistency in comparison with single file stores.
80,Prerequisites
80,Enable global state and configure a global persistent location if you are configuring embedded caches.
80,Procedure
80,Add the persistence element to your cache configuration.
80,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
80,Include the single-file-store element.
80,Specify false as the value for the shared attribute.
80,Configure any other attributes as appropriate.
80,Include the write-behind element to configure the cache store as write behind instead of as write through.
80,Single file cache store configuration
80,XML
80,<distributed-cache>
80,"<persistence passivation=""true"">"
80,"<single-file-store shared=""false"""
80,"preload=""true""/>"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"" : {"
80,"""passivation"" : true,"
80,"""single-file-store"" : {"
80,"""shared"" : false,"
80,"""preload"" : true"
80,YAML
80,distributedCache:
80,persistence:
80,"passivation: ""true"""
80,singleFileStore:
80,"shared: ""false"""
80,"preload: ""true"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence().passivation(true)
80,.addStore(SingleFileStoreConfigurationBuilder.class)
80,.shared(false)
80,.preload(true);
80,6.9. JDBC connection factories
80,Infinispan provides different ConnectionFactory implementations that allow you to connect to databases.
80,You use JDBC connections with SQL cache stores and JDBC string-based caches stores.
80,Connection pools
80,Connection pools are suitable for standalone Infinispan deployments and are based on Agroal.
80,XML
80,<distributed-cache>
80,<persistence>
80,"<connection-pool connection-url=""jdbc:h2:mem:infinispan;DB_CLOSE_DELAY=-1"""
80,"username=""sa"""
80,"password=""changeme"""
80,"driver=""org.h2.Driver""/>"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""connection-pool"": {"
80,"""connection-url"": ""jdbc:h2:mem:infinispan_string_based"","
80,"""driver"": ""org.h2.Driver"","
80,"""username"": ""sa"","
80,"""password"": ""changeme"""
80,YAML
80,distributedCache:
80,persistence:
80,connectionPool:
80,"connectionUrl: ""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"""
80,driver: org.h2.Driver
80,username: sa
80,password: changeme
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence()
80,.connectionPool()
80,".connectionUrl(""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"")"
80,".username(""sa"")"
80,".driverClass(""org.h2.Driver"");"
80,Managed datasources
80,Datasource connections are suitable for managed environments such as application servers.
80,XML
80,<distributed-cache>
80,<persistence>
80,"<data-source jndi-url=""java:/StringStoreWithManagedConnectionTest/DS"" />"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""data-source"": {"
80,"""jndi-url"": ""java:/StringStoreWithManagedConnectionTest/DS"""
80,YAML
80,distributedCache:
80,persistence:
80,dataSource:
80,"jndiUrl: ""java:/StringStoreWithManagedConnectionTest/DS"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence()
80,.dataSource()
80,".jndiUrl(""java:/StringStoreWithManagedConnectionTest/DS"");"
80,Simple connections
80,Simple connection factories create database connections on a per invocation basis and are intended for use with test or development environments only.
80,XML
80,<distributed-cache>
80,<persistence>
80,"<simple-connection connection-url=""jdbc:h2://localhost"""
80,"username=""sa"""
80,"password=""changeme"""
80,"driver=""org.h2.Driver""/>"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""simple-connection"": {"
80,"""connection-url"": ""jdbc:h2://localhost"","
80,"""driver"": ""org.h2.Driver"","
80,"""username"": ""sa"","
80,"""password"": ""changeme"""
80,YAML
80,distributedCache:
80,persistence:
80,simpleConnection:
80,"connectionUrl: ""jdbc:h2://localhost"""
80,driver: org.h2.Driver
80,username: sa
80,password: changeme
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence()
80,.simpleConnection()
80,".connectionUrl(""jdbc:h2://localhost"")"
80,".driverClass(""org.h2.Driver"")"
80,".username(""admin"")"
80,".password(""changeme"");"
80,Additional resources
80,PooledConnectionFactoryConfigurationBuilder
80,ManagedConnectionFactoryConfigurationBuilder
80,SimpleConnectionFactoryConfigurationBuilder
80,6.9.1. Configuring managed datasources
80,Create managed datasources as part of your Infinispan Server configuration to optimize connection pooling and performance for JDBC database connections.
80,"You can then specify the JDNI name of the managed datasources in your caches, which centralizes JDBC connection configuration for your deployment."
80,Prerequisites
80,Copy database drivers to the server/lib directory in your Infinispan Server installation.
80,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
80,install org.postgresql:postgresql:42.4.4
80,Procedure
80,Open your Infinispan Server configuration for editing.
80,Add a new data-source to the data-sources section.
80,Uniquely identify the datasource with the name attribute or field.
80,Specify a JNDI name for the datasource with the jndi-name attribute or field.
80,You use the JNDI name to specify the datasource in your JDBC cache store
80,configuration.
80,Set true as the value of the statistics attribute or field to enable statistics for the datasource through the /metrics endpoint.
80,Provide JDBC driver details that define how to connect to the datasource in the connection-factory section.
80,Specify the name of the database driver with the driver attribute or field.
80,Specify the JDBC connection url with the url attribute or field.
80,Specify credentials with the username and password attributes or fields.
80,Provide any other configuration as appropriate.
80,Define how Infinispan Server nodes pool and reuse connections with connection pool tuning properties in the connection-pool section.
80,Save the changes to your configuration.
80,Verification
80,"Use the Infinispan Command Line Interface (CLI) to test the datasource connection, as follows:"
80,Start a CLI session.
80,bin/cli.sh
80,List all datasources and confirm the one you created is available.
80,server datasource ls
80,Test a datasource connection.
80,server datasource test my-datasource
80,Managed datasource configuration
80,XML
80,"<server xmlns=""urn:infinispan:server:14.0"">"
80,<data-sources>
80,<!-- Defines a unique name for the datasource and JNDI name that you
80,reference in JDBC cache store configuration.
80,"Enables statistics for the datasource, if required. -->"
80,"<data-source name=""ds"""
80,"jndi-name=""jdbc/postgres"""
80,"statistics=""true"">"
80,<!-- Specifies the JDBC driver that creates connections. -->
80,"<connection-factory driver=""org.postgresql.Driver"""
80,"url=""jdbc:postgresql://localhost:5432/postgres"""
80,"username=""postgres"""
80,"password=""changeme"">"
80,<!-- Sets optional JDBC driver-specific connection properties. -->
80,"<connection-property name=""name"">value</connection-property>"
80,</connection-factory>
80,<!-- Defines connection pool tuning properties. -->
80,"<connection-pool initial-size=""1"""
80,"max-size=""10"""
80,"min-size=""3"""
80,"background-validation=""1000"""
80,"idle-removal=""1"""
80,"blocking-timeout=""1000"""
80,"leak-detection=""10000""/>"
80,</data-source>
80,</data-sources>
80,</server>
80,JSON
80,"""server"": {"
80,"""data-sources"": [{"
80,"""name"": ""ds"","
80,"""jndi-name"": ""jdbc/postgres"","
80,"""statistics"": true,"
80,"""connection-factory"": {"
80,"""driver"": ""org.postgresql.Driver"","
80,"""url"": ""jdbc:postgresql://localhost:5432/postgres"","
80,"""username"": ""postgres"","
80,"""password"": ""changeme"","
80,"""connection-properties"": {"
80,"""name"": ""value"""
80,"""connection-pool"": {"
80,"""initial-size"": 1,"
80,"""max-size"": 10,"
80,"""min-size"": 3,"
80,"""background-validation"": 1000,"
80,"""idle-removal"": 1,"
80,"""blocking-timeout"": 1000,"
80,"""leak-detection"": 10000"
80,YAML
80,server:
80,dataSources:
80,- name: ds
80,jndiName: 'jdbc/postgres'
80,statistics: true
80,connectionFactory:
80,"driver: ""org.postgresql.Driver"""
80,"url: ""jdbc:postgresql://localhost:5432/postgres"""
80,"username: ""postgres"""
80,"password: ""changeme"""
80,connectionProperties:
80,name: value
80,connectionPool:
80,initialSize: 1
80,maxSize: 10
80,minSize: 3
80,backgroundValidation: 1000
80,idleRemoval: 1
80,blockingTimeout: 1000
80,leakDetection: 10000
80,Configuring caches with JNDI names
80,When you add a managed datasource to Infinispan Server you can add the JNDI name to a JDBC-based cache store configuration.
80,Prerequisites
80,Configure Infinispan Server with a managed datasource.
80,Procedure
80,Open your cache configuration for editing.
80,Add the data-source element or field to the JDBC-based cache store configuration.
80,Specify the JNDI name of the managed datasource as the value of the jndi-url attribute.
80,Configure the JDBC-based cache stores as appropriate.
80,Save the changes to your configuration.
80,JNDI name in cache configuration
80,XML
80,<distributed-cache>
80,<persistence>
80,<jdbc:string-keyed-jdbc-store>
80,<!-- Specifies the JNDI name of a managed datasource on Infinispan Server. -->
80,"<jdbc:data-source jndi-url=""jdbc/postgres""/>"
80,"<jdbc:string-keyed-table drop-on-exit=""true"" create-on-start=""true"" prefix=""TBL"">"
80,"<jdbc:id-column name=""ID"" type=""VARCHAR(255)""/>"
80,"<jdbc:data-column name=""DATA"" type=""BYTEA""/>"
80,"<jdbc:timestamp-column name=""TS"" type=""BIGINT""/>"
80,"<jdbc:segment-column name=""S"" type=""INT""/>"
80,</jdbc:string-keyed-table>
80,</jdbc:string-keyed-jdbc-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""string-keyed-jdbc-store"": {"
80,"""data-source"": {"
80,"""jndi-url"": ""jdbc/postgres"""
80,"""string-keyed-table"": {"
80,"""prefix"": ""TBL"","
80,"""drop-on-exit"": true,"
80,"""create-on-start"": true,"
80,"""id-column"": {"
80,"""name"": ""ID"","
80,"""type"": ""VARCHAR(255)"""
80,"""data-column"": {"
80,"""name"": ""DATA"","
80,"""type"": ""BYTEA"""
80,"""timestamp-column"": {"
80,"""name"": ""TS"","
80,"""type"": ""BIGINT"""
80,"""segment-column"": {"
80,"""name"": ""S"","
80,"""type"": ""INT"""
80,YAML
80,distributedCache:
80,persistence:
80,stringKeyedJdbcStore:
80,dataSource:
80,"jndi-url: ""jdbc/postgres"""
80,stringKeyedTable:
80,"prefix: ""TBL"""
80,dropOnExit: true
80,createOnStart: true
80,idColumn:
80,"name: ""ID"""
80,"type: ""VARCHAR(255)"""
80,dataColumn:
80,"name: ""DATA"""
80,"type: ""BYTEA"""
80,timestampColumn:
80,"name: ""TS"""
80,"type: ""BIGINT"""
80,segmentColumn:
80,"name: ""S"""
80,"type: ""INT"""
80,Connection pool tuning properties
80,You can tune JDBC connection pools for managed datasources in your Infinispan Server configuration.
80,Property
80,Description
80,initial-size
80,Initial number of connections the pool should hold.
80,max-size
80,Maximum number of connections in the pool.
80,min-size
80,Minimum number of connections the pool should hold.
80,blocking-timeout
80,Maximum time in milliseconds to block while waiting for a connection before throwing an exception.
80,This will never throw an exception if creating a new connection takes an inordinately long period of time.
80,Default is 0 meaning that a call will wait indefinitely.
80,background-validation
80,Time in milliseconds between background validation runs. A duration of 0 means that this feature is disabled.
80,validate-on-acquisition
80,"Connections idle for longer than this time, specified in milliseconds, are validated before being acquired (foreground validation). A duration of 0 means that this feature is disabled."
80,idle-removal
80,Time in minutes a connection has to be idle before it can be removed.
80,leak-detection
80,Time in milliseconds a connection has to be held before a leak warning.
80,6.9.2. Configuring JDBC connection pools with Agroal properties
80,You can use a properties file to configure pooled connection factories for JDBC string-based cache stores.
80,Procedure
80,"Specify JDBC connection pool configuration with org.infinispan.agroal.* properties, as in the following example:"
80,org.infinispan.agroal.metricsEnabled=false
80,org.infinispan.agroal.minSize=10
80,org.infinispan.agroal.maxSize=100
80,org.infinispan.agroal.initialSize=20
80,org.infinispan.agroal.acquisitionTimeout_s=1
80,org.infinispan.agroal.validationTimeout_m=1
80,org.infinispan.agroal.leakTimeout_s=10
80,org.infinispan.agroal.reapTimeout_m=10
80,org.infinispan.agroal.metricsEnabled=false
80,org.infinispan.agroal.autoCommit=true
80,org.infinispan.agroal.jdbcTransactionIsolation=READ_COMMITTED
80,org.infinispan.agroal.jdbcUrl=jdbc:h2:mem:PooledConnectionFactoryTest;DB_CLOSE_DELAY=-1
80,org.infinispan.agroal.driverClassName=org.h2.Driver.class
80,org.infinispan.agroal.principal=sa
80,org.infinispan.agroal.credential=sa
80,Configure Infinispan to use your properties file with the properties-file attribute or the PooledConnectionFactoryConfiguration.propertyFile() method.
80,XML
80,"<connection-pool properties-file=""path/to/agroal.properties""/>"
80,JSON
80,"""persistence"": {"
80,"""connection-pool"": {"
80,"""properties-file"": ""path/to/agroal.properties"""
80,YAML
80,persistence:
80,connectionPool:
80,propertiesFile: path/to/agroal.properties
80,ConfigurationBuilder
80,".connectionPool().propertyFile(""path/to/agroal.properties"")"
80,Additional resources
80,Agroal
80,6.10. SQL cache stores
80,SQL cache stores let you load Infinispan caches from existing database tables.
80,Infinispan offers two types of SQL cache store:
80,Table
80,Infinispan loads entries from a single database table.
80,Query
80,"Infinispan uses SQL queries to load entries from single or multiple database tables, including from sub-columns within those tables, and perform insert, update, and delete operations."
80,Visit the code tutorials to try a SQL cache store in action.
80,See the Persistence code tutorial with remote caches.
80,Both SQL table and query stores:
80,Allow read and write operations to persistent storage.
80,Can be read-only and act as a cache loader.
80,Support keys and values that correspond to a single database column or a composite of multiple database columns.
80,"For composite keys and values, you must provide Infinispan with Protobuf schema (.proto files) that describe the keys and values."
80,With Infinispan Server you can add schema through the Infinispan Console or Command Line Interface (CLI) with the schema command.
80,The SQL cache store is intended for use with an existing database table.
80,"As a result, it does not store any metadata, which includes expiration, segments, and, versioning metadata."
80,"Due to the absence of version storage, SQL store does not support optimistic transactional caching and asynchronous cross-site replication."
80,This limitation also extends to Hot Rod versioned operations.
80,Use expiration with the SQL cache store when it is configured as read only.
80,"Expiration removes stale values from memory, causing the cache to fetch the values from the database again and cache them anew."
80,Additional resources
80,DatabaseType Enum lists supported database dialects
80,Infinispan SQL store configuration reference
80,6.10.1. Data types for keys and values
80,"Infinispan loads keys and values from columns in database tables via SQL cache stores, automatically using the appropriate data types."
80,"The following CREATE statement adds a table named ""books"" that has two columns, isbn and title:"
80,Database table with two columns
80,CREATE TABLE books (
80,"isbn NUMBER(13),"
80,title varchar(120)
80,PRIMARY KEY(isbn)
80,"When you use this table with a SQL cache store, Infinispan adds an entry to the cache using the isbn column as the key and the title column as the value."
80,Additional resources
80,Infinispan SQL store configuration reference
80,Composite keys and values
80,You can use SQL stores with database tables that contain composite primary keys or composite values.
80,"To use composite keys or values, you must provide Infinispan with Protobuf schema that describe the data types."
80,You must also add schema configuration to your SQL store and specify the message names for keys and values.
80,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
80,"You can then upload your Protobuf schema for remote caches through the Infinispan Console, CLI, or REST API."
80,Composite values
80,The following database table holds a composite value of the title and author columns:
80,CREATE TABLE books (
80,"isbn NUMBER(13),"
80,"title varchar(120),"
80,author varchar(80)
80,PRIMARY KEY(isbn)
80,Infinispan adds an entry to the cache using the isbn column as the key.
80,"For the value, Infinispan requires a Protobuf schema that maps the title column and the author columns:"
80,package library;
80,message books_value {
80,optional string title = 1;
80,optional string author = 2;
80,Composite keys and values
80,"The following database table holds a composite primary key and a composite value, with two columns each:"
80,CREATE TABLE books (
80,"isbn NUMBER(13),"
80,"reprint INT,"
80,"title varchar(120),"
80,author varchar(80)
80,"PRIMARY KEY(isbn, reprint)"
80,"For both the key and the value, Infinispan requires a Protobuf schema that maps the columns to keys and values:"
80,package library;
80,message books_key {
80,required string isbn = 1;
80,required int32 reprint = 2;
80,message books_value {
80,optional string title = 1;
80,optional string author = 2;
80,Additional resources
80,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
80,Infinispan SQL store configuration reference
80,Embedded keys
80,"Protobuf schema can include keys within values, as in the following example:"
80,Protobuf schema with an embedded key
80,package library;
80,message books_key {
80,required string isbn = 1;
80,required int32 reprint = 2;
80,message books_value {
80,required string isbn = 1;
80,required string reprint = 2;
80,optional string title = 3;
80,optional string author = 4;
80,"To use embedded keys, you must include the embedded-key=""true"" attribute or embeddedKey(true) method in your SQL store configuration."
80,SQL types to Protobuf types
80,The following table contains default mappings of SQL data types to Protobuf data types:
80,SQL type
80,Protobuf type
80,int4
80,int32
80,int8
80,int64
80,float4
80,float
80,float8
80,double
80,numeric
80,double
80,bool
80,bool
80,char
80,string
80,varchar
80,string
80,"text, tinytext, mediumtext, longtext"
80,string
80,"bytea, tinyblob, blob, mediumblob, longblob"
80,bytes
80,Additional resources
80,Cache encoding and marshalling
80,6.10.2. Loading Infinispan caches from database tables
80,Add a SQL table cache store to your configuration if you want Infinispan to load data from a database table.
80,"When it connects to the database, Infinispan uses metadata from the table to detect column names and data types."
80,Infinispan also automatically determines which columns in the database are part of the primary key.
80,Prerequisites
80,Have JDBC connection details.
80,You can add JDBC connection factories directly to your cache configuration.
80,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
80,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
80,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
80,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
80,Procedure
80,Add database drivers to your Infinispan deployment.
80,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
80,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
80,install org.postgresql:postgresql:42.4.4
80,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file.
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-cachestore-sql</artifactId>
80,</dependency>
80,Open your Infinispan configuration for editing.
80,Add a SQL table cache store.
80,Declarative
80,"table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
80,Programmatic
80,persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
80,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
80,"Configure the SQL cache store with the properties you require, for example:"
80,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
80,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
80,"Name the database table that loads the cache with table-name=""<database_table_name>"" or table.name(""<database_table_name>"")."
80,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
80,Specify the package name with the package attribute or package() method.
80,Specify composite values with the message-name attribute or messageName() method.
80,Specify composite keys with the key-message-name attribute or keyMessageName() method.
80,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
80,Save the changes to your configuration.
80,SQL table store configuration
80,"The following example loads a distributed cache from a database table named ""books"" using composite values defined in a Protobuf schema:"
80,XML
80,<distributed-cache>
80,<persistence>
80,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
80,"dialect=""H2"""
80,"shared=""true"""
80,"table-name=""books"">"
80,"<schema message-name=""books_value"""
80,"package=""library""/>"
80,</table-jdbc-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""table-jdbc-store"": {"
80,"""dialect"": ""H2"","
80,"""shared"": ""true"","
80,"""table-name"": ""books"","
80,"""schema"": {"
80,"""message-name"": ""books_value"","
80,"""package"": ""library"""
80,YAML
80,distributedCache:
80,persistence:
80,tableJdbcStore:
80,"dialect: ""H2"""
80,"shared: ""true"""
80,"tableName: ""books"""
80,schema:
80,"messageName: ""books_value"""
80,"package: ""library"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
80,.dialect(DatabaseType.H2)
80,".shared(""true"")"
80,".tableName(""books"")"
80,.schemaJdbcConfigurationBuilder()
80,".messageName(""books_value"")"
80,".packageName(""library"");"
80,Additional resources
80,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
80,Persistence code tutorial with remote caches
80,JDBC connection factories
80,DatabaseType Enum lists supported database dialects
80,Infinispan SQL store configuration reference
80,6.10.3. Using SQL queries to load data and perform operations
80,"SQL query cache stores let you load caches from multiple database tables, including from sub-columns in database tables, and perform insert, update, and delete operations."
80,Prerequisites
80,Have JDBC connection details.
80,You can add JDBC connection factories directly to your cache configuration.
80,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
80,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
80,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
80,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
80,Procedure
80,Add database drivers to your Infinispan deployment.
80,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
80,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
80,install org.postgresql:postgresql:42.4.4
80,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file and make sure database drivers are on your application classpath.
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-cachestore-sql</artifactId>
80,</dependency>
80,Open your Infinispan configuration for editing.
80,Add a SQL query cache store.
80,Declarative
80,"query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
80,Programmatic
80,persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
80,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
80,"Configure the SQL cache store with the properties you require, for example:"
80,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
80,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
80,Define SQL query statements that load caches with data and modify database tables with the queries element or the queries() method.
80,Query statement
80,Description
80,SELECT
80,Loads a single entry into caches.
80,You can use wildcards but must specify parameters for keys.
80,You can use labelled expressions.
80,SELECT ALL
80,Loads multiple entries into caches.
80,You can use the * wildcard if the number of columns returned match the key and value columns.
80,You can use labelled expressions.
80,SIZE
80,Counts the number of entries in the cache.
80,DELETE
80,Deletes a single entry from the cache.
80,DELETE ALL
80,Deletes all entries from the cache.
80,UPSERT
80,Modifies entries in the cache.
80,"DELETE, DELETE ALL, and UPSERT statements do not apply to read only cache stores but are required if cache stores allow modifications."
80,Parameters in DELETE statements must match parameters in SELECT statements exactly.
80,Variables in UPSERT statements must have the same number of uniquely named variables that SELECT and SELECT ALL statements return.
80,"For example, if SELECT returns foo and bar this statement must take only :foo and :bar as variables."
80,However you can apply the same named variable more than once in a statement.
80,"SQL queries can include JOIN, ON, and any other clauses that the database supports."
80,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
80,Specify the package name with the package attribute or package() method.
80,Specify composite values with the message-name attribute or messageName() method.
80,Specify composite keys with the key-message-name attribute or keyMessageName() method.
80,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
80,Save the changes to your configuration.
80,Additional resources
80,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
80,Persistence code tutorial with remote caches
80,JDBC connection factories
80,DatabaseType Enum lists supported database dialects
80,Infinispan SQL store configuration reference
80,SQL query store configuration
80,"This section provides an example configuration for a SQL query cache store that loads a distributed cache with data from two database tables: ""person"" and ""address""."
80,SQL statements
80,"The following examples show SQL data definition language (DDL) statements for the ""person"" and ""address"" tables."
80,The data types described in the example are only valid for PostgreSQL database.
80,"SQL statement for the ""person"" table"
80,CREATE TABLE Person (
80,"name VARCHAR(255) NOT NULL,"
80,"picture BYTEA,"
80,"sex VARCHAR(255),"
80,"birthdate TIMESTAMP,"
80,"accepted_tos BOOLEAN,"
80,"notused VARCHAR(255),"
80,PRIMARY KEY (name)
80,"SQL statement for the ""address"" table"
80,CREATE TABLE Address (
80,"name VARCHAR(255) NOT NULL,"
80,"street VARCHAR(255),"
80,"city VARCHAR(255),"
80,"zip INT,"
80,PRIMARY KEY (name)
80,Protobuf schemas
80,"Protobuf schema for the ""person"" and ""address"" tables are as follows:"
80,"Protobuf schema for the ""address"" table"
80,package com.example;
80,message Address {
80,optional string street = 1;
80,"optional string city = 2 [default = ""San Jose""];"
80,optional int32 zip = 3 [default = 0];
80,"Protobuf schema for the ""person"" table"
80,package com.example;
80,import com.example.Address;
80,enum Sex {
80,FEMALE = 1;
80,MALE = 2;
80,message Person {
80,optional string name = 1;
80,optional Address address = 2;
80,optional bytes picture = 3;
80,optional Sex sex = 4;
80,optional fixed64 birthDate = 5 [default = 0];
80,optional bool accepted_tos = 6 [default = false];
80,Cache configuration
80,"The following example loads a distributed cache from the ""person"" and ""address"" tables using a SQL query that includes a JOIN clause:"
80,XML
80,<distributed-cache>
80,<persistence>
80,"<query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
80,"dialect=""POSTGRES"""
80,"shared=""true"" key-columns=""name"">"
80,"<connection-pool driver=""org.postgresql.Driver"""
80,"connection-url=""jdbc:postgresql://localhost:5432/postgres"""
80,"username=""postgres"""
80,"password=""changeme""/>"
80,"<queries select-single=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
80,"select-all=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
80,"delete-single=""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
80,"delete-all=""DELETE FROM Person; DELETE FROM Address"""
80,"upsert=""INSERT INTO Person (name,"
80,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
80,"size=""SELECT COUNT(*) FROM Person"""
80,"<schema message-name=""Person"""
80,"package=""com.example"""
80,"embedded-key=""true""/>"
80,</query-jdbc-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""query-jdbc-store"": {"
80,"""dialect"": ""POSTGRES"","
80,"""shared"": ""true"","
80,"""key-columns"": ""name"","
80,"""queries"": {"
80,"""select-single"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"","
80,"""select-all"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"","
80,"""delete-single"": ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"","
80,"""delete-all"": ""DELETE FROM Person; DELETE FROM Address"","
80,"""upsert"": ""INSERT INTO Person (name,"
80,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"","
80,"""size"": ""SELECT COUNT(*) FROM Person"""
80,"""schema"": {"
80,"""message-name"": ""Person"","
80,"""package"": ""com.example"","
80,"""embedded-key"": ""true"""
80,YAML
80,distributedCache:
80,persistence:
80,queryJdbcStore:
80,"dialect: ""POSTGRES"""
80,"shared: ""true"""
80,"keyColumns: ""name"""
80,queries:
80,"selectSingle: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
80,"selectAll: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
80,"deleteSingle: ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
80,"deleteAll: ""DELETE FROM Person; DELETE FROM Address"""
80,"upsert: ""INSERT INTO Person (name,"
80,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
80,"size: ""SELECT COUNT(*) FROM Person"""
80,schema:
80,"messageName: ""Person"""
80,"package: ""com.example"""
80,"embeddedKey: ""true"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
80,.dialect(DatabaseType.POSTGRES)
80,".shared(""true"")"
80,".keyColumns(""name"")"
80,.queriesJdbcConfigurationBuilder()
80,".select(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"")"
80,".selectAll(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"")"
80,".delete(""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"")"
80,".deleteAll(""DELETE FROM Person; DELETE FROM Address"")"
80,".upsert(""INSERT INTO Person (name,"
80,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"")"
80,".size(""SELECT COUNT(*) FROM Person"")"
80,.schemaJdbcConfigurationBuilder()
80,".messageName(""Person"")"
80,".packageName(""com.example"")"
80,.embeddedKey(true);
80,Additional resources
80,Infinispan SQL store configuration reference
80,6.10.4. SQL cache store troubleshooting
80,Find out about common issues and errors with SQL cache stores and how to troubleshoot them.
80,"ISPN008064: No primary keys found for table <table_name>, check case sensitivity"
80,Infinispan logs this message in the following cases:
80,The database table does not exist.
80,"The database table name is case sensitive and needs to be either all lower case or all upper case, depending on the database provider."
80,The database table does not have any primary keys defined.
80,To resolve this issue you should:
80,Check your SQL cache store configuration and ensure that you specify the name of an existing table.
80,Ensure that the database table name conforms to an case sensitivity requirements.
80,Ensure that your database tables have primary keys that uniquely identify the appropriate rows.
80,6.11. JDBC string-based cache stores
80,"JDBC String-Based cache stores, JdbcStringBasedStore, use JDBC drivers to load and store values in the underlying database."
80,JDBC String-Based cache stores:
80,Store each entry in its own row in the table to increase throughput for concurrent loads.
80,Use a simple one-to-one mapping that maps each key to a String object using the key-to-string-mapper interface.
80,"Infinispan provides a default implementation, DefaultTwoWayKey2StringMapper, that handles primitive types."
80,"In addition to the data table used to store cache entries, the store also creates a _META table for storing metadata."
80,This table is used to ensure that any existing database content is compatible with the current Infinispan version and configuration.
80,"By default Infinispan shares are not stored, which means that all nodes in the"
80,cluster write to the underlying store on each update. If you want operations to
80,"write to the underlying database once only, you must configure JDBC store as"
80,shared.
80,Segmentation
80,JdbcStringBasedStore uses segmentation by default and requires a column in the database table to represent the segments to which entries belong.
80,Additional resources
80,DatabaseType Enum lists supported database dialects
80,6.11.1. Configuring JDBC string-based cache stores
80,Configure Infinispan caches with JDBC string-based cache stores that can connect to databases.
80,Prerequisites
80,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
80,Embedded caches: Add the infinispan-cachestore-jdbc dependency to your pom file.
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-cachestore-jdbc</artifactId>
80,</dependency>
80,Procedure
80,Create a JDBC string-based cache store configuration in one of the following ways:
80,"Declaratively, add the persistence element or field then add string-keyed-jdbc-store with the following schema namespace:"
80,"xmlns=""urn:infinispan:config:store:jdbc:14.0"""
80,"Programmatically, add the following methods to your ConfigurationBuilder:"
80,persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
80,Specify the dialect of the database with either the dialect attribute or the dialect() method.
80,Configure any properties for the JDBC string-based cache store as appropriate.
80,"For example, specify if the cache store is shared with multiple cache instances with either the shared attribute or the shared() method."
80,Add a JDBC connection factory so that Infinispan can connect to the database.
80,Add a database table that stores cache entries.
80,JDBC string-based cache store configuration
80,XML
80,<distributed-cache>
80,<persistence>
80,"<string-keyed-jdbc-store xmlns=""urn:infinispan:config:store:jdbc:14.0"""
80,"dialect=""H2"">"
80,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
80,"username=""sa"""
80,"password=""changeme"""
80,"driver=""org.h2.Driver""/>"
80,"<string-keyed-table create-on-start=""true"""
80,"prefix=""ISPN_STRING_TABLE"">"
80,"<id-column name=""ID_COLUMN"""
80,"type=""VARCHAR(255)"" />"
80,"<data-column name=""DATA_COLUMN"""
80,"type=""BINARY"" />"
80,"<timestamp-column name=""TIMESTAMP_COLUMN"""
80,"type=""BIGINT"" />"
80,"<segment-column name=""SEGMENT_COLUMN"""
80,"type=""INT""/>"
80,</string-keyed-table>
80,</string-keyed-jdbc-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"": {"
80,"""string-keyed-jdbc-store"": {"
80,"""dialect"": ""H2"","
80,"""string-keyed-table"": {"
80,"""prefix"": ""ISPN_STRING_TABLE"","
80,"""create-on-start"": true,"
80,"""id-column"": {"
80,"""name"": ""ID_COLUMN"","
80,"""type"": ""VARCHAR(255)"""
80,"""data-column"": {"
80,"""name"": ""DATA_COLUMN"","
80,"""type"": ""BINARY"""
80,"""timestamp-column"": {"
80,"""name"": ""TIMESTAMP_COLUMN"","
80,"""type"": ""BIGINT"""
80,"""segment-column"": {"
80,"""name"": ""SEGMENT_COLUMN"","
80,"""type"": ""INT"""
80,"""connection-pool"": {"
80,"""connection-url"": ""jdbc:h2:mem:infinispan"","
80,"""driver"": ""org.h2.Driver"","
80,"""username"": ""sa"","
80,"""password"": ""changeme"""
80,YAML
80,distributedCache:
80,persistence:
80,stringKeyedJdbcStore:
80,"dialect: ""H2"""
80,stringKeyedTable:
80,"prefix: ""ISPN_STRING_TABLE"""
80,createOnStart: true
80,idColumn:
80,"name: ""ID_COLUMN"""
80,"type: ""VARCHAR(255)"""
80,dataColumn:
80,"name: ""DATA_COLUMN"""
80,"type: ""BINARY"""
80,timestampColumn:
80,"name: ""TIMESTAMP_COLUMN"""
80,"type: ""BIGINT"""
80,segmentColumn:
80,"name: ""SEGMENT_COLUMN"""
80,"type: ""INT"""
80,connectionPool:
80,"connectionUrl: ""jdbc:h2:mem:infinispan"""
80,"driver: ""org.h2.Driver"""
80,"username: ""sa"""
80,"password: ""changeme"""
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
80,.dialect(DatabaseType.H2)
80,.table()
80,.dropOnExit(true)
80,.createOnStart(true)
80,".tableNamePrefix(""ISPN_STRING_TABLE"")"
80,".idColumnName(""ID_COLUMN"").idColumnType(""VARCHAR(255)"")"
80,".dataColumnName(""DATA_COLUMN"").dataColumnType(""BINARY"")"
80,".timestampColumnName(""TIMESTAMP_COLUMN"").timestampColumnType(""BIGINT"")"
80,".segmentColumnName(""SEGMENT_COLUMN"").segmentColumnType(""INT"")"
80,.connectionPool()
80,".connectionUrl(""jdbc:h2:mem:infinispan"")"
80,".username(""sa"")"
80,".password(""changeme"")"
80,".driverClass(""org.h2.Driver"");"
80,Additional resources
80,JDBC connection factories
80,6.12. RocksDB cache stores
80,RocksDB provides key-value filesystem-based storage with high performance and
80,reliability for highly concurrent environments.
80,"RocksDB cache stores, RocksDBStore, use two databases. One database provides"
80,a primary cache store for data in memory; the other database holds entries that
80,Infinispan expires from memory.
80,Table 1. Configuration parameters
80,Parameter
80,Description
80,location
80,Specifies the path to the RocksDB database that provides the primary cache
80,"store. If you do not set the location, it is automatically created. Note that"
80,the path must be relative to the global persistent location.
80,expiredLocation
80,Specifies the path to the RocksDB database that provides the cache store for
80,"expired data. If you do not set the location, it is automatically created. Note"
80,that the path must be relative to the global persistent location.
80,expiryQueueSize
80,Sets the size of the in-memory queue for expiring entries. When the queue
80,"reaches the size, Infinispan flushes the expired into the RocksDB cache store."
80,clearThreshold
80,Sets the maximum number of entries before deleting and re-initializing
80,"(re-init) the RocksDB database. For smaller size cache stores, iterating"
80,through all entries and removing each one individually can provide a faster
80,method.
80,Tuning parameters
80,You can also specify the following RocksDB tuning parameters:
80,compressionType
80,blockSize
80,cacheSize
80,Configuration properties
80,Optionally set properties in the configuration as follows:
80,Prefix properties with database to adjust and tune RocksDB databases.
80,Prefix properties with data to configure the column families in which RocksDB stores your data.
80,"<property name=""database.max_background_compactions"">2</property>"
80,"<property name=""data.write_buffer_size"">64MB</property>"
80,"<property name=""data.compression_per_level"">kNoCompression:kNoCompression:kNoCompression:kSnappyCompression:kZSTD:kZSTD</property>"
80,Segmentation
80,RocksDBStore supports segmentation and creates a separate column family per
80,segment. Segmented RocksDB cache stores improve lookup performance
80,and iteration but slightly lower performance of write operations.
80,You should not configure more than a few hundred segments. RocksDB is not
80,designed to have an unlimited number of column families. Too many segments also
80,significantly increases cache store start time.
80,RocksDB cache store configuration
80,XML
80,<local-cache>
80,<persistence>
80,"<rocksdb-store xmlns=""urn:infinispan:config:store:rocksdb:14.0"""
80,"path=""rocksdb/data"">"
80,"<expiration path=""rocksdb/expired""/>"
80,</rocksdb-store>
80,</persistence>
80,</local-cache>
80,JSON
80,"""local-cache"": {"
80,"""persistence"": {"
80,"""rocksdb-store"": {"
80,"""path"": ""rocksdb/data"","
80,"""expiration"": {"
80,"""path"": ""rocksdb/expired"""
80,YAML
80,localCache:
80,persistence:
80,rocksdbStore:
80,"path: ""rocksdb/data"""
80,expiration:
80,"path: ""rocksdb/expired"""
80,ConfigurationBuilder
80,Configuration cacheConfig = new ConfigurationBuilder().persistence()
80,.addStore(RocksDBStoreConfigurationBuilder.class)
80,.build();
80,EmbeddedCacheManager cacheManager = new DefaultCacheManager(cacheConfig);
80,"Cache<String, User> usersCache = cacheManager.getCache(""usersCache"");"
80,"usersCache.put(""raytsang"", new User(...));"
80,ConfigurationBuilder with properties
80,Properties props = new Properties();
80,"props.put(""database.max_background_compactions"", ""2"");"
80,"props.put(""data.write_buffer_size"", ""512MB"");"
80,Configuration cacheConfig = new ConfigurationBuilder().persistence()
80,.addStore(RocksDBStoreConfigurationBuilder.class)
80,".location(""rocksdb/data"")"
80,".expiredLocation(""rocksdb/expired"")"
80,.properties(props)
80,.build();
80,Reference
80,RocksDB cache store configuration schema
80,RocksDBStore
80,RocksDBStoreConfiguration
80,rocksdb.org
80,RocksDB Tuning Guide
80,RocksDB Cache Store test
80,RocksDB Cache Store test configuration
80,6.13. Remote cache stores
80,"Remote cache stores, RemoteStore, use the Hot Rod protocol to store data on"
80,Infinispan clusters.
80,If you configure remote cache stores as shared you cannot preload data.
80,"In other words if shared=""true"" in your configuration then you must set preload=""false""."
80,Segmentation
80,RemoteStore supports segmentation and can publish keys and entries by
80,"segment, which makes bulk operations more efficient. However, segmentation is"
80,available only with Infinispan Hot Rod protocol version 2.3 or later.
80,"When you enable segmentation for RemoteStore, it uses the number of segments"
80,that you define in your Infinispan server configuration.
80,If the source cache is segmented and uses a different number of segments than
80,"RemoteStore, then incorrect values are returned for bulk operations. In this"
80,"case, you should disable segmentation for RemoteStore."
80,Remote cache store configuration
80,XML
80,<distributed-cache>
80,<persistence>
80,"<remote-store xmlns=""urn:infinispan:config:store:remote:14.0"""
80,"cache=""mycache"""
80,"raw-values=""true"">"
80,"<remote-server host=""one"""
80,"port=""12111"" />"
80,"<remote-server host=""two"" />"
80,"<connection-pool max-active=""10"""
80,"exhausted-action=""CREATE_NEW"" />"
80,</remote-store>
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""remote-store"": {"
80,"""cache"": ""mycache"","
80,"""raw-values"": ""true"","
80,"""remote-server"": ["
80,"""host"": ""one"","
80,"""port"": ""12111"""
80,"""host"": ""two"""
80,"""connection-pool"": {"
80,"""max-active"": ""10"","
80,"""exhausted-action"": ""CREATE_NEW"""
80,YAML
80,distributedCache:
80,remoteStore:
80,"cache: ""mycache"""
80,"rawValues: ""true"""
80,remoteServer:
80,"- host: ""one"""
80,"port: ""12111"""
80,"- host: ""two"""
80,connectionPool:
80,"maxActive: ""10"""
80,"exhaustedAction: ""CREATE_NEW"""
80,ConfigurationBuilder
80,ConfigurationBuilder b = new ConfigurationBuilder();
80,b.persistence().addStore(RemoteStoreConfigurationBuilder.class)
80,.ignoreModifications(false)
80,.purgeOnStartup(false)
80,".remoteCacheName(""mycache"")"
80,.rawValues(true)
80,.addServer()
80,".host(""one"").port(12111)"
80,.addServer()
80,".host(""two"")"
80,.connectionPool()
80,.maxActive(10)
80,.exhaustedAction(ExhaustedAction.CREATE_NEW)
80,.async().enable();
80,Reference
80,Remote cache store configuration schema
80,RemoteStore
80,RemoteStoreConfigurationBuilder
80,6.14. Cluster cache loaders
80,ClusterCacheLoader retrieves data from other Infinispan cluster members but
80,"does not persist data. In other words, ClusterCacheLoader is not a cache"
80,store.
80,ClusterLoader is deprecated and planned for removal in a future version.
80,ClusterCacheLoader provides a non-blocking partial alternative to state
80,transfer. ClusterCacheLoader fetches keys from other nodes on demand if those
80,"keys are not available on the local node, which is similar to lazily loading"
80,cache content.
80,The following points also apply to ClusterCacheLoader:
80,Preloading does not take effect (preload=true).
80,Segmentation is not supported.
80,Cluster cache loader configuration
80,XML
80,<distributed-cache>
80,<persistence>
80,"<cluster-loader preload=""true"" remote-timeout=""500""/>"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"" : {"
80,"""cluster-loader"" : {"
80,"""preload"" : true,"
80,"""remote-timeout"" : ""500"""
80,YAML
80,distributedCache:
80,persistence:
80,clusterLoader:
80,"preload: ""true"""
80,"remoteTimeout: ""500"""
80,ConfigurationBuilder
80,ConfigurationBuilder b = new ConfigurationBuilder();
80,b.persistence()
80,.addClusterLoader()
80,.remoteCallTimeout(500);
80,Additional resources
80,Infinispan configuration schema
80,ClusterLoader
80,ClusterLoaderConfiguration
80,6.15. Creating custom cache store implementations
80,You can create custom cache stores through the Infinispan persistent SPI.
80,6.15.1. Infinispan Persistence SPI
80,The Infinispan Service Provider Interface (SPI) enables read and write
80,operations to external storage through the NonBlockingStore interface and has
80,the following features:
80,Portability across JCache-compliant vendors
80,Infinispan maintains compatibility between the NonBlockingStore interface
80,and the JSR-107 JCache specification by using an adapter that handles
80,blocking code.
80,Simplified transaction integration
80,Infinispan automatically handles locking so your implementations do not need
80,to coordinate concurrent access to persistent stores. Depending on the locking
80,"mode you use, concurrent writes to the same key generally do not occur."
80,"However, you should expect operations on the persistent storage to originate"
80,from multiple threads and create implementations to tolerate this behavior.
80,Parallel iteration
80,Infinispan lets you iterate over entries in persistent stores with multiple
80,threads in parallel.
80,Reduced serialization resulting in less CPU usage
80,Infinispan exposes stored entries in a serialized format that can be
80,"transmitted remotely. For this reason, Infinispan does not need to deserialize"
80,entries that it retrieves from persistent storage and then serialize again when
80,writing to the wire.
80,Additional resources
80,Persistence SPI
80,NonBlockingStore
80,JSR-107
80,6.15.2. Creating cache stores
80,Create custom cache stores with implementations of the NonBlockingStore API.
80,Procedure
80,Implement the appropriate Infinispan persistent SPIs.
80,Annotate your store class with the @ConfiguredBy annotation if it has a custom configuration.
80,Create a custom cache store configuration and builder if desired.
80,Extend AbstractStoreConfiguration and AbstractStoreConfigurationBuilder.
80,Optionally add the following annotations to your store Configuration class to ensure that your
80,custom configuration builder parses your cache store configuration from XML:
80,@ConfigurationFor
80,@BuiltBy
80,"If you do not add these annotations, then CustomStoreConfigurationBuilder parses the common"
80,store attributes defined in AbstractStoreConfiguration and any additional elements are ignored.
80,If a configuration does not declare the
80,"@ConfigurationFor annotation, a warning message is logged when Infinispan"
80,initializes the cache.
80,6.15.3. Examples of custom cache store configuration
80,The following are examples show how to configure Infinispan with custom cache store implementations:
80,XML
80,<distributed-cache>
80,<persistence>
80,"<store class=""org.infinispan.persistence.example.MyInMemoryStore"" />"
80,</persistence>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""persistence"" : {"
80,"""store"" : {"
80,"""class"" : ""org.infinispan.persistence.example.MyInMemoryStore"""
80,YAML
80,distributedCache:
80,persistence:
80,store:
80,"class: ""org.infinispan.persistence.example.MyInMemoryStore"""
80,ConfigurationBuilder
80,Configuration config = new ConfigurationBuilder()
80,.persistence()
80,.addStore(CustomStoreConfigurationBuilder.class)
80,.build();
80,6.15.4. Deploying custom cache stores
80,"To use your cache store implementation with Infinispan Server, you must provide it with a JAR file."
80,Prerequisites
80,Stop Infinispan Server if it is running.
80,Infinispan loads JAR files at startup only.
80,Procedure
80,Package your custom cache store implementation in a JAR file.
80,Add your JAR file to the server/lib directory of your Infinispan Server installation.
80,6.16. Migrating data between cache stores
80,Infinispan provides a utility to migrate data from one cache store to another.
80,6.16.1. Cache store migrator
80,Infinispan provides the StoreMigrator.java utility that recreates data for the latest Infinispan cache store implementations.
80,StoreMigrator takes a cache store from a previous version of Infinispan as source and uses a cache store implementation as target.
80,"When you run StoreMigrator, it creates the target cache with the cache store type that you define using the EmbeddedCacheManager interface."
80,StoreMigrator then loads entries from the source store into memory and then puts them into the target cache.
80,"StoreMigrator also lets you migrate data from one type of cache store to another. For example, you can migrate from a JDBC string-based cache store to a RocksDB cache store."
80,StoreMigrator cannot migrate data from segmented cache stores to:
80,Non-segmented cache store.
80,Segmented cache stores that have a different number of segments.
80,6.16.2. Getting the cache store migrator
80,"StoreMigrator is available as part of the Infinispan tools library,"
80,"infinispan-tools, and is included in the Maven repository."
80,Procedure
80,Configure your pom.xml for StoreMigrator as follows:
80,"<?xml version=""1.0"" encoding=""UTF-8""?>"
80,"<project xmlns=""http://maven.apache.org/POM/4.0.0"""
80,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
80,"xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">"
80,<modelVersion>4.0.0</modelVersion>
80,<groupId>org.infinispan.example</groupId>
80,<artifactId>jdbc-migrator-example</artifactId>
80,<version>1.0-SNAPSHOT</version>
80,<dependencies>
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-tools</artifactId>
80,</dependency>
80,<!-- Additional dependencies -->
80,</dependencies>
80,<build>
80,<plugins>
80,<plugin>
80,<groupId>org.codehaus.mojo</groupId>
80,<artifactId>exec-maven-plugin</artifactId>
80,<version>1.2.1</version>
80,<executions>
80,<execution>
80,<goals>
80,<goal>java</goal>
80,</goals>
80,</execution>
80,</executions>
80,<configuration>
80,<mainClass>org.infinispan.tools.store.migrator.StoreMigrator</mainClass>
80,<arguments>
80,<argument>path/to/migrator.properties</argument>
80,</arguments>
80,</configuration>
80,</plugin>
80,</plugins>
80,</build>
80,</project>
80,6.16.3. Configuring the cache store migrator
80,Use the migrator.properties file to configure properties for source and target cache stores.
80,Procedure
80,Create a migrator.properties file.
80,Configure properties for source and target cache store using the migrator.properties file.
80,Add the source. prefix to all configuration properties for the source cache store.
80,Example source cache store
80,source.type=SOFT_INDEX_FILE_STORE
80,source.cache_name=myCache
80,source.location=/path/to/source/sifs
80,source.version=<version>
80,"For migrating data from segmented cache stores, you must also configure the number of segments using the source.segment_count property."
80,The number of segments must match clustering.hash.numSegments in your Infinispan configuration.
80,"If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
80,Add the target. prefix to all configuration properties for the target cache store.
80,Example target cache store
80,target.type=SINGLE_FILE_STORE
80,target.cache_name=myCache
80,target.location=/path/to/target/sfs.dat
80,Configuration properties for the cache store migrator
80,Configure source and target cache stores in a StoreMigrator properties.
80,Table 2. Cache Store Type Property
80,Property
80,Description
80,Required/Optional
80,type
80,Specifies the type of cache store for a source or target cache store.
80,.type=JDBC_STRING
80,.type=JDBC_BINARY
80,.type=JDBC_MIXED
80,.type=LEVELDB
80,.type=ROCKSDB
80,.type=SINGLE_FILE_STORE
80,.type=SOFT_INDEX_FILE_STORE
80,.type=JDBC_MIXED
80,Required
80,Table 3. Common Properties
80,Property
80,Description
80,Example Value
80,Required/Optional
80,cache_name
80,The name of the cache that you want to back up.
80,.cache_name=myCache
80,Required
80,segment_count
80,The number of segments for target cache stores that can use
80,segmentation.
80,The number of segments must match clustering.hash.numSegments in the
80,"Infinispan configuration. If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
80,.segment_count=256
80,Optional
80,Table 4. JDBC Properties
80,Property
80,Description
80,Required/Optional
80,dialect
80,Specifies the dialect of the underlying database.
80,Required
80,version
80,Specifies the marshaller version for source cache stores.
80,Set the value that matches the Infinispan major version of the source cluster. For example; set a value of 14 for Infinispan 14.x.
80,Required for source stores only.
80,marshaller.class
80,Specifies a custom marshaller class.
80,Required if using custom marshallers.
80,marshaller.externalizers
80,Specifies a comma-separated list of custom AdvancedExternalizer implementations to load in this format: [id]:<Externalizer class>
80,Optional
80,connection_pool.connection_url
80,Specifies the JDBC connection URL.
80,Required
80,connection_pool.driver_class
80,Specifies the class of the JDBC driver.
80,Required
80,connection_pool.username
80,Specifies a database username.
80,Required
80,connection_pool.password
80,Specifies a password for the database username.
80,Required
80,db.disable_upsert
80,Disables database upsert.
80,Optional
80,db.disable_indexing
80,Specifies if table indexes are created.
80,Optional
80,table.string.table_name_prefix
80,Specifies additional prefixes for the table name.
80,Optional
80,table.string.<id|data|timestamp>.name
80,Specifies the column name.
80,Required
80,table.string.<id|data|timestamp>.type
80,Specifies the column type.
80,Required
80,key_to_string_mapper
80,Specifies the TwoWayKey2StringMapper class.
80,Optional
80,"To migrate from Binary cache stores in older Infinispan versions, change"
80,table.string.* to table.binary.\* in the following properties:
80,source.table.binary.table_name_prefix
80,source.table.binary.<id\|data\|timestamp>.name
80,source.table.binary.<id\|data\|timestamp>.type
80,# Example configuration for migrating to a JDBC String-Based cache store
80,target.type=STRING
80,target.cache_name=myCache
80,target.dialect=POSTGRES
80,target.marshaller.class=org.example.CustomMarshaller
80,"target.marshaller.externalizers=25:Externalizer1,org.example.Externalizer2"
80,target.connection_pool.connection_url=jdbc:postgresql:postgres
80,target.connection_pool.driver_class=org.postrgesql.Driver
80,target.connection_pool.username=postgres
80,target.connection_pool.password=redhat
80,target.db.disable_upsert=false
80,target.db.disable_indexing=false
80,target.table.string.table_name_prefix=tablePrefix
80,target.table.string.id.name=id_column
80,target.table.string.data.name=datum_column
80,target.table.string.timestamp.name=timestamp_column
80,target.table.string.id.type=VARCHAR
80,target.table.string.data.type=bytea
80,target.table.string.timestamp.type=BIGINT
80,target.key_to_string_mapper=org.infinispan.persistence.keymappers. DefaultTwoWayKey2StringMapper
80,Table 5. RocksDB Properties
80,Property
80,Description
80,Required/Optional
80,location
80,Sets the database directory.
80,Required
80,compression
80,Specifies the compression type to use.
80,Optional
80,# Example configuration for migrating from a RocksDB cache store.
80,source.type=ROCKSDB
80,source.cache_name=myCache
80,source.location=/path/to/rocksdb/database
80,source.compression=SNAPPY
80,Table 6. SingleFileStore Properties
80,Property
80,Description
80,Required/Optional
80,location
80,Sets the directory that contains the cache store .dat file.
80,Required
80,# Example configuration for migrating to a Single File cache store.
80,target.type=SINGLE_FILE_STORE
80,target.cache_name=myCache
80,target.location=/path/to/sfs.dat
80,Table 7. SoftIndexFileStore Properties
80,Property
80,Description
80,Value
80,Required/Optional
80,location
80,Sets the database directory.
80,Required
80,index_location
80,Sets the database index directory.
80,# Example configuration for migrating to a Soft-Index File cache store.
80,target.type=SOFT_INDEX_FILE_STORE
80,target.cache_name=myCache
80,target.location=path/to/sifs/database
80,target.location=path/to/sifs/index
80,6.16.4. Migrating Infinispan cache stores
80,You can use the StoreMigrator to migrate data between cache stores with different Infinispan versions or to migrate data from one type of cache store to another.
80,Prerequisites
80,Have a
80,infinispan-tools.jar.
80,Have the source and target cache store configured in the migrator.properties file.
80,Procedure
80,"If you built the infinispan-tools.jar from the source code, do the following:"
80,Add infinispan-tools.jar to your classpath.
80,"Add dependencies for your source and target databases, such as JDBC drivers to your classpath."
80,Specify migrator.properties file as an argument for StoreMigrator.
80,"If you pulled infinispan-tools.jar from the Maven repository, run the following command:"
80,mvn exec:java
80,7. Configuring Infinispan to handle network partitions
80,Infinispan clusters can split into network partitions in which subsets of nodes become isolated from each other.
80,This condition results in loss of availability or consistency for clustered caches.
80,Infinispan automatically detects crashed nodes and resolves conflicts to merge caches back together.
80,7.1. Split clusters and network partitions
80,"Network partitions are the result of error conditions in the running environment, such as when a network router crashes."
80,"When a cluster splits into partitions, nodes create a JGroups cluster view that includes only the nodes in that partition."
80,This condition means that nodes in one partition can operate independently of nodes in the other partition.
80,Detecting a split
80,"To automatically detect network partitions, Infinispan uses the FD_ALL protocol in the default JGroups stack to determine when nodes leave the cluster abruptly."
80,Infinispan cannot detect what causes nodes to leave abruptly.
80,"This can happen not only when there is a network failure but also for other reasons, such as when Garbage Collection (GC) pauses the JVM."
80,Infinispan suspects that nodes have crashed after the following number of milliseconds:
80,FD_ALL[2|3].timeout + FD_ALL[2|3].interval + VERIFY_SUSPECT[2].timeout + GMS.view_ack_collection_timeout
80,"When it detects that the cluster is split into network partitions, Infinispan uses a strategy for handling cache operations."
80,Depending on your application requirements Infinispan can:
80,Allow read and/or write operations for availability
80,Deny read and write operations for consistency
80,Merging partitions together
80,"To fix a split cluster, Infinispan merges the partitions back together."
80,"During the merge, Infinispan uses the .equals() method for values of cache entries to determine if any conflicts exist."
80,"To resolve any conflicts between replicas it finds on partitions, Infinispan uses a merge policy that you can configure."
80,7.1.1. Data consistency in a split cluster
80,Network outages or errors that cause Infinispan clusters to split into partitions can result in data loss or consistency issues regardless of any handling strategy or merge policy.
80,Between the split and detection
80,"If a write operation takes place on a node that is in a minor partition when a split occurs, and before Infinispan detects the split, that value is lost when Infinispan transfers state to that minor partition during the merge."
80,In the event that all partitions are in the DEGRADED mode that value is not lost because no state transfer occurs but the entry can have an inconsistent value.
80,"For transactional caches write operations that are in progress when the split occurs can be committed on some nodes and rolled back on other nodes, which also results in inconsistent values."
80,"During the split and the time that Infinispan detects it, it is possible to get stale reads from a cache in a minor partition that has not yet entered DEGRADED mode."
80,During the merge
80,When Infinispan starts removing partitions nodes reconnect to the cluster with a series of merge events.
80,"Before this merge process completes it is possible that write operations on transactional caches succeed on some nodes but not others, which can potentially result in stale reads until the entries are updated."
80,7.2. Cache availability and degraded mode
80,"To preserve data consistency, Infinispan can put caches into DEGRADED mode if you configure them to use either the DENY_READ_WRITES or ALLOW_READS partition handling strategy."
80,Infinispan puts caches in a partition into DEGRADED mode when the following conditions are true:
80,At least one segment has lost all owners.
80,This happens when a number of nodes equal to or greater than the number of owners for a distributed cache have left the cluster.
80,There is not a majority of nodes in the partition.
80,"A majority of nodes is any number greater than half the total number of nodes in the cluster from the most recent stable topology, which was the last time a cluster rebalancing operation completed successfully."
80,"When caches are in DEGRADED mode, Infinispan:"
80,Allows read and write operations only if all replicas of an entry reside in the same partition.
80,Denies read and write operations and throws an AvailabilityException if the partition does not include all replicas of an entry.
80,"With the ALLOW_READS strategy, Infinispan allows read operations on caches in DEGRADED mode."
80,DEGRADED mode guarantees consistency by ensuring that write operations do not take place for the same key in different partitions.
80,Additionally DEGRADED mode prevents stale read operations that happen when a key is updated in one partition but read in another partition.
80,If all partitions are in DEGRADED mode then the cache becomes available again after merge only if the cluster contains a majority of nodes from the most recent stable topology and there is at least one replica of each entry.
80,"When the cluster has at least one replica of each entry, no keys are lost and Infinispan can create new replicas based on the number of owners during cluster rebalancing."
80,In some cases a cache in one partition can remain available while entering DEGRADED mode in another partition.
80,When this happens the available partition continues cache operations as normal and Infinispan attempts to rebalance data across those nodes.
80,To merge the cache together Infinispan always transfers state from the available partition to the partition in DEGRADED mode.
80,7.2.1. Degraded cache recovery example
80,This topic illustrates how Infinispan recovers from split clusters with caches that use the DENY_READ_WRITES partition handling strategy.
80,"As an example, a Infinispan cluster has four nodes and includes a distributed cache with two replicas for each entry (owners=2)."
80,"There are four entries in the cache, k1, k2, k3 and k4."
80,"With the DENY_READ_WRITES strategy, if the cluster splits into partitions, Infinispan allows cache operations only if all replicas of an entry are in the same partition."
80,"In the following diagram, while the cache is split into partitions, Infinispan allows read and write operations for k1 on partition 1 and k4 on partition 2."
80,"Because there is only one replica for k2 and k3 on either partition 1 or partition 2, Infinispan denies read and write operations for those entries."
80,"When network conditions allow the nodes to re-join the same cluster view, Infinispan merges the partitions without state transfer and restores normal cache operations."
80,7.2.2. Verifying cache availability during network partitions
80,Determine if caches on Infinispan clusters are in AVAILABLE mode or DEGRADED mode during a network partition.
80,"When Infinispan clusters split into partitions, nodes in those partitions can enter DEGRADED mode to guarantee data consistency."
80,In DEGRADED mode clusters do not allow cache operations resulting in loss of availability.
80,Procedure
80,Verify availability of clustered caches in network partitions in one of the following ways:
80,Check Infinispan logs for ISPN100011 messages that indicate if the cluster is available or if at least one cache is in DEGRADED mode.
80,Get the availability of remote caches through the Infinispan Console or with the REST API.
80,"Open the Infinispan Console in any browser, select the Data Container tab, and then locate the availability status in the Health column."
80,Retrieve cache health from the REST API.
80,GET /rest/v2/cache-managers/<cacheManagerName>/health
80,Programmatically retrieve the availability of embedded caches with the getAvailability() method in the AdvancedCache API.
80,Additional resources
80,REST API: Getting cluster health
80,org.infinispan.AdvancedCache.getAvailability
80,Enum AvailabilityMode
80,7.2.3. Making caches available
80,Make caches available for read and write operations by forcing them out of DEGRADED mode.
80,You should force clusters out of DEGRADED mode only if your deployment can tolerate data loss and inconsistency.
80,Procedure
80,Make caches available in one of the following ways:
80,Open the Infinispan Console and select the Make available option.
80,Change the availability of remote caches with the REST API.
80,POST /rest/v2/caches/<cacheName>?action=set-availability&availability=AVAILABLE
80,Programmatically change the availability of embedded caches with the AdvancedCache API.
80,AdvancedCache ac = cache.getAdvancedCache();
80,// Retrieve cache availability
80,boolean available = ac.getAvailability() == AvailabilityMode.AVAILABLE;
80,// Make the cache available
80,if (!available) {
80,ac.setAvailability(AvailabilityMode.AVAILABLE);
80,Additional resources
80,REST API: Setting cache availability
80,org.infinispan.AdvancedCache
80,7.3. Configuring partition handling
80,Configure Infinispan to use a partition handling strategy and merge policy so it can resolve split clusters when network issues occur.
80,By default Infinispan uses a strategy that provides availability at the cost of lowering consistency guarantees for your data.
80,When a cluster splits due to a network partition clients can continue to perform read and write operations on caches.
80,"If you require consistency over availability, you can configure Infinispan to deny read and write operations while the cluster is split into partitions."
80,Alternatively you can allow read operations and deny write operations.
80,You can also specify custom merge policy implementations that configure Infinispan to resolve splits with custom logic tailored to your requirements.
80,Prerequisites
80,Have a Infinispan cluster where you can create either a replicated or distributed cache.
80,Partition handling configuration applies only to replicated and distributed caches.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add partition handling configuration to your cache with either the partition-handling element or partitionHandling() method.
80,Specify a strategy for Infinispan to use when the cluster splits into partitions with the when-split attribute or whenSplit() method.
80,The default partition handling strategy is ALLOW_READ_WRITES so caches remain availabile.
80,"If your use case requires data consistency over cache availability, specify the DENY_READ_WRITES strategy."
80,Specify a policy that Infinispan uses to resolve conflicting entries when merging partitions with the merge-policy attribute or mergePolicy() method.
80,By default Infinispan does not resolve conflicts on merge.
80,Save the changes to your Infinispan configuration.
80,Partition handling configuration
80,XML
80,<distributed-cache>
80,"<partition-handling when-split=""DENY_READ_WRITES"""
80,"merge-policy=""PREFERRED_ALWAYS""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""partition-handling"" : {"
80,"""when-split"": ""DENY_READ_WRITES"","
80,"""merge-policy"": ""PREFERRED_ALWAYS"""
80,YAML
80,distributedCache:
80,partitionHandling:
80,whenSplit: DENY_READ_WRITES
80,mergePolicy: PREFERRED_ALWAYS
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
80,.partitionHandling()
80,.whenSplit(PartitionHandling.DENY_READ_WRITES)
80,.mergePolicy(MergePolicy.PREFERRED_NON_NULL);
80,7.4. Partition handling strategies
80,Partition handling strategies control if Infinispan allows read and write operations when a cluster is split.
80,The strategy you configure determines whether you get cache availability or data consistency.
80,Table 8. Partition handling strategies
80,Strategy
80,Description
80,Availability or consistency
80,ALLOW_READ_WRITES
80,Infinispan allows read and write operations on caches while a cluster is split into network partitions.
80,Nodes in each partition remain available and function independently of each other.
80,This is the default partition handling strategy.
80,Availability
80,DENY_READ_WRITES
80,Infinispan allows read and write operations only if all replicas of an entry are in the partition.
80,"If a partition does not include all replicas of an entry, Infinispan prevents cache operations for that entry."
80,Consistency
80,ALLOW_READS
80,Infinispan allows read operations for entries and prevents write operations unless the partition includes all replicas of an entry.
80,Consistency with read availability
80,7.5. Merge policies
80,Merge policies control how Infinispan resolves conflicts between replicas when bringing cluster partitions together.
80,You can use one of the merge policies that Infinispan provides or you can create a custom implementation of the EntryMergePolicy API.
80,Table 9. Infinispan merge policies
80,Merge policy
80,Description
80,Considerations
80,NONE
80,Infinispan does not resolve conflicts when merging split clusters. This is the default merge policy.
80,"Nodes drop segments for which they are not the primary owner, which can result in data loss."
80,PREFERRED_ALWAYS
80,Infinispan finds the value that exists on the majority of nodes in the cluster and uses it to resolve conflicts.
80,"Infinispan could use stale values to resolve conflicts. Even if an entry is available the majority of nodes, the last update could happen on the minority partition."
80,PREFERRED_NON_NULL
80,Infinispan uses the first non-null value that it finds on the cluster to resolve conflicts.
80,Infinispan could restore deleted entries.
80,REMOVE_ALL
80,Infinispan removes any conflicting entries from the cache.
80,Results in loss of any entries that have different values when merging split clusters.
80,7.6. Configuring custom merge policies
80,Configure Infinispan to use custom implementations of the EntryMergePolicy API when handling network partitions.
80,Prerequisites
80,Implement the EntryMergePolicy API.
80,"public class CustomMergePolicy implements EntryMergePolicy<String, String> {"
80,@Override
80,"public CacheEntry<String, String> merge(CacheEntry<String, String> preferredEntry, List<CacheEntry<String, String>> otherEntries) {"
80,// Decide which entry resolves the conflict
80,return the_solved_CacheEntry;
80,Procedure
80,Deploy your merge policy implementation to Infinispan Server if you use remote caches.
80,Package your classes as a JAR file that includes a META-INF/services/org.infinispan.conflict.EntryMergePolicy file that contains the fully qualified class name of your merge policy.
80,# List implementations of EntryMergePolicy with the full qualified class name
80,org.example.CustomMergePolicy
80,Add the JAR file to the server/lib directory.
80,Use the install command with the Infinispan Command Line Interface (CLI) to download the JAR to the server/lib directory.
80,Open your Infinispan configuration for editing.
80,Configure cache encoding with the encoding element or encoding() method as appropriate.
80,"For remote caches, if you use only object metadata for comparison when merging entries then you can use application/x-protostream as the media type. In this case Infinispan returns entries to the EntryMergePolicy as byte[]."
80,If you require the object itself when merging conflicts then you should configure caches with the application/x-java-object media type. In this case you must deploy the relevant ProtoStream marshallers to Infinispan Server so it can perform byte[] to object transformations if clients use Protobuf encoding.
80,Specify your custom merge policy with the merge-policy attribute or mergePolicy() method as part of the partition handling configuration.
80,Save your changes.
80,Custom merge policy configuration
80,XML
80,"<distributed-cache name=""mycache"">"
80,"<partition-handling when-split=""DENY_READ_WRITES"""
80,"merge-policy=""org.example.CustomMergePolicy""/>"
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""partition-handling"" : {"
80,"""when-split"": ""DENY_READ_WRITES"","
80,"""merge-policy"": ""org.example.CustomMergePolicy"""
80,YAML
80,distributedCache:
80,partitionHandling:
80,whenSplit: DENY_READ_WRITES
80,mergePolicy: org.example.CustomMergePolicy
80,ConfigurationBuilder
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
80,.partitionHandling()
80,.whenSplit(PartitionHandling.DENY_READ_WRITES)
80,.mergePolicy(new CustomMergePolicy());
80,Additional resources
80,org.infinispan.conflict.EntryMergePolicy
80,7.7. Manually merging partitions in embedded caches
80,Detect and resolve conflicting entries to manually merge embedded caches after network partitions occur.
80,Procedure
80,"Retrieve the ConflictManager from the EmbeddedCacheManager to detect and resolve conflicting entries in a cache, as in the following example:"
80,"EmbeddedCacheManager manager = new DefaultCacheManager(""example-config.xml"");"
80,"Cache<Integer, String> cache = manager.getCache(""testCache"");"
80,"ConflictManager<Integer, String> crm = ConflictManagerFactory.get(cache.getAdvancedCache());"
80,// Get all versions of a key
80,"Map<Address, InternalCacheValue<String>> versions = crm.getAllVersions(1);"
80,// Process conflicts stream and perform some operation on the cache
80,"Stream<Map<Address, CacheEntry<Integer, String>>> conflicts = crm.getConflicts();"
80,conflicts.forEach(map -> {
80,"CacheEntry<Integer, String> entry = map.values().iterator().next();"
80,Object conflictKey = entry.getKey();
80,cache.remove(conflictKey);
80,});
80,// Detect and then resolve conflicts using the configured EntryMergePolicy
80,crm.resolveConflicts();
80,// Detect and then resolve conflicts using the passed EntryMergePolicy instance
80,"crm.resolveConflicts((preferredEntry, otherEntries) -> preferredEntry);"
80,"Although the ConflictManager::getConflicts stream is processed per entry, the underlying spliterator lazily loads cache entries on a per segment basis."
80,8. Security authorization with role-based access control
80,Role-based access control (RBAC) capabilities use different permissions levels to restrict user interactions with Infinispan.
80,"For information on creating users and configuring authorization specific to remote or embedded caches, see:"
80,Configuring user roles and permissions with Infinispan Server
80,Programmatically configuring user roles and permissions
80,8.1. Infinispan user roles and permissions
80,Infinispan includes several roles that provide users with permissions to access caches and Infinispan resources.
80,Role
80,Permissions
80,Description
80,admin
80,ALL
80,Superuser with all permissions including control of the Cache Manager lifecycle.
80,deployer
80,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR, CREATE"
80,Can create and delete Infinispan resources in addition to application permissions.
80,application
80,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR"
80,Has read and write access to Infinispan resources in addition to observer permissions. Can also listen to events and execute server tasks and scripts.
80,observer
80,"ALL_READ, MONITOR"
80,Has read access to Infinispan resources in addition to monitor permissions.
80,monitor
80,MONITOR
80,Can view statistics via JMX and the metrics endpoint.
80,Additional resources
80,org.infinispan.security.AuthorizationPermission Enum
80,Infinispan configuration schema reference
80,8.1.1. Permissions
80,User roles are sets of permissions with different access levels.
80,Table 10. Cache Manager permissions
80,Permission
80,Function
80,Description
80,CONFIGURATION
80,defineConfiguration
80,Defines new cache configurations.
80,LISTEN
80,addListener
80,Registers listeners against a Cache Manager.
80,LIFECYCLE
80,stop
80,Stops the Cache Manager.
80,CREATE
80,"createCache, removeCache"
80,Create and remove container resources
80,"such as caches, counters, schemas, and scripts."
80,MONITOR
80,getStats
80,Allows access to JMX statistics and the metrics endpoint.
80,ALL
80,Includes all Cache Manager permissions.
80,Table 11. Cache permissions
80,Permission
80,Function
80,Description
80,READ
80,"get, contains"
80,Retrieves entries from a cache.
80,WRITE
80,"put, putIfAbsent, replace, remove, evict"
80,"Writes, replaces, removes, evicts data in a cache."
80,EXEC
80,"distexec, streams"
80,Allows code execution against a cache.
80,LISTEN
80,addListener
80,Registers listeners against a cache.
80,BULK_READ
80,"keySet, values, entrySet, query"
80,Executes bulk retrieve operations.
80,BULK_WRITE
80,"clear, putAll"
80,Executes bulk write operations.
80,LIFECYCLE
80,"start, stop"
80,Starts and stops a cache.
80,ADMIN
80,"getVersion, addInterceptor*, removeInterceptor, getInterceptorChain, getEvictionManager, getComponentRegistry, getDistributionManager, getAuthorizationManager, evict, getRpcManager, getCacheConfiguration, getCacheManager, getInvocationContextContainer, setAvailability, getDataContainer, getStats, getXAResource"
80,Allows access to underlying components and internal structures.
80,MONITOR
80,getStats
80,Allows access to JMX statistics and the metrics endpoint.
80,ALL
80,Includes all cache permissions.
80,ALL_READ
80,Combines the READ and BULK_READ permissions.
80,ALL_WRITE
80,Combines the WRITE and BULK_WRITE permissions.
80,Additional resources
80,Infinispan Security API
80,8.1.2. Role and permission mappers
80,Infinispan implements users as a collection of principals.
80,"Principals represent either an individual user identity, such as a username, or a group to which the users belong. Internally, these are implemented with the javax.security.auth.Subject class."
80,"To enable authorization, the principals must be mapped to role names, which are then expanded into a set of permissions."
80,"Infinispan includes the PrincipalRoleMapper API for associating security principals to roles, and the RolePermissionMapper API for associating roles with specific permissions."
80,Infinispan provides the following role and permission mapper implementations:
80,Cluster role mapper
80,Stores principal to role mappings in the cluster registry.
80,Cluster permission mapper
80,Stores role to permission mappings in the cluster registry. Allows you to dynamically modify user roles and permissions.
80,Identity role mapper
80,"Uses the principal name as the role name. The type or format of the principal name depends on the source. For example, in an LDAP directory the principal name could be a Distinguished Name (DN)."
80,Common name role mapper
80,"Uses the Common Name (CN) as the role name. You can use this role mapper with an LDAP directory or with client certificates that contain Distinguished Names (DN); for example cn=managers,ou=people,dc=example,dc=com maps to the managers role."
80,Mapping users to roles and permissions in Infinispan
80,"Consider the following user retrieved from an LDAP server, as a collection of DNs:"
80,"CN=myapplication,OU=applications,DC=mycompany"
80,"CN=dataprocessors,OU=groups,DC=mycompany"
80,"CN=finance,OU=groups,DC=mycompany"
80,"Using the Common name role mapper, the user would be mapped to the following roles:"
80,dataprocessors
80,finance
80,Infinispan has the following role definitions:
80,dataprocessors: ALL_WRITE ALL_READ
80,finance: LISTEN
80,The user would have the following permissions:
80,ALL_WRITE ALL_READ LISTEN
80,Additional resources
80,Infinispan Security API
80,org.infinispan.security.PrincipalRoleMapper
80,org.infinispan.security.RolePermissionMapper
80,org.infinispan.security.mappers.IdentityRoleMapper
80,org.infinispan.security.mappers.CommonNameRoleMapper
80,8.1.3. Configuring role mappers
80,Infinispan enables the cluster role mapper and cluster permission mapper by default.
80,"To use a different implementation for role mapping, you must configure the role mappers."
80,Procedure
80,Open your Infinispan configuration for editing.
80,Declare the role mapper as part of the security authorization in the Cache Manager configuration.
80,Save the changes to your configuration.
80,With embedded caches you can programmatically configure role and permission mappers with the principalRoleMapper() and rolePermissionMapper() methods.
80,Role mapper configuration
80,XML
80,<cache-container>
80,<security>
80,<authorization>
80,<common-name-role-mapper />
80,</authorization>
80,</security>
80,</cache-container>
80,JSON
80,"""infinispan"" : {"
80,"""cache-container"" : {"
80,"""security"" : {"
80,"""authorization"" : {"
80,"""common-name-role-mapper"": {}"
80,YAML
80,infinispan:
80,cacheContainer:
80,security:
80,authorization:
80,commonNameRoleMapper: ~
80,Additional resources
80,Infinispan configuration schema reference
80,8.2. Configuring caches with security authorization
80,Add security authorization to caches to enforce role-based access control (RBAC).
80,This requires Infinispan users to have a role with a sufficient level of permission to perform cache operations.
80,Prerequisites
80,Create Infinispan users and either grant them with roles or assign them to groups.
80,Procedure
80,Open your Infinispan configuration for editing.
80,Add a security section to the configuration.
80,Specify roles that users must have to perform cache operations with the authorization element.
80,You can implicitly add all roles defined in the Cache Manager or explicitly define a subset of roles.
80,Save the changes to your configuration.
80,Implicit role configuration
80,The following configuration implicitly adds every role defined in the Cache Manager:
80,XML
80,<distributed-cache>
80,<security>
80,<authorization/>
80,</security>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""security"": {"
80,"""authorization"": {"
80,"""enabled"": true"
80,YAML
80,distributedCache:
80,security:
80,authorization:
80,enabled: true
80,Explicit role configuration
80,The following configuration explicitly adds a subset of roles defined in the Cache Manager.
80,In this case Infinispan denies cache operations for any users that do not have one of the configured roles.
80,XML
80,<distributed-cache>
80,<security>
80,"<authorization roles=""admin supervisor""/>"
80,</security>
80,</distributed-cache>
80,JSON
80,"""distributed-cache"": {"
80,"""security"": {"
80,"""authorization"": {"
80,"""enabled"": true,"
80,"""roles"": [""admin"",""supervisor""]"
80,YAML
80,distributedCache:
80,security:
80,authorization:
80,enabled: true
80,"roles: [""admin"",""supervisor""]"
80,9. Configuring transactions
80,"Data that resides on a distributed system is vulnerable to errors that can arise from temporary network outages, system failures, or just simple human error."
80,These external factors are uncontrollable but can have serious consequences for quality of your data.
80,The effects of data corruption range from lower customer satisfaction to costly system reconciliation that results in service unavailability.
80,"Infinispan can carry out ACID (atomicity, consistency, isolation, durability) transactions to ensure the cache state is consistent."
80,9.1. Transactions
80,Infinispan can be configured to use and to participate in JTA compliant transactions.
80,"Alternatively, if transaction support is disabled, it is equivalent to using autocommit in JDBC calls, where modifications are potentially replicated after every change (if replication is enabled)."
80,On every cache operation Infinispan does the following:
80,Retrieves the current Transaction associated with the thread
80,"If not already done, registers XAResource with the transaction manager to be notified when a transaction commits or is rolled back."
80,"In order to do this, the cache has to be provided with a reference to the environment’s TransactionManager."
80,This is usually done by configuring the cache with the class name of an implementation of the TransactionManagerLookup interface.
80,"When the cache starts, it will create an instance of this class and invoke its getTransactionManager() method, which returns a reference to the TransactionManager."
80,Infinispan ships with several transaction manager lookup classes:
80,Transaction manager lookup implementations
80,EmbeddedTransactionManagerLookup:
80,This provides with a basic transaction manager which should only be used for embedded mode when no other implementation is available.
80,This implementation has some severe limitations to do with concurrent transactions and recovery.
80,JBossStandaloneJTAManagerLookup:
80,"If you’re running Infinispan in a standalone environment, or in JBoss AS 7 and earlier, and WildFly 8, 9, and 10, this should be your default choice for transaction manager."
80,It’s a fully fledged transaction manager based on JBoss Transactions which overcomes all the deficiencies of the EmbeddedTransactionManager.
80,WildflyTransactionManagerLookup:
80,"If you’re running Infinispan in WildFly 11 or later, this should be your default choice for transaction manager."
80,GenericTransactionManagerLookup:
80,This is a lookup class that locate transaction managers in the most popular Java EE application servers.
80,"If no transaction manager can be found, it defaults on the EmbeddedTransactionManager."
80,"Once initialized, the TransactionManager can also be obtained from the Cache itself:"
80,//the cache must have a transactionManagerLookupClass defined
80,Cache cache = cacheManager.getCache();
80,//equivalent with calling TransactionManagerLookup.getTransactionManager();
80,TransactionManager tm = cache.getAdvancedCache().getTransactionManager();
80,9.1.1. Configuring transactions
80,Transactions are configured at cache level.
80,Below is the configuration that affects a transaction behaviour and a small description of each configuration attribute.
80,<locking
80,"isolation=""READ_COMMITTED""/>"
80,<transaction
80,"locking=""OPTIMISTIC"""
80,"auto-commit=""true"""
80,"complete-timeout=""60000"""
80,"mode=""NONE"""
80,"notifications=""true"""
80,"reaper-interval=""30000"""
80,"recovery-cache=""__recoveryInfoCacheName__"""
80,"stop-timeout=""30000"""
80,"transaction-manager-lookup=""org.infinispan.transaction.lookup.GenericTransactionManagerLookup""/>"
80,or programmatically:
80,ConfigurationBuilder builder = new ConfigurationBuilder();
80,builder.locking()
80,.isolationLevel(IsolationLevel.READ_COMMITTED);
80,builder.transaction()
80,.lockingMode(LockingMode.OPTIMISTIC)
80,.autoCommit(true)
80,.completedTxTimeout(60000)
80,.transactionMode(TransactionMode.NON_TRANSACTIONAL)
80,.useSynchronization(false)
80,.notifications(true)
80,.reaperWakeUpInterval(30000)
80,.cacheStopTimeout(30000)
80,.transactionManagerLookup(new GenericTransactionManagerLookup())
80,.recovery()
80,.enabled(false)
80,".recoveryInfoCacheName(""__recoveryInfoCacheName__"");"
80,isolation - configures the isolation level. Check section Isolation Levels for more details.
80,Default is REPEATABLE_READ.
80,locking - configures whether the cache uses optimistic or pessimistic locking. Check section Transaction Locking for more details.
80,Default is OPTIMISTIC.
80,"auto-commit - if enable, the user does not need to start a transaction manually for a single operation. The transaction is automatically started and committed."
80,Default is true.
80,complete-timeout - the duration in milliseconds to keep information about completed transactions. Default is 60000.
80,mode - configures whether the cache is transactional or not. Default is NONE. The available options are:
80,NONE - non transactional cache
80,FULL_XA - XA transactional cache with recovery enabled. Check section Transaction recovery for more details about recovery.
80,NON_DURABLE_XA - XA transactional cache with recovery disabled.
80,NON_XA - transactional cache with integration via Synchronization instead of XA.
80,Check section Enlisting Synchronizations for details.
80,BATCH-
80,transactional cache using batch to group operations. Check section Batching for details.
80,notifications - enables/disables triggering transactional events in cache listeners. Default is true.
80,reaper-interval - the time interval in millisecond at which the thread that cleans up transaction completion information kicks in.
80,Defaults is 30000.
80,recovery-cache - configures the cache name to store the recovery information. Check section Transaction recovery for more details about recovery.
80,Default is recoveryInfoCacheName.
80,stop-timeout - the time in millisecond to wait for ongoing transaction when the cache is stopping. Default is
80,30000.
80,transaction-manager-lookup - configures the fully qualified class name of a class that looks up a reference to a javax.transaction.TransactionManager.
80,Default is org.infinispan.transaction.lookup.GenericTransactionManagerLookup.
80,For more details on how Two-Phase-Commit (2PC) is implemented in Infinispan and how locks are being acquired see the section below.
80,More details about the configuration settings are available in Configuration reference.
80,9.1.2. Isolation levels
80,Infinispan offers two isolation levels - READ_COMMITTED and REPEATABLE_READ.
80,"These isolation levels determine when readers see a concurrent write, and are internally implemented using different subclasses of MVCCEntry, which have different behaviour in how state is committed back to the data container."
80,Here’s a more detailed example that should help understand the difference between READ_COMMITTED and REPEATABLE_READ in the context of Infinispan.
80,"With READ_COMMITTED, if between two consecutive read calls on the same key, the key has been updated by another transaction, the second read may return the new updated value:"
80,Thread1: tx1.begin()
80,Thread1: cache.get(k) // returns v
80,Thread2:
80,tx2.begin()
80,Thread2:
80,cache.get(k) // returns v
80,Thread2:
80,"cache.put(k, v2)"
80,Thread2:
80,tx2.commit()
80,Thread1: cache.get(k) // returns v2!
80,Thread1: tx1.commit()
80,"With REPEATABLE_READ, the final get will still return v."
80,"So, if you’re going to retrieve the same key multiple times within a transaction, you should use REPEATABLE_READ."
80,"However, as read-locks are not acquired even for REPEATABLE_READ, this phenomena can occur:"
80,"cache.get(""A"") // returns 1"
80,"cache.get(""B"") // returns 1"
80,Thread1: tx1.begin()
80,"Thread1: cache.put(""A"", 2)"
80,"Thread1: cache.put(""B"", 2)"
80,Thread2:
80,tx2.begin()
80,Thread2:
80,"cache.get(""A"") // returns 1"
80,Thread1: tx1.commit()
80,Thread2:
80,"cache.get(""B"") // returns 2"
80,Thread2:
80,tx2.commit()
80,9.1.3. Transaction locking
80,Pessimistic transactional cache
80,"From a lock acquisition perspective, pessimistic transactions obtain locks on keys at the time the key is written."
80,A lock request is sent to the primary owner (can be an explicit lock request or an operation)
80,The primary owner tries to acquire the lock:
80,"If it succeed, it sends back a positive reply;"
80,"Otherwise, a negative reply is sent and the transaction is rollback."
80,As an example:
80,transactionManager.begin();
80,"cache.put(k1,v1); //k1 is locked."
80,cache.remove(k2); //k2 is locked when this returns
80,transactionManager.commit();
80,"When cache.put(k1,v1) returns, k1 is locked and no other transaction running anywhere in the cluster can write to it."
80,Reading k1 is still possible.
80,The lock on k1 is released when the transaction completes (commits or rollbacks).
80,"For conditional operations, the validation is performed in the originator."
80,Optimistic transactional cache
80,With optimistic transactions locks are being acquired at transaction prepare time and are only being held up to the point the transaction commits (or rollbacks).
80,This is different from the 5.0 default locking model where local locks are being acquire on writes and cluster locks are being acquired during prepare time.
80,The prepare is sent to all the owners.
80,The primary owners try to acquire the locks needed:
80,"If locking succeeds, it performs the write skew check."
80,"If the write skew check succeeds (or is disabled), send a positive reply."
80,"Otherwise, a negative reply is sent and the transaction is rolled back."
80,As an example:
80,transactionManager.begin();
80,"cache.put(k1,v1);"
80,cache.remove(k2);
80,"transactionManager.commit(); //at prepare time, K1 and K2 is locked until committed/rolled back."
80,"For conditional commands, the validation still happens on the originator."
80,What do I need - pessimistic or optimistic transactions?
80,"From a use case perspective, optimistic transactions should be used when there is not a lot of contention between multiple transactions running at the same time."
80,That is because the optimistic transactions rollback if data has changed between the time it was read and the time it was committed (with write skew check enabled).
80,"On the other hand, pessimistic transactions might be a better fit when there is high contention on the keys and transaction rollbacks are less desirable."
80,Pessimistic transactions are more costly by their nature: each write operation potentially involves a RPC for lock acquisition.
80,9.1.4. Write Skews
80,Write skews occur when two transactions independently and simultaneously read and write to the same key. The result of a write skew is that both transactions successfully commit updates to the same key but with different values.
80,Infinispan automatically performs write skew checks to ensure data consistency for REPEATABLE_READ isolation levels in optimistic transactions. This allows Infinispan to detect and roll back one of the transactions.
80,"When operating in LOCAL mode, write skew checks rely on Java object"
80,"references to compare differences, which provides a reliable technique for"
80,checking for write skews.
80,Forcing write locks on keys in pessimitic transactions
80,"To avoid write skews with pessimistic transactions, lock keys at read-time with Flag.FORCE_WRITE_LOCK."
80,"In non-transactional caches, Flag.FORCE_WRITE_LOCK does not work. The get() call reads the key value but does not acquire locks remotely."
80,You should use Flag.FORCE_WRITE_LOCK with transactions in which the entity is updated later in the same transaction.
80,Compare the following code snippets for an example of Flag.FORCE_WRITE_LOCK:
80,// begin the transaction
80,if (!cache.getAdvancedCache().lock(key)) {
80,// abort the transaction because the key was not locked
80,} else {
80,cache.get(key);
80,"cache.put(key, value);"
80,// commit the transaction
80,// begin the transaction
80,try {
80,// throws an exception if the key is not locked.
80,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(key);
80,"cache.put(key, value);"
80,} catch (CacheException e) {
80,// mark the transaction rollback-only
80,// commit or rollback the transaction
80,9.1.5. Dealing with exceptions
80,"If a CacheException (or a subclass of it) is thrown by a cache method within the scope of a JTA transaction, then the transaction is automatically marked for rollback."
80,9.1.6. Enlisting Synchronizations
80,By default Infinispan registers itself as a first class participant in distributed transactions through XAResource.
80,"There are situations where Infinispan is not required to be a participant in the transaction, but only to be notified by its lifecycle (prepare, complete): e.g. in the case Infinispan is used as a 2nd level cache in Hibernate."
80,Infinispan allows transaction enlistment through Synchronization.
80,To enable it just use NON_XA transaction mode.
80,Synchronizations have the advantage that they allow TransactionManager to optimize 2PC with a 1PC where only one other resource is enlisted with that transaction (last resource commit optimization).
80,"E.g. Hibernate second level cache: if Infinispan registers itself with the TransactionManager as a XAResource than at commit time, the TransactionManager sees two XAResource (cache and database) and does not make this optimization."
80,Having to coordinate between two resources it needs to write the tx log to disk.
80,"On the other hand, registering Infinispan as a Synchronization makes the TransactionManager skip writing the log to the disk (performance improvement)."
80,9.1.7. Batching
80,"Batching allows atomicity and some characteristics of a transaction, but not full-blown JTA or XA capabilities."
80,Batching is often a lot lighter and cheaper than a full-blown transaction.
80,"Generally speaking, one should use batching API whenever the only participant in the transaction is an Infinispan cluster."
80,"On the other hand, JTA transactions (involving TransactionManager) should be used whenever the transactions involves multiple systems."
80,"E.g. considering the ""Hello world!"" of transactions: transferring money from one bank account to the other."
80,"If both accounts are stored within Infinispan, then batching can be used."
80,"If one account is in a database and the other is Infinispan, then distributed transactions are required."
80,You do not have to have a transaction manager defined to use batching.
80,API
80,"Once you have configured your cache to use batching, you use it by calling startBatch() and endBatch() on Cache. E.g.,"
80,Cache cache = cacheManager.getCache();
80,// not using a batch
80,"cache.put(""key"", ""value""); // will replicate immediately"
80,// using a batch
80,cache.startBatch();
80,"cache.put(""k1"", ""value"");"
80,"cache.put(""k2"", ""value"");"
80,"cache.put(""k2"", ""value"");"
80,cache.endBatch(true); // This will now replicate the modifications since the batch was started.
80,// a new batch
80,cache.startBatch();
80,"cache.put(""k1"", ""value"");"
80,"cache.put(""k2"", ""value"");"
80,"cache.put(""k3"", ""value"");"
80,"cache.endBatch(false); // This will ""discard"" changes made in the batch"
80,Batching and JTA
80,"Behind the scenes, the batching functionality starts a JTA transaction, and all the invocations in that scope are associated with it."
80,For this it uses a very simple (e.g. no recovery) internal TransactionManager implementation.
80,"With batching, you get:"
80,Locks you acquire during an invocation are held until the batch completes
80,Changes are all replicated around the cluster in a batch as part of the batch completion process. Reduces replication chatter for each update in the batch.
80,"If synchronous replication or invalidation are used, a failure in replication/invalidation will cause the batch to roll back."
80,All the transaction related configurations apply for batching as well.
80,9.1.8. Transaction recovery
80,"Recovery is a feature of XA transactions, which deal with the eventuality of a resource or possibly even the transaction manager failing, and recovering accordingly from such a situation."
80,When to use recovery
80,Consider a distributed transaction in which money is transferred from an account stored in an external database to an account stored in Infinispan.
80,"When TransactionManager.commit() is invoked, both resources prepare successfully (1st phase). During the commit (2nd) phase, the database successfully applies the changes whilst Infinispan fails before receiving the commit request from the transaction manager."
80,At this point the system is in an inconsistent state: money is taken from the account in the external database but not visible yet in Infinispan (since locks are only released during 2nd phase of a two-phase commit protocol).
80,Recovery deals with this situation to make sure data in both the database and Infinispan ends up in a consistent state.
80,How does it work
80,Recovery is coordinated by the transaction manager.
80,"The transaction manager works with Infinispan to determine the list of in-doubt transactions that require manual intervention and informs the system administrator (via email, log alerts, etc)."
80,"This process is transaction manager specific, but generally requires some configuration on the transaction manager."
80,"Knowing the in-doubt transaction ids, the system administrator can now connect to the Infinispan cluster and replay the commit of transactions or force the rollback."
80,Infinispan provides JMX tooling for this - this is explained extensively in the Transaction recovery and reconciliation section.
80,Configuring recovery
80,Recovery is not enabled by default in Infinispan.
80,"If disabled, the TransactionManager won’t be able to work with Infinispan to determine the in-doubt transactions."
80,The Transaction configuration section shows how to enable it.
80,NOTE: recovery-cache attribute is not mandatory and it is configured per-cache.
80,"For recovery to work, mode must be set to FULL_XA, since full-blown XA transactions are needed."
80,Enable JMX support
80,In order to be able to use JMX for managing recovery JMX support must be explicitly enabled.
80,Recovery cache
80,"In order to track in-doubt transactions and be able to reply them, Infinispan caches all transaction state for future use."
80,"This state is held only for in-doubt transaction, being removed for successfully completed transactions after when the commit/rollback phase completed."
80,This in-doubt transaction data is held within a local cache: this allows one to configure swapping this info to disk through cache loader in the case it gets too big.
80,This cache can be specified through the recovery-cache configuration attribute.
80,If not specified Infinispan will configure a local cache for you.
80,It is possible (though not mandated) to share same recovery cache between all the Infinispan caches that have recovery enabled.
80,"If the default recovery cache is overridden, then the specified recovery cache must use a TransactionManagerLookup that returns a different transaction manager than the one used by the cache itself."
80,Integration with the transaction manager
80,"Even though this is transaction manager specific, generally a transaction manager would need a reference to a XAResource implementation in order to invoke XAResource.recover() on it."
80,In order to obtain a reference to an Infinispan XAResource following API can be used:
80,XAResource xar = cache.getAdvancedCache().getXAResource();
80,It is a common practice to run the recovery in a different process from the one running the transaction.
80,Reconciliation
80,The transaction manager informs the system administrator on in-doubt transaction in a proprietary way.
80,At this stage it is assumed that the system administrator knows transaction’s XID (a byte array).
80,A normal recovery flow is:
80,"STEP 1: The system administrator connects to an Infinispan server through JMX, and lists the in doubt transactions."
80,The image below demonstrates JConsole connecting to an Infinispan node that has an in doubt transaction.
80,Figure 8. Show in-doubt transactions
80,"The status of each in-doubt transaction is displayed(in this example "" PREPARED "")."
80,"There might be multiple elements in the status field, e.g. ""PREPARED"" and ""COMMITTED"" in the case the transaction committed on certain nodes but not on all of them."
80,"STEP 2: The system administrator visually maps the XID received from the transaction manager to an Infinispan internal id, represented as a number."
80,"This step is needed because the XID, a byte array, cannot conveniently be passed to the JMX tool (e.g. JConsole) and then re-assembled on Infinispan’s side."
80,"STEP 3: The system administrator forces the transaction’s commit/rollback through the corresponding jmx operation, based on the internal id."
80,The image below is obtained by forcing the commit of the transaction based on its internal id.
80,Figure 9. Force commit
80,"All JMX operations described above can be executed on any node, regardless of where the transaction originated."
80,Force commit/rollback based on XID
80,XID-based JMX operations for forcing in-doubt transactions' commit/rollback are available as well: these methods receive byte[] arrays describing the XID instead of the number associated with the transactions (as previously described at step 2).
80,These can be useful e.g. if one wants to set up an automatic completion job for certain in-doubt transactions.
80,This process is plugged into transaction manager’s recovery and has access to the transaction manager’s XID objects.
80,10. Configuring locking and concurrency
80,Infinispan uses multi-versioned concurrency control (MVCC) to improve access to shared data.
80,Allowing concurrent readers and writers
80,Readers and writers do not block one another
80,Write skews can be detected and handled
80,Internal locks can be striped
80,10.1. Locking and concurrency
80,Multi-versioned concurrency control (MVCC) is a concurrency scheme popular with relational databases and other data stores.
80,MVCC offers many advantages over coarse-grained Java synchronization and even JDK Locks for access to shared data.
80,"Infinispan’s MVCC implementation makes use of minimal locks and synchronizations, leaning heavily towards lock-free techniques such as compare-and-swap and lock-free data structures wherever possible, which helps optimize for multi-CPU and multi-core environments."
80,"In particular, Infinispan’s MVCC implementation is heavily optimized for readers."
80,"Reader threads do not acquire explicit locks for entries, and instead directly read the entry in question."
80,"Writers, on the other hand, need to acquire a write lock."
80,"This ensures only one concurrent writer per entry, causing concurrent writers to queue up to change an entry."
80,"To allow concurrent reads, writers make a copy of the entry they intend to modify, by wrapping the entry in an MVCCEntry."
80,This copy isolates concurrent readers from seeing partially modified state.
80,"Once a write has completed, MVCCEntry.commit() will flush changes to the data container and subsequent readers will see the changes written."
80,10.1.1. Clustered caches and locks
80,"In Infinispan clusters, primary owner nodes are responsible for locking keys."
80,"For non-transactional caches, Infinispan forwards the write operation to the primary owner of the key so it can attempt to lock it."
80,Infinispan either then forwards the write operation to the other owners or throws an exception if it cannot lock the key.
80,"If the operation is conditional and fails on the primary owner, Infinispan does not forward it to the other owners."
80,"For transactional caches, primary owners can lock keys with optimistic and pessimistic locking modes."
80,Infinispan also supports different isolation levels to control concurrent reads between transactions.
80,10.1.2. The LockManager
80,The LockManager is a component that is responsible for locking an entry for writing.
80,The LockManager makes use of a LockContainer to locate/hold/create locks.
80,"LockContainers come in two broad flavours, with support for lock striping and with support for one lock per entry."
80,10.1.3. Lock striping
80,"Lock striping entails the use of a fixed-size, shared collection of locks for the entire cache, with locks being allocated to entries based on the entry’s key’s hash code."
80,"Similar to the way the JDK’s ConcurrentHashMap allocates locks, this allows for a highly scalable, fixed-overhead locking mechanism in exchange for potentially unrelated entries being blocked by the same lock."
80,The alternative is to disable lock striping - which would mean a new lock is created per entry.
80,"This approach may give you greater concurrent throughput, but it will be at the cost of additional memory usage, garbage collection churn, etc."
80,Default lock striping settings
80,"lock striping is disabled by default, due to potential deadlocks that can happen if locks for different keys end up in the same lock stripe."
80,The size of the shared lock collection used by lock striping can be tuned using the concurrencyLevel attribute of the <locking /> configuration element.
80,Configuration example:
80,"<locking striping=""false|true""/>"
80,new ConfigurationBuilder().locking().useLockStriping(false|true);
80,10.1.4. Concurrency levels
80,"In addition to determining the size of the striped lock container, this concurrency level is also used to tune any JDK ConcurrentHashMap based collections where related, such as internal to DataContainers."
80,"Please refer to the JDK ConcurrentHashMap Javadocs for a detailed discussion of concurrency levels, as this parameter is used in exactly the same way in Infinispan."
80,Configuration example:
80,"<locking concurrency-level=""32""/>"
80,new ConfigurationBuilder().locking().concurrencyLevel(32);
80,10.1.5. Lock timeout
80,"The lock timeout specifies the amount of time, in milliseconds, to wait for a contented lock."
80,Configuration example:
80,"<locking acquire-timeout=""10000""/>"
80,new ConfigurationBuilder().locking().lockAcquisitionTimeout(10000);
80,//alternatively
80,"new ConfigurationBuilder().locking().lockAcquisitionTimeout(10, TimeUnit.SECONDS);"
80,10.1.6. Consistency
80,The fact that a single owner is locked (as opposed to all owners being locked) does not break the following consistency guarantee:
80,"if key K is hashed to nodes {A, B} and transaction TX1 acquires a lock for K, let’s say on A."
80,"If another transaction, TX2, is started on B (or any other node) and TX2 tries to lock K then it will fail with a timeout as the lock is already held by TX1."
80,"The reason for this is the that the lock for a key K is always, deterministically, acquired on the same node of the cluster, regardless of where the transaction originates."
80,10.1.7. Data Versioning
80,Infinispan supports two forms of data versioning: simple and external.
80,The simple versioning is used in transactional caches for write skew check.
80,"The external versioning is used to encapsulate an external source of data versioning within Infinispan, such as when using Infinispan with Hibernate which in turn gets its data version information directly from a database."
80,"In this scheme, a mechanism to pass in the version becomes necessary, and overloaded versions of put() and putForExternalRead() will be provided in AdvancedCache to take in an external data version."
80,This is then stored on the InvocationContext and applied to the entry at commit time.
80,Write skew checks cannot and will not be performed in the case of external data versioning.
80,11. Using clustered counters
80,Infinispan provides counters that record the count of objects and are distributed across all nodes in a cluster.
80,11.1. Clustered Counters
80,Clustered counters are counters which are distributed and shared among all nodes in the Infinispan cluster.
80,Counters can have different consistency levels: strong and weak.
80,"Although a strong/weak consistent counter has separate interfaces, both support updating its value,"
80,return the current value and they provide events when its value is updated.
80,Details are provided below in this document to help you choose which one fits best your uses-case.
80,11.1.1. Installation and Configuration
80,"In order to start using the counters, you needs to add the dependency in your Maven pom.xml file:"
80,pom.xml
80,<dependency>
80,<groupId>org.infinispan</groupId>
80,<artifactId>infinispan-clustered-counter</artifactId>
80,</dependency>
80,The counters can be configured Infinispan configuration file or on-demand via the CounterManager interface detailed
80,later in this document.
80,A counters configured in Infinispan configuration file is created at boot time when the EmbeddedCacheManager is starting.
80,These counters are started eagerly and they are available in all the cluster’s nodes.
80,configuration.xml
80,<infinispan>
80,<cache-container ...>
80,"<!-- To persist counters, you need to configure the global state. -->"
80,<global-state>
80,<!-- Global state configuration goes here. -->
80,</global-state>
80,<!-- Cache configuration goes here. -->
80,"<counters xmlns=""urn:infinispan:config:counters:14.0"" num-owners=""3"" reliability=""CONSISTENT"">"
80,"<strong-counter name=""c1"" initial-value=""1"" storage=""PERSISTENT""/>"
80,"<strong-counter name=""c2"" initial-value=""2"" storage=""VOLATILE"" lower-bound=""0""/>"
80,"<strong-counter name=""c3"" initial-value=""3"" storage=""PERSISTENT"" upper-bound=""5""/>"
80,"<strong-counter name=""c4"" initial-value=""4"" storage=""VOLATILE"" lower-bound=""0"" upper-bound=""10""/>"
80,"<strong-counter name=""c5"" initial-value=""0"" upper-bound=""100"" lifespan=""60000""/>"
80,"<weak-counter name=""c6"" initial-value=""5"" storage=""PERSISTENT"" concurrency-level=""1""/>"
80,</counters>
80,</cache-container>
80,</infinispan>
80,"or programmatically, in the GlobalConfigurationBuilder:"
80,GlobalConfigurationBuilder globalConfigurationBuilder = ...;
80,CounterManagerConfigurationBuilder builder = globalConfigurationBuilder.addModule(CounterManagerConfigurationBuilder.class);
80,builder.numOwner(3).reliability(Reliability.CONSISTENT);
80,"builder.addStrongCounter().name(""c1"").initialValue(1).storage(Storage.PERSISTENT);"
80,"builder.addStrongCounter().name(""c2"").initialValue(2).lowerBound(0).storage(Storage.VOLATILE);"
80,"builder.addStrongCounter().name(""c3"").initialValue(3).upperBound(5).storage(Storage.PERSISTENT);"
80,"builder.addStrongCounter().name(""c4"").initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE);"
80,"builder.addStrongCounter().name(""c5"").initialValue(0).upperBound(100).lifespan(60000);"
80,"builder.addWeakCounter().name(""c6"").initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT);"
80,"On other hand, the counters can be configured on-demand, at any time after the EmbeddedCacheManager is initialized."
80,CounterManager manager = ...;
80,"manager.defineCounter(""c1"", CounterConfiguration.builder(CounterType.UNBOUNDED_STRONG).initialValue(1).storage(Storage.PERSISTENT).build());"
80,"manager.defineCounter(""c2"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(2).lowerBound(0).storage(Storage.VOLATILE).build());"
80,"manager.defineCounter(""c3"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(3).upperBound(5).storage(Storage.PERSISTENT).build());"
80,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE).build());"
80,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(0).upperBound(100).lifespan(60000).build());"
80,"manager.defineCounter(""c6"", CounterConfiguration.builder(CounterType.WEAK).initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT).build());"
80,CounterConfiguration is immutable and can be reused.
80,The method defineCounter() will return true if the counter is successful configured or false otherwise.
80,"However, if the configuration is invalid, the method will throw a CounterConfigurationException."
80,"To find out if a counter is already defined, use the method isDefined()."
80,CounterManager manager = ...
80,"if (!manager.isDefined(""someCounter"")) {"
80,"manager.define(""someCounter"", ...);"
80,Additional resources
80,Infinispan configuration schema reference
80,List counter names
80,"To list all the counters defined, the method CounterManager.getCounterNames() returns a collection of all counter"
80,names created cluster-wide.
80,11.1.2. CounterManager interface
80,"The CounterManager interface is the entry point to define, retrieve and remove counters."
80,Embedded deployments
80,CounterManager automatically listen to the creation of EmbeddedCacheManager and proceeds with the registration
80,of an
80,instance of it per EmbeddedCacheManager.
80,It starts the caches needed to store the counter state and configures the default counters.
80,Retrieving the CounterManager is as simple as invoke the
80,EmbeddedCounterManagerFactory.asCounterManager(EmbeddedCacheManager)
80,as shown in the example below:
80,// create or obtain your EmbeddedCacheManager
80,EmbeddedCacheManager manager = ...;
80,// retrieve the CounterManager
80,CounterManager counterManager = EmbeddedCounterManagerFactory.asCounterManager(manager);
80,Server deployments
80,"For Hot Rod clients, the CounterManager is registered in the RemoteCacheManager and can be retrieved as follows:"
80,// create or obtain your RemoteCacheManager
80,RemoteCacheManager manager = ...;
80,// retrieve the CounterManager
80,CounterManager counterManager = RemoteCounterManagerFactory.asCounterManager(manager);
80,Remove a counter via CounterManager
80,There is a difference between remove a counter via the Strong/WeakCounter interfaces and the CounterManager.
80,The CounterManager.remove(String) removes the counter value from the cluster and removes all the listeners registered
80,in the counter in the local counter instance.
80,"In addition, the counter instance is no longer reusable and it may return an invalid results."
80,"On the other side, the Strong/WeakCounter removal only removes the counter value."
80,The instance can still be reused and the listeners still works.
80,The counter is re-created if it is accessed after a removal.
80,11.1.3. The Counter
80,A counter can be strong (StrongCounter) or weakly consistent (WeakCounter) and both is identified by a name.
80,"They have a specific interface but they share some logic, namely, both of them are asynchronous"
80,"( a CompletableFuture is returned by each operation), provide an update event and can be reset to its initial value."
80,"If you don’t want to use the async API, it is possible to return a synchronous counter via sync() method."
80,The API is the same but without the CompletableFuture return value.
80,The following methods are common to both interfaces:
80,String getName();
80,CompletableFuture<Long> getValue();
80,CompletableFuture<Void> reset();
80,<T extends CounterListener> Handle<T> addListener(T listener);
80,CounterConfiguration getConfiguration();
80,CompletableFuture<Void> remove();
80,SyncStrongCounter sync(); //SyncWeakCounter for WeakCounter
80,getName() returns the counter name (identifier).
80,getValue() returns the current counter’s value.
80,reset() allows to reset the counter’s value to its initial value.
80,addListener() register a listener to receive update events.
80,More details about it in the Notification and Events section.
80,getConfiguration() returns the configuration used by the counter.
80,remove() removes the counter value from the cluster. The instance can still be used and the listeners are kept.
80,sync() creates a synchronous counter.
80,The counter is re-created if it is accessed after a removal.
80,The StrongCounter interface: when the consistency or bounds matters.
80,The strong counter provides uses a single key stored in Infinispan cache to provide the consistency needed.
80,All the updates are performed under the key lock to updates its values.
80,"On other hand, the reads don’t acquire any locks and reads the current value."
80,"Also, with this scheme, it allows to bound the counter value and provide atomic operations like compare-and-set/swap."
80,A StrongCounter can be retrieved from the CounterManager by using the getStrongCounter() method.
80,As an example:
80,CounterManager counterManager = ...
80,"StrongCounter aCounter = counterManager.getStrongCounter(""my-counter"");"
80,"Since every operation will hit a single key, the StrongCounter has a higher contention rate."
80,The StrongCounter interface adds the following method:
80,default CompletableFuture<Long> incrementAndGet() {
80,return addAndGet(1L);
80,default CompletableFuture<Long> decrementAndGet() {
80,return addAndGet(-1L);
80,CompletableFuture<Long> addAndGet(long delta);
80,"CompletableFuture<Boolean> compareAndSet(long expect, long update);"
80,"CompletableFuture<Long> compareAndSwap(long expect, long update);"
80,incrementAndGet() increments the counter by one and returns the new value.
80,decrementAndGet() decrements the counter by one and returns the new value.
80,addAndGet() adds a delta to the counter’s value and returns the new value.
80,compareAndSet() and compareAndSwap() atomically set the counter’s value if the current value is the expected.
80,A operation is considered completed when the CompletableFuture is completed.
80,The difference between compare-and-set and compare-and-swap is that the former returns true if the operation succeeds
80,while the later returns the previous value.
80,The compare-and-swap is successful if the return value is the same as the expected.
80,Bounded StrongCounter
80,"When bounded, all the update method above will throw a CounterOutOfBoundsException when they reached the"
80,lower or upper bound.
80,The exception has the following methods to check which side bound has been reached:
80,public boolean isUpperBoundReached();
80,public boolean isLowerBoundReached();
80,Uses cases
80,The strong counter fits better in the following uses cases:
80,"When counter’s value is needed after each update (example, cluster-wise ids generator or sequences)"
80,"When a bounded counter is needed (example, rate limiter)"
80,Usage Examples
80,"StrongCounter counter = counterManager.getStrongCounter(""unbounded_counter"");"
80,// incrementing the counter
80,"System.out.println(""new value is "" + counter.incrementAndGet().get());"
80,// decrement the counter's value by 100 using the functional API
80,counter.addAndGet(-100).thenApply(v -> {
80,"System.out.println(""new value is "" + v);"
80,return null;
80,}).get();
80,"// alternative, you can do some work while the counter is updated"
80,CompletableFuture<Long> f = counter.addAndGet(10);
80,// ... do some work ...
80,"System.out.println(""new value is "" + f.get());"
80,"// and then, check the current value"
80,"System.out.println(""current value is "" + counter.getValue().get());"
80,"// finally, reset to initial value"
80,counter.reset().get();
80,"System.out.println(""current value is "" + counter.getValue().get());"
80,// or set to a new value if zero
80,"System.out.println(""compare and set succeeded? "" + counter.compareAndSet(0, 1));"
80,"And below, there is another example using a bounded counter:"
80,"StrongCounter counter = counterManager.getStrongCounter(""bounded_counter"");"
80,// incrementing the counter
80,try {
80,"System.out.println(""new value is "" + counter.addAndGet(100).get());"
80,} catch (ExecutionException e) {
80,Throwable cause = e.getCause();
80,if (cause instanceof CounterOutOfBoundsException) {
80,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
80,"System.out.println(""ops, upper bound reached."");"
80,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
80,"System.out.println(""ops, lower bound reached."");"
80,// now using the functional API
80,"counter.addAndGet(-100).handle((v, throwable) -> {"
80,if (throwable != null) {
80,Throwable cause = throwable.getCause();
80,if (cause instanceof CounterOutOfBoundsException) {
80,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
80,"System.out.println(""ops, upper bound reached."");"
80,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
80,"System.out.println(""ops, lower bound reached."");"
80,return null;
80,"System.out.println(""new value is "" + v);"
80,return null;
80,}).get();
80,Compare-and-set vs Compare-and-swap examples:
80,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
80,"long oldValue, newValue;"
80,do {
80,oldValue = counter.getValue().get();
80,newValue = someLogic(oldValue);
80,"} while (!counter.compareAndSet(oldValue, newValue).get());"
80,"With compare-and-swap, it saves one invocation counter invocation (counter.getValue())"
80,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
80,long oldValue = counter.getValue().get();
80,"long currentValue, newValue;"
80,do {
80,currentValue = oldValue;
80,newValue = someLogic(oldValue);
80,"} while ((oldValue = counter.compareAndSwap(oldValue, newValue).get()) != currentValue);"
80,"To use a strong counter as a rate limiter, configure upper-bound and lifespan parameters as follows:"
80,// 5 request per minute
80,CounterConfiguration configuration = CounterConfiguration.builder(CounterType.BOUNDED_STRONG)
80,.upperBound(5)
80,.lifespan(60000)
80,.build();
80,"counterManager.defineCounter(""rate_limiter"", configuration);"
80,"StrongCounter counter = counterManager.getStrongCounter(""rate_limiter"");"
80,"// on each operation, invoke"
80,try {
80,counter.incrementAndGet().get();
80,// continue with operation
80,} catch (InterruptedException e) {
80,Thread.currentThread().interrupt();
80,} catch (ExecutionException e) {
80,if (e.getCause() instanceof CounterOutOfBoundsException) {
80,// maximum rate. discard operation
80,return;
80,} else {
80,"// unexpected error, handling property"
80,The lifespan parameter is an experimental capability and may be removed in a future version.
80,The WeakCounter interface: when speed is needed
80,The WeakCounter stores the counter’s value in multiple keys in Infinispan cache.
80,The number of keys created is configured by the concurrency-level attribute.
80,Each key stores a partial state of the counter’s value and it can be updated concurrently.
80,It main advantage over the StrongCounter is the lower contention in the cache.
80,"On other hand, the read of its value is more expensive and bounds are not allowed."
80,The reset operation should be handled with caution.
80,It is not atomic and it produces intermediates values.
80,These value may be seen by a read operation and by any listener registered.
80,A WeakCounter can be retrieved from the CounterManager by using the getWeakCounter() method.
80,As an example:
80,CounterManager counterManager = ...
80,"StrongCounter aCounter = counterManager.getWeakCounter(""my-counter);"
80,Weak Counter Interface
80,The WeakCounter adds the following methods:
80,default CompletableFuture<Void> increment() {
80,return add(1L);
80,default CompletableFuture<Void> decrement() {
80,return add(-1L);
80,CompletableFuture<Void> add(long delta);
80,They are similar to the `StrongCounter’s methods but they don’t return the new value.
80,Uses cases
80,The weak counter fits best in uses cases where the result of the update operation is not needed or the counter’s value
80,is not required too often.
80,Collecting statistics is a good example of such an use case.
80,Examples
80,"Below, there is an example of the weak counter usage."
80,"WeakCounter counter = counterManager.getWeakCounter(""my_counter"");"
80,// increment the counter and check its result
80,counter.increment().get();
80,"System.out.println(""current value is "" + counter.getValue());"
80,CompletableFuture<Void> f = counter.add(-100);
80,//do some work
80,f.get(); //wait until finished
80,"System.out.println(""current value is "" + counter.getValue().get());"
80,//using the functional API
80,"counter.reset().whenComplete((aVoid, throwable) -> System.out.println(""Reset done "" + (throwable == null ? ""successfully"" : ""unsuccessfully""))).get();"
80,"System.out.println(""current value is "" + counter.getValue().get());"
80,11.1.4. Notifications and Events
80,Both strong and weak counter supports a listener to receive its updates events.
80,The listener must implement CounterListener and it can be registered by the following method:
80,<T extends CounterListener> Handle<T> addListener(T listener);
80,The CounterListener has the following interface:
80,public interface CounterListener {
80,void onUpdate(CounterEvent entry);
80,The Handle object returned has the main goal to remove the CounterListener when it is not longer needed.
80,"Also, it allows to have access to the CounterListener instance that is it handling."
80,It has the following interface:
80,public interface Handle<T extends CounterListener> {
80,T getCounterListener();
80,void remove();
80,"Finally, the CounterEvent has the previous and current value and state."
80,It has the following interface:
80,public interface CounterEvent {
80,long getOldValue();
80,State getOldState();
80,long getNewValue();
80,State getNewState();
80,The state is always State.VALID for unbounded strong counter and weak counter.
80,State.LOWER_BOUND_REACHED and State.UPPER_BOUND_REACHED are only valid for bounded strong counters.
80,The weak counter reset() operation will trigger multiple notification with intermediate values.
80,12. Listeners and notifications
80,Use listeners with Infinispan to get notifications when events occur for the Cache Manager or for caches.
80,12.1. Listeners and notifications
80,"Infinispan offers a listener API, where clients can register for and get notified when events take place."
80,This annotation-driven API applies to 2 different levels: cache level events and Cache Manager level events.
80,Events trigger a notification which is dispatched to listeners.
80,Listeners are simple POJOs annotated with @Listener and registered using the methods defined in the Listenable interface.
80,"Both Cache and CacheManager implement Listenable, which means you can attach listeners to either a cache or a Cache Manager, to receive either cache-level or Cache Manager-level notifications."
80,"For example, the following class defines a listener to print out some information every time a new entry is added to the cache, in a non blocking fashion:"
80,@Listener
80,public class PrintWhenAdded {
80,Queue<CacheEntryCreatedEvent> events = new ConcurrentLinkedQueue<>();
80,@CacheEntryCreated
80,public CompletionStage<Void> print(CacheEntryCreatedEvent event) {
80,events.add(event);
80,return null;
80,"For more comprehensive examples, please see the Javadocs for @Listener."
80,12.2. Cache-level notifications
80,"Cache-level events occur on a per-cache basis, and by default are only raised on nodes where the events occur."
80,Note in a distributed cache these events are only raised on the owners of data being affected.
80,"Examples of cache-level events are entries being added, removed, modified, etc."
80,These events trigger notifications to listeners registered to a specific cache.
80,"Please see the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for a comprehensive list of all cache-level notifications, and their respective method-level annotations."
80,Please refer to the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for the list of cache-level notifications available in Infinispan.
80,Cluster listeners
80,The cluster listeners should be used when it is desirable to listen to the cache events on a single node.
80,To do so all that is required is set to annotate your listener as being clustered.
80,@Listener (clustered = true)
80,public class MyClusterListener { .... }
80,There are some limitations to cluster listeners from a non clustered listener.
80,"A cluster listener can only listen to @CacheEntryModified, @CacheEntryCreated, @CacheEntryRemoved and @CacheEntryExpired events."
80,Note this means any other type of event will not be listened to for this listener.
80,"Only the post event is sent to a cluster listener, the pre event is ignored."
80,Event filtering and conversion
80,All applicable events on the node where the listener is installed will be raised to the listener.
80,"It is possible to dynamically filter what events are raised by using a KeyFilter (only allows filtering on keys) or CacheEventFilter (used to filter for keys, old value, old metadata, new value, new metadata, whether command was retried, if the event is before the event (ie. isPre) and also the command type)."
80,The example here shows a simple KeyFilter that will only allow events to be raised when an event modified the entry for the key Only Me.
80,public class SpecificKeyFilter implements KeyFilter<String> {
80,private final String keyToAccept;
80,public SpecificKeyFilter(String keyToAccept) {
80,if (keyToAccept == null) {
80,throw new NullPointerException();
80,this.keyToAccept = keyToAccept;
80,public boolean accept(String key) {
80,return keyToAccept.equals(key);
80,...
80,"cache.addListener(listener, new SpecificKeyFilter(""Only Me""));"
80,...
80,This can be useful when you want to limit what events you receive in a more efficient manner.
80,There is also a CacheEventConverter that can be supplied that allows for converting a value to another before raising the event.
80,This can be nice to modularize any code that does value conversions.
80,The mentioned filters and converters are especially beneficial when used in conjunction with a Cluster Listener.
80,This is because the filtering and conversion is done on the node where the event originated and not on the node where event is listened to.
80,This can provide benefits of not having to replicate events across the cluster (filter) or even have reduced payloads (converter).
80,Initial State Events
80,When a listener is installed it will only be notified of events after it is fully installed.
80,It may be desirable to get the current state of the cache contents upon first registration of listener by having an event generated of type @CacheEntryCreated for each element in the cache.
80,Any additionally generated events during this initial phase will be queued until appropriate events have been raised.
80,This only works for clustered listeners at this time.
80,ISPN-4608 covers adding this for non clustered listeners.
80,Duplicate Events
80,It is possible in a non transactional cache to receive duplicate events.
80,This is possible when the primary owner of a key goes down while trying to perform a write operation such as a put.
80,"Infinispan internally will rectify the put operation by sending it to the new primary owner for the given key automatically, however there are no guarantees in regards to if the write was first replicated to backups."
80,"Thus more than 1 of the following write events (CacheEntryCreatedEvent, CacheEntryModifiedEvent & CacheEntryRemovedEvent) may be sent on a single operation."
80,If more than one event is generated Infinispan will mark the event that it was generated by a retried command to help the user to know when this occurs without having to pay attention to view changes.
80,@Listener
80,public class MyRetryListener {
80,@CacheEntryModified
80,public void entryModified(CacheEntryModifiedEvent event) {
80,if (event.isCommandRetried()) {
80,// Do something
80,Also when using a CacheEventFilter or CacheEventConverter the EventType contains a method isRetry to tell if the event was generated due to retry.
80,12.3. Cache Manager notifications
80,"Events that occur on a Cache Manager level are cluster-wide and involve events that affect all caches created by a single Cache Manager. Examples of Cache Manager events are nodes joining or leaving a cluster, or caches starting or stopping."
80,"See the org.infinispan.notifications.cachemanagerlistener.annotation package for a comprehensive list of all Cache Manager notifications,"
80,and their respective method-level annotations.
80,12.4. Synchronicity of events
80,"By default, all async notifications are dispatched in the notification thread pool."
80,Sync notifications will delay the operation from continuing until the listener method completes or the CompletionStage
80,"completes (the former causing the thread to block). Alternatively, you could annotate your listener as asynchronous in"
80,"which case the operation will continue immediately, while the notification is completed asynchronously on the notification thread pool."
80,"To do this, simply annotate your listener such:"
80,Asynchronous Listener
80,@Listener (sync = false)
80,public class MyAsyncListener {
80,@CacheEntryCreated
80,void listen(CacheEntryCreatedEvent event) { }
80,Blocking Synchronous Listener
80,@Listener
80,public class MySyncListener {
80,@CacheEntryCreated
80,void listen(CacheEntryCreatedEvent event) { }
80,Non-Blocking Listener
80,@Listener
80,public class MyNonBlockingListener {
80,@CacheEntryCreated
80,CompletionStage<Void> listen(CacheEntryCreatedEvent event) { }
80,Asynchronous thread pool
80,"To tune the thread pool used to dispatch such asynchronous notifications, use the <listener-executor /> XML element in your configuration file."
80,Last updated 2024-03-13 12:05:42 UTC
81,Configure an external PostgreSQL for SAS Viya in your Open-Source Kubernetes deployments – part 2
81,Community
81,Home
81,Welcome
81,Getting Started
81,Community Memo
81,All Things Community
81,SAS Customer Recognition Awards (2023)
81,SAS Community Library
81,SAS Product Suggestions
81,Upcoming Events
81,SAS Customer Recognition Awards
81,All Recent Topics
81,Learn
81,New SAS User
81,SAS Software for Learning Community
81,Ask the Expert
81,SAS Certification
81,SAS Tips from the Community
81,SAS Training
81,Programming 1 and 2
81,Advanced Programming
81,SAS Academy for Data Science
81,Course Case Studies and Challenges
81,SAS Global Forum Proceedings 2021
81,Programming
81,SAS Programming
81,SAS Procedures
81,SAS Enterprise Guide
81,SAS Studio
81,Graphics Programming
81,ODS and Base Reporting
81,SAS Code Examples
81,SAS Web Report Studio
81,Developers
81,Analytics
81,Statistical Procedures
81,SAS Data Science
81,"Mathematical Optimization, Discrete-Event Simulation, and OR"
81,SAS/IML Software and Matrix Computations
81,SAS Forecasting and Econometrics
81,Streaming Analytics
81,Research and Science from SAS
81,SAS Viya
81,SAS Viya
81,SAS Viya on Microsoft Azure
81,SAS Viya Release Updates
81,Moving to SAS Viya
81,SAS Visual Analytics
81,Valentine's Day Data Viz Challenge
81,SAS Visual Analytics Gallery
81,Your Journey to Success
81,Administration
81,Administration and Deployment
81,Architecture
81,SAS Hot Fix Announcements
81,SAS Product Release Announcements
81,SUGA
81,Solutions
81,Microsoft Integration with SAS
81,Decisioning
81,Data Management
81,Customer Intelligence
81,SAS Customer Intelligence
81,SAS Customer Intelligence 360 Release Notes
81,SAS 360 Match
81,Risk and Fraud
81,SAS Risk Management
81,"Fraud, AML and Security Intelligence"
81,Risk & Finance Analytics
81,Insurance Fraud
81,SAS Health
81,SAS Health and Life Sciences
81,SAS Life Science Analytics Framework
81,Hubs
81,Regional Hubs
81,SAS User Groups
81,SAS Community Nordic
81,SAS Network for Women in Norway
81,AML Nordic User Group
81,SAS Japan
81,SAS Korea
81,CoDe SAS German
81,SAS Plattform Netzwerk
81,SAS Partners in D/AT/CH
81,SAS Brazil Community
81,SAS Spanish Community
81,SAS Users Group in Israel
81,Polish SAS Users Group
81,SAS User Group UK & Ireland
81,欢迎来到SAS中文社区！
81,SAS User Group Middle East
81,Special Interest Hubs
81,SAS Innovate 2024
81,SAS Explore
81,SAS Analytics Explorers
81,The Curiosity Cup
81,SAS Hacker's Hub
81,SAS Inner Circle Panel
81,SAS Hackathon Team Profiles (Past)
81,Sign In
81,All communityThis categoryKnowledge baseUsersProducts
81,cancel
81,Turn on suggestions
81,Auto-suggest helps you quickly narrow down your search results by suggesting possible matches as you type.
81,Showing results for
81,Search instead for
81,Did you mean:
81,Home
81,SAS Communities Library
81,Configure an external PostgreSQL for SAS Viya in your Open-Source Kube...
81,Options
81,RSS Feed
81,Mark as New
81,Mark as Read
81,Bookmark
81,Subscribe
81,Printer Friendly Page
81,Report Inappropriate Content
81,BookmarkSubscribeRSS Feed
81,Configure an external PostgreSQL for SAS Viya in your Open-Source Kubernetes deployments – part 2
81,Started
81,‎01-08-2024
81,RPoumarede
81,Modified
81,‎01-08-2024
81,RPoumarede
81,Article Options
81,Article History
81,RSS Feed
81,Mark as New
81,Mark as Read
81,Bookmark
81,Subscribe
81,Printer Friendly Page
81,Report Inappropriate Content
81,Views
81,491
81,"In the first part of the series, we focused on the reasons to choose an external PostgreSQL instance for the SAS Viya Data Infrastructure Server and how to provision it with the IaC tool for Kubernetes."
81,"In this 2nd article, we detail how the viya4-deployment GitHub project (aka DaC tool) can now be configured to deploy SAS Viya (with an external PostgreSQL server)."
81,"Then we'll open up on additional considerations around the utilization of an external PostgreSQL instance for the Viya platform (HA, connections, tuning , technical benefits)."
81,"Finally, in the last section of the article, we compare the Kubernetes cluster CPU and memory resource usage and reservation between two environments (one using the internal Crunchy Data Server and one using the external PostgreSQL instance running on a distinct VM)."
81,How to configure SAS Viya for it (with the DaC tool)
81,The option to use an external PostgreSQL server as the SAS Infrastructure Data Server has been available for some time.
81,This excellent article from Edoardo Riva is the perfect starting point to understand how to configure Viya to use it.
81,"Basically, you have to provide your Viya environment with some connection information and credentials, so all the Viya components that need to interact with the PostgreSQL Server database (write or read data from the SAS Infrastructure Data Server) can do so."
81,"Here, we just look at an example in the case of an Opensource Kubernetes platform with the most natural companion to the IaC, the viya4-deployment tool (aka ""DaC"" for ""Deployment As Code"")."
81,PostgreSQL Server details
81,"The variable used to define the PostgreSQL details (internal or external) in the DaC tool is named V4_CFG_POSTGRES_SERVERS and is actually a ""map"" of objects."
81,"The variables documentation page shows the various available options, such as the server FQDN and port, the database name, the size of the internal PostgreSQL and pgBackrest PVCs, etc…"
81,"Note that several SAS Viya platform offerings require a second internal Postgres instance referred to as SAS Common Data Store or CDS PostgreSQL (the CDS PostgreSQL can also be provisioned by the IaC tool, as noted in the first part of the series)."
81,Here is an example for an external PostgreSQL server :
81,# External servers
81,V4_CFG_POSTGRES_SERVERS:
81,default:
81,internal: false
81,admin: pgadmin
81,"password: ""password"""
81,fqdn: mydbserver.local
81,server_port: 5432
81,ssl_enforcement_enabled: true
81,database: SharedServices
81,other_db:
81,internal: false
81,admin: pgadmin
81,"password: ""password"""
81,fqdn: 10.10.10.10
81,server_port: 5432
81,ssl_enforcement_enabled: true
81,database: OtherDB
81,Trusted certificate for the PostgreSQL Server
81,"In order to encrypt the communications between the Viya ""clients"" (all services needing to persist or request information in the SAS Infrastructure Data Server) and the PostgreSQL service we also need to provide the PostgreSQL certificate file location, so the DaC tool can add it in the SAS Viya trust store."
81,"Select any image to see a larger version.Mobile users: To view the images, select the ""Full"" version at the bottom of the page."
81,"The PEM-encoded certificate file that we need is on the PostgreSQL server and, as shown in the first part of the article, its location can be found in the PostgreSQL settings."
81,"SELECT Name,Setting from pg_settings WHERE category='Connections and Authentication / SSL';"
81,"So, in this case the V4_CFG_TLS_TRUSTED_CA_CERTSvariable in the DaC ansible vars file should point to the file located on the external PostgreSQL server in /etc/ssl/certs/ssl-cert-sas-rext03-0038.pem."
81,External PostgreSQL Considerations
81,HA PostgreSQL
81,"The SAS Infrastructure Data Server component is a critical component of the Viya platform… if, at some point, it fails or is not working properly, the entire Viya platform stop working as designed."
81,"So, running it as a ""stand-alone"" service would introduce a major SPOF (Single Point Of Failure) in the Viya platform architecture."
81,One of the benefits of the internal PostgreSQL server implementation with the Crunchy Data PostgreSQL operator is that we automatically have a clustered installation of PostgreSQL (1 primary and 2 replicas) which makes the SAS Infrastructure Data Server component Highly-Available.
81,Now the question is How can we have the same High-Availability with external PostgreSQL ?
81,There are various possibilities.
81,The external PostgreSQL implementation also matters.
81,"As of today, according to the official  documentation we support the following options for the external  Postgres implementation:"
81,PostgreSQL (Open Source)
81,Microsoft Azure Database for PostgreSQL - Flexible Server
81,Amazon RDS for PostgreSQL
81,GCP Cloud SQL for PostgreSQL
81,"If you decide to use one of the supported Cloud managed external PostgreSQL database, you need to check their current documentation. However, they generally propose options for High Availability (HA) configurations with automatic failover capabilities."
81,"For example in Azure, with the PostgreSQL Flexible server, “When high availability is configured, flexible server automatically provisions and manages a standby. Write-ahead-logs (WAL) is streamed to the replica in synchronous mode using PostgreSQL streaming replication.”"
81,An alternative is to use the Open Source version of PostgreSQL.
81,"It can be installed on a standalone VM. And that’s exactly what is done when using the IaC for Kubernetes tool. But it would not provide any High Availability by default. In order to have HA, the PostgreSQL should be deployed in a cluster of at least 3 machines and be complemented with something like PGPool for the request load-balancing and a virtual IP address (this is similar  to the Viya 3.5 architecture). Implementing such kind of setup is not trivial."
81,Another possibility would be to rely on Kubernetes itself to provide this HA capabilities by deploying  PostgreSQL with an operator. The operator could be deployed in the same Kubernetes cluster as Viya.
81,Here is a list of PG Operators implementation that could potentially be used:
81,Crunchy Data
81,Zalando
81,CloudNativePG
81,Percona
81,Stackgres
81,(credits to @Carus Kyle for providing this list 😊)
81,"While the experience and feedback on these options are still limited, they will likely allow the installation of a Highly-Available PostgreSQL server. It would be up to the customer team to ensure the ongoing operation of this PostgreSQL server. While these implementations are opensource, most of them (such as CloudNativePG or Percona) can come with a ""full support"" model provided by specialized vendors."
81,Pgbouncer
81,One of the challenges when using a Cloud Managed Service for the external PostgreSQL is to ensure that the maximum number of connections to the database is high enough for SAS Viya (we have a lot of microservices that need to open connections with the PostgreSQL database).
81,"As stated in the official requirements : “An external PostgreSQL server should support a maximum number of connections, max_connections on some providers, and max_prepared_transactions of at least 1024.”"
81,"Cloud providers sometimes apply limits and thresholds depending on the instance type that underpins the database, including on “max_connections”. For example, it used to be problematic with the Azure Database for PostgreSQL - Single Server as higher (and very costly) pricing tier was required to meet this requirement."
81,"Today though, the default Azure Database for PostgreSQL implementation is ""Flexible Server"" and it supports up to 5000 connections. It is much better, however even with 5000 connections, it might not be enough for  some  large multi-tenant environments."
81,"That’s where PGBouncer can help. You might have heard about it, it is also briefly mentioned in our SAS Viya tuning guide."
81,"PGBouncer is a connection pooler for PostgreSQL. It allows a larger number of frontend connections to share a smaller pool of backend connections. The aim of PGBouncer is to lower the performance impact of opening new connections to PostgreSQL, by re-using database connections."
81,"Finally, note that Kubernetes Operators for Postgres, such as Zalando usually include the PGBouncer component by default."
81,Technical Benefits?
81,"There are several benefits in the utilization of an external PostgreSQL implementation for the SAS Data Infrastructure server. Here is a screenshot of a colleague’s comment (in an internal discussion about pros and cons of the ""external postgres"" decision) that I found quite enlightening :"
81,"Finally, from my perspective I was also curious to know if, there was a significant reduction for the Kubernetes Hardware requirement when the SAS infrastructure Data server was located outside of the Kubernetes cluster? So it made me wonder “What is the impact on the cluster resources usage of running PostgreSQL outside of the Kubernetes cluster ?”"
81,"Well, actually it is the question that I have tried to answer in the next section 😊"
81,Resource consumption comparison
81,"When running the OOTB Crunchy Data Server in the Viya namespace, we have 3 PostgreSQL pods corresponding to our clustered instances as well as the Postgres Operator pod and a PgBackRest instance pod when backups are running."
81,"So, surely replacing that with an external PostgreSQL outside of the cluster will reduce the overall Viya CPU and memory footprint on the cluster itself !?"
81,"Since I was very curious about that, I made some comparisons using k9s and some of the Grafana Dashboards coming with the  SAS Viya 4 Monitoring for Kubernetes."
81,I deployed the same Viya order in 2 Upstream Opensource Kubernetes environments : one was deployed with the internal Crunchy Data Server and the other was using an extra machine with an external PostgreSQL Stand-alone deployment (outside of the Kubernetes cluster).
81,"Please, note that these are very simple, ""one time test"", and the values reported there were varying overtime. Also the comparisons are made when the system idle, as we are not generating user activity."
81,"It is clearly not something as thorough and systematic as an official benchmark done by our Performance teams at SAS. However, I found the results quite insightful and I believe it gives a rough idea of the differences in terms of resource reservation and utilization."
81,"Kubernetes objects, overall CPU and memory utilization"
81,"With Crunchy Data Server (""Internal PostgreSQL"")"
81,"Without Crunchy Data Server (""External PostgreSQL"")"
81,"As we can see with the k9s ""pulses"" view, running PostgreSQL outside of the cluster reduces the number of Kubernetes objects (Pods, ReplicaSets, StatefulSets, PersistentVolumes,). In our case the overall impact of the CPU and memory used is low, however it is one data point for a single measure in an idle and empty environment."
81,Overall resources usage
81,"With Crunchy Data Server (""Internal PostgreSQL"")"
81,Without Crunchy Data Server (“External PostgreSQL”)
81,"While the overall cluster differences in terms of usage for the CPU and memory are not really significant (around 1%), there is a slightly bigger difference in the resource requests commitments (49.9% vs 53.2% for the CPU). Less CPU and memory resource are ""reserved"" on the nodes when the external PostgreSQL Server is used."
81,CPU utilization for the Viya namespace
81,"With Crunchy Data Server (""Internal PostgreSQL"")"
81,"Without Crunchy Data Server (""External PostgreSQL"")"
81,"Now if we specifically zoom on the Viya namespace (""dac""), we clearly see the difference in the CPU Requests between the 2 environments (9.23% vs 6.57%)."
81,MEMORY utilization for the Viya namespace
81,"With Crunchy Data Server (""Internal PostgreSQL"")"
81,"Without Crunchy Data Server (""External PostgreSQL"")"
81,"Finally, the two Grafana graphs confirms a relatively low difference on the memory usage in an idle/empty environment."
81,Here are the main takeaways of this first and quick comparison :
81,"The amount of CPU and memory resources reserved on the Kubernetes is reduced when PostgreSQL is running outside of the Kubernetes cluster (each of the four Crunchy Data PostgreSQL Instance requests almost half of a CPU and 3GB of memory),"
81,...but it remains a relatively small reduction in the overall Viya Platform resources requirements.
81,"In an idle, freshly deployed environment without any user’s activity, there is not significant difference in the CPU and memory utilization between the two environments."
81,Conclusion
81,And that’s all for this time.
81,I hope this  series was useful to reiterate some of the established architecture principles and configurations for the SAS infrastructure Data Server component … Thanks for reading !
81,Like
81,Comments
81,touwen_k
81,‎01-08-2024
81,08:05 AM
81,Mark as Read
81,Mark as New
81,Bookmark
81,Permalink
81,Print
81,Report Inappropriate Content
81,‎01-08-2024
81,08:05 AM
81,"Thank you for this article, at this moment we are intending to stay with internal crunchy database as long as it will be provided by SAS because it is less complicated for us as a customer and it provides  high availability. However, an interesting point in the article is, that there are HA capabilities by deploying  PostgreSQL with an operator. In simple words, if you use such operator, do you also need three databases?"
81,Likes
81,RPoumarede
81,‎01-08-2024
81,08:24 AM
81,Mark as Read
81,Mark as New
81,Bookmark
81,Permalink
81,Print
81,Report Inappropriate Content
81,‎01-08-2024
81,08:24 AM
81,Hi @touwen_k
81,"Yes the PostgreSQL database can be deployed inside the Kubernetes cluster and managed by a Kubernetes operator. While is it not mandatory, most of the Postgres Kubernetes operator opensource projects (ex: Zalando, CloudNativePG, etc...) offer the possibility to provide HA by running replicas of the primary database instance (in the same way as the Crunchy Data operator already does with the current Internal postgres implementation)."
81,Likes
81,Version history
81,Last update:
81,‎01-08-2024
81,04:14 AM
81,Updated by:
81,RPoumarede
81,Contributors
81,RPoumarede
81,ANNOUNCEMENT
81,The early bird rate has been extended! Register by March 18 for just $695 - $100 off the standard rate.
81,"Check out the agenda and get ready for a jam-packed event featuring workshops, super demos, breakout sessions, roundtables, inspiring keynotes and incredible networking events."
81,Register now!
81,Free course: Data Literacy Essentials
81,"Data Literacy is for all, even absolute beginners. Jump on board with this free e-learning  and boost your career prospects."
81,Get Started
81,Article Labels
81,Administration and Deployment
81,Architecture
81,SAS Viya
81,Article Tags
81,Find more articles tagged with:GEL
83,Scaling Bitbucket Data Center | Bitbucket Data Center 8.19 | Atlassian Documentation
83,Products
83,Bitbucket Support
83,Documentation
83,Knowledge base
83,Resources
83,Search
83,Log in
83,View account
83,View requests
83,Log out
83,...
83,Knowledge base
83,Products
83,Jira Software
83,Project and issue tracking
83,Jira Service Management
83,Service management and customer support
83,Jira Work Management
83,Manage any business project
83,Confluence
83,Document collaboration
83,Bitbucket
83,Git code management
83,See all
83,Resources
83,Documentation
83,Usage and admin help
83,Community
83,"Answers, support, and inspiration"
83,Suggestions and bugs
83,Feature suggestions and bug reports
83,Marketplace
83,Product apps
83,Billing and licensing
83,Frequently asked questions
83,Log out
83,Log in to account
83,Contact support
83,Training & Certification
83,Cloud Migration Center
83,GDPR guides
83,Enterprise services
83,Atlassian partners
83,Developers
83,User groups
83,Automation for Jira
83,Atlassian.com
83,Page
83,View in Confluence
83,Edit Page
83,Viewport
83,Manage Viewport
83,Confluence
83,Dashboard
83,Space Directory
83,People Directory
83,Bitbucket Data Center 8.19 (Latest)
83,Documentation
83,Unable to load
83,Atlassian Support
83,Bitbucket 8.19
83,Documentation
83,Administer Bitbucket Data Center
83,Scaling Bitbucket Data Center
83,Cloud
83,Data Center 8.19
83,Versions
83,8.19
83,8.18
83,8.17
83,8.16
83,8.15
83,8.14
83,8.13
83,8.12
83,8.11
83,8.10
83,8.9
83,8.8
83,8.7
83,8.6
83,8.5
83,8.4
83,8.3
83,8.2
83,8.1
83,8.0
83,7.21
83,7.20
83,7.18
83,7.19
83,See all
83,Scaling Bitbucket Data Center
83,Administer Bitbucket Data Center
83,Users and groups
83,Advanced repository management
83,External user directories
83,Global permissions
83,Setting up your mail server
83,Integrate with Atlassian applications
83,Connect Bitbucket to an external database
83,Migrating Bitbucket Data Center to another server
83,Migrate Bitbucket Server from Windows to Linux
83,Run Bitbucket in AWS
83,Specify the Bitbucket base URL
83,Configuring the application navigator
83,Managing apps
83,View and configure the audit log
83,Update your license key
83,Configuration properties
83,Change Bitbucket's context path
83,Data recovery and backups
83,Disable HTTP(S) access to Git repositories
83,Mirrors
83,Bitbucket Mesh
83,Export and import projects and repositories
83,Git Large File Storage
83,Git Virtual File System (GVFS)
83,Enable SSH access to Git repositories
83,Signed system commits
83,Secret scanning
83,Use diff transcoding
83,Change the port Bitbucket listens on
83,Lockout recovery process
83,Proxy and secure Bitbucket
83,High availability for Bitbucket
83,Diagnostics for third-party apps
83,Enabling JMX counters for performance monitoring
83,Bitbucket guardrails
83,Enable debug logging
83,Scaling Bitbucket Data Center
83,Add a shortcut link to a repository
83,Administer code search
83,Adding additional storage for your repository data
83,Add a system-wide announcement banner
83,Configuring Project links across Applications
83,Improving instance stability with rate limiting
83,Use a CDN with Atlassian Data Center applications
83,Manage keys and tokens
83,Link to other applications
83,Setting a system-wide default branch name
83,Automatically decline inactive pull requests
83,Secure Bitbucket configuration properties
83,Data pipeline
83,Monitor application performance
83,Xcode for Bitbucket Data Center
83,On this page
83,In this section
83,Scaling Bitbucket Data Centre for Continuous Integration performance
83,Bitbucket Data Center production server data
83,Related content
83,No related content found
83,Still need help?
83,The Atlassian Community is here for you.
83,Ask the community
83,"This page discusses performance and hardware considerations when using Bitbucket Data Center.Note that Bitbucket Data Center resources, not discussed on this page, uses a cluster of nodes to provide Active/Active failover, and is the deployment option of choice for larger enterprises that require high availability and performance at scale.Hardware requirementsThe type of hardware you require to run Bitbucket Data Center depends on a number of factors:The count and concurrency of clone operations, which are the most resource-intensive operation Bitbucket Data Center performs. One major source of clone operations is continuous integration. When your CI builds involve multiple parallel stages, Bitbucket Data Center will be asked to perform multiple clones concurrently, putting significant load on your system.The size of your repositories. There are many operations in Bitbucket Data Center that require more CPU, memory and I/O when working with very large repositories. Furthermore, huge Git repositories (larger than a few GBs) are likely to impact the performance of Git clients as well as Bitbucket Data Center.The number of users."
83,On this page:
83,Related pages: Use Bitbucket in the enterpriseResources for migrating to GitBitbucket Data Center production server dataScaling Bitbucket Data Centre for Continuous Integration performancePotential performance impact of embedded Crowd directory orderingConfiguration properties
83,"Here are some rough guidelines for choosing your hardware:Estimate the number of concurrent clones that are expected to happen regularly (look at continuous integration). Add one CPU for every 2 concurrent clone operations. Estimate or calculate the average repository size and allocate 1.5 x number of concurrent clone operations x min(repository size, 700MB) of memory.If you’re running Bitbucket Data Center, check your size using the Bitbucket Data Center load profiles. If your instance is Large or XLarge, take a look at our infrastructure recommendations for Bitbucket Data Center AWS deployments.See Scaling Bitbucket Data Center for Continuous Integration performance for some additional information about how Bitbucket Data Center SCM cache can help the system scale."
83,"Understanding Bitbucket Data Center resource usageMost of the things you do in Bitbucket Data Center involve both the Bitbucket Data Center instance and one or more Git processes. For instance, when you view a file in the web application, Bitbucket Data Center processes the incoming request, performs permission checks, creates a Git process to retrieve the file contents and formats the resulting webpage. The same is true for the 'hosting' operations like pushing commits, cloning a repository, or fetching the latest changes. As a result, when configuring Bitbucket Data Center for performance, CPU and memory consumption for both Bitbucket Data Center and Git should be taken into account.CPUIn Bitbucket Data Center, much of the heavy lifting is delegated to Git. As a result, when deciding on the required hardware to run Bitbucket Data Center, the CPU usage of the Git processes is the most important factor to consider. Cloning repositories is the most CPU intensive Git operation. When you clone a repository, Git on the server side will create a pack file (a compressed file containing all the commits and file versions in the repository) that is sent to the client. Git can use multiple CPUs while compressing objects to generate a pack, resulting in spikes of very high CPU usage. Other phases of the cloning process are single-threaded and will, at most, max out a single CPU.Encryption (either SSH or HTTPS) may impose a significant CPU overhead if enabled. As for whether SSH or HTTPS should be preferred, there's no clear winner. Each has advantages and disadvantages as described in the following table:"
83,"HTTPHTTPSSSHEncryptionNo CPU overhead for encryption, but plain-text transfer and basic authentication may be unacceptable for security.Encryption has CPU overhead, but this can be offloaded to a separate proxy server (if the SSL/TLS is terminated there).Encryption has CPU overhead.AuthenticationAuthentication is slower – it requires remote authentication with the LDAP or Crowd server.Authentication is generally faster, but may still require an LDAP or Crowd request to verify the connecting user is still active.CloningCloning a repository is slightly slower over HTTP. It requires at least 2 separate requests–and potentially significantly more–each performing its own authentication and permission checks. The extra overhead is typically small, but depends heavily on the latency between client and server.Cloning a repository takes only a single request.MemoryWhen deciding on how much memory to allocate for Bitbucket Data Center, the most important factor to consider is the amount of memory required for Git. Some Git operations are fairly expensive in terms of memory consumption, most notably the initial push of a large repository to Bitbucket Data Center and cloning large repositories from Bitbucket Data Center. For large repositories, it is not uncommon for Git to use hundreds of megabytes, or even multiple gigabytes, of memory during the clone process. The numbers vary from repository to repository, but as a rule of thumb 1.5x the repository size on disk (contents of the .git/objects directory) is a reasonable initial estimate of the required memory for a single clone operation. For large repositories, or repositories that contain large files, memory usage is effectively only bounded by the amount of RAM in the system.In addition to being the most CPU-intensive, cloning repositories is also the most memory intensive Git operation. Most other Git operations, such as viewing file history, file contents and commit lists are lightweight by comparison. Clone operations also tend to retain their memory for significantly longer than other operations.Bitbucket Data Center has been designed to have fairly stable memory usage. Pages that could show large amounts of data (e.g. viewing the source of a multi-megabyte file) perform incremental loading or have hard limits in place to prevent Bitbucket Data Center from holding on to large amounts of memory at any time. In general, the default memory settings (-Xmx1g) should be sufficient to run Bitbucket Data Center. Installing third-party apps may increase the system's memory usage. The maximum amount of memory available to Bitbucket Data Center can be configured in _start-webapp.sh or _start-webapp.bat."
83,"The memory consumption of Git is not managed by the memory settings in _start-webapp.sh or _start-webapp.bat. Git processes are executed outside the Java virtual machine, so JVM memory settings do not apply.Allocating a large heap for Bitbucket Data Center JVM may constrain the amount of memory available for Git processes, which may result in poor performance. A heap of 1-2GB is generally sufficient for Bitbucket Data Center JVM."
83,"DiskGit repository data is stored entirely on the filesystem. Storage with low latency/high IOPS will result in significantly better repository performance, which translates to faster overall performance and improved scaling. Storage with high latency is generally unsuitable for Git operations, even if it can provide high throughput, and will result in poor repository performance. Filesystems like Amazon EFS are not recommended for Bitbucket Data Center home or shared home due to their high latency.Available disk space in $BITBUCKET_HOME/caches, where Bitbucket Data Center SCM cache stores packs to allow them to be reused to serve subsequent clones, is also important for scaling. The SCM cache allows Bitbucket Data Center to trade increased disk usage for reduced CPU and memory usage, since streaming a previously-built pack uses almost no resources compared to creating a pack. When possible, $BITBUCKET_HOME/caches should have a similar amount of total disk space to $BITBUCKET_HOME/shared/data/repositories, where the Git repositories are stored.NetworkCloning a Git repository, by default, includes the entire history. As a result, Git repositories can become quite large, especially if they’re used to track binary files, and serving clones can use a significant amount of network bandwidth.There’s no fixed bandwidth threshold we can document for the system since it will depend heavily on things like; repository size, how heavy CI (Bamboo, Jenkins, etc.) load is, and more. However, it’s worth calling out that Bitbucket Data Center network usage will likely far exceed other Atlassian products like Jira or Confluence.Additionally, when configuring a Data Center cluster, because repository data must be stored on a shared home which is mounted via NFS, Bitbucket Data Center's bandwidth needs are even higher -and its performance is far more sensitive to network latency. Ideally, in a Data Center installation, nodes would use separate NICs and networks for client-facing requests (like hosting) and NFS access to prevent either from starving the other. The NFS network configuration should be as low latency as possible, which excludes using technologies like Amazon EFS.DatabaseThe size of the database required for Bitbucket Data Center primarily depends on the number of repositories the system is hosting and the number of commits in those repositories.A very rough guideline is: 100 + ((total number of commits across all repositories) / 2500) MB.So, for example, for 20 repositories with an average of 25,000 commits each, the database would need 100 + (20 * 25,000 / 2500) = 300MB.Note that repository data is not stored in the database; it’s stored on the filesystem. As a result, having multi-gigabyte repositories does not necessarily mean the system will use dramatically more database space.Where possible, it is preferable to have Bitbucket Data Center database on a separate machine or VM, so the two are not competing for CPU, memory and disk I/O."
83,"Clones examinedSince cloning a repository is the most demanding operation in terms of CPU and memory, it is worthwhile analyzing the clone operation a bit closer. The following graphs show the CPU and memory usage of a clone of a 220 MB repository:"
83,Git process (blue line)CPU usage goes up to 100% while the pack file is created on the server side.CPU peaks at 120% when the pack file is compressed (multiple CPUs used).CPU drops back to 0.5% while the pack file is sent back to the client.Bitbucket Data Center (red bottom line)CPU usage briefly peaks at 30% while the clone request is processed.CPU drops back to 0% while Git prepares the pack file.CPU hovers around 1% while the pack file is sent to the client.Git process (blue line)Memory usage slowly climbs to 270 MB while preparing the pack file.Memory stays at 270 MB while the pack file is transmitted to the client.Memory drops back to 0 when the pack file transmission is complete.Bitbucket Data Center (red upper line)Memory usage hovers around 800 MB and is not affected by the clone operation.This graph shows how concurrency affects average response times for clones:Vertical axis: average response times.Horizontal axis: number of concurrent clone operations.The measurements for this graph were done on a 4 CPU server with 12 GB of memory.Response times become exponentially worse as the number of concurrent clone operationsexceed the number of CPUs.
83,"In-memory cache sizesBitbucket contains a number of in-memory caches. These caches improve performance by reducing the need to query the database frequently. Many of these caches are bounded to avoid exhausting the Java heap memory, and the default sizes of these caches are designed to suit the default Java maximum heap memory configuration of 1 gigabyte.For Bitbucket instances with a large number of users or groups, additional tuning of these in-memory cache sizes can significantly increase performance. Specifically:improve performance of user and group selectorsimprove performance of authorization or permission checks that are carried out for every request for a resource requiring permissionsdecrease the load on the databaseSetting user and group count sizing hintsThe following two properties can be used to provide sizing hints for multiple internal caches:sizing-hint.cache.userssizing.hint.cache.groupsA starting point for calculating appropriate sizes is as follows:PropertySuggested settingsizing-hint.cache.usersA reasonable starting point is to set it to between 50-100% of the licensed user count. More specifically, if all users are likely to be active during the same time of the day, then closer to 100% is better. Whereas, for a geographically distributed user-base where only a subset of users are likely to be active at a given time, a setting closer to 50% may be a good balance.If in doubt, and you have ample free system memory to apply below the Java heap size tuning, then lean towards 100%.To find the licensed user count, navigate to Administration > Licensing > Licensed users.sizing.hint.cache.groupsA reasonable starting point is to set it to between 50-100% of group membership, specifically count of distinct groups of the set of licensed users. As this can be hard to calculate, start with the total number of distinct groups with members by running the following database query:"
83,select count(distinct lower_parent_name)
83,from cwd_membership
83,where group_type='GROUP' and membership_type='GROUP_USER';
83,"Using this value and the same logic described for users, decide if the value should be closer to 50% or 100%.For the default values, see Bitbucket configuration properties. Decreasing these values below the default is neither necessary nor recommended. However, if you find that you need to increase these values:Navigate to the directory $BITBUCKET_HOME/sharedOpen the file bitbucket.properties in your text editor of choice.Add the following lines and replace the value 1234 with the settings you've derived."
83,sizing-hint.cache.users=1234
83,sizing-hint.cache.groups=1234
83,"Save the file.If necessary, increase the maximum Java heap memory size. Learn how to increase Java heap memory sizeRestart Bitbucket for this setting to take effect.Increasing Java heap memory size"
83,Do not set the maximum Java heap memory size (-Xmx) arbitrarily large. Bitbucket requires memory for both forked Git processes and the operating system page cache in order to perform well. Unnecessary memory allocation to the Java virtual machine reduces the memory available for the above operations.
83,"If you've increased the value of sizing-hint.cache.users from its default value, it is necessary to increase the maximum Java heap memory that the Java virtual machine allows. The heap memory sizes should be set as follows:Value of sizing-hint.cache.usersMaximum Java heap size (-Xmx)Up to 50001g5000-149992g15000-249993g25000+4gThese values should be considered as a starting point. Other specific tuning you may have carried out or plugins you've installed may mandate a further increase in heap memory sizes. As always, analysis of Java garbage collection logs should be the principal guide for heap memory tuning.To set an increased maximum Java heap size:Navigate to the bin directory in the Bitbucket installation directory:"
83,cd <Bitbucket installation directory>/bin
83,Open the _start-webapp.sh file in your editor of choice.Locate the following section:
83,# The following 2 settings control the minimum and maximum memory allocated to the Java virtual machine.
83,"#For larger instances, the maximum amount will need to be increased."
83,"if [ -z ""${JVM_MINIMUM_MEMORY}"" ]; then"
83,JVM_MINIMUM_MEMORY=512m
83,"if [ -z ""${JVM_MAXIMUM_MEMORY}"" ]; then"
83,JVM_MAXIMUM_MEMORY=1g
83,"Set the JVM_MAXIMUM_MEMORY variable to the desired value (for example, increase from 1g to 2g).Save the file.Restart Bitbucket for this setting to take effect."
83,"MonitoringIn order to effectively diagnose performance issues and tune Bitbucket Data Center scaling settings, it is important to configure monitoring. While exactly how to set up monitoring is beyond the scope of this page, there are some guidelines that may be useful:At a minimum, monitoring should include data about CPU, memory, disk I/O (for any disks where Bitbucket Data Center is storing data), free disk space, and network I/O.Monitoring free disk space can be very important for detecting when the SCM cache is nearing free space limits, which could result in it being automatically disabled.When Bitbucket Data Center is used to host large repositories, it can consume a large amount of network bandwidth. If repositories are stored on NFS, for a cluster, bandwidth requirements are even higher.Bitbucket Data Center exposes many JMX counters which may be useful for assembling dashboards to monitor overall system performance and utilization.In particular, tracking used/free tickets for the various buckets, described below, can be very useful for detecting unusual or escalating load.Retaining historical data for monitoring can be very useful for helping to track increases in resource usage over time as well as detecting significant shifts in performance.As users create more repositories, push more commits, open more pull requests and generally just use the system, resource utilization will increase over time.Historical averages can be useful in determining when the system is approaching a point where additional hardware may be required or when it may be time to consider adding another cluster node."
83,"Tickets and throttlingBitbucket Data Center uses a ticket-based approach to throttling requests. The system uses a limited number of different ticket buckets to throttle different types of requests independently, meaning one request type may be at or near its limit, while another type still has free capacity.Each ticket bucket has a default size that will be sufficient in many systems, but as usage grows, the sizes may need to be tuned. In addition to a default size, each bucket has a default timeout which defines the longest a client request is allowed to wait to acquire a ticket before the request is rejected. Rejecting requests under heavy load helps prevent cascading failures, like running out of Tomcat request threads because too many requests are waiting for tickets.Ticket bucketsThe following table shows ticket buckets the system uses, the default size and acquisition timeout, and what each is used for:"
83,"BucketSizeTimeoutUsagescm-command50 (Fixed)2 seconds“scm-command” is used to throttle most of the day-to-day Git commands the system runs. For example:git diff, used to show commit and pull request diffsgit rev-list, used to show commit lists and information about specific commitsgit merge, used to merge pull requests“scm-command” tickets are typically directly connected to web UI and REST requests, and generally have very quick turnaround - most commands typically complete in tens to hundreds of milliseconds. Because a user is typically waiting, “scm-command” tickets apply a very short timeout in order to favor showing users an error over displaying spinners for extended periods.scm-hosting1x-4x (Adaptive; see below)5 minutes“scm-hosting” is used to throttle git clone and git fetch. During a clone or fetch, after sending a ref advertisement, the server generates a pack file (on the fly) which contains the objects the client has requested. For large repositories, this can be a very CPU, memory and I/O-intensive operation. Servicing clones and fetches produces the majority of the load on most Bitbucket Data Center instances.For SSH only, “scm-hosting” is also used to throttle git upload-archive requests. git upload-archive does not currently support HTTP(S) remotes. Generating an archive is less resource-intensive than generating a pack, but is likely to still use more resources (and for longer) than serving a ref advertisement or push.“scm-hosting” uses an adaptive throttling mechanism (described in detail below) which allows the system to dynamically adjust the number of available tickets in response to system load. The default range is proportional to a configurable scaling factor, which defaults to the number of CPUs reported by the JVM. For example, if the JVM reports 8 CPUs, the system will default to 1x8=8 tickets minimum and 4x8=32 tickets maximum.scm-refs8x (Fixed proportional)1 minute“scm-refs” is used to throttle ref advertisements, which are the first step in the process of servicing both pushes and pulls.Additionally, because most of the CPU and memory load are client side, pushes are throttled using the “scm-refs” bucket. Unlike a clone or a fetch, the pack for a push is generated using the client’s CPU, memory and I/O. While processing the received pack does produce load on the server side, it’s minimal compared to generating a pack for a clone or fetch.The default size for the “scm-refs” bucket is proportional to a configurable scaling factor, which defaults to the number of CPUs reported by the JVM. For example, if the JVM reports 8 CPUs, the system will default to 8x8=64 “scm-refs” tickets.Ref advertisements are generally served fairly quickly, even for repositories with large numbers of refs, so the default timeout for “scm-refs” is shorter than the default for “scm-hosting”.git-lfs80 (Fixed)Immediate“git-lfs” is used to throttle requests for large objects using Git LFS. LFS requests are much more similar to a basic file download than a pack request, and produce little system load. The primary reason they’re throttled at all is to prevent large numbers of concurrent LFS requests from consuming all of Tomcat's limited HTTP request threads, thereby blocking access to users trying to browse the web UI, or make REST or hosting operations.Because LFS is predominantly used for large objects, the amount of time a single LFS ticket may be held can vary widely. Since it’s hard to make a reasonable guess about when a ticket might become available, requests for “git-lfs” tickets timeout immediately when the available tickets are all in use.mirror-hosting2x (Fixed proportional)1 hour""mirror-hosting"" is Data Center-only and is used to throttle git clone and git fetch requests from smart mirrors and mirror farms. Using a separate bucket for mirror requests allows different configuration, like using a longer timeout. No user would wait an hour to acquire a ticket, but mirrors will. A separate bucket ensures busy mirrors don't consume all of the system's ""scm-hosting"" tickets and prevent users or CI from being able to push or pull.Unlike ""scm-hosting"", ""mirror-hosting"" is not adaptive. It uses a fixed number of tickets based on the number of CPUs reported by the JVM. For example, if the JVM reports 8 CPUs, the system will default to 2x8=16 ""mirror-hosting"" tickets. The default limit is generally sufficient, but for instances with a large number of mirrors, or large mirror farms, it may be necessary to increase it. Administrators will need to balance the number of ""mirror-hosting"" tickets they allow against the number of ""scm-hosting"" tickets they allow to prevent excessive combined load between the two."
83,Earlier versions
83,"Prior to Bitbucket 7.3, ""scm-hosting"" tickets were used to throttle all parts of hosting operations, including ref advertisements and pushes. This meant that the ""scm-hosting"" bucket often needed to be sized very generously to prevent fast-completing ref advertisements from getting blocked behind slow-running clones or fetches when competing for tickets. However, when the ""scm-hosting"" limit was very high, if a large number of clone or fetch requests were initiated concurrently, it could result in a load spike that effectively crippled or even crashed the server. ""scm-refs"" tickets were introduced to combat that risk. With ""scm-refs"", administrators can configure the system to allow for heavy polling load (typically from CI servers like Bamboo or Jenkins) without necessarily increasing the number of available ""scm-hosting"" tickets."
83,"Adaptive throttlingAdaptive throttling uses a combination of total physical memory, evaluated once during startup, and CPU load, evaluated periodically while the system is running, to dynamically adjust the number of available ""scm-hosting"" tickets within a configurable range.During startup, the total physical memory on the machine is used to determine the maximum number of tickets the machine can safely support. This is done by considering how much memory Bitbucket Data Center and the bundled search server need for their JVMs and an estimate of how much memory each Git hosting operation consumes on average while running, and may produce a safe upper bound that is lower (but never higher) than the configured upper bound.To illustrate this more concretely, consider a system with 8 CPU cores and 8GB of physical RAM. With 8 CPU cores, the default adaptive range will be 8-32 tickets.The total is reduced by 1GB for Bitbucket Data Center default heap: 7GBThe total is reduced by 512MB for bundled search: 6.5GBThe remainder is divided by 256MB: 6656 / 256 = 26In this example, the actual upper bound for the adaptive range will be 26 tickets, rather than the 32 calculated from CPU cores, because the system doesn't have enough RAM to safely handle 32 tickets.While the system is running, Bitbucket Data Center periodically samples CPU usage (every 5 seconds by default) and increases or decreases the number of available tickets based on a target load threshold (75% by default). A smoothing factor is applied to CPU measurements so the system doesn't overreact by raising or lowering the number of available tickets too aggressively in response to bursty load.Adaptive throttling is enabled by default, but the system may automatically revert to fixed throttling if any of the following conditions are met:A non-default fixed number of tickets has been set; for example throttle.resource.scm-hosting=25A fixed throttling strategy is configured explicitly; for example throttle.resource.scm-hosting.strategy=fixed throttle.resource.scm-hosting.fixed.limit=25The adaptive throttling configuration is invalid in same wayThe total physical memory on the machine is so limited that even the minimum number of tickets is considered unsafeAdaptive throttling is only available for the ""scm-hosting"" ticket bucket. Other buckets, like ""scm-refs"", do not support adaptive throttling; they use fixed limits for the number of tickets. This prevents high CPU usage from git clone and git fetch requests from reducing the number of tickets available in other buckets, which generally don't use much CPU."
83,"CachingBuilding pack files to serve clone requests is one of the most resource-intensive operations Bitbucket Data Center performs, consuming significant amounts of CPU, memory and disk I/O. To reduce load, and allow instances to service more requests, Bitbucket Data Center can cache packs between requests so they only need to be built once. When a pack is served from the cache, no ""scm-hosting"" ticket is used. This can be particularly beneficial for systems with heavy continuous integration (CI) load, from systems like Bamboo or Jenkins, where a given repository may be cloned several times either concurrently or in short succession.Cached packs are stored in $BITBUCKET_HOME/caches/scm, with individual subdirectories for each repository. On Data Center nodes, packs are cached per node and are not shared. If free space on the disk where $BITBUCKET_HOME/caches/scm is stored falls below a configurable threshold, pack file caching will be automatically disabled until free space increases. Cached packs will automatically be evicted using a least-recently-used (LRU) strategy to try and free up space when free space approaches the threshold.ConsiderationsBecause clones typically include the full history for the repository, cached packs are often close to the same size as the repository being cloned. This means cached packs can consume a significant amount of disk space–often more than the repository itself consumes if multiple packs are cachedIt may be desirable to use a separate disk or partition mounted at $BITBUCKET_HOME/caches to allow for more disk space and to ensure cached packs don't fill up the same disk where other system data is storedUsing single-branch clones (e.g. git clone --single-branch) can result in a large number of distinct cached packs for a single repository. In general, for maximizing cache hits, it's better to use full clonesLimitationsPack files for git fetch requests are not cached. Unlike clones, where it's likely the same clone will be executed multiple times, fetches tend to be much more unique and are unlikely to produce many cache hits"
83,Earlier versions
83,"Prior to Bitbucket 7.4 the system supported caching ref advertisements as well as packs. Unlike pack file caching, enabling ref advertisement caching did little to reduce system load. Instead, the primary benefit of ref advertisement caching was reduced contention for ""scm-hosting"" tickets. Ref advertisement caching was disabled by default because it could result in advertising stale refs. This meant administrators had to balance the risk of stale data against the reduction in ""scm-hosting"" ticket usage.Bitbucket 7.3 introduced a new ""scm-refs"" bucket for throttling ref advertisements, eliminating contention for ""scm-hosting"" tickets, and Bitbucket 7.4 introduced significant reductions in the number of threads used to service HTTP and SSH hosting operations. The combination of those improvements eliminate the benefits of ref advertisement caching, leaving only its downsides, so support for ref advertisement caching has been removed."
83,"HTTPS and SSHBitbucket Data Center can serve hosting operations via HTTPS and SSH protocols. Each has its pros and cons, and it's possible to disable serving hosting operations via either protocol if desired.HTTPSServing hosting operations via HTTPS requires 2 or more requests to complete the overall operation. By default Git will attempt to reuse the same connection for subsequent requests, to reduce overhead, but it's not always possible to do so. Additionally, Git does not support upload-archive over HTTP(S); it's only available over SSH. One advantage of HTTPS for hosting is that encryption overhead can be offloaded to a proxy to reduce CPU load on the Bitbucket Data Center instance.Git's HTTPS support offers 2 wire protocols, referred to as ""smart"" and ""dumb"". Bitbucket Data Center only supports the ""smart"" wire protocol, which has 2 versions: v0 and v2 (v1 was a transitional protocol and is not generally used). The v0 ""smart"" wire protocol is always supported; v2 is only supported when Git 2.18+ is installed on both Bitbucket Data Center and on clients.By default, Tomcat allows up to 200 threads to process incoming requests. Bitbucket 7.4 introduced the use of asynchronous requests to move processing for hosting operations to a background threadpool, freeing up Tomcat's threads to handle other requests (like web UI or REST requests). The background threadpool allows 250 threads by default. If the background threadpool is fully utilized, subsequent HTTPS hosting operations are handled directly on Tomcat's threads. When all 200 Tomcat threads are in use, a small number of additional requests are allowed to queue (50 by default) before subsequent requests are rejected.SSHHosting operations via SSH are handled using a single request, with bidirectional communication between the client and server. SSH supports the full range of hosting operations: receive-pack (push), upload-archive (archive) and upload-pack (pull). One disadvantage of SSH is that its encryption overhead cannot be offloaded to a proxy; it must be handled by the Bitbucket Data Center JVM.Git's SSH support only offers a single wire protocol, which is roughly equivalent to HTTPS's ""smart"" wire protocol. As with the HTTPS ""smart"" wire protocol, Git's SSH wire protocol supports 2 versions: v0 and v2. The v0 wire protocol is always supported; v2 is only supported when Git 2.18+ is installed on both Bitbucket Data Center and on clients.By default, Bitbucket Data Center allows up to 250 simultaneous SSH sessions, and sessions over that limit are rejected to prevent backlogs."
83,Earlier versions
83,"Prior to Bitbucket 7.4, hosting operations via HTTPS and SSH used 5 threads per request. One of these was the actual HTTPS or SSH request thread, and the rest were overhead related to using blocking I/O to communicate with the git process that was servicing the request. In Bitbucket 7.4, that blocking I/O approach was replaced with a non-blocking approach which eliminated the 4 overhead threads, allowing HTTPS and SSH hosting operations to be serviced by a single thread."
83,"Configuring Bitbucket Data Center scaling options and system propertiesThe sizes and timeouts for the various ticket buckets are all configurable; see Configuration properties.When the configured limit is reached for the given resource, requests will wait until a currently running request has completed. If no request completes within a configurable timeout, the request will be rejected. When requests while accessing the Bitbucket Data Center UI are rejected, users will see either a 501 error page indicating the server is under load, or a popup indicating part of the current page failed to load. When Git client 'hosting' commands (pull/push/clone) are rejected, Bitbucket Data Center does a number of things:Bitbucket Data Center will return an error message to the client which the user will see on the command line: ""Bitbucket is currently under heavy load and is not able to service your request. Please wait briefly and try your request again.""A warning message will be logged for every time a request is rejected due to the resource limits, using the following format:""A [scm-hosting] ticket could not be acquired (0/12)""The ticket bucket is shown in brackets, and may be any of the available buckets (e.g. “scm-command”, “scm-hosting”, “scm-refs” or “git-lfs”).For five minutes after a request is rejected, Bitbucket Data Center will display a red banner in the UI for all users to warn that the server is under load.This period is also configurable.The hard, machine-level limits throttling is intended to prevent hitting are very OS- and hardware-dependent, so you may need to tune the configured limits for your instance of Bitbucket Data Center. When hyperthreading is enabled for the server CPU, for example, the default number of “scm-hosting” and “scm-refs” tickets may be too high, since the JVM will report double the number of physical CPU cores. In such cases, we recommend starting off with a less aggressive value; the value can be increased later if hosting operations begin to back up and system monitoring shows CPU, memory and I/O still have headroom."
83,"Last modified on Nov 3, 2023"
83,Was this helpful?
83,Yes
83,It wasn't accurate
83,It wasn't clear
83,It wasn't relevant
83,Provide feedback about this article
83,In this section
83,Scaling Bitbucket Data Centre for Continuous Integration performance
83,Bitbucket Data Center production server data
83,Related content
83,No related content found
83,Powered by Confluence and Scroll Viewport.
83,Atlassian
83,Notice at Collection
83,Privacy Policy
83,Terms of Use
83,Security
83,Atlassian
86,Entity Framework Core performance tips | InfoWorld
86,Close Ad
86,infoworld
86,UNITED STATES
86,United States
86,United Kingdom
86,More from the Foundry Network
86,About Us |
86,Contact |
86,Republication Permissions |
86,Privacy Policy |
86,Cookie Policy |
86,Copyright Notice |
86,Terms of Service |
86,European Privacy Settings |
86,Member Preferences |
86,Advertising |
86,Foundry Careers |
86,Ad Choices |
86,E-commerce Links |
86,California: Do Not Sell My Personal Info |
86,Follow Us
86,Close
86,Home
86,Software Development
86,Microsoft .NET
86,.NET Programming
86,"By Joydip Kanjilal,"
86,"Contributor,"
86,InfoWorld
86,How to improve data access performance in EF Core
86,Take advantage of these 10 strategies to improve data access performance when using Entity Framework Core in your data-driven .NET applications.
86,Table of Contents
86,Create a console application project in Visual Studio
86,Retrieve only the data you need
86,Split your large data context into many smaller data contexts
86,Use batch updates for large numbers of entities
86,Disable change tracking for read-only queries
86,Use DbContext pooling
86,Use IQueryable instead of IEnumerable
86,Use eager loading instead of lazy loading
86,Disable lazy loading
86,Use asynchronous instead of synchronous code
86,Show More
86,"Entity Framework Core (EF Core) is an open source ORM (object-relational mapping) framework that bridges the gap between the object model of your application and the data model of your database. EF Core makes life simpler by allowing you to work with the database using .NET objects, instead of having to write data access code.In other words, EF Core lets you write code to execute CRUD actions (create, read, update, and delete) without understanding how the data is persisted in the underlying database. You can more easily retrieve entities from the data store, add, change, and delete entities, and traverse entity graphs by working directly in C#."
86,[ Coding with AI: Tips and best practices from developers ]
86,"You can improve data access performance in EF Core in many different ways, ranging from using eager loading to reducing the database round trips required by your queries. In this article, we will explore 10 tips and tricks or strategies we can use in EF Core to improve the data access performance of our .NET Core applications.To work with the code examples provided below, you should have Visual Studio 2022 installed in your system. If you don’t already have a copy, you can download Visual Studio 2022 here.Create a console application project in Visual StudioFirst off, let’s create a .NET Core console application project in Visual Studio. Assuming Visual Studio 2022 is installed in your system, follow the steps outlined below to create a new .NET Core console application project."
86,Launch the Visual Studio IDE.
86,Click on “Create new project.”
86,"In the “Create new project” window, select “Console App (.NET Core)” from the list of templates displayed."
86,Click Next.
86,"In the “Configure your new project” window, specify the name and location for the new project."
86,Click Next.
86,"In the “Additional information” window shown next, choose “.NET 7.0 (Standard Term Support)” as the Framework version you want to use."
86,Click Create.
86,"We’ll use this project to work with EF Core 7 throughout this article. In the sections that follow, we’ll discuss 10 ways we can improve data access speed in EF Core, illustrated by code examples wherever appropriate. Let’s get started! Retrieve only the data you needWhen dealing with massive volumes of data, you should strive to retrieve only the required records for the specific query. When fetching data, you should use projections to pick just the required fields and avoid retrieving unnecessary fields.The following code snippet shows how to obtain data in a paged fashion. Notice how the beginning page index and page size have been used to choose just the required data. int pageSize = 50, startingPageIndex = 1;var dataContext = new OrderProcessingDbContext();var data = dataContext.Orders.Take(pageSize).Skip(startingPageIndex * pageSize).ToList();Split your large data context into many smaller data contextsThe data context in your application represents your database. Hence, you may wonder whether the application should have only one or more data contexts. In Entity Framework Core, the startup time of a large data context represents a significant performance constraint. As a result, instead of using a single vast data context, you should break the data context into numerous smaller data contexts.Ideally, you should only have one data context per module or unit of work. To use multiple data contexts, simply create a new class for each data context and extend it from the DbContext class.Use batch updates for large numbers of entitiesThe default behavior of EF Core is to send individual update statements to the database when there is a batch of update statements to be executed. Naturally, multiple hits to the database entail a significant performance overhead. To change this behavior and optimize batch updates, you can take advantage of the UpdateRange() method as shown in the code snippet given below.public class DataContext : DbContext"
86,public void BatchUpdateAuthors(List<Author> authors)
86,var students = this.Authors.Where(a => a.Id >10).ToList();
86,this.UpdateRange(authors);
86,SaveChanges();
86,protected override void OnConfiguring
86,(DbContextOptionsBuilder options)
86,"options.UseInMemoryDatabase(""AuthorDb"");"
86,public DbSet<Author> Authors { get; set; }
86,public DbSet<Book> Books { get; set; }
86,"}If you’re using EF Core 7 or later, you can use the ExecuteUpdate and ExecuteDelete methods to perform batch updates and eliminate multiple database hits. For example: _context.Authors.Where(a => a.Id > 10).ExecuteUpdate();Disable change tracking for read-only queriesThe default behavior of EF Core is to track objects retrieved from the database. Tracking is required when you want to update an entity with new data, but it is a costly operation when you’re dealing with large data sets. Hence, you can improve performance by disabling tracking when you won’t be modifying the entities.For read-only queries, i.e., when you want to retrieve entities without modifying them, you should use AsNoTracking to improve performance. The following code snippet illustrates how AsNoTracking can be used to disable tracking for an individual query in EF Core.var dbModel = await this._context.Authors.AsNoTracking()"
86,".FirstOrDefaultAsync(e => e.Id == author.Id);The code snippet given below shows how you can retrieve entities directly from the database for read-only purposes, without tracking and without loading them into the memory.public class DataContext : DbContext"
86,public IQueryable<Author> GetAuthors()
86,return Set<Author>().AsNoTracking();
86,"}Use DbContext poolingAn application typically has multiple data contexts. Because DbContext objects may be costly to create and dispose of, EF Core offers a mechanism for pooling them. By pooling, DbContext objects are created once, then reused when needed. Using a DbContext pool in EF Core can improve performance by reducing the overhead involved in building and disposing of DbContext objects. Your application may also use less memory as a result.The following code snippet illustrates how you can configure DbContext pooling in the Program.cs file.builder.Services.AddDbContextPool<MyDbContext>(options => options.UseSqlServer(connection));Use IQueryable instead of IEnumerableWhen you’re quering data in EF Core, use IQueryable instead of IEnumerable. When you use IQueryable, the SQL statements will be executed on the server side, where the data is stored, whereas IEnumerable requires the query to be executed on the client side. Moreover, while IQueryable supports query optimizations and lazy loading, IEnumerable does not. This explains why IQueryable executes queries faster than IEnumerable.The following code snippet shows how you can use IQueryable to query data.IQueryable<Author> query = _context.Authors;"
86,query = query.Where(e => e.Id == 5);
86,query = query.OrderBy(e => e.Id);
86,"List<Author> entities = query.ToList();Use eager loading instead of lazy loadingEF Core uses lazy loading by default. With lazy loading, the related entities are loaded into the memory only when they are accessed. The benefit is that data aren’t loaded unless they are needed. However, lazy loading can be costly in terms of performance because multiple database queries may be required to load the data.To solve this problem for specific scenarios, you can use eager loading in EF Core. Eager loading fetches your entities and related entities in a single query, reducing the number of round trips to the database. The following code snippet shows how eager loading can be used.public class DataContext : DbContext"
86,public List<Author> GetEntitiesWithEagerLoading()
86,List<Author> entities = this.Set<Author>()
86,.Include(e => e.Books)
86,.ToList();
86,return entities;
86,"}Disable lazy loadingBy eliminating the need to load unnecessary related entities (as in explicit loading), lazy loading seems to relieve the developer from dealing with related entities entirely. Because EF Core is adept at automatically loading related entities from the database when accessed by your code, lazy loading seems like a nice feature.However, lazy loading is especially prone to generating unnecessary additional round trips, which could slow down your application. You can turn off lazy loading by specifying the following in your data context:ChangeTracker.LazyLoadingEnabled = false;Use asynchronous instead of synchronous codeYou should use async code to improve the performance and responsiveness of your application. Below I’ll share a code example that shows how you can execute queries asynchronously in EF Core. First, consider the following two model classes.public class Author"
86,public int Id { get; set; }
86,public string FirstName { get; set; }
86,public string LastName { get; set; }
86,public List<Book> Books { get; set; }
86,public class Book
86,public int Id { get; set; }
86,public string Title { get; set; }
86,public Author Author { get; set; }
86,"}In the code snippet that follows, we’ll create a custom data context class by extending the DbContext class of EF Core library.public class DataContext : DbContext"
86,protected readonly IConfiguration Configuration;
86,public DataContext(IConfiguration configuration)
86,Configuration = configuration;
86,protected override void OnConfiguring
86,(DbContextOptionsBuilder options)
86,"options.UseInMemoryDatabase(""AuthorDb"");"
86,public DbSet<Author> Authors { get; set; }
86,public DbSet<Book> Books { get; set; }
86,}Note that we're using an in-memory database here for simplicity. The following code snippet illustrates how you can use async code to update an entity in the database using EF Core.public async Task<int> Update(Author author)
86,var dbModel = await this._context.Authors
86,.FirstOrDefaultAsync(e => e.Id == author.Id);
86,dbModel.Id = author.Id;
86,dbModel.FirstName = author.FirstName;
86,dbModel.LastName = author.LastName;
86,dbModel.Books = author.Books;
86,return await this._context.SaveChangesAsync();
86,"}Reduce the round trips to the databaseYou can significantly reduce the number of round trips to the database by avoiding the N+1 selects problem. The N+1 selects problem has plagued database performance since the early days of ORMs. The name refers to the problem of sending N+1 small queries to the database to retrieve data that could be retrieved with one big query.In EF Core, the N+1 problem can occur when you’re trying to load data from two tables having a one-to-many or many-to-many relationship. For example, let’s say you’re loading author data from the Authors table and also book data from the Books table. Consider the following piece of code.foreach (var author in this._context.Authors)"
86,author.Books.ForEach(b => b.Title.ToUpper());
86,"}Note that the outer foreach loop will fetch all authors using one query. This is the “1” in your N+1 queries. The inner foreach that fetches the books represents the “N” in your N+1 problem, because the inner foreach will be executed N times.To solve this problem, you should fetch the related data in advance (using eager loading) as part of the “1” query. In other words, you should include the book data in your initial query for the author data, as shown in the code snippet given below.var entitiesQuery = this._context.Authors"
86,.Include(b => b.Books);
86,foreach (var entity in entitiesQuery)
86,entity.Books.ForEach(b => b.Title.ToUpper());
86,"}By doing so, you reduce the number of round trips to the database from N+1 to just one. This is because by using Include, we enable eager loading. The outer query, i.e., the entitiesQuery, executes just once to load all the author records together with the related book data. Instead of making round trips to the database, the two foreach loops work on the available data in the memory.Incidentally, EF Core 7 reduces some round trips to the database for free. The transaction management for single insert statements was dropped from EF Core 7 because it is no longer necessary. As a result, EF Core 7 omits two round trips that were used in previous versions of EF Core to begin and commit a transaction. The upshot is that EF Core 7 provides a significant performance gain when inserting data into a database using a single insert statement compared to predecessors."
86,[ Keep up with the latest developments in software development. Subscribe to the InfoWorld First Look newsletter ]
86,"Performance should be a featureIn this article we examined 10 key strategies you can use to improve data access performance in EF Core. Additionally, you should fine-tune your database design, indexes, queries, and stored procedures to get maximum benefits. Performance should be a feature of your application. It is imperative that you keep performance in mind from the outset whenever you are building applications that use a lot of data. Finally, every application has different data access requirements and characteristics. You should benchmark your EF Core performance before and after you apply any of the changes we discussed here to assess the results for your specific application. An excellent tool for the task is BenchmarkDotNet, which you can read about in my previous post here."
86,Next read this:
86,Why companies are leaving the cloud
86,5 easy ways to run an LLM locally
86,Coding with AI: Tips and best practices from developers
86,Meet Zig: The modern alternative to C
86,What is generative AI? Artificial intelligence that creates
86,The best open source software of 2023
86,Related:
86,Microsoft .NET
86,Development Libraries and Frameworks
86,Web Development
86,Software Development
86,"Joydip Kanjilal is a Microsoft MVP in ASP.NET, as well as a speaker and author of several books and articles. He has more than 20 years of experience in IT including more than 16 years in Microsoft .NET and related technologies."
86,Follow
86,"Copyright © 2023 IDG Communications, Inc."
86,InfoWorld
86,Follow us
86,About Us
86,Contact
86,Republication Permissions
86,Privacy Policy
86,Cookie Policy
86,Copyright Notice
86,Terms of Service
86,European Privacy Settings
86,Member Preferences
86,Advertising
86,Foundry Careers
86,Ad Choices
86,E-commerce Links
86,California: Do Not Sell My Personal Info
86,"Copyright © 2024 IDG Communications, Inc."
86,Explore the Foundry Network descend
86,CIO
86,Computerworld
86,CSO Online
86,InfoWorld
86,Network World
87,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML) | Managing site performance and scalability | Drupal Wiki guide on Drupal.org"
87,Skip to main content
87,Skip to search
87,"Can we use first and third party cookies and web beacons to understand our audience, and to tailor promotions you see?Yes, pleaseNo, do not track me"
87,Drupal.org home
87,Why Drupal?About Drupal
87,Platform overview
87,Drupal 10
87,Content Authoring
87,Content as a Service
87,Decoupled
87,Accessibility
87,Marketing Automation
87,Multilingual
87,Security
87,Personalization
87,Case studies
87,Video series
87,News
87,Use casesFor Developers
87,For Marketers
87,E-commerce
87,Education
87,FinTech
87,Government
87,Healthcare
87,High Tech
87,Nonprofit
87,Retail
87,Travel
87,ResourcesInstalling Drupal
87,Documentation
87,User guide
87,Local Development Guide
87,Security
87,News
87,Blog
87,Drupal 7 Migrations
87,ServicesFind an Agency Partner
87,Find a Migration Partner
87,Integrations & Hosting
87,Training
87,Become a Certified Partner
87,Partner Press
87,CommunityHow to Contribute
87,About the Community
87,Support
87,Community Governance
87,Jobs/Careers
87,EventsDrupalCon Portland 2024
87,DrupalCon Barcelona 2024
87,Community Events
87,DownloadDownload
87,Modules
87,Themes
87,Distributions
87,Issue queues
87,Browse Repository
87,GiveDrupal Association
87,Become a Supporter
87,Become a Certified Partner
87,Become a Member
87,Make a Donation
87,Discover Drupal
87,Drupal Swag Shop
87,DemoDemo online
87,Download
87,Return to content
87,Search form
87,Search
87,Log in
87,Create account
87,Documentation
87,Search
87,Drupal WikiDrupal 7Managing site performance and scalability
87,Support for Drupal 7 is ending on 5 January 2025—it’s time to migrate to Drupal 10! Learn about the many benefits of Drupal 10 and find migration tools in our resource center.
87,Learn more
87,Advertising sustains the DA. Ads are hidden for members. Join today
87,On this page
87,Basic settings
87,Theme optimization
87,Coding standard and proper use of already existing core API
87,Secure codes
87,DB Query optimization in codes
87,DB table optimization
87,Disable unnecessary modules
87,Remove unnecessary contents and others
87,Cache modules
87,Make changes according to Google Pagespeed and yahoo YSlow suggestions
87,MySQL Settings
87,Apache settings
87,"Also, we can check these options :"
87,1) Turn Page Caching On
87,2) Turn Views caching on
87,Managing site performance and scalability
87,Planning for Performance
87,Caching to improve performance
87,Changing PHP memory limits
87,Content Delivery Network [CDN]
87,Design for Low Bandwidth
87,Increase upload size in your php.ini
87,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
87,Optimizing MySQL
87,Randomizing MySQL Users For Exceeded max_questions Error
87,Server tuning considerations
87,Tuning php.ini for Drupal
87,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
87,Last updated on
87,26 November 2023
87,"Drupal 7 will no longer be supported after January 5, 2025. Learn more and find resources for Drupal 7 sites"
87,"This documentation needs review. See ""Help improve this page"" in the sidebar."
87,Basic settings
87,Configure cron job (for Drupal 6 http://drupal.org/project/poormanscron)
87,Make sure all cache tables are clearing properly especially cache_form
87,Enable cache options on the performance page
87,"(For Drupal 6, http://drupal.org/project/advagg )"
87,Theme optimization
87,Manually Remove blankspaces and comments from .tpl
87,No indentation in .tpl
87,Turn on CSS and JS aggregation in the performance page
87,Manually reduce css file size by removing duplicate and combine similar together
87,Move codes to functions that should be in a custom common module. Use functions for similar problems instead of coding separately. Refer core API
87,Coding standard and proper use of already existing core API
87,http://drupal.org/coding-standards
87,https://drupalize.me/videos/understanding-drupal-coding-standards?p=2012
87,Secure codes
87,http://drupal.org/writing-secure-code
87,DB Query optimization in codes
87,Join db queries whenever possible
87,"For Db update and insert, use core API"
87,Use drupal standard http://drupal.org/coding-standards
87,DB table optimization
87,http://drupal.org/project/db_maintenance
87,Disable unnecessary modules
87,Devel
87,Statistics
87,Update status
87,Use syslog instead of Database logging
87,Remove unnecessary contents and others
87,Cache modules
87,"Make use of object caches to reduce database overhead, e.g. Memcache, Redis or APC"
87,https://drupal.org/project/authcache
87,Some module may help improve
87,http://drupal.org/project/ajaxblocks (not available with Drupal 8 & 9 )
87,Make changes according to Google Pagespeed and yahoo YSlow suggestions
87,MySQL Settings
87,Cache Size say 32MB in MySQL
87,Use https://github.com/initlabopen/mysqlconfigurer for fully automated MySQL performance tuning
87,Apache settings
87,DNS lookup : OFF
87,Set FollowSymLinks everywhere and never set SymLinksIfOwnerMatch
87,Avoid content negotiation. Or use type-map files rather than Options MultiViews directive
87,"KeepAlive on, and KeepAliveTimeout very low (1 or 2 sec)"
87,Disable or comment access.log settings
87,Enable mod_deflate or mod_gzip
87,Install APC server with higher memory limit apc.shm_size = 64
87,"Also, we can check these options :"
87,1) Turn Page Caching On
87,"What page caching does is that instead of using a bunch of database queries to get the data used in making a typical web page, the rendered contents of the web page are stored in a separate database cache table so that it can be recalled quicker. If you have 10 people visiting the site from different computers, Drupal first looks into the database cache table to see if the page is there, if it is, it just gives them the page. Think of saving the output of 50 separate queries so that is accessible with a single query. You obviously are reducing the SQL queries required by a lot. What the page cache table actually stores is HTML content."
87,"Page Caching is that it only works to optimize the page load time for Anonymous users. This is because when you are logged in, you might have blocks that show up on the page that are customized for you, if it served everybody on the same page, they would see your customized information (think of a My Recent Posts block), so Drupal does not use the Page Cache for Authenticated users automatically. This allows you to turn Page Caching on and still get the benefit of Anonymous user page load times but does not break the site for Authenticated users. There are other caching options that will help with Authenticated user page performance, we will talk about those later."
87,"To enable Page Caching, you go to Configuration | Development and select the checkbox next to ""Cache pages for anonymous users""."
87,2) Turn Views caching on
87,"As mentioned when talking about Page Caching only working for anonymous users above, there are other caching options for helping with Authenticated user page performance. One of those options is to turn on caching for blocks and pages that you create using the Views module. This allows you to cache the output of the query used to generate the view, or the end HTML output of your View, and you can tune the cache for them separately. And realize too that this means you can cache portions of a page if you are using one or several Views blocks in the page, it will just cache that block in the page, not the whole page."
87,See more (Drupal7) @ https://www.lullabot.com/articles/a-beginners-guide-to-caching-data-in-d...
87,Help improve this page
87,Page status:
87,Needs review
87,You can:
87,"Log in, click Edit, and edit this page"
87,"Log in, click Discuss, update the Page status value, and suggest an improvement"
87,Log in and create a Documentation issue with your suggestion
87,"Drupal’s online documentation is © 2000-2024 by the individual contributors and can be used in accordance with the Creative Commons License, Attribution-ShareAlike 2.0. PHP code is distributed under the GNU General Public License."
87,Thank you to these Drupal contributors
87,Top Drupal contributor Acquia would like to thank their partners for their contributions to Drupal.
87,Infrastructure management for Drupal.org provided by
87,News itemsNews
87,Planet Drupal
87,Social media
87,Sign up for Drupal news
87,Security advisories
87,Jobs
87,Our communityCommunity
87,"Services, Training & Hosting"
87,Contributor guide
87,Groups & meetups
87,DrupalCon
87,Code of conduct
87,DocumentationDocumentation
87,Drupal Guide
87,Drupal User Guide
87,Developer docs
87,API.Drupal.org
87,Drupal code baseDownload & Extend
87,Drupal core
87,Modules
87,Themes
87,Distributions
87,Governance of communityAbout
87,Web accessibility
87,Drupal Association
87,About Drupal.org
87,Terms of service
87,Privacy policy
87,Drupal is a registered trademark of Dries Buytaert.
88,AEM 6.x | Performance Tuning Tips | Adobe Experience Manager
88,Documentation
88,AEM 6.x | Performance Tuning Tips
88,Last update: Mon Feb 05 2024 00:00:00 GMT+0000 (Coordinated Universal Time)
88,"Learn effective strategies and tips for optimizing Adobe Experience Manager (AEM) performance, load testing, JVM parameters and cache tuning."
88,Description description
88,Environment
88,Adobe Experience Manager 6.4
88,Adobe Experience Manager 6.5
88,Issue/Symptoms
88,"Response time is poor when authors edit content, or websites respond slowly to visitor requests."
88,These Performance Tuning tips can help accelerate queries and performance.
88,Cause
88,The following factors influence performance problems in AEM:
88,Improper design
88,Application code
88,Lack of caching
88,Bad disk I/O configuration
88,Memory sizing
88,Network bandwidth and latency
88,AEM installed on some select windows 2008 and 2012 version where memory management is an issue
88,Modifying out-of-the-box configurations as described below can help improve performance in AEM.
88,Resolution resolution
88,Preventing performance issues
88,Here are some steps you can take to ensure that you find and fix performance issues before they have an impact on your users:
88,"Implement and execute load tests that simulate realistic scenarios in both author and publish instances. Researching and defining the expected load is a crucial step in this process. This step helps you demonstrate whether the AEM application, architecture and AEM installation will perform well once it is live in a production environment.Results of this exercise help determine whether there is a misconfiguration, application issue, sizing, hardware problem, or other issue affecting the system performance. See also the performance guidelines and monitoring guidelines."
88,"In addition to load testing, stress testing helps to define the maximum load the system can handle. This test can help you prepare for traffic spikes. More information on performance testing can be found here."
88,"Install the recommended AEM service packs, cumulative fix packs and hotfixes: Adobe Experience Manager release updates."
88,"If you are using Windows server, then review this article."
88,"If you are planning to load large amounts of assets (images, videos, and so on) into AEM, then make sure you apply the Assets best practices."
88,"Provision enough RAM and avoid IO saturationIf you are intending to run production at any scale then the Linux environment should be provisioned with as much RAM as the segment tar files will grow to between offline compaction (or online compaction peaks). In addition, the following will avoid IO saturation."
88,"Separate OS, data, and logging discs"
88,Mount data discs with Noatime.
88,Set read-ahead buffers to 32 on the data disc.
88,"Ideally, use XFS over ext4 on the data discs."
88,"If RedHat is running in a VM, make certain the entropy pool is always > 1K bits (use rngtools if necessary)"
88,"Disable Transparent Huge Pages on LinuxAEM performs fine-grained reads/writes, while Linux Transparent Huge Pages are optimized for large operations, so it is recommended to disable Transparent Huge Pages when using Mongo or Tar storage."
88,Enabling transient workflowsTransient workflows can be used for any workflows that:
88,are run often.
88,do not need the workflow history.
88,They will generate a performance boost in those situations.This use case is typically met when there are high volumes of assets ingestion.Follow the procedure documented on Performance tuning Assets.
88,"Tuning Sling Job QueuesBulk upload of large assets is typically a very resource-intensive process. By default, the number of concurrent threads per job queue is equal to the number of CPU cores. As such, this value setting may cause an overall performance impact and high Java heap consumption.Adobe recommends that you do not exceed 50% of the CPU cores. To adjust this value, go to the following: https:/host:port/system/console/configMgr/org.apache.sling.event.jobs.QueueConfigurationSet queue.maxparallel to a value that represents 50% of the CPU cores of the server that hosts your AEM instance. For example, for 8 CPU cores, set the value to 4."
88,Tuning your Oak RepositoryFirst make sure that you have the latest Oak version installed for your AEM 6 instance. Check the recommended hotfixes page mentioned above.
88,Oak Query Engine/Index optimizations
88,Create custom oak indexes for all frequently used search queries.
88,For information on how to analyze slow queries see this article.
88,Create the custom indexes under the oak:index node for all search properties that you want to search with by following this article.
88,"For each custom Lucene-based index, try to set includedPaths (String) setting to restrict the index to only apply to certain content paths. Then restrict applicable searches to those paths that are included by the index."
88,"JVM parametersAdd these JVM parameters in the AEM start script to prevent expansive queries from overloading the systems. Please note, these are default values starting AEM 6.3."
88,Doak.queryLimitInMemory=500000 (see also Oak documentation)
88,Doak.queryLimitReads=100000 (see also Oak documentation)
88,"Dupdate.limit=250000 (only for DocumentNodeStore, eg. MongoMK, RDBMK)"
88,"The following option might as well improve performance, but changes the meaning of the result size call. Especially, only query restrictions that are part of the used index are considered when calculating the size.Additionally, ACLs are not applied to the results, so nodes that are not visible to the current session will still be included in the count returned. As such, the count returned can be higher than the actual number of results and the accurate count can only be determined by iterating through the results:"
88,Doak.fastQuerySize=true (see also Result Size in Oak documentation)
88,"Caution: Enabling fastQuerySize results in faster query responses. However, AEM returns inaccurate result counts for some queries. If you rely on precise result counts in your application, do not use fastQuerySize."
88,Lucene index configurationOpen /system/console/configMgr/org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService and
88,enable CopyOnRead (enabled by default since AEM 6.2)
88,enable CopyOnWrite (enabled by default since AEM 6.2)
88,enable Prefetch Index Files (enabled by default since AEM 6.2)
88,"See https://jackrabbit.apache.org/oak/docs/query/lucene.html for more information about the available parametersBecause some paths do not need to be indexed, you can do the following:In CRXDE Lite, go to /oak:index/lucene, set a multivalue string property (String)named excludedPaths with these values /var, /etc/workflow/instances, /etc/replication."
88,"Data StoreIf you use AEM Assets or have an AEM application that uses binary files extensively, Adobe recommends that you use an external datastore. Using an external datastore helps ensure maximum performance. See documentation for detailed instructions.When using a FileDataStore, tune cacheSizeInMB to a percentage of your available heap. A conservative value is 2% of the max heap. For example, for an 8 GB heap:"
88,maxCachedBinarySize=1048576
88,cacheSizeInMB=164
88,"Note that maxCachedBinarySize is set to 1 MB (1048576). As such, it only caches files that are a maximum of 1 MB. Tuning this setting to a smaller value may also make sense.When dealing with a large number of binaries, you want to maximize performance. Therefore, Adobe recommends that an external datastore be used instead of the default node stores. In addition, Adobe recommends you tune the following parameters:"
88,maxCachedBinarySize=10485760
88,cacheSizeInMB=4096
88,"Note: The cacheSizeInMB setting can cause the Java process to run out of memory if it is set too high. For example, if you have the max heap size set to 8 GB (-Xmx8g) and you expect AEM and your application to utilize a combined heap of 4 GB, then it makes sense to set cacheSizeInMB to 82 instead of 164. In the range of 2-10% of the max heap is a safe configuration. However, Adobe recommends that you validate setting changes with load testing while also monitoring memory utilization."
88,Mongo storage tuning
88,"MongoBlobStore cache size: The blobstore is used to store and read large binary objects. Internally, the store with cache is implemented which splits the binaries in relatively small blocks (data or hash code or indirect hash), so that each block fits in memory. In a default setup, the MongoBlobStore uses a fixed cache size of 16MB. For deployments where more RAM is available and blob storage is frequently accessed (for example, when Lucene index is large), increase the cache size. This cache size only applies when you use MongoBlobStore (default), not when using an external blobstore."
88,You can configure the cache size (in MB) by way of blobCacheSize setting on DocumentNodeStoreService.For example: blobCacheSize=1024
88,Please also review AEM-MongoDB checklist.
88,"Document cache size: To optimize the performance of reading nodes from MongoDB you need to tune the caches sizes of DocumentNodeStore. The default size of the cache is set to 256 MB, which is distributed among various caches used in DocumentNodeStore. See http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html#cache"
88,"You can configure the cache size (MB) by way of the cache setting on DocumentNodeStoreService. For example, cache=2048."
88,Set all of the following cache configurations in crx-quickstart/install/org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService.configand then load test with various values to see what the optimal configuration is for your environment. Note that remaining cache percent is given to the document cache:
88,cache=2048
88,nodeCachePercentage=35
88,childrenCachePercentage=20
88,diffCachePercentage=30
88,docChildrenCachePercentage=10
88,"With the above configuration, the percentages total 95%. The remaining 5% of the cache is given to documentCache. documentCache = cache - nodeCache - childrenCache - diffCache - docChildrenCache"
88,"While distributing the cache percentages, ensure that cache left for documentCache is not very large. That is, keep it to 500 MB max or less; a large documentCache can lead to an increase in the time taken to perform cache invalidation."
88,Cache settings in AEM 6.2 with Oak 1.4.x:
88,"In AEM 6.2, changes were made to the way these cache settings work. In AEM 6.2 with Oak 1.4, there is a new cache: prevDocCache. You can configure this cache using the setting prevDocCachePercentage. Default is 4."
88,The documentCache uses the remaining cache MB (cache setting minus the size of all other caches): documentCache = cache - nodeCache - childrenCache - diffCache - docChildrenCache - prevDocCache
88,"Implement the MongoDB production checklist:https://docs.mongodb.org/manual/administration/production-checklist/ - according to Mongo DB support, many of the items have a large impact on performance. For any questions, contact MongoDB Support directly."
88,Read performance: Add this query string parameter to your Mongo DB URL on each AEM node: ?readPreference=secondaryPreferredThis parameter tells the system to do reads from the secondary which gives some added read performance.
88,Increase thread pool for oak-observation: open /system/console/configMgr/org.apache.sling.commons.threads.impl.DefaultThreadPool.factory Set the name to oak-observation and set the min and max pool size to 20.
88,Increase observation queue length: Create a file named com.adobe.granite.repository.impl.SlingRepositoryManager.cfg containing parameter oak.observation.queue‐length=50000. Place it under the /crx-‐quickstart/install folder.
88,Avoid long-running queries: Set the system property in the JVM parameters: -Doak.mongo.maxQueryTimeMS=60000 to avoid queries running longer than 1 minute.
88,"Tar storage tuningMicrokernels do not call memory-mapped files directly. However, JDK internally uses memory-mapped files for efficient reading. On certain Windows 64-bit operating system could fail to clean up memory-mapped files and consume all the native OS memory. Make sure to install the performance-related patches/hotfix from microsoft (see KB 2731284) and Oracle."
88,An alternative option is to disable the memory map mode by adding tarmk.mode=32 in SegmentNodeStoreService.config until the operating system issue is resolved. The downside of disabling makes I/O intensive. Make sure to raise the I/O Page Lock Limit.
88,"TarMK revision clean (compaction)Starting AEM 6.3, the online compaction (also known as online revision cleanup) is enabled by default. See this page for more information."
88,Cloud Manager for Adobe Managed Services (AMS) customersCloud Manager (AMS customers only) allows customers to ensure successful AEM deployment with guided performance testing and autoscaling.
88,recommendation-more-help
88,3d58f420-19b5-47a0-a122-5c9dab55ec7f
89,How to Write Efficient SQL Queries
89,FeaturesDatabasesPricingBuyWhat's newSupportDownload for free
89,PERFORMANCE
89,How to work with SQL query optimization
89,Author:
89,Antonello Zanini
89,Length:
89,10 MINS
89,Type:
89,GUIDE
89,Published:
89,"2023-11-09introIn this article, you will find out what SQL query optimization is and why it is so important. You will also see some tips for building fast SQL queries and learn what SQL has to offer when it comes to analyzing a query for optimization. SQL query optimization is critical to writing more efficient SQL queries. This ensures that SQL queries are executed quickly, making applications that rely on that data faster. In this article, you will dig into the concept of SQL query optimization, understand its importance, and how to apply it. You will also learn some best practices for writing efficient SQL queries. It’s time to improve your SQL query writing skills!"
89,Tools used in the tutorial
89,Tool
89,Description
89,Link
89,DBVISUALIZER
89,TOP RATED DATABASE MANAGEMENT TOOL AND SQL CLIENT
89,DOWNLOAD
89,"What is SQL Query Optimization?SQL query optimization is the process of making SQL queries faster and more efficient. The idea behind query optimization is to identify and address performance bottlenecks. This typically involves changing the query, defining indexes, or modifying the database schema. The ultimate goal of SQL query optimization is to reduce the time and resources required to execute a query. This way, the performance of applications that rely on that query will improve accordingly.Note that the performance of a query depends on the hardware and load of the database server, but also on how you write the query. At the same time, most popular DBMS technologies come with a query optimizer. This reorders or rewrites queries behind the scene to improve performance. Specifically, the DBMS optimizes the query execution plan devised to run the query. Thus, two different queries can be translated into the same execution plan.Why Is SQL Query Optimization Important?There are at least three good reasons to optimize queries to make them more efficient. Let’s see them all.Improved performanceSQL performance tuning helps to reduce the amount of time and resources required by the database server. Since many applications may depend on the same database server, that means improving the performance of several applications.Scaling becomes easierSlow queries become even slower as the volume of data increases. In contrast, SQL query optimization allows you to ensure that queries continue to execute efficiently as the amount of data grows.Cost reductionOptimizing SQL queries can help to reduce the CPU and memory usage required to run a database server. This means saving money on server hosting.5 Tips for SQL Performance Tuning Let’s now see some tips and common mistakes to avoid for writing efficient SQL queries.1. Select the exact number of columnsOne of the most common mistakes is to use the * operator in SELECT statements, even when you are only interested in retrieving a few columns. Using * makes your queries easier to write, but also less efficient. Example of query where all columns get selected:"
89,Copy
89,SELECT * FROM users
89,"When writing the SELECT clause, use only the number of columns you need to select. This will speed up your query. Example of a query where only the columns you need are selected:"
89,Copy
89,"SELECT id, name, surname FROM users"
89,"2. Avoid useless WHERE conditions Often, SQL queries are written without thinking too much or having the underlying data structure well in mind. For example, let’s say you want to retrieve all users with a value in the points field. You will write the following query:"
89,Copy
89,"SELECT id, name, surname"
89,FROM users
89,WHERE points IS NOT NULL
89,"This works like a charm and would return what you expect. However, if you had inspected the users table before writing the query, you would have noticed that points is a non-nullable integer with 0 as the default value. In other terms, points is always NOT NULL. So, the WHERE condition you added will only make your query unnecessarily slower.This is just an example. Yet, before writing WHERE conditions, you should always check that what you are adding is actually useful for filtering the data.3. Avoid Negative SearchesWhen it comes to query optimization, you have to take into account even the small details. For example, using the NOT operator in a WHERE clause can make a query slower. This is because the DBMS may have to check all the rows in the table and exclude the ones that do not match the condition, which takes more time and resources than just selecting the rows that match the condition. So, positive queries are typically more efficient than equivalent negative queries. Example of a negative query:"
89,Copy
89,"SELECT id, name, surname"
89,FROM users
89,"WHERE NOT name = ""Jhon"""
89,Example of the equivalent query in positive form:
89,Copy
89,"SELECT id, name, surname"
89,FROM users
89,"WHERE name != ""Jhon"""
89,"Not considering the low-level optimizations made by most DBMS, the second query is faster than the first one.4. Use temporary tablesYou can use temporary tables, also known as temp tables, for SQL query optimization. Specifically, temp tables are useful to store and manipulate intermediate results within a query. This can help improve performance by reducing the amount of data to be processed. Also, temp tables are automatically created when running ALTER TABLE queries on large tables.For example, you can use a temp table to store the results of a JOIN operation. Then, use that table in the following queries to filter some data. This can be more efficient than performing the JOIN and filtering the data in a single query. Also, you can add indexes to a temp table to improve performance.5. Avoid the DISTINCT keywordIn SQL, the DISTINCT keyword forces the DBMS to return unique values from a query. When used in a SELECT statement, DISTINCT removes duplicate rows from the result set. This requires an extra operation, which makes your queries slower. So, if you do not really need to use the DISTINCT keyword, try to avoid it.Example of a query with DISTINCT:"
89,Copy
89,"SELECT DISTINCT name, surname"
89,FROM users
89,WHERE country = 'USA';
89,This query selects all unique first and last name combinations of users living in the United States.Let’s rewrite an equivalent query without DISTINCT:
89,Copy
89,"SELECT name, surname"
89,FROM users
89,WHERE country = 'USA';
89,"GROUP BY name, surname;"
89,"Both of these queries will return the same result set. In detail, the first query uses DISTINCT to avoid duplicates, whereas the second query uses GROUP BY to accomplish the same goal. How EXPLAIN works in SQLIn SQL, the EXPLAIN command shows you the execution plan of a query. In particular, EXPLAIN provides information about how the query optimizer will execute the query. This includes:The order in which the tables will be accessed.Whether and which indexes will be used.Information about the computational cost of each operation that will be executed.Note that this info changes based on the DBMS specific implementation of EXPLAIN. You can run the EXPLAIN command as follows:"
89,Copy
89,EXPLAIN
89,<YOUR_SQL_QUERY>
89,"Replace <YOUR_SQL_QUERY> with the SQL code of your query, as in the example below:"
89,Copy
89,EXPLAIN
89,"SELECT ""name"", ""matchId"""
89,"FROM ""Events"""
89,"WHERE ""goldenPeriodId"" IS NOT NULL"
89,"GROUP BY ""name"", ""matchId"""
89,"When running an EXPLAIN query, this returns a set of rows, each of which represents a step in the execution plan."
89,"Note the rows returned by the EXPLAIN query run in DbVisualizer.Keep in mind that EXPLAIN does not run the query. For this reason, If you want to get some info on the query execution time, you need to use EXPLAIN ANALYZE."
89,"Note the time info returned by the EXPLAIN ANALYZE query run in DbVisualizer.As you can see, EXPLAIN ANALYZE also returns info about the time spent planning and the time required to run the query. Note that only PostgreSQL supports EXECUTE ANALYZE. However, other DBMSs may offer the same feature under different commands.EXPLAIN can be especially useful when you are trying to optimize the performance of a query. By examining the execution plan, you can identify any steps that may be causing poor performance, such as full table scans or inefficient JOIN operations. At the same time, analyzing and understanding the information returned by EXPLAIN is not easy. This is where an advanced SQL client such as DbVisualizer comes into play!Explain Plan in DbVisualizerDbVisualizer comes with an Explain Plan feature, which allows you to visually analyze how a query is processed by the database. Specifically, the Explain Plan executes your query and records the execution plan that the database devised to perform it. By looking at the plan, you can find out if the database is using the right indexes and joining your tables in the most efficient way. This allows you to understand what the DBMS does behind the scene and helps you optimize your queries accordingly. Note that the Explain Plan is available for both single queries and SQL scripts.To analyze a query with the Explain Plan in DbVisualizer:Write the query in the “SQL Commander” editor.Click “Execute Explain Plan” button in the toolbar.Examine the result."
89,Running a query with the Explain Plan in DbVisualizer.Let’s now learn how to use DbVisualizer’s Explain Plan in a complete example.Step #1: Running the raw queryLet’s assume you want to run the following query in a PostgreSQL database:
89,Copy
89,"SELECT * FROM ""Events"" E"
89,"WHERE E.""matchId"" IS NOT NULL"
89,"ORDER BY ""matchId"" ASC, ""timePosition"" ASC"
89,"Events is a table with nearly 1 million records containing event data related to Judo matches.As you can see, this query does not follow any SQL performance tuning tips presented earlier. Let’s run it in the Explain Plan."
89,"Exploring the results of the Explain Plan in DbVisualizerIn the “Graph View” section, you can see the execution plan devised by the DBMS as a chart. This helps you visually understand how your query will be executed. While in the “Tree View” part, you can see all the information in a more compact form. In detail, note that the Total Cost is 223563.23  If you are not familiar with this concept, Total Cost refers to the overall computational cost required to execute a query. This involves the amount of memory used, the number of CPU cycles required, and/or the number of disk I/O operations performed. The lower this value is, the more efficient a query is.Step #2: Applying the optimization tipsFirst, let’s change the query to select only the required columns:"
89,Copy
89,"SELECT E.""id"", E.""matchId"", E.""timePosition"", E.""name"""
89,"FROM ""Events"" E"
89,"WHERE E.""matchId"" IS NOT NULL"
89,"ORDER BY ""matchId"" ASC, ""timePosition"" ASC"
89,"This significantly reduces Total Cost to 136423.13.Also, matchId is defined as a non-nullable field. So, let’s remove the WHERE condition:"
89,Copy
89,"SELECT E.""id"", E.""matchId"", E.""timePosition"", E.""name"""
89,"FROM ""Events"" E"
89,"ORDER BY ""matchId"" ASC, ""timePosition"" ASC"
89,"Total Cost falls to 136435.12.Step #3: Adding an indexBy analyzing the result provided by the Explain Plan, we can notice that no index is used. So, let’s create an index with the following query:"
89,Copy
89,CREATE INDEX idx_events_complete
89,"ON ""Events""(""matchId"" ASC, ""timePosition"" ASC);"
89,"Running the index query in DbVisualizer.Now, it is time to execute the query again in the Explain Plan:"
89,Copy
89,"SELECT E.""id"", E.""matchId"", E.""timePosition"", E.""name"""
89,"FROM ""Events"" E"
89,"ORDER BY ""matchId"" ASC, ""timePosition"" ASC"
89,"The results of the optimization process.As you can see, the execution plan now involves only a single step using the index defined above. Total Cost is now 66522.0 and has more than halved from the previous step.By applying the query optimization tips presented above and using the Explain Plan feature of DbVisualizer, we have been able to take a query from a Total Cost of 223563.23 to 66522.0. This is a threefold improvement!Et voilà! SQL query optimization has never been easier!ConclusionAs you learned here, SQL query optimization is a crucial aspect to make queries faster. By understanding the principles of query optimization and applying best practices, you can write efficient SQL queries. In detail, here you saw five useful tips for SQL performance tuning. Also, you understood what the EXPLAIN command is and how it can help you analyze the execution plan devised by the DBMS to run a query. The EXPLAIN command is powerful but sometimes difficult to use and understand. Fortunately, DbVisualizer comes with the Explain Plan feature! This allows you to visually explore the steps taken by the DBMS to execute the query, the indexes used, and where to intervene to improve the performance. This is just one of the many features offered by DbViualizer that can help you make a query more efficient. Try DbVisualizer for free today!"
89,About the author
89,Antonello Zanini
89,"Antonello is a software engineer, and often refers to himself as a technology bishop. His mission is to spread knowledge through writing."
89,Sign up to receive The Table's roundup
89,Submit ->
89,"By submitting this form, I agree to the DbVis Privacy Policy"
89,Submit ->
89,More from the table
89,Title
89,Author
89,Tags
89,Length
89,Published
89,title
89,Why Are Your Databases Slow with Large Tables? An Overview
89,author
89,Lukas Vileikis
89,tags
89,PERFORMANCE
89,9 MINS
89,2023-04-27
89,title
89,A Guide to Multithreading in SQL
89,author
89,Ochuko Onojakpor
89,tags
89,PERFORMANCE
89,8 MINS
89,2023-04-14
89,title
89,10x Query Performance with a Database Index
89,author
89,Lukas Vileikis
89,tags
89,INDEXES
89,PERFORMANCE
89,6 MINS
89,2023-04-04
89,title
89,Deadlocks in Databases: A Guide
89,author
89,Lukas Vileikis
89,tags
89,PERFORMANCE
89,6 MINS
89,2023-03-28
89,title
89,Postgres TEXT vs VARCHAR: Comparing String Data Types
89,author
89,Antonello Zanini
89,tags
89,POSTGRESQL
89,TEXT
89,VARCHAR
89,6 min
89,2024-03-14
89,title
89,ALTER TABLE ADD COLUMN in SQL: A Comprehensive Guide
89,author
89,TheTable
89,tags
89,SQL
89,5 min
89,2024-03-12
89,title
89,Schemas in PostgreSQL
89,author
89,Leslie S. Gyamfi
89,tags
89,POSTGRESQL
89,SCHEMA
89,6 min
89,2024-03-11
89,title
89,Database Schema Design: A Comprehensive Guide for Beginners
89,author
89,Ochuko Onojakpor
89,tags
89,SCHEMA
89,13 min
89,2024-03-07
89,title
89,SQL Comment: A Comprehensive Guide
89,author
89,TheTable
89,tags
89,5 min
89,2024-03-06
89,title
89,Introduction to Database Migration: A Beginner's Guide
89,author
89,Ochuko Onojakpor
89,tags
89,MIGRATION
89,11 min
89,2024-03-04
89,"Read more from theTableThe content provided on dbvis.com/thetable, including but not limited to code and examples, is intended for educational and informational purposes only. We do not make any warranties or representations of any kind. Read more here.ENGINEERED IN NACKA, SWEDEN.Copyright 2024.ProductDownloadFeature listDatabase featuresSupportReleasesPartnersResellersPartnersLegalLegal termsPrivacy PolicyCookie PolicySecure SoftwareEULACompanyTheTable BlogOur storyWork with usContact usBrand assetsSocialTwitterFacebookLinkedInENGINEERED IN NACKA, SWEDEN.Copyright 2024."
89,Close ✗
89,Cookie policyWe use cookies to ensure that we give you the best experience on our website. However you can change your cookie settings at any time in your browser settings. Please find our cookie policy here ↗
89,Cookies are fine
90,"Explaining The Postgres MemeAvestura's Personal WebsiteBlogProjectsOpen SourcePresentationsDriveAboutMorePublished onMonday, July 10, 2023Explaining The Postgres MemeAuthorsNameAryan EbrahimpourGitHub@avestura  78 min readTable of ContentI spend a significant amount of my time online, and on a regular day,"
90,"I am either learning about STEM topics, indulging in memes, or both. On one such day,"
90,I came across a meme that truly caught my attention. It sparked numerous questions above my
90,"head, leading to a moment of deafening silence within me:"
90,I already knew that data
90,"storage and retrieval ain't ever been one of my strong suits, but after seeing this meme it kind of"
90,made me unsecure as I had basically zero effing clue about a huge portion of it.
90,"I felt the urge that I have to know what this is all about, so I have decided to learn from"
90,multiple sources.
90,"One of the best ways to learn something is to explain it, and this blog post"
90,"aims to do exactly that. Let's review and explain every part of this meme,"
90,while unraveling its meaning and secrets.
90,CREDITSShout out to Jordan Lewis (and friends) for
90,creating this meme. This was initially published in a tweet on twitter
90,"and then went viral on other social platforms. I've personally seen it on a Telegram group,"
90,and didn't know about the origin of it while I was writing this blog post until I've
90,finished writing about the half of it.
90,"ATTENTIONThe meme is called ""The SQL Iceberg"" and it is a general SQL meme, not a"
90,"PostgreSQL one. However, as we want to analyze it while wearing our PostgreSQL hat, I think"
90,"it is safe to title this post ""Explaining The Postgres Meme"", because it is hard"
90,to target all or even major database management systems in one blog post for such a
90,highly detailed photo.
90,"The creators of this meme happen to be the developers of CockroachDB, which is a"
90,"highly compatible database with PostgreSQL,"
90,"so we are probably not much far from what they had in mind when creating this meme.EDIT: Midway through writing this blog post, I discovered the origin of this meme"
90,and watched the creator's explanations. It seems that the content of this post aligns
90,closely with the creator's intentions.
90,Levels
90,Let's name each level in the meme:
90,"Level 0: Sky Zone: CREATE TABLE, JOIN, NULL, ..."
90,"Level 1: Surface Zone: ACID, outer joins, normal forms, ..."
90,"Level 2: Sunlight Zone: Connection pools, LATERAL Join, Stored Procedures, ..."
90,"Level 3: Twilight Zone: Isolation levels, ZigZag Join, Triggers, ..."
90,"Level 4: Midnight Zone: Denormalization, SELECT FOR UPDATE, star schemas, ..."
90,"Level 5: Abyssal Zone: MATCH PARTIAL foreign keys, 'null'::jsonb IS NULL = false, ..."
90,"Level 6: Hadal Zone: volcano model, join ordering is NP Hard, ..."
90,"Level 7: Pitch Black Zone: NULL, the halloween problem, fsyncgate, ..."
90,Level 0: Sky Zone
90,Welcome to Sky Zone! These are the very high level concepts which everyone seem to have
90,encountered while working with Relational Database Management Systems like PostgreSQL.
90,"Without any further ado, let's get into the topics on the sky level."
90,Data Types
90,PostgreSQL supports a large number of different data types varying from
90,"numeric, monetary, arrays, json, and xml to things like geometric, network address,"
90,and composite types. Here is a long list of supported data types is PostgreSQL.
90,This query shows the types that are interesting to an application developer.
90,It results 87 different data types on PostgreSQL version 14.1:
90,"sqlselect typname, typlen, nspname"
90,from pg_type t
90,join pg_namespace n
90,on t.typnamespace = n.oid
90,where nspname = 'pg_catalog'
90,and typname !~ '(^_|^pg_|^reg|_handlers$)'
90,"order by nspname, typname;"
90,"As an example, if you want to store the audit logs of the actions done by admin users and"
90,"need to store their IPs, you can use the inet type in PostgreSQL instead of storing it as text."
90,"This will help you to store those data more efficiently, and validate them more easily, compared"
90,to a system that doesn't support such a type (e.g. Sqlite).
90,CREATE TABLE
90,"SQL (Structured Query Language) is composed of several areas, and each of them has a specific"
90,sub-language.
90,One of these sub-languages is called DDL which stands for data definition language. It consists of
90,"statements like CREATE, ALTER, and DROP, which are used to defined on-disk data structures."
90,Here is an example of a create table query:
90,"sqlcreate table ""audit_log"" ("
90,"id serial primary key,"
90,"ip inet,"
90,"action text,"
90,"actor text,"
90,"description text,"
90,created_at timestamp default NOW()
90,"This will create an audit_log table with columns such as id, ip, action, etc."
90,"SELECT, INSERT, UPDATE, DELETE"
90,DML is another one of SQL sub-languages and stands for data manipulation language. It
90,"covers the insert, update, and delete statements which are used to feed data into the"
90,database system.
90,select also helps us to retrieve data from the database. This is probably one of the simplest
90,select queries in SQL:
90,sqlselect 0;
90,Here are some of the examples of such DML queries:
90,"insertupdatedeleteselectsqlinsert into ""audit_log"" (ip, action, actor, description) values ("
90,"'127.0.0.1',"
90,"'delete user',"
90,"'admin',"
90,'admin deleted the user x'
90,The table table_name command can also be used to select an entire table. This sql command:
90,sqltable users;
90,is equivalent to
90,sqlselect * from users;
90,ORDER BY
90,"SQL does not guarantee any kind of ordering of the result set of any query, unless you specify an"
90,order by clause.
90,simple order byk-nearest-neighbor orderingsqlselect *
90,"from ""audit_log"""
90,order by created_at desc;
90,LIMIT and OFFSET
90,LIMIT and OFFSET allow you to retrieve just a portion of the rows that are generated
90,by the rest of the query. The below query returns audit logs number 100 to 109:
90,sqlselect *
90,"from ""audit_log"""
90,offset 100
90,limit 10;
90,"Beware: This method for pagination might be slow!In many cases, using offset will slow down the performance of your query as the database"
90,"must count all rows from the beginning until it reaches the requested page. For more information,"
90,read the Keyset pagination section.
90,GROUP BY
90,"The group by clause introduces Aggregates (aka Map/Reduce) in PostgreSQL, which enables us to"
90,map our rows into different groups and then reduce the result set into a single value.
90,"Assuming we have a Student table definition with id, class_no and grade columns, we can"
90,find the average grade of each class using this query:
90,"aggregate using group by clausetable definitionsqlselect class_no, avg(grade) as class_avg"
90,from student
90,group by class_no;
90,Note that the Student table defined this way for demonstration purposes only.
90,NULL
90,"In PostgreSQL, NULL means undefined value, or simply not knowing the value, rather than the absence of a value."
90,"That is why true = NULL, false = NULL, and NULL = NULL checks all result in a NULL."
90,sqlselect
90,"true = NULL as a,"
90,"false = NULL as b,"
90,NULL = NULL as c;
90,-- result
90,-- a = NULL
90,-- b = NULL
90,-- c = NULL
90,"Now that you know the meaning of NULL, you should be more careful with its semantics. The following"
90,query returns no rows:
90,sqlselect x
90,"from generate_series(1, 100) as t(x) -- `generate_series(1, 100)` creates rows 1,2,3,...,99,100"
90,"where x not in (1, 2, 3, null)"
90,-- total rows: 0
90,Indexes
90,"When used correctly, Indexes in PostgreSQL allow you to access your data much faster"
90,because they prevent the need for a sequential scan when an index is present.
90,"Additionally, certain constraints like PRIMARY KEY and UNIQUE are only possible"
90,using a backing index.
90,Here is a simple query to create an index on last_name column of student table
90,using GiST method.
90,sqlcreate index on student using gist(last_name);
90,"An index cannot alter the result of a query. It duplicates data to optimize searches,"
90,hence why each index adds write costs to your DML queries.
90,"Therefore, it is not a good idea to put index on everything even if you have infinite storage."
90,You will still need to pay the maintenance cost of indexes.
90,Click for more details: Why do Primary Key and UNIQUE constraint need backing indexes?
90,Click for more details: What is GiST?
90,JOIN
90,"Queries can access multiple tables at once, or access the same table in such a way that multiple"
90,rows of the table are being processed at the same time. Queries that access multiple tables
90,(or multiple instances of the same table) at one time are called join queries.
90,We can also see joins as a way to craft new Relations from a pair of existing ones. A
90,relation in PostgreSQL is a set of data having a common set of properties.
90,The simple query below retrieves the admin user with its role name:
90,"sqlselect u.username, u.email, r.role_name"
90,"from ""user"" as u"
90,"join ""role"" as r"
90,on u.role_id = r.role_id -- equivalent: using(role_id)
90,where u.username = 'admin';
90,"There are multiple kinds of joins, including but not limited to:"
90,Inner Joins: Only keep the rows that satisfy the join condition for both side of involved relations (left and right).
90,"Left/Right/Full Outer Joins: Retrieve all records from table even for those with no matching value in either left, right, or both side of the relations."
90,"Cross Join: A cartesian product of left and right relations, giving all the possible combinations from the left table rows"
90,joined with the right table rows.
90,There are also some other types of joins which we will discuss in deeper levels.
90,Click for more details: What is a natural join?
90,Foreign Keys
90,Foreign key constraints help you to maintain the referential integrity of your data.
90,"Assuming you have Author and Book tables, you can reference Author from the Book"
90,"table, and PostgreSQL will make sure that the referencing author exists in the Author table"
90,when inserting a row into the Book table:
90,sqlcreate table author (
90,name text primary key
90,create table book (
90,"name text primary key,"
90,author text references author(name)
90,insert into author values ('George Orwell');
90,"insert into book values ('Animal Farm', 'George Orwell'); -- OK"
90,"insert into book values ('Anna Karenina', 'Leo Tolstoy'); -- NOT OK"
90,-- ERROR:
90,"insert or update on table ""book"" violates foreign key constraint ""book_author_fkey"""
90,-- DETAIL:
90,"Key (author)=(Leo Tolstoy) is not present in table ""author""."
90,PostgreSQL enforces the presence of either a unique or primary key constraint on the
90,target column of the target table.
90,ORMs
90,"Object-relational Mapping (ORM, O/RM, and also known as O/R Mapping tool) is a technique for"
90,mapping data to and from relational databases and an object-oriented programming
90,language. ORMs help programmer to interact and alter the data within the database
90,using the language constructs defined in an object-oriented programming language.
90,"In other words, ORM acts as a bridge between the object-oriented world, and the mathematical"
90,relational world.
90,"JavaC#Pythonjava// Java, Hibernate ORM"
90,@Entity
90,"@Table(name = ""Person"")"
90,public class Person {
90,@Id
90,@GeneratedValue(strategy = GenerationType.IDENTITY)
90,private Long id;
90,private String name;
90,private int age;
90,"Configuration configuration = new Configuration().configure(""hibernate.cfg.xml"");"
90,SessionFactory sessionFactory = configuration.buildSessionFactory();
90,try (Session session = sessionFactory.openSession()) {
90,"List<Person> persons = session.createQuery(""FROM Person"", Person.class).list();"
90,} catch (Exception e) {
90,e.printStackTrace();
90,Level 1: Surface Zone
90,"Welcome to Surface Zone! Now that we have got past the sky level, we can get familiar with"
90,some of the more advanced features and concepts and dive deeper into the fundamental
90,components and functionalities of PostgreSQL .
90,These topics will give you a solid grounding and understanding of the database system.
90,Transactions
90,"A transaction turns a bundle of steps/actions into a single ""all or nothing"" operation."
90,The intermediate steps are not visible to other concurrently running transactions.
90,"Generally speaking, a transaction represents any change in a database."
90,"In PostgreSQL, a transaction is surrounded by BEGIN and COMMIT commands."
90,PostgreSQL treats every SQL statement as being executed within a transaction.
90,"If you do not issue a BEGIN command, then each individual statement has an implicit BEGIN"
90,and (if successful) COMMIT wrapped around it. The below example
90,shows transferring of a coin from Player1 to Player2 in the database of a video game server
90,(the example is oversimplified):
90,sqlBEGIN;
90,"update accounts set coins = coins - 1 where name = ""Player1"";"
90,"update accounts set coins = coins + 1 where name = ""Player2"";"
90,COMMIT;
90,"Here we want to make sure that either all the updates are applied to database, or none of"
90,"them happen. We do not want a system failure decrease coins from Player1, but no coin is added"
90,to Player2's inventory. Grouping a set of operations into a transaction gives us such
90,guarantee.
90,ACID
90,"ACID is an acronym for Atomicity, Consistency, Isolation, and Durability. These are"
90,a set of properties of database transactions intended to guarantee data
90,"validity despite errors, power failures, and other mishaps."
90,A database transaction should be ACID by definition:
90,"Atomicity: A transaction must either be complete in its entirety, or have no effect."
90,Atomicity
90,"guarantees that each transaction is treated as a single ""unit""."
90,Consistency: Ensures that a transaction can only bring the database
90,"from one consistent state to another, and prevent database corruption by an illegal transaction."
90,"As an example, a transaction should not allow a NOT NULL column to have a NULL value after a COMMIT."
90,Isolation: Transactions are often executed concurrently (multiple reads and writes at a time).
90,"As we have stated in the previous section, the intermediate steps are not visible to other"
90,"concurrently running transactions, which means a concurrently executed transaction shouldn't"
90,have a different result compared to when the transactions were executed sequentially.
90,Durability: The database management system is not allowed to miss any committed transaction
90,after a restart or any kind of crash. All the committed transactions should be written on
90,non-volatile memory.
90,Query plans and EXPLAIN
90,Every database system needs a planner to create a query plan out of your SQL queries.
90,"A good query planner is critical for good performance. In PostgreSQL, the EXPLAIN command is"
90,used to know what query plan is created for the input query.
90,"sqlexplain select ""name"", ""author"" from ""book"";"
90,-- output:
90,-- Seq Scan on book
90,(cost=0.00..18.80 rows=880 width=64)
90,For a more complex query like this:
90,sqlselect
90,"w.temp_lo,"
90,"w.temp_hi,"
90,"w.city,"
90,c.location as city_location
90,from weather as w
90,join city as c
90,on c.name = w.city;
90,We get more information on things like how is PostgreSQL trying to find the record
90,"(using a sequential scan or hash, etc), costs, timing, and performance information."
90,Below information is obtained using EXPLAIN ANALYZE command. Using ANALYSE option alongside
90,EXPLAIN shows the exact row counts and true run time along with estimates provided by the
90,EXPLAIN:
90,json[
90,"""Plan"": {"
90,"""Node Type"": ""Hash Join"","
90,"""Parallel Aware"": false,"
90,"""Async Capable"": false,"
90,"""Join Type"": ""Inner"","
90,"""Startup Cost"": 18.10,"
90,"""Total Cost"": 55.28,"
90,"""Plan Rows"": 648,"
90,"""Plan Width"": 202,"
90,"""Actual Startup Time"": 0.024,"
90,"""Actual Total Time"": 0.027,"
90,"""Actual Rows"": 3,"
90,"""Actual Loops"": 1,"
90,"""Inner Unique"": false,"
90,"""Hash Cond"": ""((w.city)::text = (c.name)::text)"","
90,"""Plans"": ["
90,"""Node Type"": ""Seq Scan"","
90,"""Parent Relationship"": ""Outer"","
90,"""Parallel Aware"": false,"
90,"""Async Capable"": false,"
90,"""Relation Name"": ""weather"","
90,"""Alias"": ""w"","
90,"""Startup Cost"": 0.00,"
90,"""Total Cost"": 13.60,"
90,"""Plan Rows"": 360,"
90,"""Plan Width"": 186,"
90,"""Actual Startup Time"": 0.010,"
90,"""Actual Total Time"": 0.010,"
90,"""Actual Rows"": 3,"
90,"""Actual Loops"": 1"
90,"""Node Type"": ""Hash"","
90,"""Parent Relationship"": ""Inner"","
90,"""Parallel Aware"": false,"
90,"""Async Capable"": false,"
90,"""Startup Cost"": 13.60,"
90,"""Total Cost"": 13.60,"
90,"""Plan Rows"": 360,"
90,"""Plan Width"": 194,"
90,"""Actual Startup Time"": 0.008,"
90,"""Actual Total Time"": 0.008,"
90,"""Actual Rows"": 3,"
90,"""Actual Loops"": 1,"
90,"""Hash Buckets"": 1024,"
90,"""Original Hash Buckets"": 1024,"
90,"""Hash Batches"": 1,"
90,"""Original Hash Batches"": 1,"
90,"""Peak Memory Usage"": 9,"
90,"""Plans"": ["
90,"""Node Type"": ""Seq Scan"","
90,"""Parent Relationship"": ""Outer"","
90,"""Parallel Aware"": false,"
90,"""Async Capable"": false,"
90,"""Relation Name"": ""city"","
90,"""Alias"": ""c"","
90,"""Startup Cost"": 0.00,"
90,"""Total Cost"": 13.60,"
90,"""Plan Rows"": 360,"
90,"""Plan Width"": 194,"
90,"""Actual Startup Time"": 0.004,"
90,"""Actual Total Time"": 0.005,"
90,"""Actual Rows"": 3,"
90,"""Actual Loops"": 1"
90,"""Triggers"": ["
90,Click to view the explain table
90,"If you have pgAdmin installed, it can show you a graphical output as well:"
90,Inverted Indexes
90,"An inverted index is an index structure storing a set of (key, posting list) pairs, where"
90,posting list is a set of row IDs in which the key occurs.
90,"Inverted indexes are used when we want to index composite values (called an item), where each"
90,"element value in the item is a key. As an example, a document is an item, and the word we're"
90,searching for inside the document is the key.
90,"PostgreSQL supports GIN, which stands for Generalized Inverted Index."
90,GIN is generalized in the sense that the GIN access method code does not need to know
90,the specific operations that it accelerates.
90,Keyset Pagination
90,There are many ways in which one can implement pagination to read only a portion of
90,"the rows from the database. As we have suggested in the OFFSET/LIMIT section,"
90,in many cases using offset will slow down the performance of your query as the database
90,must count all rows from the beginning until it reaches the requested page. One way to
90,overcome this is to use the Keyset pagination:
90,sqlselect *
90,"from ""audit_log"""
90,where created_at < ?
90,order by created_at desc
90,limit 10; -- equivalent standard SQL: fetch first 10 rows only
90,"Here instead of skipping records, we simply use keyset_column > x where x is the last record"
90,from the previous page we have fetched.
90,Read more: Paging Through Results by Markus Winand
90,Computed Columns
90,A computed or a generated column in a table is a column which its value is a function of
90,"other column in the same row. In other words, a computed column for columns is what a view"
90,is for tables. The value of a
90,"computed column can be read, but it can not be directly written."
90,A computed/generated column is defined using GENERATED ALWAYS AS in PostgreSQL:
90,sqlcreate table people (
90,"...,"
90,"height_cm numeric,"
90,"height_in numeric GENERATED ALWAYS AS (height_cm / 2.54) -- this won't work, will explain why in the next section"
90,Stored Columns
90,A generated column can either be stored or virtual:
90,Stored: computed when it is written (inserted or updated) and occupies storage as if it were a normal column
90,Virtual: occupies no storage and is computed when it is read
90,"Thus, a virtual generated column is similar to a view and a stored generated"
90,column is similar to a materialized view (except that it is always updated automatically).
90,If you have tried the previous query you might have faced an error. This is because at
90,"the time of writing this post PostgreSQL only implements stored generated columns, therefore"
90,you need to mark the column using STORED:
90,sqlcreate table people (
90,"...,"
90,"height_cm numeric,"
90,height_in numeric GENERATED ALWAYS AS (height_cm / 2.54) STORED -- works fine
90,ORDER BY Aggregates
90,An aggregate function computes a single result from a set of input values. Some of the most
90,"famous aggregate functions are min, max, sum, and avg which are used to calculate"
90,"minimum, maximum, sum, and the average of a set of results. The query below calculates"
90,the average of high and low temperatures from the weather records in the weather table:
90,sqlselect
90,"avg(temp_lo) as temp_lo_average,"
90,avg(temp_hi) as temp_hi_average
90,from weather;
90,The input of some aggregate functions are introduced by ORDER BY.
90,These functions are sometimes referred to as “inverse distribution” functions. As an
90,"example, the below query shows the median rank of all players for each game server:"
90,ORDER BY aggregatetable definitionssqlSELECT
90,"""server"","
90,percentile_cont(0.5) WITHIN GROUP (ORDER BY rank DESC) AS median_rank
90,"FROM ""player"""
90,"GROUP BY ""server"""
90,-- server
90,median_rank
90,-- asia
90,2.5
90,-- europe
90,Window Functions
90,Window Functions are very powerful tools that let you process several values of the result set
90,"at a time. This might be similar to what we can achieve with aggregate functions,"
90,"however, window functions do not cause rows to become grouped into a single output"
90,row like non-window aggregate calls would.
90,A window function call always contains an OVER clause directly following the
90,window function's name and argument(s):
90,sqlselect
90,"city,"
90,"temp_lo,"
90,"temp_hi,"
90,"avg(temp_lo) over (partition by city) as temp_lo_average,"
90,rank() over (partition by city order by temp_hi) as temp_hi_rank
90,from weather
90,-- city
90,temp_lo
90,temp_hi
90,temp_lo_average
90,temp_hi_rank
90,-- Lahijan
90,10.33333333
90,-- Lahijan
90,10.33333333
90,-- Lahijan
90,10.33333333
90,-- Rasht
90,-- Rasht
90,-- Rasht
90,"When using Window Functions, understanding the concept of window frame is necessary."
90,"For each row, there is a set of rows within its partition called its window frame."
90,"Some window functions act only on the rows of the window frame, rather than of the whole partition."
90,"In general, these are the window frame rules of thumb:"
90,If ORDER BY is supplied then the frame consists of all rows from the start of the partition up through the current row (plus any following rows that are equal to the current row according to the ORDER BY clause)
90,When ORDER BY is omitted the default frame consists of all rows in the partition
90,"OVEROVER (ORDER BY salary)sqlselect salary, sum(salary) over () from empsalary;"
90,salary |
90,sum
90,-- --------+-------
90,5200 | 47100
90,5000 | 47100
90,3500 | 47100
90,4800 | 47100
90,3900 | 47100
90,4200 | 47100
90,4500 | 47100
90,4800 | 47100
90,6000 | 47100
90,5200 | 47100
90,-- (10 rows)
90,Click for more details: More frame specifications!
90,Outer Joins
90,"As we have mentioned earlier, an outer join retrieve all records from table even for"
90,"those with no matching value in either left, right, or both side of the relations."
90,sqlselect *
90,from weather left outer join cities ON weather.city = cities.name;
90,city
90,| temp_lo | temp_hi | prcp |
90,date
90,name
90,| location
90,-- ---------------+---------+---------+------+------------+---------------+-----------
90,Hayward
90,37 |
90,54 |
90,| 1994-11-29 |
90,San Francisco |
90,46 |
90,"50 | 0.25 | 1994-11-27 | San Francisco | (-194,53)"
90,San Francisco |
90,43 |
90,57 |
90,"0 | 1994-11-29 | San Francisco | (-194,53)"
90,-- (3 rows)
90,Above query shows a left outer join because the table mentioned on the left
90,"of the join operator (weather) will have each of its rows at least once in the output,"
90,whereas the table on the right (cities) will only have those rows that match a row on
90,the left table.
90,"In PostgreSQL, left outer join, right outer join, and full outer join are used to do outer joins."
90,CTEs
90,"WITH queries, or Common Table Expressions (CTEs) can be thought of as defining"
90,"temporary tables that exist just for one query. Using a WITH clause, we can define an"
90,auxiliary statement which can be attached to a primary statement.
90,"In the query below, we first define two auxiliary tables called hottest_weather_of_city, and"
90,"not_so_hot_cities, and then we use them in the primary select query:"
90,sqlwith hottest_weather_of_city as (
90,"select city, max(temp_hi) as max_temp_hi"
90,from weather
90,group by city
90,"), not_so_hot_cities as ("
90,select city
90,from hottest_weather_of_city
90,where max_temp_hi < 35
90,select * from weather
90,where city in (select city from not_so_hot_cities)
90,"In short, Common Table Expressions is just another name for WITH clauses."
90,Normal Forms
90,Database normalization is the process of structuring a relational database in accordance
90,with a series of so-called normal forms to reduce data redundancy and improve data integrity.
90,"There are several levels of normalization, and a higher level of database normalization"
90,cannot be achieved unless the previous levels have been satisfied.
90,Here are some of the normal forms:
90,"1NF: Columns cannot contain relations or composite values (each cell is single-values),"
90,and there are no duplicated rows in the table
90,2NF: Non-key columns are dependent on all of the key (it should not be dependent on a part of the composite key).
90,"In other words, there are no partial dependencies."
90,3NF: Table has no transitive dependencies.
90,"Other normals forms like EKNF, BCNF, 4NF, 5NF, DKNF, and 6NF are not covered in this blog post."
90,You can read more about them at the Wikipedia page of normal forms.
90,Level 2: Sunlight Zone
90,"Welcome to Sunlight Zone! As we descend further, we'll explore more advanced"
90,features and techniques in PostgreSQL. Get ready to bask in the glow of knowledge
90,and expand your database skills.
90,Connection Pools
90,"Connecting to a database server consists of several time-consuming steps (create a socket, initial handshake, parse connection string, authentication, etc)."
90,Connection pools are a way to further improve performance by pooling users’ connections
90,to a database. The idea is to decrease the total number of connections that must be opened.
90,"Whenever a client wants to connect to the database, an open connection from the pool is reused"
90,instead of creating a new one.
90,There are many tools and libraries for different programming languages which can help
90,"you create connection pools, as well as server-side connection pooling software"
90,"that works for all connection types, not just within a single software stack."
90,"You can create or debug connection pools with tools like Amazon RDS Proxy, pgpool, pgbouncer, pg_crash, etc."
90,The DUAL Table
90,The DUAL table is a single-row single-column dummy table which was initially added
90,as an underlying object in the Oracle database systems by Charles Weiss. This table is used
90,for situations when you want to select something but no from clause is needed.
90,"In Oracle, FROM clause is mandatory, so you would need the dual table. However, in PostgreSQL, creating"
90,such table is not required as you can select without a from clause.
90,sql--- postgresql
90,select 1 + 1;
90,-- oracle
90,select 1 + 1 from dual;
90,"That being said, this table can be created in postgres as a view to ease porting problems"
90,from Oracle to PostgreSQL.
90,This allows code to remain somewhat compatible with Oracle SQL without annoying the
90,Postgres parser:
90,sqlcreate table dual();
90,LATERAL Joins
90,"PostgreSQL added the LATERAL join technique since PostgreSQL 9.3. Using lateral joins, you can"
90,look at the left hand table:
90,sqlselect * from weather as w
90,join lateral (
90,select city.location
90,from city
90,"where city.name = w.city -- only possible to reference ""w"" because of lateral"
90,) c on true;
90,"In the above query, the inner subquery became a correlated subquery to the outer select query."
90,"Without lateral, each subquery is evaluated independently and as a result, cannot cross-reference any"
90,other FROM item. You would get this error if you haven't used LATERAL:
90,"ERROR: invalid reference to FROM-clause entry for table ""w"""
90,"HINT: There is an entry for table ""w"", but it cannot be referenced from this part of the query."
90,Recursive CTEs
90,WITH clauses can be used with the optional RECURSIVE option.
90,This modifier changes WITH from a mere syntactic convenience into a feature that accomplishes
90,"things not otherwise possible in standard SQL. Using RECURSIVE, a WITH query can refer to its own output."
90,The below query creates the fibonacci sequence:
90,"sqlwith recursive fib(a, b) AS ("
90,"values (0, 1)"
90,union all
90,"select b, a + b from fib where b < 1000"
90,select a from fib;
90,-- 0
90,-- 1
90,-- 1
90,-- 2
90,-- 3
90,-- 5
90,-- ...
90,ORMs create bad queries
90,"As mentioned in previous sections, ORMs are essentially abstractions built on top of SQL"
90,to simplify interactions with your database. Some people use ORMs to write code
90,"using the language structures provided by their programming language, rather than"
90,crafting SQL queries themselves. The ORM serves as a layer between the relational
90,"database and the application, generating the necessary queries."
90,"On the downside, ORMs abstract away database features, can be more challenging"
90,"to debug than raw queries, and occasionally generate suboptimal queries that are"
90,significantly slower than well-written SQL for the same task. One well-known issue
90,is the N+1 query problem.
90,Let's assume we want to retrieve a blog post along with its comments. A common mistake
90,we often encounter is the N+1 query problem:
90,One select query to fetch the post and n additional queries to select
90,comments for each post (n + 1 queries in total). This is easy
90,"to fix in raw SQL with a simple join. However, when using an ORM, you have less control"
90,"on the generated queries, and it can sometimes be challenging to determine"
90,whether you've encountered this problem or not without using a profiler:
90,N+1 RoundtripFix with Joinsql-- one query to fetch the posts
90,"select id, title from blog_post;"
90,-- N query in a loop to fetch the comments of each post:
90,select body from comments where post_id = :post_id;
90,Stored Procedures
90,Stored procedures are server-side procedures with names that are typically written in
90,"various languages, with SQL being the most common. The following displays the definition"
90,of a procedure in SQL:
90,"sqlcreate procedure insert_person(id integer, first_name text, last_name text, cash integer)"
90,language sql
90,as $$
90,"insert into person values (id, first_name, last_name);"
90,"insert into cash values (id, cash);"
90,$$;
90,Stored procedures are invoked using the CALL statement:
90,"sqlcall insert_person(1, 'Maryam', 'Mirzakhani', 1000000);"
90,One distinguishing feature of PostgreSQL compared to other database systems is that
90,it allows you to write your procedures in any programming language you prefer. This is
90,in contrast to most database engines that restrict you to using only a
90,predefined set of languages.
90,Additional languages can be easily integrated into the PostgreSQL server
90,using the CREATE LANGUAGE command:
90,sqlcreate language myLovelyLanguage
90,handler my_language_handler -- the function that glue postgresql with any external language
90,validator my_language_validator -- check syntax errors before executing function
90,"Functions are a concept similar to stored procedures, but they are distinct entities. Traditionally, people used the term ""Stored Procedure"" to refer to both, but there are differences between them. One key distinction is that functions can return values, whereas stored procedures do not. However, this is not the only difference."
90,One of the most significant differences between stored procedures and functions is
90,"that functions can be used within a SELECT statement, but they cannot start or commit"
90,transactions:
90,sqlselect my_func(last_name) from person;
90,"On the other hand, stored procedures can start and commit transactions, but they can not be used"
90,inside select statements.
90,sqlcall sp_can_commit_a_transaction();
90,In short:
90,"Functions have return values, but stored procedures do not."
90,"Functions can be used in select statements, but stored procedures do not."
90,"Functions can not start or commit transactions, but stored procedures can."
90,"There's also a concept called Trigger Functions, but we will talk about them when we're"
90,deeper in the ocean!
90,Cursors
90,The idea behind CURSOR is that the data is generated only when needed (via a FETCH).
90,This mechanism allows us to consume the result set while the database is generating
90,"the results. In contrast, it avoids waiting for the database engine to complete its"
90,work and send all the results at once:
90,sqldeclare my_cursor scroll cursor for select * from films; -- you can read more about SCROLL in the collapsible box
90,"fetch forward 5 from my_cursor; -- FORWARD is the direction, PostgreSQL supports many directions."
90,"-- Outputs five rows 1, 2, 3, 4, and 5"
90,-- Cursor is now at position 5
90,"fetch prior from my_cursor; -- outputs row number 4, PRIOR is also a direction."
90,close my_cursor;
90,Click for more details: What is an scroll cursor?
90,There are no non-nullable types
90,"As mentioned previously, SQL's null is a marker, not a value."
90,"SQL null means unknown, and not having nullable types means we can control the"
90,nullability of columns solely through the use of not null check constraints.
90,"This approach results in flexible data types where any user of the database can input data into the system,"
90,even if the data for a column with such a data type is missing or unknown.
90,"PostgreSQL permits the creation of user-defined types using CREATE TYPE, and it's"
90,possible to specify a default for the data type in case a user desires columns of that
90,"data type to default to something other than a null value. However, it remains"
90,valid to set the column value to null if there are no not null check constraints defined.
90,Optimizers don't work without table statistics
90,"As previously mentioned, PostgreSQL strives to generate an optimal execution"
90,"plan for your SQL queries. Various plans can produce the same result set, but a"
90,well-designed planner/optimizer can produce a faster and more efficient execution plan.
90,"Query optimization is an art of science, and in order to come up with a good plan, PostgreSQL"
90,"needs data. PostgreSQL uses a cost-based optimizer which utilizes data statistics, not static rules."
90,"The planner/optimizer estimates the cost of each step in the plan, and picks a plan that"
90,has the least cost for the system.
90,"Additionally, PostgreSQL switches to a Genetic Query Optimizer when the number"
90,of joins exceeds a defined threshold (set by the geqo_threshold variable).
90,"This is because among all relational operators, joins are often the most complex"
90,and challenging to process and optimize.
90,"As a result, PostgreSQL is equipped with a cumulative statistics system which collects and"
90,reports information related to the database server activities. These statistics can come in
90,handy for the optimizer. You can read more about the statistics that PostgreSQL collect at
90,The Cumulative Statistics System.
90,Plan hints
90,"As we already know, PostgreSQL does its best to come up with a good plan execution using"
90,"statistics, estimates, and guessing. While this approach is generally effective for"
90,"optimizing user queries, there are situations where users may want to provide"
90,hints to the database engine to manually influence certain decisions in execution plans.
90,These hints can make their way to the planner/optimizer using approaches like
90,the pg_hint_plan project and adding SQL Comments before the queries:
90,sql/*+ SeqScan(users) */ explain select * from users where id < 10;
90,QUERY PLAN
90,-- ------------------------------------------------------
90,-- Seq Scan on a
90,(cost=0.00..1483.00 rows=10 width=47)
90,The comment /*+ SeqScan(users) */ instructs the planner to utilize a sequential
90,"scan when searching for items in the ""users"" table. Similarly, hints for joins can"
90,be provided using HashJoin(weather city) syntax within the comment.
90,MVCC Garbage Collection
90,MVCC stands for Multiversion Concurrency Control. Every database engine needs to somehow
90,"manage concurrent access to data, and PostgreSQL as an advanced database engine is no"
90,"exception. As the name suggests, MVCC is the concurrency control mechanism inside Postgres."
90,Multiversion means that each statement sees it's own version of the database (aka snapshot) as
90,it was sometimes ago. This prevents statements from viewing inconsistent data.
90,"MVCC enables the read and write locks not to conflict with each other, so reading"
90,never blocks writing and writing never blocks reading (and PostgreSQL was the first
90,database to be designed with this feature).
90,Keeping multiple copies/versions of data produces garbage data that takes a lot of space
90,on the disk which degrades the performance of the database in the long run. Postgres uses
90,"VACUUM to garbage-collect data. VACUUM reclaims storage occupied by dead tuples,"
90,which are tuples that are deleted or obsoleted by an update are not physically
90,"removed from their table. It is necessary to do VACUUM periodically, especially on"
90,"frequently-updated tables, hence why PostgreSQL includes an “autovacuum” facility"
90,which can automate routine vacuum maintenance.
90,Read more: Google Cloud Blogs: A deep dive into VACUUM FAQs
90,Level 3: Twilight Zone
90,Welcome to Twilight Zone! Things are starting to get intriguing as we venture into more
90,complex database concepts. Brace yourself for a deeper understanding of PostgreSQL's
90,inner workings.
90,COUNT(*) vs COUNT(1)
90,The COUNT function is an aggregate function that can take the form of either count(*)
90,— which counts the total number of input rows — or count(expression) — which counts the
90,number of input rows where the value of the expression is not null.
90,"sql-- Number of all rows, including nulls and duplicates."
90,"-- Performance Warning: PostgreSQL uses a sequential scan of the entire table,"
90,-- or the entirety of an index that includes all rows.
90,select count(*) from person;
90,-- Number of all rows where `middle_name` is not null
90,select count(middle_name) from person;
90,-- Number of all unique and not-null `middle_name`s
90,select count(distinct middle_name) from person;
90,count(1) has no functional difference with count(*) as every row is being counted as
90,constant 1:
90,sqlselect count(1) from person;
90,-- equivalent result:
90,select count(*) from person;
90,"However, an ongoing myth claims that:"
90,using count(1) is better than count(*) because
90,count(*) unnecessarily selects all the columns.
90,"The above statement ↑ is wrong. In PostgreSQL, count(*) is faster because it is a special"
90,hard-coded syntax with no arguments for the count aggregate function. count(1) is
90,specifically slower as it follows the count(expression) syntax and it needs to check if constant 1 is
90,not equal to null for each row.
90,Isolation Levels and Phantom Reads
90,"As we have mentioned earlier, the I in ACID stands for Isolation. A transaction"
90,must be isolated from other concurrent transactions running in the database.
90,"As an example, when you want to backup your database using tools like pg_dump,"
90,you don't want your backup to be affected by other write operations in the system.
90,The SQL standard defines 4 levels of transaction isolation. These isolation level are
90,"defined in terms of phenomena, and each of these levels either prohibits these phenomena,"
90,or does not guarantee of them not happening. The phenomena are listed below:
90,dirty read: A transaction reads data written by a concurrent uncommitted transaction.
90,nonrepeatable read: A transaction re-reads data it has previously read and finds
90,that data has been modified by another transaction (that committed since the initial read).
90,phantom read: A transaction re-executes a query returning a set of rows that satisfy
90,a search condition and finds that the set of rows satisfying the condition has
90,changed due to another recently-committed transaction.
90,serialization anomaly: The result of successfully committing a group of transactions
90,is inconsistent with all possible orderings of running those transactions one at a time.
90,Write skew is the simplest form of serialization anomaly.
90,The four isolation levels in databases are:
90,Read uncommitted
90,Read committed
90,"Repeatable read (aka Snapshot Isolation, or Anomaly Serializable)"
90,Serializable (aka Serializable Snapshot Isolation)
90,It's important to note that PostgreSQL does not implement the read uncommitted isolation level.
90,"Instead, PostgreSQL's Read Uncommitted mode behaves like Read Committed."
90,This is because it is the only sensible way to map the standard isolation levels to
90,PostgreSQL's multiversion concurrency control architecture.
90,This approach aligns with the SQL Standard because the
90,"standard defines the minimum guarantees, not the maximum ones. Therefore, PostgreSQL can and does"
90,disallow phantom reads even in the repeatable read isolation level:
90,Isolation LevelDirty ReadNonrepeatable ReadPhantom ReadSerialization AnomalyRead uncommitted⚠️ Possible (✅ not in PG)⚠️ Possible⚠️ Possible⚠️ PossibleRead committed✅ Not possible⚠️ Possible⚠️ Possible⚠️ PossibleRepeatable read✅ Not possible✅ Not possible⚠️ Possible (✅ not in PG)⚠️ PossibleSerializable✅ Not possible✅ Not possible✅ Not possible✅ Not possible
90,"The term ""Serializable"" execution means that a transaction can run as if it has a"
90,"""serial execution,"" where no concurrent operation is affecting it."
90,Click for more details: PostgreSQL Serializable Isolation is an art of science!
90,Write skew
90,"Write skew is the simplest form of serialization anomaly, and the Serializable"
90,"isolation level protects you from it. However, the Repeatable Read isolation level"
90,does not provide the same protection against write skew.
90,Assume a table with a column which has either Black or White as the value.
90,"Two users concurrently try to make all rows contain matching color values,"
90,but their attempts go in opposite directions. One is trying to update all white rows
90,to black and the other is trying to update all black rows to white.
90,"In such a case, two concurrent transactions each determine what they are writing based"
90,"on reading a data set (rows with black/white column), and that dataset overlaps what"
90,the other is writing. In this case we can get a state which could not occur if either
90,had run before the other
90,"If these updates are run serially, all colors will match: one of the transactions"
90,"turn all rows to white, and the other turns all rows to black. If they are run concurrently"
90,"in REPEATABLE READ mode, the values will be switched, which is not consistent with any"
90,"serial order of runs. If they are run concurrently in SERIALIZABLE mode, PostgreSQL's"
90,Serializable Snapshot Isolation (SSI) will notice
90,the write skew and roll back one of the transactions.
90,Read more:
90,PostgreSQL's Serializable Snapshot Isolation (SSI) vs plain Snapshot Isolation (SI)
90,PostgreSQL's Serializable Wiki
90,Serializable restarts require retry loops on all statements
90,Restarting a serializable transaction requires retry on all the statements of
90,"the transaction, not just the failed one. And if the generated transaction on the backend"
90,"relies on computed values outside of the sql code, those codes needs to be re-executed as"
90,well:
90,ts// This code snippet is for demonstration-purposes only
90,let retryCount = 0;
90,while(retryCount <= 3) {
90,try {
90,"const computedSpecies = computeSpecies(""cat"")"
90,const catto = await db.transaction().execute(async (trx) => {
90,const armin = await trx.insertInto('person')
90,.values({
90,first_name: 'Armin'
90,.returning('id')
90,.executeTakeFirstOrThrow()
90,return await trx.insertInto('pet')
90,.values({
90,"owner_id: armin.id,"
90,"name: 'Catto',"
90,"species: computedSpecies,"
90,"is_favorite: false,"
90,.returningAll()
90,.executeTakeFirstOrThrow()
90,continue;
90,} catch {
90,retryCount++;
90,await delay(1000);
90,Partial Indexes
90,"A partial index is an index built over a subset of a table specified using a where clause,"
90,and is useful when you know that the column values are unique only in certain
90,circumstances. One major use-case for partial indexes is for the times that you don't want to
90,"put common items in the index, as their frequent changes can increase the size of index"
90,and slow the index down because of the recurring updates.
90,"As an example, let's assume that most of your customers have the same nationality (at least 25% or so),"
90,"and there are just a handful of different values in the table, it could be a good idea to"
90,create a partial index on the column:
90,sqlcreate index nationality_idx on person(nationality)
90,"where nationality not in ('american', 'iranian', 'indian');"
90,Generator functions zip when cross joined
90,"Generator functions, also known as Set Returning Functions (SRF), are functions that can"
90,return more than one row. Unlike many other databases in which only scalar values
90,"can appear in select clauses, PostgreSQL allows set-returning functions to appear in"
90,select. One of the most famous generator functions is the generate_series functions which
90,"accepts start, stop, and step (optional) parameters, and generates a series of values"
90,from start to stop with a step size. This function can generate different types of
90,"series including integer, bigint, numeric, and even timestamp:"
90,"sqlselect * from generate_series(2,4); -- or `select generate_series(2,4);`"
90,generate_series
90,-- -----------------
90,-- (3 rows)
90,"select * from generate_series('2008-03-01 00:00'::timestamp,"
90,"'2008-03-04 12:00', '10 hours');"
90,generate_series
90,-- ---------------------
90,2008-03-01 00:00:00
90,2008-03-01 10:00:00
90,2008-03-01 20:00:00
90,2008-03-02 06:00:00
90,2008-03-02 16:00:00
90,2008-03-03 02:00:00
90,2008-03-03 12:00:00
90,2008-03-03 22:00:00
90,2008-03-04 08:00:00
90,-- (9 rows)
90,"In SQL, you can cross join two table using either of these two syntaxes:"
90,sqlselect * from table_a cross join table_b; -- with `cross join`
90,-- or --
90,"select * from table_a, table_b;"
90,-- with comma
90,"And for functions, you can call them using either of these two syntaxes:"
90,"sqlselect * from generate_series(1, 3); -- with `select * from`"
90,-- or --
90,"select generate_series(1, 3);"
90,-- with `select` only
90,"Combining these two syntaxes, when we execute the same cross join syntax for generator functions using"
90,"select * from f() and select f() syntaxes, one of them turns into a zip operation"
90,instead of a cross join:
90,"sqlselect * from generate_series(1, 3) as a, generate_series(5, 7) as b; -- cross joins"
90,-- a	b
90,-- 1	5
90,-- 1	6
90,-- 1	7
90,-- 2	5
90,-- 2	6
90,-- 2	7
90,-- 3	5
90,-- 3	6
90,-- 3	7
90,"select generate_series(1, 3) as a, generate_series(5, 7) as b; -- zips"
90,-- a	b
90,-- 1	5
90,-- 2	6
90,-- 3	7
90,In the second case we get the result from two
90,"generator functions side by side (this is called the zip of two results),"
90,instead of a cartesian product. This is because a join plan can not be created
90,without a from/merge clause (omitted from clause is intended for computing the results
90,"of simple expressions), and PostgreSQL creates a so called ProjectSet node"
90,in the plan to project(display) the items coming from the generator functions.
90,Sharding
90,Sharding in database is the ability to horizontally partition data across one more
90,database shards. While partitioning feature allows a table to be partitioned into multiple
90,"tables, sharding allows a table to be partitioned in a way so parts of it live on"
90,external foreign servers.
90,"PostgreSQL uses the Foreign Data Wrappers (FDW) approach to implement sharding, but it is"
90,still in progress.
90,Citus is an open-source extension for
90,PostgreSQL which enables it to achieve horizontal scalability through sharding and replication.
90,"One key advantage of Citus is that it is not a fork of PostgreSQL but an extension,"
90,which allows it to stay in sync with the community release.
90,"In contrast, many other forks of PostgreSQL often lag behind the community release"
90,in terms of updates and features.
90,ZigZag Join
90,"We've previously discussed logical joins, including left join, right join, inner join,"
90,"cross join, and full joins. These joins are logical in a sense that they are simple"
90,joins that we write in our SQL codes. There is another category of joins called physical joins.
90,Physical joins represent the actual join operations that the database performs to join your data.
90,"These include Nested Loop Join, Hash Join, and Merge Join. You can use explain functionality to see"
90,which kind of physical join your database is using in the plan to execute the logical
90,join you have defined in your SQL.
90,"ZigZag join is a physical join strategy, and can be thought of as a more performant nested loop join."
90,Assume we have a table like this:
90,"sqlcreate table vehicle(id integer primary key, tires integer, wings integer);"
90,create index on vehicle(tires);
90,create index on vehicle(wings);
90,And we have a select query like this:
90,sqlselect * from vehicle where tires = 0 and wings = 1;
90,"Normally this would be a tough task for a database without a Zig-Zag join, because using"
90,"one of the secondary indexes and the primary index in our join plan, we still need to fetch many"
90,records if we're having such a situation:
90,"there are many vehicles with tires = 0, or wings = 1"
90,but not many vehicles with both tires = 0 and wings = 1.
90,ZigZag join can make use of both indexes to reduce the number of fetched records:
90,"In the zig-zag join shown in the image above, we continually switch between the"
90,secondary indexes while comparing with the primary index:
90,We first look at the tires index for values of tires = 0. The first id is equal to 1.
90,"Therefore, we need to jump (zig) to the other index and look for rows where id = 1 (match) or"
90,"id > 1 (skip). So in general, we jump to the row where id >= 1."
90,"Zig to the wings index, and as id = 1, we have found a match. We can safely lookup"
90,the next record of the same index.
90,The next record has the id = 2. We zag to the tires index where id >= 2.
90,"The current record has id = 10. It's not a match, but we've already skipped a lot of"
90,records.
90,"We zig to the wings index again, looking for records where id >= 10"
90,And so on...
90,MERGE
90,"Merge conditionally inserts, updates, or delete rows of a table using a data source:"
90,sqlmerge into customer_account
90,as ca
90,using recent_transactions as tx
90,on tx.customer_id = ca.customer_id
90,when matched then
90,update set balance = balance + transaction_value
90,when not matched then
90,"insert (customer_id, balance)"
90,"values (t.customer_id, t.transaction_value);"
90,Using merge we simplify multiple procedural language statements into a single
90,merge statement.
90,Triggers
90,"Triggers are a mechanism in PostgreSQL that can execute a function before, after, or"
90,instead of the operation when a certain event is occurred on a table. These events
90,"can be either of insert, update, delete, or truncate."
90,Trigger functions have access to special variables that store data both before
90,"and after an edit, hence why they are more powerful than check constraints:"
90,sqlcreate trigger log_update
90,after update on system_actions
90,for each row
90,when (NEW.action_type = 'sensitive' or OLD.action_type = 'sensitive')
90,execute function log_sensitive_system_action_change();
90,"As you can see, the when clause can refer to columns of the old and/or new row values"
90,by writing OLD.column_name or NEW.column_name respectively.
90,"Grouping sets, Cube, Rollup"
90,Imagine you want to see the sum of the salaries of a different departments based on the
90,gender of the employees. One way to do this is to have multiple group by clauses and
90,then union the result rows together:
90,sql
90,"select dept_id, gender, SUM(salary) from employee group by dept_id, gender"
90,union all
90,"select dept_id, NULL, SUM(salary)"
90,from employee group by dept_id
90,union all
90,"select NULL, gender, SUM(salary)"
90,from employee group by gender
90,union all
90,"select NULL, NULL, SUM(salary)"
90,from employee;
90,-- dept_id
90,gender
90,sum
90,-- 1
90,1000
90,-- 1
90,1500
90,-- 2
90,1700
90,-- 2
90,1650
90,-- 2
90,NULL
90,3350
90,-- 1
90,NULL
90,2500
90,-- NULL
90,2700
90,-- NULL
90,3150
90,-- NULL
90,NULL
90,5850
90,"However, this would be a pain if we want to report the sum of salaries for"
90,different groups of data. Grouping sets allow us to define a set of groupings and write a
90,simpler query. The equivalent of the above query using grouping sets would be:
90,"sqlselect dept_id, gender, SUM(salary) from employee"
90,group by
90,grouping sets (
90,"(dept_id, gender),"
90,"(dept_id),"
90,"(gender),"
90,"There are two types of grouping sets, each with its own syntax sugar due to their"
90,common usages: rollup and cube.
90,"sqlrollup (e1, e2, e3, ...)"
90,is equivalent of:
90,sqlgrouping sets (
90,"( e1, e2, e3, ... ),"
90,...
90,"( e1, e2 ),"
90,"( e1 ),"
90,( )
90,and
90,"sqlcube ( a, b, ... )"
90,is equivalent of:
90,sqlGROUPING SETS (
90,"( a, b, c ),"
90,"( a, b"
90,"( a,"
90,"c ),"
90,( a
90,"b, c ),"
90,"c ),"
90,Grouping sets can also be combined together:
90,"sqlgroup by a, cube (b, c), grouping sets ((d), (e))"
90,-- equivalent:
90,group by grouping sets (
90,"(a, b, c, d), (a, b, c, e),"
90,"(a, b, d),"
90,"(a, b, e),"
90,"(a, c, d),"
90,"(a, c, e),"
90,"(a, d),"
90,"(a, e)"
90,Level 4: Midnight Zone
90,"Welcome to Midnight Zone! In this level, we'll delve into the depths of PostgreSQL."
90,Prepare to navigate the challenges and professional topics of a modern database system.
90,Denormalization
90,One of the first steps of designing a database application
90,"is going thorough the normalization process (1NF, 2NF, ...) in order to reduce"
90,"data redundancy and improve data integrity. Even though relational database, specially Postgres,"
90,"are well optimized for having many primary keys and foreign keys in tables, and are"
90,"capable of joining between many tables and handling many constraints, a heavily normalized"
90,schema might still be challenging to deal with because of performance penalties.
90,"In such a scenario, if maintaining a fully normalized table becomes challenging, the database designer"
90,"can go thorough a process known as denormalization. In other words, it improves the"
90,"read performance of your data, with the cost of reducing the write performance, as you"
90,may need to write multiple copies of your data.
90,"PostgreSQL natively supports many denormalized data types including array, composite types"
90,"created via create type, enum, xml, and json types. Materialized views are"
90,also used to implement faster reads with slower writes trade-off.
90,A materialized view is a view that is stored on disk.
90,"You can create a denormalized and materialized view of your data for fast reads, and then"
90,use refresh materialized view my_view to refresh this cache.
90,NULLs in CHECK constraints are truthy
90,"As we have mentioned earlier, null in SQL means not knowing the value,"
90,"rather than the absence of a value, and such a select will return null:"
90,sqlselect null > 7; -- null
90,If we create a column with a check constraint on it like this:
90,"sqlcreate table mature_person(id integer primary key, age integer check(age >= 18));"
90,"and then we try to insert a row where the age equals 15, we will get this error:"
90,ERROR:
90,"new row for relation ""mature_person"" violates check constraint ""mature_person_age_check"""
90,DETAIL:
90,"Failing row contains (1, 15)."
90,"However, this insert will succeed:"
90,"sqlinsert into mature_person(id, age) values (1, null)"
90,-- INSERT 0 1
90,-- Query returned successfully in 80 msec.
90,It might not make sense to assume something satisfies a check constraint when you
90,"don't know the value of it (null), but in SQL we have to let the row thorough because"
90,nulls in check constraints are truthy.
90,Transaction Contention
90,"Resource contention is a conflict over access to a shared resource like RAM, network interface,"
90,"storage, etc. In case of SQL databases, a resource contention can appear in form of"
90,"transaction contentions, and that is when multiple transactions want to write to a"
90,"row at the same time. A transaction contention might require delays, retries, or halts to fix"
90,as they might cause deadlocks/livelocks. This is actually configurable using the deadlock_timeout
90,config.
90,A contention usually slows down your database without leaving many clues for you to
90,"debug them, and the negative effect gets worse when you have multiple nodes or clusters. That"
90,"being said, projects like Postgres-BDR might provides tools to diagnose and correct"
90,contention problems.
90,SELECT FOR UPDATE
90,"select clause is used to read data from database, but sometimes you want to select"
90,rows in order to write them. If any of the below lock strengths are specified:
90,select ... for update
90,select ... for no key update
90,select ... for share
90,select ... for key share
90,the select statement locks the entire selected rows (not just the columns) against
90,concurrent updates:
90,sqlbegin;
90,select * from users WHERE group_id = 1 FOR UPDATE;
90,update users set balance = 0.00 WHERE group_id = 1;
90,commit;
90,This is sometimes known as pessimistic locking. You should be careful when using explicit
90,"locking, because if you perform long-running works in a transaction, the database"
90,will lock the rows for the entirety of time:
90,sqlbegin;
90,select * from users WHERE group_id = 1 FOR UPDATE; -- rows will remain locked and cause performance degradation
90,-- doing a lot of time consuming calculations
90,update users set balance = 0.00 WHERE group_id = 1;
90,commit;
90,You can switch to optimistic locking in such cases. Optimistic locking assumes that
90,"others won't update the same record and verifies this during update time, rather"
90,than locking the record throughout the entire processing duration on the client side.
90,timestamptz doesn't store a timezone
90,If we run this query:
90,"sqlselect typname, typlen"
90,from pg_type
90,"where typname in ('timestamp', 'timestamptz');"
90,-- typname
90,typlen
90,-- timestamp
90,-- timestamptz
90,"we will realize that timestamp and timestamp with time zone types have the same size,"
90,which means PostgreSQL doesn't actually store the timezone for timestamptz. All it does
90,is that it formats the same value using a different timezone:
90,sqlselect now()::timestamp
90,"-- ""2023-08-31 16:56:54.541131"""
90,select now()::timestamp with time zone
90,"-- ""2023-08-31 16:56:58.541131"""
90,set timezone = 'asia/tehran'
90,select now()::timestamp
90,"-- ""2023-08-31 16:56:54.541131"""
90,select now()::timestamp with time zone
90,"-- ""2023-08-31 16:56:23.73028+04:30"""
90,"Read more: The Long, Painful History of Time by Erik Naggum"
90,Watch: The Problem with Time & Timezones - Computerphile
90,Star Schemas
90,Star schema is a database modeling approach adopted by relational data warehouses.
90,It requires modelers to classify their model tables as either dimension or fact.
90,The star schema consists of one or more fact tables referencing to any number of
90,dimension tables.
90,"In data warehousing, a fact table consists of measurements, metrics or facts"
90,"of a business process, while a dimension table is a structure that categorizes"
90,facts and measures in order to enable users to answer business questions.
90,"Commonly used dimensions are people, products, place and time."
90,Sargability
90,"In relational databases, a condition (or predicate) in a query is said to be sargable"
90,if the DBMS engine can take advantage of an index to speed up the execution of the query.
90,The ideal SQL search condition has the general form:
90,<column> <comparison operator> <literal>
90,A common thing that can make a query non-sargible is using an indexed column inside a
90,"function, for example using this query:"
90,sqlselect birthday from users
90,where get_year(birthday) = 2008
90,instead of the equivalent sargible one:
90,sqlselect birthday from users
90,where birthday >= '01-01-2008' AND birthday < '01-01-2009'
90,Or as another example:
90,non-sargiblesargiblesqlselect *
90,from
90,players
90,where
90,SQRT(score) > 7.5
90,"SARG is a contraction for Search ARGument. In the early days, IBM researchers named"
90,"these kinds of search conditions ""sargable predicates""."
90,"In later days, Microsoft and Sybase redefined ""sargable"" to mean ""can be"
90,"looked up via the index."""
90,Ascending Key Problem
90,Assume that we have such a time-series event table:
90,"sqlcreate table event(t timestamp primary key, content text);"
90,"In this situation, the primary key or index of such tables are continuously"
90,increasing over time and the rows are always being added to the end of table.
90,This can result in data fragmentation as the end of the table is the only spot
90,"being written, and the inserting point is not evenly distributed among the rows."
90,"This can lead to various issues, such as contention at the end of the table, making it"
90,"challenging to distribute and shard the table, and slower data retrievals due"
90,to the lack of table statistics at the end.
90,"As mentioned earlier, a database system relies on statistics to generate more"
90,optimal execution plans. It's important to note that statistics for the most
90,"recently inserted rows are not included in the database statistics, e.g. pg_stat_database."
90,One way to fix the ascending key problem in PostgreSQL is to use the block range index (BRIN index).
90,These indexes give performance improvements when the data is naturally ordered as it
90,"is added to the table, such as t timestamp columns or a naturally auto incremented columns."
90,sqlcreate table event (
90,"event_time timestamp with time zone not null,"
90,event_data jsonb not null
90,create index on event using BRIN (event_time);
90,Ambiguous Network Errors
90,"Various types of network errors can occur when working with databases, such as"
90,"disconnecting from the database during a transaction. In such cases,"
90,it's your responsibility to verify whether your interaction with the
90,database was successful or not.
90,Ambiguous network errors may not provide clear indications of where the process was
90,"interrupted, or even if a network problem exists. If you are running a streaming replication,"
90,a young connection in the pg_stat_replication table might be a sign of a network problem or
90,other kind of reliability issues.
90,utf8mb4
90,"utf8mb4 stands for ""UTF-8 Multibyte 4"" and is a MySQL type. It has nothing to do with"
90,PostgreSQL or the SQL standard.
90,"Historically, MySQL has used utf8 as an alias for utf8mb3. That means that it can"
90,only store Basic Multilingual Plane unicode characters (3 byte unicode characters).
90,"If you want to be able to store all unicode characters, you need to explicitly use"
90,utf8mb4 type.
90,"Beginning with MySQL 8.0.28, utf8mb3 is used exclusively in the output of show statements and in"
90,Information Schema tables when this character set is meant.
90,At some point in the MySQL history utf8 is expected to become a reference to utf8mb4.
90,"To avoid ambiguity about the meaning of utf8, MySQL users (and MariaDB users, because"
90,MariaDB is a fork of MySQL) should consider specifying utf8mb4 explicitly
90,for character set references instead of utf8.
90,Level 5: Abyssal Zone
90,"Welcome to Abyssal Zone! Here, we'll explore the abyss of PostgreSQL's concepts."
90,Things that you might haven't heard before!
90,Cost models don't reflect reality
90,"When explaining a query, you can enable the ""cost"" option, which provides you with"
90,the estimated statement execution cost. This cost represents the planner's
90,"estimate of how long it will take to execute the statement, typically"
90,"measured in cost units, conventionally indicating disk page fetches."
90,The planner uses the table statistics to come up with the
90,"best plan it can with the lowest cost. However, that computed cost can be utterly wrong"
90,"as it's just an estimated value, or could be based on the wrong statistics"
90,(as we've seen in the ascending key problem).
90,'null'::jsonb IS NULL = false
90,"NULL in SQL means not knowing the value, while JSON's null is JavaScript's null and"
90,represents the intentional absence of any value. This is why JSON's null in
90,PostgreSQL's jsonb data-type is not equivalent to SQL's null:
90,sqlselect 'null'::jsonb is null;
90,-- false
90,"select '{""name"": null}'::jsonb->'name' is null;"
90,"-- false, because JSON's null != SQL's null"
90,"select '{""name"": null}'::jsonb->'last_name' is null;"
90,"-- true, because 'last_name' key doesn't exists in JSON, and the result is an SQL null"
90,TPCC requires wait times
90,"TPC-C stands for ""Transaction Processing Performance Council - Benchmark C"" and is an"
90,online transaction processing benchmark hosted at tpc.org.
90,TPC-C involves a mix of five concurrent transactions of different types and complexity
90,either executed on-line or queued for deferred execution. The database is
90,comprised of nine types of tables with a wide range of record and population sizes.
90,TPC-C is measured in transactions per minute (tpmC).
90,TPC-C benchmarks includes two type of wait times: The keying time represents the time spent
90,entering data at the terminal (pressing keys on the keyboard) and the think time
90,"represents the time spent, by the operator, to read the result of the transaction at"
90,the terminal before requesting another transaction. Each transaction has a minimum keying
90,time and a minimum think time.
90,These times will help the benchmark to be closer to real-world scenarios. Benchmarking
90,transactions without wait-time will make the system under test slower and slower overtime
90,as the system internals will have no free resources to operate.
90,"pgbench is the command-line tool used to benchmark PostgreSQL databases, and it supports TPC and"
90,many different command-line arguments including wait-time/schedule-lag-time.
90,DEFERRABLE INITIALLY IMMEDIATE
90,Constraints on columns can be either deferred or immediate.
90,"Immediate constraints are checked at the end of each statement, while deferred constraints are"
90,not checked until transaction commit. Each constraint has its own IMMEDIATE or DEFERRED mode.
90,"Upon creation, a constraint is given one of three characteristics:"
90,"not deferrable (default, equivalent to immediate): the constraint is checked immediately after"
90,each statement. This behavior can NOT be changed using the set constraint command. e.g. set constraint pk_name deferred;
90,deferrable initially immediate: the constraint is checked immediately after
90,"each statement, however, this behavior can later be altered using the set constraint command."
90,deferrable initially deferred: the constraints are not checked until transaction commit.
90,This behavior can later be altered using the set constraint command.
90,sqlcreate table book (
90,"name text primary key,"
90,author text references author(name) on delete cascade deferrable initially immediate;
90,"As you see in the SQL code above, deferrable initially immediate is specified while"
90,"defining the schema of the table, not on runtime."
90,EXPLAIN approximates SELECT COUNT(*)
90,Using explain with select count(*) can give you an estimate of how many rows PostgreSQL
90,think are in your table using table statistics.
90,sqlexplain select count(*) from users;
90,"If you don't need an exact count, the current statistic from the catalog table"
90,pg_class might be good enough and is much faster to retrieve for big tables:
90,pg_class estimatepg_class estimate with exact schemasqlselect reltuples as estimate_count from pg_class where relname = 'table_name';
90,MATCH PARTIAL Foreign Keys
90,"match full, match partial, and match simple(default) are three table column"
90,constraints for the foreign keys. Foreign keys are supposed
90,"to guarantee the referential integrity of our database, and in order to do so, database"
90,needs to know how to match the referencing column value with the referenced column value
90,in case of nulls.
90,match full: will not allow one column of a multi-column foreign key to be null
90,"unless all foreign key columns are null; if they are all null, the row is not required"
90,to have a match in the referenced table.
90,match simple(default): allows any of the foreign key columns to be null; if any
90,"of them are null, the row is not required to have a match in the referenced table."
90,"match partial: if all referencing columns are null, then the row of the referencing"
90,"table passes the constraint check. If at least one referencing columns is not null,"
90,then the row passes the constraint check if and only if there is a row of the
90,referenced table that matches all the non-null referencing columns. This is not yet implemented
90,"in PostgreSQL, but a workaround is to use"
90,not null constraints on the referencing column(s) to prevent these cases from arising.
90,Causal Reverse
90,Causal reverse is a transaction anomaly that can be encountered even with
90,"Serializable isolation levels. To address this anomaly, a higher level of"
90,serializability known as Strict Serializability is required.
90,Here is a simple example for the causal reverse anomaly:
90,"Thomas executes select * from events, doesn't get a respond yet."
90,"Ava executes insert into events (id, time, content) values (1, '2023-09-01 02:01:16.679037', 'hello') and commit."
90,"Emma executes insert into events (id, time, content) values (2, '2023-09-01 02:02:56.819018', 'hi') and commit."
90,"Thomas gets the respond of the select query from step 1. He gets the Emma's row, but not"
90,Ava's.
90,"In the causal reverse anomaly, a later write which was caused by an earlier write,"
90,time-travels to a point in the serial order prior to the earlier write.
90,Read more: Correctness Anomalies Under Serializable Isolation
90,Level 6: Hadal Zone
90,"Welcome to Hadal Zone! As we reach extreme depths, we'll discuss specialized"
90,"PostgreSQL topics like learned indexes, TXID Exhaustion, and more!"
90,Vectorized doesn't mean SIMD
90,"SIMD stands for ""Single instruction, multiple data"", and is a type of parallel processing"
90,when a single CPU instruction is simultaneously applied to multiple different data streams.
90,"The term ""Vector"" and ""Vectorized"" usually come with the term ""SIMD"" in computer"
90,literature. Vector programming (aka Array programming) refers to solutions that allow us
90,to apply operations to an entire set of values at once.
90,"As a matter of fact, the extension"
90,instruction set which was added to the x86 instruction set architecture to perofrm
90,SIMD is called Advanced Vector Extensions (AVX).
90,"SIMD is one approach to leverage vector computations and not the only way. That being said,"
90,PostgreSQL vectors are backed by SIMD cpu instructions.
90,NULLs are equal in DISTINCT but unequal in UNIQUE
90,Assume you have a table called unique_items with such a definition:
90,sqlcreate table unique_items(item text unique);
90,"PostgreSQL will prevent you to insert duplicate 'hi' values, as the second one"
90,would violate the unique constraint:
90,sqlinsert into unique_items values ('hi');
90,-- INSERT 0 1
90,-- Query returned successfully in 89 msec.
90,insert into unique_items values ('hi');
90,-- ERROR:
90,"duplicate key value violates unique constraint ""unique_items_item_key"""
90,-- DETAIL:
90,Key (item)=(hi) already exists.
90,"However, we can insert as many nulls as we want:"
90,sqlinsert into unique_items values (null); -- INSERT 0 1; Query returned successfully
90,insert into unique_items values (null); -- INSERT 0 1; Query returned successfully
90,insert into unique_items values (null); -- INSERT 0 1; Query returned successfully
90,table unique_items;
90,-- item
90,"-- ""hi"""
90,-- `null`
90,-- `null`
90,-- `null`
90,"This means that to SQL, null values are not the same, as they are unknown values."
90,"But now if we select distinct items of the unique_items table, we will get this result:"
90,sqlselect distinct item from unique_items;
90,-- item
90,-- `null`
90,"-- ""hi"""
90,"All of the null values are shown as a single item, as if PostgreSQL grouped all the unknown"
90,values in one value.
90,Volcano Model
90,"""Volcano - An Extensible and Parallel Query Evaluation System"" is a research paper by"
90,Goetz Graefe that was published in the IEEE Transactions on Knowledge and Data Engineering
90,"(Volume: 6, Issue: 1) on February 1994. This evaluation system is called Volcano Model,"
90,"Volcano iterator model, or sometimes simply referred to as the Iterator model."
90,"Each relational-algebraic operator produces a tuple stream, and a consumer can iterate"
90,"over its input streams. The tuple stream interface is essentially: open, next, and"
90,"close; all operators offer the same interface, and the implementation is opaque."
90,"Each next call produces a new tuple from the stream, if one is available."
90,"To obtain the query output, one ""next-next-next""s on the final RA operator;"
90,"that one will in turn use ""next""s on its inputs to pull tuples allowing it to"
90,"produce output tuples, etc. Some ""next""s will take an extremely long time, since many"
90,"""next""s on previous operators will be required before they emit any output."
90,Example: select max(v) from t may need to go trough all of t in order to find that
90,maximum.
90,A highly simplified pseudocode of the volcano iteration model:
90,define volcano_iterator_evaluate(root):
90,q = root // operator `q` is the root of the query plan
90,open(q)
90,t = next(q)
90,while t != null:
90,emit(t) // ship current row to application
90,t = next(q)
90,close(q)
90,Click for more details: Abstract of the Paper
90,Read more: Volcano-An Extensible and Parallel Query Evaluation System
90,Join ordering is NP Hard
90,"When you have multiple joins in your SQL query, the database engine needs to find"
90,an order to perform the joins. Finding the best join order is an NP-hard problem.
90,"This is why database engines use estimates, statistics, and soft computing approaches to find an order"
90,since finding the optimal solution would take forever.
90,This is a simplified table of problem classes:
90,Problem ClassVerify SolutionFind SolutionExampleP😁 Easy😁 EasyMultiply numbersNP😁 Easy😥 Hard8 QueensNP-hard😥 Hard😭 HardBest next move in Chess
90,NP-hard problems are at least as hard as the hardest problems in NP. That means
90,"if P ≠ NP (which is probabely the case, at least for now), NP-hard problems could not be"
90,solved in polynomial time.
90,"If P=NP, then the world would be a profoundly different place than we usually"
90,"assume it to be. There would be no special value in “creative leaps,”"
90,no fundamental gap between solving a problem and recognizing the solution once
90,it's found. Everyone who could appreciate a symphony would be Mozart; everyone
90,who could follow a step-by-step argument would be Gauss; everyone who
90,could recognize a good investment strategy would be Warren Buffett.
90,Scott Aaronson
90,Database Cracking
90,Cracking is a technique that shifts the cost of index maintenance from updates
90,to query processing. The query pipeline optimizers are used to massage the query plans to
90,crack and to propagate this information. The technique allows for improved access times
90,and self-organized behavior.
90,"In other words, Database cracking is an approach for data indexing and index maintainance in"
90,"a self-organized way. In a database system where database cracking is used, an"
90,incoming query requesting all elements which satisfy a certain condition c does not
90,only return a result but it also causes a reodering of the physical database so that
90,all elements satisfying c are stored in a contiguous memery space. Therefore the
90,physical database is devided into multiple parts (cracked).
90,By using this mechanism
90,the database reorganizes itself in the most favourable way according to the workload
90,which is put on it.
90,Read more: Database Cracking by David Werner
90,WCOJ
90,Traditional binary join algorithms such as hash join operate over two relations
90,at a time (r1 join r2); joins between more than two relations are implemented by
90,repeatedly applying binary joins (r1 join (r2 join r3)).
90,WCOJ (Worst-Case Optimal Join) is a kind of join algorithm whose running
90,"time is worst-case optimal for all natural join queries, and is asymptotically"
90,faster in worst case than any join algorithm based on such iterated binary joins.
90,Read more: Worst-case Optimal Join Algorithms
90,Learned Indexes
90,Learned Indexes are indexing strategies that utilize artificial intelligence approaches
90,and deep-learning models to outperform cache-optimized
90,B-Trees and reduce memory usage.
90,Google and MIT engineers developed such a model and published their work as a pioneer
90,paper with the title
90,"""The Case for Learned Index Structures"". The key idea is that"
90,a model can learn the sort order or structure of lookup keys and use this signal
90,to effectively predict the position or existence of records.
90,Read more: The Case for Learned Index Structures
90,TXID Exhaustion
90,"Transaction ID Exhaustion, often referred to as the ""Wraparound Problem,"""
90,arises due to the limited number of transaction IDs available and the absence
90,"of regular database maintenance, known as vacuuming."
90,PostgreSQL's MVCC transaction semantics depend on being able to compare transaction ID (XID)
90,numbers: a row version with an insertion XID greater than the current transaction's XID
90,is “in the future” and should not be visible to the current transaction.
90,But since transaction IDs have limited size (32 bits) a cluster that runs for a long time
90,(more than 4 billion transactions) would suffer transaction ID wraparound: the XID
90,"counter wraps around to zero, and all of a sudden transactions that were in the past"
90,appear to be in the future — which means their output become invisible.
90,"In short, catastrophic data loss. (Actually the data is still there, but that's cold comfort if you cannot get at it.)"
90,"To avoid this, it is necessary to vacuum every table in every database at least once"
90,every two billion transactions.
90,Read more: Routine Vacuuming: Preventing Transaction ID Wraparound Failures
90,Level 7: Pitch Black Zone
90,Welcome to Pitch Black Zone! Congratulations on your journey to the deepest reaches of
90,"PostgreSQL knowledge. Brace yourself for esoteric and cutting-edge topics,"
90,where only the boldest dare to venture!
90,The halloween problem
90,Halloween Problem is a database error that a database system developer needs to be
90,aware of.
90,"On the Halloween day of 1976, a couple of computer engineers were working on a query that was"
90,"supposed to give a 10% raise to every employee who earned less than $25,000:"
90,raise by 10% for salary < 25000table definitionsqlupdate employee set salary = salary + (salary / 10) where salary < 25000
90,"This query would run successfully in their database, but when finished"
90,"all the employees in the database earned at least $25,000."
90,"This is because the updated rows were also visible to the query execution engine,"
90,"and as the match criteria in the where clause was still true, the database continued"
90,"to increase their salaries until they are over $25,000."
90,This could even cause an infinite loop in some cases where updates continually
90,place the updated record ahead of the scan performing the update operation.
90,"ATTENTIONPostgreSQL does NOT have this problem. Halloween Problem is an error in database design,"
90,and any database with such a problem is not reliable.
90,Dee and Dum
90,Table dee is the table that has no columns but a single row.
90,It plays the role of True.
90,Table dum is the table that has no columns and no rows.
90,It plays the role of False.
90,These theoretical tables and terminology was created by Hugh Darwen.
90,You can read more about the implmentation of these tables in PostgreSQL
90,at Creating Tables Dum and Dee in PostgreSQL by Lukas Eder.
90,PostgreSQL trigger functions are used to enforce these rules.
90,SERIAL is non-transactional
90,Serial types in PostgreSQL are used to create autoincrementing columns.
90,"These data types (smallserial, serial, and bigserial) are not true types,"
90,but merely a syntactic sugar for creating unique identifier columns:
90,serialequivalent query with sequencesqlcreate table tablename (
90,colname serial
90,"Because serial types are implemented using sequences, there may be ""holes"" or gaps"
90,"in the sequence of values which appears in the column, even if no rows are ever deleted."
90,"A value allocated from the sequence is still ""used up"" even if a row containing"
90,"that value is never successfully inserted into the table column. This may happen,"
90,"for example, if the inserting transaction rolls back. This is why serial types are considered"
90,non-transactional as they won't rollback their value in case of a transaction rollback.
90,sqlcreate table counter(c serial primary key);
90,-- CREATE TABLE; Query returned successfully.
90,insert into counter values (default);
90,-- INSERT 0 1; Query returned successfully. <- uses id 1
90,insert into counter values (default);
90,-- INSERT 0 1; Query returned successfully. <- uses id 2
90,begin;
90,insert into counter values (default);
90,abort;
90,"-- ROLLBACK; Query returned successfully. <- uses id 3, rollback doesn't give it back"
90,insert into counter values (default);
90,-- INSERT 0 1; Query returned successfully. <- uses id 4
90,table counter;
90,-- c
90,-- 1
90,-- 2
90,-- 4 <- the number 3 is missing
90,Sequences are the only SQL objects with non-transactional behavior.
90,allballs
90,The 'allballs' string will turn into the midnight time (00:00:00) when converted to
90,time.
90,"This is because ""allballs"" is an slang for ""all zeros""."
90,This slang was historically used in military communications.
90,sqlselect 'allballs'::time;
90,-- time
90,-- 00:00:00
90,Read more: PostgreSQL mailing list: Why is 'allballs' accepted as a literal for time?
90,fsyncgate
90,"fsync is an OS system call, and in Linux it is used to synchronize a file's in-core"
90,"state with storage device. In other words, this system call ensures that the data written"
90,to a file is indeed written on
90,the storage device and persisted by transfering/flushing all modified in-core data of
90,the file to the disk or other permanent storage device.
90,"The term ""fsyncgate 2018"" is referred to the scandals and controversies around the"
90,reliability issues of the fsync system call on the PostgreSQL mailing list and elsewhere
90,"(or as some people say, how ""PostgreSQL used fsync incorrectly for 20 years"")."
90,The issue was raised by Craig Ringer. Quoted from the mailing list:
90,Hi all
90,Some time ago I ran into an issue where a user encountered data corruption
90,after a storage error. PostgreSQL played a part in that corruption by
90,allowing checkpoint what should've been a fatal error.
90,TL;DR: Pg should PANIC on fsync() EIO return. Retrying fsync() is not OK at
90,"least on Linux. When fsync() returns success it means ""all writes since the"
90,"last fsync have hit disk"" but we assume it means ""all writes since the last"
90,"SUCCESSFUL fsync have hit disk""."
90,...
90,Read more: PostgreSQL mailing list: PostgreSQL's handling of fsync() errors is unsafe and risks data loss at least on XFS
90,Every SQL operator is actually a join
90,Every SQL operator can be represented using a join.
90,"One way to think about joins is that they ""look stuff up"" in a relation."
90,"sqlselect age, age * age as age_squared from person;"
90,"For example in the above query, rather than computing age * age explicitly,"
90,we could also just look it up in
90,the squares function table (with columns x and xx):
90,"sqlselect age, xx as age_squared from person join squares on age = x;"
90,Read more: JOIN: The Ultimate Projection
90,NULL
90,NULL can be tricky sometimes. Don't you think?
90,Conclusion
90,We've seen a cool meme on the internet and we've tried to understand it.
90,"This was a journey from the skies on top of the SQL iceberg, to the deepest parts of the ocean"
90,where everything was pitch-black. We've looked at each part of this meme while wearing our
90,PostgreSQL hat to see how these topics are related to the PostgreSQL implementation of
90,SQL and relational databases.
90,"Yet again, shout out to Jordan Lewis and his friends for creating"
90,this cool and informative meme.
90,Discussions
90,HackerNews :: Explaining the Postgres iceberg
90,Lobsters :: Explaining The Postgres Meme
90,Reddit :: r/programming Explaining the Postgres Meme
90,Youtube :: Scaling Postgres / Postgres Meme
90,YouTube :: Theo / Postgres Meme Reaction VOD
90,References
90,Resources I've used to write this blog post:
90,PostgreSQL Documentation. [Online]. postgresql.org/docs
90,"Fontaine, D. (2019b). The Art of PostgreSQL: Turn Thousands of Lines of Code Into Simple Queries."
90,"Schönig, H. (2023). Mastering PostgreSQL 15: Advanced techniques to build and manage scalable, reliable, and fault-tolerant database applications. Packt Publishing Ltd."
90,"Dombrovskaya, H., Novikov, B., & Bailliekova, A. (2021). PostgreSQL query Optimization: The Ultimate Guide to Building Efficient Queries. Apress."
90,"Riggs, S., & Ciolli, G. (2022). PostgreSQL 14 Administration Cookbook: Over 175 Proven Recipes for Database Administrators to Manage Enterprise Databases Effectively."
90,"Use the Index, Luke! A Guide to Database Performance for Developers. [Online]. use-the-index-luke.com"
90,"Gulutzan, P., & Pelzer, T. (2003). SQL Performance Tuning. Addison-Wesley Professional."
90,Cockroach Labs Blog. [Online]. Cockroach Labs Blog
90,Justin Jaffray's Blog. [Online]. Justin Jaffray's Blog
90,Discuss on TwitterTagspostgresdatabasemathPrevious ArticleHow optimistic or pessimistic are the Kurzgesagt videos?← Back to the blogMail • GitHub • © 2024 • Alt/Option keyaltO  to navigate
91,pgBackRest User Guide - Debian & Ubuntu
91,pgBackRest User Guide
91,Debian & Ubuntu
91,Home
91,User Guides
91,Releases
91,Configuration
91,Commands
91,FAQ
91,Metrics
91,Table of Contents
91,Introduction
91,Concepts
91,Backup
91,Restore
91,Write Ahead Log (WAL)
91,Encryption
91,Upgrading pgBackRest
91,Upgrading pgBackRest from v1 to v2
91,Upgrading pgBackRest from v2.x to v2.y
91,Build
91,Installation
91,Quick Start
91,Setup Demo Cluster
91,Configure Cluster Stanza
91,Create the Repository
91,Configure Archiving
91,Configure Retention
91,Configure Repository Encryption
91,Create the Stanza
91,Check the Configuration
91,Perform a Backup
91,Schedule a Backup
91,Backup Information
91,Restore a Backup
91,Monitoring
91,In PostgreSQL
91,Using jq
91,Backup
91,File Bundling
91,Block Incremental
91,Backup Annotations
91,Retention
91,Full Backup Retention
91,Differential Backup Retention
91,Archive Retention
91,Restore
91,File Ownership
91,Delta Option
91,Restore Selected Databases
91,Point-in-Time Recovery
91,Delete a Stanza
91,Multiple Repositories
91,Azure-Compatible Object Store Support
91,S3-Compatible Object Store Support
91,SFTP Support
91,GCS-Compatible Object Store Support
91,Dedicated Repository Host
91,Installation
91,Setup Passwordless SSH
91,Configuration
91,Create and Check Stanza
91,Perform a Backup
91,Restore a Backup
91,Parallel Backup / Restore
91,Starting and Stopping
91,Replication
91,Installation
91,Setup Passwordless SSH
91,Hot Standby
91,Streaming Replication
91,Multiple Stanzas
91,Installation
91,Setup Passwordless SSH
91,Configuration
91,Setup Demo Cluster
91,Create the Stanza and Check Configuration
91,Asynchronous Archiving
91,Archive Push
91,Archive Get
91,Backup from a Standby
91,Upgrading PostgreSQL
91,Introduction
91,"This user guide is intended to be followed sequentially from beginning to end — each section depends on the last. For example, the Restore section relies on setup that is performed in the Quick Start section. Once pgBackRest is up and running then skipping around is possible but following the user guide in order is recommended the first time through."
91,"Although the examples in this guide are targeted at Debian/Ubuntu and PostgreSQL 15, it should be fairly easy to apply the examples to any Unix distribution and PostgreSQL version. The only OS-specific commands are those to create, start, stop, and drop PostgreSQL clusters. The pgBackRest commands will be the same on any Unix system though the location of the executable may vary. While pgBackRest strives to operate consistently across versions of PostgreSQL, there are subtle differences between versions of PostgreSQL that may show up in this guide when illustrating certain examples, e.g. PostgreSQL path/file names and settings."
91,Configuration information and documentation for PostgreSQL can be found in the PostgreSQL Manual.
91,A somewhat novel approach is taken to documentation in this user guide. Each command is run on a virtual machine when the documentation is built from the XML source. This means you can have a high confidence that the commands work correctly in the order presented. Output is captured and displayed below the command when appropriate. If the output is not included it is because it was deemed not relevant or was considered a distraction from the narrative.
91,All commands are intended to be run as an unprivileged user that has sudo privileges for both the root and postgres users. It's also possible to run the commands directly as their respective users without modification and in that case the sudo commands can be stripped off.
91,Concepts
91,"The following concepts are defined as they are relevant to pgBackRest, PostgreSQL, and this user guide."
91,Backup
91,"A backup is a consistent copy of a database cluster that can be restored to recover from a hardware failure, to perform Point-In-Time Recovery, or to bring up a new standby."
91,Full Backup: pgBackRest copies the entire contents of the database cluster to the backup. The first backup of the database cluster is always a Full Backup. pgBackRest is always able to restore a full backup directly. The full backup does not depend on any files outside of the full backup for consistency.
91,"Differential Backup: pgBackRest copies only those database cluster files that have changed since the last full backup. pgBackRest restores a differential backup by copying all of the files in the chosen differential backup and the appropriate unchanged files from the previous full backup. The advantage of a differential backup is that it requires less disk space than a full backup, however, the differential backup and the full backup must both be valid to restore the differential backup."
91,"Incremental Backup: pgBackRest copies only those database cluster files that have changed since the last backup (which can be another incremental backup, a differential backup, or a full backup). As an incremental backup only includes those files changed since the prior backup, they are generally much smaller than full or differential backups. As with the differential backup, the incremental backup depends on other backups to be valid to restore the incremental backup. Since the incremental backup includes only those files since the last backup, all prior incremental backups back to the prior differential, the prior differential backup, and the prior full backup must all be valid to perform a restore of the incremental backup. If no differential backup exists then all prior incremental backups back to the prior full backup, which must exist, and the full backup itself must be valid to restore the incremental backup."
91,Restore
91,A restore is the act of copying a backup to a system where it will be started as a live database cluster. A restore requires the backup files and one or more WAL segments in order to work correctly.
91,Write Ahead Log (WAL)
91,"WAL is the mechanism that PostgreSQL uses to ensure that no committed changes are lost. Transactions are written sequentially to the WAL and a transaction is considered to be committed when those writes are flushed to disk. Afterwards, a background process writes the changes into the main database cluster files (also known as the heap). In the event of a crash, the WAL is replayed to make the database consistent."
91,WAL is conceptually infinite but in practice is broken up into individual 16MB files called segments. WAL segments follow the naming convention 0000000100000A1E000000FE where the first 8 hexadecimal digits represent the timeline and the next 16 digits are the logical sequence number (LSN).
91,Encryption
91,Encryption is the process of converting data into a format that is unrecognizable unless the appropriate password (also referred to as passphrase) is provided.
91,"pgBackRest will encrypt the repository based on a user-provided password, thereby preventing unauthorized access to data stored within the repository."
91,Upgrading pgBackRest
91,Upgrading pgBackRest from v1 to v2
91,"Upgrading from v1 to v2 is fairly straight-forward. The repository format has not changed and all non-deprecated options from v1 are accepted, so for most installations it is simply a matter of installing the new version."
91,"However, there are a few caveats:"
91,The deprecated thread-max option is no longer valid. Use process-max instead.
91,The deprecated archive-max-mb option is no longer valid. This has been replaced with the archive-push-queue-max option which has different semantics.
91,The default for the backup-user option has changed from backrest to pgbackrest.
91,"In v2.02 the default location of the pgBackRest configuration file has changed from /etc/pgbackrest.conf to /etc/pgbackrest/pgbackrest.conf. If /etc/pgbackrest/pgbackrest.conf does not exist, the /etc/pgbackrest.conf file will be loaded instead, if it exists."
91,"Many option names have changed to improve consistency although the old names from v1 are still accepted. In general, db-* options have been renamed to pg-* and backup-*/retention-* options have been renamed to repo-* when appropriate."
91,"PostgreSQL and repository options must be indexed when using the new names introduced in v2, e.g. pg1-host, pg1-path, repo1-path, repo1-type, etc."
91,Upgrading pgBackRest from v2.x to v2.y
91,"Upgrading from v2.x to v2.y is straight-forward. The repository format has not changed, so for most installations it is simply a matter of installing binaries for the new version. It is also possible to downgrade if you have not used new features that are unsupported by the older version."
91,Build
91,Installing pgBackRest from a package is preferable to building from source. See Installation for more information about packages.
91,When building from source it is best to use a build host rather than building on production. Many of the tools required for the build should generally not be installed in production. pgBackRest consists of a single executable so it is easy to copy to a new host once it is built.
91,build ⇒ Download version 2.50 of pgBackRest to /build path
91,mkdir -p /build
91,wget -q -O - \
91,https://github.com/pgbackrest/pgbackrest/archive/release/2.50.tar.gz | \
91,tar zx -C /build
91,build ⇒ Install build dependencies
91,sudo apt-get install make gcc libpq-dev libssl-dev libxml2-dev pkg-config \
91,liblz4-dev libzstd-dev libbz2-dev libz-dev libyaml-dev libssh2-1-dev
91,build ⇒ Configure and compile pgBackRest
91,cd /build/pgbackrest-release-2.50/src && ./configure && make
91,Installation
91,A new host named pg-primary is created to contain the demo cluster and run pgBackRest examples.
91,"Installing pgBackRest from a package is preferable to building from source. When installing from a package the rest of the instructions in this section are generally not required, but it is possible that a package will skip creating one of the directories or apply incorrect permissions. In that case it may be necessary to manually create directories or update permissions."
91,Debian/Ubuntu packages for pgBackRest are available at apt.postgresql.org.
91,If packages are not provided for your distribution/version you can build from source and then install manually as shown here.
91,pg-primary ⇒ Install dependencies
91,sudo apt-get install postgresql-client libxml2 libssh2-1
91,pg-primary ⇒ Copy pgBackRest binary from build host
91,sudo scp build:/build/pgbackrest-release-2.50/src/pgbackrest /usr/bin
91,sudo chmod 755 /usr/bin/pgbackrest
91,pgBackRest requires log and configuration directories and a configuration file.
91,pg-primary ⇒ Create pgBackRest configuration file and directories
91,sudo mkdir -p -m 770 /var/log/pgbackrest
91,sudo chown postgres:postgres /var/log/pgbackrest
91,sudo mkdir -p /etc/pgbackrest
91,sudo mkdir -p /etc/pgbackrest/conf.d
91,sudo touch /etc/pgbackrest/pgbackrest.conf
91,sudo chmod 640 /etc/pgbackrest/pgbackrest.conf
91,sudo chown postgres:postgres /etc/pgbackrest/pgbackrest.conf
91,pgBackRest should now be properly installed but it is best to check. If any dependencies were missed then you will get an error when running pgBackRest from the command line.
91,pg-primary ⇒ Make sure the installation worked
91,sudo -u postgres pgbackrest
91,pgBackRest 2.50 - General help
91,Usage:
91,pgbackrest [options] [command]
91,Commands:
91,annotate
91,Add or modify backup annotation.
91,archive-get
91,Get a WAL segment from the archive.
91,archive-push
91,Push a WAL segment to the archive.
91,backup
91,Backup a database cluster.
91,check
91,Check the configuration.
91,expire
91,Expire backups that exceed retention.
91,help
91,Get help.
91,info
91,Retrieve information about backups.
91,repo-get
91,Get a file from a repository.
91,repo-ls
91,List files in a repository.
91,restore
91,Restore a database cluster.
91,server
91,pgBackRest server.
91,server-ping
91,Ping pgBackRest server.
91,stanza-create
91,Create the required stanza data.
91,stanza-delete
91,Delete a stanza.
91,stanza-upgrade
91,Upgrade a stanza.
91,start
91,Allow pgBackRest processes to run.
91,stop
91,Stop pgBackRest processes from running.
91,verify
91,Verify contents of the repository.
91,version
91,Get version.
91,Use 'pgbackrest help [command]' for more information.
91,Quick Start
91,"The Quick Start section will cover basic configuration of pgBackRest and PostgreSQL and introduce the backup, restore, and info commands."
91,Setup Demo Cluster
91,"Creating the demo cluster is optional but is strongly recommended, especially for new users, since the example commands in the user guide reference the demo cluster; the examples assume the demo cluster is running on the default port (i.e. 5432). The cluster will not be started until a later section because there is still some configuration to do."
91,pg-primary ⇒ Create the demo cluster
91,sudo -u postgres /usr/lib/postgresql/15/bin/initdb \
91,-D /var/lib/postgresql/15/demo -k -A peer
91,sudo pg_createcluster 15 demo
91,"Configuring already existing cluster (configuration: /etc/postgresql/15/demo, data: /var/lib/postgresql/15/demo, owner: 102:103)"
91,Ver Cluster Port Status Owner
91,Data directory
91,Log file
91,demo
91,5432 down
91,postgres /var/lib/postgresql/15/demo /var/log/postgresql/postgresql-15-demo.log
91,Configure Cluster Stanza
91,"A stanza is the configuration for a PostgreSQL database cluster that defines where it is located, how it will be backed up, archiving options, etc. Most db servers will only have one PostgreSQL database cluster and therefore one stanza, whereas backup servers will have a stanza for every database cluster that needs to be backed up."
91,"It is tempting to name the stanza after the primary cluster but a better name describes the databases contained in the cluster. Because the stanza name will be used for the primary and all replicas it is more appropriate to choose a name that describes the actual function of the cluster, such as app or dw, rather than the local cluster name, such as main or prod."
91,The name 'demo' describes the purpose of this cluster accurately so that will also make a good stanza name.
91,pgBackRest needs to know where the base data directory for the PostgreSQL cluster is located. The path can be requested from PostgreSQL directly but in a recovery scenario the PostgreSQL process will not be available. During backups the value supplied to pgBackRest will be compared against the path that PostgreSQL is running on and they must be equal or the backup will return an error. Make sure that pg-path is exactly equal to data_directory in postgresql.conf.
91,By default Debian/Ubuntu stores clusters in /var/lib/postgresql/[version]/[cluster] so it is easy to determine the correct path for the data directory.
91,"When creating the /etc/pgbackrest/pgbackrest.conf file, the database owner (usually postgres) must be granted read privileges."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure the PostgreSQL cluster data directory
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,pgBackRest configuration files follow the Windows INI convention. Sections are denoted by text in brackets and key/value pairs are contained in each section. Lines beginning with # are ignored and can be used as comments.
91,There are multiple ways the pgBackRest configuration files can be loaded:
91,"config and config-include-path are default: the default config file will be loaded, if it exists, and *.conf files in the default config include path will be appended, if they exist."
91,config option is specified: only the specified config file will be loaded and is expected to exist.
91,"config-include-path is specified: *.conf files in the config include path will be loaded and the path is required to exist. The default config file will be be loaded if it exists. If it is desirable to load only the files in the specified config include path, then the --no-config option can also be passed."
91,"config and config-include-path are specified: using the user-specified values, the config file will be loaded and *.conf files in the config include path will be appended. The files are expected to exist."
91,config-path is specified: this setting will override the base path for the default location of the config file and/or the base path of the default config-include-path setting unless the config and/or config-include-path option is explicitly set.
91,"The files are concatenated as if they were one big file; order doesn't matter, but there is precedence based on sections. The precedence (highest to lowest) is:"
91,[stanza:command]
91,[stanza]
91,[global:command]
91,[global]
91,NOTE:
91,"--config, --config-include-path and --config-path are command-line only options."
91,pgBackRest can also be configured using environment variables as described in the command reference.
91,pg-primary ⇒ Configure log-path using the environment
91,sudo -u postgres bash -c ' \
91,export PGBACKREST_LOG_PATH=/path/set/by/env && \
91,pgbackrest --log-level-console=error help backup log-path'
91,pgBackRest 2.50 - 'backup' command - 'log-path' option help
91,Path where log files are stored.
91,The log path provides a location for pgBackRest to store log files. Note that
91,if log-level-file=off then no log path is required.
91,current: /path/set/by/env
91,default: /var/log/pgbackrest
91,Create the Repository
91,The repository is where pgBackRest stores backups and archives WAL segments.
91,"It may be difficult to estimate in advance how much space you'll need. The best thing to do is take some backups then record the size of different types of backups (full/incr/diff) and measure the amount of WAL generated per day. This will give you a general idea of how much space you'll need, though of course requirements will likely change over time as your database evolves."
91,For this demonstration the repository will be stored on the same host as the PostgreSQL server. This is the simplest configuration and is useful in cases where traditional backup software is employed to backup the database host.
91,pg-primary ⇒ Create the pgBackRest repository
91,sudo mkdir -p /var/lib/pgbackrest
91,sudo chmod 750 /var/lib/pgbackrest
91,sudo chown postgres:postgres /var/lib/pgbackrest
91,The repository path must be configured so pgBackRest knows where to find it.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure the pgBackRest repository path
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-path=/var/lib/pgbackrest
91,Multiple repositories may also be configured. See Multiple Repositories for details.
91,Configure Archiving
91,Backing up a running PostgreSQL cluster requires WAL archiving to be enabled. Note that at least one WAL segment will be created during the backup process even if no explicit writes are made to the cluster.
91,pg-primary:/etc/postgresql/15/demo/postgresql.conf ⇒ Configure archive settings
91,archive_command = 'pgbackrest --stanza=demo archive-push %p'
91,archive_mode = on
91,max_wal_senders = 3
91,wal_level = replica
91,%p is how PostgreSQL specifies the location of the WAL segment to be archived. Setting wal_level to at least replica and increasing max_wal_senders is a good idea even if there are currently no replicas as this will allow them to be added later without restarting the primary cluster.
91,The PostgreSQL cluster must be restarted after making these changes and before performing a backup.
91,pg-primary ⇒ Restart the demo cluster
91,sudo pg_ctlcluster 15 demo restart
91,"When archiving a WAL segment is expected to take more than 60 seconds (the default) to reach the pgBackRest repository, then the pgBackRest archive-timeout option should be increased. Note that this option is not the same as the PostgreSQL archive_timeout option which is used to force a WAL segment switch; useful for databases where there are long periods of inactivity. For more information on the PostgreSQL archive_timeout option, see PostgreSQL Write Ahead Log."
91,"The archive-push command can be configured with its own options. For example, a lower compression level may be set to speed archiving without affecting the compression used for backups."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Config archive-push to use a lower compression level
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-path=/var/lib/pgbackrest
91,[global:archive-push]
91,compress-level=3
91,"This configuration technique can be used for any command and can even target a specific stanza, e.g. demo:archive-push."
91,Configure Retention
91,pgBackRest expires backups based on retention options.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure retention to 2 full backups
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,[global:archive-push]
91,compress-level=3
91,More information about retention can be found in the Retention section.
91,Configure Repository Encryption
91,The repository will be configured with a cipher type and key to demonstrate encryption. Encryption is always performed client-side even if the repository type (e.g. S3 or other object store) supports encryption.
91,"It is important to use a long, random passphrase for the cipher key. A good way to generate one is to run: openssl rand -base64 48."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pgBackRest repository encryption
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,[global:archive-push]
91,compress-level=3
91,"Once the repository has been configured and the stanza created and checked, the repository encryption settings cannot be changed."
91,Create the Stanza
91,The stanza-create command must be run to initialize the stanza. It is recommended that the check command be run after stanza-create to ensure archiving and backups are properly configured.
91,pg-primary ⇒ Create the stanza and check the configuration
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info stanza-create
91,P00
91,INFO: stanza-create command begin 2.50: --exec-id=434-aa3b987e --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --stanza=demo
91,P00
91,INFO: stanza-create for stanza 'demo' on repo1
91,P00
91,INFO: stanza-create command end: completed successfully
91,Check the Configuration
91,"The check command validates that pgBackRest and the archive_command setting are configured correctly for archiving and backups for the specified stanza. It will attempt to check all repositories and databases that are configured for the host on which the command is run. It detects misconfigurations, particularly in archiving, that result in incomplete backups because required WAL segments did not reach the archive. The command can be run on the PostgreSQL or repository host. The command may also be run on the standby host, however, since pg_switch_xlog()/pg_switch_wal() cannot be performed on the standby, the command will only test the repository configuration."
91,Note that pg_create_restore_point('pgBackRest Archive Check') and pg_switch_xlog()/pg_switch_wal() are called to force PostgreSQL to archive a WAL segment.
91,pg-primary ⇒ Check the configuration
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info check
91,P00
91,INFO: check command begin 2.50: --exec-id=443-81758f35 --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --stanza=demo
91,P00
91,INFO: check repo1 configuration (primary)
91,P00
91,INFO: check repo1 archive for WAL (primary)
91,P00
91,INFO: WAL segment 000000010000000000000001 successfully archived to '/var/lib/pgbackrest/archive/demo/15-1/0000000100000000/000000010000000000000001-fdad8197c7146055eb078892124b63632ec2523b.gz' on repo1
91,P00
91,INFO: check command end: completed successfully
91,Perform a Backup
91,"By default pgBackRest will wait for the next regularly scheduled checkpoint before starting a backup. Depending on the checkpoint_timeout and checkpoint_segments settings in PostgreSQL it may be quite some time before a checkpoint completes and the backup can begin. Generally, it is best to set start-fast=y so that the backup starts immediately. This forces a checkpoint, but since backups are usually run once a day an additional checkpoint should not have a noticeable impact on performance. However, on very busy clusters it may be best to pass --start-fast on the command-line as needed."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure backup fast start
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,To perform a backup of the PostgreSQL cluster run pgBackRest with the backup command.
91,pg-primary ⇒ Backup the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo \
91,--log-level-console=info backup
91,P00
91,INFO: backup command begin 2.50: --exec-id=472-0317e7cc --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-full=2 --stanza=demo --start-fast
91,P00
91,"WARN: no prior backup exists, incr backup has been changed to full"
91,P00
91,INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes
91,P00
91,"INFO: backup start archive = 000000010000000000000002, lsn = 0/2000028"
91,[filtered 3 lines of output]
91,P00
91,INFO: check archive for segment(s) 000000010000000000000002:000000010000000000000003
91,P00
91,INFO: new backup label = 20240122-113304F
91,P00
91,"INFO: full backup size = 21.8MB, file total = 961"
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=472-0317e7cc --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-full=2 --stanza=demo
91,"By default pgBackRest will attempt to perform an incremental backup. However, an incremental backup must be based on a full backup and since no full backup existed pgBackRest ran a full backup instead."
91,The type option can be used to specify a full or differential backup.
91,pg-primary ⇒ Differential backup of the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo --type=diff \
91,--log-level-console=info backup
91,[filtered 7 lines of output]
91,P00
91,INFO: check archive for segment(s) 000000010000000000000004:000000010000000000000005
91,P00
91,INFO: new backup label = 20240122-113304F_20240122-113308D
91,P00
91,"INFO: diff backup size = 8.3KB, file total = 961"
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=498-1562f2d9 --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-full=2 --stanza=demo
91,"This time there was no warning because a full backup already existed. While incremental backups can be based on a full or differential backup, differential backups must be based on a full backup. A full backup can be performed by running the backup command with --type=full."
91,During an online backup pgBackRest waits for WAL segments that are required for backup consistency to be archived. This wait time is governed by the pgBackRest archive-timeout option which defaults to 60 seconds. If archiving an individual segment is known to take longer then this option should be increased.
91,Schedule a Backup
91,Backups can be scheduled with utilities such as cron.
91,"In the following example, two cron jobs are configured to run; full backups are scheduled for 6:30 AM every Sunday with differential backups scheduled for 6:30 AM Monday through Saturday. If this crontab is installed for the first time mid-week, then pgBackRest will run a full backup the first time the differential job is executed, followed the next day by a differential backup."
91,#m h
91,dom mon dow
91,command
91,30 06
91,pgbackrest --type=full --stanza=demo backup
91,30 06
91,1-6
91,pgbackrest --type=diff --stanza=demo backup
91,"Once backups are scheduled it's important to configure retention so backups are expired on a regular schedule, see Retention."
91,Backup Information
91,Use the info command to get information about backups.
91,pg-primary ⇒ Get info for the demo cluster
91,sudo -u postgres pgbackrest info
91,stanza: demo
91,status: ok
91,cipher: aes-256-cbc
91,db (current)
91,wal archive min/max (15): 000000010000000000000001/000000010000000000000005
91,full backup: 20240122-113304F
91,timestamp start/stop: 2024-01-22 11:33:04+00 / 2024-01-22 11:33:07+00
91,wal start/stop: 000000010000000000000002 / 000000010000000000000003
91,"database size: 21.8MB, database backup size: 21.8MB"
91,"repo1: backup set size: 2.9MB, backup size: 2.9MB"
91,diff backup: 20240122-113304F_20240122-113308D
91,timestamp start/stop: 2024-01-22 11:33:08+00 / 2024-01-22 11:33:10+00
91,wal start/stop: 000000010000000000000004 / 000000010000000000000005
91,"database size: 21.8MB, database backup size: 8.3KB"
91,"repo1: backup set size: 2.9MB, backup size: 496B"
91,backup reference list: 20240122-113304F
91,The info command operates on a single stanza or all stanzas. Text output is the default and gives a human-readable summary of backups for the stanza(s) requested. This format is subject to change with any release.
91,For machine-readable output use --output=json. The JSON output contains far more information than the text output and is kept stable unless a bug is found.
91,"Each stanza has a separate section and it is possible to limit output to a single stanza with the --stanza option. The stanza 'status' gives a brief indication of the stanza's health. If this is 'ok' then pgBackRest is functioning normally. If there are multiple repositories, then a status of 'mixed' indicates that the stanza is not in a healthy state on one or more of the repositories; in this case the state of the stanza will be detailed per repository. For cases in which an error on a repository occurred that is not one of the known error codes, then an error code of 'other' will be used and the full error details will be provided. The 'wal archive min/max' shows the minimum and maximum WAL currently stored in the archive and, in the case of multiple repositories, will be reported across all repositories unless the --repo option is set. Note that there may be gaps due to archive retention policies or other reasons."
91,The 'backup/expire running' message will appear beside the 'status' information if one of those commands is currently running on the host.
91,"The backups are displayed oldest to newest. The oldest backup will always be a full backup (indicated by an F at the end of the label) but the newest backup can be full, differential (ends with D), or incremental (ends with I)."
91,The 'timestamp start/stop' defines the time period when the backup ran. The 'timestamp stop' can be used to determine the backup to use when performing Point-In-Time Recovery. More information about Point-In-Time Recovery can be found in the Point-In-Time Recovery section.
91,The 'wal start/stop' defines the WAL range that is required to make the database consistent when restoring. The backup command will ensure that this WAL range is in the archive before completing.
91,The 'database size' is the full uncompressed size of the database while 'database backup size' is the amount of data in the database to actually back up (these will be the same for full backups).
91,The 'repo' indicates in which repository this backup resides. The 'backup set size' includes all the files from this backup and any referenced backups in the repository that are required to restore the database from this backup while 'backup size' includes only the files in this backup (these will also be the same for full backups). Repository sizes reflect compressed file sizes if compression is enabled in pgBackRest.
91,The 'backup reference list' contains the additional backups that are required to restore this backup.
91,Restore a Backup
91,"Backups can protect you from a number of disaster scenarios, the most common of which are hardware failure and data corruption. The easiest way to simulate data corruption is to remove an important PostgreSQL cluster file."
91,pg-primary ⇒ Stop the demo cluster and delete the pg_control file
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres rm /var/lib/postgresql/15/demo/global/pg_control
91,Starting the cluster without this important file will result in an error.
91,pg-primary ⇒ Attempt to start the corrupted demo cluster
91,sudo pg_ctlcluster 15 demo start
91,Error: /usr/lib/postgresql/15/bin/pg_ctl /usr/lib/postgresql/15/bin/pg_ctl start -D /var/lib/postgresql/15/demo -l /var/log/postgresql/postgresql-15-demo.log -s -o
91,"-c config_file=""/etc/postgresql/15/demo/postgresql.conf"""
91,exited with status 1:
91,postgres: could not find the database system
91,"Expected to find it in the directory ""/var/lib/postgresql/15/demo"","
91,"but could not open file ""/var/lib/postgresql/15/demo/global/pg_control"": No such file or directory"
91,Examine the log output.
91,To restore a backup of the PostgreSQL cluster run pgBackRest with the restore command. The cluster needs to be stopped (in this case it is already stopped) and all files must be removed from the PostgreSQL data directory.
91,pg-primary ⇒ Remove old files from demo cluster
91,sudo -u postgres find /var/lib/postgresql/15/demo -mindepth 1 -delete
91,pg-primary ⇒ Restore the demo cluster and start PostgreSQL
91,sudo -u postgres pgbackrest --stanza=demo restore
91,sudo pg_ctlcluster 15 demo start
91,This time the cluster started successfully since the restore replaced the missing pg_control file.
91,More information about the restore command can be found in the Restore section.
91,Monitoring
91,Monitoring is an important part of any production system. There are many tools available and pgBackRest can be monitored on any of them with a little work.
91,pgBackRest can output information about the repository in JSON format which includes a list of all backups for each stanza and WAL archive info.
91,In PostgreSQL
91,The PostgreSQL COPY command allows pgBackRest info to be loaded into a table. The following example wraps that logic in a function that can be used to perform real-time queries.
91,pg-primary ⇒ Load pgBackRest info function for PostgreSQL
91,sudo -u postgres cat \
91,/var/lib/postgresql/pgbackrest/doc/example/pgsql-pgbackrest-info.sql
91,-- An example of monitoring pgBackRest from within PostgreSQL
91,-- Use copy to export data from the pgBackRest info command into the jsonb
91,-- type so it can be queried directly by PostgreSQL.
91,-- Create monitor schema
91,create schema monitor;
91,-- Get pgBackRest info in JSON format
91,create function monitor.pgbackrest_info()
91,returns jsonb AS $$
91,declare
91,data jsonb;
91,begin
91,-- Create a temp table to hold the JSON data
91,create temp table temp_pgbackrest_data (data text);
91,-- Copy data into the table directly from the pgBackRest info command
91,copy temp_pgbackrest_data (data)
91,from program
91,'pgbackrest --output=json info' (format text);
91,"select replace(temp_pgbackrest_data.data, E'\n', '\n')::jsonb"
91,into data
91,from temp_pgbackrest_data;
91,drop table temp_pgbackrest_data;
91,return data;
91,end $$ language plpgsql;
91,sudo -u postgres psql -f \
91,/var/lib/postgresql/pgbackrest/doc/example/pgsql-pgbackrest-info.sql
91,Now the monitor.pgbackrest_info() function can be used to determine the last successful backup time and archived WAL for a stanza.
91,pg-primary ⇒ Query last successful backup time and archived WAL
91,sudo -u postgres cat \
91,/var/lib/postgresql/pgbackrest/doc/example/pgsql-pgbackrest-query.sql
91,-- Get last successful backup for each stanza
91,-- Requires the monitor.pgbackrest_info function.
91,with stanza as
91,"select data->'name' as name,"
91,data->'backup'->(
91,"jsonb_array_length(data->'backup') - 1) as last_backup,"
91,data->'archive'->(
91,jsonb_array_length(data->'archive') - 1) as current_archive
91,from jsonb_array_elements(monitor.pgbackrest_info()) as data
91,"select name,"
91,to_timestamp(
91,"(last_backup->'timestamp'->>'stop')::numeric) as last_successful_backup,"
91,current_archive->>'max' as last_archived_wal
91,from stanza;
91,sudo -u postgres psql -f \
91,/var/lib/postgresql/pgbackrest/doc/example/pgsql-pgbackrest-query.sql
91,name
91,| last_successful_backup |
91,last_archived_wal
91,--------+------------------------+--------------------------
91,"""demo"" | 2024-01-22 11:33:10+00 | 000000010000000000000006"
91,(1 row)
91,Using jq
91,jq is a command-line utility that can easily extract data from JSON.
91,pg-primary ⇒ Install jq utility
91,sudo apt-get install jq
91,Now jq can be used to query the last successful backup time for a stanza.
91,pg-primary ⇒ Query last successful backup time
91,sudo -u postgres pgbackrest --output=json --stanza=demo info | \
91,jq '.[0] | .backup[-1] | .timestamp.stop'
91,1705923190
91,Or the last archived WAL.
91,pg-primary ⇒ Query last archived WAL
91,sudo -u postgres pgbackrest --output=json --stanza=demo info | \
91,jq '.[0] | .archive[-1] | .max'
91,"""000000010000000000000006"""
91,NOTE:
91,This syntax requires jq v1.5.
91,NOTE:
91,jq may round large numbers such as system identifiers. Test your queries carefully.
91,Backup
91,"When multiple repositories are configured, pgBackRest will backup to the highest priority repository (e.g. repo1) unless the --repo option is specified."
91,pgBackRest does not have a built-in scheduler so it's best to run it from cron or some other scheduling mechanism.
91,See Perform a Backup for more details and examples.
91,File Bundling
91,Bundling files together in the repository saves time during the backup and some space in the repository. This is especially pronounced when the repository is stored on an object store such as S3. Per-file creation time on object stores is higher and very small files might cost as much to store as larger files.
91,The file bundling feature is enabled with the repo-bundle option.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-bundle
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,"A full backup without file bundling will have 1000+ files in the backup path, but with bundling the total number of files is greatly reduced. An additional benefit is that zero-length files are not stored (except in the manifest), whereas in a normal backup each zero-length file is stored individually."
91,pg-primary ⇒ Perform a full backup
91,sudo -u postgres pgbackrest --stanza=demo --type=full backup
91,pg-primary ⇒ Check file total
91,sudo -u postgres find /var/lib/pgbackrest/backup/demo/latest/ -type f | wc -l
91,"The repo-bundle-size and repo-bundle-limit options can be used for tuning, though the defaults should be optimal in most cases."
91,"While file bundling is generally more efficient, the downside is that it is more difficult to manually retrieve files from the repository. It may not be ideal for deduplicated storage since each full backup will arrange files in the bundles differently. Lastly, file bundles cannot be resumed, so be careful not to set repo-bundle-size too high."
91,Block Incremental
91,Block incremental backups save space by only storing the parts of a file that have changed since the prior backup rather than storing the entire file.
91,The block incremental feature is enabled with the repo-block option and it works best when enabled for all backup types. File bundling must also be enabled.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-block
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,Backup Annotations
91,Users can attach informative key/value pairs to the backup. This option may be used multiple times to attach multiple annotations.
91,pg-primary ⇒ Perform a full backup with annotations
91,"sudo -u postgres pgbackrest --stanza=demo --annotation=source=""demo backup"" \"
91,--annotation=key=value --type=full backup
91,Annotations are output by the info command text output when a backup is specified with --set and always appear in the JSON output.
91,pg-primary ⇒ Get info for the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo --set=20240122-113324F info
91,stanza: demo
91,status: ok
91,cipher: aes-256-cbc
91,db (current)
91,wal archive min/max (15): 000000020000000000000008/00000002000000000000000A
91,full backup: 20240122-113324F
91,timestamp start/stop: 2024-01-22 11:33:24+00 / 2024-01-22 11:33:27+00
91,wal start/stop: 000000020000000000000009 / 00000002000000000000000A
91,lsn start/stop: 0/9000028 / 0/A000050
91,"database size: 21.8MB, database backup size: 21.8MB"
91,repo1: backup size: 2.9MB
91,database list: postgres (5)
91,annotation(s)
91,key: value
91,source: demo backup
91,"Annotations included with the backup command can be added, modified, or removed afterwards using the annotate command."
91,pg-primary ⇒ Change backup annotations
91,sudo -u postgres pgbackrest --stanza=demo --set=20240122-113324F \
91,--annotation=key= --annotation=new_key=new_value annotate
91,sudo -u postgres pgbackrest --stanza=demo --set=20240122-113324F info
91,stanza: demo
91,status: ok
91,cipher: aes-256-cbc
91,db (current)
91,wal archive min/max (15): 000000020000000000000008/00000002000000000000000A
91,full backup: 20240122-113324F
91,timestamp start/stop: 2024-01-22 11:33:24+00 / 2024-01-22 11:33:27+00
91,wal start/stop: 000000020000000000000009 / 00000002000000000000000A
91,lsn start/stop: 0/9000028 / 0/A000050
91,"database size: 21.8MB, database backup size: 21.8MB"
91,repo1: backup size: 2.9MB
91,database list: postgres (5)
91,annotation(s)
91,new_key: new_value
91,source: demo backup
91,Retention
91,"Generally it is best to retain as many backups as possible to provide a greater window for Point-in-Time Recovery, but practical concerns such as disk space must also be considered. Retention options remove older backups once they are no longer needed."
91,"pgBackRest does full backup rotation based on the retention type which can be a count or a time period. When a count is specified, then expiration is not concerned with when the backups were created but with how many must be retained. Differential and Incremental backups are count-based but will always be expired when the backup they depend on is expired. See sections Full Backup Retention and Differential Backup Retention for details and examples. Archived WAL is retained by default for backups that have not expired, however, although not recommended, this schedule can be modified per repository with the retention-archive options. See section Archive Retention for details and examples."
91,"The expire command is run automatically after each successful backup and can also be run by the user. When run by the user, expiration will occur as defined by the retention settings for each configured repository. If the --repo option is provided, expiration will occur only on the specified repository. Expiration can also be limited by the user to a specific backup set with the --set option and, unless the --repo option is specified, all repositories will be searched and any matching the set criteria will be expired. It should be noted that the archive retention schedule will be checked and performed any time the expire command is run."
91,Full Backup Retention
91,"The repo1-retention-full-type determines how the option repo1-retention-full is interpreted; either as the count of full backups to be retained or how many days to retain full backups. New backups must be completed before expiration will occur — that means if repo1-retention-full-type=count and repo1-retention-full=2 then there will be three full backups stored before the oldest one is expired, or if repo1-retention-full-type=time and repo1-retention-full=20 then there must be one full backup that is at least 20 days old before expiration can occur."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-retention-full
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,Backup repo1-retention-full=2 but currently there is only one full backup so the next full backup to run will not expire any full backups.
91,pg-primary ⇒ Perform a full backup
91,sudo -u postgres pgbackrest --stanza=demo --type=full \
91,--log-level-console=detail backup
91,[filtered 973 lines of output]
91,P00
91,INFO: repo1: remove expired backup 20240122-113321F
91,"P00 DETAIL: repo1: 15-1 archive retention on backup 20240122-113324F, start = 000000020000000000000009"
91,P00
91,"INFO: repo1: 15-1 remove archive, start = 000000020000000000000008, stop = 000000020000000000000008"
91,P00
91,INFO: expire command end: completed successfully
91,Archive is expired because WAL segments were generated before the oldest backup. These are not useful for recovery — only WAL segments generated after a backup can be used to recover that backup.
91,pg-primary ⇒ Perform a full backup
91,sudo -u postgres pgbackrest --stanza=demo --type=full \
91,--log-level-console=info backup
91,[filtered 11 lines of output]
91,P00
91,INFO: repo1: expire full backup 20240122-113324F
91,P00
91,INFO: repo1: remove expired backup 20240122-113324F
91,P00
91,"INFO: repo1: 15-1 remove archive, start = 000000020000000000000009, stop = 00000002000000000000000B"
91,P00
91,INFO: expire command end: completed successfully
91,The 20240122-113304F full backup is expired and archive retention is based on the 20240122-113328F which is now the oldest full backup.
91,Differential Backup Retention
91,Set repo1-retention-diff to the number of differential backups required. Differentials only rely on the prior full backup so it is possible to create a rolling set of differentials for the last day or more. This allows quick restores to recent points-in-time but reduces overall space consumption.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-retention-diff
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=1
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,Backup repo1-retention-diff=1 so two differentials will need to be performed before one is expired. An incremental backup is added to demonstrate incremental expiration. Incremental backups cannot be expired independently — they are always expired with their related full or differential backup.
91,pg-primary ⇒ Perform differential and incremental backups
91,sudo -u postgres pgbackrest --stanza=demo --type=diff backup
91,sudo -u postgres pgbackrest --stanza=demo --type=incr backup
91,Now performing a differential backup will expire the previous differential and incremental backups leaving only one differential backup.
91,pg-primary ⇒ Perform a differential backup
91,sudo -u postgres pgbackrest --stanza=demo --type=diff \
91,--log-level-console=info backup
91,[filtered 10 lines of output]
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=990-85c13593 --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-diff=1 --repo1-retention-full=2 --stanza=demo
91,P00
91,"INFO: repo1: expire diff backup set 20240122-113332F_20240122-113335D, 20240122-113332F_20240122-113337I"
91,P00
91,INFO: repo1: remove expired backup 20240122-113332F_20240122-113337I
91,P00
91,INFO: repo1: remove expired backup 20240122-113332F_20240122-113335D
91,P00
91,INFO: expire command end: completed successfully
91,Archive Retention
91,"Although pgBackRest automatically removes archived WAL segments when expiring backups (the default expires WAL for full backups based on the repo1-retention-full option), it may be useful to expire archive more aggressively to save disk space. Note that full backups are treated as differential backups for the purpose of differential archive retention."
91,"Expiring archive will never remove WAL segments that are required to make a backup consistent. However, since Point-in-Time-Recovery (PITR) only works on a continuous WAL stream, care should be taken when aggressively expiring archive outside of the normal backup expiration process. To determine what will be expired without actually expiring anything, the dry-run option can be provided on the command line with the expire command."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-retention-diff
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=2
91,repo1-retention-full=2
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,pg-primary ⇒ Perform differential backup
91,sudo -u postgres pgbackrest --stanza=demo --type=diff \
91,--log-level-console=info backup
91,[filtered 6 lines of output]
91,P00
91,"INFO: backup stop archive = 000000020000000000000018, lsn = 0/18000050"
91,P00
91,INFO: check archive for segment(s) 000000020000000000000017:000000020000000000000018
91,P00
91,INFO: new backup label = 20240122-113332F_20240122-113343D
91,P00
91,"INFO: diff backup size = 8.3KB, file total = 961"
91,P00
91,INFO: backup command end: completed successfully
91,[filtered 2 lines of output]
91,pg-primary ⇒ Expire archive
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=detail \
91,--repo1-retention-archive-type=diff --repo1-retention-archive=1 expire
91,P00
91,INFO: expire command begin 2.50: --exec-id=1074-531cbf12 --log-level-console=detail --log-level-stderr=off --no-log-timestamp --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo1-retention-archive=1 --repo1-retention-archive-type=diff --repo1-retention-diff=2 --repo1-retention-full=2 --stanza=demo
91,"P00 DETAIL: repo1: 15-1 archive retention on backup 20240122-113328F, start = 00000002000000000000000C, stop = 00000002000000000000000C"
91,"P00 DETAIL: repo1: 15-1 archive retention on backup 20240122-113332F, start = 00000002000000000000000D, stop = 00000002000000000000000E"
91,"P00 DETAIL: repo1: 15-1 archive retention on backup 20240122-113332F_20240122-113340D, start = 000000020000000000000013, stop = 000000020000000000000014"
91,"P00 DETAIL: repo1: 15-1 archive retention on backup 20240122-113332F_20240122-113343D, start = 000000020000000000000017"
91,P00
91,"INFO: repo1: 15-1 remove archive, start = 00000002000000000000000F, stop = 000000020000000000000012"
91,P00
91,"INFO: repo1: 15-1 remove archive, start = 000000020000000000000015, stop = 000000020000000000000016"
91,P00
91,INFO: expire command end: completed successfully
91,The 20240122-113332F_20240122-113340D differential backup has archived WAL segments that must be retained to make the older backups consistent even though they cannot be played any further forward with PITR. WAL segments generated after 20240122-113332F_20240122-113340D but before 20240122-113332F_20240122-113343D are removed. WAL segments generated after the new backup 20240122-113332F_20240122-113343D remain and can be used for PITR.
91,"Since full backups are considered differential backups for the purpose of differential archive retention, if a full backup is now performed with the same settings, only the archive for that full backup is retained for PITR."
91,Restore
91,"The restore command automatically defaults to selecting the latest backup from the first repository where backups exist (see Quick Start - Restore a Backup). The order in which the repositories are checked is dictated by the pgbackrest.conf (e.g. repo1 will be checked before repo2). To select from a specific repository, the --repo option can be passed (e.g. --repo=1). The --set option can be passed if a backup other than the latest is desired."
91,"When PITR of --type=time or --type=lsn is specified, then the target time or target lsn must be specified with the --target option. If a backup is not specified via the --set option, then the configured repositories will be checked, in order, for a backup that contains the requested time or lsn. If no matching backup is found, the latest backup from the first repository containing backups will be used for --type=time while no backup will be selected for --type=lsn. For other types of PITR, e.g. xid, the --set option must be provided if the target is prior to the latest backup. See Point-in-Time Recovery for more details and examples."
91,Replication slots are not included per recommendation of PostgreSQL. See Backing Up The Data Directory in the PostgreSQL documentation for more information.
91,The following sections introduce additional restore command features.
91,File Ownership
91,If a restore is run as a non-root user (the typical scenario) then all files restored will belong to the user/group executing pgBackRest. If existing files are not owned by the executing user/group then an error will result if the ownership cannot be updated to the executing user/group. In that case the file ownership will need to be updated by a privileged user before the restore can be retried.
91,If a restore is run as the root user then pgBackRest will attempt to recreate the ownership recorded in the manifest when the backup was made. Only user/group names are stored in the manifest so the same names must exist on the restore host for this to work. If the user/group name cannot be found locally then the user/group of the PostgreSQL data directory will be used and finally root if the data directory user/group cannot be mapped to a name.
91,Delta Option
91,"Restore a Backup in Quick Start required the database cluster directory to be cleaned before the restore could be performed. The delta option allows pgBackRest to automatically determine which files in the database cluster directory can be preserved and which ones need to be restored from the backup — it also removes files not present in the backup manifest so it will dispose of divergent changes. This is accomplished by calculating a SHA-1 cryptographic hash for each file in the database cluster directory. If the SHA-1 hash does not match the hash stored in the backup then that file will be restored. This operation is very efficient when combined with the process-max option. Since the PostgreSQL server is shut down during the restore, a larger number of processes can be used than might be desirable during a backup when the PostgreSQL server is running."
91,"pg-primary ⇒ Stop the demo cluster, perform delta restore"
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres pgbackrest --stanza=demo --delta \
91,--log-level-console=detail restore
91,[filtered 2 lines of output]
91,P00 DETAIL: check '/var/lib/postgresql/15/demo' exists
91,P00 DETAIL: remove 'global/pg_control' so cluster will not start if restore does not complete
91,P00
91,INFO: remove invalid files/links/paths from '/var/lib/postgresql/15/demo'
91,P00 DETAIL: remove invalid file '/var/lib/postgresql/15/demo/backup_label.old'
91,P00 DETAIL: remove invalid file '/var/lib/postgresql/15/demo/base/1/pg_internal.init'
91,[filtered 14 lines of output]
91,"P01 DETAIL: restore file /var/lib/postgresql/15/demo/backup_label (260B, 0.00%) checksum d4a881d118f904df01bf317250a038915dd36bae"
91,"P01 DETAIL: restore file /var/lib/postgresql/15/demo/pg_multixact/members/0000 - exists and matches backup (bundle 20240122-113332F/1/0, 8KB, 0.04%) checksum 0631457264ff7f8d5fb1edc2c0211992a67c73e6"
91,"P01 DETAIL: restore file /var/lib/postgresql/15/demo/PG_VERSION - exists and matches backup (bundle 20240122-113332F/1/40, 3B, 0.04%) checksum 587b596f04f7db9c2cad3d6b87dd2b3a05de4f35"
91,"P01 DETAIL: restore file /var/lib/postgresql/15/demo/global/pg_filenode.map - exists and matches backup (bundle 20240122-113332F/1/64, 512B, 0.04%) checksum 8426f71eec225fb3087aa80427d8e6b4e6a8a65b"
91,"P01 DETAIL: restore file /var/lib/postgresql/15/demo/global/6247 - exists and matches backup (bundle 20240122-113332F/1/232, 8KB, 0.07%) checksum ea40c8171261ed36b40f1597297f0a111790313c"
91,[filtered 985 lines of output]
91,pg-primary ⇒ Restart PostgreSQL
91,sudo pg_ctlcluster 15 demo start
91,Restore Selected Databases
91,There may be cases where it is desirable to selectively restore specific databases from a cluster backup. This could be done for performance reasons or to move selected databases to a machine that does not have enough space to restore the entire cluster backup.
91,To demonstrate this feature two databases are created: test1 and test2.
91,pg-primary ⇒ Create two test databases
91,"sudo -u postgres psql -c ""create database test1;"""
91,CREATE DATABASE
91,"sudo -u postgres psql -c ""create database test2;"""
91,CREATE DATABASE
91,Each test database will be seeded with tables and data to demonstrate that recovery works with selective restore.
91,pg-primary ⇒ Create a test table in each database
91,"sudo -u postgres psql -c ""create table test1_table (id int); \"
91,"insert into test1_table (id) values (1);"" test1"
91,CREATE TABLE
91,INSERT 0 1
91,"sudo -u postgres psql -c ""create table test2_table (id int); \"
91,"insert into test2_table (id) values (2);"" test2"
91,CREATE TABLE
91,INSERT 0 1
91,A fresh backup is run so pgBackRest is aware of the new databases.
91,pg-primary ⇒ Perform a backup
91,sudo -u postgres pgbackrest --stanza=demo --type=incr backup
91,One of the main reasons to use selective restore is to save space. The size of the test1 database is shown here so it can be compared with the disk utilization after a selective restore.
91,pg-primary ⇒ Show space used by test1 database
91,sudo -u postgres du -sh /var/lib/postgresql/15/demo/base/32768
91,7.3M	/var/lib/postgresql/15/demo/base/32768
91,"If the database to restore is not known, use the info command set option to discover databases that are part of the backup set."
91,pg-primary ⇒ Show database list for backup
91,sudo -u postgres pgbackrest --stanza=demo \
91,--set=20240122-113332F_20240122-113352I info
91,[filtered 12 lines of output]
91,repo1: backup size: 2.0MB
91,"backup reference list: 20240122-113332F, 20240122-113332F_20240122-113343D"
91,"database list: postgres (5), test1 (32768), test2 (32769)"
91,"Stop the cluster and restore only the test2 database. Built-in databases (template0, template1, and postgres) are always restored."
91,WARNING:
91,Recovery may error unless --type=immediate is specified. This is because after consistency is reached PostgreSQL will flag zeroed pages as errors even for a full-page write. For PostgreSQL ≥ 13 the ignore_invalid_pages setting may be used to ignore invalid pages. In this case it is important to check the logs after recovery to ensure that no invalid pages were reported in the selected databases.
91,pg-primary ⇒ Restore from last backup including only the test2 database
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres pgbackrest --stanza=demo --delta \
91,--db-include=test2 --type=immediate --target-action=promote restore
91,sudo pg_ctlcluster 15 demo start
91,Once recovery is complete the test2 database will contain all previously created tables and data.
91,pg-primary ⇒ Demonstrate that the test2 database was recovered
91,"sudo -u postgres psql -c ""select * from test2_table;"" test2"
91,----
91,(1 row)
91,"The test1 database, despite successful recovery, is not accessible. This is because the entire database was restored as sparse, zeroed files. PostgreSQL can successfully apply WAL on the zeroed files but the database as a whole will not be valid because key files contain no data. This is purposeful to prevent the database from being accidentally used when it might contain partial data that was applied during WAL replay."
91,pg-primary ⇒ Attempting to connect to the test1 database will produce an error
91,"sudo -u postgres psql -c ""select * from test1_table;"" test1"
91,"psql: error: connection to server on socket ""/var/run/postgresql/.s.PGSQL.5432"" failed: FATAL:"
91,"relation mapping file ""base/32768/pg_filenode.map"" contains invalid data"
91,"Since the test1 database is restored with sparse, zeroed files it will only require as much space as the amount of WAL that is written during recovery. While the amount of WAL generated during a backup and applied during recovery can be significant it will generally be a small fraction of the total database size, especially for large databases where this feature is most likely to be useful."
91,It is clear that the test1 database uses far less disk space during the selective restore than it would have if the entire database had been restored.
91,pg-primary ⇒ Show space used by test1 database after recovery
91,sudo -u postgres du -sh /var/lib/postgresql/15/demo/base/32768
91,8.0K	/var/lib/postgresql/15/demo/base/32768
91,At this point the only action that can be taken on the invalid test1 database is drop database. pgBackRest does not automatically drop the database since this cannot be done until recovery is complete and the cluster is accessible.
91,pg-primary ⇒ Drop the test1 database
91,"sudo -u postgres psql -c ""drop database test1;"""
91,DROP DATABASE
91,Now that the invalid test1 database has been dropped only the test2 and built-in databases remain.
91,pg-primary ⇒ List remaining databases
91,"sudo -u postgres psql -c ""select oid, datname from pg_database order by oid;"""
91,oid
91,datname
91,-------+-----------
91,1 | template1
91,4 | template0
91,5 | postgres
91,32769 | test2
91,(4 rows)
91,Point-in-Time Recovery
91,"Restore a Backup in Quick Start performed default recovery, which is to play all the way to the end of the WAL stream. In the case of a hardware failure this is usually the best choice but for data corruption scenarios (whether machine or human in origin) Point-in-Time Recovery (PITR) is often more appropriate."
91,"Point-in-Time Recovery (PITR) allows the WAL to be played from a backup to a specified lsn, time, transaction id, or recovery point. For common recovery scenarios time-based recovery is arguably the most useful. A typical recovery scenario is to restore a table that was accidentally dropped or data that was accidentally deleted. Recovering a dropped table is more dramatic so that's the example given here but deleted data would be recovered in exactly the same way."
91,pg-primary ⇒ Create a table with very important data
91,"sudo -u postgres psql -c ""begin; \"
91,create table important_table (message text); \
91,insert into important_table values ('Important Data'); \
91,commit; \
91,"select * from important_table;"""
91,[filtered 4 lines of output]
91,message
91,----------------
91,Important Data
91,(1 row)
91,It is important to represent the time as reckoned by PostgreSQL and to include timezone offsets. This reduces the possibility of unintended timezone conversions and an unexpected recovery result.
91,pg-primary ⇒ Get the time from PostgreSQL
91,"sudo -u postgres psql -Atc ""select current_timestamp"""
91,2024-01-22 11:34:01.874583+00
91,"Now that the time has been recorded the table is dropped. In practice finding the exact time that the table was dropped is a lot harder than in this example. It may not be possible to find the exact time, but some forensic work should be able to get you close."
91,pg-primary ⇒ Drop the important table
91,"sudo -u postgres psql -c ""begin; \"
91,drop table important_table; \
91,commit; \
91,"select * from important_table;"""
91,BEGIN
91,DROP TABLE
91,COMMITERROR:
91,"relation ""important_table"" does not exist"
91,LINE 1: ...le important_table;
91,commit;
91,select * from important_...
91,If the wrong backup is selected for restore then recovery to the required time target will fail. To demonstrate this a new incremental backup is performed where important_table does not exist.
91,pg-primary ⇒ Perform an incremental backup
91,sudo -u postgres pgbackrest --stanza=demo --type=incr backup
91,sudo -u postgres pgbackrest info
91,[filtered 38 lines of output]
91,"backup reference list: 20240122-113332F, 20240122-113332F_20240122-113343D"
91,incr backup: 20240122-113332F_20240122-113403I
91,timestamp start/stop: 2024-01-22 11:34:03+00 / 2024-01-22 11:34:05+00
91,wal start/stop: 00000004000000000000001B / 00000004000000000000001B
91,[filtered 2 lines of output]
91,"It will not be possible to recover the lost table from this backup since PostgreSQL can only play forward, not backward."
91,pg-primary ⇒ Attempt recovery from an incorrect backup
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres pgbackrest --stanza=demo --delta \
91,--set=20240122-113332F_20240122-113403I --target-timeline=current \
91,"--type=time ""--target=2024-01-22 11:34:01.874583+00"" --target-action=promote restore"
91,sudo pg_ctlcluster 15 demo start
91,[filtered 11 lines of output]
91,LOG:
91,database system is ready to accept read-only connections
91,LOG:
91,"redo done at 0/1B000100 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.01 s"
91,FATAL:
91,recovery ended before configured recovery target was reached
91,LOG:
91,startup process (PID 1482) exited with exit code 1
91,LOG:
91,terminating any other active server processes
91,[filtered 3 lines of output]
91,"A reliable method is to allow pgBackRest to automatically select a backup capable of recovery to the time target, i.e. a backup that ended before the specified time."
91,NOTE:
91,pgBackRest cannot automatically select a backup when the restore type is xid or name.
91,pg-primary ⇒ Restore the demo cluster to 2024-01-22 11:34:01.874583+00
91,sudo -u postgres pgbackrest --stanza=demo --delta \
91,"--type=time ""--target=2024-01-22 11:34:01.874583+00"" \"
91,--target-action=promote restore
91,sudo -u postgres cat /var/lib/postgresql/15/demo/postgresql.auto.conf
91,[filtered 9 lines of output]
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:34:07
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,recovery_target_time = '2024-01-22 11:34:01.874583+00'
91,recovery_target_action = 'promote'
91,pgBackRest has generated the recovery settings in postgresql.auto.conf so PostgreSQL can be started immediately. %f is how PostgreSQL specifies the WAL segment it needs and %p is the location where it should be copied. Once PostgreSQL has finished recovery the table will exist again and can be queried.
91,pg-primary ⇒ Start PostgreSQL and check that the important table exists
91,sudo pg_ctlcluster 15 demo start
91,"sudo -u postgres psql -c ""select * from important_table"""
91,message
91,----------------
91,Important Data
91,(1 row)
91,The PostgreSQL log also contains valuable information. It will indicate the time and transaction where the recovery stopped and also give the time of the last transaction to be applied.
91,pg-primary ⇒ Examine the PostgreSQL log output
91,sudo -u postgres cat /var/log/postgresql/postgresql-15-demo.log
91,[filtered 4 lines of output]
91,LOG:
91,database system was interrupted; last known up at 2024-01-22 11:33:52 UTC
91,LOG:
91,"restored log file ""00000004.history"" from archive"
91,LOG:
91,starting point-in-time recovery to 2024-01-22 11:34:01.874583+00
91,LOG:
91,"restored log file ""00000004.history"" from archive"
91,LOG:
91,"restored log file ""00000004000000000000001A"" from archive"
91,[filtered 3 lines of output]
91,LOG:
91,database system is ready to accept read-only connections
91,LOG:
91,"restored log file ""00000004000000000000001B"" from archive"
91,LOG:
91,"recovery stopping before commit of transaction 734, time 2024-01-22 11:34:03.154648+00"
91,LOG:
91,"redo done at 0/1A025398 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.06 s"
91,LOG:
91,last completed transaction was at log time 2024-01-22 11:34:00.598831+00
91,LOG:
91,"restored log file ""00000004000000000000001A"" from archive"
91,LOG:
91,selected new timeline ID: 5
91,[filtered 4 lines of output]
91,Delete a Stanza
91,The stanza-delete command removes data in the repository associated with a stanza.
91,WARNING:Use this command with caution — it will permanently remove all backups and archives from the pgBackRest repository for the specified stanza.To delete a stanza:
91,"Shut down the PostgreSQL cluster associated with the stanza (or use --force to override).Run the stop command on the host where the stanza-delete command will be run.Run the stanza-delete command.Once the command successfully completes, it is the responsibility of the user to remove the stanza from all pgBackRest configuration files and/or environment variables."
91,"A stanza may only be deleted from one repository at a time. To delete the stanza from multiple repositories, repeat the stanza-delete command for each repository while specifying the --repo option."
91,pg-primary ⇒ Stop PostgreSQL cluster to be removed
91,sudo pg_ctlcluster 15 demo stop
91,pg-primary ⇒ Stop pgBackRest for the stanza
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info stop
91,P00
91,INFO: stop command begin 2.50: --exec-id=1613-bc4e6fdd --log-level-console=info --log-level-stderr=off --no-log-timestamp --stanza=demo
91,P00
91,INFO: stop command end: completed successfully
91,pg-primary ⇒ Delete the stanza from one repository
91,sudo -u postgres pgbackrest --stanza=demo --repo=1 \
91,--log-level-console=info stanza-delete
91,P00
91,INFO: stanza-delete command begin 2.50: --exec-id=1621-ab868874 --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo=1 --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --stanza=demo
91,P00
91,INFO: stanza-delete command end: completed successfully
91,Multiple Repositories
91,Multiple repositories may be configured as demonstrated in S3 Support. A potential benefit is the ability to have a local repository for fast restores and a remote repository for redundancy.
91,"Some commands, e.g. stanza-create/stanza-upgrade, will automatically work with all configured repositories while others, e.g. stanza-delete, will require a repository to be specified using the repo option. See the command reference for details on which commands require the repository to be specified."
91,"Note that the repo option is not required when only repo1 is configured in order to maintain backward compatibility. However, the repo option is required when a single repo is configured as, e.g. repo2. This is to prevent command breakage if a new repository is added later."
91,"The archive-push command will always push WAL to the archive in all configured repositories but backups will need to be scheduled individually for each repository. In many cases this is desirable since backup types and retention will vary by repository. Likewise, restores must specify a repository. It is generally better to specify a repository for restores that has low latency/cost even if that means more recovery time. Only restore testing can determine which repository will be most efficient."
91,Azure-Compatible Object Store Support
91,pgBackRest supports locating repositories in Azure-compatible object stores. The container used to store the repository must be created in advance — pgBackRest will not do it automatically. The repository can be located in the container root (/) but it's usually best to place it in a subpath so object store logs or other data can also be stored in the container without conflicts.
91,WARNING:
91,Do not enable hierarchical namespace as this will cause errors during expire.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure Azure
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=4
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=2
91,repo1-retention-full=2
91,repo2-azure-account=pgbackrest
91,repo2-azure-container=demo-container
91,repo2-azure-key=YXpLZXk=
91,repo2-path=/demo-repo
91,repo2-retention-full=4
91,repo2-type=azure
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,Shared access signatures may be used by setting the repo2-azure-key-type option to sas and the repo2-azure-key option to the shared access signature token.
91,Commands are run exactly as if the repository were stored on a local disk.
91,pg-primary ⇒ Create the stanza
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info stanza-create
91,P00
91,INFO: stanza-create command begin 2.50: --exec-id=1696-a7635f0b --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo2-type=azure --stanza=demo
91,P00
91,INFO: stanza-create for stanza 'demo' on repo1
91,P00
91,INFO: stanza-create for stanza 'demo' on repo2
91,P00
91,INFO: stanza-create command end: completed successfully
91,File creation time in object stores is relatively slow so commands benefit by increasing process-max to parallelize file creation.
91,pg-primary ⇒ Backup the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo --repo=2 \
91,--log-level-console=info backup
91,P00
91,INFO: backup command begin 2.50: --exec-id=1705-1a28aa7c --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=4 --repo=2 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-block --repo1-bundle --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo2-type=azure --stanza=demo --start-fast
91,P00
91,"WARN: no prior backup exists, incr backup has been changed to full"
91,P00
91,INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes
91,P00
91,"INFO: backup start archive = 00000005000000000000001C, lsn = 0/1C000028"
91,[filtered 3 lines of output]
91,P00
91,INFO: check archive for segment(s) 00000005000000000000001C:00000005000000000000001C
91,P00
91,INFO: new backup label = 20240122-113417F
91,P00
91,"INFO: full backup size = 29.0MB, file total = 1263"
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=1705-1a28aa7c --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo=2 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo2-type=azure --stanza=demo
91,S3-Compatible Object Store Support
91,pgBackRest supports locating repositories in S3-compatible object stores. The bucket used to store the repository must be created in advance — pgBackRest will not do it automatically. The repository can be located in the bucket root (/) but it's usually best to place it in a subpath so object store logs or other data can also be stored in the bucket without conflicts.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure S3
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=4
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=2
91,repo1-retention-full=2
91,repo2-azure-account=pgbackrest
91,repo2-azure-container=demo-container
91,repo2-azure-key=YXpLZXk=
91,repo2-path=/demo-repo
91,repo2-retention-full=4
91,repo2-type=azure
91,repo3-path=/demo-repo
91,repo3-retention-full=4
91,repo3-s3-bucket=demo-bucket
91,repo3-s3-endpoint=s3.us-east-1.amazonaws.com
91,repo3-s3-key=accessKey1
91,repo3-s3-key-secret=verySecretKey1
91,repo3-s3-region=us-east-1
91,repo3-type=s3
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,NOTE:
91,The region and endpoint will need to be configured to where the bucket is located. The values given here are for the us-east-1 region.
91,"A role should be created to run pgBackRest and the bucket permissions should be set as restrictively as possible. If the role is associated with an instance in AWS then pgBackRest will automatically retrieve temporary credentials when repo3-s3-key-type=auto, which means that keys do not need to be explicitly set in /etc/pgbackrest/pgbackrest.conf."
91,This sample Amazon S3 policy will restrict all reads and writes to the bucket and repository path.
91,"""Version"": ""2012-10-17"","
91,"""Statement"": ["
91,"""Effect"": ""Allow"","
91,"""Action"": ["
91,"""s3:ListBucket"""
91,"""Resource"": ["
91,"""arn:aws:s3:::demo-bucket"""
91,"""Condition"": {"
91,"""StringEquals"": {"
91,"""s3:prefix"": ["
91,""""","
91,"""demo-repo"""
91,"""s3:delimiter"": ["
91,"""/"""
91,"""Effect"": ""Allow"","
91,"""Action"": ["
91,"""s3:ListBucket"""
91,"""Resource"": ["
91,"""arn:aws:s3:::demo-bucket"""
91,"""Condition"": {"
91,"""StringLike"": {"
91,"""s3:prefix"": ["
91,"""demo-repo/*"""
91,"""Effect"": ""Allow"","
91,"""Action"": ["
91,"""s3:PutObject"","
91,"""s3:PutObjectTagging"","
91,"""s3:GetObject"","
91,"""s3:DeleteObject"""
91,"""Resource"": ["
91,"""arn:aws:s3:::demo-bucket/demo-repo/*"""
91,Commands are run exactly as if the repository were stored on a local disk.
91,pg-primary ⇒ Create the stanza
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info stanza-create
91,[filtered 4 lines of output]
91,P00
91,INFO: stanza 'demo' already exists on repo2 and is valid
91,P00
91,INFO: stanza-create for stanza 'demo' on repo3
91,P00
91,INFO: stanza-create command end: completed successfully
91,File creation time in object stores is relatively slow so commands benefit by increasing process-max to parallelize file creation.
91,pg-primary ⇒ Backup the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo --repo=3 \
91,--log-level-console=info backup
91,P00
91,INFO: backup command begin 2.50: --exec-id=1770-10ada33e --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=4 --repo=3 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-block --repo1-bundle --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo3-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo3-retention-full=4 --repo3-s3-bucket=demo-bucket --repo3-s3-endpoint=s3.us-east-1.amazonaws.com --repo3-s3-key= --repo3-s3-key-secret= --repo3-s3-region=us-east-1 --repo2-type=azure --repo3-type=s3 --stanza=demo --start-fast
91,P00
91,"WARN: no prior backup exists, incr backup has been changed to full"
91,P00
91,INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes
91,P00
91,"INFO: backup start archive = 00000005000000000000001D, lsn = 0/1D000028"
91,[filtered 3 lines of output]
91,P00
91,INFO: check archive for segment(s) 00000005000000000000001D:00000005000000000000001E
91,P00
91,INFO: new backup label = 20240122-113425F
91,P00
91,"INFO: full backup size = 29.0MB, file total = 1263"
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=1770-10ada33e --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo=3 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo3-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo3-retention-full=4 --repo3-s3-bucket=demo-bucket --repo3-s3-endpoint=s3.us-east-1.amazonaws.com --repo3-s3-key= --repo3-s3-key-secret= --repo3-s3-region=us-east-1 --repo2-type=azure --repo3-type=s3 --stanza=demo
91,SFTP Support
91,pgBackRest supports locating repositories on SFTP hosts. SFTP file transfer is relatively slow so commands benefit by increasing process-max to parallelize file transfer.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure SFTP
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=4
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=2
91,repo1-retention-full=2
91,repo2-azure-account=pgbackrest
91,repo2-azure-container=demo-container
91,repo2-azure-key=YXpLZXk=
91,repo2-path=/demo-repo
91,repo2-retention-full=4
91,repo2-type=azure
91,repo3-path=/demo-repo
91,repo3-retention-full=4
91,repo3-s3-bucket=demo-bucket
91,repo3-s3-endpoint=s3.us-east-1.amazonaws.com
91,repo3-s3-key=accessKey1
91,repo3-s3-key-secret=verySecretKey1
91,repo3-s3-region=us-east-1
91,repo3-type=s3
91,repo4-bundle=y
91,repo4-path=/demo-repo
91,repo4-sftp-host=sftp-server
91,repo4-sftp-host-key-hash-type=sha1
91,repo4-sftp-host-user=pgbackrest
91,repo4-sftp-private-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp
91,repo4-sftp-public-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp.pub
91,repo4-type=sftp
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,"When utilizing SFTP, if libssh2 is compiled against OpenSSH then repo4-sftp-public-key-file is optional."
91,pg-primary ⇒ Generate SSH keypair for SFTP backup
91,sudo -u postgres mkdir -m 750 -p /var/lib/postgresql/.ssh
91,sudo -u postgres ssh-keygen -f /var/lib/postgresql/.ssh/id_rsa_sftp \
91,"-t rsa -b 4096 -N """" -m PEM"
91,sftp-server ⇒ Copy pg-primary SFTP backup public key to sftp-server
91,sudo -u pgbackrest mkdir -m 750 -p /home/pgbackrest/.ssh
91,(sudo ssh root@pg-primary cat /var/lib/postgresql/.ssh/id_rsa_sftp.pub) | \
91,sudo -u pgbackrest tee -a /home/pgbackrest/.ssh/authorized_keys
91,Commands are run exactly as if the repository were stored on a local disk.
91,pg-primary ⇒ Add sftp-server fingerprint to known_hosts file since repo4-sftp-host-key-check-type defaults to strict
91,ssh-keyscan -H sftp-server >> /var/lib/postgresql/.ssh/known_hosts 2>/dev/null
91,pg-primary ⇒ Create the stanza
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info stanza-create
91,[filtered 6 lines of output]
91,P00
91,INFO: stanza 'demo' already exists on repo3 and is valid
91,P00
91,INFO: stanza-create for stanza 'demo' on repo4
91,P00
91,INFO: stanza-create command end: completed successfully
91,pg-primary ⇒ Backup the demo cluster
91,sudo -u postgres pgbackrest --stanza=demo --repo=4 \
91,--log-level-console=info backup
91,P00
91,INFO: backup command begin 2.50: --exec-id=1855-65aa75b9 --log-level-console=info --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=4 --repo=4 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-block --repo1-bundle --repo4-bundle --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo3-path=/demo-repo --repo4-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo3-retention-full=4 --repo3-s3-bucket=demo-bucket --repo3-s3-endpoint=s3.us-east-1.amazonaws.com --repo3-s3-key= --repo3-s3-key-secret= --repo3-s3-region=us-east-1 --repo4-sftp-host=sftp-server --repo4-sftp-host-key-hash-type=sha1 --repo4-sftp-host-user=pgbackrest --repo4-sftp-private-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp --repo4-sftp-public-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp.pub --repo2-type=azure --repo3-type=s3 --repo4-type=sftp --stanza=demo --start-fast
91,P00
91,"WARN: option 'repo4-retention-full' is not set for 'repo4-retention-full-type=count', the repository may run out of space"
91,"HINT: to retain full backups indefinitely (without warning), set option 'repo4-retention-full' to the maximum."
91,P00
91,"WARN: no prior backup exists, incr backup has been changed to full"
91,P00
91,INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes
91,P00
91,"INFO: backup start archive = 00000005000000000000001F, lsn = 0/1F000028"
91,[filtered 3 lines of output]
91,P00
91,INFO: check archive for segment(s) 00000005000000000000001F:000000050000000000000020
91,P00
91,INFO: new backup label = 20240122-113433F
91,P00
91,"INFO: full backup size = 29.0MB, file total = 1263"
91,P00
91,INFO: backup command end: completed successfully
91,P00
91,INFO: expire command begin 2.50: --exec-id=1855-65aa75b9 --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo=4 --repo2-azure-account= --repo2-azure-container=demo-container --repo2-azure-key= --repo1-cipher-pass= --repo1-cipher-type=aes-256-cbc --repo1-path=/var/lib/pgbackrest --repo2-path=/demo-repo --repo3-path=/demo-repo --repo4-path=/demo-repo --repo1-retention-diff=2 --repo1-retention-full=2 --repo2-retention-full=4 --repo3-retention-full=4 --repo3-s3-bucket=demo-bucket --repo3-s3-endpoint=s3.us-east-1.amazonaws.com --repo3-s3-key= --repo3-s3-key-secret= --repo3-s3-region=us-east-1 --repo4-sftp-host=sftp-server --repo4-sftp-host-key-hash-type=sha1 --repo4-sftp-host-user=pgbackrest --repo4-sftp-private-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp --repo4-sftp-public-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp.pub --repo2-type=azure --repo3-type=s3 --repo4-type=sftp --stanza=demo
91,P00
91,INFO: expire command end: completed successfully
91,GCS-Compatible Object Store Support
91,pgBackRest supports locating repositories in GCS-compatible object stores. The bucket used to store the repository must be created in advance — pgBackRest will not do it automatically. The repository can be located in the bucket root (/) but it's usually best to place it in a subpath so object store logs or other data can also be stored in the bucket without conflicts.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure GCS
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=4
91,repo1-block=y
91,repo1-bundle=y
91,repo1-cipher-pass=zWaf6XtpjIVZC5444yXB+cgFDFl7MxGlgkZSaoPvTGirhPygu4jOKOXf9LO4vjfO
91,repo1-cipher-type=aes-256-cbc
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-diff=2
91,repo1-retention-full=2
91,repo2-azure-account=pgbackrest
91,repo2-azure-container=demo-container
91,repo2-azure-key=YXpLZXk=
91,repo2-path=/demo-repo
91,repo2-retention-full=4
91,repo2-type=azure
91,repo3-path=/demo-repo
91,repo3-retention-full=4
91,repo3-s3-bucket=demo-bucket
91,repo3-s3-endpoint=s3.us-east-1.amazonaws.com
91,repo3-s3-key=accessKey1
91,repo3-s3-key-secret=verySecretKey1
91,repo3-s3-region=us-east-1
91,repo3-type=s3
91,repo4-bundle=y
91,repo4-path=/demo-repo
91,repo4-sftp-host=sftp-server
91,repo4-sftp-host-key-hash-type=sha1
91,repo4-sftp-host-user=pgbackrest
91,repo4-sftp-private-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp
91,repo4-sftp-public-key-file=/var/lib/postgresql/.ssh/id_rsa_sftp.pub
91,repo4-type=sftp
91,repo5-gcs-bucket=demo-bucket
91,repo5-gcs-key=/etc/pgbackrest/gcs-key.json
91,repo5-path=/demo-repo
91,repo5-type=gcs
91,start-fast=y
91,[global:archive-push]
91,compress-level=3
91,When running in GCE set repo5-gcs-key-type=auto to automatically authenticate using the instance service account.
91,Commands are run exactly as if the repository were stored on a local disk.
91,File creation time in object stores is relatively slow so commands benefit by increasing process-max to parallelize file creation.
91,Dedicated Repository Host
91,The configuration described in Quickstart is suitable for simple installations but for enterprise configurations it is more typical to have a dedicated repository host where the backups and WAL archive files are stored. This separates the backups and WAL archive from the database server so database host failures have less impact. It is still a good idea to employ traditional backup software to backup the repository host.
91,"On PostgreSQL hosts, pg1-path is required to be the path of the local PostgreSQL cluster and no pg1-host should be configured. When configuring a repository host, the pgbackrest configuration file must have the pg-host option configured to connect to the primary and standby (if any) hosts. The repository host has the only pgbackrest configuration that should be aware of more than one PostgreSQL host. Order does not matter, e.g. pg1-path/pg1-host, pg2-path/pg2-host can be primary or standby."
91,Installation
91,A new host named repository is created to store the cluster backups.
91,NOTE:
91,The pgBackRest version installed on the repository host must exactly match the version installed on the PostgreSQL host.
91,The pgbackrest user is created to own the pgBackRest repository. Any user can own the repository but it is best not to use postgres (if it exists) to avoid confusion.
91,repository ⇒ Create pgbackrest user
91,"sudo adduser --disabled-password --gecos """" pgbackrest"
91,"Installing pgBackRest from a package is preferable to building from source. When installing from a package the rest of the instructions in this section are generally not required, but it is possible that a package will skip creating one of the directories or apply incorrect permissions. In that case it may be necessary to manually create directories or update permissions."
91,Debian/Ubuntu packages for pgBackRest are available at apt.postgresql.org.
91,If packages are not provided for your distribution/version you can build from source and then install manually as shown here.
91,repository ⇒ Install dependencies
91,sudo apt-get install postgresql-client libxml2 libssh2-1
91,repository ⇒ Copy pgBackRest binary from build host
91,sudo scp build:/build/pgbackrest-release-2.50/src/pgbackrest /usr/bin
91,sudo chmod 755 /usr/bin/pgbackrest
91,pgBackRest requires log and configuration directories and a configuration file.
91,repository ⇒ Create pgBackRest configuration file and directories
91,sudo mkdir -p -m 770 /var/log/pgbackrest
91,sudo chown pgbackrest:pgbackrest /var/log/pgbackrest
91,sudo mkdir -p /etc/pgbackrest
91,sudo mkdir -p /etc/pgbackrest/conf.d
91,sudo touch /etc/pgbackrest/pgbackrest.conf
91,sudo chmod 640 /etc/pgbackrest/pgbackrest.conf
91,sudo chown pgbackrest:pgbackrest /etc/pgbackrest/pgbackrest.conf
91,repository ⇒ Create the pgBackRest repository
91,sudo mkdir -p /var/lib/pgbackrest
91,sudo chmod 750 /var/lib/pgbackrest
91,sudo chown pgbackrest:pgbackrest /var/lib/pgbackrest
91,Setup Passwordless SSH
91,"pgBackRest can use passwordless SSH to enable communication between the hosts. It is also possible to use TLS, see Setup TLS."
91,repository ⇒ Create repository host key pair
91,sudo -u pgbackrest mkdir -m 750 /home/pgbackrest/.ssh
91,sudo -u pgbackrest ssh-keygen -f /home/pgbackrest/.ssh/id_rsa \
91,"-t rsa -b 4096 -N """""
91,pg-primary ⇒ Create pg-primary host key pair
91,sudo -u postgres mkdir -m 750 -p /var/lib/postgresql/.ssh
91,sudo -u postgres ssh-keygen -f /var/lib/postgresql/.ssh/id_rsa \
91,"-t rsa -b 4096 -N """""
91,Exchange keys between repository and pg-primary.
91,repository ⇒ Copy pg-primary public key to repository
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@pg-primary cat /var/lib/postgresql/.ssh/id_rsa.pub) | \
91,sudo -u pgbackrest tee -a /home/pgbackrest/.ssh/authorized_keys
91,pg-primary ⇒ Copy repository public key to pg-primary
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@repository cat /home/pgbackrest/.ssh/id_rsa.pub) | \
91,sudo -u postgres tee -a /var/lib/postgresql/.ssh/authorized_keys
91,Test that connections can be made from repository to pg-primary and vice versa.
91,repository ⇒ Test connection from repository to pg-primary
91,sudo -u pgbackrest ssh postgres@pg-primary
91,pg-primary ⇒ Test connection from pg-primary to repository
91,sudo -u postgres ssh pgbackrest@repository
91,NOTE:
91,ssh has been configured to only allow pgBackRest to be run via passwordless ssh. This enhances security in the event that one of the service accounts is hijacked.
91,Configuration
91,The repository host must be configured with the pg-primary host/user and database path. The primary will be configured as pg1 to allow a standby to be added later.
91,repository:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pg1-host/pg1-host-user and pg1-path
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,"The database host must be configured with the repository host/user. The default for the repo1-host-user option is pgbackrest. If the postgres user does restores on the repository host it is best not to also allow the postgres user to perform backups. However, the postgres user can read the repository directly if it is in the same group as the pgbackrest user."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure repo1-host/repo1-host-user
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,log-level-file=detail
91,repo1-host=repository
91,PostgreSQL configuration may be found in the Configure Archiving section.
91,Commands are run the same as on a single host configuration except that some commands such as backup and expire are run from the repository host instead of the database host.
91,Create and Check Stanza
91,Create the stanza in the new repository.
91,repository ⇒ Create the stanza
91,sudo -u pgbackrest pgbackrest --stanza=demo stanza-create
91,Check that the configuration is correct on both the database and repository hosts. More information about the check command can be found in Check the Configuration.
91,pg-primary ⇒ Check the configuration
91,sudo -u postgres pgbackrest --stanza=demo check
91,repository ⇒ Check the configuration
91,sudo -u pgbackrest pgbackrest --stanza=demo check
91,Perform a Backup
91,To perform a backup of the PostgreSQL cluster run pgBackRest with the backup command on the repository host.
91,repository ⇒ Backup the demo cluster
91,sudo -u pgbackrest pgbackrest --stanza=demo backup
91,P00
91,"WARN: no prior backup exists, incr backup has been changed to full"
91,Since a new repository was created on the repository host the warning about the incremental backup changing to a full backup was emitted.
91,Restore a Backup
91,To perform a restore of the PostgreSQL cluster run pgBackRest with the restore command on the database host.
91,"pg-primary ⇒ Stop the demo cluster, restore, and restart PostgreSQL"
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres pgbackrest --stanza=demo --delta restore
91,sudo pg_ctlcluster 15 demo start
91,Parallel Backup / Restore
91,pgBackRest offers parallel processing to improve performance of compression and transfer. The number of processes to be used for this feature is set using the --process-max option.
91,"It is usually best not to use more than 25% of available CPUs for the backup command. Backups don't have to run that fast as long as they are performed regularly and the backup process should not impact database performance, if at all possible."
91,The restore command can and should use all available CPUs because during a restore the PostgreSQL cluster is shut down and there is generally no other important work being done on the host. If the host contains multiple clusters then that should be considered when setting restore parallelism.
91,repository ⇒ Perform a backup with single process
91,sudo -u pgbackrest pgbackrest --stanza=demo --type=full backup
91,repository:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pgBackRest to use multiple backup processes
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=3
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,repository ⇒ Perform a backup with multiple processes
91,sudo -u pgbackrest pgbackrest --stanza=demo --type=full backup
91,repository ⇒ Get backup info for the demo cluster
91,sudo -u pgbackrest pgbackrest info
91,stanza: demo
91,status: ok
91,cipher: none
91,db (current)
91,wal archive min/max (15): 000000060000000000000026/000000060000000000000028
91,full backup: 20240122-113500F
91,timestamp start/stop: 2024-01-22 11:35:00+00 / 2024-01-22 11:35:03+00
91,wal start/stop: 000000060000000000000026 / 000000060000000000000026
91,"database size: 29.0MB, database backup size: 29.0MB"
91,"repo1: backup set size: 3.9MB, backup size: 3.9MB"
91,full backup: 20240122-113505F
91,timestamp start/stop: 2024-01-22 11:35:05+00 / 2024-01-22 11:35:08+00
91,wal start/stop: 000000060000000000000027 / 000000060000000000000028
91,"database size: 29.0MB, database backup size: 29.0MB"
91,"repo1: backup set size: 3.9MB, backup size: 3.9MB"
91,"The performance of the last backup should be improved by using multiple processes. For very small backups the difference may not be very apparent, but as the size of the database increases so will time savings."
91,Starting and Stopping
91,"Sometimes it is useful to prevent pgBackRest from running on a system. For example, when failing over from a primary to a standby it's best to prevent pgBackRest from running on the old primary in case PostgreSQL gets restarted or can't be completely killed. This will also prevent pgBackRest from running on cron."
91,pg-primary ⇒ Stop the pgBackRest services
91,sudo -u postgres pgbackrest stop
91,New pgBackRest processes will no longer run.
91,repository ⇒ Attempt a backup
91,sudo -u pgbackrest pgbackrest --stanza=demo backup
91,P00
91,WARN: unable to check pg1: [StopError] raised from remote-0 ssh protocol on 'pg-primary': stop file exists for all stanzas
91,P00
91,ERROR: [056]: unable to find primary cluster - cannot proceed
91,HINT: are all available clusters in recovery?
91,Specify the --force option to terminate any pgBackRest process that are currently running. If pgBackRest is already stopped then stopping again will generate a warning.
91,pg-primary ⇒ Stop the pgBackRest services again
91,sudo -u postgres pgbackrest stop
91,P00
91,WARN: stop file already exists for all stanzas
91,Start pgBackRest processes again with the start command.
91,pg-primary ⇒ Start the pgBackRest services
91,sudo -u postgres pgbackrest start
91,It is also possible to stop pgBackRest for a single stanza.
91,pg-primary ⇒ Stop pgBackRest services for the demo stanza
91,sudo -u postgres pgbackrest --stanza=demo stop
91,New pgBackRest processes for the specified stanza will no longer run.
91,repository ⇒ Attempt a backup
91,sudo -u pgbackrest pgbackrest --stanza=demo backup
91,P00
91,WARN: unable to check pg1: [StopError] raised from remote-0 ssh protocol on 'pg-primary': stop file exists for stanza demo
91,P00
91,ERROR: [056]: unable to find primary cluster - cannot proceed
91,HINT: are all available clusters in recovery?
91,The stanza must also be specified when starting the pgBackRest processes for a single stanza.
91,pg-primary ⇒ Start the pgBackRest services for the demo stanza
91,sudo -u postgres pgbackrest --stanza=demo start
91,Replication
91,Replication allows multiple copies of a PostgreSQL cluster (called standbys) to be created from a single primary. The standbys are useful for balancing reads and to provide redundancy in case the primary host fails.
91,Installation
91,A new host named pg-standby is created to run the standby.
91,"Installing pgBackRest from a package is preferable to building from source. When installing from a package the rest of the instructions in this section are generally not required, but it is possible that a package will skip creating one of the directories or apply incorrect permissions. In that case it may be necessary to manually create directories or update permissions."
91,Debian/Ubuntu packages for pgBackRest are available at apt.postgresql.org.
91,If packages are not provided for your distribution/version you can build from source and then install manually as shown here.
91,pg-standby ⇒ Install dependencies
91,sudo apt-get install postgresql-client libxml2 libssh2-1
91,pg-standby ⇒ Copy pgBackRest binary from build host
91,sudo scp build:/build/pgbackrest-release-2.50/src/pgbackrest /usr/bin
91,sudo chmod 755 /usr/bin/pgbackrest
91,pgBackRest requires log and configuration directories and a configuration file.
91,pg-standby ⇒ Create pgBackRest configuration file and directories
91,sudo mkdir -p -m 770 /var/log/pgbackrest
91,sudo chown postgres:postgres /var/log/pgbackrest
91,sudo mkdir -p /etc/pgbackrest
91,sudo mkdir -p /etc/pgbackrest/conf.d
91,sudo touch /etc/pgbackrest/pgbackrest.conf
91,sudo chmod 640 /etc/pgbackrest/pgbackrest.conf
91,sudo chown postgres:postgres /etc/pgbackrest/pgbackrest.conf
91,Setup Passwordless SSH
91,"pgBackRest can use passwordless SSH to enable communication between the hosts. It is also possible to use TLS, see Setup TLS."
91,pg-standby ⇒ Create pg-standby host key pair
91,sudo -u postgres mkdir -m 750 -p /var/lib/postgresql/.ssh
91,sudo -u postgres ssh-keygen -f /var/lib/postgresql/.ssh/id_rsa \
91,"-t rsa -b 4096 -N """""
91,Exchange keys between repository and pg-standby.
91,repository ⇒ Copy pg-standby public key to repository
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@pg-standby cat /var/lib/postgresql/.ssh/id_rsa.pub) | \
91,sudo -u pgbackrest tee -a /home/pgbackrest/.ssh/authorized_keys
91,pg-standby ⇒ Copy repository public key to pg-standby
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@repository cat /home/pgbackrest/.ssh/id_rsa.pub) | \
91,sudo -u postgres tee -a /var/lib/postgresql/.ssh/authorized_keys
91,Test that connections can be made from repository to pg-standby and vice versa.
91,repository ⇒ Test connection from repository to pg-standby
91,sudo -u pgbackrest ssh postgres@pg-standby
91,pg-standby ⇒ Test connection from pg-standby to repository
91,sudo -u postgres ssh pgbackrest@repository
91,Hot Standby
91,A hot standby performs replication using the WAL archive and allows read-only queries.
91,pgBackRest configuration is very similar to pg-primary except that the standby recovery type will be used to keep the cluster in recovery mode when the end of the WAL stream has been reached.
91,pg-standby:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pgBackRest on the standby
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,log-level-file=detail
91,repo1-host=repository
91,The demo cluster must be created (even though it will be overwritten on restore) in order to create the PostgreSQL configuration files.
91,pg-standby ⇒ Create demo cluster
91,sudo pg_createcluster 15 demo
91,Now the standby can be created with the restore command.
91,IMPORTANT:
91,"If the cluster is intended to be promoted without becoming the new primary (e.g. for reporting or testing), use --archive-mode=off or set archive_mode=off in postgresql.conf to disable archiving. If archiving is not disabled then the repository may be polluted with WAL that can make restores more difficult."
91,pg-standby ⇒ Restore the demo standby cluster
91,sudo -u postgres pgbackrest --stanza=demo --delta --type=standby restore
91,sudo -u postgres cat /var/lib/postgresql/15/demo/postgresql.auto.conf
91,# Do not edit this file manually!
91,# It will be overwritten by the ALTER SYSTEM command.
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:33:12
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:33:46
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:34:07
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Removed by pgBackRest restore on 2024-01-22 11:34:53 # recovery_target_time = '2024-01-22 11:34:01.874583+00'
91,# Removed by pgBackRest restore on 2024-01-22 11:34:53 # recovery_target_action = 'promote'
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:34:53
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:35:21
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,"The hot_standby setting must be enabled before starting PostgreSQL to allow read-only connections on pg-standby. Otherwise, connection attempts will be refused. The rest of the configuration is in case the standby is promoted to a primary."
91,pg-standby:/etc/postgresql/15/demo/postgresql.conf ⇒ Configure PostgreSQL
91,archive_command = 'pgbackrest --stanza=demo archive-push %p'
91,archive_mode = on
91,hot_standby = on
91,max_wal_senders = 3
91,wal_level = replica
91,pg-standby ⇒ Start PostgreSQL
91,sudo pg_ctlcluster 15 demo start
91,The PostgreSQL log gives valuable information about the recovery. Note especially that the cluster has entered standby mode and is ready to accept read-only connections.
91,pg-standby ⇒ Examine the PostgreSQL log output for log messages indicating success
91,sudo -u postgres cat /var/log/postgresql/postgresql-15-demo.log
91,[filtered 3 lines of output]
91,LOG:
91,"listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432"""
91,LOG:
91,database system was interrupted; last known up at 2024-01-22 11:35:05 UTC
91,LOG:
91,entering standby mode
91,LOG:
91,"restored log file ""00000006.history"" from archive"
91,LOG:
91,"restored log file ""000000060000000000000027"" from archive"
91,[filtered 4 lines of output]
91,An easy way to test that replication is properly configured is to create a table on pg-primary.
91,pg-primary ⇒ Create a new table on the primary
91,"sudo -u postgres psql -c "" \"
91,begin; \
91,create table replicated_table (message text); \
91,insert into replicated_table values ('Important Data'); \
91,commit; \
91,"select * from replicated_table"";"
91,[filtered 4 lines of output]
91,message
91,----------------
91,Important Data
91,(1 row)
91,And then query the same table on pg-standby.
91,pg-standby ⇒ Query new table on the standby
91,"sudo -u postgres psql -c ""select * from replicated_table;"""
91,ERROR:
91,"relation ""replicated_table"" does not exist"
91,LINE 1: select * from replicated_table;
91,"So, what went wrong? Since PostgreSQL is pulling WAL segments from the archive to perform replication, changes won't be seen on the standby until the WAL segment that contains those changes is pushed from pg-primary."
91,This can be done manually by calling pg_switch_wal() which pushes the current WAL segment to the archive (a new WAL segment is created to contain further changes).
91,pg-primary ⇒ Call pg_switch_wal()
91,"sudo -u postgres psql -c ""select *, current_timestamp from pg_switch_wal()"";"
91,pg_switch_wal |
91,current_timestamp
91,---------------+-------------------------------
91,0/29019A48
91,| 2024-01-22 11:35:28.117945+00
91,(1 row)
91,Now after a short delay the table will appear on pg-standby.
91,pg-standby ⇒ Now the new table exists on the standby (may require a few retries)
91,"sudo -u postgres psql -c "" \"
91,"select *, current_timestamp from replicated_table"""
91,message
91,current_timestamp
91,----------------+-------------------------------
91,Important Data | 2024-01-22 11:35:29.307516+00
91,(1 row)
91,Check the standby configuration for access to the repository.
91,pg-standby ⇒ Check the configuration
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info check
91,P00
91,INFO: check command begin 2.50: --exec-id=521-a497bf04 --log-level-console=info --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-host=repository --stanza=demo
91,P00
91,INFO: check repo1 (standby)
91,P00
91,INFO: switch wal not performed because this is a standby
91,P00
91,INFO: check command end: completed successfully
91,Streaming Replication
91,"Instead of relying solely on the WAL archive, streaming replication makes a direct connection to the primary and applies changes as soon as they are made on the primary. This results in much less lag between the primary and standby."
91,Streaming replication requires a user with the replication privilege.
91,pg-primary ⇒ Create replication user
91,"sudo -u postgres psql -c "" \"
91,"create user replicator password 'jw8s0F4' replication"";"
91,CREATE ROLE
91,The pg_hba.conf file must be updated to allow the standby to connect as the replication user. Be sure to replace the IP address below with the actual IP address of your pg-standby. A reload will be required after modifying the pg_hba.conf file.
91,pg-primary ⇒ Create pg_hba.conf entry for replication user
91,sudo -u postgres sh -c 'echo \
91,"""host"
91,replication
91,replicator
91,172.17.0.8/32
91,"md5"" \"
91,>> /etc/postgresql/15/demo/pg_hba.conf'
91,sudo pg_ctlcluster 15 demo reload
91,The standby needs to know how to contact the primary so the primary_conninfo setting will be configured in pgBackRest.
91,pg-standby:/etc/pgbackrest/pgbackrest.conf ⇒ Set primary_conninfo
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,recovery-option=primary_conninfo=host=172.17.0.6 port=5432 user=replicator
91,[global]
91,log-level-file=detail
91,repo1-host=repository
91,It is possible to configure a password in the primary_conninfo setting but using a .pgpass file is more flexible and secure.
91,pg-standby ⇒ Configure the replication password in the .pgpass file.
91,sudo -u postgres sh -c 'echo \
91,"""172.17.0.6:*:replication:replicator:jw8s0F4"" \"
91,>> /var/lib/postgresql/.pgpass'
91,sudo -u postgres chmod 600 /var/lib/postgresql/.pgpass
91,Now the standby can be created with the restore command.
91,pg-standby ⇒ Stop PostgreSQL and restore the demo standby cluster
91,sudo pg_ctlcluster 15 demo stop
91,sudo -u postgres pgbackrest --stanza=demo --delta --type=standby restore
91,sudo -u postgres cat /var/lib/postgresql/15/demo/postgresql.auto.conf
91,# Do not edit this file manually!
91,# It will be overwritten by the ALTER SYSTEM command.
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:33:12
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:33:46
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:34:07
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Removed by pgBackRest restore on 2024-01-22 11:34:53 # recovery_target_time = '2024-01-22 11:34:01.874583+00'
91,# Removed by pgBackRest restore on 2024-01-22 11:34:53 # recovery_target_action = 'promote'
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:34:53
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,# Recovery settings generated by pgBackRest restore on 2024-01-22 11:35:32
91,primary_conninfo = 'host=172.17.0.6 port=5432 user=replicator'
91,"restore_command = 'pgbackrest --stanza=demo archive-get %f ""%p""'"
91,NOTE:
91,The primary_conninfo setting has been written into the postgresql.auto.conf file because it was configured as a recovery-option in pgbackrest.conf. The --type=preserve option can be used with the restore to leave the existing postgresql.auto.conf file in place if that behavior is preferred.
91,pg-standby ⇒ Start PostgreSQL
91,sudo pg_ctlcluster 15 demo start
91,The PostgreSQL log will confirm that streaming replication has started.
91,pg-standby ⇒ Examine the PostgreSQL log output for log messages indicating success
91,sudo -u postgres cat /var/log/postgresql/postgresql-15-demo.log
91,[filtered 11 lines of output]
91,LOG:
91,consistent recovery state reached at 0/28000050
91,LOG:
91,database system is ready to accept read-only connections
91,LOG:
91,started streaming WAL from primary at 0/2A000000 on timeline 6
91,Now when a table is created on pg-primary it will appear on pg-standby quickly and without the need to call pg_switch_wal().
91,pg-primary ⇒ Create a new table on the primary
91,"sudo -u postgres psql -c "" \"
91,begin; \
91,create table stream_table (message text); \
91,insert into stream_table values ('Important Data'); \
91,commit; \
91,"select *, current_timestamp from stream_table"";"
91,[filtered 4 lines of output]
91,message
91,current_timestamp
91,----------------+-------------------------------
91,Important Data | 2024-01-22 11:35:38.699743+00
91,(1 row)
91,pg-standby ⇒ Query table on the standby
91,"sudo -u postgres psql -c "" \"
91,"select *, current_timestamp from stream_table"""
91,message
91,current_timestamp
91,----------------+-------------------------------
91,Important Data | 2024-01-22 11:35:38.878795+00
91,(1 row)
91,Multiple Stanzas
91,pgBackRest supports multiple stanzas. The most common usage is sharing a repository host among multiple stanzas.
91,Installation
91,A new host named pg-alt is created to run the new primary.
91,"Installing pgBackRest from a package is preferable to building from source. When installing from a package the rest of the instructions in this section are generally not required, but it is possible that a package will skip creating one of the directories or apply incorrect permissions. In that case it may be necessary to manually create directories or update permissions."
91,Debian/Ubuntu packages for pgBackRest are available at apt.postgresql.org.
91,If packages are not provided for your distribution/version you can build from source and then install manually as shown here.
91,pg-alt ⇒ Install dependencies
91,sudo apt-get install postgresql-client libxml2 libssh2-1
91,pg-alt ⇒ Copy pgBackRest binary from build host
91,sudo scp build:/build/pgbackrest-release-2.50/src/pgbackrest /usr/bin
91,sudo chmod 755 /usr/bin/pgbackrest
91,pgBackRest requires log and configuration directories and a configuration file.
91,pg-alt ⇒ Create pgBackRest configuration file and directories
91,sudo mkdir -p -m 770 /var/log/pgbackrest
91,sudo chown postgres:postgres /var/log/pgbackrest
91,sudo mkdir -p /etc/pgbackrest
91,sudo mkdir -p /etc/pgbackrest/conf.d
91,sudo touch /etc/pgbackrest/pgbackrest.conf
91,sudo chmod 640 /etc/pgbackrest/pgbackrest.conf
91,sudo chown postgres:postgres /etc/pgbackrest/pgbackrest.conf
91,Setup Passwordless SSH
91,"pgBackRest can use passwordless SSH to enable communication between the hosts. It is also possible to use TLS, see Setup TLS."
91,pg-alt ⇒ Create pg-alt host key pair
91,sudo -u postgres mkdir -m 750 -p /var/lib/postgresql/.ssh
91,sudo -u postgres ssh-keygen -f /var/lib/postgresql/.ssh/id_rsa \
91,"-t rsa -b 4096 -N """""
91,Exchange keys between repository and pg-alt.
91,repository ⇒ Copy pg-alt public key to repository
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@pg-alt cat /var/lib/postgresql/.ssh/id_rsa.pub) | \
91,sudo -u pgbackrest tee -a /home/pgbackrest/.ssh/authorized_keys
91,pg-alt ⇒ Copy repository public key to pg-alt
91,"(echo -n 'no-agent-forwarding,no-X11-forwarding,no-port-forwarding,' && \"
91,"echo -n 'command=""/usr/bin/pgbackrest ${SSH_ORIGINAL_COMMAND#* }"" ' && \"
91,sudo ssh root@repository cat /home/pgbackrest/.ssh/id_rsa.pub) | \
91,sudo -u postgres tee -a /var/lib/postgresql/.ssh/authorized_keys
91,Test that connections can be made from repository to pg-alt and vice versa.
91,repository ⇒ Test connection from repository to pg-alt
91,sudo -u pgbackrest ssh postgres@pg-alt
91,pg-alt ⇒ Test connection from pg-alt to repository
91,sudo -u postgres ssh pgbackrest@repository
91,Configuration
91,pgBackRest configuration is nearly identical to pg-primary except that the demo-alt stanza will be used so backups and archive will be stored in a separate location.
91,pg-alt:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pgBackRest on the new primary
91,[demo-alt]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,log-level-file=detail
91,repo1-host=repository
91,repository:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pg1-host/pg1-host-user and pg1-path
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/15/demo
91,[demo-alt]
91,pg1-host=pg-alt
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,process-max=3
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,Setup Demo Cluster
91,pg-alt ⇒ Create the demo cluster
91,sudo -u postgres /usr/lib/postgresql/15/bin/initdb \
91,-D /var/lib/postgresql/15/demo -k -A peer
91,sudo pg_createcluster 15 demo
91,"Configuring already existing cluster (configuration: /etc/postgresql/15/demo, data: /var/lib/postgresql/15/demo, owner: 102:103)"
91,Ver Cluster Port Status Owner
91,Data directory
91,Log file
91,demo
91,5432 down
91,postgres /var/lib/postgresql/15/demo /var/log/postgresql/postgresql-15-demo.log
91,pg-alt:/etc/postgresql/15/demo/postgresql.conf ⇒ Configure PostgreSQL settings
91,archive_command = 'pgbackrest --stanza=demo-alt archive-push %p'
91,archive_mode = on
91,max_wal_senders = 3
91,wal_level = replica
91,pg-alt ⇒ Start the demo cluster
91,sudo pg_ctlcluster 15 demo restart
91,Create the Stanza and Check Configuration
91,The stanza-create command must be run to initialize the stanza. It is recommended that the check command be run after stanza-create to ensure archiving and backups are properly configured.
91,pg-alt ⇒ Create the stanza and check the configuration
91,sudo -u postgres pgbackrest --stanza=demo-alt --log-level-console=info stanza-create
91,P00
91,INFO: stanza-create command begin 2.50: --exec-id=395-278a2dd7 --log-level-console=info --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-host=repository --stanza=demo-alt
91,P00
91,INFO: stanza-create for stanza 'demo-alt' on repo1
91,P00
91,INFO: stanza-create command end: completed successfully
91,sudo -u postgres pgbackrest --log-level-console=info check
91,P00
91,INFO: check command begin 2.50: --exec-id=405-5623ea8c --log-level-console=info --log-level-file=detail --log-level-stderr=off --no-log-timestamp --repo1-host=repository
91,P00
91,INFO: check stanza 'demo-alt'
91,P00
91,INFO: check repo1 configuration (primary)
91,P00
91,INFO: check repo1 archive for WAL (primary)
91,P00
91,INFO: WAL segment 000000010000000000000001 successfully archived to '/var/lib/pgbackrest/archive/demo-alt/15-1/0000000100000000/000000010000000000000001-18d1fc3d54bf87ad6c8041c49ebc57f4e1fd143c.gz' on repo1
91,P00
91,INFO: check command end: completed successfully
91,If the check command is run from the repository host then all stanzas will be checked.
91,repository ⇒ Check the configuration for all stanzas
91,sudo -u pgbackrest pgbackrest --log-level-console=info check
91,P00
91,INFO: check command begin 2.50: --exec-id=1256-747b65e6 --log-level-console=info --log-level-stderr=off --no-log-timestamp --repo1-path=/var/lib/pgbackrest
91,P00
91,INFO: check stanza 'demo'
91,P00
91,INFO: check repo1 configuration (primary)
91,P00
91,INFO: check repo1 archive for WAL (primary)
91,P00
91,INFO: WAL segment 00000006000000000000002A successfully archived to '/var/lib/pgbackrest/archive/demo/15-1/0000000600000000/00000006000000000000002A-730e1926e62ab99cc75b182609b63654997ec5e5.gz' on repo1
91,P00
91,INFO: check stanza 'demo-alt'
91,P00
91,INFO: check repo1 configuration (primary)
91,P00
91,INFO: check repo1 archive for WAL (primary)
91,P00
91,INFO: WAL segment 000000010000000000000002 successfully archived to '/var/lib/pgbackrest/archive/demo-alt/15-1/0000000100000000/000000010000000000000002-3e897cd02bfa4e8ad23a5b42a23df253d2f4d7ea.gz' on repo1
91,P00
91,INFO: check command end: completed successfully
91,Asynchronous Archiving
91,Asynchronous archiving is enabled with the archive-async option. This option enables asynchronous operation for both the archive-push and archive-get commands.
91,A spool path is required. The commands will store transient data here but each command works quite a bit differently so spool path usage is described in detail in each section.
91,pg-primary ⇒ Create the spool directory
91,sudo mkdir -p -m 750 /var/spool/pgbackrest
91,sudo chown postgres:postgres /var/spool/pgbackrest
91,pg-standby ⇒ Create the spool directory
91,sudo mkdir -p -m 750 /var/spool/pgbackrest
91,sudo chown postgres:postgres /var/spool/pgbackrest
91,"The spool path must be configured and asynchronous archiving enabled. Asynchronous archiving automatically confers some benefit by reducing the number of connections made to remote storage, but setting process-max can drastically improve performance by parallelizing operations. Be sure not to set process-max so high that it affects normal database operations."
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Configure the spool path and asynchronous archiving
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,archive-async=y
91,log-level-file=detail
91,repo1-host=repository
91,spool-path=/var/spool/pgbackrest
91,[global:archive-get]
91,process-max=2
91,[global:archive-push]
91,process-max=2
91,pg-standby:/etc/pgbackrest/pgbackrest.conf ⇒ Configure the spool path and asynchronous archiving
91,[demo]
91,pg1-path=/var/lib/postgresql/15/demo
91,recovery-option=primary_conninfo=host=172.17.0.6 port=5432 user=replicator
91,[global]
91,archive-async=y
91,log-level-file=detail
91,repo1-host=repository
91,spool-path=/var/spool/pgbackrest
91,[global:archive-get]
91,process-max=2
91,[global:archive-push]
91,process-max=2
91,NOTE:
91,process-max is configured using command sections so that the option is not used by backup and restore. This also allows different values for archive-push and archive-get.
91,For demonstration purposes streaming replication will be broken to force PostgreSQL to get WAL using the restore_command.
91,pg-primary ⇒ Break streaming replication by changing the replication password
91,"sudo -u postgres psql -c ""alter user replicator password 'bogus'"""
91,ALTER ROLE
91,pg-standby ⇒ Restart standby to break connection
91,sudo pg_ctlcluster 15 demo restart
91,Archive Push
91,The asynchronous archive-push command offloads WAL archiving to a separate process (or processes) to improve throughput. It works by looking ahead to see which WAL segments are ready to be archived beyond the request that PostgreSQL is currently making via the archive_command. WAL segments are transferred to the archive directly from the pg_xlog/pg_wal directory and success is only returned by the archive_command when the WAL segment has been safely stored in the archive.
91,The spool path holds the current status of WAL archiving. Status files written into the spool directory are typically zero length and should consume a minimal amount of space (a few MB at most) and very little IO. All the information in this directory can be recreated so it is not necessary to preserve the spool directory if the cluster is moved to new hardware.
91,IMPORTANT:
91,"In the original implementation of asynchronous archiving, WAL segments were copied to the spool directory before compression and transfer. The new implementation copies WAL directly from the pg_xlog directory. If asynchronous archiving was utilized in v1.12 or prior, read the v1.13 release notes carefully before upgrading."
91,The [stanza]-archive-push-async.log file can be used to monitor the activity of the asynchronous process. A good way to test this is to quickly push a number of WAL segments.
91,pg-primary ⇒ Test parallel asynchronous archiving
91,"sudo -u postgres psql -c "" \"
91,select pg_create_restore_point('test async push'); select pg_switch_wal(); \
91,select pg_create_restore_point('test async push'); select pg_switch_wal(); \
91,select pg_create_restore_point('test async push'); select pg_switch_wal(); \
91,select pg_create_restore_point('test async push'); select pg_switch_wal(); \
91,"select pg_create_restore_point('test async push'); select pg_switch_wal();"""
91,sudo -u postgres pgbackrest --stanza=demo --log-level-console=info check
91,P00
91,INFO: check command begin 2.50: --exec-id=2464-599a138b --log-level-console=info --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --repo1-host=repository --stanza=demo
91,P00
91,INFO: check repo1 configuration (primary)
91,P00
91,INFO: check repo1 archive for WAL (primary)
91,P00
91,INFO: WAL segment 000000060000000000000030 successfully archived to '/var/lib/pgbackrest/archive/demo/15-1/0000000600000000/000000060000000000000030-c4b6fe2caa97312600ff9a8c75e7099eb63692c0.gz' on repo1
91,P00
91,INFO: check command end: completed successfully
91,"Now the log file will contain parallel, asynchronous activity."
91,pg-primary ⇒ Check results in the log
91,sudo -u postgres cat /var/log/pgbackrest/demo-archive-push-async.log
91,-------------------PROCESS START-------------------
91,P00
91,INFO: archive-push:async command begin 2.50: [/var/lib/postgresql/15/demo/pg_wal] --archive-async --exec-id=2450-9c8a5f33 --log-level-console=off --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=2 --repo1-host=repository --spool-path=/var/spool/pgbackrest --stanza=demo
91,P00
91,INFO: push 1 WAL file(s) to archive: 00000006000000000000002B
91,P01 DETAIL: pushed WAL file '00000006000000000000002B' to the archive
91,P00
91,INFO: archive-push:async command end: completed successfully
91,-------------------PROCESS START-------------------
91,P00
91,INFO: archive-push:async command begin 2.50: [/var/lib/postgresql/15/demo/pg_wal] --archive-async --exec-id=2468-15aacbb5 --log-level-console=off --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=2 --repo1-host=repository --spool-path=/var/spool/pgbackrest --stanza=demo
91,P00
91,INFO: push 5 WAL file(s) to archive: 00000006000000000000002C...000000060000000000000030
91,P01 DETAIL: pushed WAL file '00000006000000000000002C' to the archive
91,P02 DETAIL: pushed WAL file '00000006000000000000002D' to the archive
91,P01 DETAIL: pushed WAL file '00000006000000000000002E' to the archive
91,P02 DETAIL: pushed WAL file '00000006000000000000002F' to the archive
91,P01 DETAIL: pushed WAL file '000000060000000000000030' to the archive
91,Archive Get
91,The asynchronous archive-get command maintains a local queue of WAL to improve throughput. If a WAL segment is not found in the queue it is fetched from the repository along with enough consecutive WAL to fill the queue. The maximum size of the queue is defined by archive-get-queue-max. Whenever the queue is less than half full more WAL will be fetched to fill it.
91,"Asynchronous operation is most useful in environments that generate a lot of WAL or have a high latency connection to the repository storage (i.e., S3 or other object stores). In the case of a high latency connection it may be a good idea to increase process-max."
91,The [stanza]-archive-get-async.log file can be used to monitor the activity of the asynchronous process.
91,pg-standby ⇒ Check results in the log
91,sudo -u postgres cat /var/log/pgbackrest/demo-archive-get-async.log
91,-------------------PROCESS START-------------------
91,P00
91,"INFO: archive-get:async command begin 2.50: [000000060000000000000027, 000000060000000000000028, 000000060000000000000029, 00000006000000000000002A, 00000006000000000000002B, 00000006000000000000002C, 00000006000000000000002D, 00000006000000000000002E] --archive-async --exec-id=746-442cb793 --log-level-console=off --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=2 --repo1-host=repository --spool-path=/var/spool/pgbackrest --stanza=demo"
91,P00
91,INFO: get 8 WAL file(s) from archive: 000000060000000000000027...00000006000000000000002E
91,P01 DETAIL: found 000000060000000000000027 in the repo1: 15-1 archive
91,P02 DETAIL: found 000000060000000000000028 in the repo1: 15-1 archive
91,P01 DETAIL: found 000000060000000000000029 in the repo1: 15-1 archive
91,P02 DETAIL: found 00000006000000000000002A in the repo1: 15-1 archive
91,P00 DETAIL: unable to find 00000006000000000000002B in the archive
91,P00
91,INFO: archive-get:async command end: completed successfully
91,[filtered 14 lines of output]
91,P00
91,"INFO: archive-get:async command begin 2.50: [00000006000000000000002B, 00000006000000000000002C, 00000006000000000000002D, 00000006000000000000002E, 00000006000000000000002F, 000000060000000000000030, 000000060000000000000031, 000000060000000000000032] --archive-async --exec-id=797-ece40301 --log-level-console=off --log-level-file=detail --log-level-stderr=off --no-log-timestamp --pg1-path=/var/lib/postgresql/15/demo --process-max=2 --repo1-host=repository --spool-path=/var/spool/pgbackrest --stanza=demo"
91,P00
91,INFO: get 8 WAL file(s) from archive: 00000006000000000000002B...000000060000000000000032
91,P02 DETAIL: found 00000006000000000000002C in the repo1: 15-1 archive
91,P01 DETAIL: found 00000006000000000000002B in the repo1: 15-1 archive
91,P02 DETAIL: found 00000006000000000000002D in the repo1: 15-1 archive
91,P01 DETAIL: found 00000006000000000000002E in the repo1: 15-1 archive
91,P02 DETAIL: found 00000006000000000000002F in the repo1: 15-1 archive
91,P01 DETAIL: found 000000060000000000000030 in the repo1: 15-1 archive
91,P00 DETAIL: unable to find 000000060000000000000031 in the archive
91,P00
91,INFO: archive-get:async command end: completed successfully
91,[filtered 11 lines of output]
91,pg-primary ⇒ Fix streaming replication by changing the replication password
91,"sudo -u postgres psql -c ""alter user replicator password 'jw8s0F4'"""
91,ALTER ROLE
91,Backup from a Standby
91,pgBackRest can perform backups on a standby instead of the primary. Standby backups require the pg-standby host to be configured and the backup-standby option enabled. If more than one standby is configured then the first running standby found will be used for the backup.
91,repository:/etc/pgbackrest/pgbackrest.conf ⇒ Configure pg2-host/pg2-host-user and pg2-path
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/15/demo
91,pg2-host=pg-standby
91,pg2-path=/var/lib/postgresql/15/demo
91,[demo-alt]
91,pg1-host=pg-alt
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,backup-standby=y
91,process-max=3
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,"Both the primary and standby databases are required to perform the backup, though the vast majority of the files will be copied from the standby to reduce load on the primary. The database hosts can be configured in any order. pgBackRest will automatically determine which is the primary and which is the standby."
91,repository ⇒ Backup the demo cluster from pg2
91,sudo -u pgbackrest pgbackrest --stanza=demo --log-level-console=detail backup
91,[filtered 2 lines of output]
91,P00
91,INFO: execute non-exclusive backup start: backup begins after the requested immediate checkpoint completes
91,P00
91,"INFO: backup start archive = 000000060000000000000032, lsn = 0/32000028"
91,P00
91,INFO: wait for replay on the standby to reach 0/32000028
91,P00
91,INFO: replay on the standby reached 0/32000028
91,P00
91,INFO: check archive for prior segment 000000060000000000000031
91,"P01 DETAIL: backup file pg-primary:/var/lib/postgresql/15/demo/global/pg_control (8KB, 0.53%) checksum 19fda5d971a818c5ad39d6a2bcd59be4de908b7f"
91,"P01 DETAIL: backup file pg-primary:/var/lib/postgresql/15/demo/pg_logical/replorigin_checkpoint (8B, 0.53%) checksum 347fc8f2df71bd4436e38bd1516ccd7ea0d46532"
91,"P02 DETAIL: backup file pg-standby:/var/lib/postgresql/15/demo/base/5/1249 (456KB, 31.18%) checksum d8f11445af3d7f83450ac96751507a6f19ae8e77"
91,"P03 DETAIL: backup file pg-standby:/var/lib/postgresql/15/demo/base/5/2658 (120KB, 39.24%) checksum e99ed46c6b5b1f83d2ae6debe728b867ccc92770"
91,[filtered 1274 lines of output]
91,This incremental backup shows that most of the files are copied from the pg-standby host and only a few are copied from the pg-primary host.
91,"pgBackRest creates a standby backup that is identical to a backup performed on the primary. It does this by starting/stopping the backup on the pg-primary host, copying only files that are replicated from the pg-standby host, then copying the remaining few files from the pg-primary host. This means that logs and statistics from the primary database will be included in the backup."
91,Upgrading PostgreSQL
91,"Immediately after upgrading PostgreSQL to a newer major version, the pg-path for all pgBackRest configurations must be set to the new database location and the stanza-upgrade command run. If there is more than one repository configured on the host, the stanza will be upgraded on each. If the database is offline use the --no-online option."
91,"The following instructions are not meant to be a comprehensive guide for upgrading PostgreSQL, rather they outline the general process for upgrading a primary and standby with the intent of demonstrating the steps required to reconfigure pgBackRest. It is recommended that a backup be taken prior to upgrading."
91,pg-primary ⇒ Stop old cluster
91,sudo pg_ctlcluster 15 demo stop
91,Stop the old cluster on the standby since it will be restored from the newly upgraded cluster.
91,pg-standby ⇒ Stop old cluster
91,sudo pg_ctlcluster 15 demo stop
91,Create the new cluster and perform upgrade.
91,pg-primary ⇒ Create new cluster and perform the upgrade
91,sudo -u postgres /usr/lib/postgresql/16/bin/initdb \
91,-D /var/lib/postgresql/16/demo -k -A peer
91,sudo pg_createcluster 16 demo
91,sudo -u postgres sh -c 'cd /var/lib/postgresql && \
91,/usr/lib/postgresql/16/bin/pg_upgrade \
91,--old-bindir=/usr/lib/postgresql/15/bin \
91,--new-bindir=/usr/lib/postgresql/16/bin \
91,--old-datadir=/var/lib/postgresql/15/demo \
91,--new-datadir=/var/lib/postgresql/16/demo \
91,"--old-options="" -c config_file=/etc/postgresql/15/demo/postgresql.conf"" \"
91,"--new-options="" -c config_file=/etc/postgresql/16/demo/postgresql.conf""'"
91,[filtered 42 lines of output]
91,Checking for extension updates
91,Upgrade Complete
91,----------------
91,Optimizer statistics are not transferred by pg_upgrade.
91,[filtered 3 lines of output]
91,Configure the new cluster settings and port.
91,pg-primary:/etc/postgresql/16/demo/postgresql.conf ⇒ Configure PostgreSQL
91,archive_command = 'pgbackrest --stanza=demo archive-push %p'
91,archive_mode = on
91,max_wal_senders = 3
91,wal_level = replica
91,Update the pgBackRest configuration on all systems to point to the new cluster.
91,pg-primary:/etc/pgbackrest/pgbackrest.conf ⇒ Upgrade the pg1-path
91,[demo]
91,pg1-path=/var/lib/postgresql/16/demo
91,[global]
91,archive-async=y
91,log-level-file=detail
91,repo1-host=repository
91,spool-path=/var/spool/pgbackrest
91,[global:archive-get]
91,process-max=2
91,[global:archive-push]
91,process-max=2
91,pg-standby:/etc/pgbackrest/pgbackrest.conf ⇒ Upgrade the pg-path
91,[demo]
91,pg1-path=/var/lib/postgresql/16/demo
91,recovery-option=primary_conninfo=host=172.17.0.6 port=5432 user=replicator
91,[global]
91,archive-async=y
91,log-level-file=detail
91,repo1-host=repository
91,spool-path=/var/spool/pgbackrest
91,[global:archive-get]
91,process-max=2
91,[global:archive-push]
91,process-max=2
91,"repository:/etc/pgbackrest/pgbackrest.conf ⇒ Upgrade pg1-path and pg2-path, disable backup from standby"
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/16/demo
91,pg2-host=pg-standby
91,pg2-path=/var/lib/postgresql/16/demo
91,[demo-alt]
91,pg1-host=pg-alt
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,backup-standby=n
91,process-max=3
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,pg-primary ⇒ Copy hba configuration
91,sudo cp /etc/postgresql/15/demo/pg_hba.conf \
91,/etc/postgresql/16/demo/pg_hba.conf
91,"Before starting the new cluster, the stanza-upgrade command must be run."
91,pg-primary ⇒ Upgrade the stanza
91,sudo -u postgres pgbackrest --stanza=demo --no-online \
91,--log-level-console=info stanza-upgrade
91,P00
91,INFO: stanza-upgrade command begin 2.50: --exec-id=2887-2547ee96 --log-level-console=info --log-level-file=detail --log-level-stderr=off --no-log-timestamp --no-online --pg1-path=/var/lib/postgresql/16/demo --repo1-host=repository --stanza=demo
91,P00
91,INFO: stanza-upgrade for stanza 'demo' on repo1
91,P00
91,INFO: stanza-upgrade command end: completed successfully
91,Start the new cluster and confirm it is successfully installed.
91,pg-primary ⇒ Start new cluster
91,sudo pg_ctlcluster 16 demo start
91,Test configuration using the check command.
91,pg-primary ⇒ Check configuration
91,sudo -u postgres pg_lsclusters
91,sudo -u postgres pgbackrest --stanza=demo check
91,Remove the old cluster.
91,pg-primary ⇒ Remove old cluster
91,sudo pg_dropcluster 15 demo
91,Install the new PostgreSQL binaries on the standby and create the cluster.
91,pg-standby ⇒ Remove old cluster and create the new cluster
91,sudo pg_dropcluster 15 demo
91,sudo pg_createcluster 16 demo
91,Run the check on the repository host. The warning regarding the standby being down is expected since the standby cluster is down. Running this command demonstrates that the repository server is aware of the standby and is configured properly for the primary server.
91,repository ⇒ Check configuration
91,sudo -u pgbackrest pgbackrest --stanza=demo check
91,P00
91,"WARN: unable to check pg2: [DbConnectError] raised from remote-0 ssh protocol on 'pg-standby': unable to connect to 'dbname='postgres' port=5432': connection to server on socket ""/var/run/postgresql/.s.PGSQL.5432"" failed: No such file or directory"
91,Is the server running locally and accepting connections on that socket?
91,Run a full backup on the new cluster and then restore the standby from the backup. The backup type will automatically be changed to full if incr or diff is requested.
91,repository ⇒ Run a full backup
91,sudo -u pgbackrest pgbackrest --stanza=demo --type=full backup
91,pg-standby ⇒ Restore the demo standby cluster
91,sudo -u postgres pgbackrest --stanza=demo --delta --type=standby restore
91,pg-standby:/etc/postgresql/16/demo/postgresql.conf ⇒ Configure PostgreSQL
91,hot_standby = on
91,pg-standby ⇒ Start PostgreSQL and check the pgBackRest configuration
91,sudo pg_ctlcluster 16 demo start
91,sudo -u postgres pgbackrest --stanza=demo check
91,Backup from standby can be enabled now that the standby is restored.
91,repository:/etc/pgbackrest/pgbackrest.conf ⇒ Reenable backup from standby
91,[demo]
91,pg1-host=pg-primary
91,pg1-path=/var/lib/postgresql/16/demo
91,pg2-host=pg-standby
91,pg2-path=/var/lib/postgresql/16/demo
91,[demo-alt]
91,pg1-host=pg-alt
91,pg1-path=/var/lib/postgresql/15/demo
91,[global]
91,backup-standby=y
91,process-max=3
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,start-fast=y
91,"Copyright © 2015-2024, The PostgreSQL Global Development Group, MIT License. Updated January 22, 2024"
92,"Optimize Supabase API: Rate Limiting GuideMansueli's tipsFollowMansueli's tipsFollowRate Limiting Supabase Requests with PostgreSQL and pg_headerkitRodrigo Mansueli·Sep 5, 2023Table of contentsIntroductionPrerequisitesSetting up the EnvironmentCreating the Rate Limiting InfrastructureThe request_log TableThe register_request FunctionCleaning Old RequestsImplementing Rate LimitingThe exceeded_rate_limit FunctionThe check_rate_limit FunctionConfiguring pg_headerkit with PostgRESTScheduled CleanupConclusionReferencesIntroduction"
92,"Rate limiting is a critical aspect of web applications that ensures fair usage of resources and prevents abuse. In this blog post, we'll explore how to implement rate limiting for Supabase requests using PostgreSQL and the pg_headerkit extension. This article is part of a series on optimizing Supabase performance, and it builds upon our previous guide on Boosting Supabase Reliability."
92,"Supabase, a powerful backend platform built on top of PostgreSQL, relies on PostgreSQL as its underlying database. By leveraging PostgreSQL and pg_headerkit, we can efficiently control the rate at which requests are made to Supabase, ensuring optimal performance and resource allocation."
92,Prerequisites
92,"Before we dive into the implementation, make sure you have the following prerequisites:"
92,Supabase Project: Ensure you have an existing Supabase project with the necessary API endpoints set up.
92,"PostgreSQL Database: Use PostgreSQL as your backend database for Supabase. If you haven't set up PostgreSQL with Supabase yet, follow the official documentation to get started."
92,pg_headerkit: You will need to install the pg_headerkit library. Find installation instructions and more information on this library at https://database.dev/burggraf/pg_headerkit.
92,Setting up the Environment
92,"Before we dive into rate limiting, let's ensure we have the necessary prerequisites in place:"
92,Supabase Account: Make sure you have a Supabase account set up.
92,Database.dev: Install dbdev using https://database.dev/installer.
92,"Next, let's install and set up pg_headerkit:"
92,SELECT dbdev.install('burggraf-pg_headerkit');
92,"CREATE EXTENSION ""burggraf-pg_headerkit"" VERSION '1.0.0';"
92,Creating the Rate Limiting Infrastructure
92,"In this section, we'll dive into the process of creating the essential infrastructure for rate limiting within your Supabase-powered application. Rate limiting is a crucial mechanism that allows you to control the number of requests made to your Supabase endpoints, ensuring fair usage of resources and maintaining system stability."
92,The request_log Table
92,We begin with the creation of the request_log table. This table serves as a main component for tracking and monitoring incoming requests. Here's how we set it up:
92,CREATE UNLOGGED TABLE request_log (
92,"id BIGINT GENERATED BY DEFAULT AS IDENTITY,"
92,"ip inet NOT NULL,"
92,timestamp timestamptz DEFAULT NOW()
92,The request_log table has three essential columns:
92,"id: A unique identifier for each log entry, automatically generated."
92,"ip: This column captures the client's IP address, helping us identify the source of each request."
92,"timestamp: It records the exact time each request was made, ensuring accurate tracking."
92,The register_request Function
92,"With the request_log table in place, we proceed to create the register_request function. This function plays a pivotal role in the rate-limiting process by logging every incoming request and associating it with the client's IP address. Here's how it's defined:"
92,CREATE OR REPLACE FUNCTION register_request(ip_in TEXT)
92,RETURNS VOID
92,LANGUAGE plpgsql AS $$
92,BEGIN
92,INSERT INTO request_log (ip)
92,VALUES (inet(ip_in));
92,END;
92,$$;
92,"The register_request function takes the client's IP address as input and inserts a corresponding entry into the request_log table. This action ensures that we have a comprehensive record of all incoming requests, which is essential for rate limiting and analytics."
92,"With the infrastructure for tracking requests established, we're now ready to move forward with the rate-limiting implementation. In the following sections, we'll explore how to set rate limits and enforce them effectively."
92,Cleaning Old Requests
92,"To maintain the efficiency of our system, it's crucial to regularly clean up old request logs. The clean_old_requests function takes care of this task:"
92,CREATE OR REPLACE FUNCTION clean_old_requests()
92,RETURNS VOID
92,LANGUAGE plpgsql AS $$
92,BEGIN
92,-- Delete request logs older than 12 hours
92,DELETE FROM request_log
92,WHERE timestamp < NOW() - INTERVAL '12 hours';
92,END;
92,$$;
92,This function ensures that our database remains clutter-free and retains only the most relevant request data.
92,Implementing Rate Limiting
92,"Now, let's delve into implementing rate limiting within our Supabase-powered application. Rate limiting is essential to prevent abuse and ensure fair resource allocation. We achieve this through the exceeded_rate_limit and check_rate_limit functions."
92,The exceeded_rate_limit Function
92,"The exceeded_rate_limit function is responsible for checking if a client has exceeded the rate limit, which in this example is set at 5 requests per minute. Here's how it's defined:"
92,CREATE OR REPLACE FUNCTION exceeded_rate_limit(ip_in TEXT)
92,RETURNS BOOLEAN
92,LANGUAGE plpgsql AS $$
92,DECLARE
92,request_count INTEGER;
92,BEGIN
92,SELECT count(*) INTO request_count
92,FROM request_log
92,WHERE ip = inet(ip_in) AND timestamp > NOW() - INTERVAL '1 minute';
92,RETURN request_count >= 5; -- limit of 5 requests per minute
92,END;
92,$$;
92,This function counts the number of requests made by a client within the last minute and returns true if the limit is exceeded.
92,The check_rate_limit Function
92,"The check_rate_limit function is pivotal for enforcing rate limits. It effectively manages rate limiting by logging the current request using the register_request function and verifying if the rate limit has been surpassed. If the limit is exceeded, it raises an exception:"
92,CREATE OR REPLACE FUNCTION check_rate_limit()
92,RETURNS VOID
92,LANGUAGE plpgsql
92,"SET search_path = public, hdr, extensions"
92,SECURITY DEFINER
92,AS $$
92,DECLARE
92,current_ip TEXT := hdr.ip();
92,"request_method TEXT := current_setting('request.method', TRUE);"
92,BEGIN
92,-- Only log non-GET requests because they are run
92,-- in read-only transactions
92,IF request_method IS NULL OR request_method <> 'GET' THEN
92,PERFORM register_request(current_ip);
92,END IF;
92,-- Check if the rate limit has been exceeded
92,-- and raise an exception if necessary
92,IF exceeded_rate_limit(current_ip) THEN
92,RAISE EXCEPTION 'Rate limit exceeded';
92,END IF;
92,END;
92,$$;
92,"This function is a crucial component of your rate-limiting strategy, ensuring that each incoming request is correctly monitored and preventing clients from exceeding their allocated rate limits. It's important to note that this function primarily focuses on rate limiting for insert operations. While rate limiting for GET requests is possible, it may introduce performance concerns, such as making network requests that insert rate-limiting data."
92,Configuring pg_headerkit with PostgREST
92,"To seamlessly integrate rate limiting with your Supabase-powered application, configure the pgrst.db_pre_request option to utilize the check_rate_limit function as a pre-request action within PostgREST:"
92,ALTER ROLE authenticator
92,SET pgrst.db_pre_request = 'check_rate_limit';
92,"NOTIFY pgrst, 'reload config';"
92,"This configuration ensures that every request made to Supabase undergoes rate limit validation before execution, guaranteeing a fair and controlled usage of resources. Now, we can test the rate limit by sending a few post requests to a table:"
92,Scheduled Cleanup
92,Maintaining the performance of your database requires periodic cleanup of old request logs. Schedule the clean_old_requests function to run automatically every midnight:
92,SELECT cron.schedule(
92,"'clean_old_requests',"
92,"'0 0 * * *', -- Run every midnight"
92,$$ SELECT clean_old_requests(); $$
92,"This automated cleanup process is crucial for keeping your database in an optimal state, free from unnecessary clutter, and ensuring efficient resource management."
92,Conclusion
92,"In this comprehensive blog post, we've delved into the intricacies of implementing rate limiting for your Supabase-powered applications. Leveraging the power of PostgreSQL and the versatile pg_headerkit extension, we've provided you with a step-by-step guide to ensure fair resource allocation and safeguard your application against abuse."
92,"Rate limiting is a fundamental tool in your arsenal to maintain top-notch performance and deliver a consistently excellent user experience. Armed with the knowledge gained from this article, you're now well-prepared to seamlessly integrate rate limiting into your Supabase application."
92,"Don't stop here; take the concepts discussed in this post and adapt them to your specific use cases. Experiment, explore, and fine-tune your rate-limiting strategy to perfectly align with your application's unique requirements."
92,"If you found this article valuable, you might also be interested in exploring related topics:"
92,Boosting Supabase Reliability: A Guide to Using Postgres Foreign Data Wrappers: Learn how to enhance the reliability of your Supabase applications.
92,Exploring Data Relationships with Supabase and PostgreSQL: Dive deeper into understanding data relationships in Supabase.
92,Safeguarding Data Integrity with pg-safeupdate in PostgreSQL and Supabase: Explore techniques to maintain data integrity in your database.
92,"For further information and guidance, consider these valuable references. For any questions, feedback, or assistance, please don't hesitate to reach out to me. We're here to help you on your journey to mastering rate limiting in Supabase."
92,References
92,"For further information and guidance, consider these valuable references:"
92,Supabase Official Documentation: Explore Supabase's official documentation for in-depth insights into this powerful backend platform.
92,"PostgreSQL Official Documentation: Dive into the official documentation of PostgreSQL, the robust database system at the core of Supabase."
92,"pg_headerkit Extension: For more details on pg_headerkit, visit the official GitHub repository."
92,PostgREST Official Documentation: Discover PostgREST's documentation for additional information on this helpful tool.
92,PostgreSQLratelimitsupabaseDatabasesAPIs Share this
94,JDBC To Other Databases - Spark 3.4.1 Documentation
94,3.4.1
94,Overview
94,Programming Guides
94,Quick Start
94,"RDDs, Accumulators, Broadcasts Vars"
94,"SQL, DataFrames, and Datasets"
94,Structured Streaming
94,Spark Streaming (DStreams)
94,MLlib (Machine Learning)
94,GraphX (Graph Processing)
94,SparkR (R on Spark)
94,PySpark (Python on Spark)
94,API Docs
94,Scala
94,Java
94,Python
94,"SQL, Built-in Functions"
94,Deploying
94,Overview
94,Submitting Applications
94,Spark Standalone
94,Mesos
94,YARN
94,Kubernetes
94,More
94,Configuration
94,Monitoring
94,Tuning Guide
94,Job Scheduling
94,Security
94,Hardware Provisioning
94,Migration Guide
94,Building Spark
94,Contributing to Spark
94,Third Party Projects
94,Spark SQL Guide
94,Getting Started
94,Data Sources
94,Generic Load/Save Functions
94,Generic File Source Options
94,Parquet Files
94,ORC Files
94,JSON Files
94,CSV Files
94,Text Files
94,Hive Tables
94,JDBC To Other Databases
94,Avro Files
94,Protobuf data
94,Whole Binary Files
94,Troubleshooting
94,Performance Tuning
94,Distributed SQL Engine
94,PySpark Usage Guide for Pandas with Apache Arrow
94,Migration Guide
94,SQL Reference
94,Error Conditions
94,JDBC To Other Databases
94,Data Source Option
94,Spark SQL also includes a data source that can read data from other databases using JDBC. This
94,functionality should be preferred over using JdbcRDD.
94,This is because the results are returned
94,as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
94,The JDBC data source is also easier to use from Java or Python as it does not require the user to
94,provide a ClassTag.
94,"(Note that this is different than the Spark SQL JDBC server, which allows other applications to"
94,run queries using Spark SQL).
94,To get started you will need to include the JDBC driver for your particular database on the
94,"spark classpath. For example, to connect to postgres from the Spark Shell you would run the"
94,following command:
94,./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
94,Data Source Option
94,Spark supports the following case-insensitive options for JDBC. The Data source options of JDBC can be set via:
94,the .option/.options methods of
94,DataFrameReader
94,DataFrameWriter
94,OPTIONS clause at CREATE TABLE USING DATA_SOURCE
94,"For connection properties, users can specify the JDBC connection properties in the data source options."
94,user and password are normally provided as connection properties for
94,logging into the data sources.
94,Property NameDefaultMeaningScope
94,url
94,(none)
94,"The JDBC URL of the form jdbc:subprotocol:subname to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret"
94,read/write
94,dbtable
94,(none)
94,The JDBC table that should be read from or written into. Note that when using it in the read
94,path anything that is valid in a FROM clause of a SQL query can be used.
94,"For example, instead of a full table you could also use a subquery in parentheses. It is not"
94,allowed to specify dbtable and query options at the same time.
94,read/write
94,query
94,(none)
94,A query that will be used to read data into Spark. The specified query will be parenthesized and used
94,as a subquery in the FROM clause. Spark will also assign an alias to the subquery clause.
94,"As an example, spark will issue a query of the following form to the JDBC Source."
94,SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
94,Below are a couple of restrictions while using this option.
94,It is not allowed to specify dbtable and query options at the same time.
94,It is not allowed to specify query and partitionColumn options at the same time. When specifying
94,"partitionColumn option is required, the subquery can be specified using dbtable option instead and"
94,partition columns can be qualified using the subquery alias provided as part of dbtable.
94,Example:
94,"spark.read.format(""jdbc"")"
94,".option(""url"", jdbcUrl)"
94,".option(""query"", ""select c1, c2 from t1"")"
94,.load()
94,read/write
94,prepareQuery
94,(none)
94,A prefix that will form the final query together with query.
94,As the specified query will be parenthesized as a subquery in the FROM clause and some databases do not
94,"support all clauses in subqueries, the prepareQuery property offers a way to run such complex queries."
94,"As an example, spark will issue a query of the following form to the JDBC Source."
94,<prepareQuery> SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
94,Below are a couple of examples.
94,MSSQL Server does not accept WITH clauses in subqueries but it is possible to split such a query to prepareQuery and query:
94,"spark.read.format(""jdbc"")"
94,".option(""url"", jdbcUrl)"
94,".option(""prepareQuery"", ""WITH t AS (SELECT x, y FROM tbl)"")"
94,".option(""query"", ""SELECT * FROM t WHERE x > 10"")"
94,.load()
94,MSSQL Server does not accept temp table clauses in subqueries but it is possible to split such a query to prepareQuery and query:
94,"spark.read.format(""jdbc"")"
94,".option(""url"", jdbcUrl)"
94,".option(""prepareQuery"", ""(SELECT * INTO #TempTable FROM (SELECT * FROM tbl) t)"")"
94,".option(""query"", ""SELECT * FROM #TempTable"")"
94,.load()
94,read/write
94,driver
94,(none)
94,The class name of the JDBC driver to use to connect to this URL.
94,read/write
94,"partitionColumn, lowerBound, upperBound"
94,(none)
94,"These options must all be specified if any of them is specified. In addition,"
94,numPartitions must be specified. They describe how to partition the table when
94,reading in parallel from multiple workers.
94,"partitionColumn must be a numeric, date, or timestamp column from the table in question."
94,Notice that lowerBound and upperBound are just used to decide the
94,"partition stride, not for filtering the rows in table. So all rows in the table will be"
94,partitioned and returned. This option applies only to reading.
94,Example:
94,"spark.read.format(""jdbc"")"
94,".option(""url"", jdbcUrl)"
94,".option(""dbtable"", ""(select c1, c2 from t1) as subq"")"
94,".option(""partitionColumn"", ""c1"")"
94,".option(""lowerBound"", ""1"")"
94,".option(""upperBound"", ""100"")"
94,".option(""numPartitions"", ""3"")"
94,.load()
94,read
94,numPartitions
94,(none)
94,The maximum number of partitions that can be used for parallelism in table reading and
94,writing. This also determines the maximum number of concurrent JDBC connections.
94,"If the number of partitions to write exceeds this limit, we decrease it to this limit by"
94,calling coalesce(numPartitions) before writing.
94,read/write
94,queryTimeout
94,The number of seconds the driver will wait for a Statement object to execute to the given
94,"number of seconds. Zero means there is no limit. In the write path, this option depends on"
94,"how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver"
94,checks the timeout of each query instead of an entire JDBC batch.
94,read/write
94,fetchsize
94,"The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows)."
94,read
94,batchsize
94,1000
94,"The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing."
94,write
94,isolationLevel
94,READ_UNCOMMITTED
94,"The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in java.sql.Connection."
94,write
94,sessionInitStatement
94,(none)
94,"After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(""sessionInitStatement"", """"""BEGIN execute immediate 'alter session set ""_serial_direct_read""=true'; END;"""""")"
94,read
94,truncate
94,false
94,"This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off truncate option to use DROP TABLE again. Also, due to the different behavior of TRUNCATE TABLE among DBMS, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect, the user option truncate is ignored."
94,write
94,cascadeTruncate
94,"the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect"
94,"This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care."
94,write
94,createTableOptions
94,"This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.)."
94,write
94,createTableColumnTypes
94,(none)
94,"The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: ""name CHAR(64), comments VARCHAR(1024)""). The specified types should be valid spark sql data types."
94,write
94,customSchema
94,(none)
94,"The custom schema to use for reading data from JDBC connectors. For example, ""id DECIMAL(38, 0), name STRING"". You can also specify partial fields, and the others use the default type mapping. For example, ""id DECIMAL(38, 0)"". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults."
94,read
94,pushDownPredicate
94,true
94,"The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source."
94,read
94,pushDownAggregate
94,false
94,"The option to enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Otherwise, if sets to true, aggregates will be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If numPartitions equals to 1 or the group by key is the same as partitionColumn, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output."
94,read
94,pushDownLimit
94,false
94,"The option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is false, in which case Spark does not push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to true, LIMIT or LIMIT with SORT is pushed down to the JDBC data source. If numPartitions is greater than 1, Spark still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and numPartitions equals to 1, Spark will not apply LIMIT or LIMIT with SORT on the result from data source."
94,read
94,pushDownOffset
94,false
94,"The option to enable or disable OFFSET push-down into V2 JDBC data source. The default value is false, in which case Spark will not push down OFFSET to the JDBC data source. Otherwise, if sets to true, Spark will try to push down OFFSET to the JDBC data source. If pushDownOffset is true and numPartitions is equal to 1, OFFSET will be pushed down to the JDBC data source. Otherwise, OFFSET will not be pushed down and Spark still applies OFFSET on the result from data source."
94,read
94,pushDownTableSample
94,false
94,"The option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is false, in which case Spark does not push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to true, TABLESAMPLE is pushed down to the JDBC data source."
94,read
94,keytab
94,(none)
94,"Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by --files option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise --files assumed. If both keytab and principal are defined then Spark tries to do kerberos authentication."
94,read/write
94,principal
94,(none)
94,Specifies kerberos principal name for the JDBC client. If both keytab and principal are defined then Spark tries to do kerberos authentication.
94,read/write
94,refreshKrb5Config
94,false
94,This option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before
94,"establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false."
94,"The default value is false. Note that if you set this option to true and try to establish multiple connections,"
94,a race condition can occur. One possble situation would be like as follows.
94,refreshKrb5Config flag is set with security context 1
94,A JDBC connection provider is used for the corresponding DBMS
94,The krb5.conf is modified but the JVM not yet realized that it must be reloaded
94,Spark authenticates successfully for security context 1
94,The JVM loads security context 2 from the modified krb5.conf
94,Spark restores the previously saved security context 1
94,The modified krb5.conf content just gone
94,read/write
94,connectionProvider
94,(none)
94,"The name of the JDBC connection provider to use to connect to this URL, e.g. db2, mssql."
94,Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle
94,the specified driver and options. The selected provider must not be disabled by spark.sql.sources.disabledJdbcConnProviderList.
94,read/write
94,preferTimestampNTZ
94,false
94,"When the option is set to true, all timestamps are inferred as TIMESTAMP WITHOUT TIME ZONE."
94,"Otherwise, timestamps are read as TIMESTAMP with local time zone."
94,read
94,Note that kerberos authentication with keytab is not always supported by the JDBC driver.
94,"Before using keytab and principal configuration options, please make sure the following requirements are met:"
94,The included JDBC driver version supports kerberos authentication with keytab.
94,There is a built-in connection provider which supports the used database.
94,There is a built-in connection providers for the following databases:
94,DB2
94,MariaDB
94,MS Sql
94,Oracle
94,PostgreSQL
94,"If the requirements are not met, please consider using the JdbcConnectionProvider developer API to handle custom authentication."
94,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
94,// Loading data from a JDBC source
94,val jdbcDF = spark.read
94,".format(""jdbc"")"
94,".option(""url"", ""jdbc:postgresql:dbserver"")"
94,".option(""dbtable"", ""schema.tablename"")"
94,".option(""user"", ""username"")"
94,".option(""password"", ""password"")"
94,.load()
94,val connectionProperties = new Properties()
94,"connectionProperties.put(""user"", ""username"")"
94,"connectionProperties.put(""password"", ""password"")"
94,val jdbcDF2 = spark.read
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
94,// Specifying the custom data types of the read schema
94,"connectionProperties.put(""customSchema"", ""id DECIMAL(38, 0), name STRING"")"
94,val jdbcDF3 = spark.read
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
94,// Saving data to a JDBC source
94,jdbcDF.write
94,".format(""jdbc"")"
94,".option(""url"", ""jdbc:postgresql:dbserver"")"
94,".option(""dbtable"", ""schema.tablename"")"
94,".option(""user"", ""username"")"
94,".option(""password"", ""password"")"
94,.save()
94,jdbcDF2.write
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
94,// Specifying create table column data types on write
94,jdbcDF.write
94,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
94,"Find full example code at ""examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"" in the Spark repo."
94,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
94,// Loading data from a JDBC source
94,Dataset<Row> jdbcDF = spark.read()
94,".format(""jdbc"")"
94,".option(""url"", ""jdbc:postgresql:dbserver"")"
94,".option(""dbtable"", ""schema.tablename"")"
94,".option(""user"", ""username"")"
94,".option(""password"", ""password"")"
94,.load();
94,Properties connectionProperties = new Properties();
94,"connectionProperties.put(""user"", ""username"");"
94,"connectionProperties.put(""password"", ""password"");"
94,Dataset<Row> jdbcDF2 = spark.read()
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
94,// Saving data to a JDBC source
94,jdbcDF.write()
94,".format(""jdbc"")"
94,".option(""url"", ""jdbc:postgresql:dbserver"")"
94,".option(""dbtable"", ""schema.tablename"")"
94,".option(""user"", ""username"")"
94,".option(""password"", ""password"")"
94,.save();
94,jdbcDF2.write()
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
94,// Specifying create table column data types on write
94,jdbcDF.write()
94,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
94,"Find full example code at ""examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java"" in the Spark repo."
94,# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
94,# Loading data from a JDBC source
94,jdbcDF = spark.read \
94,".format(""jdbc"") \"
94,".option(""url"", ""jdbc:postgresql:dbserver"") \"
94,".option(""dbtable"", ""schema.tablename"") \"
94,".option(""user"", ""username"") \"
94,".option(""password"", ""password"") \"
94,.load()
94,jdbcDF2 = spark.read \
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
94,"properties={""user"": ""username"", ""password"": ""password""})"
94,# Specifying dataframe column data types on read
94,jdbcDF3 = spark.read \
94,".format(""jdbc"") \"
94,".option(""url"", ""jdbc:postgresql:dbserver"") \"
94,".option(""dbtable"", ""schema.tablename"") \"
94,".option(""user"", ""username"") \"
94,".option(""password"", ""password"") \"
94,".option(""customSchema"", ""id DECIMAL(38, 0), name STRING"") \"
94,.load()
94,# Saving data to a JDBC source
94,jdbcDF.write \
94,".format(""jdbc"") \"
94,".option(""url"", ""jdbc:postgresql:dbserver"") \"
94,".option(""dbtable"", ""schema.tablename"") \"
94,".option(""user"", ""username"") \"
94,".option(""password"", ""password"") \"
94,.save()
94,jdbcDF2.write \
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
94,"properties={""user"": ""username"", ""password"": ""password""})"
94,# Specifying create table column data types on write
94,jdbcDF.write \
94,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"") \"
94,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
94,"properties={""user"": ""username"", ""password"": ""password""})"
94,"Find full example code at ""examples/src/main/python/sql/datasource.py"" in the Spark repo."
94,# Loading data from a JDBC source
94,"df <- read.jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
94,# Saving data to a JDBC source
94,"write.jdbc(df, ""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
94,"Find full example code at ""examples/src/main/r/RSparkSQLExample.R"" in the Spark repo."
94,CREATE TEMPORARY VIEW jdbcTable
94,USING org.apache.spark.sql.jdbc
94,OPTIONS (
94,"url ""jdbc:postgresql:dbserver"","
94,"dbtable ""schema.tablename"","
94,"user 'username',"
94,password 'password'
94,INSERT INTO TABLE jdbcTable
94,SELECT * FROM resultTable
95,Query Performance Tuning — Citus 11.3 documentation
95,Skip to main content >
95,Citus
95,v11.3
95,Get Started
95,What is Citus?
95,Citus Gives You Postgres At Any Scale
95,When to Use Citus
95,Multi-Tenant SaaS Database
95,Real-Time Analytics
95,Considerations for Use
95,When Citus is Inappropriate
95,Quick Tutorials
95,Multi-tenant Applications
95,Data model and sample data
95,Creating tables
95,Distributing tables and loading data
95,Running queries
95,Real-time Analytics
95,Data model and sample data
95,Creating tables
95,Distributing tables and loading data
95,Running queries
95,Install
95,Single-Node Citus
95,Docker (Mac or Linux)
95,Ubuntu or Debian
95,"Fedora, CentOS, or Red Hat"
95,Multi-Node Citus
95,Ubuntu or Debian
95,Steps to be executed on all nodes
95,Steps to be executed on the coordinator node
95,"Fedora, CentOS, or Red Hat"
95,Steps to be executed on all nodes
95,Steps to be executed on the coordinator node
95,Managed Service
95,Use-Case Guides
95,Multi-tenant Applications
95,Let’s Make an App – Ad Analytics
95,Scaling the Relational Data Model
95,Preparing Tables and Ingesting Data
95,Try it Yourself
95,Integrating Applications
95,Sharing Data Between Tenants
95,Online Changes to the Schema
95,When Data Differs Across Tenants
95,Scaling Hardware Resources
95,Dealing with Big Tenants
95,Where to Go From Here
95,Real-Time Dashboards
95,Data Model
95,Rollups
95,Expiring Old Data
95,Approximate Distinct Counts
95,Unstructured Data with JSONB
95,Timeseries Data
95,Scaling Timeseries Data on Citus
95,Automating Partition Creation
95,Archiving with Columnar Storage
95,Archiving a Row Partition to Columnar Storage
95,Architecture
95,Concepts
95,Nodes
95,Coordinator and Workers
95,Distributed Data
95,Table Types
95,Type 1: Distributed Tables
95,Type 2: Reference Tables
95,Type 3: Local Tables
95,Shards
95,Shard Placements
95,Co-Location
95,Parallelism
95,Query Execution
95,Develop
95,Determining Application Type
95,At a Glance
95,Examples and Characteristics
95,Choosing Distribution Column
95,Multi-Tenant Apps
95,Best Practices
95,Real-Time Apps
95,Best Practices
95,Timeseries Data
95,Best Practices
95,Table Co-Location
95,Data co-location in Citus for hash-distributed tables
95,A practical example of co-location
95,Using Regular PostgreSQL Tables
95,Distributing tables by ID
95,Distributing tables by tenant
95,Co-location means better feature support
95,Query Performance
95,Migrating an Existing App
95,Identify Distribution Strategy
95,Pick distribution key
95,Identify types of tables
95,Prepare Source Tables for Migration
95,Add distribution keys
95,Backfill newly created columns
95,Prepare Application for Citus
95,Set up Development Citus Cluster
95,Include distribution column in keys
95,Add distribution key to queries
95,Ruby on Rails
95,Django Multitenant
95,ASP.NET
95,Java Hibernate
95,Other (SQL Principles)
95,Enable Secure Connections
95,Check for cross-node traffic
95,Migrate Production Data
95,Small Database Migration
95,Big Database Migration
95,Duplicate schema
95,Enable logical replication
95,Open access for network connection
95,Begin Replication
95,Switch over to Citus and stop all connections to old database
95,SQL Reference
95,Creating and Modifying Distributed Objects (DDL)
95,Creating And Distributing Tables
95,Reference Tables
95,Distributing Coordinator Data
95,Co-Locating Tables
95,Dropping Tables
95,Modifying Tables
95,Adding/Modifying Columns
95,Adding/Removing Constraints
95,Using NOT VALID Constraints
95,Adding/Removing Indices
95,Types and Functions
95,Manual Modification
95,"Ingesting, Modifying Data (DML)"
95,Inserting Data
95,“From Select” Clause (Distributed Rollups)
95,COPY Command (Bulk load)
95,Caching Aggregations with Rollups
95,Updates and Deletion
95,Maximizing Write Performance
95,Querying Distributed Tables (SQL)
95,Aggregate Functions
95,Count (Distinct) Aggregates
95,Estimating Top N Items
95,Percentile Calculations
95,Limit Pushdown
95,Views on Distributed Tables
95,Joins
95,Co-located joins
95,Reference table joins
95,Repartition joins
95,Query Processing
95,Distributed Query Planner
95,Distributed Query Executor
95,Subquery/CTE Push-Pull Execution
95,PostgreSQL planner and executor
95,Manual Query Propagation
95,Running on all Workers
95,Running on all Shards
95,Limitations
95,SQL Support and Workarounds
95,Workarounds
95,Work around limitations using CTEs
95,Temp Tables: the Workaround of Last Resort
95,Citus API
95,Citus Utility Functions
95,Table and Shard DDL
95,create_distributed_table
95,truncate_local_data_after_distributing_table
95,undistribute_table
95,alter_distributed_table
95,alter_table_set_access_method
95,remove_local_tables_from_metadata
95,create_reference_table
95,citus_add_local_table_to_metadata
95,update_distributed_table_colocation
95,create_distributed_function
95,alter_columnar_table_set
95,create_time_partitions
95,drop_old_time_partitions
95,alter_old_partitions_set_access_method
95,Metadata / Configuration Information
95,citus_add_node
95,citus_update_node
95,citus_set_node_property
95,citus_add_inactive_node
95,citus_activate_node
95,citus_disable_node
95,citus_add_secondary_node
95,citus_remove_node
95,citus_get_active_worker_nodes
95,citus_backend_gpid
95,citus_check_cluster_node_health
95,citus_set_coordinator_host
95,master_get_table_metadata
95,get_shard_id_for_distribution_column
95,column_to_column_name
95,citus_relation_size
95,citus_table_size
95,citus_total_relation_size
95,citus_stat_statements_reset
95,Cluster Management And Repair Functions
95,citus_move_shard_placement
95,citus_rebalance_start
95,citus_rebalance_status
95,citus_rebalance_stop
95,citus_rebalance_wait
95,rebalance_table_shards
95,get_rebalance_table_shards_plan
95,get_rebalance_progress
95,citus_add_rebalance_strategy
95,citus_set_default_rebalance_strategy
95,citus_remote_connection_stats
95,citus_drain_node
95,isolate_tenant_to_new_shard
95,citus_create_restore_point
95,Citus Tables and Views
95,Coordinator Metadata
95,Partition table
95,Shard table
95,Shard information view
95,Shard placement table
95,Worker node table
95,Distributed object table
95,Citus tables view
95,Time partitions view
95,Co-location group table
95,Rebalancer strategy table
95,Query statistics table
95,Tenant-level query statistics view
95,Distributed Query Activity
95,Tables on all Nodes
95,Connection Credentials Table
95,Connection Pooling Credentials
95,Configuration Reference
95,General configuration
95,citus.max_background_task_executors_per_node (integer)
95,citus.max_worker_nodes_tracked (integer)
95,citus.use_secondary_nodes (enum)
95,citus.cluster_name (text)
95,citus.enable_version_checks (boolean)
95,citus.log_distributed_deadlock_detection (boolean)
95,citus.distributed_deadlock_detection_factor (floating point)
95,citus.node_connection_timeout (integer)
95,citus.node_conninfo (text)
95,citus.local_hostname (text)
95,citus.show_shards_for_app_name_prefixes (text)
95,Query Statistics
95,citus.stat_statements_purge_interval (integer)
95,citus.stat_statements_max (integer)
95,citus.stat_statements_track (enum)
95,Data Loading
95,citus.multi_shard_commit_protocol (enum)
95,citus.shard_count (integer)
95,citus.shard_max_size (integer)
95,citus.replicate_reference_tables_on_activate (boolean)
95,citus.metadata_sync_mode (enum)
95,Planner Configuration
95,citus.local_table_join_policy (enum)
95,citus.limit_clause_row_fetch_count (integer)
95,citus.count_distinct_error_rate (floating point)
95,citus.task_assignment_policy (enum)
95,Intermediate Data Transfer
95,citus.binary_worker_copy_format (boolean)
95,citus.max_intermediate_result_size (integer)
95,DDL
95,citus.enable_ddl_propagation (boolean)
95,citus.enable_local_reference_table_foreign_keys (boolean)
95,citus.enable_change_data_capture (boolean)
95,Executor Configuration
95,General
95,Explain output
95,External Integrations
95,Change Data Capture (CDC)
95,Differences from single-node PostgreSQL
95,Logical replication of distributed tables to PostgreSQL tables
95,Logical decoding caveats
95,Ingesting Data from Kafka
95,Caveats
95,Ingesting Data from Spark
95,Business Intelligence with Tableau
95,Administer
95,Cluster Management
95,Choosing Cluster Size
95,Shard Count
95,Multi-Tenant SaaS Use-Case
95,Real-Time Analytics Use-Case
95,Initial Hardware Size
95,Multi-Tenant SaaS Use-Case
95,Real-Time Analytics Use-Case
95,Scaling the cluster
95,Add a worker
95,Rebalance Shards without Downtime
95,Parallel Rebalancing
95,How it Works
95,Adding a coordinator
95,Dealing With Node Failures
95,Worker Node Failures
95,Coordinator Node Failures
95,Tenant Isolation
95,Viewing Query Statistics
95,Tenant-level Statistics
95,Statistics Expiration
95,Resource Conservation
95,Limiting Long-Running Queries
95,Security
95,Connection Management
95,Setup Certificate Authority signed certificates
95,Increasing Worker Security
95,Row-Level Security
95,PostgreSQL extensions
95,Creating a New Database
95,Table Management
95,Determining Table and Relation Size
95,Vacuuming Distributed Tables
95,Analyzing Distributed Tables
95,Columnar Storage
95,Usage
95,Measuring compression
95,Example
95,Gotchas
95,Limitations
95,Upgrading Citus
95,Upgrading Citus Versions
95,Patch Version Upgrade
95,Major and Minor Version Upgrades
95,Step 1. Update Citus Package
95,Step 2. Apply Update in DB
95,Upgrading PostgreSQL version from 14 to 15
95,For Every Node
95,Troubleshoot
95,Query Performance Tuning
95,Table Distribution and Shards
95,PostgreSQL tuning
95,Scaling Out Performance
95,Distributed Query Performance Tuning
95,General
95,Subquery/CTE Network Overhead
95,Advanced
95,Connection Management
95,Task Assignment Policy
95,Intermediate Data Transfer Format
95,Binary protocol
95,Scaling Out Data Ingestion
95,Real-time Insert and Updates
95,Insert Throughput
95,Update Throughput
95,Insert and Update: Throughput Checklist
95,Insert and Update: Latency
95,Staging Data Temporarily
95,Bulk Copy (250K - 2M/s)
95,Useful Diagnostic Queries
95,Finding which shard contains data for a specific tenant
95,Finding the distribution column for a table
95,Detecting locks
95,Querying the size of your shards
95,Querying the size of all distributed tables
95,Identifying unused indices
95,Monitoring client connection count
95,Viewing system queries
95,Active queries
95,Why are queries waiting
95,Index hit rate
95,Cache hit rate
95,Common Error Messages
95,Could not receive query results
95,Resolution
95,Canceling the transaction since it was involved in a distributed deadlock
95,Resolution
95,Could not connect to server: Cannot assign requested address
95,Resolution
95,SSL error: certificate verify failed
95,Resolution
95,Could not connect to any active placements
95,Resolution
95,Remaining connection slots are reserved for non-replication superuser connections
95,Resolution
95,PgBouncer cannot connect to server
95,Resolution
95,Relation foo is not distributed
95,Resolution
95,Unsupported clause type
95,Resolution
95,Cannot open new connections after the first modification command within a transaction
95,Resolution
95,Cannot create uniqueness constraint
95,Resolution
95,Function create_distributed_table does not exist
95,Resolution
95,STABLE functions used in UPDATE queries cannot be called with column references
95,Resolution
95,FAQ
95,Frequently Asked Questions
95,Can I create primary keys on distributed tables?
95,How do I add nodes to an existing Citus cluster?
95,How does Citus handle failure of a worker node?
95,How does Citus handle failover of the coordinator node?
95,Are there any PostgreSQL features not supported by Citus?
95,How do I choose the shard count when I hash-partition my data?
95,How do I change the shard count for a hash partitioned table?
95,How does citus support count(distinct) queries?
95,In which situations are uniqueness constraints supported on distributed tables?
95,"How do I create database roles, functions, extensions etc in a Citus cluster?"
95,What if a worker node’s address changes?
95,Which shard contains data for a particular tenant?
95,"I forgot the distribution column of a table, how do I find it?"
95,Can I distribute a table by multiple keys?
95,Why does pg_relation_size report zero bytes for a distributed table?
95,Why am I seeing an error about max_intermediate_result_size?
95,Can I run Citus on Microsoft Azure?
95,Can I shard by schema on Citus for multi-tenant applications?
95,How does cstore_fdw work with Citus?
95,What happened to pg_shard?
95,Articles
95,Related Articles
95,Efficient Rollup Tables with HyperLogLog in Postgres
95,Rollup tables without HLL—using GitHub events data as an example
95,"Without HLL, rollup tables have a few limitations"
95,HLL to the rescue
95,"HLL and rollup tables in action, together"
95,What kinds of queries can HLL answer?
95,A rollup table with HLL is worth a thousand rollup tables without HLL
95,Want to learn more about HLL in Postgres?
95,Distributed Distinct Count with HyperLogLog on Postgres
95,What does HLL do behind the curtains?
95,Hash all elements
95,Observe the data for rare patterns
95,Stochastic Averaging
95,More?
95,HLL in distributed systems
95,Hands on with HLL
95,Setup
95,Examples
95,Conclusion
95,Postgres Parallel Indexing in Citus
95,Real-time Event Aggregation at Scale Using Postgres with Citus
95,How Distributed Outer Joins on PostgreSQL with Citus Work
95,Distributed Outer Joins with Citus
95,Designing your SaaS Database for Scale with Postgres
95,Building a Scalable Postgres Metrics Backend using the Citus Extension
95,Time-Series Metrics
95,Events
95,Sharding a Multi-Tenant App with Postgres
95,Tenancy
95,"Multi-tenancy and co-location, a perfect pair"
95,In conclusion
95,Sharding Postgres with Semi-Structured Data and Its Performance Implications
95,"One large table, without joins"
95,Enter Citus
95,The query workload
95,Every distribution has its thorns
95,Scalable Real-time Product Search using PostgreSQL with Citus
95,Read the Docs
95,v: v11.3
95,Versions
95,latest
95,stable
95,v12.0
95,v11.3
95,v11.2
95,v11.1
95,v11.0
95,v10.2
95,v10.1
95,v10.0
95,v9.5
95,v9.4
95,v9.3
95,v9.2
95,v9.1
95,v9.0
95,v8.3
95,v8.2
95,v8.1
95,v8.0
95,v7.5
95,v7.4
95,v7.3
95,v7.2
95,v7.1
95,v7.0
95,v6.2.2
95,v6.2
95,v6.1
95,v6.0
95,v5.2
95,v5.1
95,v5.0
95,a11y-2
95,Downloads
95,pdf
95,epub
95,On Read the Docs
95,Project Home
95,Builds
95,Citus
95,Query Performance Tuning
95,Edit on GitHub
95,Query Performance Tuning
95,"In this section, we describe how you can tune your Citus cluster to get maximum performance. We begin by explaining how choosing the right distribution column affects performance. We then describe how you can first tune your database for high performance on one PostgreSQL server and then scale it out across all the CPUs in the cluster. In this section, we also discuss several performance related configuration parameters wherever relevant."
95,Table Distribution and Shards
95,The first step while creating a distributed table is choosing the right distribution column. This helps Citus push down several operations directly to the worker shards and prune away unrelated shards which lead to significant query speedups.
95,"Typically, you should pick that column as the distribution column which is the most commonly used join key or on which most queries have filters. For filters, Citus uses the distribution column ranges to prune away unrelated shards, ensuring that the query hits only those shards which overlap with the WHERE clause ranges. For joins, if the join key is the same as the distribution column, then Citus executes the join only between those shards which have matching / overlapping distribution column ranges. All these shard joins can be executed in parallel on the workers and hence are more efficient."
95,"In addition, Citus can push down several operations directly to the worker shards if they are based on the distribution column. This greatly reduces both the amount of computation on each node and the network bandwidth involved in transferring data across nodes."
95,"Once you choose the right distribution column, you can then proceed to the next step, which is tuning worker node performance."
95,PostgreSQL tuning
95,"The Citus coordinator partitions an incoming query into fragment queries, and sends them to the workers for parallel processing. The workers are just extended PostgreSQL servers and they apply PostgreSQL’s standard planning and execution logic for these queries. So, the first step in tuning Citus is tuning the PostgreSQL configuration parameters on the workers for high performance."
95,Tuning the parameters is a matter of experimentation and often takes several attempts to achieve acceptable performance. Thus it’s best to load only a small portion of your data when tuning to make each iteration go faster.
95,"To begin the tuning process create a Citus cluster and load data in it. From the coordinator node, run the EXPLAIN command on representative queries to inspect performance. Citus extends the EXPLAIN command to provide information about distributed query execution. The EXPLAIN output shows how each worker processes the query and also a little about how the coordinator node combines their results."
95,Here is an example of explaining the plan for a particular example query. We use the VERBOSE flag to see the actual queries which were sent to the worker nodes.
95,EXPLAIN VERBOSE
95,"SELECT date_trunc('minute', created_at) AS minute,"
95,sum((payload->>'distinct_size')::int) AS num_commits
95,FROM github_events
95,WHERE event_type = 'PushEvent'
95,GROUP BY minute
95,ORDER BY minute;
95,Sort
95,(cost=0.00..0.00 rows=0 width=0)
95,Sort Key: remote_scan.minute
95,HashAggregate
95,(cost=0.00..0.00 rows=0 width=0)
95,Group Key: remote_scan.minute
95,Custom Scan (Citus Adaptive)
95,(cost=0.00..0.00 rows=0 width=0)
95,Task Count: 32
95,Tasks Shown: One of 32
95,Task
95,"Query: SELECT date_trunc('minute'::text, created_at) AS minute, sum(((payload OPERATOR(pg_catalog.->>) 'distinct_size'::text))::integer) AS num_commits FROM github_events_102042 github_events WHERE (event_type OPERATOR(pg_catalog.=) 'PushEvent'::text) GROUP BY (date_trunc('minute'::text, created_at))"
95,Node: host=localhost port=5433 dbname=postgres
95,HashAggregate
95,(cost=93.42..98.36 rows=395 width=16)
95,"Group Key: date_trunc('minute'::text, created_at)"
95,Seq Scan on github_events_102042 github_events
95,(cost=0.00..88.20 rows=418 width=503)
95,Filter: (event_type = 'PushEvent'::text)
95,(13 rows)
95,"This tells you several things. To begin with there are thirty-two shards, and the planner chose the Citus adaptive executor to execute this query:"
95,Custom Scan (Citus Adaptive)
95,(cost=0.00..0.00 rows=0 width=0)
95,Task Count: 32
95,"Next it picks one of the workers and shows you more about how the query behaves there. It indicates the host, port, database, and the query that was sent to the worker so you can connect to the worker directly and try the query if desired:"
95,Tasks Shown: One of 32
95,Task
95,"Query: SELECT date_trunc('minute'::text, created_at) AS minute, sum(((payload OPERATOR(pg_catalog.->>) 'distinct_size'::text))::integer) AS num_commits FROM github_events_102042 github_events WHERE (event_type OPERATOR(pg_catalog.=) 'PushEvent'::text) GROUP BY (date_trunc('minute'::text, created_at))"
95,Node: host=localhost port=5433 dbname=postgres
95,Distributed EXPLAIN next shows the results of running a normal PostgreSQL EXPLAIN on that worker for the fragment query:
95,HashAggregate
95,(cost=93.42..98.36 rows=395 width=16)
95,"Group Key: date_trunc('minute'::text, created_at)"
95,Seq Scan on github_events_102042 github_events
95,(cost=0.00..88.20 rows=418 width=503)
95,Filter: (event_type = 'PushEvent'::text)
95,"You can now connect to the worker at ‘localhost’, port ‘5433’ and tune query performance for the shard github_events_102042 using standard PostgreSQL techniques. As you make changes run EXPLAIN again from the coordinator or right on the worker."
95,"The first set of such optimizations relates to configuration settings. PostgreSQL by default comes with conservative resource settings; and among these settings, shared_buffers and work_mem are probably the most important ones in optimizing read performance. We discuss these parameters in brief below. Apart from them, several other configuration settings impact query performance. These settings are covered in more detail in the PostgreSQL manual and are also discussed in the PostgreSQL 9.0 High Performance book."
95,"shared_buffers defines the amount of memory allocated to the database for caching data, and defaults to 128MB. If you have a worker node with 1GB or more RAM, a reasonable starting value for shared_buffers is 1/4 of the memory in your system. There are some workloads where even larger settings for shared_buffers are effective, but given the way PostgreSQL also relies on the operating system cache, it’s unlikely you’ll find using more than 25% of RAM to work better than a smaller amount."
95,"If you do a lot of complex sorts, then increasing work_mem allows PostgreSQL to do larger in-memory sorts which will be faster than disk-based equivalents. If you see lot of disk activity on your worker node inspite of having a decent amount of memory, then increasing work_mem to a higher value can be useful. This will help PostgreSQL in choosing more efficient query plans and allow for greater amount of operations to occur in memory."
95,"Other than the above configuration settings, the PostgreSQL query planner relies on statistical information about the contents of tables to generate good plans. These statistics are gathered when ANALYZE is run, which is enabled by default. You can learn more about the PostgreSQL planner and the ANALYZE command in greater detail in the PostgreSQL documentation."
95,"Lastly, you can create indexes on your tables to enhance database performance. Indexes allow the database to find and retrieve specific rows much faster than it could do without an index. To choose which indexes give the best performance, you can run the query with EXPLAIN to view query plans and optimize the slower parts of the query. After an index is created, the system has to keep it synchronized with the table which adds overhead to data manipulation operations. Therefore, indexes that are seldom or never used in queries should be removed."
95,"For write performance, you can use general PostgreSQL configuration tuning to increase INSERT rates. We commonly recommend increasing checkpoint_timeout and max_wal_size settings. Also, depending on the reliability requirements of your application, you can choose to change fsync or synchronous_commit values."
95,"Once you have tuned a worker to your satisfaction you will have to manually apply those changes to the other workers as well. To verify that they are all behaving properly, set this configuration variable on the coordinator:"
95,SET citus.explain_all_tasks = 1;
95,"This will cause EXPLAIN to show the query plan for all tasks, not just one."
95,EXPLAIN
95,"SELECT date_trunc('minute', created_at) AS minute,"
95,sum((payload->>'distinct_size')::int) AS num_commits
95,FROM github_events
95,WHERE event_type = 'PushEvent'
95,GROUP BY minute
95,ORDER BY minute;
95,Sort
95,(cost=0.00..0.00 rows=0 width=0)
95,Sort Key: remote_scan.minute
95,HashAggregate
95,(cost=0.00..0.00 rows=0 width=0)
95,Group Key: remote_scan.minute
95,Custom Scan (Citus Adaptive)
95,(cost=0.00..0.00 rows=0 width=0)
95,Task Count: 32
95,Tasks Shown: All
95,Task
95,Node: host=localhost port=5433 dbname=postgres
95,HashAggregate
95,(cost=93.42..98.36 rows=395 width=16)
95,"Group Key: date_trunc('minute'::text, created_at)"
95,Seq Scan on github_events_102042 github_events
95,(cost=0.00..88.20 rows=418 width=503)
95,Filter: (event_type = 'PushEvent'::text)
95,Task
95,Node: host=localhost port=5434 dbname=postgres
95,HashAggregate
95,(cost=103.21..108.57 rows=429 width=16)
95,"Group Key: date_trunc('minute'::text, created_at)"
95,Seq Scan on github_events_102043 github_events
95,(cost=0.00..97.47 rows=459 width=492)
95,Filter: (event_type = 'PushEvent'::text)
95,-- ... repeats for all 32 tasks
95,alternating between workers one and two
95,"(running in this case locally on ports 5433, 5434)"
95,(199 rows)
95,"Differences in worker execution can be caused by tuning configuration differences, uneven data distribution across shards, or hardware differences between the machines. To get more information about the time it takes the query to run on each shard you can use EXPLAIN ANALYZE."
95,Note
95,"Note that when citus.explain_all_tasks is enabled, EXPLAIN plans are retrieved sequentially, which may take a long time for EXPLAIN ANALYZE."
95,"Citus, by default, sorts tasks by execution time in descending order. If citus.explain_all_tasks is disabled, then Citus shows the single longest-running task. Please note that this functionality can be used only with EXPLAIN ANALYZE, since regular EXPLAIN doesn’t execute the queries, and therefore doesn’t know any execution times."
95,"To change the sort order, you can use citus.explain_analyze_sort_method (enum)."
95,Scaling Out Performance
95,"As mentioned, once you have achieved the desired performance for a single shard you can set similar configuration parameters on all your workers. As Citus runs all the fragment queries in parallel across the worker nodes, users can scale out the performance of their queries to be the cumulative of the computing power of all of the CPU cores in the cluster assuming that the data fits in memory."
95,"Users should try to fit as much of their working set in memory as possible to get best performance with Citus. If fitting the entire working set in memory is not feasible, we recommend using SSDs over HDDs as a best practice. This is because HDDs are able to show decent performance when you have sequential reads over contiguous blocks of data, but have significantly lower random read / write performance. In cases where you have a high number of concurrent queries doing random reads and writes, using SSDs can improve query performance by several times as compared to HDDs. Also, if your queries are highly compute intensive, it might be beneficial to choose machines with more powerful CPUs."
95,"To measure the disk space usage of your database objects, you can log into the worker nodes and use PostgreSQL administration functions for individual shards. The pg_total_relation_size() function can be used to get the total disk space used by a table. You can also use other functions mentioned in the PostgreSQL docs to get more specific size information. On the basis of these statistics for a shard and the shard count, users can compute the hardware requirements for their cluster."
95,"Another factor which affects performance is the number of shards per worker node. Citus partitions an incoming query into its fragment queries which run on individual worker shards. Hence, the degree of parallelism for each query is governed by the number of shards the query hits. To ensure maximum parallelism, you should create enough shards on each node such that there is at least one shard per CPU core. Another consideration to keep in mind is that Citus will prune away unrelated shards if the query has filters on the distribution column. So, creating more shards than the number of cores might also be beneficial so that you can achieve greater parallelism even after shard pruning."
95,Distributed Query Performance Tuning
95,"Once you have distributed your data across the cluster, with each worker optimized for best performance, you should be able to see high performance gains on your queries. After this, the final step is to tune a few distributed performance tuning parameters."
95,"Before we discuss the specific configuration parameters, we recommend that you measure query times on your distributed cluster and compare them with the single shard performance. This can be done by enabling \timing and running the query on the coordinator node and running one of the fragment queries on the worker nodes. This helps in determining the amount of time spent on the worker nodes and the amount of time spent in fetching the data to the coordinator node. Then, you can figure out what the bottleneck is and optimize the database accordingly."
95,"In this section, we discuss the parameters which help optimize the distributed query planner and executor. There are several relevant parameters and we discuss them in two sections:- general and advanced. The general performance tuning section is sufficient for most use-cases and covers all the common configs. The advanced performance tuning section covers parameters which may provide performance gains in specific use cases."
95,General
95,"For higher INSERT performance, the factor which impacts insert rates the most is the level of concurrency. You should try to run several concurrent INSERT statements in parallel. This way you can achieve very high insert rates if you have a powerful coordinator node and are able to use all the CPU cores on that node together."
95,Subquery/CTE Network Overhead
95,"In the best case Citus can execute queries containing subqueries and CTEs in a single step. This is usually because both the main query and subquery filter by tables’ distribution column in the same way, and can be pushed down to worker nodes together. However, Citus is sometimes forced to execute subqueries before executing the main query, copying the intermediate subquery results to other worker nodes for use by the main query. This technique is called Subquery/CTE Push-Pull Execution."
95,"It’s important to be aware when subqueries are executed in a separate step, and avoid sending too much data between worker nodes. The network overhead will hurt performance. The EXPLAIN command allows you to discover how queries will be executed, including whether multiple steps are required. For a detailed example see Subquery/CTE Push-Pull Execution."
95,"Also you can defensively set a safeguard against large intermediate results. Adjust the max_intermediate_result_size limit in a new connection to the coordinator node. By default the max intermediate result size is 1GB, which is large enough to allow some inefficient queries. Try turning it down and running your queries:"
95,-- set a restrictive limit for intermediate results
95,SET citus.max_intermediate_result_size = '512kB';
95,-- attempt to run queries
95,-- SELECT …
95,"If the query has subqueries or CTEs that exceed this limit, the query will be canceled and you will see an error message:"
95,ERROR:
95,the intermediate result size exceeds citus.max_intermediate_result_size (currently 512 kB)
95,DETAIL:
95,Citus restricts the size of intermediate results of complex subqueries and CTEs to avoid accidentally pulling large result sets into once place.
95,HINT:
95,"To run the current query, set citus.max_intermediate_result_size to a higher value or -1 to disable."
95,The size of intermediate results and their destination is available in EXPLAIN ANALYZE output:
95,EXPLAIN ANALYZE
95,WITH deleted_rows AS (
95,"DELETE FROM page_views WHERE tenant_id IN (3, 4) RETURNING *"
95,"), viewed_last_week AS ("
95,SELECT * FROM deleted_rows WHERE view_time > current_timestamp - interval '7 days'
95,SELECT count(*) FROM viewed_last_week;
95,Custom Scan (Citus Adaptive)
95,(cost=0.00..0.00 rows=0 width=0) (actual time=570.076..570.077 rows=1 loops=1)
95,Distributed Subplan 31_1
95,Subplan Duration: 6978.07 ms
95,Intermediate Data Size: 26 MB
95,Result destination: Write locally
95,Custom Scan (Citus Adaptive)
95,(cost=0.00..0.00 rows=0 width=0) (actual time=364.121..364.122 rows=0 loops=1)
95,Task Count: 2
95,Tuple data received from nodes: 0 bytes
95,Tasks Shown: One of 2
95,Task
95,Tuple data received from node: 0 bytes
95,Node: host=localhost port=5433 dbname=postgres
95,Delete on page_views_102016 page_views
95,(cost=5793.38..49272.28 rows=324712 width=6) (actual time=362.985..362.985 rows=0 loops=1)
95,Bitmap Heap Scan on page_views_102016 page_views
95,(cost=5793.38..49272.28 rows=324712 width=6) (actual time=362.984..362.984 rows=0 loops=1)
95,"Recheck Cond: (tenant_id = ANY ('{3,4}'::integer[]))"
95,Bitmap Index Scan on view_tenant_idx_102016
95,(cost=0.00..5712.20 rows=324712 width=0) (actual time=19.193..19.193 rows=325733 loops=1)
95,"Index Cond: (tenant_id = ANY ('{3,4}'::integer[]))"
95,Planning Time: 0.050 ms
95,Execution Time: 363.426 ms
95,Planning Time: 0.000 ms
95,Execution Time: 364.241 ms
95,Task Count: 1
95,Tuple data received from nodes: 6 bytes
95,Tasks Shown: All
95,Task
95,Tuple data received from node: 6 bytes
95,Node: host=localhost port=5432 dbname=postgres
95,Aggregate
95,(cost=33741.78..33741.79 rows=1 width=8) (actual time=565.008..565.008 rows=1 loops=1)
95,Function Scan on read_intermediate_result intermediate_result
95,(cost=0.00..29941.56 rows=1520087 width=0) (actual time=326.645..539.158 rows=651466 loops=1)
95,Filter: (view_time > (CURRENT_TIMESTAMP - '7 days'::interval))
95,Planning Time: 0.047 ms
95,Execution Time: 569.026 ms
95,Planning Time: 1.522 ms
95,Execution Time: 7549.308 ms
95,"In the above EXPLAIN ANALYZE output, you can see the following information about the intermediate results:"
95,Intermediate Data Size: 26 MB
95,Result destination: Write locally
95,"It tells us how large the intermediate results were, and where the intermediate results were written to. In this case,"
95,"they were written to the node coordinating the query execution, as specified by “Write locally”. For some other queries"
95,it can also be of the following format:
95,Intermediate Data Size: 26 MB
95,Result destination: Send to 2 nodes
95,Which means the intermediate result was pushed to 2 worker nodes and it involved more network traffic.
95,"When using CTEs, or joins between CTEs and distributed tables, you can avoid push-pull execution by following these rules:"
95,Tables should be colocated
95,"The CTE queries should not require any merge steps (e.g., LIMIT or GROUP BY on a non-distribution key)"
95,Tables and CTEs should be joined on distribution keys
95,Also PostgreSQL 12 or above allows Citus to take advantage of CTE inlining to push CTEs down to workers in more circumstances. The inlining behavior can be controlled with the MATERIALIZED keyword – see the PostgreSQL docs for details.
95,Advanced
95,"In this section, we discuss advanced performance tuning parameters. These parameters are applicable to specific use cases and may not be required for all deployments."
95,Connection Management
95,"When executing multi-shard queries, Citus must balance the gains from"
95,parallelism with the overhead from database connections. The
95,Query Execution section explains the steps of turning queries into
95,worker tasks and obtaining database connections to the workers.
95,Set citus.max_adaptive_executor_pool_size (integer) to a low value like 1 or 2 for
95,transactional workloads with short queries (e.g. < 20ms of latency). For
95,"analytical workloads where parallelism is critical, leave this setting at its"
95,default value of 16.
95,Set citus.executor_slow_start_interval (integer) to a high value like 100ms for
95,transactional workloads comprised of short queries that are bound on network
95,latency rather than parallelism.
95,"For analytical workloads, leave this"
95,setting at its default value of 10ms.
95,The default value of 1 for citus.max_cached_conns_per_worker (integer) is
95,reasonable.
95,A larger value such as 2 might be helpful for clusters that use
95,"a small number of concurrent sessions, but it’s not wise to go much further"
95,"(e.g. 16 would be too high). If set too high, sessions will hold idle"
95,connections and use worker resources unnecessarily.
95,Set citus.max_shared_pool_size (integer) to match the max_connections
95,setting of your worker nodes. This setting is mainly a fail-safe.
95,Task Assignment Policy
95,The Citus query planner assigns tasks to the worker nodes based on shard locations. The algorithm used while making these assignments can be chosen by setting the citus.task_assignment_policy configuration parameter. Users can alter this configuration parameter to choose the policy which works best for their use case.
95,"The greedy policy aims to distribute tasks evenly across the workers. This policy is the default and works well in most of the cases. The round-robin policy assigns tasks to workers in a round-robin fashion alternating between different replicas. This enables much better cluster utilization when the shard count for a table is low compared to the number of workers. The third policy is the first-replica policy which assigns tasks on the basis of the insertion order of placements (replicas) for the shards. With this policy, users can be sure of which shards will be accessed on each machine. This helps in providing stronger memory residency guarantees by allowing you to keep your working set in memory and use it for querying."
95,Intermediate Data Transfer Format
95,"On Postgres 13 and lower, Citus defaults to transfering intermediate query data between workers in textual format. For certain data types, like hll or hstore arrays, the cost of serializing and deserializing data can be high. In such cases, using the binary format to transfer intermediate data can improve query performance. You can enable the citus.binary_worker_copy_format (boolean) configuration option to use the binary format."
95,Binary protocol
95,"In some cases, a large part of query time is spent in sending query results"
95,from workers to the coordinator. This mostly happens when queries request many
95,"rows (such as select * from table), or when result columns use big types"
95,(like hll or tdigest from the postgresql-hll and tdigest extensions).
95,In those cases it can be beneficial to set citus.enable_binary_protocol to
95,"true, which will change the encoding of the results to binary, rather than"
95,using text encoding. Binary encoding significantly reduces bandwidth for types
95,"that have a compact binary representation, such as hll, tdigest,"
95,timestamp and double precision.
95,"For Postgres 14 and higher, the default for this setting is already true."
95,So explicitly enabling it for those Postgres versions has no effect.
95,Scaling Out Data Ingestion
95,"Citus lets you scale out data ingestion to very high rates, but there are several trade-offs to consider in terms of application integration, throughput, and latency. In this section, we discuss different approaches to data ingestion, and provide guidelines for expected throughput and latency numbers."
95,Real-time Insert and Updates
95,"On the Citus coordinator, you can perform INSERT, INSERT .. ON CONFLICT, UPDATE, and DELETE commands directly on distributed tables. When you issue one of these commands, the changes are immediately visible to the user."
95,"When you run an INSERT (or another ingest command), Citus first finds the right shard placements based on the value in the distribution column. Citus then connects to the worker nodes storing the shard placements, and performs an INSERT on each of them. From the perspective of the user, the INSERT takes several milliseconds to process because of the network latency to worker nodes. The Citus coordinator node, however, can process concurrent INSERTs to reach high throughputs."
95,Insert Throughput
95,"To measure data ingest rates with Citus, we use a standard tool called pgbench and provide repeatable benchmarking steps."
95,"We also used these steps to run pgbench across different Citus Cloud formations on AWS and observed the following ingest rates for transactional INSERT statements. For these benchmark results, we used the default configuration for Citus Cloud formations, and set pgbench’s concurrent thread count to 64 and client count to 256. We didn’t apply any optimizations to improve performance numbers; and you can get higher ingest ratios by tuning your database setup."
95,Coordinator Node
95,Worker Nodes
95,Latency (ms)
95,Transactions per sec
95,2 cores - 7.5GB RAM
95,2 * (1 core - 15GB RAM)
95,28.5
95,"9,000"
95,4 cores -
95,15GB RAM
95,2 * (1 core - 15GB RAM)
95,15.3
95,"16,600"
95,8 cores -
95,30GB RAM
95,2 * (1 core - 15GB RAM)
95,15.2
95,"16,700"
95,8 cores -
95,30GB RAM
95,4 * (1 core - 15GB RAM)
95,8.6
95,"29,600"
95,"We have three observations that follow from these benchmark numbers. First, the top row shows performance numbers for an entry level Citus cluster with one c4.xlarge (two physical cores) as the coordinator and two r4.large (one physical core each) as worker nodes. This basic cluster can deliver 9K INSERTs per second, or 775 million transactional INSERT statements per day."
95,"Second, a more powerful Citus cluster that has about four times the CPU capacity can deliver 30K INSERTs per second, or 2.75 billion INSERT statements per day."
95,"Third, across all data ingest benchmarks, the network latency combined with the number of concurrent connections PostgreSQL can efficiently handle, becomes the"
95,"performance bottleneck. In a production environment with hundreds of tables and indexes, this bottleneck will likely shift to a different resource."
95,Update Throughput
95,"To measure UPDATE throughputs with Citus, we used the same benchmarking steps and ran pgbench across different Citus Cloud formations on AWS."
95,Coordinator Node
95,Worker Nodes
95,Latency (ms)
95,Transactions per sec
95,2 cores - 7.5GB RAM
95,2 * (1 core - 15GB RAM)
95,25.0
95,"10,200"
95,4 cores -
95,15GB RAM
95,2 * (1 core - 15GB RAM)
95,19.6
95,"13,000"
95,8 cores -
95,30GB RAM
95,2 * (1 core - 15GB RAM)
95,20.3
95,"12,600"
95,8 cores -
95,30GB RAM
95,4 * (1 core - 15GB RAM)
95,10.7
95,"23,900"
95,These benchmark numbers show that Citus’s UPDATE throughput is slightly lower than those of INSERTs. This is because pgbench creates a primary key index for UPDATE statements and an UPDATE incurs more work on the worker nodes. It’s also worth noting an additional differences between INSERT and UPDATEs.
95,"UPDATE statements cause bloat in the database and VACUUM needs to run regularly to clean up this bloat. In Citus, since VACUUM runs in parallel across worker nodes, your workloads are less likely to be impacted by VACUUM."
95,Insert and Update: Throughput Checklist
95,"When you’re running the above pgbench benchmarks on a moderately sized Citus cluster, you can generally expect 10K-50K INSERTs per second. This translates to approximately 1 to 4 billion INSERTs per day. If you aren’t observing these throughputs numbers, remember the following checklist:"
95,Check the network latency between your application and your database. High latencies will impact your write throughput.
95,"Ingest data using concurrent threads. If the roundtrip latency during an INSERT is 4ms, you can process 250 INSERTs/second over one thread. If you run 100 concurrent threads, you will see your write throughput increase with the number of threads."
95,"Check whether the nodes in your cluster have CPU or disk bottlenecks. Ingested data passes through the coordinator node, so check whether your coordinator is bottlenecked on CPU."
95,Avoid closing connections between INSERT statements. This avoids the overhead of connection setup.
95,Remember that column size will affect insert speed. Rows with big JSON blobs will take longer than those with small columns like integers.
95,Insert and Update: Latency
95,"The benefit of running INSERT or UPDATE commands, compared to issuing bulk COPY commands, is that changes are immediately visible to other queries. When you issue an INSERT or UPDATE command, the Citus coordinator node directly routes this command to related worker node(s). The coordinator node also keeps connections to the workers open within the same session, which means subsequent commands will see lower response times."
95,-- Set up a distributed table that keeps account history information
95,"CREATE TABLE pgbench_history (tid int, bid int, aid int, delta int, mtime timestamp);"
95,"SELECT create_distributed_table('pgbench_history', 'aid');"
95,-- Enable timing to see reponse times
95,\timing on
95,"-- First INSERT requires connection set-up, second will be faster"
95,"INSERT INTO pgbench_history VALUES (10, 1, 10000, -5000, CURRENT_TIMESTAMP); -- Time: 10.314 ms"
95,"INSERT INTO pgbench_history VALUES (10, 1, 22000, 5000, CURRENT_TIMESTAMP); -- Time: 3.132 ms"
95,Staging Data Temporarily
95,"When loading data for temporary staging, consider using an unlogged table. These are tables which are not backed by the Postgres write-ahead log. This makes them faster for inserting rows, but not suitable for long term data storage. You can use an unlogged table as a place to load incoming data, prior to manipulating the data and moving it to permanent tables."
95,-- example unlogged table
95,CREATE UNLOGGED TABLE unlogged_table (
95,"key text,"
95,value text
95,-- its shards will be unlogged as well when
95,-- the table is distributed
95,"SELECT create_distributed_table('unlogged_table', 'key');"
95,-- ready to load data
95,Bulk Copy (250K - 2M/s)
95,"Distributed tables support COPY from the Citus coordinator for bulk ingestion, which can achieve much higher ingestion rates than INSERT statements."
95,"COPY can be used to load data directly from an application using COPY .. FROM STDIN, from a file on the server, or program executed on the server."
95,COPY pgbench_history FROM STDIN WITH (FORMAT CSV);
95,"In psql, the \COPY command can be used to load data from the local machine. The \COPY command actually sends a COPY .. FROM STDIN command to the server before sending the local data, as would an application that loads data directly."
95,"psql -c ""\COPY pgbench_history FROM 'pgbench_history-2016-03-04.csv' (FORMAT CSV)"""
95,"A powerful feature of COPY for distributed tables is that it asynchronously copies data to the workers over many parallel connections, one for each shard placement. This means that data can be ingested using multiple workers and multiple cores in parallel. Especially when there are expensive indexes such as a GIN, this can lead to major performance boosts over ingesting into a regular PostgreSQL table."
95,"From a throughput standpoint, you can expect data ingest ratios of 250K - 2M rows per second when using COPY. To learn more about COPY performance across different scenarios, please refer to the following blog post."
95,Note
95,Make sure your benchmarking setup is well configured so you can observe optimal COPY performance. Follow these tips:
95,"We recommend a large batch size (~ 50000-100000). You can benchmark with multiple files (1, 10, 1000, 10000 etc), each of that batch size."
95,"Use parallel ingestion. Increase the number of threads/ingestors to 2, 4, 8, 16 and run benchmarks."
95,Use a compute-optimized coordinator. For the workers choose memory-optimized boxes with a decent number of vcpus.
95,"Go with a relatively small shard count, 32 should suffice but you could benchmark with 64, too."
95,"Ingest data for a suitable amount of time (say 2, 4, 8, 24 hrs). Longer tests are more representative of a production setup."
95,Previous
95,Next
95,"© Copyright 2023, Citus Data, a Microsoft Company."
95,Revision 2d3da38e.
96,Performance Tuning Transactions in YSQL | YugabyteDB Docs
96,Star us on Github
96,Join us on Slack
96,Start Now
96,YugabyteDB ManagedTry our fully-managed cloud DBaaS for free.No credit card requiredSign up
96,YugabyteDBTry our open source distributed SQL database on your laptop.Download
96,Products
96,YugabyteDBThe open source distributed SQL database
96,YugabyteDB AnywhereSelf-managed cloud DBaaS
96,YugabyteDB ManagedFully-managed cloud DBaaS
96,YugabyteDB VoyagerDatabase migration service
96,Tutorials
96,Integrations
96,FAQ
96,Releases
96,YugabyteDB
96,YugabyteDB Anywhere
96,YugabyteDB Managed
96,YugabyteDB Voyager
96,v2.19 Preview
96,v2.20 LTS
96,v2.19 Preview
96,v2.18 STS
96,v2.16 STS
96,v2.14 LTS
96,Unsupported versions
96,Docs Menu
96,v2.19 Preview
96,v2.20 LTS
96,v2.19 Preview
96,v2.18 STS
96,v2.16 STS
96,v2.14 LTS
96,Unsupported versions
96,Docs Home
96,YugabyteDB
96,Key benefits
96,Quick Start
96,Explore
96,SQL features
96,SQL feature support
96,PostgreSQL compatibility
96,Schemas and tables
96,Data types
96,Data manipulation
96,Expressions and operators
96,Queries and joins
96,Join strategies
96,Indexes and constraints
96,Primary keys
96,Foreign keys
96,Secondary indexes
96,Unique indexes
96,Partial indexes
96,Covering indexes
96,Expression indexes
96,GIN indexes
96,Other constraints
96,Index backfill
96,JSON support
96,Stored procedures
96,Triggers
96,Advanced features
96,Cursors
96,Table partitioning
96,Views
96,Savepoints
96,Collations
96,Foreign data wrappers
96,Going beyond SQL
96,Follower reads
96,Tablespaces
96,PostgreSQL extensions
96,Install extensions
96,auto_explain
96,file_fdw
96,fuzzystrmatch
96,HypoPG
96,passwordcheck
96,pg_stat_statements
96,pgcrypto
96,pgvector
96,postgres_fdw
96,postgresql-hll
96,spi
96,tablefunc
96,uuid-ossp
96,YCQL features
96,Cassandra feature support
96,Keyspaces and tables
96,Data types
96,Indexes and constraints
96,Primary keys
96,Secondary indexes
96,Unique indexes
96,Partial indexes
96,Covering indexes
96,Secondary indexes with JSONB
96,JSON support
96,Continuous availability
96,HA during failures
96,HA of transactions
96,Horizontal scalability
96,Horizontal vs vertical
96,Data distribution
96,Adding nodes
96,Scaling reads
96,Scaling writes
96,Scaling transactions
96,Large datasets
96,Scale out a universe
96,Transactions
96,Distributed transactions
96,Isolation levels
96,Explicit locking
96,Multi-region deployments
96,Synchronous (3+ regions)
96,Row-level geo-partitioning
96,xCluster (2+ regions)
96,Read replicas
96,Change data capture
96,Overview
96,Debezium connector
96,Get started
96,Monitor
96,Cluster management
96,Point-in-time recovery
96,Observability
96,Prometheus integration
96,Grafana dashboard
96,Live queries
96,Terminated queries
96,Data transfer status
96,Query tuning
96,Introduction
96,Get query statistics
96,Analyze queries
96,Optimize YSQL queries
96,Security
96,YSQL Connection Manager
96,Develop
96,Build global applications
96,Global database
96,Duplicate indexes
96,Active-active single-master
96,Active-active multi-master
96,Latency-optimized geo-partitioning
96,Locality-optimized geo-partitioning
96,Follower reads
96,Read replicas
96,Real world scenarios
96,Global and geo-local tables
96,Build multi-cloud applications
96,Multi-cloud setup
96,Multi-cloud migration
96,Hybrid cloud
96,Common patterns
96,Time series
96,Global ordering by time
96,Ordering by time per entity
96,Automatic data expiration
96,Partition data by time
96,Key-value
96,Learn app development
96,Data modeling
96,Data types
96,SQL vs NoSQL
96,Transactions
96,Transaction retries
96,Performance tuning
96,Global applications
96,Error codes
96,Text search
96,Pattern matching
96,Similarity search
96,Full-text search
96,Phonetic search
96,Aggregations
96,Batch operations
96,Date and time
96,Strings and text
96,TTL for data expiration
96,Best practices
96,Drivers and ORMs
96,Smart drivers
96,Java
96,Connect an app
96,Use an ORM
96,Connect an app
96,Use an ORM
96,Python
96,Connect an app
96,Use an ORM
96,Node.js
96,Connect an app
96,Use an ORM
96,Connect an app
96,C++
96,Connect an app
96,Connect an app
96,Use an ORM
96,Ruby
96,Connect an app
96,Use an ORM
96,Rust
96,Connect an app
96,Use an ORM
96,PHP
96,Connect an app
96,Use an ORM
96,Scala
96,Connect an app
96,Build apps using ORMs
96,Java
96,Python
96,Node.js
96,Rust
96,PHP
96,Real-world examples
96,Yugastore
96,Install Yugastore
96,IoT Fleet Management
96,Quality of service
96,Rate limiting connections
96,Write-heavy workloads
96,Transaction priorities
96,Cloud-native development
96,Codespaces
96,Gitpod
96,Secure
96,Security checklist
96,Enable authentication
96,Enable users
96,Create login profiles
96,Configure client authentication
96,Authentication methods
96,Password authentication
96,LDAP authentication
96,Host-based authentication
96,Trust authentication
96,Role-based access control
96,Overview
96,Manage users and roles
96,Grant privileges
96,Row-level security
96,Column-level security
96,Encryption in transit
96,Create server certificates
96,Enable server-to-server encryption
96,Enable client-to-server encryption
96,Connect to clusters
96,TLS and authentication
96,Encryption at rest
96,Column-level encryption
96,Audit logging
96,Trace statements
96,Configure audit logging
96,Session-level audit logging
96,Object-level audit logging
96,Vulnerability disclosure policy
96,Launch and Manage
96,Deploy
96,Deployment checklist
96,Manual deployment
96,1. System configuration
96,2. Install software
96,3. Start YB-Masters
96,4. Start YB-TServers
96,5. Verify deployment
96,Kubernetes
96,Single-zone
96,Open Source
96,Amazon EKS
96,Google Kubernetes Engine
96,Azure Kubernetes Service
96,Multi-zone
96,Amazon EKS
96,Google Kubernetes Engine
96,Multi-cluster
96,Google Kubernetes Engine
96,Best practices
96,Connect Clients
96,Public clouds
96,Amazon Web Services
96,Google Cloud Platform
96,Microsoft Azure
96,Multi-DC deployments
96,Three+ data center (3DC)
96,xCluster deployments
96,xCluster
96,Transactional xCluster
96,Set up replication
96,Failover
96,Switchover
96,Tables and indexes
96,Read replicas
96,Manage
96,Backup and restore
96,Export and import
96,Distributed snapshots
96,Point-in-time recovery
96,Migrate data
96,Bulk export
96,Bulk import
96,Change cluster configuration
96,Diagnostics reporting
96,Upgrade YugabyteDB
96,Monitor
96,Metrics
96,Throughput+latency metrics
96,Connection metrics
96,Cache and storage metrics
96,Raft metrics
96,YB-Master metrics
96,Replication metrics
96,Troubleshoot
96,Cluster-level issues
96,YCQL API connection issues
96,YEDIS API connection Issues
96,Recover YB-TServer and YB-Master
96,Replace a failed YB-TServer
96,Replace a failed YB-Master
96,Manual remote bootstrap of failed peer
96,Recover YB-TServer from crash loop
96,Performance issues
96,Node-level issues
96,Check servers
96,Inspect logs
96,System statistics
96,Disk failure
96,Disk Full
96,Common error messages
96,Benchmark
96,TPC-C
96,sysbench
96,YCSB
96,Key-value workload
96,Large datasets
96,Scalability
96,Scaling queries
96,Resilience
96,Jepsen testing
96,Reference
96,Architecture
96,Design goals
96,Key concepts
96,Universe
96,YB-TServer service
96,YB-Master service
96,Core functions
96,Universe creation
96,Table creation
96,Write I/O path
96,Read I/O path
96,High availability
96,Layered architecture
96,Query layer
96,Overview
96,DocDB transactions layer
96,Overview
96,Isolation levels
96,Concurrency control
96,Transaction priorities
96,Read Committed
96,Read Restart error
96,Single-row transactions
96,Distributed transactions
96,Transactional I/O path
96,DocDB sharding layer
96,Hash and range sharding
96,Tablet splitting
96,Colocated tables
96,DocDB replication layer
96,Synchronous
96,xCluster
96,Read replicas
96,Change data capture (CDC)
96,DocDB storage layer
96,Persistence
96,Performance
96,APIs
96,YCQL
96,ALTER KEYSPACE
96,ALTER ROLE
96,ALTER TABLE
96,CREATE INDEX
96,CREATE KEYSPACE
96,CREATE ROLE
96,CREATE TABLE
96,CREATE TYPE
96,DROP INDEX
96,DROP KEYSPACE
96,DROP ROLE
96,DROP TABLE
96,DROP TYPE
96,GRANT PERMISSION
96,GRANT ROLE
96,REVOKE PERMISSION
96,REVOKE ROLE
96,USE
96,INSERT
96,SELECT
96,EXPLAIN
96,UPDATE
96,DELETE
96,TRANSACTION
96,TRUNCATE
96,Simple expressions
96,Subscripted expressions
96,Function call
96,Operators
96,BLOB
96,BOOLEAN
96,Collection
96,FROZEN
96,INET
96,Integer and counter
96,Non-integer
96,TEXT
96,"DATE, TIME, and TIMESTAMP"
96,UUID and TIMEUUID
96,JSONB
96,Date and time
96,BATCH
96,YSQL
96,The SQL language
96,SQL statements
96,ABORT
96,ALTER DATABASE
96,ALTER DEFAULT PRIVILEGES
96,ALTER DOMAIN
96,ALTER FOREIGN DATA WRAPPER
96,ALTER FOREIGN TABLE
96,ALTER FUNCTION
96,ALTER GROUP
96,ALTER INDEX
96,ALTER MATERIALIZED VIEW
96,ALTER POLICY
96,ALTER PROCEDURE
96,ALTER ROLE
96,ALTER SCHEMA
96,ALTER SEQUENCE
96,ALTER SERVER
96,ALTER TABLE
96,ALTER USER
96,ANALYZE
96,BEGIN
96,CALL
96,CLOSE
96,COMMENT
96,COMMIT
96,COPY
96,CREATE AGGREGATE
96,CREATE CAST
96,CREATE DATABASE
96,CREATE DOMAIN
96,CREATE EXTENSION
96,CREATE FOREIGN DATA WRAPPER
96,CREATE FOREIGN TABLE
96,CREATE FUNCTION
96,CREATE GROUP
96,CREATE INDEX
96,CREATE MATERIALIZED VIEW
96,CREATE OPERATOR
96,CREATE OPERATOR CLASS
96,CREATE POLICY
96,CREATE PROCEDURE
96,CREATE ROLE
96,CREATE RULE
96,CREATE SCHEMA
96,CREATE SEQUENCE
96,CREATE SERVER
96,CREATE TABLE
96,CREATE TABLE AS
96,CREATE TABLESPACE
96,CREATE TRIGGER
96,CREATE TYPE
96,CREATE USER
96,CREATE USER MAPPING
96,CREATE VIEW
96,DEALLOCATE
96,DECLARE
96,DELETE
96,DROP AGGREGATE
96,DROP CAST
96,DROP DATABASE
96,DROP DOMAIN
96,DROP EXTENSION
96,DROP FOREIGN DATA WRAPPER
96,DROP FOREIGN TABLE
96,DROP FUNCTION
96,DROP GROUP
96,DROP INDEX
96,DROP MATERIALIZED VIEW
96,DROP OPERATOR
96,DROP OPERATOR CLASS
96,DROP OWNED
96,DROP POLICY
96,DROP PROCEDURE
96,DROP ROLE
96,DROP RULE
96,DROP SCHEMA
96,DROP SEQUENCE
96,DROP SERVER
96,DROP TABLE
96,DROP TABLESPACE
96,DROP TRIGGER
96,DROP TYPE
96,DROP USER
96,END
96,EXECUTE
96,EXPLAIN
96,FETCH
96,GRANT
96,IMPORT FOREIGN SCHEMA
96,INSERT
96,LOCK
96,MOVE
96,PREPARE
96,REASSIGN OWNED
96,REFRESH MATERIALIZED VIEW
96,RELEASE SAVEPOINT
96,RESET
96,REVOKE
96,ROLLBACK
96,ROLLBACK TO SAVEPOINT
96,SAVEPOINT
96,SELECT
96,SET
96,SET CONSTRAINTS
96,SET ROLE
96,SET SESSION AUTHORIZATION
96,SET TRANSACTION
96,SHOW
96,SHOW TRANSACTION
96,START TRANSACTION
96,TRUNCATE
96,UPDATE
96,VALUES
96,Temporary schema-objects
96,"Temp tables, views, sequences, and indexes"
96,Temp schema-objects of all kinds
96,Globality of metadata and privacy of use of temp objects
96,Paradigm for creating temporary objects
96,WITH clause
96,WITH clause—SQL syntax and semantics
96,Recursive CTE
96,Case study: traversing an employee hierarchy
96,Traversing general graphs
96,Graph representation
96,Common code
96,Undirected cyclic graph
96,Directed cyclic graph
96,Directed acyclic graph
96,Rooted tree
96,Unique containing paths
96,Stress testing find_paths()
96,Case study: Bacon Numbers from IMDb
96,Bacon numbers for synthetic data
96,Bacon numbers for IMDb data
96,Transaction model for top-level SQL statements
96,Names and identifiers
96,Name resolution in top-level SQL
96,Cursors
96,Built-in functions and operators
96,Aggregate functions
96,Informal functionality overview
96,Invocation syntax and semantics
96,"Grouping sets, rollup, cube"
96,Per function signature and purpose
96,"avg(), count(), max(), min(), sum()"
96,"array_agg(), string_agg(), jsonb_agg(), jsonb_object_agg()"
96,"bit_and(), bit_or(), bool_and(), bool_or()"
96,"variance(), var_pop(), var_samp(), stddev(), stddev_pop(), stddev_samp()"
96,linear regression
96,"covar_pop(), covar_samp(), corr()"
96,regr_%()
96,"mode(), percentile_disc(), percentile_cont()"
96,"rank(), dense_rank(), percent_rank(), cume_dist()"
96,"Case study: percentile_cont() and the ""68–95–99.7"" rule"
96,Case study: linear regression on COVID data
96,Download the COVIDcast data
96,Ingest the COVIDcast data
96,Inspect the COVIDcast data
96,Copy the .csv files to staging tables
96,Check staged data conforms to the rules
96,Join the staged data into a single table
96,SQL scripts
96,Create cr_staging_tables()
96,Create cr_copy_from_scripts()
96,Create assert_assumptions_ok()
96,Create xform_to_covidcast_fb_survey_results()
96,ingest-the-data.sql
96,Analyze the COVIDcast data
96,symptoms vs mask-wearing by day
96,Data for scatter-plot for 21-Oct-2020
96,Scatter-plot for 21-Oct-2020
96,SQL scripts
96,analysis-queries.sql
96,synthetic-data.sql
96,currval()
96,Geo-partitioning helper functions
96,yb_is_local_table()
96,yb_server_cloud()
96,yb_server_region()
96,yb_server_zone()
96,lastval()
96,nextval()
96,setval()
96,Window functions
96,Informal functionality overview
96,Invocation syntax and semantics
96,Per function signature and purpose
96,"row_number(), rank() and dense_rank()"
96,"percent_rank(), cume_dist() and ntile()"
96,"first_value(), nth_value(), last_value()"
96,"lag(), lead()"
96,Tables for the code examples
96,table t1
96,table t2
96,table t3
96,table t4
96,Case study: analyzing a normal distribution
96,Bucket allocation scheme
96,do_clean_start.sql
96,cr_show_t4.sql
96,cr_dp_views.sql
96,cr_int_views.sql
96,cr_pr_cd_equality_report.sql
96,cr_bucket_using_width_bucket.sql
96,cr_bucket_dedicated_code.sql
96,do_assert_bucket_ok
96,cr_histogram.sql
96,cr_do_ntile.sql
96,cr_do_percent_rank.sql
96,cr_do_cume_dist.sql
96,do_populate_results.sql
96,do_report_results.sql
96,do_compare_dp_results.sql
96,do_demo.sql
96,Reports
96,Histogram report
96,dp-results
96,compare-dp-results
96,int-results
96,yb_hash_code()
96,User-defined subprograms and anonymous blocks
96,«Commit» in user-defined subprograms
96,Subprogram attributes
96,"""Depends on extension"" semantics"
96,Alterable subprogram attributes
96,Alterable function-only attributes
96,Immutable function examples
96,"""language sql"" subprograms"
96,"""language plpgsql"" subprograms"
96,Create-time and execution model
96,"""language plpgsql"" syntax and semantics"
96,Declaration section
96,Executable section
96,Basic statements
96,"""assert"" statement"
96,"""get diagnostics"" statement"
96,"""raise"" statement"
96,"""return"" statement"
96,Cursor manipulation
96,Doing SQL from PL/pgSQL
96,Compound statements
96,"The ""if"" statement"
96,"The ""case"" statement"
96,"The ""loop"", ""exit"", and ""continue"" statements"
96,Infinite and while loops
96,Integer for loop
96,Array foreach loop
96,Query for loop
96,"Jumping out of a block statement with ""exit"""
96,Two case studies
96,Exception section
96,Case study: PL/pgSQL procedures-for role provisioning
96,Subprogram overloading
96,Variadic and polymorphic subprograms
96,Name resolution in subprograms
96,"The ""pg_proc"" catalog table"
96,Data types
96,Array
96,array[] constructor
96,Literals
96,Text typecasting and literals
96,Array of primitive values
96,Row
96,Array of rows
96,FOREACH loop (PL/pgSQL)
96,array of DOMAINs
96,Functions and operators
96,ANY and ALL
96,Array comparison
96,Array slice operator
96,Array concatenation
96,Array properties
96,"array_agg(), unnest(), generate_subscripts()"
96,array_fill()
96,"array_position(), array_positions()"
96,array_remove()
96,array_replace() / set value
96,array_to_string()
96,string_to_array()
96,Binary
96,Boolean
96,Character
96,Date and time
96,Conceptual background
96,Section contents
96,Timezones and UTC offsets
96,Catalog views
96,Extended_timezone_names
96,Unrestricted full projection
96,Real timezones with DST
96,Real timezones no DST
96,Synthetic timezones no DST
96,Offset/timezone-sensitive operations
96,Timestamptz to/from timestamp conversion
96,Pure 'day' interval arithmetic
96,Four ways to specify offset
96,Name-resolution rules
96,1 case-insensitive resolution
96,2 ~names.abbrev never searched
96,3 'set timezone' string not resolved in ~abbrevs.abbrev
96,4 ~abbrevs.abbrev before ~names.name
96,Helper functions
96,Syntax contexts for offset
96,Recommended practice
96,Typecasting between date-time and text-values
96,Semantics of the date-time data types
96,Date data type
96,Time data type
96,Plain timestamp and timestamptz
96,Interval data type
96,Interval representation
96,Ad hoc examples
96,Representation model
96,Interval value limits
96,Declaring intervals
96,Justify() and extract(epoch...)
96,Interval arithmetic
96,Interval-interval comparison
96,Interval-interval addition and subtraction
96,Interval-number multiplication
96,"Moment-moment overloads of ""-"""
96,"Moment-interval overloads of ""+"" and ""-"""
96,Custom interval domains
96,Interval utility functions
96,Typecasting between date-time data types
96,Operators
96,Test comparison overloads
96,Test addition overloads
96,Test subtraction overloads
96,Test multiplication overloads
96,Test division overloads
96,General-purpose functions
96,Creating date-time values
96,Manipulating date-time values
96,Current date-time moment
96,Delaying execution
96,Miscellaneous
96,Function age()
96,Function extract() | date_part()
96,Implementations that model the overlaps operator
96,Formatting functions
96,Case study: SQL stopwatch
96,Download & install the date-time utilities
96,JSON
96,JSON literals
96,Primitive and compound data types
96,Code example conventions
96,Indexes and check constraints
96,Functions & operators
96,"::jsonb, ::json, ::text (typecast)"
96,"->, ->>, #>, #>> (JSON subvalues)"
96,- and #- (remove)
96,|| (concatenation)
96,= (equality)
96,@> and <@ (containment)
96,? and ?| and ?& (key or value existence)
96,array_to_json()
96,jsonb_agg()
96,jsonb_array_elements()
96,jsonb_array_elements_text()
96,jsonb_array_length()
96,jsonb_build_object()
96,jsonb_build_array()
96,jsonb_each()
96,jsonb_each_text()
96,jsonb_extract_path()
96,jsonb_extract_path_text() and json_extract_path_text()
96,jsonb_object()
96,jsonb_object_agg()
96,jsonb_object_keys()
96,jsonb_populate_record()
96,jsonb_populate_recordset()
96,jsonb_pretty()
96,jsonb_set() and jsonb_insert()
96,jsonb_strip_nulls()
96,jsonb_to_record()
96,jsonb_to_recordset()
96,jsonb_typeof()
96,row_to_json()
96,to_jsonb()
96,Money
96,Numeric
96,Range
96,Serial
96,UUID
96,Keywords
96,Reserved names
96,CLIs
96,ysqlsh
96,Meta-commands
96,pset options
96,Examples
96,ycqlsh
96,yb-admin
96,yb-ts-cli
96,ysql_dump
96,ysql_dumpall
96,yb-ctl
96,yb-docker-ctl
96,Configuration
96,yb-tserver
96,yb-master
96,yugabyted
96,Operating systems
96,Default ports
96,Drivers and ORMs
96,JDBC Drivers
96,Node.js Drivers
96,C# Drivers
96,Go Drivers
96,Python Drivers
96,Rust Drivers
96,Client drivers for YSQL
96,Client drivers for YCQL
96,Third party tools
96,Apache Superset
96,Arctype
96,DBeaver
96,DbSchema
96,Metabase
96,pgAdmin
96,SQL Workbench/J
96,TablePlus
96,Sample datasets
96,Chinook
96,Northwind
96,PgExercises
96,SportsDB
96,Retail Analytics
96,Misc
96,YEDIS
96,Quick start
96,Develop
96,Build an application
96,C++
96,Java
96,NodeJS
96,Python
96,API reference
96,APPEND
96,AUTH
96,CONFIG
96,CREATEDB
96,DELETEDB
96,LISTDB
96,SELECT
96,DEL
96,ECHO
96,EXISTS
96,EXPIRE
96,EXPIREAT
96,FLUSHALL
96,FLUSHDB
96,GET
96,GETRANGE
96,GETSET
96,HDEL
96,HEXISTS
96,HGET
96,HGETALL
96,HINCRBY
96,HKEYS
96,HLEN
96,HMGET
96,HMSET
96,HSET
96,HSTRLEN
96,HVALS
96,INCR
96,INCRBY
96,KEYS
96,MONITOR
96,PEXPIRE
96,PEXPIREAT
96,PTTL
96,ROLE
96,SADD
96,SCARD
96,RENAME
96,SET
96,SETEX
96,PSETEX
96,SETRANGE
96,SISMEMBER
96,SMEMBERS
96,SREM
96,STRLEN
96,ZRANGE
96,TSADD
96,TSCARD
96,TSGET
96,TSLASTN
96,TSRANGEBYTIME
96,TSREM
96,TSREVRANGEBYTIME
96,TTL
96,ZADD
96,ZCARD
96,ZRANGEBYSCORE
96,ZREM
96,ZREVRANGE
96,ZSCORE
96,PUBSUB
96,PUBLISH
96,SUBSCRIBE
96,UNSUBSCRIBE
96,PSUBSCRIBE
96,PUNSUBSCRIBE
96,Legal
96,Third-party software
96,Contribute
96,Core database
96,Contribution checklist
96,Build the source
96,Configure a CLion project
96,Build and test
96,Coding style
96,Documentation
96,Docs checklist
96,Docs layout
96,Build the docs
96,Editor setup
96,Edit the docs
96,Docs page structure
96,Widgets and shortcodes
96,Syntax diagrams
96,Page with elements
96,Style guide
96,Download
96,Join our community
96,Slack
96,Yugabyte University
96,GitHub
96,Yugabyte Friday Tech Talks
96,Forum
96,Contact Support
96,YugabyteDB
96,Develop
96,Learn app development
96,Performance tuning
96,Try a preview of AI-powered search below!
96,Performance tuning in YSQL
96,Contribute
96,Report a doc issue
96,Suggest new content
96,Edit this page
96,Contributor guide
96,"As a versatile distributed database, YugabyteDB can be deployed in a variety of configurations for a variety of use cases. The following best practices and tips can greatly improve the performance of a YugabyteDB cluster."
96,Setup
96,"To run the following examples, first set up a cluster and database schema as described in the Prerequisites."
96,Fast single-row transactions
96,"YugabyteDB has specific optimizations to improve the performance of transactions in certain scenarios where transactions operate on a single row. These transactions are referred to as single-row or fast-path transactions. These are much faster than distributed transactions that impact a set of rows distributed across shards that are themselves spread across multiple nodes distributed across a data center, region, or globally."
96,"For example, consider a common scenario in transactions where a single row is updated and the new value is fetched. This is usually done in multiple steps as follows:"
96,BEGIN;
96,SELECT v FROM txndemo WHERE k=1 FOR UPDATE;
96,UPDATE txndemo SET v = v + 3 WHERE k=1;
96,SELECT v FROM txndemo WHERE k=1;
96,COMMIT;
96,"In this formulation, when the rows are locked in the first SELECT statement, YugabyteDB does not know what rows are going to be modified in subsequent commands. As a result, it considers the transaction to be distributed."
96,"However, if you write it as a single statement, YugabyteDB can confidently treat it as a single-row transaction. To update a row and return its new value using a single statement, use the RETURNING clause as follows:"
96,UPDATE txndemo SET v = v + 3 WHERE k=1 RETURNING v;
96,"YugabyteDB treats this as a single-row transaction, which executes much faster. This also saves one round trip and immediately fetches the updated value."
96,Minimize conflict errors
96,The INSERT statement has an optional ON CONFLICT clause that can be helpful to circumvent certain errors and avoid multiple statements.
96,"For example, if concurrent transactions are inserting the same row, this could cause a UniqueViolation. Instead of letting the server throw an error and handling it in code, you could just ask the server to ignore it as follows:"
96,"INSERT INTO txndemo VALUES (1,10) ON CONFLICT DO NOTHING;"
96,"With DO NOTHING, the server does not throw an error, resulting in one less round trip between the application and the server."
96,"You can also simulate an upsert by using DO UPDATE SET instead of doing a INSERT, fail, and UPDATE, as follows:"
96,"INSERT INTO txndemo VALUES (1,10)"
96,ON CONFLICT (k)
96,DO UPDATE SET v=10;
96,"Now, the server automatically updates the row when it fails to insert. Again, this results in one less round trip between the application and the server."
96,Avoid long waits
96,"In the READ COMMITTED isolation level, clients do not need to retry or handle serialization errors. During conflicts, the server retries indefinitely based on the retry options and Wait-On-Conflict policy."
96,"To avoid getting stuck in a wait loop because of starvation, you should use a reasonable timeout for the statements, similar to the following:"
96,SET statement_timeout = '10s';
96,This ensures that the transaction would not be blocked for more than 10 seconds.
96,Handle idle applications
96,"When an application takes a long time between two statements in a transaction or just hangs, it could be holding the locks on the provisional records during that period. It would hit a timeout if the idle_in_transaction_session_timeout is set accordingly. After that timeout is reached, the connection is disconnected and the client would have to reconnect. The typical error message would be:"
96,FATAL:
96,25P03: terminating connection due to idle-in-transaction timeout
96,"By default, the idle_in_transaction_session_timeout is set to 0. You can set the timeout to a specific value in ysqlsh using the following command:"
96,SET idle_in_transaction_session_timeout = '10s';
96,"To view the current value, use the following command:"
96,SHOW idle_in_transaction_session_timeout;
96,idle_in_transaction_session_timeout
96,-------------------------------------
96,10s
96,Setting this timeout can avoid deadlock scenarios where applications acquire locks and then hang unintentionally.
96,Large scans and batch jobs
96,"When a transaction is in SERIALIZABLE isolation level and READ ONLY mode, if the transaction property DEFERRABLE is set, then that transaction executes with much lower overhead and is never canceled because of a serialization failure. This can be used for batch or long-running jobs, which need a consistent snapshot of the database without interfering or being interfered with by other transactions. For example:"
96,BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE READ ONLY DEFERRABLE;
96,SELECT * FROM very_large_table;
96,COMMIT;
96,Optimistic concurrency control
96,"As noted, all transactions are dynamically assigned a priority. This is a value in the range of [0.0, 1.0]. The current priority can be fetched using the yb_get_current_transaction_priority setting as follows:"
96,SELECT yb_get_current_transaction_priority();
96,yb_get_current_transaction_priority
96,-------------------------------------------
96,0.000000000 (Normal priority transaction)
96,(1 row)
96,"The priority value is bound by two settings, namely yb_transaction_priority_lower_bound and yb_transaction_priority_upper_bound. If an application would like a specific transaction to be given higher priority, it can issue statements like the following:"
96,SET yb_transaction_priority_lower_bound=0.9;
96,SET yb_transaction_priority_upper_bound=1.0;
96,This ensures that the priority assigned to your transaction is in the range [0.9-1.0] and thereby making it a high-priority transaction.
96,Stored procedures: minimize round trips
96,A transaction block executed from the client that has multiple statements requires multiple round trips between the client and the server. Consider the following transaction:
96,BEGIN TRANSACTION;
96,UPDATE txndemo SET v = 11 WHERE k = 1;
96,UPDATE txndemo SET v = 22 WHERE k = 2;
96,UPDATE txndemo SET v = 33 WHERE k = 3;
96,COMMIT;
96,"This would entail five round trips between the application and server, which means five times the latency between the application and the server. This would be very detrimental even if the latency is low. These round trips can be avoided if these transactions are wrapped in a stored procedure. A stored procedure is executed in the server and can incorporate loops and error handling. Stored procedures can be invoked from the client in just one call as follows:"
96,CALL stored_procedure_name(argument_list);
96,"Depending on the complexity of your transaction block, this can vastly improve the performance."
96,Learn more
96,Transaction error codes - Various error codes returned during transaction processing.
96,Transaction error handling - Methods to handle various error codes to design highly available applications.
96,Transaction isolation levels - Various isolation levels supported by YugabyteDB.
96,Concurrency control - Policies to handle conflicts between transactions.
96,Transaction priorities - Priority buckets for transactions.
96,Transaction options - Options supported by transactions.
96,Fast single-row transactions
96,Minimize conflict errors
96,Avoid long waits
96,Handle idle applications
96,Large scans and batch jobs
96,Optimistic concurrency control
96,Stored procedures: minimize round trips
96,Learn more
96,"© 2024 Yugabyte, Inc. All Rights ReservedPrivacy Policy |"
96,Terms of Service
98,PGSQL Phriday – A monthly community blog event for the PostgreSQL community!
98,Skip to the content
98,Search
98,PGSQL PhridayA monthly community blog event for the PostgreSQL community!
98,Menu
98,Home
98,About
98,Rules
98,Calendar
98,Search
98,Search for:
98,Close search
98,Close Menu
98,Home
98,About
98,Rules
98,Calendar
98,Categories
98,PostgreSQL
98,PGSQL Phriday #016
98,Post author
98,By Ryan Booz
98,Post date
98,"March 6, 2024"
98,Invitation from Ryan Booz
98,"Many of the PostgreSQL newbies that I’ve engaged with lately are coming from other databases often because of company priorities to start adopting Postgres and migrating projects. Inevitably, one of the first things they want to learn once data starts flowing is how to tune queries. They typically have a lot of experience doing this type of work previously, just not with Postgres."
98,"With that as the backdrop, the invitation for March is presented below."
98,The Challenge
98,"For this month’s PGSQL Phriday, I’m asking you to discuss your process for tuning difficult queries. Specifically, try to focus on that one problematic query that really challenged you and you always use it as an example when helping or teaching others your methods."
98,"For this post, try to think beyond the standard answers, otherwise we’ll have a lot of similar posts that reference EXPLAIN plans, stats views, and a few online plan viewing/analysis tools. You can absolutely talk about those things (they are essential after all) but try to focus on something about your process when a query is about to bring the server down."
98,How did you identify that this was the problematic query?
98,Where do you start when trying to dig in to get things running again?
98,Could you solve this particular problem with configuration changes rather than query changes?
98,How do you get a representative EXPLAIN plan and with what options?
98,How did you test your modified plan or server configuration?
98,"If you don’t have access to Production, how could you verify things?"
98,Do you use a product like Postgres.ai or Neon to branch the database for quick and easy iterations without impacting production?
98,{ insert whatever question you think is valuable to answer! }
98,"I’d be delighted to have a group of posts to point new users to around the topic of query tuning. While there are many presentations and resources on the pieces that help get the job done, making it personal and talking about your specific environment, challenges, and “ah ha!” moments to solve the query tuning case! And who knows, your post might be the foundation of a great conference talk in the future. (hint, hint… 😉)"
98,Remember the (simple) rules
98,"Write your article and post it Friday, March 8 any time. We’re pretty lenient here, so if you post it early or sometime over the following weekend, that’s OK."
98,Use “PGSQL Phriday #016” or “#PGSQLPhriday 016” in the title or first paragraph of your post.
98,"Link back to this invitation post at softwareandbooz.com. If you don’t, I may not include it in the final summary post."
98,Announce your post somewhere on social media or the #pgsqlphriday channel on the PostgreSQL Slack. Use the hashtag #pgsqlphriday to get noticed and included.
98,Tags
98,2024
98,Categories
98,PostgreSQL
98,PGSQL Phriday #015
98,Post author
98,By Ryan Booz
98,Post date
98,"January 27, 2024"
98,Invitation from Lætitia Avrot
98,"Lætitia is hosting for the second time, and the topic is sure to get a great discussion going – just like the first time she hosted and the conversation really Triggered some healthy debate. 🙂"
98,Lætitia’s invitation and challenge
98,"It’s sometimes difficult to find a great topic and I like the kind of topics where there is no consensus. This one is a little particular: in my humble opinion, the database community has a consensus, it’s that developers don’t agree."
98,Let’s organize a debate between database people and developers! I hope this will lead to great conversation and better understanding between those two groups!
98,The challenge
98,"Without further ado, here is the topic of the month: UUDI! In particular, I’d like to hear about (feel free to embrace the subject and not follow this guide):"
98,Your use case (why do you absolutely need them)
98,The problems you encountered (including performance issues and how you solved them)
98,what kind of UUID do you use?
98,"what are your internal rules about them (all UUIDs, mix between regular IDs and UUIDs, no IDs) and how and why you came up with"
98,UUIDs with sharding
98,sorting UUIDs
98,extension you might use
98,any other thing related to UUIDs
98,Tags
98,2024
98,Categories
98,PostgreSQL
98,PGSQL Phriday #014
98,Post author
98,By Ryan Booz
98,Post date
98,"November 28, 2023"
98,Invitation from Pavlo Golub and summary post
98,"To finish out 2023, Pavlo is inviting us to think about PostgreSQL events and write about our experiences. While the world of in-person events started to open up again in 2022, there’s no doubt that 2023 has been really busy in all parts of the world, especially for our beloved PostgreSQL database.🐘"
98,"As a retrospective, and in anticipation of things to come, Pavlo’s invitation includes many different prompts to help you get started. And there’s no doubt that PostgreSQL users old and new will benefit from hearing your experience of how to get the most value out of attending a conference."
98,Pavlo’s invitation…
98,"As PostgreSQL enthusiasts, we all know that the community’s heartbeat lies in PostgreSQL Events, whether local meetups, big conferences, or virtual gatherings, that have become prevalent during the COVID-19 pandemic."
98,"During a thrilling conference season, the PostgreSQL community is abuzz with activity. We’ve just wrapped up the PASS Data Community Summit, which featured a dedicated PostgreSQL presence that left us inspired and eager for more. The DOAG conference is underway, where numerous PostgreSQL experts are sharing their insights. And on the horizon, we eagerly anticipate the upcoming PostgreSQL Conference Europe in just a couple of weeks."
98,"I invite you to share your PostgreSQL Events experiences in your dedicated blog post. Whether you’re a seasoned attendee, a speaker, a trainer, a sponsor, an organizer, or a first-timer, your unique perspective contributes to the canvas of the PostgreSQL community."
98,"Read the rest of his invitation and prepare to publish your post this Friday, December 1, 2023."
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #013
98,Post author
98,By Ryan Booz
98,Post date
98,"September 29, 2023"
98,Invitation from Chris Ellis
98,"I first met Chris in Brussels for the FOSDEM PGDay, even though we had known each other virtually for a bit. He seems to be everywhere all at once with PostgreSQL stuff right now, including the design of some amazing little LED pins for PGDay.UK a few weeks ago."
98,Chris has enthusiastically stepped this month at short notice to provide the invitation below. I think it’s a perfect topic! I know I always learn best from the real-world experiences of others!
98,Usecases and Why PostgreSQL
98,"On Friday, October 6th 2023, publish a post on your blog telling a story about what you (or your team, client) built with PostgreSQL and how PostgreSQL helped you deliver."
98,"I’d love to read about the weird and varied things that people are using PostgreSQL for. If you think your usecase is boring, I’m sure it will be of use to someone. Plus you can always focus more on the story and how PostgreSQL helped to deliver a project, or could have, or didn’t!"
98,"Did things like: Full Text Search, JSONB, PostGIS, etc enable you to build a better application"
98,"Did using PostgreSQL remove the need for other dependencies, or change your approach"
98,Did you learn some SQL which made you realise stuff has moved along alot since SQL92
98,"Maybe it was that disaster of a project where in hindsight using PostgreSQL would have been a winner. Or maybe PostgreSQL hindered you, so let’s hear about what went wrong."
98,"I’d like to see people focus on telling some real world, practical examples of where PostgreSQL shined like a crazy diamond."
98,Read the invitation blog and get writing!!
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #012
98,Post author
98,By Ryan Booz
98,Post date
98,"August 25, 2023"
98,Invitation from Ryan Booz and summary post of all submissions
98,🎉 Woot! Woot! 🎉
98,Can you believe it? We’ve made it one complete year! This is the final invitation for the first year of PGSQL Phriday!
98,"I’ll let the invitation blog and challenge summary speak for itself. Suffice it to say, PostgreSQL 16 is just around the corner!!!"
98,The challenge from Ryan
98,"On Friday, September 1, 2023, publish a post on your blog about something (or a group of things) that you’re excited about using when PostgreSQL 16 is released."
98,Try to give some background on why this feature or improvement is important to you. Maybe reference the `pgsql-hackers` mailing list and look at the discussion that led to this feature being merged into the next release.
98,"What have done before this feature was available that you’ll do differently now? If possible, consider giving a brief example or code sample to help illustrate how it’s used. Get your reading audience excited to try this feature or improvement once they start using PostgreSQL 16!"
98,"I know these kinds of posts have already started to show up on various blogs. After all, we’re currently in beta 3 of PostgreSQL 16. But there are so many features and improvements, I think collectively we can shine some light on the value we get from the consistent cadence of PostgreSQL releases."
98,"In other words, I’m hoping we don’t end up with only posts about Logical Replication and the new (awesome) pg_stat_io view. 😉"
98,Go look at the invitation post and start
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #011
98,Post author
98,By Ryan Booz
98,Post date
98,"August 1, 2023"
98,Invitation from Tomasz Gintowt
98,"We’re a little behind on the invitation this month, so there will certainly be some wiggle room for posting responses over the weekend into Monday. Remember, this is all volunteer work. 😉"
98,"Tomasz is a new PostgreSQL friend for me and I love the topic he’s picked: Partitioning vs. Sharding. There are so many approaches in the PostgreSQL community around how to effectively and efficiently keep data light and accessible, including different approaches in various PostgreSQL extensions and database-related projects."
98,Let’s see how all of you approach this important data management tool!
98,The challenge from Tomasz
98,"I’ve spent last few months on digging into partitioning and I believe it’s natural step when our database is growing. At some point you have to realize that single table it’s not enough for you. What we can do ? Lets use partition or sharding. Many developers still don’t know the difference, use these word interchangeably. I’d like to make a PostgreSQL word a little bit better and share knowledge about partitions and shards."
98,Let’s put some light on difference between these two concepts. Please focus on:
98,difference between architecture
98,what business problem are you trying to solve
98,performance tips
98,"how tables should be designed ( relations, primary keys, FK etc)"
98,best practices
98,"Go look at the post and start working on your responses! When you do, ensure you tag #PGSQLPhriday on one of the social platforms so that it gets noticed."
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #010
98,Post author
98,By Ryan Booz
98,Post date
98,"July 3, 2023"
98,Invitation from Alicja Kucharczyk and summary post of all submissions
98,"Quick story! Alicja was one of two people that hoped on an impromptu video call with me a few years ago after asking for some help connecting with the PostgreSQL community on Twitter. Since then, she and others like Andreas Scherbaum (host of PGSQL Phriday #002 and PostgreSQL Person of the Week) continue to pour lots of energy into the community."
98,I’m thankful for their support and efforts. So many of us benefit daily from contributions like theirs!
98,"Now, onto the challenge for this month…"
98,The challenge from Alicja
98,"For this month’s PGSQL Phriday, we’re focusing on pgBadger, an exceptional PostgreSQL log analyzer renowned for its swift analysis and detailed reporting capabilities. pgBadger, a handy Perl script, has time and again proven itself an indispensable tool in the realm of database health checks and troubleshooting."
98,"Your mission, should you decide to accept it, is to pen an enlightening blog post about pgBadger on Friday, July 7th, 2023."
98,Here are some thought-provoking questions to help guide your writing:
98,How has pgBadger improved the performance of your PostgreSQL database?
98,How do you leverage the reporting capabilities of pgBadger in your routine tasks?
98,"Have you encountered any challenges while using pgBadger, and how did you overcome them?"
98,Are there any unique or innovative ways in which you have used pgBadger?
98,"How do you use information from specific tabs in the pgBadger report? Is there a particular tab, like ‘Events’, ‘Vacuum’, ‘Locks’, or ‘Top’, that has provided invaluable insights?"
98,Could you describe a use case where pgBadger helped you “save the day”?
98,"You might find it helpful to explore the official documentation, GitHub repository and this sample report as you research and write your blog post."
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #009
98,Post author
98,By Ryan Booz
98,Post date
98,"May 26, 2023"
98,Invitation from Dian M Fay and summary post of all submissions
98,"The host for this month’s event is Dian Fay, someone I’ve only met virtually because of some excellent content Dian has published for previous PGSQL Phriday blogging events."
98,"For me, that speaks to the power of these little monthly challenges, prompting a group of people to write about a common topic because we all learn so much from one another. I’ve been really blessed by the detail and thought process that Dian provides in each blog post."
98,"For the challenge this month, Dian is asking about a topic near and dear to me, managing database changes!"
98,The challenge from Dian
98,"This month’s topic is database change management, aka schema evolution. I’ve been doing this in one form or another, using one framework or another (and on one less-memorable-than-you’d-think occasion writing my own in a thousand lines of Ant XML) for almost as long as I’ve worked in software. If you interact with databases in more than a read-only capacity, you’ve probably done your share of it as well. It’s common, it’s necessary, it’s not very glamorous."
98,Some starting points:
98,how does a change make it into production? Do you have a dev-QA-staging or similar series of environments it must pass through first? Who reviews changes and what are they looking for?
98,what’s different about modifying huge tables with many millions or billions of rows? How do you tackle those changes? Do you use the same strategy for smaller tables?
98,how does Postgres make certain kinds of change easier or more difficult compared to other databases?
98,"do you believe that “rolling back” a schema change is a useful and/or meaningful concept? When and why, or why not?"
98,"how do you validate a successful schema change? Do you have any useful processes, automated or manual, that have helped you track down problems with rollout, replication, data quality or corruption, and the like?"
98,"what schema evolution or migration tools have you used? What did you like about them, what do you wish they did better or (not) at all?"
98,tales of terror in the Kletzian mode are also of course very welcome!
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #008
98,Post author
98,By Ryan Booz
98,Post date
98,"April 28, 2023"
98,Invitation from Michael Christofides and summary post of all submissions
98,"Believe it or not, we’re now eight months into this PGSQL Phriday thing and I couldn’t be happier with the community support. So many really smart, thoughtful, and community-minded folks learning from and helping one another! 🎉"
98,"This month, Michael Christofides of pgMustard is our host and he’s chosen pg_stat_statements as the topic."
98,"I’m really excited for Michael to host this month because he’s been such an encouragement to me throughout my PostgreSQL journey. Not long after I started using PostgreSQL again full-time, I went looking for tools that could help me understand EXPLAIN output better. When I happened upon pgMustard, I didn’t just find a great tool, but a sincerely kind chap that continues to work (pretty silently) to help the PostgreSQL community in many ways. He’s one half of the weekly Postgres.fm podcast as well. Well worth a listen!"
98,Check out the full invitation and “rules” for participating! I can’t wait to hear all of the ways folks are using pg_stat_statements to improve their usage of PostgreSQL!
98,The challenge from Michael
98,"I picked pg_stat_statements as the topic for a few reasons! Firstly, I wanted a topic I could easily contribute a post on myself… but more importantly, I wanted one that people with all sorts of different backgrounds and experience levels could weigh in on. It’s also the most common answer to the question “What’s your favourite extension?” in Andreas’ excellent PostgreSQL Person of the Week series."
98,"So, please feel free to take your post in any direction you want, as long as it has something to do with pg_stat_statements!"
98,Tags
98,2023
98,Categories
98,PostgreSQL
98,PGSQL Phriday #007
98,Post author
98,By Ryan Booz
98,Post date
98,"March 31, 2023"
98,Invitation from Lætita Avrot and summary post of all submissions
98,"Lætita is a long-time PostgreSQL DBA, developer, psql evangelist, and all-round 💯 contributor to the Postgres community. I always appreciate her approach to helping people learn about PostgreSQL and encouragement for folks to get involved."
98,"**Trigger warning**: This challenge is about… triggers! (sorry, I couldn’t resist)"
98,Check out the full invitation and “rules” for participating! I can’t wait to hear both sides of the debate!
98,The challenge from Lætita
98,"Your mission, should you choose to accept it, is to write a blog post by Friday the 7th of April 2023."
98,"I’m very excited because I choose a very controversial topic: triggers! Do you love them? Do you hate them? Do you sometimes love them sometimes hate them? And, most importantly, why? Do you know legitimate use cases for them? How to mitigate their drawbacks (if you think they have any)?"
98,Tags
98,2023
98,Posts navigation
98,← Newer Posts1
98,Older Posts →
98,2024							PGSQL Phriday
98,Powered by WordPress
98,To the top ↑
98,Up ↑
99,Tuning Your Database – Senzing®
99,Home
99,Search all resources
99,Contact Free Support
99,Sign in
99,Senzing®
99,Education
99,General
99,Articles in this section
99,The Path to a Successful Proof of Concept (PoC)
99,Senzing Architecture
99,Tuning Your Database
99,[Advanced] Real-time replication and analytics
99,G2Engine - Flag Values
99,How to create an entity resolution truth set
99,Scoring Search Results
99,G2Engine Configuration and the Senzing API
99,How to force records together or apart
99,Guidelines for Successful Entity Searching
99,See more
99,Tuning Your Database
99,Ant
99,"August 03, 2023 14:39"
99,Updated
99,Follow
99,Overview
99,"You may have arrived here due to perceived poor performance in general or a warning from Senzing your entity repository database is performing slowly; for example during the startup of G2Loader. The Senzing checkDBPerf() function tests how many auto-commit inserts can be performed on the Senzing entity repository within a few seconds. The faster this completes, the higher scalability you can expect from you system. This isn't the only performance area impacting Senzing but is the #1 outside of data mapping."
99,"Database performance with Senzing is highly related to latency. Issues with performance, in order, tend to focus around:"
99,Disk IO performance of the database server - see the Disk Performance article
99,Lack of database tuning for an auto-commit OLTP workload
99,Network bottlenecks preventing high-speed communication between the Senzing API and the database
99,Latency between the database server and non-direct attached storage subsystems
99,Info
99,The database parameters outlined here are the minimim required and not an exhaustive list for optimal database performance. We are happy to assist in making further recommendations but you should always include your Database Administrator in helping to monitor and tune your underlying Senzing database.
99,Tip
99,"Our customers have systems running as fast as low 10s of milliseconds for searches and mid 10s of milliseconds for loads all while running 100s or 1000s of operations in parallel -- performance depends on data, config, architecture, and hardware. If you aren't seeing the performance you are looking for, reach out so we can help you get the most from your setup."
99,Auto-Commit Tuning
99,For each database system the following parameters should be set for the Senzing auto-commit workload.
99,SQLite
99,The Senzing engine automatically sets a couple of SQLite pragmas suited to the Senzing workload. If your database is small and you have RAM to spare you could consider using tmpfs to improve performance. See the Disk Performance article.The pragmas set for SQLite:
99,synchronous = 0
99,secure_delete = 0
99,journal_mode = WAL
99,journal_size_limit = 1000000
99,DB2
99,db2set DB2_LOGGER_NON_BUFFERED_IO=ONdb2set DB2_SKIP_LOG_WAIT=YESdb2 update db cfg for <database_name> using PAGE_AGE_TRGT_MCR 10db2set DB2_USE_ALTERNATE_PAGE_CLEANING=ON
99,PostgreSQL
99,synchronous_commit=offwal_writer_delay=1000enable_seqscan=off
99,Aurora PostgreSQL
99,"Ensure that the ec2 instances running the Senzing API are in the same AZ (availability zone), VPC, and subnet as the Aurora PostgreSQL server.  This will greatly reduce latency and increase performance."
99,"If you have an Aurora PostgreSQL read replica in another AZ, all commits will be synchronous.  Either disable replicas for the large historical/initial loads or ensure the replicas are in the same AZ."
99,Note: Aurora PostgreSQL Server-less v2 is showing significantly degraded performance compared to v1.
99,The following settings are per cluster database group:
99,autovacuum_max_workers: 5
99,enable_seqscan: 0
99,pglogical.synchronous_commit: 0
99,"synchronous_commit: ""off"""
99,MS SQL and Azure Hyperscale SQL
99,"If you are running on Azure, ensure that your Azure Hyperscale database is in the same proximity zone as your applications/services that have integrated the Senzing API."
99,"For MS SQL 2019, make sure the database is set to a UTF-8 character set.  If your database is not in UTF-8 (e.g. running MS SQL 2017) it is critical that your odbc.ini set ""AutoTranslate = No"" for the configuration so data is not corrupted."
99,ALTER DATABASE <G2 DB> SET DELAYED_DURABILITY = Forced;ALTER DATABASE <G2 DB> SET AUTO_CREATE_STATISTICS ON; ALTER DATABASE <G2 DB> SET AUTO_UPDATE_STATISTICS_ASYNC ON;
99,MS SQL also has some other special case items.
99,It doesn't update the query plans automatically so the DBA will need to be involved to update statistics and flush the procedure cache.
99,"On RedHat, unixODBC is not built with --enable-fastvalidate which severely limits scaling per process.  Microsoft explains the issue here.  Ubuntu and Debian ship with this flag enabled."
99,"On Ubuntu, Microsoft supplies a version of unixODBC that does NOT contain --enable-fastvalidate for the build. This creates a 10x performance problem. You must override its versions by downgrading to the Ubuntu-built versions. Currently, that is:"
99,sudo apt install libodbc1=2.3.6-0.1build1 unixodbc=2.3.6-0.1build1
99,MySQL
99,Note: Aurora MySQL v3 does not support turning off flush on commit so it will not perform well with Senzing.
99,innodb_flush_log_at_trx_commit = 0innodb_flush_method = O_DIRECTinnodb_file_per_table = 1innodb_doublewrite=0innodb_flush_neighbors=0skip-log-bin
99,On larger Senzing systems running a high number of threads you may see errors that additional prepared statements can't be created:
99,42000 Can't create more than max_prepared_stmt_count statements (current value: 16382)
99,"If you know you will be running Senzing on a larger system with a higher than default number of threads, or you receive an error similar to the above, you will need to increase the number of prepared statements for the database. For example, on MySQL by increasing max_prepared_stmt_count:"
99,max_prepared_stmt_count = 100000
99,"100,000 is a suggested value but this will vary depending on threads and speed of system. A suggested formula is 2000 * number of Senzing threads + 500"
99,Network Bottlenecks
99,"Networking used to be very straightforward, easy to monitor and manage. There was the exceptional case of devices hitting max packets per second or driver/OS/PCI bottlenecks but those were rare."
99,The popularity of cloud environments extracts much of the network topology from the user. Your application and database could be on different sides of a data center (or continent) or even communicating through very limited virtual systems when they are co-located. Cloud networking is often a black box and the hardest component to troubleshoot.
99,We have seen systems scale from 30 records per second to 1000 records per second simply by switching the type of underlying cloud network fabric:
99,Make sure your systems are co-located as close as possible when provisioned
99,"If there are options between virtual and physical switches, test with both options"
99,Ask about both bandwidth and packets per second limits
99,Facebook
99,Twitter
99,LinkedIn
99,Was this article helpful?
99,1 out of 1 found this helpful
99,Have more questions? Submit a request
99,Return to top
99,Related articles
99,Scaling Out Your Database With Clustering
99,Setup MSSQL on Linux
99,Senzing Architecture
99,[Advanced] Replicating the Senzing results to a Data Warehouse
99,Engine Error codes
99,Comments
99,0 comments
99,Please sign in to leave a comment.
99,"SENZING®, SMARTER ENTITY RESOLUTION® and SZ® are registered trademarks of Senzing, Inc. and may not be used without prior written permission. SENZING INSIDE™ is a trademark of Senzing, Inc. and may also not be used without prior written permission."
