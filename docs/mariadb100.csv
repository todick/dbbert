filenr,sentence
3,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks - MariaDB.org
3,Skip to content
3,Download
3,Documentation
3,Contribute
3,Server Fest
3,Events
3,Sponsor
3,Blog
3,Planet MariaDB Server
3,About
3,Back
3,"Latest releases 11.4.1 (RC), 11.3.2, 11.2.3, 11.1.4, 11.0.5, 10.11.7, 10.6.17,  10.5.24, 10.4.33. Vote in our poll!"
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Search for:
3,Download
3,Documentation
3,Contribute
3,Server Fest
3,Events
3,Sponsor
3,Blog
3,Planet MariaDB Server
3,About
3,Back
3,Search for:
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Post navigation
3,Older Posts Older post:Newer Posts Newer posts:
3,"Tags: chatgpt, openai"
3,Post navigation
3,Older Posts Older post:Newer Posts Newer posts:
3,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks
3,Written by Written by Ian Gilfillan 2023-04-212023-04-21
3,"This is the next in the series of posts exploring ChatGPT and its insights about MariaDB. The series began with ChatGPT, OpenAI and MariaDB Foundation, followed by Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for Beginners."
3,"This week’s topic, suggested by ChatGPT itself, was “MariaDB Performance Tuning”. So how did it do?"
3,"MariaDB is undoubtedly one of the most popular open-source database management systems available today, with a strong focus on performance and ease of use. Whether you are a developer, database administrator, or an IT professional, ensuring that your MariaDB installation performs efficiently is crucial. In this blog, we will discuss some key tips and tricks for tuning your MariaDB server, improving its performance in various environments."
3,Identifying Bottlenecks
3,"Before diving into any optimization techniques, it’s crucial to identify the areas where performance might need improvement. Some common tools that will help you to pinpoint performance bottlenecks are:"
3,"MariaDB’s slow query log: This log records queries that take longer-than-expected to execute, helping you identify problematic SQL code."
3,"Performance Schema: A feature in MariaDB that allows you to monitor various metrics, such as query performance and resource usage."
3,mtop and mytop: These are real-time monitoring tools that show the MySQL server’s status and queries being executed at any given time.
3,"I thought we were talking about MariaDB here? And as far as I know, mtop was last updated in 2004, so I’m not sure this would be very useful. A pet bugbear about ChatGPT is its reluctance to provide links. It sometimes behaves a bit like an ignorant loudmouth, confidently spouting forth on something or other, but when you challenge them to provide evidence, nothing is forthcoming. At least it doesn’t launch into ad hominems. Its advice to use the slow query log and the performance schema can be helpful, but links would be useful. So, here are links to find out more about the slow query log, Performance Schema, as well as mtop and mytop."
3,Using these tools will give you an idea of the specific areas in which your MariaDB server needs tuning.
3,General Performance Tips
3,Optimize SQL queries
3,"Poorly written queries can lead to significant performance hits, and as such, properly optimizing SQL code is a critical step in improving the performance of your MariaDB server."
3,"Use EXPLAIN: The EXPLAIN command helps you understand how the MariaDB optimizer executes a query, allowing you to identify potentially slow operations and improve the query strategy."
3,"Proper indexing: Design and use indexes effectively to optimize search performance. Keep in mind that while more indexes can speed up SELECT queries, they may also slow down INSERT, UPDATE, and DELETE operations."
3,"Avoid using SELECT *: Instead of using SELECT * (which retrieves all columns), specify only the required columns in your queries."
3,"True, but a rather lacking in specifics, and again, links to actually find out anything useful. Try EXPLAIN and Optimizing and Indexes. To save repetition, I’ll add links directly into GPT’s text for the rest of this post."
3,Update MariaDB to the latest version
3,"MariaDB receives regular updates and improvements, so keeping your installation up-to-date is an essential step in getting the best performance possible. New releases might include feature enhancements, bug fixes, and security patches, all of which contribute to better overall performance."
3,Server-specific Performance Tips
3,Configuring InnoDB Variables
3,"InnoDB is the default storage engine for MariaDB, and adjusting its settings can significantly improve the performance of your server. Some critical InnoDB settings to consider are:"
3,"innodb_buffer_pool_size: Allocate more memory to the InnoDB buffer pool to reduce disk I/O. Typically, this value should be set to at least 70-80% of the available memory."
3,"innodb_log_file_size: A larger log file size reduces the need for frequent log rotations, thus improving write performance. However, a log size that is too large might slow down the recovery process after a crash."
3,Tweak Query Cache Variables
3,"MariaDB’s query cache is a feature that stores the results of executed queries in memory, allowing the server to serve identical queries without re-executing them."
3,"query_cache_size: Increase the size of the query cache if you have spare memory, which can improve performance for repeat queries."
3,"query_cache_limit: This value sets the maximum size of an individual query’s result that can be cached. A larger limit might cache more queries, but it might also cause more fragmentation."
3,"This isn’t very good advice. There’s a reason the query cache is disabled by default, as in many environments it actually slows things down. It may help in certain contexts, but you’d want to test this, and not blindly enable it."
3,Hardware Optimizations
3,"When it comes to improving database performance, hardware optimization is often overlooked. Some quick hardware-related adjustments you can make include:"
3,Use fast SSDs for your storage: SSDs can significantly improve read/write performance and offer better reliability than traditional hard disk drives.
3,"Increase available memory: More RAM will allow you to allocate larger InnoDB buffer pools and query caches, reducing I/O operations and improving performance."
3,"In conclusion, performance tuning for MariaDB is a continuous process. Regularly monitoring and optimizing your queries, staying up-to-date with new releases, and fine-tuning server settings as needed will contribute to the overall performance of your MariaDB installation. By following the tips and tricks outlined in this article, you are well on your way to maintaining a high-performance MariaDB server."
3,"Overall, very light on details, but a reasonably helpful high-level overview with just enough wrong to lead someone following it down some dead ends. My guess is that it overly prioritises some well-respected, but now dated, sources. It would help to train GPT to prioritise the official MariaDB Server documentation."
3,Post navigation
3,Older Posts Older posts: Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for BeginnersNewer Posts Newer posts: Code of Conduct Update
3,Contact
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Governance
3,Maintenance policy
3,Security policy
3,Privacy policy
3,Logos and badges
3,Usage statistics
3,Service providers
3,Copyright @ 2009 - 2024 MariaDB Foundation.
6,InnoDB System Variables - MariaDB Knowledge Base
6,Search
6,Products
6,Services
6,Resources
6,About
6,Contact
6,Login
6,Copyright © 2024 MariaDB. All rights reserved.
6,The Ultimate Guide to High Availability with MariaDB
6,Download Now
6,Knowledge Base
6,Contact
6,Login
6,Search
6,Products
6,Services
6,Pricing
6,Resources
6,About Us
6,Download
6,Knowledge Base
6,» MariaDB Server Documentation
6,"» Columns, Storage Engines, and Plugins"
6,» Storage Engines
6,» InnoDB
6,» InnoDB System Variables
6,Home
6,Open Questions
6,MariaDB Server
6,MariaDB MaxScale
6,MariaDB ColumnStore
6,Connectors
6,History
6,Source
6,Flag as Spam / Inappropriate
6,Translate
6,Created
6,"10 years, 10 months ago"
6,Modified
6,2 months ago
6,Type
6,article
6,Status
6,active
6,License
6,CC BY-SA / Gnu FDL
6,History
6,Comments
6,EditAttachments
6,No attachments exist
6,Parents
6,InnoDB
6,System Variables
6,InnoDB System Variables
6,Contents
6,have_innodb
6,ignore_builtin_innodb
6,innodb_adaptive_checkpoint
6,innodb_adaptive_flushing
6,innodb_adaptive_flushing_lwm
6,innodb_adaptive_flushing_method
6,innodb_adaptive_hash_index
6,innodb_adaptive_hash_index_partitions
6,innodb_adaptive_hash_index_parts
6,innodb_adaptive_max_sleep_delay
6,innodb_additional_mem_pool_size
6,innodb_api_bk_commit_interval
6,innodb_api_disable_rowlock
6,innodb_api_enable_binlog
6,innodb_api_enable_mdl
6,innodb_api_trx_level
6,innodb_auto_lru_dump
6,innodb_autoextend_increment
6,innodb_autoinc_lock_mode
6,innodb_background_scrub_data_check_interval
6,innodb_background_scrub_data_compressed
6,innodb_background_scrub_data_interval
6,innodb_background_scrub_data_uncompressed
6,innodb_blocking_buffer_pool_restore
6,innodb_buf_dump_status_frequency
6,innodb_buffer_pool_chunk_size
6,innodb_buffer_pool_dump_at_shutdown
6,innodb_buffer_pool_dump_now
6,innodb_buffer_pool_dump_pct
6,innodb_buffer_pool_evict
6,innodb_buffer_pool_filename
6,innodb_buffer_pool_instances
6,innodb_buffer_pool_load_abort
6,innodb_buffer_pool_load_at_startup
6,innodb_buffer_pool_load_now
6,innodb_buffer_pool_load_pages_abort
6,innodb_buffer_pool_populate
6,innodb_buffer_pool_restore_at_startup
6,innodb_buffer_pool_shm_checksum
6,innodb_buffer_pool_shm_key
6,innodb_buffer_pool_size
6,innodb_change_buffer_dump
6,innodb_change_buffer_max_size
6,innodb_change_buffering
6,innodb_change_buffering_debug
6,innodb_checkpoint_age_target
6,innodb_checksum_algorithm
6,innodb_checksums
6,innodb_cleaner_lsn_age_factor
6,innodb_cmp_per_index_enabled
6,innodb_commit_concurrency
6,innodb_compression_algorithm
6,innodb_compression_default
6,innodb_compression_failure_threshold_pct
6,innodb_compression_level
6,innodb_compression_pad_pct_max
6,innodb_concurrency_tickets
6,innodb_corrupt_table_action
6,innodb_data_file_buffering
6,innodb_data_file_path
6,innodb_data_file_write_through
6,innodb_data_home_dir
6,innodb_deadlock_detect
6,innodb_deadlock_report
6,innodb_default_page_encryption_key
6,innodb_default_encryption_key_id
6,innodb_default_row_format
6,innodb_defragment
6,innodb_defragment_fill_factor
6,innodb_defragment_fill_factor_n_recs
6,innodb_defragment_frequency
6,innodb_defragment_n_pages
6,innodb_defragment_stats_accuracy
6,innodb_dict_size_limit
6,innodb_disable_sort_file_cache
6,innodb_disallow_writes
6,innodb_doublewrite
6,innodb_doublewrite_file
6,innodb_empty_free_list_algorithm
6,innodb_enable_unsafe_group_commit
6,innodb_encrypt_log
6,innodb_encrypt_tables
6,innodb_encrypt_temporary_tables
6,innodb_encryption_rotate_key_age
6,innodb_encryption_rotation_iops
6,innodb_encryption_threads
6,innodb_extra_rsegments
6,innodb_extra_undoslots
6,innodb_fake_changes
6,innodb_fast_checksum
6,innodb_fast_shutdown
6,innodb_fatal_semaphore_wait_threshold
6,innodb_file_format
6,innodb_file_format_check
6,innodb_file_format_max
6,innodb_file_per_table
6,innodb_fill_factor
6,innodb_flush_log_at_timeout
6,innodb_flush_log_at_trx_commit
6,innodb_flush_method
6,innodb_flush_neighbor_pages
6,innodb_flush_neighbors
6,innodb_flush_sync
6,innodb_flushing_avg_loops
6,innodb_force_load_corrupted
6,innodb_force_primary_key
6,innodb_force_recovery
6,innodb_foreground_preflush
6,innodb_ft_aux_table
6,innodb_ft_cache_size
6,innodb_ft_enable_diag_print
6,innodb_ft_enable_stopword
6,innodb_ft_max_token_size
6,innodb_ft_min_token_size
6,innodb_ft_num_word_optimize
6,innodb_ft_result_cache_limit
6,innodb_ft_server_stopword_table
6,innodb_ft_sort_pll_degree
6,innodb_ft_total_cache_size
6,innodb_ft_user_stopword_table
6,innodb_ibuf_accel_rate
6,innodb_ibuf_active_contract
6,innodb_ibuf_max_size
6,innodb_idle_flush_pct
6,innodb_immediate_scrub_data_uncompressed
6,innodb_import_table_from_xtrabackup
6,innodb_instant_alter_column_allowed
6,innodb_instrument_semaphores
6,innodb_io_capacity
6,innodb_io_capacity_max
6,innodb_kill_idle_transaction
6,innodb_large_prefix
6,innodb_lazy_drop_table
6,innodb_lock_schedule_algorithm
6,innodb_lock_wait_timeout
6,innodb_locking_fake_changes
6,innodb_locks_unsafe_for_binlog
6,innodb_log_arch_dir
6,innodb_log_arch_expire_sec
6,innodb_log_archive
6,innodb_log_block_size
6,innodb_log_buffer_size
6,innodb_log_checksum_algorithm
6,innodb_log_checksums
6,innodb_log_compressed_pages
6,innodb_log_file_buffering
6,innodb_log_file_size
6,innodb_log_file_write_through
6,innodb_log_files_in_group
6,innodb_log_group_home_dir
6,innodb_log_optimize_ddl
6,innodb_log_write_ahead_size
6,innodb_lru_flush_size
6,innodb_lru_scan_depth
6,innodb_max_bitmap_file_size
6,innodb_max_changed_pages
6,innodb_max_dirty_pages_pct
6,innodb_max_dirty_pages_pct_lwm
6,innodb_max_purge_lag
6,innodb_max_purge_lag_delay
6,innodb_max_purge_lag_wait
6,innodb_max_undo_log_size
6,innodb_merge_sort_block_size
6,innodb_mirrored_log_groups
6,innodb_mtflush_threads
6,innodb_monitor_disable
6,innodb_monitor_enable
6,innodb_monitor_reset
6,innodb_monitor_reset_all
6,innodb_numa_interleave
6,innodb_old_blocks_pct
6,innodb_old_blocks_time
6,innodb_online_alter_log_max_size
6,innodb_open_files
6,innodb_optimize_fulltext_only
6,innodb_page_cleaners
6,innodb_page_size
6,innodb_pass_corrupt_table
6,innodb_prefix_index_cluster_optimization
6,innodb_print_all_deadlocks
6,innodb_purge_batch_size
6,innodb_purge_rseg_truncate_frequency
6,innodb_purge_threads
6,innodb_random_read_ahead
6,innodb_read_ahead
6,innodb_read_ahead_threshold
6,innodb_read_io_threads
6,innodb_read_only
6,innodb_read_only_compressed
6,innodb_recovery_stats
6,innodb_recovery_update_relay_log
6,innodb_replication_delay
6,innodb_rollback_on_timeout
6,innodb_rollback_segments
6,innodb_safe_truncate
6,innodb_scrub_log
6,innodb_scrub_log_interval
6,innodb_scrub_log_speed
6,innodb_sched_priority_cleaner
6,innodb_show_locks_held
6,innodb_show_verbose_locks
6,innodb_simulate_comp_failures
6,innodb_sort_buffer_size
6,innodb_spin_wait_delay
6,innodb_stats_auto_recalc
6,innodb_stats_auto_update
6,innodb_stats_include_delete_marked
6,innodb_stats_method
6,innodb_stats_modified_counter
6,innodb_stats_on_metadata
6,innodb_stats_persistent
6,innodb_stats_persistent_sample_pages
6,innodb_stats_sample_pages
6,innodb_stats_traditional
6,innodb_stats_transient_sample_pages
6,innodb_stats_update_need_lock
6,innodb_status_output
6,innodb_status_output_locks
6,innodb_strict_mode
6,innodb_support_xa
6,innodb_sync_array_size
6,innodb_sync_spin_loops
6,innodb_table_locks
6,innodb_thread_concurrency
6,innodb_thread_concurrency_timer_based
6,innodb_thread_sleep_delay
6,innodb_temp_data_file_path
6,innodb_tmpdir
6,innodb_track_changed_pages
6,innodb_track_redo_log_now
6,innodb_truncate_temporary_tablespace_now
6,innodb_undo_directory
6,innodb_undo_log_truncate
6,innodb_undo_logs
6,innodb_undo_tablespaces
6,innodb_use_atomic_writes
6,innodb_use_fallocate
6,innodb_use_global_flush_log_at_trx_commit
6,innodb_use_mtflush
6,innodb_use_native_aio
6,innodb_use_purge_thread
6,innodb_use_stacktrace
6,innodb_use_sys_malloc
6,innodb_use_sys_stats_table
6,innodb_use_trim
6,innodb_version
6,innodb_write_io_threads
6,"This page documents system variables related to the InnoDB storage engine. For options that are not system variables, see InnoDB Options."
6,See Server System Variables for a complete list of system variables and instructions on setting them.
6,"Also see the Full list of MariaDB options, system and status variables."
6,have_innodb
6,"Description: If the server supports InnoDB tables, will be set to YES, otherwise will be set to NO. Removed in MariaDB 10.0, use the Information Schema PLUGINS table or SHOW ENGINES instead."
6,Scope: Global
6,Dynamic: No
6,Removed: MariaDB 10.0
6,ignore_builtin_innodb
6,"Description: Setting this to 1 results in the built-in InnoDB storage engine being ignored. In some versions of MariaDB, XtraDB is the default and is always present, so this variable is ignored and setting it results in a warning. From MariaDB 10.0.1 to MariaDB 10.0.8, when InnoDB was the default instead of XtraDB, this variable needed to be set. Usually used in conjunction with the plugin-load=innodb=ha_innodb option to use the InnoDB plugin."
6,Commandline: --ignore-builtin-innodb
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,innodb_adaptive_checkpoint
6,"Description: Replaced with innodb_adaptive_flushing_method. Controls adaptive checkpointing. InnoDB's fuzzy checkpointing can cause stalls, as many dirty blocks are flushed at once as the checkpoint age nears the maximum. Adaptive checkpointing aims for more consistent flushing, approximately modified age / maximum checkpoint age. Can result in larger transaction log files"
6,"reflex Similar to innodb_max_dirty_pages_pct flushing but flushes blocks constantly and contiguously based on the oldest modified age. If the age exceeds 1/2 of the maximum age capacity, flushing will be weak contiguous. If the age exceeds 3/4, flushing will be strong. Strength can be adjusted by the variable innodb_io_capacity."
6,"estimate The default, and independent of innodb_io_capacity. If the oldest modified age exceeds 1/2 of the maximum age capacity, blocks will be flushed every second at a rate determined by the number of modified blocks, LSN progress speed and the average age of all modified blocks."
6,keep_average Attempts to keep the I/O rate constant by using a shorter loop cycle of one tenth of a second. Designed for SSD cards.
6,Commandline: --innodb-adaptive-checkpoint=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value: estimate
6,"Valid Values: none or 0, reflex or 1, estimate or 2, keep_average or 3"
6,Removed: XtraDB 5.5 - replaced with innodb_adaptive_flushing_method
6,innodb_adaptive_flushing
6,"Description: If set to 1, the default, the server will dynamically adjust the flush rate of dirty pages in the InnoDB buffer pool. This assists to reduce brief bursts of I/O activity. If set to 0, adaptive flushing will only take place when the limit specified by innodb_adaptive_flushing_lwm is reached."
6,Commandline: --innodb-adaptive-flushing={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_adaptive_flushing_lwm
6,Description: Adaptive flushing is enabled when this low water mark percentage of the InnoDB redo log capacity is reached. Takes effect even if innodb_adaptive_flushing is disabled.
6,Commandline: --innodb-adaptive-flushing-lwm=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: double
6,Default Value: 10.000000
6,Range: 0 to 70
6,innodb_adaptive_flushing_method
6,"Description: Determines the method of flushing dirty blocks from the InnoDB buffer pool. If set to native or 0, the original InnoDB method is used. The maximum checkpoint age is determined by the total length of all transaction log files. When the checkpoint age reaches the maximum checkpoint age, blocks are flushed. This can cause lag if there are many updates per second and many blocks with an almost identical age need to be flushed. If set to estimate or 1, the default, the oldest modified age will be compared with the maximum age capacity. If it's more than 1/4 of this age, blocks are flushed every second. The number of blocks flushed is determined by the number of modified blocks, the LSN progress speed and the average age of all modified blocks. It's therefore independent of the innodb_io_capacity for the 1-second loop, but not entirely so for the 10-second loop. If set to keep_average or 2, designed specifically for SSD cards, a shorter loop cycle is used in an attempt to keep the I/O rate constant."
6,Removed in MariaDB 10.0/XtraDB 5.6 and replaced with InnoDB flushing method from MySQL 5.6.
6,Commandline: innodb-adaptive-flushing-method=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: estimate
6,"Valid Values: native or 0, estimate or 1, keep_average or 2"
6,Removed: MariaDB 10.0 - replaced with InnoDB flushing method from MySQL 5.6
6,innodb_adaptive_hash_index
6,"Description: If set to 1, the default until MariaDB 10.5, the InnoDB hash index is enabled. Based on performance testing (MDEV-17492), the InnoDB adaptive hash index helps performance in mostly read-only workloads, and could slow down performance in other environments, especially DROP TABLE, TRUNCATE TABLE, ALTER TABLE, or DROP INDEX operations."
6,Commandline: --innodb-adaptive-hash-index={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,"Default Value: OFF (>= MariaDB 10.5), ON (<= MariaDB 10.4)"
6,innodb_adaptive_hash_index_partitions
6,"Description: Specifies the number of partitions for use in adaptive searching. If set to 1, no extra partitions are created. XtraDB-only. From MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB), this is an alias for innodb_adaptive_hash_index_parts to allow for easier upgrades."
6,Commandline: innodb-adaptive-hash-index-partitions=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1
6,Range: 1 to 64
6,innodb_adaptive_hash_index_parts
6,"Description: Specifies the number of partitions for use in adaptive searching. If set to 1, no extra partitions are created."
6,Commandline: innodb-adaptive-hash-index-parts=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 8
6,Range: 1 to 512
6,innodb_adaptive_max_sleep_delay
6,"Description: Maximum time in microseconds to automatically adjust the innodb_thread_sleep_delay value to, based on the workload. Useful in extremely busy systems with hundreds of thousands of simultaneous connections. 0 disables any limit. Deprecated and ignored from MariaDB 10.5.5."
6,Commandline: --innodb-adaptive-max-sleep-delay=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,0 (>= MariaDB 10.5.5)
6,150000 (<= MariaDB 10.5.4)
6,Range: 0 to 1000000
6,Introduced: MariaDB 10.0
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_additional_mem_pool_size
6,"Description: Size in bytes of the InnoDB memory pool used for storing information about internal data structures. Defaults to 8MB, if your application has many tables and a large structure, and this is exceeded, operating system memory will be allocated and warning messages written to the error log, in which case you should increase this value. Deprecated in MariaDB 10.0 and removed in MariaDB 10.2 along with InnoDB's internal memory allocator."
6,Commandline: --innodb-additional-mem-pool-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 8388608
6,Range: 2097152 to 4294967295
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.2.2
6,innodb_api_bk_commit_interval
6,Description: Time in seconds between auto-commits for idle connections using the InnoDB memcached interface (not implemented in MariaDB).
6,Commandline: --innodb-api-bk-commit-interval=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 5
6,Range: 1 to 1073741824
6,Introduced: MariaDB 10.0
6,Removed: MariaDB 10.2.4
6,innodb_api_disable_rowlock
6,Description: For use with MySQL's memcached (not implemented in MariaDB)
6,Commandline: --innodb-api-disable-rowlock={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,Removed: MariaDB 10.2.4
6,innodb_api_enable_binlog
6,Description: For use with MySQL's memcached (not implemented in MariaDB)
6,Commandline: --innodb-api-enable-binlog={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,Removed: MariaDB 10.2.4
6,innodb_api_enable_mdl
6,Description: For use with MySQL's memcached (not implemented in MariaDB)
6,Commandline: --innodb-api-enable-mdl={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,Removed: MariaDB 10.2.4
6,innodb_api_trx_level
6,Description: For use with MySQL's memcached (not implemented in MariaDB)
6,Commandline: --innodb-api-trx-level=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Introduced: MariaDB 10.0
6,Removed: MariaDB 10.2.4
6,innodb_auto_lru_dump
6,"Description: Renamed innodb_buffer_pool_restore_at_startup since XtraDB 5.5.10-20.1, which was in turn replaced by innodb_buffer_pool_load_at_startup in MariaDB 10.0."
6,Commandline: --innodb-auto-lru-dump=#
6,Removed: XtraDB 5.5.10-20.1
6,innodb_autoextend_increment
6,"Description: Size in MB to increment an auto-extending shared tablespace file when it becomes full. If innodb_file_per_table was set to 1, this setting does not apply to the resulting per-table tablespace files, which are automatically extended in their own way."
6,Commandline: --innodb-autoextend-increment=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,"Default Value: 64 (from MariaDB 10.0) 8 (before MariaDB 10.0),"
6,Range: 1 to 1000
6,innodb_autoinc_lock_mode
6,Description: The lock mode that is used when generating AUTO_INCREMENT values for InnoDB tables.
6,Valid values are:
6,0 is the traditional lock mode.
6,1 is the consecutive lock mode.
6,2 is the interleaved lock mode.
6,"In order to use Galera Cluster, the lock mode needs to be set to 2."
6,See AUTO_INCREMENT Handling in InnoDB: AUTO_INCREMENT Lock Modes for more information.
6,Commandline: --innodb-autoinc-lock-mode=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1
6,Range: 0 to 2
6,innodb_background_scrub_data_check_interval
6,Description: Check if spaces needs scrubbing every innodb_background_scrub_data_check_interval seconds. See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2.
6,Commandline: --innodb-background-scrub-data-check-interval=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 3600
6,Range: 1 to 4294967295
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_background_scrub_data_compressed
6,Description: Enable scrubbing of compressed data by background threads (same as encryption_threads). See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2.
6,Commandline: --innodb-background-scrub-data-compressed={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 0
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_background_scrub_data_interval
6,Description: Scrub spaces that were last scrubbed longer than this number of seconds ago. See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2.
6,Commandline: --innodb-background-scrub-data-interval=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 604800
6,Range: 1 to 4294967295
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_background_scrub_data_uncompressed
6,Description: Enable scrubbing of uncompressed data by background threads (same as encryption_threads). See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2.
6,Commandline: --innodb-background-scrub-data-uncompressed={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 0
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_blocking_buffer_pool_restore
6,"Description: If set to 1 (0 is default), XtraDB will wait until the least-recently used (LRU) dump is completely restored upon restart before reporting back to the server that it has successfully started up. Available with XtraDB only, not InnoDB."
6,Commandline: innodb-blocking-buffer-pool-restore={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.0.0
6,innodb_buf_dump_status_frequency
6,"Description: Determines how often (as a percent) the buffer pool dump status should be printed in the logs. For example, 10 means that the buffer pool dump status is printed when every 10% of the number of buffer pool pages are dumped. The default is 0 (only start and end status is printed)."
6,Commandline: --innodb-buf-dump-status-frequency=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 100
6,innodb_buffer_pool_chunk_size
6,"Description: Chunk size used for dynamically resizing the buffer pool. Note that changing this setting can change the size of the buffer pool. When large-pages is used this value is effectively rounded up to the next multiple of large-page-size. See Setting Innodb Buffer Pool Size Dynamically. From MariaDB 10.8.0, the variable is autosized based on the buffer pool size."
6,Commandline: --innodb-buffer-pool-chunk-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value:
6,"autosize (0), resulting in innodb_buffer_pool_size/64, if large_pages round down to multiple of largest page size, with 1MiB minimum (>= MariaDB 10.8.1)"
6,134217728 (<= MariaDB 10.8.0)
6,Range:
6,"0, as autosize, and then 1048576 to 18446744073709551615 (>= MariaDB 10.8)"
6,1048576 to innodb_buffer_pool_size/innodb_buffer_pool_instances (<= MariaDB 10.7)
6,innodb_buffer_pool_dump_at_shutdown
6,"Description: Whether to record pages cached in the buffer pool on server shutdown, which reduces the length of the warmup the next time the server starts. The related innodb_buffer_pool_load_at_startup specifies whether the buffer pool is automatically warmed up at startup."
6,Commandline: --innodb-buffer-pool-dump-at-shutdown={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value:
6,innodb_buffer_pool_dump_now
6,"Description: Immediately records pages stored in the buffer pool. The related innodb_buffer_pool_load_now does the reverse, and will immediately warm up the buffer pool."
6,Commandline: --innodb-buffer-pool-dump-now={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,innodb_buffer_pool_dump_pct
6,Description: Dump only the hottest N% of each buffer pool.
6,Commandline: --innodb-buffer-pool-dump-pct={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value:
6,Range: 1 to 100
6,innodb_buffer_pool_evict
6,Description: Evict pages from the buffer pool.
6,"If set to ""uncompressed"" then all uncompressed pages are evicted from the buffer pool."
6,Variable to be used only for testing. Only exists in DEBUG builds.
6,Commandline: --innodb-buffer-pool-evict=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,"Default Value: """""
6,"Valid Values: """" or ""uncompressed"""
6,innodb_buffer_pool_filename
6,Description: The file that holds the buffer pool list of page numbers set by innodb_buffer_pool_dump_at_shutdown and innodb_buffer_pool_dump_now.
6,Commandline: --innodb-buffer-pool-filename=file
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value: ib_buffer_pool
6,Introduced: MariaDB 10.0
6,innodb_buffer_pool_instances
6,"Description: If innodb_buffer_pool_size is set to more than 1GB, innodb_buffer_pool_instances divides the InnoDB buffer pool into the specified number of instances. The default was 1 in MariaDB 5.5, but for large systems with buffer pools of many gigabytes, many instances could help reduce contention concurrency through MariaDB 10.2. The default is 8 in MariaDB 10 (except on Windows 32-bit, where it varies according to innodb_buffer_pool_size, or from MariaDB 10.2.2, where it is set to 1 if innodb_buffer_pool_size < 1GB). Each instance manages its own data structures and takes an equal portion of the total buffer pool size, so for example if innodb_buffer_pool_size is 4GB and innodb_buffer_pool_instances is set to 4, each instance will be 1GB. Each instance should ideally be at least 1GB in size."
6,"Starting with MariaDB 10.3, performance improvements intended to reduce the overhead of context-switching between buffer pools changed the recommended number of innodb_buffer_pool_instances to one for every 128GB of buffer pool size. Based on these changes, the variable is deprecated and ignored from MariaDB 10.5.1, where the buffer pool runs in a single instance regardless of size."
6,Commandline: --innodb-buffer-pool-instances=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: >= MariaDB 10.0.4:
6,"8, 1 (>= MariaDB 10.2.2 if innodb_buffer_pool_size < 1GB), or dependent on innodb_buffer_pool_size (Windows 32-bit)"
6,Deprecated: MariaDB 10.5.1
6,Removed: MariaDB 10.6.0
6,innodb_buffer_pool_load_abort
6,Description: Aborts the process of restoring buffer pool contents started by innodb_buffer_pool_load_at_startup or innodb_buffer_pool_load_now.
6,Commandline: --innodb-buffer-pool-load-abort={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_buffer_pool_load_at_startup
6,"Description: Specifies whether the buffer pool is automatically warmed up when the server starts by loading the pages held earlier. The related innodb_buffer_pool_dump_at_shutdown specifies whether pages are saved at shutdown. If the buffer pool is large and taking a long time to load, increasing innodb_io_capacity at startup may help."
6,Commandline: --innodb-buffer-pool-load-at-startup={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value:
6,innodb_buffer_pool_load_now
6,"Description: Immediately warms up the buffer pool by loading the stored data pages. The related innodb_buffer_pool_dump_now does the reverse, and immediately records pages stored in the buffer pool."
6,Commandline: --innodb-buffer-pool-load-now={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,innodb_buffer_pool_load_pages_abort
6,Description: Number of pages during a buffer pool load to process before signaling innodb_buffer_pool_load_abort=1. Debug builds only.
6,Commandline: --innodb-buffer-pool-load-pages-abort=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 9223372036854775807
6,Range: 1 to 9223372036854775807
6,Introduced: MariaDB 10.3
6,innodb_buffer_pool_populate
6,"Description: When set to 1 (0 is default), XtraDB will preallocate pages in the buffer pool on starting up so that NUMA allocation decisions are made while the buffer cache is still clean. XtraDB only. This option was made ineffective in MariaDB 10.0.23. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: innodb-buffer-pool-populate={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.0.23
6,Removed: MariaDB 10.3.0
6,innodb_buffer_pool_restore_at_startup
6,"Description: Time in seconds between automatic buffer pool dumps. If set to a non-zero value, XtraDB will also perform an automatic restore of the buffer pool at startup. If set to 0, automatic dumps are not performed, nor automatic restores on startup. Replaced by innodb_buffer_pool_load_at_startup in MariaDB 10.0."
6,Commandline: innodb-buffer-pool-restore-at-startup
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range - 32 bit: 0 to 4294967295
6,Range - 64 bit: 0 to 18446744073709547520
6,Removed: MariaDB 10.0 - replaced by innodb_buffer_pool_load_at_startup
6,innodb_buffer_pool_shm_checksum
6,Description: Used with Percona's SHM buffer pool patch in XtraDB 5.5. Was shortly deprecated and removed in XtraDB 5.6. XtraDB only.
6,Commandline: innodb-buffer-pool-shm-checksum={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Removed: MariaDB 10.0
6,innodb_buffer_pool_shm_key
6,"Description: Used with Percona's SHM buffer pool patch in XtraDB 5.5. Later deprecated in XtraDB 5.5, and removed in XtraDB 5.6."
6,Commandline: innodb-buffer-pool-shm-key={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: 0
6,Removed: MariaDB 10.0
6,innodb_buffer_pool_size
6,"Description: InnoDB buffer pool size in bytes. The primary value to adjust on a database server with entirely/primarily InnoDB tables, can be set up to 80% of the total memory in these environments. See the InnoDB Buffer Pool for more on setting this variable, and also Setting InnoDB Buffer Pool Size Dynamically if doing so dynamically."
6,Commandline: --innodb-buffer-pool-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 134217728 (128MiB)
6,Range:
6,Minimum: 5242880 (5MiB ) for InnoDB Page Size <= 16k otherwise 25165824 (24MiB) for InnoDB Page Size > 16k (for versions less than next line)
6,"Minimium: 2MiB InnoDB Page Size = 4k, 3MiB InnoDB Page Size = 8k,"
6,"5MiB InnoDB Page Size = 16k, 10MiB InnoDB Page Size = 32k, 20MiB InnoDB Page Size = 64k, (>= MariaDB 10.2.42, >= MariaDB 10.3.33, >= MariaDB 10.4.23, >= MariaDB 10.5.14, >= MariaDB 10.6.6, >= MariaDB 10.7.2)"
6,Minimum: 1GiB for innodb_buffer_pool_instances > 1 (<= MariaDB 10.7)
6,Maximium: 9223372036854775807 (8192PB) (all versions)
6,innodb_change_buffer_dump
6,"Description: If set, causes the contents of the InnoDB change buffer to be dumped to the server error log at startup. Only available in debug builds."
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,"Introduced: MariaDB 10.2.28, MariaDB 10.3.19, MariaDB 10.4.9"
6,innodb_change_buffer_max_size
6,"Description: Maximum size of the InnoDB Change Buffer as a percentage of the total buffer pool. The default is 25%, and this can be increased up to 50% for servers with high write activity, and lowered down to 0 for servers used exclusively for reporting."
6,Commandline: --innodb-change-buffer-max-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 25
6,Range: 0 to 50
6,Introduced: MariaDB 10.0
6,Deprecated: MariaDB 10.9.0
6,Removed: MariaDB 11.0.0
6,innodb_change_buffering
6,Description: Sets how InnoDB change buffering is performed. See InnoDB Change Buffering for details on the settings. Deprecated and ignored from MariaDB 10.9.0.
6,Commandline: --innodb-change-buffering=#
6,Scope: Global
6,Dynamic: Yes
6,"Data Type: enumeration (>= MariaDB 10.3.7), string (<= MariaDB 10.3.6)"
6,Default Value:
6,">= MariaDB 10.5.15, MariaDB 10.6.7, MariaDB 10.7.3, MariaDB 10.8.2: none"
6,"<= MariaDB 10.5.14, MariaDB 10.6.6, MariaDB 10.7.2, MariaDB 10.8.1:all"
6,"Valid Values: inserts, none, deletes, purges, changes, all"
6,Deprecated: MariaDB 10.9.0
6,Removed: MariaDB 11.0.0
6,innodb_change_buffering_debug
6,"Description: If set to 1, an InnoDB Change Buffering debug flag is set. 1 forces all changes to the change buffer, while 2 causes a crash at merge. 0, the default, indicates no flag is set. Only available in debug builds."
6,Commandline: --innodb-change-buffering-debug=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 2
6,innodb_checkpoint_age_target
6,"Description: The maximum value of the checkpoint age. If set to 0, has no effect. Removed in MariaDB 10.0/XtraDB 5.6 and replaced with InnoDB flushing method from MySQL 5.6."
6,Commandline: innodb-checkpoint-age-target=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 upwards
6,Removed: MariaDB 10.0 - replaced with InnoDB flushing method from MySQL 5.6.
6,innodb_checksum_algorithm
6,Description: Specifies how the InnoDB tablespace checksum is generated and verified.
6,"innodb: Backwards compatible with earlier versions (<= MariaDB 5.5). Deprecated in MariaDB 10.3.29, MariaDB 10.4.19, MariaDB 10.5.10 and removed in MariaDB 10.6. If really needed, data files can still be converted with innochecksum."
6,"crc32: A newer, faster algorithm, but incompatible with earlier versions. Tablespace blocks will be converted to the new format over time, meaning that a mix of checksums may be present."
6,"full_crc32 and strict_full_crc32: From MariaDB 10.4.3. Permits encryption to be supported over a SPATIAL INDEX, which crc32 does not support. Newly-created data files will carry a flag that indicates that all pages of the file will use a full CRC-32C checksum over the entire page contents (excluding the bytes where the checksum is stored, at the very end of the page). Such files will always use that checksum, no matter what parameter innodb_checksum_algorithm is assigned to. Even if innodb_checksum_algorithm is modified later, the same checksum will continue to be used. A special flag will be set in the FSP_SPACE_FLAGS in the first data page to indicate the new format of checksum and encryption/page_compressed. ROW_FORMAT=COMPRESSED tables will only use the old format."
6,"These tables do not support new features, such as larger innodb_page_size or instant ADD/DROP COLUMN. Also cleans up the MariaDB tablespace flags - flags are reserved to store the page_compressed compression algorithm, and to store the compressed payload length, so that checksum can be computed over the compressed (and possibly encrypted) stream and can be validated without decrypting or decompressing the page. In the full_crc32 format, there no longer are separate before-encryption and after-encryption checksums for pages. The single checksum is computed on the page contents that is written to the file.See MDEV-12026 for details."
6,"none: Writes a constant rather than calculate a checksum. Deprecated in MariaDB 10.3.29, MariaDB 10.4.19, MariaDB 10.5.10 and removed in MariaDB 10.6 as was mostly used to disable the original, slow, page checksum for benchmarking purposes."
6,"strict_crc32, strict_innodb and strict_none: The options are the same as the regular options, but InnoDB will halt if it comes across a mix of checksum values. These are faster, as both new and old checksum values are not required, but can only be used when setting up tablespaces for the first time."
6,Commandline: --innodb-checksum-algorithm=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value:
6,full_crc32 (>= MariaDB 10.5.0)
6,crc32 (>= MariaDB 10.2.2 to <= MariaDB 10.4)
6,innodb (<= MariaDB 10.2.1)
6,Valid Values:
6,">= MariaDB 10.6.0: crc32, full_crc32, strict_crc32, strict_full_crc32"
6,"MariaDB 10.5, >= MariaDB 10.4.3: innodb, crc32, full_crc32, none, strict_innodb, strict_crc32, strict_none, strict_full_crc32"
6,"<= MariaDB 10.4.2: innodb, crc32, none, strict_innodb, strict_crc32, strict_none"
6,innodb_checksums
6,"Description: By default, InnoDB performs checksum validation on all pages read from disk, which provides extra fault tolerance. You would usually want this set to 1 in production environments, although setting it to 0 can provide marginal performance improvements. Deprecated and functionality replaced by innodb_checksum_algorithm in MariaDB 10.0, and should be removed to avoid conflicts. ON is equivalent to --innodb_checksum_algorithm=innodb and OFF to --innodb_checksum_algorithm=none."
6,"Commandline: --innodb-checksums, --skip-innodb-checksums"
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.5.0
6,innodb_cleaner_lsn_age_factor
6,"Description: XtraDB has enhanced page cleaner heuristics, and with these in place, the default InnoDB adaptive flushing may be too aggressive. As a result, a new LSN age factor formula has been introduced, controlled by this variable. The default setting, high_checkpoint, uses the new formula, while the alternative, legacy, uses the original algorithm. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: --innodb-cleaner-lsn-age-factor=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value:
6,deprecated
6,Valid Values:
6,"deprecated, high_checkpoint, legacy"
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_cmp_per_index_enabled
6,"Description: If set to ON (OFF is default), per-index compression statistics are stored in the INFORMATION_SCHEMA.INNODB_CMP_PER_INDEX table. These are expensive to record, so this setting should only be changed with care, such as for performance tuning on development or replica servers."
6,Commandline: --innodb-cmp-per-index-enabled={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 10.0
6,innodb_commit_concurrency
6,"Description: Limit to the number of transaction threads that can can commit simultaneously. 0, the default, imposes no limit. While you can change from one positive limit to another at runtime, you cannot set this variable to 0, or change it from 0, while the server is running. Deprecated and ignored from MariaDB 10.5.5."
6,Commandline: --innodb-commit-concurrency=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 1000
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_compression_algorithm
6,Description: Compression algorithm used for InnoDB page compression. The supported values are:
6,none: Pages are not compressed.
6,zlib: Pages are compressed using the bundled zlib compression algorithm.
6,lz4: Pages are compressed using the lz4 compression algorithm.
6,lzo: Pages are compressed using the lzo compression algorithm.
6,lzma: Pages are compressed using the lzma compression algorithm.
6,bzip2: Pages are compressed using the bzip2 compression algorithm.
6,snappy: Pages are compressed using the snappy algorithm.
6,"On many distributions, MariaDB may not support all page compression algorithms by default. From MariaDB 10.7, libraries can be installed as a plugin. See Compression Plugins."
6,See InnoDB Page Compression: Configuring the InnoDB Page Compression Algorithm for more information.
6,Commandline: --innodb-compression-algorithm=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value: zlib
6,"Valid Values:none, zlib, lz4, lzo, lzma, bzip2 or snappy"
6,innodb_compression_default
6,Description: Whether or not InnoDB page compression is enabled by default for new tables.
6,"The default value is OFF, which means new tables are not compressed."
6,See InnoDB Page Compression: Enabling InnoDB Page Compression by Default for more information.
6,Commandline: --innodb-compression-default={0|1}
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_compression_failure_threshold_pct
6,"Description: Specifies the percentage cutoff for expensive compression failures during updates to a table that uses InnoDB page compression, after which free space is added to each new compressed page, dynamically adjusted up to the level set by innodb_compression_pad_pct_max. Zero disables checking of compression efficiency and adjusting padding."
6,See InnoDB Page Compression: Configuring the Failure Threshold and Padding for more information.
6,Commandline: --innodb-compression-failure-threshold-pct=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 5
6,Range: 0 to 100
6,Introduced: MariaDB 10.0
6,innodb_compression_level
6,Description: Specifies the default level of compression for tables that use InnoDB page compression.
6,"Only a subset of InnoDB page compression algorithms support compression levels. If an InnoDB page compression algorithm does not support compression levels, then the compression level value is ignored."
6,"The compression level can be set to any value between 1 and 9. The default compression level is 6. The range goes from the fastest to the most compact, which means that 1 is the fastest and 9 is the most compact."
6,See InnoDB Page Compression: Configuring the Default Compression Level for more information.
6,Commandline: --innodb-compression-level=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 6
6,Range: 1 to 9
6,Introduced: MariaDB 10.0
6,innodb_compression_pad_pct_max
6,"Description: The maximum percentage of reserved free space within each compressed page for tables that use InnoDB page compression. Reserved free space is used when the page's data is reorganized and might be recompressed. Only used when innodb_compression_failure_threshold_pct is not zero, and the rate of compression failures exceeds its setting."
6,See InnoDB Page Compression: Configuring the Failure Threshold and Padding for more information.
6,Commandline: --innodb-compression-pad-pct-max=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 50
6,Range: 0 to 75
6,Introduced: MariaDB 10.0
6,innodb_concurrency_tickets
6,Description: Number of times a newly-entered thread can enter and leave InnoDB until it is again subject to the limitations of innodb_thread_concurrency and may possibly be queued. Deprecated and ignored from MariaDB 10.5.5.
6,Commandline: --innodb-concurrency-tickets=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,0 (>= MariaDB 10.5.5)
6,5000 (<= MariaDB 10.5.4)
6,Range: 1 to 18446744073709551615
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_corrupt_table_action
6,Description: What action to perform when a corrupt table is found. XtraDB only.
6,"When set to assert, the default, XtraDB will intentionally crash the server when it detects corrupted data in a single-table tablespace, with an assertion failure."
6,"When set to warn, it will pass corruption as corrupt table instead of crashing, and disable all further I/O (except for deletion) on the table file."
6,"If set to salvage, read access is permitted, but corrupted pages are ignored. innodb_file_per_table must be enabled for this option. Previously named innodb_pass_corrupt_table."
6,Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-corrupt-table-action=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value:
6,assert (<= MariaDB 10.1)
6,deprecated (<= MariaDB 10.2.6)
6,Valid Values:
6,"deprecated, assert, warn, salvage"
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_data_file_buffering
6,"Description: Whether to enable the file system cache for data files. Set to OFF by default, will be set to ON if innodb_flush_method is set to fsync, littlesync, nosync, or (Windows specific) normal."
6,Commandline: --innodb-data-file-buffering={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 11.0.0
6,innodb_data_file_path
6,"Description: Individual InnoDB data files, paths and sizes. The value of innodb_data_home_dir is joined to each path specified by innodb_data_file_path to get the full directory path. If innodb_data_home_dir is an empty string, absolute paths can be specified here. A file size is specified with K for kilobytes, M for megabytes and G for gigabytes, and whether or not to autoextend the data file is also specified."
6,Commandline: --innodb-data-file-path=name
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,"Default Value: ibdata1:12M:autoextend (from MariaDB 10.0), ibdata1:10M:autoextend (before MariaDB 10.0)"
6,innodb_data_file_write_through
6,"Description: Whether writes to InnoDB data files (including the temporary tablespace) are write through. Set to OFF by default, will be set to ON if innodb_flush_method is set to O_DSYNC. On systems that support FUA it may make sense to enable write-through, to avoid extra system calls."
6,Commandline: --innodb-data-file-write-through={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 11.0.0
6,innodb_data_home_dir
6,"Description: Directory path for all InnoDB data files in the shared tablespace (assuming innodb_file_per_table is not enabled). File-specific information can be added in innodb_data_file_path, as well as absolute paths if innodb_data_home_dir is set to an empty string."
6,Commandline: --innodb-data-home-dir=path
6,Scope: Global
6,Dynamic: No
6,Data Type: directory name
6,Default Value: The MariaDB data directory
6,innodb_deadlock_detect
6,"Description: By default, the InnoDB deadlock detector is enabled. If set to off, deadlock detection is disabled and MariaDB will rely on innodb_lock_wait_timeout instead. This may be more efficient in systems with high concurrency as deadlock detection can cause a bottleneck when a number of threads have to wait for the same lock."
6,Commandline: --innodb-deadlock-detect
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 1
6,innodb_deadlock_report
6,Description: How to report deadlocks (if innodb_deadlock_detect=ON).
6,off: Do not report any details of deadlocks.
6,basic: Report transactions and waiting locks.
6,"full: Default. Report transactions, waiting locks and blocking locks."
6,Commandline: --innodb-deadlock-report=val
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value: full
6,"Valid Values: off, basic, full"
6,Introduced: MariaDB 10.6.0
6,innodb_default_page_encryption_key
6,Description: Encryption key used for page encryption.
6,See Data-at-Rest Encryption and InnoDB Encryption Keys for more information.
6,Commandline: --innodb-default-page-encryption-key=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range: 1 to 255
6,Introduced: MariaDB 10.1.3
6,Removed: MariaDB 10.1.4
6,innodb_default_encryption_key_id
6,Description: ID of encryption key used by default to encrypt InnoDB tablespaces.
6,See Data-at-Rest Encryption and InnoDB Encryption Keys for more information.
6,Commandline: --innodb-default-encryption-key-id=#
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range: 1 to 4294967295
6,innodb_default_row_format
6,Description: Specifies the default row format to be used for InnoDB tables. The compressed row format cannot be set as the default.
6,See InnoDB Row Formats Overview: Default Row Format for more information.
6,Commandline: --innodb-default-row-format=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value: dynamic
6,"Valid Values: redundant, compact or dynamic"
6,innodb_defragment
6,"Description: When set to 1 (the default is 0), InnoDB defragmentation is enabled. When set to FALSE, all existing defragmentation will be paused and new defragmentation commands will fail. Paused defragmentation commands will resume when this variable is set to true again. See Defragmenting InnoDB Tablespaces."
6,Commandline: --innodb-defragment={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_defragment_fill_factor
6,Description:. Indicates how full defragmentation should fill a page. Together with innodb_defragment_fill_factor_n_recs ensures defragmentation won’t pack the page too full and cause page split on the next insert on every page. The variable indicating more defragmentation gain is the one effective. See Defragmenting InnoDB Tablespaces.
6,Commandline: --innodb-defragment-fill-factor=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: double
6,Default Value: 0.9
6,Range: 0.7 to 1
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_defragment_fill_factor_n_recs
6,"Description: Number of records of space that defragmentation should leave on the page. This variable, together with innodb_defragment_fill_factor, is introduced so defragmentation won't pack the page too full and cause page split on the next insert on every page. The variable indicating more defragmentation gain is the one effective. See Defragmenting InnoDB Tablespaces."
6,Commandline: --innodb-defragment-fill-factor-n-recs=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 20
6,Range: 1 to 100
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_defragment_frequency
6,"Description: Maximum times per second for defragmenting a single index. This controls the number of times the defragmentation thread can request X_LOCK on an index. The defragmentation thread will check whether 1/defragment_frequency (s) has passed since it last worked on this index, and put the index back in the queue if not enough time has passed. The actual frequency can only be lower than this given number. See Defragmenting InnoDB Tablespaces."
6,Commandline: --innodb-defragment-frequency=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: integer
6,Default Value: 40
6,Range: 1 to 1000
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_defragment_n_pages
6,Description: Number of pages considered at once when merging multiple pages to defragment. See Defragmenting InnoDB Tablespaces.
6,Commandline: --innodb-defragment-n-pages=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 7
6,Range: 2 to 32
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_defragment_stats_accuracy
6,"Description: Number of defragment stats changes there are before the stats are written to persistent storage. Defaults to zero, meaning disable defragment stats tracking. See Defragmenting InnoDB Tablespaces."
6,Commandline: --innodb-defragment-stats-accuracy=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 4294967295
6,Deprecated: MariaDB 11.0.1
6,Removed: MariaDB 11.1.0
6,innodb_dict_size_limit
6,"Description: Size in bytes of a soft limit the memory used by tables in the data dictionary. Once this limit is reached, XtraDB will attempt to remove unused entries. If set to 0, the default and standard InnoDB behavior, there is no limit to memory usage. Removed in MariaDB 10.0/XtraDB 5.6 and replaced by MySQL 5.6's new table_definition_cache implementation."
6,Commandline: innodb-dict-size-limit=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Default Value - 32 bit: 2147483648
6,Default Value - 64 bit: 9223372036854775807
6,Removed: MariaDB 10.0 - replaced by MySQL 5.6's table_definition_cache implementation.
6,innodb_disable_sort_file_cache
6,"Description: If set to 1 (0 is default), the operating system file system cache for merge-sort temporary files is disabled."
6,Commandline: --innodb-disable-sort-file-cache={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_disallow_writes
6,Description: Tell InnoDB to stop any writes to disk.
6,Commandline: None
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,"Removed: MariaDB 10.3.35, MariaDB 10.4.25, MariaDB 10.5.16, MariaDB 10.6.8, MariaDB 10.7.4"
6,innodb_doublewrite
6,"Description: If set to 1, the default, to improve fault tolerance InnoDB first stores data to a doublewrite buffer before writing it to data file. Disabling will provide a marginal peformance improvement."
6,"Commandline: --innodb-doublewrite, --skip-innodb-doublewrite"
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,innodb_doublewrite_file
6,"Description: The absolute or relative path and filename to a dedicated tablespace for the doublewrite buffer. In heavy workloads, the doublewrite buffer can impact heavily on the server, and moving it to a different drive will reduce contention on random reads. Since the doublewrite buffer is mostly sequential writes, a traditional HDD is a better choice than SSD. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: innodb-doublewrite-file=filename
6,Scope: Global
6,Dynamic: No
6,Data Type: filename
6,Default Value: NULL
6,Removed: MariaDB 10.0
6,innodb_empty_free_list_algorithm
6,"Description: XtraDB 5.6.13-61 introduced an algorithm to assist with reducing mutex contention when the buffer pool free list is empty, controlled by this variable. If set to backoff, the default until MariaDB 10.1.24, the new algorithm will be used. If set to legacy, the original InnoDB algorithm will be used. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades. See #1651657 for the reasons this was changed back to legacy in XtraDB 5.6.36-82.0. When upgrading from 10.0 to 10.1 (>= 10.1.24), for large buffer pools the default will remain backoff, while for small ones it will be changed to legacy."
6,Commandline: innodb-empty-free-list-algorithm=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value:
6,deprecated
6,Valid Values:
6,"deprecated, backoff, legacy"
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_enable_unsafe_group_commit
6,"Description: Unneeded after XtraDB 1.0.5. If set to 0, the default, InnoDB will keep transactions between the transaction log and binary logs in the same order. Safer, but slower. If set to 1, transactions can be group-committed, but there is no guarantee of the order being kept, and a small risk of the two logs getting out of sync. In write-intensive environments, can lead to a significant improvement in performance."
6,Commandline: --innodb-enable-unsafe-group-commit
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 1
6,Removed: Not needed after XtraDB 1.0.5
6,innodb_encrypt_log
6,"Description: Enables encryption of the InnoDB redo log. This also enables encryption of some temporary files created internally by InnoDB, such as those used for merge sorts and row logs."
6,See Data-at-Rest Encryption and InnoDB / XtraDB Enabling Encryption: Enabling Encryption for Redo Log for more information.
6,Commandline: --innodb-encrypt-log
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,innodb_encrypt_tables
6,Description: Enables automatic encryption of all InnoDB tablespaces.
6,OFF - Disables table encryption for all new and existing tables that have the ENCRYPTED table option set to DEFAULT.
6,"ON - Enables table encryption for all new and existing tables that have the ENCRYPTED table option set to DEFAULT, but allows unencrypted tables to be created."
6,"FORCE - Enables table encryption for all new and existing tables that have the ENCRYPTED table option set to DEFAULT, and doesn't allow unencrypted tables to be created (CREATE TABLE ... ENCRYPTED=NO will fail)."
6,See Data-at-Rest Encryption and InnoDB / XtraDB Enabling Encryption: Enabling Encryption for Automatically Encrypted Tablespaces for more information.
6,Commandline: --innodb-encrypt-tables={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,"Valid Values: ON, OFF, FORCE"
6,innodb_encrypt_temporary_tables
6,Description: Enables automatic encryption of the InnoDB temporary tablespace.
6,See Data-at-Rest Encryption and InnoDB Enabling Encryption: Enabling Encryption for Temporary Tablespaces for more information.
6,Commandline: --innodb-encrypt-temporary-tables={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,"Valid Values: ON, OFF"
6,"Introduced: MariaDB 10.2.26, MariaDB 10.3.17, MariaDB 10.4.7"
6,innodb_encryption_rotate_key_age
6,"Description: Re-encrypt in background any page having a key older than this number of key versions. When setting up encryption, this variable must be set to a non-zero value."
6,"Otherwise, when you enable encryption through innodb_encrypt_tables MariaDB won't be able to automatically encrypt any unencrypted tables."
6,See Data-at-Rest Encryption and InnoDB Encryption Keys: Key Rotation for more information.
6,Commandline: --innodb-encryption-rotate-key-age=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range: 0 to 4294967295
6,innodb_encryption_rotation_iops
6,Description: Use this many iops for background key rotation operations performed by the background encryption threads.
6,See Data-at-Rest Encryption and InnoDB Encryption Keys: Key Rotation for more information.
6,Commandline: --innodb-encryption-rotation_iops=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 100
6,Range: 0 to 4294967295
6,innodb_encryption_threads
6,"Description: Number of background encryption threads threads performing background key rotation and scrubbing. When setting up encryption, this variable must be set to a non-zero value. Otherwise, when you enable encryption through innodb_encrypt_tables MariaDB won't be able to automatically encrypt any unencrypted tables. Recommended never be set higher than 255."
6,See Data-at-Rest Encryption and InnoDB Background Encryption Threads for more information.
6,Commandline: --innodb-encryption-threads=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range:
6,0 to 4294967295
6,"(<= MariaDB 10.1.45, MariaDB 10.2.32, MariaDB 10.3.23, MariaDB 10.4.13, MariaDB 10.5.3)"
6,0 to 255
6,(>=
6,"MariaDB 10.1.46, MariaDB 10.2.33, MariaDB 10.3.24, MariaDB 10.4.14, MariaDB 10.5.4)"
6,innodb_extra_rsegments
6,Description: Removed in XtraDB 5.5 and replaced by innodb_rollback_segments. Usually there is one rollback segment protected by
6,"single mutex, a source of contention in high write environments. This option specifies a number of extra user rollback segments. Changing the default will make the data readable by XtraDB only, and is incompatible with InnoDB. After modifying, the server must be slow-shutdown. If there is existing data, it must be dumped before changing, and re-imported after the change has taken effect."
6,Commandline: --innodb-extra-rsegments=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 126
6,Removed: XtraDB 5.5 - replaced by innodb_rollback_segments
6,innodb_extra_undoslots
6,"Description: Usually, InnoDB has 1024 undo slots in its rollback segment, so 1024 transactions can run in parallel. New transactions will fail if all slots are used. Setting this variable to 1 expands the available undo slots to 4072. Not recommended unless you get the Warning: cannot find a free slot for an undo log error in the error log, as it makes data files unusable for ibbackup, or MariaDB servers not run with this option. See also undo log."
6,Commandline: --innodb-extra-undoslots={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: XtraDB 5.5
6,innodb_fake_changes
6,"Description: From MariaDB 5.5 until MariaDB 10.1, XtraDB-only option that enables the fake changes feature. In replication, setting up or restarting a replica can cause a replication reads to perform more slowly, as MariaDB is single-threaded and needs to read the data before it can execute the queries. This can be speeded up by prefetching threads to warm the server, replaying the statements and then rolling back at commit. This however has an overhead from locking rows only then to undo changes at rollback. Fake changes attempts to reduce this overhead by reading the rows for INSERT, UPDATE and DELETE statements but not updating them. The rollback is then very fast with little or nothing to do. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades. Not present in MariaDB 10.3 and beyond."
6,Commandline: --innodb-fake-changes={0|1}
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_fast_checksum
6,"Description: Implements a more CPU efficient XtraDB checksum algorithm, useful for write-heavy loads with high I/O. If set to 1 on a server with tables that have been created with it set to 0, reads will be slower, so tables should be recreated (dumped and reloaded). XtraDB will fail to start if set to 0 and there are tables created while set to 1. Replaced with innodb_checksum_algorithm in MariaDB 10.0/XtraDB 5.6."
6,Commandline: --innodb-fast-checksum={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.0/XtraDB 5.6 - replaced with innodb_checksum_algorithm
6,innodb_fast_shutdown
6,Description: The shutdown mode.
6,0 -
6,"InnoDB performs a slow shutdown, including full purge (before MariaDB 10.3.6, not always, due to MDEV-13603) and change buffer merge. Can be very slow, even taking hours in extreme cases."
6,"1 - the default, InnoDB performs a fast shutdown, not performing a full purge or an insert buffer merge."
6,"2, the InnoDB redo log is flushed and a cold shutdown takes place, similar to a crash. The resulting startup then performs crash recovery. Extremely fast, in cases of emergency, but risks corruption. Not suitable for upgrades between major versions!"
6,"3 (from MariaDB 10.3.6) - active transactions will not be rolled back, but all changed pages will be written to data files. The active transactions will be rolled back by a background thread on a subsequent startup."
6,The fastest option that will not involve InnoDB redo log apply on subsequent startup. See MDEV-15832.
6,Commandline: --innodb-fast-shutdown[=#]
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range:
6,"0 to 3 (>= MariaDB 10.3.6), 0 to 2 (<= MariaDB 10.3.5)"
6,innodb_fatal_semaphore_wait_threshold
6,"Description: In MariaDB, the fatal semaphore timeout is configurable. This variable sets the maximum number of seconds for semaphores to time out in InnoDB."
6,Commandline: --innodb-fatal-semaphore-wait-threshold=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 600
6,Range: 1 to 4294967295
6,innodb_file_format
6,"Description: File format for new InnoDB tables. Can either be Antelope, the default and the original format, or Barracuda, which supports compression. Note that this value is also used when a table is re-created with an ALTER TABLE which requires a table copy. See XtraDB/InnoDB File Format for more on the file formats. Removed in 10.3.1 and restored as a deprecated and unused variable in 10.4.3 for compatibility purposes."
6,Commandline: --innodb-file-format=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value:
6,Barracuda
6,"Valid Values: Antelope, Barracuda"
6,Deprecated: MariaDB 10.2
6,Removed: MariaDB 10.3.1
6,Re-introduced: MariaDB 10.4.3 (for compatibility purposes)
6,Removed: MariaDB 10.6.0
6,innodb_file_format_check
6,"Description: If set to 1, the default, InnoDB checks the shared tablespace file format tag. If this is higher than the current version supported by XtraDB/InnoDB (for example Barracuda when only Antelope is supported), XtraDB/InnoDB will will not start. If it the value is not higher, XtraDB/InnoDB starts correctly and the innodb_file_format_max value is set to this value. If innodb_file_format_check is set to 0, no checking is performed. See XtraDB/InnoDB File Format for more on the file formats."
6,Commandline: --innodb-file-format-check={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.2
6,Removed: MariaDB 10.3.1
6,innodb_file_format_max
6,"Description: The highest XtraDB/InnoDB file format. This is set to the value of the file format tag in the shared tablespace on startup (see innodb_file_format_check). If the server later creates a higher table format, innodb_file_format_max is set to that value. See XtraDB/InnoDB File Format for more on the file formats."
6,Commandline: --innodb-file-format-max=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value: Antelope
6,"Valid Values: Antelope, Barracuda"
6,Deprecated: MariaDB 10.2
6,Removed: MariaDB 10.3.1
6,innodb_file_per_table
6,"Description: If set to ON, then new InnoDB tables are created with their own InnoDB file-per-table tablespaces. If set to OFF, then new tables are created in the InnoDB system tablespace instead. Page compression is only available with file-per-table tablespaces. Note that this value is also used when a table is re-created with an ALTER TABLE which requires a table copy. Deprecated in MariaDB 11.0 as there's no benefit to setting to OFF, the original InnoDB default."
6,Commandline: --innodb-file-per-table
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 11.0.1
6,innodb_fill_factor
6,"Description: Percentage of B-tree page filled during bulk insert (sorted index build). Used as a hint rather than an absolute value. Setting to 70, for example, reserves 30% of the space on each B-tree page for the index to grow in future."
6,Commandline: --innodb-fill-factor=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 100
6,Range: 10 to 100
6,innodb_flush_log_at_timeout
6,"Description: Interval in seconds to write and flush the InnoDB redo log. Before MariaDB 10, this was fixed at one second, which is still the default, but this can now be changed. It's usually increased to reduce flushing and avoid impacting performance of binary log group commit."
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range: 0 to 2700
6,innodb_flush_log_at_trx_commit
6,"Description: Set to 1, along with sync_binlog=1 for the greatest level of fault tolerance. The value of innodb_use_global_flush_log_at_trx_commit determines whether this variable can be reset with a SET statement or not."
6,"1 The default, the log buffer is written to the InnoDB redo log file and a flush to disk performed after each transaction. This is required for full ACID compliance."
6,"0 Nothing is done on commit; rather the log buffer is written and flushed to the InnoDB redo log once a second. This gives better performance, but a server crash can erase the last second of transactions."
6,"2 The log buffer is written to the InnoDB redo log after each commit, but flushing takes place every innodb_flush_log_at_timeout seconds (by default once a second). Performance is slightly better, but a OS or power outage can cause the last second's transactions to be lost."
6,"3 Emulates MariaDB 5.5 group commit (3 syncs per group commit). See Binlog group commit and innodb_flush_log_at_trx_commit. This option has not been working correctly since 10.2 and may be removed in future, see https://github.com/MariaDB/server/pull/1873"
6,Commandline: --innodb-flush-log-at-trx-commit[=#]
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: 1
6,"Valid Values: 0, 1, 2 or 3"
6,innodb_flush_method
6,"Description: InnoDB flushing method. Windows always uses async_unbuffered and this variable then has no effect. On Unix, before MariaDB 10.6.0, by default fsync() is used to flush data and logs. Adjusting this variable can give performance improvements, but behavior differs widely on different filesystems, and changing from the default has caused problems in some situations, so test and benchmark carefully before adjusting. In MariaDB, Windows recognises and correctly handles the Unix methods, but if none are specified it uses own default - unbuffered write (analog of O_DIRECT) + syncs (e.g FileFlushBuffers()) for all files."
6,"O_DSYNC - O_DSYNC is used to open and flush logs, and fsync() to flush the data files."
6,"O_DIRECT - O_DIRECT or directio(), is used to open data files, and fsync() to flush data and logs. Default on Unix from MariaDB 10.6.0."
6,fsync
6,"- Default on Unix until MariaDB 10.5. Can be specified directly, but if the variable is unset on Unix, fsync() will be used by default."
6,"O_DIRECT_NO_FSYNC - introduced in MariaDB 10.0. Uses O_DIRECT during flushing I/O, but skips fsync() afterwards. Not suitable for XFS filesystems. Generally not recommended over O_DIRECT, as does not get the benefit of innodb_use_native_aio=ON."
6,"ALL_O_DIRECT - introduced in MariaDB 5.5 and available with XtraDB only. Uses O_DIRECT for opening both data and logs and fsync() to flush data but not logs. Use with large InnoDB files only, otherwise may cause a performance degradation. Set innodb_log_block_size to 4096 on ext4 filesystems. This is the default log block size on ext4 and will avoid unaligned AIO/DIO warnings."
6,unbuffered - Windows-only default
6,"async_unbuffered - Windows-only, alias for unbuffered"
6,"normal - Windows-only, alias for fsync"
6,littlesync - for internal testing only
6,nosync - for internal testing only
6,"Deprecated in MariaDB 11.0 and replaced by four boolean dynamic variables that can be changed while the server is running: innodb_log_file_buffering (disable O_DIRECT, added by MDEV-28766 in 10.8.4, 10.9.2), innodb_data_file_buffering (disable O_DIRECT on data files), innodb_log_file_write_through (enable O_DSYNC on the log), innodb_data_file_write_through (enable O_DSYNC on persistent data files)From MariaDB 11.0, if set to one of the following values, then the values of the four boolean flags will be set as follows:"
6,"O_DSYNC: innodb_log_file_write_through=ON, innodb_data_file_write_through=ON,"
6,"innodb_data_file_buffering=OFF, and (if supported) innodb_log_file_buffering=OFF."
6,"fsync, littlesync, nosync, or (Microsoft Windows specific) normal: innodb_log_file_write_through=OFF, innodb_data_file_write_through=OFF, and innodb_data_file_buffering=ON."
6,Commandline: --innodb-flush-method=name
6,Scope: Global
6,Dynamic: No
6,"Data Type: enumeration (>= MariaDB 10.3.7), string (<= MariaDB 10.3.6)"
6,Default Value:
6,"O_DIRECT (Unix, >= MariaDB 10.6.0)"
6,"fsync (Unix, >= MariaDB 10.3.7, <= MariaDB 10.5)"
6,Not set (<= MariaDB 10.3.6)
6,Valid Values:
6,"Unix: fsync, O_DSYNC, littlesync, nosync. O_DIRECT, O_DIRECT_NO_FSYNC"
6,"Windows: unbuffered, async_unbuffered, normal"
6,Deprecated: MariaDB 11.0
6,innodb_flush_neighbor_pages
6,"Description: Determines whether, when dirty pages are flushed to the data file, neighboring pages in the data file are flushed at the same time. If set to none, the feature is disabled. If set to area, the default, the standard InnoDB behavior is used. For each page to be flushed, dirty neighboring pages are flushed too. If there's little head seek delay, such as SSD or large enough write buffer, one of the other two options may be more efficient. If set to cont, for each page to be flushed, neighboring contiguous blocks are flushed at the same time. Being contiguous, a sequential I/O is used, unlike the random I/O used in area. Replaced by innodb_flush_neighbors in MariaDB 10.0/XtraDB 5.6."
6,Commandline: innodb-flush-neighbor-pages=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: area
6,"Valid Values: none or 0, area or 1, cont or 2"
6,Removed: MariaDB 10.0/XtraDB 5.6 - replaced by innodb_flush_neighbors
6,innodb_flush_neighbors
6,Description:
6,"Determines whether flushing a page from the buffer pool will flush other dirty pages in the same group of pages (extent). In high write environments, if flushing is not aggressive enough, it can fall behind"
6,"resulting in higher memory usage, or if flushing is too aggressive, cause excess I/O activity. SSD devices, with low seek times, would be less likely to require dirty neighbor flushing to be set. Since MariaDB 10.4.4 an attempt is made under Windows and Linux to determine SSD status which was exposed in information_schema.innodb_tablespaces_scrubbing_table. This variable is ignored for table spaces that are detected as stored on SSD (and the 0 behavior applies)."
6,"1: The default, flushes contiguous dirty pages in the same extent from the buffer pool."
6,0: No other dirty pages are flushed.
6,2: Flushes dirty pages in the same extent from the buffer pool.
6,Commandline: --innodb-flush-neighbors=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: 1
6,"Valid Values: 0, 1, 2"
6,innodb_flush_sync
6,"Description: If set to ON, the default,"
6,the innodb_io_capacity setting is ignored for I/O bursts occuring at checkpoints.
6,Commandline: --innodb-flush-sync={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_flushing_avg_loops
6,"Description: Determines how quickly adaptive flushing will respond to changing workloads. The value is the number of iterations that a previously calculated flushing state snapshot is kept. Increasing the value smooths and slows the rate that the flushing operations change, while decreasing it causes flushing activity to spike quickly in response to workload changes."
6,Commandline: --innodb-flushing-avg-loops=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 30
6,Range: 1 to 1000
6,innodb_force_load_corrupted
6,"Description: Set to 0 by default, if set to 1, InnoDB will be permitted to load tables marked as corrupt. Only use this to recover data you can't recover any other way, or in troubleshooting. Always restore to 0 when the returning to regular use. Given that MDEV-11412 in MariaDB 10.5.4 aims to allow any metadata for a missing or corrupted table to be dropped, and given that MDEV-17567 and MDEV-25506 and related tasks made DDL operations crash-safe, the parameter no longer serves any purpose and was removed in MariaDB 10.6.6."
6,Commandline: --innodb-force-load-corrupted
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.6.6
6,innodb_force_primary_key
6,"Description: If set to 1 (0 is default) CREATE TABLEs without a primary or unique key where all keyparts are NOT NULL will not be accepted, and will return an error."
6,Commandline: --innodb-force-primary-key
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_force_recovery
6,"Description: InnoDB crash recovery mode. 0 is the default. The other modes are for recovery purposes only, and no data can be changed while another mode is active. Some queries relying on indexes are also blocked. See InnoDB Recovery Modes for more on mode specifics."
6,Commandline: --innodb-force-recovery=#
6,Scope: Global
6,Dynamic: No
6,Data Type: enumeration
6,Default Value: 0
6,Range: 0 to 6
6,innodb_foreground_preflush
6,"Description: Before XtraDB 5.6.13-61.0, if the checkpoint age is in the sync preflush zone while a thread is writing to the XtraDB redo log, it will try to advance the checkpoint by issuing a flush list flush batch if this is not already being done. XtraDB has enhanced page cleaner tuning, and"
6,"may already be performing furious flushing, resulting in the flush simply adding unneeded mutex pressure. Instead, the thread now waits for the flushes to finish, and then has two options, controlled by this variable. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,"exponential_backoff - thread sleeps while it waits for the flush list flush to occur. The sleep time randomly progressively increases, periodically reset to avoid runaway sleeps."
6,"sync_preflush - thread issues a flush list batch, and waits for it to complete. This is the same as is used when the page cleaner thread is not running."
6,Commandline: innodb-foreground-preflush=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value:
6,deprecated
6,Valid Values:
6,"deprecated, exponential_backoff, sync_preflush"
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_ft_aux_table
6,"Description: Diagnostic variable intended only to be set at runtime. It specifies the qualified name (for example test/ft_innodb) of an InnoDB table that has a FULLTEXT index, and after being set the INFORMATION_SCHEMA tables INNODB_FT_INDEX_TABLE, INNODB_FT_INDEX_CACHE, INNODB_FT_CONFIG, INNODB_FT_DELETED, and INNODB_FT_BEING_DELETED will contain search index information for the specified table."
6,Commandline: --innodb-ft-aux-table=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,innodb_ft_cache_size
6,Description: Cache size available for a parsed document while creating an InnoDB FULLTEXT index.
6,Commandline: --innodb-ft-cache-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 8000000
6,innodb_ft_enable_diag_print
6,"Description: If set to 1, additional full-text search diagnostic output is enabled."
6,Commandline: --innodb-ft-enable-diag-print={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_ft_enable_stopword
6,"Description: If set to 1, the default, a set of stopwords is associated with an InnoDB FULLTEXT index when it is created. The stopword list comes from the table set by the session variable innodb_ft_user_stopword_table, if set, otherwise the global variable innodb_ft_server_stopword_table, if that is set, or the built-in list if neither variable is set."
6,Commandline: --innodb-ft-enable-stopword={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_ft_max_token_size
6,"Description: Maximum length of words stored in an InnoDB FULLTEXT index. A larger limit will increase the size of the index, slowing down queries, but permit longer words to be searched for. In most normal situations, longer words are unlikely search terms."
6,Commandline: --innodb-ft-max-token-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 84
6,Range: 10 to 84
6,innodb_ft_min_token_size
6,"Description: Minimum length of words stored in an InnoDB FULLTEXT index. A smaller limit will increase the size of the index, slowing down queries, but permit shorter words to be searched for. For data stored in a Chinese, Japanese or Korean character set, a value of 1 should be specified to preserve functionality."
6,Commandline: --innodb-ft-min-token-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 3
6,Range: 0 to 16
6,innodb_ft_num_word_optimize
6,Description:
6,"Number of words processed during each OPTIMIZE TABLE on an InnoDB FULLTEXT index. To ensure all changes are incorporated, multiple OPTIMIZE TABLE statements could be run in case of a substantial change to the index."
6,Commandline: --innodb-ft-num-word-optimize=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 2000
6,Range: 1000 to 10000
6,innodb_ft_result_cache_limit
6,"Description: Limit in bytes of the InnoDB FULLTEXT index query result cache per fulltext query. The latter stages of the full-text search are handled in memory, and limiting this prevents excess memory usage. If the limit is exceeded, the query returns an error."
6,Commandline: --innodb-ft-result-cache-limit=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 2000000000
6,Range: 1000000 to 18446744073709551615
6,innodb_ft_server_stopword_table
6,"Description: Table name containing a list of stopwords to ignore when creating an InnoDB FULLTEXT index, in the format db_name/table_name. The specified table must exist before this option is set, and must be an InnoDB table with a single column, a VARCHAR named VALUE. See also innodb_ft_enable_stopword."
6,Commandline: --innodb-ft-server-stopword-table=db_name/table_name
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value: Empty
6,innodb_ft_sort_pll_degree
6,Description:
6,Number of parallel threads used when building an InnoDB FULLTEXT index. See also innodb_sort_buffer_size.
6,Commandline: --innodb-ft-sort-pll-degree=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 2
6,Range: 1 to 32
6,innodb_ft_total_cache_size
6,Description:Total memory allocated for the cache for all InnoDB FULLTEXT index tables. A force sync is triggered if this limit is exceeded.
6,Commandline: --innodb-ft-total-cache-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 640000000
6,Range: 32000000 to 1600000000
6,Introduced: MariaDB 10.0.9
6,innodb_ft_user_stopword_table
6,"Description: Table name containing a list of stopwords to ignore when creating an InnoDB FULLTEXT index, in the format db_name/table_name. The specified table must exist before this option is set, and must be an InnoDB table with a single column, a VARCHAR named VALUE. See also innodb_ft_enable_stopword."
6,Commandline: --innodb-ft-user-stopword-table=db_name/table_name
6,Scope: Session
6,Dynamic: Yes
6,Data Type: string
6,Default Value: Empty
6,innodb_ibuf_accel_rate
6,"Description: Allows the insert buffer activity to be adjusted. The following formula is used: [real activity] = [default activity] * (innodb_io_capacity/100) * (innodb_ibuf_accel_rate/100). As innodb_ibuf_accel_rate is increased from its default value of 100, the lowest setting, insert buffer activity is increased. See also innodb_io_capacity. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: innodb-ibuf-accel-rate=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 100
6,Range: 100 to 999999999
6,Removed: MariaDB 10.0
6,innodb_ibuf_active_contract
6,"Description: Specifies whether the insert buffer can be processed before it's full. If set to 0, the standard InnoDB method is used, and the buffer is not processed until it's full. If set to 1, the default, the insert buffer can be processed before it is full. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: innodb-ibuf-active-contract=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1
6,Range: 0 to 1
6,Removed: MariaDB 10.0
6,innodb_ibuf_max_size
6,"Description: Maximum size in bytes of the insert buffer. Defaults to half the size of the buffer pool so you may want to reduce if you have a very large buffer pool. If set to 0, the insert buffer is disabled, which will cause all secondary index updates to be performed synchronously, usually at a cost to performance. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: innodb-ibuf-max-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1/2 the size of the InnoDB buffer pool
6,Range: 0 to 1/2 the size of the InnoDB buffer pool
6,Removed: MariaDB 10.0
6,innodb_idle_flush_pct
6,"Description: Up to what percentage of dirty pages should be flushed when innodb finds it has spare resources to do so. Has had no effect since merging InnoDB 5.7 from mysql-5.7.9 (MariaDB 10.2.2). Deprecated in MariaDB 10.2.37, MariaDB 10.3.28, MariaDB 10.4.18 and removed in MariaDB 10.5.9."
6,Commandline: --innodb-idle-flush-pct=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 100
6,Range: 0 to 100
6,"Deprecated: MariaDB 10.2.37, MariaDB 10.3.28, MariaDB 10.4.18"
6,Removed: MariaDB 10.5.9
6,innodb_immediate_scrub_data_uncompressed
6,Description: Enable scrubbing of data. See Data Scrubbing.
6,Commandline: --innodb-immediate-scrub-data-uncompressed={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_import_table_from_xtrabackup
6,"Description: If set to 1, permits importing of .ibd files exported with the XtraBackup --export option. Previously named innodb_expand_import. Removed in MariaDB 10.0/XtraDB 5.6 and replaced with MySQL 5.6's transportable tablespaces."
6,Commandline: innodb-import-table-from-xtrabackup=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 1
6,Removed: MariaDB 10.0
6,innodb_instant_alter_column_allowed
6,Description:
6,"If a table is altered using ALGORITHM=INSTANT, it can force the table to use a non-canonical"
6,"format: A hidden metadata record at the start of the clustered index is used to store each column's DEFAULT value. This makes it possible to add new columns that have default values without rebuilding the table. Starting with MariaDB 10.4, a BLOB in the hidden metadata record is used to store column mappings. This makes"
6,"it possible to drop or reorder columns without rebuilding the table. This also makes it possible to add columns to any position or drop columns from any position in the table without rebuilding the table. If a column is dropped without rebuilding the table, old records will contain garbage in that column's former position, and new records"
6,"will be written with NULL values, empty strings, or dummy values."
6,"This is generally not a problem. However, there may be cases where"
6,you want to avoid putting a table into this format.
6,"For example, to ensure that future UPDATE operations"
6,"after an ADD COLUMN will be performed in-place, to reduce write"
6,amplification. (Instantly added columns are essentially always
6,variable-length.) Also avoid bugs similar to
6,"MDEV-19916, or to be able to export tables to"
6,older versions of the server.
6,"This variable has been introduced as a result, with the following values:"
6,"never (0): Do not allow instant add/drop/reorder,"
6,to maintain format compatibility with MariaDB 10.x and MySQL 5.x.
6,"If the table (or partition) is not in the canonical format, then"
6,any ALTER TABLE (even one that does not involve instant column
6,operations) will force a table rebuild.
6,"add_last (1, default in 10.3): Store a hidden metadata record that"
6,allows columns to be appended to the table instantly (MDEV-11369).
6,"In 10.4 or later, if the table (or partition) is not in this format,"
6,then any ALTER TABLE (even one that does not involve column changes)
6,will force a table rebuild.
6,"add_drop_reorder (2, default): From MariaDB 10.4 only. Like 'add_last', but allow the"
6,"metadata record to store a column map, to support instant"
6,add/drop/reorder of columns.
6,Commandline: --innodb-instant-alter-column-allowed=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Valid Values:
6,"<= MariaDB 10.3: never, add_last"
6,">= MariaDB 10.4: never, add_last, add_drop_reorder"
6,Default Value:
6,<= MariaDB 10.3: add_last
6,>= MariaDB 10.4: add_drop_reorder
6,"Introduced: MariaDB 10.3.23, MariaDB 10.4.13, MariaDB 10.5.3"
6,innodb_instrument_semaphores
6,Description: Enable semaphore request instrumentation. This could have some effect on performance but allows better information on long semaphore wait problems.
6,Commandline: --innodb-instrument-semaphores={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.5 (treated as if OFF)
6,Removed: MariaDB 10.3.0
6,innodb_io_capacity
6,"Description: Limit on I/O activity for InnoDB background tasks, including merging data from the insert buffer and flushing pages. Should be set to around the number of I/O operations per second that system can handle, based on the type of drive/s being used. You can also set it higher when the server starts to help with the extra workload at that time, and then reduce for normal use. Ideally, opt for a lower setting, as at higher value data is removed from the buffers too quickly, reducing the effectiveness of caching. See also innodb_flush_sync."
6,See InnoDB Page Flushing: Configuring the InnoDB I/O Capacity for more information.
6,Commandline: --innodb-io-capacity=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 200
6,Range: 100 to 18446744073709551615 (264-1)
6,innodb_io_capacity_max
6,Description: Upper limit to which InnoDB can extend innodb_io_capacity in case of emergency. See InnoDB Page Flushing: Configuring the InnoDB I/O Capacity for more information.
6,Commandline: --innodb-io-capacity-max=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,"Default Value: 2000 or twice innodb_io_capacity, whichever is higher."
6,Range : 100 to 18446744073709551615 (264-1)
6,innodb_kill_idle_transaction
6,"Description: Time in seconds before killing an idle XtraDB transaction. If set to 0 (the default), the feature is disabled. Used to prevent accidental user locks. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 9223372036854775807
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_large_prefix
6,"Description: If set to 1, tables that use specific row formats are permitted to have index key prefixes up to 3072 bytes (for 16k pages, smaller otherwise). If not set, the limit is 767 bytes."
6,This applies to the DYNAMIC and COMPRESSED row formats.
6,Removed in 10.3.1 and restored as a deprecated and unused variable in 10.4.3 for compatibility purposes.
6,Commandline: --innodb-large-prefix
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value:
6,Deprecated: MariaDB 10.2
6,Removed: MariaDB 10.3.1
6,Re-introduced: MariaDB 10.4.3 (for compatibility purposes)
6,Removed: MariaDB 10.6.0
6,innodb_lazy_drop_table
6,"Description: Deprecated and removed in XtraDB 5.6. DROP TABLE processing can take a long time when innodb_file_per_table is set to 1 and there's a large buffer pool. If innodb_lazy_drop_table is set to 1 (0 is default), XtraDB attempts to optimize DROP TABLE processing by deferring the dropping of related pages from the buffer pool until there is time, only initially marking them."
6,Commandline: innodb-lazy-drop-table={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 0
6,Deprecated: XtraDB 5.5.30-30.2
6,Removed: MariaDB 10.0.0
6,innodb_lock_schedule_algorithm
6,"Description: Removed in MariaDB 10.6.0 due to problems with the VATS implementation (MDEV-16664). Specifies the algorithm that InnoDB uses to decide which of the waiting transactions should be granted the lock once it has been released. The possible values are: FCFS (First-Come-First-Served) where locks are granted in the order they appear in the lock queue and VATS (Variance-Aware-Transaction-Scheduling) where locks are granted based on the Eldest-Transaction-First heuristic. Note that VATS should not be used with Galera, and InnoDB will refuse to start if VATS is used with Galera. It is also not recommended to set to VATS even in the general case (MDEV-16664). From MariaDB 10.2.12, the value was changed to FCFS and a warning produced when using Galera."
6,Commandline: --innodb-lock-schedule-algorithm=#
6,Scope: Global
6,"Dynamic: No (>= MariaDB 10.2.12, MariaDB 10.1.30), Yes (<= MariaDB 10.2.11, MariaDB 10.1.29)"
6,Data Type: enum
6,"Valid Values: FCFS, VATS"
6,"Default Value: FCFS (MariaDB 10.3.9, MariaDB 10.2.17), VATS (MariaDB 10.2.3), FCFS (MariaDB 10.1)"
6,"Deprecated: MariaDB 10.5.7, MariaDB 10.4.16, MariaDB 10.3.26, MariaDB 10.2.35"
6,Removed: MariaDB 10.6.0
6,innodb_lock_wait_timeout
6,"Description: Time in seconds that an InnoDB transaction waits for an InnoDB record lock (or table lock) before giving up with the error ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction. When this occurs, the statement (not transaction) is rolled back. The whole transaction can be rolled back if the innodb_rollback_on_timeout option is used. Increase this for data warehousing applications or where other long-running operations are common, or decrease for OLTP and other highly interactive applications. This setting does not apply to deadlocks, which InnoDB detects immediately, rolling back a deadlocked transaction. 0 means no wait. See WAIT and NOWAIT. Setting to 100000000 or more (from MariaDB 10.6.3, 100000000 is the maximum) means the timeout is infinite."
6,Commandline: --innodb-lock-wait-timeout=#
6,"Scope: Global, Session"
6,Dynamic: Yes
6,"Data Type: INT UNSIGNED (>= MariaDB 10.6.3), BIGINT UNSIGNED (<= MariaDB 10.6.2)"
6,Default Value: 50
6,Range:
6,0 to 100000000 (>= MariaDB 10.6.3)
6,0 to 1073741824 (>= MariaDB 10.3 to <= MariaDB 10.6.2)
6,innodb_locking_fake_changes
6,"Description: From MariaDB 5.5 to MariaDB 10.1, XtraDB-only option that if set to OFF, fake transactions (see innodb_fake_changes) don't take row locks. This is an experimental feature to attempt to deal with drawbacks in fake changes blocking real locks. It is not safe for use in all environments. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: --innodb-locking-fake-changes
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_locks_unsafe_for_binlog
6,"Description: Set to 0 by default, in which case XtraDB/InnoDB uses gap locking. If set to 1, gap locking is disabled for searches and index scans. Deprecated in MariaDB 10.0, and removed in MariaDB 10.5, use READ COMMITTED transaction isolation level instead."
6,Commandline: --innodb-locks-unsafe-for-binlog
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.5.0
6,innodb_log_arch_dir
6,Description: The directory for XtraDB redo log archiving. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: --innodb-log-arch-dir=name
6,Scope: Global
6,Dynamic: No
6,Data Type: string
6,Default Value: ./
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_log_arch_expire_sec
6,Description: Time in seconds since the last change after which the archived XtraDB redo log should be deleted. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: --innodb-log-arch-expire-sec=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_log_archive
6,Description: Whether or not XtraDB redo log archiving is enabled. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: --innodb-log-archive={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_log_block_size
6,"Description: Size in bytes of the XtraDB redo log records. Generally 512, the default, or 4096, are the only two useful values. If the server is restarted and this value is changed, all old log files need to be removed. Should be set to 4096 for SSD cards or if innodb_flush_method is set to ALL_O_DIRECT on ext4 filesystems. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: innodb-log-block-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 512
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_log_buffer_size
6,Description: Size in bytes of the buffer for writing InnoDB redo log files to disk. Increasing this means larger transactions can run without needing to perform disk I/O before committing.
6,Commandline: --innodb-log-buffer-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 16777216 (16MB)
6,Range: 262144 to 4294967295 (256KB to 4096MB)
6,innodb_log_checksum_algorithm
6,"Description: Experimental feature (as of MariaDB 10.0.9), this variable specifies how to generate and verify XtraDB redo log checksums. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,"none - No checksum. A constant value is instead written to logs, and no checksum validation is performed."
6,"innodb - The default, and the original InnoDB algorithm. This is inefficient, but compatible with all MySQL, MariaDB and Percona versions that don't support other checksum algorithms."
6,"crc32 - CRC32© is used for log block checksums, which also permits recent CPUs to use hardware acceleration (on SSE4.2 x86 machines and Power8 or later) for the checksums."
6,"strict_* - Whether or not to accept checksums from other algorithms. If strict mode is used, checksums blocks will be considered corrupt if they don't match the specified algorithm. Normally they are considered corrupt only if no other algorithm matches."
6,Commandline: innodb-log-checksum-algorithm=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enum
6,Default Value:
6,deprecated (>= MariaDB 10.2.6)
6,innodb (<= MariaDB 10.1)
6,Valid Values:
6,"deprecated, innodb, none, crc32, strict_none, strict_innodb, strict_crc32 (>= MariaDB 10.2.6)"
6,"innodb, none, crc32, strict_none, strict_innodb, strict_crc32 (<= MariaDB 10.1)"
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_log_checksums
6,"Description: If set to 1, the CRC32C for Innodb or innodb_log_checksum_algorithm for XtraDB algorithm is used for InnoDB redo log pages. If disabled, the checksum field contents are ignored. From MariaDB 10.5.0, the variable is deprecated, and checksums are always calculated, as previously, the InnoDB redo log used the slow innodb algorithm, but with hardware or SIMD assisted CRC-32C computation being available, there is no reason to allow checksums to be disabled on the redo log."
6,Commandline: innodb-log-checksums={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.5.0
6,Removed: MariaDB 10.6.0
6,innodb_log_compressed_pages
6,Description: Whether or not images of recompressed pages are stored in the InnoDB redo log.
6,Deprecated and ignored from MariaDB 10.5.3.
6,Commandline: --innodb-log-compressed-pages={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value:
6,Deprecated: MariaDB 10.5.3
6,Removed: MariaDB 10.6.0
6,innodb_log_file_buffering
6,"Description: Whether the file system cache for ib_logfile0 is enabled. In MariaDB 10.8.3, MariaDB disabled the file system cache on the InnoDB write-ahead log file (ib_logfile0) by default on Linux."
6,"With innodb_flush_trx_log_at_commit=2 in particular, writing to the log via the file system cache typically improves throughput, especially on slow storage or at a small number of concurrent transactions. For other values of innodb_flush_log_at_trx_commit, direct writes were observed to be mostly but not always faster. Whether it pays off to disable the file system cache on the log may depend on the type of storage, the workload, and the operating system kernel version. If the server is started up with innodb_flush_log_at_trx_commit=2, the value will be changed to ON. Will be set to OFF if innodb_flush_method is set to O_DSYNC."
6,"On Linux, when the physical block size cannot be determined to be a power of 2 between 64 and 4096 bytes, the file system cache cannot be disabled, and innodb_log_file_buffering=ON cannot be changed. Linux and Windows only."
6,Commandline: --innodb-log-file-buffering={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,"Introduced: MariaDB 10.8.4, MariaDB 10.9.2"
6,innodb_log_file_size
6,"Description: Size in bytes of each InnoDB redo log file in the log group. The combined size can be no more than 512GB. Larger values mean less disk I/O due to less flushing checkpoint activity, but also slower recovery from a crash. In MariaDB 10.5, crash recovery has been improved and shouldn't run out of memory, so the default has been increased. It can safely be set higher to reduce checkpoint flushing, even larger than innodb_buffer_pool_size.From MariaDB 10.9 the variable is dynamic, and the server no longer needs to be restarted for the resizing to take place. Unless the log is located in a persistent memory file system (PMEM), an attempt to SET GLOBAL innodb_log_file_size to less than innodb_log_buffer_size will be refused. Log resizing can be aborted by killing the connection that is executing the SET GLOBAL statement."
6,Commandline: --innodb-log-file-size=#
6,Scope: Global
6,"Dynamic: Yes (>= MariaDB 10.9), No (<= MariaDB 10.8)"
6,Data Type: numeric
6,"Default Value: 100663296 (96MB) (>= MariaDB 10.5), 50331648 (48MB) (<= MariaDB 10.4)"
6,Range:
6,>= MariaDB 10.8.3: 4194304 to 512GB (4MB to 512GB)
6,<= MariaDB 10.8.2: 1048576 to 512GB (1MB to 512GB)
6,innodb_log_file_write_through
6,"Description: Whether each write to ib_logfile0 is write through (disabling any caching, as in O_SYNC or O_DSYNC). Set to OFF by default, will be set to ON if innodb_flush_method is set to O_DSYNC. On systems that support FUA it may make sense to enable write-through, to avoid extra system calls."
6,Commandline: --innodb-log-file-write-through={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 11.0.0
6,innodb_log_files_in_group
6,Description: Number of physical files in the InnoDB redo log. Deprecated and ignored from MariaDB 10.5.2
6,Commandline: --innodb-log-files-in-group=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,"Default Value: 1 (>= MariaDB 10.5), 2 (<= MariaDB 10.4)"
6,Range: 1 to 100 (>= MariaDB 10.2.4)
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_log_group_home_dir
6,"Description: Path to the InnoDB redo log files. If none is specified, innodb_log_files_in_group files named ib_logfile0 and so on, with a size of innodb_log_file_size are created in the data directory."
6,Commandline: --innodb-log-group-home-dir=path
6,Scope: Global
6,Dynamic: No
6,Data Type: directory name
6,innodb_log_optimize_ddl
6,"Description: Whether InnoDB redo log activity should be reduced when natively creating indexes or rebuilding tables. Reduced logging requires additional page flushing and interferes with Mariabackup. Enabling this may slow down backup and cause delay due to page flushing. Deprecated and ignored from MariaDB 10.5.1. Deprecated (but not ignored) from MariaDB 10.4.16, MariaDB 10.3.26 and MariaDB 10.2.35."
6,Commandline: --innodb-log-optimize-ddl={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value:
6,"OFF (>= MariaDB 10.5.1, MariaDB 10.4.16, MariaDB 10.3.26, MariaDB 10.2.35)"
6,"ON (<= MariaDB 10.5.0, MariaDB 10.4.15, MariaDB 10.3.25, MariaDB 10.2.34)"
6,"Introduced: MariaDB 10.2.17, MariaDB 10.3.9"
6,"Deprecated: MariaDB 10.5.1, MariaDB 10.4.16, MariaDB 10.3.26, MariaDB 10.2.35"
6,Removed: MariaDB 10.6.0
6,innodb_log_write_ahead_size
6,"Description: InnoDB redo log write ahead unit size to avoid read-on-write. Should match the OS cache block IO size. Removed in MariaDB 10.8, and instead on Linux and Windows, the physical block size of the underlying storage is detected and used."
6,Commandline: --innodb-log-write-ahead-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 8192
6,Range: 512 to innodb_page_size
6,Removed: MariaDB 10.8
6,innodb_lru_flush_size
6,Description: Number of pages to flush on LRU eviction.
6,Commandline: --innodb-lru-flush-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 32
6,Range: 1 to 18446744073709551615
6,Introduced: MariaDB 10.5.7
6,innodb_lru_scan_depth
6,"Description: Specifies how far down the buffer pool least-recently used (LRU) list the cleaning thread should look for dirty pages to flush. This process is performed once a second. In an I/O intensive-workload, can be increased if there is spare I/O capacity, or decreased if in a write-intensive workload with little spare I/O capacity."
6,See InnoDB Page Flushing for more information.
6,Commandline: --innodb-lru-scan-depth=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,1536 (>= MariaDB 10.5.7)
6,1024 (<= MariaDB 10.5.6)
6,Range - 32bit: 100 to 232-1
6,Range - 64bit: 100 to 264-1
6,innodb_max_bitmap_file_size
6,"Description: Limit in bytes of the changed page bitmap files. For faster incremental backup with Xtrabackup, XtraDB tracks pages with changes written to them according to the XtraDB redo log and writes the information to special changed page bitmap files. These files are rotated when the server restarts or when this limit is reached. XtraDB only. See also innodb_track_changed_pages and innodb_max_changed_pages."
6,Deprecated and ignored in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-max-bitmap-file-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 4096 (4KB)
6,Range: 4096 (4KB) to 18446744073709551615 (16EB)
6,Deprecated: MariaDB 10.2.6
6,innodb_max_changed_pages
6,Description: Limit to the number of changed page bitmap files (stored in the Information Schema INNODB_CHANGED_PAGES table). Zero is unlimited. See innodb_max_bitmap_file_size and innodb_track_changed_pages. Previously named innodb_changed_pages_limit. XtraDB only.
6,Deprecated and ignored in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-max-changed-pages=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1000000
6,Range: 0 to 18446744073709551615
6,Deprecated: MariaDB 10.2.6
6,innodb_max_dirty_pages_pct
6,Description: Maximum percentage of unwritten (dirty) pages in the buffer pool.
6,See InnoDB Page Flushing for more information.
6,Commandline: --innodb-max-dirty-pages-pct=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,90.000000 (>= MariaDB 10.5.7)
6,75.000000 (<= MariaDB 10.5.6)
6,Range: 0 to 99.999
6,innodb_max_dirty_pages_pct_lwm
6,Description: Low water mark percentage of dirty pages that will enable preflushing to lower the dirty page ratio. The value 0 (default) means 'refer to innodb_max_dirty_pages_pct'.
6,See InnoDB Page Flushing for more information.
6,Commandline: --innodb-max-dirty-pages-pct-lwm=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 99.999
6,innodb_max_purge_lag
6,"Description: When purge operations are lagging on a busy server, setting innodb_max_purge_lag can help. By default set to 0, no lag, the figure is used to calculate a time lag for each INSERT, UPDATE, and DELETE when the system is lagging. InnoDB keeps a list of transactions with delete-marked index records due to UPDATE and DELETE statements. The length of this list is purge_lag, and the calculation, performed every ten seconds, is as follows: ((purge_lag/innodb_max_purge_lag)×10)–5 microseconds."
6,Commandline: --innodb-max-purge-lag=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 4294967295
6,innodb_max_purge_lag_delay
6,"Description: Maximum delay in milliseconds imposed by the innodb_max_purge_lag setting. If set to 0, the default, there is no maximum."
6,Commandline: --innodb-max-purge-lag-delay=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,innodb_max_purge_lag_wait
6,Description: Wait until History list length is below the specified limit.
6,Commandline: --innodb-max-purge-wait=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 4294967295
6,Range: 0 to 4294967295
6,"Introduced: MariaDB 10.5.7, MariaDB 10.4.16, MariaDB 10.3.26, MariaDB 10.2.35"
6,innodb_max_undo_log_size
6,"Description: If an undo tablespace is larger than this, it will be marked for truncation if innodb_undo_log_truncate is set."
6,Commandline: --innodb-max-undo-log-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,10485760
6,Range: 10485760 to 18446744073709551615
6,innodb_merge_sort_block_size
6,Description: Size in bytes of the block used for merge sorting in fast index creation. Replaced in MariaDB 10.0/XtraDB 5.6 by innodb_sort_buffer_size.
6,Commandline: innodb-merge-sort-block-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1048576 (1M)
6,Range: 1048576 (1M) to 1073741824 (1G)
6,Removed: MariaDB 10.0 - replaced by innodb_sort_buffer_size
6,innodb_mirrored_log_groups
6,Description: Unused. Restored as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.2.2 - MariaDB 10.2.5
6,innodb_mtflush_threads
6,"Description: Sets the number of threads to use in Multi-Threaded Flush operations. For more information, see Fusion-io Multi-threaded Flush."
6,"InnoDB's multi-thread flush feature was deprecated in MariaDB 10.2.9 and removed from MariaDB 10.3.2. In later versions of MariaDB, use innodb_page_cleaners system variable instead."
6,See InnoDB Page Flushing: Page Flushing with Multi-threaded Flush Threads for more information.
6,Commandline: --innodb-mtflush-threads=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 8
6,Range: 1 to 64
6,Deprecated: MariaDB 10.2.9
6,Removed: MariaDB 10.3.2
6,innodb_monitor_disable
6,Description: Disables the specified counters in the INFORMATION_SCHEMA.INNODB_METRICS table.
6,Commandline: --innodb-monitor-disable=string
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,innodb_monitor_enable
6,Description: Enables the specified counters in the INFORMATION_SCHEMA.INNODB_METRICS table.
6,Commandline: --innodb-monitor-enable=string
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,innodb_monitor_reset
6,Description: Resets the count value of the specified counters in the INFORMATION_SCHEMA.INNODB_METRICS table to zero.
6,Commandline: --innodb-monitor-reset=string
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,innodb_monitor_reset_all
6,Description: Resets all values for the specified counters in the INFORMATION_SCHEMA.INNODB_METRICS table.
6,Commandline: ---innodb-monitor-reset-all=string
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,innodb_numa_interleave
6,"Description: Whether or not to use the NUMA interleave memory policy to allocate the InnoDB buffer pool. Before MariaDB 10.2.4, required that MariaDB be compiled on a NUMA-enabled Linux system."
6,Commandline: innodb-numa-interleave={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,"Removed: MariaDB 10.2.23, MariaDB 10.3.14, MariaDB 10.4.4"
6,innodb_old_blocks_pct
6,Description: Percentage of the buffer pool to use for the old block sublist.
6,Commandline: --innodb-old-blocks-pct=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 37
6,Range: 5 to 95
6,innodb_old_blocks_time
6,"Description: Time in milliseconds an inserted block must stay in the old sublist after its first access before it can be moved to the new sublist. '0' means ""no delay"". Setting"
6,a non-zero value can help prevent full table scans clogging the buffer pool. See also innodb_old_blocks_pct.
6,Commandline: --innodb-old-blocks-time=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 1000
6,Range: 0 to 232-1
6,innodb_online_alter_log_max_size
6,"Description: The maximum size for temporary log files during online DDL (data and index structure changes). The temporary log file is used for each table being altered, or index being created, to store data changes to the table while the process is underway. The table is extended by innodb_sort_buffer_size up to the limit set by this variable. If this limit is exceeded, the online DDL operation fails and all uncommitted changes are rolled back. A lower value reduces the time a table could lock at the end of the operation to apply all the log's changes, but also increases the chance of the online DDL changes failing."
6,Commandline: --innodb-online-alter-log-max-size=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 134217728
6,Range: 65536 to 264-1
6,innodb_open_files
6,"Description: Maximum .ibd files MariaDB can have open at the same time. Only applies to systems with multiple XtraDB/InnoDB tablespaces, and is separate to the table cache and open_files_limit. The default, if innodb_file_per_table is disabled, is 300 or the value of table_open_cache, whichever is higher. It will also auto-size up to the default value if it is set to a value less than 10."
6,Commandline: --innodb-open-files=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: autosized
6,Range: 10 to 4294967295
6,innodb_optimize_fulltext_only
6,"Description: When set to 1 (0 is default), OPTIMIZE TABLE will only process InnoDB FULLTEXT index data. Only intended for use during fulltext index maintenance."
6,Commandline: --innodb-optimize-fulltext-only={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_page_cleaners
6,"Description: Number of page cleaner threads. The default is 4, but the value will be set to the number of innodb_buffer_pool_instances if this is lower. If set to 1, only a single cleaner thread is used, as was the case until MariaDB 10.2.1. Cleaner threads flush dirty pages from the buffer pool, performing flush list and least-recently used (LRU) flushing. Deprecated and ignored from MariaDB 10.5.1, as the original reasons for for splitting the buffer pool have mostly gone away."
6,See InnoDB Page Flushing: Page Flushing with Multiple InnoDB Page Cleaner Threads for more information.
6,Commandline: --innodb-page-cleaners=#
6,Scope: Global
6,"Dynamic: Yes (>= MariaDB 10.3.3), No (<= MariaDB 10.3.2)"
6,Data Type: numeric
6,Default Value: 4 (or set to innodb_buffer_pool_instances if lower)
6,Range: 1 to 64
6,Deprecated: MariaDB 10.5.1
6,Removed: MariaDB 10.6.0
6,innodb_page_size
6,"Description: Specifies the page size in bytes for all InnoDB tablespaces. The default, 16k, is suitable for most uses."
6,"A smaller InnoDB page size might work more effectively in a situation with many small writes (OLTP), or with SSD storage, which usually has smaller block sizes."
6,A larger InnoDB page size can provide a larger maximum row size.
6,"InnoDB's page size can be as large as 64k for tables using the following row formats: DYNAMIC, COMPACT, and REDUNDANT."
6,InnoDB's page size must still be 16k or less for tables using the COMPRESSED row format.
6,"This system variable's value cannot be changed after the datadir has been initialized. InnoDB's page size is set when a MariaDB instance starts, and it remains constant afterwards."
6,Commandline: --innodb-page-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: enumeration
6,Default Value: 16384
6,"Valid Values: 4k or 4096, 8k or 8192, 16k or 16384, 32k and 64k."
6,innodb_pass_corrupt_table
6,Removed: XtraDB 5.5
6,- renamed innodb_corrupt_table_action.
6,innodb_prefix_index_cluster_optimization
6,"Description: Enable prefix optimization to sometimes avoid cluster index lookups. Deprecated and ignored from MariaDB 10.10, as the optimization is now always enabled."
6,Commandline: --innodb-prefix-index-cluster-optimization={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.10.1
6,innodb_print_all_deadlocks
6,"Description: If set to 1 (0 is default), all XtraDB/InnoDB transaction deadlock information is written to the error log."
6,Commandline: --innodb-print-all-deadlocks={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_purge_batch_size
6,Description: Number of
6,InnoDB undo log pages to purge in one batch from the history list. Together with innodb_purge_threads has a small effect on tuning.
6,Commandline: --innodb-purge-batch-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value:
6,"1000 (>= MariaDB 10.6.16, MariaDB 10.10.7, MariaDB 10.11.6, MariaDB 11.0.4, MariaDB 11.1.3 MariaDB 11.2.2)"
6,"300 (<= MariaDB 10.6.15, MariaDB 10.10.6, MariaDB 10.11.5, MariaDB 11.0.3, MariaDB 11.1.2 MariaDB 11.2.1)"
6,Range: 1 to 5000
6,innodb_purge_rseg_truncate_frequency
6,"Description: Frequency with which undo records are purged. Set by default to every 128 times, reducing this increases the frequency at which rollback segments are freed. See also innodb_undo_log_truncate. The motivation for introducing this in MySQL seems to have been to avoid stalls due to freeing undo log pages or truncating undo log tablespaces. In MariaDB, innodb_undo_log_truncate=ON should be a much lighter operation because it will not involve any log checkpoint, hence this is deprecated and ignored from MariaDB 10.6.16, MariaDB 10.10.7, MariaDB 10.11.6, MariaDB 11.0.4, MariaDB 11.1.3 and MariaDB 11.2.2."
6,(MDEV-32050)
6,Commandline: -- innodb-purge-rseg-truncate-frequency=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 128
6,Range: 1 to 128
6,"Deprecated: MariaDB 10.6.16, MariaDB 10.10.7, MariaDB 10.11.6, MariaDB 11.0.4, MariaDB 11.1.3, MariaDB 11.2.2"
6,innodb_purge_threads
6,"Description: Number of background threads dedicated to InnoDB purge operations. The range is 1 to 32. At least one background thread is always used. Setting to a value greater than 1 creates that many separate purge threads. This can improve efficiency in some cases, such as when performing DML operations on many tables. See also innodb_purge_batch_size."
6,Commandline: --innodb-purge-threads=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 4
6,Range: 1 to 32
6,innodb_random_read_ahead
6,"Description: Originally, random read-ahead was always set as an optimization technique, but was removed in MariaDB 5.5. innodb_random_read_ahead permits it to be re-instated if set to 1 (0) is default."
6,Commandline: --innodb-random-read-ahead={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_read_ahead
6,"Description: If set to linear, the default, XtraDB/InnoDB will automatically fetch remaining pages if there are enough within the same extent that can be accessed sequentially. If set to none, read-ahead is disabled. random has been removed and is now ignored, while both sets to both linear and random. Also see innodb_read_ahead_threshold for more control on read-aheads. Removed in MariaDB 10.0/XtraDB 5.6 and replaced by MySQL 5.6's innodb_random_read_ahead."
6,Commandline: innodb-read-ahead=value
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: linear
6,"Valid Values: none, random, linear, both"
6,Removed: MariaDB 10.0/XtraDB 5.6 - replaced by MySQL 5.6's innodb_random_read_ahead
6,innodb_read_ahead_threshold
6,Description: Minimum number of pages InnoDB must read from an extent of 64 before initiating an asynchronous read for the following extent.
6,Commandline: --innodb-read-ahead-threshold=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 56
6,Range: 0 to 64
6,innodb_read_io_threads
6,Description: Number of I/O threads for InnoDB reads. You may on rare occasions need to reduce this default on Linux systems running multiple MariaDB servers to avoid exceeding system limits.
6,Commandline: --innodb-read-io-threads=#
6,Scope: Global
6,"Dynamic: Yes (>= MariaDB 10.11), No (<= MariaDB 10.10)"
6,Data Type: numeric
6,Default Value: 4
6,Range: 1 to 64
6,innodb_read_only
6,"Description: If set to 1 (0 is default), the server will be read-only. For use in distributed applications, data warehouses or read-only media."
6,Commandline: --innodb-read-only={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,innodb_read_only_compressed
6,"Description: If set (the default before MariaDB 10.6.6), ROW_FORMAT=COMPRESSED tables will be read-only. This was intended to be the first step towards removing write support and deprecating the feature, but this plan has been abandoned."
6,"Commandline: --innodb-read-only-compressed, --skip-innodb-read-only-compressed"
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,"Default Value: OFF (>= MariaDB 10.6.6), ON (<= MariaDB 10.6.5)"
6,Introduced: MariaDB 10.6.0
6,innodb_recovery_stats
6,"Description: If set to 1 (0 is default) and recovery is necessary on startup, the server will write detailed recovery statistics to the error log at the end of the recovery process. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: No
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.0
6,innodb_recovery_update_relay_log
6,"Description: If set to 1 (0 is default), the relay log info file will be overwritten on crash recovery if the information differs from the InnoDB record. Should not be used if multiple storage engine types are being replicated. Previously named innodb_overwrite_relay_log_info. Removed in MariaDB 10.0/XtraDB 5.6 and replaced by MySQL 5.6's relay-log-recovery"
6,Commandline: innodb-recovery-update-relay-log={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.0 - replaced by MySQL 5.6's relay-log-recovery
6,innodb_replication_delay
6,Description: Time in milliseconds for the replica server to delay the replication thread if innodb_thread_concurrency is reached. Deprecated and ignored from MariaDB 10.5.5.
6,Commandline: --innodb-replication-delay=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 4294967295
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_rollback_on_timeout
6,"Description: InnoDB usually rolls back the last statement of a transaction that's been timed out (see innodb_lock_wait_timeout). If innodb_rollback_on_timeout is set to 1 (0 is default), InnoDB will roll back the entire transaction. Before MariaDB 5.5, rolling back the entire transaction was the default behavior."
6,Commandline: --innodb-rollback-on-timeout
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: 0
6,innodb_rollback_segments
6,"Description: Specifies the number of rollback segments that XtraDB/InnoDB will use within a transaction (see undo log). Deprecated and replaced by innodb_undo_logs in MariaDB 10.0. Removed in MariaDB 10.5 as part of an InnoDB cleanup, as it makes sense to always create and use the maximum number of rollback segments. |"
6,Commandline: --innodb-rollback-segments=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 128
6,Range: 1 to 128
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.5.0
6,innodb_safe_truncate
6,Description: Use a backup-safe TRUNCATE TABLE implementation and crash-safe rename operations inside InnoDB. This is not compatible with hot backup tools other than Mariabackup. Users who need to use such tools may set this to OFF.
6,Commandline: --innodb-safe-truncate={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Introduced: MariaDB 10.2.19
6,Removed: MariaDB 10.3.0
6,innodb_scrub_log
6,"Description: Enable InnoDB redo log scrubbing. See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2, as never"
6,"really worked (MDEV-13019 and MDEV-18370). If old log contents should be kept secret, then enabling innodb_encrypt_log or setting a smaller innodb_log_file_size could help."
6,Commandline: --innodb-scrub-log
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_scrub_log_interval
6,Description: Used with Data Scrubbing in 10.1.3 only - replaced in 10.1.4 by innodb_scrub_log_speed. InnoDB redo log scrubbing interval in milliseconds.
6,Commandline: --innodb-scrub-log-interval=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 56
6,Range: 0 to 50000
6,Introduced: MariaDB 10.1.3
6,Removed: MariaDB 10.1.4
6,innodb_scrub_log_speed
6,Description: InnoDB redo log scrubbing speed in bytes/sec. See Data Scrubbing. Deprecated and ignored from MariaDB 10.5.2.
6,Commandline: --innodb-scrub-log-speed=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 256
6,Range: 1 to 50000
6,Deprecated: MariaDB 10.5.2
6,Removed: MariaDB 10.6.0
6,innodb_sched_priority_cleaner
6,Description: Set a thread scheduling priority for cleaner and least-recently used (LRU) manager threads. The range from 0 to 39 corresponds in reverse order to Linux nice values of -20 to 19. So 0 is the lowest priority (Linux nice value 19) and 39 is the highest priority (Linux nice value -20). XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-sched-priority-cleaner=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 19
6,Range: 0 to 39
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_show_locks_held
6,Description: Specifies the number of locks held for each InnoDB transaction to be displayed in SHOW ENGINE INNODB STATUS output. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-show-locks-held=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 10
6,Range: 0 to 1000
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_show_verbose_locks
6,"Description: If set to 1, and innodb_status_output_locks is also ON, the traditional InnoDB behavior is followed and locked records will be shown in SHOW ENGINE INNODB STATUS output. If set to 0, the default, only high-level information about the lock is shown. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: innodb-show-verbose-locks=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 1
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_simulate_comp_failures
6,Description: Simulate compression failures. Used for testing robustness against random compression failures. XtraDB only.
6,Commandline: None
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 99
6,innodb_sort_buffer_size
6,"Description: Size of the sort buffers used for sorting data when an InnoDB index is created, as well as the amount by which the temporary log file is extended during online DDL operations to record concurrent writes. The larger the setting, the fewer merge phases are required between buffers while sorting. When a CREATE TABLE or ALTER TABLE creates a new index, three buffers of this size are allocated, as well as pointers for the rows in the buffer."
6,Commandline: --innodb-sort-buffer-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1048576 (1M)
6,Range: 65536 to 67108864
6,innodb_spin_wait_delay
6,"Description: Maximum delay (not strictly corresponding to a time unit) between spin lock polls. Default changed from 6 to 4 in MariaDB 10.3.5, as this was verified to give the best throughput by OLTP update index and read-write benchmarks on Intel Broadwell (2/20/40) and ARM (1/46/46)."
6,Commandline: --innodb-spin-wait-delay=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,"Default Value: 4 (>= MariaDB 10.3.5), 6 (<= MariaDB 10.3.4)"
6,Range: 0 to 4294967295
6,innodb_stats_auto_recalc
6,"Description: If set to 1 (the default), persistent statistics are automatically recalculated when the table changes significantly (more than 10% of the rows). Affects tables created or altered with STATS_PERSISTENT=1 (see CREATE TABLE), or when innodb_stats_persistent is enabled. innodb_stats_persistent_sample_pages determines how much data to sample when recalculating. See InnoDB Persistent Statistics."
6,Commandline: --innodb-stats-auto-recalc={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_stats_auto_update
6,"Description: If set to 0 (1 is default), index statistics will not be automatically calculated except when an ANALYZE TABLE is run, or the table is first opened. Replaced by innodb_stats_auto_recalc in MariaDB 10.0/XtraDB 5.6."
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 1
6,Removed: MariaDB 10.0 - replaced by innodb_stats_auto_recalc.
6,innodb_stats_include_delete_marked
6,Description: Include delete marked records when calculating persistent statistics.
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_stats_method
6,Description: Determines how NULLs are treated for InnoDB index statistics purposes.
6,"nulls_equal: The default, all NULL index values are treated as a single group. This is usually fine, but if you have large numbers of NULLs the average group size is slanted higher, and the optimizer may miss using the index for ref accesses when it would be useful."
6,"nulls_unequal: The opposite approach to nulls_equal is taken, with each NULL forming its own group of one. Conversely, the average group size is slanted lower, and the optimizer may use the index for ref accesses when not suitable."
6,nulls_ignored: Ignore NULLs altogether from index group calculations.
6,"See also Index Statistics, aria_stats_method and myisam_stats_method."
6,Commandline: --innodb-stats-method=name
6,Scope: Global
6,Dynamic: Yes
6,Data Type: enumeration
6,Default Value: nulls_equal
6,"Valid Values: nulls_equal, nulls_unequal, nulls_ignored"
6,innodb_stats_modified_counter
6,"Description: The number of rows modified before we calculate new statistics. If set to 0, the default, current limits are used."
6,Commandline: --innodb-stats-modified-counter=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 18446744073709551615
6,innodb_stats_on_metadata
6,"Description: If set to 1, the default, XtraDB/InnoDB updates statistics when accessing the INFORMATION_SCHEMA.TABLES or INFORMATION_SCHEMA.STATISTICS tables, and when running metadata statements such as SHOW INDEX or SHOW TABLE STATUS. If set to 0, statistics are not updated at those times, which can reduce the access time for large schemas, as well as make execution plans more stable."
6,Commandline: --innodb-stats-on-metadata
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_stats_persistent
6,"Description: ANALYZE TABLE produces index statistics, and this setting determines whether they will be stored on disk, or be required to be recalculated more frequently, such as when the server restarts. This information is stored for each table, and can be set with the STATS_PERSISTENT clause when creating or altering tables (see CREATE TABLE). See InnoDB Persistent Statistics."
6,Commandline: --innodb-stats-persistent={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_stats_persistent_sample_pages
6,"Description: Number of index pages sampled when estimating cardinality and statistics for indexed columns. Increasing this value will increases index statistics accuracy, but use more I/O resources when running ANALYZE TABLE. See InnoDB Persistent Statistics."
6,Commandline: --innodb-stats-persistent-sample-pages=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 20
6,innodb_stats_sample_pages
6,"Description: Gives control over the index distribution statistics by determining the number of index pages to sample. Higher values produce more disk I/O, but, especially for large tables, produce more accurate statistics and therefore make more effective use of the query optimizer. Lower values than the default are not recommended, as the statistics can be quite inaccurate."
6,"If innodb_stats_traditional is enabled, then the exact number of pages configured by this system variable will be sampled for statistics."
6,"If innodb_stats_traditional is disabled, then the number of pages to sample for statistics is calculated using a logarithmic algorithm, so the exact number can change depending on the size of the table. This means that more samples may be used for larger tables."
6,"If persistent statistics are enabled, then the innodb_stats_persistent_sample_pages system variable applies instead. persistent statistics are enabled with the innodb_stats_persistent system variable."
6,This system variable has been deprecated. The innodb_stats_transient_sample_pages system variable should be used instead.
6,Commandline: --innodb-stats-sample-pages=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 8
6,Range: 1 to 264-1
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.5.0
6,innodb_stats_traditional
6,"Description: This system variable affects how the number of pages to sample for transient statistics is determined, in particular how innodb_stats_transient_sample_pages is used."
6,"If innodb_stats_traditional is enabled, then the exact number of pages configured by the system variable will be sampled for statistics."
6,"If innodb_stats_traditional is disabled, then the number of pages to sample for statistics is calculated using a logarithmic algorithm, so the exact number can change depending on the size of the table. This means that more samples may be used for larger tables."
6,This system variable does not affect the calculation of persistent statistics.
6,Commandline: --innodb-stats-traditional={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_stats_transient_sample_pages
6,"Description: Gives control over the index distribution statistics by determining the number of index pages to sample. Higher values produce more disk I/O, but, especially for large tables, produce more accurate statistics and therefore make more effective use of the query optimizer. Lower values than the default are not recommended, as the statistics can be quite inaccurate."
6,"If innodb_stats_traditional is enabled, then the exact number of pages configured by this system variable will be sampled for statistics."
6,"If innodb_stats_traditional is disabled, then the number of pages to sample for statistics is calculated using a logarithmic algorithm, so the exact number can change depending on the size of the table. This means that more samples may be used for larger tables."
6,"If persistent statistics are enabled, then the innodb_stats_persistent_sample_pages system variable applies instead. persistent statistics are enabled with the innodb_stats_persistent system variable."
6,Commandline: --innodb-stats-transient-sample-pages=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 8
6,Range: 1 to 264-1
6,innodb_stats_update_need_lock
6,"Description: Setting to 0 (1 is default) may help reduce contention of the &dict_operation_lock, but also disables the Data_free option in SHOW TABLE STATUS. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: 1
6,Removed: MariaDB 10.0/XtraDB 5.6
6,innodb_status_output
6,Description: Enable InnoDB monitor output to the error log.
6,Commandline: --innodb-status-output={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_status_output_locks
6,Description: Enable InnoDB lock monitor output to the error log and SHOW ENGINE INNODB STATUS. Also requires innodb_status_output=ON to enable output to the error log.
6,Commandline: --innodb-status-output-locks={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_strict_mode
6,"Description: If set to 1 (the default), InnoDB will return errors instead of warnings in certain cases, similar to strict SQL mode. See InnoDB Strict Mode for details."
6,Commandline: --innodb-strict-mode={0|1}
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_support_xa
6,"Description: If set to 1, the default, XA transactions are supported. XA support ensures data is written to the binary log in the same order to the actual database, which is critical for replication and disaster recovery, but comes at a small performance cost. If your database is set up to only permit one thread to change data (for example, on a replication replica with only the replication thread writing), it is safe to turn this option off. Removed in MariaDB 10.3, XA transactions are always supported."
6,Commandline: --innodb-support-xa
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.2
6,Removed: MariaDB 10.3.0
6,innodb_sync_array_size
6,"Description: By default 1, can be increased to split internal thread co-ordinating, giving higher concurrency when there are many waiting threads."
6,Commandline: --innodb-sync-array-size=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1
6,Range: 1 to 1024
6,Removed: MariaDB 10.6.0
6,innodb_sync_spin_loops
6,Description: The number of times a thread waits for an InnoDB mutex to be freed before the thread is suspended.
6,Commandline: --innodb-sync-spin-loops=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 30
6,Range: 0 to 4294967295
6,innodb_table_locks
6,"Description: If autocommit is set to to 0 (1 is default), setting innodb_table_locks to 1, the default, will cause InnoDB to lock a table internally upon a LOCK TABLE."
6,Commandline: --innodb-table-locks
6,"Scope: Global, Session"
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,innodb_thread_concurrency
6,"Description: Once this number of threads is reached (excluding threads waiting for locks), XtraDB/InnoDB will place new threads in a wait state in a first-in, first-out queue for execution, in order to limit the number of threads running concurrently. A setting of 0, the default, permits as many threads as necessary. A suggested setting is twice the number of CPU's plus the number of disks. Deprecated and ignored from MariaDB 10.5.5."
6,Commandline: --innodb-thread-concurrency=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 0
6,Range: 0 to 1000
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_thread_concurrency_timer_based
6,"Description: If set to 1, thread concurrency will be handled in a lock-free timer-based manner rather than the default mutex-based method. Depends on atomic op builtins being available. This Percona XtraDB variable has not been ported to XtraDB 5.6."
6,Commandline: innodb-thread-concurrency-timer-based={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Removed: MariaDB 10.0/XtraDB 5.6
6,innodb_thread_sleep_delay
6,Description: Time in microseconds that InnoDB threads sleep before joining the queue. Setting to 0 disables sleep. Deprecated and ignored from MariaDB 10.5.5
6,Commandline: --innodb-thread-sleep-delay=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value:
6,0 (>= MariaDB 10.5.5.)
6,10000 (<= MariaDB 10.5.4)
6,Range: 0 to 1000000
6,Deprecated: MariaDB 10.5.5
6,Removed: MariaDB 10.6.0
6,innodb_temp_data_file_path
6,Description:
6,Commandline: --innodb-temp-data-file-path=path
6,Scope: Global
6,Dynamic: No
6,Data Type: string
6,Default Value: ibtmp1:12M:autoextend
6,innodb_tmpdir
6,"Description: Allows an alternate location to be set for temporary non-tablespace files. If not set (the default), files will be created in the usual tmpdir location."
6,Commandline: --innodb-tmpdir=path
6,Scope: Global
6,Dynamic: Yes
6,Data Type: string
6,Default Value: Empty
6,innodb_track_changed_pages
6,"Description: For faster incremental backup with Xtrabackup, XtraDB tracks pages with changes written to them according to the XtraDB redo log and writes the information to special changed page bitmap files. This read-only variable is used for controlling this feature. See also innodb_max_changed_pages and innodb_max_bitmap_file_size. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: innodb-track-changed-pages={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.6
6,innodb_track_redo_log_now
6,Description: Available on debug builds only. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades.
6,Commandline: innodb-track-redo-log-now={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.6
6,innodb_truncate_temporary_tablespace_now
6,Description: Set to ON to shrink the temporary tablespace.
6,Commandline: innodb-truncate-temporary-tablespace-now={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,Introduced: MariaDB 11.3.0
6,innodb_undo_directory
6,"Description: Path to the directory (relative or absolute) that InnoDB uses to create separate tablespaces for the undo logs. . (the default value before 10.2.2) leaves the undo logs in the same directory as the other log files. From MariaDB 10.2.2, the default value is NULL, and if no path is specified, undo tablespaces will be created in the directory defined by datadir. Use together with innodb_undo_logs and innodb_undo_tablespaces. Undo logs are most usefully placed on a separate storage device."
6,Commandline: --innodb-undo-directory=name
6,Scope: Global
6,Dynamic: No
6,Data Type: string
6,Default Value: NULL
6,innodb_undo_log_truncate
6,"Description: When enabled, innodb_undo_tablespaces that are larger than innodb_max_undo_log_size will be marked for truncation. See also innodb_purge_rseg_truncate_frequency. Enabling this setting may cause stalls during heavy write workloads."
6,Commandline: --innodb-undo-log-truncate[={0|1}]
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: OFF
6,innodb_undo_logs
6,"Description: Specifies the number of rollback segments that XtraDB/InnoDB will use within a transaction (or the number of active undo logs). By default set to the maximum, 128, it can be reduced to avoid allocating unneeded rollback segments. See the Innodb_available_undo_logs status variable for the number of undo logs available. See also innodb_undo_directory and innodb_undo_tablespaces. Replaced innodb_rollback_segments in MariaDB 10.0. The Information Schema XTRADB_RSEG Table contains information about the XtraDB rollback segments. Deprecated and ignored in MariaDB 10.5.0, as it always makes sense to use the maximum number of rollback segments."
6,Commandline: --innodb-undo-logs=#
6,Scope: Global
6,Dynamic: Yes
6,Data Type: numeric
6,Default Value: 128
6,Range: 0 to 128
6,Deprecated: MariaDB 10.5.0
6,Removed: MariaDB 10.6.0
6,innodb_undo_tablespaces
6,"Description: Number of tablespaces files used for dividing up the undo logs. Zero (the default before MariaDB 11.0) means that undo logs are all part of the system tablespace, which contains one undo tablespace more than the innodb_undo_tablespaces setting. A value of 1 is reset to 0 as 2 or more are needed for separate tablespaces. When the undo logs can grow large, splitting them over multiple tablespaces will reduce the size of any single tablespace. Until MariaDB 10.11.1, must be set before InnoDB is initialized, or else MariaDB will fail to start, with an error saying that InnoDB did not find the expected number of undo tablespaces. The files are created in the directory specified by innodb_undo_directory, and are named undoN, N being an integer. The default size of an undo tablespace is 10MB.From MariaDB 10.11, multiple undo tablespaces are enabled by default, and the default is changed to 3 so that the space occupied by possible bursts of undo log records can be reclaimed after innodb_undo_log_truncate is set. Before MariaDB 10.6, innodb_undo_logs must have a non-zero setting for innodb_undo_tablespaces to take effect."
6,Commandline: --innodb-undo-tablespaces=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,"Default Value: 3 (>= MariaDB 11.0), 0 (<= MariaDB 10.11)"
6,"Range: 0, or 2 to 95"
6,innodb_use_atomic_writes
6,Description: Implement atomic writes on supported SSD devices. See atomic write support for other variables affected when this is set.
6,Commandline: innodb-use-atomic-writes={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,innodb_use_fallocate
6,"Description: Preallocate files fast, using operating system functionality. On POSIX systems, posix_fallocate system call is used."
6,Automatically set to 1 when innodb_use_atomic_writes is set - see FusionIO DirectFS atomic write support.
6,See InnoDB Page Compression: Saving Storage Space with Sparse Files for more information.
6,Commandline: innodb-use-fallocate={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.5 (treated as if ON)
6,Removed: MariaDB 10.3.0
6,innodb_use_global_flush_log_at_trx_commit
6,"Description: Determines whether a user can set the variable innodb_flush_log_at_trx_commit. If set to 1, a user cannot reset the value with a SET command, while if set to 1, a user can reset the value of innodb_flush_log_at_trx_commit. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: innodb-use-global-flush-log-at-trx_commit={0|1}
6,Scope: Global
6,Dynamic: Yes
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_use_mtflush
6,Description: Whether to enable Multi-Threaded Flush operations.
6,"For more information, see Fusion."
6,"InnoDB's multi-thread flush feature was deprecated in MariaDB 10.2.9 and removed from MariaDB 10.3.2. In later versions of MariaDB, use innodb_page_cleaners system variable instead."
6,See InnoDB Page Flushing: Page Flushing with Multi-threaded Flush Threads for more information.
6,Commandline: --innodb-use-mtflush={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.9
6,Removed: MariaDB 10.3.2
6,innodb_use_native_aio
6,"Description: For Linux systems only, specified whether to use Linux's asynchronous I/O subsystem. Set to ON by default, it may be changed to 0 at startup if InnoDB detects a problem, or from MariaDB 10.6.5/MariaDB 10.7.1, if a 5.11 - 5.15 Linux kernel is detected, to avoid an io-uring bug/incompatibility (MDEV-26674). MariaDB-10.6.6/MariaDB-10.7.2 and later also consider 5.15.3+ as a fixed kernel and default to ON. To really benefit from the setting, the files should be opened in O_DIRECT mode (innodb_flush_method=O_DIRECT, default from MariaDB 10.6), to bypass the file system cache. In this way, the reads and writes can be submitted with DMA, using the InnoDB buffer pool directly, and no processor cycles need to be used for copying data."
6,Commandline: --innodb-use-native-aio={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,innodb_use_purge_thread
6,"Description: Usually with InnoDB, data changed by a transaction is written to an undo space to permit read consistency, and freed when the transaction is complete. Many, or large, transactions, can cause the main tablespace to grow dramatically, reducing performance. This option, introduced in XtraDB 5.1 and removed for 5.5, allows multiple threads to perform the purging, resulting in slower, but much more stable performance."
6,Commandline: --innodb-use-purge-thread=#
6,Scope: Global
6,Dynamic: No
6,Data Type: numeric
6,Default Value: 1
6,Range: 0 to 32
6,Removed: XtraDB 5.5
6,innodb_use_stacktrace
6,"Description: If set to ON (OFF is default), a signal handler for SIGUSR2 is installed when the InnoDB server starts. When a long semaphore wait is detected at sync/sync0array.c, a SIGUSR2 signal is sent to the waiting thread and thread that has acquired the RW-latch. For both threads a full stacktrace is produced as well as if possible. XtraDB only. Added as a deprecated and ignored option in MariaDB 10.2.6 (which uses InnoDB as default instead of XtraDB) to allow for easier upgrades."
6,Commandline: --innodb-use-stacktrace={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: OFF
6,Deprecated: MariaDB 10.2.6
6,Removed: MariaDB 10.3.0
6,innodb_use_sys_malloc
6,"Description: If set the 1, the default, XtraDB/InnoDB will use the operating system's memory allocator. If set to 0 it will use its own. Deprecated in MariaDB 10.0 and removed in MariaDB 10.2 along with InnoDB's internal memory allocator."
6,Commandline: --innodb-use-sys-malloc={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.0
6,Removed: MariaDB 10.2.2
6,innodb_use_sys_stats_table
6,"Description: If set to 1 (0 is default), XtraDB will use the SYS_STATS system table for extra table index statistics. When a table is opened for the first time, statistics will then be loaded from SYS_STATS instead of sampling the index pages. Statistics are designed to be maintained only by running an ANALYZE TABLE. Replaced by MySQL 5.6's Persistent Optimizer Statistics."
6,Commandline: innodb-use-sys-stats-table={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: 0
6,Removed: MariaDB 10.0/XtraDB 5.6
6,innodb_use_trim
6,Description: Use trim to free up space of compressed blocks.
6,See InnoDB Page Compression: Saving Storage Space with Sparse Files for more information.
6,Commandline: --innodb-use-trim={0|1}
6,Scope: Global
6,Dynamic: No
6,Data Type: boolean
6,Default Value: ON
6,Deprecated: MariaDB 10.2.4
6,Removed: MariaDB 10.3.0
6,innodb_version
6,"Description: InnoDB version number. From MariaDB 10.3.7, as the InnoDB implementation in MariaDB has diverged from MySQL, the MariaDB version is instead reported. For example, the InnoDB version reported in MariaDB 10.1 (which is based on MySQL 5.6) included encryption and variable-size page compression before MySQL 5.7 introduced them. MariaDB 10.2 (based on MySQL 5.7) introduced persistent AUTO_INCREMENT (MDEV-6076) in a GA release before MySQL 8.0. MariaDB 10.3 (based on MySQL 5.7) introduced instant ADD COLUMN (MDEV-11369) before MySQL."
6,Scope: Global
6,Dynamic: No
6,Data Type: string
6,Removed: MariaDB 10.10
6,innodb_write_io_threads
6,Description: Number of I/O threads for InnoDB writes. You may on rare occasions need to reduce this default on Linux systems running multiple MariaDB servers to avoid exceeding system limits.
6,Commandline: --innodb-write-io-threads=#
6,Scope: Global
6,"Dynamic: Yes (>= MariaDB 10.11), No (<= MariaDB 10.10)"
6,Data Type: numeric
6,Default Value: 4
6,Range: 1 to 64
6,← InnoDB Troubleshooting
6,↑ InnoDB ↑
6,InnoDB Server Status Variables →
6,Comments
6,Comments loading...
6,"Content reproduced on this site is the property of its respective owners,"
6,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
6,expressed by this content do not necessarily represent those of MariaDB or any other party.
6,↑ InnoDB ↑
6,InnoDB Versions
6,InnoDB Limitations
6,InnoDB Troubleshooting
6,InnoDB System Variables
6,InnoDB Server Status Variables
6,AUTO_INCREMENT Handling in InnoDB
6,InnoDB Buffer Pool
6,InnoDB Change Buffering
6,InnoDB Doublewrite Buffer
6,InnoDB Tablespaces
6,InnoDB File Format
6,InnoDB Row Formats
6,InnoDB Strict Mode
6,InnoDB Redo Log
6,InnoDB Undo Log
6,InnoDB Page Flushing
6,InnoDB Purge
6,Information Schema InnoDB Tables
6,InnoDB Online DDL
6,Binary Log Group Commit and InnoDB Flushing Performance
6,InnoDB Page Compression
6,InnoDB Data Scrubbing
6,InnoDB Lock Modes
6,InnoDB Monitors
6,InnoDB Encryption Overview
6,InnoDB - Unmaintained
6,Products
6,Services
6,Pricing
6,Resources
6,About Us
6,Download MariaDB
6,Subscribe to our newsletter!
6,Legal
6,Privacy Policy
6,Cookie Policy
6,Copyright © 2024 MariaDB. All rights reserved.
8,hash_join_cardinality optimizer_switch Flag - MariaDB Knowledge Base
8,Search
8,Products
8,Services
8,Resources
8,About
8,Contact
8,Login
8,Copyright © 2024 MariaDB. All rights reserved.
8,The Ultimate Guide to High Availability with MariaDB
8,Download Now
8,Knowledge Base
8,Contact
8,Login
8,Search
8,Products
8,Services
8,Pricing
8,Resources
8,About Us
8,Download
8,Knowledge Base
8,» MariaDB Server Documentation
8,» High Availability & Performance Tuning
8,» Optimization and Tuning
8,» Query Optimizations
8,» hash_join_cardinality optimizer_switch Flag
8,Home
8,Open Questions
8,MariaDB Server
8,MariaDB MaxScale
8,MariaDB ColumnStore
8,Connectors
8,History
8,Source
8,Flag as Spam / Inappropriate
8,Translate
8,Created
8,"9 months, 1 week ago"
8,Modified
8,"7 months, 2 weeks ago"
8,Type
8,article
8,Status
8,active
8,License
8,CC BY-SA / Gnu FDL
8,History
8,Comments
8,EditAttachments
8,No attachments exist
8,Product Versions
8,MariaDB starting with 10.6.13
8,hash_join_cardinality optimizer_switch Flag
8,MariaDB starting with 10.6.13The hash_join_cardinality optimizer_switch flag was added in
8,"MariaDB 11.0.2, MariaDB 10.11.3,"
8,"MariaDB 10.10.4, MariaDB 10.9.6, MariaDB 10.8.8 and MariaDB 10.6.13."
8,"In MySQL and MariaDB, the output cardinality of a part of query has historically been tied to the used access method(s). This is different from the approach used in database textbooks. There, the cardinality ""x JOIN y"" is the same regardless of which access methods are used to compute it."
8,Example
8,Consider a query joining customers with their orders:
8,select *
8,from
8,"customer, orders, ..."
8,where
8,customer.id = orders.customer_id and ...
8,"Suppose, table orders has an index IDX on orders.customer_id."
8,"If the query plan is using this index to fetch orders for each customer, the optimizer will use index statistics from IDX to estimate the number of rows in the customer-joined-with-orders."
8,"On the other hand, if the optimizer considers a query plan that"
8,"joins customer with orders without use of indexes, it will ignore the customer.id = orders.customer_id equality completely and will compute the"
8,output cardinality as if customer was cross-joined with orders.
8,Hash Join
8,"MariaDB supports Block Hash Join. It is not enabled by default, one needs to set it join_cache_level to 3 or a bigger value to enable it."
8,"Before MDEV-30812, Query optimization for Block Hash Join would work as described in the above example: It would assume that the join operation is a cross join."
8,"MDEV-30812 introduces a new optimizer_switch flag, hash_join_cardinality. In MariaDB versions before 11.0, it is off by default."
8,"If one sets it to ON, the optimizer will make use of column histograms when computing the cardinality of hash join operation output."
8,"One can see the computation in the Optimizer Trace,"
8,search for hash_join_cardinality.
8,← GUID/UUID Performance
8,↑ Query Optimizations ↑
8,IGNORE INDEX →
8,Comments
8,Comments loading...
8,"Content reproduced on this site is the property of its respective owners,"
8,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
8,expressed by this content do not necessarily represent those of MariaDB or any other party.
8,↑ Query Optimizations ↑
8,Index Hints: How to Force Query Plans
8,Subquery Optimizations
8,Optimization Strategies
8,Optimizations for Derived Tables
8,Table Elimination
8,Statistics for Optimizing Queries
8,Filesort with Small LIMIT Optimization
8,LIMIT ROWS EXAMINED
8,index_merge sort_intersection
8,MariaDB 5.3 Optimizer Debugging
8,optimizer_switch
8,How to Quickly Insert Data Into MariaDB
8,Index Condition Pushdown
8,Query Limits and Timeouts
8,Aborting Statements that Exceed a Certain Time to Execute
8,Partition Pruning and Selection
8,Big DELETEs
8,Charset Narrowing Optimization
8,Data Sampling: Techniques for Efficiently Finding a Random Row
8,Data Warehousing High Speed Ingestion
8,Data Warehousing Summary Tables
8,Data Warehousing Techniques
8,Equality propagation optimization
8,FORCE INDEX
8,Groupwise Max in MariaDB
8,GUID/UUID Performance
8,hash_join_cardinality optimizer_switch Flag
8,IGNORE INDEX
8,not_null_range_scan Optimization
8,optimizer_adjust_secondary_key_costs
8,"Optimizing for ""Latest News""-style Queries"
8,Pagination Optimization
8,Pivoting in MariaDB
8,Rollup Unique User Counts
8,Rowid Filtering Optimization
8,Sargable DATE and YEAR
8,Sargable UPPER
8,USE INDEX
8,Products
8,Services
8,Pricing
8,Resources
8,About Us
8,Download MariaDB
8,Subscribe to our newsletter!
8,Legal
8,Privacy Policy
8,Cookie Policy
8,Copyright © 2024 MariaDB. All rights reserved.
10,Performance tuning in Athena - Amazon AthenaPerformance tuning in Athena - Amazon AthenaAWSDocumentationAmazon AthenaUser GuideService quotasResource limitsQuery optimization
10,techniquesData optimization
10,techniquesAdditional resourcesPerformance tuning in AthenaThis topic provides general information and specific suggestions for improving the
10,"performance of your Athena queries, and how to work around errors related to limits and"
10,resource usage.
10,Service quotas
10,"Athena enforces quotas for metrics like query running time, the number of concurrent"
10,"queries in an account, and API request rates. For more information about these quotas,"
10,see Service Quotas. Exceeding these
10,"quotas causes a query to fail — either when it is submitted, or during query"
10,execution.
10,Many of the performance optimization tips on this page can help reduce the running
10,time of queries. Optimization frees up capacity so that you can run more queries within
10,the concurrency quota and keeps queries from being cancelled for running too
10,long.
10,Quotas on the number of concurrent queries and API requests are per AWS account and
10,AWS Region. We recommend running one workload per AWS account (or using separate
10,provisioned capacity reservations) to keep workloads from competing for the same
10,quota.
10,"If you run two workloads in the same account, one of the workloads can run a burst of"
10,queries. This can cause the remaining workload to be throttled or blocked from running
10,"queries. To avoid this, you can move the workloads to separate accounts to give each"
10,workload its own concurrency quota. Creating a provisioned capacity reservation for one
10,or both of the workloads accomplishes the same goal.
10,Quotas in other
10,services
10,"When Athena runs a query, it can call other services that enforce quotas. During"
10,"query execution, Athena can make API calls to the AWS Glue Data Catalog, Amazon S3, and other AWS"
10,"services like IAM and AWS KMS. If you use federated queries, Athena also"
10,calls AWS Lambda. All of these services have their own limits and quotas that can be
10,"exceeded. When a query execution encounters errors from these services, it fails and"
10,"includes the error from the source service. Recoverable errors are retried, but"
10,queries can still fail if the issue does not resolve itself in time. Make sure to
10,read error messages thoroughly to determine if they come from Athena or from another
10,service. Some of the relevant errors are covered in this document.
10,"For more information about working around errors caused by Amazon S3 service quotas,"
10,see Avoid having too
10,many files later in this
10,"document. For more information about Amazon S3 performance optimization, see Best practices design patterns: optimizing Amazon S3 performance in the"
10,Amazon S3 User Guide.
10,Resource limits
10,"Athena runs queries in a distributed query engine. When you submit a query, the Athena"
10,engine query planner estimates the compute capacity required to run the query and
10,prepares a cluster of compute nodes accordingly. Some queries like DDL queries run on
10,only one node. Complex queries over large data sets run on much bigger clusters. The
10,"nodes are uniform, with the same memory, CPU, and disk configurations. Athena scales out,"
10,"not up, to process more demanding queries."
10,Sometimes the demands of a query exceed the resources available to the cluster running
10,"the query. When this happens, the query fails with the error Query exhausted"
10,resources at this scale factor.
10,"The resource most commonly exhausted is memory, but in rare cases it can also be disk"
10,space. Memory errors commonly occur when the engine performs a join or a window
10,"function, but they can also occur in distinct counts and aggregations."
10,"Even if a query fails with an 'out of resource' error once, it might succeed when you"
10,run it again. Query execution is not deterministic. Factors such as how long it takes to
10,load data and how intermediate datasets are distributed over the nodes can result in
10,"different resource usage. For example, imagine a query that joins two tables and has a"
10,heavy skew in the distribution of the values for the join condition. Such a query can
10,succeed most of the time but occasionally fail when the most common values end up being
10,processed by the same node.
10,"To prevent your queries from exceeding available resources, use the performance tuning"
10,"tips mentioned in this document. In particular, for tips on how to optimize queries that"
10,"exhaust the resources available, see Optimizing joins, Optimizing window"
10,"functions, and Optimizing queries by using approximations."
10,Query optimization
10,techniques
10,Use the query optimization techniques described in this section to make queries run
10,faster or as workarounds for queries that exceed resource limits in Athena.
10,Optimizing joins
10,There are many different strategies for executing joins in a distributed query
10,engine. Two of the most common are distributed hash joins and queries with complex
10,join conditions.
10,Distributed hash
10,join
10,The most common type of join uses an equality comparison as the join
10,condition. Athena runs this type of join as a distributed hash join.
10,"In a distributed hash join, the engine builds a lookup table (hash table) from"
10,one of the sides of the join. This side is called the build
10,side. The records of the build side are distributed across the
10,nodes. Each node builds a lookup table for its subset. The other side of the
10,"join, called the probe side, is then streamed through the"
10,nodes. The records from the probe side are distributed over the nodes in the
10,same way as the build side. This enables each node to perform the join by
10,looking up the matching records in its own lookup table.
10,When the lookup tables created from the build side of the join don't fit into
10,"memory, queries can fail. Even if the total size of the build side is less than"
10,"the available memory, queries can fail if the distribution of the records has"
10,"significant skew. In an extreme case, all records could have the same value for"
10,the join condition and have to fit into memory on a single node. Even a query
10,with less skew can fail if a set of values gets sent to the same node and the
10,values add up to more than the available memory. Nodes do have the ability to
10,"spill records to disk, but spilling slows query execution and can be"
10,insufficient to prevent the query from failing.
10,"Athena attempts to reorder joins to use the larger relation as the probe side,"
10,"and the smaller relation as the build side. However, because Athena does not"
10,"manage the data in tables, it has limited information and often must assume that"
10,the first table is the larger and the second table is the smaller.
10,"When writing joins with equality-based join conditions, assume that the table"
10,to the left of the JOIN keyword is the probe side and the table to
10,"the right is the build side. Make sure that the right table, the build side, is"
10,the smaller of the tables. If it is not possible to make the build side of the
10,"join small enough to fit into memory, consider running multiple queries that"
10,join subsets of the build table.
10,Other join types
10,"Queries with complex join conditions (for example, queries that use"
10,"LIKE , >, or other operators), are often"
10,"computationally demanding. In the worst case, every record from one side of the"
10,join must be compared to every record on the other side of the join. Because the
10,"execution time grows with the square of the number of records, such queries run"
10,the risk of exceeding the maximum execution time.
10,"To find out how Athena will execute your query in advance, you can use the"
10,"EXPLAIN statement. For more information, see Using EXPLAIN and EXPLAIN ANALYZE in"
10,Athena and Understanding Athena EXPLAIN statement
10,results.
10,Optimizing window
10,functions
10,"Because window functions are resource intensive operations, they can make queries"
10,run slow or even fail with the message Query exhausted resources at this
10,scale factor. Window functions keep all the records that they
10,operate on in memory in order to calculate their result. When the window is very
10,"large, the window function can run out of memory."
10,"To make sure your queries run within the available memory limits, reduce the size"
10,"of the windows that your window functions operate over. To do so, you can add a"
10,PARTITIONED BY clause or narrow the scope of existing partitioning
10,clauses.
10,Use
10,non-window functions instead
10,Sometimes queries with window functions can be rewritten without window
10,"functions. For example, instead of using row_number to find the top"
10,"N records, you can use ORDER BY and"
10,LIMIT. Instead of using row_number or
10,"rank to deduplicate records, you can use aggregate functions"
10,"like max_by, min_by, and arbitrary."
10,"For example, suppose you have a dataset with updates from a sensor. The sensor"
10,periodically reports its battery status and includes some metadata like
10,location. If you want to know the last battery status for each sensor and its
10,"location, you can use this query:"
10,"SELECT sensor_id,"
10,"arbitrary(location) AS location,"
10,"max_by(battery_status, updated_at) AS battery_status"
10,FROM sensor_readings
10,GROUP BY sensor_id
10,"Because metadata like location is the same for every record, you can use the"
10,arbitrary function to pick any value from the group.
10,"To get the last battery status, you can use the max_by function."
10,The max_by function picks the value for a column from the record
10,"where the maximum value of another column was found. In this case, it returns"
10,the battery status for the record with the last update time within the group.
10,This query runs faster and uses less memory than an equivalent query with a
10,window function.
10,Optimizing
10,aggregations
10,"When Athena performs an aggregation, it distributes the records across worker nodes"
10,using the columns in the GROUP BY clause. To make the task of matching
10,"records to groups as efficient as possible, the nodes attempt to keep records in"
10,memory but spill them to disk if necessary.
10,It is also a good idea to avoid including redundant columns in GROUP
10,"BY clauses. Because fewer columns require less memory, a query that"
10,describes a group using fewer columns is more efficient. Numeric columns also use
10,"less memory than strings. For example, when you aggregate a dataset that has both a"
10,"numeric category ID and a category name, use only the category ID column in the"
10,GROUP BY clause.
10,Sometimes queries include columns in the GROUP BY clause to work
10,around the fact that a column must either be part of the GROUP BY
10,"clause or an aggregate expression. If this rule is not followed, you can receive an"
10,error message like the following:
10,EXPRESSION_NOT_AGGREGATE: line 1:8: 'category' must be an aggregate
10,expression or appear in GROUP BY clause
10,"To avoid having to add a redundant columns to the GROUP BY clause,"
10,"you can use the arbitrary function, as in the following example."
10,"SELECT country_id,"
10,"arbitrary(country_name) AS country_name,"
10,COUNT(*) AS city_count
10,FROM world_cities
10,GROUP BY country_id
10,The ARBITRARY function returns an arbitrary value from the group. The
10,function is useful when you know all records in the group have the same value for a
10,"column, but the value does not identify the group."
10,Optimizing top N
10,queries
10,The ORDER BY clause returns the results of a query in sorted order.
10,Athena uses distributed sort to run the sort operation in parallel on multiple
10,nodes.
10,"If you don't strictly need your result to be sorted, avoid adding an ORDER"
10,"BY clause. Also, avoid adding ORDER BY to inner queries if"
10,"they are not strictly necessary. In many cases, the query planner can remove"
10,"redundant sorting, but this is not guaranteed. An exception to this rule is if an"
10,"inner query is doing a top N operation, such as finding the"
10,"N most recent, or N most common values."
10,"When Athena sees ORDER BY together with LIMIT, it"
10,understands that you are running a top N query and uses dedicated
10,operations accordingly.
10,NoteAlthough Athena can also often detect window functions like
10,"row_number that use top N, we recommend the"
10,simpler version that uses ORDER BY and LIMIT. For more
10,"information, see Optimizing window"
10,functions.
10,Include only
10,required columns
10,"If you don't strictly need a column, don't include it in your query. The less data"
10,"a query has to process, the faster it will run. This reduces both the amount of"
10,memory required and the amount of data that has to be sent between nodes. If you are
10,"using a columnar file format, reducing the number columns also reduces the amount of"
10,data that is read from Amazon S3.
10,"Athena has no specific limit on the number of columns in a result, but how queries"
10,are executed limits the possible combined size of columns. The combined size of
10,columns includes their names and types.
10,"For example, the following error is caused by a relation that exceeds the size"
10,limit for a relation descriptor:
10,GENERIC_INTERNAL_ERROR:
10,io.airlift.bytecode.CompilationException
10,"To work around this issue, reduce the number of columns in the query, or create"
10,subqueries and use a JOIN that retrieves a smaller amount of data. If
10,"you have queries that do SELECT * in the outermost query, you should"
10,change the * to a list of only the columns that you need.
10,Optimizing queries by using approximations
10,Athena has support for approximation
10,"aggregate functions for counting distinct values, the most frequent"
10,"values, percentiles (including approximate medians), and creating histograms. Use"
10,these functions whenever exact values are not needed.
10,"Unlike COUNT(DISTINCT col) operations, approx_distinct uses much less memory and runs faster. Similarly, using"
10,numeric_histogram instead of histogram uses approximate methods and therefore less memory.
10,Optimizing LIKE
10,"You can use LIKE to find matching strings, but with long strings,"
10,"this is compute intensive. The regexp_like function is in most cases a faster alternative, and also"
10,provides more flexibility.
10,Often you can optimize a search by anchoring the substring that you are looking
10,"for. For example, if you're looking for a prefix, it is much better to use"
10,'substr%' instead of
10,"'%substr%'. Or, if you're using"
10,"regexp_like, '^substr'."
10,Use UNION ALL
10,instead of UNION
10,UNION ALL and UNION are two ways to combine the results of
10,two queries into one result. UNION ALL concatenates the records from
10,"the first query with the second, and UNION does the same, but also"
10,removes duplicates. UNION needs to process all the records and find the
10,"duplicates, which is memory and compute intensive, but UNION ALL is a"
10,"relatively quick operation. Unless you need to deduplicate records, use UNION"
10,ALL for the best performance.
10,Use UNLOAD for
10,large result sets
10,"When the results of a query are expected to be large (for example, tens of"
10,"thousands of rows or more), use UNLOAD to export the results. In most cases, this is"
10,"faster than running a regular query, and using UNLOAD also gives you"
10,more control over the output.
10,"When a query finishes executing, Athena stores the result as a single uncompressed"
10,"CSV file on Amazon S3. This takes longer than UNLOAD, not only because the"
10,"result is uncompressed, but also because the operation cannot be parallelized. In"
10,"contrast, UNLOAD writes results directly from the worker nodes and"
10,"makes full use of the parallelism of the compute cluster. In addition, you can"
10,configure UNLOAD to write the results in compressed format and in other
10,file formats such as JSON and Parquet.
10,"For more information, see UNLOAD."
10,Use CTAS or Glue ETL to materialize frequently used aggregations
10,'Materializing' a query is a way of accelerating query performance by storing
10,"pre-computed complex query results (for example, aggregations and joins) for reuse"
10,in subsequent queries.
10,"If many of your queries include the same joins and aggregations, you can"
10,materialize the common subquery as a new table and then run queries against that
10,"table. You can create the new table with Creating a table from query results (CTAS), or a dedicated ETL tool like Glue"
10,ETL.
10,"For example, suppose you have a dashboard with widgets that show different aspects"
10,"of an orders dataset. Each widget has its own query, but the queries all share the"
10,"same joins and filters. An order table is joined with a line items table, and there"
10,is a filter to show only the last three months. If you identify the common features
10,"of these queries, you can create a new table that the widgets can use. This reduces"
10,duplication and improves performance. The disadvantage is that you must keep the new
10,table up to date.
10,Reuse query results
10,It's common for the same query to run multiple times within a short duration. For
10,"example, this can occur when multiple people open the same data dashboard. When you"
10,"run a query, you can tell Athena to reuse previously calculated results. You specify"
10,the maximum age of the results to be reused. If the same query was previously run
10,"within that time frame, Athena returns those results instead of running the query"
10,"again. For more information, see Reusing query results here in the"
10,Amazon Athena User Guide and Reduce cost and improve query performance with Amazon Athena Query Result
10,Reuse in the AWS Big Data Blog.
10,Data optimization
10,techniques
10,"Performance depends not only on queries, but also importantly on how your dataset is"
10,organized and on the file format and compression that it uses.
10,Partition your data
10,Partitioning divides your table into parts and keeps the related data together
10,"based on properties such as date, country, or region. Partition keys act as virtual"
10,columns. You define partition keys at table creation and use them for filtering your
10,"queries. When you filter on partition key columns, only data from matching"
10,"partitions is read. For example, if your dataset is partitioned by date and your"
10,"query has a filter that matches only the last week, only the data for the last week"
10,"is read. For more information about partitioning, see Partitioning data in Athena."
10,Pick partition keys that will support your queries
10,"Because partitioning has a significant impact on query performance, be sure to"
10,consider how you partition carefully when you design your dataset and tables. Having
10,too many partition keys can result in fragmented datasets with too many files and
10,"files that are too small. Conversely, having too few partition keys, or no"
10,"partitioning at all, leads to queries that scan more data than necessary."
10,Avoid
10,optimizing for rare queries
10,A good strategy is to optimize for the most common queries and avoid
10,"optimizing for rare queries. For example, if your queries look at time spans of"
10,"days, don't partition by hour, even if some queries filter to that level. If"
10,"your data has a granular timestamp column, the rare queries that filter by hour"
10,can use the timestamp column. Even if rare cases scan a little more data than
10,"necessary, reducing overall performance for the sake of rare cases is usually"
10,not a good tradeoff.
10,"To reduce the amount of data that queries have to scan, and thereby improve"
10,"performance, use a columnar file format and keep the records sorted. Instead of"
10,"partitioning by hour, keep the records sorted by timestamp. For queries on"
10,"shorter time windows, sorting by timestamp is almost as efficient as"
10,"partitioning by hour. Furthermore, sorting by timestamp does not typically hurt"
10,the performance of queries on time windows counted in days. For more
10,"information, see Use columnar file"
10,formats.
10,Note that queries on tables with tens of thousands of partitions perform
10,better if there are predicates on all partition keys. This is another reason to
10,design your partitioning scheme for the most common queries. For more
10,"information, see Query partitions"
10,by equality.
10,Use partition
10,projection
10,Partition projection is an Athena feature that stores partition information not in
10,"the AWS Glue Data Catalog, but as rules in the properties of the table in AWS Glue. When Athena"
10,"plans a query on a table configured with partition projection, it reads the table's"
10,partition projection rules. Athena computes the partitions to read in memory based on
10,the query and the rules instead of looking up partitions in the AWS Glue Data Catalog.
10,"Besides simplifying partition management, partition projection can improve"
10,performance for datasets that have large numbers of partitions. When a query
10,"includes ranges instead of specific values for partition keys, looking up matching"
10,partitions in the catalog takes longer the more partitions there are. With partition
10,"projection, the filter can be computed in memory without going to the catalog, and"
10,can be much faster.
10,"In certain circumstances, partition projection can result in worse performance."
10,"One example occurs when a table is ""sparse."" A sparse table does not have"
10,data for every permutation of the partition key values described by the partition
10,"projection configuration. With a sparse table, the set of partitions calculated from"
10,the query and the partition projection configuration are all listed on Amazon S3 even
10,when they have no data.
10,"When you use partition projection, make sure to include predicates on all"
10,partition keys. Narrow the scope of possible values to avoid unnecessary Amazon S3
10,listings. Imagine a partition key that has a range of one million values and a query
10,"that does not have any filters on that partition key. To run the query, Athena must"
10,perform at least one million Amazon S3 list operations. Queries are fastest when you
10,"query on specific values, regardless of whether you use partition projection or"
10,"store partition information in the catalog. For more information, see Query partitions"
10,by equality.
10,"When you configure a table for partition projection, make sure that the ranges"
10,that you specify are reasonable. If a query doesn't include a predicate on a
10,"partition key, all the values in the range for that key are used. If your dataset"
10,"was created on a specific date, use that date as the starting point for any date"
10,ranges. Use NOW as the end of date ranges. Avoid numeric ranges that
10,"have large number of values, and consider using the injected type"
10,instead.
10,"For more information about partition projection, see Partition projection with Amazon Athena."
10,Use partition
10,indexes
10,Partition indexes are a feature in the AWS Glue Data Catalog that improves partition lookup
10,performance for tables that have large numbers of partitions.
10,The list of partitions in the catalog is like a table in a relational database.
10,The table has columns for the partition keys and an additional column for the
10,"partition location. When you query a partitioned table, the partition locations are"
10,looked up by scanning this table.
10,"Just as with relational databases, you can increase the performance of queries by"
10,adding indexes. You can add multiple indexes to support different query patterns.
10,The AWS Glue Data Catalog partition index supports both equality and comparison operators like
10,">, >=, and < combined with the"
10,"AND operator. For more information, see Working with partition indexes in"
10,AWS Glue in the AWS Glue Developer Guide and Improve Amazon Athena query performance using AWS Glue Data Catalog partition indexes
10,in the AWS Big Data Blog.
10,Always use STRING as the type for partition keys
10,"When you query on partition keys, remember that Athena requires partition keys to"
10,be of type STRING in order to push down partition filtering into AWS Glue.
10,"If the number of partitions is not small, using other types can lead to worse"
10,"performance. If your partition key values are date-like or number-like, cast them to"
10,the appropriate type in your query.
10,Remove old and
10,empty partitions
10,"If you remove data from a partition on Amazon S3 (for example, by using Amazon S3 lifecycle), you should also remove the partition entry from the"
10,"AWS Glue Data Catalog. During query planning, any partition matched by the query is listed on"
10,"Amazon S3. If you have many empty partitions, the overhead of listing these partitions"
10,can be detrimental.
10,"Also, if you have many thousands of partitions, consider removing partition"
10,"metadata for old data that is no longer relevant. For example, if queries never look"
10,"at data older than a year, you can periodically remove partition metadata for the"
10,"older partitions. If the number of partitions grows into the tens of thousands,"
10,removing unused partitions can speed up queries that don't include predicates on all
10,partition keys. For information about including predicates on all partition keys in
10,"your queries, see Query partitions"
10,by equality.
10,Query partitions
10,by equality
10,Queries that include equality predicates on all partition keys run faster because
10,the partition metadata can be loaded directly. Avoid queries in which one or more of
10,"the partition keys does not have a predicate, or the predicate selects a range of"
10,"values. For such queries, the list of all partitions has to be filtered to find"
10,"matching values. For most tables, the overhead is minimal, but for tables with tens"
10,"of thousands or more partitions, the overhead can become significant."
10,"If it is not possible to rewrite your queries to filter partitions by equality,"
10,"you can try partition projection. For more information, see Use partition"
10,projection.
10,Avoid using MSCK REPAIR TABLE for partition maintenance
10,"Because MSCK REPAIR TABLE can take a long time to run, only adds new"
10,"partitions, and does not remove old partitions, it is not an efficient way to manage"
10,partitions (see Considerations and
10,limitations).
10,"Partitions are better managed manually using the AWS Glue Data Catalog APIs, ALTER TABLE ADD PARTITION,"
10,or AWS Glue
10,"crawlers. As an alternative, you can use partition projection, which"
10,"removes the need to manage partitions altogether. For more information, see Partition projection with Amazon Athena."
10,Validate that your queries are compatible with the partitioning scheme
10,You can check in advance which partitions a query will scan by using the EXPLAIN statement. Prefix your query with the
10,"EXPLAIN keyword, then look for the source fragment (for example,"
10,Fragment 2 [SOURCE]) for each table near the bottom of the
10,EXPLAIN output. Look for assignments where the right side is
10,defined as a partition key. The line underneath includes a list of all the values
10,for that partition key that will be scanned when the query is run.
10,"For example, suppose you have a query on a table with a dt partition"
10,key and prefix the query with EXPLAIN. If the values in the query are
10,"dates, and a filter selects a range of three days, the EXPLAIN output"
10,might look something like this:
10,dt := dt:string:PARTITION_KEY
10,":: [[2023-06-11], [2023-06-12], [2023-06-13]]"
10,The EXPLAIN output shows that the planner found three values for this
10,partition key that matched the query. It also shows you what those values are. For
10,"more information about using EXPLAIN, see Using EXPLAIN and EXPLAIN ANALYZE in"
10,Athena
10,and Understanding Athena EXPLAIN statement
10,results.
10,Use columnar file
10,formats
10,Columnar file formats like Parquet and ORC are designed for distributed analytics
10,workloads. They organize data by column instead of by row. Organizing data in
10,columnar format offers the following advantages:
10,Only the columns needed for the query are loaded
10,The overall amount of data that needs to be loaded is reduced
10,"Column values are stored together, so data can be compressed efficiently"
10,Files can contain metadata that allow the engine to skip loading unneeded
10,data
10,"As an example of how file metadata can be used, file metadata can contain"
10,information about the minimum and maximum values in a page of data. If the values
10,"queried are not in the range noted in the metadata, the page can be skipped."
10,One way to use this metadata to improve performance is to ensure that data within
10,"the files are sorted. For example, suppose you have queries that look for records"
10,where the created_at entry is within a short time span. If your data is
10,"sorted by the created_at column, Athena can use the minimum and maximum"
10,values in the file metadata to skip the unneeded parts of the data files.
10,"When using columnar file formats, make sure that your files aren't too small. As"
10,noted in Avoid having too
10,"many files, datasets with"
10,many small files cause performance issues. This is particularly true with columnar
10,"file formats. For small files, the overhead of the columnar file format outweighs"
10,the benefits.
10,Note that Parquet and ORC are internally organized by row groups (Parquet) and
10,"stripes (ORC). The default size for row groups is 128 MB, and for stripes, 64 MB. If"
10,"you have many columns, you can increase the row group and stripe size for better"
10,performance. Decreasing the row group or stripe size to less than their default
10,values is not recommended.
10,"To convert other data formats to Parquet or ORC, you can use AWS Glue ETL or Athena."
10,"For more information about using Athena for ETL, see Using CTAS and INSERT INTO for ETL and data"
10,analysis.
10,Compress data
10,Athena supports a wide range of compression formats. Querying compressed data is
10,faster and also cheaper because you pay for the number of bytes scanned before
10,decompression.
10,The gzip format provides
10,good compression ratios and has wide range support across other tools and services.
10,The zstd (Zstandard) format is
10,a newer compression format with a good balance between performance and compression
10,ratio.
10,"When compressing text files such as JSON and CSV data, try to achieve a balance"
10,between the number of files and the size of the files. Most compression formats
10,require the reader to read files from the beginning. This means that compressed text
10,"files cannot, in general, be processed in parallel. Big uncompressed files are often"
10,"split between workers to achieve higher parallelism during query processing, but"
10,this is not possible with most compression formats.
10,As discussed in Avoid having too
10,"many files, it's better to"
10,have neither too many files nor too few. Because the number of files is the limit
10,"for how many workers can process the query, this rule is especially true for"
10,compressed files.
10,"For more information about using compression in Athena, see Athena compression support."
10,Use bucketing for lookups on keys with high cardinality
10,Bucketing is a technique for distributing records into separate files based on the
10,value of one of the columns. This ensures that all records with the same value will
10,be in the same file. Bucketing is useful when you have a key with high cardinality
10,and many of your queries look up specific values of the key.
10,"For example, suppose you query a set of records for a specific user. If the data"
10,"is bucketed by user ID, Athena knows in advance which files contain records for a"
10,specific ID and which files do not. This enables Athena to read only the files that
10,"can contain the ID, greatly reducing the amount of data read. It also reduces the"
10,compute time that otherwise would be required to search through the data for the
10,specific ID.
10,Disadvantages of
10,bucketing
10,Bucketing is less valuable when queries frequently search for multiple values
10,"in the column that the data is bucketed by. The more values queried, the higher"
10,"the likelihood that all or most files will have to be read. For example, if you"
10,"have three buckets, and a query looks for three different values, all files"
10,might have to be read. Bucketing works best when queries look up single
10,values.
10,"For more information, see Partitioning and bucketing in"
10,Athena.
10,Avoid having too
10,many files
10,Datasets that consist of many small files result in poor overall query
10,"performance. When Athena plans a query, it lists all partition locations, which takes"
10,time. Handling and requesting each file also has a computational overhead.
10,"Therefore, loading a single bigger file from Amazon S3 is faster than loading the same"
10,records from many smaller files.
10,"In extreme cases, you might encounter Amazon S3 service limits. Amazon S3 supports up to"
10,"5,500 requests per second to a single index partition. Initially, a bucket is"
10,"treated as a single index partition, but as request loads increase, it can be split"
10,into multiple index partitions.
10,Amazon S3 looks at request patterns and splits based on key prefixes. If your dataset
10,"consists of many thousands of files, the requests coming from Athena can exceed the"
10,"request quota. Even with fewer files, the quota can be exceeded if multiple"
10,concurrent queries are made against the same dataset. Other applications that access
10,the same files can contribute to the total number of requests.
10,"When the request rate limit is exceeded, Amazon S3 returns the following"
10,error. This error is included in the status information for the query in
10,Athena.
10,SlowDown: Please reduce your request rate
10,"To troubleshoot, start by determining if the error is caused by a single query or"
10,"by multiple queries that read the same files. If the latter, coordinate the running"
10,"of queries so that they don't run at the same time. To achieve this, add a queuing"
10,mechanism or even retries in your application.
10,"If running a single query triggers the error, try combining data files or"
10,modifying the query to read fewer files. The best time to combine small files is
10,"before they are written. To do so, consider the following techniques:"
10,Change the process that writes the files to write larger files. For
10,"example, you could buffer records for a longer time before they are written."
10,Put files in a location on Amazon S3 and use a tool like Glue ETL to combine
10,"them into larger files. Then, move the larger files into the location that"
10,"the table points to. For more information, see Reading input files in"
10,larger groups in the AWS Glue Developer Guide and
10,How can I configure an AWS Glue ETL job to output larger files? in
10,the AWS re:Post Knowledge Center.
10,Reduce the number of partition keys. When you have too many partition
10,"keys, each partition might have only a few records, resulting in an"
10,excessive number of small files. For information about deciding which
10,"partitions to create, see Pick partition keys that will support your queries."
10,Avoid additional storage hierarchies beyond the partition
10,"To avoid query planning overhead, store files in a flat structure in each"
10,partition location. Do not use any additional directory hierarchies.
10,"When Athena plans a query, it lists all files in all partitions matched by the"
10,"query. Although Amazon S3 doesn't have directories per se, the convention is to interpret"
10,the / forward slash as a directory separator. When Athena lists
10,"partition locations, it recursively lists any directory it finds. When files within"
10,"a partition are organized into a hierarchy, multiple rounds of listings"
10,occur.
10,"When all files are directly in the partition location, most of the time only one"
10,"list operation has to be performed. However, multiple sequential list operations are"
10,required if you have more than 1000 files in a partition because Amazon S3 returns only
10,1000 objects per list operation. Having more than 1000 files in a partition can also
10,"create other, more serious performance issues. For more information, see Avoid having too"
10,many files.
10,Use SymlinkTextInputFormat only when necessary
10,Using the SymlinkTextInputFormat technique can be a way to work around situations
10,"when the files for a table are not neatly organized into partitions. For example,"
10,symlinks can be useful when all files are in the same prefix or files with different
10,schemas are in the same location.
10,"However, using symlinks adds levels of indirection to the query execution. These"
10,"levels of indirection impact overall performance. The symlink files have to be read,"
10,and the locations they define have to be listed. This adds multiple round trips to
10,"Amazon S3 that usual Hive tables do not require. In conclusion, you should use"
10,SymlinkTextInputFormat only when better options like reorganizing
10,files are not available.
10,Additional resources
10,"For additional information about performance tuning in Athena, consider the following"
10,resources:
10,Read the AWS Big Data blog post Top 10
10,performance tuning tips for Amazon Athena
10,For an article on using predicate pushdown to improve performance in federated
10,"queries, see Improve federated queries with predicate pushdown in Amazon Athena in"
10,the AWS Big Data Blog.
10,Read other Athena
10,posts in the AWS big data blog
10,Ask a question on AWS
10,re:Post using the Amazon Athena tag
10,Consult the Athena
10,topics in the AWS knowledge center
10,"Contact AWS Support (in the AWS Management Console, click Support,"
10,Support Center)
10,"Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document Conventions Athena capacity reservation APIsPreventing throttlingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better."
11,MySQL Performance Tuning Guide
11,ProductAutopilot for MySQLMySQL MonitoringCompatibilityFAQSQL Query OptimizationSolutionsvs MySQLTunerFor Hosting ProvidersFor AWS RDSFor DevelopersPricingDocumentationLog inSign up free
11,← ALL POSTSReleem Help CenterMySQL Performance Tuning CenterHow to tune MySQL?MySQL Health ChecksReleem Performance ScoreMySQL Performance MetricsMySQL LatencyMySQL ThroughputMySQL Slow QueriesMySQL Aborted ClientsMySQL Performance ParametersMySQL Server StatusMySQL Configurationbulk_insert_buffer_sizeinnodb_buffer_pool_chunk_sizeinnodb_buffer_pool_instancesinnodb_buffer_pool_sizeinnodb_change_bufferinginnodb_flush_log_at_trx_commitinnodb_flush_methodinnodb_log_buffer_sizeinnodb_log_file_sizeinnodb_max_dirty_pages_pctinnodb_page_cleanersinnodb_purge_threadsinnodb_read_io_threadsinnodb_thread_concurrencyinnodb_write_io_threadsjoin_buffer_sizekey_buffer_sizemax_allowed_packetmax_connectionsmax_heap_table_sizemyisam_sort_buffer_sizeoptimizer_search_depthread_rnd_buffer_sizesort_buffer_sizetable_definition_cachetable_open_cachethread_cache_sizethread_pool_sizethread_stacktmp_table_sizetransaction_prealloc_sizeHow to increase open_files_limitPrivacymain/ blog/ MySQL Performance Tuning GuideMySQL Performance Tuning Guide Comprehensive guide on MySQL Performance tuning
11,"JUL 05, 2023 • WRITTEN BY ROMAN AGABEKOVMySQL is a powerful and widely-used open-source relational database management system (RDBMS). To unlock its full potential, it is crucial for database administrators (DBAs) to optimize and fine-tune their MySQL servers. This comprehensive guide walks through eight critical steps for effective MySQL tuning that can help improve the performance, efficiency, and stability of MySQL databases. MySQL/MariaDB DocumentationStudy MySQL Configuration Best PracticesAnalyze Monitoring Data- What Metrics Should DBAs Monitor?Analyze MySQL Status- Cache Performance Metrics- Database Efficiency Metrics- Temporary Data MetricsUse Scripts for Configuration Recommendations- MySQLTuner- Tuning-Primer Script- Percona Toolkit- phpMyAdmin Advisor- MysqlreportCalculate Values of MySQL Performance SettingsCreate New Configuration FileApply New Configuration FileHow does Releem Help? 1. MySQL/MariaDB Documentation MySQL has excellent documentation resources that are useful for all, even veteran database administrators with years of experience. MySQL provides server reference manuals for each currently supported version:MySQL 8.0 Reference Manual;MySQL 5.7 Reference Manual;MySQL 5.6 Reference Manual (MySQL 5.6 has not been supported since February 2021.);MariaDB Reference ManualIt's helpful for database administrators to be intimately familiar with these resources. And it's highly recommended to take the time to work through the documentation to better understand how MySQL works and how different parameter settings affect database performance.We published the full list of MySQL variables which impact on database performance. 2. Study MySQL Configuration Best Practices There are an array of resources available both online and in print to learn how to configure MySQL. MySQL has hundreds of configuration options, but for many servers, only a handful are critical. The tuning will vary depending on workload and hardware specifications, but DBAs that familiarize themselves with best practices (for their specific version of MySQL) will be better equipped to understand and solve performance issues in a timely manner.Releem has assembled a useful list of articles and resources that are related to MySQL/ MariaDB/Percona configuration. Here are some general articles for database administrators interested in studying or refreshing MySQL best practices:Ten MySQL performance tuning settings after installation;MySQL Server and SQL Performance TuningMySQL Performance Cheat SheetPerformance Tuning and Configurations for your MySQL ServerMAKING IT BETTER: BASIC MYSQL PERFORMANCE TUNING (MYSQLD)InnoDB Performance Optimization BasicsMySQL 101: Tuning MySQL After Upgrading MemoryHow MySQL Opens and Closes Tables 3. Analyze Monitoring Data After studying MySQL documentation and learning best practices, DBAs can use monitoring software to analyze data from the MySQL server. These tools will help monitor server health while providing unique ways to visualize metrics and handle alerts.Zabbix is an open-source monitoring tool capable of monitoring networks, servers, cloud, applications, and services. Zabbix is highly secure and easily scalable.Prometheus is open-source monitoring software marketing its simplicity and visualization tools.Percona Monitoring and Management is an open-source monitoring solution aimed at helping improve database performance and improving data securityNagios XI is a premium monitoring software but offers a free trial for new users. Nagios XI promises to be limitlessly scalable and highly customizable.Releem is an excellent option for easy-to-use monitoring MySQL databases, as it offers a robust and user-friendly solution for database administrators and developers seeking to optimize their systems. By incorporating a wide range of features and benefits, Releem helps database engineers effortlessly monitor and improve their MySQL performance.What Metrics Should DBAs Monitor? Database engineers should examine various indicators of database health to guarantee that databases operate smoothly and efficiently. Resource utilization is one of the most important health check categories. Each operation within a server primarily relies on four main system resources:The CPU is the powerhouse behind the system; The memory encodes, stores, and retrieves information;Disk I/O is the input and output process for data moving from storage to other hardware components;The network consists of the client connections to the server; When these resources are not optimized, this can lead to performance degradation of both the operating system and database. Ultimately, the most critical metric is the speed at which a query is received and the data returned by the server. The following MySQL metrics are associated with the four system resources:Database Connection Utilization;MySQL Latency;MySQL Throughput (Queries per Second);Memory Utilization;Disk Space Usage;CPU Utilization.By monitoring resource utilization, database administrators can identify potential bottlenecks or capacity issues and make informed decisions about resource allocation or scaling. High resource utilization may signify the need for hardware upgrades, resource reallocation, or query optimizations to ensure optimal database performance. 4. Analyze MySQL Status Before utilizing tools like Releem or other performance optimization techniques, it's essential for database administrators to ensure that their MySQL server has been running for at least 24 hours. This allows the server to accumulate a sufficient amount of data and provide more accurate insights into its performance.To analyze MySQL server status and detect any variables that require configuration adjustments, database administrators can query the server using the SHOW GLOBAL STATUS; command. This command will deliver various metrics that reflect the current performance and status of the MySQL server.These metrics are crucial for understanding the health of the database system and can be categorized into different areas, such as cache performance, database efficiency, and temporary data metrics. With Releem, database administrators can easily manage and monitor these health checks. Each of the metrics noted below is tracked by Releem.Cache Performance Metrics;Database Efficiency Metrics;Temporary Data MetricsThread Cache Hit Rate;MyISAM Key Write Ratio;Temporary Disk Data"
11,Thread Cache Ratio;InnoDB Log File Size
11,MyISAM Cache Hit Rate;Sort Merge Passes Ratio
11,InnoDB Cache Hit Rate;Flushing Logs
11,Table Cache Hit Rate
11,"QCache FragmentationCache Performance Metrics These metrics evaluate the efficiency of cache systems in the database, identifying bottlenecks and optimization opportunities. By measuring hit rate and fragmentation across various cache types (e.g., thread, table, MyISAM, and InnoDB), they help ensure data accessibility and optimized cache usage.Thread Cache Hit Rate – Measures the efficiency of thread caching by calculating the percentage of threads reused from the cache instead of creating new ones.Thread Cache Ratio – Represents the proportion of threads created and cached, which helps evaluate the effectiveness of the thread cache configuration.MyISAM Cache Hit Rate – Calculates the percentage of key reads served from the key buffer in MyISAM tables, indicating the efficiency of the key buffer configuration.InnoDB Cache Hit Rate – Determines the percentage of InnoDB data served from the buffer pool, reflecting the efficiency of the InnoDB buffer pool size.Table Cache Hit Rate – Measures the effectiveness of table cache configuration by calculating the percentage of table open requests served from the cache.QCache Fragmentation – Assesses the level of fragmentation within the query cache, which can impact cache efficiency and query performance.Database Efficiency Metrics Monitoring overall database efficiency and performance, these metrics track key write ratios, log file sizes, and sort merge passes. They assess the database's management of data writes, storage, and sorting, helping pinpoint areas where there's room for optimization.MyISAM Key Write Ratio – Indicates the proportion of key buffer writes in relation to key writes requested, which helps assess the effectiveness of the key buffer size in MyISAM tables.InnoDB Log File Size – Evaluates the appropriateness of the InnoDB log file size, which can affect transaction processing and recovery times.Sort Merge Passes Ratio – Computes the proportion of merge passes required during sort operations, with a lower ratio indicating better performance.Flushing Logs – Tracks the frequency of log flushes, which can impact database performance and durability.Temporary Data Metrics Focusing on the creation and management of temporary data during database operations, these metrics help detect issues with temporary storage systems or query execution inefficiencies that cause excessive temporary data creation.Temporary Disk Data – Monitors the amount of temporary data created on disk during query execution, which can impact performance if it's too high. 5. Use Scripts for Configuration Recommendations Tuning MySQL to improve performance requires continuous monitoring and adjustments to the database configuration. One effective approach to achieve this is by using scripts and tools that provide recommendations for improving performance. Let's take a look at some of these tools:MySQLTuner MySQLTuner is a Perl script that analyzes a server's MySQL configuration and provides suggestions for improving performance. By reviewing various server settings and status variables, MySQLTuner identifies potential issues and recommends adjustments to optimize the database. Some of the key features of MySQLTuner include:Analyzing MySQL configuration file (my.cnf or my.ini);Evaluating server status variables and performance metrics;Providing recommendations for adjusting key performance-related settings;Highlighting potential security vulnerabilities and suggesting remediation;To use MySQLTuner, download the script, ensure that Perl is installed on the system, and execute the script. The output will include a summary of the server's status, along with specific recommendations for improving performance. Tuning-Primer Script This script uses data from ""SHOW STATUS LIKE..."" and ""SHOW VARIABLES LIKE..."" commands to generate reasonable recommendations for optimizing server variables. The original script is no longer maintained, but a Tuning-primer version on Github fully supports MariaDB. Percona Toolkit This toolkit consists of advanced open-source command-line tools designed to simplify complex or challenging MySQL tasks, allowing DBAs to focus on tasks that contribute to business goals.phpMyAdmin Advisor The Advisor system offers suggestions on server variables by analyzing MySQL status variables. phpMyAdmin is a free PHP-based tool designed for administering MySQL through a web interface. Mysqlreport Mysqlreport converts values from SHOW STATUS into an easily understandable report, providing a detailed overview of MySQL performance. It is a superior and virtually the only alternative to manually interpreting SHOW STATUS. 6. Calculate Values of MySQL Performance Settings In order to optimize MySQL performance, it's essential to understand how to properly calculate the values of various MySQL settings. This process, commonly referred to as MySQL tuning, involves adjusting parameters to improve database efficiency, increase server speed, and enhance read and write performance. By utilizing Releem, DBAs can essentially skip the manual process of this step because Releem automatically calculates and tunes all of the following variables:Thread_cache_size: Controls the number of threads to be cached for reuse. Query_cache_type: Determines the type of query caching mechanism used.query_cache_size: Sets the size of the query cache in bytes. query_cache_limit: Defines the maximum size of a single query that can be cached.query_cache_min_res_unit: Specifies the minimum result size in bytes for caching.key_buffer_size: Sets the size of the buffer used for index blocks in MyISAM tables. max_allowed_packet: Defines the maximum size of a packet that can be sent between the client and server. max_heap_table_size: Determines the maximum size of a heap table in bytes. tmp_table_size: Sets the maximum size of internal in-memory temporary tables. innodb_file_per_table: Controls whether InnoDB creates a separate file for each table. sort_buffer_size: Specifies the buffer size for sorting operations. read_rnd_buffer_size: Sets the buffer size for random read operations. bulk_insert_buffer_size: Controls the buffer size for bulk insert operations. myisam_sort_buffer_size: Specifies the buffer size for sorting MyISAM indexes during repair. innodb_buffer_pool_chunk_size: Determines the size of each chunk in the InnoDB buffer pool. join_buffer_size: Sets the buffer size for join operations. table_open_cache: Controls the number of open tables that can be cached. table_definition_cache: Determines the number of table definitions to be cached. innodb_flush_log_at_trx_commit: Controls when logs are flushed during a transaction commit. innodb_log_buffer_size: Specifies the size of the InnoDB log buffer. innodb_write_io_threads: Sets the number of I/O threads for writing to the InnoDB buffer pool. innodb_read_io_threads: Sets the number of I/O threads for reading from the InnoDB buffer pool.innodb_flush_method: Determines the method used for flushing data to InnoDB data files. innodb_thread_concurrency: Controls the number of user threads allowed inside InnoDB concurrently. optimizer_search_depth: Specifies the depth of the search tree for the query optimizer. innodb_purge_threads: Sets the number of threads used for purging operations in InnoDB. thread_handling: Determines how threads are managed by the server. max_connections: Controls the maximum number of concurrent connections to the server. innodb_buffer_pool_size: Sets the size of the InnoDB buffer pool. innodb_log_file_size: Specifies the size of the InnoDB log file.thread_pool_size: Determines the number of threads in the thread pool. 7. Create New Configuration File After using tuner tools to analyze a database's performance and gather recommendations for improvements, the next step is to create a new configuration file that incorporates these changes. When MySQL is first installed, a standard configuration file (my.cnf or my.ini) is created in the base directory. This file contains various settings that control the behavior and performance of the MySQL server. By updating this file with the recommended changes, database administrators can optimize server performance and ensure that the database runs efficiently.Here are the essential steps for creating a new configuration file:Backup the original configuration file – Before making any changes, database administrators should create a backup of the original configuration file. This will allow them to revert to the previous settings in case of any issues or unexpected behavior;Create a new configuration file – To create a new configuration file, open the original my.cnf or my.ini file in a text editor. Start making the manual calculated or recommended changes based on the analysis performed using Releem, MySQLTuner, mysqlslap, or other performance analysis tools;Update the parameters – In the new configuration file, update the relevant parameters with the recommended values. These adjustments can include changes to buffer sizes, cache settings, query optimizations, and other performance-related settings. Ensure that these changes are made in the [mysqld] sections of the configuration file;Save and close the new configuration file – Once all the necessary changes have been made, save the new configuration file, ensuring that the original file extension (i.e., .cnf or .ini) is maintained. Close the text editor once the file is saved;"
11,"8. Apply New Configuration File Once the new configuration file has been created and saved, it can be applied to the MySQL server as follows:Restart the MySQL server – To apply the changes made in the new configuration file, restart the MySQL server. This can be done by restarting the MySQL service through the system's control panel or using the appropriate command for the operating system;"
11,systemctl restart mysqld
11,In case of change 'innodb_log_file_size' only in MySQL 5.6.7 or earlier perform the following commands
11,"mysql -e""SET GLOBAL innodb_fast_shutdown = 1"""
11,systemctl stop mysqld
11,mv /var/lib/mysql/ib_logfile[01] /tmp
11,systemctl start mysqld
11,"Test the new settings – After restarting the MySQL server, test the new settings by running some queries, monitoring server performance, and checking system resources. This will help ensure that the changes have been applied correctly and that the database is functioning as expected;How Does Releem Help? Releem has been uniquely developed to help DBAs skip many of the more time-intensive steps outlined above, accelerating the process and allowing database administrators to efficiently and easily monitor and improve MySQL server performance. Releem can provide a new recommended MySQL configuration by analyzing workload, server, and database information and detecting areas of performance degradation."
11,WRITTEN BY ROMAN AGABEKOVReady to dive in? Try Releem today for FREE! No credit card required.Sign Up FreeProductAutopilot for MySQLMySQL MonitoringSQL Query Optimization Competitors
11,Pricing
11,DocumentationReport a BugRequest a New FeatureSolutionsFor Developers
11,For AWS RDS
11,For Hosting Providers Wall of LoveCompanyFAQ
11,Blog
11,Privacy
11,Terms of use
11,About us
11,Sitemap SocialTwitter
11,Facebook
11,LinkedIn
11,GitHub
11,"Slack © 2023 Releem, Inc. +1 984 368-5788 500 Westover Drive #11329 Sanford, NC 27330 US"
14,Reddit - Dive into anything
14,Skip to main content
14,Reddit and its partners use cookies and similar technologies to provide you with a better experience.
14,"By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising."
14,"By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform."
14,"For more information, please see our"
14,Cookie Notice
14,and our
14,Privacy Policy.
14,Open menu
14,Open navigation
14,Go to Reddit Home
14,r/synology
14,A chip
14,A close button
14,Get app
14,Get the Reddit app
14,Log In
14,Log in to Reddit
14,Expand user menu
14,Open settings menu
14,Log In / Sign Up
14,Advertise on Reddit
14,Shop Collectible Avatars
14,Get the Reddit app
14,Scan this QR code to download the app now
14,Or check it out in the app stores
14,Go to synology
14,r/synology
14,r/synology
14,A community to discuss Synology NAS and networking devices
14,Members
14,Online
14,CallMeGooglyBear
14,ADMIN
14,MOD
14,Speeding up MariaDB - is it possible
14,DSM
14,false
14,I'm trying to move away from my Linux host to Syno for Mariadb.
14,It is slower than molasses in winter. Running on a 920 with 8 GB RAM. Very low utilization otherwise
14,"Looking at logs, it takes seconds for basic transactions. Here is my my.cnf config"
14,[mysqld]
14,skip-networking=0
14,skip-bind-address
14,skip-name-resolve=1
14,character_set_server=utf8
14,aria_pagecache_buffer_size=1G
14,optimizer_search_depth=0
14,innodb_buffer_pool_chunk_size=1G
14,innodb_buffer_pool_instances=4
14,innodb_adaptive_hash_index=OFF
14,innodb_buffer_pool_dump_at_shutdown=ON
14,innodb_buffer_pool_load_at_startup=ON
14,Read more
14,Archived post. New comments cannot be posted and votes cannot be cast.
14,&nbsp;
14,TOPICS
14,Gaming
14,Valheim
14,Genshin Impact
14,Minecraft
14,Pokimane
14,Halo Infinite
14,Call of Duty: Warzone
14,Path of Exile
14,Hollow Knight: Silksong
14,Escape from Tarkov
14,Watch Dogs: Legion
14,Sports
14,NFL
14,NBA
14,Megan Anderson
14,Atlanta Hawks
14,Los Angeles Lakers
14,Boston Celtics
14,Arsenal F.C.
14,Philadelphia 76ers
14,Premier League
14,UFC
14,Business
14,GameStop
14,Moderna
14,Pfizer
14,Johnson & Johnson
14,AstraZeneca
14,Walgreens
14,Best Buy
14,Novavax
14,SpaceX
14,Tesla
14,Crypto
14,Cardano
14,Dogecoin
14,Algorand
14,Bitcoin
14,Litecoin
14,Basic Attention Token
14,Bitcoin Cash
14,Television
14,The Real Housewives of Atlanta
14,The Bachelor
14,Sister Wives
14,90 Day Fiance
14,Wife Swap
14,The Amazing Race Australia
14,Married at First Sight
14,The Real Housewives of Dallas
14,My 600-lb Life
14,Last Week Tonight with John Oliver
14,Celebrity
14,Kim Kardashian
14,Doja Cat
14,Iggy Azalea
14,Anya Taylor-Joy
14,Jamie Lee Curtis
14,Natalie Portman
14,Henry Cavill
14,Millie Bobby Brown
14,Tom Hiddleston
14,Keanu Reeves
14,RESOURCES
14,About Reddit
14,Advertise
14,Help
14,Blog
14,Careers
14,Press
14,Communities
14,Best of Reddit
14,Topics
14,Impressum
14,Content Policy
14,Privacy Policy
14,User Agreement
14,"Reddit, Inc. © 2024. All rights reserved."
14,Top 1%
14,Rank by size
14,Want to browse anonymously?
14,Scan this QR code to download the app now
16,鲲鹏社区-官网丨凝心聚力 共创行业新价值鲲鹏社区鲲鹏开发者鲲鹏开发者社区鲲鹏众智鲲鹏MVP鲲鹏生态创新中心Powered by Kunpeng请启用JavaScript
17,GitHub - major/MySQLTuner-perl: MySQLTuner is a script written in Perl that will assist you with your MySQL configuration and make recommendations for increased performance and stability.
17,Skip to content
17,Toggle navigation
17,Sign in
17,Product
17,Actions
17,Automate any workflow
17,Packages
17,Host and manage packages
17,Security
17,Find and fix vulnerabilities
17,Codespaces
17,Instant dev environments
17,Copilot
17,Write better code with AI
17,Code review
17,Manage code changes
17,Issues
17,Plan and track work
17,Discussions
17,Collaborate outside of code
17,Explore
17,All features
17,Documentation
17,GitHub Skills
17,Blog
17,Solutions
17,For
17,Enterprise
17,Teams
17,Startups
17,Education
17,By Solution
17,CI/CD & Automation
17,DevOps
17,DevSecOps
17,Resources
17,Learning Pathways
17,"White papers, Ebooks, Webinars"
17,Customer Stories
17,Partners
17,Open Source
17,GitHub Sponsors
17,Fund open source developers
17,The ReadME Project
17,GitHub community articles
17,Repositories
17,Topics
17,Trending
17,Collections
17,Pricing
17,Search or jump to...
17,"Search code, repositories, users, issues, pull requests..."
17,Search
17,Clear
17,Search syntax tips
17,Provide feedback
17,"We read every piece of feedback, and take your input very seriously."
17,Include my email address so I can be contacted
17,Cancel
17,Submit feedback
17,Saved searches
17,Use saved searches to filter your results more quickly
17,Name
17,Query
17,"To see all available qualifiers, see our documentation."
17,Cancel
17,Create saved search
17,Sign in
17,Sign up
17,You signed in with another tab or window. Reload to refresh your session.
17,You signed out in another tab or window. Reload to refresh your session.
17,You switched accounts on another tab or window. Reload to refresh your session.
17,Dismiss alert
17,major
17,MySQLTuner-perl
17,Public
17,Notifications
17,Fork
17,1.3k
17,Star
17,8.5k
17,MySQLTuner is a script written in Perl that will assist you with your MySQL configuration and make recommendations for increased performance and stability.
17,License
17,GPL-3.0 license
17,8.5k
17,stars
17,1.3k
17,forks
17,Branches
17,Tags
17,Activity
17,Star
17,Notifications
17,Code
17,Issues
17,Pull requests
17,Actions
17,Projects
17,Security
17,Insights
17,Additional navigation options
17,Code
17,Issues
17,Pull requests
17,Actions
17,Projects
17,Security
17,Insights
17,major/MySQLTuner-perl
17,"This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository."
17,"masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History1,581 Commits.github/workflows.github/workflows  VagrantVagrant  buildbuild  examples/github/resultexamples/github/result  templatestemplates  .gitignore.gitignore  .perltidy.perltidy  CODE_OF_CONDUCT.mdCODE_OF_CONDUCT.md  CONTRIBUTING.mdCONTRIBUTING.md  DockerfileDockerfile  FEATURES.mdFEATURES.md  INTERNALS.mdINTERNALS.md  JenkinsFileJenkinsFile  LICENSELICENSE  MakefileMakefile  README.fr.mdREADME.fr.md  README.it.mdREADME.it.md  README.mdREADME.md  README.ru.mdREADME.ru.md  SECURITY.mdSECURITY.md  USAGE.mdUSAGE.md  basic_passwords.txtbasic_passwords.txt  mtlogo.pngmtlogo.png  mysqltuner.plmysqltuner.pl  mysqltuner.pngmysqltuner.png  renovate.jsonrenovate.json  template_example.tpltemplate_example.tpl  vulnerabilities.csvvulnerabilities.csv  View all filesRepository files navigationREADMECode of conductGPL-3.0 licenseSecurity"
17,MySQLTuner is a script written in Perl that allows you to review a MySQL installation quickly and make adjustments to increase performance and stability. The current configuration variables and status data is retrieved and presented in a brief format along with some basic performance suggestions.
17,MySQLTuner supports ~300 indicators for MySQL/MariaDB/Percona Server in this latest version.
17,"MySQLTuner is actively maintained supporting many configurations such as Galera Cluster, TokuDB, Performance schema, Linux OS metrics, InnoDB, MyISAM, Aria, ..."
17,You can find more details on these indicators here:
17,Indicators description.
17,MySQLTuner needs you
17,"MySQLTuner needs contributors for documentation, code and feedback:"
17,Please join us on our issue tracker at GitHub tracker.
17,Contribution guide is available following MySQLTuner contributing guide
17,Star MySQLTuner project at MySQLTuner Git Hub Project
17,Paid support for LightPath here: jmrenouard@lightpath.fr
17,Paid support for Releem available here: Releem App
17,Stargazers over time
17,Compatibility
17,Test result are available here:
17,"MySQL 8.0, 8.2, 8.3 (full support)"
17,"Percona Server 8.0, 8.2, 8.3 (full support)"
17,"MariaDB 10.4, 10.5, 10.6, 10.11, 11.0, 11.1, 11.2 (full support)"
17,Galera replication (full support)
17,Percona XtraDB cluster (full support)
17,"Mysql Replications (partial support, no test environment)"
17,"MySQL 8.1 (not supported, deprecated version)"
17,"Percona Server 5.7 (not supported, deprecated version)"
17,"MySQL 5.7 (not supported, deprecated version)"
17,"MySQL 5.6 and earlier (not supported, deprecated version)"
17,"Percona Server 5.6 (not supported, deprecated version)"
17,"MariaDB 10.7, 10.8, 10.9, 10.10 (not supported, deprecated version)"
17,"MariaDB 10.3 and earlier (not supported, deprecated version)"
17,"MariaDB 5.5 (not supported, deprecated version)"
17,Windows Support is partial
17,Windows is now supported at this time
17,Successfully run MySQLtuner across WSL2 (Windows Subsystem Linux)
17,https://docs.microsoft.com/en-us/windows/wsl/
17,UNSUPPORTED ENVIRONMENTS - NEED HELP WITH THAT
17,"Cloud based is not supported at this time (Help wanted! GCP, AWS, Azure support requested)"
17,Unsupported storage engines: PRs welcome
17,NDB is not supported feel free to create a Pull Request
17,Archive
17,Spider
17,ColummStore
17,Connect
17,Unmaintenained staff from MySQL or MariaDB:
17,MyISAM is too old and no longer active
17,RockDB is not maintained anymore
17,TokuDB is not maintained anymore
17,XtraDB is not maintained anymore
17,CVE vulnerabilities detection support from https://cve.mitre.org
17,MINIMAL REQUIREMENTS
17,Perl 5.6 or later (with perl-doc package)
17,"Unix/Linux based operating system (tested on Linux, BSD variants, and Solaris variants)"
17,Unrestricted read access to the MySQL server
17,OS root access recommended for MySQL < 5.1
17,WARNING
17,It is important for you to fully understand each change
17,you make to a MySQL database server.
17,If you don't understand portions
17,"of the script's output, or if you don't understand the recommendations,"
17,you should consult a knowledgeable DBA or system administrator
17,that you trust.
17,"Always test your changes on staging environments, and"
17,always keep in mind that improvements in one area can adversely affect
17,MySQL in other areas.
17,"It's also important to wait at least 24 hours of uptime to get accurate results. In fact, running"
17,mysqltuner on a fresh restarted server is completely useless.
17,Also review the FAQ section below.
17,Security recommendations
17,Hi directadmin user!
17,"We detected that you run mysqltuner with da_admin's credentials taken from /usr/local/directadmin/conf/my.cnf, which might bring to a password discovery!"
17,Read link for more details Issue #289.
17,What is MySQLTuner checking exactly ?
17,All checks done by MySQLTuner are documented in MySQLTuner Internals documentation.
17,Download/Installation
17,Choose one of these methods:
17,Script direct download (the simplest and shortest method):
17,wget http://mysqltuner.pl/ -O mysqltuner.pl
17,wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/basic_passwords.txt -O basic_passwords.txt
17,wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/vulnerabilities.csv -O vulnerabilities.csv
17,You can download the entire repository by using git clone or git clone --depth 1 -b master followed by the cloning URL above.
17,Optional Sysschema installation for MySQL 5.6
17,Sysschema is installed by default under MySQL 5.7 and MySQL 8 from Oracle.
17,"By default, on MySQL 5.6/5.7/8, performance schema is enabled by default."
17,"For previous MySQL 5.6 version, you can follow this command to create a new database sys containing very useful view on Performance schema:"
17,Sysschema for MySQL old version
17,"curl ""https://codeload.github.com/mysql/mysql-sys/zip/master"" > sysschema.zip"
17,# check zip file
17,unzip -l sysschema.zip
17,unzip sysschema.zip
17,cd mysql-sys-master
17,mysql -uroot -p < sys_56.sql
17,Sysschema for MariaDB old version
17,"curl ""https://github.com/FromDual/mariadb-sys/archive/refs/heads/master.zip"" > sysschema.zip"
17,# check zip file
17,unzip -l sysschema.zip
17,unzip sysschema.zip
17,cd mariadb-sys-master
17,mysql -u root -p < ./sys_10.sql
17,Performance schema setup
17,"By default, performance_schema is enabled and sysschema is installed on latest version."
17,"By default, on MariaDB, performance schema is disabled by default (MariaDB<10.6)."
17,Consider activating performance schema across your my.cnf configuration file:
17,[mysqld]
17,performance_schema = on
17,performance-schema-consumer-events-statements-history-long = ON
17,performance-schema-consumer-events-statements-history = ON
17,performance-schema-consumer-events-statements-current = ON
17,performance-schema-consumer-events-stages-current=ON
17,performance-schema-consumer-events-stages-history=ON
17,performance-schema-consumer-events-stages-history-long=ON
17,performance-schema-consumer-events-transactions-current=ON
17,performance-schema-consumer-events-transactions-history=ON
17,performance-schema-consumer-events-transactions-history-long=ON
17,performance-schema-consumer-events-waits-current=ON
17,performance-schema-consumer-events-waits-history=ON
17,performance-schema-consumer-events-waits-history-long=ON
17,performance-schema-instrument='%=ON'
17,max-digest-length=2048
17,performance-schema-max-digest-length=2018
17,Sysschema installation for MariaDB < 10.6
17,Sysschema is not installed by default under MariaDB prior to 10.6 MariaDB sys
17,You can follow this command to create a new database sys containing a useful view on Performance schema:
17,"curl ""https://codeload.github.com/FromDual/mariadb-sys/zip/master"" > mariadb-sys.zip"
17,# check zip file
17,unzip -l mariadb-sys.zip
17,unzip mariadb-sys.zip
17,cd mariadb-sys-master/
17,mysql -u root -p < ./sys_10.sql
17,Errors & solutions for performance schema installation
17,ERROR 1054 (42S22) at line 78 in file: './views/p_s/metrics_56.sql': Unknown column 'STATUS' in 'field list'
17,This error can be safely ignored
17,Consider using a recent MySQL/MariaDB version to avoid this kind of issue during sysschema installation
17,"In recent versions, sysschema is installed and integrated by default as sys schema (SHOW DATABASES)"
17,"ERROR at line 21: Failed to open file './tables/sys_config_data_10.sql -- ported', error: 2"
17,Have a look at #452 solution given by @ericx
17,Fixing sysctl configuration (/etc/sysctl.conf)
17,It is a system wide setting and not a database setting: Linux FS Kernel settings
17,You can check its values via:
17,$ cat /proc/sys/fs/aio-*
17,65536
17,2305
17,"For example, to set the aio-max-nr value, add the following line to the /etc/sysctl.conf file:"
17,fs.aio-max-nr = 1048576
17,To activate the new setting:
17,$ sysctl -p /etc/sysctl.conf
17,Specific usage
17,Usage: Minimal usage locally
17,perl mysqltuner.pl --host 127.0.0.1
17,"Of course, you can add the execute bit (chmod +x mysqltuner.pl) so you can execute it without calling Perl directly."
17,Usage: Minimal usage remotely
17,"In previous version, --forcemem shoud be set manually, in order to be able to run an MySQLTuner analysis"
17,"Since 2.1.10, memory and swap are defined to 1Gb by default."
17,"If you want a more accurate value according to your remote server, feel free to setup --forcemem and --forceswap to real RAM value"
17,perl mysqltuner.pl --host targetDNS_IP --user admin_user --pass admin_password
17,Usage: Enable maximum output information around MySQL/MariaDb without debugging
17,perl mysqltuner.pl --verbose
17,perl mysqltuner.pl --buffers --dbstat --idxstat --sysstat --pfstat --tbstat
17,Usage: Enable CVE vulnerabilities check for your MariaDB or MySQL version
17,perl mysqltuner.pl --cvefile=vulnerabilities.csv
17,Usage: Write your result in a file with information displayed
17,perl mysqltuner.pl --outputfile /tmp/result_mysqltuner.txt
17,Usage: Write your result in a file without outputting information
17,perl mysqltuner.pl --silent --outputfile /tmp/result_mysqltuner.txt
17,Usage: Using template model to customize your reporting file based on Text::Template syntax.
17,perl mysqltuner.pl --silent --reportfile /tmp/result_mysqltuner.txt --template=/tmp/mymodel.tmpl
17,"Important: Text::Template module is mandatory for --reportfile and/or --template options, because this module is needed to generate appropriate output based on a text template."
17,Usage: Dumping all information_schema and sysschema views as csv file into results subdirectory
17,perl mysqltuner.pl --verbose --dumpdir=./result
17,Usage: Enable debugging information
17,perl mysqltuner.pl --debug
17,Usage: Update MySQLTuner and data files (password and cve) if needed
17,perl mysqltuner.pl --checkversion --updateversion
17,HTML reports based on
17,Python Jinja2
17,HTML generation is based on Python/Jinja2
17,HTML generation Procedure
17,Generate mysqltuner.pl report using JSON format (--json)
17,Generate HTML report using j2 python tools
17,Jinja2 Templates are located under templates sub directory
17,A basic example is called basic.html.j2
17,Installation Python j2
17,python -mvenv j2
17,source ./j2/bin/activate
17,(j2) pip install j2
17,Using Html report generation
17,perl mysqltuner.pl --verbose --json > reports.json
17,cat reports.json
17,j2 -f json MySQLTuner-perl/templates/basic.html.j2 > variables.html
17,perl mysqltuner.pl --verbose --json | j2 -f json MySQLTuner-perl/templates/basic.html.j2 > variables.html
17,HTML reports based on AHA
17,HTML generation is based on AHA
17,HTML generation Procedure
17,Generate mysqltuner.pl report using standard text reports
17,Generate HTML report using aha
17,Installation Aha
17,Follow instructions from Github repo
17,GitHub AHA main repository
17,Using AHA Html report generation
17,perl mysqltuner.pl --verbose --color > reports.txt
17,"aha --black --title ""MySQLTuner"" -f ""reports.txt"" > ""reports.html"""
17,"perl mysqltuner.pl --verbose --color | aha --black --title ""MySQLTuner"" > reports.html"
17,FAQ
17,Question: What are the prerequisites for running MySQL tuner ?
17,"Before running MySQL tuner, you should have the following:"
17,A MySQL server installation
17,Perl installed on your system
17,Administrative access to your MySQL server
17,Question: Can MySQL tuner make changes to my configuration automatically ?
17,"No., MySQL tuner only provides recommendations. It does not make any changes to your configuration files automatically. It is up to the user to review the suggestions and implement them as needed."
17,Question: How often should I run MySQL tuner ?
17,"It is recommended to run MySQL tuner periodically, especially after significant changes to your MySQL server or its workload."
17,"For optimal results, run the script after your server has been running for at least 24 hours to gather sufficient performance data."
17,Question: How do I interpret the results from MySQL tuner ?
17,MySQL tuner provides output in the form of suggestions and warnings.
17,Review each recommendation and consider implementing the changes in your MySQL configuration file (usually 'my.cnf' or 'my.ini').
17,Be cautious when making changes and always backup your configuration file before making any modifications.
17,Question: Can MySQL tuner cause harm to my database or server ?
17,"While MySQL tuner itself will not make any changes to your server, blindly implementing its recommendations without understanding the impact can cause issues."
17,Always ensure you understand the implications of each suggestion before applying it to your server.
17,Question: Can I use MySQL tuner for optimizing other database systems like PostgreSQL or SQL Server ?
17,MySQL tuner is specifically designed for MySQL servers.
17,"To optimize other database systems, you would need to use tools designed for those systems, such as pgTune for PostgreSQL or SQL Server's built-in performance tools."
17,Question: Does MySQL tuner support MariaDB and Percona Server ?
17,"Yes, MySQL tuner supports MariaDB and Percona Server since they are derivatives of MySQL and share a similar architecture. The script can analyze and provide recommendations for these systems as well."
17,Question: What should I do if I need help with MySQL tuner or have questions about the recommendations ?
17,"If you need help with MySQL tuner or have questions about the recommendations provided by the script, you can consult the MySQL tuner documentation, seek advice from online forums, or consult a MySQL expert."
17,Be cautious when implementing changes to ensure the stability and performance of your server.
17,Question: Will MySQLTuner fix my slow MySQL server ?
17,No.
17,MySQLTuner is a read only script.
17,"It won't write to any configuration files, change the status of any daemons."
17,It will give you an overview of your server's performance and make some basic recommendations for improvements that you can make after it completes.
17,Question: Can I fire my DBA now?
17,MySQLTuner will not replace your DBA in any form or fashion.
17,"If your DBA constantly takes your parking spot and steals your lunch from the fridge, then you may want to consider it - but that's your call."
17,Question: Why does MySQLTuner keep asking me the login credentials for MySQL over and over?
17,The script will try its best to log in via any means possible.
17,"It will check for ~/.my.cnf files, Plesk password files, and empty password root logins."
17,"If none of those are available, then you'll be prompted for a password."
17,"If you'd like the script to run in an automated fashion without user intervention, then create a .my.cnf file in your home directory which contains:"
17,[client]
17,user=someusername
17,password=thatuserspassword
17,"Once you create it, make sure it's owned by your user and the mode on the file is 0600."
17,This should prevent the prying eyes from getting your database login credentials under normal conditions.
17,Question: Is there another way to secure credentials on latest MySQL and MariaDB distributions ?
17,You could use mysql_config_editor utilities.
17,$ mysql_config_editor set --login-path=client --user=someusername --password --host=localhost
17,Enter password: ********
17,"After which, ~/.mylogin.cnf will be created with the appropriate access."
17,"To get information about stored credentials, use the following command:"
17,$mysql_config_editor print
17,[client]
17,user = someusername
17,password = *****
17,host = localhost
17,Question: What's minimum privileges needed by a specific mysqltuner user in database ?
17,"mysql>GRANT SELECT, PROCESS,EXECUTE, REPLICATION CLIENT,"
17,"SHOW DATABASES,SHOW VIEW"
17,ON *.*
17,TO 'mysqltuner'@'localhost' identified by pwd1234;
17,Question: It's not working on my OS! What gives?!
17,These kinds of things are bound to happen. Here are the details I need from you to investigate the issue:
17,OS and OS version
17,"Architecture (x86, x86_64, IA64, Commodore 64)"
17,Exact MySQL version
17,"Where you obtained your MySQL version (OS package, source, etc)"
17,The full text of the error
17,A copy of SHOW VARIABLES and SHOW GLOBAL STATUS output (if possible)
17,Question: How to perform CVE vulnerability checks?
17,Download vulnerabilities.csv from this repository.
17,use option --cvefile to perform CVE checks
17,Question: How to use mysqltuner from a remote host?
17,Thanks to
17,@rolandomysqldba
17,You will still have to connect like a mysql client:
17,Connection and Authentication
17,--host <hostname> Connect to a remote host to perform tests (default: localhost)
17,--socket <socket> Use a different socket for a local connection
17,--port <port>
17,Port to use for connection (default: 3306)
17,--user <username> Username to use for authentication
17,--pass <password> Password to use for authentication
17,--defaults-file <path> defaults file for credentials
17,"Since you are using a remote host, use parameters to supply values from the OS"
17,--forcemem <size>
17,Amount of RAM installed in megabytes
17,--forceswap <size> Amount of swap memory configured in megabytes
17,You may have to contact your remote SysAdmin to ask how much RAM and swap you have
17,"If the database has too many tables, or very large table, use this:"
17,--skipsize
17,Don't enumerate tables and their types/sizes (default: on)
17,(Recommended for servers with many tables)
17,Question: Can I install this project using homebrew on Apple Macintosh?
17,Yes! brew install mysqltuner can be used to install this application using homebrew on Apple Macintosh.
17,MySQLTuner and Vagrant
17,MySQLTuner contains following Vagrant configurations:
17,Fedora Core 30 / Docker
17,Vagrant File is stored in Vagrant subdirectory.
17,Follow following step after vagrant installation:
17,$ vagrant up
17,MySQLTuner contains a Vagrant configurations for test purpose and development
17,Install VirtualBox and Vagrant
17,https://www.virtualbox.org/wiki/Downloads
17,https://www.vagrantup.com/downloads.html
17,Clone repository
17,git clone https://github.com/major/MySQLTuner-perl.git
17,Install Vagrant plugins vagrant-hostmanager and
17,vagrant-vbguest
17,vagrant plugin install vagrant-hostmanager
17,vagrant plugin install vagrant-vbguest
17,Add Fedora Core 30 box for official Fedora Download Website
17,vagrant box add --name generic/fedora30
17,Create a data directory
17,mkdir data
17,setup test environments
17,$ sh build/createTestEnvs.sh
17,$ source build/bashrc
17,$ mysql_percona80 sakila
17,sakila> ...
17,$ docker images
17,mariadb
17,10.1
17,fc612450e1f1
17,12 days ago
17,352MB
17,mariadb
17,10.2
17,027b7c57b8c6
17,12 days ago
17,340MB
17,mariadb
17,10.3
17,47dff68107c4
17,12 days ago
17,343MB
17,mariadb
17,10.4
17,92495405fc36
17,12 days ago
17,356MB
17,mysql
17,5.6
17,95e0fc47b096
17,2 weeks ago
17,257MB
17,mysql
17,5.7
17,383867b75fd2
17,2 weeks ago
17,373MB
17,mysql
17,8.0
17,b8fd9553f1f0
17,2 weeks ago
17,445MB
17,percona/percona-server
17,5.7
17,ddd245ed3496
17,5 weeks ago
17,585MB
17,percona/percona-server
17,5.6
17,ed0a36e0cf1b
17,6 weeks ago
17,421MB
17,percona/percona-server
17,8.0
17,390ae97d57c6
17,6 weeks ago
17,697MB
17,mariadb
17,5.5
17,c7bf316a4325
17,4 months ago
17,352MB
17,mariadb
17,10.0
17,d1bde56970c6
17,4 months ago
17,353MB
17,mysql
17,5.5
17,d404d78aa797
17,4 months ago
17,205MB
17,$ docker ps
17,CONTAINER ID
17,IMAGE
17,COMMAND
17,CREATED
17,STATUS
17,PORTS
17,NAMES
17,da2be9b050c9
17,mariadb:5.5
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5311->3306/tcp
17,mariadb55
17,5deca25d5ac8
17,mariadb:10.0
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5310->3306/tcp
17,mariadb100
17,73aaeb37e2c2
17,mariadb:10.1
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5309->3306/tcp
17,mariadb101
17,72ffa77e01ec
17,mariadb:10.2
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5308->3306/tcp
17,mariadb102
17,f5996f2041df
17,mariadb:10.3
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5307->3306/tcp
17,mariadb103
17,4890c52372bb
17,mariadb:10.4
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:5306->3306/tcp
17,mariadb104
17,6b9dc078e921
17,percona/percona-server:5.6
17,"""/docker-entrypoint.…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:4308->3306/tcp
17,percona56
17,3a4c7c826d4c
17,percona/percona-server:5.7
17,"""/docker-entrypoint.…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:4307->3306/tcp
17,percona57
17,3dda408c91b0
17,percona/percona-server:8.0
17,"""/docker-entrypoint.…"""
17,7 hours ago
17,Up 7 hours
17,"33060/tcp, 0.0.0.0:4306->3306/tcp"
17,percona80
17,600a4e7e9dcd
17,mysql:5.5
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:3309->3306/tcp
17,mysql55
17,4bbe54342e5d
17,mysql:5.6
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,0.0.0.0:3308->3306/tcp
17,mysql56
17,a49783249a11
17,mysql:5.7
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,"33060/tcp, 0.0.0.0:3307->3306/tcp"
17,mysql57
17,d985820667c2
17,mysql:8.0
17,"""docker-entrypoint.s…"""
17,7 hours ago
17,Up 7 hours
17,"0.0.0.0:3306->3306/tcp, 33060/tcp"
17,mysql 8
17,Contributions welcome !
17,How to contribute using Pull Request ? Follow this guide : Pull request creation
17,Simple steps to create a pull request:
17,Fork this Github project
17,Clone it to your local system
17,Make a new branch
17,Make your changes
17,Push it back to your repo
17,Click the Compare & pull request button
17,Click Create pull request to open a new pull request
17,About
17,MySQLTuner is a script written in Perl that will assist you with your MySQL configuration and make recommendations for increased performance and stability.
17,Resources
17,Readme
17,License
17,GPL-3.0 license
17,Code of conduct
17,Code of conduct
17,Security policy
17,Security policy
17,Activity
17,Stars
17,8.5k
17,stars
17,Watchers
17,334
17,watching
17,Forks
17,1.3k
17,forks
17,Report repository
17,Releases
17,February 2024 release
17,Latest
17,"Feb 6, 2024"
17,+ 11 releases
17,Packages
17,No packages published
17,Contributors
17,118
17,+ 104 contributors
17,Languages
17,Perl
17,88.5%
17,Shell
17,4.9%
17,Jinja
17,3.3%
17,HTML
17,1.4%
17,Makefile
17,1.1%
17,Smarty
17,0.5%
17,Dockerfile
17,0.3%
17,Footer
17,"© 2024 GitHub, Inc."
17,Footer navigation
17,Terms
17,Privacy
17,Security
17,Status
17,Docs
17,Contact
17,Manage cookies
17,Do not share my personal information
17,You can’t perform that action at this time.
18,"MySQL vs. MariaDB | NexcessChat with usProductsProductsProductsProductsEnterpriseEnterpriseResourcesResourcesWhy NexcessWhy NexcessPartner ProgramsPartner ProgramsPricingPricingContact UsContact UsSign inSign inMySQL vs. MariaDB: Nexcess application stack with MariaDBKnowledge Base HomeSearch Notice anything different?We've enhanced the appearance of our portal and we're working on updating screenshots. Things might look different, but the functionality remains the same.April 27, 2023By Zachary ArmstrongMySQL is one of the earliest database management systems. At one time, it was available by default with almost every website hosting provider. It was the de facto standard until Oracle Corporation bought Sun Microsystems, along with MySQL. This sale eventually led to the creation of MariaDB, a fork of MySQL.Locate the reliable hosting solution you need In this article, we’ll go over the history of MySQL and MariaDB and see how they compare — MySQL vs. MariaDB — and the performance rationale why the Nexcess application stack is built using MariaDB.How Oracle’s purchase led to MariaDB’s creationGiven that Oracle's main product is Oracle SQL, the MySQL community was apprehensive about its future after the sale. Although Oracle said it would continue to develop MySQL, the rapidly released version 5.5 didn’t eliminate any bugs.Oracle also switched the database to an “open-core” model — selling addons alongside the open-source code — and reduced support options. As a result, some MySQL users looked for alternatives. And some developers decided to leave Oracle and pursue their own projects, taking the then-still free MySQL codebase.One of those projects became MariaDB, which is identical to the original MySQL version at the protocol file format and SQL language level. This level of compatibility means any application built to run on MySQL should run on MariaDB without issue.However, MariaDB is faster and offers some unique features. For example, the Sphinx storage engine (SphinxSE) is integrated into the server itself and doesn’t have to be installed separately. MariaDB also offers advanced backup and data management capabilities.MySQL vs. MariaDB: Tech giants weigh inIn 2013, Red Hat Enterprise Linux switched to using MariaDB by default. The change came in part because of the difficulty in contributing certain patches and features to MySQL after the changes made by Oracle.The same year, Google also announced that it was moving its database infrastructure to MariaDB. A Google engineer noted that although Oracle offers quality development work, it doesn’t offer enough public visibility.MariaDB later partnered with Google to launch its Database-as-a-Service (DBaaS) SkySQL on the Google Cloud Platform. In 2022, Acronis announced a collaboration with MariaDB to offer advanced functionality and convenience for MariaDB users when setting up their backup repositories.In addition, some newer packages of Linux use MariaDB instead of MySQL. And the tech community is very involved in MariaDB’s development. For instance, Alibaba and Wikipedia have submitted several patches to the code. Other big names that use MariaDB include Samsung and Nokia.Comparison: MySQL vs. MariaDB performanceNow that you know a bit about the history of MySQL and MariaDB, let’s take a look at their performance.Rundown of the data storage enginesOracle acquired the InnoDB storage engine when it purchased Innobase in 2005. InnoDB is also available for use on MariaDB. MyISAM was the default data storage engine for MySQL until 2009. It’s still available to use in MySQL, and MariaDB uses some of its code.MariaDB’s Aria storage engine is faster than MyISAM and InnoDB. It also recovers significantly faster than MyISAM after a server crash. MariaDB can also use Percona’s XtraDB, which is based on InnoDB code. However, it performs better than InnoDB, thanks to patches from Google and Percona.Testing MySQL vs. MariaDBThe internet is full of tests conducted by enthusiasts and MariaDB.To better understand why the optimizations performed in MariaDB are so good, we conducted several comparative tests on the Nexcess server. The mysqlslap utility was used in our tests:#mysqlslap --auto-generate-sql --concurrency=$i --number-of-queries=$(($i*400)) --iterations=3The $i variable changes from 10 to 200 in a loop while emulating the simultaneous operation of 10 to 200 clients, each making 400 database requests. Next, it measures the total test execution time.Aria storage engine (MariaDB) vs. MyISAM storage engine (MySQL)The graph below compares the performance of MySQL’s MyISAM storage engine vs. MariaDB’s Aria storage engine. The X axis shows the number of simultaneously working clients. The Y axis shows the time in seconds spent on the test:MariaDB completed the test twice as fast as MySQL.XtraDB storage engine (MariaDB) vs. InnoDB storage engine (MySQL)The graph below compares MariaDB’s XtraDB storage engine vs. MySQL’s InnoDB storage engine:The situation is similar here in that MariaDB won, but by a much more significant margin.MariaDB performance tuning toolsDespite MariaDB’s advantages over MySQL, you’ll still want to optimize its performance to ensure fast and efficient operation. Thankfully, help is at hand. Many MariaDB performance tuning tools are available, and plenty of them work for MySQL, too.MySQLTunerThis Perl-based performance tuner is a good choice for directly configuring MariaDB. However, if you’re just dipping your feet into using this relational database, you’ll like the succinct and clear suggestions for performance improvement.Percona ToolkitThe Percona Toolkit comprises over 30 command-line tools. In particular, the tried-and-true pt-query-digest tool helps analyze queries in MariaDB and pinpoint the slowest ones.MariaDB MaxScaleThis performance tuner from MariaDB has various features, such as load balancing, ensuring an even distribution of resources.MariaDB query cacheDepending on your database setup, enabling MariaDB’s query cache (it’s disabled by default) could give you serious performance optimization for SELECT query results that are repeated often.Performance SchemaPerformance Schema is another MariaDB tuning feature that’s disabled by default. Its primary goal is monitoring the performance of MariaDB servers. Although it appears as a storage engine, it doesn’t function exactly like one.The Performance Schema's various tables give a good overview of specific performance problems, letting you quickly identify the root cause of any issue or issues.WordPress: MySQL vs. MariaDBIf you’re already familiar with MySQL, you won’t have problems using MariaDB. They have the same syntax base, and it’ll only take a few days to ramp up on the minor differences.For example, here’s the code for MySQL vs. MariaDB:If you’re a beginner and need help deciding what to choose, feel free to start with MariaDB. The learning process will be the same, and the Nexcess Knowledge Base can help answer your questions.Almost every WordPress hosting service uses MariaDB instead of MySQL in its application stack. This choice is because MariaDB offers faster performance, greater reliability, and functions that aren’t available in MySQL.Taking advantage of MariaDB's superior performance for your WordPress website with help from NexcessWhen you choose fully managed WordPress hosting from Nexcess, you can focus on your business and let us sort the rest. As just one example documented here, we have a hosting infrastructure and technology stack with proven performance.Hosting support that goes beyond other providersWe believe in a service that goes beyond; one that empowers users and provides stability. Make your web hosting experience simple with proactive site monitoring, security hardening, and a dedicated support team available 24/7/365.See what our customers say about Nexcess Our technical support specialists will help you every step of the way, whether you have questions about improving your databases or complex technical problems requiring deep knowledge of WordPress. You can create a ticket, open a live chat, or give us a call.Click to see how we do fully managed ""Woo"" too!"
18,"Contact us Free migration from Nexcess guarantees a safe transition from any web host. Nexcess also offers pre-installed plugins, the latest updates, powerful server protection, and a free SSL certificate.Contact Nexcess today to choose the hosting plan that’s right for you.Related resourcesWhat is managed hosting? Everything you need to knowWhy choose Nexcess?Compare Nexcess to other managed hostsHow to monitor running MariaDB and MySQL queriesMariaDB performance tuning tips at NexcessMariaDB databases and MariaDB setup in the Nexcess CloudWhat is Nexcess Cloud auto scaling?Managed WordPress & WooCommerce: How to make a WordPress site liveHow to add an SSH key to the server for your Nexcess Cloud accountZachary ArmstrongZachary is a Linux Technician and Cloud Technology Specialist. Since childhood, he has had an interest in computers and the Internet. He decided to share his knowledge and passion through writing. In fact, Zachary believes that a good article can inspire people or help them to disassemble a complex idea into a very simple one for better comprehension. Zachary is a writer who specializes in breaking down complex subjects and making them easy to understand. He has a passion for technology, believes it can change the world for the better, and wants to tell the whole world about it.On a personal note, Zachary loves art and science, especially the field of Neuroscience. If you are also passionate about how the brain works, then you have something to talk about with him always.There is funny story about his work for Nexcess and Liquid Web. Before Zachary explained how he tells people about interesting things on the Internet, his six-year-old sister thought he was a TikToker!Subscribe For Monthly TipsGrow your online business faster with news, tips, strategies, and inspiration.Featured ArticlesTransfer from Pantheon Hosting to Nexcess HostingRefer a friend and get $100Transfer from SiteGround Hosting to Nexcess HostingTransfer from Bluehost hosting to Nexcess hostingMigration guide: transfer my Wix website to Nexcess Transferring Webflow websites to Nexcess hostingTransfer from HostGator hosting to Nexcess hostingMigration Guide: Transfer a Shopify Store to NexcessTransfer from InMotion Hosting to Nexcess hostingTransfer from Hostinger hosting to Nexcess hostingTransfer from Flywheel hosting to Nexcess hostingTransfer from Kinsta hosting to Nexcess hosting Transferring a domain from GoDaddyCategories.htaccessAffiliatesApplicationsBackupsBilling BusinessCDNCDN SSLClient PortalContent Delivery Networks (CDNs)Control Panel ToolsCraft CMSCron JobsDatabasesDev SitesDomain ManagementDrupalEcommerceEmail Enterprise HostingExpressionEngineFTPFile ManagementGetting StartedHostingIP ManagementMagentoMagento 1Magento 2Membership sitesMiscellaneous NexcessNexcess Email ServicesNodeWorxOther ApplicationsOther Best PracticesPCI DSSPWAPerformanceReports and MonitoringSSHSSL ScriptsSecuritySiteWorxStoreBuilderThird Party ClientsWPQuickStartWeb designWeb developmentWebsite ManagementWebsitesWooCommerceWordPressProducts+MagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingProductsMagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingAdd Ons+AutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceAdd OnsAutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceFeatures+Application StackPCI ComplianceDNSFree MigrationFeaturesApplication StackPCI ComplianceDNSFree MigrationPlugins+EventsFundraisingWordPress LMSPluginsEventsFundraisingWordPress LMSResources+Customer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQResourcesCustomer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQCompany+AboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutCompanyAboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutLEGAL  |  © 2024 Nexcess.Net, LLC All Rights Reserved.We use cookies to understand how you interact with our site, to personalize and streamline your experience, and to tailor advertising. By continuing to use our site, you accept our use of cookies and accept our Privacy Policy.Accept"
19,Comparison between equivalent Intel & Graviton instances for MariaDB & PostgreSQL on Amazon RDS
19,Search
19,Browse
19,Community
19,About Community
19,Private Forums
19,Private Forums
19,Intel oneAPI Toolkits Private Forums
19,All other private forums and groups
19,Intel AI Software - Private Forums
19,GEH Pilot Community Sandbox
19,Intel® Connectivity Research Program (Private)
19,Intel-Habana Gaudi Technology Forum
19,Developer Software Forums
19,Developer Software Forums
19,Toolkits & SDKs
19,Software Development Tools
19,Software Development Topics
19,Software Development Technologies
19,Intel® DevCloud
19,Intel® Developer Cloud
19,"oneAPI Registration, Download, Licensing and Installation"
19,GPU Compute Software
19,Software Archive
19,Edge Developer Toolbox
19,Product Support Forums
19,Product Support Forums
19,Memory & Storage
19,Embedded Products
19,Visual Computing
19,FPGA
19,Graphics
19,Processors
19,Wireless
19,Ethernet Products
19,Server Products
19,Intel® Enpirion® Power Solutions
19,Intel Unite® App
19,Intel vPro® Platform
19,Intel® Trusted Execution Technology (Intel® TXT)
19,Intel® Unison™ App
19,Intel® QuickAssist Technology (Intel® QAT)
19,Gaming Forums
19,Gaming Forums
19,Intel® ARC™ Graphics
19,Gaming on Intel® Processors with Intel® Graphics
19,Developing Games on Intel Graphics
19,Blogs
19,Blogs
19,@Intel
19,Products and Solutions
19,Tech Innovation
19,Thought Leadership
19,Cloud
19,Examine critical components of Cloud computing with Intel® software experts
19,Success!
19,Subscription added.
19,Success!
19,Subscription removed.
19,"Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your"
19,profile.
19,Intel Community
19,Blogs
19,Tech Innovation
19,Cloud
19,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
19,107 Discussions
19,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
19,Subscribe
19,Article Options
19,Subscribe to RSS Feed
19,Mark as New
19,Mark as Read
19,Bookmark
19,Subscribe
19,Printer Friendly Page
19,Report Inappropriate Content
19,Mohan_Potheri
19,Employee
19,‎03-31-2023
19,08:00 AM
19,"9,823"
19,Introduction:
19,"Databases are typically the crown jewel of enterprise applications. All workloads including web-based e-commerce, social media, cloud services are typically backed by a database. Open-source databases[i] have become completely mainstream over the past decade and are the primary leaders in innovation in the database space. Open-source software has many attributes that make them successful in this cloud era. One of the major benefits is that developers can use open-source software and databases, without any licensing fees. Open-source software as developers code in features that they need quickly and contribute it back to the community. Open-source projects are therefore more agile and have out-evolved closed source alternatives since the early 2000s."
19,"AWS is positioning Graviton (an ARM based processor) aggressively from a price perspective compared to Intel 3rd generation Xeon Scalable Processors based instances.[ii] They are using cost savings as the primary mechanism to lure customers away from Intel based instances. Re-platforming is needed for customers moving to Graviton, which requires enterprise re-certification of the software with associated porting cost.  Intel Xeon leads across most popular database, web, and throughput related workloads. The cloud ecosystem for Intel has developed over the past 15 years, whereas ARM is relatively new and untested. Customers can potentially experience cloud vendor lock-in as Graviton is unique to AWS."
19,"The critical nature of open-source databases in the cloud makes them a good workload to compare Amazon EC2 Intel 3rd generation Xeon Scalable and Graviton instances. MariaDB[iii]  is an open-source variant of MySQL that offers a consistent set of advanced features and functionality across all major cloud platforms. PostgreSQL is one of the most powerful open-source databases known for its proven architecture, reliability, data integrity, robust feature set and extensibility. We will use MariaDB and PostgreSQL as the two open-source relational databases used in comparison testing on AWS EC2 between Intel 3rd generation Xeon Scalable processors and Amazon Graviton."
19,Amazon RDS:
19,"The Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.  Amazon RDS is used in modern applications for data storage in web and mobile applications. Customers move to managed databases from RDS to avoid having to manage their own databases. Many customers want to leverage open-source databases in the public cloud and break free from legacy databases"
19,MariaDB and PostgreSQL are popular open-source relational databases used for cloud-based applications. We will be deploying identically sized RDS instances for these two databases on Intel 3rd generation Xeon Scalable and Graviton based instances and running the commonly used Sysbench workload to compare their relative performance.
19,Instance Configuration:
19,The details about the Amazon EC2 instance choices that were made with Intel and Graviton instances are shown in Table 1.
19,Category
19,Attribute
19,Config1
19,Config2
19,Run Info
19,Testing Date
19,"Nov 3-11, 2022"
19,"Nov 3-11, 2022"
19,Cloud
19,AWS
19,AWS
19,Instance Type and CPU
19,Instance Type
19,db.r6g.4xlarge or
19,db.r6i.4xlarge
19,db.r6g.8xlarge or db.r6i.8xlarge
19,CPU(s)
19,Memory
19,128GB
19,256GB
19,Network BW / Instance
19,12.5 Gbps
19,25 Gbps
19,Storage: Direct attached
19,SSD GP2
19,SSD GP2
19,Drive Summary
19,1 volume 75GB
19,1 volume 75GB
19,Table 1: Instance configuration details for the testing
19,Workload Configuration:
19,Details about the workload and its attributes are shown in Table 2. Sysbench 1.0.18 was run 4 times per configuration and the results were then averaged for both MariaDB and PostgreSQL.
19,Category
19,Attribute
19,Config1
19,Config2
19,Run Info
19,Benchmark
19,sysbench 1.0.18
19,sysbench 1.0.18
19,Dates
19,"Nov 3-11, 2022"
19,"Nov 3-11, 2022"
19,CPUs
19,Thread(s) per Core
19,"1,2,4"
19,"0.5,1,2"
19,Core(s)
19,CPU Models
19,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
19,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
19,BIOS
19,Workload Specific Details
19,Workload
19,MariaDB 10.6.10
19,PostgreSQL 14.4-R1
19,Command Line
19,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --mysql-user=sbtest --mysql-password=password --mysql-host=mariadb-r6g-4xl-v1.couqinukves2.us-east-1.rds.amazonaws.com  --db-driver=mysql --mysql-db=sbtest run
19,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --pgsql-user=sbtest --pgsql-password=password --pgsql-host=pg-r6i-16xl-v1.couqinukves2.us-east-1.rds.amazonaws.com --pgsql-port=5432 --db-driver=pgsql --pgsql-db=sbtest run
19,Table 2: Workload configuration details for the testing
19,MariaDB Results:
19,The results from the sysbench testing for MariaDB are shown in Table 3. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
19,Queries per second
19,Threads
19,r6g.4xlarge (Graviton)
19,r6i.4xlarge (Intel)
19,Abs Diff
19,Percentage
19,Difference (qps)
19,45420.95
19,55450.305
19,10029.355
19,22%
19,82310.385
19,103531.905
19,21221.52
19,26%
19,138068.393
19,161312.673
19,23244.28
19,17%
19,Threads
19,r6g.8xlarge (Graviton)
19,r6i.8xlarge (Intel)
19,Abs Diff
19,Percentage
19,Difference (qps)
19,102649.813
19,127593.153
19,24943.34
19,24%
19,169209.105
19,216709.785
19,47500.68
19,28%
19,250122.328
19,302356.915
19,52234.5875
19,21%
19,Table 3: MariaDB QPS comparison between Intel and Graviton Instances.
19,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 20-30% for MariaDB.
19,Figure 1: Graphical comparison of sysbench performance for MariaDB between Intel and Graviton based instances (Higher is better)
19,PostgreSQL Results:
19,The results from the sysbench testing for PostgreSQL are shown in Table 4. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
19,Queries per second
19,Threads
19,r6g.4xlarge (Graviton)
19,r6i.4xlarge (Intel)
19,Abs Diff
19,Percentage
19,Difference (qps)
19,58117.2825
19,65466.75
19,7349.4675
19,13%
19,99423.5025
19,114673.365
19,15249.8625
19,15%
19,140116.51
19,152913.408
19,12796.8975
19,Queries per second - 32 vCPU (8xlarge)
19,Threads
19,r6g.8xlarge (Graviton)
19,r6i.8xlarge
19,(Intel)
19,Abs Diff
19,Percentage
19,Difference (qps)
19,81133.195
19,125247.73
19,44114.535
19,54%
19,141914.253
19,208751.038
19,66836.785
19,47%
19,219109.908
19,288519.455
19,69409.5475
19,32%
19,Table 4: PostgreSQL QPS comparison between Intel and Graviton Instances.
19,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 30-50% for PostgreSQL.
19,Figure 2: Graphical comparison of sysbench performance for PostgreSQL between Intel and Graviton based instances (Higher is better)
19,Conclusion:
19,"Customers need to be careful with their choice of instances for their workloads in the cloud. Our results show that not all instances are created equal. Intel 3rd generation Xeon Scalable processors-based instances outperform Amazon similar Graviton based instances for open-source relational databases by 20-50% as the results have shown. Intel’s active participation in the open-source community and its innovative HW and SW optimizations work to boost performance of Database workloads as we have shown. By choosing Intel instances and right sizing them based on their performance characteristics in Amazon RDS, lower TCO with optimal performance can be attained."
19,Disclosure text:
19,"Tests were performed in October-November 2022 on AWS in region us-east-1. All configurations used general Purpose SSD gp2 storage. Baseline I/O performance for gp2 storage is 3 IOPS for each GiB, with a minimum of 100 IOPS. This relationship means that larger volumes have better performance. For our experiments we used. 550GiB storage with a baseline performance of 1500 IOPS. We ran the following Database Engines: - MariaDB 10.6.10, - PostgreSQL 14.4-R1. These were run on each of 4 DB server Instances described below. Database server used AWS RDS servers with 4 DB Instance types."
19,db.r6g – memory-optimized instance classes powered by AWS Graviton2 processors
19,"db.r6g.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
19,"db.r6g.4xlarge, 16 vCPU, 128             GB Memory  & Up to 10 Gbps Network interface"
19,db.r6i – memory-optimized instance classes powered by 3rd Generation Intel Xeon Scalable processors
19,"db.r6i.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
19,"db.r6i.4xlarge, 16 vCPU, 128               GB Memory & Up to 10 Gbps Network interface"
19,Pricing URL for MariaDB: https://aws.amazon.com/rds/mariadb/pricing/
19,For PostgreSQL:   https://aws.amazon.com/rds/postgresql/pricing/
19,"DB Client machine details:  For Database client machine, we used the EC2 instance type: c6i.4xlarge with 16vCPU (8 core), with 32 GB Memory, 75 GB GP2 Storage volume  with 12.5GB Network bandwidth powered by 3rd Generation Intel Xeon Scalable processors. The client machines use the following Software Image (AMI) with Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2022-09-14 & ami-0149b2da6ceec4bb0. All DB Instances, as well as the client Instances were run in US-EAST-1 region. Benchmarking Software: We used sysbench tool to load data and to run oltp_read tests on all these configurations. We used sysbench version"
19,1.0.18 (using system LuaJIT 2.1.0-beta3) for all the DB testing.
19,Disclaimer text:
19,"Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation."
19,Bibliography
19,[i] https://www.dbta.com/BigDataQuarterly/Articles/The-Past-Present-and-Future-of-Open-Source-Databases-150954.aspx discusses the present and future of open source databases
19,"[ii] https://www.percona.com/blog/comparing-graviton-arm-performance-to-intel-and-amd-for-mysql-part-3/ compares DB Engines (and clients on the same instances) on M6i.* (Intel) , M6a.* (AMD),  M6g.*(Graviton) EC2 instances."
19,[iii] https://mariadb.com/database-topics/mariadb-vs-mysql/ provides a good comparison of MySQL and MariaDB.
19,Appendix A: DB Configuration Tuning:
19,------------------------------------------------------------------------------------------------------
19,Tuning for MariaDB:
19,We followed this article for performance tuning mariadb:
19,https://mariadb.com/resources/blog/10-database-tuning-tips-for-peak-workloads/
19,The following parameters were tuned.
19,1. InnoDB Buffer Pool Size
19,Making the InnoDB buffer pool size as large as possible ensures you use memory rather than disks for most read operations (because the buffer pool is where data and indexes are cached).
19,LEFT IT UNCHANGED from the RDS default which is {DBInstanceClassMemory*3/4}
19,where
19,DBInstanceClassMemory is a Formula variable with this description:
19,(from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ParamValuesRef.html
19,2. InnoDB Log File Size
19,"The redo logs make sure writes are fast and durable, and  the InnoDB redo space size is important for write-intensive workloads. The logs’ size is determined by innodb_log-file-size. For best results, generally you’ll want to set a combined total size to be at least 1/4 (or even 1/2) of the InnoDB buffer pool size, or equal to one hour’s worth of log entries during peak load. For MariaDB, we set innodb_log_file_size as {DBInstanceClassMemory*(3/4)*(1/4)}:"
19,- We computed and entered the number in a custom parameter group.
19,innodb_log_file_size
19,For 4xl instance this is   25769803776 (24GB of log file size for 128GB of RAM in that instance)
19,For 8xl instance this is   51539607552 (48GB of log file size for 256GB of RAM in that instance)
19,Tuning for PostgreSQL:
19,+-----------------------------------------------------------------------------+
19,CHANGED THIS FOR EVERY DB Instance class before DB Instance creation:
19,+-----------------------------------------------------------------------------+
19,max_wal_size = '96GB'
19,-->Default is 2048 (specified in MB)
19,16xl - 393216
19,8xl - 196608
19,4xl = 98304
19,2xl - 49152
19,+-----------------------------------------------------------------------------+
19,Refer to the blog:
19,https://www.percona.com/blog/2021/01/22/postgresql-on-arm-based-aws-ec2-instances-is-it-any-good/
19,Tags (4)
19,Tags:MariaDBopen sourcePostgreSQLRelational Databases
19,Kudo
19,"You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in."
19,Comment
19,About the Author
19,"Mohan Potheri is a Cloud Solutions Architect with more than 20 years in IT infrastructure, with in depth experience on Cloud architecture. He currently focuses on educating customers and partners on Intel capabilities and optimizations available on Amazon AWS. He is actively engaged with the Intel and AWS Partner communities to develop compelling solutions with Intel and AWS. He is a VMware vExpert (VCDX#98) with extensive knowledge on premises and hybrid cloud. He also has extensive experience with business critical applications such as SAP, Oracle, SQL and Java across UNIX, Linux and Windows environments. Mohan Potheri is an expert on AI/ML, HPC and has been a speaker in multiple conferences such as VMWorld, GTC, ISC and"
19,other Partner events.
19,Community support is provided during standard business hours (Monday to Friday 7AM - 5PM PST). Other contact methods are available here.
19,"Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade."
19,"For more complete information about compiler optimizations, see our Optimization Notice."
19,©Intel Corporation
19,Terms of Use
19,*Trademarks
19,Cookies
19,Privacy
19,Supply Chain Transparency
19,Site Map
21,SLES 15 SP3 | System Analysis and Tuning Guide
21,"Jump to contentdocumentation.suse.com / System Analysis and Tuning Guide On this pageSUSE Linux Enterprise Server 15 SP3System Analysis and Tuning Guide   This guide supports administrators in problem detection,"
21,resolution and optimization. Publication Date:
21,"March 14, 2024"
21,PrefaceAvailable documentationImproving the documentationDocumentation conventionsSupportI Basics1 General notes on system tuning1.1 Be sure what problem to solve1.2 Rule out common problems1.3 Finding the bottleneck1.4 Step-by-step tuningII System monitoring2 System monitoring utilities2.1 Multi-purpose tools2.2 System information2.3 Processes2.4 Memory2.5 Networking2.6 The /proc file system2.7 Hardware information2.8 Files and file systems2.9 User information2.10 Time and date2.11 Graph your data: RRDtool3 System log files3.1 System log files in /var/log/3.2 Viewing and parsing log files3.3 Managing log files with logrotate3.4 Monitoring log files with logwatch3.5 Configuring mail forwarding for root3.6 Forwarding log messages to a central syslog server3.7 Using logger to make system log entriesIII Kernel monitoring4 SystemTap—filtering and analyzing system data4.1 Conceptual overview4.2 Installation and setup4.3 Script syntax4.4 Example script4.5 User space probing4.6 More information5 Kernel probes5.1 Supported architectures5.2 Types of kernel probes5.3 Kprobes API5.4 debugfs Interface5.5 More information6 Hardware-based performance monitoring with Perf6.1 Hardware-based monitoring6.2 Sampling and counting6.3 Installing Perf6.4 Perf subcommands6.5 Counting particular types of event6.6 Recording events specific to particular commands6.7 More information7 OProfile—system-wide profiler7.1 Conceptual overview7.2 Installation and requirements7.3 Available OProfile utilities7.4 Using OProfile7.5 Generating reports7.6 More information8 Dynamic debug—kernel debugging messages8.1 Benefits of dynamic debugging8.2 Checking the status of dynamic debug8.3 Using dynamic debug8.4 Viewing the dynamic debug messagesIV Resource management9 General system resource management9.1 Planning the installation9.2 Disabling unnecessary services9.3 File systems and disk access10 Kernel control groups10.1 Overview10.2 Resource accounting10.3 Setting resource limits10.4 Preventing fork bombs with TasksMax10.5 Controlling I/O with proportional weight policy10.6 More information11 Automatic Non-Uniform Memory Access (NUMA) balancing11.1 Implementation11.2 Configuration11.3 Monitoring11.4 Impact12 Power management12.1 Power management at CPU Level12.2 In-kernel governors12.3 The cpupower tools12.4 Special tuning options12.5 Troubleshooting12.6 More information12.7 Monitoring power consumption with powerTOPV Kernel tuning13 Tuning I/O performance13.1 Switching I/O scheduling13.2 Available I/O elevators with blk-mq I/O path13.3 I/O barrier tuning14 Tuning the task scheduler14.1 Introduction14.2 Process classification14.3 Completely Fair Scheduler14.4 More information15 Tuning the memory management subsystem15.1 Memory usage15.2 Reducing memory usage15.3 Virtual memory manager (VM) tunable parameters15.4 Monitoring VM behavior16 Tuning the network16.1 Configurable kernel socket buffers16.2 Detecting network bottlenecks and analyzing network traffic16.3 Netfilter16.4 Improving the network performance with receive packet steering (RPS)17 Tuning SUSE Linux Enterprise for SAP17.1 Tuning SLE Systems with sapconf 5VI Handling system dumps18 Tracing tools18.1 Tracing system calls with strace18.2 Tracing library calls with ltrace18.3 Debugging and profiling with Valgrind18.4 More information19 Kexec and Kdump19.1 Introduction19.2 Required packages19.3 Kexec internals19.4 Calculating crashkernel allocation size19.5 Basic Kexec usage19.6 How to configure Kexec for routine reboots19.7 Basic Kdump configuration19.8 Analyzing the crash dump19.9 Advanced Kdump configuration19.10 More information20 Using systemd-coredump to debug application crashes20.1 Use and configurationVII Synchronized clocks with Precision Time Protocol21 Precision Time Protocol21.1 Introduction to PTP21.2 Using PTP21.3 Synchronizing the clocks with phc2sys21.4 Examples of configurations21.5 PTP and NTPA GNU licensesA.1 GNU Free Documentation LicenseList of Figures2.1 Example graph created with RRDtool12.1 powerTOP in interactive mode12.2 HTML powerTOP report19.1 YaST Kdump module: start-up pageList of Tables2.1 List of query options of ethtool12.1 C-states13.1 MQ-DEADLINE tunable parameters13.2 BFQ tunable parameters13.3 KYBER tunable parametersList of Examples2.1 vmstat output on a lightly used machine2.2 vmstat output on a heavily used machine (CPU bound)3.1 Example for /etc/logrotate.conf4.1 Simple SystemTap script4.2 Probe with timer event4.3 printf Function with format specifiers4.4 Using global variables4.5 Monitoring incoming TCP connections with tcp_connections.stp12.1 Example output of cpupower frequency-info12.2 Example output of cpupower idle-info12.3 Example cpupower monitor output17.1 Checking Parameters19.1 Kdump: example configuration using a static IP setup21.1 Slave clock using software time stamping21.2 Slave clock using hardware time stamping21.3 Master clock using hardware time stamping21.4 Master clock using software time stamping (not generally recommended)
21,Copyright © 2006–2024
21,SUSE LLC and contributors. All rights reserved.
21,"Permission is granted to copy, distribute and/or modify this document under"
21,"the terms of the GNU Free Documentation License, Version 1.2 or (at your"
21,option) version 1.3; with the Invariant Section being this copyright notice
21,and license. A copy of the license version 1.2 is included in the section
21,entitled “GNU Free Documentation License”.
21,"For SUSE trademarks, see"
21,https://www.suse.com/company/legal/. All
21,third-party trademarks are the property of their respective owners. Trademark
21,"symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates."
21,Asterisks (*) denote third-party trademarks.
21,All information found in this book has been compiled with utmost attention to
21,"detail. However, this does not guarantee complete accuracy. Neither"
21,"SUSE LLC, its affiliates, the authors nor the translators shall be"
21,held liable for possible errors or the consequences thereof.
21,Preface #  1 Available documentation #  Online documentation
21,Our documentation is available online at https://documentation.suse.com.
21,Browse or download the documentation in various formats.
21,Note: Latest updates
21,The latest updates are usually available in the English-language version of this documentation.
21,SUSE Knowledgebase
21,"If you have run into an issue, also check out the Technical Information"
21,Documents (TIDs) that are available online at https://www.suse.com/support/kb/.
21,Search the SUSE Knowledgebase for known solutions driven by customer need.
21,Release notes
21,"For release notes, see"
21,https://www.suse.com/releasenotes/.
21,In your system
21,"For offline use, the release notes are also available under"
21,/usr/share/doc/release-notes on your system.
21,The documentation for individual packages is available at
21,/usr/share/doc/packages.
21,Many commands are also described in their manual
21,"pages. To view them, run man, followed"
21,by a specific command name. If the man command is
21,"not installed on your system, install it with sudo zypper"
21,install man.
21,2 Improving the documentation #
21,Your feedback and contributions to this documentation are welcome.
21,The following channels for giving feedback are available:
21,Service requests and support
21,"For services and support options available for your product, see"
21,https://www.suse.com/support/.
21,"To open a service request, you need a SUSE subscription registered at"
21,SUSE Customer Center.
21,"Go to https://scc.suse.com/support/requests, log"
21,"in, and click Create New."
21,Bug reports
21,Report issues with the documentation at https://bugzilla.suse.com/.
21,"To simplify this process, click the Report"
21,an issue icon next to a headline in the HTML
21,version of this document. This preselects the right product and
21,category in Bugzilla and adds a link to the current section.
21,You can start typing your bug report right away.
21,A Bugzilla account is required.
21,Contributions
21,"To contribute to this documentation, click the Edit source"
21,document icon next to a headline in the HTML version of
21,"this document. This will take you to the source code on GitHub, where you"
21,can open a pull request.
21,A GitHub account is required.
21,Note: Edit source document only available for English
21,The Edit source document icons are only available for the
21,"English version of each document. For all other languages, use the"
21,Report an issue icons instead.
21,For more information about the documentation environment used for this
21,"documentation, see the repository's README."
21,Mail
21,You can also report errors and send feedback concerning the
21,documentation to <doc-team@suse.com>. Include the
21,"document title, the product version, and the publication date of the"
21,"document. Additionally, include the relevant section number and title (or"
21,provide the URL) and provide a concise description of the problem.
21,3 Documentation conventions #
21,The following notices and typographic conventions are used in this
21,document:
21,/etc/passwd: Directory names and file names
21,PLACEHOLDER: Replace
21,PLACEHOLDER with the actual value
21,PATH: An environment variable
21,"ls, --help: Commands, options, and"
21,parameters
21,user: The name of a user or group
21,package_name: The name of a software package
21,"Alt, Alt–F1: A key to press or a key combination. Keys"
21,are shown in uppercase as on a keyboard.
21,"File, File › Save"
21,"As: menu items, buttons"
21,AMD/Intel
21,This paragraph is only relevant for the AMD64/Intel 64 architectures. The
21,arrows mark the beginning and the end of the text block.
21,"IBM Z, POWER"
21,This paragraph is only relevant for the architectures
21,IBM Z and POWER. The arrows
21,mark the beginning and the end of the text block.
21,"Chapter 1, “Example chapter”:"
21,A cross-reference to another chapter in this guide.
21,Commands that must be run with root privileges. You can also
21,prefix these commands with the sudo command to run them
21,as a non-privileged user:
21,# command
21,> sudo command
21,Commands that can be run by non-privileged users:
21,> command
21,Commands can be split into two or multiple lines by a backslash character
21,(\) at the end of a line. The backslash informs the shell that
21,the command invocation will continue after the line's end:
21,> echo a b \
21,c d
21,A code block that shows both the command (preceded by a prompt)
21,and the respective output returned by the shell:
21,> command
21,output
21,Notices
21,Warning: Warning notice
21,Vital information you must be aware of before proceeding. Warns you about
21,"security issues, potential loss of data, damage to hardware, or physical"
21,hazards.
21,Important: Important notice
21,Important information you should be aware of before proceeding.
21,Note: Note notice
21,"Additional information, for example about differences in software"
21,versions.
21,Tip: Tip notice
21,"Helpful information, like a guideline or a piece of practical advice."
21,Compact Notices
21,"Additional information, for example about differences in software"
21,versions.
21,"Helpful information, like a guideline or a piece of practical advice."
21,4 Support #
21,Find the support statement for SUSE Linux Enterprise Server and general information about
21,technology previews below.
21,"For details about the product lifecycle, see"
21,https://www.suse.com/lifecycle.
21,"If you are entitled to support, find details on how to collect information"
21,for a support ticket at
21,https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html.
21,4.1 Support statement for SUSE Linux Enterprise Server #
21,"To receive support, you need an appropriate subscription with SUSE."
21,"To view the specific support offers available to you, go to"
21,https://www.suse.com/support/ and select your product.
21,The support levels are defined as follows:
21,"Problem determination, which means technical support designed to provide"
21,"compatibility information, usage support, ongoing maintenance,"
21,information gathering and basic troubleshooting using available
21,documentation.
21,"Problem isolation, which means technical support designed to analyze"
21,"data, reproduce customer problems, isolate a problem area and provide a"
21,resolution for problems not resolved by Level 1 or prepare for
21,Level 3.
21,"Problem resolution, which means technical support designed to resolve"
21,problems by engaging engineering to resolve product defects which have
21,been identified by Level 2 Support.
21,"For contracted customers and partners, SUSE Linux Enterprise Server is delivered with L3"
21,"support for all packages, except for the following:"
21,Technology previews.
21,"Sound, graphics, fonts, and artwork."
21,Packages that require an additional customer contract.
21,Some packages shipped as part of the module Workstation
21,Extension are L2-supported only.
21,Packages with names ending in -devel (containing header
21,files and similar developer resources) will only be supported together
21,with their main packages.
21,SUSE will only support the usage of original packages.
21,"That is, packages that are unchanged and not recompiled."
21,4.2 Technology previews #
21,"Technology previews are packages, stacks, or features delivered by SUSE"
21,to provide glimpses into upcoming innovations.
21,Technology previews are included for your convenience to give you a chance
21,to test new technologies within your environment.
21,We would appreciate your feedback.
21,"If you test a technology preview, please contact your SUSE representative"
21,and let them know about your experience and use cases.
21,Your input is helpful for future development.
21,Technology previews have the following limitations:
21,Technology previews are still in development.
21,"Therefore, they may be functionally incomplete, unstable, or otherwise"
21,not suitable for production use.
21,Technology previews are not supported.
21,Technology previews may only be available for specific hardware
21,architectures.
21,Details and functionality of technology previews are subject to change.
21,"As a result, upgrading to subsequent releases of a technology preview may"
21,be impossible and require a fresh installation.
21,"SUSE may discover that a preview does not meet customer or market needs,"
21,or does not comply with enterprise standards.
21,Technology previews can be removed from a product at any time.
21,SUSE does not commit to providing a supported version of such
21,technologies in the future.
21,"For an overview of technology previews shipped with your product, see the"
21,release notes at https://www.suse.com/releasenotes.
21,Part I Basics #  1 General notes on system tuning
21,This manual discusses how to find the reasons for performance problems
21,and provides means to solve these problems. Before you start tuning your
21,"system, you should make sure you have ruled out common problems and have"
21,found the cause for the problem. You should also have a detailed plan on
21,"how to tune the system, because applying random tuning tips often will"
21,not help and could make things worse.
21,1 General notes on system tuning #
21,This manual discusses how to find the reasons for performance problems
21,and provides means to solve these problems. Before you start tuning your
21,"system, you should make sure you have ruled out common problems and have"
21,found the cause for the problem. You should also have a detailed plan on
21,"how to tune the system, because applying random tuning tips often will"
21,not help and could make things worse.
21,Procedure 1.1: General approach when tuning a system #
21,Specify the problem that needs to be solved.
21,"In case the degradation is new, identify any recent changes to the"
21,system.
21,Identify why the issue is considered a performance problem.
21,Specify a metric that can be used to analyze performance. This metric
21,"could for example be latency, throughput, the maximum number of"
21,"users that are simultaneously logged in, or the maximum number of active users."
21,Measure current performance using the metric from the previous step.
21,Identify the subsystem(s) where the application is spending the most
21,time.
21,Monitor the system and/or the application.
21,"Analyze the data, categorize where time is being spent."
21,Tune the subsystem identified in the previous step.
21,Remeasure the current performance without monitoring using the same
21,metric as before.
21,"If performance is still not acceptable, start over with"
21,Step 3.
21,1.1 Be sure what problem to solve #
21,"Before starting to tuning a system, try to describe the problem as"
21,exactly as possible. A statement like “The system is slow!”
21,"is not a helpful problem description. For example, it could make a"
21,difference whether the system speed needs to be improved in general or
21,only at peak times.
21,"Furthermore, make sure you can apply a measurement to your problem,"
21,otherwise you cannot verify if the tuning was a success or
21,not. You should always be able to compare “before” and
21,“after”. Which metrics to use depends on the scenario or
21,"application you are looking into. Relevant Web server metrics, for"
21,"example, could be expressed in terms of:"
21,Latency
21,The time to deliver a page
21,Throughput
21,Number of pages served per second or megabytes transferred per second
21,Active users
21,The maximum number of users that can be downloading pages while still
21,receiving pages within an acceptable latency
21,1.2 Rule out common problems #
21,"A performance problem often is caused by network or hardware problems,"
21,"bugs, or configuration issues. Make sure to rule out problems such as the"
21,ones listed below before attempting to tune your system:
21,Check the output of the systemd journal (see
21,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”) for unusual entries."
21,Check (using top or ps) whether a
21,certain process misbehaves by eating up unusual amounts of CPU time or
21,memory.
21,Check for network problems by inspecting
21,/proc/net/dev.
21,"In case of I/O problems with physical disks, make sure it is not caused"
21,by hardware problems (check the disk with the
21,smartmontools) or by a full disk.
21,Ensure that background jobs are scheduled to be carried out in times
21,the server load is low. Those jobs should also run with low priority
21,(set via nice).
21,"If the machine runs several services using the same resources, consider"
21,moving services to another server.
21,"Last, make sure your software is up-to-date."
21,1.3 Finding the bottleneck #
21,Finding the bottleneck very often is the hardest part when tuning a
21,system. SUSE Linux Enterprise Server offers many tools to help you with this task.
21,"See Part II, “System monitoring” for detailed information on"
21,general system monitoring applications and log file analysis. If the
21,"problem requires a long-time in-depth analysis, the Linux kernel offers"
21,means to perform such analysis. See
21,"Part III, “Kernel monitoring” for coverage."
21,"Once you have collected the data, it needs to be analyzed. First, inspect"
21,"if the server's hardware (memory, CPU, bus) and its I/O capacities (disk,"
21,"network) are sufficient. If these basic conditions are met, the system"
21,might benefit from tuning.
21,1.4 Step-by-step tuning #
21,Make sure to carefully plan the tuning itself. It is of vital importance
21,to only do one step at a time. Only by doing so can you
21,measure whether the change made an improvement or even had a negative
21,impact. Each tuning activity should be measured over a sufficient time
21,period to ensure you can do an analysis based on significant
21,"data. If you cannot measure a positive effect, do not make the change"
21,"permanent. Chances are, that it might have a negative effect in the"
21,future.
21,Part II System monitoring #  2 System monitoring utilities
21,"There are number of programs, tools, and utilities which you can use to"
21,examine the status of your system. This chapter introduces some
21,and describes their most important and frequently used parameters.
21,"3 System log filesSystem log file analysis is one of the most important tasks when analyzing the system. In fact, looking at the system log files should be the first thing to do when maintaining or troubleshooting a system. SUSE Linux Enterprise Server automatically logs almost everything that happens on the system i…2 System monitoring utilities #"
21,"There are number of programs, tools, and utilities which you can use to"
21,examine the status of your system. This chapter introduces some
21,and describes their most important and frequently used parameters.
21,Note:
21,Gathering and Analyzing System Information with
21,supportconfig
21,"Apart from the utilities presented in the following, SUSE Linux Enterprise Server"
21,"also contains supportconfig, a tool to create reports"
21,"about the system such as: current kernel version, hardware, installed"
21,"packages, partition setup and much more. These reports are used to"
21,provide the SUSE support with needed information in case a support
21,"ticket is created. However, they can also be analyzed for known issues to"
21,"help resolve problems faster. For this purpose, SUSE Linux Enterprise Server provides"
21,both an appliance and a command line tool for Supportconfig Analysis
21,"(SCA). See Book “Administration Guide”, Chapter 39 “Gathering system information for support” for details."
21,"For each of the described commands, examples of the relevant outputs are"
21,"presented. In the examples, the first line is the command itself (after"
21,the tux > or root #). Omissions are indicated with
21,square brackets ([...]) and long lines are wrapped
21,where necessary. Line breaks for long lines are indicated by a backslash
21,(\).
21,> command -x -y
21,output line 1
21,output line 2
21,"output line 3 is annoyingly long, so long that \"
21,we need to break it
21,output line 4
21,[...]
21,output line 98
21,output line 99
21,The descriptions have been kept short so that we can include as many
21,utilities as possible. Further information for all the commands can be
21,found in the manual pages. Most of the commands also understand the
21,"parameter --help, which produces a brief list of possible"
21,parameters.
21,2.1 Multi-purpose tools #
21,While most Linux system monitoring tools monitor only a single aspect of
21,"the system, there are a few tools with a broader scope. To get"
21,"an overview and find out which part of the system to examine further, use"
21,these tools first.
21,2.1.1 vmstat #
21,"vmstat collects information about processes, memory, I/O, interrupts and"
21,CPU:
21,vmstat [options] [delay [count]]
21,"When called without values for delay and count, it displays average values"
21,"since the last reboot. When called with a value for delay (in seconds), it"
21,displays values for the given period (two seconds in the examples
21,below). The value for count specifies the number of updates vmstat should
21,"perform. If not specified, it will run until manually stopped."
21,Example 2.1: vmstat output on a lightly used machine #  > vmstat 2
21,procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
21,swpd
21,free
21,buff
21,cache
21,cs us sy id wa st
21,44264
21,81520
21,424 935736
21,0 98
21,44264
21,81552
21,424 935736
21,0 100
21,44264
21,81520
21,424 935732
21,0 100
21,44264
21,81520
21,424 935732
21,0 100
21,44264
21,81552
21,424 935732
21,0 100
21,0Example 2.2: vmstat output on a heavily used machine (CPU bound) #  > vmstat 2
21,procs -----------memory----------- ---swap-- -----io---- -system-- -----cpu------
21,swpd
21,free
21,buff
21,cache
21,cs us sy id wa st
21,26236 459640 110240 6312648
21,9944
21,2 4552 6597 95
21,26236 396728 110336 6136224
21,9588
21,0 4468 6273 94
21,26236 554920 110508 6166508
21,7684 27992 4474 4700 95
21,26236 518184 110516 6039996
21,0 10830
21,4 4446 4670 94
21,26236 716468 110684 6074872
21,8734 20534 4512 4061 96
21,0Tip: First line of output
21,The first line of the vmstat output always displays average values
21,since the last reboot.
21,The columns show the following:
21,Shows the number of processes in a runnable state. These processes
21,are either executing or waiting for a free CPU slot. If the number
21,of processes in this column is constantly higher than the number of
21,"CPUs available, this may be an indication of insufficient CPU power."
21,Shows the number of processes waiting for a resource other than a
21,CPU. A high number in this column may indicate an I/O problem
21,(network or disk).
21,swpd
21,The amount of swap space (KB) currently used.
21,free
21,The amount of unused memory (KB).
21,inact
21,Recently unused memory that can be reclaimed. This column is only
21,visible when calling vmstat with the parameter
21,-a (recommended).
21,active
21,Recently used memory that normally does not get reclaimed. This
21,column is only visible when calling vmstat with
21,the parameter -a (recommended).
21,buff
21,File buffer cache (KB) in RAM that contains file system metadata. This
21,column is not visible when calling vmstat with
21,the parameter -a.
21,cache
21,Page cache (KB) in RAM with the actual contents of files. This
21,column is not visible when calling vmstat with
21,the parameter -a.
21,si / so
21,Amount of data (KB) that is moved from swap to RAM
21,(si) or from RAM to swap (so)
21,per second. High so values over a long period of
21,time may indicate that an application is leaking memory and the
21,leaked memory is being swapped out. High si values
21,over a long period of time could mean that an application that was
21,inactive for a very long time is now active again. Combined high
21,si and so values for prolonged
21,periods of time are evidence of swap thrashing and may indicate that
21,more RAM needs to be installed in the system because there is not
21,enough memory to hold the working set size.
21,Number of blocks per second received from a block device (for
21,"example, a disk read). Note that swapping also impacts the values"
21,shown here. The block size may vary between file systems but can
21,be determined using the stat utility. If throughput data is
21,required then iostat may be used.
21,"Number of blocks per second sent to a block device (for example, a"
21,disk write). Note that swapping also impacts the values shown here.
21,Interrupts per second. A high value may indicate a high I/O level
21,"(network and/or disk), but could also be triggered for other reasons"
21,such as inter-processor interrupts triggered by another activity.
21,Make sure to also check /proc/interrupts to
21,identify the source of interrupts.
21,Number of context switches per second. This is the number of times
21,that the kernel replaces executable code of one program in memory
21,with that of another program.
21,Percentage of CPU usage executing application code.
21,Percentage of CPU usage executing kernel code.
21,Percentage of CPU time spent idling. If this value is zero over a
21,"longer time, your CPU(s) are working to full capacity. This"
21,is not necessarily a bad sign—rather refer to the values in
21,columns r and b to determine if
21,your machine is equipped with sufficient CPU power.
21,"If ""wa"" time is non-zero, it indicates throughput lost because of"
21,"waiting for I/O. This may be inevitable, for example, if a file is"
21,"being read for the first time, background writeback cannot keep up,"
21,and so on. It can also be an indicator for a hardware bottleneck
21,"(network or hard disk). Lastly, it can indicate a potential for"
21,tuning the virtual memory manager (refer to
21,"Chapter 15, Tuning the memory management subsystem)."
21,Percentage of CPU time stolen from a virtual machine.
21,See vmstat --help for more options.
21,2.1.2 dstat #
21,dstat is a replacement for tools such as
21,"vmstat, iostat,"
21,"netstat, or ifstat."
21,dstat displays information about the system
21,"resources in real time. For example, you can compare disk usage"
21,"in combination with interrupts from the IDE controller, or compare"
21,network bandwidth with the disk throughput (in the same interval).
21,"By default, its output is presented in readable tables."
21,"Alternatively, CSV output can be produced which is suitable as a"
21,spreadsheet import format.
21,It is written in Python and can be enhanced with plug-ins.
21,This is the general syntax:
21,dstat [-afv] [OPTIONS..] [DELAY [COUNT]]
21,All options and parameters are optional.
21,"Without any parameter, dstat"
21,"displays statistics about CPU (-c,"
21,"--cpu), disk (-d,"
21,"--disk), network (-n,"
21,"--net), paging (-g,"
21,"--page), and the interrupts and context switches of"
21,"the system (-y, --sys); it refreshes"
21,the output every second ad infinitum:
21,# dstat
21,"You did not select any stats, using -cdngy by default."
21,----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--
21,usr sys idl wai hiq siq| read
21,writ| recv
21,send|
21,out | int
21,csw
21,0 100
21,15k
21,44k|
21,0 |
21,82B| 148
21,194
21,0 100
21,0 |5430B
21,170B|
21,0 | 163
21,187
21,0 100
21,0 |6363B
21,842B|
21,0 | 196
21,"185-a, --all"
21,equal to -cdngy (default)
21,"-f, --full"
21,"expand -C, -D,"
21,"-I, -N and -S"
21,discovery lists
21,"-v, --vmstat"
21,"equal to -pmgdsc, -D total"
21,DELAY
21,delay in seconds between each update
21,COUNT
21,the number of updates to display before exiting
21,The default delay is 1 and the count is unspecified (unlimited).
21,"For more information, see the man page of dstat and"
21,its Web page at http://dag.wieers.com/home-made/dstat/.
21,2.1.3 System activity information: sar #
21,sar can generate extensive reports on almost all
21,"important system activities, among them CPU, memory, IRQ usage, I/O, and"
21,networking. It can also generate reports in real time.
21,The sar command gathers data from the
21,/proc file system.
21,Note: sysstat package
21,The sar command is a part of the
21,"sysstat package. Install it with YaST, or with"
21,"the zypper in sysstat command. sysstat.service does not start by default,"
21,and must be enabled and started with the following command:
21,> sudo systemctl enable --now sysstat2.1.3.1 Generating reports with sar #
21,"To generate reports in real time, call sar with an"
21,interval (seconds) and a count. To generate reports from files specify
21,a file name with the option -f instead of interval and
21,"count. If file name, interval and count are not specified,"
21,sar attempts to generate a report from
21,"/var/log/sa/saDD, where"
21,DD stands for the current day. This is the
21,default location to where sadc (the system
21,activity data collector) writes its data.
21,Query multiple files with multiple -f options.
21,sar 2 10
21,"# real time report, 10 times every 2 seconds"
21,sar -f ~/reports/sar_2014_07_17
21,# queries file sar_2014_07_17
21,sar
21,# queries file from today in /var/log/sa/
21,cd /var/log/sa && \
21,sar -f sa01 -f sa02
21,# queries files /var/log/sa/0[12]
21,Find examples for useful sar calls and their
21,interpretation below. For detailed information on the meaning of each
21,"column, refer to the man (1) of"
21,sar.
21,Note: sysstat reporting when the service stops
21,"When the sysstat service is stopped (for example, during"
21,"reboot or shutdown), the tool still collects last-minute statistics by"
21,automatically running the /usr/lib64/sa/sa1 -S ALL 1 1
21,command. The collected binary data is stored in the system activity data
21,file.
21,2.1.3.1.1 CPU usage report: sar #
21,"When called with no options, sar shows a basic"
21,"report about CPU usage. On multi-processor machines, results for all"
21,CPUs are summarized. Use the option -P ALL to also
21,see statistics for individual CPUs.
21,# sar 10 5
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,17:51:29
21,CPU
21,%user
21,%nice
21,%system
21,%iowait
21,%steal
21,%idle
21,17:51:39
21,all
21,"57,93"
21,"0,00"
21,"9,58"
21,"1,01"
21,"0,00"
21,"31,47"
21,17:51:49
21,all
21,"32,71"
21,"0,00"
21,"3,79"
21,"0,05"
21,"0,00"
21,"63,45"
21,17:51:59
21,all
21,"47,23"
21,"0,00"
21,"3,66"
21,"0,00"
21,"0,00"
21,"49,11"
21,17:52:09
21,all
21,"53,33"
21,"0,00"
21,"4,88"
21,"0,05"
21,"0,00"
21,"41,74"
21,17:52:19
21,all
21,"56,98"
21,"0,00"
21,"5,65"
21,"0,10"
21,"0,00"
21,"37,27"
21,Average:
21,all
21,"49,62"
21,"0,00"
21,"5,51"
21,"0,24"
21,"0,00"
21,"44,62"
21,%iowait displays the percentage of time that the
21,CPU was idle while waiting for an I/O request. If this value is
21,"significantly higher than zero over a longer time, there is a"
21,bottleneck in the I/O system (network or hard disk). If the
21,"%idle value is zero over a longer time,"
21,your CPU is working at capacity.
21,2.1.3.1.2 Memory usage report: sar -r #
21,Generate an overall picture of the system memory (RAM) by using the
21,option -r:
21,# sar -r 10 5
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,17:55:27 kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty
21,17:55:37
21,104232
21,1834624
21,94.62
21,627340
21,2677656
21,66.24
21,802052
21,828024
21,1744
21,17:55:47
21,98584
21,1840272
21,94.92
21,624536
21,2693936
21,66.65
21,808872
21,826932
21,2012
21,17:55:57
21,87088
21,1851768
21,95.51
21,605288
21,2706392
21,66.95
21,827260
21,821304
21,1588
21,17:56:07
21,86268
21,1852588
21,95.55
21,599240
21,2739224
21,67.77
21,829764
21,820888
21,3036
21,17:56:17
21,104260
21,1834596
21,94.62
21,599864
21,2730688
21,67.56
21,811284
21,821584
21,3164
21,Average:
21,96086
21,1842770
21,95.04
21,611254
21,2709579
21,67.03
21,815846
21,823746
21,2309
21,The columns kbcommit and %commit
21,show an approximation of the maximum amount of memory (RAM and swap)
21,that the current workload could need. While
21,"kbcommit displays the absolute number in kilobytes,"
21,%commit displays a percentage.
21,2.1.3.1.3 Paging statistics report: sar -B #
21,Use the option -B to display the kernel paging
21,statistics.
21,# sar -B 10 5
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,18:23:01 pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff
21,18:23:11
21,366.80
21,11.60
21,542.50
21,1.10
21,4354.80
21,0.00
21,0.00
21,0.00
21,0.00
21,18:23:21
21,0.00
21,333.30 1522.40
21,0.00 18132.40
21,0.00
21,0.00
21,0.00
21,0.00
21,18:23:31
21,47.20
21,127.40 1048.30
21,0.10 11887.30
21,0.00
21,0.00
21,0.00
21,0.00
21,18:23:41
21,46.40
21,2.50
21,336.10
21,0.10
21,7945.00
21,0.00
21,0.00
21,0.00
21,0.00
21,18:23:51
21,0.00
21,583.70 2037.20
21,0.00 17731.90
21,0.00
21,0.00
21,0.00
21,0.00
21,Average:
21,92.08
21,211.70 1097.30
21,0.26 12010.28
21,0.00
21,0.00
21,0.00
21,0.00
21,The majflt/s (major faults per second) column shows
21,how many pages are loaded from disk into memory. The source of the
21,"faults may be file accesses or faults. At times, many"
21,"major faults are normal. For example, during application start-up"
21,time. If major faults are experienced for the entire lifetime of the
21,application it may be an indication that there is insufficient main
21,"memory, particularly if combined with large amounts of direct scanning"
21,(pgscand/s).
21,The %vmeff column shows the number of pages scanned
21,(pgscand/s) in relation to the ones being reused
21,from the main memory cache or the swap cache
21,(pgsteal/s). It is a measurement of the efficiency
21,of page reclaim. Healthy values are either near 100 (every inactive
21,page swapped out is being reused) or 0 (no pages have been scanned).
21,The value should not drop below 30.
21,2.1.3.1.4 Block device statistics report: sar -d #
21,Use the option -d to display the block device (hard
21,"disk, optical drive, USB storage device, etc.). Make sure to use the"
21,additional option -p (pretty-print) to make the
21,DEV column readable.
21,# sar -d -p 10 5
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,18:46:09 DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,18:46:19 sda
21,1.70
21,33.60
21,0.00
21,19.76
21,0.00
21,0.47
21,0.47
21,0.08
21,18:46:19 sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,18:46:19 DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,18:46:29 sda
21,8.60
21,114.40
21,518.10
21,73.55
21,0.06
21,7.12
21,0.93
21,0.80
21,18:46:29 sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,18:46:29 DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,18:46:39 sda 40.50
21,3800.80
21,454.90
21,105.08
21,0.36
21,8.86
21,0.69
21,2.80
21,18:46:39 sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,18:46:39 DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,18:46:49 sda
21,1.40
21,0.00
21,204.90
21,146.36
21,0.00
21,0.29
21,0.29
21,0.04
21,18:46:49 sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,18:46:49 DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,18:46:59 sda
21,3.30
21,0.00
21,503.80
21,152.67
21,0.03
21,8.12
21,1.70
21,0.56
21,18:46:59 sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,Average: DEV
21,tps rd_sec/s
21,wr_sec/s
21,avgrq-sz
21,avgqu-sz
21,await
21,svctm
21,%util
21,Average: sda 11.10
21,789.76
21,336.34
21,101.45
21,0.09
21,8.07
21,0.77
21,0.86
21,Average: sr0
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,0.00
21,Compare the Average values for
21,"tps, rd_sec/s, and"
21,wr_sec/s of all disks. Constantly high values in
21,the svctm and %util columns
21,could be an indication that I/O subsystem is a bottleneck.
21,"If the machine uses multiple disks, then it is best if I/O is"
21,interleaved evenly between disks of equal speed and capacity. It will
21,be necessary to take into account whether the storage has multiple
21,"tiers. Furthermore, if there are multiple paths to storage then"
21,consider what the link saturation will be when balancing how storage
21,is used.
21,2.1.3.1.5 Network statistics reports: sar -n KEYWORD #
21,The option -n lets you generate multiple network
21,related reports. Specify one of the following keywords along with the
21,-n:
21,DEV: Generates a statistic report for all
21,network devices
21,EDEV: Generates an error statistics report for
21,all network devices
21,NFS: Generates a statistic report for an NFS
21,client
21,NFSD: Generates a statistic report for an NFS
21,server
21,SOCK: Generates a statistic report on sockets
21,ALL: Generates all network statistic reports
21,2.1.3.2 Visualizing sar data #
21,sar reports are not always easy to parse for humans.
21,"kSar, a Java application visualizing your sar data,"
21,creates easy-to-read graphs. It can even generate PDF reports. kSar
21,"takes data generated in real time, and past data from a file. kSar"
21,is licensed under the BSD license and is available from
21,https://sourceforge.net/projects/ksar/.
21,2.2 System information #  2.2.1 Device load information: iostat #
21,"To monitor the system device load, use iostat. It"
21,generates reports that can be useful for better balancing the load
21,between physical disks attached to your system.
21,"To be able to use iostat, install the package"
21,sysstat.
21,The first iostat report shows statistics collected
21,since the system was booted. Subsequent reports cover the time since the
21,previous report.
21,> iostat
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(4 CPU)
21,avg-cpu:
21,%user
21,%nice %system %iowait
21,%steal
21,%idle
21,17.68
21,4.49
21,4.24
21,0.29
21,0.00
21,73.31
21,Device:
21,tps
21,kB_read/s
21,kB_wrtn/s
21,kB_read
21,kB_wrtn
21,sdb
21,2.02
21,36.74
21,45.73
21,3544894
21,4412392
21,sda
21,1.05
21,5.12
21,13.47
21,493753
21,1300276
21,sdc
21,0.02
21,0.14
21,0.00
21,13641
21,Invoking iostat in this way will help you find out
21,"whether throughput is different from your expectation, but not why."
21,Such questions can be better answered by an extended report which can be
21,generated by invoking iostat -x.
21,"Extended reports additionally include, for example, information on average"
21,queue sizes and average wait times.
21,It may also be easier to evaluate the data if idle block devices are
21,excluded using the -z switch.
21,Find definitions for each of the displayed column titles in the
21,man page of iostat (man 1 iostat).
21,You can also specify that a certain device should be monitored at specified
21,intervals.
21,"For example, to generate five reports at three-second intervals for the"
21,"device sda, use:"
21,> iostat -p sda 3 5
21,"To show statistics of network file systems (NFS), there are two similar"
21,utilities:
21,nfsiostat-sysstat is included with the
21,package sysstat.
21,nfsiostat is included with the package
21,nfs-client.
21,Note: Using iostat in multipath setups
21,The iostat command might not show all controllers
21,"that are listed by nvme list-subsys. By default,"
21,iostat filters out all block devices with no I/O.
21,To make iostat show all
21,"devices, use the following command:"
21,iostat -p ALL2.2.2 Processor activity monitoring: mpstat #
21,The utility mpstat examines activities of each
21,"available processor. If your system has one processor only, the global"
21,average statistics will be reported.
21,The timing arguments work the same way as with the
21,iostat command. Entering mpstat 2
21,5 prints five reports for all processors in two-second
21,intervals.
21,# mpstat 2 5
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,13:51:10
21,CPU
21,%usr
21,%nice
21,%sys
21,%iowait
21,%irq
21,%soft
21,%steal
21,%guest
21,%gnice
21,%idle
21,13:51:12
21,all
21,"8,27"
21,"0,00"
21,"0,50"
21,"0,00"
21,"0,00"
21,"0,00"
21,"0,00"
21,"0,00"
21,"0,00"
21,"91,23"
21,13:51:14
21,all
21,"46,62"
21,"0,00"
21,"3,01"
21,"0,00"
21,"0,00"
21,"0,25"
21,"0,00"
21,"0,00"
21,"0,00"
21,"50,13"
21,13:51:16
21,all
21,"54,71"
21,"0,00"
21,"3,82"
21,"0,00"
21,"0,00"
21,"0,51"
21,"0,00"
21,"0,00"
21,"0,00"
21,"40,97"
21,13:51:18
21,all
21,"78,77"
21,"0,00"
21,"5,12"
21,"0,00"
21,"0,00"
21,"0,77"
21,"0,00"
21,"0,00"
21,"0,00"
21,"15,35"
21,13:51:20
21,all
21,"51,65"
21,"0,00"
21,"4,30"
21,"0,00"
21,"0,00"
21,"0,51"
21,"0,00"
21,"0,00"
21,"0,00"
21,"43,54"
21,Average:
21,all
21,"47,85"
21,"0,00"
21,"3,34"
21,"0,00"
21,"0,00"
21,"0,40"
21,"0,00"
21,"0,00"
21,"0,00"
21,"48,41"
21,"From the mpstat data, you can see:"
21,The ratio between the %usr and %sys.
21,"For example, a ratio"
21,of 10:1 indicates the workload is mostly running application code
21,and analysis should focus on the application. A ratio of 1:10
21,indicates the workload is mostly kernel-bound and tuning the kernel
21,"is worth considering. Alternatively, determine why the application is"
21,kernel-bound and see if that can be alleviated.
21,Whether there is a subset of CPUs that are nearly fully
21,utilized even if the system is lightly loaded overall. Few
21,hot CPUs can indicate that the workload is not parallelized and
21,could benefit from executing on a machine with a smaller number of
21,faster processors.
21,2.2.3 Processor frequency monitoring: turbostat #
21,"turbostat shows frequencies, load, temperature, and power"
21,of AMD64/Intel 64 processors. It can operate in two modes: If called
21,"with a command, the command process is forked and statistics are displayed"
21,"upon command completion. When run without a command, it will display"
21,updated statistics every five seconds. Note that
21,turbostat requires the kernel module
21,msr to be loaded.
21,> sudo turbostat find /etc -type d -exec true {} \;
21,0.546880 sec
21,CPU Avg_MHz
21,Busy% Bzy_MHz TSC_MHz
21,416
21,28.43
21,1465
21,3215
21,631
21,37.29
21,1691
21,3215
21,416
21,27.14
21,1534
21,3215
21,270
21,24.30
21,1113
21,3215
21,406
21,26.57
21,1530
21,3214
21,505
21,32.46
21,1556
21,3214
21,270
21,22.79
21,1184
21,3214
21,The output depends on the CPU type and may vary. To display more details
21,"such as temperature and power, use the --debug option. For"
21,"more command line options and an explanation of the field descriptions,"
21,refer to man 8 turbostat.
21,2.2.4 Task monitoring: pidstat #
21,"If you need to see what load a particular task applies to your system,"
21,use pidstat command. It prints activity of every
21,selected task or all tasks managed by Linux kernel if no task is
21,specified. You can also set the number of reports to be displayed and
21,the time interval between them.
21,"For example, pidstat -C firefox 2 3"
21,prints the load statistic for tasks whose command name includes the
21,string “firefox”. There will be three reports printed at
21,two second intervals.
21,# pidstat -C firefox 2 3
21,Linux 4.4.21-64-default (jupiter)
21,10/12/16
21,_x86_64_
21,(2 CPU)
21,14:09:11
21,UID
21,PID
21,%usr %system
21,%guest
21,%CPU
21,CPU
21,Command
21,14:09:13
21,1000
21,387
21,"22,77"
21,"0,99"
21,"0,00"
21,"23,76"
21,firefox
21,14:09:13
21,UID
21,PID
21,%usr %system
21,%guest
21,%CPU
21,CPU
21,Command
21,14:09:15
21,1000
21,387
21,"46,50"
21,"3,00"
21,"0,00"
21,"49,50"
21,firefox
21,14:09:15
21,UID
21,PID
21,%usr %system
21,%guest
21,%CPU
21,CPU
21,Command
21,14:09:17
21,1000
21,387
21,"60,50"
21,"7,00"
21,"0,00"
21,"67,50"
21,firefox
21,Average:
21,UID
21,PID
21,%usr %system
21,%guest
21,%CPU
21,CPU
21,Command
21,Average:
21,1000
21,387
21,"43,19"
21,"3,65"
21,"0,00"
21,"46,84"
21,firefox
21,"Similarly, pidstat -d can be"
21,"used to estimate how much I/O tasks are doing, whether they are"
21,sleeping on that I/O and how many clock ticks the task was stalled.
21,2.2.5 Kernel ring buffer: dmesg #
21,The Linux kernel keeps certain messages in a ring buffer. To view these
21,"messages, enter the command dmesg -T."
21,Older events are logged in the systemd journal. See
21,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal” for more information on the journal."
21,2.2.6 List of open files: lsof #
21,To view a list of all the files open for the process with process ID
21,"PID, use -p. For example, to"
21,"view all the files used by the current shell, enter:"
21,# lsof -p $$
21,COMMAND
21,PID USER
21,TYPE DEVICE SIZE/OFF
21,NODE NAME
21,bash
21,8842 root
21,cwd
21,DIR
21,"0,32"
21,222
21,6772 /root
21,bash
21,8842 root
21,rtd
21,DIR
21,"0,32"
21,166
21,256 /
21,bash
21,8842 root
21,txt
21,REG
21,"0,32"
21,656584 31066 /bin/bash
21,bash
21,8842 root
21,mem
21,REG
21,"0,32"
21,1978832 22993 /lib64/libc-2.19.so
21,[...]
21,bash
21,8842 root
21,CHR
21,"136,2"
21,0t0
21,5 /dev/pts/2
21,bash
21,8842 root
21,255u
21,CHR
21,"136,2"
21,0t0
21,5 /dev/pts/2
21,"The special shell variable $$, whose value is the"
21,"process ID of the shell, has been used."
21,"When used with -i, lsof lists"
21,currently open Internet files as well:
21,# lsof -i
21,COMMAND
21,PID USER
21,TYPE DEVICE SIZE/OFF NODE NAME
21,wickedd-d
21,917 root
21,IPv4
21,16627
21,0t0
21,UDP *:bootpc
21,wickedd-d
21,918 root
21,IPv6
21,20752
21,0t0
21,UDP [fe80::5054:ff:fe72:5ead]:dhcpv6-client
21,sshd
21,3152 root
21,IPv4
21,18618
21,0t0
21,TCP *:ssh (LISTEN)
21,sshd
21,3152 root
21,IPv6
21,18620
21,0t0
21,TCP *:ssh (LISTEN)
21,master
21,4746 root
21,13u
21,IPv4
21,20588
21,0t0
21,TCP localhost:smtp (LISTEN)
21,master
21,4746 root
21,14u
21,IPv6
21,20589
21,0t0
21,TCP localhost:smtp (LISTEN)
21,sshd
21,8837 root
21,IPv4 293709
21,0t0
21,TCP jupiter.suse.de:ssh->venus.suse.de:33619 (ESTABLISHED)
21,sshd
21,8837 root
21,IPv6 294830
21,0t0
21,TCP localhost:x11 (LISTEN)
21,sshd
21,8837 root
21,10u
21,IPv4 294831
21,0t0
21,TCP localhost:x11 (LISTEN)2.2.7 Kernel and udev event sequence viewer: udevadm monitor #
21,udevadm monitor listens to the kernel uevents and
21,events sent out by a udev rule and prints the device path (DEVPATH) of
21,the event to the console. This is a sequence of events while connecting
21,a USB memory stick:
21,Note: Monitoring udev events
21,Only root user is allowed to monitor udev events by running the
21,udevadm command.
21,UEVENT[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2
21,UEVENT[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
21,UEVENT[1138806687] add@/class/scsi_host/host4
21,UEVENT[1138806687] add@/class/usb_device/usbdev4.10
21,UDEV
21,[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2
21,UDEV
21,[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
21,UDEV
21,[1138806687] add@/class/scsi_host/host4
21,UDEV
21,[1138806687] add@/class/usb_device/usbdev4.10
21,UEVENT[1138806692] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
21,UEVENT[1138806692] add@/block/sdb
21,UEVENT[1138806692] add@/class/scsi_generic/sg1
21,UEVENT[1138806692] add@/class/scsi_device/4:0:0:0
21,UDEV
21,[1138806693] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
21,UDEV
21,[1138806693] add@/class/scsi_generic/sg1
21,UDEV
21,[1138806693] add@/class/scsi_device/4:0:0:0
21,UDEV
21,[1138806693] add@/block/sdb
21,UEVENT[1138806694] add@/block/sdb/sdb1
21,UDEV
21,[1138806694] add@/block/sdb/sdb1
21,UEVENT[1138806694] mount@/block/sdb/sdb1
21,UEVENT[1138806697] umount@/block/sdb/sdb12.3 Processes #  2.3.1 Interprocess communication: ipcs #
21,The command ipcs produces a list of the IPC resources
21,currently in use:
21,# ipcs
21,------ Message Queues --------
21,key
21,msqid
21,owner
21,perms
21,used-bytes
21,messages
21,------ Shared Memory Segments --------
21,key
21,shmid
21,owner
21,perms
21,bytes
21,nattch
21,status
21,0x00000000 65536
21,tux
21,600
21,524288
21,dest
21,0x00000000 98305
21,tux
21,600
21,4194304
21,dest
21,0x00000000 884738
21,root
21,600
21,524288
21,dest
21,0x00000000 786435
21,tux
21,600
21,4194304
21,dest
21,0x00000000 12058628
21,tux
21,600
21,524288
21,dest
21,0x00000000 917509
21,root
21,600
21,524288
21,dest
21,0x00000000 12353542
21,tux
21,600
21,196608
21,dest
21,0x00000000 12451847
21,tux
21,600
21,524288
21,dest
21,0x00000000 11567114
21,root
21,600
21,262144
21,dest
21,0x00000000 10911763
21,tux
21,600
21,2097152
21,dest
21,0x00000000 11665429
21,root
21,600
21,2336768
21,dest
21,0x00000000 11698198
21,root
21,600
21,196608
21,dest
21,0x00000000 11730967
21,root
21,600
21,524288
21,dest
21,------ Semaphore Arrays --------
21,key
21,semid
21,owner
21,perms
21,nsems
21,0xa12e0919 32768
21,tux
21,666
21,22.3.2 Process list: ps #
21,The command ps produces a list of processes. Most
21,parameters must be written without a minus sign. Refer to ps
21,--help for a brief help or to the man page for extensive help.
21,"To list all processes with user and command line information, use"
21,ps axu:
21,> ps axu
21,USER
21,PID %CPU %MEM
21,VSZ
21,RSS TTY
21,STAT START
21,TIME COMMAND
21,root
21,0.0
21,0.3
21,34376
21,4608 ?
21,Jul24
21,0:02 /usr/lib/systemd/systemd
21,root
21,0.0
21,0.0
21,0 ?
21,Jul24
21,0:00 [kthreadd]
21,root
21,0.0
21,0.0
21,0 ?
21,Jul24
21,0:00 [ksoftirqd/0]
21,root
21,0.0
21,0.0
21,0 ?
21,Jul24
21,0:00 [kworker/0:0H]
21,root
21,0.0
21,0.0
21,0 ?
21,Jul24
21,0:00 [kworker/u2:0]
21,root
21,0.0
21,0.0
21,0 ?
21,Jul24
21,0:00 [migration/0]
21,[...]
21,tux
21,12583
21,0.0
21,0.1 185980
21,2720 ?
21,10:12
21,0:00 /usr/lib/gvfs/gvfs-mtp-volume-monitor
21,tux
21,12587
21,0.0
21,0.1 198132
21,3044 ?
21,10:12
21,0:00 /usr/lib/gvfs/gvfs-gphoto2-volume-monitor
21,tux
21,12591
21,0.0
21,0.1 181940
21,2700 ?
21,10:12
21,0:00 /usr/lib/gvfs/gvfs-goa-volume-monitor
21,tux
21,12594
21,8.1 10.6 1418216 163564 ?
21,10:12
21,0:03 /usr/bin/gnome-shell
21,tux
21,12600
21,0.0
21,0.3 393448
21,5972 ?
21,10:12
21,0:00 /usr/lib/gnome-settings-daemon-3.0/gsd-printer
21,tux
21,12625
21,0.0
21,0.6 227776 10112 ?
21,10:12
21,0:00 /usr/lib/gnome-control-center-search-provider
21,tux
21,12626
21,0.5
21,1.5 890972 23540 ?
21,10:12
21,0:00 /usr/bin/nautilus --no-default-window
21,[...]
21,"To check how many sshd processes are running, use the"
21,option -p together with the command
21,"pidof, which lists the process IDs of the given"
21,processes.
21,> ps -p $(pidof sshd)
21,PID TTY
21,STAT
21,TIME COMMAND
21,1545 ?
21,0:00 /usr/sbin/sshd -D
21,4608 ?
21,0:00 sshd: root@pts/0
21,The process list can be formatted according to your needs. The option
21,L returns a list of all keywords. Enter the following
21,command to issue a list of all processes sorted by memory usage:
21,"> ps ax --format pid,rss,cmd --sort rss"
21,PID
21,RSS CMD
21,PID
21,RSS CMD
21,0 [kthreadd]
21,0 [ksoftirqd/0]
21,0 [kworker/0:0]
21,0 [kworker/0:0H]
21,0 [kworker/u2:0]
21,0 [migration/0]
21,0 [rcu_bh]
21,[...]
21,12518 22996 /usr/lib/gnome-settings-daemon-3.0/gnome-settings-daemon
21,12626 23540 /usr/bin/nautilus --no-default-window
21,12305 32188 /usr/bin/Xorg :0 -background none -verbose
21,12594 164900 /usr/bin/gnome-shellUseful ps calls #  ps aux--sort
21,COLUMN
21,Sort the output by COLUMN. Replace
21,COLUMN with
21,pmem for physical memory ratiopcpu for CPU ratiorss for resident set size (non-swapped physical
21,"memory)ps axo pid,%cpu,rss,vsz,args,wchan"
21,"Shows every process, their PID, CPU usage ratio, memory size"
21,"(resident and virtual), name, and their syscall."
21,"ps axfo pid,args"
21,Show a process tree.
21,2.3.3 Process tree: pstree #
21,The command pstree produces a list of processes in
21,the form of a tree:
21,> pstree
21,systemd---accounts-daemon---{gdbus}
21,|-{gmain}
21,|-at-spi-bus-laun---dbus-daemon
21,|-{dconf worker}
21,|-{gdbus}
21,|-{gmain}
21,|-at-spi2-registr---{gdbus}
21,|-cron
21,|-2*[dbus-daemon]
21,|-dbus-launch
21,|-dconf-service---{gdbus}
21,|-{gmain}
21,|-gconfd-2
21,|-gdm---gdm-simple-slav---Xorg
21,|-gdm-session-wor---gnome-session---gnome-setti+
21,|-gnome-shell+++
21,|-{dconf work+
21,|-{gdbus}
21,|-{gmain}
21,|-{gdbus}
21,|-{gmain}
21,|-{gdbus}
21,|-{gmain}
21,|-{gdbus}
21,|-{gmain}
21,[...]
21,The parameter -p adds the process ID to a given name.
21,"To have the command lines displayed as well, use the -a"
21,parameter:
21,2.3.4 Table of processes: top #
21,The command top (an abbreviation of “table of
21,processes”) displays a list of processes that is refreshed every
21,"two seconds. To terminate the program, press q. The"
21,parameter -n 1 terminates the program after a single
21,display of the process list. The following is an example output of the
21,command top -n 1:
21,> top -n 1
21,"Tasks: 128 total,"
21,"1 running, 127 sleeping,"
21,"0 stopped,"
21,0 zombie
21,%Cpu(s):
21,"2.4 us,"
21,"1.2 sy,"
21,"0.0 ni, 96.3 id,"
21,"0.1 wa,"
21,"0.0 hi,"
21,"0.0 si,"
21,0.0 st
21,KiB Mem:
21,"1535508 total,"
21,"699948 used,"
21,"835560 free,"
21,880 buffers
21,KiB Swap:
21,"1541116 total,"
21,"0 used,"
21,1541116 free.
21,377000 cached Mem
21,PID USER
21,VIRT
21,RES
21,SHR S
21,%CPU
21,%MEM
21,TIME+ COMMAND
21,1 root
21,116292
21,4660
21,2028 S 0.000 0.303
21,0:04.45 systemd
21,2 root
21,0 S 0.000 0.000
21,0:00.00 kthreadd
21,3 root
21,0 S 0.000 0.000
21,0:00.07 ksoftirqd+
21,5 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 kworker/0+
21,6 root
21,0 S 0.000 0.000
21,0:00.00 kworker/u+
21,7 root
21,0 S 0.000 0.000
21,0:00.00 migration+
21,8 root
21,0 S 0.000 0.000
21,0:00.00 rcu_bh
21,9 root
21,0 S 0.000 0.000
21,0:00.24 rcu_sched
21,10 root
21,0 S 0.000 0.000
21,0:00.01 watchdog/0
21,11 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 khelper
21,12 root
21,0 S 0.000 0.000
21,0:00.00 kdevtmpfs
21,13 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 netns
21,14 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 writeback
21,15 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 kintegrit+
21,16 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 bioset
21,17 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 crypto
21,18 root
21,0 -20
21,0 S 0.000 0.000
21,0:00.00 kblockd
21,By default the output is sorted by CPU usage (column
21,"%CPU, shortcut Shift–P). Use the following key combinations to"
21,change the sort field:
21,Shift–M: Resident Memory (RES) Shift–N: Process ID (PID) Shift–T: Time (TIME+)
21,"To use any other field for sorting, press F and select"
21,"a field from the list. To toggle the sort order, Use Shift–R."
21,The parameter -U UID
21,monitors only the processes associated with a particular user. Replace
21,UID with the user ID of the user. Use
21,top -U $(id -u) to show processes of the current user
21,2.3.5 IBM Z hypervisor monitor: hyptop #
21,hyptop provides a dynamic real-time view of an
21,"IBM Z hypervisor environment, using the kernel infrastructure via"
21,debugfs. It works with either the z/VM or the LPAR hypervisor. Depending
21,"on the available data it, for example, shows CPU and memory consumption"
21,of active LPARs or z/VM guests. It provides a curses based user
21,interface similar to the top command.
21,hyptop provides two windows:
21,sys_list: Lists systems that the
21,current hypervisor is running
21,sys: Shows one system in more detail
21,You can run hyptop in interactive mode (default) or
21,in batch mode with the -b option. Help in the
21,interactive mode is available by pressing ? after
21,hyptop is started.
21,Output for the sys_list window under LPAR:
21,12:30:48 | CPU-T: IFL(18) CP(3) UN(3)
21,?=help
21,system
21,#cpu
21,cpu
21,mgm
21,Cpu+
21,Mgm+
21,online
21,(str)
21,(#)
21,(%)
21,(%)
21,(hm)
21,(hm)
21,(dhm)
21,H05LP30
21,10 461.14 10.18 1547:41
21,8:15 11:05:59
21,H05LP33
21,4 133.73
21,7.57
21,220:53
21,6:12 11:05:54
21,H05LP50
21,99.26
21,0.01
21,146:24
21,0:12 10:04:24
21,H05LP02
21,99.09
21,0.00
21,269:57
21,0:00 11:05:58
21,TRX2CFA
21,2.14
21,0.03
21,3:24
21,0:04 11:06:01
21,H05LP13
21,1.36
21,0.34
21,4:23
21,0:54 11:05:56
21,TRX1
21,1.22
21,0.14
21,13:57
21,0:22 11:06:01
21,TRX2
21,1.16
21,0.11
21,26:05
21,0:25 11:06:00
21,H05LP55
21,0.00
21,0.00
21,0:22
21,0:00 11:05:52
21,H05LP56
21,0.00
21,0.00
21,0:00
21,0:00 11:05:52
21,413 823.39 23.86 3159:57 38:08 11:06:01
21,"Output for the ""sys_list"" window under z/VM:"
21,12:32:21 | CPU-T: UN(16)
21,?=help
21,system
21,#cpu
21,cpu
21,Cpu+
21,online memuse memmax wcur
21,(str)
21,(#)
21,(%)
21,(hm)
21,(dhm)
21,(GiB)
21,(GiB)
21,(#)
21,T6360004
21,6 100.31
21,959:47 53:05:20
21,1.56
21,2.00
21,100
21,T6360005
21,0.44
21,1:11
21,3:02:26
21,0.42
21,0.50
21,100
21,T6360014
21,0.27
21,0:45 10:18:41
21,0.54
21,0.75
21,100
21,DTCVSW1
21,0.00
21,0:00 53:16:42
21,0.01
21,0.03
21,100
21,T6360002
21,0.00
21,166:26 40:19:18
21,1.87
21,2.00
21,100
21,OPERATOR
21,0.00
21,0:00 53:16:42
21,0.00
21,0.03
21,100
21,T6360008
21,0.00
21,0:37 30:22:55
21,0.32
21,0.75
21,100
21,T6360003
21,0.00 3700:57 53:03:09
21,4.00
21,4.00
21,100
21,NSLCF1
21,0.00
21,0:02 53:16:41
21,0.03
21,0.25
21,500
21,EREP
21,0.00
21,0:00 53:16:42
21,0.00
21,0.03
21,100
21,PERFSVM
21,0.00
21,0:53
21,2:21:12
21,0.04
21,0.06
21,TCPIP
21,0.00
21,0:01 53:16:42
21,0.01
21,0.12 3000
21,DATAMOVE
21,0.00
21,0:05 53:16:42
21,0.00
21,0.03
21,100
21,DIRMAINT
21,0.00
21,0:04 53:16:42
21,0.01
21,0.03
21,100
21,DTCVSW2
21,0.00
21,0:00 53:16:42
21,0.01
21,0.03
21,100
21,RACFVM
21,0.00
21,0:00 53:16:42
21,0.01
21,0.02
21,100
21,75 101.57 5239:47 53:16:42
21,15.46
21,22.50 3000
21,Output for the sys window under LPAR:
21,14:08:41 | H05LP30 | CPU-T: IFL(18) CP(3) UN(3)
21,? = help
21,cpuid
21,type
21,cpu
21,mgm visual.
21,(#)
21,(str)
21,(%)
21,(%) (vis)
21,IFL
21,96.91
21,1.96 |############################################ |
21,IFL
21,81.82
21,1.46 |#####################################
21,IFL
21,88.00
21,2.43 |########################################
21,IFL
21,92.27
21,1.29 |##########################################
21,IFL
21,83.32
21,1.05 |#####################################
21,IFL
21,92.46
21,2.59 |##########################################
21,IFL
21,0.00
21,0.00 |
21,IFL
21,0.00
21,0.00 |
21,IFL
21,0.00
21,0.00 |
21,IFL
21,0.00
21,0.00 |
21,534.79 10.78
21,Output for the sys window under z/VM:
21,15:46:57 | T6360003 | CPU-T: UN(16)
21,? = help
21,cpuid
21,cpu visual
21,(#)
21,(%) (vis)
21,548.72 |#########################################
21,548.722.3.6 A top-like I/O monitor: iotop #
21,The iotop utility displays a table of I/O usage by
21,processes or threads.
21,Note: Installing iotop
21,iotop is not installed by default. You need to
21,install it manually with zypper in iotop as
21,root.
21,iotop displays columns for the I/O bandwidth read and
21,written by each process during the sampling period. It also displays the
21,percentage of time the process spent while swapping in and while waiting
21,"on I/O. For each process, its I/O priority (class/level) is shown. In"
21,"addition, the total I/O bandwidth read and written during the sampling"
21,period is displayed at the top of the interface.
21,The ← and → keys
21,change the sorting.
21,R reverses the sort order.
21,O toggles between showing all processes and threads
21,(default view) and showing only those doing I/O. (This function is
21,similar to adding --only on command line.)
21,P toggles between showing threads (default view) and
21,processes. (This function is similar to --only.)
21,A toggles between showing the current I/O bandwidth
21,(default view) and accumulated I/O operations since
21,iotop was started. (This function is similar to
21,--accumulated.)
21,I lets you change the priority of a thread or a
21,process's threads.
21,Q quits iotop.
21,Pressing any other key will force a refresh.
21,Following is an example output of the command iotop
21,"--only, while find and"
21,emacs are running:
21,# iotop --only
21,Total DISK READ: 50.61 K/s | Total DISK WRITE: 11.68 K/s
21,TID
21,PRIO
21,USER
21,DISK READ
21,DISK WRITE
21,SWAPIN
21,IO>
21,COMMAND
21,3416 be/4 tux
21,50.61 K/s
21,0.00 B/s
21,0.00 %
21,4.05 % find /
21,275 be/3 root
21,0.00 B/s
21,3.89 K/s
21,0.00 %
21,2.34 % [jbd2/sda2-8]
21,5055 be/4 tux
21,0.00 B/s
21,3.89 K/s
21,0.00 %
21,0.04 % emacs
21,iotop can be also used in a batch mode
21,(-b) and its output stored in a file for later
21,"analysis. For a complete set of options, see the manual page"
21,(man 8 iotop).
21,2.3.7 Modify a process's niceness: nice and renice #
21,The kernel determines which processes require more CPU time than others
21,"by the process's nice level, also called niceness. The higher the"
21,"“nice” level of a process is, the less CPU time it will"
21,take from other processes. Nice levels range from -20 (the least
21,“nice” level) to 19. Negative values can only be set by
21,root.
21,Adjusting the niceness level is useful when running a non time-critical
21,"process that lasts long and uses large amounts of CPU time. For example,"
21,compiling a kernel on a system that also performs other tasks. Making
21,"such a process “nicer”, ensures that the other tasks, for"
21,"example a Web server, will have a higher priority."
21,Calling nice without any parameters prints the
21,current niceness:
21,> nice
21,Running nice COMMAND
21,increments the current nice level for the given command by 10. Using
21,nice -n
21,LEVEL
21,COMMAND lets you specify a new niceness
21,relative to the current one.
21,"To change the niceness of a running process, use"
21,renice PRIORITY -p
21,"PROCESS_ID, for example:"
21,> renice +5 3266
21,"To renice all processes owned by a specific user, use the option"
21,-u USER.
21,Process groups are reniced by the option -g PROCESS_GROUP_ID.
21,2.4 Memory #  2.4.1 Memory usage: free #
21,The utility free examines RAM and swap usage. Details
21,of both free and used memory and swap areas are shown:
21,> free
21,total
21,used
21,free
21,shared
21,buffers
21,cached
21,Mem:
21,32900500
21,32703448
21,197052
21,255668
21,5787364
21,-/+ buffers/cache:
21,26660416
21,6240084
21,Swap:
21,2046972
21,304680
21,1742292
21,"The options -b, -k,"
21,"-m, -g show the output in bytes, KB,"
21,"MB, or GB, respectively. The parameter -s delay ensures"
21,that the display is refreshed every DELAY
21,"seconds. For example, free -s 1.5 produces an update"
21,every 1.5 seconds.
21,2.4.2 Detailed memory usage: /proc/meminfo #
21,Use /proc/meminfo to get more detailed information
21,on memory usage than with free. Actually
21,free uses some data from this file. See an
21,example output from a 64-bit system below. Note that it slightly differs
21,on 32-bit systems because of different memory management:
21,MemTotal:
21,1942636 kB
21,MemFree:
21,1294352 kB
21,MemAvailable:
21,1458744 kB
21,Buffers:
21,876 kB
21,Cached:
21,278476 kB
21,SwapCached:
21,0 kB
21,Active:
21,368328 kB
21,Inactive:
21,199368 kB
21,Active(anon):
21,288968 kB
21,Inactive(anon):
21,10568 kB
21,Active(file):
21,79360 kB
21,Inactive(file):
21,188800 kB
21,Unevictable:
21,80 kB
21,Mlocked:
21,80 kB
21,SwapTotal:
21,2103292 kB
21,SwapFree:
21,2103292 kB
21,Dirty:
21,44 kB
21,Writeback:
21,0 kB
21,AnonPages:
21,288592 kB
21,Mapped:
21,70444 kB
21,Shmem:
21,11192 kB
21,Slab:
21,40916 kB
21,SReclaimable:
21,17712 kB
21,SUnreclaim:
21,23204 kB
21,KernelStack:
21,2000 kB
21,PageTables:
21,10996 kB
21,NFS_Unstable:
21,0 kB
21,Bounce:
21,0 kB
21,WritebackTmp:
21,0 kB
21,CommitLimit:
21,3074608 kB
21,Committed_AS:
21,1407208 kB
21,VmallocTotal:
21,34359738367 kB
21,VmallocUsed:
21,145996 kB
21,VmallocChunk:
21,34359588844 kB
21,HardwareCorrupted:
21,0 kB
21,AnonHugePages:
21,86016 kB
21,HugePages_Total:
21,HugePages_Free:
21,HugePages_Rsvd:
21,HugePages_Surp:
21,Hugepagesize:
21,2048 kB
21,DirectMap4k:
21,79744 kB
21,DirectMap2M:
21,2017280 kB
21,These entries stand for the following:
21,MemTotal
21,Total amount of RAM.
21,MemFree
21,Amount of unused RAM.
21,MemAvailable
21,Estimate of how much memory is available for starting new applications
21,without swapping.
21,Buffers
21,File buffer cache in RAM containing file system metadata.
21,Cached
21,Page cache in RAM.
21,"This excludes buffer cache and swap cache, but includes"
21,Shmem memory.
21,SwapCached
21,Page cache for swapped-out memory.
21,"Active, Active(anon),"
21,Active(file)
21,Recently used memory that will not be reclaimed unless necessary or on
21,explicit request.
21,Active is the sum of Active(anon)
21,and Active(file):
21,Active(anon) tracks swap-backed memory.
21,This includes private and shared anonymous mappings and
21,private file pages after copy-on-write.
21,Active(file) tracks other file system backed
21,memory.
21,"Inactive, Inactive(anon),"
21,Inactive(file)
21,Less recently used memory that will usually be reclaimed first.
21,Inactive is the sum of
21,Inactive(anon) and Inactive(file):
21,Inactive(anon) tracks swap backed memory.
21,This includes private and shared anonymous mappings and
21,private file pages after copy-on-write.
21,Inactive(file) tracks other file system backed
21,memory.
21,Unevictable
21,"Amount of memory that cannot be reclaimed (for example, because it is"
21,Mlocked or used as a RAM disk).
21,Mlocked
21,Amount of memory that is backed by the
21,mlock system call.
21,mlock allows processes to define which part of
21,physical RAM their virtual memory should be mapped to.
21,"However, mlock does not guarantee this"
21,placement.
21,SwapTotal
21,Amount of swap space.
21,SwapFree
21,Amount of unused swap space.
21,Dirty
21,"Amount of memory waiting to be written to disk, because it contains"
21,changes compared to the backing storage. Dirty data can be explicitly
21,synchronized either by the application or by the kernel after a short
21,delay. A large amount of dirty data may take considerable time to write
21,to disk resulting in stalls. The total amount of dirty data that can
21,exist at any time can be controlled with the
21,sysctl parameters vm.dirty_ratio
21,"or vm.dirty_bytes (refer to Section 15.1.5, “Writeback” for more details)."
21,Writeback
21,Amount of memory that is currently being written to disk.
21,Mapped
21,Memory claimed with the mmap system call.
21,Shmem
21,"Memory shared between groups of processes, such as IPC data,"
21,"tmpfs data, and shared anonymous memory."
21,Slab
21,Memory allocation for internal data structures of the kernel.
21,SReclaimable
21,"Slab section that can be reclaimed, such as caches (inode, dentry, etc.)."
21,SUnreclaim
21,Slab section that cannot be reclaimed.
21,KernelStack
21,Amount of kernel space memory used by applications (through system calls).
21,PageTables
21,Amount of memory dedicated to page tables of all processes.
21,NFS_Unstable
21,"NFS pages that have already been sent to the server, but are not yet"
21,committed there.
21,Bounce
21,Memory used for bounce buffers of block devices.
21,WritebackTmp
21,Memory used by FUSE for temporary writeback buffers.
21,CommitLimit
21,Amount of memory available to the system based on the overcommit
21,ratio setting. This is only enforced if strict overcommit accounting
21,is enabled.
21,Committed_AS
21,An approximation of the total amount of memory (RAM and swap) that the
21,current workload would need in the worst case.
21,VmallocTotal
21,Amount of allocated kernel virtual address space.
21,VmallocUsed
21,Amount of used kernel virtual address space.
21,VmallocChunk
21,The largest contiguous block of available kernel virtual address space.
21,HardwareCorrupted
21,Amount of failed memory (can only be detected when using ECC RAM).
21,AnonHugePages
21,Anonymous hugepages that are mapped into user space page tables.
21,These are allocated transparently for processes without being
21,"specifically requested, therefore they are also known as"
21,transparent hugepages (THP).
21,HugePages_Total
21,Number of preallocated hugepages for use by
21,SHM_HUGETLB and
21,MAP_HUGETLB or through the
21,"hugetlbfs file system, as defined in"
21,/proc/sys/vm/nr_hugepages.
21,HugePages_Free
21,Number of hugepages available.
21,HugePages_Rsvd
21,Number of hugepages that are committed.
21,HugePages_Surp
21,Number of hugepages available beyond
21,"HugePages_Total (“surplus”), as defined"
21,in /proc/sys/vm/nr_overcommit_hugepages.
21,Hugepagesize
21,Size of a hugepage—on AMD64/Intel 64 the default is 2048 KB.
21,DirectMap4k etc.
21,Amount of kernel memory that is mapped to pages with a given size (in the
21,example: 4 kB).
21,2.4.3 Process memory usage: smaps #
21,Exactly determining how much memory a certain process is consuming is
21,not possible with standard tools like top or
21,"ps. Use the smaps subsystem, introduced in kernel"
21,"2.6.14, if you need exact data. It can be found at"
21,/proc/PID/smaps and
21,shows you the number of clean and dirty memory pages the process with
21,the ID PID is using at that time. It
21,"differentiates between shared and private memory, so you can see"
21,how much memory the process is using without including memory shared
21,with other processes. For more information see
21,/usr/src/linux/Documentation/filesystems/proc.txt
21,(requires the package
21,kernel-source to be
21,installed).
21,smaps is expensive to read. Therefore it is not recommended to monitor
21,"it regularly, but only when closely monitoring a certain process."
21,2.4.4 numaTOP #
21,numaTOP is a tool for NUMA (Non-uniform Memory Access)
21,systems. The tool helps to identify NUMA-related performance
21,bottlenecks by providing real-time analysis of a NUMA system.
21,"Generally speaking, numaTOP allows you to identify and"
21,investigate processes and threads with poor locality (that is
21,poor ratio of local versus remote memory usage) by analyzing
21,"the number of Remote Memory Accesses (RMA), the number of Local Memory"
21,"Accesses (LMA), and the RMA/LMA ratio."
21,numaTOP is supported on PowerPC and the following Intel Xeon
21,"processors: 5500-series, 6500/7500-series, 5600-series,"
21,"E7-x8xx-series, and E5-16xx/24xx/26xx/46xx-series."
21,"numaTOP is available in the official software repositories, and"
21,you can install the tool using the sudo zypper in
21,"numatop command. To launch numaTOP, run the"
21,numatop command. To get an overview of
21,"numaTOP functionality and usage, use the man"
21,numatop command.
21,2.5 Networking #  Tip: Traffic shaping
21,"In case the network bandwidth is lower than expected, you should first"
21,check if any traffic shaping rules are active for your network segment.
21,2.5.1 Basic network diagnostics: ip #
21,ip is a powerful tool to set up and control network
21,interfaces. You can also use it to quickly view basic statistics about
21,"network interfaces of the system. For example, whether the interface is"
21,"up or how many errors, dropped packets, or packet collisions there are."
21,"If you run ip with no additional parameter, it"
21,"displays a help output. To list all network interfaces, enter"
21,ip addr show (or abbreviated as ip
21,a). ip addr show up lists only running
21,network interfaces. ip -s link show
21,DEVICE lists statistics for the specified
21,interface only:
21,# ip -s link show br0
21,"6: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT"
21,link/ether 00:19:d1:72:d4:30 brd ff:ff:ff:ff:ff:ff
21,RX: bytes
21,packets
21,errors
21,dropped overrun mcast
21,6346104756 9265517
21,10860
21,TX: bytes
21,packets
21,errors
21,dropped carrier collsns
21,3996204683 3655523
21,ip can also show interfaces
21,"(link), routing tables (route), and"
21,much more—refer to man 8 ip for details.
21,# ip route
21,default via 192.168.2.1 dev eth1
21,192.168.2.0/24 dev eth0
21,proto kernel
21,scope link
21,src 192.168.2.100
21,192.168.2.0/24 dev eth1
21,proto kernel
21,scope link
21,src 192.168.2.101
21,192.168.2.0/24 dev eth2
21,proto kernel
21,scope link
21,src 192.168.2.102# ip link
21,"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default"
21,link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
21,"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
21,link/ether 52:54:00:44:30:51 brd ff:ff:ff:ff:ff:ff
21,"3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
21,link/ether 52:54:00:a3:c1:fb brd ff:ff:ff:ff:ff:ff
21,"4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
21,link/ether 52:54:00:32:a4:09 brd ff:ff:ff:ff:ff:ff2.5.2
21,Show the network usage of processes: nethogs
21,"In some cases, for example if the network traffic suddenly becomes very"
21,"high, it is desirable to quickly find out which application(s) is/are"
21,"causing the traffic. nethogs, a tool with a design"
21,"similar to top, shows incoming and outgoing traffic for"
21,all relevant processes:
21,PID
21,USER
21,PROGRAM
21,DEV
21,SENT
21,RECEIVED
21,27145 root
21,zypper
21,eth0
21,5.719
21,391.749 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30015
21,0.102
21,2.326 KB/sec
21,26635 tux
21,/usr/lib64/firefox/firefox
21,eth0
21,0.026
21,0.026 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30045
21,0.000
21,0.021 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30045
21,0.000
21,0.018 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30015
21,0.000
21,0.018 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30045
21,0.000
21,0.017 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30045
21,0.000
21,0.017 KB/sec
21,root
21,..0:113:80c0:8080:10:160:0:100:30045
21,0.069
21,0.000 KB/sec
21,root
21,unknown TCP
21,0.000
21,0.000 KB/sec
21,TOTAL
21,5.916
21,394.192 KB/sec
21,"Like in top, nethogs features"
21,interactive commands:
21,"M: cycle between display modes (kb/s, kb, b, mb)"
21,R: sort by RECEIVED
21,S: sort by SENTQ: quit2.5.3 Ethernet cards in detail: ethtool #
21,ethtool can display and change detailed aspects of
21,your Ethernet network device. By default it prints the current setting
21,of the specified device.
21,# ethtool eth0
21,Settings for eth0:
21,Supported ports: [ TP ]
21,Supported link modes:
21,10baseT/Half 10baseT/Full
21,100baseT/Half 100baseT/Full
21,1000baseT/Full
21,Supports auto-negotiation: Yes
21,Advertised link modes:
21,10baseT/Half 10baseT/Full
21,100baseT/Half 100baseT/Full
21,1000baseT/Full
21,Advertised pause frame use: No
21,[...]
21,Link detected: yes
21,The following table shows ethtool options that you
21,can use to query the device for specific information:
21,Table 2.1: List of query options of ethtool #
21,ethtool option
21,it queries the device for
21,pause parameter information
21,interrupt coalescing information
21,Rx/Tx (receive/transmit) ring parameter information
21,associated driver information
21,offload information
21,NIC and driver-specific statistics
21,2.5.4 Show the network status: ss #
21,ss is a tool to dump socket statistics and replaces
21,the netstat command. To list all
21,connections use ss without parameters:
21,# ss
21,Netid
21,State
21,Recv-Q Send-Q
21,Local Address:Port
21,Peer Address:Port
21,u_str
21,ESTAB
21,* 14082
21,* 14083
21,u_str
21,ESTAB
21,* 18582
21,* 18583
21,u_str
21,ESTAB
21,* 19449
21,* 19450
21,u_str
21,ESTAB
21,@/tmp/dbus-gmUUwXABPV 18784
21,* 18783
21,u_str
21,ESTAB
21,/var/run/dbus/system_bus_socket 19383 * 19382
21,u_str
21,ESTAB
21,@/tmp/dbus-gmUUwXABPV 18617
21,* 18616
21,u_str
21,ESTAB
21,@/tmp/dbus-58TPPDv8qv 19352
21,* 19351
21,u_str
21,ESTAB
21,* 17658
21,* 17657
21,u_str
21,ESTAB
21,* 17693
21,* 17694
21,[..]
21,"To show all network ports currently open, use the following command:"
21,# ss -l
21,Netid
21,State
21,Recv-Q Send-Q
21,Local Address:Port
21,Peer Address:Port
21,UNCONN
21,rtnl:4195117
21,UNCONN
21,rtnl:wickedd-auto4/811
21,UNCONN
21,rtnl:wickedd-dhcp4/813
21,UNCONN
21,rtnl:4195121
21,UNCONN
21,rtnl:4195115
21,UNCONN
21,rtnl:wickedd-dhcp6/814
21,UNCONN
21,rtnl:kernel
21,UNCONN
21,rtnl:wickedd/817
21,UNCONN
21,rtnl:4195118
21,UNCONN
21,rtnl:nscd/706
21,UNCONN
21,4352
21,tcpdiag:ss/2381
21,[...]
21,"When displaying network connections, you can specify the socket type to"
21,display: TCP (-t) or UDP (-u) for
21,example. The -p option shows the PID and name of the
21,program to which each socket belongs.
21,The following example lists all TCP connections and the programs using
21,these connections. The -a option make sure all
21,established connections (listening and non-listening) are shown. The
21,-p option shows the PID and name of the program to
21,which each socket belongs.
21,# ss -t -a -p
21,State
21,Recv-Q Send-Q
21,Local Address:Port
21,Peer Address:Port
21,LISTEN
21,128
21,*:ssh
21,*:*
21,"users:((""sshd"",1551,3))"
21,LISTEN
21,100
21,127.0.0.1:smtp
21,*:*
21,"users:((""master"",1704,13))"
21,ESTAB
21,132
21,10.120.65.198:ssh
21,10.120.4.150:55715
21,"users:((""sshd"",2103,5))"
21,LISTEN
21,128
21,:::ssh
21,:::*
21,"users:((""sshd"",1551,4))"
21,LISTEN
21,100
21,::1:smtp
21,:::*
21,"users:((""master"",1704,14))2.6 The /proc file system #"
21,The /proc file system is a pseudo file system in
21,which the kernel reserves important information in the form of virtual
21,"files. For example, display the CPU type with this command:"
21,> cat /proc/cpuinfo
21,processor
21,: 0
21,vendor_id
21,: GenuineIntel
21,cpu family
21,: 6
21,model
21,: 30
21,model name
21,: Intel(R) Core(TM) i5 CPU
21,750
21,@ 2.67GHz
21,stepping
21,: 5
21,microcode
21,: 0x6
21,cpu MHz
21,: 1197.000
21,cache size
21,: 8192 KB
21,physical id
21,: 0
21,siblings
21,: 4
21,core id
21,: 0
21,cpu cores
21,: 4
21,apicid
21,: 0
21,initial apicid
21,: 0
21,fpu
21,: yes
21,fpu_exception
21,: yes
21,cpuid level
21,: 11
21,: yes
21,flags
21,: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm ida dtherm tpr_shadow vnmi flexpriority ept vpid
21,bogomips
21,: 5333.85
21,clflush size
21,: 64
21,cache_alignment : 64
21,address sizes
21,": 36 bits physical, 48 bits virtual"
21,power management:
21,[...]Tip: Detailed processor information
21,Detailed information about the processor on the AMD64/Intel 64 architecture is
21,also available by running x86info.
21,Query the allocation and use of interrupts with the following command:
21,> cat /proc/interrupts
21,CPU0
21,CPU1
21,CPU2
21,CPU3
21,121
21,IO-APIC-edge
21,timer
21,IO-APIC-edge
21,rtc0
21,IO-APIC-fasteoi
21,acpi
21,16:
21,11933
21,IO-APIC-fasteoi
21,ehci_hcd:+
21,18:
21,IO-APIC-fasteoi
21,i801_smbus
21,19:
21,117978
21,IO-APIC-fasteoi
21,"ata_piix,+"
21,22:
21,3275185
21,IO-APIC-fasteoi
21,enp5s1
21,23:
21,417927
21,IO-APIC-fasteoi
21,ehci_hcd:+
21,40:
21,2727916
21,HPET_MSI-edge
21,hpet2
21,41:
21,2749134
21,HPET_MSI-edge
21,hpet3
21,42:
21,2759148
21,HPET_MSI-edge
21,hpet4
21,43:
21,2678206
21,HPET_MSI-edge
21,hpet5
21,45:
21,PCI-MSI-edge
21,"aerdrv, P+"
21,46:
21,PCI-MSI-edge
21,"PCIe PME,+"
21,47:
21,PCI-MSI-edge
21,"PCIe PME,+"
21,48:
21,PCI-MSI-edge
21,"PCIe PME,+"
21,49:
21,387
21,PCI-MSI-edge
21,snd_hda_i+
21,50:
21,933117
21,PCI-MSI-edge
21,nvidia
21,NMI:
21,2102
21,2023
21,2031
21,1920
21,Non-maskable interrupts
21,LOC:
21,Local timer interrupts
21,SPU:
21,Spurious interrupts
21,PMI:
21,2102
21,2023
21,2031
21,1920
21,Performance monitoring int+
21,IWI:
21,47331
21,45725
21,52464
21,46775
21,IRQ work interrupts
21,RTR:
21,APIC ICR read retries
21,RES:
21,472911
21,396463
21,339792
21,323820
21,Rescheduling interrupts
21,CAL:
21,48389
21,47345
21,54113
21,50478
21,Function call interrupts
21,TLB:
21,28410
21,26804
21,24389
21,26157
21,TLB shootdowns
21,TRM:
21,Thermal event interrupts
21,THR:
21,Threshold APIC interrupts
21,MCE:
21,Machine check exceptions
21,MCP:
21,Machine check polls
21,ERR:
21,MIS:
21,The address assignment of executables and libraries is contained in the
21,maps file:
21,> cat /proc/self/maps
21,08048000-0804c000 r-xp 00000000 03:03 17753
21,/bin/cat
21,0804c000-0804d000 rw-p 00004000 03:03 17753
21,/bin/cat
21,0804d000-0806e000 rw-p 0804d000 00:00 0
21,[heap]
21,b7d27000-b7d5a000 r--p 00000000 03:03 11867
21,/usr/lib/locale/en_GB.utf8/
21,b7d5a000-b7e32000 r--p 00000000 03:03 11868
21,/usr/lib/locale/en_GB.utf8/
21,b7e32000-b7e33000 rw-p b7e32000 00:00 0
21,b7e33000-b7f45000 r-xp 00000000 03:03 8837
21,/lib/libc-2.3.6.so
21,b7f45000-b7f46000 r--p 00112000 03:03 8837
21,/lib/libc-2.3.6.so
21,b7f46000-b7f48000 rw-p 00113000 03:03 8837
21,/lib/libc-2.3.6.so
21,b7f48000-b7f4c000 rw-p b7f48000 00:00 0
21,b7f52000-b7f53000 r--p 00000000 03:03 11842
21,/usr/lib/locale/en_GB.utf8/
21,[...]
21,b7f5b000-b7f61000 r--s 00000000 03:03 9109
21,/usr/lib/gconv/gconv-module
21,b7f61000-b7f62000 r--p 00000000 03:03 9720
21,/usr/lib/locale/en_GB.utf8/
21,b7f62000-b7f76000 r-xp 00000000 03:03 8828
21,/lib/ld-2.3.6.so
21,b7f76000-b7f78000 rw-p 00013000 03:03 8828
21,/lib/ld-2.3.6.so
21,bfd61000-bfd76000 rw-p bfd61000 00:00 0
21,[stack]
21,ffffe000-fffff000 ---p 00000000 00:00 0
21,[vdso]
21,A lot more information can be obtained from the /proc file system. Some
21,important files and their contents are:
21,/proc/devices
21,Available devices
21,/proc/modules
21,Kernel modules loaded
21,/proc/cmdline
21,Kernel command line
21,/proc/meminfo
21,Detailed information about memory usage
21,/proc/config.gz
21,gzip-compressed configuration file of the kernel
21,currently running
21,/proc/PID/
21,Find information about processes currently running in the
21,"/proc/NNN directories,"
21,where NNN is the process ID (PID) of the
21,relevant process. Every process can find its own characteristics in
21,/proc/self/.
21,Further information is available in the text file
21,/usr/src/linux/Documentation/filesystems/proc.txt
21,(this file is available when the package
21,kernel-source is installed).
21,2.6.1 procinfo #
21,Important information from the /proc file system is
21,summarized by the command procinfo:
21,> procinfo
21,Linux 3.11.10-17-desktop (geeko@buildhost) (gcc 4.8.1 20130909) #1 4CPU [jupiter.example.com]
21,Memory:
21,Total
21,Used
21,Free
21,Shared
21,Buffers
21,Cached
21,Mem:
21,8181908
21,8000632
21,181276
21,85472
21,2850872
21,Swap:
21,10481660
21,1576
21,10480084
21,Bootup: Mon Jul 28 09:54:13 2014
21,Load average: 1.61 0.85 0.74 2/904 25949
21,user
21,1:54:41.84
21,12.7%
21,page in :
21,2107312
21,disk 1:
21,52212r
21,20199w
21,nice
21,0:00:00.46
21,0.0%
21,page out:
21,1714461
21,disk 2:
21,19387r
21,10928w
21,system:
21,0:25:38.00
21,2.8%
21,page act:
21,466673
21,disk 3:
21,548r
21,10w
21,IOwait:
21,0:04:16.45
21,0.4%
21,page dea:
21,272297
21,hw irq:
21,0:00:00.42
21,0.0%
21,page flt:
21,105754526
21,sw irq:
21,0:01:26.48
21,0.1%
21,swap in :
21,idle
21,12:14:43.65
21,81.5%
21,swap out:
21,394
21,guest :
21,0:02:18.59
21,0.2%
21,uptime:
21,3:45:22.24
21,context :
21,99809844
21,irq
21,121 timer
21,irq 41:
21,3238224 hpet3
21,irq
21,1 rtc0
21,irq 42:
21,3251898 hpet4
21,irq
21,0 acpi
21,irq 43:
21,3156368 hpet5
21,irq 16:
21,14589 ehci_hcd:usb1
21,irq 45:
21,"0 aerdrv, PCIe PME"
21,irq 18:
21,0 i801_smbus
21,irq 46:
21,"0 PCIe PME, pciehp"
21,irq 19:
21,"124861 ata_piix, ata_piix, f irq 47:"
21,"0 PCIe PME, pciehp"
21,irq 22:
21,3742817 enp5s1
21,irq 48:
21,"0 PCIe PME, pciehp"
21,irq 23:
21,479248 ehci_hcd:usb2
21,irq 49:
21,387 snd_hda_intel
21,irq 40:
21,3216894 hpet2
21,irq 50:
21,1088673 nvidia
21,"To see all the information, use the parameter -a. The"
21,parameter -nN produces updates of the information every
21,"N seconds. In this case, terminate the"
21,program by pressing Q.
21,"By default, the cumulative values are displayed. The parameter"
21,-d produces the differential values. procinfo
21,-dn5 displays the values that have changed in the last five
21,seconds:
21,2.6.2 System control parameters: /proc/sys/ #
21,System control parameters are used to modify the Linux kernel parameters
21,at runtime. They reside in /proc/sys/ and can be
21,viewed and modified with the sysctl command. To list
21,"all parameters, run sysctl -a. A"
21,single parameter can be listed with sysctl
21,PARAMETER_NAME.
21,Parameters are grouped into categories and can be listed with
21,sysctl CATEGORY or by
21,listing the contents of the respective directories. The most important
21,categories are listed below. The links to further readings require the
21,installation of the package
21,kernel-source.
21,sysctl dev (/proc/sys/dev/)
21,Device-specific information.
21,sysctl fs (/proc/sys/fs/)
21,"Used file handles, quotas, and other file system-oriented parameters."
21,For details see
21,/usr/src/linux/Documentation/sysctl/fs.txt.
21,sysctl kernel (/proc/sys/kernel/)
21,"Information about the task scheduler, system shared memory, and other"
21,kernel-related parameters. For details see
21,/usr/src/linux/Documentation/sysctl/kernel.txt
21,sysctl net (/proc/sys/net/)
21,"Information about network bridges, and general network parameters"
21,(mainly the ipv4/ subdirectory). For details see
21,/usr/src/linux/Documentation/sysctl/net.txt
21,sysctl vm (/proc/sys/vm/)
21,"Entries in this path relate to information about the virtual memory,"
21,"swapping, and caching. For details see"
21,/usr/src/linux/Documentation/sysctl/vm.txt
21,"To set or change a parameter for the current session, use the command"
21,sysctl -w
21,PARAMETER=VALUE.
21,"To permanently change a setting, add a line"
21,PARAMETER=VALUE to
21,/etc/sysctl.conf.
21,2.7 Hardware information #  2.7.1 PCI resources: lspci #  Note: Accessing PCI configuration.
21,Most operating systems require root user privileges to grant access to
21,the computer's PCI configuration.
21,The command lspci lists the PCI resources:
21,# lspci
21,00:00.0 Host bridge: Intel Corporation 82845G/GL[Brookdale-G]/GE/PE \
21,DRAM Controller/Host-Hub Interface (rev 01)
21,00:01.0 PCI bridge: Intel Corporation 82845G/GL[Brookdale-G]/GE/PE \
21,Host-to-AGP Bridge (rev 01)
21,00:1d.0 USB Controller: Intel Corporation 82801DB/DBL/DBM \
21,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #1 (rev 01)
21,00:1d.1 USB Controller: Intel Corporation 82801DB/DBL/DBM \
21,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #2 (rev 01)
21,00:1d.2 USB Controller: Intel Corporation 82801DB/DBL/DBM \
21,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #3 (rev 01)
21,00:1d.7 USB Controller: Intel Corporation 82801DB/DBM \
21,(ICH4/ICH4-M) USB2 EHCI Controller (rev 01)
21,00:1e.0 PCI bridge: Intel Corporation 82801 PCI Bridge (rev 81)
21,00:1f.0 ISA bridge: Intel Corporation 82801DB/DBL (ICH4/ICH4-L) \
21,LPC Interface Bridge (rev 01)
21,00:1f.1 IDE interface: Intel Corporation 82801DB (ICH4) IDE \
21,Controller (rev 01)
21,00:1f.3 SMBus: Intel Corporation 82801DB/DBL/DBM (ICH4/ICH4-L/ICH4-M) \
21,SMBus Controller (rev 01)
21,00:1f.5 Multimedia audio controller: Intel Corporation 82801DB/DBL/DBM \
21,(ICH4/ICH4-L/ICH4-M) AC'97 Audio Controller (rev 01)
21,"01:00.0 VGA compatible controller: Matrox Graphics, Inc. G400/G450 (rev 85)"
21,02:08.0 Ethernet controller: Intel Corporation 82801DB PRO/100 VE (LOM) \
21,Ethernet Controller (rev 81)
21,Using -v results in a more detailed listing:
21,# lspci -v
21,[...]
21,00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet \
21,Controller (rev 02)
21,Subsystem: Intel Corporation PRO/1000 MT Desktop Adapter
21,"Flags: bus master, 66MHz, medium devsel, latency 64, IRQ 19"
21,"Memory at f0000000 (32-bit, non-prefetchable) [size=128K]"
21,I/O ports at d010 [size=8]
21,Capabilities: [dc] Power Management version 2
21,Capabilities: [e4] PCI-X non-bridge device
21,Kernel driver in use: e1000
21,Kernel modules: e1000
21,Information about device name resolution is obtained from the file
21,/usr/share/pci.ids. PCI IDs not listed in this file
21,are marked “Unknown device.”
21,The parameter -vv produces all the information that
21,"could be queried by the program. To view the pure numeric values, use"
21,the parameter -n.
21,2.7.2 USB devices: lsusb #
21,The command lsusb lists all USB devices. With the
21,"option -v, print a more detailed list. The detailed"
21,information is read from the directory
21,/proc/bus/usb/. The following is the output of
21,"lsusb with these USB devices attached: hub, memory"
21,"stick, hard disk and mouse."
21,# lsusb
21,"Bus 004 Device 007: ID 0ea0:2168 Ours Technology, Inc. Transcend JetFlash \"
21,2.0 / Astone USB Drive
21,Bus 004 Device 006: ID 04b4:6830 Cypress Semiconductor Corp. USB-2.0 IDE \
21,Adapter
21,"Bus 004 Device 005: ID 05e3:0605 Genesys Logic, Inc."
21,Bus 004 Device 001: ID 0000:0000
21,Bus 003 Device 001: ID 0000:0000
21,Bus 002 Device 001: ID 0000:0000
21,"Bus 001 Device 005: ID 046d:c012 Logitech, Inc. Optical Mouse"
21,Bus 001 Device 001: ID 0000:00002.7.3
21,Monitoring and tuning the thermal subsystem: tmon
21,"tmon is a tool to help visualize, tune, and test the"
21,"complex thermal subsystem. When started without parameters,"
21,tmon runs in monitoring mode:
21,┌──────THERMAL ZONES(SENSORS)──────────────────────────────┐
21,│Thermal Zones:
21,acpitz00
21,│Trip Points:
21,└──────────────────────────────────────────────────────────┘
21,┌─────────── COOLING DEVICES ──────────────────────────────┐
21,│ID
21,Cooling Dev
21,Cur
21,Max
21,Thermal Zone Binding
21,│00
21,Processor
21,││││││││││││
21,│01
21,Processor
21,││││││││││││
21,│02
21,Processor
21,││││││││││││
21,│03
21,Processor
21,││││││││││││
21,│04 intel_powerc
21,││││││││││││
21,└──────────────────────────────────────────────────────────┘
21,┌──────────────────────────────────────────────────────────┐
21,40 │
21,│acpitz 0:[
21,8][>>>>>>>>>P9
21,C31
21,└──────────────────────────────────────────────────────────┘
21,┌────────────────── CONTROLS ──────────────────────────────┐
21,│PID gain: kp=0.36 ki=5.00 kd=0.19 Output 0.00
21,"│Target Temp: 65.0C, Zone: 0, Control Device: None"
21,└──────────────────────────────────────────────────────────┘
21,Ctrl-c - Quit
21,TAB - Tuning
21,"For detailed information on how to interpret the data, how to log thermal"
21,data and how to use tmon to test and tune cooling
21,"devices and sensors, refer to the man page: man 8"
21,tmon. The package tmon is not installed by
21,default.
21,2.7.4 MCELog: machine check exceptions (MCE) #  Note: AvailabilityThis tool is only available on AMD64/Intel 64 systems.
21,The mcelog package logs and
21,"parses/translates Machine Check Exceptions (MCE) on hardware errors, including"
21,"I/O, CPU, and memory errors. In addition, mcelog handles predictive bad page"
21,offlining and automatic core offlining when cache errors happen.
21,Formerly this was managed by a cron job executed hourly. Now hardware
21,errors are immediately processed by an mcelog daemon.
21,Note: Support for AMD scalable MCA
21,SUSE Linux Enterprise Server supports AMD's Scalable Machine Check
21,Architecture (Scalable MCA). Scalable MCA improves hardware error
21,reporting in AMD Zen processors. It expands information logged in
21,MCA banks for improved error handling and better diagnosability.
21,mcelog captures MCA messages
21,(rasdaemon and
21,dmesg also capture MCA messages).
21,"See section 3.1, Machine Check Architecture of"
21,Processor Programming Reference (PPR) for AMD Family
21,"17h Model 01h, Revision B1 Processors for detailed"
21,"information,"
21,http://developer.amd.com/wordpress/media/2017/11/54945_PPR_Family_17h_Models_00h-0Fh.pdf.
21,mcelog is configured in /etc/mcelog/mcelog.conf.
21,Configuration options are documented in
21,"man mcelog, and at"
21,http://mcelog.org/. The following example shows
21,only changes to the default file:
21,daemon = yes
21,filter = yes
21,filter-memory-errors = yes
21,no-syslog = yes
21,logfile = /var/log/mcelog
21,run-credentials-user = root
21,run-credentials-group = nobody
21,client-group = root
21,socket-path = /var/run/mcelog-client
21,The mcelog service is not enabled by default. The service can either be
21,"enabled and started via the YaST system services editor, or via command line:"
21,# systemctl enable mcelog
21,# systemctl start mcelog2.7.5 AMD64/Intel 64: dmidecode: DMI table decoder #
21,dmidecode shows the machine's DMI table containing
21,information such as serial numbers and BIOS revisions of the hardware.
21,# dmidecode
21,# dmidecode 2.12
21,SMBIOS 2.5 present.
21,27 structures occupying 1298 bytes.
21,Table at 0x000EB250.
21,"Handle 0x0000, DMI type 4, 35 bytes"
21,Processor Information
21,Socket Designation: J1PR
21,Type: Central Processor
21,Family: Other
21,Manufacturer: Intel(R) Corporation
21,ID: E5 06 01 00 FF FB EB BF
21,Version: Intel(R) Core(TM) i5 CPU
21,750
21,@ 2.67GHz
21,Voltage: 1.1 V
21,External Clock: 133 MHz
21,Max Speed: 4000 MHz
21,Current Speed: 2667 MHz
21,"Status: Populated, Enabled"
21,Upgrade: Other
21,L1 Cache Handle: 0x0004
21,L2 Cache Handle: 0x0003
21,L3 Cache Handle: 0x0001
21,Serial Number: Not Specified
21,Asset Tag: Not Specified
21,Part Number: Not Specified
21,[..]2.7.6 POWER: list hardware #
21,lshw extracts and displays the hardware
21,configuration of the machine.
21,2.8 Files and file systems #
21,"For file system-specific information, refer to"
21,Book “Storage Administration Guide”.
21,2.8.1 Determine the file type: file #
21,The command file determines the type of a file or a
21,list of files by checking /usr/share/misc/magic.
21,> file /usr/bin/file
21,"/usr/bin/file: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), \"
21,"for GNU/Linux 2.6.4, dynamically linked (uses shared libs), stripped"
21,The parameter -f LIST
21,specifies a file with a list of file names to examine. The
21,-z allows file to look inside
21,compressed files:
21,> file /usr/share/man/man1/file.1.gz
21,"/usr/share/man/man1/file.1.gz: gzip compressed data, from Unix, max compression"
21,> file -z /usr/share/man/man1/file.1.gz
21,/usr/share/man/man1/file.1.gz: troff or preprocessor input text \
21,"(gzip compressed data, from Unix, max compression)"
21,The parameter -i outputs a mime type string rather than
21,the traditional description.
21,> file -i /usr/share/misc/magic
21,"/usr/share/misc/magic: text/plain charset=utf-82.8.2 File systems and their usage: mount, df and du #"
21,The command mount shows which file system (device and
21,type) is mounted at which mount point:
21,# mount
21,"/dev/sda2 on / type ext4 (rw,acl,user_xattr)"
21,proc on /proc type proc (rw)
21,sysfs on /sys type sysfs (rw)
21,debugfs on /sys/kernel/debug type debugfs (rw)
21,"devtmpfs on /dev type devtmpfs (rw,mode=0755)"
21,"tmpfs on /dev/shm type tmpfs (rw,mode=1777)"
21,"devpts on /dev/pts type devpts (rw,mode=0620,gid=5)"
21,/dev/sda3 on /home type ext3 (rw)
21,securityfs on /sys/kernel/security type securityfs (rw)
21,fusectl on /sys/fs/fuse/connections type fusectl (rw)
21,gvfs-fuse-daemon on /home/tux/.gvfs type fuse.gvfs-fuse-daemon \
21,"(rw,nosuid,nodev,user=tux)"
21,Obtain information about total usage of the file systems with the
21,command df. The parameter -h (or
21,--human-readable) transforms the output into a form
21,understandable for common users.
21,> df -h
21,Filesystem
21,Size
21,Used Avail Use% Mounted on
21,/dev/sda2
21,20G
21,"5,9G"
21,13G
21,32% /
21,devtmpfs
21,"1,6G"
21,236K
21,"1,6G"
21,1% /dev
21,tmpfs
21,"1,6G"
21,668K
21,"1,6G"
21,1% /dev/shm
21,/dev/sda3
21,208G
21,40G
21,159G
21,20% /home
21,Display the total size of all the files in a given directory and its
21,subdirectories with the command du. The parameter
21,-s suppresses the output of detailed information and
21,gives only a total for each argument. -h again
21,transforms the output into a human-readable form:
21,> du -sh /opt
21,192M
21,/opt2.8.3 Additional information about ELF binaries #
21,Read the content of binaries with the readelf
21,utility. This even works with ELF files that were built for other
21,hardware architectures:
21,> readelf --file-header /bin/ls
21,ELF Header:
21,Magic:
21,7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
21,Class:
21,ELF64
21,Data:
21,"2's complement, little endian"
21,Version:
21,1 (current)
21,OS/ABI:
21,UNIX - System V
21,ABI Version:
21,Type:
21,EXEC (Executable file)
21,Machine:
21,Advanced Micro Devices X86-64
21,Version:
21,0x1
21,Entry point address:
21,0x402540
21,Start of program headers:
21,64 (bytes into file)
21,Start of section headers:
21,95720 (bytes into file)
21,Flags:
21,0x0
21,Size of this header:
21,64 (bytes)
21,Size of program headers:
21,56 (bytes)
21,Number of program headers:
21,Size of section headers:
21,64 (bytes)
21,Number of section headers:
21,Section header string table index: 312.8.4 File properties: stat #
21,The command stat displays file properties:
21,> stat /etc/profile
21,File: `/etc/profile'
21,Size: 9662
21,Blocks: 24
21,IO Block: 4096
21,regular file
21,Device: 802h/2050d
21,Inode: 132349
21,Links: 1
21,Access: (0644/-rw-r--r--)
21,Uid: (
21,root)
21,Gid: (
21,root)
21,Access: 2009-03-20 07:51:17.000000000 +0100
21,Modify: 2009-01-08 19:21:14.000000000 +0100
21,Change: 2009-03-18 12:55:31.000000000 +0100
21,The parameter --file-system produces details of the
21,properties of the file system in which the specified file is located:
21,> stat /etc/profile --file-system
21,"File: ""/etc/profile"""
21,ID: d4fb76e70b4d1746 Namelen: 255
21,Type: ext2/ext3
21,Block size: 4096
21,Fundamental block size: 4096
21,Blocks: Total: 2581445
21,Free: 1717327
21,Available: 1586197
21,Inodes: Total: 655776
21,Free: 4903122.9 User information #  2.9.1 User accessing files: fuser #
21,It can be useful to determine what processes or users are currently
21,"accessing certain files. Suppose, for example, you want to unmount a"
21,file system mounted at /mnt.
21,"umount returns ""device is busy."" The command"
21,fuser can then be used to determine what processes
21,are accessing the device:
21,> fuser -v /mnt/*
21,USER
21,PID ACCESS COMMAND
21,/mnt/notes.txt
21,tux
21,26597 f....
21,less
21,"Following termination of the less process, which was"
21,"running on another terminal, the file system can successfully be"
21,"unmounted. When used with -k option,"
21,fuser will terminate processes accessing the file as
21,well.
21,2.9.2 Who is doing what: w #
21,"With the command w, find out who is logged in to the"
21,system and what each user is doing. For example:
21,> w
21,"16:00:59 up 1 day,"
21,"2:41,"
21,"3 users,"
21,"load average: 0.00, 0.01, 0.05"
21,USER
21,TTY
21,FROM
21,LOGIN@
21,IDLE
21,JCPU
21,PCPU WHAT
21,tux
21,console
21,Wed13
21,?xdm?
21,8:15
21,0.03s /usr/lib/gdm/gd
21,tux
21,console
21,Wed13
21,26:41m
21,0.00s
21,0.03s /usr/lib/gdm/gd
21,tux
21,pts/0
21,Wed13
21,20:11
21,0.10s
21,2.89s /usr/lib/gnome-
21,"If any users of other systems have logged in remotely, the parameter"
21,-f shows the computers from which they have established
21,the connection.
21,2.10 Time and date #  2.10.1 Time measurement with time #
21,Determine the time spent by commands with the time
21,utility. This utility is available in two versions: as a Bash built-in
21,and as a program (/usr/bin/time).
21,> time find . > /dev/null
21,real
21,0m4.051s1
21,user
21,0m0.042s2
21,sys
21,0m0.205s31
21,The real time that elapsed from the command's start-up until it
21,finished.
21,CPU time of the user as reported by the times
21,system call.
21,CPU time of the system as reported by the times
21,system call.
21,The output of /usr/bin/time is much more detailed.
21,It is recommended to run it with the -v switch to
21,produce human-readable output.
21,/usr/bin/time -v find . > /dev/null
21,"Command being timed: ""find ."""
21,User time (seconds): 0.24
21,System time (seconds): 2.08
21,Percent of CPU this job got: 25%
21,Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.03
21,Average shared text size (kbytes): 0
21,Average unshared data size (kbytes): 0
21,Average stack size (kbytes): 0
21,Average total size (kbytes): 0
21,Maximum resident set size (kbytes): 2516
21,Average resident set size (kbytes): 0
21,Major (requiring I/O) page faults: 0
21,Minor (reclaiming a frame) page faults: 1564
21,Voluntary context switches: 36660
21,Involuntary context switches: 496
21,Swaps: 0
21,File system inputs: 0
21,File system outputs: 0
21,Socket messages sent: 0
21,Socket messages received: 0
21,Signals delivered: 0
21,Page size (bytes): 4096
21,Exit status: 02.11 Graph your data: RRDtool #
21,"There are a lot of data in the world around you, which can be easily"
21,"measured in time. For example, changes in the temperature, or the number"
21,of data sent or received by your computer's network interface. RRDtool
21,can help you store and visualize such data in detailed and customizable
21,graphs.
21,RRDtool is available for most Unix platforms and Linux distributions.
21,SUSE® Linux Enterprise Server ships RRDtool as well. Install it either with
21,YaST or by entering
21,zypper install
21,rrdtool in the command line as root.
21,Tip: Bindings
21,"There are Perl, Python, Ruby, and PHP bindings available for RRDtool, so"
21,that you can write your own monitoring scripts in your preferred
21,scripting language.
21,2.11.1 How RRDtool works #
21,RRDtool is an abbreviation of Round Robin Database
21,tool. Round Robin is a method for
21,manipulating with a constant amount of data. It uses the principle of a
21,"circular buffer, where there is no end nor beginning to the data row"
21,which is being read. RRDtool uses Round Robin Databases to store and
21,read its data.
21,"As mentioned above, RRDtool is designed to work with data that change in"
21,time. The ideal case is a sensor which repeatedly reads measured data
21,"(like temperature, speed etc.) in constant periods of time, and then"
21,exports them in a given format. Such data are perfectly ready for
21,"RRDtool, and it is easy to process them and create the desired output."
21,Sometimes it is not possible to obtain the data automatically and
21,regularly. Their format needs to be pre-processed before it is supplied
21,"to RRDtool, and often you need to manipulate RRDtool even manually."
21,The following is a simple example of basic RRDtool usage. It illustrates
21,all three important phases of the usual RRDtool workflow:
21,"creating a database, updating"
21,"measured values, and viewing the output."
21,2.11.2 A practical example #
21,Suppose we want to collect and view information about the memory usage
21,in the Linux system as it changes in time. To make the example more
21,"vivid, we measure the currently free memory over a period of 40 seconds"
21,in 4-second intervals. Three applications that usually consume a lot of
21,"system memory are started and closed: the Firefox Web browser, the"
21,"Evolution e-mail client, and the Eclipse development framework."
21,2.11.2.1 Collecting data #
21,RRDtool is very often used to measure and visualize network traffic. In
21,"such case, the Simple Network Management Protocol (SNMP) is used. This"
21,protocol can query network devices for relevant values of their
21,internal counters. Exactly these values are to be stored with RRDtool.
21,"For more information on SNMP, see"
21,http://www.net-snmp.org/.
21,Our situation is different—we need to obtain the data
21,manually. A helper script free_mem.sh repetitively
21,reads the current state of free memory and writes it to the standard
21,output.
21,> cat free_mem.sh
21,INTERVAL=4
21,for steps in {1..10}
21,DATE=`date +%s`
21,"FREEMEM=`free -b | grep ""Mem"" | awk '{ print $4 }'`"
21,sleep $INTERVAL
21,"echo ""rrdtool update free_mem.rrd $DATE:$FREEMEM"""
21,done
21,"The time interval is set to 4 seconds, and is implemented with the"
21,sleep command.
21,RRDtool accepts time information in a special format - so called
21,Unix time. It is defined as the number of
21,"seconds since the midnight of January 1, 1970 (UTC). For example,"
21,1272907114 represents 2010-05-03 17:18:34.
21,The free memory information is reported in bytes with
21,free -b. Prefer to supply basic
21,units (bytes) instead of multiple units (like kilobytes).
21,The line with the echo ... command contains the
21,"future name of the database file (free_mem.rrd),"
21,and together creates a command line for updating
21,RRDtool values.
21,"After running free_mem.sh, you see an output similar"
21,to this:
21,> sh free_mem.sh
21,rrdtool update free_mem.rrd 1272974835:1182994432
21,rrdtool update free_mem.rrd 1272974839:1162817536
21,rrdtool update free_mem.rrd 1272974843:1096269824
21,rrdtool update free_mem.rrd 1272974847:1034219520
21,rrdtool update free_mem.rrd 1272974851:909438976
21,rrdtool update free_mem.rrd 1272974855:832454656
21,rrdtool update free_mem.rrd 1272974859:829120512
21,rrdtool update free_mem.rrd 1272974863:1180377088
21,rrdtool update free_mem.rrd 1272974867:1179369472
21,rrdtool update free_mem.rrd 1272974871:1181806592
21,It is convenient to redirect the command's output to a file with
21,sh free_mem.sh > free_mem_updates.log
21,to simplify its future execution.
21,2.11.2.2 Creating the database #
21,Create the initial Robin Round database for our example with the
21,following command:
21,> rrdtool create free_mem.rrd --start 1272974834 --step=4 \
21,DS:memory:GAUGE:600:U:U RRA:AVERAGE:0.5:1:24Points to notice #
21,This command creates a file called free_mem.rrd
21,for storing our measured values in a Round Robin type database.
21,The --start option specifies the time (in Unix time)
21,"when the first value will be added to the database. In this example,"
21,it is one less than the first time value of the
21,free_mem.sh output (1272974835).
21,The --step specifies the time interval in seconds
21,with which the measured data will be supplied to the database.
21,The DS:memory:GAUGE:600:U:U part introduces a new
21,data source for the database. It is called
21,"memory, its type is gauge,"
21,"the maximum number between two updates is 600 seconds, and the"
21,minimal and maximal value
21,in the measured range are unknown (U).
21,RRA:AVERAGE:0.5:1:24 creates Round Robin archive
21,(RRA) whose stored data are processed with the
21,consolidation functions (CF) that calculates the
21,average of data points. 3 arguments of the
21,consolidation function are appended to the end of the line.
21,"If no error message is displayed, then"
21,free_mem.rrd database is created in the current
21,directory:
21,> ls -l free_mem.rrd
21,-rw-r--r-- 1 tux users 776 May
21,5 12:50 free_mem.rrd2.11.2.3 Updating database values #
21,"After the database is created, you need to fill it with the measured"
21,"data. In Section 2.11.2.1, “Collecting data”, we already"
21,prepared the file free_mem_updates.log which
21,consists of rrdtool update commands. These commands
21,do the update of database values for us.
21,> sh free_mem_updates.log; ls -l free_mem.rrd
21,-rw-r--r--
21,1 tux users
21,776 May
21,5 13:29 free_mem.rrd
21,"As you can see, the size of free_mem.rrd remained"
21,the same even after updating its data.
21,2.11.2.4 Viewing measured values #
21,"We have already measured the values, created the database, and stored"
21,"the measured value in it. Now we can play with the database, and"
21,retrieve or view its values.
21,"To retrieve all the values from our database, enter the following on"
21,the command line:
21,> rrdtool fetch free_mem.rrd AVERAGE --start 1272974830 \
21,--end 1272974871
21,memory
21,1272974832: nan
21,1272974836: 1.1729059840e+09
21,1272974840: 1.1461806080e+09
21,1272974844: 1.0807572480e+09
21,1272974848: 1.0030243840e+09
21,1272974852: 8.9019289600e+08
21,1272974856: 8.3162112000e+08
21,1272974860: 9.1693465600e+08
21,1272974864: 1.1801251840e+09
21,1272974868: 1.1799787520e+09
21,1272974872: nanPoints to notice #
21,AVERAGE will fetch average value points from the
21,"database, because only one data source is defined"
21,"(Section 2.11.2.2, “Creating the database”) with"
21,AVERAGE processing and no other function is
21,available.
21,The first line of the output prints the name of the data source as
21,"defined in Section 2.11.2.2, “Creating the database”."
21,"The left results column represents individual points in time, while"
21,the right one represents corresponding measured average values in
21,scientific notation.
21,The nan in the last line stands for “not a
21,number”.
21,Now a graph representing the values stored in the database is drawn:
21,> rrdtool graph free_mem.png \
21,--start 1272974830 \
21,--end 1272974871 \
21,--step=4 \
21,DEF:free_memory=free_mem.rrd:memory:AVERAGE \
21,LINE2:free_memory#FF0000 \
21,"--vertical-label ""GB"" \"
21,"--title ""Free System Memory in Time"" \"
21,--zoom 1.5 \
21,--x-grid SECOND:1:SECOND:4:SECOND:10:0:%XPoints to notice #
21,free_mem.png is the file name of the graph to be
21,created.
21,--start and --end limit the time
21,range within which the graph will be drawn.
21,--step specifies the time resolution (in seconds) of
21,the graph.
21,The DEF:... part is a data definition called
21,free_memory. Its data are read from the
21,free_mem.rrd database and its data source called
21,memory. The average value
21,"points are calculated, because no others were defined in"
21,"Section 2.11.2.2, “Creating the database”."
21,The LINE... part specifies properties of the line
21,"to be drawn into the graph. It is 2 pixels wide, its data come from"
21,"the free_memory definition, and its color is"
21,red.
21,--vertical-label sets the label to be printed along
21,"the y axis, and --title sets"
21,the main label for the whole graph.
21,--zoom specifies the zoom factor for the graph. This
21,value must be greater than zero.
21,--x-grid specifies how to draw grid lines and their
21,"labels into the graph. Our example places them every second, while"
21,major grid lines are placed every 4 seconds. Labels are placed every
21,10 seconds under the major grid lines.
21,Figure 2.1: Example graph created with RRDtool #  2.11.3 More information #
21,RRDtool is a very complex tool with a lot of sub-commands and command
21,"line options. Some are easy to understand, but to make it"
21,produce the results you want and fine-tune them according to your liking
21,may require a lot of effort.
21,Apart from RRDtool's man page (man 1 rrdtool) which
21,"gives you only basic information, you should have a look at the"
21,RRDtool home
21,page. There is a detailed
21,documentation
21,of the rrdtool command and all its sub-commands.
21,There are also several
21,tutorials
21,to help you understand the common RRDtool workflow.
21,"If you are interested in monitoring network traffic, have a look at"
21,MRTG (Multi Router
21,Traffic Grapher). MRTG can graph the activity of many network
21,devices. It can use RRDtool.
21,3 System log files #
21,System log file analysis is one of the most important tasks when analyzing
21,"the system. In fact, looking at the system log files should be the first"
21,thing to do when maintaining or troubleshooting a system. SUSE Linux Enterprise Server
21,automatically logs almost everything that happens on the system in detail.
21,"Since the move to systemd, kernel messages and messages of system"
21,services registered with systemd are logged in systemd journal
21,"(see Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”). Other log files (mainly those of"
21,system applications) are written in plain text and can be easily read
21,using an editor or pager. It is also possible to parse them using scripts.
21,This allows you to filter their content.
21,3.1 System log files in /var/log/ #
21,System log files are always located under the
21,/var/log directory. The following list presents an
21,overview of all system log files from SUSE Linux Enterprise Server present after a
21,"default installation. Depending on your installation scope,"
21,/var/log also contains log files from other services
21,and applications not listed here. Some files and directories described
21,"below are “placeholders” and are only used, when the"
21,corresponding application is installed. Most log files are only visible
21,for the user root.
21,apparmor/
21,"AppArmor log files. For more information about AppArmor, see"
21,Book “Security and Hardening Guide”.
21,audit/
21,Logs from the audit framework. See Book “Security and Hardening Guide” for
21,details.
21,ConsoleKit/
21,Logs of the ConsoleKit daemon
21,(daemon for tracking what users are logged in and how they interact
21,with the computer).
21,cups/
21,Access and error logs of the Common Unix Printing System
21,(cups).
21,firewall
21,Firewall logs.
21,gdm/
21,Log files from the GNOME display manager.
21,krb5/
21,Log files from the Kerberos network authentication system.
21,lastlog
21,A database containing information on the last login of each user. Use
21,the command lastlog to view. See man 8
21,lastlog for more information.
21,localmessages
21,"Log messages of some boot scripts, for example the log of the DHCP"
21,client.
21,mail*
21,"Mail server (postfix,"
21,sendmail) logs.
21,messages
21,This is the default place where all kernel and system log messages go
21,and should be the first place (along with
21,/var/log/warn) to look at in case of problems.
21,NetworkManager
21,NetworkManager log files.
21,news/
21,Log messages from a news server.
21,chrony/
21,Logs from the Network Time Protocol daemon
21,(chrony).
21,pk_backend_zypp*
21,PackageKit (with libzypp
21,back-end) log files.
21,samba/
21,"Log files from Samba, the Windows SMB/CIFS file server."
21,warn
21,Log of all system warnings and errors. This should be the first place
21,(along with the output of the systemd journal) to look in case of
21,problems.
21,wtmp
21,"Database of all login/logout activities,"
21,and remote connections. Use the command last to
21,view. See man 1 last for more information.
21,Xorg.NUMBER.log
21,X.Org start-up log file. Refer to these files in case you have
21,problems starting X.Org.
21,The NUMBER in the file name is the
21,"display number. For example, the default Xorg.0.log is the log for display number"
21,"0, and Xorg.1.log is the log for display number"
21,1. Copies from previous X.Org starts are named as Xorg.NUMBER.log.old.
21,Note
21,The X.Org log files are available in the
21,/var/log/ directory
21,only if you start an X.Org session as root.
21,"If you start an X.Org session as any other user, you can locate the log files in the ~/.local/share/xorg/ directory."
21,YaST2/
21,All YaST log files.
21,zypp/
21,libzypp log files. Refer to
21,these files for the package installation history.
21,zypper.log
21,Logs from the command line installer zypper.
21,3.2 Viewing and parsing log files #
21,"To view log files, you can use any text editor. There is also a simple"
21,YaST module for viewing the system log available in the YaST
21,control center under Miscellaneous › System Log.
21,"For viewing log files in a text console, use the commands"
21,less or more. Use
21,head and tail to view the beginning
21,or end of a log file. To view entries appended to a log file in real-time
21,use tail -f. For information about
21,"how to use these tools, see their man pages."
21,To search for strings or regular expressions in log files use
21,grep. awk is useful for parsing and
21,rewriting log files.
21,3.3 Managing log files with logrotate #
21,Log files under /var/log grow on a daily basis and
21,quickly become very large. logrotate is a tool that
21,helps you manage log files and their growth. It allows automatic
21,"rotation, removal, compression, and mailing of log files. Log files can"
21,"be handled periodically (daily, weekly, or monthly) or when exceeding a"
21,particular size.
21,"logrotate is usually run daily by systemd,"
21,"and thus usually modifies log files only once a day. However, exceptions"
21,"occur when a log file is modified because of its size, if"
21,"logrotate is run multiple times a day, or if"
21,--force is enabled.
21,Use
21,/var/lib/misc/logrotate.status to find out when a
21,particular file was last rotated.
21,The main configuration file of logrotate is
21,/etc/logrotate.conf. System packages and
21,"programs that produce log files (for example,"
21,apache2) put their own
21,configuration files in the /etc/logrotate.d/
21,directory. The content of /etc/logrotate.d/ is
21,included via /etc/logrotate.conf.
21,Example 3.1: Example for /etc/logrotate.conf #
21,"# see ""man logrotate"" for details"
21,# rotate log files weekly
21,weekly
21,# keep 4 weeks worth of backlogs
21,rotate 4
21,# create new (empty) log files after rotating old ones
21,create
21,# use date as a suffix of the rotated file
21,dateext
21,# uncomment this if you want your log files compressed
21,#compress
21,# comment these to switch compression to use gzip or another
21,# compression scheme
21,compresscmd /usr/bin/bzip2
21,uncompresscmd /usr/bin/bunzip2
21,# RPM packages drop log rotation information into this directory
21,include /etc/logrotate.dImportant: Avoid permission conflicts
21,The create option pays heed to the modes and
21,ownership of files specified in /etc/permissions*.
21,"If you modify these settings, make sure no conflicts arise."
21,3.4 Monitoring log files with logwatch #
21,"logwatch is a customizable, pluggable log-monitoring"
21,"script. It parses system logs, extracts the important information and"
21,presents them in a human readable manner. To use
21,"logwatch, install the"
21,logwatch package.
21,logwatch can either be used at the command line to
21,"generate on-the-fly reports, or via cron to regularly create custom"
21,"reports. Reports can either be printed on the screen, saved to a file, or"
21,be mailed to a specified address. The latter is especially useful when
21,automatically generating reports via cron.
21,"On the command line, you can tell logwatch for which"
21,service and time span to generate a report and how much detail should be
21,included:
21,# Detailed report on all kernel messages from yesterday
21,logwatch --service kernel --detail High --range Yesterday --print
21,# Low detail report on all sshd events recorded (incl. archived logs)
21,logwatch --service sshd --detail Low --range All --archives --print
21,# Mail a report on all smartd messages from May 5th to May 7th to root@localhost
21,logwatch --service smartd --range 'between 5/5/2005 and 5/7/2005' \
21,--mailto root@localhost --print
21,The --range option has got a complex syntax—see
21,logwatch --range help for details. A
21,list of all services that can be queried is available with the following
21,command:
21,> ls /usr/share/logwatch/default.conf/services/ | sed 's/\.conf//g'
21,"logwatch can be customized to great detail. However,"
21,the default configuration should usually be sufficient. The default
21,configuration files are located under
21,/usr/share/logwatch/default.conf/. Never change them
21,because they would get overwritten again with the next update. Rather
21,place custom configuration in /etc/logwatch/conf/
21,"(you may use the default configuration file as a template, though). A"
21,detailed HOWTO on customizing logwatch is available at
21,/usr/share/doc/packages/logwatch/HOWTO-Customize-LogWatch.
21,The following configuration files exist:
21,logwatch.conf
21,The main configuration file. The default version is extensively
21,commented. Each configuration option can be overwritten on the command
21,line.
21,ignore.conf
21,Filter for all lines that should globally be ignored by
21,logwatch.
21,services/*.conf
21,The service directory holds configuration files for each service you
21,can generate a report for.
21,logfiles/*.conf
21,Specifications on which log files should be parsed for each service.
21,3.5 Configuring mail forwarding for root #
21,"System daemons, cron jobs, systemd"
21,"timers, and other applications can generate messages and send them to the"
21,"root user of the system. By default, each user account owns a local"
21,mailbox and will be notified about new mail messages upon login.
21,These messages can contain security relevant reports and incidents that might
21,require a quick response by the system administrator. To get notified about
21,"these messages in a timely fashion, it is strongly recommended to forward"
21,these mails to a dedicated remote email account that is regularly checked.
21,Procedure 3.1: Configure mail forwarding for the root user #
21,"To forward mail for the root user, perform the following steps:"
21,Install the yast2-mail package:
21,# zypper in yast2-mail
21,Run the interactive YaST mail configuration:
21,# yast mail
21,Choose Permanent as Connection type
21,and proceed with Next.
21,Enter the address of the Outgoing mail server. If
21,"necessary, configure Authentication. It is strongly"
21,recommended to Enforce TLS encryption
21,to prevent
21,potentially sensitive system data from being sent unencrypted over the
21,network. Proceed with Next.
21,Enter the email address to Forward root's mail to and
21,Finish the configuration.
21,Important: Do not accept remote SMTP connections
21,Do not enable Accept remote SMTP
21,"connections, otherwise the local machine will act as a mail"
21,relay.
21,Send a message to test whether mail forwarding works correctly:
21,> mail root
21,subject: test
21,test
21,Use the mailq command to verify that the test message
21,"has been sent. Upon success, the queue should be empty. The message should"
21,be received by the dedicated mail address configured previously.
21,Depending on the number of managed machines and the number of persons who
21,"need to be informed about system events, different email address models can"
21,be established:
21,Collect messages from different systems in an email account that is only
21,accessed by a single person.
21,Collect messages from different systems in a group email account (aliases
21,or mailing list) that can be accessed by all relevant persons.
21,Create separate email accounts for each system.
21,It is crucial that administrators regularly check the related email accounts.
21,"To facilitate this effort and identify impoetant events, avoid sending"
21,unnecessary information. Configure applications to only send relevant
21,information.
21,3.6 Forwarding log messages to a central syslog server #
21,System log data can be forwarded from individual systems to a central
21,syslog server on the network. This allows administrators to get an overview
21,"of events on all hosts, and prevents attackers that succeed in taking over a"
21,system from manipulating system logs to cover their tracks.
21,Setting up a central syslog server consists of two parts. First you configure
21,"the central log server, then the clients for remote logging."
21,3.6.1 Set up the central syslog server #  Procedure 3.2: Configure the central rsyslog server #
21,"To set up a central syslog server, perform the following steps:"
21,Edit the configuration file
21,/etc/rsyslog.d/remote.conf.
21,Uncomment the following lines in the UDP Syslog Server
21,or TCP Syslog Server section of the configuration file.
21,Assign an IP address and port for rsyslogd.
21,TCP example:
21,$ModLoad imtcp.so
21,$UDPServerAddress IP1
21,$InputTCPServerRun PORT2
21,UDP example:
21,$ModLoad imudp.so
21,$UDPServerAddress IP1
21,$UDPServerRun PORT21
21,IP address of the interface for rsyslogd to listen on. If no address is
21,"given, the daemon listens on all interfaces."
21,Port for rsyslogd to listen on.
21,Select a privileged port below 1024. The default is 514.
21,Important: TCP versus UDP protocol
21,Traditionally syslog uses the UDP protocol to transmit log messages over
21,"the network. This involves less overhead, but lacks reliability. Log"
21,messages can get lost under high load.
21,The TCP protocol is more reliable and should be preferred over UDP.
21,Note: UDPServerAddress with TCP
21,The $UDPServerAddress configuration parameter in the
21,TCP example is no error. Despite its name it is used for both TCP and
21,UDP.
21,Save the file.
21,Restart the rsyslog service:
21,> sudo systemctl restart rsyslog.serviceOpen the respective port in the firewall. For firewalld with TCP on port 514 run:
21,> sudo firewall-cmd --add-port 514/tcp --permanent
21,> sudo firewall-cmd --reload
21,"You have now configured the central syslog server. Next, configure clients"
21,for remote logging.
21,3.6.2 Set up the client machines #  Procedure 3.3: Configure a rsyslog instance for remote logging #
21,"To configure a machine for remote logging on a central syslog server,"
21,perform the following steps:
21,Edit the configuration file
21,/etc/rsyslog.d/remote.conf.
21,Uncomment the appropriate line (TCP or UDP) and replace
21,remote-host with the address of the central syslog
21,"server set up in Section 3.6.1, “Set up the central syslog server”."
21,TCP example:
21,# Remote Logging using TCP for reliable delivery
21,"# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional"
21,*.* @@remote-host
21,UDP example:
21,# Remote Logging using UDP
21,"# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional"
21,*.* @remote-host
21,Save the file.
21,Restart the rsyslog service:
21,> sudo systemctl restart rsyslog.service
21,Verify the proper function of the syslog forwarding:
21,"> logger ""hello world"""
21,The log message hello world should now appear on the
21,central syslog server.
21,You have now configured a system for remote logging to your central syslog
21,server. Repeat this procedure for all systems that should log remotely.
21,3.6.3 More information #
21,This basic setup does not include encryption and is only suitable for
21,"trusted internal networks. TLS encryption is strongly recommended, but"
21,requires a certificate infrastructure.
21,"In this configuration, all messages from remote hosts will be treated the"
21,same on the central syslog server. Consider filtering messages into separate
21,files by remote host or classify them by message category.
21,"For more information about encryption, filtering, and other advanced topics,"
21,consult the RSyslog documentation at
21,https://www.rsyslog.com/doc/master/index.html#manual.
21,3.7 Using logger to make system log entries #
21,logger is a tool for making entries in the system log.
21,It provides a shell command interface to the rsyslogd system log module.
21,"For example, the following line outputs its message in"
21,/var/log/messages or directly in the journal (if no
21,logging facility is running):
21,"> logger -t Test ""This message comes from $USER"""
21,"Depending on the current user and host name, the log contains a line"
21,similar to this:
21,"Sep 28 13:09:31 venus Test: This message comes from tuxPart III Kernel monitoring #  4 SystemTap—filtering and analyzing system dataSystemTap provides a command line interface and a scripting language to examine the activities of a running Linux system, particularly the kernel, in fine detail. SystemTap scripts are written in the SystemTap scripting language, are then compiled to C-code kernel modules and inserted into the kerne…5 Kernel probes"
21,Kernel probes are a set of tools to collect Linux kernel debugging and
21,performance information. Developers and system administrators usually use
21,"them either to debug the kernel, or to find system performance"
21,bottlenecks. The reported data can then be used to tune the system for
21,better performance.
21,6 Hardware-based performance monitoring with Perf
21,Perf is an interface to access the performance monitoring unit (PMU) of a
21,processor and to record and display software events such as page faults.
21,"It supports system-wide, per-thread, and KVM virtualization guest"
21,monitoring.
21,7 OProfile—system-wide profiler
21,OProfile is a profiler for dynamic program analysis. It investigates
21,the behavior of a running program and gathers information. This
21,information can be viewed and gives hints for further optimization.
21,It is not necessary to recompile or use wrapper libraries to
21,"use OProfile. Not even a kernel patch is needed. Usually, when"
21,"profiling an application, a small overhead is expected, depending on the"
21,workload and sampling frequency.
21,8 Dynamic debug—kernel debugging messages
21,Dynamic debug is a powerful debugging feature in the Linux kernel that
21,allows you to enable and disable debugging messages at runtime without
21,the need to recompile the kernel or reboot the system.
21,4 SystemTap—filtering and analyzing system data #
21,SystemTap provides a command line interface and a scripting language to
21,"examine the activities of a running Linux system, particularly the kernel,"
21,in fine detail. SystemTap scripts are written in the SystemTap scripting
21,"language, are then compiled to C-code kernel modules and inserted into the"
21,"kernel. The scripts can be designed to extract, filter and summarize data,"
21,thus allowing the diagnosis of complex performance problems or functional
21,problems. SystemTap provides information similar to the output of tools
21,"like netstat, ps,"
21,"top, and iostat. However, more"
21,filtering and analysis options can be used for the collected information.
21,4.1 Conceptual overview #
21,"Each time you run a SystemTap script, a SystemTap session is started."
21,Several passes are done on the script before it is allowed to run.
21,"Then, the script is compiled into a kernel module and loaded. If the"
21,script has been executed before and no system components have changed
21,"(for example, different compiler or kernel versions, library paths, or"
21,"script contents), SystemTap does not compile the script again. Instead,"
21,it uses the *.c and *.ko data
21,stored in the SystemTap cache (~/.systemtap).
21,"The module is unloaded when the tap has finished running. For an example, see"
21,"the test run in Section 4.2, “Installation and setup” and the"
21,respective explanation.
21,4.1.1 SystemTap scripts #
21,SystemTap usage is based on SystemTap scripts
21,(*.stp). They tell SystemTap which type of
21,"information to collect, and what to do once that information is"
21,collected. The scripts are written in the SystemTap scripting language
21,"that is similar to AWK and C. For the language definition, see"
21,https://sourceware.org/systemtap/langref/. A lot of
21,useful example scripts are available from
21,http://www.sourceware.org/systemtap/examples/.
21,The essential idea behind a SystemTap script is to name
21,"events, and to give them handlers."
21,"When SystemTap runs the script, it monitors for certain events. When an"
21,"event occurs, the Linux kernel runs the handler as a sub-routine, then"
21,"resumes. Thus, events serve as the triggers for handlers to run."
21,Handlers can record specified data and print it in a certain manner.
21,"The SystemTap language only uses a few data types (integers, strings,"
21,"and associative arrays of these), and full control structures (blocks,"
21,"conditionals, loops, functions). It has a lightweight punctuation"
21,(semicolons are optional) and does not need detailed declarations (types
21,are inferred and checked automatically).
21,"For more information about SystemTap scripts and their syntax, refer to"
21,"Section 4.3, “Script syntax” and to the"
21,stapprobes and stapfuncs man
21,"pages, that are available with the"
21,systemtap-docs package.
21,4.1.2 Tapsets #
21,Tapsets are a library of pre-written probes and functions that can be
21,"used in SystemTap scripts. When a user runs a SystemTap script,"
21,SystemTap checks the script's probe events and handlers against the
21,tapset library. SystemTap then loads the corresponding probes and
21,functions before translating the script to C. Like SystemTap scripts
21,"themselves, tapsets use the file name extension"
21,*.stp.
21,"However, unlike SystemTap scripts, tapsets are not meant for direct"
21,execution. They constitute the library from which other scripts can pull
21,"definitions. Thus, the tapset library is an abstraction layer designed"
21,to make it easier for users to define events and functions. Tapsets
21,provide aliases for functions that users could want to specify as an
21,event. Knowing the proper alias is often easier than remembering
21,specific kernel functions that might vary between kernel versions.
21,4.1.3 Commands and privileges #
21,The main commands associated with SystemTap are stap
21,"and staprun. To execute them, you either need"
21,root privileges or must be a member of the
21,stapdev or
21,stapusr group.
21,stap
21,"SystemTap front-end. Runs a SystemTap script (either from file, or"
21,"from standard input). It translates the script into C code, compiles"
21,"it, and loads the resulting kernel module into a running Linux"
21,"kernel. Then, the requested system trace or probe functions are"
21,performed.
21,staprun
21,SystemTap back-end. Loads and unloads kernel modules produced by the
21,SystemTap front-end.
21,"For a list of options for each command, use --help. For"
21,"details, refer to the stap and the"
21,staprun man pages.
21,To avoid giving root access to users solely to enable them to work
21,"with SystemTap, use one of the following SystemTap groups. They are not available"
21,"by default on SUSE Linux Enterprise Server, but you can create the groups and modify the"
21,access rights accordingly.
21,Also adjust the permissions of the
21,staprun command if the security implications are
21,appropriate for your environment.
21,stapdev
21,Members of this group can run SystemTap scripts with
21,"stap, or run SystemTap instrumentation modules"
21,with staprun. As running stap
21,involves compiling scripts into kernel modules and loading them into
21,"the kernel, members of this group still have effective root"
21,access.
21,stapusr
21,Members of this group are only allowed to run SystemTap
21,"instrumentation modules with staprun. In addition,"
21,they can only run those modules from
21,/lib/modules/KERNEL_VERSION/systemtap/.
21,This directory must be owned by root and must only be
21,writable for the root user.
21,4.1.4 Important files and directories #
21,The following list gives an overview of the SystemTap main files and
21,directories.
21,/lib/modules/KERNEL_VERSION/systemtap/
21,Holds the SystemTap instrumentation modules.
21,/usr/share/systemtap/tapset/
21,Holds the standard library of tapsets.
21,/usr/share/doc/packages/systemtap/examples
21,Holds several example SystemTap scripts for various purposes.
21,Only available if the
21,systemtap-docs package is
21,installed.
21,~/.systemtap/cache
21,Data directory for cached SystemTap files.
21,/tmp/stap*
21,"Temporary directory for SystemTap files, including translated C code"
21,and kernel object.
21,4.2 Installation and setup #
21,"As SystemTap needs information about the kernel, some additional"
21,kernel-related packages must be installed. For each kernel you want to
21,"probe with SystemTap, you need to install a set of the following"
21,packages. This set should exactly match the kernel version and flavor
21,(indicated by * in the overview below).
21,Important: Repository for packages with debugging information
21,"If you subscribed your system for online updates, you can find"
21,“debuginfo” packages in the
21,*-Debuginfo-Updates online installation repository
21,relevant for SUSE Linux Enterprise Server 15 SP3. Use YaST to
21,enable the repository.
21,"For the classic SystemTap setup, install the following packages (using"
21,either YaST or zypper).
21,systemtap
21,systemtap-server
21,systemtap-docs (optional)
21,kernel-*-base
21,kernel-*-debuginfo
21,kernel-*-devel
21,kernel-source-*
21,gcc
21,To get access to the man pages and to a helpful collection of example
21,"SystemTap scripts for various purposes, additionally install the"
21,systemtap-docs package.
21,To check if all packages are correctly installed on the machine and if
21,"SystemTap is ready to use, execute the following command as"
21,root.
21,"# stap -v -e 'probe vfs.read {printf(""read performed\n""); exit()}'"
21,It probes the currently used kernel by running a script and returning an
21,"output. If the output is similar to the following, SystemTap is"
21,successfully deployed and ready to use:
21,Pass 1: parsed user script and 59 library script(s) in 80usr/0sys/214real ms.
21,"Pass 2: analyzed script: 1 probe(s), 11 function(s), 2 embed(s), 1 global(s) in"
21,140usr/20sys/412real ms.
21,Pass 3: translated to C into
21,"""/tmp/stapDwEk76/stap_1856e21ea1c246da85ad8c66b4338349_4970.c"" in 160usr/0sys/408real ms."
21,"Pass 4: compiled C into ""stap_1856e21ea1c246da85ad8c66b4338349_4970.ko"" in"
21,2030usr/360sys/10182real ms.
21,Pass 5: starting run.
21,read performed
21,Pass 5: run completed in 10usr/20sys/257real ms.1
21,Checks the script against the existing tapset library in
21,/usr/share/systemtap/tapset/ for any tapsets used.
21,Tapsets are scripts that form a library of pre-written probes and
21,functions that can be used in SystemTap scripts.
21,Examines the script for its components.
21,Translates the script to C. Runs the system C compiler to create a
21,kernel module from it. Both the resulting C code
21,(*.c) and the kernel module
21,"(*.ko) are stored in the SystemTap cache,"
21,~/.systemtap.
21,Loads the module and enables all the probes (events and handlers) in
21,the script by hooking into the kernel. The event being probed is a
21,"Virtual File System (VFS) read. As the event occurs on any processor, a"
21,valid handler is executed (prints the text read
21,performed) and closed with no errors.
21,"After the SystemTap session is terminated, the probes are disabled, and"
21,the kernel module is unloaded.
21,"In case any error messages appear during the test, check the output for"
21,hints about any missing packages and make sure they are installed
21,correctly. Rebooting and loading the appropriate kernel may also be
21,needed.
21,4.3 Script syntax #
21,SystemTap scripts consist of the following two components:
21,SystemTap events (probe points)
21,Name the kernel events at the associated handler should be executed.
21,"Examples for events are entering or exiting a certain function, a"
21,"timer expiring, or starting or terminating a session."
21,SystemTap handlers (probe body)
21,Series of script language statements that specify the work to be done
21,whenever a certain event occurs. This normally includes extracting
21,"data from the event context, storing them into internal variables, or"
21,printing results.
21,An event and its corresponding handler is collectively called a
21,probe. SystemTap events are also called probe
21,points. A probe's handler is also called a probe
21,body.
21,Comments can be inserted anywhere in the SystemTap script in various
21,"styles: using either #, /* */, or"
21,// as marker.
21,4.3.1 Probe format #
21,A SystemTap script can have multiple probes. They must be written in the
21,following format:
21,probe EVENT {STATEMENTS}
21,Each probe has a corresponding statement block. This statement block
21,must be enclosed in { } and contains the statements
21,to be executed per event.
21,Example 4.1: Simple SystemTap script #
21,The following example shows a simple SystemTap script.
21,probe1 begin2
21,"printf4 (""hello world\n"")5"
21,exit ()6
21,}71
21,Start of the probe.
21,Event begin (the start of the SystemTap session).
21,"Start of the handler definition, indicated by {."
21,First function defined in the handler: the printf
21,function.
21,"String to be printed by the printf function,"
21,followed by a line break (/n).
21,Second function defined in the handler: the exit()
21,function. Note that the SystemTap script will continue to run until
21,the exit() function executes. If you want to stop
21,"the execution of the script before, stop it manually by pressing"
21,Ctrl–C.
21,"End of the handler definition, indicated by }."
21,The event begin
21,(the start of the SystemTap session) triggers the handler enclosed in
21,"{ }. Here, that is the printf"
21,function
21,"In this case, it prints hello world followed by a"
21,new line
21,"Then, the script exits."
21,"If your statement block holds several statements, SystemTap executes"
21,these statements in sequence—you do not need to insert special
21,separators or terminators between multiple statements. A statement block
21,"can also be nested within another statement blocks. Generally, statement"
21,blocks in SystemTap scripts use the same syntax and semantics as in the
21,C programming language.
21,4.3.2 SystemTap events (probe points) #
21,SystemTap supports several built-in events.
21,The general event syntax is a dotted-symbol sequence. This allows a
21,breakdown of the event namespace into parts. Each component identifier
21,"may be parameterized by a string or number literal, with a syntax like a"
21,"function call. A component may include a * character,"
21,to expand to other matching probe points. A probe point may be followed
21,"by a ? character, to indicate that it is optional,"
21,and that no error should result if it fails to expand.
21,"Alternately, a probe point may be followed by a !"
21,character to indicate that it is both optional and sufficient.
21,SystemTap supports multiple events per probe—they need to be
21,"separated by a comma (,). If multiple events are"
21,"specified in a single probe, SystemTap will execute the handler when any"
21,of the specified events occur.
21,"In general, events can be classified into the following categories:"
21,Synchronous events: Occur when any process executes an instruction at
21,a particular location in kernel code. This gives other events a
21,reference point (instruction address) from which more contextual data
21,may be available.
21,An example for a synchronous event is
21,vfs.FILE_OPERATION: The
21,entry to the FILE_OPERATION event for
21,"Virtual File System (VFS). For example, in"
21,"Section 4.2, “Installation and setup”, read"
21,is the FILE_OPERATION event used for VFS.
21,Asynchronous events: Not tied to a particular instruction or location
21,"in code. This family of probe points consists mainly of counters,"
21,"timers, and similar constructs."
21,Examples for asynchronous events are: begin (start
21,"of a SystemTap session—when a SystemTap script is run,"
21,"end (end of a SystemTap session), or timer events."
21,"Timer events specify a handler to be executed periodically, like"
21,example
21,"timer.s(SECONDS), or"
21,timer.ms(MILLISECONDS).
21,"When used together with other probes that collect information,"
21,timer events allow you to print periodic updates and see how that
21,information changes over time.
21,Example 4.2: Probe with timer event #
21,"For example, the following probe would print the text “hello"
21,world” every 4 seconds:
21,probe timer.s(4)
21,"printf(""hello world\n"")"
21,"For detailed information about supported events, refer to the"
21,stapprobes man page. The See
21,Also section of the man page also contains links to other
21,man pages that discuss supported events for specific subsystems and
21,components.
21,4.3.3 SystemTap handlers (probe body) #
21,Each SystemTap event is accompanied by a corresponding handler defined
21,"for that event, consisting of a statement block."
21,4.3.3.1 Functions #
21,"If you need the same set of statements in multiple probes, you can"
21,place them in a function for easy reuse. Functions are defined by the
21,keyword function followed by a name. They take any
21,number of string or numeric arguments (by value) and may return a
21,single string or number.
21,function FUNCTION_NAME(ARGUMENTS) {STATEMENTS}
21,probe EVENT {FUNCTION_NAME(ARGUMENTS)}
21,The statements in FUNCTION_NAME are executed
21,when the probe for EVENT executes. The
21,ARGUMENTS are optional values passed into
21,the function.
21,Functions can be defined anywhere in the script. They may take any
21,One of the functions needed very often was already introduced in
21,"Example 4.1, “Simple SystemTap script”: the printf"
21,function for printing data in a formatted way. When using the
21,"printf function, you can specify how arguments"
21,should be printed by using a format string. The format string is
21,"included in quotation marks and can contain further format specifiers,"
21,introduced by a % character.
21,Which format strings to use depends on your list of arguments. Format
21,strings can have multiple format specifiers—each matching a
21,corresponding argument. Multiple arguments can be separated by a comma.
21,"Example 4.3: printf Function with format specifiers #  printf (""1%s2(%d3) open\n4"", execname(), pid())1"
21,"Start of the format string, indicated by ""."
21,String format specifier.
21,Integer format specifier.
21,"End of the format string, indicated by ""."
21,The example above prints the current executable name
21,(execname()) as a string and the process ID
21,"(pid()) as an integer in brackets. Then, a space,"
21,the word open and a line break follow:
21,[...]
21,vmware-guestd(2206) open
21,hald(2360) open
21,[...]
21,Apart from the two functions execname()and
21,pid()) used in
21,"Example 4.3, “printf Function with format specifiers”, a variety of other"
21,functions can be used as printf arguments.
21,Among the most commonly used SystemTap functions are the following:
21,tid()
21,ID of the current thread.
21,pid()
21,Process ID of the current thread.
21,uid()
21,ID of the current user.
21,cpu()
21,Current CPU number.
21,execname()
21,Name of the current process.
21,gettimeofday_s()
21,"Number of seconds since Unix epoch (January 1, 1970)."
21,ctime()
21,Convert time into a string.
21,pp()
21,String describing the probe point currently being handled.
21,thread_indent()
21,Useful function for organizing print results. It (internally) stores
21,an indentation counter for each thread (tid()).
21,"The function takes one argument, an indentation delta, indicating"
21,how many spaces to add or remove from the thread's indentation
21,counter. It returns a string with some generic trace data along with
21,an appropriate number of indentation spaces. The generic data
21,returned includes a time stamp (number of microseconds since the
21,"initial indentation for the thread), a process name, and the thread"
21,"ID itself. This allows you to identify what functions were called,"
21,"who called them, and how long they took."
21,Call entries and exits often do not immediately precede each other
21,(otherwise it would be easy to match them). In between a first call
21,"entry and its exit, usually other call entries and exits"
21,are made. The indentation counter helps you match an entry with its
21,corresponding exit as it indents the next function call in case it
21,is not the exit of the previous one.
21,"For more information about supported SystemTap functions, refer to the"
21,stapfuncs man page.
21,4.3.3.2 Other basic constructs #
21,"Apart from functions, you can use other common constructs in"
21,"SystemTap handlers, including variables, conditional statements (like"
21,"if/else, while"
21,"loops, for loops, arrays or command line arguments."
21,4.3.3.2.1 Variables #
21,"Variables may be defined anywhere in the script. To define one, simply"
21,choose a name and assign a value from a function or expression to it:
21,foo = gettimeofday( )
21,Then you can use the variable in an expression. From the type of
21,"values assigned to the variable, SystemTap automatically infers the"
21,type of each identifier (string or number). Any inconsistencies will
21,"be reported as errors. In the example above, foo"
21,would automatically be classified as a number and could be printed via
21,printf() with the integer format specifier
21,(%d).
21,"However, by default, variables are local to the probe they are used"
21,"in: They are initialized, used and disposed of at each handler"
21,"evocation. To share variables between probes, declare them global"
21,"anywhere in the script. To do so, use the global"
21,keyword outside of the probes:
21,"Example 4.4: Using global variables #  global count_jiffies, count_ms"
21,probe timer.jiffies(100) { count_jiffies ++ }
21,probe timer.ms(100) { count_ms ++ }
21,probe timer.ms(12345)
21,hz=(1000*count_jiffies) / count_ms
21,"printf (""jiffies:ms ratio %d:%d => CONFIG_HZ=%d\n"","
21,"count_jiffies, count_ms, hz)"
21,exit ()
21,This example script computes the CONFIG_HZ setting of the kernel by
21,"using timers that count jiffies and milliseconds, then computing"
21,accordingly. (A jiffy is the duration of one tick of the system timer
21,"interrupt. It is not an absolute time interval unit, since its"
21,duration depends on the clock interrupt frequency of the particular
21,hardware platform). With the global statement it
21,is possible to use the variables count_jiffies and
21,count_ms also in the probe
21,timer.ms(12345). With ++ the
21,value of a variable is incremented by 1.
21,4.3.3.2.2 Conditional statements #
21,There are several conditional statements that you can use in
21,SystemTap scripts. The following are probably the most common:
21,If/else statements
21,They are expressed in the following format:
21,if (CONDITION)1STATEMENT12
21,else3STATEMENT24
21,The if statement compares an integer-valued
21,expression to zero. If the condition expression
21,"is non-zero, the first statement"
21,"is executed. If the condition expression is zero, the second"
21,statement
21,is executed. The else clause
21,and
21,is optional. Both
21,and
21,can also be statement blocks.
21,While loops
21,They are expressed in the following format:
21,while (CONDITION)1STATEMENT2
21,"As long as condition is non-zero, the statement"
21,is executed.
21,can also be a statement block. It must change a value so
21,condition will eventually be zero.
21,For loops
21,They are a shortcut for while loops and are
21,expressed in the following format:
21,for (INITIALIZATION1; CONDITIONAL2; INCREMENT3) statement
21,The expression specified in
21,is used to initialize a counter for the number of loop iterations
21,and is executed before execution of the loop starts. The execution
21,of the loop continues until the loop condition
21,is false. (This expression is checked at the beginning of each loop
21,iteration). The expression specified in
21,is used to increment the loop counter. It is executed at the end of
21,each loop iteration.
21,Conditional operators
21,The following operators can be used in conditional statements:
21,==:
21,Is equal to
21,!=:
21,Is not equal to
21,>=:
21,Is greater than or equal to
21,<=:
21,Is less than or equal to
21,4.4 Example script #
21,If you have installed the
21,"systemtap-docs package, you can"
21,find several useful SystemTap example scripts in
21,/usr/share/doc/packages/systemtap/examples.
21,This section describes a rather simple example script in more detail:
21,/usr/share/doc/packages/systemtap/examples/network/tcp_connections.stp.
21,Example 4.5: Monitoring incoming TCP connections with tcp_connections.stp #  #! /usr/bin/env stap
21,probe begin {
21,"printf(""%6s %16s %6s %6s %16s\n"","
21,"""UID"", ""CMD"", ""PID"", ""PORT"", ""IP_SOURCE"")"
21,"probe kernel.function(""tcp_accept"").return?,"
21,"kernel.function(""inet_csk_accept"").return? {"
21,sock = $return
21,if (sock != 0)
21,"printf(""%6d %16s %6d %6d %16s\n"", uid(), execname(), pid(),"
21,"inet_get_local_port(sock), inet_get_ip_source(sock))"
21,This SystemTap script monitors the incoming TCP connections and helps to
21,identify unauthorized or unwanted network access requests in real time.
21,It shows the following information for each new incoming TCP connection
21,accepted by the computer:
21,User ID (UID)
21,Command accepting the connection (CMD)
21,Process ID of the command (PID)
21,Port used by the connection (PORT)
21,IP address from which the TCP connection originated
21,(IP_SOUCE)
21,"To run the script, execute"
21,stap /usr/share/doc/packages/systemtap/examples/network/tcp_connections.stp
21,"and follow the output on the screen. To manually stop the script, press"
21,Ctrl–C.
21,4.5 User space probing #
21,"For debugging user space applications (like DTrace can do),"
21,SUSE Linux Enterprise Server 15 SP3 supports user space probing with
21,SystemTap: Custom probe points can be inserted in any user space
21,"application. Thus, SystemTap lets you use both kernel space and user space"
21,probes to debug the behavior of the whole system.
21,To get the required utrace infrastructure and the uprobes kernel module
21,"for user space probing, you need to install the"
21,kernel-trace package in
21,addition to the packages listed in
21,"Section 4.2, “Installation and setup”."
21,utrace implements a framework for controlling
21,user space tasks. It provides an interface that can be used by various
21,"tracing “engines”, implemented as loadable kernel modules."
21,"The engines register callback functions for specific events, then attach"
21,to whichever thread they want to trace. As the callbacks are made from
21,"“safe” places in the kernel, this allows for great leeway in"
21,the kinds of processing the functions can do. Various events can be
21,"watched via utrace, for example, system call entry and exit, fork(),"
21,"signals being sent to the task, etc. More details about the utrace"
21,infrastructure are available at
21,https://sourceware.org/systemtap/wiki/utrace.
21,SystemTap includes support for probing the entry into and return from a
21,"function in user space processes, probing predefined markers in"
21,"user space code, and monitoring user-process events."
21,To check if the currently running kernel provides the needed utrace
21,"support, use the following command:"
21,> sudo grep CONFIG_UTRACE /boot/config-`uname -r`
21,"For more details about user space probing, refer to"
21,https://sourceware.org/systemtap/SystemTap_Beginners_Guide/userspace-probing.html.
21,4.6 More information #
21,This chapter only provides a short SystemTap overview. Refer to the
21,following links for more information about SystemTap:
21,https://sourceware.org/systemtap/
21,SystemTap project home page.
21,https://sourceware.org/systemtap/wiki/
21,"Huge collection of useful information about SystemTap, ranging from"
21,detailed user and developer documentation to reviews and comparisons
21,"with other tools, or Frequently Asked Questions and tips. Also"
21,"contains collections of SystemTap scripts, examples and usage stories"
21,and lists recent talks and papers about SystemTap.
21,https://sourceware.org/systemtap/documentation.html
21,"Features a SystemTap Tutorial, a"
21,"SystemTap Beginner's Guide, a Tapset"
21,"Developer's Guide, and a SystemTap Language"
21,Reference in PDF and HTML format. Also lists the relevant
21,man pages.
21,You can also find the SystemTap language reference and SystemTap tutorial
21,in your installed system under
21,/usr/share/doc/packages/systemtap. Example SystemTap
21,scripts are available from the example subdirectory.
21,5 Kernel probes #
21,Kernel probes are a set of tools to collect Linux kernel debugging and
21,performance information. Developers and system administrators usually use
21,"them either to debug the kernel, or to find system performance"
21,bottlenecks. The reported data can then be used to tune the system for
21,better performance.
21,"You can insert these probes into any kernel routine, and specify a handler"
21,to be invoked after a particular break-point is hit. The main advantage of
21,kernel probes is that you no longer need to rebuild the kernel and reboot
21,the system after you make changes in a probe.
21,"To use kernel probes, you typically need to write or obtain a specific"
21,kernel module. Such modules include both the init and
21,the exit function. The init function (such as
21,"register_kprobe()) registers one or more probes,"
21,while the exit function unregisters them. The registration function
21,defines where the probe will be inserted and
21,which handler will be called after the probe is hit.
21,"To register or unregister a group of probes at one time, you can use"
21,relevant
21,register_<PROBE_TYPE>probes()
21,unregister_<PROBE_TYPE>probes()
21,functions.
21,Debugging and status messages are typically reported with the
21,printk kernel routine.
21,printk is a kernel space equivalent of a
21,user space printf routine. For more information
21,"on printk, see"
21,Logging
21,"kernel messages. Normally, you can view these messages by"
21,inspecting the output of the systemd journal (see
21,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”). For more information on log files, see"
21,"Chapter 3, System log files."
21,5.1 Supported architectures #
21,Kernel probes are fully implemented on the following
21,architectures:
21,x86
21,AMD64/Intel 64
21,Arm
21,POWER
21,Kernel probes are partially implemented on the
21,following architectures:
21,IA64 (does not support probes on instruction
21,slot1)
21,sparc64 (return probes not yet implemented)
21,5.2 Types of kernel probes #
21,"There are three types of kernel probes: Kprobes,"
21,"Jprobes, and Kretprobes."
21,Kretprobes are sometimes called return
21,probes. You can find source code examples of all three type of
21,probes in the Linux kernel. See the directory
21,/usr/src/linux/samples/kprobes/ (package
21,kernel-source).
21,5.2.1 Kprobes #
21,Kprobes can be attached to any instruction in the Linux kernel.
21,"When Kprobes is registered, it inserts a break-point at the first"
21,byte of the probed instruction. When the processor hits this
21,"break-point, the processor registers are saved, and the processing"
21,"passes to Kprobes. First, a pre-handler is"
21,"executed, then the probed instruction is stepped, and, finally a"
21,post-handler is executed. The control is then passed
21,to the instruction following the probe point.
21,5.2.2 Jprobes #
21,Jprobes is implemented through the Kprobes mechanism. It is
21,inserted on a function's entry point and allows direct access to the
21,arguments of the function which is being probed. Its handler routine
21,must have the same argument list and return value as the probed
21,"function. To end it, call the jprobe_return()"
21,function.
21,"When a jprobe is hit, the processor registers are saved, and the"
21,instruction pointer is directed to the jprobe handler routine. The
21,control then passes to the handler with the same register contents as the
21,"function being probed. Finally, the handler calls the"
21,"jprobe_return() function, and switches the"
21,control back to the control function.
21,"In general, you can insert multiple probes on one function. Jprobe is,"
21,"however, limited to only one instance per function."
21,5.2.3 Return probe #
21,Return probes are also implemented through Kprobes. When the
21,"register_kretprobe() function is called, a"
21,kprobe is attached to the entry of the probed function.
21,"After hitting the probe, the kernel probes mechanism saves the probed"
21,function return address and calls a user-defined return handler. The
21,control is then passed back to the probed function.
21,"Before you call register_kretprobe(), you need"
21,"to set a maxactive argument, which specifies"
21,how many instances of the function can be probed at the same time. If
21,"set too low, you will miss a certain number of probes."
21,5.3 Kprobes API #
21,The programming interface of Kprobes consists of functions which are
21,"used to register and unregister all used kernel probes, and associated"
21,probe handlers. For a more detailed description of these functions and
21,"their arguments, see the information sources in"
21,"Section 5.5, “More information”."
21,register_kprobe()
21,Inserts a break-point on a specified address. When the break-point is
21,"hit, the pre_handler and"
21,post_handler are called.
21,register_jprobe()
21,Inserts a break-point in the specified address. The address needs to
21,be the address of the first instruction of the probed function. When
21,"the break-point is hit, the specified handler is run. The handler"
21,should have the same argument list and return type as the probed.
21,register_kretprobe()
21,Inserts a return probe for the specified function. When the probed
21,"function returns, a specified handler is run. This function returns 0"
21,"on success, or a negative error number on failure."
21,"unregister_kprobe(), unregister_jprobe(), unregister_kretprobe()"
21,Removes the specified probe. You can use it any time after the probe
21,has been registered.
21,"register_kprobes(), register_jprobes(), register_kretprobes()"
21,Inserts each of the probes in the specified array.
21,"unregister_kprobes(), unregister_jprobes(), unregister_kretprobes()"
21,Removes each of the probes in the specified array.
21,"disable_kprobe(), disable_jprobe(), disable_kretprobe()"
21,Disables the specified probe temporarily.
21,"enable_kprobe(), enable_jprobe(), enable_kretprobe()"
21,Temporarily enables disabled probes.
21,5.4 debugfs Interface #
21,"In recent Linux kernels, the Kprobes instrumentation uses the"
21,kernel's debugfs interface. It can list all
21,registered probes and globally switch all probes on or off.
21,5.4.1 Listing registered kernel probes #
21,The list of all currently registered probes is in the
21,/sys/kernel/debug/kprobes/list file.
21,saturn.example.com:~ # cat /sys/kernel/debug/kprobes/list
21,c015d71a
21,vfs_read+0x0
21,[DISABLED]
21,c011a316
21,do_fork+0x0
21,c03dedc5
21,tcp_v4_rcv+0x0
21,The first column lists the address in the kernel where the probe is
21,inserted. The second column prints the type of the probe:
21,"k for kprobe, j for jprobe, and"
21,r for return probe. The third column specifies the
21,"symbol, offset and optional module name of the probe. The following"
21,optional columns include the status information of the probe. If the
21,"probe is inserted on a virtual address which is not valid anymore, it is"
21,marked with [GONE]. If the probe is temporarily
21,"disabled, it is marked with [DISABLED]."
21,5.4.2 Globally enabling/disabling kernel probes #
21,The /sys/kernel/debug/kprobes/enabled file
21,represents a switch with which you can globally and forcibly turn on or
21,"off all the registered kernel probes. To turn them off, simply enter"
21,"# echo ""0"" > /sys/kernel/debug/kprobes/enabled"
21,"on the command line as root. To turn them on again, enter"
21,"# echo ""1"" > /sys/kernel/debug/kprobes/enabled"
21,Note that this way you do not change the status of the probes. If a
21,"probe is temporarily disabled, it will not be enabled automatically but"
21,will remain in the [DISABLED] state after entering
21,the latter command.
21,5.5 More information #
21,"To learn more about kernel probes, look at the following sources of"
21,information:
21,Thorough but more technically oriented information about kernel probes
21,is in /usr/src/linux/Documentation/kprobes.txt
21,(package kernel-source).
21,Examples of all three types of probes (together with related
21,Makefile) are in the
21,/usr/src/linux/samples/kprobes/ directory (package
21,kernel-source).
21,In-depth information about Linux kernel modules and
21,printk kernel routine can be found at
21,The
21,Linux Kernel Module Programming Guide
21,6 Hardware-based performance monitoring with Perf #
21,Perf is an interface to access the performance monitoring unit (PMU) of a
21,processor and to record and display software events such as page faults.
21,"It supports system-wide, per-thread, and KVM virtualization guest"
21,monitoring.
21,You can store resulting information in a report.
21,"This report contains information about, for example, instruction pointers or"
21,what code a thread was executing.
21,Perf consists of two parts:
21,Code integrated into the Linux kernel that is responsible for instructing
21,the hardware.
21,The perf user space utility that allows you to use the
21,kernel code and helps you analyze gathered data.
21,6.1 Hardware-based monitoring #
21,Performance monitoring means collecting information related to how an
21,application or system performs.
21,This information can be obtained either through software-based means or from
21,the CPU or chipset.
21,Perf integrates both of these methods.
21,Many modern processors contain a performance monitoring unit (PMU).
21,The design and functionality of a PMU is CPU-specific.
21,"For example, the number of registers, counters and features supported will"
21,vary by CPU implementation.
21,Each PMU model consists of a set of registers: the performance monitor
21,configuration (PMC) and the performance monitor data (PMD).
21,"Both can be read, but only PMCs are writable."
21,These registers store configuration information and data.
21,6.2 Sampling and counting #
21,Perf supports several profiling modes:
21,Counting.
21,Count the number of occurrences of an event.
21,Event-based sampling.
21,A less exact way of counting: A sample is recorded whenever a certain
21,threshold number of events has occurred.
21,Time-based sampling.
21,A less exact way of counting: A sample is recorded in a defined frequency.
21,Instruction-based sampling (AMD64 only).
21,The processor follows instructions appearing in a given interval and
21,samples which events they produce.
21,This allows following up on individual instructions and seeing which of
21,them is critical to performance.
21,6.3 Installing Perf #
21,The Perf kernel code is already included with the default kernel.
21,"To be able to use the user space utility, install the package"
21,perf.
21,6.4 Perf subcommands #
21,"To gather the required information, the perf tool has"
21,several subcommands. This section gives an overview of the most often used
21,commands.
21,"To see help in the form of a man page for any of the subcommands, use either"
21,perf helpSUBCOMMAND
21,man perf-SUBCOMMAND.
21,perf stat
21,Start a program and create a statistical overview that is displayed after
21,the program quits.
21,perf stat is used to count events.
21,perf record
21,Start a program and create a report with performance counter information.
21,The report is stored as perf.data in the current
21,directory.
21,perf record is used to sample events.
21,perf report
21,Display a report that was previously created with
21,perf record.
21,perf annotate
21,Display a report file and an annotated version of the executed
21,code.
21,"If debug symbols are installed, you will also see the source code"
21,displayed.
21,perf list
21,List event types that Perf can report with the current kernel and with
21,your CPU.
21,"You can filter event types by category—for example, to see hardware"
21,"events only, use perf list hw."
21,The man page for perf_event_open has short descriptions
21,for the most important events.
21,"For example, to find a description of the event"
21,"branch-misses, search for"
21,BRANCH_MISSES (note the spelling differences):
21,> man perf_event_open | grep -A5 BRANCH_MISSES
21,"Sometimes, events may be ambiguous."
21,Note that the lowercase hardware event names are not the name of raw
21,hardware events but instead the name of aliases created by Perf.
21,These aliases map to differently named but similarly defined hardware
21,events on each supported processor.
21,"For example, the cpu-cycles event is mapped to"
21,the hardware event UNHALTED_CORE_CYCLES on
21,Intel processors.
21,"On AMD processors, however, it is mapped to the hardware event"
21,CPU_CLK_UNHALTED.
21,Perf also allows measuring raw events specific to your hardware.
21,"To look up their descriptions, see the"
21,Architecture Software Developer's Manual of your CPU vendor.
21,The relevant documents for AMD64/Intel 64 processors are linked to in
21,"Section 6.7, “More information”."
21,perf top
21,Display system activity as it happens.
21,perf trace
21,This command behaves similarly to strace.
21,"With this subcommand, you can see which system calls are executed by a"
21,particular thread or process and which signals it receives.
21,6.5 Counting particular types of event #
21,"To count the number of occurrences of an event, such as those displayed by"
21,"perf list, use:"
21,# perf stat -e EVENT -a
21,"To count multiple types of events at once, list them separated by commas."
21,"For example, to count cpu-cycles and"
21,"instructions, use:"
21,"# perf stat -e cpu-cycles,instructions -a"
21,"To stop the session, press"
21,Ctrl–C.
21,You can also count the number of occurrences of an event within a particular
21,time:
21,# perf stat -e EVENT -a -- sleep TIME
21,Replace TIME by a value in seconds.
21,6.6 Recording events specific to particular commands #
21,There are various ways to sample events specific to a particular command:
21,"To create a report for a newly invoked command, use:"
21,# perf record COMMAND
21,"Then, use the started process normally."
21,"When you quit the process, the Perf session will also stop."
21,To create a report for the entire system while a newly invoked command is
21,"running, use:"
21,# perf record -a COMMAND
21,"Then, use the started process normally."
21,"When you quit the process, the Perf session will also stop."
21,"To create a report for an already running process, use:"
21,# perf record -p PID
21,Replace PID with a process ID.
21,"To stop the session, press"
21,Ctrl–C.
21,Now you can view the gathered data (perf.data)
21,using:
21,> perf report
21,This will open a pseudo-graphical interface.
21,"To receive help, press H."
21,"To quit, press Q."
21,"If you prefer a graphical interface, try the GTK+ interface of Perf:"
21,> perf report --gtk
21,"However, note that the GTK+ interface is very limited in functionality."
21,6.7 More information #
21,This chapter only provides a short overview. Refer to the following links
21,for more information:
21,https://perf.wiki.kernel.org/index.php/Main_Page
21,The project home page.
21,It also features a tutorial on using perf.
21,http://www.brendangregg.com/perf.html
21,Unofficial page with many one-line examples of how to use
21,perf.
21,http://web.eece.maine.edu/~vweaver/projects/perf_events/
21,"Unofficial page with several resources, mostly relating to the Linux"
21,kernel code of Perf and its API.
21,"This page includes, for example, a CPU compatibility table and a"
21,programming guide.
21,https://www-ssl.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.pdf
21,"The Intel Architectures Software Developer's Manual,"
21,Volume 3B.
21,https://support.amd.com/TechDocs/24593.pdf
21,"The AMD Architecture Programmer's Manual, Volume 2."
21,"Chapter 7, OProfile—system-wide profiler"
21,Consult this chapter for other performance optimizations.
21,7 OProfile—system-wide profiler #
21,OProfile is a profiler for dynamic program analysis. It investigates
21,the behavior of a running program and gathers information. This
21,information can be viewed and gives hints for further optimization.
21,It is not necessary to recompile or use wrapper libraries to
21,"use OProfile. Not even a kernel patch is needed. Usually, when"
21,"profiling an application, a small overhead is expected, depending on the"
21,workload and sampling frequency.
21,7.1 Conceptual overview #
21,OProfile consists of a kernel driver and a daemon for collecting data.
21,It uses the hardware performance counters provided on many
21,processors. OProfile is capable of profiling all code
21,"including the kernel, kernel modules, kernel interrupt handlers, system"
21,"shared libraries, and other applications."
21,Modern processors support profiling through the hardware by performance
21,"counters. Depending on the processor, there can be many counters and each"
21,of these can be programmed with an event to count. Each counter has a
21,"value which determines how often a sample is taken. The lower the value,"
21,the more often it is used.
21,"During the post-processing step, all information is collected and"
21,instruction addresses are mapped to a function name.
21,7.2 Installation and requirements #
21,"To use OProfile, install the oprofile package."
21,"OProfile works on AMD64/Intel 64, IBM Z, and POWER processors."
21,It is useful to install the *-debuginfo package for
21,the respective application you want to profile. If you want to profile
21,"the kernel, you need the debuginfo package as well."
21,7.3 Available OProfile utilities #
21,OProfile contains several utilities to handle the profiling process and
21,its profiled data. The following list is a short summary of programs used
21,in this chapter:
21,opannotate
21,Outputs annotated source or assembly listings mixed with profile
21,information. An annotated report can be used in combination with
21,addr2line to identify the source file and line
21,where hotspots potentially exist. See man addr2line
21,for more information.
21,operf
21,"Profiler tool. After profiling stops, the data that is by default stored in"
21,CUR_DIR/oprofile_data/samples/current
21,"can be processed by opreport, for example."
21,ophelp
21,Lists available events with short descriptions.
21,opimport
21,Converts sample database files from a foreign binary format to the
21,native format.
21,opreport
21,Generates reports from profiled data.
21,7.4 Using OProfile #
21,"With OProfile, you can profile both the kernel and applications. When"
21,"profiling the kernel, tell OProfile where to find the"
21,vmlinuz* file. Use the --vmlinux
21,option and point it to vmlinuz* (usually in
21,"/boot). If you need to profile kernel modules,"
21,"OProfile does this by default. However, make sure you read"
21,http://oprofile.sourceforge.net/doc/kernel-profiling.html.
21,"Applications usually do not need to profile the kernel, therefore you"
21,should use the --no-vmlinux option to reduce the amount
21,of information.
21,7.4.1 Creating a report #
21,"Starting the daemon, collecting data, stopping the daemon, and creating"
21,a report for the application COMMAND.
21,Open a shell and log in as root.
21,Decide if you want to profile with or without the Linux kernel:
21,Profile with the Linux kernel.
21,"Execute the following commands, because"
21,operf can only work with uncompressed
21,images:
21,> cp /boot/vmlinux-`uname -r`.gz /tmp
21,> gunzip /tmp/vmlinux*.gz
21,> operf--vmlinux=/tmp/vmlinux* COMMANDProfile without the Linux kernel.
21,Use the following command:
21,# operf --no-vmlinux COMMAND
21,To see which functions call other functions in the
21,"output, additionally use the --callgraph option and"
21,set a maximum DEPTH:
21,# operf --no-vmlinux --callgraph
21,DEPTH COMMAND
21,operf writes its data to CUR_DIR/oprofile_data/samples/current.
21,After the operf command is finished (or is aborted by
21,"Ctrl–C),"
21,the data can be analyzed with oreport:
21,# opreport
21,Overflow stats not available
21,"CPU: CPU with timer interrupt, speed 0 MHz (estimated)"
21,Profiling through timer interrupt
21,TIMER:0|
21,samples|
21,------------------
21,84877 98.3226 no-vmlinux
21,...7.4.2 Getting event configurations #
21,The general procedure for event configuration is as follows:
21,Use first the events CPU-CLK_UNHALTED and
21,INST_RETIRED to find optimization opportunities.
21,"Use specific events to find bottlenecks. To list them, use the command"
21,perf list.
21,"If you need to profile certain events, first check the available events"
21,supported by your processor with the ophelp command
21,(example output generated from Intel Core i5 CPU):
21,# ophelp
21,"oprofile: available events for CPU type ""Intel Architectural Perfmon"""
21,See Intel 64 and IA-32 Architectures Software Developer's Manual
21,Volume 3B (Document 253669) Chapter 18 for architectural perfmon events
21,This is a limited set of fallback events because oprofile does not know your CPU
21,CPU_CLK_UNHALTED: (counter: all))
21,Clock cycles when not halted (min count: 6000)
21,INST_RETIRED: (counter: all))
21,number of instructions retired (min count: 6000)
21,LLC_MISSES: (counter: all))
21,Last level cache demand requests from this core that missed the LLC (min count: 6000)
21,Unit masks (default 0x41)
21,----------
21,0x41: No unit mask
21,LLC_REFS: (counter: all))
21,Last level cache demand requests from this core (min count: 6000)
21,Unit masks (default 0x4f)
21,----------
21,0x4f: No unit mask
21,BR_MISS_PRED_RETIRED: (counter: all))
21,number of mispredicted branches retired (precise) (min count: 500)
21,Specify the performance counter events with the option
21,--event. Multiple options are possible. This option
21,"needs an event name (from ophelp) and a sample rate,"
21,for example:
21,# operf --events CPU_CLK_UNHALTED:100000Warning: Setting sampling rates with CPU_CLK_UNHALTED
21,Setting low sampling rates can seriously impair the system performance
21,while high sample rates can disrupt the system to such a high degree
21,that the data is useless. It is recommended to tune the performance
21,metric for being monitored with and without OProfile and to
21,experimentally determine the minimum sample rate that disrupts the
21,performance the least.
21,7.5 Generating reports #
21,"Before generating a report, make sure the operf has"
21,stopped. Unless you have provided an output directory with
21,"--session-dir, operf has written its"
21,"data to CUR_DIR/oprofile_data/samples/current,"
21,and the reporting tools opreport and
21,opannotate will look there by default.
21,Calling opreport without any options gives a complete
21,"summary. With an executable as an argument, retrieve profile data only"
21,"from this executable. If you analyze applications written in C++, use the"
21,--demangle smart option.
21,The opannotate generates output with annotations from
21,source code. Run it with the following options:
21,# opannotate --source \
21,--base-dirs=BASEDIR \
21,--search-dirs=SEARCHDIR \
21,--output-dir=annotated/ \
21,/lib/libfoo.so
21,The option --base-dir contains a comma separated list of
21,paths which is stripped from debug source files. These paths were
21,searched prior to looking in --search-dirs. The
21,--search-dirs option is also a comma separated list of
21,directories to search for source files.
21,Note: Inaccuracies in annotated source
21,"Because of compiler optimization, code can disappear and appear in a"
21,different place. Use the information in
21,http://oprofile.sourceforge.net/doc/debug-info.html
21,to fully understand its implications.
21,7.6 More information #
21,This chapter only provides a short overview. Refer to the following links
21,for more information:
21,http://oprofile.sourceforge.net
21,The project home page.
21,Manpages
21,Details descriptions about the options of the different tools.
21,/usr/share/doc/packages/oprofile/oprofile.html
21,Contains the OProfile manual.
21,http://developer.intel.com/
21,Architecture reference for Intel processors.
21,https://www.ibm.com/support/knowledgecenter/ssw_aix_71/assembler/idalangref_arch_overview.html
21,"Architecture reference for PowerPC64 processors in IBM iSeries,"
21,"pSeries, and Blade server systems."
21,8 Dynamic debug—kernel debugging messages #
21,Dynamic debug is a powerful debugging feature in the Linux kernel that
21,allows you to enable and disable debugging messages at runtime without
21,the need to recompile the kernel or reboot the system.
21,"You can use dynamic debugging in several situations, such as:"
21,Troubleshooting kernel issues
21,Developing drivers for new hardware
21,Tracing and auditing security events
21,8.1 Benefits of dynamic debugging #
21,Certain benefits of dynamic debugging are listed below:
21,Real-time debugging
21,Dynamic debugging enables debugging messages without requiring a
21,system reboot. This real-time capability is crucial for diagnosing
21,issues in production environments.
21,Selective debugging
21,You can enable debugging messages for specific parts of the kernel
21,"or even individual modules, allowing you to focus on relevant"
21,information.
21,Performance tuning
21,Use dynamic debugging to monitor and optimize kernel performance by
21,selectively enabling or disabling debugging messages based on the
21,current analysis requirements.
21,8.2 Checking the status of dynamic debug #
21,"For supported kernel versions that are installed by default, dynamic"
21,"debug is already built-in. To check the status of dynamic debug, run the"
21,following command as the root user:
21,# zcat /proc/config.gz | grep CONFIG_DYNAMIC_DEBUG
21,"If dynamic debug is compiled into the kernel, you should see an output"
21,similar to the following:
21,CONFIG_DYNAMIC_DEBUG=y
21,CONFIG_DYNAMIC_DEBUG_CORE=y8.3 Using dynamic debug #
21,"To enable specific debug messages or logs within the running kernel, you"
21,can use the echo command and write to the
21,/sys/kernel/debug/dynamic_debug/control file.
21,The following examples illustrate certain simple uses of dynamic debug:
21,Note
21,"Dynamic debug relies on specific debugging macros, such as"
21,"pr_debug, embedded in the kernel code. These"
21,macros are used by kernel developers to insert debugging messages into
21,the code.
21,The examples in this section assume that the
21,pr_debug macro works correctly because dynamic
21,debug is allowed for the running kernel.
21,Enabling debug messages for a specific kernel source code file
21,To enable the debug messages for a specific kernel source code
21,"file, use the following example:"
21,"# echo ""file FILE_NAME.c +p"" > /sys/kernel/debug/dynamic_debug/controlEnabling debug messages for a specific kernel module"
21,"To enable debug messages for a specific kernel module, use the"
21,following example:
21,"# echo ""module MODULE_NAME +p"" > /sys/kernel/debug/dynamic_debug/controlDisabling debug messages"
21,To disable previously enabled debugging messages for a specific
21,"kernel source code file or a kernel module, run the"
21,echo command with the -p
21,option. For example:
21,"# echo ""file FILE_NAME.c -p"" > /sys/kernel/debug/dynamic_debug/control# echo ""module MODULE_NAME -p"" > /sys/kernel/debug/dynamic_debug/control"
21,"For detailed information about dynamic debug and its use cases, refer to"
21,its
21,official
21,documentation.
21,8.4 Viewing the dynamic debug messages #
21,You can view the dynamic debug messages that were generated based on the
21,"configurations you enabled, by running dmesg and"
21,filtering the output with grep. For example:
21,"# dmesg | grep -i ""FILE_NAME.c"""
21,"Optionally, to continuously monitor the system messages as they are"
21,"generated, you can use the tail command with the"
21,-f option:
21,# tail -f /var/log/messagesPart IV Resource management #  9 General system resource management
21,Tuning the system is not only about optimizing the kernel or getting the
21,"most out of your application, it begins with setting up a lean and fast"
21,system. The way you set up your partitions and file systems can
21,influence the server's speed. The number of active services and the way
21,routine tasks are scheduled also affects performance.
21,10 Kernel control groups
21,Kernel Control Groups (“cgroups”) are a kernel feature for
21,assigning and limiting hardware and system resources for processes.
21,Processes can also be organized in a hierarchical tree structure.
21,11 Automatic Non-Uniform Memory Access (NUMA) balancing
21,There are physical limitations to hardware that are encountered when
21,many CPUs and lots of memory are required. In this
21,"chapter, the important limitation is that there is limited communication"
21,bandwidth between the CPUs and the memory. One architecture modification
21,that was introduced to address this is Non-Uniform Memory Access (NUMA).
21,"In this configuration, there are multiple nodes. Each of the nodes"
21,contains a subset of all CPUs and memory. The access speed to main
21,memory is determined by the location of the memory relative to the CPU.
21,The performance of a workload depends on the application threads
21,accessing data that is local to the CPU the thread is executing on.
21,Automatic NUMA Balancing migrates data on demand to memory nodes that are
21,local to the CPU accessing that data.
21,"Depending on the workload, this can dramatically boost performance when"
21,using NUMA hardware.
21,12 Power management
21,Power management aims at reducing operating costs for energy and cooling
21,systems while at the same time keeping the performance of a system at a
21,"level that matches the current requirements. Thus, power management is"
21,always a matter of balancing the actual performance needs and power
21,saving options for a system. Power management can be implemented and
21,used at different levels of the system. A set of specifications for
21,power management functions of devices and the operating system interface
21,to them has been defined in the Advanced Configuration and Power
21,Interface (ACPI). As power savings in server environments can primarily
21,"be achieved at the processor level, this chapter introduces some"
21,main concepts and highlights some tools for analyzing and influencing
21,relevant parameters.
21,9 General system resource management #
21,Tuning the system is not only about optimizing the kernel or getting the
21,"most out of your application, it begins with setting up a lean and fast"
21,system. The way you set up your partitions and file systems can
21,influence the server's speed. The number of active services and the way
21,routine tasks are scheduled also affects performance.
21,9.1 Planning the installation #
21,A carefully planned installation ensures that the system is set up
21,exactly as you need it for the given purpose. It also saves considerable
21,time when fine tuning the system. All changes suggested in this section
21,can be made in the Installation Settings step during
21,"the installation. See Book “Deployment Guide”, Chapter 8 “Installation steps”, Section 8.15 “Installation settings” for details."
21,9.1.1 Partitioning #
21,"Depending on the server's range of applications and the hardware layout,"
21,the partitioning scheme can influence the machine's performance
21,(although to a lesser extent only). It is beyond the scope of this
21,manual to suggest different partitioning schemes for particular
21,"workloads. However, the following rules will positively affect"
21,performance. They do not apply when using an external storage system.
21,"Make sure there always is some free space available on the disk, since"
21,a full disk delivers inferior performance
21,"Disperse simultaneous read and write access onto different disks by,"
21,for example:
21,"using separate disks for the operating system, data, and log files"
21,placing a mail server's spool directory on a separate disk
21,distributing the user directories of a home server between different
21,disks
21,9.1.2 Installation scope #
21,The installation scope has no direct influence on the machine's
21,"performance, but a carefully chosen scope of packages has advantages. It"
21,is recommended to install the minimum of packages needed to run the
21,server. A system with a minimum set of packages is easier to maintain
21,"and has fewer potential security issues. Furthermore, a tailor made"
21,installation scope also ensures that no unnecessary services are started
21,by default.
21,SUSE Linux Enterprise Server lets you customize the installation scope on the
21,"Installation Summary screen. By default, you can select or remove"
21,"preconfigured patterns for specific tasks, but it is also possible to"
21,start the YaST Software Manager for a fine-grained package-based
21,selection.
21,One or more of the following default patterns may not be needed in all
21,cases:
21,GNOME desktop environment
21,Servers rarely need a full desktop environment. In case a graphical
21,"environment is needed, a more economical solution such as IceWM can"
21,be sufficient.
21,X Window System
21,When solely administrating the server and its applications via
21,"command line, consider not installing this pattern. However, keep in"
21,mind that it is needed to run GUI applications from a remote machine.
21,If your application is managed by a GUI or if you prefer the GUI
21,"version of YaST, keep this pattern."
21,Print server
21,This pattern is only needed if you want to print from the machine.
21,9.1.3 Default target #
21,A running X Window System consumes many resources and is rarely needed on
21,a server. It is strongly recommended to start the system in target
21,multi-user.target. You will still be able to
21,remotely start graphical applications.
21,9.2 Disabling unnecessary services #
21,The default installation starts several services (the number varies
21,"with the installation scope). Since each service consumes resources, it"
21,is recommended to disable the ones not needed. Run YaST › System › Services Manager to start the services
21,management module.
21,"If you are using the graphical version of YaST, you can click the"
21,column headlines to sort the list of services. Use this to get an
21,overview of which services are currently running.
21,Use the Start/Stop button to disable the service for
21,"the running session. To permanently disable it, use the"
21,Enable/Disable button.
21,The following list shows services that are started by default after the
21,"installation of SUSE Linux Enterprise Server. Check which of the components you need,"
21,and disable the others:
21,alsasound
21,Loads the Advanced Linux Sound System.
21,auditd
21,A daemon for the Audit system (see Book “Security and Hardening Guide” for
21,details). Disable this if you do not use Audit.
21,bluez-coldplug
21,Handles cold plugging of Bluetooth dongles.
21,cups
21,A printer daemon.
21,java.binfmt_misc
21,Enables the execution of *.class or
21,*.jar Java programs.
21,nfs
21,Services needed to mount NFS.
21,smbfs
21,Services needed to mount SMB/CIFS file systems from a Windows* server.
21,splash / splash_early
21,Shows the splash screen on start-up.
21,9.3 File systems and disk access #
21,Hard disks are the slowest components in a computer system and therefore
21,often the cause for a bottleneck. Using the file system that best suits
21,your workload helps to improve performance. Using special mount options
21,or prioritizing a process's I/O priority are further means to speed up
21,the system.
21,9.3.1 File systems #
21,"SUSE Linux Enterprise Server ships with several file systems,"
21,"including Btrfs, Ext4, Ext3, Ext2, and XFS. Each file system has"
21,its own advantages and disadvantages. Refer to
21,"Book “Storage Administration Guide”, Chapter 1 “Overview of file systems in Linux” for detailed information."
21,9.3.1.1 NFS #
21,NFS (Version 3) tuning is covered in detail in the NFS Howto at
21,http://nfs.sourceforge.net/nfs-howto/. The first
21,thing to experiment with when mounting NFS shares is increasing the
21,read write blocksize to 32768 by using the mount
21,options wsize and rsize.
21,9.3.2 Time stamp update policy #
21,Each file and directory in a file system has three time stamps associated
21,with it: a time when the file was last read called access
21,"time, a time when the file data was last modified called"
21,"modification time, and a time when the file metadata"
21,was last modified called change time. Keeping access
21,time always up to date has significant performance overhead since every
21,read-only access will incur a write operation. Thus by default every file
21,system updates access time only if current file access time is older than a
21,day or if it is older than file modification or change time.
21,This feature
21,is called relative access time and the corresponding
21,mount option is relatime. Updates of access time can be
21,"completely disabled using the noatime mount option,"
21,however you need to verify your applications do not use it. This can be
21,true for file and Web servers or for network storage. If the default
21,"relative access time update policy is not suitable for your applications,"
21,use the strictatime mount option.
21,Some file systems (for example Ext4) also support lazy time stamp updates.
21,When this feature is enabled using the lazytime mount
21,"option, updates of all time stamps happen in memory but they are not"
21,written to disk. That happens only in response to
21,fsync or sync system
21,"calls, when the file information is written due to another reason such as"
21,"file size update, when time stamps are older than 24 hours, or when cached"
21,file information needs to be evicted from memory.
21,"To update mount options used for a file system, either edit"
21,"/etc/fstab directly, or use the Fstab"
21,Options dialog when editing or adding a partition with the
21,YaST Partitioner.
21,9.3.3 Prioritizing disk access with ionice #
21,The ionice command lets you prioritize disk access
21,for single processes. This enables you to give less I/O priority to
21,"background processes with heavy disk access that are not time-critical,"
21,such as backup jobs. ionice also lets you raise the
21,I/O priority for a specific process to make sure this process always has
21,immediate access to the disk. The caveat of this feature is that standard
21,writes are cached in the page cache and are written back to persistent
21,storage only later by an independent kernel process. Thus the I/O priority
21,setting generally does not apply for these writes. Also be aware that
21,I/O class and priority setting are obeyed only by
21,BFQ I/O scheduler for blk-mq I/O path (refer
21,"to Section 13.2, “Available I/O elevators with blk-mq I/O path”)."
21,You can set
21,the following three scheduling classes:
21,Idle
21,A process from the idle scheduling class is only granted disk access
21,when no other process has asked for disk I/O.
21,Best effort
21,The default scheduling class used for any process that has not asked
21,for a specific I/O priority. Priority within this class can be
21,adjusted to a level from 0 to 7
21,(with 0 being the highest priority). Programs
21,running at the same best-effort priority are served in a round-robin
21,fashion. Some kernel versions treat priority within the best-effort
21,"class differently—for details, refer to the"
21,ionice(1) man page.
21,Real-time
21,Processes in this class are always granted disk access first.
21,Fine-tune the priority level from 0 to
21,7 (with 0 being the highest
21,"priority). Use with care, since it can starve other processes."
21,For more details and the exact command syntax refer to the
21,ionice(1) man page. If you need more reliable
21,"control over bandwidth available to each application, use"
21,Kernel Control Groups as described in
21,"Chapter 10, Kernel control groups."
21,10 Kernel control groups #
21,Kernel Control Groups (“cgroups”) are a kernel feature for
21,assigning and limiting hardware and system resources for processes.
21,Processes can also be organized in a hierarchical tree structure.
21,10.1 Overview #
21,Every process is assigned exactly one administrative cgroup. cgroups are
21,"ordered in a hierarchical tree structure. You can set resource limitations,"
21,"such as CPU, memory, disk I/O, or network bandwidth usage, for single"
21,processes or for whole branches of the hierarchy tree.
21,"On SUSE Linux Enterprise Server, systemd uses cgroups to organize all processes in"
21,"groups, which systemd calls slices. systemd also provides an interface"
21,for setting cgroup properties.
21,The command systemd-cgls displays the hierarchy tree.
21,There are two versions of cgroup APIs provided by the kernel. These differ
21,"in the cgroup attributes they provide, and in the organization of controller"
21,hierarchies. systemd attempts to abstract the differences away. By default
21,"systemd runs in the hybrid mode, which means"
21,controllers are used through the v1 API. cgroup v2 is only used for
21,systemd's own tracking. There is also the unified mode
21,when the controllers are used though v2 API. You may set only one mode.
21,"To enable the unified control group hierarchy, append"
21,systemd.unified_cgroup_hierarchy=1 as a kernel command line
21,"parameter to the GRUB 2 boot loader. (Refer to Book “Administration Guide”, Chapter 14 “The boot loader GRUB 2”"
21,for more details about configuring GRUB 2.)
21,10.2 Resource accounting #
21,Organizing processes into different cgroups can be used to obtain per-cgroup
21,resource consumption data.
21,"The accounting has relatively small but non-zero overhead, whose impact"
21,depends on the workload. Activating accounting for one unit will also
21,"implicitly activate it for all units in the same slice, and for all its"
21,"parent slices, and the units contained in them."
21,The accounting can be set on a per-unit basis with directives such as
21,MemoryAccounting= or globally for all units in
21,/etc/systemd/system.conf with the directive
21,DefaultMemoryAccounting=. Refer to man
21,systemd.resource-control for the exhaustive list of possible
21,directives.
21,10.3 Setting resource limits #  Note: Implicit resource consumption
21,Be aware that resource consumption implicitly depends on the environment
21,"where your workload executes (for example, size of data structures in"
21,"libraries/kernel, forking behavior of utilities, computational efficiency)."
21,Hence it is recommended to (re)calibrate your limits should the environment
21,change.
21,Limitations to cgroups can be set with the systemctl
21,set-property command. The syntax is:
21,# systemctl set-property [--runtime] NAME PROPERTY1=VALUE [PROPERTY2=VALUE]
21,"The configured value is applied immediately. Optionally, use the"
21,"--runtime option, so that the new values do not persist"
21,after reboot.
21,"Replace NAME with a systemd service, scope, or"
21,slice name.
21,"For a complete list of properties and more details, see man"
21,systemd.resource-control.
21,10.4 Preventing fork bombs with TasksMax #
21,systemd supports configuring task count limits both for each individual
21,"leaf unit, or aggregated on slices. Upstream systemd ships with defaults"
21,"that limit the number of tasks in each unit (15% of the kernel global limit,"
21,run /usr/sbin/sysctl kernel.pid_max to see the total
21,"limit). Each user's slice is limited to 33% of the kernel limit. However,"
21,this is different for SLE.
21,10.4.1 Finding the current default TasksMax values #
21,"It became apparent, in practice, that there is not a single default that"
21,applies to all use cases. SUSE Linux Enterprise Server ships with two custom
21,configurations that override the upstream defaults for system units and for
21,"user slices, and sets them both to infinity."
21,/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf
21,contains these lines:
21,[Manager]
21,DefaultTasksMax=infinity
21,/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf
21,contains these lines:
21,[Slice]
21,TasksMax=infinity
21,Use systemctl to verify the DefaultTasksMax value:
21,> systemctl show --property DefaultTasksMax
21,DefaultTasksMax=infinity
21,infinity means having no limit. It is not a requirement
21,"to change the default, but setting some limits may help to prevent system"
21,crashes from runaway processes.
21,10.4.2 Overriding the DefaultTasksMax value #
21,Change the global DefaultTasksMax value by creating a
21,"new override file,"
21,"/etc/systemd/system.conf.d/90-system-tasksmax.conf,"
21,and write the following lines to set a new default limit of 256 tasks per
21,system unit:
21,[Manager]
21,DefaultTasksMax=256
21,"Load the new setting, then verify that it changed:"
21,> sudo systemctl daemon-reload
21,> systemctl show --property DefaultTasksMax
21,DefaultTasksMax=256
21,Adjust this default value to suit your needs. You can set different limits
21,on individual services as needed. This example is for MariaDB. First check
21,the current active value:
21,> systemctl status mariadb.service
21,● mariadb.service - MariaDB database server
21,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
21,Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
21,Docs: man:mysqld(8)
21,https://mariadb.com/kb/en/library/systemd/
21,Main PID: 11845 (mysqld)
21,"Status: ""Taking your SQL requests now..."""
21,Tasks: 30 (limit: 256)
21,CGroup: /system.slice/mariadb.service
21,└─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
21,"The Tasks line shows that MariaDB currently has 30 tasks running, and has"
21,"an upper limit of the default 256, which is inadequate for a database. The"
21,following example demonstrates how to raise MariaDB's limit to 8192.
21,> sudo systemctl set-property mariadb.service TasksMax=8192
21,> systemctl status mariadb.service
21,● mariadb.service - MariaDB database server
21,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
21,Drop-In: /etc/systemd/system/mariadb.service.d
21,└─50-TasksMax.conf
21,Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
21,Docs: man:mysqld(8)
21,https://mariadb.com/kb/en/library/systemd/
21,"Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>"
21,"Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>"
21,Main PID: 3452 (mysqld)
21,"Status: ""Taking your SQL requests now..."""
21,Tasks: 30 (limit: 8192)
21,CGroup: /system.slice/mariadb.service
21,└─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
21,systemctl set-property applies the new limit and creates
21,"a drop-in file for persistence,"
21,"/etc/systemd/system/mariadb.service.d/50-TasksMax.conf,"
21,that contains only the changes you want to apply to the existing unit file.
21,"The value does not have to be 8192, but should be whatever limit is"
21,appropriate for your workloads.
21,10.4.3 Default TasksMax limit on users #
21,"The default limit on users should be fairly high, because user sessions"
21,need more resources. Set your own default for any user by creating a new
21,"file, for example"
21,/etc/systemd/system/user-.slice.d/40-user-taskmask.conf.
21,The following example sets a default of 16284:
21,[Slice]
21,TasksMax=16284Note: Numeric prefixes reference
21,See
21,https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in
21,to learn what numeric prefixes are expected for drop-in files.
21,"Then reload systemd to load the new value, and verify the change:"
21,> sudo systemctl daemon-reload
21,> systemctl show --property TasksMax user-1000.slice
21,TasksMax=16284
21,How do you know what values to use? This varies according to your
21,"workloads, system resources, and other resource configurations. When your"
21,"TasksMax value is too low, you will see error messages"
21,such as Failed to fork (Resources temporarily
21,"unavailable), Can't create thread to handle new"
21,"connection, and Error: Function call 'fork' failed"
21,"with error code 11, 'Resource temporarily unavailable'."
21,"For more information on configuring system resources in systemd, see"
21,systemd.resource-control (5).
21,10.5 Controlling I/O with proportional weight policy #
21,This section introduces using the Linux kernel's block I/O controller to
21,prioritize I/O operations. The cgroup blkio subsystem controls and monitors
21,access to I/O on block devices. State objects that contain the subsystem
21,parameters for a cgroup are represented as pseudo-files within the cgroup
21,"virtual file system, also called a pseudo-file system."
21,The examples in this section show how writing values to some of these
21,"pseudo-files limits access or bandwidth, and reading values from some of"
21,these pseudo-files provides information on I/O operations. Examples are
21,provided for both cgroup-v1 and cgroup-v2.
21,You need a test directory containing two files for testing performance and
21,changed settings. A quick way to create test files fully populated with text
21,is using the yes command. The following example commands
21,"create a test directory, and then populate it with two 537 MB text files:"
21,host1:~ # mkdir /io-cgroup
21,host1:~ # cd /io-cgroup
21,host1:~ # yes this is a test file | head -c 537MB > file1.txt
21,host1:~ # yes this is a test file | head -c 537MB > file2.txt
21,To run the examples open three command shells. Two shells are for reader
21,"processes, and one shell is for running the steps that control I/O. In the"
21,"examples, each command prompt is labeled to indicate if it represents one of"
21,"the reader processes, or I/O."
21,10.5.1 Using cgroup-v1 #
21,The following proportional weight policy files can be used to grant a
21,reader process a higher priority for I/O operations than other reader
21,processes accessing the same disk.
21,blkio.bfq.weight (available in kernels starting with
21,version 5.0 with blk-mq and when using the BFQ I/O scheduler)
21,"To test this, run a single reader process (in the examples, reading from an"
21,"SSD) without controlling its I/O, using file2.txt:"
21,[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5251
21,131072+0 records in
21,131072+0 records out
21,"536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s"
21,Now run a background process reading from the same disk:
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
21,5220
21,...
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5251
21,131072+0 records in
21,131072+0 records out
21,"536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s"
21,"Each process gets half of the throughput for I/O operations. Next, set up"
21,two control groups—one for each process—verify that BFQ is
21,"used, and set a different weight for reader2:"
21,[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/blkio/
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader1
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader2
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5220 > reader1/cgroup.procs
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5251 > reader2/cgroup.procs
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat /sys/block/sda/queue/scheduler
21,mq-deadline kyber [bfq] none
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
21,100
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 200 > reader2/blkio.bfq.weight
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
21,200
21,"With these settings and reader1 in the background, reader2 should have"
21,higher throughput than previously:
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
21,5220
21,...
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5251
21,131072+0 records in
21,131072+0 records out
21,"536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s"
21,The higher proportional weight resulted in higher throughput for reader2.
21,Now double its weight again:
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
21,100
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 400 > reader2/blkio.bfq.weight
21,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
21,400
21,This results in another increase in throughput for reader2:
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
21,5220
21,...
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5251
21,131072+0 records in
21,131072+0 records out
21,"536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s10.5.2 Using cgroup-v2 #"
21,First set up your test environment as shown at the beginning of this
21,chapter.
21,"Then make sure that the Block IO controller is not active, as that is for"
21,"cgroup-v1. To do this, boot with kernel parameter"
21,"cgroup_no_v1=blkio. Verify that this parameter was used,"
21,and that the IO controller (cgroup-v2) is available:
21,[io-controller] host1:/io-cgroup # cat /proc/cmdline
21,BOOT_IMAGE=... cgroup_no_v1=blkio ...
21,[io-controller] host1:/io-cgroup # cat /sys/fs/cgroup/unified/cgroup.controllers
21,"Next, enable the IO controller:"
21,[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/unified/
21,[io-controller] host1:/sys/fs/cgroup/unified # echo '+io' > cgroup.subtree_control
21,[io-controller] host1:/sys/fs/cgroup/unified # cat cgroup.subtree_control
21,"Now run all the test steps, similarly to the steps for cgroup-v1. Note that"
21,some of the directories are different. Run a single reader process (in the
21,"examples, reading from an SSD) without controlling its I/O, using"
21,file2.txt:
21,[io-controller] host1:/sys/fs/cgroup/unified # cd -
21,[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5633
21,[...]
21,Run a background process reading from the same disk and note your
21,throughput values:
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
21,5633
21,[...]
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,5703
21,[...]
21,Each process gets half of the throughput for I/O operations. Set up two
21,control groups—one for each process—verify that BFQ is the
21,"active scheduler, and set a different weight for reader2:"
21,[io-controller] host1:/io-cgroup # cd -
21,[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader1
21,[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader2
21,[io-controller] host1:/sys/fs/cgroup/unified # echo 5633 > reader1/cgroup.procs
21,[io-controller] host1:/sys/fs/cgroup/unified # echo 5703 > reader2/cgroup.procs
21,[io-controller] host1:/sys/fs/cgroup/unified # cat /sys/block/sda/queue/scheduler
21,mq-deadline kyber [bfq] none
21,[io-controller] host1:/sys/fs/cgroup/unified # cat reader1/io.bfq.weight
21,default 100
21,[io-controller] host1:/sys/fs/cgroup/unified # echo 200 > reader2/io.bfq.weight
21,[io-controller] host1:/sys/fs/cgroup/unified # cat reader2/io.bfq.weight
21,default 200
21,Test your throughput with the new settings. reader2 should show an increase
21,in throughput.
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1 of=/dev/null bs=4k
21,5633
21,[...]
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2 of=/dev/null bs=4k count=131072
21,5703
21,[...]
21,"Try doubling the weight again for reader2, and testing the new setting:"
21,[reader2] host1:/io-cgroup # echo 400 > reader1/blkio.bfq.weight
21,[reader2] host1:/io-cgroup # cat reader2/blkio.bfq.weight
21,400
21,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
21,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
21,[...]
21,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
21,[...]10.6 More information #
21,Kernel documentation (package kernel-source):
21,files in
21,/usr/src/linux/Documentation/admin-guide/cgroup-v1
21,and file
21,/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst.
21,"https://lwn.net/Articles/604609/—Brown, Neil:"
21,"Control Groups Series (2014, 7 parts)."
21,"https://lwn.net/Articles/243795/—Corbet,"
21,Jonathan: Controlling memory use in containers (2007).
21,"https://lwn.net/Articles/236038/—Corbet,"
21,Jonathan: Process containers (2007).
21,11 Automatic Non-Uniform Memory Access (NUMA) balancing #
21,There are physical limitations to hardware that are encountered when
21,many CPUs and lots of memory are required. In this
21,"chapter, the important limitation is that there is limited communication"
21,bandwidth between the CPUs and the memory. One architecture modification
21,that was introduced to address this is Non-Uniform Memory Access (NUMA).
21,"In this configuration, there are multiple nodes. Each of the nodes"
21,contains a subset of all CPUs and memory. The access speed to main
21,memory is determined by the location of the memory relative to the CPU.
21,The performance of a workload depends on the application threads
21,accessing data that is local to the CPU the thread is executing on.
21,Automatic NUMA Balancing migrates data on demand to memory nodes that are
21,local to the CPU accessing that data.
21,"Depending on the workload, this can dramatically boost performance when"
21,using NUMA hardware.
21,11.1 Implementation #
21,Automatic NUMA balancing happens in three basic steps:
21,A task scanner periodically scans a portion of a task's address space
21,and marks the memory to force a page fault when the data is next
21,accessed.
21,The next access to the data will result in a NUMA Hinting Fault. Based
21,"on this fault, the data can be migrated to a memory node associated"
21,with the task accessing the memory.
21,"To keep a task, the CPU it is using and the memory it is accessing"
21,"together, the scheduler groups tasks that share data."
21,"The unmapping of data and page fault handling incurs overhead. However,"
21,commonly the overhead will be offset by threads accessing data associated
21,with the CPU.
21,11.2 Configuration #
21,Static configuration has been the recommended way of tuning workloads on
21,"NUMA hardware for some time. To do this, memory policies can be set with"
21,"numactl, taskset or"
21,cpusets. NUMA-aware applications can use special APIs.
21,"In cases where the static policies have already been created, automatic"
21,NUMA balancing should be disabled as the data access should already be
21,local.
21,numactl --hardware will show the
21,memory configuration of the machine and whether it supports NUMA or not.
21,This is example output from a 4-node machine.
21,> numactl --hardware
21,available: 4 nodes (0-3)
21,node 0 cpus: 0 4 8 12 16 20 24 28 32 36 40 44
21,node 0 size: 16068 MB
21,node 0 free: 15909 MB
21,node 1 cpus: 1 5 9 13 17 21 25 29 33 37 41 45
21,node 1 size: 16157 MB
21,node 1 free: 15948 MB
21,node 2 cpus: 2 6 10 14 18 22 26 30 34 38 42 46
21,node 2 size: 16157 MB
21,node 2 free: 15981 MB
21,node 3 cpus: 3 7 11 15 19 23 27 31 35 39 43 47
21,node 3 size: 16157 MB
21,node 3 free: 16028 MB
21,node distances:
21,node
21,Automatic NUMA balancing can be enabled or disabled for the current
21,session by writing 1 or 0
21,to /proc/sys/kernel/numa_balancing which will
21,enable or disable the feature respectively. To permanently enable or
21,"disable it, use the kernel command line option"
21,numa_balancing=[enable|disable].
21,"If Automatic NUMA Balancing is enabled, the task scanner behavior can be"
21,configured.
21,The task scanner balances the overhead of Automatic NUMA Balancing with
21,the amount of time it takes to identify the best placement of data.
21,numa_balancing_scan_delay_ms
21,The amount of CPU time a thread must consume before its data is
21,scanned. This prevents creating overhead because of short-lived
21,processes.
21,numa_balancing_scan_period_min_ms and
21,numa_balancing_scan_period_max_ms
21,Controls how frequently a task's data is scanned. Depending on the
21,locality of the faults the scan rate will increase or decrease. These
21,settings control the min and max scan rates.
21,numa_balancing_scan_size_mb
21,Controls how much address space is scanned when the task scanner is
21,active.
21,11.3 Monitoring #
21,The most important task is to assign metrics to your workload and measure
21,the performance with Automatic NUMA Balancing enabled and disabled to
21,measure the impact. Profiling tools can be used to monitor local and
21,remote memory accesses if the CPU supports such monitoring. Automatic
21,NUMA Balancing activity can be monitored via the following parameters in
21,/proc/vmstat:
21,numa_pte_updates
21,The amount of base pages that were marked for NUMA hinting faults.
21,numa_huge_pte_updates
21,The amount of transparent huge pages that were marked for NUMA hinting
21,faults. In combination with numa_pte_updates the
21,total address space that was marked can be calculated.
21,numa_hint_faults
21,Records how many NUMA hinting faults were trapped.
21,numa_hint_faults_local
21,Shows how many of the hinting faults were to local nodes. In
21,"combination with numa_hint_faults, the percentage"
21,of local versus remote faults can be calculated. A high percentage of
21,local hinting faults indicates that the workload is closer to being
21,converged.
21,numa_pages_migrated
21,Records how many pages were migrated because they were misplaced. As
21,"migration is a copying operation, it contributes the largest part of"
21,the overhead created by NUMA balancing.
21,11.4 Impact #
21,The following illustrates a simple test case of a 4-node NUMA machine
21,running the SpecJBB 2005
21,using a single instance of the JVM with no static tuning around memory
21,"policies. Note, however, that the impact for each workload will vary and"
21,that this example is based on a pre-release version of SUSE Linux Enterprise Server
21,12.
21,Balancing disabled
21,Balancing enabled
21,TPut 1
21,26629.00 (
21,0.00%)
21,26507.00 ( -0.46%)
21,TPut 2
21,55841.00 (
21,0.00%)
21,53592.00 ( -4.03%)
21,TPut 3
21,86078.00 (
21,0.00%)
21,86443.00 (
21,0.42%)
21,TPut 4
21,116764.00 (
21,0.00%)
21,113272.00 ( -2.99%)
21,TPut 5
21,143916.00 (
21,0.00%)
21,141581.00 ( -1.62%)
21,TPut 6
21,166854.00 (
21,0.00%)
21,166706.00 ( -0.09%)
21,TPut 7
21,195992.00 (
21,0.00%)
21,192481.00 ( -1.79%)
21,TPut 8
21,222045.00 (
21,0.00%)
21,227143.00 (
21,2.30%)
21,TPut 9
21,248872.00 (
21,0.00%)
21,250123.00 (
21,0.50%)
21,TPut 10
21,270934.00 (
21,0.00%)
21,279314.00 (
21,3.09%)
21,TPut 11
21,297217.00 (
21,0.00%)
21,301878.00 (
21,1.57%)
21,TPut 12
21,311021.00 (
21,0.00%)
21,326048.00 (
21,4.83%)
21,TPut 13
21,324145.00 (
21,0.00%)
21,346855.00 (
21,7.01%)
21,TPut 14
21,345973.00 (
21,0.00%)
21,378741.00 (
21,9.47%)
21,TPut 15
21,354199.00 (
21,0.00%)
21,394268.00 ( 11.31%)
21,TPut 16
21,378016.00 (
21,0.00%)
21,426782.00 ( 12.90%)
21,TPut 17
21,392553.00 (
21,0.00%)
21,437772.00 ( 11.52%)
21,TPut 18
21,396630.00 (
21,0.00%)
21,456715.00 ( 15.15%)
21,TPut 19
21,399114.00 (
21,0.00%)
21,484020.00 ( 21.27%)
21,TPut 20
21,413907.00 (
21,0.00%)
21,493618.00 ( 19.26%)
21,TPut 21
21,413173.00 (
21,0.00%)
21,510386.00 ( 23.53%)
21,TPut 22
21,420256.00 (
21,0.00%)
21,521016.00 ( 23.98%)
21,TPut 23
21,425581.00 (
21,0.00%)
21,536214.00 ( 26.00%)
21,TPut 24
21,429052.00 (
21,0.00%)
21,532469.00 ( 24.10%)
21,TPut 25
21,426127.00 (
21,0.00%)
21,526548.00 ( 23.57%)
21,TPut 26
21,422428.00 (
21,0.00%)
21,531994.00 ( 25.94%)
21,TPut 27
21,424378.00 (
21,0.00%)
21,488340.00 ( 15.07%)
21,TPut 28
21,419338.00 (
21,0.00%)
21,543016.00 ( 29.49%)
21,TPut 29
21,403347.00 (
21,0.00%)
21,529178.00 ( 31.20%)
21,TPut 30
21,408681.00 (
21,0.00%)
21,510621.00 ( 24.94%)
21,TPut 31
21,406496.00 (
21,0.00%)
21,499781.00 ( 22.95%)
21,TPut 32
21,404931.00 (
21,0.00%)
21,502313.00 ( 24.05%)
21,TPut 33
21,397353.00 (
21,0.00%)
21,522418.00 ( 31.47%)
21,TPut 34
21,382271.00 (
21,0.00%)
21,491989.00 ( 28.70%)
21,TPut 35
21,388965.00 (
21,0.00%)
21,493012.00 ( 26.75%)
21,TPut 36
21,374702.00 (
21,0.00%)
21,502677.00 ( 34.15%)
21,TPut 37
21,367578.00 (
21,0.00%)
21,500588.00 ( 36.19%)
21,TPut 38
21,367121.00 (
21,0.00%)
21,496977.00 ( 35.37%)
21,TPut 39
21,355956.00 (
21,0.00%)
21,489430.00 ( 37.50%)
21,TPut 40
21,350855.00 (
21,0.00%)
21,487802.00 ( 39.03%)
21,TPut 41
21,345001.00 (
21,0.00%)
21,468021.00 ( 35.66%)
21,TPut 42
21,336177.00 (
21,0.00%)
21,462260.00 ( 37.50%)
21,TPut 43
21,329169.00 (
21,0.00%)
21,467906.00 ( 42.15%)
21,TPut 44
21,329475.00 (
21,0.00%)
21,470784.00 ( 42.89%)
21,TPut 45
21,323845.00 (
21,0.00%)
21,450739.00 ( 39.18%)
21,TPut 46
21,323878.00 (
21,0.00%)
21,435457.00 ( 34.45%)
21,TPut 47
21,310524.00 (
21,0.00%)
21,403914.00 ( 30.07%)
21,TPut 48
21,311843.00 (
21,0.00%)
21,459017.00 ( 47.19%)
21,Balancing Disabled
21,Balancing Enabled
21,Expctd Warehouse
21,48.00 (
21,0.00%)
21,48.00 (
21,0.00%)
21,Expctd Peak Bops
21,310524.00 (
21,0.00%)
21,403914.00 ( 30.07%)
21,Actual Warehouse
21,25.00 (
21,0.00%)
21,29.00 ( 16.00%)
21,Actual Peak Bops
21,429052.00 (
21,0.00%)
21,543016.00 ( 26.56%)
21,SpecJBB Bops
21,6364.00 (
21,0.00%)
21,9368.00 ( 47.20%)
21,SpecJBB Bops/JVM
21,6364.00 (
21,0.00%)
21,9368.00 ( 47.20%)
21,Automatic NUMA Balancing simplifies
21,tuning workloads for high performance on NUMA machines. Where
21,"possible, it is still recommended to statically tune the workload to"
21,"partition it within each node. However, in all other cases, automatic"
21,NUMA balancing should boost performance.
21,12 Power management #
21,Power management aims at reducing operating costs for energy and cooling
21,systems while at the same time keeping the performance of a system at a
21,"level that matches the current requirements. Thus, power management is"
21,always a matter of balancing the actual performance needs and power
21,saving options for a system. Power management can be implemented and
21,used at different levels of the system. A set of specifications for
21,power management functions of devices and the operating system interface
21,to them has been defined in the Advanced Configuration and Power
21,Interface (ACPI). As power savings in server environments can primarily
21,"be achieved at the processor level, this chapter introduces some"
21,main concepts and highlights some tools for analyzing and influencing
21,relevant parameters.
21,12.1 Power management at CPU Level #
21,"At the CPU level, you can control power usage in various ways. For"
21,"example by using idling power states (C-states), changing CPU frequency"
21,"(P-states), and throttling the CPU (T-states). The following sections"
21,give a short introduction to each approach and its significance for power
21,savings. Detailed specifications can be found at
21,http://www.acpi.info/spec.htm.
21,12.1.1 C-states (processor operating states) #
21,Modern processors have several power saving modes called
21,C-states. They reflect the capability of an idle
21,processor to turn off unused components to save power.
21,"When a processor is in the C0 state, it is executing"
21,instructions. A processor running in any other C-state is idle. The
21,"higher the C number, the deeper the CPU sleep mode: more components are"
21,shut down to save power. Deeper sleep states can save large amounts of
21,"energy. Their downside is that they introduce latency. This means, it"
21,takes more time for the CPU to go back to C0.
21,"Depending on workload (threads waking up, triggering CPU usage and then"
21,going back to sleep again for a short period of time) and hardware (for
21,"example, interrupt activity of a network device), disabling the deepest"
21,sleep states can significantly increase overall performance. For details
21,"on how to do so, refer to"
21,"Section 12.3.2, “Viewing kernel idle statistics with cpupower”."
21,Some states also have submodes with different power saving latency
21,levels. Which C-states and submodes are supported depends on the
21,"respective processor. However, C1 is always"
21,available.
21,"Table 12.1, “C-states” gives an overview of the most"
21,common C-states.
21,Table 12.1: C-states #
21,Mode
21,Definition
21,Operational state. CPU fully turned on.
21,First idle state. Stops CPU main internal clocks via software. Bus
21,interface unit and APIC are kept running at full speed.
21,Stops CPU main internal clocks via hardware. State in which the
21,"processor maintains all software-visible states, but may take"
21,longer to wake up through interrupts.
21,Stops all CPU internal clocks. The processor does not need to keep
21,"its cache coherent, but maintains other states. Some processors"
21,have variations of the C3 state that differ in how long it takes to
21,wake the processor through interrupts.
21,"To avoid needless power consumption, it is recommended to test your"
21,workloads with deep sleep states enabled versus deep sleep states
21,"disabled. For more information, refer to"
21,"Section 12.3.2, “Viewing kernel idle statistics with cpupower” or the"
21,cpupower-idle-set(1) man page.
21,12.1.2 P-states (processor performance states) #
21,"While a processor operates (in C0 state), it can be in one of several"
21,CPU performance states (P-states). Whereas C-states
21,"are idle states (all but C0), P-states are"
21,operational states that relate to CPU frequency and voltage.
21,"The higher the P-state, the lower the frequency and voltage at which the"
21,processor runs. The number of P-states is processor-specific and the
21,"implementation differs across the various types. However,"
21,"P0 is always the highest-performance state (except for Section 12.1.3, “Turbo features”). Higher"
21,P-state numbers represent slower processor speeds and lower power
21,"consumption. For example, a processor in P3 state runs"
21,more slowly and uses less power than a processor running in the
21,"P1 state. To operate at any P-state, the processor"
21,"must be in the C0 state, which means that it is"
21,working and not idling. The CPU P-states are also defined in the ACPI
21,"specification, see http://www.acpi.info/spec.htm."
21,C-states and P-states can vary independently of one another.
21,12.1.3 Turbo features #
21,Turbo features allow to dynamically overtick active CPU
21,cores while other cores are in deep sleep states. This increases the performance
21,of active threads while still
21,complying with Thermal Design Power (TDP) limits.
21,"However, the conditions under which a CPU core can use turbo frequencies"
21,are architecture-specific. Learn how to evaluate the efficiency of those
21,"new features in Section 12.3, “The cpupower tools”."
21,12.2 In-kernel governors #   The in-kernel governors belong to the Linux kernel CPUfreq infrastructure and can be
21,used to dynamically scale processor frequencies at runtime. You can think of the governors as a
21,sort of preconfigured power scheme for the CPU. The CPUfreq governors use P-states to
21,change frequencies and lower power consumption. The dynamic governors can switch between CPU
21,"frequencies, based on CPU usage, to allow for power savings while not sacrificing performance."
21,The following governors are available with the CPUfreq subsystem:
21,Performance governor
21,The CPU frequency is statically set to the highest possible for
21,"maximum performance. Consequently, saving power is not the focus of"
21,this governor.
21,"See also Section 12.4.1, “Tuning options for P-states”."
21,Powersave governor
21,The CPU frequency is statically set to the lowest possible. This can
21,"have severe impact on the performance, as the system will never rise"
21,above this frequency no matter how busy the processors are. An important
21,exception is the intel_pstate which defaults to the
21,powersave mode. This is due to a hardware-specific
21,decision but functionally it operates similarly to the
21,on-demand governor.
21,"However, using this governor often does not lead to the expected"
21,power savings as the highest savings can usually be achieved at idle
21,"through entering C-states. With the powersave governor, processes run"
21,at the lowest frequency and thus take longer to finish. This means it
21,takes longer until the system can go into an idle C-state.
21,Tuning options: The range of minimum frequencies available to the
21,"governor can be adjusted (for example, with the"
21,cpupower command line tool).
21,On-demand governor
21,The kernel implementation of a dynamic CPU frequency policy: The
21,governor monitors the processor usage. When it exceeds a
21,"certain threshold, the governor will set the frequency to the highest"
21,"available. If the usage is less than the threshold, the next lowest"
21,"frequency is used. If the system continues to be underemployed, the"
21,frequency is again reduced until the lowest available frequency is
21,set.
21,Important: Drivers and in-kernel governorsNot all drivers use the in-kernel governors to dynamically scale power frequency at
21,"runtime. For example, the intel_pstate driver adjusts power frequency itself. Use"
21,the cpupower frequency-info command to find out which driver your system
21,uses.12.3 The cpupower tools #  The cpupower tools are designed to give an overview
21,of all CPU power-related parameters that are supported
21,"on a given machine, including turbo (or boost) states."
21,Use the tool set to
21,view and modify settings of the kernel-related CPUfreq and cpuidle systems
21,and other settings not related to frequency scaling or idle states. The
21,integrated monitoring framework can access both kernel-related parameters
21,"and hardware statistics. Therefore, it is ideally suited for performance"
21,benchmarks. It also helps you to identify the dependencies between turbo and
21,idle states.
21,"After installing the cpupower package, view the"
21,available cpupower subcommands with
21,cpupower --help. Access the general man page with
21,"man cpupower, and the man pages of the subcommands with"
21,man cpupower-SUBCOMMAND.
21,12.3.1 Viewing current settings with cpupower #
21,The cpupower frequency-info command shows the
21,"statistics of the cpufreq driver used in the kernel. Additionally, it"
21,shows if turbo (boost) states are supported and enabled in the BIOS.
21,"Run without any options, it shows an output similar to the following:"
21,Example 12.1: Example output of cpupower frequency-info #  # cpupower frequency-info
21,analyzing CPU 0:
21,driver: intel_pstate
21,CPUs which run at the same hardware frequency: 0
21,CPUs which need to have their frequency coordinated by software: 0
21,maximum transition latency: 0.97 ms.
21,hardware limits: 1.20 GHz - 3.80 GHz
21,"available cpufreq governors: performance, powersave"
21,current policy: frequency should be within 1.20 GHz and 3.80 GHz.
21,"The governor ""powersave"" may decide which speed to use"
21,within this range.
21,current CPU frequency is 3.40 GHz (asserted by call to hardware).
21,boost state support:
21,Supported: yes
21,Active: yes
21,3500 MHz max turbo 4 active cores
21,3600 MHz max turbo 3 active cores
21,3600 MHz max turbo 2 active cores
21,3800 MHz max turbo 1 active cores
21,"To get the current values for all CPUs, use"
21,cpupower -c all frequency-info.
21,12.3.2 Viewing kernel idle statistics with cpupower #
21,The idle-info subcommand shows the statistics of the
21,cpuidle driver used in the kernel. It works on all architectures that
21,use the cpuidle kernel framework.
21,Example 12.2: Example output of cpupower idle-info #  # cpupower idle-info
21,CPUidle driver: intel_idle
21,CPUidle governor: menu
21,Analyzing CPU 0:
21,Number of idle states: 6
21,Available idle states: POLL C1-SNB C1E-SNB C3-SNB C6-SNB C7-SNB
21,POLL:
21,Flags/Description: CPUIDLE CORE POLL IDLE
21,Latency: 0
21,Usage: 163128
21,Duration: 17585669
21,C1-SNB:
21,Flags/Description: MWAIT 0x00
21,Latency: 2
21,Usage: 16170005
21,Duration: 697658910
21,C1E-SNB:
21,Flags/Description: MWAIT 0x01
21,Latency: 10
21,Usage: 4421617
21,Duration: 757797385
21,C3-SNB:
21,Flags/Description: MWAIT 0x10
21,Latency: 80
21,Usage: 2135929
21,Duration: 735042875
21,C6-SNB:
21,Flags/Description: MWAIT 0x20
21,Latency: 104
21,Usage: 53268
21,Duration: 229366052
21,C7-SNB:
21,Flags/Description: MWAIT 0x30
21,Latency: 109
21,Usage: 62593595
21,Duration: 324631233978
21,After finding out which processor idle states are supported with
21,"cpupower idle-info, individual states can be"
21,disabled using the cpupower idle-set command.
21,"Typically one wants to disable the deepest sleep state, for example:"
21,"# cpupower idle-set -d 5Or, for disabling all CPUs with latencies equal to or higher than 80:# cpupower idle-set -D 8012.3.3 Monitoring kernel and hardware statistics with cpupower #"
21,"Use the monitor subcommand to report processor topology, and monitor frequency"
21,and idle power state statistics over a certain period of time. The
21,"default interval is 1 second, but it can be changed"
21,with the -i. Independent processor sleep states and
21,frequency counters are implemented in the tool—some retrieved
21,"from kernel statistics, others reading out hardware registers. The"
21,available monitors depend on the underlying hardware and the system.
21,List them with cpupower monitor -l.
21,"For a description of the individual monitors, refer to the"
21,cpupower-monitor man page.
21,The monitor subcommand allows you to execute
21,performance benchmarks. To compare kernel statistics with hardware
21,"statistics for specific workloads, concatenate the respective command, for example:cpupower monitor db_test.shExample 12.3: Example cpupower monitor output #  # cpupower monitor"
21,|Mperf
21,|| Idle_Stats
21,CPU | C0
21,| Cx
21,| Freq || POLL | C1
21,| C2
21,| C3
21,3.71| 96.29|
21,2833||
21,0.00|
21,0.00|
21,0.02| 96.32
21,1| 100.0| -0.00|
21,2833||
21,0.00|
21,0.00|
21,0.00|
21,0.00
21,9.06| 90.94|
21,1983||
21,0.00|
21,7.69|
21,6.98| 76.45
21,7.43| 92.57|
21,2039||
21,0.00|
21,2.60| 12.62| 77.521
21,"Mperf shows the average frequency of a CPU, including boost"
21,"frequencies, over time. Additionally, it shows the"
21,percentage of time the CPU has been active (C0)
21,or in any sleep state (Cx). As the turbo states are managed by the
21,"BIOS, it is impossible to get the frequency values at a given"
21,instant. On modern processors with turbo features the Mperf monitor
21,is the only way to find out about the frequency a certain CPU has
21,been running in.
21,Idle_Stats shows the statistics of the cpuidle kernel subsystem. The
21,kernel updates these values every time an idle state is entered or
21,left. Therefore there can be some inaccuracy when cores are in an
21,idle state for some time when the measure starts or ends.
21,"Apart from the (general) monitors in the example above, other"
21,architecture-specific monitors are available. For detailed
21,"information, refer to the cpupower-monitor man"
21,page.
21,"By comparing the values of the individual monitors, you can find"
21,correlations and dependencies and evaluate how well the power saving
21,mechanism works for a certain workload. In
21,Example 12.3 you can
21,see that CPU 0 is idle (the value of
21,"Cx is near 100%), but runs at a very high frequency."
21,This is because the CPUs 0 and 1
21,have the same frequency values which means that there is a dependency
21,between them.
21,12.3.4 Modifying current settings with cpupower #
21,You can use
21,cpupower frequency-set command as root to
21,modify current settings. It allows you to set values for the minimum or
21,maximum CPU frequency the governor may select or to create a new
21,"governor. With the -c option, you can also specify for"
21,which of the processors the settings should be modified. That makes it
21,easy to use a consistent policy across all processors without adjusting
21,the settings for each processor individually. For more details and the
21,"available options, see the man page"
21,cpupower-frequency-set or run
21,cpupower frequency-set
21,--help.
21,12.4 Special tuning options #
21,The following sections highlight important settings.
21,12.4.1 Tuning options for P-states #
21,The CPUfreq subsystem offers several tuning options for P-states:
21,"You can switch between the different governors, influence minimum or"
21,maximum CPU frequency to be used or change individual governor
21,parameters.
21,"To switch to another governor at runtime, use"
21,cpupower frequency-set with the -g option. For
21,"example, running the following command (as root) will activate the"
21,performance governor:
21,# cpupower frequency-set -g performance
21,To set values for the minimum or maximum CPU frequency the governor may
21,"select, use the -d or -u option,"
21,respectively.
21,12.5 Troubleshooting #  BIOS options enabled?
21,"To use C-states or P-states, check your BIOS options:"
21,"To use C-states, make sure to enable CPU C State"
21,or similar options to benefit from power savings at idle.
21,"To use P-states and the CPUfreq governors, make sure to enable"
21,Processor Performance States options or similar.
21,"Even if P-states and C-states are available, it is possible that the"
21,platform firmware is managing CPU frequencies which may be sub-optimal.
21,"For example, if pcc-cpufreq is loaded then the"
21,"OS is only giving hints to the firmware, which is free to ignore the"
21,"hints. This can be addressed by selecting ""OS Management"" or similar"
21,"for CPU frequency managed in the BIOS. After reboot, an alternative"
21,driver will be used but the performance impact should be carefully
21,measured.
21,"In case of a CPU upgrade, make sure to upgrade your BIOS, too. The"
21,BIOS needs to know the new CPU and its frequency stepping to
21,pass this information on to the operating system.
21,Log file information?
21,"Check the systemd journal (see Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”)"
21,for any output regarding the CPUfreq subsystem. Only severe
21,errors are reported there.
21,If you suspect problems with the CPUfreq subsystem on your
21,"machine, you can also enable additional debug output. To do so, either"
21,use cpufreq.debug=7 as boot parameter or execute
21,the following command as root:
21,# echo 7 > /sys/module/cpufreq/parameters/debug
21,This will cause CPUfreq to log more information to
21,"dmesg on state transitions, which is useful for"
21,diagnosis. But as this additional output of kernel messages can be
21,"rather comprehensive, use it only if you are fairly sure that a"
21,problem exists.
21,12.6 More information #
21,Platforms with a Baseboard Management Controller (BMC) may have additional
21,power management configuration options accessible via the service
21,processor. These configurations are vendor specific and therefore not
21,"subject of this guide. For more information, refer to the manuals provided"
21,by your vendor.
21,12.7 Monitoring power consumption with powerTOP #
21,powerTOP helps to identify the causes of unnecessary high power
21,consumption. This is especially
21,"useful for laptops, where minimizing power consumption is"
21,more important. It supports both Intel and AMD processors.
21,Install it in the usual way:
21,> sudo zypper in powertop
21,powerTOP combines various sources of information (analysis of
21,"programs, device drivers, kernel options, number and sources of"
21,interrupts waking up processors from sleep states) and provides
21,"several ways of viewing them. You can launch it in interactive mode,"
21,which runs in an ncurses session (see
21,"Figure 12.1, “powerTOP in interactive mode”):"
21,> sudo powertopFigure 12.1: powerTOP in interactive mode #
21,powerTOP supports exporting reports to HTML and CSV.
21,The following example generates a single report of a 240-second run:
21,> sudo powertop --iteration=1 --time=240 --html=POWERREPORT.HTML
21,It can be useful to run separate reports over time.
21,"The following example runs powerTOP 10 times for 20 seconds each time,"
21,and creates a separate HTML report for each run:
21,> sudo powertop --iteration=10 --time=20 --html=POWERREPORT.HTML
21,This creates 10 time-stamped reports:
21,powerreport-20200108-104512.html
21,powerreport-20200108-104451.html
21,powerreport-20200108-104431.html
21,[...]
21,"An HTML report looks like Figure 12.2, “HTML powerTOP report”:"
21,Figure 12.2: HTML powerTOP report #
21,"The Tuning tab of the HTML reports, and the Tunables tab in the"
21,"interactive mode, both provide commands for testing the various power"
21,"settings. The HTML report prints the commands, which you can copy"
21,"to a root command line for testing, for example"
21,echo '0' > '/proc/sys/kernel/nmi_watchdog'.
21,The ncurses mode provides a simple toggle between Good
21,and Bad. Good runs a command
21,"to enable power saving, and Bad turns off power saving."
21,Enable all powerTOP settings with one command:
21,> sudo powertop --auto-tune
21,None of these changes survive a reboot. To make any changes
21,"permanent, use sysctl, udev,"
21,or systemd to run your selected commands at
21,"boot. powerTOP includes a systemd service file,"
21,/usr/lib/systemd/system/powertop.service. This
21,starts powerTOP with the --auto-tune option:
21,ExecStart=/usr/sbin/powertop --auto-tune
21,"Test this carefully before launching the systemd service,"
21,to see if it gives the results that you want.
21,You probably do not want USB keyboards and mice to go into
21,powersave modes because it is a nuisance to continually wake
21,"them up, and there may be other devices you want left alone. For easier"
21,"testing and configuration editing, extract the commands from"
21,an HTML report with awk:
21,> awk -F '</?td ?>' '/tune/ { print $4 }' POWERREPORT.HTML
21,"In calibrate mode, powerTOP sets up several runs that use different idle"
21,"settings for backlight, CPU, Wi-Fi, USB devices, and disks, and helps to"
21,identify optimal brightness settings on battery power:
21,> sudo powertop --calibrate
21,You may call a file that creates a workload for more accurate calibration:
21,> sudo powertop --calibrate --workload=FILENAME --html=POWERREPORT.HTML
21,"For more information, see:"
21,The powerTOP project page at https://01.org/powertop
21,"Section 2.6.2, “System control parameters: /proc/sys/”"
21,"Book “Administration Guide”, Chapter 15 “The systemd daemon”"
21,"Book “Administration Guide”, Chapter 24 “Dynamic kernel device management with udev”"
21,Part V Kernel tuning #  13 Tuning I/O performance
21,I/O scheduling controls how input/output operations will be submitted to
21,storage. SUSE Linux Enterprise Server offers various I/O algorithms—called
21,elevators—suiting different workloads.
21,Elevators can help to reduce seek operations and can prioritize I/O requests.
21,"14 Tuning the task schedulerModern operating systems, such as SUSE® Linux Enterprise Server, normally run many tasks at the same time. For example, you can be searching in a text file while receiving an e-mail and copying a big file to an external hard disk. These simple tasks require many additional processes to be run by the…15 Tuning the memory management subsystem"
21,To understand and tune the memory management behavior of the
21,"kernel, it is important to first have an overview of how it works and"
21,cooperates with other subsystems.
21,"16 Tuning the networkThe network subsystem is complex and its tuning highly depends on the system use scenario and on external factors such as software clients or hardware components (switches, routers, or gateways) in your network. The Linux kernel aims more at reliability and low latency than low overhead and high thr…17 Tuning SUSE Linux Enterprise for SAPThis chapter presents information about preparing and tuning SUSE Linux Enterprise Server to work optimally with SAP applications with sapconf. sapconf is for SUSE Linux Enterprise systems that install SAP applications. Customers who have SUSE Linux Enterprise Server for SAP Applications should use …13 Tuning I/O performance #"
21,I/O scheduling controls how input/output operations will be submitted to
21,storage. SUSE Linux Enterprise Server offers various I/O algorithms—called
21,elevators—suiting different workloads.
21,Elevators can help to reduce seek operations and can prioritize I/O requests.
21,"Choosing the best suited I/O elevator not only depends on the workload,"
21,"but on the hardware, too. Single ATA disk systems, SSDs, RAID arrays, or"
21,"network storage systems, for example, each require different tuning"
21,strategies.
21,13.1 Switching I/O scheduling #
21,"SUSE Linux Enterprise Server picks a default I/O scheduler at boot-time, which can be"
21,changed on the fly per block device. This makes it possible to set different
21,"algorithms, for example, for the device hosting the system partition and the"
21,device hosting a database.
21,The default I/O scheduler is chosen for each device based on whether the
21,"device reports to be rotational disk or not. For rotational disks, the"
21,BFQ I/O scheduler is picked.
21,Other devices default to MQ-DEADLINE or NONE.
21,"To change the elevator for a specific device in the running system, run"
21,the following command:
21,> sudo echo SCHEDULER > /sys/block/DEVICE/queue/scheduler
21,"Here, SCHEDULER is one of"
21,"bfq, none,"
21,"kyber, or mq-deadline."
21,DEVICE is the block device
21,(sda for example). Note that this change will not
21,persist during reboot. For permanent I/O scheduler change for a particular
21,"device, copy /usr/lib/udev/rules.d/60-io-scheduler.rules to"
21,"/etc/udev/rules.d/60-io-scheduler.rules, and edit"
21,the latter file to suit your needs.
21,Note: Default scheduler on IBM Z
21,"On IBM Z, the default I/O scheduler for a storage device is"
21,set by the device driver.
21,Note: elevator boot parameter removed
21,"The elevator boot parameter has been removed. The blk-mq I/O path replaces cfq, and does not include the"
21,elevator boot parameter.
21,13.2 Available I/O elevators with blk-mq I/O path #
21,Below is a list of elevators available on SUSE Linux Enterprise Server for devices
21,that use the blk-mq I/O path.
21,"If an elevator has tunable parameters, they can be set with the"
21,command:
21,echo VALUE > /sys/block/DEVICE/queue/iosched/TUNABLE
21,"In the command above, VALUE is the desired value for the"
21,TUNABLE and DEVICE is
21,the block device.
21,To find out what elevators are available for a device
21,"(sda for example), run the following"
21,command (the currently selected scheduler is listed in brackets):
21,> cat /sys/block/sda/queue/scheduler
21,[mq-deadline] kyber bfq noneNote: Scheduler options when switching from
21,Legacy Block to blk-mq I/O path
21,"When switching from legacy block to blk-mq I/O path for a device,"
21,the none option is roughly comparable to
21,"noop, mq-deadline is comparable"
21,"to deadline, and bfq is"
21,comparable to cfq.
21,13.2.1 MQ-DEADLINE #
21,MQ-DEADLINE is a
21,latency-oriented I/O scheduler. MQ-DEADLINE has the following
21,tunable parameters:
21,Table 13.1: MQ-DEADLINE tunable parameters #  FileDescriptionwrites_starved Controls how many times reads are preferred
21,over writes. A value of 3 means that
21,three read operations can be done before writes and reads
21,are dispatched on the same selection criteria.
21,Default is 3.
21,read_expire Sets the deadline (current time plus the
21,read_expire value) for read operations in milliseconds.
21,Default is 500.
21,write_expire Sets the deadline (current time plus the
21,write_expire value) for write operations in
21,milliseconds.
21,Default is 5000.
21,front_merges Enables (1) or disables (0) attempts to front
21,merge requests.
21,Default is 1.fifo_batch Sets the maximum number of requests per batch
21,(deadline expiration is only checked for batches). This
21,parameter allows to balance between latency and
21,"throughput. When set to 1 (that is, one"
21,"request per batch), it results in ""first come, first served"""
21,behavior and usually lowest latency. Higher values usually
21,increase throughput.
21,Default is 16.
21,13.2.2 NONE #
21,When NONE is selected
21,"as I/O elevator option for blk-mq, no I/O scheduler"
21,"is used, and I/O requests are passed down to the"
21,device without further I/O scheduling interaction.
21,NONE is the default for
21,NVM Express devices. With no overhead compared to other I/O
21,"elevator options, it is considered the fastest way of passing down"
21,I/O requests on multiple queues to such devices.
21,There are no tunable parameters for NONE.
21,13.2.3 BFQ (Budget Fair Queueing) #
21,BFQ is a
21,"fairness-oriented scheduler. It is described as ""a"
21,proportional-share storage-I/O scheduling algorithm based on the
21,"slice-by-slice service scheme of CFQ. But BFQ assigns budgets,"
21,"measured in number of sectors, to processes instead of time"
21,"slices."" (Source:"
21,linux-4.12/block/bfq-iosched.c)
21,BFQ allows to assign
21,I/O priorities to tasks which are taken into account during
21,"scheduling decisions (see Section 9.3.3, “Prioritizing disk access with ionice”)."
21,BFQ scheduler has the
21,following tunable parameters:
21,Table 13.2: BFQ tunable parameters #  FileDescriptionslice_idleValue in milliseconds specifies how long to
21,"idle, waiting for next request on an empty queue."
21,Default is 8.
21,slice_idle_usSame as slice_idle but in
21,microseconds.
21,Default is 8000.
21,low_latencyEnables (1) or disables (0) BFQ's low latency mode. This
21,"mode prioritizes certain applications (for example, if interactive)"
21,such that they observe lower latency.
21,Default is 1.
21,back_seek_max Maximum value (in Kbytes) for backward seeking.
21,Default is 16384.
21,back_seek_penalty Used to compute the cost of backward seeking.
21,Default is 2.
21,fifo_expire_async Value (in milliseconds) is used to set the
21,timeout of asynchronous requests.
21,Default is 250.
21,fifo_expire_sync Value in milliseconds specifies the
21,timeout of synchronous requests.
21,Default is 125.
21,timeout_sync Maximum time in milliseconds that a task
21,(queue) is serviced after it has been selected.
21,Default is 124.
21,max_budget Limit for number of sectors that are served
21,at maximum within timeout_sync. If set to
21,0 BFQ internally calculates a
21,value based on timeout_sync and an
21,estimated peak rate.
21,Default is 0
21,(set to auto-tuning). strict_guarantees Enables (1) or disables (0) BFQ specific queue handling
21,required to give stricter bandwidth sharing guarantees
21,under certain conditions.
21,Default is 0.
21,13.2.4 KYBER #
21,KYBER is a
21,latency-oriented I/O scheduler. It makes it possible to set target latencies
21,for reads and synchronous writes and throttles I/O requests in
21,order to try to meet these target latencies.
21,Table 13.3: KYBER tunable parameters #  FileDescriptionread_lat_nsecSets the target latency for read operations in
21,nanoseconds.
21,Default is 2000000.
21,write_lat_nsecSets the target latency for write operations in
21,nanoseconds.
21,Default is 10000000.
21,13.3 I/O barrier tuning #
21,"Some file systems (for example, Ext3 or Ext4) send write"
21,barriers to disk after fsync or during transaction commits. Write
21,"barriers enforce proper ordering of writes, making volatile disk write"
21,caches safe to use (at some performance penalty). If your disks are
21,"battery-backed in one way or another, disabling barriers can safely"
21,improve performance.
21,Important: nobarrier is deprecated in XFS
21,Note that the nobarrier option has been completely deprecated
21,"for XFS, and it is not a valid mount option in SUSE Linux Enterprise 15 SP2 and upward. Any"
21,XFS mount command that explicitly specifies the flag will fail to mount the
21,"file system. To prevent this from happening, make sure that no scripts or"
21,fstab entries contain the nobarrier option.
21,Sending write barriers can be disabled using the
21,nobarrier mount option.
21,Warning: Disabling barriers can lead to data loss
21,Disabling barriers when disks cannot guarantee caches are properly
21,written in case of power failure can lead to severe file system
21,corruption and data loss.
21,14 Tuning the task scheduler #
21,"Modern operating systems, such as SUSE® Linux Enterprise Server, normally run many"
21,"tasks at the same time. For example, you can be searching in a"
21,text file while receiving an e-mail and copying a big file to an external
21,hard disk. These simple tasks require many additional processes to be run
21,"by the system. To provide each task with its required system resources,"
21,the Linux kernel needs a tool to distribute available system resources to
21,individual tasks. And this is exactly what the task
21,scheduler does.
21,The following sections explain the most important terms related to a
21,process scheduling. They also introduce information about the task
21,"scheduler policy, scheduling algorithm, description of the task scheduler"
21,"used by SUSE Linux Enterprise Server, and references to other sources of relevant"
21,information.
21,14.1 Introduction #
21,The Linux kernel controls the way that tasks (or processes) are managed
21,"on the system. The task scheduler, sometimes called process"
21,"scheduler, is the part of the kernel that decides which task"
21,to run next. It is responsible for best using system resources to
21,guarantee that multiple tasks are being executed simultaneously. This
21,makes it a core component of any multitasking operating system.
21,14.1.1 Preemption #
21,The theory behind task scheduling is very simple. If there are runnable
21,"processes in a system, at least one process must always be running. If"
21,"there are more runnable processes than processors in a system, not all"
21,the processes can be running all the time.
21,"Therefore, some processes need to be stopped temporarily, or"
21,"suspended, so that others can be running again. The"
21,scheduler decides what process in the queue will run next.
21,"As already mentioned, Linux, like all other Unix variants, is a"
21,multitasking operating system. That means that
21,several tasks can be running at the same time. Linux provides a so
21,"called preemptive multitasking, where the scheduler"
21,decides when a process is suspended. This forced suspension is called
21,preemption. All Unix flavors have been providing
21,preemptive multitasking since the beginning.
21,14.1.2 Timeslice #
21,The time period for which a process will be running before it is
21,preempted is defined in advance. It is called a
21,timeslice of a process and represents the amount of
21,processor time that is provided to each process. By assigning
21,"timeslices, the scheduler makes global decisions for the running system,"
21,and prevents individual processes from dominating over the processor
21,resources.
21,14.1.3 Process priority #
21,The scheduler evaluates processes based on their priority. To calculate
21,"the current priority of a process, the task scheduler uses complex"
21,"algorithms. As a result, each process is given a value according to"
21,which it is “allowed” to run on a processor.
21,14.2 Process classification #
21,Processes are usually classified according to their purpose and behavior.
21,"Although the borderline is not always clearly distinct, generally two"
21,criteria are used to sort them. These criteria are independent and do not
21,exclude each other.
21,One approach is to classify a process either
21,I/O-bound or processor-bound.
21,I/O-bound
21,"I/O stands for Input/Output devices, such as keyboards, mice, or"
21,optical and hard disks. I/O-bound processes spend
21,the majority of time submitting and waiting for requests. They are run
21,"very frequently, but for short time intervals, not to block other"
21,processes waiting for I/O requests.
21,processor-bound
21,"On the other hand, processor-bound tasks use"
21,"their time to execute a code, and usually run until they are preempted"
21,by the scheduler. They do not block processes waiting for I/O
21,"requests, and, therefore, can be run less frequently but for longer"
21,time intervals.
21,Another approach is to divide processes by type into
21,"interactive, batch, and"
21,real-time processes.
21,Interactive processes spend a lot of time waiting
21,"for I/O requests, such as keyboard or mouse operations. The scheduler"
21,"must wake up such processes quickly on user request, or the user will"
21,find the environment unresponsive. The typical delay is approximately
21,"100 ms. Office applications, text editors or image manipulation"
21,programs represent typical interactive processes.
21,Batch processes often run in the background and do
21,not need to be responsive. They usually receive lower priority from the
21,"scheduler. Multimedia converters, database search engines, or log files"
21,analyzers are typical examples of batch processes.
21,Real-time processes must never be blocked by
21,"low-priority processes, and the scheduler guarantees a short response"
21,time to them. Applications for editing multimedia content are a good
21,example here.
21,14.3 Completely Fair Scheduler #
21,"Since the Linux kernel version 2.6.23, a new approach has been taken to"
21,the scheduling of runnable processes. Completely Fair Scheduler (CFS)
21,"became the default Linux kernel scheduler. Since then, important changes"
21,and improvements have been made. The information in this chapter applies
21,to SUSE Linux Enterprise Server with kernel version 2.6.32 and higher (including 3.x
21,"kernels). The scheduler environment was divided into several parts, and"
21,three main new features were introduced:
21,Modular scheduler core
21,The core of the scheduler was enhanced with scheduling
21,classes. These classes are modular and represent scheduling
21,policies.
21,Completely Fair Scheduler
21,"Introduced in kernel 2.6.23 and extended in 2.6.24, CFS tries to"
21,assure that each process obtains its “fair” share of the
21,processor time.
21,Group scheduling
21,"For example, if you split processes into groups according to which"
21,"user is running them, CFS tries to provide each of these groups with"
21,the same amount of processor time.
21,"As a result, CFS brings optimized scheduling for both servers and"
21,desktops.
21,14.3.1 How CFS works #
21,CFS tries to guarantee a fair approach to each runnable task. To find
21,"the most balanced way of task scheduling, it uses the concept of"
21,red-black tree. A red-black tree is a type of
21,self-balancing data search tree which provides inserting and removing
21,entries in a reasonable way so that it remains well balanced.
21,When CFS schedules a task it accumulates “virtual
21,runtime” or vruntime. The next task picked
21,to run is always the task with the minimum accumulated vruntime so
21,far. By balancing the red-black tree when tasks are inserted into the
21,run queue (a planned time line of processes to be
21,"executed next), the task with the minimum vruntime is always the first"
21,entry in the red-black tree.
21,The amount of vruntime a task accrues is related to its priority.
21,High priority tasks gain vruntime at a slower rate than low priority
21,"tasks, which results in high priority tasks being picked to run on the"
21,processor more often.
21,14.3.2 Grouping processes #
21,"Since the Linux kernel version 2.6.24, CFS can be tuned to be fair"
21,to groups rather than to tasks only. Runnable tasks are then grouped
21,"to form entities, and CFS tries to be fair to these entities instead"
21,of individual runnable tasks. The scheduler also tries to be fair to
21,individual tasks within these entities.
21,The kernel scheduler lets you group runnable tasks using control
21,"groups. For more information, see Chapter 10, Kernel control groups."
21,14.3.3 Kernel configuration options #
21,Basic aspects of the task scheduler behavior can be set through the
21,kernel configuration options. Setting these options is part of the
21,kernel compilation process. Because kernel compilation process is a
21,"complex task and out of this document's scope, refer to relevant source"
21,of information.
21,Warning: Kernel compilation
21,"If you run SUSE Linux Enterprise Server on a kernel that was not shipped with it,"
21,"for example on a self-compiled kernel, you lose the entire support"
21,entitlement.
21,14.3.4 Terminology #
21,Documents regarding task scheduling policy often use several technical
21,terms which you need to know to understand the information correctly.
21,Here are some:
21,Latency
21,Delay between the time a process is scheduled to run and the actual
21,process execution.
21,Granularity
21,The relation between granularity and latency can be expressed by the
21,following equation:
21,gran = ( lat / rtasks ) - ( lat / rtasks / rtasks )
21,"where gran stands for granularity,"
21,"lat stand for latency, and"
21,rtasks is the number of running tasks.
21,14.3.4.1 Scheduling policies #
21,The Linux kernel supports the following scheduling policies:
21,SCHED_FIFO
21,Scheduling policy designed for special time-critical applications.
21,It uses the First In-First Out scheduling algorithm.
21,SCHED_BATCH
21,Scheduling policy designed for CPU-intensive tasks.
21,SCHED_IDLE
21,Scheduling policy intended for very low
21,prioritized tasks.
21,SCHED_OTHER
21,Default Linux time-sharing scheduling policy used by the majority of
21,processes.
21,SCHED_RR
21,"Similar to SCHED_FIFO, but uses the Round"
21,Robin scheduling algorithm.
21,14.3.5 Changing real-time attributes of processes with chrt #
21,The chrt command sets or retrieves the real-time
21,"scheduling attributes of a running process, or runs a command with the"
21,specified attributes. You can get or retrieve both the scheduling policy
21,and priority of a process.
21,"In the following examples, a process whose PID is 16244 is used."
21,To retrieve the real-time attributes of an existing
21,task:
21,# chrt -p 16244
21,pid 16244's current scheduling policy: SCHED_OTHER
21,pid 16244's current scheduling priority: 0
21,"Before setting a new scheduling policy on the process, you need to find"
21,out the minimum and maximum valid priorities for each scheduling
21,algorithm:
21,# chrt -m
21,SCHED_SCHED_OTHER min/max priority : 0/0
21,SCHED_SCHED_FIFO min/max priority : 1/99
21,SCHED_SCHED_RR min/max priority : 1/99
21,SCHED_SCHED_BATCH min/max priority : 0/0
21,SCHED_SCHED_IDLE min/max priority : 0/0
21,"In the above example, SCHED_OTHER, SCHED_BATCH, SCHED_IDLE polices only"
21,"allow for priority 0, while that of SCHED_FIFO and SCHED_RR can range"
21,from 1 to 99.
21,To set SCHED_BATCH scheduling policy:
21,# chrt -b -p 0 16244
21,pid 16244's current scheduling policy: SCHED_BATCH
21,pid 16244's current scheduling priority: 0
21,"For more information on chrt, see its man page"
21,(man 1 chrt).
21,14.3.6 Runtime tuning with sysctl #
21,The sysctl interface for examining and changing
21,kernel parameters at runtime introduces important variables by means of
21,which you can change the default behavior of the task scheduler. The
21,"syntax of the sysctl is simple, and all the following"
21,commands must be entered on the command line as root.
21,"To read a value from a kernel variable, enter"
21,# sysctl VARIABLE
21,"To assign a value, enter"
21,# sysctl VARIABLE=VALUE
21,"To get a list of all scheduler related variables, run the"
21,"sysctl command, and use grep"
21,to filter the output:
21,"# sysctl -A | grep ""sched"" | grep -v ""domain"""
21,kernel.sched_cfs_bandwidth_slice_us = 5000
21,kernel.sched_child_runs_first = 0
21,kernel.sched_compat_yield = 0
21,kernel.sched_latency_ns = 24000000
21,kernel.sched_migration_cost_ns = 500000
21,kernel.sched_min_granularity_ns = 8000000
21,kernel.sched_nr_migrate = 32
21,kernel.sched_rr_timeslice_ms = 25
21,kernel.sched_rt_period_us = 1000000
21,kernel.sched_rt_runtime_us = 950000
21,kernel.sched_schedstats = 0
21,kernel.sched_shares_window_ns = 10000000
21,kernel.sched_time_avg_ms = 1000
21,kernel.sched_tunable_scaling = 1
21,kernel.sched_wakeup_granularity_ns = 10000000
21,Note that variables ending with “_ns” and
21,"“_us” accept values in nanoseconds and microseconds,"
21,respectively.
21,A list of the most important task scheduler sysctl
21,tuning variables (located at /proc/sys/kernel/)
21,with a short description follows:
21,sched_cfs_bandwidth_slice_us
21,"When CFS bandwidth control is in use, this parameter controls"
21,the amount of run-time (bandwidth) transferred to a run queue from the
21,task's control group bandwidth pool. Small values allow the global
21,"bandwidth to be shared in a fine-grained manner among tasks, larger"
21,values reduce transfer overhead. See
21,https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt.
21,sched_child_runs_first
21,A freshly forked child runs before the parent continues execution.
21,Setting this parameter to 1 is beneficial for an
21,application in which the child performs an execution after fork. For
21,example make
21,-j<NO_CPUS>
21,performs better when sched_child_runs_first is turned off. The
21,default value is 0.
21,sched_compat_yield
21,Enables the aggressive CPU yielding behavior of the old
21,O(1) scheduler by moving the relinquishing task to
21,the end of the runnable queue (right-most position in the red-black
21,tree). Applications that depend on the sched_yield(2)
21,syscall behavior may see performance improvements by giving other
21,processes a chance to run when there are highly contended resources
21,"(such as locks). On the other hand, given that this call occurs in"
21,"context switching, misusing the call can hurt the workload. Only use it"
21,when you see a drop in performance. The default value is
21,sched_migration_cost_ns
21,Amount of time after the last execution that a task is considered to
21,be “cache hot” in migration decisions. A
21,"“hot” task is less likely to be migrated to another CPU,"
21,so increasing this variable reduces task migrations. The default value is
21,500000 (ns).
21,If the CPU idle time is higher than expected when there are runnable
21,"processes, try reducing this value. If tasks bounce between CPUs or"
21,"nodes too often, try increasing it."
21,sched_latency_ns
21,Targeted preemption latency for CPU bound tasks. Increasing this
21,variable increases a CPU bound task's timeslice. A task's timeslice
21,is its weighted fair share of the scheduling period:
21,timeslice = scheduling period * (task's weight/total weight of tasks
21,in the run queue)
21,The task's weight depends on the task's nice level and the scheduling
21,"policy. Minimum task weight for a SCHED_OTHER task is 15,"
21,"corresponding to nice 19. The maximum task weight is 88761,"
21,corresponding to nice -20.
21,Timeslices become smaller as the load increases. When the number of
21,runnable tasks exceeds
21,"sched_latency_ns/sched_min_granularity_ns,"
21,the slice becomes number_of_running_tasks *
21,"sched_min_granularity_ns. Prior to that, the"
21,slice is equal to sched_latency_ns.
21,This value also specifies the maximum amount of time during which a
21,sleeping task is considered to be running for entitlement
21,calculations. Increasing this variable increases the amount of time a
21,"waking task may consume before being preempted, thus increasing"
21,scheduler latency for CPU bound tasks. The default value is
21,6000000 (ns).
21,sched_min_granularity_ns
21,Minimal preemption granularity for CPU bound tasks. See
21,sched_latency_ns for details. The default
21,value is 4000000 (ns).
21,sched_wakeup_granularity_ns
21,The wake-up preemption granularity. Increasing this variable reduces
21,"wake-up preemption, reducing disturbance of compute bound tasks."
21,Lowering it improves wake-up latency and throughput for latency
21,"critical tasks, particularly when a short duty cycle load component"
21,must compete with CPU bound components. The default value is
21,2500000 (ns).
21,Warning: Setting the right wake-up granularity value
21,Settings larger than half of
21,sched_latency_ns will result in no wake-up
21,preemption. Short duty cycle tasks will be unable to compete with
21,CPU hogs effectively.
21,sched_rr_timeslice_ms
21,Quantum that SCHED_RR tasks are allowed to run before they are
21,preempted and put to the end of the task list.
21,sched_rt_period_us
21,Period over which real-time task bandwidth enforcement is measured.
21,The default value is 1000000 (µs).
21,sched_rt_runtime_us
21,Quantum allocated to real-time tasks during sched_rt_period_us.
21,"Setting to -1 disables RT bandwidth enforcement. By default, RT tasks"
21,"may consume 95%CPU/sec, thus leaving 5%CPU/sec or 0.05s to be used by"
21,SCHED_OTHER tasks. The default value is 950000
21,(µs).
21,sched_nr_migrate
21,Controls how many tasks can be migrated across processors for
21,load-balancing purposes. Because balancing iterates the runqueue
21,"with interrupts disabled (softirq), it can incur in irq-latency"
21,penalties for real-time tasks.
21,Therefore increasing this value
21,may give a performance boost to large SCHED_OTHER threads at the
21,expense of increased irq-latencies for real-time tasks. The default
21,value is 32.
21,sched_time_avg_ms
21,This parameter sets the period over which the time spent running
21,real-time tasks is averaged. That average assists CFS in making
21,load-balancing decisions and gives an indication of how busy a CPU is
21,with high-priority real-time tasks.
21,The optimal setting for this parameter is highly workload
21,"dependent and depends, among other things, on how frequently"
21,real-time tasks are running and for how long.
21,14.3.7 Debugging interface and scheduler statistics #
21,"CFS comes with a new improved debugging interface, and provides runtime"
21,statistics information. Relevant files were added to the
21,"/proc file system, which can be examined simply"
21,with the cat or less command. A
21,list of the related /proc files follows with their
21,short description:
21,/proc/sched_debug
21,"Contains the current values of all tunable variables (see Section 14.3.6, “Runtime tuning with sysctl”) that affect the task"
21,"scheduler behavior, CFS statistics, and information about the run queues"
21,"(CFS, RT and deadline) on all available processors."
21,A summary of the
21,"task running on each processor is also shown, with the task name and"
21,"PID, along with scheduler specific statistics. The first"
21,"being tree-key column, it indicates the task's virtual"
21,"runtime, and its name comes from the kernel sorting all runnable tasks"
21,by this key in a red-black tree. The switches column
21,"indicates the total number of switches (involuntary or not), and"
21,naturally the prio refers to the process priority. The
21,wait-time value indicates the amount of time the task
21,waited to be scheduled. Finally both sum-exec and
21,sum-sleep account for the total amount of time (in
21,"nanoseconds) the task was running on the processor or asleep,"
21,respectively.
21,# cat /proc/sched_debug
21,"Sched Debug Version: v0.11, 4.4.21-64-default #1"
21,ktime
21,: 23533900.395978
21,sched_clk
21,: 23543587.726648
21,cpu_clk
21,: 23533900.396165
21,jiffies
21,: 4300775771
21,sched_clock_stable
21,: 0
21,sysctl_sched
21,.sysctl_sched_latency
21,: 6.000000
21,.sysctl_sched_min_granularity
21,: 2.000000
21,.sysctl_sched_wakeup_granularity
21,: 2.500000
21,.sysctl_sched_child_runs_first
21,: 0
21,.sysctl_sched_features
21,: 154871
21,.sysctl_sched_tunable_scaling
21,: 1 (logaritmic)
21,"cpu#0, 2666.762 MHz"
21,.nr_running
21,: 1
21,.load
21,: 1024
21,.nr_switches
21,: 1918946
21,[...]
21,cfs_rq[0]:/
21,.exec_clock
21,: 170176.383770
21,.MIN_vruntime
21,: 0.000001
21,.min_vruntime
21,: 347375.854324
21,.max_vruntime
21,: 0.000001
21,[...]
21,rt_rq[0]:/
21,.rt_nr_running
21,: 0
21,.rt_throttled
21,: 0
21,.rt_time
21,: 0.000000
21,.rt_runtime
21,: 950.000000
21,dl_rq[0]:
21,.dl_nr_running
21,: 0
21,task
21,PID
21,tree-key
21,switches
21,prio
21,wait-time
21,[...]
21,------------------------------------------------------------------------
21,cc1 63477
21,98876.717832
21,197
21,120
21,0.000000
21,.../proc/schedstat
21,Displays statistics relevant to the current run queue. Also
21,domain-specific statistics for SMP systems are displayed for all
21,"connected processors. Because the output format is not user-friendly,"
21,read the contents of
21,/usr/src/linux/Documentation/scheduler/sched-stats.txt
21,for more information.
21,/proc/PID/sched
21,Displays scheduling information on the process with id
21,PID.
21,# cat /proc/$(pidof gdm)/sched
21,"gdm (744, #threads: 3)"
21,-------------------------------------------------------------------
21,se.exec_start
21,8888.758381
21,se.vruntime
21,6062.853815
21,se.sum_exec_runtime
21,7.836043
21,se.statistics.wait_start
21,0.000000
21,se.statistics.sleep_start
21,8888.758381
21,se.statistics.block_start
21,0.000000
21,se.statistics.sleep_max
21,1965.987638
21,[...]
21,se.avg.decay_count
21,8477
21,policy
21,prio
21,120
21,clock-delta
21,128
21,mm->numa_scan_seq
21,"numa_migrations, 0"
21,"numa_faults_memory, 0, 0, 1, 0, -1"
21,"numa_faults_memory, 1, 0, 0, 0, -114.4 More information #"
21,"To get a compact knowledge about Linux kernel task scheduling, you need"
21,to explore several information sources. Here are some:
21,"For task scheduler System Calls description, see the relevant manual"
21,page (for example man 2 sched_setaffinity).
21,A useful lecture on Linux scheduler policy and algorithm is available
21,http://www.inf.fu-berlin.de/lehre/SS01/OS/Lectures/Lecture08.pdf.
21,A good overview of Linux process scheduling is given in
21,Linux Kernel Development by Robert Love
21,(ISBN-10: 0-672-32512-8). See
21,https://www.informit.com/articles/article.aspx?p=101760.
21,A very comprehensive overview of the Linux kernel internals is given in
21,Understanding the Linux Kernel by Daniel P.
21,Bovet and Marco Cesati (ISBN 978-0-596-00565-8).
21,Technical information about task scheduler is covered in files under
21,/usr/src/linux/Documentation/scheduler.
21,15 Tuning the memory management subsystem #
21,To understand and tune the memory management behavior of the
21,"kernel, it is important to first have an overview of how it works and"
21,cooperates with other subsystems.
21,"The memory management subsystem, also called the virtual memory manager,"
21,will subsequently be called “VM”. The role of the VM
21,is to manage the allocation of physical memory (RAM) for the entire kernel
21,and user programs. It is also responsible for providing a virtual memory
21,environment for user processes (managed via POSIX APIs with Linux
21,"extensions). Finally, the VM is responsible for freeing up RAM when there"
21,"is a shortage, either by trimming caches or swapping out"
21,“anonymous” memory.
21,The most important thing to understand when examining and tuning VM is how
21,its caches are managed. The basic goal of the VM's caches is to minimize
21,the cost of I/O as generated by swapping and file system operations
21,(including network file systems). This is achieved by avoiding I/O
21,"completely, or by submitting I/O in better patterns."
21,Free memory will be used and filled up by these caches as required. The
21,"more memory is available for caches and anonymous memory, the more"
21,"effectively caches and swapping will operate. However, if a memory"
21,"shortage is encountered, caches will be trimmed or memory will be swapped"
21,out.
21,"For a particular workload, the first thing that can be done to improve"
21,performance is to increase memory and reduce the frequency that memory
21,must be trimmed or swapped. The second thing is to change the way caches
21,are managed by changing kernel parameters.
21,"Finally, the workload itself should be examined and tuned as well. If an"
21,"application is allowed to run more processes or threads, effectiveness of"
21,"VM caches can be reduced, if each process is operating in its own area of"
21,the file system. Memory overheads are also increased. If applications
21,"allocate their own buffers or caches, larger caches will mean that less"
21,"memory is available for VM caches. However, more processes and threads can"
21,"mean more opportunity to overlap and pipeline I/O, and may take better"
21,advantage of multiple cores. Experimentation will be required for the best
21,results.
21,15.1 Memory usage #
21,Memory allocations in general can be characterized as
21,"“pinned” (also known as “unreclaimable”),"
21,“reclaimable” or “swappable”.
21,15.1.1 Anonymous memory #
21,"Anonymous memory tends to be program heap and stack memory (for example,"
21,">malloc()). It is reclaimable, except in special"
21,cases such as mlock or if there is no available swap
21,space. Anonymous memory must be written to swap before it can be
21,reclaimed. Swap I/O (both swapping in and swapping out pages) tends to
21,"be less efficient than pagecache I/O, because of allocation and access"
21,patterns.
21,15.1.2 Pagecache #
21,"A cache of file data. When a file is read from disk or network, the"
21,"contents are stored in pagecache. No disk or network access is required,"
21,if the contents are up-to-date in pagecache. tmpfs and shared memory
21,segments count toward pagecache.
21,"When a file is written to, the new data is stored in pagecache before"
21,being written back to a disk or the network (making it a write-back
21,"cache). When a page has new data not written back yet, it is called"
21,“dirty”. Pages not classified as dirty are
21,“clean”. Clean pagecache pages can be reclaimed if there is
21,a memory shortage by simply freeing them. Dirty pages must first be made
21,clean before being reclaimed.
21,15.1.3 Buffercache #
21,"This is a type of pagecache for block devices (for example, /dev/sda). A"
21,file system typically uses the buffercache when accessing its on-disk
21,"metadata structures such as inode tables, allocation bitmaps, and so"
21,forth. Buffercache can be reclaimed similarly to pagecache.
21,15.1.4 Buffer heads #
21,Buffer heads are small auxiliary structures that tend to be allocated
21,upon pagecache access. They can generally be reclaimed easily when the
21,pagecache or buffercache pages are clean.
21,15.1.5 Writeback #
21,"As applications write to files, the pagecache becomes dirty"
21,and the buffercache may become dirty. When the amount of
21,dirty memory reaches a specified number of pages in bytes
21,"(vm.dirty_background_bytes), or when the"
21,amount of dirty memory reaches a specific ratio to total memory
21,"(vm.dirty_background_ratio), or when the pages"
21,have been dirty for longer than a specified amount of time
21,"(vm.dirty_expire_centisecs), the kernel begins"
21,writeback of pages starting with files that had the pages dirtied first.
21,The background bytes and ratios are mutually exclusive and setting one
21,will overwrite the other. Flusher threads perform writeback in the
21,background and allow applications to continue running. If the I/O
21,"cannot keep up with applications dirtying pagecache, and dirty data"
21,reaches a critical setting (vm.dirty_bytes or
21,"vm.dirty_ratio), then applications begin to be"
21,throttled to prevent dirty data exceeding this threshold.
21,15.1.6 Readahead #
21,The VM monitors file access patterns and may attempt to perform
21,readahead. Readahead reads pages into the pagecache from the file system
21,"that have not been requested yet. It is done to allow fewer,"
21,larger I/O requests to be submitted (more efficient). And for I/O to be
21,pipelined (I/O performed at the same time as the application is
21,running).
21,15.1.7 VFS caches #  15.1.7.1 Inode cache #
21,This is an in-memory cache of the inode structures for each file
21,"system. These contain attributes such as the file size, permissions and"
21,"ownership, and pointers to the file data."
21,15.1.7.2 Directory entry cache #
21,This is an in-memory cache of the directory entries in the system.
21,"These contain a name (the name of a file), the inode which it refers"
21,"to, and children entries. This cache is used when traversing the"
21,directory structure and accessing a file by name.
21,15.2 Reducing memory usage #  15.2.1 Reducing malloc (anonymous) usage #
21,Applications running on SUSE Linux Enterprise Server 15 SP3 can allocate
21,more memory compared to older releases. This is because of
21,glibc changing its default
21,behavior while allocating user space memory. See
21,http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html
21,for explanation of these parameters.
21,"To restore behavior similar to older releases, M_MMAP_THRESHOLD should"
21,be set to 128*1024. This can be done with mallopt() call from the
21,"application, or via setting MALLOC_MMAP_THRESHOLD_"
21,environment variable before running the application.
21,15.2.2 Reducing kernel memory overheads #
21,"Kernel memory that is reclaimable (caches, described above) will be"
21,trimmed automatically during memory shortages. Most other kernel memory
21,cannot be easily reduced but is a property of the workload given to the
21,kernel.
21,Reducing the requirements of the user space workload will reduce the
21,"kernel memory usage (fewer processes, fewer open files and sockets,"
21,etc.)
21,15.2.3 Memory controller (memory cgroups) #
21,"If the memory cgroups feature is not needed, it can be switched off by"
21,"passing cgroup_disable=memory on the kernel command line, reducing"
21,memory consumption of the kernel a bit. There is also a slight
21,performance benefit as there is a small amount of accounting overhead
21,when memory cgroups are available even if none are configured.
21,15.3 Virtual memory manager (VM) tunable parameters #
21,When tuning the VM it should be understood that some
21,changes will
21,take time to affect the workload and take full effect. If the workload
21,"changes throughout the day, it may behave very differently at different"
21,times. A change that increases throughput under some conditions may
21,decrease it under other conditions.
21,15.3.1 Reclaim ratios #  /proc/sys/vm/swappiness
21,This control is used to define how aggressively the kernel swaps out
21,anonymous memory relative to pagecache and other caches. Increasing
21,the value increases the amount of swapping. The default value is
21,60.
21,"Swap I/O tends to be much less efficient than other I/O. However,"
21,some pagecache pages will be accessed much more frequently than less
21,used anonymous memory. The right balance should be found here.
21,"If swap activity is observed during slowdowns, it may be worth"
21,reducing this parameter. If there is a lot of I/O activity and the
21,"amount of pagecache in the system is rather small, or if there are"
21,"large dormant applications running, increasing this value might"
21,improve performance.
21,"Note that the more data is swapped out, the longer the system will"
21,take to swap data back in when it is needed.
21,/proc/sys/vm/vfs_cache_pressure
21,This variable controls the tendency of the kernel to reclaim the
21,"memory which is used for caching of VFS caches, versus pagecache and"
21,swap. Increasing this value increases the rate at which VFS caches
21,are reclaimed.
21,"It is difficult to know when this should be changed, other than by"
21,experimentation. The slabtop command (part of the
21,package procps) shows top
21,"memory objects used by the kernel. The vfs caches are the ""dentry"""
21,"and the ""*_inode_cache"" objects. If these are consuming a large"
21,"amount of memory in relation to pagecache, it may be worth trying to"
21,increase pressure. Could also help to reduce swapping. The default
21,value is 100.
21,/proc/sys/vm/min_free_kbytes
21,This controls the amount of memory that is kept free for use by
21,special reserves including “atomic” allocations (those
21,which cannot wait for reclaim). This should not normally be lowered
21,unless the system is being very carefully tuned for memory usage
21,(normally useful for embedded rather than server applications). If
21,“page allocation failure” messages and stack traces are
21,"frequently seen in logs, min_free_kbytes could be increased until the"
21,"errors disappear. There is no need for concern, if these messages are"
21,very infrequent. The default value depends on the amount of RAM.
21,/proc/sys/vm/watermark_scale_factor
21,"Broadly speaking, free memory has high, low and min watermarks. When"
21,the low watermark is reached then kswapd wakes to
21,reclaim memory in the background. It stays awake until free memory
21,reaches the high watermark. Applications will stall and reclaim
21,memory when the min watermark is reached.
21,The watermark_scale_factor defines the amount
21,of memory left in a node/system before kswapd is woken up and how
21,much memory needs to be free before kswapd goes back to sleep.
21,"The unit is in fractions of 10,000. The default value of 10 means"
21,the distances between watermarks are 0.1% of the available memory
21,"in the node/system. The maximum value is 1000, or 10% of memory."
21,"Workloads that frequently stall in direct reclaim, accounted by"
21,"allocstall in /proc/vmstat,"
21,"may benefit from altering this parameter. Similarly, if"
21,"kswapd is sleeping prematurely, as accounted for by"
21,"kswapd_low_wmark_hit_quickly, then it may indicate"
21,that the number of pages kept free to avoid stalls is too low.
21,15.3.2 Writeback parameters #
21,One important change in writeback behavior since SUSE Linux Enterprise Server 10 is
21,that modification to file-backed mmap() memory is accounted immediately
21,as dirty memory (and subject to writeback). Whereas previously it would
21,"only be subject to writeback after it was unmapped, upon an msync()"
21,"system call, or under heavy memory pressure."
21,Some applications do not expect mmap modifications to be subject to such
21,"writeback behavior, and performance can be reduced. Increasing writeback"
21,ratios and times can improve this type of slowdown.
21,/proc/sys/vm/dirty_background_ratio
21,This is the percentage of the total amount of free and reclaimable
21,"memory. When the amount of dirty pagecache exceeds this percentage,"
21,writeback threads start writing back dirty memory. The default value
21,is 10 (%).
21,/proc/sys/vm/dirty_background_bytes
21,This contains the amount of dirty memory at which
21,the background kernel flusher threads will start writeback.
21,dirty_background_bytes is the counterpart of
21,"dirty_background_ratio. If one of them is set,"
21,the other one will automatically be read as 0.
21,/proc/sys/vm/dirty_ratio
21,Similar percentage value as for
21,"dirty_background_ratio. When this is exceeded,"
21,applications that want to write to the pagecache are blocked and
21,wait for kernel background flusher threads to reduce the amount of dirty
21,memory. The default value is 20 (%).
21,/proc/sys/vm/dirty_bytes
21,This file controls the same tunable as dirty_ratio
21,however the amount of dirty memory is in bytes as opposed to a
21,percentage of reclaimable memory. Since both
21,dirty_ratio and dirty_bytes
21,"control the same tunable, if one of them is set, the other one will"
21,automatically be read as 0. The minimum value allowed
21,for dirty_bytes is two pages (in bytes); any value
21,lower than this limit will be ignored and the old configuration will be
21,retained.
21,/proc/sys/vm/dirty_expire_centisecs
21,Data which has been dirty in-memory for longer than this interval
21,will be written out next time a flusher thread wakes up. Expiration
21,is measured based on the modification time of a file's inode.
21,"Therefore, multiple dirtied pages from the same file will all be"
21,written when the interval is exceeded.
21,dirty_background_ratio and
21,dirty_ratio together determine the pagecache
21,"writeback behavior. If these values are increased, more dirty memory is"
21,kept in the system for a longer time. With more dirty memory allowed in
21,"the system, the chance to improve throughput by avoiding writeback I/O"
21,"and to submitting more optimal I/O patterns increases. However, more"
21,dirty memory can either harm latency when memory needs to be reclaimed
21,or at points of data integrity (“synchronization points”) when it
21,needs to be written back to disk.
21,15.3.3 Timing differences of I/O writes between SUSE Linux Enterprise 12 and SUSE Linux Enterprise 11 #
21,The system is required to limit what percentage of the system's memory
21,contains file-backed data that needs writing to disk. This guarantees
21,that the system can always allocate the necessary data structures to
21,complete I/O. The maximum amount of memory that can be dirty and
21,requires writing at any time is controlled by
21,vm.dirty_ratio
21,(/proc/sys/vm/dirty_ratio). The defaults are:
21,SLE-11-SP3:
21,vm.dirty_ratio = 40
21,SLE-12:
21,vm.dirty_ratio = 20
21,The primary advantage of using the lower ratio in SUSE Linux Enterprise 12 is that
21,page reclamation and allocation in low memory situations completes
21,faster as there is a higher probability that old clean pages will be
21,quickly found and discarded. The secondary advantage is that if all
21,"data on the system must be synchronized, then the time to complete the"
21,operation on SUSE Linux Enterprise 12 will be lower than SUSE Linux Enterprise 11 SP3 by default.
21,Most workloads will not notice this change as data is synchronized with
21,fsync() by the application or data is not dirtied
21,quickly enough to hit the limits.
21,"There are exceptions and if your application is affected by this, it"
21,will manifest as an unexpected stall during writes. To prove it is
21,affected by dirty data rate limiting then monitor
21,/proc/PID_OF_APPLICATION/stack
21,and it will be observed that the application spends significant time in
21,balance_dirty_pages_ratelimited. If this is observed
21,"and it is a problem, then increase the value of"
21,vm.dirty_ratio to 40 to restore the SUSE Linux Enterprise 11 SP3
21,behavior.
21,It is important to note that the overall I/O throughput is the same
21,regardless of the setting. The only difference is the timing of when
21,the I/O is queued.
21,This is an example of using dd to asynchronously
21,write 30% of memory to disk which would happen to be affected by the
21,change in vm.dirty_ratio:
21,# MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`
21,# sysctl vm.dirty_ratio=40
21,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
21,"2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s"
21,# sysctl vm.dirty_ratio=20
21,dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
21,"2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s"
21,Note that the parameter affects the time it takes for the command to
21,complete and the apparent write speed of the device. With
21,"dirty_ratio=40, more of the data is cached and"
21,written to disk in the background by the kernel. It is very important
21,to note that the speed of I/O is identical in both cases. To
21,"demonstrate, this is the result when dd synchronizes"
21,the data before exiting:
21,# sysctl vm.dirty_ratio=40
21,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
21,"2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s"
21,# sysctl vm.dirty_ratio=20
21,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
21,"2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s"
21,Note that dirty_ratio had almost no impact here and
21,"is within the natural variability of a command. Hence,"
21,dirty_ratio does not directly impact I/O performance
21,but it may affect the apparent performance of a workload that writes
21,data asynchronously without synchronizing.
21,15.3.4 Readahead parameters #  /sys/block/<bdev>/queue/read_ahead_kb
21,"If one or more processes are sequentially reading a file, the kernel"
21,reads some data in advance (ahead) to reduce the amount of
21,time that processes need to wait for data to be available. The actual
21,"amount of data being read in advance is computed dynamically, based"
21,"on how much ""sequential"" the I/O seems to be. This parameter sets the"
21,maximum amount of data that the kernel reads ahead for a single file.
21,If you observe that large sequential reads from a file are not fast
21,"enough, you can try increasing this value. Increasing it too far may"
21,result in readahead thrashing where pagecache used for readahead is
21,"reclaimed before it can be used, or slowdowns because of a large"
21,amount of useless I/O. The default value is 512
21,(KB).
21,15.3.5 Transparent HugePage parameters #
21,Transparent HugePages (THP) provide a way to dynamically allocate huge
21,pages either on‑demand by the process or deferring the allocation
21,until later via the khugepaged kernel thread. This
21,method is distinct from the use of hugetlbfs to
21,manually manage their allocation and use. Workloads with contiguous memory
21,access patterns can benefit greatly from THP. A 1000-fold decrease in page
21,faults can be observed when running synthetic workloads with contiguous
21,memory access patterns.
21,There are cases when THP may be undesirable. Workloads with sparse memory
21,access patterns can perform poorly with THP due to excessive memory
21,"usage. For example, 2 MB of memory may be used at fault time instead of 4"
21,KB for each fault and ultimately lead to premature page reclaim.
21,"On releases older than SUSE Linux Enterprise 12 SP2, it was"
21,possible for an application to stall for long periods of time trying to
21,allocate a THP which frequently led to a recommendation of disabling
21,THP. Such recommendations should be re-evaluated for SUSE Linux Enterprise 12 SP3 and
21,later releases.
21,The behavior of THP may be configured via the
21,transparent_hugepage= kernel parameter or via
21,"sysfs. For example, it may be disabled by adding the kernel parameter"
21,"transparent_hugepage=never, rebuilding your grub2"
21,"configuration, and rebooting. Verify if THP is disabled with:"
21,# cat /sys/kernel/mm/transparent_hugepage/enabled
21,always madvise [never]
21,"If disabled, the value never is shown"
21,in square brackets like in the example above. A value of
21,always will always try and use THP at fault
21,time but defer to khugepaged if the allocation
21,fails. A value of madvise will only allocate THP
21,for address spaces explicitly specified by an application.
21,/sys/kernel/mm/transparent_hugepage/defrag
21,This parameter controls how much effort an application commits when
21,allocating a THP. A value of always is the default
21,for SUSE Linux Enterprise 12 SP1 and earlier releases
21,"that supported THP. If a THP is not available, the application tries to defragment memory."
21,It potentially incurs large stalls in an application if the memory is fragmented and a THP
21,is not available.
21,A value of madvise means that THP allocation
21,requests will only defragment if the application explicitly requests
21,it. This is the default for SUSE Linux Enterprise 12
21,SP2 and later
21,releases.
21,"defer is only available on SUSE Linux Enterprise 12 SP2 and later releases. If a THP is not available, the"
21,application will fall back to using small pages if a THP is not
21,available. It will wake the kswapd and
21,kcompactd kernel threads to defragment memory in
21,the background and a THP will be allocated later by
21,khugepaged.
21,The final option never will use small pages if
21,a THP is unavailable but no other action will take place.
21,15.3.6 khugepaged parameters #
21,khugepaged will be automatically started when
21,transparent_hugepage is set to
21,"always or madvise, and it will be"
21,automatically shut down if it is set to never. Normally
21,this runs at low frequency but the behavior can be tuned.
21,/sys/kernel/mm/transparent_hugepage/khugepaged/defrag
21,A value of 0 will disable khugepaged even though
21,THP may still be used at fault time. This may be important for
21,latency-sensitive applications that benefit from THP but cannot
21,tolerate a stall if khugepaged tries to update an
21,application memory usage.
21,/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan
21,This parameter controls how many pages are scanned by
21,khugepaged in a single pass. A scan identifies
21,small pages that can be reallocated as THP. Increasing this value
21,will allocate THP in the background faster at the cost of CPU
21,usage.
21,/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs
21,khugepaged sleeps for a short interval specified
21,by this parameter after each pass to limit how much CPU usage is
21,used. Reducing this value will allocate THP in the background faster
21,at the cost of CPU usage. A value of 0 will force continual scanning.
21,/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs
21,This parameter controls how long khugepaged will
21,sleep in the event it fails to allocate a THP in the background waiting
21,for kswapd and kcompactd to
21,take action.
21,The remaining parameters for khugepaged are rarely
21,useful for performance tuning but are fully documented in
21,/usr/src/linux/Documentation/vm/transhuge.txt
21,15.3.7 Further VM parameters #
21,"For the complete list of the VM tunable parameters, see"
21,/usr/src/linux/Documentation/sysctl/vm.txt
21,(available after having installed the
21,kernel-source package).
21,15.4 Monitoring VM behavior #
21,Some simple tools that can help monitor VM behavior:
21,vmstat: This tool gives a good overview of what the VM is doing. See
21,"Section 2.1.1, “vmstat” for details."
21,/proc/meminfo: This file gives a detailed
21,breakdown of where memory is being used. See
21,"Section 2.4.2, “Detailed memory usage: /proc/meminfo” for details."
21,slabtop: This tool provides detailed information
21,"about kernel slab memory usage. buffer_head, dentry, inode_cache,"
21,"ext3_inode_cache, etc. are the major caches. This command is available"
21,with the package procps.
21,/proc/vmstat: This file gives a detailed breakdown of
21,internal VM behavior. The information contained within is implementation
21,specific and may not always be available. Some information is duplicated in
21,/proc/meminfo and other information can be presented
21,"in a friendly fashion by utilities. For maximum utility, this file needs to"
21,be monitored over time to observe rates of change. The most important
21,pieces of information that are hard to derive from other sources are as
21,follows:
21,"pgscan_kswapd_*, pgsteal_kswapd_*"
21,These report respectively the number of pages scanned and reclaimed
21,by kswapd since the system started. The ratio
21,between these values can be interpreted as the reclaim efficiency
21,with a low efficiency implying that the system is struggling to
21,reclaim memory and may be thrashing. Light activity here is
21,generally not something to be concerned with.
21,"pgscan_direct_*, pgsteal_direct_*"
21,These report respectively the number of pages scanned and
21,reclaimed by an application directly. This is correlated with
21,increases in the allocstall counter. This is
21,more serious than kswapd activity as these
21,events indicate that processes are stalling. Heavy activity
21,here combined with kswapd and high rates of
21,"pgpgin, pgpout and/or high"
21,rates of pswapin or pswpout
21,are signs that a system is thrashing heavily.
21,More detailed information can be obtained using tracepoints.
21,"thp_fault_alloc, thp_fault_fallback"
21,These counters correspond to how many THPs were allocated directly
21,by an application and how many times a THP was not available and
21,small pages were used. Generally a high fallback rate is harmless
21,unless the application is very sensitive to TLB pressure.
21,"thp_collapse_alloc, thp_collapse_alloc_failed"
21,These counters correspond to how many THPs were allocated by
21,khugepaged and how many times a THP was not
21,available and small pages were used. A high fallback rate implies
21,that the system is fragmented and THPs are not being used even
21,when the memory usage by applications would allow them. It is
21,only a problem for applications that are sensitive to TLB pressure.
21,"compact_*_scanned, compact_stall, compact_fail,"
21,compact_success
21,These counters may increase when THP is enabled and the system is
21,fragmented. compact_stall is incremented when
21,an application stalls allocating THP.
21,The remaining counters
21,"account for pages scanned, the number of defragmentation events"
21,that succeeded or failed.
21,16 Tuning the network #
21,The network subsystem is complex and its tuning highly depends on
21,the system use scenario and on external factors such as software
21,"clients or hardware components (switches, routers, or gateways) in your"
21,network. The Linux kernel aims more at reliability and low latency than
21,"low overhead and high throughput. Other settings can mean less security,"
21,but better performance.
21,16.1 Configurable kernel socket buffers #
21,Networking is largely based on the TCP/IP protocol and a socket interface
21,"for communication; for more information about TCP/IP, see"
21,"Book “Administration Guide”, Chapter 19 “Basic networking”. The Linux kernel handles data it receives"
21,or sends via the socket interface in socket buffers. These kernel socket
21,buffers are tunable.
21,Important: TCP autotuning
21,Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer
21,size exists. This means that manual tuning usually will not
21,improve networking performance considerably. It is often the best not to
21,"touch the following variables, or, at least, to check the outcome of"
21,tuning efforts carefully.
21,"If you update from an older kernel, it is recommended to remove manual"
21,TCP tunings in favor of the autotuning feature.
21,The special files in the /proc file system can
21,modify the size and behavior of kernel socket buffers; for general
21,"information about the /proc file system, see"
21,"Section 2.6, “The /proc file system”. Find networking related files in:"
21,/proc/sys/net/core
21,/proc/sys/net/ipv4
21,/proc/sys/net/ipv6
21,General net variables are explained in the
21,kernel documentation
21,(linux/Documentation/sysctl/net.txt). Special
21,ipv4 variables are explained in
21,linux/Documentation/networking/ip-sysctl.txt and
21,linux/Documentation/networking/ipvs-sysctl.txt.
21,"In the /proc file system, for example, it is"
21,possible to either set the Maximum Socket Receive Buffer and Maximum
21,"Socket Send Buffer for all protocols, or both these options for the TCP"
21,protocol only (in ipv4) and thus overriding the
21,setting for all protocols (in core).
21,/proc/sys/net/ipv4/tcp_moderate_rcvbuf
21,If /proc/sys/net/ipv4/tcp_moderate_rcvbuf is set
21,"to 1, autotuning is active and buffer size is"
21,adjusted dynamically.
21,/proc/sys/net/ipv4/tcp_rmem
21,"The three values setting the minimum, initial, and maximum size of the"
21,Memory Receive Buffer per connection. They define the actual memory
21,"usage, not only TCP window size."
21,/proc/sys/net/ipv4/tcp_wmem
21,"The same as tcp_rmem, but for Memory Send Buffer"
21,per connection.
21,/proc/sys/net/core/rmem_max
21,Set to limit the maximum receive buffer size that applications can
21,request.
21,/proc/sys/net/core/wmem_max
21,Set to limit the maximum send buffer size that applications can
21,request.
21,Via /proc it is possible to disable TCP features
21,that you do not need (all TCP features are switched on by default). For
21,"example, check the following files:"
21,/proc/sys/net/ipv4/tcp_timestamps
21,TCP time stamps are defined in RFC1323.
21,/proc/sys/net/ipv4/tcp_window_scaling
21,TCP window scaling is also defined in RFC1323.
21,/proc/sys/net/ipv4/tcp_sack
21,Select acknowledgments (SACKS).
21,Use sysctl to read or write variables of the
21,/proc file system. sysctl is
21,preferable to cat (for reading) and
21,"echo (for writing), because it also reads settings"
21,"from /etc/sysctl.conf and, thus, those settings"
21,survive reboots reliably. With sysctl you can read all
21,variables and their values easily; as root use the following
21,command to list TCP related settings:
21,> sudo sysctl -a | grep tcpNote: Side effects of tuning network variables
21,Tuning network variables can affect other system resources such as CPU
21,or memory use.
21,16.2 Detecting network bottlenecks and analyzing network traffic #
21,"Before starting with network tuning, it is important to isolate network"
21,bottlenecks and network traffic patterns. There are some tools that can
21,help you with detecting those bottlenecks.
21,The following tools can help analyzing your network traffic:
21,"netstat, tcpdump, and"
21,wireshark. Wireshark is a network traffic analyzer.
21,16.3 Netfilter #
21,The Linux firewall and masquerading features are provided by the
21,Netfilter kernel modules. This is a highly configurable rule based
21,"framework. If a rule matches a packet, Netfilter accepts or denies it or"
21,takes special action (“target”) as defined by rules such as
21,address translation.
21,There are quite a lot of properties Netfilter can take into account.
21,"Thus, the more rules are defined, the longer packet processing may last."
21,"Also advanced connection tracking could be rather expensive and, thus,"
21,slowing down overall networking.
21,"When the kernel queue becomes full, all new packets are dropped, causing"
21,existing connections to fail. The 'fail-open' feature allows a user to
21,temporarily disable the packet inspection and maintain the connectivity
21,"under heavy network traffic. For reference, see https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/."
21,"For more information, see the home page of the Netfilter and iptables"
21,"project, http://www.netfilter.org"
21,16.4 Improving the network performance with receive packet steering (RPS) #
21,Modern network interface devices can move so many packets that the host
21,can become the limiting factor for achieving maximum performance.
21,"To keep up, the system must be able to distribute the work across"
21,multiple CPU cores.
21,Some modern network interfaces can help distribute the work to multiple
21,CPU cores through the implementation of multiple transmission and
21,"multiple receive queues in hardware. However, others are only equipped"
21,with a single queue and the driver must deal with all incoming packets in
21,"a single, serialized stream. To work around this issue, the operating"
21,"system must ""parallelize"" the stream to distribute the work across"
21,multiple CPUs. On SUSE Linux Enterprise Server this is done via Receive Packet
21,Steering (RPS). RPS can also be used in virtual environments.
21,RPS creates a unique hash for each data stream using IP addresses and
21,port numbers. The use of this hash ensures that packets for the same data
21,"stream are sent to the same CPU, which helps to increase performance."
21,RPS is configured per network device receive queue and interface. The
21,configuration file names match the following scheme:
21,/sys/class/net/<device>/queues/<rx-queue>/rps_cpus
21,<device> stands for the network
21,"device, such as eth0, eth1."
21,"<rx-queue> stands for the receive queue,"
21,"such as rx-0, rx-1."
21,"If the network interface hardware only supports a single receive queue,"
21,only rx-0 will exist. If it supports multiple receive
21,"queues, there will be an rx-N directory for"
21,each receive queue.
21,These configuration files contain a comma-delimited list of CPU bitmaps.
21,"By default, all bits are set to 0. With this setting"
21,RPS is disabled and therefore the CPU that handles the interrupt will
21,also process the packet queue.
21,To enable RPS and enable specific CPUs to process packets for the receive
21,"queue of the interface, set the value of their positions in the bitmap to"
21,"1. For example, to enable CPUs 0-3 to process packets"
21,"for the first receive queue for eth0, set the bit positions"
21,0-3 to 1 in binary: 00001111. This representation then
21,needs to be converted to hex—which results in F in
21,this case. Set this hex value with the following command:
21,"> sudo echo ""f"" > /sys/class/net/eth0/queues/rx-0/rps_cpus"
21,If you wanted to enable CPUs 8-15:
21,1111 1111 0000 0000 (binary)
21,0 (decimal)
21,0 (hex)
21,The command to set the hex value of ff00 would be:
21,"> sudo echo ""ff00"" > /sys/class/net/eth0/queues/rx-0/rps_cpus"
21,"On NUMA machines, best performance can be achieved by configuring RPS to"
21,use the CPUs on the same NUMA node as the interrupt for the interface's
21,receive queue.
21,"On non-NUMA machines, all CPUs can be used. If the interrupt rate is very"
21,"high, excluding the CPU handling the network interface can boost"
21,performance. The CPU being used for the network interface can be
21,determined from /proc/interrupts. For example:
21,> sudo cat /proc/interrupts
21,CPU0
21,CPU1
21,CPU2
21,CPU3
21,...
21,51:
21,113915241
21,Phys-fasteoi
21,eth0
21,...
21,"In this case, CPU 0 is the only CPU processing"
21,"interrupts for eth0, since only"
21,CPU0 contains a non-zero value.
21,"On x86 and AMD64/Intel 64 platforms, irqbalance can be used"
21,to distribute hardware interrupts across CPUs. See man 1
21,irqbalance for more details.
21,17 Tuning SUSE Linux Enterprise for SAP #
21,This chapter presents information about preparing and tuning SUSE Linux Enterprise Server
21,to work optimally with SAP applications with sapconf. sapconf is for
21,SUSE Linux Enterprise systems that install SAP applications. Customers who have
21,SUSE Linux Enterprise Server for SAP Applications should use saptune.
21,Note: The sapconf command has been removed
21,"In SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 11 and 12, the sapconf command"
21,was included in the package with the same name.
21,"For SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15, this has been changed:"
21,The command sapconf has been removed from the
21,sapconf package. The package contains a systemd service
21,"only. There is no sapconf command line tool anymore, no sapconf/tuned profiles,"
21,and no tuned.
21,17.1 Tuning SLE Systems with sapconf 5 #
21,The package sapconf is available in SUSE Linux Enterprise Server and SUSE Linux Enterprise Server for SAP Applications.
21,It sets recommended parameters for the following types of SAP applications:
21,"SAP NetWeaver, SAP HANA, and SAP HANA-based applications."
21,Overview of sapconf5 in SUSE® Linux Enterprise Server 12 #  sapconf5 (without tuned)sapconf-netweaver (sapconf profile as a replacement for tuned profile)sapconf-hana (sapconf profile as a replacement for tuned profile)sapconf-bobj (sapconf profile as a replacement for tuned profile)sapconf-ase (sapconf profile as a replacement for tuned profile)Overview of sapconf5 in SUSE® Linux Enterprise Server 15 #  sapconf5 (without tuned)no profiles anymore
21,"Note that if you previously made changes to the system tuning, those"
21,changes may be overwritten by sapconf.
21,sapconf 5 ships a systemd service which applies the tuning and ensures that
21,related services are running.
21,"To use sapconf, make sure"
21,that the package sapconf is installed on your system.
21,Note: No profiles in SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15 SP3
21,"In SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15, sapconf no longer supports profiles."
21,17.1.1 Verifying sapconf setup #
21,"With sapconf 5.0.2 and up, the check tool sapconf_check is available,"
21,which verifies the correct setup of sapconf. For example:
21,# sapconf_check
21,This is sapconf_check v1.0.
21,It verifies if sapconf is set up correctly and will give advice to do so.
21,Please keep in mind:
21,"{{ - This tool does not check, if the tuning itself works correctly.}}"
21,{{ - Follow the hints from top to down to minimize side effects.}}
21,Checking sapconf
21,================
21,[ OK ] sapconf package has version 5.0.2
21,[ OK ] saptune.service is inactive
21,[ OK ] saptune.service is disabled
21,"[WARN] tuned.service is enabled/active with profile 'virtual-guest -> Sapconf does not require tuned! Run 'systemctl stop tuned.service', if not needed otherwise."
21,[FAIL] sapconf.service is inactive -> Run 'systemctl start sapconf.service' to activate the tuning now.
21,[FAIL] sapconf.service is disabled -> Run 'systemctl enable sapconf.service' to activate sapconf at boot.1 warning(s) have been found.
21,2 error(s) have been found.
21,Sapconf will not work properly!
21,"If sapconf_check finds problems, it will give hints"
21,on how to resolve the issue.
21,The tool will not verify whether the system has been tuned correctly. It
21,only checks that sapconf
21,is set up correctly and has been started.
21,17.1.2 Enabling and disabling sapconf and viewing its status #
21,"After the installation of sapconf, the sapconf service is enabled."
21,You can inspect or change the status of sapconf as described in the
21,following:
21,To see the status of the service
21,sapconf:
21,# systemctl status sapconf
21,The service should be displayed as active (exited).
21,To start the service
21,sapconf:
21,# systemctl start sapconf
21,"Should sapconf be disabled,"
21,enable and start it with:
21,# systemctl enable --now sapconf
21,To stop the service
21,sapconf:
21,# systemctl stop sapconf
21,This command will disable the vast majority of optimizations immediately. The only
21,exceptions from this rule are options that require a system reboot to enable/disable.
21,"To disable sapconf, use:"
21,# systemctl disable sapconf
21,If you have not specifically enabled any of the services that sapconf
21,"depends on yourself, this will also disable most tuning parameters and"
21,all services used by sapconf.
21,Tip: Additional services that sapconf relies on
21,In addition to the sapconf service it also relies on the following two services:
21,sysstat which collects data on
21,system activity.
21,uuidd which generates time-based
21,UUIDs that are guaranteed to be unique even in settings where many
21,processor cores are involved. This is necessary for SAP applications.
21,17.1.3 Configuring sapconf5 #
21,"In general, the default configuration of sapconf already uses the"
21,"parameter values recommended by SAP. However, if you have special"
21,"needs, you can configure the tool to better suit those."
21,All parameters of sapconf can be found in the file
21,/etc/sysconfig/sapconf.
21,The file can be edited directly. All parameters in this file are
21,"explained by means of comments and references to SAP Notes, which can"
21,be viewed at https://launchpad.support.sap.com/.
21,"When sapconf is updated, all customized parameters from this file will"
21,"be preserved as much as possible. However, sometimes parameters cannot"
21,"be transferred cleanly to the new configuration file. Therefore, after"
21,updating it is advisable to check the difference between the previous
21,"custom configuration, which during the update is moved to"
21,"/etc/sysconfig/sapconf.rpmsave,"
21,and the new version at /etc/sysconfig/sapconf.
21,Log messages related to this file are written to
21,/var/log/sapconf.log.
21,"When editing either of these files, you will find that some values are"
21,commented by means of a # character at the beginning of
21,"the line. This means that while the parameter is relevant for tuning, there"
21,is no suitable default for it.
21,"Conversely, you can add # characters to the beginning of"
21,"the line to comment specific parameters. However, you should avoid this"
21,"practice, as it can lead to sapconf not properly applying the profile."
21,"To apply edited configuration, restart sapconf:"
21,# systemctl restart sapconf
21,Confirming that a certain parameter value was applied correctly works
21,"differently for different parameters. Hence, the following serves as an"
21,example only:
21,Example 17.1: Checking Parameters #
21,To confirm that the setting for TCP_SLOW_START was
21,"applied, do the following:"
21,View the log file of sapconf to see whether it applied the value.
21,"Within /var/log/sapconf.log, check for a line"
21,containing this text:
21,Change net.ipv4.tcp_slow_start_after_idle from 1 to 0
21,"Alternatively, the parameter may have already been set correctly"
21,"before sapconf was started. In this case, sapconf will not change"
21,its value:
21,Leaving net.ipv4.tcp_slow_start_after_idle unchanged at 1
21,The underlying option behind TCP_SLOW_START can be
21,manually configured at
21,/proc/sys/net.ipv4.tcp_slow_start_after_idle.
21,"To check its actual current value, use:"
21,# sysctl net.ipv4.tcp_slow_start_after_idle17.1.4 Removing sapconf #
21,"To remove sapconf from a system, uninstall its package with:"
21,# zypper rm sapconf
21,"Note that when doing this, dependencies of sapconf will remain installed."
21,"However, the service sysstat will"
21,"go into a disabled state. If it is still relevant to you, make sure to"
21,enable it again.
21,17.1.5 For more information #
21,The following man pages provide additional information about sapconf:
21,Detailed description of all tuning parameters set by sapconf:
21,man 5 sapconf
21,Information about configuring and customizing the sapconf profile:
21,man 7 sapconf
21,Also see the blog series detailing the updated version of sapconf at
21,https://www.suse.com/c/a-new-sapconf-is-available/.
21,17.1.6 Using tuned together with
21,sapconf #
21,"With version 5, sapconf does not rely on tuned anymore. This means both tools"
21,can be used independently.
21,sapconf will print a warning in its log if the tuned service
21,is started.
21,Important: Using tuned and sapconf together
21,"If you are going to use tuned and sapconf simultaneously,"
21,"be very careful, that both tools do not configure the same system parameters."
21,"Part VI Handling system dumps #  18 Tracing toolsSUSE Linux Enterprise Server comes with several tools that help you obtain useful information about your system. You can use the information for various purposes, for example, to debug and find problems in your program, to discover places causing performance drops, or to trace a running process to f…19 Kexec and Kdump"
21,Kexec is a tool to boot to another kernel from the currently running one.
21,You can perform faster system reboots without any hardware initialization.
21,You can also prepare the system to boot to another kernel if the system
21,crashes.
21,"20 Using systemd-coredump to debug application crashessystemd-coredump collects and displays core dumps, for analyzing application crashes. The core dump contains an image of the process's memory at the time of termination. When a process crashes (or all processes belonging to an application), its default is to log the core dump to the systemd journal,…18 Tracing tools #"
21,SUSE Linux Enterprise Server comes with several tools that help you obtain useful
21,information about your system. You can use the information for various
21,"purposes, for example, to debug and find problems in your program, to"
21,"discover places causing performance drops, or to trace a running process to"
21,find out what system resources it uses. Most of the
21,"tools are part of the installation media. In some cases, they need to be"
21,"installed from the SUSE Software Development Kit, which is a separate download."
21,Note: Tracing and impact on performance
21,"While a running process is being monitored for system or library calls,"
21,the performance of the process is heavily reduced. You are advised to use
21,tracing tools only for the time you need to collect the data.
21,18.1 Tracing system calls with strace #
21,The strace command traces system calls of a process
21,and signals received by the process. strace can either
21,"run a new command and trace its system calls, or you can attach"
21,strace to an already running command. Each line of the
21,"command's output contains the system call name, followed by its arguments"
21,in parentheses and its return value.
21,"To run a new command and start tracing its system calls, enter the"
21,"command to be monitored as you normally do, and add"
21,strace at the beginning of the command line:
21,> strace ls
21,"execve(""/bin/ls"", [""ls""], [/* 52 vars */]) = 0"
21,brk(0)
21,= 0x618000
21,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
21,= 0x7f9848667000
21,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
21,= 0x7f9848666000
21,"access(""/etc/ld.so.preload"", R_OK)"
21,= -1 ENOENT \
21,(No such file or directory)
21,"open(""/etc/ld.so.cache"", O_RDONLY)"
21,= 3
21,"fstat(3, {st_mode=S_IFREG|0644, st_size=200411, ...}) = 0"
21,"mmap(NULL, 200411, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f9848635000"
21,close(3)
21,= 0
21,"open(""/lib64/librt.so.1"", O_RDONLY)"
21,= 3
21,[...]
21,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
21,= 0x7fd780f79000
21,"write(1, ""Desktop\nDocuments\nbin\ninst-sys\n"", 31Desktop"
21,Documents
21,bin
21,inst-sys
21,) = 31
21,close(1)
21,= 0
21,"munmap(0x7fd780f79000, 4096)"
21,= 0
21,close(2)
21,= 0
21,exit_group(0)
21,= ?
21,"To attach strace to an already running process, you"
21,need to specify the -p with the process ID
21,(PID) of the process that you want to monitor:
21,> strace -p `pidof cron`
21,Process 1261 attached
21,restart_syscall(<... resuming interrupted call ...>) = 0
21,"stat(""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=2309, ...}) = 0"
21,"select(5, [4], NULL, NULL, {0, 0})"
21,= 0 (Timeout)
21,"socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 5"
21,"connect(5, {sa_family=AF_LOCAL, sun_path=""/var/run/nscd/socket""}, 110) = 0"
21,"sendto(5, ""\2\0\0\0\0\0\0\0\5\0\0\0root\0"", 17, MSG_NOSIGNAL, NULL, 0) = 17"
21,"poll([{fd=5, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 1 ([{fd=5, revents=POLLIN|POLLHUP}])"
21,"read(5, ""\2\0\0\0\1\0\0\0\5\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\6\0\0\0""..., 36) = 36"
21,"read(5, ""root\0x\0root\0/root\0/bin/bash\0"", 28) = 28"
21,close(5)
21,= 0
21,"rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0"
21,"rt_sigaction(SIGCHLD, NULL, {0x7f772b9ea890, [], SA_RESTORER|SA_RESTART, 0x7f772adf7880}, 8) = 0"
21,"rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0"
21,"nanosleep({60, 0}, 0x7fff87d8c580)"
21,= 0
21,"stat(""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=2309, ...}) = 0"
21,"select(5, [4], NULL, NULL, {0, 0})"
21,= 0 (Timeout)
21,"socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 5"
21,"connect(5, {sa_family=AF_LOCAL, sun_path=""/var/run/nscd/socket""}, 110) = 0"
21,"sendto(5, ""\2\0\0\0\0\0\0\0\5\0\0\0root\0"", 17, MSG_NOSIGNAL, NULL, 0) = 17"
21,"poll([{fd=5, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 1 ([{fd=5, revents=POLLIN|POLLHUP}])"
21,"read(5, ""\2\0\0\0\1\0\0\0\5\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\6\0\0\0""..., 36) = 36"
21,"read(5, ""root\0x\0root\0/root\0/bin/bash\0"", 28) = 28"
21,close(5)
21,[...]
21,The -e option understands several sub-options and
21,"arguments. For example, to trace all attempts to open or write to a"
21,"particular file, use the following:"
21,"> strace -e trace=open,write ls ~"
21,"open(""/etc/ld.so.cache"", O_RDONLY)"
21,= 3
21,"open(""/lib64/librt.so.1"", O_RDONLY)"
21,= 3
21,"open(""/lib64/libselinux.so.1"", O_RDONLY) = 3"
21,"open(""/lib64/libacl.so.1"", O_RDONLY)"
21,= 3
21,"open(""/lib64/libc.so.6"", O_RDONLY)"
21,= 3
21,"open(""/lib64/libpthread.so.0"", O_RDONLY) = 3"
21,[...]
21,"open(""/usr/lib/locale/cs_CZ.utf8/LC_CTYPE"", O_RDONLY) = 3"
21,"open(""."", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3"
21,"write(1, ""addressbook.db.bak\nbin\ncxoffice\n""..., 311) = 311"
21,"To trace only network related system calls, use -e"
21,trace=network:
21,> strace -e trace=network -p 26520
21,Process 26520 attached - interrupt to quit
21,"socket(PF_NETLINK, SOCK_RAW, 0)"
21,= 50
21,"bind(50, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0"
21,"getsockname(50, {sa_family=AF_NETLINK, pid=26520, groups=00000000}, \"
21,[12]) = 0
21,"sendto(50, ""\24\0\0\0\26\0\1\3~p\315K\0\0\0\0\0\0\0\0"", 20, 0,"
21,"{sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 20"
21,[...]
21,The -c calculates the time the kernel spent on each
21,system call:
21,> strace -c find /etc -name xorg.conf
21,/etc/X11/xorg.conf
21,% time
21,seconds
21,usecs/call
21,calls
21,errors syscall
21,------ ----------- ----------- --------- --------- ----------------
21,32.38
21,0.000181
21,181
21,execve
21,22.00
21,0.000123
21,576
21,getdents64
21,19.50
21,0.000109
21,917
21,31 open
21,19.14
21,0.000107
21,888
21,close
21,4.11
21,0.000023
21,mprotect
21,0.00
21,0.000000
21,write
21,[...]
21,0.00
21,0.000000
21,getrlimit
21,0.00
21,0.000000
21,arch_prctl
21,0.00
21,0.000000
21,1 futex
21,0.00
21,0.000000
21,set_tid_address
21,0.00
21,0.000000
21,fadvise64
21,0.00
21,0.000000
21,set_robust_list
21,------ ----------- ----------- --------- --------- ----------------
21,100.00
21,0.000559
21,3633
21,33 total
21,"To trace all child processes of a process, use -f:"
21,> strace -f systemctl status apache2.service
21,"execve(""/usr/bin/systemctl"", [""systemctl"", ""status"", ""apache2.service""], \"
21,0x7ffea44a3318 /* 56 vars */) = 0
21,brk(NULL)
21,= 0x5560f664a000
21,[...]
21,"mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f98c58a5000"
21,"mmap(NULL, 4420544, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f98c524a000"
21,"mprotect(0x7f98c53f4000, 2097152, PROT_NONE) = 0"
21,[...]
21,[pid
21,"9130] read(0, ""\342\227\217 apache2.service - The Apache""..., 8192) = 165"
21,[pid
21,"9130] read(0, """", 8027)"
21,= 0
21,"● apache2.service - The Apache Webserver227\217 apache2.service - Th""..., 193"
21,Loaded: loaded (/usr/lib/systemd/system/apache2.service; disabled; vendor preset: disabled)
21,Active: inactive (dead)
21,) = 193
21,[pid
21,"9130] ioctl(3, SNDCTL_TMR_STOP or TCSETSW, {B38400 opost isig icanon echo ...}) = 0"
21,[pid
21,9130] exit_group(0)
21,= ?
21,[pid
21,9130] +++ exited with 0 +++
21,"<... waitid resumed>{si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=9130, \"
21,"si_uid=0, si_status=0, si_utime=0, si_stime=0}, WEXITED, NULL) = 0"
21,"--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=9130, si_uid=0, \"
21,"si_status=0, si_utime=0, si_stime=0} ---"
21,exit_group(3)
21,= ?
21,+++ exited with 3 +++
21,If you need to analyze the output of strace and the
21,output messages are too long to be inspected directly in the console
21,"window, use -o. In that case, unnecessary messages, such"
21,"as information about attaching and detaching processes, are suppressed."
21,You can also suppress these messages (normally printed on the standard
21,output) with -q. To add time stamps at the beginning of each line
21,"with a system call, use -t:"
21,> strace -t -o strace_sleep.txt sleep 1; more strace_sleep.txt
21,"08:44:06 execve(""/bin/sleep"", [""sleep"", ""1""], [/* 81 vars */]) = 0"
21,08:44:06 brk(0)
21,= 0x606000
21,"08:44:06 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, \"
21,"-1, 0) = 0x7f8e78cc5000"
21,[...]
21,08:44:06 close(3)
21,= 0
21,"08:44:06 nanosleep({1, 0}, NULL)"
21,= 0
21,08:44:07 close(1)
21,= 0
21,08:44:07 close(2)
21,= 0
21,08:44:07 exit_group(0)
21,= ?
21,The behavior and output format of strace can be largely controlled. For
21,"more information, see the relevant manual page (man 1 strace)."
21,18.2 Tracing library calls with ltrace #
21,ltrace traces dynamic library calls of a process. It
21,"is used in a similar way to strace, and most of their"
21,"parameters have a very similar or identical meaning. By default,"
21,ltrace uses /etc/ltrace.conf or
21,"~/.ltrace.conf configuration files. You can,"
21,"however, specify an alternative one with the -F"
21,CONFIG_FILE option.
21,"In addition to library calls, ltrace with the"
21,-S option can trace system calls as well:
21,> ltrace -S -o ltrace_find.txt find /etc -name \
21,xorg.conf; more ltrace_find.txt
21,SYS_brk(NULL)
21,= 0x00628000
21,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
21,= 0x7f1327ea1000
21,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
21,= 0x7f1327ea0000
21,[...]
21,"fnmatch(""xorg.conf"", ""xorg.conf"", 0)"
21,= 0
21,free(0x0062db80)
21,= <void>
21,__errno_location()
21,= 0x7f1327e5d698
21,"__ctype_get_mb_cur_max(0x7fff25227af0, 8192, 0x62e020, -1, 0) = 6"
21,"__ctype_get_mb_cur_max(0x7fff25227af0, 18, 0x7f1327e5d6f0, 0x7fff25227af0,"
21,0x62e031) = 6
21,"__fprintf_chk(0x7f1327821780, 1, 0x420cf7, 0x7fff25227af0, 0x62e031"
21,<unfinished ...>
21,"SYS_fstat(1, 0x7fff25227230)"
21,= 0
21,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
21,= 0x7f1327e72000
21,"SYS_write(1, ""/etc/X11/xorg.conf\n"", 19)"
21,= 19
21,[...]
21,You can change the type of traced events with the -e
21,option. The following example prints library calls related to
21,fnmatch and strlen
21,functions:
21,"> ltrace -e fnmatch,strlen find /etc -name xorg.conf"
21,[...]
21,"fnmatch(""xorg.conf"", ""xorg.conf"", 0)"
21,= 0
21,"strlen(""Xresources"")"
21,= 10
21,"strlen(""Xresources"")"
21,= 10
21,"strlen(""Xresources"")"
21,= 10
21,"fnmatch(""xorg.conf"", ""Xresources"", 0)"
21,= 1
21,"strlen(""xorg.conf.install"")"
21,= 17
21,[...]
21,"To display only the symbols included in a specific library, use"
21,-l /path/to/library:
21,> ltrace -l /lib64/librt.so.1 sleep 1
21,"clock_gettime(1, 0x7fff4b5c34d0, 0, 0, 0)"
21,= 0
21,"clock_gettime(1, 0x7fff4b5c34c0, 0xffffffffff600180, -1, 0) = 0"
21,+++ exited (status 0) +++
21,You can make the output more readable by indenting each nested call by
21,the specified number of space with the -n
21,NUM_OF_SPACES.
21,18.3 Debugging and profiling with Valgrind #
21,Valgrind is a set of tools to debug and profile your programs so that
21,they can run both faster and with fewer errors. Valgrind can detect problems
21,"related to memory management and threading, or can also serve as a"
21,framework for building new debugging tools. It is well known that this
21,"tool can incur high overhead, causing, for example, higher runtimes or"
21,changing the normal program behavior under concurrent workloads based on timing.
21,18.3.1 Installation #
21,Valgrind is not shipped with standard SUSE Linux Enterprise Server distribution. To
21,"install it on your system, you need to obtain SUSE Software Development Kit, and either"
21,install it and run
21,zypper install
21,VALGRIND
21,"or browse through the SUSE Software Development Kit directory tree, locate the Valgrind"
21,package and install it with
21,rpm -i
21,valgrind-VERSION_ARCHITECTURE.rpm
21,The SDK is a module for SUSE Linux Enterprise and is available via an online channel from
21,the SUSE Customer Center.
21,Alternatively download it from http://download.suse.com/. (Search for SUSE Linux Enterprise
21,"Software Development Kit). Refer to Book “Deployment Guide”, Chapter 22 “Installing modules, extensions, and third party add-on products” for details."
21,18.3.2 Supported architectures #
21,SUSE Linux Enterprise Server supports Valgrind on the following architectures:
21,AMD64/Intel 64
21,POWER
21,IBM Z
21,18.3.3 General information #
21,The main advantage of Valgrind is that it works with existing compiled
21,executables. You do not need to recompile or modify your programs to
21,use it. Run Valgrind like this:
21,valgrind VALGRIND_OPTIONS
21,your-prog YOUR-PROGRAM-OPTIONS
21,"Valgrind consists of several tools, and each provides specific"
21,functionality. Information in this section is general and valid
21,regardless of the used tool. The most important configuration option is
21,--tool. This option tells Valgrind which tool to run.
21,"If you omit this option, memcheck is selected"
21,"by default. For example, to run find ~"
21,-name .bashrc with Valgrind's
21,"memcheck tools, enter the following in the"
21,command line:
21,valgrind --tool=memcheck find ~ -name
21,.bashrc
21,A list of standard Valgrind tools with a brief description follows:
21,memcheck
21,Detects memory errors. It helps you tune your programs to behave
21,correctly.
21,cachegrind
21,Profiles cache prediction. It helps you tune your programs to run
21,faster.
21,callgrind
21,Works in a similar way to cachegrind but
21,also gathers additional cache-profiling information.
21,exp-drd
21,Detects thread errors. It helps you tune your multi-threaded programs
21,to behave correctly.
21,helgrind
21,Another thread error detector. Similar to
21,exp-drd but uses different techniques for
21,problem analysis.
21,massif
21,A heap profiler. Heap is an area of memory used for dynamic memory
21,allocation. This tool helps you tune your program to use less memory.
21,lackey
21,An example tool showing instrumentation basics.
21,18.3.4 Default options #
21,Valgrind can read options at start-up. There are three places which
21,Valgrind checks:
21,The file .valgrindrc in the home directory of the
21,user who runs Valgrind.
21,The environment variable $VALGRIND_OPTS
21,The file .valgrindrc in the current directory
21,where Valgrind is run from.
21,"These resources are parsed exactly in this order, while later given"
21,options take precedence over earlier processed options. Options specific
21,to a particular Valgrind tool must be prefixed with the tool name and a
21,"colon. For example, if you want cachegrind to"
21,always write profile data to the
21,"/tmp/cachegrind_PID.log,"
21,add the following line to the .valgrindrc file in
21,your home directory:
21,--cachegrind:cachegrind-out-file=/tmp/cachegrind_%p.log
21,18.3.5 How Valgrind works #
21,Valgrind takes control of your executable before it starts. It reads
21,debugging information from the executable and related shared libraries.
21,"The executable's code is redirected to the selected Valgrind tool, and"
21,the tool adds its own code to handle its debugging. Then the code is
21,handed back to the Valgrind core and the execution continues.
21,"For example, memcheck adds its code, which"
21,"checks every memory access. As a consequence, the program runs much"
21,slower than in the native execution environment.
21,"Valgrind simulates every instruction of your program. Therefore, it not"
21,"only checks the code of your program, but also all related libraries"
21,"(including the C library), libraries used for graphical environment, and"
21,"so on. If you try to detect errors with Valgrind, it also detects errors"
21,"in associated libraries (like C, X11, or Gtk libraries). Because you"
21,"probably do not need these errors, Valgrind can selectively, suppress"
21,these error messages to suppression files. The
21,--gen-suppressions=yes tells Valgrind to report these
21,suppressions which you can copy to a file.
21,You should supply a real executable (machine code) as a Valgrind
21,"argument. If your application is run, for example, from a shell or Perl"
21,"script, you will by mistake get error reports related to"
21,/bin/sh (or /usr/bin/perl). In
21,"such cases, you can use"
21,--trace-children=yes to work
21,"around this issue. However, using the executable itself will avoid any"
21,confusion over this issue.
21,18.3.6 Messages #
21,"During its runtime, Valgrind reports messages with detailed errors and"
21,important events. The following example explains the messages:
21,> valgrind --tool=memcheck find ~ -name .bashrc
21,[...]
21,==6558== Conditional jump or move depends on uninitialised value(s)
21,==6558==
21,at 0x400AE79: _dl_relocate_object (in /lib64/ld-2.11.1.so)
21,==6558==
21,by 0x4003868: dl_main (in /lib64/ld-2.11.1.so)
21,[...]
21,==6558== Conditional jump or move depends on uninitialised value(s)
21,==6558==
21,at 0x400AE82: _dl_relocate_object (in /lib64/ld-2.11.1.so)
21,==6558==
21,by 0x4003868: dl_main (in /lib64/ld-2.11.1.so)
21,[...]
21,==6558== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0)
21,"==6558== malloc/free: in use at exit: 2,228 bytes in 8 blocks."
21,"==6558== malloc/free: 235 allocs, 227 frees, 489,675 bytes allocated."
21,"==6558== For counts of detected errors, rerun with: -v"
21,==6558== searching for pointers to 8 not-freed blocks.
21,"==6558== checked 122,584 bytes."
21,==6558==
21,==6558== LEAK SUMMARY:
21,==6558==
21,definitely lost: 0 bytes in 0 blocks.
21,==6558==
21,possibly lost: 0 bytes in 0 blocks.
21,==6558==
21,"still reachable: 2,228 bytes in 8 blocks."
21,==6558==
21,suppressed: 0 bytes in 0 blocks.
21,==6558== Rerun with --leak-check=full to see details of leaked memory.
21,The ==6558== introduces Valgrind's messages and
21,contains the process ID number (PID). You can easily distinguish
21,"Valgrind's messages from the output of the program itself, and decide"
21,which messages belong to a particular process.
21,"To make Valgrind's messages more detailed, use -v or"
21,even -v -v.
21,You can make Valgrind send its messages to three different places:
21,"By default, Valgrind sends its messages to the file descriptor 2,"
21,which is the standard error output. You can tell Valgrind to send its
21,messages to any other file descriptor with the
21,--log-fd=FILE_DESCRIPTOR_NUMBER
21,option.
21,The second and probably more useful way is to send Valgrind's messages
21,to a file with
21,--log-file=FILENAME. This
21,"option accepts several variables, for example, %p"
21,gets replaced with the PID of the currently profiled process. This way
21,you can send messages to different files based on their PID.
21,%q{env_var} is replaced with the value of the
21,related env_var environment variable.
21,The following example checks for possible memory errors during the
21,"Apache Web server restart, while following children processes and"
21,writing detailed Valgrind's messages to separate files distinguished
21,by the current process PID:
21,> valgrind -v --tool=memcheck --trace-children=yes \
21,--log-file=valgrind_pid_%p.log systemctl restart apache2.service
21,"This process created 52 log files in the testing system, and took 75"
21,seconds instead of the usual 7 seconds needed to run sudo
21,"systemctl restart apache2.service without Valgrind, which is"
21,approximately 10 times more.
21,> ls -1 valgrind_pid_*log
21,valgrind_pid_11780.log
21,valgrind_pid_11782.log
21,valgrind_pid_11783.log
21,[...]
21,valgrind_pid_11860.log
21,valgrind_pid_11862.log
21,valgrind_pid_11863.log
21,You may also prefer to send the Valgrind's messages over the network.
21,You need to specify the aa.bb.cc.dd IP address and
21,port_num port number of the network socket with the
21,--log-socket=AA.BB.CC.DD:PORT_NUM
21,"option. If you omit the port number, 1500 will be used."
21,It is useless to send Valgrind's messages to a network socket if no
21,application is capable of receiving them on the remote machine. That
21,"is why valgrind-listener, a simple listener, is"
21,shipped together with Valgrind. It accepts connections on the
21,specified port and copies everything it receives to the standard
21,output.
21,18.3.7 Error messages #
21,"Valgrind remembers all error messages, and if it detects a new error,"
21,the error is compared against old error messages. This way Valgrind
21,"checks for duplicate error messages. In case of a duplicate error, it is"
21,recorded but no message is shown. This mechanism prevents you from being
21,overwhelmed by millions of duplicate errors.
21,The -v option will add a summary of all reports (sorted
21,by their total count) to the end of the Valgrind's execution output.
21,"Moreover, Valgrind stops collecting errors if it detects either 1000"
21,"different errors, or 10 000 000 errors in total. If you want to suppress"
21,"this limit and wish to see all error messages, use"
21,--error-limit=no.
21,"Some errors usually cause other ones. Therefore, fix errors in the same"
21,order as they appear and re-check the program continuously.
21,18.4 More information #
21,"For a complete list of options related to the described tracing tools,"
21,"see the corresponding man page (man 1 strace,"
21,"man 1 ltrace, and man 1"
21,valgrind).
21,To describe advanced usage of Valgrind is beyond the scope of this
21,"document. It is very well documented, see"
21,Valgrind
21,User Manual. These pages are indispensable if you need more
21,advanced information on Valgrind or the usage and purpose of its
21,standard tools.
21,19 Kexec and Kdump #
21,Kexec is a tool to boot to another kernel from the currently running one.
21,You can perform faster system reboots without any hardware initialization.
21,You can also prepare the system to boot to another kernel if the system
21,crashes.
21,19.1 Introduction #
21,"With Kexec, you can replace the running kernel with another one without a"
21,hard reboot. The tool is useful for several reasons:
21,Faster system rebooting
21,"If you need to reboot the system frequently, Kexec can save you"
21,significant time.
21,Avoiding unreliable firmware and hardware
21,Computer hardware is complex and serious problems may occur during the
21,system start-up. You cannot always replace unreliable hardware
21,immediately. Kexec boots the kernel to a controlled environment with the
21,hardware already initialized. The risk of unsuccessful system start is
21,then minimized.
21,Saving the dump of a crashed kernel
21,Kexec preserves the contents of the physical memory. After the
21,"production kernel fails, the"
21,capture kernel (an additional kernel running in a
21,reserved memory range) saves the state of the failed kernel. The saved
21,image can help you with the subsequent analysis.
21,Booting without GRUB 2 configuration
21,"When the system boots a kernel with Kexec, it skips the boot loader"
21,stage. The normal booting procedure can fail because of an error in the
21,"boot loader configuration. With Kexec, you do not depend on a working"
21,boot loader configuration.
21,19.2 Required packages #
21,To use Kexec on SUSE® Linux Enterprise Server to speed up reboots or avoid potential
21,"hardware problems, make sure that the package"
21,kexec-tools is installed. It contains a script
21,"called kexec-bootloader, which reads the boot loader"
21,configuration and runs Kexec using the same kernel options as the normal
21,boot loader.
21,To set up an environment that helps you obtain debug information in case of
21,"a kernel crash, make sure that the package"
21,makedumpfile is installed.
21,The preferred method of using Kdump in SUSE Linux Enterprise Server is through the YaST
21,"Kdump module. To use the YaST module, make sure that the package"
21,yast2-kdump is installed.
21,19.3 Kexec internals #
21,The most important component of Kexec is the
21,/sbin/kexec command. You can load a kernel with Kexec
21,in two different ways:
21,Load the kernel to the address space of a production kernel for a regular
21,reboot:
21,# kexec -l KERNEL_IMAGE
21,You can later boot to this kernel with kexec
21,-e.
21,Load the kernel to a reserved area of memory:
21,# kexec -p KERNEL_IMAGE
21,This kernel will be booted automatically when the system crashes.
21,If you want to boot another kernel and preserve the data of the production
21,"kernel when the system crashes, you need to reserve a dedicated area of the"
21,system memory. The production kernel never loads to this area because it
21,must be always available. It is used for the capture kernel so that the
21,memory pages of the production kernel can be preserved.
21,"To reserve the area, append the option crashkernel to the"
21,boot command line of the production kernel. To determine the necessary
21,"values for crashkernel, follow the instructions in"
21,"Section 19.4, “Calculating crashkernel allocation size”."
21,Note that this is not a parameter of the capture kernel. The capture kernel
21,does not use Kexec.
21,The capture kernel is loaded to the reserved area and waits for the kernel
21,"to crash. Then, Kdump tries to invoke the capture kernel because the"
21,production kernel is no longer reliable at this stage. This means that even
21,Kdump can fail.
21,"To load the capture kernel, you need to include the kernel boot parameters."
21,"Usually, the initial RAM file system is used for booting. You can specify it"
21,with
21,--initrd=FILENAME.
21,With
21,"--append=CMDLINE,"
21,you append options to the command line of the kernel to boot.
21,It is required to include the command line of the production kernel. You can
21,simply copy the command line with
21,"--append=""$(cat"
21,"/proc/cmdline)"" or add more options with"
21,"--append=""$(cat"
21,"/proc/cmdline) more_options""."
21,"For example, to load the /boot/vmlinuz-5.14.21-150500.53-default kernel image"
21,with the command line of the currently running production kernel and the
21,"/boot/initrd file, run the following command:"
21,kexec -l /boot/vmlinuz-5.14.21-150500.53-default \
21,"--append=""$(cat /proc/cmdline)"" --initrd=/boot/initrd"
21,You can always unload the previously loaded kernel. To unload a kernel that
21,"was loaded with the -l option, use the"
21,kexec -u command. To unload a crash
21,"kernel loaded with the -p option, use"
21,kexec -p -u command.
21,19.4 Calculating crashkernel allocation size #
21,"To use Kexec with a capture kernel and to use Kdump in any way, RAM"
21,needs to be allocated for the capture kernel. The allocation size depends on
21,"the expected hardware configuration of the computer, therefore you need to"
21,specify it.
21,The allocation size also depends on the hardware architecture of your
21,computer. Make sure to follow the procedure intended for your system
21,architecture.
21,Procedure 19.1: Allocation size on AMD64/Intel 64 #
21,"To find out the base value for the computer, run the following command:"
21,# kdumptool calibrate
21,Total: 49074
21,Low: 72
21,High: 180
21,MinLow: 72
21,MaxLow: 3085
21,MinHigh: 0
21,MaxHigh: 45824
21,All values are given in megabytes.
21,Take note of the values of Low and
21,High.
21,Note: Significance of Low and High values
21,"On AMD64/Intel 64 computers, the High value stands for the"
21,memory reservation for all available memory. The Low
21,"value stands for the memory reservation in the DMA32 zone, that is, all"
21,the memory up to the 4 GB mark.
21,SIZE_LOW is the amount of memory required by 32-bit-only devices. The
21,kernel will allocate 64M for DMA32 bounce buffers. If your server does
21,"not have any 32-bit-only devices, everything should work with the default"
21,allocation of 72M for SIZE_LOW. A possible exception
21,"to this is on NUMA machines, which may make it appear that more"
21,Low memory is needed. The Kdump kernel may be booted
21,with numa=off to make sure normal kernel allocations
21,do not use Low memory.
21,Adapt the High value from the previous step for the
21,number of LUN kernel paths (paths to storage devices) attached to the
21,computer. A sensible value in megabytes can be calculated using this
21,formula:
21,SIZE_HIGH = RECOMMENDATION + (LUNs / 2)
21,The following parameters are used in this formula:
21,SIZE_HIGH.
21,The resulting value for High.
21,RECOMMENDATION.
21,The value recommended by kdumptool calibrate for
21,High.
21,LUNs.
21,The maximum number of LUN kernel paths that you expect to ever create
21,"on the computer. Exclude multipath devices from this number, as these"
21,are ignored. To get the current number of LUNs available on your
21,"system, run the following command:"
21,cat /proc/scsi/scsi | grep Lun | wc -l
21,"If the drivers for your device make many reservations in the DMA32 zone,"
21,"the Low value also needs to be adjusted. However, there"
21,is no simple formula to calculate these. Finding the right size can
21,therefore be a process of trial and error.
21,"For the beginning, use the Low value recommended by"
21,kdumptool calibrate.
21,The values now need to be set in the correct location.
21,If you are changing the kernel command line directly
21,Append the following kernel option to your boot loader configuration:
21,"crashkernel=SIZE_HIGH,high crashkernel=SIZE_LOW,low"
21,Replace the placeholders SIZE_HIGH and
21,SIZE_LOW with the appropriate value from the
21,previous steps and append the letter M (for
21,megabytes).
21,"As an example, the following is valid:"
21,"crashkernel=36M,high crashkernel=72M,lowIf you are using the YaST GUI:"
21,Set Kdump Low Memory to the determined
21,Low value.
21,Set Kdump High Memory to the determined
21,High value.
21,If you are using the YaST command line interface:
21,Use the following command:
21,"# yast kdump startup enable alloc_mem=LOW,HIGH"
21,Replace LOW with the determined
21,Low value. Replace HIGH
21,with the determined HIGH value.
21,Procedure 19.2: Allocation size on POWER and IBM Z #
21,"To find out the basis value for the computer, run the following in a"
21,terminal:
21,# kdumptool calibrate
21,This command returns a list of values. All values are given in megabytes.
21,Write down the value of Low.
21,Adapt the Low value from the previous step for the
21,number of LUN kernel paths (paths to storage devices) attached to the
21,computer. A sensible value in megabytes can be calculated using this
21,formula:
21,SIZE_LOW = RECOMMENDATION + (LUNs / 2)
21,The following parameters are used in this formula:
21,SIZE_LOW.
21,The resulting value for Low.
21,RECOMMENDATION.
21,The value recommended by kdumptool calibrate for
21,Low.
21,LUNs.
21,The maximum number of LUN kernel paths that you expect to ever create
21,"on the computer. Exclude multipath devices from this number, as these"
21,are ignored.
21,The values now need to be set in the correct location.
21,If you are working on the command line
21,Append the following kernel option to your boot loader configuration:
21,crashkernel=SIZE_LOW
21,Replace the placeholderSIZE_LOW with the
21,appropriate value from the previous step and append the letter
21,M (for megabytes).
21,"As an example, the following is valid:"
21,crashkernel=108MIf you are working in YaST
21,Set Kdump Memory to the determined
21,Low value.
21,Tip: Excluding unused and inactive CCW devices on IBM Z
21,Depending on the number of available devices the calculated amount of
21,memory specified by the crashkernel kernel parameter may
21,"not be sufficient. Instead of increasing the value, you may alternatively"
21,limit the amount of devices visible to the kernel. This will lower the
21,"required amount of memory for the ""crashkernel"" setting."
21,To ignore devices you can run the cio_ignore tool to
21,"generate an appropriate stanza to ignore all devices, except the ones"
21,currently active or in use.
21,> sudo cio_ignore -u -k
21,"cio_ignore=all,!da5d,!f500-f502"
21,"When you run cio_ignore -u -k, the blacklist will"
21,become active and replace any existing blacklist immediately. Unused
21,"devices are not being purged, so they still appear in the channel"
21,subsystem. But adding new channel devices (via CP ATTACH under z/VM or
21,dynamic I/O configuration change in LPAR) will treat them as blacklisted.
21,"To prevent this, preserve the original setting by running sudo"
21,cio_ignore -l first and reverting to that state after running
21,"cio_ignore -u -k. As an alternative, add the generated"
21,stanza to the regular kernel boot parameters.
21,Now add the cio_ignore kernel parameter with the stanza
21,from above to KDUMP_CMDLINE_APPEND in
21,"/etc/sysconfig/kdump, for example:"
21,"KDUMP_COMMANDLINE_APPEND=""cio_ignore=all,!da5d,!f500-f502"""
21,Activate the setting by restarting
21,kdump:
21,systemctl restart kdump.service19.5 Basic Kexec usage #
21,"To use Kexec, ensure the respective service is enabled and running:"
21,Make sure the Kexec service is loaded at system start:
21,> sudo systemctl enable kexec-load.service
21,Make sure the Kexec service is running:
21,> sudo systemctl start kexec-load.service
21,"To verify if your Kexec environment works properly, try rebooting into a"
21,new Kernel with Kexec. Make sure no users are currently logged in and no
21,important services are running on the system. Then run the following
21,command:
21,systemctl kexec
21,The new kernel previously loaded to the address space of the older kernel
21,rewrites it and takes control immediately. It displays the usual start-up
21,"messages. When the new kernel boots, it skips all hardware and firmware"
21,checks. Make sure no warning messages appear.
21,Tip: Using Kexec with the reboot command
21,To make reboot use Kexec rather than performing a
21,"regular reboot, run the following command:"
21,ln -s /usr/lib/systemd/system/kexec.target /etc/systemd/system/reboot.target
21,You can revert this at any time by deleting
21,etc/systemd/system/reboot.target.
21,19.6 How to configure Kexec for routine reboots #
21,"Kexec is often used for frequent reboots. For example, if it takes a long"
21,time to run through the hardware detection routines or if the start-up is
21,not reliable.
21,Note that firmware and the boot loader are not used when the system reboots
21,with Kexec. Any changes you make to the boot loader configuration will be
21,ignored until the computer performs a hard reboot.
21,19.7 Basic Kdump configuration #
21,"You can use Kdump to save kernel dumps. If the kernel crashes, it is"
21,useful to copy the memory image of the crashed environment to the file
21,system. You can then debug the dump file to find the cause of the kernel
21,crash. This is called “core dump”.
21,"Kdump works similarly to Kexec (see Chapter 19, Kexec and Kdump)."
21,The capture kernel is executed after the running production kernel crashes.
21,The difference is that Kexec replaces the production kernel with the
21,"capture kernel. With Kdump, you still have access to the memory space of"
21,the crashed production kernel. You can save the memory snapshot of the
21,crashed kernel in the environment of the Kdump kernel.
21,Tip: Dumps over network
21,"In environments with limited local storage, you need to set up kernel dumps"
21,over the network. Kdump supports configuring the specified network
21,interface and bringing it up via initrd. Both LAN
21,and VLAN interfaces are supported. Specify the network interface and the
21,"mode (DHCP or static) either with YaST, or using the"
21,KDUMP_NETCONFIG option in the
21,/etc/sysconfig/kdump file.
21,Important: Target file system for Kdump must be mounted during configuration
21,"When configuring Kdump, you can specify a location to which the dumped"
21,images will be saved (default: /var/crash). This
21,"location must be mounted when configuring Kdump, otherwise the"
21,configuration will fail.
21,19.7.1 Manual Kdump configuration #
21,Kdump reads its configuration from the
21,/etc/sysconfig/kdump file. To make sure that Kdump
21,"works on your system, its default configuration is sufficient. To use"
21,"Kdump with the default settings, follow these steps:"
21,Determine the amount of memory needed for Kdump by following the
21,"instructions in Section 19.4, “Calculating crashkernel allocation size”. Make sure"
21,to set the kernel parameter crashkernel.
21,Reboot the computer.
21,Enable the Kdump service:
21,# systemctl enable kdump
21,You can edit the options in /etc/sysconfig/kdump.
21,Reading the comments will help you understand the meaning of individual
21,options.
21,Execute the init script once with sudo systemctl start
21,"kdump, or reboot the system."
21,"After configuring Kdump with the default values, check if it works as"
21,expected. Make sure that no users are currently logged in and no important
21,services are running on your system. Then follow these steps:
21,Switch to the rescue target with systemctl isolate
21,rescue.target
21,Restart the Kdump service:
21,# systemctl start kdump
21,Unmount all the disk file systems except the root file system with:
21,# umount -a
21,Remount the root file system in read-only mode:
21,"# mount -o remount,ro /"
21,Invoke a “kernel panic” with the procfs
21,interface to Magic SysRq keys:
21,# echo c > /proc/sysrq-triggerImportant: Size of kernel dumps
21,The KDUMP_KEEP_OLD_DUMPS option controls the number of
21,"preserved kernel dumps (default is 5). Without compression, the size of"
21,the dump can take up to the size of the physical RAM memory. Make sure you
21,have sufficient space on the /var partition.
21,The capture kernel boots and the crashed kernel memory snapshot is saved to
21,the file system. The save path is given by the
21,KDUMP_SAVEDIR option and it defaults to
21,/var/crash. If
21,KDUMP_IMMEDIATE_REBOOT is set to yes
21,", the system automatically reboots the production kernel. Log in and check"
21,that the dump has been created under /var/crash.
21,19.7.1.1 Static IP configuration for Kdump #
21,In case Kdump is configured to use a static IP configuration from a
21,"network device, you need to add the network configuration to the"
21,KDUMP_COMMANDLINE_APPEND variable in
21,/etc/sysconfig/kdump.
21,Important: Changes to the Kdump configuration file
21,After making changes to the /etc/sysconfig/kdump
21,"file, you need to run systemctl restart kdump.service."
21,"Otherwise, the changes will only take effect next time you reboot the"
21,system.
21,Example 19.1: Kdump: example configuration using a static IP setup #
21,The following setup has been configured:
21,eth0 has been configured with the static IP address
21,192.168.1.1/24
21,eth1 has been configured with the static IP address
21,10.50.50.100/20
21,The Kdump configuration in /etc/sysconfig/kdump
21,looks like:
21,KDUMP_CPUS=1
21,KDUMP_IMMEDIATE_REBOOT=yes
21,KDUMP_SAVEDIR=ftp://anonymous@10.50.50.140/crashdump/
21,KDUMP_KEEP_OLD_DUMPS=5
21,KDUMP_FREE_DISK_SIZE=64
21,KDUMP_VERBOSE=3
21,KDUMP_DUMPLEVEL=31
21,KDUMP_DUMPFORMAT=lzo
21,KDUMP_CONTINUE_ON_ERROR=yes
21,KDUMP_NETCONFIG=eth1:static
21,KDUMP_NET_TIMEOUT=30
21,"Using this configuration, Kdump fails to reach the network when trying"
21,"to write the dump to the FTP server. To solve this issue, add the network"
21,configuration to KDUMP_COMMANDLINE_APPEND in
21,/etc/sysconfig/kdump. The general pattern for this
21,looks like the following:
21,KDUMP_COMMANDLINE_APPEND='ip=CLIENT IP:SERVER IP:GATEWAY IP:NETMASK:CLIENT HOSTNAME:DEVICE:PROTOCOL'
21,For the example configuration this would result in:
21,KDUMP_COMMANDLINE_APPEND='ip=10.50.50.100:10.50.50.140:10.60.48.1:255.255.240.0:dump-client:eth1:none'19.7.2 YaST configuration #
21,"To configure Kdump with YaST, you need to install the"
21,yast2-kdump package. Then either start the
21,Kernel Kdump module in the System
21,"category of YaST Control Center, or enter yast2 kdump in the"
21,command line as root.
21,Figure 19.1: YaST Kdump module: start-up page #
21,"In the Start-Up window, select Enable"
21,Kdump.
21,The values for Kdump Memory are automatically
21,"generated the first time you open the window. However, that does not mean"
21,"that they are always sufficient. To set the right values, follow the"
21,"instructions in Section 19.4, “Calculating crashkernel allocation size”."
21,"Important: After hardware changes, set Kdump memory values again"
21,If you have set up Kdump on a computer and later decide to change the
21,"amount of RAM or hard disks available to it, YaST will continue to"
21,display and use outdated memory values.
21,"To work around this, determine the necessary memory again, as described in"
21,"Section 19.4, “Calculating crashkernel allocation size”. Then set it manually in"
21,YaST.
21,"Click Dump Filtering in the left pane, and check what"
21,pages to include in the dump. You do not need to include the following
21,memory content to be able to debug kernel problems:
21,Pages filled with zero
21,Cache pages
21,User data pages
21,Free pages
21,"In the Dump Target window, select the type of the dump"
21,target and the URL where you want to save the dump. If you selected a
21,"network protocol, such as FTP or SSH, you need to enter relevant access"
21,information as well.
21,Tip: Sharing the dump directory with other applications
21,It is possible to specify a path for saving Kdump dumps where other
21,"applications also save their dumps. When cleaning its old dump files,"
21,Kdump will safely ignore other applications' dump files.
21,Fill the Email Notification window information if you
21,want Kdump to inform you about its events via e-mail and confirm your
21,changes with OK after fine tuning Kdump in the
21,Expert Settings window. Kdump is now configured.
21,19.7.3 Kdump over SSH #
21,Dump files usually contain sensitive data which should be protected from
21,unauthorized disclosure. To allow transmission of such data over an
21,"insecure network, Kdump can save dump files to a remote machine using the"
21,SSH protocol.
21,The target host identity must be known to Kdump. This is needed to
21,ensure that sensitive data is never sent to an imposter. When Kdump
21,"generates a new initrd, it runs"
21,ssh-keygen -F TARGET_HOST
21,to query the target host's identity. This works only if
21,TARGET_HOST public key is already known. An
21,easy way to achieve that is to make an SSH connection to
21,TARGET_HOST as root on the Kdump host.
21,Kdump must be able to authenticate to the target machine. Only public
21,"key authentication is currently available. By default, Kdump will use"
21,"root's private key, but it is advisable to make a separate key for"
21,Kdump. This can be done with ssh-keygen:
21,# ssh-keygen -f ~/.ssh/kdump_key
21,"Press Enter when prompted for passphrase (that is,"
21,do not use any passphrase).
21,Open /etc/sysconfig/kdump and set
21,KDUMP_SSH_IDENTITY to
21,kdump_key. You can use full path to the file
21,if it is not placed under ~/.ssh.
21,Set up the Kdump SSH key to authorize logins to the remote host.
21,# ssh-copy-id -i ~/.ssh/kdump_key TARGET_HOST
21,Set up KDUMP_SAVEDIR. There are two options:
21,Secure File Transfer Protocol (SFTP)
21,SFTP is the preferred method for transmitting files over SSH. The
21,target host must enable the SFTP subsystem (SLE default). Example:
21,KDUMP_SAVEDIR=sftp://TARGET_HOST/path/to/dumpsSecure Shell protocol (SSH)
21,Some other distributions use SSH to run some commands on the target
21,host. SUSE Linux Enterprise Server can also use this method. The Kdump user on the
21,target host must have a login shell that can execute these commands:
21,"mkdir, dd and"
21,mv. Example:
21,KDUMP_SAVEDIR=ssh://TARGET_HOST/path/to/dumps
21,Restart the Kdump service to use the new configuration.
21,19.8 Analyzing the crash dump #
21,"After you obtain the dump, it is time to analyze it. There are several"
21,options.
21,The original tool to analyze the dumps is GDB. You can even use it in the
21,"latest environments, although it has several disadvantages and limitations:"
21,GDB was not specifically designed to debug kernel dumps.
21,GDB does not support ELF64 binaries on 32-bit platforms.
21,GDB does not understand other formats than ELF dumps (it cannot debug
21,compressed dumps).
21,That is why the crash utility was implemented. It
21,analyzes crash dumps and debugs the running system as well. It provides
21,functionality specific to debugging the Linux kernel and is much more
21,suitable for advanced debugging.
21,"If you want to debug the Linux kernel, you need to install its debugging"
21,information package in addition. Check if the package is installed on your
21,system with:
21,> zypper se kernel | grep debugImportant: Repository for packages with debugging information
21,"If you subscribed your system for online updates, you can find"
21,“debuginfo” packages in the
21,*-Debuginfo-Updates online installation repository
21,relevant for SUSE Linux Enterprise Server 15 SP3. Use YaST to enable the
21,repository.
21,To open the captured dump in crash on the machine that
21,"produced the dump, use a command like this:"
21,crash /boot/vmlinux-2.6.32.8-0.1-default.gz \
21,/var/crash/2010-04-23-11\:17/vmcore
21,The first parameter represents the kernel image. The second parameter is the
21,dump file captured by Kdump. You can find this file under
21,/var/crash by default.
21,Tip: Getting basic information from a kernel crash dump
21,SUSE Linux Enterprise Server ships with the utility kdumpid (included
21,in a package with the same name) for identifying unknown kernel dumps. It
21,can be used to extract basic information such as architecture and kernel
21,"release. It supports lkcd, diskdump, Kdump files and ELF dumps. When"
21,called with the -v switch it tries to extract additional
21,"information such as machine type, kernel banner string and kernel"
21,configuration flavor.
21,19.8.1 Kernel binary formats #
21,The Linux kernel comes in Executable and Linkable Format (ELF). This file
21,is usually called vmlinux and is directly generated in
21,"the compilation process. Not all boot loaders support ELF binaries,"
21,especially on the AMD64/Intel 64 architecture. The following solutions exist on
21,different architectures supported by SUSE® Linux Enterprise Server.
21,19.8.1.1 AMD64/Intel 64 #
21,Kernel packages for AMD64/Intel 64 from SUSE contain two kernel files:
21,vmlinuz and vmlinux.gz.
21,vmlinuz.
21,This is the file executed by the boot loader.
21,The Linux kernel consists of two parts: the kernel itself
21,(vmlinux) and the setup code run by the boot loader.
21,These two parts are linked together to create
21,vmlinuz (note the distinction: z
21,compared to x).
21,"In the kernel source tree, the file is called"
21,bzImage.
21,vmlinux.gz.
21,This is a compressed ELF image that can be used by
21,crash and GDB. The ELF image is never used by the
21,"boot loader itself on AMD64/Intel 64. Therefore, only a compressed version is"
21,shipped.
21,19.8.1.2 POWER #
21,The yaboot boot loader on POWER also supports
21,"loading ELF images, but not compressed ones. In the POWER kernel"
21,"package, there is an ELF Linux kernel file vmlinux."
21,"Considering crash, this is the easiest architecture."
21,"If you decide to analyze the dump on another machine, you must check both"
21,the architecture of the computer and the files necessary for debugging.
21,You can analyze the dump on another computer only if it runs a Linux
21,"system of the same architecture. To check the compatibility, use the"
21,command uname -i on both computers and
21,compare the outputs.
21,"If you are going to analyze the dump on another computer, you also need"
21,the appropriate files from the kernel and
21,kernel debug packages.
21,"Put the kernel dump, the kernel image from /boot,"
21,and its associated debugging info file from
21,/usr/lib/debug/boot into a single empty directory.
21,"Additionally, copy the kernel modules from"
21,/lib/modules/$(uname -r)/kernel/ and the associated
21,debug info files from /usr/lib/debug/lib/modules/$(uname
21,-r)/kernel/ into a subdirectory named
21,modules.
21,"In the directory with the dump, the kernel image, its debug info file,"
21,"and the modules subdirectory, start the"
21,crash utility:
21,> crash VMLINUX-VERSION vmcore
21,"Regardless of the computer on which you analyze the dump, the crash"
21,utility will produce output similar to this:
21,> crash /boot/vmlinux-5.3.18-8-default.gz \
21,/var/crash/2020-04-23-11\:17/vmcore
21,crash 7.2.1
21,Copyright (C) 2002-2017
21,"Red Hat, Inc."
21,"Copyright (C) 2004, 2005, 2006, 2010"
21,IBM Corporation
21,Copyright (C) 1999-2006
21,Hewlett-Packard Co
21,"Copyright (C) 2005, 2006, 2011, 2012"
21,Fujitsu Limited
21,"Copyright (C) 2006, 2007"
21,VA Linux Systems Japan K.K.
21,"Copyright (C) 2005, 2011"
21,NEC Corporation
21,"Copyright (C) 1999, 2002, 2007"
21,"Silicon Graphics, Inc."
21,"Copyright (C) 1999, 2000, 2001, 2002"
21,"Mission Critical Linux, Inc."
21,"This program is free software, covered by the GNU General Public License,"
21,and you are welcome to change it and/or distribute copies of it under
21,certain conditions.
21,"Enter ""help copying"" to see the conditions."
21,This program has absolutely no warranty.
21,"Enter ""help warranty"" for details."
21,GNU gdb (GDB) 7.6
21,"Copyright (C) 2013 Free Software Foundation, Inc."
21,License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
21,This is free software: you are free to change and redistribute it.
21,"There is NO WARRANTY, to the extent permitted by law."
21,"Type ""show copying"""
21,"and ""show warranty"" for details."
21,"This GDB was configured as ""x86_64-unknown-linux-gnu""."
21,KERNEL: /boot/vmlinux-5.3.18-8-default.gz
21,DEBUGINFO: /usr/lib/debug/boot/vmlinux-5.3.18-8-default.debug
21,DUMPFILE: /var/crash/2020-04-23-11:17/vmcore
21,CPUS: 2
21,DATE: Thu Apr 23 13:17:01 2020
21,UPTIME: 00:10:41
21,"LOAD AVERAGE: 0.01, 0.09, 0.09"
21,TASKS: 42
21,NODENAME: eros
21,RELEASE: 5.3.18-8-default
21,VERSION: #1 SMP 2020-03-31 14:50:44 +0200
21,MACHINE: x86_64
21,(2999 Mhz)
21,MEMORY: 16 GB
21,"PANIC: ""SysRq : Trigger a crashdump"""
21,PID: 9446
21,"COMMAND: ""bash"""
21,TASK: ffff88003a57c3c0
21,[THREAD_INFO: ffff880037168000]
21,CPU: 1
21,STATE: TASK_RUNNING (SYSRQ)
21,crash>
21,The command output prints first useful data: There were 42 tasks running
21,at the moment of the kernel crash. The cause of the crash was a SysRq
21,trigger invoked by the task with PID 9446. It was a Bash process because
21,the echo that has been used is an internal command of
21,the Bash shell.
21,The crash utility builds upon GDB and provides many
21,additional commands. If you enter bt without any
21,"parameters, the backtrace of the task running at the moment of the crash"
21,is printed:
21,crash> bt
21,PID: 9446
21,TASK: ffff88003a57c3c0
21,CPU: 1
21,"COMMAND: ""bash"""
21,#0 [ffff880037169db0] crash_kexec at ffffffff80268fd6
21,#1 [ffff880037169e80] __handle_sysrq at ffffffff803d50ed
21,#2 [ffff880037169ec0] write_sysrq_trigger at ffffffff802f6fc5
21,#3 [ffff880037169ed0] proc_reg_write at ffffffff802f068b
21,#4 [ffff880037169f10] vfs_write at ffffffff802b1aba
21,#5 [ffff880037169f40] sys_write at ffffffff802b1c1f
21,#6 [ffff880037169f80] system_call_fastpath at ffffffff8020bfbb
21,RIP: 00007fa958991f60
21,RSP: 00007fff61330390
21,RFLAGS: 00010246
21,RAX: 0000000000000001
21,RBX: ffffffff8020bfbb
21,RCX: 0000000000000001
21,RDX: 0000000000000002
21,RSI: 00007fa959284000
21,RDI: 0000000000000001
21,RBP: 0000000000000002
21,R8: 00007fa9592516f0
21,R9: 00007fa958c209c0
21,R10: 00007fa958c209c0
21,R11: 0000000000000246
21,R12: 00007fa958c1f780
21,R13: 00007fa959284000
21,R14: 0000000000000002
21,R15: 00000000595569d0
21,ORIG_RAX: 0000000000000001
21,CS: 0033
21,SS: 002b
21,crash>
21,Now it is clear what happened: The internal echo
21,command of Bash shell sent a character to
21,/proc/sysrq-trigger. After the corresponding handler
21,"recognized this character, it invoked the crash_kexec()"
21,function. This function called panic() and Kdump
21,saved a dump.
21,In addition to the basic GDB commands and the extended version of
21,"bt, the crash utility defines other commands related to"
21,the structure of the Linux kernel. These commands understand the internal
21,data structures of the Linux kernel and present their contents in a human
21,"readable format. For example, you can list the tasks running at the moment"
21,"of the crash with ps. With sym, you"
21,"can list all the kernel symbols with the corresponding addresses, or"
21,"inquire an individual symbol for its value. With files,"
21,you can display all the open file descriptors of a process. With
21,"kmem, you can display details about the kernel memory"
21,"usage. With vm, you can inspect the virtual memory of a"
21,"process, even at the level of individual page mappings. The list of useful"
21,commands is very long and many of these accept a wide range of options.
21,The commands that we mentioned reflect the functionality of the common
21,"Linux commands, such as ps and lsof."
21,"To find out the exact sequence of events with the debugger, you need to"
21,know how to use GDB and to have strong debugging skills. Both of these are
21,"out of the scope of this document. In addition, you need to understand the"
21,Linux kernel. Several useful reference information sources are given at
21,the end of this document.
21,19.9 Advanced Kdump configuration #
21,The configuration for Kdump is stored in
21,/etc/sysconfig/kdump. You can also use YaST to
21,configure it. Kdump configuration options are available under
21,System › Kernel
21,Kdump in YaST Control Center. The following Kdump options may
21,be useful for you.
21,You can change the directory for the kernel dumps with the
21,KDUMP_SAVEDIR option. Keep in mind that the size of kernel
21,dumps can be very large. Kdump will refuse to save the dump if the free
21,"disk space, subtracted by the estimated dump size, drops below the value"
21,specified by the KDUMP_FREE_DISK_SIZE option. Note that
21,KDUMP_SAVEDIR understands the URL format
21,"PROTOCOL://SPECIFICATION, where"
21,"PROTOCOL is one of file,"
21,"ftp, sftp, nfs or"
21,"cifs, and specification varies for each"
21,"protocol. For example, to save kernel dump on an FTP server, use the"
21,following URL as a template:
21,ftp://username:password@ftp.example.com:123/var/crash.
21,Kernel dumps are usually huge and contain many pages that are not necessary
21,"for analysis. With KDUMP_DUMPLEVEL option, you can omit"
21,such pages. The option understands numeric value between 0 and 31. If you
21,"specify 0, the dump size will be largest. If you"
21,"specify 31, it will produce the smallest dump."
21,"For a complete table of possible values, see the manual page of"
21,kdump (man 7 kdump).
21,Sometimes it is very useful to make the size of the kernel dump smaller. For
21,"example, if you want to transfer the dump over the network, or if you need"
21,to save some disk space in the dump directory. This can be done with
21,KDUMP_DUMPFORMAT set to compressed. The
21,crash utility supports dynamic decompression of the
21,compressed dumps.
21,Important: Changes to the Kdump configuration file
21,After making changes to the /etc/sysconfig/kdump
21,"file, you need to run systemctl restart kdump.service."
21,"Otherwise, the changes will only take effect next time you reboot the"
21,system.
21,19.10 More information #
21,There is no single comprehensive reference to Kexec and Kdump usage.
21,"However, there are helpful resources that deal with certain aspects:"
21,"For the Kexec utility usage, see the manual page of"
21,kexec (man 8 kexec).
21,IBM provides comprehensive documentation on how to use dump tools on the
21,IBM Z architecture at
21,https://developer.ibm.com/technologies/linux/.
21,You can find general information about Kexec at
21,https://developer.ibm.com/technologies/linux/.
21,"For more details on Kdump specific to SUSE Linux Enterprise Server, see"
21,http://ftp.suse.com/pub/people/tiwai/kdump-training/kdump-training.pdf
21,An in-depth description of Kdump internals can be found at
21,http://lse.sourceforge.net/kdump/documentation/ols2oo5-kdump-paper.pdf
21,For more details on crash dump analysis and debugging
21,"tools, use the following resources:"
21,"In addition to the info page of GDB (info gdb), there"
21,are printable guides at
21,https://sourceware.org/gdb/documentation/ .
21,The crash utility features a comprehensive online help. Use
21,help COMMAND to display the
21,online help for command.
21,"If you have the necessary Perl skills, you can use Alicia to make the"
21,debugging easier. This Perl-based front-end to the crash utility can be
21,found at http://alicia.sourceforge.net/ .
21,"If you prefer to use Python instead, you should install Pykdump. This"
21,package helps you control GDB through Python scripts.
21,A very comprehensive overview of the Linux kernel internals is given in
21,Understanding the Linux Kernel by Daniel P. Bovet
21,and Marco Cesati (ISBN 978-0-596-00565-8).
21,20 Using systemd-coredump to debug application crashes #
21,"systemd-coredump collects and displays core dumps, for analyzing"
21,application crashes. The core dump contains an image of the process's
21,memory at the time of termination. When a process crashes (or all
21,"processes belonging to an application), its default is to log the core"
21,"dump to the systemd journal, including a backtrace if possible, and to"
21,store the core dump in a file in
21,/var/lib/systemd/coredump. You also have the option
21,to examine the dump file with other tools such as gdb
21,"or crash (see Section 19.8, “Analyzing the crash dump”)."
21,Core dumps stored in /var/lib/systemd/coredump
21,are deleted after three days (see the
21,d /var/lib/systemd/coredump line in
21,/usr/lib/tmpfiles.d/systemd.conf).
21,"There is an option to not store core dumps, but to log only to the"
21,"journal, which may be useful to minimize the collection and storage of"
21,sensitive information.
21,20.1 Use and configuration #
21,systemd-coredump is enabled and ready to run by default. The default
21,configuration is in /etc/systemd/coredump.conf:
21,[Coredump]
21,#Storage=external
21,#Compress=yes
21,#ProcessSizeMax=2G
21,#ExternalSizeMax=2G
21,#JournalSizeMax=767M
21,#MaxUse=
21,#KeepFree=
21,"Size units are B, K, M, G, T, P, and E. ExternalSizeMax also supports a value of infinity."
21,"The following example shows how to use Vim for simple testing, by creating a"
21,segfault to generate journal entries and a core dump.
21,Procedure 20.1: Creating a core dump with Vim #
21,Enable the debuginfo-pool and
21,debuginfo-update repositories
21,Install vim-debuginfo
21,Launch vim testfile and type a few characters
21,Get the PID and generate a segfault:
21,> ps ax | grep vim
21,2345 pts/3
21,0:00 vim testfile
21,# kill -s SIGSEGV 2345
21,Vim will emit error messages:
21,Vim: Caught deadly signal SEGV
21,Vim: Finished.
21,Segmentation fault (core dumped)
21,"List your core dumps, then examine them:"
21,# coredumpctl
21,TIME
21,PID
21,UID
21,GID SIG PRESENT EXE
21,Wed 2019-11-12 11:56:47 PST 2345 1000 100 11
21,/bin/vim
21,# coredumpctl info
21,PID: 2345 (vim)
21,UID: 0 (root)
21,GID: 0 (root)
21,Signal: 11 (SEGV)
21,Timestamp: Wed 2019-11-12 11:58:05 PST
21,Command Line: vim testfile
21,Executable: /bin/vim
21,Control Group: /user.slice/user-1000.slice/session-1.scope
21,Unit: session-1.scope
21,Slice: user-1000.slice
21,Session: 1
21,Owner UID: 1000 (tux)
21,Boot ID: b5c251b86ab34674a2222cef102c0c88
21,Machine ID: b43c44a64696799b985cafd95dc1b698
21,Hostname: linux-uoch
21,Coredump: /var/lib/systemd/coredump/core.vim.0.b5c251b86ab34674a2222cef102
21,Message: Process 2345 (vim) of user 0 dumped core.
21,Stack trace of thread 2345:
21,0x00007f21dd87e2a7 kill (libc.so.6)
21,0x000000000050cb35 may_core_dump (vim)
21,0x00007f21ddbfec70 __restore_rt (libpthread.so.0)
21,0x00007f21dd92ea33 __select (libc.so.6)
21,0x000000000050b4e3 RealWaitForChar (vim)
21,0x000000000050b86b mch_inchar (vim)
21,[...]
21,"When you have multiple core dumps, coredumpctl info"
21,"displays all of them. Filter them by PID,"
21,"COMM (command), or EXE (full path to"
21,"the executable), for example, all core dumps for Vim:"
21,# coredumpctl info /bin/vim
21,See a single core dump by PID:
21,# coredumpctl info 2345
21,Output the selected core to gdb:
21,# coredumpctl gdb 2345
21,The asterisk in the PRESENT column indicates that a
21,stored core dump is present. If the field is empty there is no stored core
21,"dump, and coredumpctl retrieves crash information from"
21,the journal. You can control this behavior in
21,/etc/systemd/coredump.conf with the
21,Storage option:
21,"Storage=none—core dumps are logged in the journal, but"
21,not stored. This is useful to minimize collecting and storing sensitive
21,"information, for example for General Data Protection Regulation (GDPR)"
21,compliance.
21,Storage=external—cores are stored in
21,/var/lib/systemd/coredump
21,Storage=journal—cores are stored in the systemd
21,journal
21,"A new instance of systemd-coredump is invoked for every core dump, so"
21,"configuration changes are applied with the next core dump, and there is no"
21,need to restart any services.
21,Core dumps are not preserved after a system restart. You may save them
21,permanently with coredumpctl. The following example
21,filters by the PID and stores the core in
21,vim.dump:
21,# coredumpctl -o vim.dump dump 2345
21,"See man systemd-coredump, man"
21,"coredumpctl, man core, and"
21,man coredump.conf for complete
21,command and option listings.
21,Part VII Synchronized clocks with Precision Time Protocol #  21 Precision Time Protocol
21,"For network environments, it is vital to keep the computer and other devices'"
21,clocks synchronized and accurate. There are several solutions to achieve
21,"this, for example the widely used Network Time Protocol (NTP) described in"
21,"Book “Administration Guide”, Chapter 30 “Time synchronization with NTP”."
21,21 Precision Time Protocol #
21,"For network environments, it is vital to keep the computer and other devices'"
21,clocks synchronized and accurate. There are several solutions to achieve
21,"this, for example the widely used Network Time Protocol (NTP) described in"
21,"Book “Administration Guide”, Chapter 30 “Time synchronization with NTP”."
21,"The Precision Time Protocol (PTP) is a protocol capable of sub-microsecond accuracy, which is"
21,better than what NTP achieves. PTP support is divided between the kernel and
21,"user space. The kernel in SUSE Linux Enterprise Server includes support for PTP clocks,"
21,which are provided by network drivers.
21,21.1 Introduction to PTP #
21,The clocks managed by PTP follow a master-slave hierarchy. The slaves are
21,synchronized to their masters. The hierarchy is updated by the
21,"best master clock (BMC) algorithm, which runs on every"
21,clock. The clock with only one port can be either master or slave. Such a
21,clock is called an ordinary clock (OC). A clock with
21,multiple ports can be master on one port and slave on another. Such a clock
21,is called a boundary clock (BC). The top-level master
21,is called the grandmaster clock. The grandmaster clock
21,can be synchronized with a Global Positioning System (GPS). This way
21,disparate networks can be synchronized with a high degree of accuracy.
21,The hardware support is the main advantage of PTP. It is supported by
21,various network switches and network interface controllers (NIC). While it
21,"is possible to use non-PTP enabled hardware within the network, having"
21,network components between all PTP clocks PTP hardware enabled achieves the
21,best possible accuracy.
21,21.1.1 PTP Linux implementation #
21,"On SUSE Linux Enterprise Server, the implementation of PTP is provided by the"
21,linuxptp package. Install it with zypper
21,install linuxptp. It includes the ptp4l and
21,phc2sys programs for clock synchronization.
21,ptp4l implements the PTP boundary clock and ordinary
21,"clock. When hardware time stamping is enabled, ptp4l"
21,synchronizes the PTP hardware clock to the master clock. With software time
21,"stamping, it synchronizes the system clock to the master clock."
21,phc2sys is needed only with hardware time stamping to
21,synchronize the system clock to the PTP hardware clock on the network
21,interface card (NIC).
21,21.2 Using PTP #  21.2.1 Network driver and hardware support #
21,PTP requires that the used kernel network driver supports either software
21,"or hardware time stamping. Moreover, the NIC must support time stamping in"
21,the physical hardware. You can verify the driver and NIC time stamping
21,capabilities with ethtool:
21,> sudo ethtool -T eth0
21,Time stamping parameters for eth0:
21,Capabilities:
21,hardware-transmit
21,(SOF_TIMESTAMPING_TX_HARDWARE)
21,software-transmit
21,(SOF_TIMESTAMPING_TX_SOFTWARE)
21,hardware-receive
21,(SOF_TIMESTAMPING_RX_HARDWARE)
21,software-receive
21,(SOF_TIMESTAMPING_RX_SOFTWARE)
21,software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
21,hardware-raw-clock
21,(SOF_TIMESTAMPING_RAW_HARDWARE)
21,PTP Hardware Clock: 0
21,Hardware Transmit Timestamp Modes:
21,off
21,(HWTSTAMP_TX_OFF)
21,(HWTSTAMP_TX_ON)
21,Hardware Receive Filter Modes:
21,none
21,(HWTSTAMP_FILTER_NONE)
21,all
21,(HWTSTAMP_FILTER_ALL)
21,Software time stamping requires the following parameters:
21,SOF_TIMESTAMPING_SOFTWARE
21,SOF_TIMESTAMPING_TX_SOFTWARE
21,SOF_TIMESTAMPING_RX_SOFTWARE
21,Hardware time stamping requires the following parameters:
21,SOF_TIMESTAMPING_RAW_HARDWARE
21,SOF_TIMESTAMPING_TX_HARDWARE
21,SOF_TIMESTAMPING_RX_HARDWARE21.2.2 Using ptp4l #
21,ptp4l uses hardware time stamping by default. As
21,"root, you need to specify the network interface capable of hardware"
21,time stamping with the -i option. The -m
21,tells ptp4l to print its output to the standard output
21,instead of the system's logging facility:
21,> sudo ptp4l -m -i eth0
21,selected eth0 as PTP clock
21,port 1: INITIALIZING to LISTENING on INITIALIZE
21,port 0: INITIALIZING to LISTENING on INITIALIZE
21,port 1: new foreign master 00a152.fffe.0b334d-1
21,selected best master clock 00a152.fffe.0b334d
21,port 1: LISTENING to UNCALIBRATED on RS_SLAVE
21,master offset -25937 s0 freq +0 path delay
21,12340
21,master offset -27887 s0 freq +0 path delay
21,14232
21,master offset -38802 s0 freq +0 path delay
21,13847
21,master offset -36205 s1 freq +0 path delay
21,10623
21,master offset
21,-6975 s2 freq -30575 path delay
21,10286
21,port 1: UNCALIBRATED to SLAVE on MASTER_CLOCK_SELECTED
21,master offset
21,-4284 s2 freq -30135 path delay
21,9892
21,The master offset value represents the measured offset
21,from the master (in nanoseconds).
21,"The s0, s1, s2"
21,indicators show the different states of the clock servo:
21,"s0 is unlocked, s1 is clock step, and"
21,s2 is locked. If the servo is in the locked state
21,"(s2), the clock will not be stepped (only slowly"
21,adjusted) if the pi_offset_const option is set to a
21,negative value in the configuration file (see man 8
21,ptp4l for more information).
21,The freq value represents the frequency adjustment of
21,"the clock (in parts per billion, ppb)."
21,The path delay value represents the estimated delay of
21,the synchronization messages sent from the master (in nanoseconds).
21,Port 0 is a Unix domain socket used for local PTP management. Port 1 is the
21,eth0 interface.
21,"INITIALIZING, LISTENING,"
21,UNCALIBRATED and SLAVE are examples
21,"of port states which change on INITIALIZE,"
21,"RS_SLAVE, and MASTER_CLOCK_SELECTED"
21,events. When the port state changes from UNCALIBRATED to
21,"SLAVE, the computer has successfully synchronized with a"
21,PTP master clock.
21,You can enable software time stamping with the -S option.
21,> sudo ptp4l -m -S -i eth3
21,You can also run ptp4l as a service:
21,> sudo systemctl start ptp4l
21,"In this case, ptp4l reads its options from the"
21,"/etc/sysconfig/ptp4l file. By default, this file tells"
21,ptp4l to read the configuration options from
21,/etc/ptp4l.conf. For more information on
21,"ptp4l options and the configuration file settings, see"
21,man 8 ptp4l.
21,"To enable the ptp4l service permanently, run the"
21,following:
21,> sudo systemctl enable ptp4l
21,"To disable it, run"
21,> sudo systemctl disable ptp4l21.2.3 ptp4l configuration file #
21,ptp4l can read its configuration from an optional
21,"configuration file. As no configuration file is used by default, you need"
21,to specify it with -f.
21,> sudo ptp4l -f /etc/ptp4l.conf
21,The configuration file is divided into sections. The global section
21,"(indicated as [global]) sets the program options, clock"
21,"options and default port options. Other sections are port specific, and"
21,they override the default port options. The name of the section is the name
21,"of the configured port—for example, [eth0]. An"
21,empty port section can be used to replace the command line option.
21,[global]
21,verbose
21,time_stamping
21,software
21,[eth0]
21,The example configuration file is an equivalent of the following command's
21,options:
21,> sudo ptp4l -i eth0 -m -S
21,"For a complete list of ptp4l configuration options, see"
21,man 8 ptp4l.
21,21.2.4 Delay measurement #
21,ptp4l measures time delay in two different ways:
21,peer-to-peer (P2P) or end-to-end
21,(E2E).
21,P2P
21,This method is specified with -P.
21,It reacts to changes in the network environment faster and is more
21,accurate in measuring the delay. It is only used in networks where each
21,port exchanges PTP messages with one other port. P2P needs to be
21,supported by all hardware on the communication path.
21,E2E
21,This method is specified with -E. This is the default.
21,Automatic method selection
21,This method is specified with -A. The automatic option
21,"starts ptp4l in E2E mode, and changes to P2P mode if"
21,a peer delay request is received.
21,Important: Common measurement method
21,All clocks on a single PTP communication path must use the same method to
21,measure the time delay. A warning will be printed if either a peer delay
21,"request is received on a port using the E2E mechanism, or an E2E delay"
21,request is received on a port using the P2P mechanism.
21,21.2.5 PTP management client: pmc #
21,You can use the pmc client to obtain more detailed
21,information about ptp41. It reads from the standard
21,input—or from the command line—actions specified by name and
21,"management ID. Then it sends the actions over the selected transport, and"
21,prints any received replies. There are three actions supported:
21,"GET retrieves the specified information,"
21,"SET updates the specified information, and"
21,CMD (or COMMAND) initiates the
21,specified event.
21,"By default, the management commands are addressed to all ports. The"
21,TARGET command can be used to select a particular clock
21,and port for the subsequent messages. For a complete list of management
21,"IDs, run pmc help."
21,> sudo pmc -u -b 0 'GET TIME_STATUS_NP'
21,sending: GET TIME_STATUS_NP
21,90f2ca.fffe.20d7e9-0 seq 0 RESPONSE MANAGMENT TIME_STATUS_NP
21,master_offset
21,283
21,ingress_time
21,1361569379345936841
21,cumulativeScaledRateOffset
21,+1.000000000
21,scaledLastGmPhaseChange
21,gmTimeBaseIndicator
21,lastGmPhaseChange
21,0x0000'0000000000000000.0000
21,gmPresent
21,true
21,gmIdentity
21,00b058.feef.0b448a
21,The -b option specifies the boundary hops value in sent
21,messages. Setting it to zero limits the boundary to the local
21,ptp4l instance. Increasing the value will retrieve the
21,messages also from PTP nodes that are further from the local instance. The
21,returned information may include:
21,stepsRemoved
21,The number of communication nodes to the grandmaster clock.
21,"offsetFromMaster, master_offset"
21,The last measured offset of the clock from the master clock
21,(nanoseconds).
21,meanPathDelay
21,The estimated delay of the synchronization messages sent from the master
21,clock (nanoseconds).
21,gmPresent
21,"If true, the PTP clock is synchronized to the master"
21,clock; the local clock is not the grandmaster clock.
21,gmIdentity
21,This is the grandmaster's identity.
21,"For a complete list of pmc command line options, see"
21,man 8 pmc.
21,21.3 Synchronizing the clocks with phc2sys #
21,Use phc2sys to synchronize the system clock to the PTP
21,hardware clock (PHC) on the network card. The system clock is considered a
21,"slave, while the network card a"
21,master. PHC itself is synchronized with
21,"ptp4l (see Section 21.2, “Using PTP”). Use"
21,-s to specify the master clock by device or network
21,interface. Use -w to wait until ptp4l is
21,in a synchronized state.
21,> sudo phc2sys -s eth0 -w
21,"PTP operates in International Atomic Time (TAI), while"
21,the system clock uses Coordinated Universal Time (UTC).
21,If you do not specify -w to wait for
21,"ptp4l synchronization, you can specify the offset in"
21,seconds between TAI and UTC with -O:
21,> sudo phc2sys -s eth0 -O -35
21,You can run phc2sys as a service as well:
21,> sudo systemctl start phc2sys
21,"In this case, phc2sys reads its options from the"
21,/etc/sysconfig/phc2sys file. For more information on
21,"phc2sys options, see man 8 phc2sys."
21,"To enable the phc2sys service permanently, run the"
21,following:
21,> sudo systemctl enable phc2sys
21,"To disable it, run"
21,> sudo systemctl disable phc2sys21.3.1 Verifying time synchronization #
21,When PTP time synchronization is working properly and hardware time
21,"stamping is used, ptp4l and phc2sys"
21,output messages with time offsets and frequency adjustments periodically to
21,the system log.
21,An example of the ptp4l output:
21,ptp4l[351.358]: selected /dev/ptp0 as PTP clock
21,ptp4l[352.361]: port 1: INITIALIZING to LISTENING on INITIALIZE
21,ptp4l[352.361]: port 0: INITIALIZING to LISTENING on INITIALIZE
21,ptp4l[353.210]: port 1: new foreign master 00a069.eefe.0b442d-1
21,ptp4l[357.214]: selected best master clock 00a069.eefe.0b662d
21,ptp4l[357.214]: port 1: LISTENING to UNCALIBRATED on RS_SLAVE
21,ptp4l[359.224]: master offset
21,3304 s0 freq
21,+0 path delay
21,9202
21,ptp4l[360.224]: master offset
21,3708 s1 freq
21,-28492 path delay
21,9202
21,ptp4l[361.224]: master offset
21,-3145 s2 freq
21,-32637 path delay
21,9202
21,ptp4l[361.224]: port 1: UNCALIBRATED to SLAVE on MASTER_CLOCK_SELECTED
21,ptp4l[362.223]: master offset
21,-145 s2 freq
21,-30580 path delay
21,9202
21,ptp4l[363.223]: master offset
21,1043 s2 freq
21,-28436 path delay
21,8972
21,[...]
21,ptp4l[371.235]: master offset
21,285 s2 freq
21,-28511 path delay
21,9199
21,ptp4l[372.235]: master offset
21,-78 s2 freq
21,-28788 path delay
21,9204
21,An example of the phc2sys output:
21,phc2sys[616.617]: Waiting for ptp4l...
21,phc2sys[628.628]: phc offset
21,66341 s0 freq
21,+0 delay
21,2729
21,phc2sys[629.628]: phc offset
21,64668 s1 freq
21,-37690 delay
21,2726
21,[...]
21,phc2sys[646.630]: phc offset
21,-333 s2 freq
21,-37426 delay
21,2747
21,phc2sys[646.630]: phc offset
21,194 s2 freq
21,-36999 delay
21,2749
21,ptp4l normally writes messages very frequently. You can
21,reduce the frequency with the summary_interval
21,"directive. Its value is an exponent of the 2^N expression. For example, to"
21,"reduce the output to every 1024 (which is equal to 2^10) seconds, add the"
21,following line to the /etc/ptp4l.conf file:
21,summary_interval 10
21,You can also reduce the frequency of the phc2sys
21,command's updates with the -u
21,SUMMARY-UPDATES option.
21,21.4 Examples of configurations #
21,This section includes several examples of ptp4l
21,configuration. The examples are not full configuration files but rather a
21,minimal list of changes to be made to the specific files. The string
21,ethX stands for the actual network interface name
21,in your setup.
21,Example 21.1: Slave clock using software time stamping #
21,/etc/sysconfig/ptp4l:
21,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
21,No changes made to the distribution /etc/ptp4l.conf.
21,Example 21.2: Slave clock using hardware time stamping #
21,/etc/sysconfig/ptp4l:
21,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
21,/etc/sysconfig/phc2sys:
21,OPTIONS=”-s ethX -w”
21,No changes made to the distribution /etc/ptp4l.conf.
21,Example 21.3: Master clock using hardware time stamping #
21,/etc/sysconfig/ptp4l:
21,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
21,/etc/sysconfig/phc2sys:
21,OPTIONS=”-s CLOCK_REALTIME -c ethX -w”
21,/etc/ptp4l.conf:
21,priority1 127Example 21.4: Master clock using software time stamping (not generally recommended) #
21,/etc/sysconfig/ptp4l:
21,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
21,/etc/ptp4l.conf:
21,priority1 12721.5 PTP and NTP #
21,"NTP and PTP time synchronization tools can coexist, synchronizing time from"
21,one to another in both directions.
21,21.5.1 NTP to PTP synchronization #
21,"When chronyd is used to synchronize the local system clock, you can"
21,configure the ptp4l to be the grandmaster clock
21,distributing the time from the local system clock via PTP. Include the
21,priority1 option in /etc/ptp4l.conf:
21,[global]
21,priority1 127
21,[eth0]
21,Then run ptp4l:
21,> sudo ptp4l -f /etc/ptp4l.conf
21,"When hardware time stamping is used, you need to synchronize the PTP"
21,hardware clock to the system clock with phc2sys:
21,> sudo phc2sys -c eth0 -s CLOCK_REALTIME -w21.5.2 Configuring PTP-NTP bridge #
21,If a highly accurate PTP grandmaster is available in a network without
21,"switches or routers with PTP support, a computer may operate as a PTP slave"
21,and a stratum-1 NTP server. Such a computer needs to have two or more
21,"network interfaces, and be close to the grandmaster or have a direct"
21,connection to it. This will ensure highly accurate synchronization in the
21,network.
21,Configure the ptp4l and phc2sys
21,programs to use one network interface to synchronize the system clock using
21,PTP. Then configure chronyd to provide the system time using the other
21,interface:
21,bindaddress 192.0.131.47
21,hwtimestamp eth1
21,local stratum 1Note: NTP and DHCP
21,When the DHCP client command dhclient receives a list
21,"of NTP servers, it adds them to NTP configuration by default. To prevent"
21,"this behavior, set"
21,"NETCONFIG_NTP_POLICY="""""
21,in the /etc/sysconfig/network/config file.
21,A GNU licenses #
21,This appendix contains the GNU Free Documentation License version 1.2.
21,GNU Free Documentation License #
21,"Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,"
21,"Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and"
21,"distribute verbatim copies of this license document, but changing it is not"
21,allowed.
21,0. PREAMBLE
21,"The purpose of this License is to make a manual, textbook, or other"
21,"functional and useful document ""free"" in the sense of freedom: to assure"
21,"everyone the effective freedom to copy and redistribute it, with or without"
21,"modifying it, either commercially or non-commercially. Secondarily, this"
21,License preserves for the author and publisher a way to get credit for their
21,"work, while not being considered responsible for modifications made by"
21,others.
21,"This License is a kind of ""copyleft"", which means that derivative works of"
21,the document must themselves be free in the same sense. It complements the
21,"GNU General Public License, which is a copyleft license designed for free"
21,software.
21,"We have designed this License to use it for manuals for free software,"
21,because free software needs free documentation: a free program should come
21,with manuals providing the same freedoms that the software does. But this
21,License is not limited to software manuals; it can be used for any textual
21,"work, regardless of subject matter or whether it is published as a printed"
21,book. We recommend this License principally for works whose purpose is
21,instruction or reference.
21,1. APPLICABILITY AND DEFINITIONS
21,"This License applies to any manual or other work, in any medium, that"
21,contains a notice placed by the copyright holder saying it can be distributed
21,"under the terms of this License. Such a notice grants a world-wide,"
21,"royalty-free license, unlimited in duration, to use that work under the"
21,"conditions stated herein. The ""Document"", below, refers to any such manual or"
21,"work. Any member of the public is a licensee, and is addressed as ""you"". You"
21,"accept the license if you copy, modify or distribute the work in a way"
21,requiring permission under copyright law.
21,"A ""Modified Version"" of the Document means any work containing the Document"
21,"or a portion of it, either copied verbatim, or with modifications and/or"
21,translated into another language.
21,"A ""Secondary Section"" is a named appendix or a front-matter section of the"
21,Document that deals exclusively with the relationship of the publishers or
21,authors of the Document to the Document's overall subject (or to related
21,matters) and contains nothing that could fall directly within that overall
21,"subject. (Thus, if the Document is in part a textbook of mathematics, a"
21,Secondary Section may not explain any mathematics.) The relationship could be
21,"a matter of historical connection with the subject or with related matters,"
21,"or of legal, commercial, philosophical, ethical or political position"
21,regarding them.
21,"The ""Invariant Sections"" are certain Secondary Sections whose titles are"
21,"designated, as being those of Invariant Sections, in the notice that says"
21,that the Document is released under this License. If a section does not fit
21,the above definition of Secondary then it is not allowed to be designated as
21,Invariant. The Document may contain zero Invariant Sections. If the Document
21,does not identify any Invariant Sections then there are none.
21,"The ""Cover Texts"" are certain short passages of text that are listed, as"
21,"Front-Cover Texts or Back-Cover Texts, in the notice that says that the"
21,Document is released under this License. A Front-Cover Text may be at most 5
21,"words, and a Back-Cover Text may be at most 25 words."
21,"A ""Transparent"" copy of the Document means a machine-readable copy,"
21,represented in a format whose specification is available to the general
21,"public, that is suitable for revising the document straightforwardly with"
21,generic text editors or (for images composed of pixels) generic paint
21,"programs or (for drawings) some widely available drawing editor, and that is"
21,suitable for input to text formatters or for automatic translation to a
21,variety of formats suitable for input to text formatters. A copy made in an
21,"otherwise Transparent file format whose markup, or absence of markup, has"
21,been arranged to thwart or discourage subsequent modification by readers is
21,not Transparent. An image format is not Transparent if used for any
21,"substantial amount of text. A copy that is not ""Transparent"" is called"
21,"""Opaque""."
21,Examples of suitable formats for Transparent copies include plain ASCII
21,"without markup, Texinfo input format, LaTeX input format, SGML or XML using a"
21,"publicly available DTD, and standard-conforming simple HTML, PostScript or"
21,PDF designed for human modification. Examples of transparent image formats
21,"include PNG, XCF and JPG. Opaque formats include proprietary formats that can"
21,"be read and edited only by proprietary word processors, SGML or XML for which"
21,"the DTD and/or processing tools are not generally available, and the"
21,"machine-generated HTML, PostScript or PDF produced by some word processors"
21,for output purposes only.
21,"The ""Title Page"" means, for a printed book, the title page itself, plus such"
21,"following pages as are needed to hold, legibly, the material this License"
21,requires to appear in the title page. For works in formats which do not have
21,"any title page as such, ""Title Page"" means the text near the most prominent"
21,"appearance of the work's title, preceding the beginning of the body of the"
21,text.
21,"A section ""Entitled XYZ"" means a named subunit of the Document whose title"
21,either is precisely XYZ or contains XYZ in parentheses following text that
21,translates XYZ in another language. (Here XYZ stands for a specific section
21,"name mentioned below, such as ""Acknowledgements"", ""Dedications"","
21,"""Endorsements"", or ""History"".) To ""Preserve the Title"" of such a section when"
21,"you modify the Document means that it remains a section ""Entitled XYZ"""
21,according to this definition.
21,The Document may include Warranty Disclaimers next to the notice which states
21,that this License applies to the Document. These Warranty Disclaimers are
21,"considered to be included by reference in this License, but only as regards"
21,disclaiming warranties: any other implication that these Warranty Disclaimers
21,may have is void and has no effect on the meaning of this License.
21,2. VERBATIM COPYING
21,"You may copy and distribute the Document in any medium, either commercially"
21,"or non-commercially, provided that this License, the copyright notices, and"
21,the license notice saying this License applies to the Document are reproduced
21,"in all copies, and that you add no other conditions whatsoever to those of"
21,this License. You may not use technical measures to obstruct or control the
21,"reading or further copying of the copies you make or distribute. However, you"
21,may accept compensation in exchange for copies. If you distribute a large
21,enough number of copies you must also follow the conditions in section 3.
21,"You may also lend copies, under the same conditions stated above, and you may"
21,publicly display copies.
21,3. COPYING IN QUANTITY
21,If you publish printed copies (or copies in media that commonly have printed
21,"covers) of the Document, numbering more than 100, and the Document's license"
21,"notice requires Cover Texts, you must enclose the copies in covers that"
21,"carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the"
21,"front cover, and Back-Cover Texts on the back cover. Both covers must also"
21,clearly and legibly identify you as the publisher of these copies. The front
21,cover must present the full title with all words of the title equally
21,prominent and visible. You may add other material on the covers in addition.
21,"Copying with changes limited to the covers, as long as they preserve the"
21,"title of the Document and satisfy these conditions, can be treated as"
21,verbatim copying in other respects.
21,"If the required texts for either cover are too voluminous to fit legibly, you"
21,should put the first ones listed (as many as fit reasonably) on the actual
21,"cover, and continue the rest onto adjacent pages."
21,If you publish or distribute Opaque copies of the Document numbering more
21,"than 100, you must either include a machine-readable Transparent copy along"
21,"with each Opaque copy, or state in or with each Opaque copy a"
21,computer-network location from which the general network-using public has
21,access to download using public-standard network protocols a complete
21,"Transparent copy of the Document, free of added material. If you use the"
21,"latter option, you must take reasonably prudent steps, when you begin"
21,"distribution of Opaque copies in quantity, to ensure that this Transparent"
21,copy will remain thus accessible at the stated location until at least one
21,year after the last time you distribute an Opaque copy (directly or through
21,your agents or retailers) of that edition to the public.
21,"It is requested, but not required, that you contact the authors of the"
21,"Document well before redistributing any large number of copies, to give them"
21,a chance to provide you with an updated version of the Document.
21,4. MODIFICATIONS
21,You may copy and distribute a Modified Version of the Document under the
21,"conditions of sections 2 and 3 above, provided that you release the Modified"
21,"Version under precisely this License, with the Modified Version filling the"
21,"role of the Document, thus licensing distribution and modification of the"
21,"Modified Version to whoever possesses a copy of it. In addition, you must do"
21,these things in the Modified Version:
21,"Use in the Title Page (and on the covers, if any) a title distinct from"
21,"that of the Document, and from those of previous versions (which should, if"
21,"there were any, be listed in the History section of the Document). You may"
21,use the same title as a previous version if the original publisher of that
21,version gives permission.
21,"List on the Title Page, as authors, one or more persons or entities"
21,"responsible for authorship of the modifications in the Modified Version,"
21,together with at least five of the principal authors of the Document (all
21,"of its principal authors, if it has fewer than five), unless they release"
21,you from this requirement.
21,"State on the Title page the name of the publisher of the Modified Version,"
21,as the publisher.
21,Preserve all the copyright notices of the Document.
21,Add an appropriate copyright notice for your modifications adjacent to the
21,other copyright notices.
21,"Include, immediately after the copyright notices, a license notice giving"
21,the public permission to use the Modified Version under the terms of this
21,"License, in the form shown in the Addendum below."
21,Preserve in that license notice the full lists of Invariant Sections and
21,required Cover Texts given in the Document's license notice.
21,Include an unaltered copy of this License.
21,"Preserve the section Entitled ""History"", Preserve its Title, and add to it"
21,"an item stating at least the title, year, new authors, and publisher of the"
21,Modified Version as given on the Title Page. If there is no section
21,"Entitled ""History"" in the Document, create one stating the title, year,"
21,"authors, and publisher of the Document as given on its Title Page, then add"
21,an item describing the Modified Version as stated in the previous sentence.
21,"Preserve the network location, if any, given in the Document for public"
21,"access to a Transparent copy of the Document, and likewise the network"
21,locations given in the Document for previous versions it was based on.
21,"These may be placed in the ""History"" section. You may omit a network"
21,location for a work that was published at least four years before the
21,"Document itself, or if the original publisher of the version it refers to"
21,gives permission.
21,"For any section Entitled ""Acknowledgements"" or ""Dedications"", Preserve the"
21,"Title of the section, and preserve in the section all the substance and"
21,tone of each of the contributor acknowledgements and/or dedications given
21,therein.
21,"Preserve all the Invariant Sections of the Document, unaltered in their"
21,text and in their titles. Section numbers or the equivalent are not
21,considered part of the section titles.
21,"Delete any section Entitled ""Endorsements"". Such a section may not be"
21,included in the Modified Version.
21,"Do not retitle any existing section to be Entitled ""Endorsements"" or to"
21,conflict in title with any Invariant Section.
21,Preserve any Warranty Disclaimers.
21,If the Modified Version includes new front-matter sections or appendices that
21,qualify as Secondary Sections and contain no material copied from the
21,"Document, you may at your option designate some or all of these sections as"
21,"invariant. To do this, add their titles to the list of Invariant Sections in"
21,the Modified Version's license notice. These titles must be distinct from any
21,other section titles.
21,"You may add a section Entitled ""Endorsements"", provided it contains nothing"
21,"but endorsements of your Modified Version by various parties--for example,"
21,statements of peer review or that the text has been approved by an
21,organization as the authoritative definition of a standard.
21,"You may add a passage of up to five words as a Front-Cover Text, and a"
21,"passage of up to 25 words as a Back-Cover Text, to the end of the list of"
21,Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
21,one of Back-Cover Text may be added by (or through arrangements made by) any
21,"one entity. If the Document already includes a cover text for the same cover,"
21,previously added by you or by arrangement made by the same entity you are
21,"acting on behalf of, you may not add another; but you may replace the old"
21,"one, on explicit permission from the previous publisher that added the old"
21,one.
21,The author(s) and publisher(s) of the Document do not by this License give
21,permission to use their names for publicity for or to assert or imply
21,endorsement of any Modified Version.
21,5. COMBINING DOCUMENTS
21,You may combine the Document with other documents released under this
21,"License, under the terms defined in section 4 above for modified versions,"
21,provided that you include in the combination all of the Invariant Sections of
21,"all of the original documents, unmodified, and list them all as Invariant"
21,"Sections of your combined work in its license notice, and that you preserve"
21,all their Warranty Disclaimers.
21,"The combined work need only contain one copy of this License, and multiple"
21,identical Invariant Sections may be replaced with a single copy. If there are
21,"multiple Invariant Sections with the same name but different contents, make"
21,"the title of each such section unique by adding at the end of it, in"
21,"parentheses, the name of the original author or publisher of that section if"
21,"known, or else a unique number. Make the same adjustment to the section"
21,titles in the list of Invariant Sections in the license notice of the
21,combined work.
21,"In the combination, you must combine any sections Entitled ""History"" in the"
21,"various original documents, forming one section Entitled ""History""; likewise"
21,"combine any sections Entitled ""Acknowledgements"", and any sections Entitled"
21,"""Dedications"". You must delete all sections Entitled ""Endorsements""."
21,6. COLLECTIONS OF DOCUMENTS
21,You may make a collection consisting of the Document and other documents
21,"released under this License, and replace the individual copies of this"
21,License in the various documents with a single copy that is included in the
21,"collection, provided that you follow the rules of this License for verbatim"
21,copying of each of the documents in all other respects.
21,"You may extract a single document from such a collection, and distribute it"
21,"individually under this License, provided you insert a copy of this License"
21,"into the extracted document, and follow this License in all other respects"
21,regarding verbatim copying of that document.
21,7. AGGREGATION WITH INDEPENDENT WORKS
21,A compilation of the Document or its derivatives with other separate and
21,"independent documents or works, in or on a volume of a storage or"
21,"distribution medium, is called an ""aggregate"" if the copyright resulting from"
21,the compilation is not used to limit the legal rights of the compilation's
21,users beyond what the individual works permit. When the Document is included
21,"in an aggregate, this License does not apply to the other works in the"
21,aggregate which are not themselves derivative works of the Document.
21,If the Cover Text requirement of section 3 is applicable to these copies of
21,"the Document, then if the Document is less than one half of the entire"
21,"aggregate, the Document's Cover Texts may be placed on covers that bracket"
21,"the Document within the aggregate, or the electronic equivalent of covers if"
21,the Document is in electronic form. Otherwise they must appear on printed
21,covers that bracket the whole aggregate.
21,8. TRANSLATION
21,"Translation is considered a kind of modification, so you may distribute"
21,translations of the Document under the terms of section 4. Replacing
21,Invariant Sections with translations requires special permission from their
21,"copyright holders, but you may include translations of some or all Invariant"
21,Sections in addition to the original versions of these Invariant Sections.
21,"You may include a translation of this License, and all the license notices in"
21,"the Document, and any Warranty Disclaimers, provided that you also include"
21,the original English version of this License and the original versions of
21,those notices and disclaimers. In case of a disagreement between the
21,translation and the original version of this License or a notice or
21,"disclaimer, the original version will prevail."
21,"If a section in the Document is Entitled ""Acknowledgements"", ""Dedications"","
21,"or ""History"", the requirement (section 4) to Preserve its Title (section 1)"
21,will typically require changing the actual title.
21,9. TERMINATION
21,"You may not copy, modify, sublicense, or distribute the Document except as"
21,"expressly provided for under this License. Any other attempt to copy, modify,"
21,"sublicense or distribute the Document is void, and will automatically"
21,"terminate your rights under this License. However, parties who have received"
21,"copies, or rights, from you under this License will not have their licenses"
21,terminated so long as such parties remain in full compliance.
21,10. FUTURE REVISIONS OF THIS LICENSE
21,"The Free Software Foundation may publish new, revised versions of the GNU"
21,Free Documentation License from time to time. Such new versions will be
21,"similar in spirit to the present version, but may differ in detail to address"
21,new problems or concerns. See
21,https://www.gnu.org/copyleft/.
21,Each version of the License is given a distinguishing version number. If the
21,"Document specifies that a particular numbered version of this License ""or any"
21,"later version"" applies to it, you have the option of following the terms and"
21,conditions either of that specified version or of any later version that has
21,been published (not as a draft) by the Free Software Foundation. If the
21,"Document does not specify a version number of this License, you may choose"
21,any version ever published (not as a draft) by the Free Software Foundation.
21,ADDENDUM: How to use this License for your documents
21,#Copyright (c) YEAR YOUR NAME.
21,"Permission is granted to copy, distribute and/or modify this document"
21,"under the terms of the GNU Free Documentation License, Version 1.2"
21,or any later version published by the Free Software Foundation;
21,"with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts."
21,A copy of the license is included in the section entitled “GNU
21,Free Documentation License”.
21,"If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,"
21,replace the “with...Texts.” line with this:
21,"with the Invariant Sections being LIST THEIR TITLES, with the"
21,"Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST."
21,"If you have Invariant Sections without Cover Texts, or some other combination"
21,"of the three, merge those two alternatives to suit the situation."
21,"If your document contains nontrivial examples of program code, we recommend"
21,releasing these examples in parallel under your choice of free software
21,"license, such as the GNU General Public License, to permit their use in free"
21,software.
21,Share this page
21,© SUSE
21,2024
22,"Oracle Database SQL Tuning Guide, 19c"
22,Previous
22,Next
22,JavaScript must be enabled to correctly display this content
22,SQL Tuning Guide
22,Oracle® Database
22,Oracle® Database
22,SQL Tuning Guide
22,19c
22,E96095-18
22,January 2024
22,Title and Copyright Information
22,"Oracle Database SQL Tuning Guide, 19c"
22,E96095-18
22,"Copyright © 2013, 2024, Oracle and/or its affiliates."
22,Primary Author: Lance Ashdown
22,"Contributing Authors: Nigel Bayliss, Maria Colgan, Tom Kyte"
22,"Contributors: Hermann Baer, Bjorn Bolltoft, Ali Cakmak, Sunil Chakkappen, Immanuel Chan, Deba Chatterjee, Chris Chiappa, Dinesh Das, Kurt Engeleiter, Leonidas Galanis, William Endress, Marcus Fallen, Bruce Golbus, Katsumi Inoue, Praveen Kumar Tupati Jaganath, Mark Jefferys, Shantanu Joshi, Adam Kociubes, Keith Laker, Allison Lee, Sue Lee, Cheng Li, David McDermid, Colin McGregor, Ajit Mylavarapu, Ted Persky, Lei Sheng, Ekrem Soylemez, Hong Su, Murali Thiyagarajah, Randy Urbano, Sahil Vazirani, Bharath Venkatakrishnan, Hailing Yu, John Zimmerman, Frederick Kush"
24,Performance recommendations - MoodleDocs
24,Forums
24,Documentation
24,Downloads
24,Demo
24,Tracker
24,Development
24,Translation
24,Search
24,Search
24,Moodle Sites
24,What are you looking for?
24,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
24,Moodle.com
24,Our social network to share and curate open educational resources.
24,MoodleNet
24,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
24,Moodle Academy
24,Moodle.com
24,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
24,MoodleNet
24,Our social network to share and curate open educational resources.
24,Moodle Academy
24,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
24,Documentation
24,Menu
24,Main pageTable of contentsDocs overviewRecent changes
24,Log in
24,4.3 docs4.2 docs
24,4.1 docs
24,Article
24,Page Comments
24,View source
24,View history
24,Performance recommendations
24,"From MoodleDocsJump to:navigation, search"
24,Main page ► Managing a Moodle site ► Performance ► Performance recommendations
24,Performance
24,Performance recommendations
24,Performance settings
24,Performance overview
24,Caching
24,Performance FAQ
24,MUC FAQ
24,"Moodle can be made to perform very well, at small usage levels or scaling up to many thousands of users. The factors involved in performance are basically the same as for any PHP-based database-driven system. When trying to optimize your server, try to focus on the factor which will make the most difference to the user. For example, if you have relatively more users browsing than accessing the database, look to improve the webserver performance."
24,Contents
24,1 Obtain a baseline benchmark
24,2 Scalability
24,2.1 Server cluster
24,3 Hardware configuration
24,4 Operating System
24,5 Caching Performance
24,6 Web Server Performance
24,6.1 PHP Performance
24,6.1.1 APC
24,6.2 Apache Performance
24,6.3 IIS Performance
24,6.4 OpenLiteSpeed
24,"6.5 Lighttpd, NginX and Cherokee Performance"
24,6.6 X-Sendfile
24,7 Cron Performance
24,8 Database Performance
24,8.1 MariaDB Performance
24,8.2 MySQL Performance
24,8.3 PostgreSQL Performance
24,8.4 Read replicas
24,8.5 Other database performance links
24,9 Performance of different Moodle modules
24,10 See also
24,Obtain a baseline benchmark
24,"Before attempting any optimization, you should obtain a baseline benchmark of the component of the system you are trying to improve. For Linux try LBS (Note: Last updated May 2002) and for Windows use the Performance Monitor. Once you have quantitative data about how your system is performing currently, you'll be able to determine if the change you have made has had any real impact."
24,"The overall aim of adjustments to improve performance is to use RAM (cacheing) and to reduce disk-based activity. It is especially important to try to eliminate swap file usage as much as you can. If your system starts swapping, this is a sign that you need more RAM."
24,"The optimization order preference is usually: primary storage (more RAM), secondary storage (faster hard disks/improved hard disk configuration), processor (more and faster)."
24,It can be interesting to install and use the Benchmark plugin in order to find the bottlenecks of your system that specifically affect Moodle or do a load test / stress test with tool like JMeter. See moodledev JMeter documentation
24,Scalability
24,Moodle's design (with clear separation of application layers) allows for strongly scalable setups. (Please check the list of large Moodle installations.)
24,"Large sites usually separate the web server and database onto separate servers, although for smaller installations this is typically not necessary."
24,"It is possible to load-balance a Moodle installation, for example by using more than one webserver. The separate webservers should query the same database and refer to the same filestore and cache areas (see Caching), but otherwise the separation of the application layers is complete enough to make this kind of clustering feasible. Similarly, the database could be a cluster of servers (e.g. a MySQL cluster), but this is not an easy task and you should seek expert support, e.g. from a Moodle Partner."
24,"On very large, load-balanced, systems the performance of the shared components become critical. It's important that your shared file areas are properly tuned and that you use an effective cache (Redis is highly recommended). A good understanding of these areas of system administration should be considered a minimum requirement."
24,Server cluster
24,Using Moodle forum discussions:
24,Moodle clustering
24,Software load balancing
24,TCP load balancing
24,Installation for 3000 simultaneous users
24,Hardware configuration
24,Note: The fastest and most effective change that you can make to improve performance is to increase the amount of RAM on your web server - get as much as possible (e.g. 4GB or more). Increasing primary memory will reduce the need for processes to swap to disk and will enable your server to handle more users.
24,"Better performance is gained by obtaining the best processor capability you can, i.e. dual or dual core processors. A modern BIOS should allow you to enable hyperthreading, but check if this makes a difference to the overall performance of the processors by using a CPU benchmarking tool."
24,"If you can afford them, use SCSI hard disks instead of SATA drives. SATA drives will increase your system's CPU utilization, whereas SCSI drives have their own integrated processors and come into their own when you have multiple drives. If you must have SATA drives, check that your motherboard and the drives themselves support NCQ (Native Command Queuing)."
24,"Purchase hard disks with a low seek time. This will improve the overall speed of your system, especially when accessing Moodle's reports. Naturally these days Solid State Drives outperform rotating media immensely, especially Enterprise-Grade SSD's."
24,Size your swap file correctly. The general advice is to set it to 4 x physical RAM.
24,"Use a RAID disk system. Although there are many different RAID configurations you can create, the following generally works best:"
24,install a hardware RAID controller (if you can)
24,the operating system and swap drive on one set of disks configured as RAID-1.
24,"Moodle, Web server and Database server on another set of disks configured as RAID-5."
24,"If your 'moodleData' area is going to be on relatively slow storage (e.g. NFS mount on to a NAS device) you will have performance issues with the default cache configuration (which writes to this storage). See the page on Caching and choose an alternative. Redis is recommended. Using GlusterFS / OCFS2 / GFS2 on a SAN device and Fiber Channel could improve performance (See more info on the Moodle forum thread, NFS performance tuing )"
24,Use gigabit ethernet for improved latency and throughput. This is especially important when you have your webserver and database server separated out on different hosts.
24,Check the settings on your network card. You may get an improvement in performance by increasing the use of buffers and transmit/receive descriptors (balance this with processor and memory overheads) and off-loading TCP checksum calculation onto the card instead of the OS.
24,Read this Case Study on a server stress test with 300 users.
24,See this accompanying report on network traffic and server loads.
24,Also see this SFSU presentation at Educause (using VMWare): [1]
24,Operating System
24,"You can use Linux(recommended), Unix-based, Windows or Mac OS X for the server operating system. *nix operating systems generally require less memory than Mac OS X or Windows servers for doing the same task as the server is configured with just a shell interface. Additionally Linux does not have licensing fees attached, but can have a big learning curve if you're used to another operating system. If you have a large number of processors running SMP, you may also want to consider using a highly tuned OS such as Solaris."
24,Check your own OS and vendor specific instructions for optimization steps.
24,For Linux look at the Linux Performance Team site.
24,"For Linux investigate the hdparm command, e.g. hdparm -m16 -d1 can be used to enable read/write on multiple sectors and DMA. Mount disks with the ""async"" and ""noatime"" options."
24,"For Windows set the sever to be optimized for network applications (Control Panel, Network Connections, LAN connection, Properties, File & Printer Sharing for Microsoft Networks, Properties, Optimization). You can also search the Microsoft TechNet site for optimization documents."
24,Caching Performance
24,"Caching in Moodle can default to disk for a lot of the different caches which is rather slow overall, and so pretty solid gains can be made by moving this to RAM, by use of a Memory Caching Application such as Redis or Memcached. In fact I'd go as far to say the single biggest improvement we made to our (relatively small) Moodle site was installing Redis, and this is amplified when you're using classic Hard Drives rather than SSD's, and especially when they slowly but surely begin to fail (the classic slow to write, but no SMART errors or write errors - just reeeeaaallly slow)."
24,"These will also cache some database queries, meaning that they don't have to be re-run, again improving performance there. Personally, I would recommend Redis over Memcached due to better security features and being more up to date/developed. For more information/how to install Redis in particular, visit the Redis cache store page."
24,Web Server Performance
24,"Most web browsers these days feature Inspector elements which will allow you to watch the time it takes for each page component to load, typically found under the ""Network"" tab. Also, the Yslow extension will evaluate your page against Yahoo's 14 rules, full text Best Practices for Speeding Up Your Web Site, (video) for fast loading websites."
24,PHP Performance
24,"PHP contains a built-in accelerator (for more recent versions of PHP, this is OpCache). Make sure it is enabled."
24,Improvements in read/write performance can be improved by putting the cached PHP pages on a TMPFS filesystem - but remember that you'll lose the cache contents when there is a power failure or the server is rebooted.
24,Performance of PHP is better when installed as an Apache/IIS6 ISAPI module (rather than a CGI). IIS 7.0/7.5 (Windows Server 2008/R2) users should choose a FastCGI installation for best performance.
24,"Also check the memory_limit in php.ini. The default value for the memory_limit directive is 128M. On some sites, it may need to be larger - especially for some backup operations."
24,Also see PHP settings by Moodle version
24,Use PHP-FPM (with apache).
24,APC
24,APC on CentOS 5.x (linux)
24,APC on Debian (linux)
24,Apache Performance
24,"If you are using Apache on a Windows server, use the build from Apache Lounge which is reported to have performance and stability improvements compared to the official Apache download. Note that this is an unofficial build, so may not keep up with official releases."
24,Set the MaxRequestWorkers directive correctly (MaxClients before Apache 2.4). Use this formula to help (which uses 80% of available memory to leave room for spare):
24,MaxRequestWorkers = Total available memory * 80% / Max memory usage of apache process
24,"Memory usage of apache process is usually 10MB but Moodle can easily use up to 100MB per process, so a general rule of thumb is to divide your available memory in megabytes by 100 to get a conservative setting for MaxClients. You are quite likely to find yourself lowering the MaxRequestWorkers from its default of 150 on a Moodle server. To get a more accurate estimate read the value from the shell command:"
24,#ps -ylC httpd --sort:rss
24,"If you need to increase the value of MaxRequestWorkers beyond 256, you will also need to set the ServerLimit directive."
24,Warning: Do not be tempted to set the value of MaxRequestWorkers higher than your available memory as your server will consume more RAM than available and start to swap to disk.
24,Consider reducing the number of modules that Apache loads in the httpd.conf file to the minimum necessary to reduce the memory needed.
24,Use the latest version of Apache - Apache 2 has an improved memory model which reduces memory usage further.
24,"For Unix/Linux systems, consider lowering MaxConnectionsPerChild (MaxRequestsPerChild before Apache 2.4) in httpd.conf to as low as 20-30 (if you set it any lower the overhead of forking begins to outweigh the benefits)."
24,"For a heavily loaded server, consider setting KeepAlive Off (do this only if your Moodle pages do not contain links to resources or uploaded images) or lowering the KeepAliveTimeout to between 2 and 5. The default is 15 (seconds) - the higher the value the more server processes will be kept waiting for possibly idle connections. A more accurate value for KeepAliveTimeout is obtained by observing how long it takes your users to download a page. After altering any of the KeepAlive variables, monitor your CPU utilization as there may be an additional overhead in initiating more worker processes/threads."
24,"As an alternative to using KeepAlive Off, consider setting-up a Reverse Proxy server in front of the Moodle server to cache HTML files with images. You can then return Apache to using keep-alives on the Moodle server."
24,"If you do not use a .htaccess file, set the AllowOverride variable to AllowOverride None to prevent .htaccess lookups."
24,Set DirectoryIndex correctly so as to avoid content-negotiation. Here's an example from a production server:
24,DirectoryIndex index.php index.html index.htm
24,"Unless you are doing development work on the server, set ExtendedStatus Off and disable mod_info as well as mod_status."
24,Leave HostnameLookups Off (as default) to reduce DNS latency.
24,Consider reducing the value of TimeOut to between 30 and 60 (seconds).
24,"For the Options directive, avoid Options Multiviews as this performs a directory scan. To reduce disk I/O further use"
24,Options -Indexes FollowSymLinks
24,Compression reduces response times by reducing the size of the HTTP response
24,Install and enable mod_deflate - refer to documentation or man pages
24,Add this code to the virtual server config file within the <directory> section for the root directory (or within the .htaccess file if AllowOverrides is On):
24,<ifModule mod_deflate.c>
24,AddOutputFilterByType DEFLATE text/html text/plain text/xml text/x-js text/javascript text/css application/javascript
24,</ifmodule>
24,Use Apache event MPM (and not the default Prefork or Worker)
24,IIS Performance
24,All alter this location in the registry:
24,HKLM\SYSTEM\CurrentControlSet\Services\Inetinfo\Parameters\
24,The equivalent to KeepAliveTimeout is ListenBackLog (IIS - registry location is HKLM\ SYSTEM\ CurrentControlSet\ Services\ Inetinfo\ Parameters). Set this to between 2 and 5.
24,Change the MemCacheSize value to adjust the amount of memory (Mb) that IIS will use for its file cache (50% of available memory by default).
24,"Change the MaxCachedFileSize to adjust the maximum size of a file cached in the file cache in bytes. Default is 262,144 (256K)."
24,"Create a new DWORD called ObjectCacheTTL to change the length of time (in milliseconds) that objects in the cache are held in memory. Default is 30,000 milliseconds (30 seconds)."
24,OpenLiteSpeed
24,"OpenLiteSpeed has its own built in cache called LSCache, which is controlled through the Web GUI, and also is compatible with PHP OpCache. More info on optimizing OpenLiteSpeed can be found on the OpenLiteSpeed page."
24,"Lighttpd, NginX and Cherokee Performance"
24,"You can increase server performance by using a light-weight webserver like lighttpd, nginx or cherokee in combination with PHP in FastCGI-mode. Lighttpd was originally created as a proof-of-concept [2] to address the C10k problem and while primarily recommended for memory-limited servers, its design origins and asynchronous-IO model make it a suitable and proven [3] alternative HTTP server for high-load websites and web apps, including Moodle. See the MoodleDocs Lighttpd page for additional information, configuration example and links."
24,"Alternatively, both lighttpd and nginx are capable of performing as a load-balancer and/or reverse-proxy to alleviate load on back-end servers [4], providing benefit without requiring an actual software change on existing servers."
24,Do note that these are likely to be the least tested server environments of all particularly if you are using advanced features such as web services and/or Moodle Networking. They are probably best considered for heavily used Moodle sites with relatively simple configurations.
24,X-Sendfile
24,X-Sendfile modules improve performance when sending large files from Moodle. It is recommended to configure your web server and Moodle to use this feature if available.
24,Configure web server:
24,Apache - https://tn123.org/mod_xsendfile/
24,Lighttpd - http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file
24,Nginx - http://wiki.nginx.org/XSendfile
24,OpenLiteSpeed - https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:config:internal-redirect
24,Enable support in config.php (see config-dist.php):
24,$CFG->xsendfile = 'X-Sendfile';
24,// Apache {@see https://tn123.org/mod_xsendfile/}
24,$CFG->xsendfile = 'X-LIGHTTPD-send-file'; // Lighttpd {@see http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file}
24,$CFG->xsendfile = 'X-Accel-Redirect';
24,// Nginx {@see http://wiki.nginx.org/XSendfile}
24,Configure file location prefixes if your server implementation requires it:
24,$CFG->xsendfilealiases = array(
24,"'/dataroot/' => $CFG->dataroot,"
24,"'/cachedir/' => '/var/www/moodle/cache',"
24,// for custom $CFG->cachedir locations
24,"'/localcachedir/' => '/var/local/cache',"
24,// for custom $CFG->localcachedir locations
24,'/tempdir/'
24,"=> '/var/www/moodle/temp',"
24,// for custom $CFG->tempdir locations
24,'/filedir'
24,"=> '/var/www/moodle/filedir',"
24,// for custom $CFG->filedir locations
24,Cron Performance
24,"Cron is a very important part of the overall performance of Moodle as many asynchronous processes are offloaded to Cron, so it needs to be running and have enough through put to handle the work being given to it by the front ends."
24,See Cron with Unix or Linux#High performance cron tasks
24,Database Performance
24,MariaDB Performance
24,"MariaDB Optimizations are similar to MySQL, but at the same time different due to the way MariaDB operates. Performance as a whole is typically better than MySQL using MariaDB, so if you're looking for Database Optimization, potentially switching from MySQL to MariaDB may help with performance, otherwise if you're already using MariaDB and looking to Optimize it, Performance Recommendations can be found on the MariaDB Page."
24,MySQL Performance
24,"The number one thing you can do to improve MySQL performance is to read, understand and implement the recommendations in the Innodb Buffer Pool article."
24,"The buffer pool size can safely be changed while your server is running, as long as your server has enough memory (RAM) to accommodate the value you set. On a machine that is dedicated to MySQL, you can safely set this value to 80% of available memory."
24,"Consider setting innodb_buffer_pool_instances to the number of cores, vCPUs, or chips you have available. Adjust this value in accordance with the recommendations in the MySQL documentation."
24,The following are MySQL specific settings which can be adjusted for better performance in your my.cnf (my.ini in Windows). The file contains a list of settings and their values. To see the current values use these commands
24,SHOW STATUS;
24,SHOW VARIABLES;
24,"Important: You must make backups of your database before attempting to change any MySQL server configuration. After any change to the my.cnf, restart mysqld."
24,"If you are able, the MySQLTuner tool can be run against your MySQL server and will calculate appropriate configuration values for most of the following settings based on your current load, status and variables automatically."
24,Enable the query cache with
24,query_cache_type = 1.
24,"For most Moodle installs, set the following:"
24,query_cache_size = 36M
24,query_cache_min_res_unit = 2K.
24,The query cache will improve performance if you are doing few updates on the database.
24,Set the table cache correctly. For Moodle 1.6 set
24,table_cache = 256 #(table_open_cache in MySQL > 5.1.2)
24,"(min), and for Moodle 1.7 set"
24,table_cache = 512 #(table_open_cache in MySQL > 5.1.2)
24,"(min). The table cache is used by all threads (connections), so monitor the value of opened_tables to further adjust - if opened_tables > 3 * table_cache(table_open_cache in MySQL > 5.1.2) then increase table_cache up to your OS limit. Note also that the figure for table_cache will also change depending on the number of modules and plugins you have installed. Find the number for your server by executing the mysql statement below. Look at the number returned and set table_cache to this value."
24,mysql>SELECT COUNT(table_name) FROM information_schema.tables WHERE table_schema='yourmoodledbname';
24,Set the thread cache correctly. Adjust the value so that your thread cache utilization is as close to 100% as possible by this formula:
24,thread cache utilization (%) = (threads_created / connections) * 100
24,"The key buffer can improve the access speed to Moodle's SELECT queries. The correct size depends on the size of the index files (.myi) and in Moodle 1.6 or later (without any additional modules and plugins), the recommendation for this value is key_buffer_size = 32M. Ideally you want the database to be reading once from the disk for every 100 requests so monitor that the value is suitable for your install by adjusting the value of key_buffer_size so that the following formulas are true:"
24,key_read / key_read_requests < 0.01
24,key_write / key_write_requests <= 1.0
24,"Set the maximum number of connections so that your users will not see a ""Too many connections"" message. Be careful that this may have an impact on the total memory used. MySQL connections usually last for milliseconds, so it is unusual even for a heavily loaded server for this value to be over 200."
24,Manage high burst activity. If your Moodle install uses a lot of quizzes and you are experiencing performance problems (check by monitoring the value of threads_connected - it should not be rising) consider increasing the value of back_log.
24,"Optimize your tables weekly and after upgrading Moodle. It is good practice to also optimize your tables after performing a large data deletion exercise, e.g. at the end of your semester or academic year. This will ensure that index files are up to date. Backup your database first and then use:"
24,mysql>CHECK TABLE mdl_tablename;
24,mysql>OPTIMIZE TABLE mdl_tablename;
24,"The common tables in Moodle to check are mdl_course_sections, mdl_forum_posts, mdl_log and mdl_sessions (if using dbsessions). Any errors need to be corrected using REPAIR TABLE (see the MySQL manual and this forum script)."
24,Maintain the key distribution. Every month or so it is a good idea to stop the mysql server and run these myisamchk commands.
24,#myisamchk -a -S /pathtomysql/data/moodledir/*.MYI
24,"Warning: You must stop the mysql database process (mysqld) before running any myisamchk command. If you do not, you risk data loss."
24,Reduce the number of temporary tables saved to disk. Check this with the created_tmp_disk_tables value. If this is relatively large (>5%) increase tmp_table_size until you see a reduction. Note that this will have an impact on RAM usage.
24,PostgreSQL Performance
24,"There are some good papers around on tuning PostgreSQL (like this one), and Moodle's case does not seem to be different to the general case."
24,The first thing to recognise is that if you really need to worry about tuning you should be using a separate machine for the database server. If you are not using a separate machine then the answers to many performance questions are substantially muddied by the memory requirements of the rest of the application.
24,"You should probably enable autovacuum, unless you know what you are doing. Many e-learning sites have predictable periods of low use, so disabling autovacuum and running a specific vacuum at those times can be a good option. Or perhaps leave autovacuum running but do a full vacuum weekly in a quiet period."
24,"Set shared_buffers to something reasonable. For versions up to 8.1 my testing has shown that peak performance is almost always obtained with buffers < 10000, so if you are using such a version, and have more than 512M of RAM just set shared_buffers to 10,000 (8MB)."
24,"The buffer management had a big overhaul in 8.2 and ""reasonable"" is now a much larger number. I have not conducted performance tests with 8.2, but the recommendations from others are generally that you should now scale shared_buffers much more with memory and may continue to reap benefits even up to values like 100,000 (80MB). Consider using 1-2% of system RAM."
24,"PostgreSQL will also assume that the operating system is caching its files, so setting effective_cache_size to a reasonable value is also a good idea. A reasonable value will usually be (total RAM - RAM in use by programs). If you are running Linux and leave the system running for a day or two you can look at 'free' and under the 'cached' column you will see what it currently is. Consider taking that number (which is kB) and dividing it by 10 (i.e. allow 20% for other programs cache needs and then divide by 8 to get pages). If you are not using a dedicated database server you will need to decrease that value to account for usage by other programs."
24,"Some other useful parameters that can have positive effects, and the values I would typically set them to on a machine with 4G RAM, are:"
24,work_mem = 10240
24,"That's 10M of RAM to use instead of on-disk sorting and so forth. That can give a big speed increase, but it is per connection and 200 connections * 10M is 2G, so it can theoretically chew up a lot of RAM."
24,maintenance_work_mem = 163840
24,"That's 160M of RAM which will be used by (e.g.) VACUUM, index rebuild, cluster and so forth. This should only be used periodically and should be freed when those processes exit, so I believe it is well worth while."
24,wal_buffers = 64
24,"These buffers are used for the write-ahead log, and there have been a number of reports on the PostgreSQL mailing lists of improvement from this level of increase."
24,This is a little out of date now (version 8.0) but still worth a read: http://www.powerpostgresql.com/Docs
24,And there is lots of good stuff here as well: http://www.varlena.com/GeneralBits/Tidbits/index.php
24,Based on Andrew McMillan's post at Tuning PostgreSQL forum thread.
24,Splitting mdl_log to several tables and using a VIEW with UNION to read them as one. (See Tim Hunt explanation on the Moodle forums)
24,Read replicas
24,Since Moodle 3.9 you can configure read replica's to be used where possible. For very large systems as much as 80-90% of the DB load can be moved away from the primary. For configuration see config-dist:
24,https://github.com/moodle/moodle/blob/master/config-dist.php#L84-L117
24,Other database performance links
24,Consider using a distributed caching system like memcached but note that memcached does not have any security features so it should be used behind a firewall.
24,Consider using PostgreSQL. See how to migrate from MySQL to PostgreSQL (forum discussion).
24,General advice on tuning MySQL parameters (advice from the MySQL manual)
24,InnoDB performance optimization taken from the MySQL performance blog site.
24,Performance of different Moodle modules
24,"Moodle's activity modules, filters, and other plugins can be activated/deactivated. If necessary, you may wish to deactivate some features (such as chat) if not required - but this isn't necessary. Some notes on the performance of certain modules:"
24,"The Chat module is said to be a hog in terms of frequent HTTP requests to the main server. This can be reduced by setting the module to use Streamed updates, or, if you're using a Unix-based webserver, by running the chat in daemon mode. When using the Chat module use the configuration settings to tune for your expected load. Pay particular attention to the chat_old_ping and chat_refresh parameters as these can have greatest impact on server load."
24,The Moodle Cron task is triggered by calling the script cron.php. If this is called over HTTP (e.g. using wget or curl) it can take a large amount of memory on large installations. If it is called by directly invoking the php command (e.g. php -f /path/to/moodle/directory/admin/cli/cron.php) efficiency can be much improved.
24,The Recent activities block is consuming too many resources if you have huge number of records mdl_log. This is being tested to optimize the SQL query.
24,"The Quiz module is known to stretch database performance. However, it has been getting better in recent versions, and we don't know of any good, up-to-date performance measurements. (Here is a case study from 2007 with 300 quiz users.). The following suggestions were described by Al Rachels in this forum thread:"
24,"make sure both Moodle, and the operating system, are installed on a solid state drive"
24,upgrade to and use PHP 7
24,run MySQLTuner and implement its recommendations
24,See Performance settings for more information on performance-related Moodle settings.
24,See also
24,Using Moodle: Hardware and Performance forum
24,Why Your Moodle Site is Slow: Five Simple Settings blog post from Jonathan Moore
24,I teach with Moodle performance testing: http://www.iteachwithmoodle.com/2012/11/17/moodle-2-4-beta-performance-test-comparison-with-moodle-2-3/
24,Moodle 2.4.5 vs 2.5.2 performance and MUC APC cahe store
24,Moodle performance testing 2.4.6 vs 2.5.2 vs 2.6dev
24,Moodle performance analysis revisited (now with MariaDB)
24,"Tim Hunt's blog (May 2, 2013) on performance testing Moodle"
24,"New Relic, Application Performance Monitoring"
24,Performance enhancements for Apache and PHP (Apache Event MPM and PHP-FPM)
24,Performance recommendations
24,Moodle performance investigation – using performance info
24,Moodle Caching at Scale
24,"There have been a lot of discussions on moodle.org about performance, here are some of the more interesting and (potentially) useful ones:"
24,Performance woes!
24,Performance perspectives - a little script
24,Comments on planned server hardware
24,Moodle performance in a pil by Martin Langhoff
24,Advice on optimising php/db code in moodle2+
24,Moodle 2.5 performance testing at the OU
24,100 active users limit with 4vCPU
24,Performance Tip ... shared...
24,"Retrieved from ""https://docs.moodle.org/403/en/index.php?title=Performance_recommendations&oldid=147626"""
24,Category: Performance
24,Tools
24,What links here
24,Related changes
24,Special pages
24,Printable version
24,Permanent link
24,Page information
24,In other languages
24,Español
24,Français
24,日本語
24,Deutsch
24,"This page was last edited on 11 January 2024, at 15:57."
24,Content is available under GNU General Public License unless otherwise noted.
24,Privacy
24,About Moodle Docs
24,Disclaimers
25,Introducing Amazon RDS for MariaDB 10.11 for up to 40% higher transaction throughput | AWS Database Blog
25,Skip to Main Content
25,Click here to return to Amazon Web Services homepage
25,Contact Us
25,Support
25,English
25,My Account
25,Sign In
25,Create an AWS Account
25,Products
25,Solutions
25,Pricing
25,Documentation
25,Learn
25,Partner Network
25,AWS Marketplace
25,Customer Enablement
25,Events
25,Explore More
25,Close
25,عربي
25,Bahasa Indonesia
25,Deutsch
25,English
25,Español
25,Français
25,Italiano
25,Português
25,Tiếng Việt
25,Türkçe
25,Ρусский
25,ไทย
25,日本語
25,한국어
25,中文 (简体)
25,中文 (繁體)
25,Close
25,My Profile
25,Sign out of AWS Builder ID
25,AWS Management Console
25,Account Settings
25,Billing & Cost Management
25,Security Credentials
25,AWS Personal Health Dashboard
25,Close
25,Support Center
25,Expert Help
25,Knowledge Center
25,AWS Support Overview
25,AWS re:Post
25,Click here to return to Amazon Web Services homepage
25,Get Started for Free
25,Contact Us
25,Products
25,Solutions
25,Pricing
25,Introduction to AWS
25,Getting Started
25,Documentation
25,Training and Certification
25,Developer Center
25,Customer Success
25,Partner Network
25,AWS Marketplace
25,Support
25,AWS re:Post
25,Log into Console
25,Download the Mobile App
25,AWS Blog Home
25,Blogs
25,Editions
25,Close
25,Architecture
25,AWS Cloud Operations & Migrations
25,AWS for Games
25,AWS Insights
25,AWS Marketplace
25,AWS News
25,AWS Partner Network
25,AWS Smart Business
25,Big Data
25,Business Intelligence
25,Business Productivity
25,Cloud Enterprise Strategy
25,Cloud Financial Management
25,Compute
25,Contact Center
25,Containers
25,Database
25,Desktop & Application Streaming
25,Developer Tools
25,DevOps
25,Front-End Web & Mobile
25,HPC
25,Industries
25,Integration & Automation
25,Internet of Things
25,Machine Learning
25,Media
25,Messaging & Targeting
25,Microsoft Workloads on AWS
25,.NET on AWS
25,Networking & Content Delivery
25,Open Source
25,Public Sector
25,Quantum Computing
25,Robotics
25,SAP
25,Security
25,Spatial Computing
25,Startups
25,Storage
25,Supply Chain & Logistics
25,Training & Certification
25,Close
25,中国版
25,日本版
25,한국 에디션
25,기술 블로그
25,Edisi Bahasa Indonesia
25,AWS Thai Blog
25,Édition Française
25,Deutsche Edition
25,Edição em Português
25,Edición en Español
25,Версия на русском
25,Türkçe Sürüm
25,AWS Database Blog
25,Introducing Amazon RDS for MariaDB 10.11 for up to 40% higher transaction throughput
25,Sai Kiran Kshatriya | on
25,21 AUG 2023 | in
25,"Advanced (300), Amazon RDS, Announcements, RDS for MariaDB |"
25,Permalink |
25,Comments |
25,Share
25,"MariaDB is a popular open-source high performance database. Amazon Relational Database Service (Amazon RDS) for MariaDB supports multiple major engine versions including 10.4, 10.5, 10.6. Today, Amazon RDS has announced support for MariaDB major version 10.11, which is the latest long-term supported major version from the MariaDB community."
25,"When compared to previous versions, Amazon RDS for MariaDB 10.11 delivers up to 40% higher transaction throughput. This means faster database operations and improved application responsiveness, especially during periods of peak usage."
25,"In this post, we benchmark Amazon RDS for MariaDB version 10.11 and compare it to our last release RDS for MariaDB 10.6. We also go over the latest features introduced by the community in MariaDB 10.11. Whether you are currently using an older version of MariaDB, or, considering migration from another database system, this post outlines the benefits and procedure of upgrading to MariaDB 10.11."
25,Performance of Amazon RDS for MariaDB 10.11
25,"We start by comparing Amazon RDS for MariaDB versions 10.11.4 and 10.6.14 to understand the performance improvement of major version 10.11 over previous versions. Our benchmarking tests were carried out using an r6g.16xlarge instance with 45,000 Provisioned IOPS and 1,500 GB EBS volume. We used sysbench to benchmark write and read operations on both 10.11 and 10.6. The dataset employed for testing consisted of two hundred and fifty tables, each containing approximately 1.3 million rows."
25,"First, we examined the write performance using the default parameters of an RDS for MariaDB instance. For the same workload, we varied the number of threads to understand the impact on write performance. The graphs below illustrate the differences in write performance observed across Amazon RDS for MariaDB 10.11.4 and Amazon RDS for MariaDB 10.6.14. We start observing the performance difference between 10.11.4 and 10.6.14 even when running a single thread. As we increase the number of threads, the performance differential continues to grow, peaking at 512 threads. At 512 threads, Amazon RDS for MariaDB 10.11 delivered a write throughput of 3,224.53 transactions per second (TPS), which is 47.25% higher than the MariaDB 10.6 TPS at 2,189.77."
25,We used the following commands for our sysbench write tests:
25,sysbench parallel_prepare --mysql-port=3306 --db-driver=mariadb --mysql-table --oltp-tables-count=250 --oltp-table-size=1290000 --num-threads=250 -–report-interval=60 –-warmup-time=60 –-db-ps-mode=disable -–reconnect=0 run
25,"sysbench oltp_write --table-size=1290000 --db-driver=mysql --tables=250 -—threads=<1,1024> --simple_ranges=0 –distinct_ranges=0 -–sum_ranges=0 -–order_ranges=0 -–time=1200 -–report-interval=60"
25,-–histogram=on --time=1200 –db-ps-mode=disable --thread-init-timeout=60 run
25,"Next, we tested a read workload using the default parameter configuration. We start observing a performance improvement on 10.11 when our tests scales to 2 threads. Similar to our write tests, as the number of threads increases, the performance gap between Amazon RDS for MariaDB 10.11.4 and 10.6.14 continues to increase, peaking at 512 thread count. At 512 threads, Amazon RDS for MariaDB 10.11 delivered a read throughput of 1255 transactions per second (TPS), which is 15.13% higher than the MariaDB 10.6 TPS at 1090.16."
25,We used the following commands for our sysbench read tests:
25,"sysbench oltp_read --table-size=1290000 --db-driver=mysql --tables=250 -—threads=<1,1024> --simple_ranges=0 -–distinct_ranges=0 -–sum_ranges=0 –-order_ranges=0 -–time=1200 –-report-interval=60 -–histogram=on --time=1200 -–db-ps-mode=disable --thread-init-timeout=60 runNote, your performance gains may vary depending on your workload and instance configuration."
25,"If you want to push the performance of your RDS for MariaDB 10.11 further, you could consider using Amazon RDS Optimized Reads and Optimized Writes. RDS Optimized Reads use NVMe-based SSD block storage, directly attached to the host server, to achieve up to 2X faster complex query processing. And, RDS Optimized Writes deliver up to 2X higher write transaction throughput. When you combine these features with Amazon RDS for MariaDB 10.11, the overall performance gains can be compounded depending on the nature of your workload."
25,"In the next section, we go over the new features introduced with MariaDB 10.11. If you would like to learn the best practices for upgrading your database instance to MariaDB 10.11, you can skip ahead to section “Upgrading to Amazon RDS for MariaDB 10.11”."
25,What’s new in Amazon RDS for MariaDB 10.11?
25,"The MariaDB community has introduced multiple new features with major version 10.11. You can find a complete list of new features in the RDS for MariaDB user guide. In this section, we highlight a select few."
25,1. Lag free ALTER TABLE replication
25,"Performing ALTER TABLE operations on a primary database often introduces replication lag, affecting the availability and consistency of replicas. MariaDB 10.11 eliminates the replication lag caused by ALTER TABLE. When you enable this feature by enabling the binlog_alter_two_phase parameter in the parameter group, the ALTER command is replicated and run simultaneously on replicas, which minimizes the replication lag and ensures a seamless and uninterrupted database experience, allowing you to perform schema changes without worrying about large replication delays."
25,"In addition to enabling the binlog_alter_two_phase parameter in the parameter group, it’s important to ensure that parallel replication is enabled on the replicas and increase the value of the slave_parallel_workers parameter greater than zero."
25,2. Authentication
25,"Amazon RDS for MariaDB 10.11 supports the GRANT TO …PUBLIC command which allows granting privileges to all users with access to the server, including those created after the privileges are granted. We recommend you use this feature cautiously as granting high level privileges to PUBLIC can lead to security issues in the database. If used cautiously, this approach proves particularly useful when you want to define a specific set of privileges for all users without having to repeat the individual grant statements for every user. This allows all users to have a uniform set of privileges, promoting consistency and standardization of privilege management. Administrators can easily review and modify the privileges assigned to all users at once. The GRANT TO … PUBLIC feature can help administrators streamline user privilege management."
25,"In the following example, we create a MariaDB user named `John` who is allowed to connect to our RDS instance."
25,mysql> CREATE USER 'John'@'%' IDENTIFIED BY 'xxxxx';
25,"Query OK, 0 rows affected (0.00 sec)"
25,"Now, we use the GRANT TO …PUBLIC command to grant SELECT privileges to all users with access to the server on the `Titles` table in the `Employees` database. We will later run the SHOW GRANTS command to confirm the privileges assigned TO PUBLIC grants."
25,mysql> GRANT SELECT ON employees.Titles TO PUBLIC;
25,mysql> SHOW GRANTS FOR public;
25,+------------------------------------------------+
25,| Grants for PUBLIC
25,+------------------------------------------------+
25,| GRANT SELECT ON `employees`.`Titles` TO PUBLIC |
25,+------------------------------------------------+
25,"Now, we login as the newly-created MySQL user, John, and query the table in the database."
25,mysql> select user();
25,+--------------------+
25,| user()
25,+--------------------+
25,| John@xxx.xx.xx.xxx |
25,+--------------------+
25,mysql> SHOW GRANTS for 'John'@'%';
25,+-----------------------------------------------------------------------------------------------------+
25,| Grants for John@%
25,+-----------------------------------------------------------------------------------------------------+
25,| GRANT USAGE ON *.* TO `John`@`%` IDENTIFIED BY PASSWORD '*2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19' |
25,+-----------------------------------------------------------------------------------------------------+
25,"From the previous command, we confirm that MariaDB user `John` does not have privileges to run the SELECT query on the `Titles` table in the `Employees` database. However, since we created a GRANT TO … PUBLIC SELECT privilege in our earlier step, SELECT privilege is granted to the MariaDB user and this is confirmed in this example:"
25,mysql> select * from `employees`.`Titles` limit 1;
25,+----+---------+-----------+---------+
25,| id | k
25,| c
25,| pad
25,+----+---------+-----------+---------+
25,1 | 4992833 | 838686419 | 6784796 |
25,+----+---------+-----------+---------+
25,3. New JSON functions
25,"MariaDB 10.11 provides several new functions for additional flexibility and functionality when working with JSON data, including JSON_EQUALS, JSON_NORMALIZE, JSON_OVERLAPS:"
25,JSON_EQUALS: You can use this function to compare the equality between JSON objects regardless of attribute order.
25,"In the following example, we use the JSON_EQUALS function to compare the equality between two JSON objects, '{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}' and '{""Sam"":[10567,34.0,1995],""John"":[10563,22,1988]}'."
25,"A value of 1 indicates that the JSON objects are equal, whereas a value of 0 indicates inequality."
25,"In our example, both JSON objects have the same structure, with the same keys and values (though some values are in different data types, for example 34 to 34.0). Since, our JSON objects have the same structure and members, regardless of member order, the function returns a value of 1, indicating equality."
25,"mysql> SELECT JSON_EQUALS('{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}', '{""Sam"":[10567,34.0,1995],""John"":[10563,22,1988]}') as JSON_EQUALS;"
25,+-------------+
25,| JSON_EQUALS |
25,+-------------+
25,1 |
25,+-------------+
25,"In the next example, we use the JSON_EQUALS function again to compare a different set of JSON objects. These JSON objects are not equal because they have different structures. For example, in the first object, the key “John” has an array with the value 1988, while in the second object, the key “John” has a different structure with only two values. Therefore, the function returns a value of 0, indicating inequality."
25,"mysql> SELECT JSON_EQUALS('{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}', '{""Sam"":[10567,34.0,1995],""John"":[10563,22]}') as JSON_EQUALS;"
25,+-------------+
25,| JSON_EQUALS |
25,+-------------+
25,0 |
25,+-------------+
25,JSON_NORMALIZE: You can use the JSON_NORMALIZE function in combination with a unique key to enforce a unique constraint on the JSON contents in a database. This function converts a JSON object into a canonical form that can be used for comparison purposes for JSON data. This ensures that any future inserts or updates with a similar JSON value that is already present in the table will result in a constraint violation error.
25,Let’s understand this with an example. We create a table named ‘bikemodel’ with two columns: `bike_id`(an auto-incrementing primary key) and `model` (a column to store JSON data)
25,CREATE TABLE bikemodel (
25,"bike_id INT UNSIGNED NOT NULL AUTO_INCREMENT,"
25,"model JSON,"
25,PRIMARY KEY (bike_id)
25,"Here, an ALTER command is run on the `bikemodel` table. A new column named model_normalize is added to store the normalized JSON form of the model column using the JSON_NORMALIZE function. Additionally, a unique key is added on the model_normalize column to enforce the uniqueness constraint on the normalized JSON values."
25,"ALTER TABLE bikemodel ADD COLUMN model_normalize JSON AS (JSON_NORMALIZE(model)) VIRTUAL, ADD UNIQUE KEY (model_normalize);"
25,"We now insert a JSON object into the bikemodel table. The JSON object has attributes like name, mode, and color."
25,"INSERT INTO bikemodel (model) VALUES ('{""name"":""val"",""mode"":""Electric"",""color"":""Magenta""}');"
25,"Query OK, 1 row affected (0.00 sec)"
25,"Now, we attempt to insert a JSON object into the table with slightly different key order and formatting. However, due to the use of the JSON_NORMALIZE function and the unique key constraint on the `model_normalize` column, this insertion fails and results in a constraint violation error. This is because the normalized form of the JSON data is the same as the previously inserted JSON object."
25,"INSERT INTO bikemodel (model) VALUES ('{""mode"":""Electric"",""name"":""val""},""color"":""Magenta""');"
25,"ERROR 1062 (23000): Duplicate entry '{""color"":""Magenta"",""mode"":""Electric"",""name"":""val""}' for key 'model_normalize'"
25,"JSON_OVERLAPS: You can use this function to compare two JSON documents to see if they have any common key/value pairs between objects. It returns true if the two JSON objects have at least one value for the key in common. This also includes array elements between two arrays, array element with scalar and two scalar documents if they have the same type and value."
25,"In the following example, the first JSON object has the keys “John” and “Year” with their corresponding values, and the second JSON object has the keys “SAM” and “Year” with their corresponding values."
25,"mysql> SELECT JSON_OVERLAPS('{""John"": 10567, ""Year"":1988}', {""SAM"":10563, ""Year"":1988}') AS JSON_OVERLAP;"
25,+--------------+
25,| JSON_OVERLAP |
25,+--------------+
25,1 |
25,+--------------+
25,1 row in set (0.00 sec)
25,"The function returns true (1) because both JSON objects share the common key ""Year"" and their corresponding values are not both NULL. This indicates that there is an overlap between the two JSON objects."
25,"In the following example, the first JSON object has the keys ""John"" and ""Year"" with their corresponding values, and the second JSON object has the keys ""JOHN"" and ""Year"" with their corresponding values."
25,"mysql> SELECT JSON_OVERLAPS('{""John"": 10567, ""Year"":1988}', '{""JOHN"":10567, ""Year"":1989}') AS JSON_OVERLAP;"
25,+--------------+
25,| JSON_OVERLAP |
25,+--------------+
25,0 |
25,+--------------+
25,1 row in set (0.00 sec)
25,"The function returns false (0) because although both JSON objects share the common key ""Year"", their corresponding values are different. Therefore, there is no overlap between the two JSON objects."
25,4. SFORMAT function for arbitrary text formatting
25,"SFORMAT function allows for arbitrary text formatting. With this function, you can create custom formatting templates for your data, which can be useful for generating reports, and structured text formatting. Let’s try an example."
25,"We create a table named `employees` with columns `employee_id`, `Name`, and `Year`."
25,"CREATE TABLE employees(employee_id int, Name char(20), Year int);"
25,"Query OK, 0 rows affected (0.01 sec)"
25,"INSERT INTO employees VALUES(10567, 'John', 1988), ('10563', 'Sam', 1965);"
25,We use the SFORMAT function to create custom formatted strings based on the data in the table as follows.
25,"SELECT SFORMAT('Our Employee {} with ID number {} has joined us in the year {}',Name, employee_id, Year) AS 'Employee Joined'"
25,FROM employees;
25,+-----------------------------------------------------------------------+
25,| Employee Joined
25,+-----------------------------------------------------------------------+
25,| Our Employee John with ID number 10567 has joined us in the year 1988 |
25,| Our Employee Sam with ID number 10563 has joined us in the year 1965 |
25,+-----------------------------------------------------------------------+
25,The function replaces the placeholders in the template string with the corresponding values from the employees table and generates formatted strings.
25,SFORMAT() is a flexible way to format strings by which you can control the format of the output and ensure that the values are formatted correctly.
25,5. Query analysis with ANALYZE FORMAT=JSON
25,"ANALYZE FORMAT=JSON in MariaDB 10.11 is a powerful tool for query analysis and optimization. By running the command, you can obtain a comprehensive JSON-based analysis of the query run, which includes both the run plan and the actual query run. This includes information like r_loops (revealing how many times each node in the query plan was run), r_total_time (showing the total run time spent on each node), and r_buffer_size (providing insights into buffer utilization of nodes during the query run). This information will help database administrators and developers to fine-tune query performance, identify areas for potential optimization."
25,"ANALYZE FORMAT=JSON <query> will run the statement and prints the information about the query in EXPLAIN FORMAT=JSON format, we recommend please proceed with caution when running this command on the production workloads and during peak business hours as this command will run the query to provide the required metrics."
25,"In the following example, I run ANALYZE FORMAT=JSON on the query."
25,mysql> ANALYZE FORMAT=JSON select * from employees.Sales where k=4998122\G
25,*************************** 1. row ***************************
25,ANALYZE: {
25,"""query_optimization"": {"
25,"""r_total_time_ms"": 0.100549301"
25,"""query_block"": {"
25,"""select_id"": 1,"
25,"""r_loops"": 1,"
25,"""r_total_time_ms"": 0.471070642,"
25,"""nested_loop"": ["
25,"""table"": {"
25,"""table_name"": ""Sales"","
25,"""access_type"": ""ref"","
25,"""possible_keys"": [""k""],"
25,"""key"": ""k"","
25,"""key_length"": ""4"","
25,"""used_key_parts"": [""k""],"
25,"""ref"": [""const""],"
25,"""r_loops"": 1,"
25,"""rows"": 98,"
25,"""r_rows"": 98,"
25,"""r_table_time_ms"": 0.454686291,"
25,"""r_other_time_ms"": 0.011758278,"
25,"""filtered"": 100,"
25,"""r_filtered"": 100"
25,`Analyze` shows information about the overall analysis of the query.
25,`query_optimization` provides information related to query optimization.
25,`r_total_time_ms` shows total time taken for query optimization in milliseconds (0.100549301 ms in this case).
25,`query_block` section provides information about the query execution process.
25,`select_id` provides information related to Identifier for the SELECT query.
25,`r_loops` shows number of execution loops (1 loop in this case).
25,`r_total_time_ms` shows total time taken for query execution in milliseconds (0.471070642 ms in this case).
25,`nested_loop` subsection indicates the presence of nested loops in the execution process and provides Information about the table used in the query execution.
25,`r_loops` shows number of execution loops (1 loop in this case).
25,`rows` shows total number of rows in the table (98 rows in this case).
25,`r_rows` shows estimated number of rows processed (98 rows in this case).
25,`r_table_time_ms` shows time taken to access the table in milliseconds (0.454686291 ms in this case).
25,`r_other_time_ms` shows other processing time in milliseconds (0.011758278 ms in this case).
25,`filtered` shows percentage of rows that passed through the filter (100% in this case).
25,`r_filtered` shows estimated percentage of rows that passed through the filter (100% in this case).
25,6. Natural sort order
25,"You can use the natural_sort_key() function to perform natural sorting directly within the database, you achieve your requirements of human-friendly sorting to the database environment. This feature allows developers to obtain results that align with user expectations, making sorting more intuitive and comprehensible. Although natural sort functionality exists in various programming languages through built-in or third-party modules, MariaDB’s natural_sort_key() function simplifies the process within the database."
25,"In this example, we create a table named `coordinates` with a single column named `value`, and you insert various string values into it."
25,mysql> create table coordinates(value text);
25,"mysql> insert into coordinates values ('beta1'),('gamma2'),('alpha2'),('alpha11'),('alpha1'),('gamma23'),('gamma36');"
25,We are running a query to select values from the `coordinates` table and sort them using the default sorting order. This default sorting treats numbers in strings as characters and displays the following output.
25,mysql> select value from coordinates order by value;
25,+---------+
25,| value
25,+---------+
25,| alpha1
25,| alpha11 |
25,| alpha2
25,| beta1
25,| gamma2
25,| gamma23 |
25,| gamma3
25,| gamma36 |
25,+---------+
25,"We are running a similar query, but this time we are using the natural_sort_key() function to achieve natural sorting. This function treats numeric portions of strings as numbers, resulting in more intuitive sorting that aligns with user expectations."
25,mysql> select value from coordinates order by natural_sort_key(value);
25,+---------+
25,| value
25,+---------+
25,| alpha1
25,| alpha2
25,| alpha11 |
25,| beta1
25,| gamma2
25,| gamma3
25,| gamma23 |
25,| gamma36 |
25,+---------+
25,MariaDB 10.11 also includes the following enhancements which we do not cover in-depth in this post:
25,"Stored functions now supports IN, OUT and IN OUT parameters qualifiers."
25,Cursors now support IN qualifier.
25,With the UUID data type you can now store 128-bit UUID data.
25,System versioning capabilities have also been enhanced with the addition of the system_versioning_insert_history setting which allows for history modification
25,Upgrading to Amazon RDS for MariaDB 10.11
25,"You have multiple options to upgrade your database to Amazon RDS for MariaDB 10.11: in-place upgrade, using a read replica and Amazon RDS Blue/Green Deployments."
25,"We recommend using Amazon RDS Blue/Green Deployments to minimize risk and downtime during the database upgrade. This managed capability creates two database environments: your current production environment, the blue environment, and a staging environment, the green environment. These two environments remain in sync with each other using native logical replication, so you may safely test your changes on the staging (green) environment and promote when changes have been validated. During testing, we recommend that you test the application on green environment carefully, as enabling writes can result in replication conflicts leading to unexpected data in production environments."
25,"In the example below, we show you how to upgrade an RDS for MariaDB 10.6 instance to MariaDB 10.11 using Amazon RDS Blue/Green Deployments. You can use the same process to upgrade from any prior RDS for MariaDB version (e.g. 10.3, 10.4, 10.5) to version 10.11."
25,"RDS Blue/Green Deployments support the AWS Command line interface (AWS CLI), as well as the AWS Management Console. For this post, we use AWS CLI commands to upgrade our cluster."
25,"You can set the Blue/Green Deployment identifier and the parameters of your database (e.g., source ARN, engine version, and DB cluster parameter group) to be modified for the staging environment. In the following example, we have a MariaDB 10.6.14 version and we create the Amazon RDS Blue/Green Deployment using the following AWS CLI command:"
25,aws rds create-blue-green-deployment \
25,--blue-green-deployment-name mariadb-bgdeployment \
25,--source arn: arn:aws:rds:us-we**-2:9***********:db:m******-10614 \
25,--target-engine-version 10.11 \
25,--target-db-parameter-group-name default.mariadb10.11\
25,--region us-west-2
25,"Amazon RDS Blue/Green Deployments automatically creates a staging environment and run automated tasks to prepare the database. After the Blue/Green Deployment is complete, we have an environment that is ready for testing and promotion, after the validation."
25,"Note that you will be charged for the resources created in the staging environment, including Multi-AZ deployments and any other features such as Amazon RDS Performance Insights that may have been enabled in the production environment."
25,"You can also do the same deployment via the AWS Console by selecting the database and choosing “Create Blue/Green Deployment” on the Actions drop-down menu. For more information, follow the guidance in the New-Fully Managed Blue/Green Deployments in Amazon Aurora and Amazon RDS."
25,"After you validate your applications in the green environment, you can use the following command to switch over the production environment:"
25,aws rds switchover-blue-green-deployment \
25,--blue-green-deployment-identifier mariadb-bgdeployment\
25,--switchover-timeout 600
25,Conclusion
25,"In this post, we benchmarked Amazon RDS for MariaDB 10.11, and observed up to 47.25% improvement in transaction throughput compared to MariaDB 10.6. We also discussed new features introduced in Amazon RDS for MariaDB 10.11. Finally, we outlined how to upgrade to Amazon RDS for MariaDB 10.11 with minimal downtime, using Amazon RDS Blue/Green Deployments. We encourage you to try Amazon RDS for MariaDB 10.11 to test the performance improvements and feature innovations with your workload."
25,About the author
25,"Sai Kiran Kshatriya is an Amazon Web Services Database Specialist Solutions Architect who specializes in Relational Database Engines. He provides Technical Assistance, operational, and database practices to customers."
25,Comments
25,View Comments
25,Resources
25,Getting Started
25,What's New
25,Blog Topics
25,Amazon Aurora
25,Amazon DocumentDB
25,Amazon DynamoDB
25,Amazon ElastiCache
25,Amazon Keyspaces (for Apache Cassandra)
25,Amazon Managed Blockchain
25,Amazon MemoryDB for Redis
25,Amazon Neptune
25,Amazon Quantum Ledger Database (Amazon QLDB)
25,Amazon RDS
25,Amazon Timestream
25,AWS Database Migration Service
25,AWS Schema Conversion Tool
25,Follow
25,Twitter
25,Facebook
25,LinkedIn
25,Twitch
25,Email Updates
25,Sign In to the Console
25,Learn About AWS
25,What Is AWS?
25,What Is Cloud Computing?
25,"AWS Inclusion, Diversity & Equity"
25,What Is DevOps?
25,What Is a Container?
25,What Is a Data Lake?
25,What is Generative AI?
25,AWS Cloud Security
25,What's New
25,Blogs
25,Press Releases
25,Resources for AWS
25,Getting Started
25,Training and Certification
25,AWS Solutions Library
25,Architecture Center
25,Product and Technical FAQs
25,Analyst Reports
25,AWS Partners
25,Developers on AWS
25,Developer Center
25,SDKs & Tools
25,.NET on AWS
25,Python on AWS
25,Java on AWS
25,PHP on AWS
25,JavaScript on AWS
25,Help
25,Contact Us
25,Get Expert Help
25,File a Support Ticket
25,AWS re:Post
25,Knowledge Center
25,AWS Support Overview
25,Legal
25,AWS Careers
25,Create an AWS Account
25,Amazon is an Equal Opportunity Employer:
25,Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.
25,Language
25,عربي
25,Bahasa Indonesia
25,Deutsch
25,English
25,Español
25,Français
25,Italiano
25,Português
25,Tiếng Việt
25,Türkçe
25,Ρусский
25,ไทย
25,日本語
25,한국어
25,中文 (简体)
25,中文 (繁體)
25,Privacy
25,Site Terms
25,Cookie Preferences
25,"© 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved."
26,MySQL :: MySQL 8.3 Reference Manual :: 10.2.1 Optimizing SELECT Statements
26,Skip to Main Content
26,The world's most popular open source database
26,Contact MySQL
26,Login  |
26,Register
26,MySQL.com
26,Downloads
26,Documentation
26,Developer Zone
26,Developer Zone
26,Downloads
26,MySQL.com
26,Documentation
26,MySQL Server
26,MySQL Enterprise
26,Workbench
26,InnoDB Cluster
26,MySQL NDB Cluster
26,Connectors
26,More
26,MySQL.com
26,Downloads
26,Developer Zone
26,Section Menu:
26,Documentation Home
26,MySQL 8.3 Reference Manual
26,Preface and Legal Notices
26,General Information
26,Installing MySQL
26,Upgrading MySQL
26,Downgrading MySQL
26,Tutorial
26,MySQL Programs
26,MySQL Server Administration
26,Security
26,Backup and Recovery
26,Optimization
26,Optimization Overview
26,Optimizing SQL Statements
26,Optimizing SELECT Statements
26,WHERE Clause Optimization
26,Range Optimization
26,Index Merge Optimization
26,Hash Join Optimization
26,Engine Condition Pushdown Optimization
26,Index Condition Pushdown Optimization
26,Nested-Loop Join Algorithms
26,Nested Join Optimization
26,Outer Join Optimization
26,Outer Join Simplification
26,Multi-Range Read Optimization
26,Block Nested-Loop and Batched Key Access Joins
26,Condition Filtering
26,Constant-Folding Optimization
26,IS NULL Optimization
26,ORDER BY Optimization
26,GROUP BY Optimization
26,DISTINCT Optimization
26,LIMIT Query Optimization
26,Function Call Optimization
26,Window Function Optimization
26,Row Constructor Expression Optimization
26,Avoiding Full Table Scans
26,"Optimizing Subqueries, Derived Tables, View References, and Common Table"
26,Expressions
26,Optimizing IN and EXISTS Subquery Predicates with Semijoin and Antijoin
26,Transformations
26,Optimizing Subqueries with Materialization
26,Optimizing Subqueries with the EXISTS Strategy
26,"Optimizing Derived Tables, View References, and Common Table Expressions"
26,with Merging or Materialization
26,Derived Condition Pushdown Optimization
26,Optimizing INFORMATION_SCHEMA Queries
26,Optimizing Performance Schema Queries
26,Optimizing Data Change Statements
26,Optimizing INSERT Statements
26,Optimizing UPDATE Statements
26,Optimizing DELETE Statements
26,Optimizing Database Privileges
26,Other Optimization Tips
26,Optimization and Indexes
26,How MySQL Uses Indexes
26,Primary Key Optimization
26,SPATIAL Index Optimization
26,Foreign Key Optimization
26,Column Indexes
26,Multiple-Column Indexes
26,Verifying Index Usage
26,InnoDB and MyISAM Index Statistics Collection
26,Comparison of B-Tree and Hash Indexes
26,Use of Index Extensions
26,Optimizer Use of Generated Column Indexes
26,Invisible Indexes
26,Descending Indexes
26,Indexed Lookups from TIMESTAMP Columns
26,Optimizing Database Structure
26,Optimizing Data Size
26,Optimizing MySQL Data Types
26,Optimizing for Numeric Data
26,Optimizing for Character and String Types
26,Optimizing for BLOB Types
26,Optimizing for Many Tables
26,How MySQL Opens and Closes Tables
26,Disadvantages of Creating Many Tables in the Same Database
26,Internal Temporary Table Use in MySQL
26,Limits on Number of Databases and Tables
26,Limits on Table Size
26,Limits on Table Column Count and Row Size
26,Optimizing for InnoDB Tables
26,Optimizing Storage Layout for InnoDB Tables
26,Optimizing InnoDB Transaction Management
26,Optimizing InnoDB Read-Only Transactions
26,Optimizing InnoDB Redo Logging
26,Bulk Data Loading for InnoDB Tables
26,Optimizing InnoDB Queries
26,Optimizing InnoDB DDL Operations
26,Optimizing InnoDB Disk I/O
26,Optimizing InnoDB Configuration Variables
26,Optimizing InnoDB for Systems with Many Tables
26,Optimizing for MyISAM Tables
26,Optimizing MyISAM Queries
26,Bulk Data Loading for MyISAM Tables
26,Optimizing REPAIR TABLE Statements
26,Optimizing for MEMORY Tables
26,Understanding the Query Execution Plan
26,Optimizing Queries with EXPLAIN
26,EXPLAIN Output Format
26,Extended EXPLAIN Output Format
26,Obtaining Execution Plan Information for a Named Connection
26,Estimating Query Performance
26,Controlling the Query Optimizer
26,Controlling Query Plan Evaluation
26,Switchable Optimizations
26,Optimizer Hints
26,Index Hints
26,The Optimizer Cost Model
26,Optimizer Statistics
26,Buffering and Caching
26,InnoDB Buffer Pool Optimization
26,The MyISAM Key Cache
26,Shared Key Cache Access
26,Multiple Key Caches
26,Midpoint Insertion Strategy
26,Index Preloading
26,Key Cache Block Size
26,Restructuring a Key Cache
26,Caching of Prepared Statements and Stored Programs
26,Optimizing Locking Operations
26,Internal Locking Methods
26,Table Locking Issues
26,Concurrent Inserts
26,Metadata Locking
26,External Locking
26,Optimizing the MySQL Server
26,Optimizing Disk I/O
26,Using Symbolic Links
26,Using Symbolic Links for Databases on Unix
26,Using Symbolic Links for MyISAM Tables on Unix
26,Using Symbolic Links for Databases on Windows
26,Optimizing Memory Use
26,How MySQL Uses Memory
26,Monitoring MySQL Memory Usage
26,Enabling Large Page Support
26,Measuring Performance (Benchmarking)
26,Measuring the Speed of Expressions and Functions
26,Using Your Own Benchmarks
26,Measuring Performance with performance_schema
26,Examining Server Thread (Process) Information
26,Accessing the Process List
26,Thread Command Values
26,General Thread States
26,Replication Source Thread States
26,Replication I/O (Receiver) Thread States
26,Replication SQL Thread States
26,Replication Connection Thread States
26,NDB Cluster Thread States
26,Event Scheduler Thread States
26,Language Structure
26,"Character Sets, Collations, Unicode"
26,Data Types
26,Functions and Operators
26,SQL Statements
26,MySQL Data Dictionary
26,The InnoDB Storage Engine
26,Alternative Storage Engines
26,Replication
26,Group Replication
26,MySQL Shell
26,Using MySQL as a Document Store
26,InnoDB Cluster
26,InnoDB ReplicaSet
26,MySQL NDB Cluster 8.3
26,Partitioning
26,Stored Objects
26,INFORMATION_SCHEMA Tables
26,MySQL Performance Schema
26,MySQL sys Schema
26,Connectors and APIs
26,MySQL Enterprise Edition
26,MySQL Workbench
26,MySQL on the OCI Marketplace
26,Telemetry
26,MySQL 8.3 Frequently Asked Questions
26,Error Messages and Common Problems
26,Indexes
26,MySQL Glossary
26,Related Documentation
26,MySQL 8.3 Release Notes
26,Download
26,this Manual
26,PDF (US Ltr)
26,- 40.8Mb
26,PDF (A4)
26,- 40.9Mb
26,Man Pages (TGZ)
26,- 294.0Kb
26,Man Pages (Zip)
26,- 409.1Kb
26,Info (Gzip)
26,- 4.0Mb
26,Info (Zip)
26,- 4.0Mb
26,Excerpts from this Manual
26,MySQL Backup and Recovery
26,MySQL NDB Cluster 8.3
26,MySQL Globalization
26,MySQL Information Schema
26,MySQL Installation Guide
26,MySQL and Linux/Unix
26,MySQL and macOS
26,MySQL Partitioning
26,MySQL Performance Schema
26,MySQL Replication
26,Using the MySQL Yum Repository
26,MySQL Restrictions and Limitations
26,Security in MySQL
26,MySQL and Solaris
26,Building MySQL from Source
26,Starting and Stopping MySQL
26,MySQL Tutorial
26,MySQL and Windows
26,version 8.3
26,8.0
26,current
26,5.7
26,8.0
26,Japanese
26,MySQL 8.3 Reference Manual  /
26,...  /
26,Optimization  /
26,Optimizing SQL Statements  /
26,Optimizing SELECT Statements
26,10.2.1 Optimizing SELECT Statements
26,10.2.1.1 WHERE Clause Optimization10.2.1.2 Range Optimization10.2.1.3 Index Merge Optimization10.2.1.4 Hash Join Optimization10.2.1.5 Engine Condition Pushdown Optimization10.2.1.6 Index Condition Pushdown Optimization10.2.1.7 Nested-Loop Join Algorithms10.2.1.8 Nested Join Optimization10.2.1.9 Outer Join Optimization10.2.1.10 Outer Join Simplification10.2.1.11 Multi-Range Read Optimization10.2.1.12 Block Nested-Loop and Batched Key Access Joins10.2.1.13 Condition Filtering10.2.1.14 Constant-Folding Optimization10.2.1.15 IS NULL Optimization10.2.1.16 ORDER BY Optimization10.2.1.17 GROUP BY Optimization10.2.1.18 DISTINCT Optimization10.2.1.19 LIMIT Query Optimization10.2.1.20 Function Call Optimization10.2.1.21 Window Function Optimization10.2.1.22 Row Constructor Expression Optimization10.2.1.23 Avoiding Full Table Scans
26,"Queries, in the form of SELECT"
26,"statements, perform all the lookup operations in the database."
26,"Tuning these statements is a top priority, whether to achieve"
26,"sub-second response times for dynamic web pages, or to chop"
26,hours off the time to generate huge overnight reports.
26,"Besides SELECT statements, the"
26,tuning techniques for queries also apply to constructs such as
26,CREATE
26,"TABLE...AS SELECT,"
26,INSERT
26,"INTO...SELECT, and WHERE clauses in"
26,DELETE statements. Those
26,statements have additional performance considerations because
26,they combine write operations with the read-oriented query
26,operations.
26,NDB Cluster supports a join pushdown optimization whereby a
26,qualifying join is sent in its entirety to NDB Cluster data
26,"nodes, where it can be distributed among them and executed in"
26,"parallel. For more information about this optimization, see"
26,Conditions for NDB pushdown joins.
26,The main considerations for optimizing queries are:
26,To make a slow SELECT ... WHERE query
26,"faster, the first thing to check is whether you can add an"
26,index. Set up indexes on
26,"columns used in the WHERE clause, to"
26,"speed up evaluation, filtering, and the final retrieval of"
26,"results. To avoid wasted disk space, construct a small set"
26,of indexes that speed up many related queries used in your
26,application.
26,Indexes are especially important for queries that reference
26,"different tables, using features such as"
26,joins and
26,foreign keys. You
26,can use the EXPLAIN statement
26,to determine which indexes are used for a
26,SELECT. See
26,"Section 10.3.1, “How MySQL Uses Indexes” and"
26,"Section 10.8.1, “Optimizing Queries with EXPLAIN”."
26,"Isolate and tune any part of the query, such as a function"
26,"call, that takes excessive time. Depending on how the query"
26,"is structured, a function could be called once for every row"
26,"in the result set, or even once for every row in the table,"
26,greatly magnifying any inefficiency.
26,Minimize the number of
26,full table scans
26,"in your queries, particularly for big tables."
26,Keep table statistics up to date by using the
26,ANALYZE TABLE statement
26,"periodically, so the optimizer has the information needed to"
26,construct an efficient execution plan.
26,"Learn the tuning techniques, indexing techniques, and"
26,configuration parameters that are specific to the storage
26,engine for each table. Both InnoDB and
26,MyISAM have sets of guidelines for
26,enabling and sustaining high performance in queries. For
26,"details, see Section 10.5.6, “Optimizing InnoDB Queries” and"
26,"Section 10.6.1, “Optimizing MyISAM Queries”."
26,You can optimize single-query transactions for
26,"InnoDB tables, using the technique in"
26,"Section 10.5.3, “Optimizing InnoDB Read-Only Transactions”."
26,Avoid transforming the query in ways that make it hard to
26,"understand, especially if the optimizer does some of the"
26,same transformations automatically.
26,If a performance issue is not easily solved by one of the
26,"basic guidelines, investigate the internal details of the"
26,specific query by reading the
26,EXPLAIN plan and adjusting
26,"your indexes, WHERE clauses, join"
26,"clauses, and so on. (When you reach a certain level of"
26,"expertise, reading the"
26,EXPLAIN plan might be your
26,first step for every query.)
26,Adjust the size and properties of the memory areas that
26,MySQL uses for caching. With efficient use of the
26,InnoDB
26,"buffer pool,"
26,"MyISAM key cache, and the MySQL query"
26,"cache, repeated queries run faster because the results are"
26,retrieved from memory the second and subsequent times.
26,Even for a query that runs fast using the cache memory
26,"areas, you might still optimize further so that they require"
26,"less cache memory, making your application more scalable."
26,Scalability means that your application can handle more
26,"simultaneous users, larger requests, and so on without"
26,experiencing a big drop in performance.
26,"Deal with locking issues, where the speed of your query"
26,might be affected by other sessions accessing the tables at
26,the same time.
26,PREV
26,HOME
26,NEXT
26,Related Documentation
26,MySQL 8.3 Release Notes
26,Download
26,this Manual
26,PDF (US Ltr)
26,- 40.8Mb
26,PDF (A4)
26,- 40.9Mb
26,Man Pages (TGZ)
26,- 294.0Kb
26,Man Pages (Zip)
26,- 409.1Kb
26,Info (Gzip)
26,- 4.0Mb
26,Info (Zip)
26,- 4.0Mb
26,Excerpts from this Manual
26,MySQL Backup and Recovery
26,MySQL NDB Cluster 8.3
26,MySQL Globalization
26,MySQL Information Schema
26,MySQL Installation Guide
26,MySQL and Linux/Unix
26,MySQL and macOS
26,MySQL Partitioning
26,MySQL Performance Schema
26,MySQL Replication
26,Using the MySQL Yum Repository
26,MySQL Restrictions and Limitations
26,Security in MySQL
26,MySQL and Solaris
26,Building MySQL from Source
26,Starting and Stopping MySQL
26,MySQL Tutorial
26,MySQL and Windows
26,Contact MySQL Sales
26,USA/Canada: +1-866-221-0634
26,(More Countries »)
26,© 2024 Oracle
26,Products
26,MySQL HeatWave
26,MySQL Enterprise Edition
26,MySQL Standard Edition
26,MySQL Classic Edition
26,MySQL Cluster CGE
26,MySQL Embedded (OEM/ISV)
26,Services
26,Training
26,Certification
26,Support
26,Downloads
26,MySQL Community Server
26,MySQL NDB Cluster
26,MySQL Shell
26,MySQL Router
26,MySQL Workbench
26,Documentation
26,MySQL Reference Manual
26,MySQL Workbench
26,MySQL NDB Cluster
26,MySQL Connectors
26,Topic Guides
26,About MySQL
26,Contact Us
26,Blogs
26,How to Buy
26,Partners
26,Job Opportunities
26,Site Map
26,© 2024 Oracle
26,Privacy /
26,Do Not Sell My Info |
26,Terms of Use |
26,Trademark Policy |
27,MySQL Query Optimization: Faster Performance & Data Retrieval | Airbyte
27,Complete the State of Data & AI Survey for a chance to win a Steam Deck!
27,View Press KitProduct
27,"ProductAirbyte CloudFully-managed, get started in minutesAirbyte Self-Managed EnterpriseSecure data movement for your entire orgAirbyte Open SourceUsed by 40k+ companiesPowered by AirbyteEmbed 100s integrations at once in your appcapabilitiesExtract & LoadReliable database and API replication at any scalePyAirbyteThe power of Airbyte to every Python developerConnector builderBuild a new connector  in 10 minAI / LLMs with Proprietary DataEmbeddings from unstructured sourcesTry our demo appExplore our public demoSolutions"
27,"Use CasesDatabase replicationHigh-volume DBs with low latencyArtificial intelligenceMake sense of unstructured data with LLMsAnalyticsMarketing, sales, product, finance, eng & moreEmbed ConnectorsEasily collect credentials from your end-usersResourcesSuccess storiesLearn from other members’ successCompare Airbyte vs. alternativesChoose the right solutions for youBuild vs. BuyEvaluate your costs in both scenariosResource centerOur guides to help you in your journeyPartnersBecome a technology or consulting partnerTHE LARGEST DATA ENGINEERING SURVEYCheck out State of DataNo items found.Developers"
27,"LearnDocsHow to use and contribute to AirbyteBlogData engineering thought leadershipTutorialsImprove your data replication gameQuickstartsDeploy your use case in minutesPublic RoadmapGet a glimpse in the futureData Engineering ResourcesPlace for all data knowledgeCommunityMonthly NewsletterStay up to date. 20k+ subscribersSupport centerAccess our knowledge baseCommunityJoin our 15,000+ data  communityCommunity Reward ProgramLeave your mark in the OSS communityEvents & community callsLive events by the Airbyte teamOur Social PlatformCommunity forumGitHub Discussions for questions, ideasSlack15,000+  share tips and get supportYoutubeLearn more about Airbyte and data engineeringDiscourse (read-only)Previous forum (still lots of knowledge)ConnectorsPricing"
27,"StarTalk to SalesTry it  freeData Engineering ResourcesMySQL Query Optimization: Faster Performance & Data RetrievalAditi Prakash••July 24, 2023•10 min readMySQL is the most used relational database management system (RDBMS) that drives countless modern applications and websites. Data engineers use structured query language (SQL) queries to access and modify the data in MySQL databases. These queries bridge the gap between code and the wealth of data stored in databases. Optimizing MySQL queries is a big part of performance tuning, which is essential for achieving optimal database performance and scalability.In this article, we will explain the core components of a MySQL Query Optimization, list the benefits of optimizing queries, and delve into the commonly used techniques during performance tuning.Understanding MySQL QueriesA MySQL query is an SQL statement that instructs the database to perform specific operations. These queries are used to retrieve, insert, update, or delete data from a MySQL database. The basic structure of a MySQL query has several components:SELECT: Specifies the columns or expressions to retrieve from the database.FROM: Specifies the table or tables from which the data is retrieved.WHERE: Optional condition that filters the data based on specified criteria.JOIN: Combines rows from multiple tables based on a related column between them (optional).GROUP BY: Groups the retrieved data based on one or more columns (optional).HAVING: Filters the grouped data based on specified conditions (optional).ORDER BY: Sorts the retrieved data based on one or more columns (optional).LIMIT: Limits the number of rows returned by the query (optional).Some standard use cases for these statements are: Data Retrieval: You typically use the SELECT statement for data retrieval. It allows you to specify the columns you want to fetch from the MySQL database and filter the data using the WHERE clause based on conditions. Example: SELECT first_name, last_name FROM employees WHERE department = 'HR';Data Insertion: You can use the INSERT statement to add new records to a table. Example:INSERT INTO employees (first_name, last_name, department) VALUES ('John', 'Doe', 'Finance');Data Updating: The UPDATE statement lets you modify existing records in a table. Example:UPDATE employees SET department = 'Marketing' WHERE id = 101;Data Deletion: The DELETE statement removes records from a table based on specified conditions. Example:DELETE FROM employees WHERE department = 'IT';MySQL queries can also involve more complex concepts, like:Joins: To retrieve data from multiple related tables.Aggregate Functions: To perform calculations on groups of data, e.g., SUM, COUNT, AVG.Subqueries: Queries within queries used to retrieve data based on intermediate results.Indexing: Creating indexes for frequently used columns for faster data retrieval.By mastering these concepts, data engineers can interact with a database effectively and perform operations to manipulate data according to their application's needs.Common issues that impact the performance of MySQL queriesThere are seven standard issues that data engineers face when implementing MySQL queries:Missing or inadequate indexes: Proper indexing of the columns used in WHERE, JOIN, and ORDER BY clauses can significantly improve MySQL database performance. Without appropriate indexes, MySQL has to perform full table scans, resulting in slower queries.Inefficient query design: Poorly written queries with complex joins, subqueries, or unnecessary calculations can slow down queries. Simplifying the query structure and optimizing it can improve performance.Large result sets: Retrieving a large number of rows from the database can impact MySQL performance and consume excessive memory. They can use pagination or LIMIT clauses to retrieve only the necessary data.Insufficient hardware resources: If the MySQL server is running on hardware with limited resources (e.g., CPU, memory, disk I/O), it can impact database performance.Locking and contention: Concurrent access to the same data can lead to locking and contention issues.Suboptimal database schema design: Poorly designed database schemas with redundant or excessive normalization can result in complex queries and slower performance.Poor network connectivity: Slow network connections between the client and the MySQL server hinders performance, especially for queries involving large result sets.Importance of MySQL Query OptimizationMySQL Query optimization is crucial for enhancing data retrieval speed and efficiency, directly impacting the application's overall performance and success. Some key benefits of optimizing MySQL performance include:Improved Performance: Optimized queries execute faster, reducing response times for your applications. This enhanced performance leads to a smoother user experience and higher customer satisfaction.Scalability: As your application grows and handles larger data volumes, optimized queries ensure that the database can efficiently handle the increased load without sacrificing performance.Resource Utilization: Efficient queries consume fewer server resources, such as CPU and memory, which lowers infrastructure costs.Reduced Downtime: Enhancing queries minimizes the risk of performance bottlenecks and potential crashes, leading to improved system stability and reduced downtime.Faster Development: Efficient queries lead to shorter development cycles, as developers spend less time troubleshooting slow queries and can focus on building new features and functionalities.Improved User Experience: Faster data retrieval and processing times lead to a more responsive application, keeping users engaged and reducing bounce rates.Database Maintenance: Well-designed queries simplify database maintenance tasks, making it easier to manage and monitor the MySQL database.Cost Savings: Efficient queries can lead to cost savings, as they reduce hardware requirements, optimize server usage, and improve overall system performance.Competitive Advantage: In a highly competitive market, faster application performance can give your business a competitive edge, attracting and retaining customers.Handling High Traffic: For web applications facing heavy user traffic, optimization ensures that the system can handle a high number of concurrent queries without compromising performance.Future-Proofing: Optimized queries can adapt to changing data patterns and growing workloads, ensuring that your application remains responsive and reliable in the long run.Techniques for Optimizing MySQL QueriesHere are some key techniques to improve MySQL performance:#1. Use appropriate indexingIdentify frequently accessed data and the columns used in WHERE, JOIN, and ORDER BY clauses and create indexes on those columns. Indexes allow MySQL to quickly find and retrieve the required data without scanning the entire table.Avoid over-indexing, as too many indexes can slow down insert, update, and delete operations.#2. Optimize SELECT statements and avoid SELECT *Only select the columns you need instead of using ""SELECT *."" This reduces the amount of data transferred and improves database performance.Use aggregate functions (e.g., SUM, COUNT, AVG) selectively to minimize data processing.#3. Utilize the Explain command to understand query executionThe EXPLAIN output shows how MySQL plans to execute the query, including the chosen indexes and the order of table access. Use this command before executing a query to analyze its execution plan, identify potential bottlenecks, and change the query accordingly.#4. Limit the amount of data retrievedUse the LIMIT clause to restrict the number of rows the query returns. This can significantly boost MySQL performance, especially for queries with large result sets.Implement pagination in applications to retrieve data in smaller chunks, reducing the server load and response times.#5. Use joins and avoid unnecessary subqueriesOptimize the use of JOINs by choosing the appropriate type of join (e.g., INNER JOIN, LEFT JOIN) based on the relationship between tables and the desired result.Minimize subqueries, as they can be less efficient than joins. Rewrite subqueries as JOINs where possible.#6. Normalize your database schemaNormalize your database schema to avoid data duplication and maintain data integrity. Use foreign keys to establish relationships between tables and enforce referential integrity.Normalization can lead to better data quality and more efficient queries, reducing the need for complex JOINs and allowing for smaller, more manageable tables.The effectiveness of these MySQL performance optimization techniques can vary depending on the specific database structure, data volume, and the complexity of the queries. Regular monitoring and benchmarking of MySQL performance is essential to find areas to optimize and ensure the efficiency of your MySQL database.Tools for Optimizing MySQL QueriesData engineers can use many tools and platforms for MySQL performance tuning. Some popular tools include:MySQL Performance SchemaMySQL Performance Schema is a built-in instrument for collecting detailed real-time information from the MySQL server. It provides valuable insights for measuring performance, including query execution, resource utilization, and overall server activity. By enabling this feature, you can monitor and diagnose performance issues and generate a slow query log, helping you identify bottlenecks and optimize queries accordingly.You can also analyze database performance and resource usage. Common tables include events_statements_summary_by_digest, events_statements_summary_by_user_by_event_name, etc.MySQL WorkbenchMySQL Workbench is an official graphical tool from MySQL that provides database design, administration, and optimization features. It includes a visual EXPLAIN feature, which helps you interpret query execution plans graphically. MySQL Workbench is user-friendly and suitable for developers and database administrators who prefer a GUI environment.Percona ToolkitPercona Toolkit is a set of command-line tools developed by Percona, a well-known MySQL consulting company. Some tools in this toolkit, like pt-query-digest and pt-query-advisor, are helpful for query analysis and optimization. Pt-query-digest processes MySQL query logs and summarizes how database queries are performing, while pt-query-advisor offers recommendations for optimizing slow queries.Real-World Examples of MySQL Query OptimizationTo help you understand how performance tuning can boost the performance of your MySQL databases, here are two example case studies:Case study 1: Optimizing a complex query for a large-scale data applicationA company operates a large-scale data analytics platform that collects and analyzes vast amounts of data from various sources. One of the queries used in their platform retrieves complex statistical data from multiple tables based on user-defined filters. The query's execution time has been increasing as the data volume grows, hindering the platform's overall performance.Steps for MySQL performance tuning:Indexing: The first step is to analyze the query's execution plan using the EXPLAIN command. For example, suppose the EXPLAIN output reveals that some critical columns used in JOIN and WHERE clauses were not indexed. In that case, appropriate indexes can be created to reduce the query execution time.Caching: Implement caching mechanisms at the application level to store the results of frequently executed queries in a cache. Using a MySQL query cache means user-defined queries don't need to be executed repeatedly.Query Rewriting: Rewrite parts of the query to eliminate redundant calculations and use efficient joins to streamline the query.Sharding: Depending on the scale of data, implementing sharding or partitioning to distribute data across multiple database servers. This reduces the data volume per server, leading to faster query execution.Hardware Optimization: Fine-tune the MySQL server configuration to ensure that the MySQL instance is appropriately utilizing CPU cores and memory.The result: With these optimization efforts, there can be a significant decrease in the execution time of the complex query. Users will experience faster response times and improved platform performance, even with the ever-increasing volume of data.Case study 2: Improving the performance of an e-commerce application with query optimizationAn e-commerce company faces slow loading times and performance issues on its product listing pages, where thousands of products are displayed. The application's database contains millions of product records, and the query fetching product data is becoming a performance bottleneck.Steps to optimize MySQL performance:SELECT Specific Columns: Instead of using ""SELECT *,"" the development team can revise the query to retrieve only the essential columns required for displaying products on the listing page. This reduces data transfer overhead and speeds up queries.Pagination and LIMIT: The team can implement pagination using the LIMIT clause to retrieve a limited number of products per page. This decreases the amount of data to be retrieved and leads to faster loading times for the listing pages.Caching: Since product listings often remain unchanged for a short period, the team can use caching mechanisms to store the query results temporarily. Cached data is served to users to avoid repetitive query execution and reduce the load on the database server.Denormalization: For read-heavy operations like product listings, denormalization can help. The data team can create a separate table with pre-joined and pre-computed data for the product listings.Load Balancing: To handle the increasing user traffic, data engineers can use a load-balanced configuration for the application's database, distributing the query load across multiple servers.The result: With the optimized query and various performance-enhancing techniques, the e-commerce application's product listing pages can load much faster. Users get a smoother and faster shopping experience, leading to higher customer satisfaction.Best Practices for MySQL Query OptimizationData engineers must focus on three factors for optimum MySQL performance:Regular monitoring and optimizationImplement regular monitoring mechanisms for query performance as part of the database maintenance routine. Use tools like MySQL Performance Schema, EXPLAIN, and query profiler to identify and optimize slow queries and bottlenecks.Also, consistently review and update database indexes to align with changing query patterns and data volume. Another area to review is MySQL server performance. Adjust configuration parameters based on workload and hardware capabilities.Training and education for the team on optimization techniquesTrain developers, data engineers, and database administrators on techniques for improving MySQL performance, interpreting EXPLAIN outputs, and indexing strategies.Foster a culture of awareness within the development team and encourage collaboration to optimize queries during code reviews and database design discussions.Incorporating optimization in the initial stages of application designDesign the database schema with a focus on normalization and efficient data retrieval. Carefully plan and optimize critical and frequently used queries during the application design phase. Consider anticipated data volume and scalability requirements when designing the database schema and query logic.The Future of MySQL Query OptimizationAdvancements in MySQL databases and related technologies might change queries and performance tuning in specific ways:Improved Query Optimizer: The query optimizer in MySQL is continually being enhanced to make smarter decisions in choosing the best execution plan for queries. As MySQL evolves, we can expect the optimizer to become more efficient and capable of handling complex queries more effectively.Indexing Innovations: Advancements in database technologies might introduce novel indexing techniques to improve data retrieval speed and reduce the overhead of maintaining indexes. Adaptive, partial, or hybrid indexing approaches could become more prevalent in MySQL performance tuning.Query Rewriting and Auto-Tuning: Future versions of MySQL could feature query rewriting capabilities that automatically optimize poorly written queries. Additionally, auto-tuning mechanisms might dynamically adjust server configuration and indexing strategies based on query patterns and workload.Parallel Query Execution: MySQL might leverage parallel query execution capabilities to process large queries faster. Multi-core processors and distributed computing could be better utilized to improve MySQL performance.Advanced Caching Mechanisms: Future MySQL versions might integrate more sophisticated caching mechanisms, such as intelligent caching based on query access patterns, to reduce the load on the database and improve response times.Hardware-Software Integration: Advancements in hardware technology, such as specialized accelerators (e.g., GPUs), could lead to better integration with MySQL, optimizing certain query operations and improving overall performance.Machine learning and AI developments can also impact queries and MySQL performance in the future. Some potential scenarios include:Query Plan Prediction: Machine learning algorithms can analyze historical query execution data and predict optimal query plans for specific types of queries. This can lead to more efficient query execution without relying solely on the traditional rule-based query optimizer.Auto-Tuning: Machine learning models can be applied to auto-tune various MySQL configuration parameters based on observed workloads, ensuring the database is optimally configured for specific application needs.Anomaly Detection: Machine learning techniques can help detect anomalies in query performance, enabling early identification of performance issues and potential optimizations.Index Recommendation: AI-powered systems can suggest appropriate indexes for frequently executed queries by analyzing historical query patterns and access frequencies.Query Rewrite Suggestions: AI can assist in recommending query rewrites or alternative formulations to improve query performance based on historical data and learned patterns.While machine learning and AI have great potential in optimization, they are not a replacement for traditional optimization methods. Combining the strengths of both approaches can lead to even more effective and efficient MySQL performance tuning.ConclusionQuery optimization builds a solid foundation for a high-performing, scalable, and successful MySQL-driven environment. It results in faster response times, reduces server load, and improves resource utilization. This can significantly enhance the speed and efficiency of query execution. Developers, database administrators, data engineers, and IT professionals must prioritize performance tuning and use it as a powerful tool to unlock the full potential of their MySQL databases and applications. If you're eager to expand your knowledge, delve into our tutorial on MySQL CDC for in-depth insights.You can learn more about databases, query optimization, and data insights on our Content Hub.Limitless data movement with free Alpha and Beta connectorsIntroducing: our Free Connector ProgramThe data movement infrastructure for the modern data teams.Try a 14-day free trialAbout the AuthorAditi Prakash is an experienced B2B SaaS writer who has specialized in data engineering, data integration, ELT and ETL best practices for industry-leading companies since 2021.About the AuthorTable of contentsExample H2Example H3Example H4Example H5Example H6Example H2Example H3Example H4Example H5Example H6Get your data syncing in minutesTry Airbyte freeJoin our newsletter to get all the insights on the data stack."
27,"Related postsData Pipeline vs. ETL: Optimize Data Flow (Beginner's Guide)•March 14, 2024•15 min readOn-Premise vs. Cloud Data Warehouses: The Comparison Guide•March 12, 2024•15 min readData Mart vs. Data Lake: Making the Best Choice•March 12, 2024•15 min readWhat is Big Data Integration: Examples and Use Cases•March 12, 2024•15 min readAirbyte is an open-source data integration engine that helps you consolidate your data in your data warehouses, lakes and databases.© 2024 Airbyte, Inc.ProductFeaturesDemo AppConnectorsConnector Development Kit (CDK)Airbyte Open SourceAirbyte CloudAirbyte Self-ManagedCompare Airbyte offersPricingChangelogRoadmapCompare top ELT solutionsRESOURCESDocumentationBlogAirbyte API DocsTerraform Provider DocsCommunityData Engineering ResourcesTutorialsQuickstartsNewsletterResource centerCommunity CallState of Data surveyTop ETL Tools""How to Sync"" TutorialsCOMPANYCompany HandbookAbout UsCareersOpen employee referral programAirbyte YC Startup ProgramPartnersPressData protection - Trust reportTerms of ServicePrivacy PolicyCookie PreferencesDo Not Sell/Share My Personal InformationContact SalesGet answers quick on Airbyte SlackHi there! Did you know our Slack is the most active Slack community on data integration? It’s also the easiest way to get help from our vibrant community.Join Airbyte SlackI’m not interested in joining"
28,How to Increase Database Performance – 6 Easy Tips - DNSstuff
28,Skip to content
28,Menu
28,Networking
28,Network Monitoring Software
28,Bandwidth Monitoring
28,Scan Network for IP addresses
28,Network Traffic Monitoring Tools
28,IP Address Conflict
28,Network Mapping Tools
28,IPAM Software
28,Observability
28,What is Observability?
28,Server Monitoring Best Practices
28,Systems
28,IT Inventory Software
28,SSL Certificate Monitoring
28,Windows Server Performance Monitoring
28,IIS Performance Monitoring
28,Databases
28,SQL Server Queries Monitoring
28,SQL Server Performance
28,Oracle Database Monitoring
28,How to Increase Database Performance
28,Security
28,Help Desk
28,ITSM
28,IT Service Management (ITSM) Tools
28,ITIL Event Management
28,Incident Management Tools
28,Help Desk vs Service Desk
28,IT Asset Management Software
28,Help Desk Ticketing System
28,Free Help Desk Software
28,Free Tools
28,Compare
28,"How to Increase Database Performance – 6 Easy TipsBy Staff Contributor on July 28, 2023"
28,"Database administrators are all too familiar with the frustration of receiving an endless stream of calls about slow online performance. Instead of trying to resolve each individual issue as it arises, a better solution is to undertake database performance tuning activities that will improve online performance for all your end users. These activities can help you identify any bottlenecks in your system and ensure your infrastructure is able to handle increased loads."
28,"There are several steps you can take to increase database performance. The following six easy tips can help you prevent or rectify possible issues with database performance. Even with these tips, it’s important to remember the best way to increase database performance is always by using the right tools. Based on my experiences and tests I can recommend SolarWinds® Database Performance Analyzer (DPA) and Database Performance Monitor (DPM)."
28,By using even the free trial versions of these tools you’ll be able to:
28,"Monitor and analyze the performance of database servers running individually, in clusters, and as cloud infrastructure."
28,Receive and implement database performance advice on specific databases and related queries.
28,"Monitor database performance in real time and analyze past periods. In addition, the software tested by us enables the detection of anomalies in the load time and the location of bottlenecks in database performance."
28,"After going through the best tips for manually improving performance, I’ll take a closer look at some of the best tools to help you improve performance even further."
28,Why Is Increasing Database Performance Important?Tips To Increase Database PerformanceTools That Can Help YouSummary
28,Why Is Increasing Database Performance Important?
28,"People often wonder whether it’s important to increase database performance. The truth is your business can only ever be as successful as your IT operations allow it to be. In fact, a high-functioning database can have a huge impact on corporate profitability. When data retrieval is slowed down by anything from a poorly written query to an indexing issue, a bottleneck that slows down performance and lowers productivity for the entire organization can emerge. When you learn how to increase database performance, you’re better able to avoid unnecessary financial loss as a result of server inefficiencies."
28,"There are also many financial gains that come with improving the end-user experience. Since your customer-facing websites and applications retrieve data from your centralized database, inefficient indexes and suboptimal queries can have just as big an impact on customers as on your internal end users. As a result, your customer satisfaction is directly linked to database performance. That means knowing how to increase database performance can be one of the most important customer service tools in your toolbox."
28,Tips to Increase Database Performance
28,"While there are many ways you can go about learning how to increase database performance, these six have proven to be some of the most effective and impactful when it comes to avoiding performance degradation."
28,Tip 1: Optimize Queries
28,"In many cases database performance issues are caused by inefficient SQL queries. Optimizing your SQL queries is one of the best ways to increase database performance. When you try to do that manually, you’ll encounter several dilemmas around choosing how best to improve query efficiency. These include understanding whether to write a join or a subquery, whether to use EXISTS or IN, and more. When you know the best path forward, you can write queries that improve efficiency and thus database performance as a whole. That means fewer bottlenecks and fewer unhappy end users."
28,The best way to optimize queries is to use a database performance analysis solution that can guide your optimization efforts by directing you to the most inefficient queries and offering expert advice on how best to improve them.
28,Tip 2: Improve Indexes
28,"In addition to queries, the other essential element of the database is the index. When done right, indexing can increase your database performance and help optimize the duration of your query execution. Indexing creates a data structure that helps keep all your data organized and makes it easier to locate information. Because it’s easier to find data, indexing increases the efficiency of data retrieval and speeds up the entire process, saving both you and the system time and effort."
28,Tip 3: Defragment Data
28,"Data defragmentation is one of the best approaches to increasing database performance. Over time, with so much data constantly being written to and deleted from your database, your data can become fragmented. That fragmentation can slow down the data retrieval process as it interferes with a query’s ability to quickly locate the information it’s looking for. When you defragment data, you allow for relevant data to be grouped together and you erase index page issues. That means your I/O related operations will run faster."
28,Tip 4: Increase Memory
28,"The efficiency of your database can suffer significantly when you don’t have enough memory available for the database to work correctly. Even if it seems like you have a lot of memory in total, you might not be meeting the demands of your database. A good way to figure out if you need more memory is to check how many page faults your system has. When the number of faults is high, it means your hosts are either running low on or completely out of available memory. Increasing your memory allocation will help boost efficiency and overall performance."
28,Tip 5: Strengthen CPU
28,"A better CPU translates directly into a more efficient database. That’s why you should consider upgrading to a higher-class CPU unit if you’re experiencing issues with your database performance. The more powerful your CPU is, the less strain it’ll have when dealing with multiple requests and applications. When assessing your CPU, you should keep track of all the elements of CPU performance, including CPU ready times, which tell you about the times your system tried to use the CPU, but couldn’t because the resources were otherwise occupied."
28,Tip 6: Review Access
28,"Once you know your database hardware is working well, you need to review your database access, including which applications are actually accessing your database. If one of your services or applications is suffering from poor database performance, it’s important not to jump to conclusions about which service or application is responsible for the issue. It’s possible a single client is experiencing the bad performance, but it’s also possible the database as a whole is having issues. Dig into who and what is accessing the database and if it’s only one service that’s having an issue, drill down into its metrics to try and find the root cause."
28,"While these tips can help you increase database performance, manual efforts can only do so much. If you want to save time and energy while strengthening your performance optimization efforts, you should invest in a database performance analysis and monitoring solution. The following tools are the best ones on the market when it comes to increasing database performance."
28,Tools That Can Help You
28,1. SolarWinds Database Performance Analyzer (DPA)
28,"SolarWinds DPA is a powerful multidimensional database performance solution that supports PostgreSQL, MySQL, IBM DB2, Amazon Aurora, SAP ASE, Oracle, Microsoft SQL Server, MariaDB, and Azure SQL databases. It focuses on bottleneck identification and offers automated index and query optimization advisors to help you target your database performance optimization efforts."
28,The solution uses response time analysis — which measures the actual amount of time it takes to complete an operation — to tune queries and improve overall database performance. DPA gives users the insights they need to align their resource provisioning with database performance to help ensure hardware isn’t interfering with performance.
28,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
28,"Working in a database environment often requires database administrators or developers (DevOp) to observe the behavior of the entire environment in which their databases and applications run. DPA easily integrates the process of monitoring databases, hardware, and other entities used by applications and databases."
28,DPA can also be integrated with SolarWinds Storage Resource Monitor (SRM) to get a deeper understanding of your database performance. When DPA and SRM are integrated you can get contextually relevant information on the storage objects related to the databases being monitored by DPA. You can then correlate storage performance with the relevant databases.
28,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
28,"You can try a 14-day free trial, during which DPA is fully functional."
28,Learn more about DPA
28,Download free trial
28,2. SolarWinds® SQL Sentry
28,"SQL Sentry is built to help you quickly identify long-running and high-impact queries, so you can resolve performance problems and quickly increase database performance."
28,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
28,Of note is how SQL Sentry can help you more easily perform the following:
28,Identifying top resource-consuming queries over time with the ability to tune them in the same interface
28,Defining alerting and automated remediation for known performance problems
28,Knowing when a new release or data churn over time has changed the performance profile for one or more queries
28,You can try SQL Sentry free for 14 days.
28,3. SolarWinds Database Performance Monitor (DPM)
28,"Database Performance Monitor is another great solution from SolarWinds. This software puts a bigger focus on monitoring database performance as compared to DPA. DPM offers a convenient and efficient means of monitoring your database performance in real time, around the clock. Check-ups on performance are simple and DPM gives you access to expert guidance that can help you improve database performance. The tool sends you instant recommendations around underperforming queries or server configuration changes. What is important is the way DPM works—it’s most often used to monitor no-SQL databases. This monitoring tool for databases such as Redis, MongoDB, and Azure can work locally and in cloud and hybrid environments. As it works in SaaS model, it allows access to the user dashboard in a web-based user interface."
28,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
28,"DPM helps you understand how your queries are impacting performance and gets to the bottom of any problems the tool detects, thanks to extensive correlated data. Try DPM free for 14 days."
28,4. SolarWinds Server Configuration Monitor (SCM)
28,"Another good solution to consider if your business runs on SQL databases is SolarWinds SCM, which runs SQL queries to connect and monitor any relational databases in your system including MySQL, Microsoft SQL Server, PostgreSQL, or Oracle. SCM collects data through queries then saves it and monitors it for changes. This can help you stay on top of any database configuration changes — including changes to user permissions and schemas — that could negatively impact database performance."
28,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
28,SCM comes with a free 30-day trial.
28,5. Paessler PRTG Network Monitor
28,"Paessler PRTG Network Monitor is another great tool if you’re looking to increase your database performance. Like DPM, it puts an emphasis on database monitoring. In fact, one of the benefits of the tool is it’s a one-stop shop for infrastructure monitoring covering databases and applications, bandwidth, packets, traffic, cloud services, uptime, ports, IPs, virtual environments, hardware, web services, security, physical environments, disk usage, and IoT devices. The tool is easy to set up and highly customizable, making it a good fit for anyone looking for a simple solution to their database and infrastructure monitoring needs."
28,© 2023 PAESSLER AG. All rights reserved.
28,A free version of PRTG is available for 30 days.
28,Increasing Database Performance Through Monitoring and Analysis
28,"If you’re looking for the best way to increase your database performance, there’s no better option than using professional software."
28,"I compared ease of use, integration with other programs, technical support during installation, and during use of the tools. This allowed me to select and recommend SolarWinds products to database administrators looking for a scalable solution for their daily work in database analysis and monitoring. Although there many monitoring and analysis products on the market, SolarWinds DPA, SQL Sentry, and SCM are a good place to start. The tools cost starts at USD$1,111.* You can check how it works with a 14 days trial version (available on official SolarWinds website: click here)."
28,Related Posts
28,10 MySQL Database Performance Tuning Tips
28,Oracle 12c Performance Tuning — Top Tips for Database Admins
28,Top Db2 Performance Tuning Tips
28,Categories Systems
28,Post navigation
28,MySQL vs. MSSQL – Performance and Main DifferencesCommon Database Problems and Performance Issues
28,Most Popular Posts
28,Ultimate Guide to Windows Event Logs for 2024
28,What Is Syslog? Syslog Server vs. Event Log Explained + Recommended Syslog Management Tool
28,5 Best Tripwire Alternatives 2024
28,"Jitter, Packet Loss, and Latency in Network Performance"
28,Best Syslog Servers in 2024
28,13 Best Service Request Management Software of 2024
28,Top 8 Observability Tools
28,10 Best IT Self-Service Software in 2024
28,8 Best Service-Level Management Tools for 2024
28,Dameware Products Review 2024
28,Languages
28,English
28,Deutsch
28,Français
28,"© 2024 SolarWinds Worldwide, LLC. All rights reserved."
28,About Us |
28,Trademarks |
28,Privacy Policy |
28,Terms Of Use
28,Scroll back to top
29,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML) | Managing site performance and scalability | Drupal Wiki guide on Drupal.org"
29,Skip to main content
29,Skip to search
29,"Can we use first and third party cookies and web beacons to understand our audience, and to tailor promotions you see?Yes, pleaseNo, do not track me"
29,Drupal.org home
29,Why Drupal?About Drupal
29,Platform overview
29,Drupal 10
29,Content Authoring
29,Content as a Service
29,Decoupled
29,Accessibility
29,Marketing Automation
29,Multilingual
29,Security
29,Personalization
29,Case studies
29,Video series
29,News
29,Use casesFor Developers
29,For Marketers
29,E-commerce
29,Education
29,FinTech
29,Government
29,Healthcare
29,High Tech
29,Nonprofit
29,Retail
29,Travel
29,ResourcesInstalling Drupal
29,Documentation
29,User guide
29,Local Development Guide
29,Security
29,News
29,Blog
29,Drupal 7 Migrations
29,ServicesFind an Agency Partner
29,Find a Migration Partner
29,Integrations & Hosting
29,Training
29,Become a Certified Partner
29,Partner Press
29,CommunityHow to Contribute
29,About the Community
29,Support
29,Community Governance
29,Jobs/Careers
29,EventsDrupalCon Portland 2024
29,DrupalCon Barcelona 2024
29,Community Events
29,DownloadDownload
29,Modules
29,Themes
29,Distributions
29,Issue queues
29,Browse Repository
29,GiveDrupal Association
29,Become a Supporter
29,Become a Certified Partner
29,Become a Member
29,Make a Donation
29,Discover Drupal
29,Drupal Swag Shop
29,DemoDemo online
29,Download
29,Return to content
29,Search form
29,Search
29,Log in
29,Create account
29,Documentation
29,Search
29,Drupal WikiDrupal 7Managing site performance and scalability
29,Support for Drupal 7 is ending on 5 January 2025—it’s time to migrate to Drupal 10! Learn about the many benefits of Drupal 10 and find migration tools in our resource center.
29,Learn more
29,Advertising sustains the DA. Ads are hidden for members. Join today
29,On this page
29,Basic settings
29,Theme optimization
29,Coding standard and proper use of already existing core API
29,Secure codes
29,DB Query optimization in codes
29,DB table optimization
29,Disable unnecessary modules
29,Remove unnecessary contents and others
29,Cache modules
29,Make changes according to Google Pagespeed and yahoo YSlow suggestions
29,MySQL Settings
29,Apache settings
29,"Also, we can check these options :"
29,1) Turn Page Caching On
29,2) Turn Views caching on
29,Managing site performance and scalability
29,Planning for Performance
29,Caching to improve performance
29,Changing PHP memory limits
29,Content Delivery Network [CDN]
29,Design for Low Bandwidth
29,Increase upload size in your php.ini
29,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
29,Optimizing MySQL
29,Randomizing MySQL Users For Exceeded max_questions Error
29,Server tuning considerations
29,Tuning php.ini for Drupal
29,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
29,Last updated on
29,26 November 2023
29,"Drupal 7 will no longer be supported after January 5, 2025. Learn more and find resources for Drupal 7 sites"
29,"This documentation needs review. See ""Help improve this page"" in the sidebar."
29,Basic settings
29,Configure cron job (for Drupal 6 http://drupal.org/project/poormanscron)
29,Make sure all cache tables are clearing properly especially cache_form
29,Enable cache options on the performance page
29,"(For Drupal 6, http://drupal.org/project/advagg )"
29,Theme optimization
29,Manually Remove blankspaces and comments from .tpl
29,No indentation in .tpl
29,Turn on CSS and JS aggregation in the performance page
29,Manually reduce css file size by removing duplicate and combine similar together
29,Move codes to functions that should be in a custom common module. Use functions for similar problems instead of coding separately. Refer core API
29,Coding standard and proper use of already existing core API
29,http://drupal.org/coding-standards
29,https://drupalize.me/videos/understanding-drupal-coding-standards?p=2012
29,Secure codes
29,http://drupal.org/writing-secure-code
29,DB Query optimization in codes
29,Join db queries whenever possible
29,"For Db update and insert, use core API"
29,Use drupal standard http://drupal.org/coding-standards
29,DB table optimization
29,http://drupal.org/project/db_maintenance
29,Disable unnecessary modules
29,Devel
29,Statistics
29,Update status
29,Use syslog instead of Database logging
29,Remove unnecessary contents and others
29,Cache modules
29,"Make use of object caches to reduce database overhead, e.g. Memcache, Redis or APC"
29,https://drupal.org/project/authcache
29,Some module may help improve
29,http://drupal.org/project/ajaxblocks (not available with Drupal 8 & 9 )
29,Make changes according to Google Pagespeed and yahoo YSlow suggestions
29,MySQL Settings
29,Cache Size say 32MB in MySQL
29,Use https://github.com/initlabopen/mysqlconfigurer for fully automated MySQL performance tuning
29,Apache settings
29,DNS lookup : OFF
29,Set FollowSymLinks everywhere and never set SymLinksIfOwnerMatch
29,Avoid content negotiation. Or use type-map files rather than Options MultiViews directive
29,"KeepAlive on, and KeepAliveTimeout very low (1 or 2 sec)"
29,Disable or comment access.log settings
29,Enable mod_deflate or mod_gzip
29,Install APC server with higher memory limit apc.shm_size = 64
29,"Also, we can check these options :"
29,1) Turn Page Caching On
29,"What page caching does is that instead of using a bunch of database queries to get the data used in making a typical web page, the rendered contents of the web page are stored in a separate database cache table so that it can be recalled quicker. If you have 10 people visiting the site from different computers, Drupal first looks into the database cache table to see if the page is there, if it is, it just gives them the page. Think of saving the output of 50 separate queries so that is accessible with a single query. You obviously are reducing the SQL queries required by a lot. What the page cache table actually stores is HTML content."
29,"Page Caching is that it only works to optimize the page load time for Anonymous users. This is because when you are logged in, you might have blocks that show up on the page that are customized for you, if it served everybody on the same page, they would see your customized information (think of a My Recent Posts block), so Drupal does not use the Page Cache for Authenticated users automatically. This allows you to turn Page Caching on and still get the benefit of Anonymous user page load times but does not break the site for Authenticated users. There are other caching options that will help with Authenticated user page performance, we will talk about those later."
29,"To enable Page Caching, you go to Configuration | Development and select the checkbox next to ""Cache pages for anonymous users""."
29,2) Turn Views caching on
29,"As mentioned when talking about Page Caching only working for anonymous users above, there are other caching options for helping with Authenticated user page performance. One of those options is to turn on caching for blocks and pages that you create using the Views module. This allows you to cache the output of the query used to generate the view, or the end HTML output of your View, and you can tune the cache for them separately. And realize too that this means you can cache portions of a page if you are using one or several Views blocks in the page, it will just cache that block in the page, not the whole page."
29,See more (Drupal7) @ https://www.lullabot.com/articles/a-beginners-guide-to-caching-data-in-d...
29,Help improve this page
29,Page status:
29,Needs review
29,You can:
29,"Log in, click Edit, and edit this page"
29,"Log in, click Discuss, update the Page status value, and suggest an improvement"
29,Log in and create a Documentation issue with your suggestion
29,"Drupal’s online documentation is © 2000-2024 by the individual contributors and can be used in accordance with the Creative Commons License, Attribution-ShareAlike 2.0. PHP code is distributed under the GNU General Public License."
29,Thank you to these Drupal contributors
29,Top Drupal contributor Acquia would like to thank their partners for their contributions to Drupal.
29,Infrastructure management for Drupal.org provided by
29,News itemsNews
29,Planet Drupal
29,Social media
29,Sign up for Drupal news
29,Security advisories
29,Jobs
29,Our communityCommunity
29,"Services, Training & Hosting"
29,Contributor guide
29,Groups & meetups
29,DrupalCon
29,Code of conduct
29,DocumentationDocumentation
29,Drupal Guide
29,Drupal User Guide
29,Developer docs
29,API.Drupal.org
29,Drupal code baseDownload & Extend
29,Drupal core
29,Modules
29,Themes
29,Distributions
29,Governance of communityAbout
29,Web accessibility
29,Drupal Association
29,About Drupal.org
29,Terms of service
29,Privacy policy
29,Drupal is a registered trademark of Dries Buytaert.
30,Inquiry Regarding Database Size and Performance Optimization -
30,EspoCRM Open Source Community Forum
30,Login or Sign Up
30,Logging in...
30,Remember me
30,Log in
30,Forgot password or user name?
30,or Sign Up
30,Log in with
30,Open source CRM support forum
30,Search in titles only
30,Search in General Discussion only
30,Search
30,Advanced Search
30,Forums
30,Today's Posts
30,Mark Channels Read
30,Member List
30,Calendar
30,"If this is your first visit, be sure to"
30,check out the FAQ by clicking the
30,link above. You may have to register
30,"before you can post: click the register link above to proceed. To start viewing messages,"
30,select the forum that you want to visit from the selection below.
30,Announcement
30,Collapse
30,No announcement yet.
30,Inquiry Regarding Database Size and Performance Optimization
30,Collapse
30,Collapse
30,Posts
30,Latest Activity
30,Photos
30,Search
30,Page
30,of 1
30,Filter
30,Time
30,All Time
30,Today
30,Last Week
30,Last Month
30,Show
30,All
30,Discussions only
30,Photos only
30,Videos only
30,Links only
30,Polls only
30,Events only
30,Filtered by:
30,Clear All
30,new posts
30,Previous
30,template
30,Next
30,yubrajkafle
30,Senior Member
30,Join Date: Oct 2020
30,Posts: 220
30,Share
30,Tweet
30,Inquiry Regarding Database Size and Performance Optimization
30,"08-15-2023, 03:30 AM"
30,"Hello Developer,"
30,"I am writing to inquire about a concern related to the size of my database and instances. Our organization has been utilizing Espo for the past three years, primarily utilizing the stream section to post updates."
30,"Recently, I performed a database backup in preparation for upgrading to the latest instance. To my surprise, the actual size of the database turned out to be approximately 20 GB. Upon further investigation using PHPMyAdmin, I discovered that a significant portion, around 98%, of the database size is attributed to a single entity, namely ""note"" (stream)."
30,"In light of this, I have a few questions regarding the potential impact on performance due to the size of the ""stream"" entity. Could you please advise if this substantial size of the ""stream"" entity might adversely affect the overall performance of our system? Additionally, I am curious about the feasibility of maintaining the ""stream"" section in its current state, especially considering the potential for further size increases in the future. Is there a possibility to optimize or reduce the size of the ""stream"" entity, or is it advisable to continue operating the software with the existing size?"
30,"Given the long-term perspective, I am concerned about the escalating size of the ""stream"" entity and its implications on system performance. Your guidance on how to effectively manage this situation would be greatly appreciated."
30,Thank you for your attention and assistance.
30,"Best regards,"
30,Yubraj Kafle
30,Tags:
30,None
30,yuri
30,Member
30,Join Date: Mar 2014
30,Posts: 7511
30,Share
30,Tweet
30,"08-15-2023, 09:55 AM"
30,"Hi,"
30,"Consider deleting note entries manually (by running SQL). Find out which type of note records you have predominantly, provide some examples. Then it will be easier to give advices."
30,The large note table can impact on performance of the Stream dashlet and the Stream panel on the user detail view. It should not impact on anything else.
30,Comment
30,Post
30,Cancel
30,dimyy
30,Active Community Member
30,Join Date: Jun 2018
30,Posts: 485
30,Share
30,Tweet
30,"08-16-2023, 04:32 AM"
30,You can enable slow query log and see which queries is impacto on performance.
30,https://dev.mysql.com/doc/refman/8.0...query-log.html (if you use mysql)
30,https://mariadb.com/kb/en/slow-query-log-overview/ (mariadb)
30,Comment
30,Post
30,Cancel
30,Previous
30,template
30,Next
30,What is CRM?
30,Documentation
30,CRM Hints and Tips
30,Video Tutorials
30,Download
30,Blog
30,Terms of Service
30,Privacy Policy
30,Go to top
30,"Powered by vBulletin® Version 5.7.5 Copyright © 2024 MH Sub I, LLC dba vBulletin. All rights reserved."
30,All times are GMT. This page was generated at 10:53 AM.
30,Working...
30,Yes
30,Cancel
33,ZFSTuningGuide - FreeBSD Wiki
33,Search:
33,Login
33,ZFSTuningGuide
33,RecentChangesFindPageHelpContentsZFSTuningGuide
33,Immutable PageCommentsInfoAttachments
33,More Actions:
33,Raw Text
33,Print View
33,Render as Docbook
33,Delete Cache
33,------------------------
33,Check Spelling
33,Like Pages
33,Local Site Map
33,------------------------
33,Rename Page
33,Delete Page
33,------------------------
33,Subscribe User
33,------------------------
33,Remove Spam
33,Revert to this revision
33,Package Pages
33,Sync Pages
33,------------------------
33,Load
33,Save
33,SlideShow
33,Contents
33,ZFS Tuning Guide
33,i386
33,amd64
33,Generic ARC discussion
33,L2ARC discussion
33,Application Issues
33,General Tuning
33,Deduplication
33,Suggestions
33,References
33,NFS tuning
33,MySQL
33,"Scrub and Resilver Performance See also: Solaris: ZFS Evil Tuning Guide, loader.conf(5), sysctl(8)."
33,ZFS Tuning Guide
33,"OpenZFS documentation recommends a minimum of 2GB of memory for ZFS; additional memory is strongly recommended when the compression and deduplication features are enabled. Depending on your workload, it may be possible to use ZFS on systems with less memory, but it requires careful tuning to avoid panics from memory exhaustion in the kernel. A 64-bit system is preferred due to its larger address space and better performance on 64-bit variables, which are used extensively by ZFS. 32-bit systems are supported though, with sufficient tuning. History of FreeBSD releases with ZFS is as follows: 7.0+ - original ZFS import, ZFS v6; requires significant tuning for stable operation (no longer supported) 7.2 - still ZFS v6, improved memory handling, amd64 may need no memory tuning (no longer supported) 7.3+ - backport of new ZFS v13 code, similar to the 8.0 code 8.0 - new ZFS v13 code, lots of bug fixes - recommended over all past versions. (no longer supported) 8.1+ - ZFS v14 8.2+ - ZFS v15 8.3+ - ZFS v28 9.0+ - ZFS v28"
33,i386
33,"Typically you need to increase vm.kmem_size_max and vm.kmem_size (with vm.kmem_size_max >= vm.kmem_size) to not get kernel panics (kmem too small). The value depends upon the workload. If you need to extend them beyond 512M, you need to recompile your kernel with increased KVA_PAGES option, e.g. add the following line to your kernel configuration file to increase available space for vm.kmem_size beyond 1 GB: options KVA_PAGES=512 To chose a good value for KVA_PAGES read the explanation in the sys/i386/conf/NOTES file. By default the kernel receives 1 GB of the 4 GB of address space available on the i386 architecture, and this is used for all of the kernel address space needs, not just the kmem map."
33,"By increasing KVA_PAGES you can allocate a larger proportion of the 4 GB address space to the kernel (2 GB in the above example), allowing more room to increase vm.kmem_size."
33,"The trade-off is that user applications have less address space available, and some programs (e.g. those that rely on mapping data at a fixed address that is now in the kernel address space, or which require close to the full 3 GB of address space themselves) may no longer run. If you change KVA_PAGES and the system reboots (no panic) after running a while this may be because the address space for userland applications is too small now. For *really* memory constrained systems it is also recommended to strip out as many unused drivers and options from the kernel (which will free a couple of MB of memory). A stable configuration with vm.kmem_size=""1536M"" has been reported using an unmodified 7.0-RELEASE kernel, relatively sparse drivers as required for the hardware and options KVA_PAGES=512. Some workloads need greatly reduced ARC size and the size of VDEV cache. ZFS manages the ARC through a multi-threaded process. If it requires more memory for ARC ZFS will allocate it. Previously it exceeded arc_max (vfs.zfs.arc_max) from time to time, but with 7.3 and 8-stable as of mid-January 2010 this is not the case anymore. On memory constrained systems it is safer to use an arbitrarily low arc_max. For example it is possible to set vm.kmem_size and vm.kmem_size_max to 512M, vfs.zfs.arc_max to 160M, keeping vfs.zfs.vdev.cache.size to half its default size of 10 Megs (setting it to 5 Megs can even achieve better stability, but this depends upon your workload). There is one example (CySchubert) of ZFS running nicely on a laptop with 768 Megs of physical RAM with the following settings in /boot/loader.conf: vm.kmem_size=""330M"" vm.kmem_size_max=""330M"" vfs.zfs.arc_max=""40M"" vfs.zfs.vdev.cache.size=""5M"" Kernel memory should be monitored while tuning to ensure a comfortable amount of free kernel address space. The following script will summarize kernel memory utilization and assist in tuning arc_max and VDEV cache size. #!/bin/sh -"
33,"TEXT=`kldstat | awk 'BEGIN {print ""16i 0"";} NR>1 {print toupper($4) ""+""} END {print ""p""}' | dc`"
33,DATA=`vmstat -m | sed -Ee '1s/.*/0/;s/.* ([0-9]+)K.*/\1+/;$s/$/1024*p/' | dc`
33,TOTAL=$((DATA + TEXT))
33,"echo TEXT=$TEXT, `echo $TEXT | awk '{print $1/1048576 "" MB""}'`"
33,"echo DATA=$DATA, `echo $DATA | awk '{print $1/1048576 "" MB""}'`"
33,"echo TOTAL=$TOTAL, `echo $TOTAL | awk '{print $1/1048576 "" MB""}'`"
33,"Note: Perhaps there is a more precise way to calculate / measure how large of a vm.kmem_size setting can be used with a particular kernel, but the authors of this wiki do not know it."
33,Experimentation does work.
33,"However, if you set vm.kmem_size too high in loader.conf, the kernel will panic on boot."
33,"You can fix this by dropping to the boot loader prompt and typing set vm.kmem_size=""512M"" (or a similar smaller number known to work.) The vm.kmem_size_max setting is not used directly during the system operation (i.e. it is not a limit which kmem can ""grow"" into) but for initial autoconfiguration of various system settings, the most important of which for this discussion is the ARC size. If kmem_size and arc_max are tuned manually, kmem_size_max will be ignored, but it is still required to be set. The issue of kernel memory exhaustion is a complex one, involving the interaction between disk speeds, application loads and the special caching ZFS does. Faster drives will write the cached data faster but will also fill the caches up faster. Generally, larger and faster drives will need more memory for ZFS. To increase performance, you may increase kern.maxvnodes (in /etc/sysctl.conf) way up if you have the RAM for it (e.g. 400000 for a 2GB system). On i386, keep an eye on vfs.numvnodes during production to see where it stabilizes. (AMD64 uses direct mapping for vnodes, so you don't have to worry about address space for vnodes on this architecture)."
33,amd64
33,"NOTE (gcooper): this blanket statement is far from true 100% of the time, depending on how the system with ZFS is being used. FreeBSD 7.2+ has improved kernel memory allocation strategy and no tuning may be necessary on systems with more than 2 GB of RAM."
33,Generic ARC discussion
33,"The value for vfs.zfs.arc_max needs to be smaller than the value for vm.kmem_size (not only ZFS is using the kmem). To monitor the ARC, you should install the sysutils/zfs-stats port;"
33,"the port is an evolution of the arc_stat.pl script available in Solaris that was ported to FreeBSD by FreeBSD contributor, jhell. To improve the random read performance, a separate L2ARC device can be used (zpool add <pool> cache <device>). A cheap solution is to add an USB memory stick (see http://www.leidinger.net/blog/2010/02/10/making-zfs-faster/). The high performance solution is to add a SSD. Using a L2ARC device will increase the amount of memory ZFS needs to allocate, see http://www.mail-archive.com/zfs-discuss@opensolaris.org/msg34674.html for more info."
33,L2ARC discussion
33,"ZFS has the ability to extend the ARC with one or more L2ARC devices, which provides the best benefit for random read workloads."
33,These L2ARC devices should be faster and/or lower latency than the storage pool.
33,Generally speaking this limits the useful choices to flash based devices.
33,In very large pools the ability to have devices faster than the pool may be problematic.
33,In smaller pools it may be tempting to use a spinning disk as a dedicated L2ARC device.
33,Generally this will result in lower pool performance (and definitely capacity) than if it was just placed in the pool.
33,"There may be scenarios in lower memory systems where a single 15K SAS disk can improve the performance of a small pool of 5.4k or 7.2 drives, but this is not a typical case. By default the L2ARC does not attempt to cache prefetched/streaming workloads, on the assumption that most data of this type is sequential and the combined throughput of your pool disks exceeds the throughput of the L2ARC devices, and therefore, this workload is best left for the pool disks to serve. This is usually the case. If you believe otherwise (number of L2ARC devices X their max throughput > number of pool disks X their max throughput, or you are not doing large amounts of sequential access), then this can be toggled with the following sysctl: vfs.zfs.l2arc_noprefetchThe default value of 1 does not allow caching of streaming and/or sequential workloads, and will not read from L2ARC when prefetching blocks."
33,"Switching it to 0 will allow prefetched/streaming reads to be cached, and may significantly improve performance if you are storing many small files in a large directory hierarchy (since many metadata blocks are read via the prefetcher and would ordinarily always be read from pool disks). The default throttling of loading the L2ARC device is 8 Mbytes/sec, on the assumption that the L2ARC is warming up from a random read workload from spinning disks, for which 8 Mbytes/sec is usually more than the spinning disks can provide. For example, at a 4 Kbyte I/O size, this is 2048 random disk IOPS, which may take at least 20 pool disks to drive. Should the L2ARC throttling be increased from 8 Mbytes, it would make no difference in many configurations, which cannot provide more random IOPS. The downside of increasing the throttling is CPU consumption: the L2ARC periodically scans the ARC to find buffers to cache, based on the throttling size. If you increase the throttling but the pool disks cannot keep up, you burn CPU needlessly. In extreme cases of tuning, this can consume an entire CPU for the ARC scan. If you are using the L2ARC in its typical use case: say, fewer than 30 pool disks, and caching a random read workload for ~4 Kbyte I/O which is mostly being pulled from the pool disks, then 8 Mbytes is usually sufficient. If you are not this typical use case: say, you are caching streaming workloads, or have several dozens of disks, then you may want to consider tuning the rate. Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default. It can be tuned by setting the following sysctls: vfs.zfs.l2arc_write_max"
33,vfs.zfs.l2arc_write_boostThe former value sets the runtime max that data will be loaded into L2ARC.
33,"The latter can be used to accelerate the loading of a freshly booted system. Note that the same caveats apply about these sysctls and pool imports as the previous one. While you can improve the L2ARC warmup rate, keep an eye on increased CPU consumption due to scanning by the l2arc_feed_thread(). Eg, use DTrace to profile on-CPU thread names (see DTrace One-Liners). The known caveats: There's no free lunch."
33,"A properly tuned L2ARC will increase read performance, but it comes at the price of decreased write performance. The pool essentially magnifies writes by writing them to the pool as well as the L2ARC device."
33,Another interesting effect that's been observed is a falloff in L2ARC performance when doing a streaming read from L2ARC while simultaneously doing a heavy write workload.
33,My conjecture is that the write can cause cache thrashing but this hasn't been confirmed at this time. Given a working set close to ARC size an L2ARC can actually hurt performance.
33,"If a system has a 14GB ARC and a 13GB working set, adding an L2ARC device will rob ARC space to map the L2ARC."
33,"If the reduced ARC size is smaller than the working set reads will be evicted from the ARC into the (ostensibly slower) L2ARC. Multiple L2ARC devices are concatenated, there's no provision for mirroring them."
33,If a heavily used L2ARC device fails the pool will continue to operate with reduced performance.
33,There's also no provision for striping reads across multiple devices.
33,If the blocks for a file end up in multiple devices you'll see striping but there's no way to force this behavior. Be very careful when adding devices to a production pool.
33,By default zpool add stripes vdevs to the pool.
33,"If you do this you'll end up striping the device you intended to add as an L2ARC to the pool, and the only way to remove it will be backing up the pool, destroying it, and recreating it. Many SSDs benefit from 4K alignment."
33,Using gpart and gnop on L2ARC devices can help with accomplishing this.
33,Because the pool ID isn't stored on hot spare or L2ARC devices they can get lost if the system changes device names. The caveat about only giving ZFS full devices is a solarism that doesn't apply to FreeBSD.
33,On Solaris write caches are disabled on drives if partitions are handed to ZFS.
33,On FreeBSD this isn't the case.
33,Application Issues
33,"ZFS is a copy-on-write filesystem. As such metadata from the top of the hierarchy is copied in order to maintain consistency in case of sudden failure, i.e. loss of power during a write operation. This obviates the need for an fsck-like requirement of ZFS filesystems at boot. However the downside to this is that applications which perform updates in place to large files, e.g. databases, will likely perform poorly in this application of the filesystem due to excessive I/O from copy-on-write (a fast SLOG device -- e.g. a SSD -- can help regarding the write performance of databases or any application which is doing synchronous writes (e.g. open with O_FSYNC) to the FS to make sure the data is on non-volatile storage when the write-call returns). Additionally, database applications, such as Oracle, maintain a large cache (called the SGA in Oracle) in memory will perform poorly due to double caching of data in the ARC and in the application's own cache. Reducing the ARC to a minimum can improve performance of applications which maintain their own cache. At ZFS Best Practices Guide there are some generic recommendations for ZFS on Solaris which mostly apply to FreeBSD too."
33,General Tuning
33,There are some changes that can be made to improve performance in certain situations and avoid the bursty IO that's often seen with ZFS. Loader tunables (in /boot/loader.conf): # Disable ZFS prefetching
33,# http://southbrain.com/south/2008/04/the-nightmare-comes-slowly-zfs.html
33,"# Increases overall speed of ZFS, but when disk flushing/writes occur,"
33,# system is less responsive (due to extreme disk I/O).
33,# NOTE: Systems with 4 GB of RAM or more have prefetch enabled by default.
33,"vfs.zfs.prefetch_disable=""1"""
33,# Decrease ZFS txg timeout value from 30 (default) to 5 seconds.
33,This
33,"# should increase throughput and decrease the ""bursty"" stalls that"
33,# happen during immense I/O with ZFS.
33,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007343.html
33,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007355.html
33,# default in FreeBSD since ZFS v28
33,"vfs.zfs.txg.timeout=""5"""
33,Sysctl variables (/etc/sysctl.conf):
33,"# Increase number of vnodes; we've seen vfs.numvnodes reach 115,000"
33,# at times.
33,"Default max is a little over 200,000."
33,Playing it safe...
33,# If numvnodes reaches maxvnode performance substantially decreases.
33,kern.maxvnodes=250000
33,# Set TXG write limit to a lower threshold.
33,"This helps ""level out"""
33,"# the throughput rate (see ""zpool iostat"")."
33,A value of 256MB works well
33,"# for systems with 4 GB of RAM, while 1 GB works well for us w/ 8 GB on"
33,# disks which have 64 MB cache. <<BR>>
33,"# NOTE: in <v28, this tunable is called 'vfs.zfs.txg.write_limit_override'."
33,vfs.zfs.write_limit_override=1073741824
33,Be aware that the vfs.zfs.write_limit_override tuning you see above
33,may need to be adjusted for your system.
33,It's up to you to figure out
33,what works best in your environment.
33,Deduplication
33,"Deduplication is a misunderstood feature in ZFS v21+; some users see it as a silver bullet for increasing capacity by reducing redundancies in data. Here are the author's (gcooper's) observations: There are some resources that suggest that one needs 2GB per TB of storage with deduplication [i] (in fact this is a misinterpretation of the text). In practice with FreeBSD, based on empirical testing and additional reading, it's closer to 5GB per TB. Using deduplication is slower than not running it. Deduplication [on 8.x/9.x at least] lies via stat(2) / statvfs(2); it reports the theoretical used space -- not the actual used space -- which can confuse scripts that look at df output, etc (TODO: find PR that mentions this)."
33,Suggestions
33,"If you are going to use deduplication and your machine is underspec'ed, you must set vfs.zfs.arc_max to a sane value or ZFS will wire down as much available memory as possible, which can create memory starvation scenarios. It's a much better idea in general to use compression -- instead of deduplication -- if you're trying to save space, and you know that you can benefit from compression. When in doubt, check how much you would actually gain from deduplication via zdb -S <zpool> instead of just turning it on. Please note that this will take a while to run, depending on the dataset/zpool selected."
33,References
33,http://blogs.oracle.com/roch/entry/dedup_performance_considerations1
33,NFS tuning
33,"The combination of ZFS and NFS stresses the ZIL to the point that performance falls significantly below expected levels. The best solution is to put the ZIL on a fast SSD (or a pair of SSDs in a mirror, for added redundancy). You can now enable/disable ZIL on a per-dataset basis (as of ZFS version 28 / FreeBSD 8.3+).  zfs set sync=disabled tank/dataset  The next best solution is to disable ZIL with the following setting in loader.conf (up to ZFS version 15): vfs.zfs.zil_disable=""1""the vfs.zfs.zil_disable loader tunable was replaced with the ""sync"" dataset property. Disabling ZIL is not recommended where data consistency is required (such as database servers) but will not result in file system corruption. See ZFS Evil Tuning Guide, section ""Disabling the ZIL (Don't)"". ZFS is designed to be used with ""raw"" drives - i.e. not over already created hardware RAID volumes (this is sometimes called ""JBOD"" or ""passthrough"" mode when used with RAID controllers), but can benefit greatly from good and fast controllers."
33,MySQL
33,This assumes lots of RAM Tweaks for MySQL innodb_flush_log_at_trx_commit=2 skip-innodb_doublewrite Tweaks for ZFS zfs set primarycache=metadata tank/db zfs set atime=off tank/db zfs set recordsize=16k tank/db/innodb zfs set recordsize=128k tank/db/logs zfs set zfs:zfs_nocacheflush = 1 zfs set sync=disabled tank/db Note: MySQL 5.6.6 and newer (and related MariaDB / Percona forks)
33,"has innodb_file_per_table = on as default, so IBD files are not created under tank/db/innodb (defined by innodb_data_home_dir in your my.cnf), they are created under tank/db/<db_name>/ and you should use recordsize=16k on this dataset too or switch back to innodb_file_per_table = off References MySQL Innodb ZFS Best Practices (Oracle)"
33,Scrub and Resilver Performance
33,"If you're getting horrible performance during a scrub or resilver, the following sysctls can be set: vfs.zfs.scrub_delay=0"
33,vfs.zfs.top_maxinflight=128
33,vfs.zfs.resilver_min_time_ms=5000
33,vfs.zfs.resilver_delay=0Setting those sysctls to those values increased my (Shawn Webb's) resilver performance from 7MB/s to 230MB/s.
33,CategoryHowTo CategoryStale CategoryNeedsContent CategoryZfs ZFSTuningGuide
33,(last edited 2023-04-02T17:15:08+0000 by GrahamPerrin)
33,Immutable PageCommentsInfoAttachments
33,More Actions:
33,Raw Text
33,Print View
33,Render as Docbook
33,Delete Cache
33,------------------------
33,Check Spelling
33,Like Pages
33,Local Site Map
33,------------------------
33,Rename Page
33,Delete Page
33,------------------------
33,Subscribe User
33,------------------------
33,Remove Spam
33,Revert to this revision
33,Package Pages
33,Sync Pages
33,------------------------
33,Load
33,Save
33,SlideShow
33,MoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01
34,"Pagination in MySQLSkip to contentProductsSolutionsResourcesDocumentationDocsPricingContactProductsSolutionsResourcesDocumentationPricingContactBlogEngineeringFollow usPagination in MySQLAn overview of the different ways to paginate in MySQL including limit/offset pagination and cursor pagination plus the pros and cons of each.Table of contentsThe importance of deterministic orderingOffset/limit pagination- Strengths of offset/limit pagination- Offset/limit pagination and drifting pages- Performance drawbacks of offset/limit pagination- Deferred joins for faster offset/limit paginationCursor pagination- Drawbacks to cursor-based pagination- Benefits of cursor-based pagination- Cursor based pagination performanceConclusionAny good DBA will tell you to ""select only what you need."" It's one of the most common aphorisms, and for good reason! We don't ever want to select data that we're just going to throw away. One way this advice manifests itself is to not use SELECT * if you don't need all the columns. By limiting the columns returned, you're selecting only what you need.Pagination is another way to ""select only what you need."" Although, this time, we're limiting the rows instead of the columns. Instead of pulling all the records out of the database, we only pull a single page that we're going to show to the user.There are two primary ways to paginate in MySQL: offset/limit and cursors. Which method you choose depends on your use case and your application's requirements. Neither is inherently better than the other. They each have their own strengths and weaknesses.The importance of deterministic orderingBefore we talk about the wonders of pagination, we need to talk about deterministic ordering. When your query is ordered deterministically, it means that MySQL has enough information to order your rows in the exact same way every single time. If you sort your rows by a column that is not unique, MySQL gets to decide which order to return these rows in. Let's look at an example.Given this table full of people named Aaron:| id | first_name | last_name |"
34,|----|------------|-----------|
34,1 | Aaron
34,| Francis
34,2 | Aaron
34,| Smith
34,3 | Aaron
34,| Jones
34,Let's run a query to order those people by their first name:SQLSELECT
34,FROM
34,people
34,ORDER BY
34,first_name
34,"Because all three people have the same first name, MySQL gets to decide which order to return the rows in! Depending on certain factors, the order may change. This is because the ordering is not deterministic enough.This result set is valid because it is ordered by first_name:| id | first_name | last_name |"
34,|----|------------|-----------|
34,2 | Aaron
34,| Smith
34,1 | Aaron
34,| Francis
34,3 | Aaron
34,| Jones
34,"But so is this result set, because it also is ordered by first_name:| id | first_name | last_name |"
34,|----|------------|-----------|
34,3 | Aaron
34,| Jones
34,2 | Aaron
34,| Smith
34,1 | Aaron
34,| Francis
34,"We haven't given MySQL specific enough instructions to produce a deterministically ordered set of results. We've asked it to order the rows by first_name, and it has dutifully complied, but it may not put them in the same order every time.The easiest way to produce deterministic ordering is to order by a unique column because every value will be distinct, and MySQL will have no choice but to return the rows in the same order every time. Of course, that's not very helpful if you need to order by a column that's not unique! In that case, appending a unique column to your ordering does the trick. In most cases, simply adding the id is the best way to go.SQLSELECT"
34,FROM
34,people
34,ORDER BY
34,"first_name, id -- Add ID to ensure deterministic ordering"
34,"Now MySQL knows that when given two first_name values that are the same, it should then look at the id column to determine the order. This is deterministic ordering, and it's a prerequisite to effective pagination.Offset/limit paginationOffset/limit pagination is likely the most common way to paginate in MySQL because it's the easiest to implement. With offset/limit pagination, we're taking advantage of two SQL keywords: OFFSET and LIMIT. The LIMIT keyword tells MySQL how many rows to return, while OFFSET tells MySQL how many rows to skip over.SQLSELECT"
34,FROM
34,people
34,ORDER BY
34,"first_name, id"
34,LIMIT
34,10 -- Only return 10 rows
34,OFFSET
34,10 -- Skip the first 10 rows
34,"In this example, we're selecting all the people from the people table, ordering them by first_name and id, and then limiting the result set to 10 rows. We're also skipping the first 10 rows, returning rows 11-20.To construct an offset/limit query, you need to know the page size and the page number. The page size is how many records you want to show per page, and the page number is what page you want to show. The LIMIT is determined by the page size, and the OFFSET is determined by both the page size and the page number.To calculate the correct offset, multiply the page_number - 1 by the page_size. This ensures that when your user is on the first page, the offset calculates to 0, meaning you're not skipping any rows.SQLSELECT"
34,FROM
34,people
34,ORDER BY
34,"first_name, id"
34,LIMIT
34,10 -- page_size
34,OFFSET
34,10 -- (page_number - 1) * page_size
34,"Tip We have a video overview of offset/limit pagination, if you prefer that medium. Strengths of offset/limit paginationOne of the great strengths of offset/limit pagination is that it's easy to implement and easy to understand. It doesn't require tracking any state over time; each request can stand alone. It doesn't matter what pages the user has visited before. The query construction is always the same. The math is simple. The query is simple.Another strength of this method is that pages are directly addressable. Users who want to navigate from page 1 directly to page 10 can do so quite easily, provided your interface exposes page links. (This is not the case with cursor pagination.) Convincing arguments have been made that directly addressable pages shouldn't ever be exposed to users because they have no semantic meaning. For example, what does page 84 mean? Why not just expose ""next"" and ""back"" buttons? That's a decision that you'll have to make for your application! Many users are used to seeing directly addressable page numbers, and it can be helpful to skip several pages ahead instead of one page at a time. It's up to you to decide what's best for your application, but if you need directly addressable pages, you will need to use offset/limit pagination.Offset/limit pagination and drifting pagesOne weakness of offset/limit pagination is that pages can drift. This is true of cursor-based pagination as well, but it's more likely to happen with offset/limit pagination.Let's look at an example in which your user is viewing page one with ten records. The last person they see on this page is ""Judge Bins."" They don't see her yet, but ""Sonya Dickens"" should be the first person on page 2.| id | first_name | last_name |"
34,|----|------------|-----------|
34,1 | Phillip
34,| Yundt
34,2 | Aaron
34,| Francis
34,3 | Amelia
34,| West
34,4 | Jennifer
34,| Becker
34,5 | Macy
34,| Lind
34,6 | Simon
34,| Lueilwitz |
34,7 | Tyler
34,| Cummerata |
34,8 | Suzanne
34,| Skiles
34,9 | Zoe
34,| Hill
34,| 10 | Judge
34,| Bins
34,|----|------------|-----------| Page break
34,| 11 | Sonya
34,| Dickens
34,| 12 | Hope
34,| Streich
34,| 13 | Kristian
34,| Kerluke
34,| 14 | Stanton
34,| Fisher
34,| 15 | Rasheed
34,| Little
34,| 16 | Deron
34,| Koss
34,| 17 | Trevor
34,| Daniel
34,| 18 | Vernie
34,| Friesen
34,| 19 | Jody
34,| Littel
34,| 20 | Jorge
34,| Nienow
34,"While your user is viewing the page, the person with the id of 2 (Aaron Francis) is deleted.| id | first_name | last_name |"
34,|----|------------|-----------|
34,1 | Phillip
34,| Yundt
34,3 | Amelia
34,| West
34,4 | Jennifer
34,| Becker
34,5 | Macy
34,| Lind
34,6 | Simon
34,| Lueilwitz |
34,7 | Tyler
34,| Cummerata |
34,8 | Suzanne
34,| Skiles
34,9 | Zoe
34,| Hill
34,| 10 | Judge
34,| Bins
34,| 11 | Sonya
34,| Dickens
34,| <-- Sonya is now on page one!
34,|----|------------|-----------| Page break
34,| 12 | Hope
34,| Streich
34,| <-- This is now the first person on page two
34,| 13 | Kristian
34,| Kerluke
34,| 14 | Stanton
34,| Fisher
34,| 15 | Rasheed
34,| Little
34,| 16 | Deron
34,| Koss
34,| 17 | Trevor
34,| Daniel
34,| 18 | Vernie
34,| Friesen
34,| 19 | Jody
34,| Littel
34,| 20 | Jorge
34,| Nienow
34,| 21 | Mara
34,| Grady
34,"The user navigates to page two, and the first person they see is Hope Streich. Because we're naively skipping over the first ten rows, Sonya Dickens has been skipped altogether. Sorry Sonya. Your user never sees her unless they navigate back to page one.Paginating ever-changing data is not an easy problem to solve, and this may be an acceptable tradeoff for you. Even cursor-based pagination is prone to some of these movements, but it's less likely to happen.Performance drawbacks of offset/limit paginationThe way that the OFFSET keyword works is that it discards the first n rows from the result set. It doesn't simply skip over them. Instead, it reads the rows and then discards them. This means that as you work into deeper and deeper pages of your result set, the performance of your query will degrade. This is because the database must read and discard more rows as you move through the result set.Very deep pages can take multiple seconds to load. This is a big issue with offset/limit pagination, and it's one reason cursor-based pagination is so popular. Cursor-based pagination doesn't have this performance drawback because it doesn't use the OFFSET keyword.Deferred joins for faster offset/limit paginationThere is a technique known as a ""deferred join"" that can optimize offset/limit pagination.The deferred join technique is an optimization solution that enables more efficient pagination. It performs the pagination on a subset of the data instead of the entire table. This subset is generated by a subquery, which is joined with the original table later. The technique is called ""deferred"" because the join operation is postponed until after the pagination is done.SQLSELECT * FROM people"
34,INNER JOIN (
34,-- Paginate the narrow subquery instead of the entire table
34,"SELECT id FROM people ORDER BY first_name, id LIMIT 10 OFFSET 450000"
34,) AS tmp USING (id)
34,ORDER BY
34,"first_name, id"
34,"This technique has been widely adopted, and there are libraries available for popular web frameworks such as Rails (FastPage) and Laravel (Fast Paginate).Here is a graph showing the performance of a deferred join vs. the standard offset/limit pagination method, taken from our blog post introducing the FastPage Rails gem.As you can see, the deferred join method is much faster than the standard offset/limit pagination method, especially for deeper pages.If you do decide that offset/limit is the right choice for your application, then you should consider using a deferred join technique to optimize your queries.Cursor paginationNow that we're thoroughly versed on the offset/limit method let's talk about cursor-based pagination. Cursor-based pagination is a method of pagination that uses a ""cursor"" to determine the next page of results. It's important to note that this differs from a database cursor, which is a different concept. When discussing cursors in the context of pagination, we're using the word to mean a pointer, an identifier, a token, or a locator.Tip We also have a video overview of cursor pagination, if you prefer that medium. The idea behind cursor-based pagination is that you have a cursor that points to the last record that the user saw. When the user requests the next page of results, they must send along the cursor, which we use to determine where to start the next page of results.Instead of using the OFFSET keyword, we use the cursor to construct a WHERE clause that filters out all the rows that the user has already seen.Let's start with a simple example. Let's say we have a table of people and want to paginate the results by the id. When the user requests the first page of results, there is no cursor, so we return the first ten rows.SQLSELECT"
34,FROM
34,people
34,ORDER BY
34,LIMIT
34,MySQL returns the following result set:| id | first_name | last_name |
34,|----|------------|-----------|
34,1 | Phillip
34,| Yundt
34,2 | Aaron
34,| Francis
34,3 | Amelia
34,| West
34,4 | Jennifer
34,| Becker
34,5 | Macy
34,| Lind
34,6 | Simon
34,| Lueilwitz |
34,7 | Tyler
34,| Cummerata |
34,8 | Suzanne
34,| Skiles
34,9 | Zoe
34,| Hill
34,| 10 | Judge
34,| Bins
34,"Here is where cursor and offset-based pagination begin to diverge. With cursor-based pagination, we must construct and send the cursor out to the frontend. The cursor is a pointer to the last record that the user has seen. Since we are only sorting by id, the cursor is the id of the last record in the result set. Usually, it would be base64 encoded, but for simplicity, we'll just leave it unencoded.The backend sends out the results and a cursor of id=10, usually called next_page or something similar.JSON{"
34,"""next_page"": ""(id=10)"","
34,"""records"": ["
34,// ...
34,"When the user requests the next page of results, they must return the cursor to the server. The cursor is used to construct a WHERE clause that filters out all the rows the user has already seen.SQLSELECT"
34,FROM
34,people
34,WHERE
34,"id > 10 -- The last id that the user saw was 10, so we start at the next id after 10"
34,ORDER BY
34,LIMIT
34,"You can see that in this query, we're not using the OFFSET keyword at all, but instead, we're jumping straight to the next record after the last record that the user saw. This is the key difference between cursor and offset-based pagination!It gets a bit more complicated if we go back to our original example of sorting by first_name and then id. Since we're sorting by both columns, the cursor must contain both values for the last record that the user has seen.Let's take this example set of records, which is 20 people sorted by first name, and then ID.| id"
34,| first_name | last_name
34,|-------|------------|------------|
34,2 | Aaron
34,| Francis
34,589 | Aaron
34,| Streich
34,3896 | Aaron
34,| Corkery
34,8441 | Aaron
34,| Kreiger
34,9179 | Aaron
34,| Wolf
34,| 10970 | Aaron
34,| Reichert
34,| 13082 | Aaron
34,| Collier
34,| 13704 | Aaron
34,| Braun
34,| 19399 | Aaron
34,| Watsica
34,| 25995 | Aaron
34,| Runte
34,|-------|------------|------------| Page break
34,| 26794 | Aaron
34,| Mayer
34,| 32075 | Aaron
34,| Hahn
34,| 32471 | Aaron
34,| Bahringer
34,| 40612 | Aaron
34,| Abbott
34,| 41202 | Aaron
34,| Willms
34,| 41571 | Aaron
34,| Nienow
34,| 46556 | Aaron
34,| Glover
34,| 48501 | Aaron
34,| Boyle
34,| 50628 | Aaron
34,| Schmeler
34,| 51656 | Aaron
34,| Williamson |
34,"In this case, the last record the user sees on page 1 has an id of 25995. This information alone is not enough for the cursor! We must also add the first_name since it is part of the sort order. The cursor for the last record on page 1 is (first_name=Aaron, id=25995).When the user sends back the cursor, we can construct a WHERE clause that filters out all the rows the user has already seen. This time, it requires a little more thought because we're sorting by two columns. We'll add a first_name filter to show any names after ""Aaron,"" but since first_name has many duplicates, we'll also add an id filter to show any ""Aaron""s that have an id after the last id that the user saw.SQLSELECT"
34,FROM
34,people
34,WHERE
34,(first_name > 'Aaron')
34,-- Names after Aaron
34,"(first_name = 'Aaron' AND id > 25995) -- Aarons, but after the last id that the user saw"
34,ORDER BY
34,"first_name, id"
34,LIMIT
34,"As you add more columns to the sort order, you'll need to add more filters to the WHERE clause.Drawbacks to cursor-based paginationAs you've seen, cursor-based pagination is more complicated to implement than offset-based pagination. Constructing the cursor and the WHERE clause requires more thought. You also have to keep track of that little piece of state: the cursor. This isn't inherently bad, and not all complexity is reducible, but it's something to keep in mind. Most frameworks have cursor-based pagination built in, so you may not have to implement it manually.Another drawback to cursor-based pagination is that it's impossible to address a specific page directly. For instance, if the requirement is to jump directly to page five, it's not possible to do so since the pages themselves are not explicitly numbered, and there is no way to create a cursor without knowing the last record that has been seen. You can only navigate to the next page.Benefits of cursor-based paginationOne of the advantages of cursor-based pagination is its resilience to shifting rows. For example, if a record is deleted, the next record that would have followed is still displayed since the query is working off of the cursor rather than a specific offset.Let's go back to our Sonya Dickens example. The last person they see on this page is ""Judge Bins."" They don't see her yet, but ""Sonya Dickens"" should be the first person on page 2.| id | first_name | last_name |"
34,|----|------------|-----------|
34,1 | Phillip
34,| Yundt
34,2 | Aaron
34,| Francis
34,3 | Amelia
34,| West
34,4 | Jennifer
34,| Becker
34,5 | Macy
34,| Lind
34,6 | Simon
34,| Lueilwitz |
34,7 | Tyler
34,| Cummerata |
34,8 | Suzanne
34,| Skiles
34,9 | Zoe
34,| Hill
34,| 10 | Judge
34,| Bins
34,| <-- The cursor points here
34,|----|------------|-----------| Page break
34,| 11 | Sonya
34,| Dickens
34,| 12 | Hope
34,| Streich
34,| 13 | Kristian
34,| Kerluke
34,| 14 | Stanton
34,| Fisher
34,| 15 | Rasheed
34,| Little
34,| 16 | Deron
34,| Koss
34,| 17 | Trevor
34,| Daniel
34,| 18 | Vernie
34,| Friesen
34,| 19 | Jody
34,| Littel
34,| 20 | Jorge
34,| Nienow
34,"While they are viewing page one, ""Aaron Francis"" is deleted.| id | first_name | last_name |"
34,|----|------------|-----------|
34,1 | Phillip
34,| Yundt
34,3 | Amelia
34,| West
34,| <-- Aaron Francis is deleted
34,4 | Jennifer
34,| Becker
34,5 | Macy
34,| Lind
34,6 | Simon
34,| Lueilwitz |
34,7 | Tyler
34,| Cummerata |
34,8 | Suzanne
34,| Skiles
34,9 | Zoe
34,| Hill
34,| 10 | Judge
34,| Bins
34,| <-- The cursor *still* points here
34,|----|------------|-----------| Page break
34,| 11 | Sonya
34,| Dickens
34,| <-- Sonya is the first person after the cursor
34,| 12 | Hope
34,| Streich
34,| 13 | Kristian
34,| Kerluke
34,| 14 | Stanton
34,| Fisher
34,| 15 | Rasheed
34,| Little
34,| 16 | Deron
34,| Koss
34,| 17 | Trevor
34,| Daniel
34,| 18 | Vernie
34,| Friesen
34,| 19 | Jody
34,| Littel
34,| 20 | Jorge
34,| Nienow
34,"This time, it doesn't matter! The cursor points to the last record that the user saw, and the next record is still Sonya Dickens. We tell the database, ""the last record I saw was ID 10, and I want to see the next ten records."" The database doesn't care that some records were deleted. It just knows that the next record is Sonya Dickens.This is true even if the cursor is pointing to a record that was deleted. If the cursor points to a record that was deleted, we're still telling the database, ""the last record I saw was ID 10, and I want to see the next ten records."" Again, the database doesn't care that the record was deleted. It just knows that the next record is Sonya Dickens.Cursor based pagination performanceCursor-based pagination can be much more performant than offset/limit simply because it accesses much less data. Instead of generating a result set and throwing away everything before the offset, the database can start at the offset and return the next N records. This is especially true if the offset is large. You will need to consider a proper indexing strategy to ensure the database can efficiently find the necessary records.ConclusionPagination is a common requirement for almost every web application or API. Now you understand the different types of pagination and the tradeoffs that come with each.Offset/limit is nice because it's easy to implement and understand, and you can directly address pages. Some downsides are that it can be slower as you navigate deeper into the pages, and it is more prone to drift.Cursor-based pagination is nice because it is more performant and more resilient to shifting rows. Some of the downsides are that it is more complicated to implement, and you cannot directly address pages.Which method you choose is up to you, but hopefully, this article has given you a better understanding of the tradeoffs, and you can now make an informed decision.Want a powerful and performant database that doesn’t slow you down?Try out PlanetScale nowShareNext postSafely making database schema changesWritten byAaron FrancisDeveloper EducationLast updated April 18, 2023Database scaling courseThis 22 lesson video course covers partitioning, caching, replication, sharding, and more.Get startedTable of contentsThe importance of deterministic orderingOffset/limit paginationStrengths of offset/limit paginationOffset/limit pagination and drifting pagesPerformance drawbacks of offset/limit paginationDeferred joins for faster offset/limit paginationCursor paginationDrawbacks to cursor-based paginationBenefits of cursor-based paginationCursor based pagination performanceConclusionRelated postsEngineeringWhat is database sharding and how does it work?EngineeringConnection pooling in VitessEngineeringZero downtime Rails migrations with the PlanetScale Rails gemDatabase scaling courseThis 22 lesson video course covers partitioning, caching, replication, sharding, and more.Get startedCompanyAboutBlogChangelogCareersProductCase studiesEnterprisePricingAI/MLResourcesDocsResourcesMySQL for DevelopersSupportStatusOpen SourceVitessVitess communityGitHubTalk to usCall +1 408 214 1997Contact us© 2024 PlanetScale, Inc. All rights reserved.PrivacyTermsCookiesDo Not Share My Personal Information"
35,Magento Performance Optimization in 7 Steps (updated 2024)
35,Processing... Please wait...Product was successfully added to your shopping cart.
35,Go to cart page
35,Continue shopping
35,Services ↓ Magento 2 Migration Service Magento Development Services Magento Speed Optimization Service Magento 2 Upgrade Service Magento Consulting Services About How to ↓ How to speed up Magento site How to migrate Magento 1 to Magento 2 How to optimize TTFB of a Magento 2 site How to upgrade Magento to the latest version How to speed up Magento 2 website How to optimize Magento time to first byte (TTFB) How to perform a Magento security audit How to optimize Magento SEO How to choose a hosting provider for a Magento storeContact
35,"Magento performance optimization - 7 steps to a faster website (2024) by Konstantin   Last updated December 31, 2023 Server response time optimizationI have been optimizing Magento 2 sites for quite a while. During the process, there are a few things I've learned. I'd like to share them with you.I’ve learned that:Time to First Byte is an important metricMagento Site speed is key to maintaining your conversionsNot all Optimization techniques require programming skillsWhile it is true that some speed optimization tweaks require extensive knowledge of the Magento platform, it is also proven that there are others that require no programming at all; yet they are effective.This post brings together the field-proven performance tuning techniques. You are free to skip steps that require extensive programming. Do your best with the easy ones and you will speed up your Magento store.This guide mainly targets Magento 2. However, if you are still running Magento 1.x, you can apply some of the tweaks found in this post. Or migrate to M2.7 Performance Fixes:Magento optimized hosting.Time to first byte audit.MySQL tuning.Full Page Cache.Varnish.PHP8.CDN.Before we dive into how these seven components can help make Magento faster, we first need to look into why Magento website performance is so important. One of the big things to look for when conducting Magento optimization is site speed. Essentially, site speed measures how fast a site loads and how fast it can travel between web pages.Since customers are always on the go, it’s really important for eCommerce businesses to make sure customers can get to their products quickly.Let’s dive a little more into why Magento site speed is important:Why is Magento Slow?There are a variety of reasons as to why your Magento website is running slowly. However, it mostly happens due to wrong configuration and wrong choice of hosting provider.You see, Magento is not like it’s other competitors. You are not restricted on SKU numbers or anything like that, but with great power comes great responsibility. You do need the right hosting and if you are not tech savvy, you need a trustworthy magento programmer to help you along the way.Another common problem is that there are too many product attributes with too many choices. Product attributes allude to the characteristics of a product. Meaning, if you have a customizable product on your Magento site, your customers would be able to choose the quantity, size, or even the color of the product.Adding color product attributes with just a few choices (black, white, green) would not hurt. However, creating 10 product attributes with hundreds of choices would really slow down a Magento website. If you take a look here, you can see the resolution product attribute with many choices, as well as other multiple choices attributes. Without proper tuning, these factors slow down the Magento website significantly.If that’s not the problem, then what may be the problem is that your website has too much code to load. Generally speaking, one of the many problems my clients face when dealing with a slow website is that the JavaScript, HTML, and CSS all take a lot of time to load because each consists of so much code.I have found what works best with my clients is that I minimize the HTML and JavaScript. As for the CSS, I take the code and I place it at the bottom of the website so that the code will load fast. By the time the CSS starts loading, the web page will already show and the CSS will load without complications.Here is a comprehensive guide on how to make Magento websites fast. Keep these steps in mind when you build your eCommerce store as well as when you run it.1. Magento Optimized HostingYou can speed up Magento with the right service provider. But, you need to do the research and get a good hosting.A common mistake among my clients is that they tend to go for the cheapest hosting offered to them. When they tell me this, I always say the same thing: Cheap hosting is not good and good hosting is not cheap. Hosting is the foundation of your eCommerce store, so be sure to lay a good foundation. Some hosting providers offer Magento optimized solutions. Be sure to do the research, talk to customer support and find the best hosting for you and your store.Shared or dedicated?Shared hosting is a type of hosting that allows many websites to be on one server. This is an ideal route if your website has a small inventory and is just starting out. On the other hand, dedicated hosting allows one website to take over its own server. If you have a lot of inventory and large amounts of traffic, your website would have fewer chances of crashing if it were hosted on its own server.If your website needs to withstand large amounts of traffic, that also probably means that your server can reach maximum capacity on a shared server. This means that your Magento website would be better off on a dedicated server.With these facts in mind, it is really up to you to decide whether to go with shared or dedicated hosting. However, if you are a small store that wants to jump into dedicated hosting, but is intimidated by the price, I can assure you that, in my experience, I have seen many stores running pretty fast on shared servers.Look for the data center that is closest to your customers because you want to eliminate network issues. I would consider choosing the right host as the most important advice. Build your eCommerce business on solid ground.2. Reduce Time to First Byte (TTFB)TTFB is the measure of how quick your website responds to browser requests. The TTFB is generally made up of three separate components:The time needed to send the HTTP requestThe time needed for the server to process the requestThe time needed for the server to send back the first byte of the response to the clientIf you were to notice the white screen on your computer before the homepage loads, that is a clear sign of a website's TTFB. Blank ScreenIn simple words, it is the main speed parameter that tells you how fast your Magento store is. In the Magento platform, it is also one of the most important indicators of Magento site speed.To inspect your site’s platform behind the scenes, there is a Magento built-in tool, called Profiler, to tweak Magento performance. However, there are other ways to reduce your Magento site’s TTFB.An ideal TTFB should be under 1s. You can use online tools like Pingdom to see what is your TTFB is. If your TTFB is over 1s - that is a sign that you should start researching for ways to reduce it.Three Reasons for a Slow TTFB Include:Web server issuesThe main server has reached its full capacitySlow commands due to database optimizationReducing your TTFB requires a few simple tools. In fact, here is a great tutorial on how to reduce your TTFB.3. Optimize MySQL databaseOne important thing that can also affect site's performance is a MySQL database. Sometimes, website databases give slow commands because they need to be optimized. MySQL is a database that is comprised of your website data. Your data can comprise of anything from a simple product listing to a fully comprehensive wishlist.The first thing to do would be to gather MySQL internal statistics with mysqltuner script. This program tells you what is wrong with your database server and what needs fixing. It requires Perl interpreter to be installed on the server.Look for MySQL Table Cache Hit Rate (table_cache). If it says 0% - you are not using a database table cache at all. MySQL Table Cache configuration lets DB keep tables in memory thus speeding up access to its data. Tweak the table_cache parameter in my.cnf file to have table cache hit rate as close to 100% as possible.Another setting to check is Table Open Cache (table_open_cache). That parameter greatly affects performance by making the same table available for different sessions.I would advise you to get an expert in MySQL optimization to check your database server. If you are on shared hosting, your host team most likely takes care of your database tuning for you. If you run dedicated server either spend time learning MySQL configuration or get your DB configured by experts.4. Enable CacheEnabling your site’s cache can improve your overall performance by allowing the site to load faster and keeping track of the pages your visitors access.4.1 Internal Magento CacheEnabling you internal Magento cache is easy. Simply go to the admin panel and do the following:4.2 APC or MemcachedAfter you have enabled your internal Magento cache, the next step is to enable the Magento cache in the app/etc/env.php file. If you need help doing this, you can always consult a certified Magento developer.Check with your hosting to see if it supports APC or Memcached, as these programs cache sessions and other important data.5. Install and configure VarnishVarnish is a little program that caches parts of your webpage. I highly recommend that you install the latest version, as shown on the screen, and run it. Magento 2.0 supports Varnish but if you need to run Varnish on Magento 1.x, you would need a free extension by Nexcess - Turpentine. It’s worth noting that Turpentine is used by many stores and it follows Varnish Cache’s Best Practices. Download VarnishIf you have any questions about installing an extension on your Magento site, feel free to ask here.You should also make sure your host supports Varnish. If you run a dedicated server you might need to install and configure Varnish. If you are not sure, check with your provider to make sure that your hosting supports Varnish.Before going live, be sure to test your varnish implementation! Check if your cart block at the top has changed and check if it contains the products you added.Giving your store a good test drive after Varnish implementation is important because Varnish has been known to break eCommerce stores.6. PHP 8You can improve Magento 2 performance by upgrading to PHP 8. The latest Adobe Commerce 2.4.6 supports it.PHP 8 has several optimizations that could improve loading speed.Perform an upgrade yourself or hire an expert. Never forget to backup data and files.7. Content Delivery Network (CDN) Content Delivery NetworkCDN is a group of regional servers that can transfer static content. In terms of content delivery, you can say that they’re the backbone of the internet. With that, I strongly recommend that you sign up for CDN services. It is cheap and it does serve the static content (ex. images, CSS/JS files, video, etc) faster than your host would. Companies with great CDN plans include MaxCDN, Cloudflare.If anything else, ask your host if they offer CDN services.Why Should I Care?Magento site speed is one of the most important keys to having a successful website and, ultimately, a successful business.Remember that 47% of customers leave a site for loading too slow. That means that for every 100 people that go to your website, a slow Magento website will cause 47 of them to leave in 3 seconds or less.Can you imagine spending so much time creating your website, gaining traction on social media & trying to rank on search engines, only to lose nearly half of the customers that enter your site in three seconds just because your Magento website did not load fast enough?Your business can lose a lot of conversions by simply having a site that is slower than your competitors. On the other hand, your business can convert that much more people if your website is much faster than your competitors.If the following reason was not enough, Magento site speed affects your Google Rankings. In fact, in July of 2018, Google started penalizing websites that had slower loading times than other pages on the SERP (Search Engine Results Page). So if you have noticed a change in your once strong rankings, but did not change your digital marketing strategy, that is a sign that it is time to check your page speed.Aside from all of that, Magento performance optimization makes checkout less irritating for your customers. In our day and age, nobody wants to wait. This is especially true when buying on the internet. Happy customers generally mean repeat customers, which not only makes you more money but also saves you money because it’s cheaper to keep a current customer in your business cycle than to introduce a new customer into your business cycle.These seven tips will make your online store fast. It will attract more potential customers.  If you find this post interesting do not hesitate to sign up for our newsletter and join the 1312 people who receive Magento news, tips and tricks regularly.Sign UpThank You!  4 thoughts on “Magento performance optimization - 7 steps to a faster website (2024)”"
35,Stella George
35,"December 31, 2020 at 8:17 amNyc Article on Magento performance optimization! Thank you for sharing!"
35,Database Optimization Techniques
35,"August 5, 2020 at 5:14 amYour article is very nice & useful. Wish to see much more like this. Thanks for sharing your information"
35,Luis Paul
35,"December 23, 2019 at 8:52 amThe tips you've mentioned in the blog post are highly recommendable."
35,Thanks for the great blog post.
35,Peter Gustafson
35,"March 27, 2019 at 9:17 amGreat article. The need for speed is huge. Thanks for keeping us updated Kon."
35,Name *
35,Email *
35,Website
35,Comment *
35,Leave a comment
35,Please wait...
35,Get Free Quote Now
35,"My ServicesMagento Speed Optimization ServiceMagento 2 Migration ServiceMagento Development ServicesMagento 2 Upgrade ServiceMagento Consulting ServicesMagento Custom Extension Development Recent Posts Adobe Commerce vs Magento 2 - 25 New Features (Updated 2024) Speed up Magento - The Ultimate Guide (Updated 2024) Magento 2 hardware requirements (Updated 2024) Magento 2 Upgrade in 3 Easy Steps (Updated 2024) Magento 2 Slow Loading - 5 Quick Fixes (updated 2024) How to Upgrade Magento 2.3 to 2.4 Tutorial (updated 2024) Magento 2 Migration - The Ultimate Guide (Updated 2024) Magento 2 Hosting Providers Suggestion from an Expert (Updated 2024) Magento performance optimization - 7 steps to a faster website (2024) Speed up Magento 2: The Ultimate Guide (Updated 2024) Magento Security Audit in 2024 Magento SEO Strategies for 2024 Magento 2 Speed Optimization - From 10.0s to 2.7s (Real Case) Magento 2 Performance Optimization (easy guide to get 90%+ PageSpeed) Magento 2 TTFB (Time To First Byte) Optimization Magento 2 and 1 Million Products WooCommerce and Magento 2 - 1 million products benchmark Magento TTFB optimization to reduce time to first byte Magento Enterprise 1.14 to Community 1.9 Downgrade Marius Strajeru, Jisse Reitsma, Rakesh Jesadiya talk about Magento 2 5 Magento Inventory Management Programs Overview + Bonus Magento 1.9.3.2 vs 2.1.4 Performance Benchmark ServicesMagento Development ServicesMagento 2 Migration HelpSpeed Up MagentoMagento 2 Upgrade ServiceMagento Consulting ServicesMagento 2Migrate to Magento 2Adobe Commerce UpgradeMagento 2 Performance OptimizationCompanyAbout MeBlogSitemapPrivacy PolicyTerms of ServiceContact InfoGOIVVY LLCOffice Location: Auburn, Georgia, 30011, USAClick to contact me TwitterLinkedin © 2024 GOIVVY LLC, Certified Magento Developers USA"
36,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
36,Home Page
36,Cloud IaaS Solutions
36,Azure VM Solutions
36,AWS VM Solutions
36,GCP VM Solutions
36,WordPress SSO
36,About Us
36,Contact Us
36,Home Page
36,Cloud IaaS Solutions
36,Azure VM Solutions
36,AWS VM Solutions
36,GCP VM Solutions
36,WordPress SSO
36,About Us
36,Contact Us
36,Home Page
36,Cloud IaaS Solutions
36,Azure VM Solutions
36,AWS VM Solutions
36,GCP VM Solutions
36,WordPress SSO
36,About Us
36,Contact Us
36,Aug
36,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
36,by Dennis Muvaa
36,in MariaDBComments
36,MariaDB Performance Tuning: Optimize MariaDB for High Traffic. MariaDB is an open source relational database management system (RDBMS) designed as a suitable alternative to MySQL. Supports multiple storage engines and enables users to easily scale out their database.
36,"To achieve optimal database performance, it’s essential to optimize MariaDB server sufficiently. It reduces query time and improves user experience. Essential when running critical applications that rely on real time data processing. Optimizing MariaDB server leads to efficient resource usage, lower CPU and memory usage, and lower disk I/O."
36,This article discusses MariaDB Performance Tuning: Optimize MariaDB for High Traffic. Read on!
36,Also ReadHow to MariaDB/MySQL Show Users and GRANTS Privileges
36,How to Optimize Your MariaDB Server for High Traffic
36,1. Optimize Your Database Schema
36,"Well, it is essential to normalize your database schema. This step involves reducing data redundancy and bolstering data integrity by breaking down large tables into smaller ones and creating relationships between them. Also, denormalize tables at times i.e merge small tables into larger tables to minimize the need for complex querying."
36,Also ReadHow to setup MariaDB on Windows in Azure/AWS/GCPHow to Setup MariaDB Ubuntu Cloud in Azure/AWS/GCP
36,2. Choose Appropriate Data Types
36,"Choose the right data types for your table columns. It saves space, improves query speed, and strengthens data integrity. In addition, consider using indexes. They greatly improve the speed of data retrieval operations. However, it’s best to note that while indexes improve read performance, they also slow down write operations."
36,3. Use Connection Pooling
36,"Connection pooling helps optimize MariaDB server performance by addressing connection overhead issues. Each new database connection comes with a significant amount of overhead due to handshaking protocols and authentication procedures. These procedures consume considerable processing power and time, especially if the application needs to frequently open and close connections. Therefore, by allowing a pool of reusable connections, the system does to have to repeat this process multiple times, hence improving MariaDB performance."
36,"Use persistent connections, that remain open over multiple requests. This significantly reduces the latency. Overall, it reduces the time it takes to execute a large number of database queries. Especially helpful in high-volume transaction."
36,"Also, leverage connection pooling solutions like ProxySQL. ProxySQL acts as a gateway between MariaDB and the application layer. It manages the connections to the database server and effectively reduces the burden on the database. The pooling solution dynamically adjusts the number of connections in the pool based on the load, allowing for better resource management. Most pooling solutions come with intelligent routing capabilities that help direct queries to different servers based on their type and load."
36,"Also ReadWhat are MariaDB Data Types (Numeric, Date, String) Explained"
36,4. Use Appropriate Storage Engines
36,Following with MariaDB Performance Tuning: Optimize MariaDB for High Traffic. is to choose the right storage engine significantly affects performance. MariaDB has 6 main storage engines:
36,InnoDBMyISAMMariaDB ColumnStoreAriaSpiderMyRocks
36,"InnoDB, the default storage engine for MariaDB and is ideal for transaction heavy workloads. It supports row-level locking, allowing for higher concurrency and faster recovery. Alternatively, use MyISAM for workloads with heady read and light write scenarios. This storage engine has a small footprint that InnoDB anand consumes less memory and disk space than InnoDB. Aria is a modern improvement of MyISAM and allows easy copying between systems."
36,ColumnStore has a parallel data architecture and is suitable for processing petabytes of data. MyRocks provides greater compression than InnoDB and is ideal for high throughput operations. Spider supports partitioning and allows handling of multiple instances as if they were on the same instance.
36,"There are lots of other MariaDB storage engines, each designed for particular operations."
36,5. Allocate Memory Sufficiently
36,Tune memory allocation to improve MariaDB performance. There are various buffers and caches to configure when allocating memory. These include:
36,join_buffer_sizenet_buffer_lengthread_buffer_sizesort_buffer_sizemax_heap_table_sizemrr_buffer_size
36,"The join_buffer_size controls the size of the buffer used for full join operations. The sort_buffer_size dictates the size of the buffer used for sorting. The read_buffer_size is used for sequential table scans, while read_rnd_buffer_size is used when the server reads rows in a sorted order following a sort operation."
36,Also ReadMariadb Create Table – How to Guide (Step by Step)
36,6. Fine-Tune Your Server Configuration
36,"Tweak server settings. Adjust InnoDB settings such as the innodb_buffer_pool_size, where InnoDB engine caches table and index data. A well proportioned buffer pool significantly boosts the database performance by lowering the amount of disk I/O."
36,"If you are using the MyISAM storage engine, consider optimizing parameters like key_buffer_size, used to cache index blocks for MyISAM tables, and thread_cache_size, which sets the number of threads that the server should cache for reuse."
36,"Remember to adjust MariaDB’s query cache feature for optimal performance. For instance, when a SELECT statement is executed, the result set is stored in the query cache. If a similar statement is executed again, the server retrieves the results from the cache instead of executing the query again."
36,7. Use the Right Table Partitioning
36,"Table partitioning improves performance of large databases. It involves dividing a single table into smaller, more manageable pieces. There are different types of table partitioning in MariaDB:"
36,List PartitioningRange PartitioningHash PartitioningKey Partitioning
36,"Range partitioning assigns a range of values to each partitioning. This means each partition contains rows for which the partitioning expression value lies within a given range. On the other hand, list partitioning assigns a list of values to each partition."
36,"In Hash partitioning, the server decides the partition for each data to achieve even partitions.  Well, it applies a consistent hash function to a key and then assigns rows based on the hash function’s result. Key partitioning is a variant of hash partitioning whereby the server assigns the distribution of rows across partitions."
36,Also ReadHow to Install MariaDB on Ubuntu 22.04
36,8. Optimize Your Query
36,"Optimizing your SQL queries helps boost MariaDB server performance. Well-written SQL query reduces the amount of time taken to fetch data. To understand how your queries perform, use the built-in EXPLAIN feature. It helps you understand how MariaDB executes your queries and enables you to see the different types of indexes are in place. With EXPLAIN, you also understand the sequence of table joins."
36,Optimize SQL query by reducing the result set size. Fetching more data than necessary not only slows your query but also adds more burden to your server. Restrict the number of rows your query returns through LIMIT clauses.
36,"Indexing all columns used in ‘where’, ‘group by’, ‘order by’ and ‘join’ clauses from the start. This ensures that the database does not do complete table scans in order to retrieve records. However, you should avoid using functions on indices as MariaDB tends to ignore the index."
36,9. Optimize Hardware Components
36,"To run an efficient MariaDB database, you need robust hardware with the right configurations. First, use solid-state drives (SSDs) for data storage rather than hard disks as they can significantly enhance disk I/O speed. SSDs provide faster read/write speeds compared to traditional hard drives."
36,"Memory optimization is also crucial, as a large memory means larger key and table caches. MariaDB stores data and indexes in memory to quicken read and write operations. To achieve optimal performance, it’s imperative to first set the server variables to utilize the available memory. In addition, use the highest RAM size per slot as it reduces latency compared to having more RAM slots on the motherboard."
36,"Secondly, you need extremely fast processors especially if you run critical workloads. A CPU with fast speeds allows for faster calculations enabling the clients to receive results quickly. CPU resources should also be allocated properly. A server with more CPUs handles more simultaneous connections and perform more operations in parallel."
36,Also ReadHow to Install MariaDB on Windows Server 2019 (Tutorial)
36,10. Use Concurrency and Threading
36,Understanding thread concurrency and how to apply it to your MariaDB server helps improve performance issues. Optimize how InnoDB multitasks between transactions and requests.
36,"InnoDB manages requests via threads, whereby each thread represents a single unit of processing. Therefore, you should configure system variables so as to have many threads executed at the same time."
36,11. Adjust Binary Log Settings
36,"The binary log records changes made to the database and is used for replication and recovery purposes. Choosing the right format for replication is crucial for performance. The row-based format logs changes made to each row, while the mixed format logs changes on a statement basis."
36,Adjust the binary log cache size to control the amount of memory allocated for caching changes before they are written to the binary log. Managing log retention with the expire_logs_days parameter avoids consuming too much disk space.
36,Also ReadHow to Install MariaDB on CentOS 8 (Step by Step Guide)
36,12. Disable DNS Reverse Lookups
36,"Disabling DNS reverse lookups improve the performance of a MariaDB database. Whenever a client connects to the database, MariaDB performs a DNS reverse lookup to convert the client’s IP address to a hostname. This process increases latency hence reducing speed although it provides additional security by verifying the identity of the client. The delay can be significant depending on the network configuration and the DNS server speed. This latency increases if there is a large number of clients connecting to the MariaDB server and is detrimental to the overall performance."
36,"By disabling DNS reverse lookups, MariaDB avoids potential delays and speeds up the process of establishing connections. This saves time especially in a system with high transaction rates or where numerous connections are being established and closed. Besides, it allows MariaDB to accept and handle more client connections in a shorter amount of time."
36,"Disabling DNS reverse lookups comes with potential security risks. DNS lookups provide an additional layer of protection by authenticating the source of incoming connections. Therefore, disabling DNS reverse lookups applies to systems with other robust security measures."
36,13. Check for Idle Connections
36,"Checking for idle connections is a crucial strategy in faster performance. Idle connections represent open database connections that are currently not in use. These connections, although inactive, consume resources. They occupy slots in the connection pool that could be used by other active connections, thereby potentially limiting the database’s ability to handle new incoming connections."
36,"Over time, a build-up of idle connections lead to a degradation in the performance of MariaDB. Identifying and properly handling these idle connections, such as by setting a timeout or closing them after a certain period of inactivity, frees up system resources and improve the overall performance."
36,14. Setting Your Disk I/O Scheduler
36,"Last point of the article MariaDB Performance Tuning: Optimize MariaDB for High Traffic. The Disk I/O Scheduler is a component of the operating system that decides in what order the block I/O operations is submitted to storage devices. Depending on the type of workload, different scheduling algorithms are more efficient. For example, some schedulers prioritize minimizing seek time for read-heavy applications, while others optimize for write-heavy or mixed workloads."
36,Thank you for reading MariaDB Performance Tuning: Optimize MariaDB Server for High Traffic. We shall conclude the rticle now.
36,Also ReadMariaDB vs MySQL Performance Differences (Pros and Cons)
36,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
36,Conclusion
36,"Optimizing MariaDB server is a continuous process, as performance is not pegged on a single factor. Use database monitoring tools to gain visibility into the state of their server and the database. By following the above practices, you are able to avoid lots of performance bottles and have your applications running seamlessly."
36,Related Posts:Optimize Memory & CPU Usage in Node.js: Performance Tuning TechniquesHyper-V Performance: Optimize VM Performance (Memory & CPU)MySQL Performance Tuning: For Optimal Database PerformanceWhat is HPC? High Performance Computing and How it WorksCloud HPC Architecture for Hybrid High-Performance ComputingUsing Redis for Caching: Best Practices and Performance Tuning
36,Tags:
36,MariaDB
36,Dennis Muvaa
36,"Dennis is an expert content writer and SEO strategist in cloud technologies such as AWS, Azure, and GCP."
36,"He's also experienced in cybersecurity, big data, and AI."
36,votes
36,Article Rating
36,Subscribe
36,Login and comment with
36,I allow to create an account
36,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
36,DisagreeAgree
36,Notify of
36,new follow-up comments
36,new replies to my comments
36,Login and comment with
36,I allow to create an account
36,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
36,DisagreeAgree
36,Please login to comment
36,0 Comments
36,Inline Feedbacks
36,View all comments
36,Cloud Infrastructure Services Ltd
36,2A Cedar Drive
36,Hatch End
36,Pinner
36,"HA5 4DE, UK"
36,Terms
36,Privacy Policy
36,Recent Posts
36,Snipe-IT IT Asset Inventory Management on Azure/AWS/GCP
36,Choosing the Right Proxy Server: A Detailed Guide for Businesses
36,Proxy Server 101: Understanding the Basics of Proxy Technology
36,Ansible Roles: Create / Use Roles for Configuration Management
36,The Future of Proxy Servers: Trends and Innovations
36,Magento Server Monitoring: Monitor Magento for Best Performance
36,Redis Techniques: Pub/Sub Messaging / Real-Time Data Analytics
36,SFTP Security: How to Secure File Transfers with SFTP
36,Squid Proxy with SquidGuard: How to Implement Content Filtering
36,How to Install Ansible on Ubuntu 23.04 Server
36,PagesContact Us
36,About Us
36,Azure Marketplace Solutions
36,AWS Marketplace Solutions
36,GCP Marketplace Solutions
36,Azure Management
36,Cloud IaaS Setup & Management Services
36,Active Directory Reporting Tool
36,WordPress SSO
36,Blog
36,Follow Us
36,"wpDiscuz00Would love your thoughts, please comment.x()x| ReplyInsert"
37,Query Store - Azure Database for MariaDB | Microsoft Learn
37,Skip to main content
37,This browser is no longer supported.
37,"Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
37,Download Microsoft Edge
37,More info about Internet Explorer and Microsoft Edge
37,Table of contents
37,Exit focus mode
37,Read in English
37,Save
37,Table of contents
37,Read in English
37,Save
37,Edit
37,Print
37,Twitter
37,LinkedIn
37,Facebook
37,Email
37,Table of contents
37,Monitor Azure Database for MariaDB performance with Query Store
37,Article
37,04/19/2023
37,7 contributors
37,Feedback
37,In this article
37,Important
37,"Azure Database for MariaDB is on the retirement path. We strongly recommend that you migrate to Azure Database for MySQL. For more information about migrating to Azure Database for MySQL, see What's happening to Azure Database for MariaDB?."
37,Applies to: Azure Database for MariaDB 10.2
37,"The Query Store feature in Azure Database for Mariadb provides a way to track query performance over time. Query Store simplifies performance troubleshooting by helping you quickly find the longest running and most resource-intensive queries. Query Store automatically captures a history of queries and runtime statistics, and it retains them for your review. It separates data by time windows so that you can see database usage patterns. Data for all users, databases, and queries is stored in the mysql schema database in the Azure Database for MariaDB instance."
37,Common scenarios for using Query Store
37,"Query store can be used in many scenarios, including the following:"
37,Detecting regressed queries
37,Determining the number of times a query was executed in a given time window
37,Comparing the average execution time of a query across time windows to see large deltas
37,Enable Query Store
37,"Query Store is an opt-in feature, so it isn't active by default on a server. The query store is enabled or disabled globally for all the databases on a given server and cannot be turned on or off per database."
37,Enable Query Store using the Azure portal
37,Sign in to the Azure portal and select your Azure Database for MariaDB server.
37,Select Server Parameters in the Settings section of the menu.
37,Search for the query_store_capture_mode parameter.
37,Set the value to ALL and Save.
37,To enable wait statistics in your Query Store:
37,Search for the query_store_wait_sampling_capture_mode parameter.
37,Set the value to ALL and Save.
37,Allow up to 20 minutes for the first batch of data to persist in the mysql database.
37,Information in Query Store
37,Query Store has two stores:
37,The runtime statistics store for persisting the query execution statistics information.
37,The wait statistics store for persisting wait statistics information.
37,"To minimize space usage, the runtime execution statistics in the runtime statistics store are aggregated over a fixed, configurable time window. The information in these stores is visible by querying the query store views."
37,The following query returns information about queries in Query Store:
37,SELECT * FROM mysql.query_store;
37,Or this query for wait statistics:
37,SELECT * FROM mysql.query_store_wait_stats;
37,Find wait queries
37,Note
37,"Wait statistics should not be enabled during peak workload hours or be turned on indefinitely for sensitive workloads. For workloads running with high CPU utilization or on servers configured with lower vCores, use caution when enabling wait statistics. It should not be turned on indefinitely."
37,"Wait event types combine different wait events into buckets by similarity. Query Store provides the wait event type, specific wait event name, and the query in question. Being able to correlate this wait information with the query runtime statistics means you can gain a deeper understanding of what contributes to query performance characteristics."
37,Here are some examples of how you can gain more insights into your workload using the wait statistics in Query Store:
37,Observation
37,Action
37,High Lock waits
37,"Check the query texts for the affected queries and identify the target entities. Look in Query Store for other queries modifying the same entity, which is executed frequently and/or have high duration. After identifying these queries, consider changing the application logic to improve concurrency, or use a less restrictive isolation level."
37,High Buffer IO waits
37,"Find the queries with a high number of physical reads in Query Store. If they match the queries with high IO waits, consider introducing an index on the underlying entity, to do seeks instead of scans. This would minimize the IO overhead of the queries. Check the Performance Recommendations for your server in the portal to see if there are index recommendations for this server that would optimize the queries."
37,High Memory waits
37,Find the top memory consuming queries in Query Store. These queries are probably delaying further progress of the affected queries. Check the Performance Recommendations for your server in the portal to see if there are index recommendations that would optimize these queries.
37,Configuration options
37,"When Query Store is enabled it saves data in 15-minute aggregation windows, up to 500 distinct queries per window."
37,The following options are available for configuring Query Store parameters.
37,Parameter
37,Description
37,Default
37,Range
37,query_store_capture_mode
37,"Turn the query store feature ON/OFF based on the value. Note: If performance_schema is OFF, turning on query_store_capture_mode will turn on performance_schema and a subset of performance schema instruments required for this feature."
37,ALL
37,"NONE, ALL"
37,query_store_capture_interval
37,The query store capture interval in minutes. Allows specifying the interval in which the query metrics are aggregated
37,5 - 60
37,query_store_capture_utility_queries
37,Turning ON or OFF to capture all the utility queries that is executing in the system.
37,"YES, NO"
37,query_store_retention_period_in_days
37,Time window in days to retain the data in the query store.
37,1 - 30
37,The following options apply specifically to wait statistics.
37,Parameter
37,Description
37,Default
37,Range
37,query_store_wait_sampling_capture_mode
37,Allows turning ON / OFF the wait statistics.
37,NONE
37,"NONE, ALL"
37,query_store_wait_sampling_frequency
37,Alters frequency of wait-sampling in seconds. 5 to 300 seconds.
37,5-300
37,Note
37,"Currently query_store_capture_mode supersedes this configuration, meaning both query_store_capture_mode and query_store_wait_sampling_capture_mode have to be enabled to ALL for wait statistics to work. If query_store_capture_mode is turned off, then wait statistics is turned off as well since wait statistics utilizes the performance_schema enabled, and the query_text captured by query store."
37,Use the Azure portal to get or set a different value for a parameter.
37,Views and functions
37,View and manage Query Store using the following views and functions. Anyone in the select privilege public role can use these views to see the data in Query Store. These views are only available in the mysql database.
37,"Queries are normalized by looking at their structure after removing literals and constants. If two queries are identical except for literal values, they'll have the same hash."
37,mysql.query_store
37,"This view returns all the data in Query Store. There is one row for each distinct database ID, user ID, and query ID."
37,Name
37,Data Type
37,IS_NULLABLE
37,Description
37,schema_name
37,varchar(64)
37,Name of the schema
37,query_id
37,bigint(20)
37,"Unique ID generated for the specific query, if the same query executes in different schema, a new ID will be generated"
37,timestamp_id
37,timestamp
37,Timestamp in which the query is executed. This is based on the query_store_interval configuration
37,query_digest_text
37,longtext
37,The normalized query text after removing all the literals
37,query_sample_text
37,longtext
37,First appearance of the actual query with literals
37,query_digest_truncated
37,bit
37,YES
37,Whether the query text has been truncated. Value will be Yes if the query is longer than 1 KB
37,execution_count
37,bigint(20)
37,The number of times the query got executed for this timestamp ID / during the configured interval period
37,warning_count
37,bigint(20)
37,Number of warnings this query generated during the internal
37,error_count
37,bigint(20)
37,Number of errors this query generated during the interval
37,sum_timer_wait
37,double
37,YES
37,Total execution time of this query during the interval
37,avg_timer_wait
37,double
37,YES
37,Average execution time for this query during the interval
37,min_timer_wait
37,double
37,YES
37,Minimum execution time for this query
37,max_timer_wait
37,double
37,YES
37,Maximum execution time
37,sum_lock_time
37,bigint(20)
37,Total amount of time spent for all the locks for this query execution during this time window
37,sum_rows_affected
37,bigint(20)
37,Number of rows affected
37,sum_rows_sent
37,bigint(20)
37,Number of rows sent to client
37,sum_rows_examined
37,bigint(20)
37,Number of rows examined
37,sum_select_full_join
37,bigint(20)
37,Number of full joins
37,sum_select_scan
37,bigint(20)
37,Number of select scans
37,sum_sort_rows
37,bigint(20)
37,Number of rows sorted
37,sum_no_index_used
37,bigint(20)
37,Number of times when the query didn't use any indexes
37,sum_no_good_index_used
37,bigint(20)
37,Number of times when the query execution engine didn't use any good indexes
37,sum_created_tmp_tables
37,bigint(20)
37,Total number of temp tables created
37,sum_created_tmp_disk_tables
37,bigint(20)
37,Total number of temp tables created in disk (generates I/O)
37,first_seen
37,timestamp
37,The first occurrence (UTC) of the query during the aggregation window
37,last_seen
37,timestamp
37,The last occurrence (UTC) of the query during this aggregation window
37,mysql.query_store_wait_stats
37,"This view returns wait events data in Query Store. There's one row for each distinct database ID, user ID, query ID, and event."
37,Name
37,Data Type
37,IS_NULLABLE
37,Description
37,interval_start
37,timestamp
37,Start of the interval (15-minute increment)
37,interval_end
37,timestamp
37,End of the interval (15-minute increment)
37,query_id
37,bigint(20)
37,Generated unique ID on the normalized query (from query store)
37,query_digest_id
37,varchar(32)
37,The normalized query text after removing all the literals (from query store)
37,query_digest_text
37,longtext
37,First appearance of the actual query with literals (from query store)
37,event_type
37,varchar(32)
37,Category of the wait event
37,event_name
37,varchar(128)
37,Name of the wait event
37,count_star
37,bigint(20)
37,Number of wait events sampled during the interval for the query
37,sum_timer_wait_ms
37,double
37,Total wait time (in milliseconds) of this query during the interval
37,Functions
37,Name
37,Description
37,mysql.az_purge_querystore_data(TIMESTAMP)
37,Purges all query store data before the given time stamp
37,mysql.az_procedure_purge_querystore_event(TIMESTAMP)
37,Purges all wait event data before the given time stamp
37,mysql.az_procedure_purge_recommendation(TIMESTAMP)
37,Purges recommendations whose expiration is before the given time stamp
37,Limitations and known issues
37,"If a MariaDB server has the parameter default_transaction_read_only on, Query Store can't capture data."
37,Query Store functionality can be interrupted if it encounters long Unicode queries (>= 6000 bytes).
37,The retention period for wait statistics is 24 hours.
37,Wait statistics uses sample ti capture a fraction of events. The frequency can be modified using the parameter query_store_wait_sampling_frequency.
37,Query store is not supported for version 10.3.
37,Next steps
37,Learn more about Query Performance Insights
37,Feedback
37,Coming soon: Throughout 2024 we will be phasing out GitHub Issues as the feedback mechanism for content and replacing it with a new feedback system. For more information see: https://aka.ms/ContentUserFeedback.
37,Submit and view feedback for
37,This product
37,This page
37,View all page feedback
37,Additional resources
37,California Consumer Privacy Act (CCPA) Opt-Out Icon
37,Your Privacy Choices
37,Theme
37,Light
37,Dark
37,High contrast
37,Previous Versions
37,Blog
37,Contribute
37,Privacy
37,Terms of Use
37,Trademarks
37,© Microsoft 2024
37,Additional resources
37,In this article
37,California Consumer Privacy Act (CCPA) Opt-Out Icon
37,Your Privacy Choices
37,Theme
37,Light
37,Dark
37,High contrast
37,Previous Versions
37,Blog
37,Contribute
37,Privacy
37,Terms of Use
37,Trademarks
37,© Microsoft 2024
38,"Performance | GORM - The fantastic ORM library for Golang, aims to be developer friendly."
38,DocsGenCommunityAPIContribute
38,English
38,English
38,简体中文
38,Deutsch
38,bahasa Indonesia
38,日本語
38,Русский
38,한국어
38,हिन्दी
38,French
38,Italiano
38,Español
38,Performance
38,"GORM optimizes many things to improve the performance, the default performance should be good for most applications, but there are still some tips for how to improve it for your application."
38,"Disable Default TransactionGORM performs write (create/update/delete) operations inside a transaction to ensure data consistency, which is bad for performance, you can disable it during initialization"
38,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
38,"SkipDefaultTransaction: true,})"
38,Caches Prepared StatementCreates a prepared statement when executing any SQL and caches them to speed up future calls
38,"// Globally modedb, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
38,"PrepareStmt: true,})// Session modetx := db.Session(&Session{PrepareStmt: true})tx.First(&user, 1)tx.Find(&users)tx.Model(&user).Update(""Age"", 18)"
38,NOTE Also refer how to enable interpolateparams for MySQL to reduce roundtrip https://github.com/go-sql-driver/mysql#interpolateparams
38,"SQL Builder with PreparedStmtPrepared Statement works with RAW SQL also, for example:"
38,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
38,"PrepareStmt: true,})db.Raw(""select sum(age) from users where role = ?"", ""admin"").Scan(&age)"
38,"You can also use GORM API to prepare SQL with DryRun Mode, and execute it with prepared statement later, checkout Session Mode for details"
38,"Select FieldsBy default GORM select all fields when querying, you can use Select to specify fields you want"
38,"db.Select(""Name"", ""Age"").Find(&Users{})"
38,Or define a smaller API struct to use the smart select fields feature
38,type User struct {
38,uint
38,Name
38,string
38,Age
38,int
38,Gender string
38,// hundreds of fields}type APIUser struct {
38,uint
38,"Name string}// Select `id`, `name` automatically when querydb.Model(&User{}).Limit(10).Find(&APIUser{})// SELECT `id`, `name` FROM `users` LIMIT 10"
38,Iteration / FindInBatchesQuery and process records with iteration or in batches
38,"Index HintsIndex is used to speed up data search and SQL query performance. Index Hints gives the optimizer information about how to choose indexes during query processing, which gives the flexibility to choose a more efficient execution plan than the optimizer"
38,"import ""gorm.io/hints""db.Clauses(hints.UseIndex(""idx_user_name"")).Find(&User{})// SELECT * FROM `users` USE INDEX (`idx_user_name`)db.Clauses(hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForJoin()).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR JOIN (`idx_user_name`,`idx_user_id`)""db.Clauses("
38,"hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForOrderBy(),"
38,"hints.IgnoreIndex(""idx_user_name"").ForGroupBy(),).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR ORDER BY (`idx_user_name`,`idx_user_id`) IGNORE INDEX FOR GROUP BY (`idx_user_name`)"""
38,"Read/Write SplittingIncrease data throughput through read/write splitting, check out Database Resolver"
38,Last updated: 2024-03-15
38,PrevNext
38,Platinum Sponsors
38,Become a Sponsor!
38,Gold Sponsors
38,Become a Sponsor!
38,Platinum Sponsors
38,Become a Sponsor!
38,Gold Sponsors
38,Become a Sponsor!
38,Contents
38,Disable Default TransactionCaches Prepared StatementSQL Builder with PreparedStmtSelect FieldsIteration / FindInBatchesIndex HintsRead/Write Splitting
38,Improve this page
38,Back to Top
38,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
38,© 2013~2024 Jinzhu
38,Documentation licensed under CC BY 4.0.
38,感谢 无闻 对域名 gorm.cn 的捐赠
38,浙ICP备2020033190号-1
38,Home
38,DocsGenCommunityAPIContribute
38,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
38,English
38,English
38,简体中文
38,Deutsch
38,bahasa Indonesia
38,日本語
38,Русский
38,한국어
38,हिन्दी
38,French
38,Italiano
38,Español
40,Performance optimization recommendations | Adobe Commerce
40,DocumentationCommerceImplementation Playbook
40,Performance optimization review
40,Last update: Fri Sep 01 2023 00:00:00 GMT+0000 (Coordinated Universal Time)
40,Topics:
40,Cloud
40,CREATED FOR:
40,Experienced
40,Admin
40,Developer
40,"Even as performance optimization can come from many aspects, there are some general recommendations that should be considered for most scenarios. General recommendations include configuration optimization for infrastructure elements, Adobe Commerce backend configuration, and architecture scalability planning."
40,TIP
40,See the Performance Best Practices Guide for more information about performance optimization.
40,Infrastructure
40,The following sections describe recommendations for infrastructure optimizations.
40,DNS lookups
40,DNS lookup is the process of finding which IP address that the domain name belongs to. The browser must wait until the DNS lookup is complete before it can download anything for each request. Reducing DNS lookups is important to improve overall page load times.
40,Content delivery network (CDN)
40,"Use a CDN to optimize asset downloading performance. Adobe Commerce uses Fastly. If you have an on-premises implementation of Adobe Commerce, you should also consider adding a CDN layer."
40,Web latency
40,The location of the datacenter affects the web latency for frontend users.
40,Network bandwidth
40,"Sufficient network bandwidth is one of the key requirements for data exchange between web nodes, databases, caching/session servers, and other services."
40,"Because Adobe Commerce effectively uses caching for high performance, your system can actively exchange data with caching servers like Redis. If Redis is installed on a remote server, you must provide a sufficient network channel between web nodes and the caching server to prevent bottlenecks on read/write operations."
40,Operating system (OS)
40,"Operating system configurations and optimizations are similar for Adobe Commerce when compared to other high-load web applications. As the number of concurrent connections handled by the server increases, the number of available sockets can become fully allocated."
40,CPU of web nodes
40,"One CPU core can serve around 2-4 Adobe Commerce requests without cache effectively. To determine how many web nodes/cores needed to process all incoming requests without putting them into queue, use the equation:"
40,N[Cores] = (N [Expected Requests] / 2) + N [Expected Cron Processes])
40,PHP-FPM settings
40,Optimizing these settings depends on the performance test results for different projects.
40,"ByteCode—To get maximum speed out of Adobe Commerce on PHP 7, you must activate the opcache module and configure it properly."
40,"APCU—Adobe recommends enabling the PHP APCu extension and configuring Composer to optimize for maximum performance. This extension caches file locations for opened files, which increases performance for Adobe Commerce server calls, including pages, Ajax calls, and endpoints."
40,Realpath_cacheconfiguration—Optimizing realpath_cache allows PHP processes to cache paths to files instead of looking them up each time a page loads.
40,Web server
40,Only slight reconfiguration is needed to use nginx as a web server. The nginx web server provides better performance and is easy to configure using the sample configuration file from Adobe Commerce (nginx.conf.sample).
40,Set up PHP-FPM with TCP properly
40,Enable HTTP/2 and Gzip
40,Optimize worker connections
40,Database
40,"This document does not provide in-depth MySQL tuning instructions because each store and environment is different, but Adobe can make general recommendations."
40,"The Adobe Commerce database (and any other database) is sensitive to the amount of memory available for storing data and indexes. To effectively use MySQL data indexation, the amount of memory available should be, at minimum, close to half the size of the data stored in the database."
40,Optimize the innodb_buffer_pool_instances setting to avoid issues with multiple threads attempting to access the same instance. The value of the max_connections parameter should correlate with the total number of PHP threads configured in the application server. Use the following formula to calculate the best value for innodb-thread-concurrency:
40,innodb-thread-concurrency = 2 * (NumCPUs+NumDisks)
40,Session caching
40,Session caching is a good candidate to configure for a separate instance of Redis. Memory configuration for this cache type should consider the site’s cart abandonment strategy and how long a session should expect to remain in the cache.
40,Redis should have enough memory allocated to hold all other caches in memory for optimal performance. Block cache is the key factor in determining the amount of memory to configure. Block cache grows relative to the number of pages on a site (number of SKU x number of store views).
40,Page caching
40,"Adobe highly recommends using Varnish for full page cache on your Adobe Commerce store. The PageCache module is still present in the codebase, but it should be used for development purposes only."
40,"Install Varnish on a separate server in front of the web tier. It should accept all incoming requests and provide cached page copies. To allow Varnish to work effectively with secured pages, an SSL termination proxy can be placed in front of Varnish. Nginx can be used for this purpose."
40,"While Varnish full page cache memory invalidation is effective, Adobe recommends allocating enough memory to Varnish to hold your most popular pages in memory."
40,Message queues
40,"The Message Queue Framework (MQF) is a system that allows a module to publish messages to queues. It also defines the consumers that receive the messages asynchronously. Adobe Commerce supports RabbitMQ as the messaging broker, which provides a scalable platform for sending and receiving messages."
40,Performance testing and monitoring
40,Performance testing before each production release is always recommended to get an estimation of the capability of your ecommerce platform. Keep monitoring after launch and have a scalability and backup plan for handling peak times.
40,NOTE
40,"Adobe Commerce on cloud infrastructure already applies the above infrastructure and architecture optimizations, except for the DNS lookup because it’s out of scope."
40,Search search-heading
40,"Elasticsearch (or OpenSearch) is required as of Adobe Commerce version 2.4, but it’s also a best practice to enable it for versions before 2.4."
40,Operating models
40,"Apart from the previously mentioned common infrastructure optimization recommendations, there are also approaches to enhance the performance for specific business modes and scales. This document does not provide in-depth tuning instructions for all of them because each scenario is different, but Adobe can provide a few high-level options for your reference."
40,Headless architecture
40,"There is a separate section dedicated to headless. In summary, it separates the storefront layer from the platform itself. It is still the same backend, but Adobe Commerce no longer processes requests directly and instead only supports custom storefronts through the GraphQL API."
40,Keep Adobe Commerce updated
40,"Adobe Commerce always has better performance when running the newest version. Even if it is not possible to keep Adobe Commerce up to date after each new version is released, it is still recommended to upgrade when Adobe Commerce introduces significant performance optimizations."
40,"For example, in 2020, Adobe released an optimization to the Redis layer, fixing many inefficiencies, connection issues, and unnecessary data transfer between Redis and Adobe Commerce. Overall performance between 2.3 and 2.4 is night and day and provided significant improvements in cart, checkout, concurrent users, just because of the Redis optimization."
40,Optimize data model
40,"Many problems originate from data, including bad data models, data that is not structured properly, and data that is missing an index."
40,"It looks fine if you’re testing a few connections, but after you deploy to production you might see serious performance degradation. It’s important that systems integrators know how to design a data model (especially for product attributes), avoid adding unnecessary attributes, and keep mandatory attributes that affect business logic (such as pricing, stock availability, and search)."
40,"For those attributes that do not affect business logic but must still be present on the storefront, combine them into a few attributes (for example, JSON format)."
40,"To optimize platform performance, if business logic is not required on the storefront from data or attributes taken from a PIM or an ERP, there is no need to add that attribute into Adobe Commerce."
40,Design for scalability
40,Scalability is important for businesses running campaigns and frequently facing peak traffic times. A scalable architecture and application design can increase resources during peak times and reduce them after it.
40,recommendation-more-help
40,754cbbf3-3a3c-4af3-b6ce-9d34390f3a60
43,HAProxy Logging- How to Tune Timeouts for Performance - Papertrail
43,Tour
43,Product
43,Technology
43,Capabilities
43,Languages
43,Tips from the Team
43,Papertrail Guides
43,Help
43,Pricing
43,Blog
43,Contact
43,Request
43,Demo
43,Log In
43,Sign Up
43,SolarWinds.com Blog Contact Us
43,PRODUCTS
43,FEATURES
43,TIPS FROM THE TEAM
43,RESOURCES
43,LOG IN
43,REQUEST DEMOFREE TRIAL
43,TECHNOLOGY
43,CAPABILITIES
43,LANGUAGES
43,Make Your Logs Work for You
43,"The days of logging in to servers and manually viewing log files are over. SolarWinds® Papertrail™ aggregates logs from applications, devices, and platforms to a central location."
43,View Technology Info
43,FEATURED TECHNOLOGY
43,Kubenetes Cluster LoggingKubernetes LoggingAWS Log ManagementGC Log AnalyzerAWS LoggingDocker LoggingDocker SyslogHeroku LoggingLogging Tools for Azure App
43,Troubleshoot Fast and Enjoy It
43,"SolarWinds® Papertrail™ provides cloud-based log management that seamlessly aggregates logs from applications, servers, network devices, services, platforms, and much more."
43,View Capabilities Info
43,FEATURED CAPABILITIES
43,Log ViewerFirewall Log AnalyzerCentralized Log ManagementLog AnalysisTail LoggingCron Job MonitoringLog AggregatorCloud Logging
43,Aggregate and Search Any Log
43,"SolarWinds® Papertrail™ provides lightning-fast search, live tail, flexible system groups, team-wide access, and integration with popular communications platforms like PagerDuty and Slack to help you quickly track down customer problems, debug app requests, or troubleshoot slow database queries."
43,View Languages Info
43,FEATURED LANGUAGES
43,Linux Logging SoftwareLinux Log ManagementRuby Logging ManagerGolang LoggingHAproxy Log AnalyzerMySQL LoggingNGINX LoggingJavascript LoggingTomcat LoggingApache Log Analyzer
43,EVENT VIEWER
43,APM INTEGRATION
43,BUILT FOR COLLABORATION
43,Meet the Event Viewer
43,It's the heart of Papertrail.
43,Start Tour
43,TAKE A TOUR
43,Live TailSeek by TimeContextElegant SearchAPM IntegrationSkim-abilitySaved SearchesLog Velocity Analytics
43,TBD - APM Integration Title
43,TBD - APM Integration Description
43,TBD Link
43,APM Integration Feature List
43,Feature 1
43,TBD - Built for Collaboration Title
43,TBD - Built for Collaboration Description
43,TBD Link
43,Built for Collaboration Feature List
43,TIPS FROM THE TEAM
43,Tips from the Team
43,"By engineers, for engineers"
43,View More Tips
43,Additional Tips
43,Kubernetes LoggingMonitoring WordPress Error LogsA Guide to Log FilteringManaging and Analyzing Firewall LogsWhat Your Router Logs Say About Your NetworkHow to Diagnose App Issues Using Crash LogsDocker Container Logs5 Reasons LaaS Is Essential for Modern Log Management6 Secrets for Successful Log ManagementHow to Live Tail Docker Log
43,Technical Resources
43,Admin Guide Getting Started Release Notes
43,Educational Resources
43,Take a Tour Tips from the Team Use Cases
43,Connect with Us
43,THWACK Community Blog COVID-19 Resource Center
43,PRODUCTS
43,TECHNOLOGY
43,Kubenetes Cluster Logging Kubernetes Logging AWS Log Management GC Log Analyzer AWS Logging Docker Logging Docker Syslog Heroku Logging Logging Tools for Azure App
43,CAPABILITIES
43,Log Viewer Firewall Log Analyzer Centralized Log Management Log Analysis Tail Logging Cron Job Monitoring Log Aggregator Cloud Logging
43,LANGUAGES
43,Linux Logging Software Linux Log Management Ruby Logging Manager Golang Logging HAproxy Log Analyzer MySQL Logging NGINX Logging Javascript Logging Tomcat Logging Apache Log Analyzer
43,FEATURES
43,EVENT VIEWER
43,Live Tail Seek by Time Context Elegant Search APM Integration Skim-ability Saved Searches Log Velocity Analytics
43,APM INTEGRATION
43,Feature 1
43,BUILT FOR COLLABORATION
43,TIPS FROM THE TEAM
43,TIPS FROM THE TEAM
43,Kubernetes Logging Monitoring WordPress Error Logs A Guide to Log Filtering Managing and Analyzing Firewall Logs What Your Router Logs Say About Your Network How to Diagnose App Issues Using Crash Logs Docker Container Logs 5 Reasons LaaS Is Essential for Modern Log Management 6 Secrets for Successful Log Management How to Live Tail Docker Log
43,RESOURCES
43,Technical Resources
43,Admin Guide Getting Started Release Notes
43,Educational Resources
43,Take a Tour Tips from the Team Use Cases
43,Connect with Us
43,THWACK Community Blog COVID-19 Resource Center
43,LOG IN
43,REQUEST DEMO
43,FREE TRIAL
43,Tips from the Team
43,HAProxy Logging- How to Tune Timeouts for Performance
43,START FREE TRIAL
43,Fully Functional for 14 Days
43,Tips from the Team
43,Logging in Docker – How to Get It Right
43,HAProxy Logging- How to Tune Timeouts for Performance
43,Kubernetes Logging: Tips to Avoid Memory and Resource Issues
43,Centralized Logging for .NET 5 Applications
43,Heroku: Most Common Errors Explained
43,Azure Web Apps Logging With Net 5—How-To
43,7 Problems to Look out for When Analyzing Garbage Collection Logs
43,Logging in Java – Best Practices and Tips
43,How to Diagnose App Issues Using Crash Logs
43,Best Practices for Centralized Logging in Microservices Architecture
43,"Last updated: February 5, 2024"
43,"HAProxy (high availability proxy) is a critical part of modern systems infrastructure. It’s ideally the first point of contact for users who access your application. When configured correctly, HAProxy improves your app’s performance significantly. Through load balancing, HAProxy makes sure each service your application depends on is accessible to users, especially under load conditions otherwise impacting application performance."
43,I’m going to take you through the process of tuning timeouts with the intent to boost application performance. You’ll see how robust HAProxy logging can help you with troubleshooting timeout issues and improve the performance of your application. I’ll quickly go through some of the HAProxy timeout configurations to lay a foundation.
43,"Before we dive into the overview, let’s go over a few reasons why we need HAProxy and the logic behind it. This should help us visualize the “how” part later and understand why it’s worth going through the tuning processes."
43,Why You’ll Need HAProxy
43,"The image above shows the basic design of how users access web. This method works fine if the application doesn’t get much traffic. Once the application gains traction and the number of users increases, you can see application performance begin to decline. When numerous users access the application at the same time, requests can back up and even overwhelm the application."
43,A good analogy is a single-lane road going from point A to point B filling up when there are too many cars. Adding HAProxy as a load balancer is like adding lanes to the road. More lanes mean more vehicles can now travel the road.
43,"Additional lanes won’t increase speed, but cars travel down the road without waiting for the car ahead to advance. However, if you add enough vehicles, even these additional lanes will become congested. This is where the concept of timeouts is essential to avoid jams."
43,"Timeouts terminate connections after a client waits for a predetermined amount of time to access the server. This frees up connections, so active users can access the application. It’s as if the stalled cars are taken off the road, so other cars can move freely. Let’s quickly cover the various types of timeouts before we get to the tuning part."
43,The Three Basic HAProxy Timeouts
43,"Just to show you what timeout configurations look like, here’s a sample including the three basic timeouts."
43,##based on Mesosphere Marathon’s servicerouter.py haproxy config
43,global
43,daemon
43,log 127.0.0.1 local0
43,log 127.0.0.1 local1 notice
43,maxconn 4096
43,tune.ssl.default-dh-param 2048
43,defaults
43,log global
43,retries 3
43,maxconn 2000
43,timeout connect 5s
43,timeout client 50s
43,timeout server 50s
43,Source: GitHub
43,1. Timeout Client
43,"The <timeout client> setting defines the maximum time a client can be inactive when connected to the server. A common value for this timeout is five minutes. You can go with shorter timeouts, even as little as thirty seconds, if you’re attempting to maximize security or the total number of active connections. As the name suggests, this timeout is handled on the client side."
43,2. Timeout Connect
43,"The <timeout connect> works like a grace period. It sets the maximum time the client has to connect to a server. The time it takes a client to connect to a server varies widely, depending on network complexity. The more complex a topology, the longer it can take the client to connect."
43,"The timeout connect allows the client to try to connect again if the initial attempt fails. In addition to the connection time, you’ll need to set the numbers of retries. The default is three, but you can adjust it to fit your environment."
43,3. Timeout Server
43,"When a client sends a request to the server, it expects a response. If the server doesn’t respond in the configured time duration, a <timeout server> is invoked. This is akin to the <timeout client>, only in reverse. You can see in the list of HTTP responses, if a <timeout serve> is invoked, you’ll get a 504 Gateway Timeout response from HAProxy."
43,HAProxy Timeout Tuning for Good Performance
43,"Just by configuring these three timeout values in your haproxy.cfg file, you can achieve a basic level of performance. If you want to take it up a notch, you can set other timeout settings to enhance performance. While the values you set will vary depending on your traffic load and environment, I’ve listed the most common configurations below. To use these, append the following to your configuration file."
43,timeout http-request 10s
43,timeout http-keep-alive 2s
43,timeout queue 5s
43,timeout tunnel 2m
43,timeout client-fin 1s
43,timeout server-fin 1s
43,Timeout HTTP-Request
43,"The <timeout http-request> variable limits the total time each request can persist. Aside from optimizing request performance, it can also defend against denial of service (DDoS) attacks by limiting the time a single request can last. Usually, ten seconds is a good limit."
43,"Some php.ini files may also have this setting, but since the proxy server is the first point of contact with an application, php.ini settings are overridden. This is true unless the application server level (php.ini) setting is shorter than the proxy determined variable."
43,Timeout HTTP-Keep-Alive
43,"As the name suggests, this is a timeout designed to keep a single connection between the client and the server “alive” for a desired amount of time. While the connection is alive, all data packets can pass without needing to ask for a connection again. This essentially makes getting responses from the server to the client faster."
43,"Keep in mind the <timeout http-request> regulates how long a single request can persist, so these two settings work hand in hand. If the <timeout http-keep-alive> isn’t set or has a value less than the <timeout http-request>, the latter determines the connection status."
43,Timeout Queue
43,"The <timeout queue> limits the number of concurrent connections, which can also impact performance. Setting the queue timeout shortens wait times by limiting connections and allowing clients to try connecting again if the queue is full. This is similar to <timeout connect>, except <timeout queue> limits the number of connections."
43,"If you don’t set the <timeout queue>, HAProxy will default to the <timeout connect> settings to manage the queue."
43,Timeout Tunnel
43,"The <timeout tunnel> variable only applies when you’re working with WebSockets. Essentially, it’s <timeout keep-alive> on steroids, often with durations exceeding minutes. It may seem counterproductive and a potential security risk to keep a connection open for that long. However, when used with other timeout configurations, it’s possible to maintain a safe yet high-performing connection."
43,"For instance, the <timeout http-request> variable would prevent an attack even if the <timeout tunnel> is set at several minutes. Remember, though, that the virtual tunnel you create by implementing this timeout requires you to terminate it at some point using the <timeout client-fin> parameter."
43,Timeout Client-Fin
43,"Say a connection drops in the middle of a client request; if you look at the HAProxy logs, you’re likely to see the lost connection is a result of client-side network issues. To handle these types of situations, HAProxy creates a list of dropped client-side connections. The <timeout client-fin> limits the amount of time a client request will be maintained on this list."
43,"This parameter starts ticking when a connection joins the list. Without it, the server will maintain a “maybe they’ll return” sort of connection while others are denied service. To optimize performance, the time values set for this timeout are usually short."
43,Timeout Server-Fin
43,"Much like the <timeout client-fin> concept, abrupt disconnections can also occur on the server side of the application. An optimal setup would include redundant servers for load-balancing. When a server has too many requests, redundancy will allow you to reroute overflow requests to less busy servers and speed up response times."
43,The <timeout server-fin> limits the time the client waits for a server response before an alternate server is queried.
43,HAProxy Logging: How to Determine Perfect Timeouts
43,"By now, you probably can see how environment variables and traffic can help you determine the time allocations for the timeouts above. Finding the perfect balance between peak performance and optimal security is a matter of trial and error. Keeping a close eye on HAProxy logs can help you see the interactions between these different configurations and find the “sweet spot” for your application and environment. You can use HAProxy logs to understand:"
43,"Timestamped metrics about traffic (timing data, connections counters, traffic size)"
43,"Detailed event logs of HAProxy operations (content switching, filtering, persistence)"
43,"Request and responses (headers, status codes, payloads)"
43,Session terminations and tracking where failures are occurring
43,"Knowing when a timeout event occurs and monitoring the events preceding it are the first steps for successfully troubleshooting and tuning timeout settings. If you’re looking for an easy cloud-based log management tool, check out SolarWinds® Papertrail™. Built by engineers for engineers, Papertrail can automatically parse HAProxy logs and help you quickly troubleshoot timeout issues."
43,"It offers a simple search syntax allowing you to search all your logs from a central interface, see events in context, and pinpoint issues. The live tail feature is particularly helpful for real-time troubleshooting. If you want to start HAProxy logging with better results, sign up for a trial or request a demo."
43,"This post was written by Taurai Mutimutema. Taurai is a systems analyst with a knack for writing, which was probably sparked by the need to document technical processes during code and implementation sessions. He enjoys learning new technology and talks about tech even more than he writes."
43,"Aggregate, organize, and manage your logs"
43,"Collect real-time log data from your applications, servers, cloud services, and more"
43,"Search log messages to analyze and troubleshoot incidents, identify trends, and set alerts"
43,"Create comprehensive per-user access control policies, automated backups, and archives of up to a year of historical data"
43,Start Free Trial Fully Functional for 30 Days
43,Let's talk it over
43,"Contact our team, anytime."
43,Toll Free: +1 (855) 679-0752
43,Phone: +1 (512) 498-6011
43,papertrailapp@solarwinds.com
43,Datasheet
43,Help
43,Contact
43,@papertrailapp
43,Legal Documents
43,California Privacy Rights
43,Software Services Agreement
43,Privacy Notice
43,GDPR Resource Center
43,SolarWinds Subscription Center
43,COVID-19 Resource Center
43,"© 2024 SolarWinds Worldwide, LLC. All rights reserved."
43,We’re Geekbuilt ™.
43,"Developed by network and systems engineers who know what it takes to manage today’s dynamic IT environments,"
43,SolarWinds has a deep connection to the IT community.
43,"The result? IT management products that are effective, accessible, and easy to use."
43,COMPANY
43,CARRER CENTER
43,EMAIL PREFNCE CENTER
43,SOLUTION FINDER
43,FOR GOVERNMENT
43,FOR PARTNERS
43,FOR TEST
43,Another Link
43,Privacy Notice
43,Terms of use
43,Security Policy
43,Another Link
43,"© 2021 SolarWinds Worldwide, LLC. All rights reserved."
44,MySQL Performance Tuning and Monitoring Scripts
44,- Systron Micronix Corporation - Wissensdatenbank
44,Main Site
44,Dedicated Servers
44,Dedicated Servers Plans
44,Customized Dedicated Servers
44,SSD Dedicated Servers
44,VPS Servers
44,About VPS Servers
44,Configure VPS
44,Parallels Virtuozzo VPS
44,Shared Hosting
44,About Shared Hosting
44,Linux Shared Hosting
44,Windows Shared Hosting
44,Server Backup
44,CDP Backup
44,Acronis Cloud Backup
44,Email Solutions
44,Hosted Cloud Email
44,Hosted OX App Suite Business
44,Google Workspace
44,Microsoft 365
44,Mein Warenkorb
44,Kundencenter Home
44,Shop
44,Alle anzeigen
44,Dedicated Servers
44,Managed NVMe SSD VPS Server
44,Unmanaged Cloud SSD VPS
44,Cloud VPS Servers
44,Linux Shared Hosting
44,Windows Shared Hosting
44,Hosted Cloud Email
44,Google Workspace
44,Microsoft 365
44,CDP Backup
44,Acronis Cloud Backup
44,Zoho Mail
44,Professional Email
44,SSL-Zertifikate
44,Email-Services
44,Website Backup
44,Website Security
44,VPN
44,SEO Tools
44,Website-Builder
44,Site Builder
44,XOVI NOW
44,Site & Server Monitoring
44,VPN
44,Domain registrieren
44,Domain transferieren
44,Ankündigungen
44,Wissensdatenbank
44,Netzwerkstatus
44,Partner
44,Kontaktieren Sie uns
44,Mehr
44,Konto
44,Einloggen
44,Passwort vergessen?
44,Support
44,Wissensdatenbank
44,Dedicated Servers/VPS
44,Database
44,MySQL Performance Tuning and Monitoring Scripts
44,Kategorien
44,Accounts & Billing
44,Dedicated Servers/VPS
44,DNS Service
44,Domain Name
44,Mail Service
44,Search Engine Optimisation
44,Web Hosting
44,Web Hosting Glossary
44,Kategorien
44,Accounts & Billing
44,(1)
44,Dedicated Servers/VPS
44,(36)
44,DNS Service
44,(4)
44,Domain Name
44,(1)
44,Mail Service
44,(46)
44,Search Engine Optimisation
44,(6)
44,Web Hosting
44,(22)
44,Web Hosting Glossary
44,(1)
44,Tag Cloud
44,.htaccess
44,.pst
44,2F-Authentication
44,anacron
44,anacrontab
44,and run-parts all have excellent information and descriptions of
44,Apple Mail
44,attack
44,backup
44,basic
44,bcc
44,because it does not exist or you do not have permission.
44,bind
44,bitcoin
44,book
44,booking
44,centos8
44,CLI
44,command
44,command line interface
44,commands
44,common terms
44,compression
44,copy remote
44,credit card
44,cron
44,crontab
44,csr
44,ddos
44,debit card
44,dedicated server
44,dns server
44,dns settings
44,domain
44,domain name
44,easy
44,egopay
44,email backup
44,email client
44,forwarding rule
44,G apps
44,generator
44,git
44,github
44,glossary
44,google
44,Google apps
44,google workspace
44,groups
44,gzip
44,hosted email
44,hosted mail
44,hotmail
44,iis
44,iis 8
44,iis10
44,iis7
44,iis8
44,IMAP Folders
44,imap4
44,Incoming Filtering service
44,InnoDB
44,ip block
44,ip filter
44,java
44,keystore
44,Latest
44,Linux
44,live
44,login 'sa'
44,mac
44,mail client
44,maria db
44,MariaDB
44,microsoft 365
44,Microsoft Office Outlook 2010
44,Microsoft Outlook Express
44,Microsoft SQL Server instance
44,Mozilla Thunderbird
44,msn
44,MX records
44,MX Records Settings
44,my.cnf
44,MyISAM
44,myself
44,mysql
44,nano
44,nginx
44,NodeJS
44,NPM
44,NPX
44,okpay
44,Optimization
44,order
44,outlook
44,OX App Suite
44,passkeys
44,passwords
44,payment methods
44,paymentwall
44,paypal
44,payza
44,Percona
44,perfect money
44,performance
44,phpMyAdmin
44,plesk
44,plesk control panel
44,pop3
44,qmail
44,register
44,relay mail
44,reset a password for the 'sa' user
44,restore
44,reverse proxy
44,run-parts
44,sa password
44,scheduled task
44,scp
44,screen
44,Secure copy
44,Secure File Transfer
44,security
44,security'
44,sending email
44,SEO
44,server
44,setting
44,settings
44,SFTP
44,shared mailbox
44,slave
44,SMTP
44,spam
44,spam filter
44,SPF Records settings
44,ssl
44,step by step
44,terminalogy
44,text editor
44,Thunderbird
44,tls
44,tomcat
44,two factor
44,web hosting
44,webmail
44,windows live mail
44,windows server
44,Support
44,Supporttickets
44,Ankündigungen
44,Wissensdatenbank
44,Downloads
44,Netzwerkstatus
44,Ticket öffnen
44,MySQL Performance Tuning and Monitoring Scripts
44,Drucken
44,"MySQL, linux, my.cnf, Percona, performance, MariaDB, InnoDB, Optimization, MyISAM, phpMyAdmin"
44,MySQL Performance Tuning and Monitoring Scripts
44,Performance Tuning Tools
44,MySQLTuner: a script written in Perl that will assist you with your MySQL configuration and make recommendations for increased performance and stability.wget http://mysqltuner.pl/ -O mysqltuner.plperl mysqltuner.pl Advanced Usage :Minimal usage remotelyperl mysqltuner.pl --host targetDNS_IP --user admin_user --pass admin_passwordUsage: Enable maximum output information around MySQL/MariaDb without debuggingperl mysqltuner.pl --verboseperl mysqltuner.pl --buffers --dbstat --idxstat --sysstat --pfstatUsage: Enable CVE vulnerabilities check for your MariaDB or MySQL versionperl mysqltuner.pl --cvefile=vulnerabilities.csvUsage: Write your result in a file with information displayedperl mysqltuner.pl --outputfile /tmp/result_mysqltuner.txtUsage: Write your result in a file without outputting informationperl mysqltuner.pl --silent --outputfile /tmp/result_mysqltuner.txtUsage: Using template model to customize your reporting file based on Text::Template syntax.perl mysqltuner.pl --silent --reportfile /tmp/result_mysqltuner.txt --template=/tmp/mymodel.tmplUsage: Enable debugging informationperl mysqltuner.pl --debug
44,"MySQL Tuning Primer Script - tuning-primer.sh - This script takes information from ""SHOW STATUS LIKE..."" and ""SHOW VARIABLES LIKE..."" then attempts to produce some recommendations for tuning server variables."
44,Currently it handles recomendations for the following:
44,Slow Query Log
44,Innodb Status
44,Max Connections
44,Worker Threads
44,Key Buffer
44,Query Cache
44,Sort Buffer
44,Joins
44,Temp Tables
44,Table (Open & Definition) Cache
44,Table Locking
44,Table Scans (read_buffer)
44,mysqlreport: makes a friendly report of important MySQL status values. mysqlreport transforms the values from SHOW STATUS into an easy-to-read report that provides an in-depth understanding of how well MySQL is running. mysqlreport is a better alternative (and practically the only alternative) to manually interpreting SHOW STATUS.
44,The Guide To Understanding mysqlreport
44,mysqlreport Documentation
44,"phpMyAdmin Advisor : This tool is very similar to the tuning-primer tool. Nice and fast, and likely the most up-to-date tool."
44,"Percona Tools for MySQL : Free online productivity tools for MySQL DBAs, SysAdmins and Developers"
44,Performance Monitoring Tools
44,mytop - a top clone for MySQL.
44,innotop: you can download the latest version of innotop from Sourceforge.
44,phpMyAdmin Monitor :
44,War diese Antwort hilfreich?
44,Nein
44,Verwandte Artikel
44,How to Back Up MySQL Databases From The Command Line
44,MySQL Databases From The Command Line
44,"While automated backups are important, sometimes you just..."
44,« Zurück
44,Tag Cloud
44,.htaccess
44,.pst
44,2F-Authentication
44,anacron
44,anacrontab
44,and run-parts all have excellent information and descriptions of
44,Apple Mail
44,attack
44,backup
44,basic
44,bcc
44,because it does not exist or you do not have permission.
44,bind
44,bitcoin
44,book
44,booking
44,centos8
44,CLI
44,command
44,command line interface
44,commands
44,common terms
44,compression
44,copy remote
44,credit card
44,cron
44,crontab
44,csr
44,ddos
44,debit card
44,dedicated server
44,dns server
44,dns settings
44,domain
44,domain name
44,easy
44,egopay
44,email backup
44,email client
44,forwarding rule
44,G apps
44,generator
44,git
44,github
44,glossary
44,google
44,Google apps
44,google workspace
44,groups
44,gzip
44,hosted email
44,hosted mail
44,hotmail
44,iis
44,iis 8
44,iis10
44,iis7
44,iis8
44,IMAP Folders
44,imap4
44,Incoming Filtering service
44,InnoDB
44,ip block
44,ip filter
44,java
44,keystore
44,Latest
44,Linux
44,live
44,login 'sa'
44,mac
44,mail client
44,maria db
44,MariaDB
44,microsoft 365
44,Microsoft Office Outlook 2010
44,Microsoft Outlook Express
44,Microsoft SQL Server instance
44,Mozilla Thunderbird
44,msn
44,MX records
44,MX Records Settings
44,my.cnf
44,MyISAM
44,myself
44,mysql
44,nano
44,nginx
44,NodeJS
44,NPM
44,NPX
44,okpay
44,Optimization
44,order
44,outlook
44,OX App Suite
44,passkeys
44,passwords
44,payment methods
44,paymentwall
44,paypal
44,payza
44,Percona
44,perfect money
44,performance
44,phpMyAdmin
44,plesk
44,plesk control panel
44,pop3
44,qmail
44,register
44,relay mail
44,reset a password for the 'sa' user
44,restore
44,reverse proxy
44,run-parts
44,sa password
44,scheduled task
44,scp
44,screen
44,Secure copy
44,Secure File Transfer
44,security
44,security'
44,sending email
44,SEO
44,server
44,setting
44,settings
44,SFTP
44,shared mailbox
44,slave
44,SMTP
44,spam
44,spam filter
44,SPF Records settings
44,ssl
44,step by step
44,terminalogy
44,text editor
44,Thunderbird
44,tls
44,tomcat
44,two factor
44,web hosting
44,webmail
44,windows live mail
44,windows server
44,Support
44,Supporttickets
44,Ankündigungen
44,Wissensdatenbank
44,Downloads
44,Netzwerkstatus
44,Ticket öffnen
44,Main Site  |   About Us   |
44,FAQ   |   Partners   |
44,Blog   |   Career   |
44,Contact Us   |   User Policy   |
44,Disclaimer   |  Spam Policy  |
44,Privacy Policy   |  Sitemap
44,Deutsch
44,USD
44,Kontaktieren Sie uns
44,AGB
44,Copyright © 2024 Systron Micronix Corporation. All Rights Reserved.
44,Close
44,Lädt...
44,Lädt...
44,Close
44,Submit
44,Sprachwahl
44,العربية
44,Azerbaijani
44,Català
44,Hrvatski
44,Čeština
44,Dansk
44,Nederlands
44,English
44,Estonian
44,Persian
44,Français
44,Deutsch
44,עברית
44,Magyar
44,Italiano
44,Macedonian
44,Norwegian
44,Português
44,Português
44,Română
44,Русский
44,Español
44,Svenska
44,Türkçe
44,Українська
44,Währung wählen
44,$ USD
44,€ EUR
44,INR
44,£ GBP
44,Apply
44,Generate Password
44,Please enter a number between 8 and 64 for the password length
44,Password Length
44,Generated Password
44,Generate new password
44,Copy
44,Close
44,Copy to clipboard and Insert
44,<-- removed to be added again -->
47,Blogs on MySQL and PostgreSQL Database Optimization | OtterTune
47,"Product TourBlogPlans and PricingContactResourcesLoginGet StartedProduct TourBlogBlogBlogBlogSearch blog hereJan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Subscribe to blog updates.Subscribe to blog updates.ProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy Policy"
49,10 Tips for MySQL Performance Tuning - YouTubeInfoPresseUrheberrechtKontaktCreatorWerbenEntwicklerImpressumVerträge hier kündigenNutzungsbedingungenDatenschutzRichtlinien & SicherheitWie funktioniert YouTube?Neue Funktionen testen© 2024 Google LLC
51,Blogs on MySQL and PostgreSQL Database Optimization | OtterTune
51,"Product TourBlogPlans and PricingContactResourcesLoginGet StartedProduct TourBlogBlogBlogBlogSearch blog hereJan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Subscribe to blog updates.Subscribe to blog updates.ProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy Policy"
53,10 Tips for MySQL Performance Tuning - YouTubeInfoPresseUrheberrechtKontaktCreatorWerbenEntwicklerImpressumVerträge hier kündigenNutzungsbedingungenDatenschutzRichtlinien & SicherheitWie funktioniert YouTube?Neue Funktionen testen© 2024 Google LLC
54,10 Strategies to Optimize Database Performance
54,Home
54,Blog
54,About Us
54,Contact Us
54,MySQL Consultation
54,MariaDB Consultation
54,PostgreSQL Consultation
54,SQL Server Consultation
54,MongoDB Consultation
54,Database Consultation
54,Search
54,Join
54,10 Strategies to Optimize Database Performance
54,OptimizDBA Team
54,OptimizDBA Team
54,"October 17, 2023 — 6 minutes read"
54,"10 Strategies to Optimize Database PerformanceOverviewWhat is database performance optimization?Database performance optimization refers to the process of improving the efficiency and speed of a database system. It involves various strategies and techniques to enhance the performance of database operations, such as query optimization, indexing, and data partitioning. By optimizing database performance, organizations can ensure faster response times, improved scalability, and better utilization of system resources. This is particularly important in today's data-driven world, where businesses rely heavily on databases for storing and retrieving large amounts of data for analytics, reporting, and decision-making.Importance of optimizing database performanceOptimizing database performance is crucial for ensuring efficient and smooth operations. A well-performing database can enhance the overall performance of an application or system, while a poorly optimized database can lead to slow response times, increased downtime, and even data loss. By optimizing database performance, organizations can improve query execution, reduce latency, and minimize database differences. This can be achieved through various strategies, such as indexing, query optimization, caching, and regular maintenance tasks.Common challenges in database performance optimizationOne of the common challenges in database performance optimization is the complexity involved in managing large datasets. As databases grow in size, it becomes increasingly difficult to ensure efficient query execution and data retrieval. Another challenge is the need to balance the trade-off between data consistency and performance. Maintaining strict data consistency can impact the overall performance of the database. Additionally, indexing plays a crucial role in optimizing database performance. Properly indexed tables can significantly improve query performance by reducing the number of disk I/O operations. Implementing caching mechanisms is another strategy to enhance database performance. By storing frequently accessed data in memory, caching reduces the need for disk I/O and improves query response time. Lastly, query optimization techniques such as rewriting queries, using appropriate join algorithms, and optimizing query execution plans can greatly improve database performance.Choosing the right database engineUnderstanding different types of database enginesThere are various types of database engines available, each with its own strengths and weaknesses. SQL (Structured Query Language) is a common language used to interact with relational databases. It provides powerful querying capabilities and allows for efficient data retrieval. However, it is important to understand the opportunities with SQL and how to optimize database performance. By optimizing queries, indexing data, and implementing caching strategies, developers can improve the overall performance of their database systems.Evaluating performance factors of database enginesWhen it comes to optimizing database performance, evaluating performance factors of database engines is crucial. Indexing is one such factor that plays a significant role in enhancing database performance. By creating indexes on frequently queried columns, the database engine can quickly locate the required data, resulting in faster query execution. Additionally, indexing can also improve data retrieval efficiency and reduce disk I/O operations.Considerations for selecting the appropriate database engineWhen choosing a database engine, it is important to consider several factors. Database performance, scalability, and data integrity are some of the key considerations. Additionally, the type of data and the workload characteristics should be taken into account. It is crucial to assess the requirements of the application and choose a database engine that can handle the expected workload efficiently. Database engines like MySQL, PostgreSQL, and MongoDB offer different features and capabilities, so it is essential to evaluate them based on the specific needs of the project. Furthermore, considering factors such as data size, read and write operations, and the need for real-time analytics can help in selecting the most suitable database engine.Optimizing database schema designNormalization and denormalization techniquesNormalization and denormalization are two techniques used to optimize database performance. Normalization is the process of organizing data into tables and eliminating redundant data. It helps to minimize data redundancy, improve data integrity, and reduce data anomalies. On the other hand, denormalization is the process of combining tables to improve query performance. It involves duplicating data and adding redundancy to eliminate the need for complex joins. By carefully applying normalization and denormalization techniques, you can achieve a balance between data integrity and query performance, ultimately optimizing database performance.Indexing strategies for efficient data retrievalWhen it comes to data management, efficient data retrieval is crucial for optimal database performance. Implementing effective indexing strategies can significantly improve query performance by reducing the time it takes to locate and retrieve the desired data. There are several indexing techniques that can be employed, such as B-tree and hash indexes. Additionally, utilizing clustered indexes can further enhance data retrieval speed by physically ordering the data on disk. It is important to carefully analyze the data access patterns and query requirements to determine the most appropriate indexing strategy for your database. Regularly monitoring and maintaining the indexes is also essential to ensure their continued effectiveness.Partitioning and sharding for scalabilityPartitioning and sharding are best practices for optimizing database performance and achieving scalability. Partitioning involves dividing a large database into smaller, more manageable parts called partitions. Each partition contains a subset of the data, which allows for faster data access and query execution. Sharding, on the other hand, involves distributing the data across multiple database instances or servers. This helps distribute the workload and allows for parallel processing, leading to improved performance. By implementing partitioning and sharding, organizations can effectively handle large volumes of data, improve response times, and ensure high availability.Query optimization techniquesUnderstanding query execution plansQuery execution plans are essential for optimizing database performance. By analyzing the execution plan, you can identify areas where the query can be improved. One way to improve MariaDB performance is to optimize the indexes used by the query. This can be done by analyzing the query execution plan and identifying the columns that are frequently used in the WHERE clause or JOIN conditions. By creating appropriate indexes on these columns, the database can quickly locate the required data, resulting in faster query execution times. Additionally, optimizing the query itself by rewriting it or using appropriate query hints can also help improve performance. It is important to regularly review and analyze the query execution plans to identify potential bottlenecks and optimize the database performance.Optimizing SQL queries with proper indexingOne of the key strategies to optimize database performance is by optimizing SQL queries with proper indexing. Indexing plays a crucial role in improving query performance by allowing the database engine to quickly locate and retrieve the required data. By creating indexes on the columns frequently used in the WHERE and JOIN clauses, the database can efficiently filter and join the data, resulting in faster query execution. Additionally, monitoring SQL industry trends can help identify new techniques and best practices for query optimization. Regularly updating the database and query optimization techniques based on the latest trends can further enhance performance and keep up with the evolving demands of the industry.Using query hints and optimizer directivesQuery hints and optimizer directives are powerful tools that can be used to improve database performance. By providing additional instructions to the query optimizer, developers can guide the execution plan generation process and achieve better query performance. Query hints allow developers to specify which indexes to use, how to join tables, and other optimizations. Optimizer directives, on the other hand, provide high-level instructions to the optimizer, such as forcing a specific join algorithm or enabling parallel execution. Improving database performance through the use of query hints and optimizer directives requires a deep understanding of the database system and query execution process.Query optimization techniques are crucial for improving the performance of your database. By implementing these techniques, you can experience transaction speeds that are at least twice as fast as before. At OptimizDBA Database Optimization Consulting, we specialize in optimizing databases to achieve optimal performance. With over 20 years of experience, we have helped over 500 clients achieve significant increases in performance. Our average speeds are often 100 times, 1000 times, or even higher! If you're looking to optimize your database and improve its performance, contact us today for a consultation. Let us help you unlock the full potential of your database!"
54,Share this post
54,The link has been copied!
54,Tags
54,Datatabase Optimization Trends
54,Newer post
54,Enhancing Database Performance: Best Practices for MariaDB
54,Older post
54,Enhancing Database Performance: Best Practices for MariaDB
54,Subscribe to new posts
54,Subscribe
54,Processing your application
54,Please check your inbox and click the link to confirm your subscription.
54,There was an error sending the email
54,"OptimizDBA Database Optimization Latest & Highlighted Articles from our blog, updates, and more."
54,Social
54,Links
54,MySQL Consultation
54,MariaDB Consultation
54,PostgreSQL Consultation
54,SQL Server Consultation
54,MongoDB Consultation
54,Links
54,Terms & Conditions
54,© OptimizDBA.com 2024.
54,You’ve successfully subscribed to OptimizDBA.com
54,Welcome back! You’ve successfully signed in.
54,Great! You’ve successfully signed up.
54,Success! Your email is updated.
54,Your link has expired
54,Success! Check your email for magic link to sign-in.
55,Performance tuning | Socket.IO
55,"Skip to main contentThank you for your interest in the user study, aimed at providing better support for Socket.IO users on Azure."
55,"Read our findings from the hundreds of responses and learn about how Azure can help with scaling out Socket.IO apps easily.Socket.IODocsGuideTutorialExamplesEmit cheatsheetServer APIClient APIEcosystemHelpTroubleshootingStack OverflowGitHub DiscussionsSlackNewsBlogTwitterToolsCDNAdmin UIAboutFAQChangelogRoadmapBecome a sponsor4.x4.x3.x2.xChangelogEnglishEnglishFrançaisPortuguês (Brasil)中文（中国）SearchSocket.IODocumentationServerClientEventsAdaptersAdvancedNamespacesCustom parserAdmin UIUsage with PM2Load testingPerformance tuningMigrationsMiscellaneousAdvancedPerformance tuningVersion: 4.xOn this pagePerformance tuningHere are some tips to improve the performance of your Socket.IO server:at the Socket.IO levelat the OS levelYou might also be interested in scaling to multiple nodes.At the Socket.IO level​Since, in most cases, the Socket.IO connection will be established with WebSocket, the performance of your Socket.IO server will be strongly linked to the performance of the underlying WebSocket server (ws, by default).Install ws native add-ons​ws comes with two optional binary add-ons which improve certain operations. Prebuilt binaries are available for the most popular platforms so you don't necessarily need to have a C++ compiler installed on your machine.bufferutil: Allows to efficiently perform operations such as masking and unmasking the data payload of the WebSocket frames.utf-8-validate: Allows to efficiently check if a message contains valid UTF-8 as required by the spec.To install those packages:$ npm install --save-optional bufferutil utf-8-validatePlease note that these packages are optional, the WebSocket server will fallback to the Javascript implementation if they are not available. More information can be found here.Use another WebSocket server implementation​For example, you can use the eiows package, which is a fork of the (now deprecated) uws package:$ npm install eiowsAnd then use the wsEngine option:const { createServer } = require(""http"");const { Server } = require(""socket.io"");const httpServer = createServer();const io = new Server(httpServer, {"
55,"wsEngine: require(""eiows"").Server});Use a custom parser​If you send binary data over the Socket.IO connection, using a custom parser like the one based on msgpack might be interesting, as by default each buffer will be sent in its own WebSocket frame.Usage:Serverconst { createServer } = require(""http"");const { Server } = require(""socket.io"");const parser = require(""socket.io-msgpack-parser"");const httpServer = createServer();const io = new Server(httpServer, {"
55,"parser});Clientconst { io } = require(""socket.io-client"");const parser = require(""socket.io-msgpack-parser"");const socket = io(""https://example.com"", {"
55,"parser});Discard the initial HTTP request​By default, a reference to the first HTTP request of each session is kept in memory. This reference is needed when working with express-session for example (see here), but can be discarded to save memory:io.engine.on(""connection"", (rawSocket) => {"
55,"rawSocket.request = null;});Before:After:At the OS level​There are lots of good articles on how to tune your OS to accept a large number of connections. Please see this one or this one for example.While load testing your Socket.IO server, you will likely reach the two following limits:maximum number of open filesIf you can't go over 1000 concurrent connections (new clients are not able to connect), you have most certainly reached the maximum number of open files:$ ulimit -n1024To increase this number, create a new file /etc/security/limits.d/custom.conf with the following content (requires root privileges):* soft nofile 1048576* hard nofile 1048576And then reload your session. Your new limit should now be updated:$ ulimit -n1048576maximum number of available local portsIf you can't go over 28000 concurrent connections, you have most certainly reached the maximum number of available local ports:$ cat /proc/sys/net/ipv4/ip_local_port_range32768"
55,"60999To increase this number, create a new file /etc/sysctl.d/net.ipv4.ip_local_port_range.conf with the following content (again, requires root privileges):net.ipv4.ip_local_port_range = 10000 65535Note: we used 10000 as a lower bound so it does not include the ports that are used by the services on the machine (like 5432 for a PostgreSQL server), but you can totally use a lower value (down to 1024).Once you reboot your machine, you will now be able to happily go to 55k concurrent connections (per incoming IP).See also:https://unix.stackexchange.com/a/130798https://making.pusher.com/ephemeral-port-exhaustion-and-how-to-avoid-it/Edit this pageLast updated on Feb 20, 2024PreviousLoad testingNextMigrating from 2.x to 3.0At the Socket.IO levelInstall ws native add-onsUse another WebSocket server implementationUse a custom parserDiscard the initial HTTP requestAt the OS levelDocumentationGuideTutorialExamplesServer APIClient APIHelpTroubleshootingStack OverflowGitHub DiscussionsSlackNewsBlogTwitterToolsCDNAdmin UIAboutFAQChangelogRoadmapBecome a sponsorCopyright © 2024 Socket.IO"
56,"Ask HN: It's 2023, how do you choose between MySQL and Postgres? | Hacker News"
56,Hacker News
56,new | past | comments | ask | show | jobs | submit
56,login
56,"Ask HN: It's 2023, how do you choose between MySQL and Postgres?"
56,219 points by debo_ 10 months ago
56,| hide | past | favorite | 347 comments
56,"Imagine you are starting a new project, and your backing datastore options are restricted to mysql or postgres (and/or their cloud-tailored equivalents.) What sort of functional requirements would cause you to choose one over the other?"
56,gmac 10 months ago
56,| next [–]
56,"Postgres. Fast, full-featured, rock-solid, and a great community.I think many of us can’t be bothered to go over (again) the issues we’ve had with MySQL in the past. The last straw for me was about ten years ago, when I caught MySQL merrily making up nonsense results for a query I’d issued that accidentally didn’t make any sense.Very likely this particular issue, and others like it, have been fixed in the meantime. But I just got the sense that MySQL was developed by people who didn’t quite know what they were doing, and that people who really did know what they were doing weren’t ever likely to be attracted to that to fix it."
56,paulddraper 10 months ago
56,| parent | next [–]
56,"FWIW, it hasn't changed in ten years.Here is an 18-year-old bug, that DELETE triggers don't work for foreign key cascades: https://bugs.mysql.com/bug.php?id=11472That makes the entire feature mostly worthless. Reported in 2005, last updated in 2008.---While I would choose PostgreSQL every time, MySQL has the following advantages:1. Write-performance, due to fundamental design tradeoffs. [1]2. Per-connection resources, due to single-process design.3. Related to #1, no vacuum requirement.[1] https://www.uber.com/blog/postgres-to-mysql-migration/"
56,digitalpacman 10 months ago
56,| root | parent | next [–]
56,Good you shouldn't be using them. You shouldn't be using foreign keys either. It just makes working with data harder and doesn't help with constraining it if your data modifications are inside transactions and properly written statements.
56,kiernanmcgowan 10 months ago
56,| parent | prev | next [–]
56,"Having used postgres for the past decade, I tried MySQL for a side project to see whats changed with it. The sad answer is that it feels like nothing has changed - Oracle seems to have let what used to be a core technology of the industry languish.I'm sure there are use cases where MySQL will be the better choice over postgres, but the future for the stack looks bleak."
56,JohnBooty 10 months ago
56,| root | parent | next [–]
56,Oracle seems to have let what used to be a core
56,technology of the industry languish
56,"I think slowly squeezing the life from MySQL was a very explicit goal for them. After the big wins (Wal-Mart, etc) MySQL had 15-20 years ago I think it was very clear MySQL was going to chip away at more and more of Oracle's business.I wonder how much Oracle spends on MySQL every year? They're spending a lot of money to keep MySQL at kind of a ""not quite good enough"" state. But they can't kill it outright - it'd be like boiling a frog fast instead of slow.In the end, I wonder what extinguishing MySQL really accomplished for them. It might have bought them some breathing room but Postgres quickly filled MySQL's old segment."
56,srcreigh 10 months ago
56,| root | parent | next [–]
56,"Strange take when FB, twitter, Square and new startups such as Faire(#4 valued private YC co) are all using MySQL to some/large degree. Stripe uses MySQL too in combination with other DBs including Postgres."
56,JohnBooty 10 months ago
56,| root | parent | next [–]
56,"You're not considering the timeline of events.I'm unfamiliar with Faire but the rest were already using MySQL at the time of Oracle's acquisition in 2010. Switching backends would have been rough for those companies and this was... 2010, meaning Postgres was not nearly as performant or full featured as it is today. As mentioned in other comments, FB's investment in customizing MySQL has been extensive. They've poured a lot into their own fork of it.More to the point, look at MySQL's progress since 2010. Do you think it has been largely stifled since then, or do you think it has kept pace with Postgres? It's been largely stagnant.I'd love to hear your alternative theory, of course. You think Oracle bought MySQL to... what, exactly? Make it amazing?"
56,connormcd 10 months ago
56,| root | parent | next [–]
56,"(Full disclosure, I work for Oracle)Anyone who says no investment has been into MySQL I suspect never took the time to read the features/release notes for MySQL 8https://dev.mysql.com/doc/refman/8.0/en/mysql-nutshell.html"
56,tapoxi 10 months ago
56,| root | parent | next [–]
56,Didn't 8 ship 5 years ago?
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Their release model changed with MySQL 8 -- they do rolling point releases every quarter with new features sprinkled in as they're ready. Quite a few new major features have been released that way, including INSTANT alters, parallel index builds, CLONE plugin, major changes to how UNDO logs are sized... it's more like Windows 10's release model.Very recently they've mentioned they'll be changing this again to have separate LTS releases, which is a positive change stability-wise."
56,ksec 10 months ago
56,| root | parent | next [–]
56,Really wish they bump up the version number or something. It makes discussions with MySQL a lot easier.
56,JohnBooty 10 months ago
56,| root | parent | prev | next [–]
56,"Well, you got me there. It stagnated for so long relative to others that I didn't realize the pace had picked up in a big way."
56,srcreigh 10 months ago
56,| root | parent | prev | next [–]
56,"Imo mysql is decent enough for many years, and PG has relatively major outstanding architecture problem (see my Other comment in this thread)I don’t think either DB is bad, but the whole MySQL is dead and needs serious work idea doesn’t make sense to me. What problem does MySQL still have that needs fixing in your opinion?"
56,evanelias 10 months ago
56,| root | parent | prev | next [–]
56,"Facebook maintains a patch-set, not a full fork. They still track Oracle's releases and apply their patches on top.Facebook definitely has the resources to migrate to Postgres, if there was any motivating reason to do so. Indeed, Facebook developed their own LSM-based key-value store (RocksDB) and MySQL storage engine based on it (MyRocks) and then migrated the majority of their db fleet to that. In total that's massively more work than migrating to Postgres would have been.Part of the reason was that MyRocks provides better compression than pretty much any other possible choice, and that adds up to an astronomical amount of cost savings at Facebook's scale. In contrast, what would Facebook have gained from moving to Postgres instead?"
56,callalex 10 months ago
56,| root | parent | prev | next [–]
56,You are looking for long term planning where there is none. The only relevant question for those in charge of the project is: “How do I make my quarterly report to investors look slightly better than last quarter’s?”
56,dunno7456 10 months ago
56,| root | parent | prev | next [–]
56,Creating a series of connections very quickly is cheaper in MySQL and MariaDB than in PostgreSQL.
56,"Typically, a connection poller is used before PostgreSQL to support connection scalability.I'm not sure if there has been a recent breakthrough that has changed that."
56,I think that still applies today. Correct me if I'm wrong.
56,reactordev 10 months ago
56,| root | parent | next [–]
56,"You can create a series of connections in postgres just as fast. The connection pooler you are referring to is when you put pgBounce or pgPool in between your pgdb and your client software to expand beyond the physical limits of connections and optimize clustered architectures. MySQL at scale is replication only. A few commercial offerings for MySQL like planetscale have brought MySQL into the 21st century. Postgres has a couple ways of clustering, sharding, scaling, beyond your Wordpress database."
56,OJFord 10 months ago
56,| root | parent | prev | next [–]
56,"It took me until here to realise we were talking about MySQL, not SQLite, because honestly 'in 2023' isn't that the comparison, pg vs sqlite?"
56,gymbeaux 10 months ago
56,| root | parent | prev | next [–]
56,"Actually I would argue that there isn’t a single reason to use MySQL over Postgres (barring the obvious- it’s what the team already knows, or the company already uses, etc.)"
56,davidgerard 10 months ago
56,| root | parent | prev | next [–]
56,"> I'm sure there are use cases where MySQL will be the better choice over postgres, but the future for the stack looks bleak.see, I'm pretty sure there basically weren't. It lucked out at the right moment in the late 1990s. Also, Slashdot used it.The only use case I can think of is when you want an application, and it requires or is highly optimised to MySQL. Otherwise, it should actively be avoided."
56,johnny22 10 months ago
56,| root | parent | prev | next [–]
56,i think one is also referring to mariadb here and not just mysql.
56,"Maybe that's better enough? I wouldn't know, I just go with postres."
56,stouset 10 months ago
56,| parent | prev | next [–]
56,"Yep. The real question here is: it's 2023, why would you choose MySQL over PostgreSQL?Not that there aren't reasons. There are some. But for starting out with a new app without a very, very good reason to do something different? PostgreSQL every day of the week."
56,mardifoufs 10 months ago
56,| root | parent | next [–]
56,Ease of updates is a very good reason. Handling connections too.
56,jesterson 10 months ago
56,| parent | prev | next [–]
56,"Working with MySQL (MariaDB, but doesn't make much difference). Never get any issues that couldn't be explained by architectural or development mistakes.Just as example - how do you create read-only user (SELECT only) in Postgres? In MySQL it's extremely simple and it works, while in Postgres it's a nightmare to create and maintain"
56,Someone 10 months ago
56,| root | parent | next [–]
56,"> Just as example - how do you create read-only user (SELECT only) in Postgres? In MySQL it's extremely simple and it works, while in Postgres it's a nightmare to createIsn’t that"
56,GRANT SELECT ON ALL TABLES IN SCHEMA foo TO bar;
56,?> and maintainIf you mean you want to grant a user select rights to whatever table gets created in the future (a somewhat questionable idea from a security viewpoint):
56,ALTER DEFAULT PRIVILEGES IN SCHEMA foo GRANT SELECT ON TABLES TO bar;
56,"I think both are possible in PostgreSQL 9 and later (https://www.postgresql.org/docs/9.0/sql-grant.html , https://www.postgresql.org/docs/9.0/sql-alterdefaultprivileg...)That version is from 2010.I guess the lesson is that both these systems evolve fairly rapidly. You can’t use quirks you remember from over 5 years ago to judge their current versions."
56,jesterson 10 months ago
56,| root | parent | next [–]
56,It is correct but apparently not sufficient - you need to give CONNECT access to user profile.
56,2muchcoffeeman 10 months ago
56,| parent | prev | next [–]
56,"It’s really unfair because a lot may have changed in 10 years so it might be worth reconsidering.But I’m like you, MySQL did some nonsense once that took me hours to work out. So now I really can’t be bothered with any potential quirks it may still have. This is not an SNL sketch."
56,donatj 10 months ago
56,| prev | next [–]
56,"Unpopular opinion on HN apparently, but MySQL- It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.- Maintenance is simpler and far less necessary in my general experience.- Replication is simpler and more reliable.- You can tell the query optimizer what to do. When this is needed, you'll be thankful. It's a godsend.That said, I wouldn't run Oracle MySQL. I opt for MariaDB on smaller projects and AWS Aurora MySQL for larger projects. Aurora scales insanely well, and replication lag is almost non-existent.In my general experience MySQL was always significantly faster but it's been a number of years since I've worked with Postgres and the comments here seem to indicate that that may no longer be the case. YMMV"
56,stouset 10 months ago
56,| parent | next [–]
56,"> It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.This doesn't really hold water in my opinion.It's not like PostgreSQL is some minefield of misfeatures and quirky behavior. Some of these features exist, but have zero impact on you unless you actually opt to use them. But if you end up needing to: they're there, and you can just start using them.Compare this to MySQL where they simply don't exist no matter how much you may need them. Need to apply an index to the result of a function to quickly fix a performance issue in prod? Sorry, you can't. Need window functions to accurately compute some analytics in a sane period of time? Sorry, you can't. The list of things you can do in PostgreSQL that you simply can't with MySQL is massive and grows every day.The odds that you'll want, need, or greatly benefit at least one of these features is not small. Having the flexibility of knowing these features exist should you ever have a use-case for them is massive."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Your examples regarding MySQL's features are not correct.Need to apply an index to the result of a function? No problem, use a functional index, supported since October 2018: https://dev.mysql.com/doc/refman/8.0/en/create-index.html#cr...Need to use a window function? No problem, supported since April 2018: https://dev.mysql.com/doc/refman/8.0/en/window-functions.htm..."
56,stouset 10 months ago
56,| root | parent | next [–]
56,Fair. I picked two instances that had long been personally painful with MySQL and didn’t exist when I finally fled to greener pastures.There are far more examples than just those two though. :)
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Sure, and there are also plenty of examples of things MySQL can do that Postgres can't.There's no silver bullets in software or databases, no ""one size fits all"". Just various trade-offs between different design decisions."
56,0xf8 10 months ago
56,| root | parent | next [–]
56,"I mean, for most mature ecosystems / technology stacks I think the “no one size fits all, just strategic design trade-offs” theory largely holds true as an"
56,objectively accurate account of the “choosing the best tool“ situation
56,"….but IMO, I think it’s worth noting that occasionally, some rare unique and functionally superlative technology comes along that in practice transcends every alternative from the onset and indefinitely going forwards, sometimes even at a more prominent scale than the MySQL / Postgres projects topic of discussion (which are not small by any means).something maybe like Git, most immediately comes to mind, as an example of the de-facto standard for distributed VCS basically since … 2005* when Linus decided to create it?edit: not 1995"
56,erhaetherth 10 months ago
56,| root | parent | next [–]
56,"> something maybe like Git, most immediately comes to mind, as an example of the de-facto standard for distributed VCS basically since … 1995 when Linus decided to create it?hot take. might be the most popular, maybe even by a large margin, but I think you'll find a good chunk of people who have actually tried different VCSs don't think it's the best."
56,0xf8 10 months ago
56,| root | parent | next [–]
56,"Yah, I think that’s probably true. But that’s also hard to reconcile with the reality of the adoption trending consistently away from any alternative and only towards Git. And “large margins” are indeed pretty objectively the case (from the largest developer surveys the breakdown 10 years ago was like 70% Git to everything, growing to ~95% in 2022). Usually the phenomenon you’re describing, leads to other alternatives becoming more popular not less (even if the most popular standard continues to eclipse the field. Here is would seem these highly likable alternatives for those who took the plunge are nevertheless dwindling into irrelevance…I suspect the die-hard proponents of Mercurial, or SVN, or whatever else, these few pagan heretics that might exist out there wherever they’re hiding, have found themselves in a camp different to the Git standard likely on the basis of electing to be intentionally contrarian / anti-normative as the general catalyst, and rather not, as a function of struggling with Git to the point of being so disillusioned they call it quits and head out looking for greener pastures."
56,"I think in practice the most common result of encountering problems with Git is, fix the problems. And functionally I think that’s resulted only in furthering it’s supremacy over alternatives, despite there existing a handful of cultish weirdos who are _really_ into Mercurial and prefer not to fux with Git as a personal lifestyle choice haha)"
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Giant companies like Google and Meta use alternatives to Git for their internal / closed-source code, because Git couldn't scale to their needs. If I recall correctly, Google transitioned from Perforce to an in-house system, and Meta transitioned from using Mercurial to a custom derivative of it.Again, no one size fits all..."
56,thfuran 10 months ago
56,| root | parent | prev | next [–]
56,Are you serious? Anyone who disagrees with you about what VCS is best must be intentionally contrarian?
56,0xf8 10 months ago
56,| root | parent | next [–]
56,"Haha. That’s a pretty aggressive and surface level read of my comment . For starters it was mostly a joke, and explicitly a conjecture. I was saying, probably most users of a different system do so on for some reason other than, Git being objectively inferior for their purposes (and as others in the thread pointed out, the latter actual is the case at the large scale end of the spectrum for big companies concerned with scaling, not choosing Git).Secondly, no one has disagreed with me on the matter, as I haven’t put forth a personal opinion, I’m simply impartially making referencing to the fact that Git is the de-facto standard. And from an intellectual perspective, was hoping someone might elucidate more into why that is the case, given my conception is an inferential deduction. at best).While I’m personally relatively familiar with Git internals, by no means an expert, its the only distributed VCS I’ve ever used and I don’t know anything substantive enough about the alternatives to credibly make a relative value comparison here. They could be the bees knees for all I know, but it seems unlikely given the position Git holds as far as consensus standard choice."
56,irrational 10 months ago
56,| root | parent | prev | next [–]
56,"What are the “plenty of examples of things MySQL can do that Postgres can’t”? I honestly can’t think of one, much less “plenty”."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"These are just a few random ones off the top of my head!* Handling several thousand connections per second without needing a proxy or pool (MySQL's connection model is thread/conn vs Postgres using process/conn)* Handle extremely high volume of primary key range scan queries (MySQL's InnoDB uses a clustered index, see https://news.ycombinator.com/item?id=35909053)* Handle workloads that lean heavily on UPDATE or DELETE, without major MVCC pain (see https://ottertune.com/blog/the-part-of-postgresql-we-hate-th... for example)* Semi-synchronous logical replication, for environments which cannot tolerate any data loss during failover; this ensures the statements have reached at least one replica but without the huge latency of synchronous replication* Use index hints, to ensure random index stats changes don't cause unexpectedly negative query plan adjustments (see discussion in subthread https://news.ycombinator.com/item?id=35909340)* Handle very high-volume OLTP workloads using direct I/O, since InnoDB's buffer pool design is completely independent of filesystem/OS caching* Achieve best-in-industry compression by using the MyRocks storage engine (MySQL's pluggable storage engine design has a lot of tradeoffs but it is inherently what makes this even possible)* Use UNSIGNED int types, to store twice as high max value in the same number of bytes, if you know negative numbers are not going to be present* Use case-insensitive or accent-insensitive collations out-of-the-box without having to monkey with confusing user-defined CREATE COLLATION* Ease-of-use commands like SHOW CREATE TABLE* Silly cosmetic things like the ability to reorder columns in an ALTER TABLE (see https://wiki.postgresql.org/wiki/Alter_column_position)* A tooling ecosystem which includes multiple battle-tested external online schema change tools, for safely making alterations of any type to tables with billions of rows"
56,bigblackrooster 10 months ago
56,| root | parent | next [–]
56,ratio
56,gymbeaux 10 months ago
56,| root | parent | prev | next [–]
56,"Yeah I can’t think of any. I mean if I had to come up with something, I would talk about tooling. MySQL Workbench (when it works) is by far the easiest to use and most feature-rich of all RDBMS.. MS. It runs on any OS too, which is a ding on SQL Server and not Postgres since Postgres’ is a web app- which has the limitations of a web app (no right click for example- which I know is possible in a web app, but last I checked not implemented for pgadmin)."
56,EamonnMR 10 months ago
56,| parent | prev | next [–]
56,I would disagree on maintenance being simpler. I have never had Postgres randomly munge a table and require me to run a command to fix it.
56,jjeaff 10 months ago
56,| root | parent | next [–]
56,"I have not had that happen in MySQL either, at least, not with innodb. what command would that be?I do remember getting bad tables with myisam tables a decade ago, sometimes after a bad shutdown."
56,EamonnMR 10 months ago
56,| root | parent | next [–]
56,I believe this was the command in question: https://dev.mysql.com/doc/refman/8.0/en/repair-table.html
56,michaelcampbell 10 months ago
56,| root | parent | prev | next [–]
56,"My needs are meager (simple CRUD, low volume), but I haven't had that happen in MySQL either, in over 15 years of running it in production.Not saying it can't happen, but I don't think it's a common occurrence."
56,EamonnMR 10 months ago
56,| root | parent | next [–]
56,WordPress may be uniquely bad.
56,rocho 10 months ago
56,| parent | prev | next [–]
56,"I had the misfortune of inheriting a MySQL 5.6 database and I had to manage it for a year and a half. It wasn't very big (less than 100GB), but:- for some reason it hang periodically and had to be restarted during the night for no apparent reason- compared to Postgres, the tooling is garbage (both for backups and even more for general database administration)- essential features are missing, the most important one of which, for me, was proper CSV import/export. CSV-related functionality is so broken and terribly inconvenient to use. In a specific case I had to write a program to export millions of records manually since MySQL could not generate correct CSV export due to some columns containing text with special characters, quotes, newlines. Any combination of the export parameters (""ENCLOSED BY"", ""ESCAPED BY"" and all the other garbage options) failed in one way or another. I even tried to use non-standard characters like \x15 and \r for column and row separation but even that failed. With Postgres, ""with csv header"" is simple and works every time.I also managed bigger Postgres databases (up to tens of terabytes) and never had the issues I encountered with MySQL."
56,drogus 10 months ago
56,| parent | prev | next [–]
56,"> It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.Imagine when you actually need any of the features that PostgreSQL provides like pub/sub, logical replication, JSONB etc. With MySQL you might have to hack a solution that is much more complex or you have to set up an entirely separate tool. What I find nice with PostgreSQL is that for simple cases you can get away without a dedicated key/value store or a queue or a full text search engine. You can do a lot of these kinds of tasks with just a single database."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"> features that PostgreSQL provides like [...] logical replicationMySQL has offered logical replication for considerably longer than Postgres, and it's a substantially more configurable and mature implementation. MySQL's built-in replication has always been logical replication. It's a feature MySQL has had for 23 years -- built-in logical replication is literally one of the top reasons why all the biggest MySQL users chose MySQL originally!"
56,PedroBatista 10 months ago
56,| root | parent | prev | next [–]
56,"Correct, but as far as I know MySQL has those mentioned features but the overall management effort seems to be considerably lower."
56,tonymet 10 months ago
56,| parent | prev | next [–]
56,"less features, simpler admin, more compatibility, more familiarity."
56,I agree
56,droobles 10 months ago
56,| parent | prev | next [–]
56,samesies
56,pawelduda 10 months ago
56,| parent | prev | next [–]
56,"What's the ratio of solving DB perf issues by optimizing it and letting the planner do its work, to telling it what to do? For me it's like 1000:1.And that one case I remember was perfectly solvable the regular way, with a little more time."
56,viraptor 10 months ago
56,| root | parent | next [–]
56,"The problem is that when you you need to tell the planner what to do, you can do it in MySQL, but not postgres. Imagine you've got a production database with lots of traffic which suddenly can't handle anything because it inserted an extra row which tipped the balance and now takes seconds to process a common query.Do you know how to fix the table statistics quickly? Do you know how to change that query to force the execution plan you want? Do you know how long the solution will last until the stats change again?MySQL is a bit more predictable for this case and if things go really bad for some unexpected reason, one comment can fix it.I'm looking at it from ops perspective. The ratio during development doesn't matter that much - all issues are solvable at that stage. For me it's rather ""which situation would I rather be in at 3am""."
56,pawelduda 10 months ago
56,| root | parent | next [–]
56,"I googled a bit and had no idea how many questions about Postgres query planner going nuts are out there. I just imagined this is a problem that creeps over time (giving you time to notice and act in advance, assuming you have monitoring/alerts set up) rather than suddenly tipping the scale - though it probably can happen suddenly after large data import.Personally never ran into this with Postgres nor had anyone I know worry about it - the query planner was reliable for me in 99.99% of cases but yeah, I admit that it's a black box for me that I expect to take care of internals - hopefully it continues to do so, but I got to give it to MySQL for allowing to override it then."
56,ants_a 10 months ago
56,| root | parent | prev | next [–]
56,"There is the pg_hint_plan extension that gives most of what you would want in a hinting system.I think a better fix lies in the direction of making queries more of a first class object with options to nail down plans, add custom logic to pick plans dependent on parameter values, etc."
56,avinassh 10 months ago
56,| root | parent | next [–]
56,"I am on GCP/AWS, its not possible to use extensions. They allow only a few whitelisted extensions. If it is built into the DB, then I can use without any hassles."
56,jitl 10 months ago
56,| root | parent | prev | next [–]
56,"Postgres query planner suddenly deciding to do something silly in the middle of the night has taken Notion down a few times. It's quite annoying, and it's very frustrating to have no recourse."
56,jhas78asd 10 months ago
56,| root | parent | next [–]
56,Any posts on this? Are there bulk data loads that make table stats more stale and affect plans? I’m wondering what would suddenly make a plan selection change a lot that might be a contributing factor.
56,natmaka 10 months ago
56,| root | parent | next [–]
56,"> bulk data loads that make table stats more staleThis is the usual culprit (cure: ""ANALYZE ((tablename))"").Collecting more samples (ALTER TABLE SET STATISTICS...) may be useful.""Extended Statistics"" covers most(?) other cases:"
56,https://www.postgresql.org/docs/current/planner-stats.html#P...
56,https://www.postgresql.org/docs/current/multivariate-statist...
56,pawelduda 10 months ago
56,| root | parent | prev | next [–]
56,Interesting - how do you approach it when it happens and you're under time pressure to bring it back online - assuming you can't just fix query plan? I'd probably start by tweaking stats options and resetting them for problematic tables but don't have further ideas from the top of my head.
56,dijit 10 months ago
56,| prev | next [–]
56,"There is almost no good reason to choose MySQL over PostgreSQL for any operational reason, I did a deep dive many moons ago (before major improvements in performance to postgres) and people were saying that MySQL was faster. I found that not to be true and the differences have only gained even more favour towards postgres.also, I assume you mean MariaDB as MySQL is owned by Oracle and I would greatly implore anyone and everyone to avoid Oracle as if it has herpes.There are a lot of historic problems with MySQL accepting invalid data, committing data even when there are constraint issues, and having very poor transactional isolation, I am not sure if these have improved.Truthfully, the only benefits you gain from using MariaDB or MySQL are:* Memory tables* Having inconsistent replicas (which can be useful when you want your downstream to have less data than your upstream and you know it won’t get updated.)"
56,0xbadcafebee 10 months ago
56,| parent | next [–]
56,"> avoid Oracle as if it has herpesherpes isn't that bad. most people will get it in their lifetime. 1 in 6 people have hsv-2, the less common variant. trying to avoid herpes is like trying to avoid chickenpox (although herpes isn't nearly as harmful as chickenpox).you should avoid Oracle like it's a blood pathogen."
56,hamilyon2 10 months ago
56,| root | parent | next [–]
56,"As a person who has herpes firmly in his nerves, I would say don't underestimate herpes."
56,mxvanzant 10 months ago
56,| root | parent | next [–]
56,I read this guy's book and he has some good ideas (and references to back them up). You don't need to get his book as his website also has all of that material+. http://doctoryourself.com/herpes.htmlAnother related site: http://orthomolecular.org/resources/omns/index.shtml (scroll down for articles).None of the above will help though in deciding between Postgres or MySQL ;)
56,scotty79 10 months ago
56,| root | parent | prev | next [–]
56,Suddenly the analogy got very accurate.
56,soperj 10 months ago
56,| root | parent | prev | next [–]
56,Chickenpox is actually caused by a herpesvirus.
56,herpes varicella zoster.
56,unethical_ban 10 months ago
56,| root | parent | prev | next [–]
56,"hello, fellow person with herpes! (I assume)The worst part about having it is having to talk about having it. It's really not bad as a condition separate from societal concern."
56,stavros 10 months ago
56,| root | parent | next [–]
56,I find similar societal concern when I tell friends I use Oracle.
56,r2_pilot 10 months ago
56,| root | parent | next [–]
56,"Having Oracle experience on the resume is a positive, I suppose, but I'm not sure it's been worth the exposure."
56,enneff 10 months ago
56,| root | parent | prev | next [–]
56,It’s not so bad for most people but if you’re one of the unfortunate few who suffer chronic symptoms it can be truly awful. Not worth playing that lottery if you can avoid it.
56,za3faran 10 months ago
56,| root | parent | prev | next [–]
56,> most people will get it in their lifetimeCitation needed.
56,yakshaving_jgt 10 months ago
56,| root | parent | next [–]
56,Here you go.https://www.who.int/news/item/28-10-2015-globally-an-estimat...
56,za3faran 10 months ago
56,| root | parent | next [–]
56,"Probably in certain countries, given that it is mainly an STD. There are many conservative nations where this won't be an issue hopefully."
56,yakshaving_jgt 10 months ago
56,| root | parent | next [–]
56,"As it says in the linked article, it’s a global epidemic.If you are an adult of typical sexual activity, it is likely you have already had sex with someone infected with herpes.That doesn’t mean you have contracted it — carriers aren’t always shedding the virus.I’m not sure I see any correlation between a country being conservative and an absence of sexually transmitted infection; the 10 countries where HIV is most prevalent are all (as far as I’m aware) relatively conservative.Furthermore, here are some statistics from Wikipedia on HSV which may be referring to some of these conservative countries you’re referring to:> Turkey— High levels of HSV-1 (97%) and HSV-2 (42%) were found amongst pregnant women in the city of Erzurum in Eastern Anatolia Region, Turkey. In Istanbul however, lower HSV-2 seroprevalence was observed; HSV-2 antibodies were found in 4.8% of sexually active adults, while HSV-1 antibodies were found in 85.3%. Only 5% of pregnant women were infected with HSV-2, and 98% were infected with HSV-1. Prevalence of these viruses was higher in sex workers of Istanbul, reaching levels of 99% and 60% for HSV-1 and HSV-2 prevalence respectively.> Jordan— The prevalence of HSV-2 in Jordan is 52.8% for men and 41.5% for women.> Israel— HSV-1 seroprevalence is 59.8% in the population of Israel and increases with age in both genders but the adolescent seroprevalence has been declining as in most industrialized nations. An estimated 9.2% of Israeli adults are infected with HSV-2. Infection of either HSV-1 or HSV-2 is higher in females; HSV-2 seroprevalence reaches 20.5% in females in their 40s. These values are similar to levels in HSV infection in Europe.> Antibodies for HSV-1 or HSV-2 are also more likely to be found individuals born outside of Israel, and individuals residing in Jerusalem and Southern Israel; people of Jewish origin living in Israel are less likely to possess antibodies against herpes. Among pregnant women in Israel a small scale cross sectional study found the prevalence of HSV-2 infection was 13.3% and that of HSV-1 was 94.9%. The HSV-2 infection rate was 3-fold higher among immigrants from the former Soviet Union (27.5%) than among Israeli-born Jewish and Arab women (9%). Approximately 78% of HSV-2 infections in Israel are asymptomatic. HSV-1 causes 66.3% of genital herpes in the Tel Aviv area.> Syria— Genital herpes infection from HSV-2 is predicted to be low in Syria although HSV-1 levels are high. HSV-1 infections is common (95%) among healthy Syrians over the age of 30, while HSV-2 prevalence is low in healthy individuals (0.15%), and persons infected with other sexually transmitted diseases (9.5%). High risk groups for acquiring HSV-2 in Syria, include prostitutes and bar girls; they have 34% and 20% seroprevalence respectively."
56,za3faran 10 months ago
56,| root | parent | next [–]
56,"> If you are an adult of typical sexual activity, it is likely you have already had sex with someone infected with herpes.Again, not in conservative populations where marriage is the typical way to have sexual relations. Syria is one example that you quoted. A smart person would get his/her partner tested if they suspect anything before getting married."
56,yakshaving_jgt 10 months ago
56,| root | parent | next [–]
56,"I'm trying to work out which logical fallacy you're employing here. No True Scotsman? Personal Incredulity?In any case, assuming that infidelity or anything else sinful/haram doesn't occur in ""conservative"" populations strikes me as frightfully naïve.The data is in. Arabs get herpes too."
56,za3faran 10 months ago
56,| root | parent | next [–]
56,"Infidelity does happen, but it's much much less common and is severely looked down upon.Edit: it seems that HSV-1 is not an STD, but it causes genital herpes if engaging in oral sex? Which can explain the big gap between the two in conservative cultures."
56,noodlesUK 10 months ago
56,| parent | prev | next [–]
56,"I think that the only reasons to choose MySQL (or Maria) over Postgres for a new project are operational. Postgres is probably the better database in almost all respects, but major version upgrades are much much more of a pain on Postgres than on almost any other system I have ever used. That being said, I would choose Postgres pretty much every time for a new project. The only reason I would use Maria or MySQL would be if I thought I later would want to have something like Vitess, for which I think there isn't really an equivalent for Postgres."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"> but major version upgrades are much much more of a pain on Postgres than on almost any other system I have ever used.This is a thread comparing MySQL and Postgres and your claim is that postgres is harder to do major version upgrades than anything you have used??Context is important here, have you honestly actually upgraded a MySQL node? It’s a lesson in pain and “major” version changes happen on minor versions, like the entire query planner completely trashing performance in 5.6->5.7Postgres has two forms of updates:1) in place binary upgrade.Fast, clean, simple, requires that you have the binaries for the old and the new database.2) dump/restore.Serialise the database into text files, load a new database and deserialise those files into it.Slow, but works flawlessly & consistently with relatively low danger.MySQL can only do option 2.You can sort of fake an “update” by abusing the fact that MYSQLs replication offers no guarantees, so you can make a new server a replica; then roll over. But it is impossible to know what data was lost in that transition and MySQL will happily continue without ever telling you.I have experienced this behaviour in large e-commerce retailers. MySQL was very popular for a very long time and I am intimately aware of operational best practices and how they are merely patching over an insane system."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"MySQL doesn't use SemVer. MySQL 5.6 vs 5.7 are different ""release series"", and switching between them is considered a ""major"" version change.MySQL absolutely fully supports in-place binary upgrades, saying otherwise is pure FUD. And the upgrade process in MySQL doesn't even iterate over your table data in any way, so claiming it will cause ""data loss"" is also pure FUD.At Facebook we automated rolling in-place updates of our entire fleet, with new point builds of fb-mysql going out several times a month, to the largest MySQL deployment in the world. Worked flawlessly and this was a full decade ago.MySQL is widely considered easier to upgrade (relative to Postgres) because MySQL's built-in replication has always been logical replication. Replicating from an older-version primary to a newer-version replica is fully supported. When upgrading a replica set, the usual dance is ""upgrade the replicas in-place one at a time, promote one of the replicas to be the new primary while temporarily booting out the old primary; upgrade the old primary and then rejoin the replica set""."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"Facebook has, at minimum, 3 teams maintaining MySQL. including a team who genuinely modifies it into submission. so much that they needed 1,700 patches to port their modified version to 8.0.It is not relevant to the discussion to discuss how Facebook has managed to munge it to work reasonably well by pouring thousands of hours of engineer time into the effort; and MySQLs in-place upgrades absolutely do not work the way you describe consistently.I know this because I have been in the code, and only after having experienced it. Maybe some of your lovely colleagues has helped out your particular version to be marginally more sane.It genuinely must be nice having a dozen people who can work around these issues though, I certainly wouldn’t consider it an operational win, most companies have no DB automation engineers, or DB performance engineers or MySQL infrastructure engineers.> Replicating from an older-version primary to a newer-version replica is fully supported.Here also be dragons, as eluded to. I know it works quite often, I have used it.FWIW: I ran global AAA online-only game profile systems on a handful of Postgres machines at about 120k transactions/s in 2016, I would have needed 5x as many instances to do it in MySQL, and this was only tiny part of our hosted infra.. which included a global edge deployment of game servers, auth servers, matchmaking, voice bridges and so on.and we only had two people responsible for the entire operation"
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Please educate me on how my statement about MySQL upgrades is incorrect, I'd love to hear this. I've been using MySQL for 20 years, and while 2 of those years were at Facebook, 18 were not. I've performed MySQL upgrades in quite a wide range of environments, and what you're saying here about lack of in-place upgrades or eating data is simply not aligned with reality.I haven't made any comments regarding performance comparisons, and have also run extremely large DB footprints with tiny teams, but I don't see how any of that is relevant to the specific topic of new-version upgrade procedure!"
56,dijit 10 months ago
56,| root | parent | next [–]
56,"Because it depends so much on your storage engine and schema, I have never seen it recommended because there are circumstances where you have data which is unrepresentative unless you are very careful or you don’t actually use the expressiveness of the DB.I mean, I’ve also seem my share of “ERROR 1071 (42000) at line xxx: Specified key was too long; max key length is xxx bytes” randomly that basically means the machine needs manual recovery.God help you if you don’t have innodb_file_per_table enabled to begin with too.I know you want me to cite exactly. That will take me time to find because I stopped caring about MySQL 7 years ago, but I will dig for you."
56,Volundr 10 months ago
56,| root | parent | next [–]
56,"FWIW while I use Postgres for my own development I've had to administer a number of MySQL servers for other devs. Upgrades have always been updating the MySQL package, restarting MySQL, then running `mysql_upgrade`, and restart the server again. I'm pretty sure the mysql_upgrade has even been missed a number of times and it's worked fine.I won't say it's impossible you ran into issues doing this, but it is the documented and supported upgrade path.I love Postgres, but as someone whose maintained both for years, upgrades (at small scale) are the one area where I'd say MySQL has Postgres beat."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"as long as you upgrade with a minor version, you will have the same experience with postgres.11.0->11.2 will work totally fine, with no command needed."
56,Volundr 10 months ago
56,| root | parent | next [–]
56,"Right, but now go from 11->12, which is the equivalent of the upgrade path I was describing for MySQL. I either need to install both versions and use pg_upgrade to convert the binary files, then remove 11 (and extensions may break this flow) or do pg_dump/restore.Minor versions on both Postgres and MySQL are painless, just install and restart the server. Major upgrades on MySQL are significantly less painful."
56,evanelias 10 months ago
56,| root | parent | prev | next [–]
56,"> I’ve also seem my share of “ERROR 1071 (42000) at line xxx: Specified key was too long; max key length is xxx bytes” randomly that basically means the machine needs manual recovery.What? This error has nothing to do with upgrades, nothing to do with manual recovery, and hasn't been a common problem for many many years.In old versions of MySQL, it just meant you needed to configure a few things to increase the InnoDB index limit to 3072 bytes, instead of the older limit of 767 bytes:innodb_file_per_table=ON"
56,innodb_large_prefix=ON
56,"innodb_file_format=barracudaand then ensure the table's row_format is DYNAMIC or COMPRESSED.But again, all of this happens by default in all modern versions of MySQL and MariaDB.Should it have been the defaults much earlier? Absolutely yes, MySQL used to have bad defaults. It doesn't anymore."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"The error I gave is a similar one to the one I used to get with “major” upgrades that happened when Ubuntu decided it was time to upgrade.It happens and I seriously never claimed that it was an ultra common problem, merely that upgrades in Postgres are more intentional and not painful except for a little extra work between major versions. The standard upgrade path within major versions; 9.x or 10.x or 11.x or 12.x is working just the same as MySQL, except I have much more experience of MySQL completely fumbling their “automatic unattended” upgrade or even the mysql_upgrade command.Mostly because in the real world outside of engineering cultures databasen are massively abused, ISAM tables that are constantly updated, InnoDB ibdata1 in the terabytes, poor configs, replicas that have skipped a few queries, column changes inside a transaction that failed but actually modified data, it happens. Usually I am called in to clean the mess.Major difference here is that Postgres doesn’t leave a mess, so I never have the kind of issues that I am describing in this thread with it, and you don’t because I am guessing that you’re there when they’re installed, someone with knowledge was actively maintaining. or you have a lot of people to help with shortcomings.I get it though. you’ve got your sunk cost knowledge of MySQL and you’ve been on large support teams for it. Maybe you’re afraid I’m suggesting that this knowledge goes out the window. and it has gotten better, but I wouldn’t give my kids watered down led infused soft drinks just because I had suffered through led poisoning. I remember coming to blows with you in other threads over the years because you think MySQL can be saved or is totally fine, but honestly, just, no."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"I'm primarily a software engineer, not a member of ""large support teams"". I've also worked for many years as an independent consultant, brought in when things go wrong, certainly not when they were first ""installed"". I'm not ""afraid"" of anything concerning my knowledge going ""out the window"". If MySQL suddenly disappeared worldwide, I could happily pivot to some other area of software engineering, or I could simply retire. Please stop make assumptions about other people who you know nothing about.I'm responding to you because you're repeatedly posting factually incorrect items, for years. For example you and I have directly discussed the ""MySQL doesn't use SemVer"" thing before on HN, and yet here you are again in this thread, claiming 5.6 to 5.7 should be a ""minor"" upgrade.Anyway, to the topic at hand, as others have also mentioned in this thread: historically the difficulty with Postgres upgrades has been the lack of cross-version replication, due to Postgres WAL replication being a low-level physical replication system. This made it difficult to perform an upgrade while keeping your site fully online. Perhaps the newer logical replication support makes this easier these days. I hope to learn more about it someday. If you can share your process for upgrading a Postgres cluster while keeping it online, that would be helpful and informative."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"1. The log-replication method of upgrading can be performed using the built-in logical replication facilities as well as using external logical replication systems such as pglogical, Slony, Londiste, and Bucardo. Most of which have existed essentially forever.2. Failovers of any database are not instant, but they are indeed quick! So let’s not claim that you can do an upgrade with zero downtime.3. In-place upgrades are extremely fast and you can test the speed using a physical replica before hand, usually it’s a couple of seconds though the docs say minutes.4. MySQLs major version being in the minor position is exactly the kind of “you should be sure you know what you’re doing but we won’t make it obvious” territory that I really despise."
56,JohnBooty 10 months ago
56,| root | parent | next [–]
56,"While you two have agreed on approximately nothing, this has been an informative discussion and I do thank you both."
56,0xf8 10 months ago
56,| root | parent | next [–]
56,"I echo the sentiment and think yours is likely the most pertinent takeaway having made it this far absent reaching any consensus whatsoever haha.It was nevertheless a pretty epic journey of dialectic discourse plunging _deep_ into the esoteric and nuanced realm of expert-level technical minutiae. A mostly intellectual journey, albeit distinctly punctuated by an undertone of emotional angst that steadily progressed in its growing intensity in a manner proportional to the magnitude of your collective disagreement… epic indeed."
56,LammyL 10 months ago
56,| parent | prev | next [–]
56,Is there a good way to do case-insensitive accent-insensitive collations yet in postgresql?
56,"It’s been a holdup for using that for some use cases like searching for data, like a person’s name, in pgsql when the casing or accents don’t match perfectly.Mssql has had this for ever, and I’m pretty sure MySQL has it as well."
56,dijit 10 months ago
56,| root | parent | next [–]
56,"Maybe this helps: https://stackoverflow.com/posts/11007216/revisions ?My gut tells me that I would do it in the query itself though, and not rely on the collation. Maybe I am misunderstanding."
56,sjamaan 10 months ago
56,| root | parent | next [–]
56,"Look under ""Update for Postgres 12 or later"", there they create a collation, an index to make use of it and then a query to make use of it:"
56,SELECT \* FROM users WHERE name = 'João' COLLATE ignore_accent;
56,EwanToo 10 months ago
56,| root | parent | prev | next [–]
56,"Not really, no, it's doable but not easily"
56,throw0101b 10 months ago
56,| parent | prev | next [–]
56,"> There is almost no good reason to choose MySQL over PostgreSQL for any operational reasonGalera is the main one I can think of:* https://galeracluster.com/library/documentation/tech-desc-in...* https://mariadb.com/kb/en/what-is-mariadb-galera-cluster/* https://packages.debian.org/search?keywords=galeraI'm not aware of any multi-master, active-active(-active) replication system that is open source for PostgreSQL."
56,jaachan 10 months ago
56,| root | parent | next [–]
56,We ran a Galera cluster for 4 days and it died on the first ALTER TABLE we did. Only option was to reduce it down to a Primary / Replica setup. I really can't recommend anyone using this.
56,dijit 10 months ago
56,| root | parent | prev | next [–]
56,Last time I looked at Galera it had a limit at 5TiB of data. Which is fine I guess.In the Postgres space there is citusdb which provides multi master.There is also BDR from 2ndquadrant if you want a paid/supported solution. https://www.enterprisedb.com/products/edb-postgres-distribut...
56,davidgerard 10 months ago
56,| root | parent | prev | next [–]
56,"yeah, that's the use case ""I run an application that requires or is highly optimised for MySQL""."
56,avinassh 10 months ago
56,| root | parent | prev | next [–]
56,how do multi master MySQL handles conflicts?
56,asdfman123 10 months ago
56,| parent | prev | next [–]
56,The main problem with herpes is the stigma against it. Don't besmirch it by associating with Oracle.
56,srcreigh 10 months ago
56,| parent | prev | next [–]
56,"Postgres is >50x slower for range queries(example below) and is akin to using array-of-pointers (ie Java) whereas MySQL supports array-of-struct (C). Illustration from Dropbox scaling talk below.Sneak peek photo [1] (from [2]). Just imagine its literally 500-1000x more convoluted per B-tree leaf node. That's every Postgres table unless you CLUSTER periodically.[1]: https://josipmisko.com/img/clustered-vs-nonclustered-index.w...[2]: https://josipmisko.com/posts/clustered-vs-non-clustered-inde...Mind boggling how many people aren't aware of primary indexes in MySQL that is not supported at all in Postgres. For certain data layouts, Postgres pays either 2x storage (covering index containing every single column), >50x worse performance by effectively N+1 bombing the disk for range queries, or blocking your table periodically (CLUSTER).In Postgres the messiness loading primary data after reaching the B-tree leaf nodes pollutes caches and takes longer. This is because you need to load one 8kb page for every row you want, instead of one 8kb with 20-30 rows packed together.Example: Dropbox file history table. They initially used autoinc id for primary key in MySQL. This causes everybodys file changes to be mixed together in chronological order on disk in a B-Tree. The first optimization they made was to change the primary key to (ns_id, latest, id) so that each users (ns_id) latest versions would be grouped together on disk.Dropbox scaling talk: https://youtu.be/PE4gwstWhmc?t=2770If a dropbox user has 1000 files and you can fit 20 file-version rows on each 8kb disk page (400bytes/row), the difference in performance for querying across those 1000 files is 20 + logN disk reads (MySQL) vs 1000 + logN disk reads (Postgres). AKA 400KiB data loaded (MySQL) vs 8.42MiB loaded (Postgres). AKA >50x improvement in query time and disk page cache utilization.In Postgres you get two bad options for doing this: 1) Put every row of the table in the index making it a covering index, and paying to store all data twice (index and PG heap). No way to disable the heap primary storage. 2) Take your DB offline every day and CLUSTER the table.Realistically, PG users pay that 50x cost without thinking about it. Any time you query a list of items in PG even using an index, you're N+1 querying against your disk and polluting your cache.This is why MySQL is faster than Postgres most of the time. Hopefully more people become aware of disk data layout and how it affects query performance.There is a hack for Postgres where you store data in an array within the row. This puts the data contiguously on disk. It works pretty well, sometimes, but it’s hacky. This strategy is part of the Timescale origin story.Open to db perf consulting. email is in my profile."
56,scosman 10 months ago
56,| root | parent | next [–]
56,"I made same comment elsewhere before finding this comment. I can vouch for this speed up in the ratio of records per page. It is very real. Only applies for small records where you can pack many rows into a page, and where you can cleanly partition by user/tenant, but that’s common enough.I will say: we kept every table we didn’t have to migrate for perf reasons in Postgres, and never regretted it.Edit: and the index “fix” for Postgres doesn’t work often. Postgres will validate the row on disk, even if it’s in the index, if the page’s visibility map isn’t set. If you data isn’t write once, there’s a decent chance your page is dirty and it will still make the “heap” fetch."
56,Shorel 10 months ago
56,| root | parent | prev | next [–]
56,"You are confusing two concepts here. In InnoDB, the tables are always ordered by the primary key when written to actual disk storage.This is not the same as ""having a primary key"", Postgres also has primary keys. It just stores the PK index separately from the bulk of the data.Oracle also has primary keys, even if the order of the rows is different to the key order. In Oracle, when the rows are stored in the same order as the keys in the primary index, it is a special case and these tables are called IOT, index ordered tables.The disadvantages of IOT are that inserts are slower, because in a normal table, the data is appended to the table, which is the fastest way to add data, and only the index needs to be reordered. In an IOT, the entire table storage is reordered to take the new data into account.Select queries, OTOH, are much faster when using IOT, for obvious reasons, and this is what you describe in your comment.If you use TEXT, BLOB, or JSON fields, even in MySQL, the actual data is stored separately."
56,srcreigh 10 months ago
56,| root | parent | next [–]
56,"I said primary index, not primary key (primary key and primary index is synonymous in mysql Dropbox example). Primary index is database theory lingo for storing all the primary row data inside a B-tree. It’s synonymous with what you say IOT although that’s a new term for me.You’re incorrect about IOT reordering the entire table at least wrt mysql. MySQL uses a B-tree to store rows, so at most it’s insertion sort on a B-tree node and rare b-tree rebalance. Most b-tree leaf nodes have empty space to allow for adding new data without shifting more than a few hundred other rows. Also, non-IOT tables also need to do a similar process to write to each of its indexes. Last, it’s sort of a tossup since if you’re appending to an IOT table frequently, the right edge of the B-tree is likely cached. (similarly for any small number of paths through the primary index B-tree). At worst Postgres heap will need to surface one new heap disk page for writing, although I’m sure they have some strategy for caching the pages they write new data to.Sorry to spam this info! Glad to see we both love databases and I’m always please to see engagement about this topic!"
56,boloust 10 months ago
56,| root | parent | prev | next [–]
56,"When using clustered indexes, one tradeoff is that if a non-clustered index isn't covering for a query, it will need to perform B-tree traversals to find rows in the clustered index. This can significantly increase the amount of (at least logical) IO compared to heap tables, where the non-clustered indexes can refer directly to the row id.Because you can only have a single clustered index, you're effectively paying for efficient range queries on a single clustering key by making all other queries slower.This tradeoff may or may not be worth it depending on your query patterns. In my experience, you can often get away with adding some subset of columns to a non-clustered index to make it covering, and get efficient range queries without making a copy of the entire dataset.And even with clustered indexes, as soon as you want a range query that's not supported by your clustered index, you're faced with the exact same choices, except that you have to pay the cost of the extra B-tree traversals."
56,srcreigh 10 months ago
56,| root | parent | next [–]
56,"Appreciate the thoughtfulness. I believe the branching factor of even wide keyed tables don’t add significant cost to point lookups. At most one or two extra disk pages needing to be read.Example: 80 bytes keys gives you branching factor of roughly 100. 10M rows and you can pack say 20 rows per page. That’s a 4GB table, give or take. That btree still only has 3 intermediate layers and primary data on a 4th layer. (Calculation is log(10M/20/0.75)/log(100)+1.) The first two layers take up less than a megabyte of ram and are therefore easily cached. So you wind up only needing 2 disk reads for the final two layers. Unless Postgres is caching the entire index for point lookups, it should come out about even.Can’t find any resource saying that btree height exceeds 5, so I’m thinking it’s at worst 2x the (very small) disk read cost vs Postgres."
56,ideal_gas 10 months ago
56,| root | parent | prev | next [–]
56,"(Admitting bias: I've only ever worked with postgres in production with update-heavy tables so I've dealt with more of its problems than MySQL's)Postgres also has other gotchas with indexes - MVCC row visibility isn't stored in the index for obvious performance reasons (writes to non-indexed columns would mean always updating all indexes instead of HOT updates [1]) so you have to hope the version information is cached in the visibility map or else don't really get the benefit of index only scans.But OTOH, I've read that secondary indexes cause other performance penalties with having to refer back to the data in clustered indexes? Never looked into the details because no need to for postgres which we've been very happy with at our scale :)[1] https://www.postgresql.org/docs/current/storage-hot.html"
56,srcreigh 10 months ago
56,| root | parent | next [–]
56,"Interesting. PG docs don’t clarify whether visibility map gets updated for HOT update. Maybe even HOT update spoils index only scans. Although I can’t see why-no new index entries, heap visibility status hasn’t changed for any indexes.. wish to find some answers here but I could not.Wrt secondary indexes, yes and no. There is a cost to traverse a B-tree for point lookups. Also, foreign keys may now be composite keys if primary key is composite as in the Dropbox example.If the secondary index is very different from the primary, it will be more expensive. However it’s pretty common to at least use a “user_id” as the first part of the primary key. This will make partial full scans a lot faster for queries regarding a single user; only need to scan that users data, and it comes at a 1-2 order of magnitude cheaper disk read cost. So you’d need a secondary index only if the data you need is spread across 1000s of pages (megabytes of data for a single user in one table) and you’re looking for only a handful of rows randomly located in that sequence.Twitter is a characteristic case where you need many different clustered sets for the same data (tweets) to power different peoples feeds. I believe twitter just stores many copies of tweets in different clusters in Redis- basically the same as having a (author_id, ts) primary key tweets table and a (follower_id, ts) primary key feed table, both having tweet data inlined. If one clustered table isn’t enough, use two."
56,frodowtf 10 months ago
56,| root | parent | prev | next [–]
56,"That was a great read in contrast to all the ""there's no reason to use mysql"" nonsense in this thread"
56,srcreigh 9 months ago
56,| root | parent | next [–]
56,"That's what I'm going for! I love this type of info, so against the grain and useful it's basically a scandal. :-) Check out your sibling comment about pg_repack though, it's a really awesome PG perf improvement tool."
56,arp242 10 months ago
56,| root | parent | prev | next [–]
56,"I've definitely run in to these kind of issues and learned about them the hard way, but I found that in PostgreSQL it's quite a bit easier to understand what is actually going on due to better documentation and tooling, and I've found this very valuable. Maybe it's just because I've spent more time with PostgreSQL than MariaDB, but MariaDB has often left me quite a bit more confused (on performance, but also other topics)."
56,singron 10 months ago
56,| root | parent | prev | next [–]
56,"If your pg database improves with CLUSTER, you can use pg_repack instead to achieve the same effect without downtime. Besides reordering the heap, it will also clear out bloat from the heap and indexes. I highly recommend partitioning if you have heavy write traffic on large tables since that will keep overhead low and make it complete faster."
56,srcreigh 9 months ago
56,| root | parent | next [–]
56,Thanks. I actually thought of similar pg_repack concept on my own a couple days ago (influenced by gh-ost probably..) I was googling stuff like how to switch a PG replica to master. I was imagining having 2 dbs and running CLUSTER on the inactive one. Probably wouldn't work but anyways I found about pg_repack after researching.I'm now super interested in the perf aspects of running pg_repack. It would definitely require scratch storage space to be able to copy over the largest table in the DB (I'd guess 2.1x the largest table vs 2x the total DB size). I imagine repacking isn't as efficient as putting stuff into a B-tree. But I wouldn't expecting it to be anything like 50x worse like I portrayed above.
56,singron 9 months ago
56,| root | parent | next [–]
56,"It does CREATE TABLE ... AS SELECT * FROM table ORDER BY, which isn't too bad. Then it runs CREATE INDEX for all your indexes, which is usually faster than CREATE INDEX CONCURRENTLY. However, while these are running, a trigger is inserting records for all new writes into a special log table. Once all the indexes are created, it replays that log table until it's empty, and then it briefly locks the source table, flushes the last bit of the log table, and swaps the tables.If you have a lot of write traffic, that log table will get really big, and pg_repack will have to do a lot of work to replay it. It's possible that the log table grows faster than pg_repack can replay it, which will cause it to never finish and eventually exhaust your storage space. You can also run into an issue where the log table never gets completely empty, and pg_repack never initiates that final lock and swap.Partitioning helps a lot because it divides both the size of the tables and the write traffic to each table."
56,JohnBooty 10 months ago
56,| root | parent | prev | next [–]
56,Thanks for that informative link. It's rare in these sorts of discussions.
56,eyelidlessness 10 months ago
56,| parent | prev | next [–]
56,"The only reason I’d consider MariaDB, which I’m surprised I don’t see mentioned currently, is its bitemporal features. There are whole worlds of problems solved that are almost universally badly done in schema/business logic instead.Granted I haven’t had to make a decision like this for several years, I’ve hardly even touched a database except to debug some stuff on localhost that’s outside of my explicit purview. So maybe Postgres solutions have narrowed the gap on this without my knowing it."
56,otabdeveloper4 10 months ago
56,| parent | prev | next [–]
56,Does Postgres have binlog replication yet?
56,dijit 10 months ago
56,| root | parent | next [–]
56,"Yes, for over half a decade at least, but “binlog” is a MySQL term, for postgresql it has the much more apt name: write-ahead log.it is the only official, in-binary replication mechanism."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Postgres WAL replication is a physical replication stream. MySQL binlog replication is a logical replication stream, a higher-level abstraction which is independent of the storage engines.Postgres does separately support logical replication now, but it has some limitations, such as not permitting replication of DDL: https://www.postgresql.org/docs/current/logical-replication-..."
56,paulddraper 10 months ago
56,| parent | prev | next [–]
56,MySQL is certainly faster for writes.https://www.uber.com/blog/postgres-to-mysql-migration/
56,arp242 10 months ago
56,| root | parent | next [–]
56,"I would be wary putting too much stock in a seven year old post on the topic; lots of stuff has changed. Specifically, at least one person claimed that a patch greatly improved PostgreSQL for this use case: https://news.ycombinator.com/item?id=26285452"
56,paulddraper 10 months ago
56,| root | parent | next [–]
56,The MVCC requirement is still there.It's like saying C is faster than Ruby. It always will be.
56,za3faran 10 months ago
56,| parent | prev | next [–]
56,"MySQL is free, regardless of Oracle's ownership."
56,supergram 10 months ago
56,| parent | prev | next [–]
56,> * Memory tablesPostgreSQL has memory tables too.
56,pyuser583 10 months ago
56,| parent | prev | next [–]
56,Does Postgres have an archive mode?
56,dijit 10 months ago
56,| root | parent | next [–]
56,"If you say what you’re trying to actually achieve I can help with a solution, but asking if it supports an arbitrary feature is not going to get the answer you want because depending on what you’re actually using an archive table for, Postgres might have something already built in but it will almost assuredly not be exactly like an archive table storage type."
56,pyuser583 10 months ago
56,| root | parent | next [–]
56,"Sorry, ARCHIVE is a MySQL storage engine. It supports only non destructive transactions: INSERT, REPLACE, and SELECT, but not DELETE and UPDATE.It’s an excellent alternative to use a WORM drive when you’re trying to preserve everything (say, a list of financial transaction).I’ve looked for something like this in Postgres (which I love!), but sadly it doesn’t seem supported.https://dev.mysql.com/doc/refman/8.0/en/archive-storage-engi..."
56,yencabulator 10 months ago
56,| root | parent | next [–]
56,"Saying that REPLACE is non-destructive is a bit weird. Overwriting data is destructive, by definition -- and even if ARCHIVE is append-only underneath, it seems this use of REPLACE is very much a ""logical overwrite"", as there doesn't seem to be any kind of time travel view of the old state.Also, you might be interested in Parquet, perhaps as seen through Delta Lake https://delta.io/ or Postgres Foreign Data Wrappers like https://github.com/adjust/parquet_fdw -- Delta Lake's simple ""LSM of Parquet files in an object store"" design is pretty sweet."
56,ants_a 10 months ago
56,| root | parent | prev | next [–]
56,Why is revoking permissions to run deletes/updates not an option?
56,NuSkooler 10 months ago
56,| prev | next [–]
56,"PostgreSQL every time, unless you have a specific reason, or as already pointed out, you're sure you don't just need SQLite.PSQL in my experience has vastly better tooling, community, and is ahead of the curve tech wise. Same with extensions availability. Or perhaps you need to move away from it to say CockroachDB, or similar which is much easier."
56,grzm 10 months ago
56,| parent | next [–]
56,nit: psql is the command line client. postgres or pg are the more common shortenings of PostgreSQL.https://www.postgresql.org/docs/15/app-psql.html
56,adoxyz 10 months ago
56,| prev | next [–]
56,Choose whichever one you/your team is more familiar with. Both are battle-tested and proven and will likely scale to whatever needs you have.
56,TehShrike 10 months ago
56,| parent | next [–]
56,"This is the correct answer.Whichever one you start out with, you will be annoyed if you switch to the other one 5 years later."
56,"I started out with mysql, and when I started working on a postgres project, I was shocked at some of the ways it was lacking (how am I supposed to store email addresses in a database without collations?).But when postgres folks grouse about stuff in mysql, I'm usually nodding along and saying ""yeah, that would be nice"".They're both great options."
56,"If anybody on your team is already an expert at one of them, use that one."
56,robertlagrant 10 months ago
56,| root | parent | next [–]
56,"> when I started working on a postgres project, I was shocked at some of the ways it was lacking (how am I supposed to store email addresses in a database without collations?)How long ago was this? :)"
56,TehShrike 10 months ago
56,| root | parent | next [–]
56,3 years ago.
56,From this comment thread https://news.ycombinator.com/item?id=35908169 I infer that Postgres still doesn't support collations.
56,robertlagrant 10 months ago
56,| root | parent | next [–]
56,"Oh - is this ci collations, not collations in general?"
56,TehShrike 10 months ago
56,| root | parent | next [–]
56,"A lot of the collations are useful in different circumstances – but the CI ones probably come up the most often, and are necessary to store email addresses in a way where you store the case information that the user typed in, but can do case-insensitive lookups against in the future."
56,funcDropShadow 10 months ago
56,| root | parent | prev | next [–]
56,"According to the documentation of Postgres 12 [1] it is possible to use so called non-deterministic collations, which may express case-insensitivity. If that is what you need.The documentation of Postgres 11 [2] states that this was not possible:"
56,Note that while this system allows creating collations that “ignore case” or
56,"“ignore accents” or similar (using the ks key), PostgreSQL does not at the moment allow such collations to act in a truly case- or accent-insensitive manner. Any strings that compare equal according to the collation but are not byte-wise equal will be sorted according to their byte values."
56,[1]: https://www.postgresql.org/docs/12/collation.html
56,[2]: https://www.postgresql.org/docs/11/collation.html
56,OJFord 10 months ago
56,| root | parent | prev | next [–]
56,"> how am I supposed to store email addresses in a database without collations?Not familiar with MySQL so trying to look that up, but with a constraint? Or just don't do that? - SO answer I found says 'it's much more useful to have johndoe@ and JohnDoe@ treated as the same than it is to support case sensitive email addresses'.. ok, it's also incompliant, but whatever's 'more useful’ I guess!"
56,vosper 10 months ago
56,| parent | prev | next [–]
56,"Having used both in production, I agree with the above. It's not going to make or break your business or project.I will also add that their are giant companies relying on both databases with great success. Facebook still runs on MySQL, and contribute back to it. Youtube I'm not sure about, but it did run on MySQL for a long time, well after it got massive. I'm sure examples exist for Postgres (Amazonm since they moved off Oracle?)"
56,bombcar 10 months ago
56,| parent | prev | next [–]
56,"And if you have experience with one and try to use the other, you may end up foot gunned by something you didn't know about."
56,erulabs 10 months ago
56,| prev | next [–]
56,"Use MySQL if you're expecting to have to do large operational database tasks re: migrations, maintenances, with no ability to go offline. gh-ost, percona-osc, the new INSTANT DDL stuff, is all quite far ahead in MySQL-land. Additionally, Vitess and Planetscale are making huge strides in MySQL performance. There are more people and guides in the world to help recover even the most mutilated of MySQL databases. MySQL gets more love in extremely large enterprise-level organizations, and it shows.Use Postgres if you need some of the quite-good extensions, most notably PostGIS, or if you just want things to work; most documentation will be postgres flavored. Postgres gets more love from web-developers, and it shows."
56,fzeindl 10 months ago
56,| parent | next [–]
56,"This is wrong. MySQL does not support transactional DDL, so you cannot run migration and abort them in the middle.Always use postgresql. It's more logical, more extensible, saner, supports many extensions and is more predictable.MySQL is inconsistent crap, that trades away consistency, correctness and stability for a little bit of performance in standard use cases.Do yourself a favor and always use postgreSQL. I switched 15 years ago and never looked back. Have done 15-50 projects since in psql."
56,WJW 10 months ago
56,| root | parent | next [–]
56,"With a sane migration tool like pt-osc or gh-ost you can absolutely abort in the middle. What's more, you can pause in the middle or even slow down in the middle based on arbitrary logic (ie, pause migration if replication delay rises above a certain value). Postgres is nice and transactional DDL has its place but postgres stopped halfway through IMO. Vanilla Postgres > vanilla MySQL, but the migration story of MySQL + tooling is so far beyond Postgres + tooling that it's not even funny.That said, if you don't expect to have tables with 100m+ rows, even vanilla postgres will be good enough."
56,amflare 10 months ago
56,| root | parent | prev | next [–]
56,Spoken like someone who switched 15 years ago and never looked back.
56,barrkel 10 months ago
56,| root | parent | prev | next [–]
56,"Production MySQL databases of any significant size use pt-osc or gh-ost for schema changes, and these can be throttled, paused, aborted and so on."
56,craigkerstiens 10 months ago
56,| prev | next [–]
56,"Having answered this a ton over the years, don't want to really take shots at MySQL. But Postgres stands in pretty unique ground.1. It's solid as a really reach data platform (more than just a relational database). It's extension framework is quite unique compared to others. It's JSONB support was the first among other relational databases and is feature rich and performant. Multiple index types. Transactional DDL. The list goes on.2. No central owner. A lot of open source is source code is open, but it's maintained by a central company.3. I mentioned extensions, but really that is understated. It can do really advanced geospatial, full text search, time series, the list goes on.Having explained this a ton of times first 10 years ago - https://www.craigkerstiens.com/2012/04/30/why-postgres/ and then again 5 years later with and updated version, most recently tried to capture more of this in an updated form on the Crunchy Data blog - https://www.crunchydata.com/why-postgres"
56,jhas78asd 10 months ago
56,| parent | next [–]
56,Hi Craig! :) https://twitter.com/andatki
56,endgame 10 months ago
56,| prev | next [–]
56,"https://web.archive.org/web/20211206040804/https://blog.sess...From a former MySQL developer:> let me point out something that I've been saying both internally and externally for the last five years (although never on a stage—which explains why I've been staying away from stages talking about MySQL): MySQL is a pretty poor database, and you should strongly consider using Postgres instead."
56,tough 10 months ago
56,| prev | next [–]
56,Pick postgres unless you have a good reason not too?
56,cryptonector 10 months ago
56,| parent | next [–]
56,And even then you pick Postgres.
56,tough 10 months ago
56,| root | parent | next [–]
56,"Can't really loose with postgres, I concur"
56,trympet 10 months ago
56,| prev | next [–]
56,The Postgres query optimizer is more powerful than the MySQL query optimizers [1]. It generally scales better for OLTP. Also tons of extensions that can accelerate your workload.[1] - https://ieeexplore.ieee.org/document/9537400
56,spudlyo 10 months ago
56,| parent | next [–]
56,"It's also more opinionated than the MySQL query optimizer, in that you can't give it hints to prevent it from making a horrible mistake."
56,jhas78asd 10 months ago
56,| root | parent | next [–]
56,There is a way to provide some type of planner hints https://pghintplan.osdn.jp/pg_hint_plan.html
56,manv1 10 months ago
56,| prev | next [–]
56,"The main reason I prefer mysql over PostgreSQL is that mysql is just more consistent - in its commands, quirks, etc.Postgres - is it pg, pgsql, psql, postgres, postgresQL? The answer is ""yes.""Plus the case behavior for tables and column names drives me crazy. It's like some leftover VMS shit. I mean seriously fix it. Can you or can you not use a capital letter for a table/column name? I can never remember. Or you can, but you have to quote it? Fuck.Until recently (which to be fair might be 8-10 years ago) postgres' performance monitoring tools sucked compared to mysql. I know at one point in the last 10 years they still used sunos4 as their base configuration because you know, the OS had been EOL for like a decade at that point.MySQL is fire and forget. psql (or postgres or pg or postgresql?) is not fire and forget. It's twitchy and requires constant vigilance. I don't want a piece of infrastructure that requires constant vigilance.That's not to say I won't use it. It's geo stuff is really great. It's JSON support is better than MongoDB's, from what I've heard. Row level security is awesome. But are those features good enough to overcome psql's quirks? Sometimes."
56,arp242 10 months ago
56,| parent | next [–]
56,"> at one point in the last 10 years they still used sunos4 as their base configuration because you know,What exactly do you mean with this? I tried to find some more information, and all I could find were some fixes from 2001[1] (SunOS 4 was supported until 2003), a minor refactor in 2008 with ""SunOS 4 is probably broken anyway""[2], and"
56,"that's pretty much it. SunOS 4 was moved to ""Unsupported Platforms"" with the release of 7.3, in 2002.[3][1]: https://postgrespro.com/list/thread-id/1598869[2]: https://www.postgresql.org/message-id/20081211091708.0726075...[3]: https://www.postgresql.org/docs/7.3/supported-platforms.html"
56,jhas78asd 10 months ago
56,| parent | prev | next [–]
56,"If you’re talking about the command line client that’s built in, it’s psql. If you can’t remember the command name to launch it or regularly type those other commands when you meant to type psql, you could add aliases to your shell that point to psql. :)Learning any new CLI client is a bit daunting at first. With repetition and intention, I think the commands become very memorable. Eg “describe table” is “dt”."
56,spudlyo 10 months ago
56,| prev | next [–]
56,"This is like asking how you'd choose between Emacs and Vim, Mac and PC, Monoliths and Microservices, Functional and Object Oriented .. you're likely going to elicit a lot of passion and not a ton of objective information.For most applications, either choice is going to be just fine. Use what your team has the most experience with. If you have no experience, try them both out and go with whatever you're most comfortable with."
56,0xf8 10 months ago
56,| parent | next [–]
56,"Nah, trivially the correct answers are:Vim, *NIX (so Mac), monoliths, and lambda calculus all the way—FP!jk, but FWIW I think sometimes, in rare instances, there does exist a pretty unequivocal consensus"
56,“right answer” to this sort of question
56,… maybe such as like:
56,Git vs any other distributed VCS ?
56,Doctor_Fegg 10 months ago
56,| prev | next [–]
56,"For anything involving location, choose Postgres because PostGIS is just so good."
56,solarkraft 10 months ago
56,| parent | next [–]
56,"PostGIS is somewhat of a standard interface for geolocation data and one of the main interfaces of QGis, the other being based on SQLite. It's kind of cool how powerful it is, being a ""real"" database instead of some more contrived format.My adventures with QGis+PostGIS have also led me to discover the fun fact that even setting the TLS mode to ""enforce"" may enforce that you're using TLS ... but not that the certificates are correct. Silly. It also won't use the system's store when set to ""full"". Oh well.By the way: I find Railway to be a great home for low-use Postgres/PostGIS databases. You basically pay for operations and the free tier is totally fine for semi-casual use (about 1-2k entities) with a few people from across the pond."
56,frizlab 10 months ago
56,| parent | prev | next [–]
56,"Good to know! I’m starting a project that will use location, and I chose Postgres, I’m happy to hear this :-)"
56,bratao 10 months ago
56,| prev | next [–]
56,"One big factor that keep us on MySQL is the MyRocks engine. We have huge databases with billions of rows. The MyRocks enable the use of it with heavy compression, that PostgreSQL can´t handle it, as it is much slower and uses 30x more disk usage, even with heavy TOAST tuning and/or ZFS compression."
56,eqvinox 10 months ago
56,| parent | next [–]
56,"To be fair, at the scale of your use case there I really hope you have a proper DBA who understands multiple database systems and their details, and is able to make the best choice for your setup."
56,(At some point the commercial and/or oddball SQL servers become an option too…)For everyone else who's in most cases not even stuffing a million rows into their database… just stick with Postgres :)
56,ahachete 10 months ago
56,| prev | next [–]
56,"At our company, we provide Postgres 24x7 support. We have partners that provide support for other databases, and for some projects we work together with companies that have multiple databases.We have over the years compared the rate of production incidents Postgres vs MySQL. It's roughly 1:10 (MySQL has around 10 times more production incidents than Postgres).You may consider this anecdotal evidence, but numbers managed here are quite significant.The gist is that Postgres is not perfect nor free from required maintenance and occasional production incidents. But for the most part, it does the job. MySQL too, but with (at least from an operational perspective) many more nuances."
56,herpderperator 10 months ago
56,| prev | next [–]
56,"I find PostgreSQL permission management quite convoluted. In MySQL it is simple to query for what grants a user has, but in PostgresSQL you need to write 100 lines of SQL to do the same... and you can't run \du and other commands without psql. Why couldn't they just come up with `SHOW` shortcuts that work in any SQL client?"
56,jhas78asd 10 months ago
56,| parent | next [–]
56,"You can likely get the SQL for a meta command, and you could run the SQL from your preferred client if you don’t use psql. Here is one example: https://dba.stackexchange.com/a/131031I also highly recommend investing in psql skills though if you are a Postgres user."
56,arp242 10 months ago
56,| root | parent | next [–]
56,"Yes, that's the ""100 lines of SQL to do the same"" the previous poster mentioned (obviously they were exaggerating a bit and it's less than literally 100 lines, but it's pretty complex)."
56,iamwil 10 months ago
56,| prev | next [–]
56,"The difference is not significant enough to matter for most projects, esp just starting out. Hence, I mostly choose Postgres, since I don't like Oracle as a company very much."
56,dylan604 10 months ago
56,| parent | next [–]
56,"Whenever I see MySQL, my brain automatically sees MariaDB. What is this Oracle thing you speak of ;-)"
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"MySQL and MariaDB have diverged quite a bit. I recently wrote a roundup of just the differences in table functionality and DDL, and despite keeping the post focused to that relatively-narrow topic, the list of differences is getting VERY long: https://www.skeema.io/blog/2023/05/10/mysql-vs-mariadb-schem..."
56,dylan604 10 months ago
56,| root | parent | next [–]
56,">MySQL and MariaDBAgain, what is this thing you are trying to compare? I just see MariaDB and MariaDB =)However, because of my blindness to actual MySQL, I have totally not paid attention to any differences between the two. I guess ""drop in replacement"" isn't actually true any more. Thanks for the info"
56,geenat 10 months ago
56,| prev | next [–]
56,"I know it seems dumb, but postgres really needs to add the simple developer experience stuff like:SHOW CREATE TABLE;SHOW TABLES;SHOW DATABASES;SHOW PROCESSLIST;CockroachDB added these aliases ages ago."
56,spacedcowboy 10 months ago
56,| parent | next [–]
56,"This.For anything at home, I would use MySQL just for those things. The psql client feels very primitive by comparison to me - even though it isn't."
56,tbarbugli 10 months ago
56,| root | parent | next [–]
56,"I highly suggest investing time learning psql, autocomplete works great and it has a ton of useful slash commands. \d for instance shows you the list of tables. Awesome tool"
56,spacedcowboy 10 months ago
56,| root | parent | next [–]
56,"I don't need it often enough to invest the time. I generally set up a database as backing store to some project, fiddle with it until I'm happy it's working at the scale/performance I want, and then move on to something else.During those few weeks I'm actively using the database on the project, I can either get frustrated beyond belief with the CLI for Postgres, or just use what's at hand with MySQL. In fact, these days SQLite is getting more of my attention anyway, and I wrote a small CLI for it a decade or so back (before the sqlite3 client gave us most of the below) to provide:- Timings for the queries (in fact I called it 'tsql')- Aligned-column displays, with | separators between columns and index-indicators- Ability to parse some blobs (Plists on the Mac, for example) and display- Things like ""SHOW DATABASES"", ""SHOW TABLES"", ""SHOW TABLES LIKE"" etc.Mainly I wrote it to do some benchmarking, but I eventually preferred it over sqlite3 as the CLI.Note that all this is personal stuff - When I do commercial stuff, the answer is always ""what is best understood by the people maintaining it afterwards""..."
56,jhas78asd 10 months ago
56,| root | parent | next [–]
56,Each of the bullets you listed have very straightforward and memorable meta commands that I use on a regular basis with psql. It may be worth learning them just for when you use Postgres. There is also a built in help. These can also be saved into your dot files so you don’t need to memorize them. Happy to show you if you’re interested!
56,Izkata 10 months ago
56,| parent | prev | next [–]
56,"I forget if there's an equivalent for the first one, but from psql there is a translation of mysql's ""DESC table"" as ""\d table"", and the rest are:\dt\lSELECT * FROM pg_stat_activity;"
56,arp242 10 months ago
56,| root | parent | next [–]
56,"> I forget if there's an equivalent for the first oneNot really; PostgreSQL doesn't store the original query, so you'll need to re-create it from pg_class, pg_attribute, and all of that (which is really what \d and such in psql do). The easiest way is probably pg_dump, but it's best to just get used to \-commands because it's really just the same thing."
56,jhas78asd 10 months ago
56,| root | parent | next [–]
56,Find the SQL from meta commands. Example: https://dba.stackexchange.com/a/131031
56,arp242 10 months ago
56,| root | parent | next [–]
56,"That is not equivalent to ""show create table"" at all."
56,funcDropShadow 10 months ago
56,| root | parent | prev | next [–]
56,There is an extension that does that: pgddl. [1][1]: https://github.com/lacanoid/pgddl
56,jpgvm 10 months ago
56,| prev | next [–]
56,"Generally speaking it should always be PostgreSQL.It's the default choice for a number of reasons but chief among them is just that it's higher quality. That is it's developed to higher standards due to community bar being really high (thanks Tom Lane, et al for your stewardship) and testing and analysis of changes to the database being ingrained into the culture.By pursuing correctness first before performance for many years PostgreSQL has built a stronger foundation that is now paying dividends in terms of both performance but also ability to ship powerful features quickly. This has generally resulted in the gap between MySQL and PostgreSQL only continuing to widen over the last 10 years.So when would you consider picking MySQL?To me that comes down to exactly one set of use-cases and that is workloads that are fundamentally incompatible with VACUUM. The PostgreSQL MVCC system requires that table heap be maintained by the VACUUM process to both ensure safety (txid wraparound) and reclaim/reuse heap storage. This process is very expensive for workloads that do a lot of updates, especially on indexed columns (as indices need VACUUMing also), less of an issue for non-indexed columns if you can use HOT (heap only tuple) updates and tune the target fill ratio of heap pages appropriately.In most cases it's highly unlikely your business is going to reach the level of write load where these deficiencies in write behaviour actually matter but it is possible. Uber famously migrated from PostgreSQL primarily because their experiences with write amplification and VACUUMing.If for instance though your data consists of a smaller live hot set and a warm set that is less frequently updated and easily separable by a deterministic factor like time you can very easily use PostgreSQL table partitioning to isolate the two and continue to scale for a very very long time on pure PostgreSQL.In practice this may be fixed in PostgreSQL one day, there was a project called zheap to implement an UNDO log style storage system for PostgreSQL (which would avoid all the VACUUM maintenance etc) but it stalled out, largely I believe because it wasn't able to provide obvious wins quick enough to stimulate further interest. However OrioleDB has picked up the torch now and does in fact seem to be showing very impressive results.If such a storage engine is merged in my mind there will no longer any reason to consider MySQL for production workloads."
56,jhas78asd 10 months ago
56,| parent | next [–]
56,"Thanks for calling out table partitioning. Besides implementing it at one level, multiple levels can be used simultaneously (eg list and range). Tables can be grouped and split out to their own database (aka functional sharding/vertical sharding) and again partitioned. This all takes more effort and investment but keeps you on PostgreSQL. As you said fillfactor can be tuned, more HOT updates. Even analyzing whether the Updates could be turned into inserts that are written at a high rate, not incurring bloat, and then fewer updates are made at a rate that does not outrun Vacuum."
56,test6554 10 months ago
56,| prev | next [–]
56,"I've been using MariaDB (MySQL) as a hobbyist for years. I just set up a couple myqsql servers with phpmyadmin on Raspberry PIs and use them for local development. Basic crud apps, etc.I've always assumed that PostgreSQL is a step up, but never really bothered to look into what I get for the effort. Do I really get anything if I'm not trying to make apps at scale?"
56,eqvinox 10 months ago
56,| parent | next [–]
56,"> I've always assumed that PostgreSQL, but never really bothered to look into what I get for the effort.You're making a (mistaken) assumption that Postgres giving you a ""step up"" means that you also have to put in more effort."
56,"You don't, at least not in my experience."
56,Both are database servers with a bunch of install & setup.
56,"There's phppgadmin if you want an 1:1 replacement for phpmyadmin (no opinion on these, haven't used either).Postgres just gets you farther if you need to at a later point.I would recommend you swap out mysql for postgres on your raspis."
56,You're gaining experience on one of the two.
56,"But experience on Postgres seems to be more useful and valuable (cf. rest of the HN comments), for the same cost of your time."
56,12907835202 10 months ago
56,| parent | prev | next [–]
56,Same position.There's so many things I want to learn I'm not sure postgres is such a step up from MySQL that it's worth being at the top of the list.
56,tmountain 10 months ago
56,| root | parent | next [–]
56,"If you’re not at the level of detail to care about transaction isolation levels, clustered indexes, and the implementation details of replication, you can invest your time elsewhere. For certain use cases and workloads, the differences can matter a lot, but many developers are just looking for a backing store for their CRUD app and in that case, either is likely fine."
56,duiker101 10 months ago
56,| prev | next [–]
56,Most answers seem written by fanboys rather than legit answers.I would say go with what you know and are most comfortable with. You are more likely to get the better outcome.
56,chunk_waffle 10 months ago
56,| parent | next [–]
56,"This.I've heard countless times that Postgres is better and I've watched talks where they show how loosey-goosey MySQL is with some things but I know how to backup, restore, tune, secure and replicate MySQL. I grok it's permissions in depth more than I ever have with Postgres and I've even written a mysql plugin in C so I have that in my toolbox if I need it. So I'd by default, usually go with MySQL (or in some cases SQLite.) but if I didn't have to administer or maintain it, and someone else was handling that I think I'd be fine with Postgres too."
56,NightMKoder 10 months ago
56,| prev | next [–]
56,"I’m going to go for the esoteric opinion: MariaDB. Specifically to get system versioned tables. Imagine having the full change history of every single row for any odd task that you need without the performance penalty of keeping historical data in the same table. It can be a huge amount of leverage.If that’s not your interest, I will admit that Postgres array support is far ahead of any of the MySQLs. Most ORMs don’t use it but you can get optimal query prefetching via array subqueries."
56,heyoni 10 months ago
56,| parent | next [–]
56,"I’m 100% with you regarding system versioned tables. However, I think they’re coming to Postgres soon-ish. I was following the tracker for a while and it looked like it was done."
56,skunkworker 10 months ago
56,| prev | next [–]
56,"Postgres for anything with a single database size < 4TB.But if you need something that can handle 100TB+, go Vitess(mysql compatible)."
56,jhas78asd 10 months ago
56,| parent | next [–]
56,What’s the reason though for Vitess? Postgres supports tables up to 32TB but hopefully you’re splitting them up using declarative partitioning in one or more ways before that. If you have tables that are smaller than a TB and a large memory DB (>1 TB RAM) Postgres should run ok right? I’d also imagine you’re splitting up your database into multiple databases and multiple instances (the writers) well before that as well right?
56,winrid 10 months ago
56,| parent | prev | next [–]
56,Why not Citus?
56,bkuehl 10 months ago
56,| prev | next [–]
56,"A lot of comments for Postgres, but it's the only major DB in 2023 that does not let you choose your character collation when creating a database. That is pretty much a deal breaker day 1. Guess you'll be doing a tolower() on every db search and not use indices which will kill performance or using column collation casts on every search query. I just don't get it.I once tried to migrate a SQL Server DB to Postgres and eventually gave up, with MySQL being a pretty easy switch with some minor stored procedure rewrites.Also it tends to do things way differently than every other DB. VACUUM is just a completely different concept that can footgun you pretty fast.Postgres is pretty powerful but it has certainly made some interesting design choices."
56,arp242 10 months ago
56,| parent | next [–]
56,"> doing a tolower() on every db search and not use indicesIf you create the index with lower() it will uses that; e.g. ""create index on tbl (lower(email))"" and then ""select * from tbl where lower(email)=lower($1)"". That's more or less the standard way to do this but there are some other ways as well. It's more explicit than MySQL, so in that way it's better. It's more effort and easy to forget, and in that way it's worse – either way: it's definitely possible to do case-insensitive searches with indexes.When I first used PostgreSQL I ran in to ""how the hell do I do this?! MySQL just lets me [...]"" kind of issues, but after many years of PostgreSQL usage I now have the opposite when I use MariaDB, which also has its share of awkwardness and issues (just different ones)."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"> It's more explicit than MySQL, so in that way it's better.It sounds like you're under the impression that MySQL just makes everything case-insensitive and is silent about this? That's decidedly not the case.MySQL 8 ships with 41 different character sets, supporting a total of 286 different collations. Collation names explicitly include ""ci"" (case-insensitive) vs ""cs"" (case-sensitive), as well as ""ai"" (accent-insensitive) vs ""as"" (accent-sensitive), and also the language/region conventions used for sorting purposes.You can choose collation at the column-level granularity, as well as setting defaults at the table, schema, and server levels. It's completely explicit and very configurable."
56,arp242 10 months ago
56,| root | parent | next [–]
56,There is no way to see from the query itself if it's case-sensitive or insensitive; that is what I meant.
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"Eh, just from a SQL query alone, there's no way to see that (lower(email)) is indexed, or see column data types etc. That metadata lives in the table definition, which is a normal place for it, so it seems reasonable for the collation / case-insensitivity to not be explicit in the query text.Besides, MySQL also supports functional indexes, so you could do the (lower(email)) approach in MySQL too if you really want!"
56,arp242 10 months ago
56,| root | parent | next [–]
56,"No, you can't see everything, but you can see the exact comparison it's making. Is that useful? You can decide that for yourself but I like being able to see as much of the logic (and thus verify the correctness) in the query itself. Also helps with copy/paste and some other things.I never said you can't use functional indexes in MySQL. Someone said ""you can't do this in PostgreSQL"" and I just commented ""here's (one way) to do this, maybe that's helpful some day"". That's it."
56,evanelias 10 months ago
56,| root | parent | next [–]
56,"My apologies, I misunderstood ""It's more explicit than MySQL"" to imply that you were saying that approach couldn't be used in MySQL."
56,bkuehl 10 months ago
56,| root | parent | prev | next [–]
56,"I was disappointed with MariaDB and tried it before using MySQL. It is so far behind MySQL that it can't be considered equivalent anymore. And I really wanted to use MariaDB instead.The ""MySQL just lets me"" stuff eventually adds up and hinders development. For example, your lower() on the param example is now incompatible with most ORMs unless you do that in code or create a special SQL statement for that. This isn't all fringe cases that you run into when you're months in either. I really wonder on some of these comments saying they've vetter both and Postgres wins hands down.Postgres is solid but it definitely has its warts and downsides too."
56,spprashant 10 months ago
56,| parent | prev | next [–]
56,"Am I missing something here? Postgres does allow you to choose character collation when creating a database, as well as when creating new columns."
56,bkuehl 10 months ago
56,| root | parent | next [–]
56,"I'm sorry, I was wrong in that Postgres does let you specify a few character collations at the DB level, but they are pretty much ASCII vs UNICODE with no case-insensitive configurations. You can create collations in v12 and assign them to particular columns."
56,pierat 10 months ago
56,| prev | next [–]
56,"Friends dont let friends use #Horracle software.That includes VirtualBox, MySQL, Horracle Cloud. Just step back. Walk away. Do not pass go, do not collect $20000 lawyers fees for unintended actions."
56,za3faran 10 months ago
56,| parent | next [–]
56,"That's quite silly. VirtualBox is great, and so is MySQL. They're also both OSS, so no lawyers in the sense you're implying."
56,pierat 10 months ago
56,| root | parent | next [–]
56,"Then you don't know what you're talking about, and are ignorant of the risks of using VirtualBox.That VirtualBox extension pack? That aint free... well, it is for personal use only because they're not shaking individuals down. However, Oracle watches what domains download that extension pack, and sues companies when too many employees download it.You can see that in this reddit thread: https://www.reddit.com/r/sysadmin/comments/d1ttzp/oracle_is_...And this article also addresses the risks of dealing with Horracle.And a company I worked for had dealings with them as well. Again, Horracle played dirty and did bullshit, except over Horracle DB itself."
56,za3faran 10 months ago
56,| root | parent | next [–]
56,"You're basically saying something that isn't free, isn't free."
56,DiabloD3 10 months ago
56,| prev | next [–]
56,Why would I choose MySQL in any year? There is no context you can provide to this question where I wouldn't always choose Postgres.
56,vermaden 10 months ago
56,| prev | next [–]
56,Its simple AF - I just always pick the well proven PostgreSQL database.... if that is too big I use SQLite.
56,scosman 10 months ago
56,| prev | next [–]
56,"Postgres for almost anything. Robustness, ecosystem, accuracy, all top notch.One exception: I did migrate two very large tables (15B+ rows) from Postgres to MySQL for performance reasons. InnodB (MySQL storage engine) can arrange the records on page by primary key, and if you have a multi-value PK (user_id, uuid) it means all records from a user are in the same set of pages. Huuuuge improvement over having your data for a user spread out over N different pages. Memory cache way more efficient. Orders of magnitude speed up on cold queries, better cache hit rate, and cost reduction from smaller servers."
56,jhas78asd 10 months ago
56,| parent | next [–]
56,Did you implement table partitioning with Postgres or consider that before moving?
56,scosman 10 months ago
56,| root | parent | next [–]
56,"Been a while. IIRC it didn’t work with millions of users, and partitioning with many users per partition would have same issues. Also looked at CLUSTER but there was no workable online version."
56,jstx1 10 months ago
56,| prev | next [–]
56,"Also interested in the responses, not because it seems like a close decision but because I would pick postgres by default for anything (anything that isn't simple enough to be done in sqlite)."
56,majestic5762 10 months ago
56,| parent | next [–]
56,Same.
56,azurelake 10 months ago
56,| prev | next [–]
56,"MySQL is still ahead operationally (no vacuum, group replication, gh-ost, optimizer hints, etc.). I would choose it unless one of the Postgres extensions is a killer fit for your project."
56,gardenhedge 10 months ago
56,| prev | next [–]
56,If Postgres was that much better than MySQL then you would expect to see exact reasons on why to pick it. Every comment so far has not listed any reason.
56,mort96 10 months ago
56,| parent | next [–]
56,"Ok, here's one: When you give MySQL invalid data, its standard behavior is to just silently store garbage in your database in many cases where PostgreSQL would've told you that your data is invalid.MySQL's handling of unicode has also been terrible historically, with way too many foot guns, but I don't know if that may be better with recent versions.People aren't providing strong reasons because the question wasn't ""what are some objective reasons for picking one over another"", but ""how do"
56,"you pick between them"". People are simply answering the question OP asked, and a lot of people's process is simply to pick PostgreSQL."
56,AdamJacobMuller 10 months ago
56,| root | parent | next [–]
56,"A lot of the MySQL issues historically have been fixed. UTF-8 is better now, invalid data handling is better (by default even! though your distros default config probably puts you back in permissive mode!) but regardless I'm still using Postgres every single time.The fact is that MySQL historically was terrible for complex schemas with complex types while postgres was a pleasure to work with. MySQL had a huge performance edge for many years but that came at a cost of resiliency and reliability. MySQL has greatly improved on these key areas and Postgres has also made significant performance improvements. Systems these days are also so powerful that the database probably isn't your benchmark.Regardless, I always use Postgres every single time because I am just scarred from years of dealing with MySQL. What even is MySQL is also an open question at this point, there's MySQL and MariaDB and Percona flavors and the founder of Percona was just ousted and I can't be bothered to put in mental energy to untangle all this to even figure out what MySQL I should be developing against.Compare this to Postgres where the community seems to have an extremely steady hand and constant progress. There's no forks, there's no infighting, there's no drama, there's a great regular release schedule with incremental improvements."
56,bcrosby95 10 months ago
56,| root | parent | next [–]
56,"In MySQLandia, we do not speak of utf8, we only speak of utf8mb4."
56,evanelias 10 months ago
56,| root | parent | prev | next [–]
56,"> When you give MySQL invalid data, its standard behavior is to just silently store garbageThis is a common misconception, but this hasn't been the case for over 7 years. MySQL 5.7, released in Oct 2015, changed its defaults to enable strict sql_mode. All prior versions have hit end-of-life for support years ago, so there is no modern version of MySQL with this silent truncation behavior.The only reason this problem persists is because Amazon RDS (all versions and forms, including Aurora) uses nonstandard default settings which disable strict mode!That all said, I do believe Postgres is an excellent database, and a great choice for quite a large range of use-cases. But please, let's compare 2023 Postgres with 2023 MySQL, not 2023 Postgres with much older MySQL. It's only fair."
56,justinclift 10 months ago
56,| root | parent | next [–]
56,"> But please, let's compare 2023 Postgres with 2023 MySQL, not 2023 Postgres with much older MySQL. It's only fair.Heh Heh HehOn a humorous note, the official MySQL page (in the early 2000's) comparing MySQL vs other databases had the same problem.They'd list the latest and greatest MySQL version, but compare it against archaic versions of the others."
56,"Clearly on purpose, because ""Marketing"" probably.Asked them (via official @postgresql.org email address) to please update that page to more a recent PostgreSQL, for fairness."
56,And was completely ignored of course.So it's kind of amusing to see a request for fairness in the opposite direction (which I agree with anyway) ~20 years later. ;)
56,jbverschoor 10 months ago
56,| root | parent | prev | next [–]
56,"Why do they have these settings? Because shit software used shit MySQL +data corruption.It might be better now. But for me it’s in the same shithole as mongodb, php (old style, so no recovery there even though it’s possible to create proper code) and most JavaScript.Other things is that people don’t even want to use Oracle’s MySQL but MariaDB. Why the hell would I want to run a fork of something, and still keep calling it something else.The only reason for MySQL is wordpress"
56,bakugo 10 months ago
56,| root | parent | prev | next [–]
56,"> MySQL's handling of unicode has also been terrible historically, with way too many foot guns, but I don't know if that may be better with recent versions.Unicode generally ""just works"" if the charset in use is utf8mb4. As of MySQL 8.0, this is the default."
56,mort96 10 months ago
56,| root | parent | next [–]
56,"Ah, that's good to hear. I haven't looked seriously at databases other than SQLite for a long time, it would be interesting to see a more up to date evaluation."
56,CaveTech 10 months ago
56,| root | parent | prev | next [–]
56,"Hasnt been the case for a few major versions, unless you want to anthropomorphise your db and and hold it accountable for past behaviour."
56,ok_dad 10 months ago
56,| root | parent | next [–]
56,"> unless you want to anthropomorphise your db and and hold it accountable for past behaviourNo one is holding the literal bits that make up the database executable accountable here, they are indicating they don't trust the devs of MySQL/MariaDB to do a good job. Whether or not that is an accurate view on their part is arguable, but it's pretty clear from context that they don't think that several if/else statements had it out for them."
56,nicoburns 10 months ago
56,| parent | prev | next [–]
56,A few reasons:- Transactional DDL statements (schema modifications)- Better support for UPSERT operations- Better JSON support (including ability to index into JSON columns)- the RETURNING statement to return data that was inserted/updatedIn general Postgres is a lot more featureful than MySQL.
56,et-al 10 months ago
56,| parent | prev | next [–]
56,"If you're doing any sort of spatial logic (e.g. mapping), you'll want to use PostGIS."
56,justin_oaks 10 months ago
56,| parent | prev | next [–]
56,"One thing that bit me on MySQL is that triggers don't execute on cascade deletes/updates: https://bugs.mysql.com/bug.php?id=11472That issue was filed in 2005 and it still isn't fixed.Another gripe I have is that MySQL's JSON Path capabilities are much more limited than the Postgres JSON Path capabilities. MySQL doesn't have filter expressions nor any operators/methods but Postgres does. Don't get me wrong, neither is jq, but I hate having to jump through extra hoops to get my JSON in the right format.Comparehttps://www.postgresql.org/docs/current/functions-json.html#...andhttps://dev.mysql.com/doc/refman/8.0/en/json.html#json-path-..."
56,l5ymep 10 months ago
56,| parent | prev | next [–]
56,"Postgres has a worse implementation of MVCC. It results in more bloat being produced, and slightly slower updates in a highly concurrent environment. Most businesses don't operate at the scale where this matters. But on the flip side, the tooling and developer experience is much better."
56,eqvinox 10 months ago
56,| parent | prev | next [–]
56,Array data type.This has saved my ass a bunch of times.
56,"Not even as a column type, just in complex queries that otherwise became unwieldy monsters. The usual ""started out simple but now it's a frankenbase"" and you're stuck with a shitty schema."
56,(The one thing worse than refactoring code:
56,refactoring databases!)In one case I was able to replace a 15-minute process with thousands of queries with one single query that aggregated all the data into a bunch of arrays.
56,It completed in a few seconds.
56,"(Doing it without arrays would have been possible, but duplicated a lot of data in the result set.)"
56,CuriouslyC 10 months ago
56,| prev | next [–]
56,"The only instance where I'd choose mysql over postgres is if your database needs are very simple, but you need to be able to scale hard, and your devops aren't skilled enough to manage an advanced postgres setup."
56,mgl 10 months ago
56,| prev | next [–]
56,We choose Postgres for extensibility and stability :)
56,david927 10 months ago
56,| parent | next [–]
56,I was going to make a similar joke: you look at them and choose Postgres
56,m0llusk 10 months ago
56,| prev | next [–]
56,PostgreSQL is a community thing and MySQL is Oracle.
56,Maybe make some basic benchmarks for comparison?
56,mixmastamyk 10 months ago
56,| prev | next [–]
56,"1) The choice is Postgres if you care about your data at all.2) Yes, if you are already HUGE and have requirements on Vitesse then by all means use it."
56,"If so, you are not asking this question—see #1.3) It's a blog or something where it doesn't matter, use a static site generator."
56,MrThoughtful 10 months ago
56,| prev | next [–]
56,By choosing SQLite.No server process and a single file per DB which I can put wherever I like.
56,mort96 10 months ago
56,| parent | next [–]
56,"I like SQLite. But I really wish its default behavior wasn't to simply allow garbage data into the database. If I have an int column, don't let me accidentally store a string."
56,eesmith 10 months ago
56,| root | parent | next [–]
56,"https://www.sqlite.org/stricttables.html""In a CREATE TABLE statement, if the ""STRICT"" table-option keyword is added to the end, after the closing "")"", then strict typing rules apply to that table. ... The STRICT keyword at the end of a CREATE TABLE statement is only recognized by SQLite version 3.37.0 (2021-11-27) and later."
56,sqlite> create table x (a int);
56,sqlite> insert into x values ('hello');
56,sqlite> select * from x;
56,hello
56,sqlite> drop table x;
56,sqlite> create table x (a int) strict;
56,sqlite> insert into x values ('hello');
56,Runtime error: cannot store TEXT value in INT column x.a (19)
56,mort96 10 months ago
56,| root | parent | next [–]
56,"Yeah, I know about that, and I'm doing that on all my tables these days. It's just sad that the default behaviour is to allow garbage data, and that if you ever forget to put 'strict' on your tables you'll have a perfectly functional application with no sign that anything is wrong until you suddenly find corrupt data in your database."
56,euroderf 10 months ago
56,| root | parent | prev | next [–]
56,create table strict
56,lisasays 10 months ago
56,| parent | prev | next [–]
56,SQLite is great for its scope - but not in the same class as a full-fledged RDBMS.
56,MrThoughtful 10 months ago
56,| root | parent | next [–]
56,Can you give a specific example of what you are missing when using SQLite?
56,mort96 10 months ago
56,| root | parent | next [–]
56,"At a certain scale, you'll want replication or replication, which SQLite doesn't really do AFAIK. At a scale below that, you'll probably want to be able to have multiple web servers talking to one database server, which SQLite doesn't really do either. I also think SQLite's performance during heavy write workloads is worse than PostgreSQL's?Basically, AFAIK, SQLite becomes problematic once you need more than one computer to handle requests."
56,justinclift 10 months ago
56,| root | parent | next [–]
56,"Just to point out, there are now SQLite replication and various ""distributed database"" projects which seem to work fairly well.They're probably not as battle tested as the PostgreSQL ones, but they are around, have users, and are actively developed.The ones I remember off the top of my head:* https://litestream.io* https://github.com/rqlite/rqlite"
56,"<-- more of a ""distributed database using RAFT"" type of thing* https://github.com/canonical/dqlite"
56,mort96 10 months ago
56,| root | parent | next [–]
56,"Yeah, there are SQLite forks or products built on SQLite which have these sorts of features, but SQLite doesn't. They remains reasons why someone may want to use another database (such as PostgreSQL, or an SQLite fork, or a database which uses SQLite as a storage engine) instead of SQLite.Honestly though, if I need these sorts of distribution features, I would probably prefer the database to have them built in. I don't really see the point in using SQLite at that scale."
56,otoolep 10 months ago
56,| root | parent | prev | next [–]
56,"rqlite[1] creator here, happy to answer any questions.[1] https://www.rqlite.io"
56,lisasays 10 months ago
56,| root | parent | prev | next [–]
56,"Here's a good place to start:https://www.sqlite.org/whentouse.htmlhttps://www.sqlite.org/quirks.htmlFull-scale RDBMSs, especially Postgres, have lots of goodies that either SQLlite doesn't have (or which it does have, but which aren't so richly featured)."
56,"Once you've gotten hooked on a few of these, the distinction will feel a lot more clear.Meanwhile the tipping points in favor of SQLite seem to be embedded systems, and its whole ""service-less"" architecture and plain ease of use."
56,"Which is why it still gets lots of love, for those contexts."
56,endisneigh 10 months ago
56,| root | parent | prev | next [–]
56,"Multi writer, access through the internet without 3rd party software, etc."
56,Avamander 10 months ago
56,| root | parent | prev | next [–]
56,"Performance, especially after a while and certain size."
56,jjav 10 months ago
56,| parent | prev | next [–]
56,"> By choosing SQLite.It's a good reminder to give some thought to whether one actually needs MySQL|Postgres. If not, SQLite is the way to go. Most of my code that uses a DB is using SQLite.But obviously, if you actually need MySQL|Postgres then SQLite is not an option."
56,89 more comments...
56,Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
56,Search:
57,Lagging replication: Slave_SQL_Running_State: Waiting for dependent transaction to commit - Percona Server for MySQL 8.0 - Percona Community Forum
57,Percona Community Forum
57,Lagging replication: Slave_SQL_Running_State: Waiting for dependent transaction to commit
57,MySQL & MariaDB
57,Percona Server for MySQL 8.0
57,mag1kan1n
57,"July 11, 2023, 10:36am"
57,Good afternoon.
57,"We have a cluster MySQL 8.0.32-24 for Linux on x86_64 (Percona Server, Release 24, Revision e5c6e9d2) Master/Slave. Database size is 916GB. Relation is lagging, in show slave Status\G shows Slave_SQL_Running_State: Waiting for dependent transaction to commit."
57,The lag floating for the last 24 hours rose to 16k and dropped to Seconds_Behind_Master: 8021.
57,telegram-cloud-photo-size-2-5289621519653259718-y1280×238 20.2 KB
57,"We tried to change the parameter replica_parallel_workers=1, but did not help."
57,Could you help with any direction or hints for improvements.
57,Here is the config
57,[mysqld]
57,datadir=/var/lib/mysql
57,socket=/var/lib/mysql/mysql.sock
57,bind-address = 0.0.0.0
57,user=mysql
57,symbolic-links=0
57,log_timestamps = SYSTEM
57,log_error = /var/log/mysql/error.log
57,log_bin_trust_function_creators = 1
57,"sql_mode = ""ERROR_FOR_DIVISION_BY_ZERO,NO_ZERO_IN_DATE,NO_ENGINE_SUBSTITUTION"""
57,log_bin = binlog
57,log_bin_index = binlog_list
57,max_binlog_size = 1G
57,expire_logs_days = 5
57,binlog_row_image = MINIMAL
57,binlog-do-db = dbname
57,server-id = 2
57,relay-log = /var/lib/mysql/slave-relay-bin
57,relay-log-index = /var/lib/mysql/slave-relay-bin.index
57,max_connections = 100
57,innodb_doublewrite = 0
57,sync_binlog = 1
57,wait_timeout=150
57,interactive_timeout=7200
57,back_log = 4096
57,"slave-skip-errors = 1062,1396"
57,key_buffer_size = 512M
57,max_allowed_packet = 1024M
57,table_open_cache = 20000
57,table_definition_cache = 10000
57,sort_buffer_size = 2M
57,read_buffer_size = 1M
57,read_rnd_buffer_size = 1M
57,net_buffer_length = 32K
57,group_concat_max_len = 4K
57,join_buffer_size = 320M
57,max_join_size = 1024M
57,tmp_table_size = 4G
57,max_heap_table_size = 8G
57,thread_cache_size = 80
57,thread_stack = 4M
57,innodb_buffer_pool_size = 72G
57,innodb_buffer_pool_instances = 16
57,innodb_log_file_size = 1G
57,innodb_log_buffer_size = 64M
57,innodb_lock_wait_timeout = 60
57,innodb_file_per_table = 1
57,innodb_page_cleaners = 8
57,innodb_read_io_threads = 32
57,innodb_write_io_threads = 8
57,innodb_flush_method = O_DSYNC
57,innodb_flush_log_at_trx_commit = 2
57,innodb_io_capacity = 100
57,skip-external-locking
57,#innodb_adaptive_hash_index=NO
57,#replica_parallel_workers=4
57,slave_sql_verify_checksum=OFF
57,1 Like
57,Denis_Subbota
57,"July 11, 2023, 11:34am"
57,"Hello mag1kan1n,"
57,"It’s expected to see in process list Waiting for dependent transaction to commit when using multithreaded replication, as not all transactions may be applied in parallel ( it’s consistency cost )"
57,The first thing I would like you to suggest is to relax ACID settings to reduce load on IO system and speedup the replication process:
57,"SET GLOBAL innodb_flush_log_at_trx_commit=2,sync_binlog=0;"
57,"But it would help if you considered the risk of the losing all transactions for the last 1 minute in case of crash of mysql, so it’s not good option to set on writer node in case if you are using this node as replica and the writer as well."
57,"However if it’s pure replica, it’s one of the best way to increase replication speed."
57,"Regards,"
57,Denis Subbota.
57,"Managed Services, Percona."
57,2 Likes
57,Denis_Subbota
57,"July 11, 2023, 11:36am"
57,also you may want to review
57,innodb_log_file_size
57,Percona Database Performance Blog – 18 Oct 17
57,How to Choose the MySQL innodb_log_file_size
57,Peter Zaitsev provides guidance on how to choose the MySQL innodb_log_file_size. Getting the innodb_log_file_file size is important to achieve balance between reasonably fast crash recovery time and good system performance.
57,Est. reading time: 3 minutes
57,1 Like
57,mag1kan1n
57,"July 11, 2023, 11:43am"
57,"Thanks, let’s try it. We also get in Slave_SQL_Running_State: Waiting for replica workers to process their queues"
57,1 Like
57,CTutte
57,"July 11, 2023, 12:13pm"
57,"Hi mag1kan1n,"
57,"There are multiple causes that can make a replica fall behind. You should check if there is CPU starvation, memory swapping , disk saturation else replication lagging behind might be due to contention. Make sure no subsystem is saturated because I see some unusually large buffer configuration and it’s possible there is swapping or high memory pressure (I am specially looking at you join_buffer_size = 320M )."
57,"Since you mentioned parallel replication, if you want to tune replica_parallel_workers please check Estimating potential for MySQL 5.7 parallel replication ."
57,"Note that even if you set multiple parallel workers, at times only 1 (or a few) transactions can be concurrently executing and might make a replica fall behind. For example DDLs and very large transactions will make the replica be single threaded for a while. You can find out parallel replication efficiency with the above blogpost"
57,If replication is mostly single threaded then you should tune your workload rather than tune MySQL configuration.
57,For running DDLs on the primary you should use pt-online-schema-change pt-online-schema-change — Percona Toolkit Documentation
57,"if you have very large transactions then you should split them into smaller chunks. I.e rather than deleting 1M rows in 1 transaction, commit every 10k rows"
57,You can also try setting innodb_flush_method = O_DIRECT rather than O_DSYNC
57,"Last, remember that every parameter that you configure in the database will be a tradeoff between using more resources or relaxing consistency at the cost of performance. Relaxing consistency (disabling innodb_doublewrite like you did) can cause corruption in the event of a crash so you should not change parameters lightly as they can cause more harm than good"
57,Regards
57,2 Likes
57,mag1kan1n
57,"July 11, 2023,"
57,2:26pm
57,"Thanks, made innodb_flush_method = O_DIRECT, will monitor the changes. So far this is the situation for an hour -"
57,изображение2830×522 33.5 KB
57,1 Like
57,matthewb
57,"July 12, 2023, 12:54am"
57,You are throttling yourself:
57,innodb_io_capacity = 100
57,<-- change to default 200
57,tmp_table_size = 4G
57,<-- The actual value used will be the smaller
57,max_heap_table_size = 8G
57,<-- of these two. You should make them the same.
57,sync_binlog = 1
57,<-- causes LOTS of disk IO. You should set to 1000 so it syncs less often
57,2 Likes
57,mag1kan1n
57,"July 12, 2023,"
57,7:10am
57,"Hi all, we were helped by including innodb_flush_method = O_DIRECT instead of O_DSYNC. Now there is no lag on the replica. But as far as I understand from the description, this is a tradeoff between fault tolerance and speed? Here’s a 24-hour graph -"
57,изображение2798×524 50 KB
57,1 Like
57,mag1kan1n
57,"July 12, 2023,"
57,8:08am
57,"Thanks for the advice, we’ll give it a try"
57,1 Like
57,matthewb
57,"July 12, 2023,"
57,2:00pm
57,mag1kan1n:
57,this is a tradeoff between fault tolerance and speed?
57,"The different between O_DSYNC and O_DIRECT is that one uses filesystem buffers, and the other does not. Technically, yes, this can be less fault tolerance because you are skipping the filesystem’s journal, but InnoDB has its own protection against this which is the doublewrite buffer. Ideally, you would turn the dblwb on and use O_DIRECT for maximum performance with data reliability."
57,3 Likes
57,mag1kan1n
57,"July 12, 2023,"
57,7:47pm
57,Thank you so much for the advice. Now with parameters
57,slave_sql_verify_checksum = ON
57,innodb_flush_method = O_DIRECT
57,There is no replication lag.
57,1 Like
57,Related Topics
57,Topic
57,Replies
57,Views
57,Activity
57,A Replication lag on Percona Server 8.0.29-21
57,Percona Distribution for MySQL
57,percona
57,new-release
57,1090
57,"January 10, 2023"
57,Percona Master-Master lag issues
57,Other MySQL® Questions
57,885
57,"March 23, 2015"
57,MySQL replication slave parallel workers only work 1 worker
57,MySQL & MariaDB
57,1718
57,"March 12, 2024"
57,Replication lag after upgrade to Percona Server 8.0
57,1992
57,"January 25, 2023"
57,Laggy master-master gtid replication in 5.7.32
57,Percona Server for MySQL 5.7
57,718
57,"November 3, 2021"
57,Home
57,Categories
57,FAQ/Guidelines
57,Terms of Service
57,Privacy Policy
57,"Powered by Discourse, best viewed with JavaScript enabled"
57,Unanswered | Unsolved | Solved
57,"MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners.Copyright © 2006 - 2021 Percona LLC. All rights reserved."
58,JDBC To Other Databases - Spark 3.3.2 Documentation
58,3.3.2
58,Overview
58,Programming Guides
58,Quick Start
58,"RDDs, Accumulators, Broadcasts Vars"
58,"SQL, DataFrames, and Datasets"
58,Structured Streaming
58,Spark Streaming (DStreams)
58,MLlib (Machine Learning)
58,GraphX (Graph Processing)
58,SparkR (R on Spark)
58,PySpark (Python on Spark)
58,API Docs
58,Scala
58,Java
58,Python
58,"SQL, Built-in Functions"
58,Deploying
58,Overview
58,Submitting Applications
58,Spark Standalone
58,Mesos
58,YARN
58,Kubernetes
58,More
58,Configuration
58,Monitoring
58,Tuning Guide
58,Job Scheduling
58,Security
58,Hardware Provisioning
58,Migration Guide
58,Building Spark
58,Contributing to Spark
58,Third Party Projects
58,Spark SQL Guide
58,Getting Started
58,Data Sources
58,Generic Load/Save Functions
58,Generic File Source Options
58,Parquet Files
58,ORC Files
58,JSON Files
58,CSV Files
58,Text Files
58,Hive Tables
58,JDBC To Other Databases
58,Avro Files
58,Whole Binary Files
58,Troubleshooting
58,Performance Tuning
58,Distributed SQL Engine
58,PySpark Usage Guide for Pandas with Apache Arrow
58,Migration Guide
58,SQL Reference
58,JDBC To Other Databases
58,Data Source Option
58,Spark SQL also includes a data source that can read data from other databases using JDBC. This
58,functionality should be preferred over using JdbcRDD.
58,This is because the results are returned
58,as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
58,The JDBC data source is also easier to use from Java or Python as it does not require the user to
58,provide a ClassTag.
58,"(Note that this is different than the Spark SQL JDBC server, which allows other applications to"
58,run queries using Spark SQL).
58,To get started you will need to include the JDBC driver for your particular database on the
58,"spark classpath. For example, to connect to postgres from the Spark Shell you would run the"
58,following command:
58,./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
58,Data Source Option
58,Spark supports the following case-insensitive options for JDBC. The Data source options of JDBC can be set via:
58,the .option/.options methods of
58,DataFrameReader
58,DataFrameWriter
58,OPTIONS clause at CREATE TABLE USING DATA_SOURCE
58,"For connection properties, users can specify the JDBC connection properties in the data source options."
58,user and password are normally provided as connection properties for
58,logging into the data sources.
58,Property NameDefaultMeaningScope
58,url
58,(none)
58,"The JDBC URL of the form jdbc:subprotocol:subname to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret"
58,read/write
58,dbtable
58,(none)
58,The JDBC table that should be read from or written into. Note that when using it in the read
58,path anything that is valid in a FROM clause of a SQL query can be used.
58,"For example, instead of a full table you could also use a subquery in parentheses. It is not"
58,allowed to specify dbtable and query options at the same time.
58,read/write
58,query
58,(none)
58,A query that will be used to read data into Spark. The specified query will be parenthesized and used
58,as a subquery in the FROM clause. Spark will also assign an alias to the subquery clause.
58,"As an example, spark will issue a query of the following form to the JDBC Source."
58,SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
58,Below are a couple of restrictions while using this option.
58,It is not allowed to specify dbtable and query options at the same time.
58,It is not allowed to specify query and partitionColumn options at the same time. When specifying
58,"partitionColumn option is required, the subquery can be specified using dbtable option instead and"
58,partition columns can be qualified using the subquery alias provided as part of dbtable.
58,Example:
58,"spark.read.format(""jdbc"")"
58,".option(""url"", jdbcUrl)"
58,".option(""query"", ""select c1, c2 from t1"")"
58,.load()
58,read/write
58,driver
58,(none)
58,The class name of the JDBC driver to use to connect to this URL.
58,read/write
58,"partitionColumn, lowerBound, upperBound"
58,(none)
58,"These options must all be specified if any of them is specified. In addition,"
58,numPartitions must be specified. They describe how to partition the table when
58,reading in parallel from multiple workers.
58,"partitionColumn must be a numeric, date, or timestamp column from the table in question."
58,Notice that lowerBound and upperBound are just used to decide the
58,"partition stride, not for filtering the rows in table. So all rows in the table will be"
58,partitioned and returned. This option applies only to reading.
58,read
58,numPartitions
58,(none)
58,The maximum number of partitions that can be used for parallelism in table reading and
58,writing. This also determines the maximum number of concurrent JDBC connections.
58,"If the number of partitions to write exceeds this limit, we decrease it to this limit by"
58,calling coalesce(numPartitions) before writing.
58,read/write
58,queryTimeout
58,The number of seconds the driver will wait for a Statement object to execute to the given
58,"number of seconds. Zero means there is no limit. In the write path, this option depends on"
58,"how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver"
58,checks the timeout of each query instead of an entire JDBC batch.
58,read/write
58,fetchsize
58,"The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows)."
58,read
58,batchsize
58,1000
58,"The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing."
58,write
58,isolationLevel
58,READ_UNCOMMITTED
58,"The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in java.sql.Connection."
58,write
58,sessionInitStatement
58,(none)
58,"After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(""sessionInitStatement"", """"""BEGIN execute immediate 'alter session set ""_serial_direct_read""=true'; END;"""""")"
58,read
58,truncate
58,false
58,"This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off truncate option to use DROP TABLE again. Also, due to the different behavior of TRUNCATE TABLE among DBMS, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect, the user option truncate is ignored."
58,write
58,cascadeTruncate
58,"the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect"
58,"This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care."
58,write
58,createTableOptions
58,"This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.)."
58,write
58,createTableColumnTypes
58,(none)
58,"The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: ""name CHAR(64), comments VARCHAR(1024)""). The specified types should be valid spark sql data types."
58,write
58,customSchema
58,(none)
58,"The custom schema to use for reading data from JDBC connectors. For example, ""id DECIMAL(38, 0), name STRING"". You can also specify partial fields, and the others use the default type mapping. For example, ""id DECIMAL(38, 0)"". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults."
58,read
58,pushDownPredicate
58,true
58,"The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source."
58,read
58,pushDownAggregate
58,false
58,"The option to enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Otherwise, if sets to true, aggregates will be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If numPartitions equals to 1 or the group by key is the same as partitionColumn, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output."
58,read
58,pushDownLimit
58,false
58,"The option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is false, in which case Spark does not push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to true, LIMIT or LIMIT with SORT is pushed down to the JDBC data source. If numPartitions is greater than 1, SPARK still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and numPartitions equals to 1, SPARK will not apply LIMIT or LIMIT with SORT on the result from data source."
58,read
58,pushDownTableSample
58,false
58,"The option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is false, in which case Spark does not push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to true, TABLESAMPLE is pushed down to the JDBC data source."
58,read
58,keytab
58,(none)
58,"Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by --files option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise --files assumed. If both keytab and principal are defined then Spark tries to do kerberos authentication."
58,read/write
58,principal
58,(none)
58,Specifies kerberos principal name for the JDBC client. If both keytab and principal are defined then Spark tries to do kerberos authentication.
58,read/write
58,refreshKrb5Config
58,false
58,This option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before
58,"establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false."
58,"The default value is false. Note that if you set this option to true and try to establish multiple connections,"
58,a race condition can occur. One possble situation would be like as follows.
58,refreshKrb5Config flag is set with security context 1
58,A JDBC connection provider is used for the corresponding DBMS
58,The krb5.conf is modified but the JVM not yet realized that it must be reloaded
58,Spark authenticates successfully for security context 1
58,The JVM loads security context 2 from the modified krb5.conf
58,Spark restores the previously saved security context 1
58,The modified krb5.conf content just gone
58,read/write
58,connectionProvider
58,(none)
58,"The name of the JDBC connection provider to use to connect to this URL, e.g. db2, mssql."
58,Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle
58,the specified driver and options. The selected provider must not be disabled by spark.sql.sources.disabledJdbcConnProviderList.
58,read/write
58,Note that kerberos authentication with keytab is not always supported by the JDBC driver.
58,"Before using keytab and principal configuration options, please make sure the following requirements are met:"
58,The included JDBC driver version supports kerberos authentication with keytab.
58,There is a built-in connection provider which supports the used database.
58,There is a built-in connection providers for the following databases:
58,DB2
58,MariaDB
58,MS Sql
58,Oracle
58,PostgreSQL
58,"If the requirements are not met, please consider using the JdbcConnectionProvider developer API to handle custom authentication."
58,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
58,// Loading data from a JDBC source
58,val jdbcDF = spark.read
58,".format(""jdbc"")"
58,".option(""url"", ""jdbc:postgresql:dbserver"")"
58,".option(""dbtable"", ""schema.tablename"")"
58,".option(""user"", ""username"")"
58,".option(""password"", ""password"")"
58,.load()
58,val connectionProperties = new Properties()
58,"connectionProperties.put(""user"", ""username"")"
58,"connectionProperties.put(""password"", ""password"")"
58,val jdbcDF2 = spark.read
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
58,// Specifying the custom data types of the read schema
58,"connectionProperties.put(""customSchema"", ""id DECIMAL(38, 0), name STRING"")"
58,val jdbcDF3 = spark.read
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
58,// Saving data to a JDBC source
58,jdbcDF.write
58,".format(""jdbc"")"
58,".option(""url"", ""jdbc:postgresql:dbserver"")"
58,".option(""dbtable"", ""schema.tablename"")"
58,".option(""user"", ""username"")"
58,".option(""password"", ""password"")"
58,.save()
58,jdbcDF2.write
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
58,// Specifying create table column data types on write
58,jdbcDF.write
58,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
58,"Find full example code at ""examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"" in the Spark repo."
58,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
58,// Loading data from a JDBC source
58,Dataset<Row> jdbcDF = spark.read()
58,".format(""jdbc"")"
58,".option(""url"", ""jdbc:postgresql:dbserver"")"
58,".option(""dbtable"", ""schema.tablename"")"
58,".option(""user"", ""username"")"
58,".option(""password"", ""password"")"
58,.load();
58,Properties connectionProperties = new Properties();
58,"connectionProperties.put(""user"", ""username"");"
58,"connectionProperties.put(""password"", ""password"");"
58,Dataset<Row> jdbcDF2 = spark.read()
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
58,// Saving data to a JDBC source
58,jdbcDF.write()
58,".format(""jdbc"")"
58,".option(""url"", ""jdbc:postgresql:dbserver"")"
58,".option(""dbtable"", ""schema.tablename"")"
58,".option(""user"", ""username"")"
58,".option(""password"", ""password"")"
58,.save();
58,jdbcDF2.write()
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
58,// Specifying create table column data types on write
58,jdbcDF.write()
58,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
58,"Find full example code at ""examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java"" in the Spark repo."
58,# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
58,# Loading data from a JDBC source
58,jdbcDF = spark.read \
58,".format(""jdbc"") \"
58,".option(""url"", ""jdbc:postgresql:dbserver"") \"
58,".option(""dbtable"", ""schema.tablename"") \"
58,".option(""user"", ""username"") \"
58,".option(""password"", ""password"") \"
58,.load()
58,jdbcDF2 = spark.read \
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
58,"properties={""user"": ""username"", ""password"": ""password""})"
58,# Specifying dataframe column data types on read
58,jdbcDF3 = spark.read \
58,".format(""jdbc"") \"
58,".option(""url"", ""jdbc:postgresql:dbserver"") \"
58,".option(""dbtable"", ""schema.tablename"") \"
58,".option(""user"", ""username"") \"
58,".option(""password"", ""password"") \"
58,".option(""customSchema"", ""id DECIMAL(38, 0), name STRING"") \"
58,.load()
58,# Saving data to a JDBC source
58,jdbcDF.write \
58,".format(""jdbc"") \"
58,".option(""url"", ""jdbc:postgresql:dbserver"") \"
58,".option(""dbtable"", ""schema.tablename"") \"
58,".option(""user"", ""username"") \"
58,".option(""password"", ""password"") \"
58,.save()
58,jdbcDF2.write \
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
58,"properties={""user"": ""username"", ""password"": ""password""})"
58,# Specifying create table column data types on write
58,jdbcDF.write \
58,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"") \"
58,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
58,"properties={""user"": ""username"", ""password"": ""password""})"
58,"Find full example code at ""examples/src/main/python/sql/datasource.py"" in the Spark repo."
58,# Loading data from a JDBC source
58,"df <- read.jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
58,# Saving data to a JDBC source
58,"write.jdbc(df, ""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
58,"Find full example code at ""examples/src/main/r/RSparkSQLExample.R"" in the Spark repo."
58,CREATE TEMPORARY VIEW jdbcTable
58,USING org.apache.spark.sql.jdbc
58,OPTIONS (
58,"url ""jdbc:postgresql:dbserver"","
58,"dbtable ""schema.tablename"","
58,"user 'username',"
58,password 'password'
58,INSERT INTO TABLE jdbcTable
58,SELECT * FROM resultTable
59,"Generate SQL Queries In Seconds For Free - SQLAI.aiSQLAI.aiAppFeatures Generate SQL With AIGenerate SQL using everyday language.Optimize SQL With AIOptimize SQL and save database resources.Fix SQL With AICheck SQL syntax and let AI fix it.Explain SQL With AIQuickly understand what a SQL query does.Train Your AI (new)Effortlessly train AI to know your database schema.PricingLoading..SQL GeneratorGenerate SQL Queries in Seconds With AIAI generates, fixes, explains and optimizes SQL queries. Add your database schema and effortlessly train AI to understand it using AI-powered vector search. This ensures unparalleled accuracy.Get StartedAppTrusted by more than 100,000 professionalsSQL Made Dirt SimpleAI enables all to benefit from SQL and experts to move faster. Trained AI ensures even more accurate AI generations.SQL GeneratorGenerate SQL Using Everyday LanguageAI turns your instructions into simple or complex SQL queries, including NoSQL.Optimized SQL GeneratorGenerate Optimized SQL Using Everyday LanguageSwitch to the performance optimized SQL generator to get faster queries and optimization tips.Fix SQL QueriesFix SQL Queries With a ClickCopy paste the SQL query that need fixing and AI will generate a fixed query.Optimize SQL QueryOptimize SQL Queries With a ClickCopy paste the SQL query to be optimized and AI generates an optimized query.Explain SQL QueryUnderstand SQL Queries With a ClickCopy paste the SQL query and AI will give a detailed inline explanation.Format SQL QueryFormat SQL Queries With a ClickCopy paste the SQL query and AI will format the query for improved readability.Generate SQL DataGenerate SQL Data Using Everyday LanguageTell AI what data you want and it will generate dummy data in seconds, e.g. 10 users and 20 comments.Data Analytics as Easy as TextingUtilize the power of AI combined with connected databases.Data AnalyticsAsk Your Data AnythingDrag'n'drop your data and AI will examine your data and return a answer.Data InsightsReal-Time Data Insights Right at Your Finger TipsRun AI-generated SQL and NoSQL queries on connected databases and effortless get real-time data insights.Read more Data DashboardsEffortlessly Build Data DashboardsGenerate insightful SQL queries with AI and simply save them to a data dashboard.Read more What our users sayAs a small business owner, this tool has been a game changer. I can now optimize our database without hiring a specialist. It's saved us time and money, and I can confidently say it's improved our operations significantly.Paul G.Verified purchaseI was always intimidated by SQL queries, but this platform has made it incredibly approachable. It feels like I have a tutor right next to me, guiding and improving my queries. Highly recommend for beginners!Maria T.Verified purchaseThe AI-driven recommendations for fixing and optimizing our SQL queries have been spot on. As a database administrator, it's like having a second set of eyes ensuring everything runs smoothly. Efficiency is up, and errors are down.Liang W.Verified purchaseI can't express enough how much time and stress this tool has saved me. The user interface is intuitive, and the AI suggestions are incredibly insightful, especially for complex database operations.Emily R.Verified purchaseComing from a background with minimal SQL experience, I found the learning curve for this tool very gentle. It not only helps fix my mistakes but also explains why they were wrong, which is invaluable for my learning.Raj P.Verified purchaseAs a data scientist, efficiency and accuracy are key. This tool has significantly reduced the time I spend debugging and optimizing queries. It's an essential part of my data toolkit now.Isabella S.Verified purchaseI was skeptical about how much an AI could help with SQL queries, but I stand corrected. This platform has revolutionized the way our team handles databases, making us faster and more reliable.Henrik O.Verified purchaseWhat impressed me most was how this tool scales for different expertise levels. Whether you're a beginner or a seasoned expert, it provides immense value in making SQL tasks simpler and more efficient.Fatima Z.Verified purchaseFor startups like ours, this tool is a lifesaver. It ensures our database management is on point, letting us focus more on product development and less on backend issues. Absolutely a must-have for small tech teams!Olivia M.Verified purchaseThe AI-powered explanations and optimizations have made a noticeable difference in our database performance. It’s like having a skilled SQL developer on the team, but at a fraction of the cost.Gabriel K.Verified purchaseWhat sets this tool apart for me is its ability to teach while it assists. It's not just about fixing errors; it's about understanding them, which has helped me grow my SQL skills substantially.Sophie A.Verified purchaseThis platform has reduced our dependency on database consultants significantly. Now, we can solve complex SQL problems in-house, saving us both time and consultancy fees.Aarav J.Verified purchaseIt’s rare to find a tool that simplifies your workflow while also educating you. The improvements in our database management processes have been remarkable. Truly a game changer for us.Michelle D.Verified purchaseThe precision and intelligence of the AI in diagnosing and fixing SQL queries are unmatched. It's not just a tool; it's a comprehensive solution that has streamlined our database management.Lucas B.Verified purchaseNever thought I'd be able to handle SQL queries with such ease. This tool has empowered me to take on database tasks that I'd usually avoid. The confidence it gives you is amazing.Ana P.Verified purchaseThe ability to generate and optimize SQL queries instantly has transformed how we work. Our projects move faster, with fewer errors, making our clients happier. It's an indispensable tool for our team.Felix U.Verified purchaseI appreciate how it provides clear, actionable advice for optimizing queries. This tool doesn't just offer fixes; it offers learning opportunities, making us better developers in the process.Hana M.Verified purchaseThe impact on our productivity and efficiency has been profound. It’s not just about fixing queries; it’s about understanding them better, which in turn, makes our database management top-notch.Yuki S.Verified purchaseThis tool has been a game changer for remote teams like ours. It allows us to collaboratively troubleshoot and optimize SQL queries in real time, regardless of our varied skill levels.Samantha L.Verified purchaseFrom the seamless integration to the intuitive learning process, this tool has everything any business, large or small, would need to streamline their SQL query management and optimization efforts.Carlos E.Verified purchaseSupported DatabasesMySQLConnectPostgreSQLConnectSQL Server (MS)ConnectOracle PL/SQLConnectBigQuerySQLMariaDBConnectSQLiteSnowflakeDB2SybaseRedshiftTrino SQL"
59,"(AWS Athena)Salesforce SOQL/SOSLPrestoMongoDBConnectDynamoDBPartiQLGraphQLShow allGet StartedBoost your skills and productivity using AI today.Get Started SQLAI.aiProfessional SQL multi-tool for generating, fixing, explaining and optimizing SQL queries and databases.AppSQL examplesAboutBlogAffiliate ProgramContact© 2024 SQLAI.ai. Berlin, Germany.Get the latest AI & SQL developments:LinkedIn or Twitter"
61,Oracle Performance Tuning - Step-by-step Guide & Tools for 2024
61,Menu
61,Close
61,Search
61,Search
61,VPN
61,By Use
61,Best VPNs of 2024
61,Business VPN
61,Netflix
61,Kodi
61,Torrenting
61,Hulu
61,Sky Go
61,Gaming
61,BBC iPlayer
61,Tor
61,By OS/Device
61,Mac
61,Windows
61,Linux
61,Windows 10
61,Firestick
61,iPhone and iPad
61,Android
61,Windows Phone
61,DD-WRT Routers
61,By Country
61,China
61,Japan
61,Canada
61,Australia
61,Germany
61,France
61,UAE & Dubai
61,Guides
61,Fastest VPNs
61,Cheapest VPNs
61,Free VPNs
61,How to access the deep web
61,Is torrenting safe and legal?
61,Build your own VPN
61,Facebook privacy and security
61,How to encrypt email
61,How to stay anonymous online
61,How we test VPNs
61,See all
61,Reviews
61,NordVPN
61,Surfshark
61,ExpressVPN
61,IPVanish
61,PrivateVPN
61,StrongVPN
61,CyberGhost
61,PureVPN
61,See all
61,Antivirus
61,Reviews
61,Norton Antivirus
61,TotalAV
61,Intego VirusBarrier X9
61,McAfee
61,VIPRE
61,Panda Security
61,Eset
61,See all
61,By OS/Device
61,Mac
61,Windows
61,Guides
61,Best Antivirus in 2024
61,Best Free Firewalls
61,Free Antivirus Software
61,Malware Statistics & Facts
61,See all
61,Compare providers
61,McAfee vs Kaspersky
61,Norton vs Kaspersky
61,McAfee vs Norton
61,Online backup
61,Streaming
61,Kodi
61,Plex
61,Sports Streaming
61,TV Streaming
61,IPTV
61,Blog
61,VPN & Privacy
61,Cloud and Online Backup
61,Information Security
61,More Comparisons
61,Password Managers
61,Identity Theft Protection
61,Usenet
61,Privacy & Security Tools
61,Internet Providers
61,Parental Control Software
61,Net Admin Tools
61,Data Privacy Management
61,Data Recovery Software
61,Crypto
61,Utilities
61,About Us
61,About Our Company
61,Press
61,Software Testing Methodology
61,Editorial Process
61,Join us
61,Contact
61,Net AdminOracle Performance Tuning Guide & Tools
61,We are funded by our readers and may receive a commission when you buy using links on our site.
61,Oracle Performance Tuning Guide & Tools
61,We take a closer look at Oracle performance tuning in this guide. We also look at some of the best database tuning tools.
61,Tim Keary
61,Network Security amd Administration Expert
61,"Updated: April 28, 2023"
61,Have you noticed your database running slower than usual? Then you might need a touch of performance tuning. Performance tuning in Oracle databases eliminates bottlenecks that make applications unresponsive and increases the load your database can handle.
61,What is Oracle Performance Tuning?
61,Performance tuning is the process of administering a database to improve performance. Performance tuning in Oracle databases includes optimizing SQL statements and query execution plans so that the requests can be completed more efficiently.
61,"The organization of a database the SQL statements used to access that data, determine the level of resources needed to respond to queries when an application communicates with the database."
61,Problems like poorly optimized SQL statements force the database to work much harder to retrieve information (resulting in more system resources being used). The more system resources that are used the greater the chance it will affect the experience of users on connected applications.
61,"In an enterprise, users will report a slow application to a database administrator who will then attempt to pinpoint the root cause of the problem. The administrator analyzes statement code and searches for database bottlenecks. The process is extensive, as the administrator has to diagnose the root cause of the problem before it can be addressed."
61,Monitoring Performance in Oracle Databases: Response Time and Throughput
61,When performance tuning an Oracle database there are two metrics that are useful to measure:
61,Response time – How long the database takes to complete a request.
61,System throughput – The number of processes completed in a period of time.
61,"High response time means that an application is providing a slow user experience. On the other hand, low system throughput means that the database only has the resources to manage a small number of tasks in a short time period. An administrator has to be able to know how they are trying to improve performance before tuning."
61,How you optimize an Oracle database comes down to your goals and the type of applications you are using. Many goals like having a fast response time or a high throughput are contradictory.
61,"Tuning for fast response times may speed up individual queries made by users but sacrifice other tasks in the workload. In contrast, achieving a high throughput would aim to optimize the performance of the entire workload to support a larger output of transactions per second (but not necessarily speed up individual queries)."
61,The type of application you’re using makes all the difference. If you’re using an online transaction process (OLTP) application then you would use throughput to measure performance. This is because of the high volume of transactions the application needs to manage.
61,"However, if you were using a decision support system (DSS) with users running queries on everything from a handful of records to thousands of records, then you would measure performance by response time (unless you were supporting lots of users running concurrent queries)!"
61,The Two Types of Tuning: Proactive Monitoring and Bottleneck Elimination
61,"Now that you know what performance tuning is, it’s important to look at the two main models of tuning:"
61,Proactive Monitoring
61,Bottleneck Elimination
61,Database administrators use these two models to manage performance issues and keep applications functioning at a high level.
61,Proactive Monitoring
61,"Proactive monitoring is the process of monitoring a database to discover and address performance issues early rather than simply reacting when there is a problem. With proactive monitoring, administrators will periodically review databases to identify the signs of performance degradation."
61,The idea behind proactive monitoring is to catch issues and inefficiencies before they develop into greater problems further down the line. Some common issues database administrators look out for include:
61,Database wait events – A high number of events can negatively affect database performance. Finding obstructive sessions and killing them can prevent performance degradation.
61,Load average – Monitoring the load average of a server will tell you if server resources are functioning as normal. A high load average can result in slow database performance.
61,Database sessions – Monitoring the number of active sessions can stop you from reaching the maximum (which will prevent you from being able to open new sessions).
61,"However, monitoring proactively does carry some risk. Any changes an administrator makes can result in a decrease in performance for the database. Administrators can mitigate the risks by being cautious before making new changes."
61,Bottleneck Elimination
61,Bottlenecks are one of the most common causes of poor performance. Bottlenecks block requests from reaching the destination and increase the response time of applications. Bottlenecks can be caused by a range of factors from badly coded SQL statements and high resource usage.
61,"Bottleneck elimination is more of a reactive process than proactive monitoring. An administrator identifies a bottleneck and then finds a way to fix it. Fixing a bottleneck is a complex process and depends on what the root cause is (and whether it is internal or external). Recoding SQL statements is one solution for fixing internal bottlenecks, which should be addressed first."
61,"Once internal bottlenecks have been resolved the administrator can start to look at external factors like CPU and storage performance that could be causing the problem. An administrator can choose between making changes to the application (or how it is used), Oracle, or the hardware configuration of the host."
61,How to Performance Tune
61,"Performance tuning an Oracle database is a very complex subject because there are so many different factors that can affect database performance. To keep things simple, we’re going to look at some basic ways you can optimize performance."
61,1. Identify High-Cost Queries
61,The first step to tuning SQL code is to identify high-cost queries that consume excessive resources. Rather than optimizing every line of code it is more efficient to focus on the most widely-used SQL statements and have the largest database / I/O footprint.
61,"One easy way to identify high-cost queries is to use Oracle database monitoring tools (we look at some of these platforms in more detail further below). One useful tool is Oracle SQL Analyze, which can identify resource-intensive SQL statements. Tuning these statements will give you the greatest return on your time investment."
61,2. Minimize the workload (Use Indexes!)
61,You can make the same query in many different ways so it is advantageous to write code that minimizes the workload as much as possible. If you only need a snapshot of data from a table it makes no sense processing thousands of rows you don’t need (all you’re doing is wasting system resources!) A full table scan takes up more database resources and I/O.
61,To eliminate the stress of sustaining a large workload you can use indexes to access small sets of rows rather than processing the entire database at once. Use indexes in those scenarios where a column is regularly queried.
61,3. Use Stateful Connections with Applications
61,Sometimes the cause of poor performance doesn’t come from code but because the connection keeps dropping between the application and the database. If your application isn’t configured correctly then it could form a connect to the database to access a table and then drop the connection once it has the information it needs.
61,"Dropping the connection after accessing the table is terrible for performance. Instead, try to keep a stateful connection so that the application stays connected to the database at all times. Maintaining the connection will stop system resources from being wasted each time the application interacts with the database."
61,4. Collect and Store Optimizer Statistics
61,Optimizer statistics are data that describe a database and its objects. These statistics are used by the database to choose the best execution plan for SQL statements. Regularly collecting and storing optimizer statistics on database objects is essential for maintaining efficiency.
61,"Collecting optimizer statistics makes sure that the database has accurate information on table contents. If the data is inaccurate then the database can choose a poor execution plan, which will affect the end-user experience. Oracle databases can automatically collect optimizer statistics or you can do so manually with the DBMS_STATS package."
61,Tools for Monitoring Oracle DB Query Performance
61,Using a software agent to monitor SQL query performance is the most effective way to manage query performance. Database performance monitoring tools can help to identify poorly performing SQL code and enable you to pinpoint the root cause of performance issues. The user can view query performance on a dashboard so they don’t need to search for information manually.
61,SolarWinds Database Performance Analyzer (FREE TRIAL)
61,SolarWinds Database Performance Analyzer is a tool that can monitor the performance of SQL queries in real-time. You can view a table of the Top SQL Statements in your database to see the highest impact SQL statements. Viewing the top statements allows you to focus your remediation efforts on those statements that have the greatest impact on performance.
61,Why do we recommend it?
61,The SolarWinds Database Performance Analyzer provides cloud-based tracking of activities and issues on Oracle databases on your own servers and on cloud platforms. The system uses AI to predict when performance issues are gathering and issue warnings for action before problems become noticeable to users.
61,Who is it recommended for?
61,"This SaaS package is able to manage and monitor a list of DBMS’s, not just Oracle. It can also watch over database instances from SQL Server, MySQL, MariaDB, PostgreSQL, and MongoDB as well as other relational and NoSQL systems. As it only monitors databases and their hosts, the tool is aimed at large businesses."
61,Pros:
61,Highly intuitive DB management system tailored for medium to large size database implementations
61,"Monitors in real-time, offering a number of alert and notification options that can integrate into popular helpdesk solutions"
61,"Threshold monitoring helps keep teams proactive, and fix issues before they impact performance"
61,Dashboards are highly customizable and can be tailored to individuals or teams
61,Built-in query analysis helps DBAs build more efficient queries
61,Leverages machine learning to identify performance bottlenecks
61,Cons:
61,Would prefer a longer trial period
61,"There is also a blocking analysis feature, which allows you to view the blocking hierarchy of the database and to view the total wait caused. Being able to see those queries that are obscuring the routes of other queries tells you where to make changes to improve performance. You can download a free trial."
61,SolarWinds Database Performance Analyzer
61,Start 14-day FREE Trial
61,dbForge Studio for Oracle (Oracle SQL Profiler)
61,"dbForge Studio for Oracle is an integrated development environment (IDE) that comes with Oracle SQL Profiler. With the profiler, you can identify queries with the highest duration. You can also view query execution plans and sessions statistics for additional information. Any changes you make will be saved so you can revert to earlier versions if you make a mistake."
61,Why do we recommend it?
61,dbForge Studio for Oracle is an on-device package that presents a view of database objects and allows queries to be tested and supports the development of code to run on your Oracle instances. This system can also be used to test database structures rather than for live monitoring.
61,"In the Session Statistics tab, you can compare the results of new queries against older versions. Differences are highlighted in red and green so you can easily tell the performance impact. Session stats you can monitor include, CPU used by this session and sorts (rows). The Session Statistics tab allows you to make sure that your changes are actually improving performance!"
61,Who is it recommended for?
61,"This tool runs on Windows, macOS, and Linux. It isn’t a networked package but it can access databases across a network. This gives the tool limited usage within a business because you have to install the software on each user’s workstation. The system is useful for DBAs and developers. It doesn’t provide database monitoring."
61,Pros:
61,Offers additional features and functionality beyond Management Studio without unnecessary tools and options
61,Excellent overview into the health of multiple SQL server databases
61,Can rewrite queries and profile data from directly inside the tool
61,Cons:
61,Only runs on Windows
61,Requires Microsoft SQL Server Management Studio
61,Tune with a Goal in Mind!
61,Having clear expectations with a specific goal in mind is critical for maintaining your database and tuning the system. Performance tuning is easier when you know what you’re trying to achieve.
61,"For example, if you want to minimize the response time of an application so that it completes queries in 1-3 seconds then you can take action to tune for that scenario. That means diagnosing bottlenecks and performance issues that slow response time down."
61,"Being able to refer back to goals will also help when you’re using a database analyzer. You’ll be able to monitor disk, CPU, and network usage to identify how performance is affected. The better you understand your goals, the better you know how to tune your resources."
61,Oracle performance tuning FAQs
61,"How can I improve my Oracle query performance?Indexing improves data retrieval speed. However, don’t over-index because these indexes could conflict and actually slow down results being returned. Partitioning tables improves data access speed because it reduces the number of records that need to be scanned in order to return the results for a query. Partitioning takes all indexes currently on the table and duplicates them automatically so that they are automatically applied to each partition."
61,"What is SQL performance tuning?SQL performance tuning is the process of ordering joins in an SQL query so that they extract records at maximum speed. There are a number of methods that can be used to improve SQL queries, such as removing outer joins and ordering lines in the WHERE clause so that as many records as possible are filtered out from the first table accessed by the query, thus reducing the amount of sorting that needs to be undertaken in the other tables accessed by the query."
61,"What is Oracle bottleneck?A bottleneck is a point in the process of access to or response from a database that has limited capacity and therefore slows disown access to results because only a limited number of queries can be served. This is different from resource locks, which occur when one process holds a resource and will not release it until another resource is available."
61,What's in this article?What is Oracle Performance Tuning?Monitoring Performance in Oracle Databases: Response Time and ThroughputThe Two Types of Tuning: Proactive Monitoring and Bottleneck EliminationHow to Performance TuneTools for Monitoring Oracle DB Query PerformanceTune with a Goal in Mind!Oracle performance tuning FAQs
61,Comments
61,Leave a Reply Cancel replyCommentName
61,Email
61,This site uses Akismet to reduce spam. Learn how your comment data is processed.
61,Search
61,Search
61,Twitter icon
61,Home
61,Blog
61,Our Authors
61,Privacy policy
61,Cookies Policy
61,Terms of use
61,Disclosure
61,About Comparitech
61,Contact Us
61,Accessibility
61,© 2024 Comparitech Limited. All rights reserved.
61,"Comparitech.com is owned and operated by Comparitech Limited, a registered company in England and Wales (Company No. 09962280), Suite 3 Falcon Court Business Centre, College Road, Maidstone, Kent, ME15 6TF, United Kingdom. Telephone +44(0)333 577 0163"
61,SolarWinds Top 5 Essential IT Tools
61,Manage and Monitor Your Network in One Simple Bundle
61,Help desk ticketing and asset management software
61,Remote support and systems management solution
61,Network configuration and automation software
61,Safe file transfer management solution
61,Network management and troubleshooting software
61,DOWNLOAD FREE TRIAL
61,Fully functional for 14 days
61,Comparitech uses cookies. More info.
61,Close
62,Behind the scenes: Introducing OpenShift Virtualization Performance and Scale
62,Skip to contentFeatured linksSupportConsoleDevelopersStart a trial
62,"All Red HatFor customersCustomer supportDocumentationSupport casesSubscription managementRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesTraining and certification For developersHybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesAnsibleFor system administratorsFor architectsRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutExplore morePlatform productsRed Hat Enterprise LinuxA flexible, stable operating system to support hybrid cloud innovation."
62,"Red Hat OpenShiftA container platform to build, modernize, and deploy applications at scale."
62,Red Hat Ansible Automation PlatformA foundation for implementing enterprise-wide automation.
62,Try & buyStart a trialAssess a product with a no-cost trial.
62,Buy onlineBuy select products and services in the Red Hat Store.
62,"Integrate with major cloud providersBuy Red Hat solutions using committed spend from providers, including:"
62,"Featured cloud servicesBuild, deploy, and scale applications quickly. We’ll manage the rest."
62,Red Hat OpenShift Service on AWS
62,Red Hat OpenShift AI
62,Microsoft Azure Red Hat OpenShift
62,See all cloud services
62,See all products
62,By category
62,Application platform
62,Artificial intelligence
62,Edge computing
62,IT automation
62,Linux standardization
62,By organization type
62,Automotive
62,Financial services
62,Healthcare
62,Industrial sector
62,Media and entertainment
62,Public sector
62,Telecommunications
62,By customer
62,British Army
62,Edenor
62,HCA Healthcare
62,Macquarie Bank
62,Tata Consultancy Services
62,UPS
62,Search all success stories
62,Explore solutions
62,Services
62,Consulting
62,Open Innovation Labs
62,Technical Account Management
62,Training & certification
62,All courses and exams
62,All certifications
62,Verify a certification
62,Skills assessment
62,Learning subscription
62,Learning community
62,Red Hat Academy
62,FAQs
62,Connect with learning experts
62,Featured
62,Red Hat System Administration I (RH124)
62,Red Hat OpenShift Administration I (DO280)
62,Red Hat Certified Engineer (RHCE)
62,Explore services
62,Topics
62,Application modernization
62,Automation
62,Cloud computing
62,Cloud-native applications
62,Containers
62,DevOps
62,Edge computing
62,Linux
62,Virtualization
62,See all topics
62,Articles
62,What are cloud services?
62,What is edge computing?
62,What is hybrid cloud?
62,Why build a Red Hat cloud?
62,Cloud vs. edge
62,Red Hat OpenShift vs. Kubernetes
62,Learning Ansible basics
62,What is Linux?
62,More to explore
62,Blog
62,Customer success stories
62,Events and webinars
62,Newsroom
62,Podcasts and video series
62,Resource library
62,Training and certification
62,Explore resources
62,For customers
62,Our partners
62,Red Hat Ecosystem Catalog
62,Find a partner
62,For partners
62,Partner Connect
62,Become a partner
62,Training
62,Support
62,Access the partner portal
62,About us
62,Our company
62,How we work
62,Our social impact
62,Development model
62,Subscription model
62,Product support
62,Open source
62,Open source commitments
62,How we contribute
62,Red Hat on GitHub
62,Company details
62,Analyst relations
62,Blog
62,Locations
62,Newsroom
62,Communities
62,Ansible
62,For system administrators
62,For architects
62,Customer advocacy
62,Explore Red Hat
62,Contact us
62,"For customersCustomer supportDocumentationSupport casesSubscription managementRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesTraining and certification For developersHybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesAnsibleFor system administratorsFor architects"
62,For you
62,"NewRecommendationsAs you browse redhat.com, we'll recommend resources you may like. For now, try these.All Red Hat productsTech topicsRed Hat resourcesRed Hat SummitSupportConsoleDevelopersStart a trialContactSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol"
62,Contact us
62,English
62,Select a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutMenu
62,Search
62,For you
62,Contact us
62,English
62,Log in
62,ProductsSolutionsTraining & servicesResourcesPartnersAboutContact usSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol
62,Red Hat blog
62,Blog menu
62,Latest posts
62,By product
62,Red Hat Enterprise Linux
62,Red Hat Insights
62,Red Hat OpenShift
62,Red Hat Ansible Automation Platform
62,Red Hat OpenStack Platform
62,Red Hat Cloud Storage and Data Services
62,All products
62,By channel
62,Red Hat News
62,Red Hat Services Speak
62,Cloud native computing
62,Red Hat Security
62,Open Hybrid Cloud
62,Management and automation
62,All channels
62,Behind the scenes: Introducing OpenShift Virtualization Performance and Scale
62,"January 9, 2024Jenifer Abrams"
62,Share
62,Back to all posts
62,"Tags:Products, Topics"
62,"Red Hat OpenShift Virtualization helps to remove workload barriers by unifying virtual machine (VM) deployment and management alongside containerized applications in a cloud-native manner. As part of the larger Performance and Scale team, we have been deeply involved in the measurement and analysis of VMs running on OpenShift since the early days of the KubeVirt open source project and have helped to drive product maturity through new feature evaluation, workload tuning and scale testing. This article dives into several of our focus areas and shares additional insights into running and tuning VM workloads on OpenShift.Tuning and scaling guidesOur team contributes to tuning and scaling documentation to help customers get the most out of their VM deployments. First, we have an overall tuning guide, which you can find in this knowledge base article. This guide covers recommendations for optimizing the Virtualization control plane for high VM ""burst"" creation rates and various tuning options at both the host and VM levels to improve workload performance.Second, we published an in-depth reference architecture that includes an example OpenShift cluster, Red Hat Ceph Storage (RHCS) cluster and network tuning details. It also examines sample VM deployment and boot storm timings, I/O latency scaling performance, VM migration and parallelism and performing cluster upgrades at scale.Team focus areasThe following sections provide an overview of some of our major focus areas and details about the testing we do to characterize and improve the performance of VMs running on OpenShift. Figure 1 below illustrates our focus areas."
62,"Figure 1: OpenShift Virtualization Performance focus areasWorkload performanceWe spend a lot of time focusing on key workloads covering compute, networking and storage components to make sure we have broad coverage. This work includes gathering continuous baselines on different hardware models, updating results as newer releases come out and diving deep into various tuning options to achieve optimal performance.One key workload focus area is database performance. We typically use HammerDB as the workload driver and focus on multiple database types, including MariaDB, PostgreSQL and MSSQL, so we can understand how databases with different characteristics perform. This template provides an example HammerDB VM definition.Another major workload focus area is a high throughput in-memory database, SAP HANA, with a goal of performing within 10% of bare metal. We achieve this by applying some isolation-style tuning at both the host and VM layer, including using CPUManager, adjusting process affinity controlled by systemd, backing the VM with hugepages, and using SRIOV network attachments.To further cover storage performance, we run a set of different I/O application patterns focusing on both IOPs (input/output operations per second) and latency using the Vdbench workload. The application patterns vary the block size, I/O operation type, size and number of files and directories, and adjust the mix of reads and writes. This allows us to cover various I/O behaviors to understand different performance characteristics. We also run another common storage microbenchmark, Fio, to measure various storage profiles. We test multiple persistent storage providers, but our main focus is OpenShift Data Foundation using Block mode RADOS Block Device (RBD) volumes in VMs.We also focus on different types of microbenchmarks to assess other component performance to round out some of these more complex workloads. For networking, we typically use the uperf workload to measure both Stream and RequestResponse test configurations for various message sizes and thread counts, focusing on both the default podnetwork and other container network interface (CNI) types, such as Linux Bridge and OVN-Kubernetes additional networks. For compute tests, we use a variety of benchmarks, such as stress-ng, blackscholes, SPECjbb2005 and others, depending on the focus area.Regression testingUsing an automation framework called benchmark-runner, we continuously run workload configurations and compare the results to known baselines to catch and fix any regressions in pre-release versions of OpenShift Virtualization. Since we care about virtualization performance, we run this continuous testing framework on bare metal systems. We compare workloads with similar configurations across pods, VMs and sandboxed containers to help us understand relative performance. This automation allows us to quickly install new pre-release versions of OpenShift and the operators we focus on, including OpenShift Virtualization, OpenShift Data Foundation, Local Storage Operator and OpenShift sandboxed containers. Characterizing the performance of pre-release versions multiple times each week allows us to catch any regressions early before they are released to customers and enables us to compare performance improvements over time as we update to newer releases with improved features.We are always expanding our continuous automated workload coverage, but the current set of workloads we run regularly includes database benchmarks, compute microbenchmarks, uperf, Vdbench, Fio, and both VM ""boot storm"" and pod startup latency tests that exercise various areas of the cluster to measure how quickly a bulk number of pods or VMs can be started at once.Migration performanceOne advantage of using a shared storage provider that allows RWX access mode is that VM workloads can more seamlessly live migrate during cluster upgrades. We consistently work to improve the speed at which VMs can migrate without significant workload interruption. This involves testing and recommending migration limits and policies to provide safe default values and testing much higher limits to uncover migration component bottlenecks. We also measure the benefits of creating a dedicated migration network and analyze node-level networking and per-VM migration metrics to characterize migration progress over the network.Scaling performanceWe regularly test high-scale environments to uncover any bottlenecks and evaluate tuning options. Our scale testing involves areas ranging from OpenShift control plane scale to Virtualization control plane scale, workload I/O latency scaling, migration parallelism, DataVolume cloning and VM ""burst"" creation tuning.Throughout this testing, we've discovered various scale-related bugs that ultimately led to improvements, allowing us to push the next round of scale tests even higher. Any scale-related best practices we find along the way we document in our general Tuning and Scaling Guide.Hosted cluster performanceAn emerging focus area for us is hosted control plane and hosted cluster performance, specifically examining on-premises bare metal hosted control planes and hosted clusters on OpenShift Virtualization, which uses the KubeVirt cluster provider.Some of our initial work areas are scale testing multiple instances of etcd (see the storage recommendation in the Important section), hosted control plane scale testing with heavy API workload, and hosted workload performance when managing hosted control plane clusters on OpenShift Virtualization. Check out our hosted cluster sizing guidance to see one of the major outcomes of this recent work.What's nextKeep an eye out for upcoming posts covering these performance and scale areas in more detail, including a deeper dive into hosted cluster sizing guidance methodology and in-depth VM migration tuning recommendations.In the meantime, we will keep measuring and analyzing the performance of VMs on OpenShift, pushing new scale boundaries, and focusing on catching and fixing any regressions before releases reach the hands of our customers!Learn more about Red Hat OpenShift Virtualization"
62,About the author
62,Jenifer Abrams
62,Principal Software Engineer
62,"Jenifer joined Red Hat in 2018 and leads the OpenShift Virtualization Performance team. Previously, she spent a decade working at IBM in the Linux Technology Center focused on Linux Performance."
62,Read full bioIcon-Red_Hat-Directional-A-Black-RGB
62,Enter keywords here to search blogs
62,UI_Icon-Red_Hat-Close-A-Black-RGB
62,Search
62,Subscribe to the feed
62,Related posts
62,Integrating Enterprise Environments with SaaS: A CRM Case Study with Red Hat Application Foundations
62,Watch now: 3 top episodes on Red Hat TV
62,Zero Trust MLOps with OpenShift Platform Plus
62,"LinkedInYouTubeFacebookTwitterProductsRed Hat Enterprise LinuxRed Hat OpenShiftRed Hat Ansible Automation PlatformCloud servicesSee all productsToolsTraining and certificationMy accountDeveloper resourcesCustomer supportRed Hat value calculatorRed Hat Ecosystem CatalogFind a partnerTry, buy, & sellProduct trial centerRed Hat MarketplaceRed Hat StoreBuy online (Japan)ConsoleCommunicateContact salesContact customer serviceContact trainingSocialAbout Red HatWe’re the world’s leading provider of enterprise open source solutions—including Linux, cloud, container, and Kubernetes. We deliver hardened solutions that make it easier for enterprises to work across platforms and environments, from the core datacenter to the network edge.Select a languageEnglish简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed Hat legal and privacy linksAbout Red HatJobsEventsLocationsContact Red HatRed Hat BlogDiversity, equity, and inclusionCool Stuff StoreRed Hat SummitRed Hat legal and privacy linksPrivacy statementTerms of useAll policies and guidelinesDigital accessibility"
63,"Connect to a Custom SQL Query - TableauTableau Desktop and Web Authoring HelpConnect to a Custom SQL QueryApplies to: Tableau Cloud, Tableau Desktop, Tableau Server"
63,"Note: Using custom SQL can affect performance of a workbook. Working with your DBA will ensure the best possible custom SQL query. In order to perform the operations necessary for building views in Tableau Desktop, Tableau must be able to control WHERE, GROUP BY and other SQL clauses. Because a custom SQL query may contain those elements, and Tableau cannot inject them into the existing SQL, Tableau must wrap the custom SQL statement within a select statement. When a custom SQL connection is performing slowly, or produces an error, it is often the result of the custom SQL wrapping that Tableau Desktop performs."
63,"For most databases, you can connect"
63,"to a specific query rather than the entire data set. Because databases have slightly different SQL syntax from each other, the custom SQL you use to connect to one database might be different from the custom SQL you might use to connect to another. However, using custom SQL can be useful when you know exactly the information you need and understand how to write SQL queries."
63,"Though there are several common reasons why you might use custom SQL, you can use custom SQL to union your data across tables, recast fields to perform cross-database joins, restructure or reduce the size of your data for analysis, etc."
63,"For Excel and text file data sources, this option is available only in workbooks that were created before Tableau Desktop 8.2 or when using Tableau Desktop on Windows with the legacy connection. To connect to Excel or text files using the legacy connection, connect to the file, and in the Open dialog box, click the Open drop-down menu, and then select Open with Legacy Connection."
63,"NOTE: Beginning with Tableau 2020.2, legacy Excel and Text connections are no longer supported. See the Legacy Connection Alternatives document in Tableau Community for alternatives to using the legacy connection."
63,Connect to a custom SQL query
63,"After connecting to your data, double-click the New Custom SQL option on the Data Source page."
63,Type or paste the query into the text box. The query must be a single SELECT* statement.
63,"When finished, click OK."
63,"When you click OK, the query runs and the custom SQL query table appears in the logical layer of the canvas. Only relevant fields"
63,from the custom SQL query display in the data grid on the Data Source page.
63,"For more information about the logical and physical layers of the canvas, see The Tableau Data Model."
63,Examples of custom SQL queries
63,Combine your tables vertically (union)
63,"If you need to append data to each other, you can use the union option in the physical layer of the canvas in Tableau. In some cases your database does not support this option, so you can use custom SQL instead."
63,"For example, suppose you have the following two tables: November and December."
63,November
63,December
63,"You can use the following custom SQL query to append the second table, December, to the first table, November:"
63,SELECT * FROM November UNION ALL SELECT * FROM December
63,The result of the query looks like this in the data grid:
63,"For more information about the union option, see Union Your Data."
63,Change the data type of a field to do a cross-database join
63,"When you want to perform a join between two tables in the physical layer of the canvas, the data type of the fields you join on must be the same. In cases when the data type of the fields are not the same, you can use custom SQL to change the data type (cast) the field before performing the join."
63,"For example, suppose you want to join two tables, Main and Sub, using the Root and ID fields, respectively. The Root field is a number type and the ID field is a string type. You can use the following custom SQL query to change the data type of Root from a number to a string so that you can join the Main and Sub tables using the Root and ID fields."
63,SELECT
63,[Main].[Root] AS [Root_Number]CAST([Main].[Root] AS INT] AS [Root_String]
63,FROM [Main]
63,The result of this query shows the original Root field and the Root field cast as a string.
63,"For more information about joins and cross-database joins, see Join Your Data."
63,Reduce the size of your data
63,"When working with very large data sets, sometimes you can save time while working with your data if you reduce its size first."
63,"For example, suppose you have a large table called FischerIris. You can use the following custom SQL query to retrieve the specified columns and records thereby reducing the size of the data set that you connect to from Tableau."
63,SELECT
63,"[FischerIris].[Species] AS [Species],"
63,"[FischerIris].[Width] AS [Petal Width],"
63,COUNT([FischerIris].[ID]) AS [Num of Species]
63,FROM [FischerIris]
63,WHERE [FischerIris].[Organ] = 'Petal'
63,AND [FischerIris].[Width] > 15.0000
63,"GROUP BY [FischerIris].[Species], [FischerIris].[Width]"
63,Restructure your data (pivot)
63,"In some cases, you might be working with a table that needs to be restructured before analysis. Though this type of task can be done in the physical layer of the canvas in Tableau by using options like pivot, your database might not support it. In this case, you can use custom SQL instead."
63,"For example, suppose you have the following table:"
63,"To change its structure and optimize your data for analysis in Tableau, you can use the following custom SQL query:"
63,"SELECT Table1.Season ID AS [Season ID],"
63,"Table1.Items - Don't like AS [Quantity],"
63,"""Don't Like"" AS [Reason]"
63,FROM Table1
63,UNION ALL
63,"SELECT Table1.Season ID AS [Season ID],"
63,"Table.Items - Defective AS [Quantity],"
63,"""Defective"" AS [Reason]"
63,FROM Table1
63,UNION ALL
63,"SELECT Table1.Season ID AS [Season ID],"
63,"Table1.Items - Too big AS [Quantity],"
63,"""Too Big"" AS [Reason]"
63,FROM Table1
63,UNION ALL
63,"SELECT Table1.Season ID AS Season ID,"
63,Table1.Items - Too small AS [Quantity]
63,"""Too Small"" AS [Reason]"
63,FROM Table1
63,The result of the query looks like this in the data grid:
63,"For more information about the pivot option, see Pivot Data from Columns to Rows."
63,Combine (join) and aggregate your data
63,"If you need to combine tables and aggregate your data, you can use both a join and default aggregation type options in the physical layer of the canvas in Tableau. In some cases you might need to use custom SQL instead."
63,"For example, suppose you have the following two tables: Orders and Vendors."
63,Orders
63,Vendors
63,You can use the following custom SQL query to find a count on the number of orders and do a left join on the Orders and Vendors tables:
63,"SELECT Vendors.Name,COUNT(Orders.Order) AS Number Of Orders"
63,FROM Orders
63,LEFT JOIN Vendors
63,ON Orders.VendorID=Vendors.VendorID
63,GROUP BY Name;
63,The result of the query looks like this:
63,"For more information about joins, see Join Your Data."
63,Errors when duplicate columns are referenced
63,If your custom SQL query references
63,"duplicate columns, you may get errors when trying to use one of"
63,the columns in your analysis in Tableau. This will happen even if the query is valid.
63,"For example, consider the following query:"
63,SELECT * FROM
63,"authors, titleauthor WHERE authors.au_id = titleauthor.au_id"
63,The
63,"query is valid, but the au_id field is ambiguous"
63,because in this case it exists in both the “authors” table and the “titleauthor”
63,table. Tableau will connect to the query but you will get an error anytime
63,you try to use the au_id field. This is because
63,Tableau doesn’t know which table you are referring to.
63,Note: It is a best practice to define column aliases with an AS clause whenever possible in a Custom SQL Query. This is because each database has its own rules when it comes to automatically generating a column name whenever an alias is not used.
63,Edit a custom SQL query
63,To edit a custom SQL query
63,"On the data source page, in the canvas, double-click the custom SQL query in the logical layer."
63,Hover over the custom SQL table in the physical layer until the arrow displays.
63,Click the arrow and then select Edit Custom SQL Query.
63,"In the dialog box, edit the custom SQL query."
63,To change a custom SQL query name
63,"When you drag a custom SQL query to the logical layer of the canvas, Tableau gives it a default name: Custom SQL Query, Custom SQL Query1, and so on. You can change the default name to something more meaningful."
63,"On the data source page, in the logical layer of the canvas, select the drop-down arrow in the custom SQL query table and select Rename."
63,Enter the name you want to use for your custom SQL query.
63,Use parameters in a custom SQL query
63,"You can use parameters in a custom SQL query statement to replace a constant value with a dynamic value. You can then update the parameter in the workbook to modify the connection. For example, you may connect to a custom SQL query that provides web traffic data for a particular page that is specified by a pageID. Instead of using a constant value for the pageID value in the SQL query, you can insert a parameter. Then after finishing the connection, you can show a parameter control in the workbook. Use the parameter control to switch out the pageID and pull in data for each page of interest without having to edit or duplicate the connection."
63,"In Tableau Desktop, you can create a parameter directly from the Custom SQL dialog box or use any parameters that are part of the workbook. If you create a new parameter, it becomes available for use in the workbook just like any other parameter. See Create Parameters to learn more."
63,"For web authoring (in Tableau Cloud or Tableau Server), you can use an existing parameter published from Tableau Desktop. You cannot create a new parameter in web authoring."
63,To add a parameter to a custom SQL query
63,"On the data source page, in the canvas, hover over the table until the edit icon displays, and then click the edit button."
63,"At the bottom of the dialog box, click Insert Parameter."
63,"Select a constant value in the SQL statement and then, from the"
63,"Insert Parameter drop-down menu select the parameter you want to use instead. If you have not created a parameter yet, select Create a new parameter. Follow the instructions in Create Parameters to create a parameter."
63,Note: Parameters can only replace literal values. They cannot replace expressions or identifiers such as table names.
63,"In the example below, the custom SQL query returns all orders that are marked as Urgent priority. In the custom SQL statement, the order priority is the constant value. If you want to change the connection to see the High priority orders, you would have to edit the data source."
63,"Instead of creating and maintaining many variations of the same query, you can replace the constant order priority value with a parameter. The parameter should contain all of the possible values for Order Priority."
63,"After you create a parameter, you can insert it into the SQL statement to replace the constant value."
63,"After you finish editing the connection, the new parameter is listed in the Parameters area at the bottom of the Data pane and the parameter control displays on the right side of the view. As you select different values, the connection updates."
63,"Note: If you are using an extract, you must refresh the extract in order to reflect changes to the parameter. Publishing a data source that uses Custom SQL parameters includes the parameters. The parameters are transferred to any workbooks that connect to the data source."
63,Tableau Catalog support for custom SQL
63,Support for custom SQL in Tableau Catalog depends on the custom SQL query.
63,"Tableau Catalog is available as part of the Data Management offering for Tableau Server and Tableau Cloud. For more information about Tableau Catalog, see ""About Tableau Catalog"" in the Tableau Server or Tableau Cloud Help."
63,Supported queries
63,"Catalog supports custom SQL queries that meet the ANSI SQL-2003 standard, with three known exceptions:"
63,Time zone expressions
63,Multiset expressions
63,Tableau parameters
63,"Starting in 2021.4, Tableau Catalog also supports use of the Transact-SQL (T-SQL) dialect in Custom SQL, with the following exceptions:"
63,Hints
63,FOR clauses
63,"OPENROWSET, OPENXML, and OPENJSON functions"
63,ODBC scalar functions
63,FOR SYSTEM_TIME
63,TABLESAMPLE
63,MATCH expression
63,CONTAINS expression
63,FREETEXT expression
63,"Starting in Tableau Cloud October 2023 and Tableau Server 2023.3, Tableau Catalog also offers support for custom SQL queries that use PostgreSQL, with the following exceptions:"
63,XML function
63,JSON functions and operators
63,Supported features and functions
63,"Catalog supports the following additional functionality for data sources, workbooks, and flows with connections that use the MySQL or PostgreSQL drivers, for example, Amazon Aurora for MySQL, Amazon RedShift, Pivotal Greenplum Database, MemSQL, Denodo, and others."
63,MySQL GROUP_CONCAT function
63,PostgreSQL arrays
63,PostgreSQL EXTRACT() function
63,"Other custom SQL scenarios and functionality might work, but Tableau doesn't specifically test for or support them."
63,Supported lineage
63,"When an asset uses custom SQL, a message with a Show Custom SQL Query button appears on the Lineage tab of the asset page. Click the button to see the custom SQL used in the connection. Then, if you would like to copy the custom SQL to your clipboard, click Copy."
63,"Some types of custom SQL can cause the upstream lineage to be incomplete. When this happens, a message appears with that information."
63,"Field details cards might not contain links to connected columns, or might not show any connected columns at all."
63,"Column details cards might not contain links to fields that use the column, or might not show any fields at all."
63,"If you are examining a table’s lineage, note that Catalog doesn't support showing column information in the lineage for table metadata gathered using custom SQL. However, if other assets use the same table and don’t use custom SQL, Tableau Catalog might be able to display information about the columns that it has discovered through these other assets."
63,"In the following screenshot, the factAccountOpportunityByQuarter table was indexed because it’s used by a data source. However, because it’s referenced by a custom SQL query, the column information isn't available."
63,"In a case where more than one data source, workbook, or flow uses a table, any of the assets downstream from that table that uses a custom SQL query are excluded when column-level filters are applied. As a result, fewer downstream assets show in the lineage than are actually used."
63,"For more information about using the lineage, see ""Use Lineage for Impact Analysis"" in the Tableau Server(Link opens in a new window) or Tableau Cloud(Link opens in a new window) Help."
63,See Also
63,Use Custom SQL and RAWSQL to perform advanced spatial analysis(Link opens in a new window)
63,Back to topThanks for your feedback!Your feedback has been successfully submitted. Thank you!LegalPrivacyCookie Preferences
65,Best Practices for Migrating MariaDB 10.2 to PolarDB for MySQL 5.7 - Alibaba Cloud Community
65,Community
65,Blog
65,Events
65,Webinars
65,Tutorials
65,Forum
65,Blog
65,Events
65,Webinars
65,Tutorials
65,Forum
65,Create Account
65,Log In
65,Community
65,Blog
65,Best Practices for Migrating MariaDB 10.2 to PolarDB for MySQL 5.7
65,Best Practices for Migrating MariaDB 10.2 to PolarDB for MySQL 5.7
65,Morningking
65,"September 26, 2023"
65,"1,272"
65,This article describes the common problems encountered during the migration process from MariaDB 10.2 to PolarDB for MySQL 5.7
65,By Daoke
65,Upgrade Overview
65,Why Did You Choose to Upgrade to PolarDB for MySQL 5.7?
65,"PolarDB for MySQL 5.7 is a next-generation distributed relational cloud-native database developed by Alibaba Cloud. It is 100% compatible with MySQL and is based on MySQL 5.7.28. It incorporates separation between storage and computation, shared storage (up to 100 TB), and physical replication mechanisms. It also offers significant architecture enhancements and kernel capabilities, providing more flexible technical solutions and achieving powerful performance improvements. The main benefits of PolarDB for MySQL 5.7 include:"
65,"Physical replication: PolarDB implements physical replication based on Redolog instead of logical replication based on binary log for the data flow between the primary database and read-only instances. This eliminates the need for XA transactions with binary logs, resulting in shorter transaction execution paths and reduced I/O overhead. In the secondary database, physical replication can be executed in multiple threads, ensuring data correctness. The overhead is much lower compared to traditional multi-replica replication."
65,"Database storage engine that integrates software and hardware: PolarDB utilizes advanced hardware technologies, such as Optane memory cards using 3DXpoint storage media, NVMe SSDs, upgraded 100G RoCE RDMA network, and Alibaba's advanced Aliflash V5 SMART-SSD technology. It optimizes hardware and software as a whole for new hardware architectures, optimizing the I/O chain across all layers of the software stack. It is the first storage engine among cloud vendors to be based on these advanced hardware integrations."
65,"Serverless: Serverless databases allow database cluster resources to dynamically scale up and down based on customer business loads, eliminating the need for complex resource evaluation and operations and maintenance."
65,"DDL enhancement and optimization: PolarDB supports parallel DDL, quick addition of fields, DDL read-ahead, multi-way merging and sorting, and DDL asynchronous I/O. These enhancements improve the performance, stability, and ease of use of DDL operations."
65,"Transaction system enhancement: PolarTrans is a new transaction system that utilizes commit timestamp (CTS) technology to optimize high-concurrency online transaction scenarios. It improves the read and write performance of databases and integrates with RDMA (Remote Direct Memory Access) technology to launch the SCC (Super Computing Cluster) function, ensuring strong consistency within high-performance clusters. SCC guarantees strict read-after-write consistency on the primary node and all read-only nodes in a cluster. By offloading strictly consistent read query workloads to read-only nodes, SCC reduces the load on the primary node and significantly improves the throughput of clusters in OLTP scenarios."
65,"Global database: A global database network (GDN) consists of multiple PolarDB for MySQL clusters distributed in multiple regions within a country. Data is synchronized across all clusters in a GDN. Each cluster in the GDN provides read services (write services are forwarded to the primary cluster for processing) and geo-disaster recovery capability, meeting application scenarios such as active geo-redundancy and geo-disaster recovery."
65,"High-concurrency optimization: This includes Concurrency Control (CCL) rules based on SQL statements, Inventory Hint for quick transaction commits and rollbacks, Statement Queue mechanism to minimize conflict overhead, hot row performance optimization, and Thread Pool optimization."
65,"Additionally, data can be migrated to PolarDB for MySQL 8.0 with a single click to leverage more enterprise-level capabilities. For more information, please refer to the Release Logs."
65,Pre-inspection
65,"During the migration process of MariaDB 10.2 to PolarDB for MySQL 5.7, the main problems often encountered are related to performance, syntax compatibility, and support for peripheral components. Performance problems with queries are generally caused by changes in the execution plan resulting from the optimizer upgrade. These problems require targeted performance optimization for statements with low performance, but they do not cause business errors or code rewriting problems. Therefore, this article will not discuss such issues."
65,"This article focuses on real compatibility problems, which require corresponding code updates or changes to environment configuration during database upgrades. The main causes of compatibility problems are syntax changes, feature updates, and removals after version upgrades. A pre-inspection provides a brief list to help users better understand the problems that need to be noticed during the upgrade process. If you encounter the following problems, you can refer to the Version Upgrade Details section for operation and inspection."
65,Ensure that the storage engine is not using a storage engine that is separately supported by MariaDB.
65,"Ensure that dynamic column functions, JSON, regular expression functions, and Window functions are not used in the application program."
65,Ensure that table column types do not contain types such as JSON and INET.
65,"Ensure that the keywords do not conflict with the reserved keywords. For more information, see the official documentation Keywords and Reserved Words."
65,"Ensure that the BLOB and TEXT types in the table definition do not have default values of definitions and expressions, and the Sequences and System-versioned attributes are not used."
65,"Ensure that the charsets are supported by MySQL 5.7. For more information, see the official documentation Charsets and Collations."
65,Version Migration Details
65,Storage Engines
65,"In addition to the standard InnoDB, MyISAM, BLACKHOLE, CSV, MEMORY, ARCHIVE, and MERGE, the MariaDB 10.2 supports MyRocks, Aria, TokuDB, CONNECT, SEQUENCE, SphinxSE, Spider, FederatedX, and OQGRAPH. Therefore, you need to convert the engine not supported by MySQL to another standard engine supported by InnoDB or PolarDB for MySQL 5.7 before migration."
65,SELECT DISTINCT ENGINE
65,FROM INFORMATION_SCHEMA.TABLES
65,"WHERE TABLE_SCHEMA NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys');"
65,"Find the corresponding and specific engine. For example, if there is connect or federatedx, execute the engine."
65,"SELECT COUNT(*) as '# TABLES',"
65,"CONCAT(ROUND(sum(data_length) / ( 1024 * 1024 * 1024 ), 2), 'G') DATA,"
65,"CONCAT(ROUND(sum(index_length) / ( 1024 * 1024 * 1024 ), 2), 'G') INDEXES,"
65,"CONCAT(sum(ROUND(( data_length + index_length ) / ( 1024 * 1024 * 1024 ), 2)), 'G') 'TOTAL SIZE', ENGINE"
65,FROM information_schema.TABLES
65,WHERE TABLE_SCHEMA
65,"NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys')"
65,GROUP BY engine;
65,+----------+-------+---------+------------+--------+
65,| # TABLES | DATA
65,| INDEXES | TOTAL SIZE | ENGINE |
65,+----------+-------+---------+------------+--------+
65,1 | 0.02G | 0.01G
65,| 0.00G
65,| Aria
65,5 | 3.00G | 2.00G
65,| 0.00G
65,| InnoDB |
65,1 | 1.00G | 1.20G
65,| 0.00G
65,| MyISAM |
65,+----------+-------+---------+------------+--------+
65,3 rows in set (0.002 sec)
65,Change the engine not supported to the InnoDB engine or another standard engine:
65,<mysql> ALTER TABLE part ENGINE = INNODB;
65,"Query OK, 0 rows affected (0.09 sec)"
65,Functions
65,"Some functions in MariaDB are not supported by the PolarDB for MySQL 5.7. You need to check whether such functions are contained in your application. If such a function exists, make a rewrite solution, such as JSON_DETAILED. You can only use JSON_PRETTY instead of JSON_DETAILED when you upgrade to PolarDB for MySQL 8.0."
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,COLUMN_ADD
65,COLUMN_CHECK
65,COLUMN_CREATE
65,COLUMN_DELETE
65,COLUMN_EXISTS
65,COLUMN_GET
65,COLUMN_JSON
65,COLUMN_LIST
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,JSON_COMPACT
65,JSON_DETAILED
65,JSON_EXISTS
65,JSON_LOOSE
65,JSON_MERGE_PATCH
65,JSON_MERGE_PRESERVE
65,JSON_QUERY
65,JSON_VALUE
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,JSON Function Official Document
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,REGEXP_INSTR
65,REGEXP_REPLACE
65,REGEXP_SUBSTR
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,Regular Expression Functions and Operators
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,CUME_DIST
65,DENSE_RANK
65,LAG
65,LAST_VALUE
65,LEAD
65,NTH_VALUE
65,NTILE
65,PERCENT_RANK
65,RANK
65,ROW_NUMBER
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,MySQL official WINDOW function
65,Category
65,Functions
65,Description
65,Dynamic columns
65,COLUMN_ADDCOLUMN_CHECKCOLUMN_CREATECOLUMN_DELETECOLUMN_EXISTSCOLUMN_GETCOLUMN_JSONCOLUMN_LIST
65,JSON
65,JSON_COMPACTJSON_DETAILEDJSON_EXISTSJSON_LOOSEJSON_MERGE_PATCHJSON_MERGE_PRESERVEJSON_QUERYJSON_VALUE
65,"For more information, see JSON Function Reference."
65,Regular Expressions
65,REGEXP_INSTRREGEXP_REPLACEREGEXP_SUBSTR
65,"For more information, see Regular Expression Functions and Operators."
65,Window Functions
65,CUME_DISTDENSE_RANKLAGLAST_VALUELEADNTH_VALUENTILEPERCENT_RANKRANKROW_NUMBER
65,"Only MySQL 8.0 has correspondent functions. For more information, see Window Functions."
65,"For more information, see the official description of MariaDB: Function Differences Between MariaDB 10.2 and MySQL 5.7"
65,System Variables
65,"MariaDB 10.2 and PolarDB MySQL 5.7 also have many differences in system variables. You can use SHOW VARIABLES to compare the differences between the two systems. For more information, see MariaDB official documentation System Variable Differences Between MariaDB 10.2 and MySQL 5.7"
65,SHOW VARIABLES;
65,"NOTES: The unit of max_statement_time is milliseconds, which is different from that of MariaDB, whose unit is seconds."
65,Encryption Compatibility
65,"The encryption method of MariaDB 10.2 implemented in InnoDB is different from that of PolarDB for MySQL 5.7. The encryption tables must be processed separately. For more information, see InnoDB encryption."
65,Keyword Compatibility
65,You must view the MySQL Keyword List to ensure that reserved keywords do not conflict with the keywords in the list.
65,Type Compatibility
65,"Some special types of MariaDB, such as JSON, INET, and MySQL, are incompatible. You can use the following statements to view the types:"
65,SELECT
65,"DATA_TYPE , count(*) TOT"
65,FROM information_schema.COLUMNS
65,WHERE TABLE_SCHEMA NOT
65,"IN ('mysql', 'sys', 'information_schema', 'performance_schema')"
65,GROUP BY 1;
65,+-----------+-----+
65,| DATA_TYPE | TOT |
65,+-----------+-----+
65,| bigint
65,14 |
65,| datetime
65,1 |
65,| inet6
65,1 |
65,| int
65,10 |
65,| longtext
65,3 |
65,| tinyint
65,2 |
65,+-----------+-----+
65,6 rows in set (0.001 sec)
65,INET6 type
65,"MariaDB INET6 is not supported in MySQL. Therefore, we recommend that you use the VARBINARY(16) type to store IPv6 values."
65,Table Types Incompatibility
65,Sequences attribute
65,"PolarDB for MySQL does not support Sequences. Therefore, you must use the following SQL statements to query:"
65,"SELECT COUNT(*), TABLE_TYPE FROM information_schema.TABLES GROUP BY table_type;"
65,+----------+------------------+
65,| COUNT(*) | TABLE_TYPE
65,+----------+------------------+
65,117 | BASE TABLE
65,2 | SEQUENCE
65,1 | SYSTEM VERSIONED |
65,79 | SYSTEM VIEW
65,101 | VIEW
65,+----------+------------------+
65,5 rows in set (0.0250 sec)
65,WITH seqlist (a) AS (
65,"SELECT CONCAT('%`',TABLE_SCHEMA,'`.`', TABLE_NAME,'`%') a"
65,FROM information_schema.TABLES
65,"WHERE table_type=""SEQUENCE"")"
65,"SELECT TABLE_NAME, COLUMN_NAME"
65,FROM information_schema.COLUMNS
65,JOIN seqlist WHERE COLUMN_DEFAULT LIKE seqlist.a;
65,+------------+-------------+
65,| TABLE_NAME | COLUMN_NAME |
65,+------------+-------------+
65,| t1
65,| a
65,+------------+-------------+
65,2 rows in set (0.023 sec)
65,Delete Sequences or replace it with auto_increment.
65,CREATE TABLE IF NOT EXISTS `t1` (
65,"`a` int(11) NOT NULL DEFAULT nextval(`mydatabase`.`s1`),"
65,......
65,CREATE TABLE IF NOT EXISTS `t1` (
65,"`a` int(11) NOT NULL auto_increment,"
65,System-versioned attribute
65,PolarDB for MySQL does not support the System-versioned table type either. You must use the following SQL statements to check whether this type is available:
65,"SELECT TABLE_SCHEMA, TABLE_NAME"
65,FROM information_schema.TABLES
65,WHERE TABLE_TYPE='system versioned';
65,+--------------+------------+
65,| TABLE_SCHEMA | TABLE_NAME |
65,+--------------+------------+
65,| test
65,| t
65,+--------------+------------+
65,1 row in set (0.0090 sec)
65,Delete the attribute
65,ALTER TABLE test.t DROP SYSTEM VERSIONING;
65,"Query OK, 0 rows affected (0.0232 sec)"
65,Table DEFAULT value
65,The SHOW CREATE TABLE function of MariaDB 10.2 does not have numeric types quoted in quotation marks.
65,In MariaDB
65,CREATE TABLE td (link TINYINT DEFAULT 1);
65,SHOW CREATE TABLE td\G
65,*************************** 1. row ***************************
65,Table: td
65,Create Table: CREATE TABLE `td` (
65,`link` tinyint(4) DEFAULT 1
65,) ENGINE=InnoDB DEFAULT CHARSET=latin1
65,In PolarDB for MySQL
65,mysql> show create table td\G
65,*************************** 1. row ***************************
65,Table: td
65,Create Table: CREATE TABLE `td` (
65,`link` tinyint(4) DEFAULT '1'
65,) ENGINE=InnoDB DEFAULT CHARSET=latin1
65,1 row in set (0.00 sec)
65,"MariaDB 10.2 also supports BLOB and TEXT DEFAULT values, but MySQL does not."
65,"MariaDB 10.2 supports the expression in the DEFAULT statement. You can use table INFORMATION_SCHEMA.COLUMNS to check whether the table is compatible. However, PolarDB for MySQL 5.7 does not support this expression. Only PolarDB for MySQL 8.0 supports this expression. For more information, see MySQL official documentation."
65,"SELECT TABLE_NAME, COLUMN_NAME"
65,FROM information_schema.COLUMNS
65,WHERE COLUMN_DEFAULT LIKE '%add_months%';
65,Empty set (0.055 sec)
65,Syntax Compatibility
65,CTE(Recursive Common Table Expressions)
65,"MariaDB 10.2 supports the CTE syntax, but PolarDB for MySQL 5.7 does not. You must upgrade PolarDB for MySQL 5.7 to PolarDB for MySQL 8.0. For more information, see MySQL official documentation."
65,Set operations
65,"MariaDB supports the INTERSECT and EXCEPT set operations. However, PolarDB for MySQL 5.7 does not support these operations. You must upgrade PolarDB for MySQL 5.7 to PolarDB for MySQL 8.0. For more information, see MySQL official documentation."
65,TRUNCATE TABLE
65,The TRUNCATE TABLE statement of PolarDB for MySQL does not support the CASCADE attribute.
65,LIST DEFAULT partition type
65,"MariaDB supports the DEFAULT partition type, but PolarDB for MySQL 5.7 does not. You must upgrade PolarDB for MySQL 5.7 to PolarDB for MySQL 8.0. For more information, see PolarDB for MySQL official documentation."
65,PARTITION BY LIST (partitioning_expression)
65,"PARTITION partition_name VALUES IN (value_list),"
65,"[ PARTITION partition_name VALUES IN (value_list), ... ]"
65,[ PARTITION partition_name DEFAULT ]
65,RETURNING syntax
65,"MariaDB supports the RETURNING syntax. PolarDB for MySQL 5.7 is implemented by using functions. For more information, see PolarDB for MySQL official documentation."
65,Case sensitivity
65,"PolarDB MySQL can convert all table names to lowercase format, and queries can also support all lowercase formats, while MariaDB depends on the case requirements of the operating system. The lower_case_table_names can be set as needed."
65,Character Set
65,"Not all character sets and collations are supported in MySQL. MariaDB supports 40 character sets and 322 collations, while MySQL supports 41 character sets (gb18030 ) and 222 collations. You can use the following SQL statements to query whether the character set is supported."
65,SHOW CHARACTER SET like 'utf16';
65,+---------+----------------+-------------------+--------+
65,| Charset | Description
65,| Default collation | Maxlen |
65,+---------+----------------+-------------------+--------+
65,| utf16
65,| UTF-16 Unicode | utf16_general_ci
65,4 |
65,+---------+----------------+-------------------+--------+
65,1 row in set (0.00 sec)
65,"For more information, see MariaDB official documentation Character Sets and Collations and MySQL 5.7 official documentation Charset and Collation."
65,References
65,Incompatibilities and Feature Differences Between MariaDB 10.3 and MySQL 5.7List of incompatibilities and feature differences between MariaDB 10.3 and MySQL 5.7.
65,Incompatibilities and Feature Differences Between MariaDB 10.2 and MySQL 5.7List of incompatibilities and feature differences between MariaDB 10.2 and MySQL 5.7.
65,How to Migrate from MariaDB to MySQL 8.0
65,Database
65,Developers
65,Migration
65,POLARDB
65,MariaDB
65,PolarDB for MySQL
65,Database Migration， Best Practices
65,Share on
65,Read previous post:
65,New Feature Syntax and Compatibility Improvements in Community Edition of PolarDB for MySQL 8.0.x
65,Read next post:
65,Best Practices for Upgrading PolarDB for MySQL 5.6/MySQL 5.6 to PolarDB for MySQL 8.0
65,Morningking
65,7 posts | 0 followers
65,Follow
65,You may also like
65,How to Choose the Most Suitable Database to Empower Your Business with Proven Performance
65,"ApsaraDB - June 18, 2021"
65,Best Practices for Upgrading PolarDB for MySQL 5.7/MySQL 5.7 to PolarDB for MySQL 8.0
65,"Morningking - September 26, 2023"
65,Best Practices for PolarDB: Akulaku Cloud-Native Database Architecture Evolution
65,"ApsaraDB - January 16, 2023"
65,How to Use Basic APT Commands to Manage Packages on Debian-Based Alibaba Cloud Servers
65,"francisndungu - December 12, 2018"
65,Alibaba Cloud Ranked as Strong Performer in Forrester Wave DaaS Q2 2019
65,"Alibaba Clouder - July 5, 2019"
65,About Database Kernel | PolarDB Optimizer Query Transformation - Join Elimination
65,"ApsaraDB - October 19, 2023"
65,Comments
65,Post
65,Morningking
65,7 posts | 0 followers
65,Follow
65,Related Products
65,PolarDB for PostgreSQL
65,Alibaba Cloud PolarDB for PostgreSQL is an in-house relational database service 100% compatible with PostgreSQL and highly compatible with the Oracle syntax.
65,Learn More
65,PolarDB for Xscale
65,Alibaba Cloud PolarDB for Xscale (PolarDB-X) is a cloud-native high-performance distributed database service independently developed by Alibaba Cloud.
65,Learn More
65,PolarDB for MySQL
65,Alibaba Cloud PolarDB for MySQL is a cloud-native relational database service 100% compatible with MySQL.
65,Learn More
65,Oracle Database Migration Solution
65,"Migrate your legacy Oracle databases to Alibaba Cloud to save on long-term costs and take advantage of improved scalability, reliability, robust security, high performance, and cloud-native features."
65,Learn More
65,More Posts
65,by Morningking
65,See All
65,MySQL Range (Min-Max Tree) Structure Analysis
65,Best Practices for Upgrading PolarDB for MySQL 5.6/MySQL 5.6 to PolarDB for MySQL 8.0
65,New Feature Syntax and Compatibility Improvements in Community Edition of PolarDB for MySQL 8.0.x
65,Code Explanation of MySQL 8.0.23 Hypergraph Join Optimizer
65,Best Practices for Upgrading PolarDB for MySQL 5.7/MySQL 5.7 to PolarDB for MySQL 8.0
65,How Does the INTERVAL Partition of PolarDB for MySQL Empower Database Administrators?
66,10 Ways To Improve Cassandra Read Performance: Ultimate Guide
66,Skip to content
66,Menu
66,Menu WindowsWindows 11Windows 10Windows 8Windows 7Windows VistaWindows XPDevelopmentJavaPythonPHPRuby/RailsDatabasesPostgreSQLMySQL (MariaDB)CassandraTipsLinux TipsAndroid TipsiPhone / iPad TipsMacOS TipsWeb TipsSoftware TipsOtherGadgetsCool Websites
66,Cassandra10 Ways To Improve Cassandra Read Performance: Ultimate Guide
66,"soodMay 20, 2023"
66,"We discuss 10 ways to improve Cassandra read performance and reduce latency. But first, we’ll discuss read vs write performance, and what is the expected read latency. Cassandra performance tuning can be daunting, but hopefully this article will get you comfortable with some of the terminology.Apache Cassandra is a distributed NoSQL database that is known for its scalability, high availability, fault tolerance, and outstanding read performance. It is a popular choice for handling large amounts of data across multiple data centers.Table of Contents"
66,"Is Cassandra read or write optimized?What is the average read latency of Cassandra?How do I improve Cassandra read performance?Final ThoughtsFrequently Asked Questions (FAQ)What is the complexity of read time in Cassandra?Why reads are faster in Cassandra?Does Cassandra tombstones affect performance?Is Cassandra read or write optimized?Cassandra is often considered more write-optimized than read-optimized. It is designed to handle high write throughput and massive scalability while providing strong durability and fault tolerance. The architecture of Cassandra distributes data across multiple nodes in a cluster, allowing for efficient parallel writes. The data is written to a commit log and then asynchronously flushed to disk in a data structure called an SSTable.However, this write optimization comes at the cost of read performance in certain scenarios. Cassandra’s distributed nature and eventual consistency model make it more challenging to achieve low read latencies compared to traditional relational databases. The data may be spread across multiple nodes, requiring coordination and network communication to retrieve it. Additionally, read operations that span multiple partitions or require complex queries can be slower due to the distributed nature of data storage.That being said, Cassandra provides features like tunable consistency, caching, and compression that can help improve read performance. With proper data modeling, caching strategies, and hardware optimizations, it is possible to achieve good read latencies in many use cases. Ultimately, the performance characteristics of Cassandra depend on the specific workload, data model, and configuration choices.What is the average read latency of Cassandra?The average read latency of Cassandra can vary significantly based on several factors, such as the cluster configuration, data model, hardware resources, and workload characteristics. It is challenging to provide a specific average read latency as it depends on the specific use case and the tuning efforts applied to the Cassandra cluster.In general, Cassandra aims to provide low-latency read operations by distributing data across multiple nodes and allowing for parallel access. With proper data modeling and efficient query design, read latencies in the range of single-digit milliseconds or even sub-millisecond responses are achievable for individual read requests. Cassandra read performance is incredible when comparing against alternative databases.However, it’s important to note that read latency can increase under certain circumstances, such as when dealing with wide rows, complex queries spanning multiple partitions, or when consistency levels requiring more extensive coordination are utilized. Moreover, if the cluster is under heavy load or experiencing hardware limitations, read latencies can increase.How do I improve Cassandra read performance?We’ve compiled 10 ways to optimize read performance. It may be worthwhile to run Cassandra benchmarks before and after to measure the improvement.Optimize Data Modeling One of the most important factors in improving Cassandra read performance is to optimize data modeling. The key to this optimization is to design your data model around your queries. This means that you should think about how your data will be accessed and structured your data model accordingly. In Cassandra, data is organized into tables, and each table has one or more columns. You should design your tables to have a relatively small number of wide rows, rather than many narrow rows, to minimize the number of reads required to retrieve the data you need.Use Appropriate Data Types The data type you choose for your columns can also impact read latency. Choosing the appropriate data type can help to improve performance. For example, using smaller data types like INT or SMALLINT instead of BIGINT can reduce the amount of disk space required and result in faster reads.Utilize Appropriate CompressionCompression can be an effective way to reduce the size of your data, which in turn can improve read latency. Cassandra offers several compression algorithms, including LZ4 and Snappy. These algorithms can significantly reduce the amount of data that needs to be read from disk, resulting in faster read times.Use the Right Consistency Level Cassandra provides a consistency level setting that allows you to control how many nodes must respond to a read request before the data is considered valid. This setting is important because it affects the read latency. If you set the consistency level too high, you may experience high read latency because the database has to wait for responses from too many nodes. On the other hand, if you set the consistency level too low, you may read stale data. You should choose the appropriate consistency level based on your application’s requirements.Utilize CachingCassandra provides several caching mechanisms, including row cache and key cache. Row cache stores entire rows in memory, while key cache stores the most frequently accessed partition keys in memory. Utilizing caching can significantly reduce read latency because the data can be retrieved from memory rather than disk. However, caching should be used judiciously because it can consume a significant amount of memory.Optimize Hardware Cassandra performance can also be affected by the hardware it is running on. Here are some tips for optimizing hardware to improve read latency:Use SSDs instead of HDDs for storage. SSDs have faster read and write times, which can significantly improve performance.Use fast network adapters to reduce network latency.Ensure that your CPU and memory resources are sufficient for your workload.Use Read Repair Read repair is a mechanism in Cassandra that automatically repairs inconsistencies in data when it is read. When you read data from Cassandra, it may be possible to retrieve data from multiple nodes, and these nodes may have different values for the same column. Read repair ensures that the most recent value is stored in all nodes, which helps to prevent stale data and reduce read latency.Optimize Bloom Filters Cassandra uses Bloom filters to determine whether data is present in a partition. Bloom filters are probabilistic data structures that can quickly determine whether a given element is likely to be in a set. Cassandra uses Bloom filters to avoid reading irrelevant data from disk, which can help to improve read latency. You can optimize Bloom filters by adjusting the size of the filter and the number of hash functions used.Use SSTable CompressionCassandra stores data in SSTables (Sorted String Tables), which are immutable data files that contain a sorted list of key-value pairs. SSTable compression can be used to reduce the size of these files, which in turn can improve read latency. By compressing SSTables, you can reduce the amount of data that needs to be read from disk, resulting in faster read times.Monitor and Tune Performance Finally, it is important to monitor and tune Cassandra performance regularly. This includes monitoring metrics such as read latency, cache hit rate, and disk usage. By monitoring performance, you can identify bottlenecks and optimize your database configuration accordingly. Cassandra provides several tools for monitoring and tuning performance, including nodetool, which can be used to view and manipulate Cassandra nodes, and Cassandra-stress, which can be used to stress test your database and identify performance issues.See also  Cassandra Data Model by Example: A Comprehensive GuideFinal ThoughtsThere are many techniques for improving Cassandra read performance. By optimizing data modeling, using appropriate data types, compression, and caching, and tuning consistency levels, Bloom filters, and SSTable compression, you can significantly improve read performance. Additionally, by optimizing hardware, using read repair, and monitoring and tuning performance, you can ensure that your Cassandra database is performing at its best. With these techniques in mind, you can build a highly performant and scalable database solution for your application.Frequently Asked Questions (FAQ)What is the complexity of read time in Cassandra?"
66,"The complexity of read time in Cassandra is generally considered to be O(log n), where “n” represents the number of nodes in the cluster. This logarithmic complexity is due to the distributed nature of Cassandra and its consistent hash ring architecture. When a read request is made, Cassandra efficiently routes the request to the appropriate node responsible for serving the data. The logarithmic complexity ensures that as the cluster grows, the read time remains scalable and performs well. However, it’s important to note that other factors such as data model design, consistency levels, network latency, and hardware resources can also impact Cassandra read performance.Why reads are faster in Cassandra?"
66,"1. Distributed Architecture Cassandra is designed to be distributed, allowing data to be spread across multiple nodes in a cluster. This enables parallel processing and retrieval of data, leading to faster read operations.Data Replication: Cassandra replicates data across multiple nodes for fault tolerance and high availability. As a result, data can be read from replicas located closer to the requesting node, reducing network latency and improving read performance.2. Memtable and SSTable StructureCassandra utilizes an in-memory data structure called memtable and an on-disk data structure called SSTable. The memtable stores recently written data in memory for fast access, while the SSTables serve as the persistent storage for data. This combination enables efficient and quick read operations.3. Bloom FiltersCassandra uses Bloom filters to determine the presence of data in a partition, allowing it to skip unnecessary disk reads. Bloom filters provide a probabilistic check, reducing I/O operations and improving read efficiency.4. Caching MechanismsCassandra offers caching mechanisms such as row cache and key cache. These caches store frequently accessed data in memory, enabling subsequent reads to be served from memory instead of disk, significantly improving read latency.Does Cassandra tombstones affect performance?Yes, Cassandra tombstones can affect performance. Tombstones are markers used to represent deleted data in Cassandra. If there are too many tombstones, they can impact read and write performance by increasing disk I/O and query execution time. Proper tombstone management is crucial to maintain good performance in Cassandra.Support us & keep this site free of annoying ads. Shop Amazon.com or Donate with PaypalLeave a CommentCommentName"
66,Email
66,"Website Save my name, email, and website in this browser for the next time I comment."
66,ΔSupport us & keep this site free of annoying ads. Shop Amazon.com or Donate with PaypalcPanel / WHM
66,Django Tips
66,Facebook Tips
66,Google Tips
66,iPhone Tips
66,Laravel Tips
66,Samsung Tips
66,Software Testing (QA)
66,Video Games
66,"VirtualizationFind us on socialFacebookTwitterAmazonRSS FeedAbout UsWelcome to HeatWare.net - A technology blog for geeks and non-geeks alike! We programming tips, code examples, Cloud technology help, Windows & application troubleshooting, and more! The owner of this blog also run a feedback and rating database for online transactions."
66,© 2024 HeatWare.net
66,Search for:
68,Community Guide to PostgreSQL GUI Tools - PostgreSQL wiki
68,"Want to edit, but don't see an edit button when logged in?"
68,Click here.
68,Community Guide to PostgreSQL GUI ToolsFrom PostgreSQL wikiJump to navigationJump to search
68,"This page is a list of miscellaneous utilities that work with Postgres (ex: data loaders, comparators etc.)."
68,"Things that don't do queries' ""enter SQL and get it back out again."""
68,"If you'd like to find clients that allows you to ""enter SQL and get it back out again"" see PostgreSQL Clients."
68,If you'd like to find DB visualization or design tools see Design Tools.
68,Contents
68,1 Open Source / Free Software
68,1.1 Libre Office
68,1.2 PASH-Viewer: PostgreSQL Active Session History Viewer
68,"1.3 pgrights: GUI for PostgreSQL roles, privileges and policies"
68,1.4 Sohag Developer
68,1.5 Beekeeper Studio
68,1.6 Execsql.py
68,2 Proprietary
68,2.1 Access
68,2.2 Five
68,2.3 dbForge Studio for PostgreSQL
68,2.4 dbForge Data Compare for PostgreSQL
68,2.5 dbForge Schema Compare for PostgreSQL
68,2.6 TablePlus
68,2.7 Ultorg
68,2.8 WaveMaker Ajax GUI Design Tool
68,2.9 Postgres Compare
68,2.10 Full Convert
68,2.11 Abris Platform
68,2.12 Replicator Pro
68,2.13 DBTools Manager
68,2.14 PostgreSQL PHP Generator
68,2.15 ConvertDB for PosttgreSQL
68,2.16 dotConnect for PostgreSQL
68,2.17 Devart PostgreSQL Data Access Components
68,2.18 Devart ODBC Driver for PostgreSQL
68,2.19 Devart Excel Add-in for PostgreSQL
68,2.20 EMS Database Management Tools for PostgreSQL
68,2.21 SQL Maestro Group products for PostgreSQL
68,2.22 Datanamic DataDiff for PostgreSQL
68,2.23 Datanamic SchemaDiff for PostgreSQL
68,2.24 DB MultiRun PostgreSQL Edition
68,2.25 DB Doc for PostgreSQL
68,2.26 SQL Blob Export
68,2.27 SQL File Import
68,2.28 SQL Image Viewer
68,2.29 SQL Multi Select
68,2.30 Devart SSIS Data Flow Components for PostgreSQL
68,2.31 EDB Postgres Enterprise Manager
68,2.32 ClusterControl by Severalnines
68,2.33 Reportizer
68,2.34 Exportizer Enterprise
68,2.35 TiCodeX SQL Schema Compare
68,2.36 pgMustard
68,2.37 ODBC Driver for PostgreSQL by CData
68,2.38 JDBC Driver for PostgreSQL by CData
68,2.39 ADO.NET Provider for PostgreSQL by CData
68,2.40 Excel Add-In
68,for PostgreSQL by CData
68,2.41 SSIS Components for PostgreSQL by CData
68,2.42 Power BI Connector for PostgreSQL by CData
68,3 Other Resources
68,Open Source / Free Software
68,"This is the list of ""open source and free"" miscellaneous utilities:"
68,Libre Office
68,http://www.libreoffice.org/discover/base/
68,"Supports MySQL/MariaDB, Adabas D, MS Access and PostgreSQL, as well as other JDBC/ODBC databases."
68,PASH-Viewer: PostgreSQL Active Session History Viewer
68,https://github.com/dbacvetkov/PASH-Viewer
68,Java (multi-platform).
68,"Open-source software which provides graphical view of active session history and help you to answer questions like ""What wait events were taking most time?"", ""Which sessions were taking most time?"", ""Which queries were taking most time and what were they doing?"". It also supports Active Session History extension by pgsentinel."
68,"Does not do DB inserts, modifications, etc."
68,"pgrights: GUI for PostgreSQL roles, privileges and policies"
68,https://github.com/apsavin/pgrights
68,"MacOS (based on Electron, so versions for other OS can be build from source code)."
68,"Open-source software which allows you to easily understand what can do (and what can't) a PostgreSQL user with a table's data. In other words, it's a viewer of results of GRANT commands and row-level security rules applied for a particular table and for a particular role."
68,"Only modifies user rights, no other capability."
68,Sohag Developer
68,Sohag Developer
68,Gnu/Linux Windows (Other OS can compile from source code).
68,Build a powerful database applications following few steps using Sohag Developer .
68,Sohag Developer currently supports PostgreSQL database and has a set of CRUD generators that generates (Qt/C++ code and ui forms - PHP web applications uses web forms and bootstrap framework ) to manipulate the data in the database tables or views.
68,Beekeeper Studio
68,https://beekeeperstudio.io/
68,"Beekeeper Studio is a modern cross-platform SQL editor and database manager available for Linux, Mac, and Windows. Some of its features include:"
68,"Clean, smooth, usable UI with dark and light themes"
68,Tabbed Interface
68,Multiple connections at the same time
68,Saved queries and run history
68,Auto-complete
68,Execsql.py
68,https://pypi.org/project/execsql/
68,Execsql.py is a SQL scripting client that reads SQL statements from a text file and sends them to the PostgreSQL server.
68,"The script file can also contain special metacommands that are interpreted by execsql.py and that can be used to import and export data, copy data between databases, conditionally execute script blocks, display data in GUI dialogs, and perform other functions."
68,Data can be exported in 19 different tabular formats or using any of three different template processors.
68,Default and custom logging features can be used to document script actions.
68,Proprietary
68,"This is a list of ""closed source"" projects, some might have some manner of free version."
68,Access
68,https://products.office.com/en-us/access
68,Windows
68,"Yes, you can use MS Access as a PostgreSQL database interface. Supports data access to PostgreSQL tables and views; many ODBC-based limitations and errors."
68,Five
68,https://five.co/
68,Rapidly develop and deliver modern business software to internal or external users.
68,Five is free to use. Develop and test applications locally free of charge. Only subscribe when you build something production worthy.
68,Features:
68,"Store, retrieve and process data from any data source (such as a PostgreSQL DB)"
68,"Rapidly implement business logic using SQL, JavaScript or TypeScript"
68,Cut down the time you spent on front-end development
68,"Scalable, secure and affordable: deploy apps in one click"
68,Keep your data secure on Five’s managed infrastructure
68,Build password-protected multi-user applications
68,Useful developer tools to accelerate your development
68,Free download available for Windows & MacOS
68,dbForge Studio for PostgreSQL
68,http://www.devart.com/dbforge/postgresql/studio/
68,"Microsoft Windows, Linux, macOS"
68,"dbForge Studio for PostgreSQL is a feature-rich IDE designed for database development and management. The tasks that can be handled with its help include completion-aided SQL coding, comparison and synchronization of databases, data management, data analysis and reporting, query optimization, and customizable generation of realistic test data."
68,Key features:
68,SQL Development: Smart code completion and formatting
68,Database Explorer: Object tree navigation and operations from the context menu
68,Database Comparison: Detection of differences in schemas and table data and generation of synchronization scripts
68,Data Import & Export: 10+ most widely used data formats
68,Query Profiler: SQL query performance tuning
68,Data Generator: Creation of meaningful dummy data for testing
68,Pivot Tables: Grouping and summarization of data for analysis
68,Master-Detail Browser: Review and analysis of data in related tables
68,Reporting: Visual data reports in 9 different formats
68,Command-Line Interface: CLI-powered automation of recurring operations
68,dbForge Data Compare for PostgreSQL
68,http://www.devart.com/dbforge/postgresql/datacompare/
68,"Microsoft Windows, Linux, macOS"
68,"dbForge Data Compare for PostgreSQL is an effective tool for table data comparison, which makes it easy to detect differences in data, analyze them, and generate SQL scripts for data synchronization."
68,Key features:
68,Identify the differences between two databases
68,Compare separate tables or table groups by table name mask
68,Compare tables with different structures
68,Compare custom query execution results
68,Generate detailed comparison reports
68,Synchronize data in tables and views fully or partially using scripts
68,Schedule data synchronization with Windows Task Scheduler
68,dbForge Schema Compare for PostgreSQL
68,http://www.devart.com/dbforge/postgresql/schemacompare/
68,"Microsoft Windows, Linux, macOS"
68,"dbForge Schema Compare for PostgreSQL and Amazon Redshift is a free tool for easy and effective comparison of database structure differences. Additionally, it generates and runs SQL scripts for synchronization of source and target schemas."
68,Key features:
68,Find differences in Redshift and PostgreSQL database schemas
68,Generate SQL scripts to update one database with the contents of another
68,Migrate PostgreSQL schemas to Amazon Redshift
68,Apply updates from development databases to staging or production
68,Compare and synchronize pre-object security permissions
68,Compare PL/pgSQL and Python code
68,TablePlus
68,https://tableplus.com/
68,"macOS, Windows, iOS"
68,"TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more."
68,True native built.
68,"Workspace supports multiple tabs, multiple windows"
68,"Powerful SQL editor with full features: auto syntax highlight, auto-suggestion, split pane, favorite and history."
68,"Data Filter & Sorting, import & export"
68,Full-dark theme & modern shortcut
68,"With plugin system, you can be able to write your own new features to work with database per your needs (export charts, pretty json…)."
68,Ultorg
68,https://www.ultorg.com
68,Ultorg is a general-purpose user interface and visual query system that works with any existing PostgreSQL database.
68,"Windows, MacOS, Linux"
68,Key features:
68,Quickly show data across many tables and one-to-many relationships.
68,"See data in continuously auto-formatted table, form, or crosstab layouts."
68,"Build the equivalent of any SQL-92 SELECT query by interacting directly with the data (choose fields, joins, filters, calculations etc.)."
68,Edit data and commit changes back to the database. Insert/delete/dropdown rows from FK relationships etc.
68,"Join PostgreSQL tables with tables from other data sources (CSV files, Google Sheets, and others)."
68,Avoid typing long SQL queries and associated formatting code.
68,WaveMaker Ajax GUI Design Tool
68,http://www.wavemaker.com/
68,"Windows, Macintosh, Linux"
68,"WaveMaker is an Ajax-based GUI design tool for Postgres. WaveMaker is built using itself! WaveMaker generates a standard Java WAR file based on Spring, Hibernate and Dojo. WaveMaker supports Postgres schema creation and import and includes a visual query editor."
68,Postgres Compare
68,frameless
68,https://www.postgrescompare.com/
68,"Windows, Mac & Linux"
68,"A comprehensive tool for identifying the differences between databases and generating an update script to synchronize them. Postgres Compare reads the system catalogs to determine the structure of the database and compares it to another to find the changes. Generate SQL and deploy the alterations, save snapshots for later. Automate the process via the command line."
68,Full Convert
68,https://www.spectralcore.com/fullconvert
68,"Database conversion and synchronization between PostgreSQL and Microsoft Access, dBase, FoxPro, Microsoft Excel, Firebird, Interbase, MySQL, Oracle, Paradox, Microsoft SQL Server, SQL Server, SQL Server Azure, SQL Server Compact(SQLCE), SQLite, Delimited text files (CSV), XML and many more via ODBC."
68,Abris Platform
68,https://abrisplatform.com
68,"Web Application for Linux/Windows, requires Apache+PHP or Docker"
68,Abris Platform is an application development platform for creating Web-based front-ends for PostgreSQL databases. Can be used to quickly create applications with convenient forms via SQL declarative description.
68,Key features:
68,"Quick setup - Abris Platform provides built-in means for flexible data structures configuration (tables, fields and relations). This can be done during system initialization as well as during system expluatation."
68,Single page application - Related entities and tables are instantly accessible for view and edit on the same screen.
68,Search - Use general in-table search and complex column filters.
68,"Charts - Data can be represented via charts of different types: bar charts, pie charts, lines any many others."
68,Maps - Build-in support for geo data. Abris platform allow to vizualize geo-data event in real-time using OpenStreet Map package.
68,"Reporting - Filter settings can be saved as a report description and data can be exported in the following formats: HTML, PDF, Excel."
68,Data import - Insert data in the current open list view from the computer clipboard.
68,"Notifications - Notification pool, that can be filled in PostgreSQL functions."
68,"Administrative tool - Built in administrative tools take care of user management, activity monitoring and auditing and allow to configure user/group access policy on the table and field database level."
68,Replicator Pro
68,https://www.spectralcore.com/replicator
68,"Replicator allows table data comparison and sync - even with heterogeneous databases. It is unique in the fact it can replicate changes only even if source is non-relational (CSV, DBF, Excel documents, Paradox...). Replicator has a built-in scheduler for easy periodic change replication."
68,DBTools Manager
68,http://www.dbtools.com.br
68,Windows
68,Admin
68,"Freeware, available for PostgreSQL and MySQL, allows managing all aspects of the database: db, table, triggers, functions, etc. Includes import/export wizards to migrate data and structure to/from other database engines. Developed by DBTools Software."
68,PostgreSQL PHP Generator
68,http://www.sqlmaestro.com/products/postgresql/phpgenerator/
68,Windows
68,"PostgreSQL PHP Generator is a freeware but powerful PostgreSQL GUI frontend that allows you to generate high-quality PHP scripts for the selected tables, views and queries for the further working with these objects through the web."
68,ConvertDB for PosttgreSQL
68,http://convertdb.com/postgresql has PostgreSQL export/ import tools
68,Windows
68,"The software is able to connect to remote PostgreSQL 9.x/7.4 located on Linux, Solaris, Mac OS X, and Windows."
68,"ConvertDB cross-database migration tools assist in data conversion and synchronization among PostgreSQL, MySQL, MS SQL Server, MS Windows SQL Azure,"
68,and MS Access databases
68,1 Million of records can be transferred in 5-10 minutes.
68,"Bi-directional synchronization between PostgreSQL, MS SQL, MySQL and Oracle"
68,Scheduling migration and synchronization jobs.
68,dotConnect for PostgreSQL
68,https://www.devart.com/dotconnect/postgresql/
68,Windows
68,"dotConnect for PostgreSQL, formerly known as PostgreSQLDirect .NET, is an enhanced ORM enabled data provider for PostgreSQL that builds on ADO.NET technology to present a complete solution for developing PostgreSQL-based database applications. It introduces new approaches for designing application architecture, boosts productivity, and leverages database applications."
68,Key features:
68,Direct Mode
68,Database Application Development Extension
68,PostgreSQL Advanced Features Support
68,Optimized Code
68,ORM Support
68,BIS Support
68,Devart PostgreSQL Data Access Components
68,Windows
68,https://www.devart.com/pgdac/
68,"PostgreSQL Data Access Components (PgDAC) is a library of components that provides native connectivity to PostgreSQL from Delphi, C++Builder, Lazarus (and Free Pascal) on Windows, Mac OS X, iOS, Android, Linux, and FreeBSD for both 32-bit and 64-bit platforms. PgDAC is designed to help programmers develop really lightweight, faster and cleaner PostgreSQL database applications without deploying any additional libraries."
68,Native Connectivity to PostgreSQL
68,PgDAC is a complete replacement for standard PostgreSQL connectivity solutions and presents an efficient alternative to the Borland Database Engine (BDE) and standard dbExpress driver for access to PostgreSQL. It provides direct access to PostgreSQL without PostgreSQL Client.
68,Devart ODBC Driver for PostgreSQL
68,https://www.devart.com/odbc/postgresql/
68,Windows
68,"Devart ODBC Driver for PostgreSQL provides high-performance and feature-rich connectivity solution for ODBC-based applications to access PostgreSQL databases from Windows, both 32-bit and 64-bit. Full support for standard ODBC API functions and data types implemented in our driver makes the interaction of your database applications with PostgreSQL fast, easy and extremely handy."
68,Devart Excel Add-in for PostgreSQL
68,https://www.devart.com/excel-addins/postgresql.html
68,Windows
68,"Devart Excel Add-in for PostgreSQL allows you to quickly and easily connect Microsoft Excel to PostgreSQL, load data from PostgreSQL to Excel, instantly refresh data in an Excel workbook from the database, edit these data, and save them back to PostgreSQL. It enables you to work with PostgreSQL data like with usual Excel worksheets, easily perform data cleansing and de-duplication, and apply all the Excel's powerful data processing and analysis capabilities to these data."
68,EMS Database Management Tools for PostgreSQL
68,http://www.sqlmanager.net/en/products/postgresql
68,Windows
68,PostgreSQL Tools Products Family:
68,EMS SQL Manager for PostgreSQL see PostgreSQL Clients.
68,"SQL Management Studio for PostgreSQL - a single workbench for administering PostgreSQL databases, managing database schema and objects as well as for database design, migration, extraction, query building, data import, export, and database comparison."
68,"SQL Manager for PostgreSQL - high performance graphical tool for PostgreSQL database administration and development. It makes creating and editing PostgreSQL database objects easy and fast, and allows you to run SQL scripts, visually design databases, build SQL queries, extract, print and search metadata, import and export PostgreSQL database data and much more."
68,"Data Export for PostgreSQL - tool to export PostgreSQL database data quickly to any of 19 available formats, including MS Access, MS Excel, MS Word, RTF, HTML, TXT, ODF and more. Data Export for PostgreSQL has a kata kata lucu friendly wizard, which allows you to set various options of PostgreSQL export process visually and a command-line utility to automate your PostgreSQL export jobs using the configuration file."
68,"Data Import for PostgreSQL - tool to import data to PostgreSQL tables from MS Excel 97-2007, MS Access, DBF, TXT, CSV, MS Word 2007, RTF, ODF and HTML files. This utility allows you to quickly import data to one or several PostgreSQL tables or views at once, save all PostgreSQL import parameters set on current wizard session, use special batch insert mode to import PostgreSQL data at the maximum possible speed and much more."
68,"Data Pump for PostgreSQL - migration tool for converting databases and importing table data from an ADO-compatible source (e.g. MS Access, MS SQL database or any other database with ADO support) to PostgreSQL databases."
68,"Data Generator for PostgreSQL - tool for generating test data to PostgreSQL database tables. The utility can help you to simulate the database production environment and allows you to populate several PostgreSQL database tables with test data simultaneously, define tables for generating data, set value ranges, control a wide variety of generation parameters for each field type and much more."
68,DB Comparer for PostgreSQL - a tool for comparing PostgreSQL database schemas and discovering differences in their structures. You can view all the differences in compared database objects and execute an automatically generated script to synchronize structure of PostgreSQL databases and eliminate these differences.
68,DB Extract for PostgreSQL - easy-to-use tool for creating PostgreSQL database backups in a form of SQL scripts. This database script utility allows you to save metadata of all PostgreSQL database objects as well as PostgreSQL table data as database snapshots.
68,"SQL Query for PostgreSQL - a useful tool that lets you quickly and simply build SQL queries to PostgreSQL databases. Visual PostgreSQL query building, as well as direct editing of a query text, is available."
68,Data Comparer for PostgreSQL - tool for PostgreSQL data comparison and synchronization. Using this utility you can view all the differences in compared PostgreSQL tables and execute an automatically generated script to eliminate these differences.
68,SQL Maestro Group products for PostgreSQL
68,http://www.sqlmaestro.com/products/postgresql/
68,Windows
68,SQL Maestro Group offers a number of tools for PostgreSQL.
68,Maestro for PostgreSQL see PostgreSQL Clients.
68,"PostgreSQL Data Wizard provides you with a number of easy-to-use wizards to transfer any database to PostgreSQL, export data from PostgreSQL tables, views and queries to most popular formats, and import data from various sources into PostgreSQL tables."
68,PostgreSQL Code Factory is a
68,GUI tool aimed at the SQL queries and scripts development.
68,PostgreSQL Data Sync is a powerful and easy-to-use tool for database contents comparison and synchronization.
68,PostgreSQL PHP Generator Professional is a frontend that allows you to generate high-quality PHP applications for your database in a few mouse clicks.
68,"SQL Maestro Group also produces similar tools for MySQL, Oracle, MS SQL Server, SQLite, Firebird, DB2, SQL Anywhere, and MaxDB."
68,Datanamic DataDiff for PostgreSQL
68,http://www.datanamic.com/datadiff-for-postgresql/
68,Windows
68,"Datanamic DataDiff for PostgreSQL is a utility for data comparison and synchronization. Compare data for selected tables in two databases, view differences and publish changes quickly and safely. Flexible comparison and synchronization settings will enable you to set up a customized comparison key and to select tables and fields for comparison and for synchronization."
68,"DB Data Difftective can be used for data migrations, verification of (corrupt) data, data auditing etc."
68,Datanamic SchemaDiff for PostgreSQL
68,http://www.datanamic.com/schemadiff-for-postgresql/index.html
68,Windows
68,"Datanamic SchemaDiff for PostgreSQL is a tool for comparison and synchronization of database schemas. It allows you to compare and synchronize tables, views, functions, sequences (generators), stored procedures, triggers and constraints between two databases."
68,DB MultiRun PostgreSQL Edition
68,http://www.datanamic.com/multirun/index.html
68,Windows
68,DB MultiRun is a simple tool to execute multiple SQL scripts on multiple databases quickly.
68,"Define a list of databases, add SQL scripts to execute on these databases and click ""execute"" to run those scripts on the databases in the list. The multi-threaded execution of the SQL scripts makes it complete the task fast. After execution of the scripts, you can examine the results of the executed scripts on each database."
68,DB Doc for PostgreSQL
68,https://www.yohz.com/dbdoc_details.htm
68,Windows
68,"DB Doc helps you document your database schema and generate shareable PDF, HTML, XML, and Microsoft Word document in only 5 steps."
68,"Furthermore, the layout of the generated documents are customizable."
68,SQL Blob Export
68,http://www.yohz.com/sbe_details.htm
68,Windows
68,SQL Blob Export exports unlimited images and files from your tables or queries in 5 simple steps.
68,SQL File Import
68,http://www.yohz.com/sfi_overview.htm
68,Windows
68,"SQL File Import allows you to upload files, images, and other data into your database, without having to write any SQL statements."
68,"SQL File Import supports PostgreSQL, Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL)."
68,A scripting engine allows you to transform data before importing them into your database.
68,A command line version is also included to allow you to perform unattended upload/import tasks.
68,SQL Image Viewer
68,http://www.yohz.com/siv_details.htm
68,Windows
68,"SQL Image Viewer allows you to retrieve, view, convert and export images stored in Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL). It supports the following image formats: BMP, GIF, JPG, PNG, PSD, and TIFF."
68,"It also allows you to export binary data and recognises the following binary file types: PDF, MP3, WAV, 7Z, BZ2, GZ, RAR, ZIP, and has experimental support for DOC, PPT and XLS file types."
68,A command line version is also included to allow you to perform unattended scheduled exports of binary data.
68,SQL Multi Select
68,http://www.yohz.com/sms_details.htm
68,Windows
68,SQL Multi Select is a query tool that allows you to run multiple scripts on multiple servers with a single click.
68,"Result sets from different servers are consolidated into a single view, allowing for easy comparison and analysis."
68,Devart SSIS Data Flow Components for PostgreSQL
68,https://www.devart.com/ssis/
68,Windows
68,Devart SSIS Data Flow Components for PostgreSQL allow you to integrate database and cloud data via SQL Server Integration Services (SSIS).
68,"Devart SSIS Data Flow Components provide easy to set up cost-effective data integration using SSIS ETL engine. They provide high performance data loading, convenient component editors, SQL support for cloud data sources and lots of data source specific features."
68,EDB Postgres Enterprise Manager
68,http://www.enterprisedb.com/products/postgres-enterprise-manager
68,"Windows, Mac OS X, Linux"
68,"Postgres Enterprise Manager is the only solution available today that allows you to intelligently manage, monitor, and tune large scale Postgres installations from a single GUI console."
68,"Monitoring features include: server auto-discovery, over 225 pre-configured ready to run probes, custom probes, alert management, personalized alerts, remote monitoring, versatile charting, custom dashboards and web client."
68,"DBA tools include: database objects management, Postgres Expert (best practice configuration settings), Audit Manager, Log Manager, Log Analysis Expert, Capacity Manager and Team Support."
68,"Developer tools include: Query Tool, Data Grid, SQL Profiler, SQL Debugger and Import tools."
68,"Tuning tools include: At-A-Glance performance dashboards, Tuning Wizard, Performance Diagnostics and Index Advisor."
68,ClusterControl by Severalnines
68,https://severalnines.com/product/clustercontrol/for_postgresql
68,"ClusterControl is an all-inclusive open source database management system that allows you to deplore, monitor, manage and scale your database environments. ClusterControl provides the basic functionality you need to get PostgreSQL up-and-running using our deployment wizard, monitoring and basic management abilities like automatic failover, backups, and restores."
68,Point and Click Replication Deployments - ClusterControl allows you to easily deploy and configure master/slave replication PostgreSQL instances.
68,Advanced Performance Monitoring - ClusterControl monitors queries and detects anomalies with built-in alerts.
68,Automated Failover Handling - ClusterControl detects master failures and automatically promotes a new master
68,"Database Automation - ClusterControl lets you manage configurations, schedule, and restore backups."
68,Reportizer
68,https://www.reportizer.net
68,"Reportizer is a database reporting tool, which allows easy creating, modifying, and printing database reports from different types of databases, including PostgreSQL. Reports can be edited in convenient visual report builder or in text mode. It supports calculating fields, multi-column reports, expressions, grouping, displaying images etc. Reportizer can export reports to HTML, XLSX, image, or internal format. There is an ability to load and print reports from command line. Reportizer allows to manage report collections, which can be held either in files or in database tables."
68,Exportizer Enterprise
68,https://www.vlsoftware.net/exportizer/
68,"Exportizer Enterprise is a database export tool, which can work with PostgreSQL database either as source or destination. It allows to export data to database, file, clipboard, or printer."
68,"Possible sources: ODBC data sources, files of DB (Paradox), DBF (dBase, FoxPro), MDB, ACCDB, XLS, XLSX, GDB, IB, FDB, HTML, UDL, DBC, TXT, CSV types, databases specified by ADO connection strings, and databases like Oracle, SQL Server, PostgreSql, DB2, Informix, SQLite, Interbase etc."
68,"Possible destinations: file formats like text, CSV, XLS, XLSX, RTF, XML, HTML, PDF, DBF, SLK, SQL script, and relational database of any supported type including PostgreSQL."
68,It is possible to export all or selected tables from an open database at once.
68,The data migration can be done in super-fast batch mode.
68,"Exportizer Enterprise can automatically detect the most known image types (JPEG, PNG, GIF, BMP, ICO) in BLOB fields and export them, for example, to HTML or XLSX."
68,Images and other BLOB data can be exported to multiple separate files during a single export operation.
68,There is an ability to specify the source-to-target field mappings.
68,Export operations can be performed either via the program interface or via command line.
68,TiCodeX SQL Schema Compare
68,https://www.ticodex.com/
68,TiCodeX SQL Schema Compare is a tools that allows database administrators to compare multiple database schema in order to manage versioning.
68,"The software runs on Windows, Linux and Mac and supports Microsoft SQL (MS-SQL), MySQL, PostgreSQL, Azure SQL and MS-SQL on Amazon RDS."
68,Key Features:
68,"Runs on Windows, Linux and MacOS"
68,"Localized in English, German and Italian"
68,Compare changes between two SQL Database schemas (as example from development to test to production)
68,View database differences and explore schema changes to see what's going on
68,Automatically create full database migration scripts
68,Securely save database and server login details
68,pgMustard
68,https://www.pgmustard.com/
68,"pgMustard is a performance tool for PostgreSQL that provides a user interface for your explain analyze output, as well as tips on what to do to speed up your query."
68,Features:
68,Performance advice – scored by estimated time-saving potential
68,Per-operation timings¹
68,The number of rows¹ returned by each operation
68,A timing bar – that pins to keep it in context
68,Collapsible subtrees – with fast ones collapsed by default
68,Descriptions of operations
68,Links to blog posts for further reading
68,"¹ Taking loops, threads, subplans, and CTEs into account."
68,Requirements:
68,"TEXT or JSON format query plans, from PostgreSQL 9.6 or newer"
68,The interface and advice are in English
68,"A GitHub or Google account, for signing in"
68,"Web application, no installation required"
68,ODBC Driver for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/odbc/
68,"The CData ODBC Driver for PostgreSQL enables real-time access to PostgreSQL data, directly from any applications that support ODBC connectivity, the most widely supported interface for connecting applications with data. The driver wraps the complexity of accessing PostgreSQL data in a standard ODBC driver compliant with ODBC 3.8. The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
68,JDBC Driver for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/jdbc/
68,"The CData JDBC Driver for PostgreSQL offers the most natural way to connect to PostgreSQL data from Java-based applications and developer technologies. The driver wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, 100%-Java JDBC driver."
68,"The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
68,ADO.NET Provider for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/ado/
68,"The CData ADO.NET Provider for PostgreSQL offers the most natural way to access PostgreSQL data from .NET applications. The provider wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, fully managed ADO.NET Data Provider."
68,"The provider hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
68,Excel Add-In
68,for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/excel/
68,"The CData Excel Add-In provides the easiest way to connect to PostgreSQL data from Excel. From the CData ribbon, you can select PostgreSQL data as tables and columns into the spreadsheet. The spreadsheet is then linked with the remote data. To update the data, edit the spreadsheet."
68,SSIS Components for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/ssis/
68,The CData SSIS Components for PostgreSQL enable you to connect SQL Server with PostgreSQL data through SSIS Workflows. The components wrap the complexity of accessing PostgreSQL data in standard SSIS data flow components. You can then connect and synchronize PostgreSQL tables with SQL Server tables.
68,"The components hide the complexity of accessing data and provide additional security features, smart caching, batching, socket management, and more."
68,Power BI Connector for PostgreSQL by CData
68,https://www.cdata.com/drivers/postgresql/powerbi/
68,The CData Power BI Connector for PostgreSQL offers self-service integration with Microsoft Power BI. The connector facilitates live access to PostgreSQL data in Power BI from the Get Data window. The connector also provides direct querying to visualize and analyze PostgreSQL data.
68,Other Resources
68,"PostgreSQL Clients - list of UI's for accessing your db contents (enter sql, get back the results)"
68,Design Tools - tools for designing and visualizing database schemas
68,"Old possibly abandoned projects, see Community_Guide_to_PostgreSQL_Tools_Abandoned"
68,"Retrieved from ""https://wiki.postgresql.org/index.php?title=Community_Guide_to_PostgreSQL_GUI_Tools&oldid=38452"""
68,Navigation menuPage actionsPageDiscussionView sourceHistoryPage actionsPageDiscussionMoreToolsIn other languagesPersonal toolsLog inNavigationMain PageRandom pageRecent changesHelpToolsWhat links hereRelated changesSpecial pagesPrintable versionPermanent linkPage informationSearch
68,"This page was last edited on 1 December 2023, at 19:59.Privacy policyAbout PostgreSQL wikiDisclaimers"
69,An Introduction to Hibernate 6
69,An Introduction to Hibernate 6
69,version 6.3.2.Final
69,Table of Contents
69,Preface
69,1. Introduction
69,1.1. Hibernate and JPA
69,1.2. Writing Java code with Hibernate
69,"1.3. Hello, Hibernate"
69,"1.4. Hello, JPA"
69,1.5. Organizing persistence logic
69,1.6. Testing persistence logic
69,1.7. Architecture and the persistence layer
69,1.8. Overview
69,2. Configuration and bootstrap
69,2.1. Including Hibernate in your project build
69,2.2. Optional dependencies
69,2.3. Configuration using JPA XML
69,2.4. Configuration using Hibernate API
69,2.5. Configuration using Hibernate properties file
69,2.6. Basic configuration settings
69,2.7. Automatic schema export
69,2.8. Logging the generated SQL
69,2.9. Minimizing repetitive mapping information
69,2.10. Nationalized character data in SQL Server
69,3. Entities
69,3.1. Entity classes
69,3.2. Access types
69,3.3. Entity class inheritance
69,3.4. Identifier attributes
69,3.5. Generated identifiers
69,3.6. Natural keys as identifiers
69,3.7. Composite identifiers
69,3.8. Version attributes
69,3.9. Natural id attributes
69,3.10. Basic attributes
69,3.11. Enumerated types
69,3.12. Converters
69,3.13. Compositional basic types
69,3.14. Embeddable objects
69,3.15. Associations
69,3.16. Many-to-one
69,3.17. One-to-one (first way)
69,3.18. One-to-one (second way)
69,3.19. Many-to-many
69,3.20. Collections of basic values and embeddable objects
69,3.21. Collections mapped to SQL arrays
69,3.22. Collections mapped to a separate table
69,3.23. Summary of annotations
69,3.24. equals() and hashCode()
69,4. Object/relational mapping
69,4.1. Mapping entity inheritance hierarchies
69,4.2. Mapping to tables
69,4.3. Mapping entities to tables
69,4.4. Mapping associations to tables
69,4.5. Mapping to columns
69,4.6. Mapping basic attributes to columns
69,4.7. Mapping associations to foreign key columns
69,4.8. Mapping primary key joins between tables
69,4.9. Column lengths and adaptive column types
69,4.10. LOBs
69,4.11. Mapping embeddable types to UDTs or to JSON
69,4.12. Summary of SQL column type mappings
69,4.13. Mapping to formulas
69,4.14. Derived Identity
69,4.15. Adding constraints
69,5. Interacting with the database
69,5.1. Persistence Contexts
69,5.2. Creating a session
69,5.3. Managing transactions
69,5.4. Operations on the persistence context
69,5.5. Cascading persistence operations
69,5.6. Proxies and lazy fetching
69,5.7. Entity graphs and eager fetching
69,5.8. Flushing the session
69,5.9. Queries
69,5.10. HQL queries
69,5.11. Criteria queries
69,5.12. A more comfortable way to write criteria queries
69,5.13. Native SQL queries
69,"5.14. Limits, pagination, and ordering"
69,5.15. Representing projection lists
69,5.16. Named queries
69,5.17. Controlling lookup by id
69,5.18. Interacting directly with JDBC
69,5.19. What to do when things go wrong
69,6. Compile-time tooling
69,6.1. Named queries and the Metamodel Generator
69,6.2. Generated query methods
69,6.3. Generating query methods as instance methods
69,6.4. Generated finder methods
69,6.5. Paging and ordering
69,6.6. Query and finder method return types
69,6.7. An alternative approach
69,7. Tuning and performance
69,7.1. Tuning the connection pool
69,7.2. Enabling statement batching
69,7.3. Association fetching
69,7.4. Batch fetching and subselect fetching
69,7.5. Join fetching
69,7.6. The second-level cache
69,7.7. Specifying which data is cached
69,7.8. Caching by natural id
69,7.9. Caching and association fetching
69,7.10. Configuring the second-level cache provider
69,7.11. Caching query result sets
69,7.12. Second-level cache management
69,7.13. Session cache management
69,7.14. Stateless sessions
69,7.15. Optimistic and pessimistic locking
69,7.16. Collecting statistics
69,7.17. Tracking down slow queries
69,7.18. Adding indexes
69,7.19. Dealing with denormalized data
69,7.20. Reactive programming with Hibernate
69,8. Advanced Topics
69,8.1. Filters
69,8.2. Multi-tenancy
69,8.3. Using custom-written SQL
69,8.4. Handling database-generated columns
69,8.5. User-defined generators
69,8.6. Naming strategies
69,8.7. Spatial datatypes
69,8.8. Ordered and sorted collections and map keys
69,8.9. Any mappings
69,8.10. Selective column lists in inserts and updates
69,8.11. Using the bytecode enhancer
69,8.12. Named fetch profiles
69,9. Credits
69,Preface
69,Hibernate 6 is a major redesign of the world’s most popular and feature-rich ORM solution.
69,"The redesign has touched almost every subsystem of Hibernate, including the APIs, mapping annotations, and the query language."
69,"This new Hibernate is more powerful, more robust, and more typesafe."
69,"With so many improvements, it’s very difficult to summarize the significance of this work."
69,But the following general themes stand out.
69,Hibernate 6:
69,"finally takes advantage of the advances in relational databases over the past decade, updating the query language to support a raft of new constructs in modern dialects of SQL,"
69,"exhibits much more consistent behavior across different databases, greatly improving portability, and generates much higher-quality DDL from dialect-independent code,"
69,"improves error reporting by more scrupulous validation of queries before access to the database,"
69,"improves the type-safety of O/R mapping annotations, clarifies the separation of API, SPI, and internal implementation, and fixes some long-standing architectural flaws,"
69,"removes or deprecates legacy APIs, laying the foundation for future evolution, and"
69,"makes far better use of Javadoc, putting much more information at the fingertips of developers."
69,"Hibernate 6 and Hibernate Reactive are now core components of Quarkus 3, the most exciting new environment for cloud-native development in Java, and Hibernate remains the persistence solution of choice for almost every major Java framework or server."
69,"Unfortunately, the changes in Hibernate 6 have obsoleted much of the information about Hibernate that’s available in books, in blog posts, and on stackoverflow."
69,"This guide is an up-to-date, high-level discussion of the current feature set and recommended usage."
69,It does not attempt to cover every feature and should be used in conjunction with other documentation:
69,"Hibernate’s extensive Javadoc,"
69,"the Guide to Hibernate Query Language, and"
69,the Hibernate User Guide.
69,The Hibernate User Guide includes detailed discussions of most aspects of Hibernate.
69,"But with so much information to cover, readability is difficult to achieve, and so it’s most useful as a reference."
69,"Where necessary, we’ll provide links to relevant sections of the User Guide."
69,1. Introduction
69,Hibernate is usually described as a library that makes it easy to map Java classes to relational database tables.
69,But this formulation does no justice to the central role played by the relational data itself.
69,So a better description might be:
69,"Hibernate makes relational data visible to a program written in Java, in a natural and typesafe form,"
69,"making it easy to write complex queries and work with their results,"
69,"letting the program easily synchronize changes made in memory with the database, respecting the ACID properties of transactions, and"
69,allowing performance optimizations to be made after the basic persistence logic has already been written.
69,"Here the relational data is the focus, along with the importance of typesafety."
69,"The goal of object/relational mapping (ORM) is to eliminate fragile and untypesafe code, and make large programs easier to maintain in the long run."
69,"ORM takes the pain out of persistence by relieving the developer of the need to hand-write tedious, repetitive, and fragile code for flattening graphs of objects to database tables and rebuilding graphs of objects from flat SQL query result sets."
69,"Even better, ORM makes it much easier to tune performance later, after the basic persistence logic has already been written."
69,"A perennial question is: should I use ORM, or plain SQL?"
69,The answer is usually: use both.
69,JPA and Hibernate were designed to work in conjunction with handwritten SQL.
69,"You see, most programs with nontrivial data access logic will benefit from the use of ORM at least somewhere."
69,"But if Hibernate is making things more difficult, for some particularly tricky piece of data access logic, the only sensible thing to do is to use something better suited to the problem!"
69,Just because you’re using Hibernate for persistence doesn’t mean you have to use it for everything.
69,"Developers often ask about the relationship between Hibernate and JPA, so let’s take a short detour into some history."
69,1.1. Hibernate and JPA
69,"Hibernate was the inspiration behind the Java (now Jakarta) Persistence API, or JPA, and includes a complete implementation of the latest revision of this specification."
69,The early history of Hibernate and JPA
69,"The Hibernate project began in 2001, when Gavin King’s frustration with Entity Beans in EJB 2 boiled over."
69,"It quickly overtook other open source and commercial contenders to become the most popular persistence solution for Java, and the book Hibernate in Action, written with Christian Bauer, was an influential bestseller."
69,"In 2004, Gavin and Christian joined a tiny startup called JBoss, and other early Hibernate contributors soon followed: Max Rydahl Andersen, Emmanuel Bernard, Steve Ebersole, and Sanne Grinovero."
69,"Soon after, Gavin joined the EJB 3 expert group and convinced the group to deprecate Entity Beans in favor of a brand-new persistence API modelled after Hibernate."
69,"Later, members of the TopLink team got involved, and the Java Persistence API evolved as a collaboration between—primarily—Sun, JBoss, Oracle, and Sybase, under the leadership of Linda Demichiel."
69,"Over the intervening two decades, many talented people have contributed to the development of Hibernate."
69,"We’re all especially grateful to Steve, who has led the project for many years, since Gavin stepped back to focus in other work."
69,We can think of the API of Hibernate in terms of three basic elements:
69,"an implementation of the JPA-defined APIs, most importantly, of the interfaces EntityManagerFactory and EntityManager, and of the JPA-defined O/R mapping annotations,"
69,"a native API exposing the full set of available functionality, centered around the interfaces SessionFactory, which extends EntityManagerFactory, and Session, which extends EntityManager, and"
69,"a set of mapping annotations which augment the O/R mapping annotations defined by JPA, and which may be used with the JPA-defined interfaces, or with the native API."
69,"Hibernate also offers a range of SPIs for frameworks and libraries which extend or integrate with Hibernate, but we’re not interested in any of that stuff here."
69,"As an application developer, you must decide whether to:"
69,"write your program in terms of Session and SessionFactory, or"
69,"maximize portability to other implementations of JPA by, wherever reasonable, writing code in terms of"
69,"EntityManager and EntityManagerFactory, falling back to the native APIs only where necessary."
69,"Whichever path you take, you will use the JPA-defined mapping annotations most of the time, and the Hibernate-defined annotations for more advanced mapping problems."
69,"You might wonder if it’s possible to develop an application using only JPA-defined APIs, and, indeed, that’s possible in principle."
69,JPA is a great baseline that really nails the basics of the object/relational mapping problem.
69,"But without the native APIs, and extended mapping annotations, you miss out on much of the power of Hibernate."
69,"Since Hibernate existed before JPA, and since JPA was modelled on Hibernate, we unfortunately have some competition and duplication in naming between the standard and native APIs."
69,For example:
69,Table 1. Examples of competing APIs with similar naming
69,Hibernate
69,JPA
69,org.hibernate.annotations.CascadeType
69,javax.persistence.CascadeType
69,org.hibernate.FlushMode
69,javax.persistence.FlushModeType
69,org.hibernate.annotations.FetchMode
69,javax.persistence.FetchType
69,org.hibernate.query.Query
69,javax.persistence.Query
69,org.hibernate.Cache
69,javax.persistence.Cache
69,@org.hibernate.annotations.NamedQuery
69,@javax.persistence.NamedQuery
69,@org.hibernate.annotations.Cache
69,@javax.persistence.Cacheable
69,"Typically, the Hibernate-native APIs offer something a little extra that’s missing in JPA, so this isn’t exactly a flaw."
69,But it’s something to watch out for.
69,1.2. Writing Java code with Hibernate
69,"If you’re completely new to Hibernate and JPA, you might already be wondering how the persistence-related code is structured."
69,"Well, typically, our persistence-related code comes in two layers:"
69,"a representation of our data model in Java, which takes the form of a set of annotated entity classes, and"
69,a larger number of functions which interact with Hibernate’s APIs to perform the persistence operations associated with your various transactions.
69,"The first part, the data or ""domain"" model, is usually easier to write, but doing a great and very clean job of it will strongly affect your success in the second part."
69,"Most people implement the domain model as a set of what we used to call ""Plain Old Java Objects"", that is, as simple Java classes with no direct dependencies on technical infrastructure, nor on application logic which deals with request processing, transaction management, communications, or interaction with the database."
69,"Take your time with this code, and try to produce a Java model that’s as close as reasonable to the relational data model. Avoid using exotic or advanced mapping features when they’re not really needed."
69,"When in the slightest doubt, map a foreign key relationship using @ManyToOne with @OneToMany(mappedBy=…​) in preference to more complicated association mappings."
69,The second part of the code is much trickier to get right. This code must:
69,"manage transactions and sessions,"
69,"interact with the database via the Hibernate session,"
69,"fetch and prepare data needed by the UI, and"
69,handle failures.
69,"Responsibility for transaction and session management, and for recovery from certain kinds of failure, is best handled in some sort of framework code."
69,"We’re going to come back soon to the thorny question of how this persistence logic should be organized, and how it should fit into the rest of the system."
69,"1.3. Hello, Hibernate"
69,"Before we get deeper into the weeds, we’ll quickly present a basic example program that will help you get started if you don’t already have Hibernate integrated into your project."
69,We begin with a simple gradle build file:
69,build.gradle
69,plugins {
69,id 'java'
69,group = 'org.example'
69,version = '1.0-SNAPSHOT'
69,repositories {
69,mavenCentral()
69,dependencies {
69,// the GOAT ORM
69,implementation 'org.hibernate.orm:hibernate-core:6.3.0.Final'
69,// Hibernate Validator
69,implementation 'org.hibernate.validator:hibernate-validator:8.0.0.Final'
69,implementation 'org.glassfish:jakarta.el:4.0.2'
69,// Agroal connection pool
69,implementation 'org.hibernate.orm:hibernate-agroal:6.3.0.Final'
69,implementation 'io.agroal:agroal-pool:2.1'
69,// logging via Log4j
69,implementation 'org.apache.logging.log4j:log4j-core:2.20.0'
69,// JPA Metamodel Generator
69,annotationProcessor 'org.hibernate.orm:hibernate-jpamodelgen:6.3.0.Final'
69,// Compile-time checking for HQL
69,//implementation 'org.hibernate:query-validator:2.0-SNAPSHOT'
69,//annotationProcessor 'org.hibernate:query-validator:2.0-SNAPSHOT'
69,// H2 database
69,runtimeOnly 'com.h2database:h2:2.1.214'
69,Only the first of these dependencies is absolutely required to run Hibernate.
69,"Next, we’ll add a logging configuration file for log4j:"
69,log4j2.properties
69,rootLogger.level = info
69,rootLogger.appenderRefs = console
69,rootLogger.appenderRef.console.ref = console
69,logger.hibernate.name = org.hibernate.SQL
69,logger.hibernate.level = info
69,appender.console.name = console
69,appender.console.type = Console
69,appender.console.layout.type = PatternLayout
69,appender.console.layout.pattern = %highlight{[%p]} %m%n
69,Now we need some Java code.
69,We begin with our entity class:
69,Book.java
69,package org.hibernate.example;
69,import jakarta.persistence.Entity;
69,import jakarta.persistence.Id;
69,import jakarta.validation.constraints.NotNull;
69,@Entity
69,class Book {
69,@Id
69,String isbn;
69,@NotNull
69,String title;
69,Book() {}
69,"Book(String isbn, String title) {"
69,this.isbn = isbn;
69,this.title = title;
69,"Finally, let’s see code which configures and instantiates Hibernate and asks it to persist and query the entity."
69,Don’t worry if this makes no sense at all right now.
69,It’s the job of this Introduction to make all this crystal clear.
69,Main.java
69,package org.hibernate.example;
69,import org.hibernate.cfg.Configuration;
69,import static java.lang.Boolean.TRUE;
69,import static java.lang.System.out;
69,import static org.hibernate.cfg.AvailableSettings.*;
69,public class Main {
69,public static void main(String[] args) {
69,var sessionFactory = new Configuration()
69,.addAnnotatedClass(Book.class)
69,// use H2 in-memory database
69,".setProperty(URL, ""jdbc:h2:mem:db1"")"
69,".setProperty(USER, ""sa"")"
69,".setProperty(PASS, """")"
69,// use Agroal connection pool
69,".setProperty(""hibernate.agroal.maxSize"", ""20"")"
69,// display SQL in console
69,".setProperty(SHOW_SQL, TRUE.toString())"
69,".setProperty(FORMAT_SQL, TRUE.toString())"
69,".setProperty(HIGHLIGHT_SQL, TRUE.toString())"
69,.buildSessionFactory();
69,// export the inferred database schema
69,sessionFactory.getSchemaManager().exportMappedObjects(true);
69,// persist an entity
69,sessionFactory.inTransaction(session -> {
69,"session.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
69,});
69,// query data using HQL
69,sessionFactory.inSession(session -> {
69,"out.println(session.createSelectionQuery(""select isbn||': '||title from Book"").getSingleResult());"
69,});
69,// query data using criteria API
69,sessionFactory.inSession(session -> {
69,var builder = sessionFactory.getCriteriaBuilder();
69,var query = builder.createQuery(String.class);
69,var book = query.from(Book.class);
69,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
69,book.get(Book_.title)));
69,out.println(session.createSelectionQuery(query).getSingleResult());
69,});
69,Here we’ve used Hibernate’s native APIs.
69,We could have used JPA-standard APIs to achieve the same thing.
69,"1.4. Hello, JPA"
69,"If we limit ourselves to the use of JPA-standard APIs, we need to use XML to configure Hibernate."
69,META-INF/persistence.xml
69,"<persistence xmlns=""https://jakarta.ee/xml/ns/persistence"""
69,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
69,"xsi:schemaLocation=""https://jakarta.ee/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
69,"version=""3.0"">"
69,"<persistence-unit name=""example"">"
69,<class>org.hibernate.example.Book</class>
69,<properties>
69,<!-- H2 in-memory database -->
69,"<property name=""jakarta.persistence.jdbc.url"""
69,"value=""jdbc:h2:mem:db1""/>"
69,<!-- Credentials -->
69,"<property name=""jakarta.persistence.jdbc.user"""
69,"value=""sa""/>"
69,"<property name=""jakarta.persistence.jdbc.password"""
69,"value=""""/>"
69,<!-- Agroal connection pool -->
69,"<property name=""hibernate.agroal.maxSize"""
69,"value=""20""/>"
69,<!-- display SQL in console -->
69,"<property name=""hibernate.show_sql"" value=""true""/>"
69,"<property name=""hibernate.format_sql"" value=""true""/>"
69,"<property name=""hibernate.highlight_sql"" value=""true""/>"
69,</properties>
69,</persistence-unit>
69,</persistence>
69,Note that our build.gradle and log4j2.properties files are unchanged.
69,Our entity class is also unchanged from what we had before.
69,"Unfortunately, JPA doesn’t offer an inSession() method, so we’ll have to implement session and transaction management ourselves."
69,"We can put that logic in our own inSession() function, so that we don’t have to repeat it for every transaction."
69,"Again, you don’t need to understand any of this code right now."
69,Main.java (JPA version)
69,package org.hibernate.example;
69,import jakarta.persistence.EntityManager;
69,import jakarta.persistence.EntityManagerFactory;
69,import java.util.Map;
69,import java.util.function.Consumer;
69,import static jakarta.persistence.Persistence.createEntityManagerFactory;
69,import static java.lang.System.out;
69,import static org.hibernate.cfg.AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION;
69,import static org.hibernate.tool.schema.Action.CREATE;
69,public class Main {
69,public static void main(String[] args) {
69,"var factory = createEntityManagerFactory(""example"","
69,// export the inferred database schema
69,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE));"
69,// persist an entity
69,"inSession(factory, entityManager -> {"
69,"entityManager.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
69,});
69,// query data using HQL
69,"inSession(factory, entityManager -> {"
69,"out.println(entityManager.createQuery(""select isbn||': '||title from Book"").getSingleResult());"
69,});
69,// query data using criteria API
69,"inSession(factory, entityManager -> {"
69,var builder = factory.getCriteriaBuilder();
69,var query = builder.createQuery(String.class);
69,var book = query.from(Book.class);
69,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
69,book.get(Book_.title)));
69,out.println(entityManager.createQuery(query).getSingleResult());
69,});
69,"// do some work in a session, performing correct transaction management"
69,"static void inSession(EntityManagerFactory factory, Consumer<EntityManager> work) {"
69,var entityManager = factory.createEntityManager();
69,var transaction = entityManager.getTransaction();
69,try {
69,transaction.begin();
69,work.accept(entityManager);
69,transaction.commit();
69,catch (Exception e) {
69,if (transaction.isActive()) transaction.rollback();
69,throw e;
69,finally {
69,entityManager.close();
69,"In practice, we never access the database directly from a main() method."
69,So now let’s talk about how to organize persistence logic in a real system.
69,The rest of this chapter is not compulsory.
69,"If you’re itching for more details about Hibernate itself, you’re quite welcome to skip straight to the next chapter, and come back later."
69,1.5. Organizing persistence logic
69,"In a real program, persistence logic like the code shown above is usually interleaved with other sorts of code, including logic:"
69,"implementing the rules of the business domain, or"
69,for interacting with the user.
69,"Therefore, many developers quickly—even too quickly, in our opinion—reach for ways to isolate the persistence logic into some sort of separate architectural layer."
69,We’re going to ask you to suppress this urge for now.
69,The easiest way to use Hibernate is to call the Session or EntityManager directly.
69,"If you’re new to Hibernate, frameworks which wrap JPA are only going to make your life more difficult."
69,We prefer a bottom-up approach to organizing our code.
69,"We like to start thinking about methods and functions, not about architectural layers and container-managed objects."
69,"To illustrate the sort of approach to code organization that we advocate, let’s consider a service which queries the database using HQL or SQL."
69,"We might start with something like this, a mix of UI and persistence logic:"
69,"@Path(""/"") @Produces(""application/json"")"
69,public class BookResource {
69,"@GET @Path(""book/{isbn}"")"
69,public Book getBook(String isbn) {
69,"var book = sessionFactory.fromTransaction(session -> session.find(Book.class, isbn));"
69,return book == null ? Response.status(404).build() : book;
69,"Indeed, we might also finish with something like that—it’s quite hard to identify anything concretely wrong with the code above, and for such a simple case it seems really difficult to justify making this code more complicated by introducing additional objects."
69,"One very nice aspect of this code, which we wish to draw your attention to, is that session and transaction management is handled by generic ""framework"" code, just as we already recommended above."
69,"In this case, we’re using the fromTransaction() method, which happens to come built in to Hibernate."
69,"But you might prefer to use something else, for example:"
69,"in a container environment like Jakarta EE or Quarkus, container-managed transactions and container-managed persistence contexts, or"
69,something you write yourself.
69,"The important thing is that calls like createEntityManager() and getTransaction().begin() don’t belong in regular program logic, because it’s tricky and tedious to get the error handling correct."
69,Let’s now consider a slightly more complicated case.
69,"@Path(""/"") @Produces(""application/json"")"
69,public class BookResource {
69,private static final RESULTS_PER_PAGE = 20;
69,"@GET @Path(""books/{titlePattern}/{page:\\d+}"")"
69,"public List<Book> findBooks(String titlePattern, int page) {"
69,var books = sessionFactory.fromTransaction(session -> {
69,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
69,".setParameter(1, titlePattern)"
69,".setPage(Page.page(RESULTS_PER_PAGE, page))"
69,.getResultList();
69,});
69,return books.isEmpty() ? Response.status(404).build() : books;
69,"This is fine, and we won’t complain if you prefer to leave the code exactly as it appears above."
69,But there’s one thing we could perhaps improve.
69,"We love super-short methods with single responsibilities, and there looks to be an opportunity to introduce one here."
69,"Let’s hit the code with our favorite thing, the Extract Method refactoring. We obtain:"
69,"static List<Book> findBooksByTitleWithPagination(Session session,"
69,"String titlePattern, Page page) {"
69,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
69,".setParameter(1, titlePattern)"
69,.setPage(page)
69,.getResultList();
69,"This is an example of a query method, a function which accepts arguments to the parameters of a HQL or SQL query, and executes the query, returning its results to the caller."
69,"And that’s all it does; it doesn’t orchestrate additional program logic, and it doesn’t perform transaction or session management."
69,"It’s even better to specify the query string using the @NamedQuery annotation, so that Hibernate can validate the query it at startup time, that is, when the SessionFactory is created, instead of when the query is first executed."
69,"Indeed, since we included the Metamodel Generator in our Gradle build, the query can even be validated at compile time."
69,"We need a place to put the annotation, so lets move our query method to a new class:"
69,@CheckHQL // validate named queries at compile time
69,"@NamedQuery(name=""findBooksByTitle"","
69,"query=""from Book where title like :title order by title"")"
69,class Queries {
69,"static List<Book> findBooksByTitleWithPagination(Session session,"
69,"String titlePattern, Page page) {"
69,"return session.createNamedQuery(""findBooksByTitle"", Book.class)"
69,".setParameter(""title"", titlePattern)"
69,.setPage(page)
69,.getResultList();
69,Notice that our query method doesn’t attempt to hide the EntityManager from its clients.
69,"Indeed, the client code is responsible for providing the EntityManager or Session to the query method."
69,This is a quite distinctive feature of our whole approach.
69,The client code may:
69,"obtain an EntityManager or Session by calling inTransaction() or fromTransaction(), as we saw above, or,"
69,"in an environment with container-managed transactions, it might obtain it via dependency injection."
69,"Whatever the case, the code which orchestrates a unit of work usually just calls the Session or EntityManager directly, passing it along to helper methods like our query method if necessary."
69,@GET
69,"@Path(""books/{titlePattern}"")"
69,public List<Book> findBooks(String titlePattern) {
69,var books = sessionFactory.fromTransaction(session ->
69,"Queries.findBooksByTitleWithPagination(session, titlePattern,"
69,"Page.page(RESULTS_PER_PAGE, page));"
69,return books.isEmpty() ? Response.status(404).build() : books;
69,You might be thinking that our query method looks a bit boilerplatey.
69,"That’s true, perhaps, but we’re much more concerned that it’s not very typesafe."
69,"Indeed, for many years, the lack of compile-time checking for HQL queries and code which binds arguments to query parameters was our number one source of discomfort with Hibernate."
69,"Fortunately, there’s now a solution to both problems: as an incubating feature of Hibernate 6.3, we now offer the possibility to have the Metamodel Generator fill in the implementation of such query methods for you."
69,"This facility is the topic of a whole chapter of this introduction, so for now we’ll just leave you with one simple example."
69,Suppose we simplify Queries to just the following:
69,interface Queries {
69,"@HQL(""where title like :title order by title"")"
69,"List<Book> findBooksByTitleWithPagination(String title, Page page);"
69,Then the Metamodel Generator automatically produces an implementation of the method annotated @HQL in a class named Queries_.
69,We can call it just like we called our handwritten version:
69,@GET
69,"@Path(""books/{titlePattern}"")"
69,public List<Book> findBooks(String titlePattern) {
69,var books = sessionFactory.fromTransaction(session ->
69,"Queries_.findBooksByTitleWithPagination(session, titlePattern,"
69,"Page.page(RESULTS_PER_PAGE, page));"
69,return books.isEmpty() ? Response.status(404).build() : books;
69,"In this case, the quantity of code eliminated is pretty trivial."
69,The real value is in improved type safety.
69,We now find out about errors in assignments of arguments to query parameters at compile time.
69,"At this point, we’re certain you’re full of doubts about this idea."
69,And quite rightly so.
69,"We would love to answer your objections right here, but that will take us much too far off track."
69,So we ask you to file away these thoughts for now.
69,We promise to make it make sense when we properly address this topic later.
69,"And, after that, if you still don’t like this approach, please understand that it’s completely optional."
69,Nobody’s going to come around to your house to force it down your throat.
69,"Now that we have a rough picture of what our persistence logic might look like, it’s natural to ask how we should test our code."
69,1.6. Testing persistence logic
69,"When we write tests for our persistence logic, we’re going to need:"
69,"a database, with"
69,"an instance of the schema mapped by our persistent entities, and"
69,"a set of test data, in a well-defined state at the beginning of each test."
69,"It might seem obvious that we should test against the same database system that we’re going to use in production, and, indeed, we should certainly have at least some tests for this configuration."
69,"But on the other hand, tests which perform I/O are much slower than tests which don’t, and most databases can’t be set up to run in-process."
69,"So, since most persistence logic written using Hibernate 6 is extremely portable between databases, it often makes good sense to test against an in-memory Java database."
69,(H2 is the one we recommend.)
69,"We do need to be careful here if our persistence code uses native SQL, or if it uses concurrency-management features like pessimistic locks."
69,"Whether we’re testing against our real database, or against an in-memory Java database, we’ll need to export the schema at the beginning of a test suite."
69,"We usually do this when we create the Hibernate SessionFactory or JPA EntityManager, and so traditionally we’ve used a configuration property for this."
69,The JPA-standard property is jakarta.persistence.schema-generation.database.action.
69,"For example, if we’re using Configuration to configure Hibernate, we could write:"
69,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
69,Action.SPEC_ACTION_DROP_AND_CREATE);
69,"Alternatively, in Hibernate 6, we may use the new SchemaManager API to export the schema, just as we did above."
69,sessionFactory.getSchemaManager().exportMappedObjects(true);
69,"Since executing DDL statements is very slow on many databases, we don’t want to do this before every test."
69,"Instead, to ensure that each test begins with the test data in a well-defined state, we need to do two things before each test:"
69,"clean up any mess left behind by the previous test, and then"
69,reinitialize the test data.
69,"We may truncate all the tables, leaving an empty database schema, using the SchemaManager."
69,sessionFactory.getSchemaManager().truncateMappedObjects();
69,"After truncating tables, we might need to initialize our test data."
69,"We may specify test data in a SQL script, for example:"
69,/import.sql
69,"insert into Books (isbn, title) values ('9781932394153', 'Hibernate in Action')"
69,"insert into Books (isbn, title) values ('9781932394887', 'Java Persistence with Hibernate')"
69,"insert into Books (isbn, title) values ('9781617290459', 'Java Persistence with Hibernate, Second Edition')"
69,"If we name this file import.sql, and place it in the root classpath, that’s all we need to do."
69,"Otherwise, we need to specify the file in the configuration property jakarta.persistence.sql-load-script-source."
69,"If we’re using Configuration to configure Hibernate, we could write:"
69,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_LOAD_SCRIPT_SOURCE,"
69,"""/org/example/test-data.sql"");"
69,The SQL script will be executed every time exportMappedObjects() or truncateMappedObjects() is called.
69,There’s another sort of mess a test can leave behind: cached data in the second-level cache.
69,We recommend disabling Hibernate’s second-level cache for most sorts of testing.
69,"Alternatively, if the second-level cache is not disabled, then before each test we should call:"
69,sessionFactory.getCache().evictAllRegions();
69,"Now, suppose you’ve followed our advice, and written your entities and query methods to minimize dependencies on ""infrastructure"", that is, on libraries other than JPA and Hibernate, on frameworks,"
69,"on container-managed objects, and even on bits of your own system which are hard to instantiate from scratch."
69,Then testing persistence logic is now straightforward!
69,You’ll need to:
69,"bootstrap Hibernate and create a SessionFactory or EntityManagerFactory and the beginning of your test suite (we’ve already seen how to do that), and"
69,"create a new Session or EntityManager inside each @Test method, using inTransaction(), for example."
69,"Actually, some tests might require multiple sessions."
69,But be careful not to leak a session between different tests.
69,Another important test we’ll need is one which validates our O/R mapping annotations against the actual database schema.
69,"This is again the job of the schema management tooling, either:"
69,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
69,Action.ACTION_VALIDATE);
69,Or:
69,sessionFactory.getSchemaManager().validateMappedObjects();
69,"This ""test"" is one which many people like to run even in production, when the system starts up."
69,1.7. Architecture and the persistence layer
69,"Let’s now consider a different approach to code organization, one we treat with suspicion."
69,"In this section, we’re going to give you our opinion."
69,"If you’re only interested in facts, or if you prefer not to read things that might undermine the opinion you currently hold, please feel free to skip straight to the next chapter."
69,"Hibernate is an architecture-agnostic library, not a framework, and therefore integrates comfortably with a wide range of Java frameworks and containers."
69,"Consistent with our place within the ecosystem, we’ve historically avoided giving out much advice on architecture."
69,"This is a practice we’re now perhaps inclined to regret, since the resulting vacuum has come to be filled with advice from people advocating architectures, design patterns, and extra frameworks which we suspect make Hibernate a bit less pleasant to use than it should be."
69,"In particular, frameworks which wrap JPA seem to add bloat while subtracting some of the fine-grained control over data access that Hibernate works so hard to provide."
69,"These frameworks don’t expose the full feature set of Hibernate, and so the program is forced to work with a less powerful abstraction."
69,"The stodgy, dogmatic, conventional wisdom, which we hesitate to challenge for simple fear of pricking ourselves on the erect hackles that inevitably accompany such dogma-baiting is:"
69,Code which interacts with the database belongs in a separate persistence layer.
69,We lack the courage—perhaps even the conviction—to tell you categorically to not follow this recommendation.
69,"But we do ask you to consider the cost in boilerplate of any architectural layer, and whether the benefits this cost buys are really worth it in the context of your system."
69,"To add a little background texture to this discussion, and at the risk of our Introduction degenerating into a rant at such an early stage, we’re going ask you to humor us while talk a little more about ancient history."
69,An epic tale of DAOs and Repositories
69,"Back in the dark days of Java EE 4, before the standardization of Hibernate, and subsequent ascendance of JPA in Java enterprise development, it was common to hand-code the messy JDBC interactions that Hibernate takes care of today."
69,"In those terrible times, a pattern arose that we used to call Data Access Objects (DAOs)."
69,"A DAO gave you a place to put all that nasty JDBC code, leaving the important program logic cleaner."
69,"When Hibernate arrived suddenly on the scene in 2001, developers loved it."
69,"But Hibernate implemented no specification, and many wished to reduce or at least localize the dependence of their project logic on Hibernate."
69,"An obvious solution was to keep the DAOs around, but to replace the JDBC code inside them with calls to the Hibernate Session."
69,We partly blame ourselves for what happened next.
69,Back in 2002 and 2003 this really seemed like a pretty reasonable thing to do.
69,"In fact, we contributed to the popularity of this approach by recommending—or at least not discouraging—the use of DAOs in Hibernate in Action."
69,"We hereby apologize for this mistake, and for taking much too long to recognize it."
69,"Eventually, some folks came to believe that their DAOs shielded their program from depending in a hard way on ORM, allowing them to ""swap out"" Hibernate, and replace it with JDBC, or with something else."
69,"In fact, this was never really true—there’s quite a deep difference between the programming model of JDBC, where every interaction with the database is explicit and synchronous, and the programming model of stateful sessions in Hibernate, where updates are implicit, and SQL statements are executed asynchronously."
69,"But then the whole landscape for persistence in Java changed in April 2006, when the final draft of JPA 1.0 was approved."
69,"Java now had a standard way to do ORM, with multiple high-quality implementations of the standard API."
69,"This was the end of the line for the DAOs, right?"
69,"Well, no."
69,It wasn’t.
69,"DAOs were rebranded ""repositories"", and continue to enjoy a sort-of zombie afterlife as a front-end to JPA."
69,"But are they really pulling their weight, or are they just unnecessary extra complexity and bloat? An extra layer of indirection that makes stack traces harder to read and code harder to debug?"
69,Our considered view is that they’re mostly just bloat.
69,"The JPA EntityManager is a ""repository"", and it’s a standard repository with a well-defined specification written by people who spend all day thinking about persistence."
69,"If these repository frameworks offered anything actually useful—and not obviously foot-shooty—over and above what EntityManager provides, we would have already added it to EntityManager decades ago."
69,"Ultimately, we’re not sure you need a separate persistence layer at all."
69,At least consider the possibility that it might be OK to call the EntityManager directly from your business logic.
69,We can already hear you hissing at our heresy.
69,"But before slamming shut the lid of your laptop and heading off to fetch garlic and a pitchfork, take a couple of hours to weigh what we’re proposing."
69,"OK, so, look, if it makes you feel better, one way to view EntityManager is to think of it as a single generic ""repository"" that works for every entity in your system."
69,"From this point of view, JPA is your persistence layer."
69,And there’s few good reasons to wrap this abstraction in a second abstraction that’s less generic.
69,"Even where a distinct persistence layer is appropriate, DAO-style repositories aren’t the unambiguously most-correct way to factorize the equation:"
69,"most nontrivial queries touch multiple entities, and so it’s often quite ambiguous which repository such a query belongs to,"
69,"most queries are extremely specific to a particular fragment of program logic, and aren’t reused in different places across the system, and"
69,the various operations of a repository rarely interact or share common internal implementation details.
69,"Indeed, repositories, by nature, exhibit very low cohesion."
69,"A layer of repository objects might make sense if you have multiple implementations of each repository, but in practice almost nobody ever does."
69,"That’s because they’re also extremely highly coupled to their clients, with a very large API surface."
69,"And, on the contrary, a layer is only easily replaceable if it has a very narrow API."
69,"Some people do indeed use mock repositories for testing, but we really struggle to see any value in this."
69,"If we don’t want to run our tests against our real database, it’s usually very easy to ""mock"" the database itself by running tests against an in-memory Java database like H2."
69,"This works even better in Hibernate 6 than in older versions of Hibernate, since HQL is now much more portable between platforms."
69,"Phew, let’s move on."
69,1.8. Overview
69,It’s now time to begin our journey toward actually understanding the code we saw earlier.
69,This introduction will guide you through the basic tasks involved in developing a program that uses Hibernate for persistence:
69,"configuring and bootstrapping Hibernate, and obtaining an instance of SessionFactory or EntityManagerFactory,"
69,"writing a domain model, that is, a set of entity classes which represent the persistent types in your program, and which map to tables of your database,"
69,"customizing these mappings when the model maps to a pre-existing relational schema,"
69,"using the Session or EntityManager to perform operations which query the database and return entity instances, or which update the data held in the database,"
69,"using the Hibernate Metamodel Generator to improve compile-time type-safety,"
69,"writing complex queries using the Hibernate Query Language (HQL) or native SQL, and, finally"
69,tuning performance of the data access logic.
69,"Naturally, we’ll start at the top of this list, with the least-interesting topic: configuration."
69,2. Configuration and bootstrap
69,We would love to make this section short.
69,"Unfortunately, there’s several distinct ways to configure and bootstrap Hibernate, and we’re going to have to describe at least two of them in detail."
69,The four basic ways to obtain an instance of Hibernate are shown in the following table:
69,"Using the standard JPA-defined XML, and the operation Persistence.createEntityManagerFactory()"
69,Usually chosen when portability between JPA implementations is important.
69,Using the Configuration class to construct a SessionFactory
69,"When portability between JPA implementations is not important, this option is quicker, adds some flexibility and saves a typecast."
69,Using the more complex APIs defined in org.hibernate.boot
69,"Used primarily by framework integrators, this option is outside the scope of this document."
69,By letting the container take care of the bootstrap process and of injecting the SessionFactory or EntityManagerFactory
69,Used in a container environment like WildFly or Quarkus.
69,Here we’ll focus on the first two options.
69,Hibernate in containers
69,"Actually, the last option is extremely popular, since every major Java application server and microservice framework comes with built-in support for Hibernate."
69,Such container environments typically also feature facilities to automatically manage the lifecycle of an EntityManager or Session and its association with container-managed transactions.
69,"To learn how to configure Hibernate in such a container environment, you’ll need to refer to the documentation of your chosen container."
69,"For Quarkus, here’s the relevant documentation."
69,"If you’re using Hibernate outside of a container environment,"
69,you’ll need to:
69,"include Hibernate ORM itself, along with the appropriate JDBC driver, as dependencies of your project, and"
69,"configure Hibernate with information about your database,"
69,by specifying configuration properties.
69,2.1. Including Hibernate in your project build
69,"First, add the following dependency to your project:"
69,org.hibernate.orm:hibernate-core:{version}
69,Where {version} is the version of Hibernate you’re using.
69,You’ll also need to add a dependency for the JDBC
69,driver for your database.
69,Table 2. JDBC driver dependencies
69,Database
69,Driver dependency
69,PostgreSQL or CockroachDB
69,org.postgresql:postgresql:{version}
69,MySQL or TiDB
69,com.mysql:mysql-connector-j:{version}
69,MariaDB
69,org.mariadb.jdbc:mariadb-java-client:{version}
69,DB2
69,com.ibm.db2:jcc:{version}
69,SQL Server
69,com.microsoft.sqlserver:mssql-jdbc:${version}
69,Oracle
69,com.oracle.database.jdbc:ojdbc11:${version}
69,com.h2database:h2:{version}
69,HSQLDB
69,org.hsqldb:hsqldb:{version}
69,Where {version} is the latest version of the JDBC driver for your databse.
69,2.2. Optional dependencies
69,"Optionally, you might also add any of the following additional features:"
69,Table 3. Optional dependencies
69,Optional feature
69,Dependencies
69,An SLF4J logging implementation
69,org.apache.logging.log4j:log4j-core
69,or org.slf4j:slf4j-jdk14
69,"A JDBC connection pool, for example, Agroal"
69,org.hibernate.orm:hibernate-agroal
69,and io.agroal:agroal-pool
69,"The Hibernate Metamodel Generator, especially if you’re using the JPA criteria query API"
69,org.hibernate.orm:hibernate-jpamodelgen
69,"The Query Validator, for compile-time checking of HQL"
69,org.hibernate:query-validator
69,"Hibernate Validator, an implementation of Bean Validation"
69,org.hibernate.validator:hibernate-validator
69,and org.glassfish:jakarta.el
69,Local second-level cache support via JCache and EHCache
69,org.hibernate.orm:hibernate-jcache
69,and org.ehcache:ehcache
69,Local second-level cache support via JCache and Caffeine
69,org.hibernate.orm:hibernate-jcache
69,and com.github.ben-manes.caffeine:jcache
69,Distributed second-level cache support via Infinispan
69,org.infinispan:infinispan-hibernate-cache-v60
69,"A JSON serialization library for working with JSON datatypes, for example, Jackson or Yasson"
69,com.fasterxml.jackson.core:jackson-databind
69,or org.eclipse:yasson
69,Hibernate Spatial
69,org.hibernate.orm:hibernate-spatial
69,"Envers, for auditing historical data"
69,org.hibernate.orm:hibernate-envers
69,You might also add the Hibernate bytecode enhancer to your
69,Gradle build if you want to use field-level lazy fetching.
69,2.3. Configuration using JPA XML
69,"Sticking to the JPA-standard approach, we would provide a file named persistence.xml, which we usually place in the META-INF directory of a persistence archive, that is, of the .jar file or directory which contains our entity classes."
69,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
69,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
69,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
69,"version=""2.0"">"
69,"<persistence-unit name=""org.hibernate.example"">"
69,<class>org.hibernate.example.Book</class>
69,<class>org.hibernate.example.Author</class>
69,<properties>
69,<!-- PostgreSQL -->
69,"<property name=""jakarta.persistence.jdbc.url"""
69,"value=""jdbc:postgresql://localhost/example""/>"
69,<!-- Credentials -->
69,"<property name=""jakarta.persistence.jdbc.user"""
69,"value=""gavin""/>"
69,"<property name=""jakarta.persistence.jdbc.password"""
69,"value=""hibernate""/>"
69,<!-- Automatic schema export -->
69,"<property name=""jakarta.persistence.schema-generation.database.action"""
69,"value=""drop-and-create""/>"
69,<!-- SQL statement logging -->
69,"<property name=""hibernate.show_sql"" value=""true""/>"
69,"<property name=""hibernate.format_sql"" value=""true""/>"
69,"<property name=""hibernate.highlight_sql"" value=""true""/>"
69,</properties>
69,</persistence-unit>
69,</persistence>
69,"The <persistence-unit> element defines a named persistence unit, that is:"
69,"a collection of associated entity types, along with"
69,"a set of default configuration settings, which may be augmented or overridden at runtime."
69,Each <class> element specifies the fully-qualified name of an entity class.
69,Scanning for entity classes
69,"In some container environments, for example, in any EE container, the <class> elements are unnecessary, since the container will scan the archive for annotated classes, and automatically recognize any class annotated @Entity."
69,Each <property> element specifies a configuration property and its value.
69,Note that:
69,"the configuration properties in the jakarta.persistence namespace are standard properties defined by the JPA spec, and"
69,properties in the hibernate namespace are specific to Hibernate.
69,We may obtain an EntityManagerFactory by calling Persistence.createEntityManagerFactory():
69,EntityManagerFactory entityManagerFactory =
69,"Persistence.createEntityManagerFactory(""org.hibernate.example"");"
69,"If necessary, we may override configuration properties specified in persistence.xml:"
69,EntityManagerFactory entityManagerFactory =
69,"Persistence.createEntityManagerFactory(""org.hibernate.example"","
69,"Map.of(AvailableSettings.JAKARTA_JDBC_PASSWORD, password));"
69,2.4. Configuration using Hibernate API
69,"Alternatively, the venerable class Configuration allows an instance of Hibernate to be configured in Java code."
69,SessionFactory sessionFactory =
69,new Configuration()
69,.addAnnotatedClass(Book.class)
69,.addAnnotatedClass(Author.class)
69,// PostgreSQL
69,".setProperty(AvailableSettings.JAKARTA_JDBC_URL, ""jdbc:postgresql://localhost/example"")"
69,// Credentials
69,".setProperty(AvailableSettings.JAKARTA_JDBC_USER, user)"
69,".setProperty(AvailableSettings.JAKARTA_JDBC_PASSWORD, password)"
69,// Automatic schema export
69,".setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
69,Action.SPEC_ACTION_DROP_AND_CREATE)
69,// SQL statement logging
69,".setProperty(AvailableSettings.SHOW_SQL, TRUE.toString())"
69,".setProperty(AvailableSettings.FORMAT_SQL, TRUE.toString())"
69,".setProperty(AvailableSettings.HIGHLIGHT_SQL, TRUE.toString())"
69,// Create a new SessionFactory
69,.buildSessionFactory();
69,"The Configuration class has survived almost unchanged since the very earliest (pre-1.0) versions of Hibernate, and so it doesn’t look particularly modern."
69,"On the other hand, it’s very easy to use, and exposes some options that persistence.xml doesn’t support."
69,Advanced configuration options
69,"Actually, the Configuration class is just a very simple facade for the more modern, much more powerful—but more complex—API defined in the package org.hibernate.boot."
69,"This API is useful if you have very advanced requirements, for example, if you’re writing a framework or implementing a container."
69,"You’ll find more information in the User Guide, and in the package-level documentation of org.hibernate.boot."
69,2.5. Configuration using Hibernate properties file
69,"If we’re using the Hibernate Configuration API, but we don’t want to put certain configuration properties directly in the Java code, we can specify them in a file named hibernate.properties, and place the file in the root classpath."
69,# PostgreSQL
69,jakarta.persistence.jdbc.url=jdbc:postgresql://localhost/example
69,# Credentials
69,jakarta.persistence.jdbc.user=hibernate
69,jakarta.persistence.jdbc.password=zAh7mY$2MNshzAQ5
69,# SQL statement logging
69,hibernate.show_sql=true
69,hibernate.format_sql=true
69,hibernate.highlight_sql=true
69,2.6. Basic configuration settings
69,The class AvailableSettings enumerates all the configuration properties understood by Hibernate.
69,"Of course, we’re not going to cover every useful configuration setting in this chapter."
69,"Instead, we’ll mention the ones you need to get started, and come back to some other important settings later, especially when we talk about performance tuning."
69,Hibernate has many—too many—switches and toggles.
69,"Please don’t go crazy messing about with these settings; most of them are rarely needed, and many only exist to provide backward compatibility with older versions of Hibernate."
69,"With rare exception, the default behavior of every one of these settings was carefully chosen to be the behavior we recommend."
69,The properties you really do need to get started are these three:
69,Table 4. JDBC connection settings
69,Configuration property name
69,Purpose
69,jakarta.persistence.jdbc.url
69,JDBC URL of your database
69,jakarta.persistence.jdbc.user and jakarta.persistence.jdbc.password
69,Your database credentials
69,"In Hibernate 6, you don’t need to specify hibernate.dialect."
69,The correct Hibernate SQL Dialect will be determined for you automatically.
69,The only reason to specify this property is if you’re using a custom user-written Dialect class.
69,"Similarly, neither hibernate.connection.driver_class nor jakarta.persistence.jdbc.driver is needed when working with one of the supported databases."
69,Pooling JDBC connections is an extremely important performance optimization.
69,You can set the size of Hibernate’s built-in connection pool using this property:
69,Table 5. Built-in connection pool size
69,Configuration property name
69,Purpose
69,hibernate.connection.pool_size
69,The size of the built-in connection pool
69,"By default, Hibernate uses a simplistic built-in connection pool."
69,"This pool is not meant for use in production, and later, when we discuss performance, we’ll see how to select a more robust implementation."
69,"Alternatively, in a container environment, you’ll need at least one of these properties:"
69,Table 6. Transaction management settings
69,Configuration property name
69,Purpose
69,jakarta.persistence.transactionType
69,"(Optional, defaults to JTA)"
69,Determines if transaction management is via JTA or resource-local transactions.
69,Specify RESOURCE_LOCAL if JTA should not be used.
69,jakarta.persistence.jtaDataSource
69,JNDI name of a JTA datasource
69,jakarta.persistence.nonJtaDataSource
69,JNDI name of a non-JTA datasource
69,"In this case, Hibernate obtains pooled JDBC database connections from a container-managed DataSource."
69,2.7. Automatic schema export
69,You can have Hibernate infer your database schema from the mapping
69,"annotations you’ve specified in your Java code, and export the schema at"
69,initialization time by specifying one or more of the following configuration
69,properties:
69,Table 7. Schema management settings
69,Configuration property name
69,Purpose
69,jakarta.persistence.schema-generation.database.action
69,"If drop-and-create, first drop the schema and then export tables, sequences, and constraints"
69,"If create, export tables, sequences, and constraints, without attempting to drop them first"
69,"If create-drop, drop the schema and recreate it on SessionFactory startup"
69,"Additionally, drop the schema on SessionFactory shutdown"
69,"If drop, drop the schema on SessionFactory shutdown"
69,"If validate, validate the database schema without changing it"
69,"If update, only export what’s missing in the schema"
69,jakarta.persistence.create-database-schemas
69,"(Optional) If true, automatically create schemas and catalogs"
69,jakarta.persistence.schema-generation.create-source
69,"(Optional) If metadata-then-script or script-then-metadata, execute an additional SQL script when exported tables and sequences"
69,jakarta.persistence.schema-generation.create-script-source
69,(Optional) The name of a SQL DDL script to be executed
69,jakarta.persistence.sql-load-script-source
69,(Optional) The name of a SQL DML script to be executed
69,This feature is extremely useful for testing.
69,"The easiest way to pre-initialize a database with test or ""reference"" data is to place a list of SQL insert statements in a file named, for example, import.sql, and specify the path to this file using the property jakarta.persistence.sql-load-script-source."
69,"We’ve already seen an example of this approach, which is cleaner than writing Java code to instantiate entity instances and calling persist() on each of them."
69,"As we mentioned earlier, it can also be useful to control schema export programmatically."
69,The SchemaManager API allows programmatic control over schema export:
69,sessionFactory.getSchemaManager().exportMappedObjects(true);
69,JPA has a more limited and less ergonomic API:
69,"Persistence.generateSchema(""org.hibernate.example"","
69,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE))"
69,2.8. Logging the generated SQL
69,"To see the generated SQL as it’s sent to the database, you have two options."
69,"One way is to set the property hibernate.show_sql to true, and Hibernate will log SQL direct to the console."
69,You can make the output much more readable by enabling formatting or highlighting.
69,These settings really help when troubleshooting the generated SQL statements.
69,Table 8. Settings for SQL logging to the console
69,Configuration property name
69,Purpose
69,hibernate.show_sql
69,"If true, log SQL directly to the console"
69,hibernate.format_sql
69,"If true, log SQL in a multiline, indented format"
69,hibernate.highlight_sql
69,"If true, log SQL with syntax highlighting via ANSI escape codes"
69,"Alternatively, you can enable debug-level logging for the category org.hibernate.SQL using your preferred SLF4J logging implementation."
69,"For example, if you’re using Log4J 2 (as above in Optional dependencies), add these lines to your log4j2.properties file:"
69,# SQL execution
69,logger.hibernate.name = org.hibernate.SQL
69,logger.hibernate.level = debug
69,# JDBC parameter binding
69,logger.jdbc-bind.name=org.hibernate.orm.jdbc.bind
69,logger.jdbc-bind.level=trace
69,# JDBC result set extraction
69,logger.jdbc-extract.name=org.hibernate.orm.jdbc.extract
69,logger.jdbc-extract.level=trace
69,But with this approach we miss out on the pretty highlighting.
69,2.9. Minimizing repetitive mapping information
69,"The following properties are very useful for minimizing the amount of information you’ll need to explicitly specify in @Table and @Column annotations, which we’ll discuss below in Object/relational mapping:"
69,Table 9. Settings for minimizing explicit mapping information
69,Configuration property name
69,Purpose
69,hibernate.default_schema
69,A default schema name for entities which do not explicitly declare one
69,hibernate.default_catalog
69,A default catalog name for entities which do not explicitly declare one
69,hibernate.physical_naming_strategy
69,A PhysicalNamingStrategy implementing your database naming standards
69,hibernate.implicit_naming_strategy
69,"An ImplicitNamingStrategy which specifies how ""logical"" names of relational objects should be inferred when no name is specified in annotations"
69,"Writing your own PhysicalNamingStrategy and/or ImplicitNamingStrategy is an especially good way to reduce the clutter of annotations on your entity classes, and to implement your database naming conventions, and so we think you should do it for any nontrivial data model."
69,We’ll have more to say about them in Naming strategies.
69,2.10. Nationalized character data in SQL Server
69,"By default, SQL Server’s char and varchar types don’t accommodate Unicode data."
69,But a Java string may contain any Unicode character.
69,"So, if you’re working with SQL Server, you might need to force Hibernate to use the nchar and nvarchar column types."
69,Table 10. Setting the use of nationalized character data
69,Configuration property name
69,Purpose
69,hibernate.use_nationalized_character_data
69,Use nchar and nvarchar instead of char and varchar
69,"On the other hand, if only some columns store nationalized data, use the @Nationalized annotation to indicate fields of your entities which map these columns."
69,"Alternatively, you can configure SQL Server to use the UTF-8 enabled collation _UTF8."
69,3. Entities
69,An entity is a Java class which represents data in a relational database table.
69,We say that the entity maps or maps to the table.
69,"Much less commonly, an entity might aggregate data from multiple tables, but we’ll get to that later."
69,An entity has attributes—properties or fields—which map to columns of the table.
69,"In particular, every entity must have an identifier or id, which maps to the primary key of the table."
69,"The id allows us to uniquely associate a row of the table with an instance of the Java class, at least within a given persistence context."
69,"We’ll explore the idea of a persistence context later. For now, think of it as a one-to-one mapping between ids and entity instances."
69,An instance of a Java class cannot outlive the virtual machine to which it belongs.
69,But we may think of an entity instance having a lifecycle which transcends a particular instantiation in memory.
69,"By providing its id to Hibernate, we may re-materialize the instance in a new persistence context, as long as the associated row is present in the database."
69,"Therefore, the operations persist() and remove() may be thought of as demarcating the beginning and end of the lifecycle of an entity, at least with respect to persistence."
69,"Thus, an id represents the persistent identity of an entity, an identity that outlives a particular instantiation in memory."
69,"And this is an important difference between entity class itself and the values of its attributes—the entity has a persistent identity, and a well-defined lifecycle with respect to persistence, whereas a String or List representing one of its attribute values doesn’t."
69,An entity usually has associations to other entities.
69,"Typically, an association between two entities maps to a foreign key in one of the database tables."
69,"A group of mutually associated entities is often called a domain model, though data model is also a perfectly good term."
69,3.1. Entity classes
69,An entity must:
69,"be a non-final class,"
69,with a non-private constructor with no parameters.
69,"On the other hand, the entity class may be either concrete or abstract, and it may have any number of additional constructors."
69,An entity class may be a static inner class.
69,Every entity class must be annotated @Entity.
69,@Entity
69,class Book {
69,Book() {}
69,...
69,"Alternatively, the class may be identified as an entity type by providing an XML-based mapping for the class."
69,Mapping entities using XML
69,"When XML-based mappings are used, the <entity> element is used to declare an entity class:"
69,<entity-mappings>
69,<package>org.hibernate.example</package>
69,"<entity class=""Book"">"
69,<attributes> ... </attributes>
69,</entity>
69,...
69,</entity-mappings>
69,"Since the orm.xml mapping file format defined by the JPA specification was modelled closely on the annotation-based mappings, it’s usually easy to go back and forth between the two options."
69,"We won’t have much more to say about XML-based mappings in this Introduction, since it’s not our preferred way to do things."
69,"""Dynamic"" models"
69,We love representing entities as classes because the classes give us a type-safe model of our data.
69,But Hibernate also has the ability to represent entities as detyped instances of java.util.Map.
69,"There’s information in the User Guide, if you’re curious."
69,This must sound like a weird feature for a project that places importance on type-safety.
69,"Actually, it’s a useful capability for a very particular sort of generic code."
69,"For example, Hibernate Envers is a great auditing/versioning system for Hibernate entities."
69,Envers makes use of maps to represent its versioned model of the data.
69,3.2. Access types
69,"Each entity class has a default access type, either:"
69,"direct field access, or"
69,property access.
69,Hibernate automatically determines the access type from the location of attribute-level annotations.
69,Concretely:
69,"if a field is annotated @Id, field access is used, or"
69,"if a getter method is annotated @Id, property access is used."
69,"Back when Hibernate was just a baby, property access was quite popular in the Hibernate community."
69,"Today, however, field access is much more common."
69,"The default access type may be specified explicitly using the @Access annotation, but we strongly discourage this, since it’s ugly and never necessary."
69,Mapping annotations should be placed consistently:
69,"if @Id annotates a field, the other mapping annotations should also be applied to fields, or,"
69,"if @Id annotates a getter, the other mapping annotations should be applied to getters."
69,It is in principle possible to mix field and property access using explicit @Access annotations at the attribute level.
69,We don’t recommend doing this.
69,"An entity class like Book, which does not extend any other entity class, is called a root entity."
69,Every root entity must declare an identifier attribute.
69,3.3. Entity class inheritance
69,An entity class may extend another entity class.
69,@Entity
69,class AudioBook extends Book {
69,AudioBook() {}
69,...
69,A subclass entity inherits every persistent attribute of every entity it extends.
69,A root entity may also extend another class and inherit mapped attributes from the other class.
69,"But in this case, the class which declares the mapped attributes must be annotated @MappedSuperclass."
69,@MappedSuperclass
69,class Versioned {
69,...
69,@Entity
69,class Book extends Versioned {
69,...
69,"A root entity class must declare an attribute annotated @Id, or inherit one from a @MappedSuperclass."
69,A subclass entity always inherits the identifier attribute of the root entity.
69,It may not declare its own @Id attribute.
69,3.4. Identifier attributes
69,An identifier attribute is usually a field:
69,@Entity
69,class Book {
69,Book() {}
69,@Id
69,Long id;
69,...
69,But it may be a property:
69,@Entity
69,class Book {
69,Book() {}
69,private Long id;
69,@Id
69,Long getId() { return id; }
69,void setId(Long id) { this.id = id; }
69,...
69,An identifier attribute must be annotated @Id or @EmbeddedId.
69,Identifier values may be:
69,"assigned by the application, that is, by your Java code, or"
69,generated and assigned by Hibernate.
69,We’ll discuss the second option first.
69,3.5. Generated identifiers
69,"An identifier is often system-generated, in which case it should be annotated @GeneratedValue:"
69,@Id @GeneratedValue
69,Long id;
69,"System-generated identifiers, or surrogate keys make it easier to evolve or refactor the relational data model."
69,"If you have the freedom to define the relational schema, we recommend the use of surrogate keys."
69,"On the other hand, if, as is more common, you’re working with a pre-existing database schema, you might not have the option."
69,"JPA defines the following strategies for generating ids, which are enumerated by GenerationType:"
69,Table 11. Standard id generation strategies
69,Strategy
69,Java type
69,Implementation
69,GenerationType.UUID
69,UUID or String
69,A Java UUID
69,GenerationType.IDENTITY
69,Long or Integer
69,An identity or autoincrement column
69,GenerationType.SEQUENCE
69,Long or Integer
69,A database sequence
69,GenerationType.TABLE
69,Long or Integer
69,A database table
69,GenerationType.AUTO
69,Long or Integer
69,"Selects SEQUENCE, TABLE, or UUID based on the identifier type and capabilities of the database"
69,"For example, this UUID is generated in Java code:"
69,@Id @GeneratedValue UUID id;
69,// AUTO strategy selects UUID based on the field type
69,"This id maps to a SQL identity, auto_increment, or bigserial column:"
69,@Id @GeneratedValue(strategy=IDENTITY) Long id;
69,The @SequenceGenerator and @TableGenerator annotations allow further control over SEQUENCE and TABLE generation respectively.
69,Consider this sequence generator:
69,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
69,Values are generated using a database sequence defined as follows:
69,create sequence seq_book start with 5 increment by 10
69,Notice that Hibernate doesn’t have to go to the database every time a new identifier is needed.
69,"Instead, a given process obtains a block of ids, of size allocationSize, and only needs to hit the database each time the block is exhausted."
69,"Of course, the downside is that generated identifiers are not contiguous."
69,"If you let Hibernate export your database schema, the sequence definition will have the right start with and increment values."
69,"But if you’re working with a database schema managed outside Hibernate, make sure the initialValue and allocationSize members of @SequenceGenerator match the start with and increment specified in the DDL."
69,Any identifier attribute may now make use of the generator named bookSeq:
69,@Id
69,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
69,// reference to generator defined elsewhere
69,Long id;
69,"Actually, it’s extremely common to place the @SequenceGenerator annotation on the @Id attribute that makes use of it:"
69,@Id
69,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
69,// reference to generator defined below
69,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
69,Long id;
69,JPA id generators may be shared between entities.
69,"A @SequenceGenerator or @TableGenerator must have a name, and may be shared between multiple id attributes."
69,This fits somewhat uncomfortably with the common practice of annotating the @Id attribute which makes use of the generator!
69,"As you can see, JPA provides quite adequate support for the most common strategies for system-generated ids."
69,"However, the annotations themselves are a bit more intrusive than they should be, and there’s no well-defined way to extend this framework to support custom strategies for id generation."
69,Nor may @GeneratedValue be used on a property not annotated @Id.
69,"Since custom id generation is a rather common requirement, Hibernate provides a very carefully-designed framework for user-defined Generators, which we’ll discuss in User-defined generators."
69,3.6. Natural keys as identifiers
69,Not every identifier attribute maps to a (system-generated) surrogate key.
69,Primary keys which are meaningful to the user of the system are called natural keys.
69,"When the primary key of a table is a natural key, we don’t annotate the identifier attribute @GeneratedValue, and it’s the responsibility of the application code to assign a value to the identifier attribute."
69,@Entity
69,class Book {
69,@Id
69,String isbn;
69,...
69,"Of particular interest are natural keys which comprise more than one database column, and such natural keys are called composite keys."
69,3.7. Composite identifiers
69,"If your database uses composite keys, you’ll need more than one identifier attribute."
69,There are two ways to map composite keys in JPA:
69,"using an @IdClass, or"
69,using an @EmbeddedId.
69,"Perhaps the most immediately-natural way to represent this in an entity class is with multiple fields annotated @Id, for example:"
69,@Entity
69,@IdClass(BookId.class)
69,class Book {
69,Book() {}
69,@Id
69,String isbn;
69,@Id
69,int printing;
69,...
69,But this approach comes with a problem: what object can we use to identify a Book and pass to methods like find() which accept an identifier?
69,The solution is to write a separate class with fields that match the identifier attributes of the entity.
69,The @IdClass annotation of the Book entity identifies the id class to use for that entity:
69,class BookId {
69,String isbn;
69,int printing;
69,BookId() {}
69,"BookId(String isbn, int printing) {"
69,this.isbn = isbn;
69,this.printing = printing;
69,@Override
69,public boolean equals(Object other) {
69,if (other instanceof BookId) {
69,BookId bookId = (BookId) other;
69,return bookId.isbn.equals(isbn)
69,&& bookId.printing == printing;
69,else {
69,return false;
69,@Override
69,public int hashCode() {
69,return isbn.hashCode();
69,Every id class should override equals() and hashCode().
69,This is not our preferred approach.
69,"Instead, we recommend that the BookId class be declared as an @Embeddable type:"
69,@Embeddable
69,class BookId {
69,String isbn;
69,int printing;
69,BookId() {}
69,"BookId(String isbn, int printing) {"
69,this.isbn = isbn;
69,this.printing = printing;
69,...
69,We’ll learn more about Embeddable objects below.
69,"Now the entity class may reuse this definition using @EmbeddedId, and the @IdClass annotation is no longer required:"
69,@Entity
69,class Book {
69,Book() {}
69,@EmbeddedId
69,BookId bookId;
69,...
69,This second approach eliminates some duplicated code.
69,"Either way, we may now use BookId to obtain instances of Book:"
69,"Book book = session.find(Book.class, new BookId(isbn, printing));"
69,3.8. Version attributes
69,An entity may have an attribute which is used by Hibernate for optimistic lock checking.
69,"A version attribute is usually of type Integer, Short, Long, LocalDateTime, OffsetDateTime, ZonedDateTime, or Instant."
69,@Version
69,LocalDateTime lastUpdated;
69,"Version attributes are automatically assigned by Hibernate when an entity is made persistent, and automatically incremented or updated each time the entity is updated."
69,"If an entity doesn’t have a version number, which often happens when mapping legacy data, we can still do optimistic locking."
69,"The @OptimisticLocking annotation lets us specify that optimistic locks should be checked by validating the values of ALL fields, or only the DIRTY fields of the entity."
69,And the @OptimisticLock annotation lets us selectively exclude certain fields from optimistic locking.
69,The @Id and @Version attributes we’ve already seen are just specialized examples of basic attributes.
69,3.9. Natural id attributes
69,"Even when an entity has a surrogate key, it should always be possible to write down a combination of fields which uniquely identifies an instance of the entity, from the point of view of the user of the system."
69,This combination of fields is its natural key.
69,"Above, we considered the case where the natural key coincides with the primary key."
69,"Here, the natural key is a second unique key of the entity, distinct from its surrogate primary key."
69,"If you can’t identify a natural key, it might be a sign that you need to think more carefully about some aspect of your data model."
69,"If an entity doesn’t have a meaningful unique key, then it’s impossible to say what event or object it represents in the ""real world"" outside your program."
69,"Since it’s extremely common to retrieve an entity based on its natural key, Hibernate has a way to mark the attributes of the entity which make up its natural key."
69,Each attribute must be annotated @NaturalId.
69,@Entity
69,class Book {
69,Book() {}
69,@Id @GeneratedValue
69,Long id; // the system-generated surrogate key
69,@NaturalId
69,String isbn; // belongs to the natural key
69,@NaturalId
69,int printing; // also belongs to the natural key
69,...
69,Hibernate automatically generates a UNIQUE constraint on the columns mapped by the annotated fields.
69,Consider using the natural id attributes to implement equals() and hashCode().
69,"The payoff for doing this extra work, as we will see much later, is that we can take advantage of optimized natural id lookups that make use of the second-level cache."
69,"Note that even when you’ve identified a natural key, we still recommend the use of a generated surrogate key in foreign keys, since this makes your data model much easier to change."
69,3.10. Basic attributes
69,A basic attribute of an entity is a field or property which maps to a single column of the associated database table.
69,The JPA specification defines a quite limited set of basic types:
69,Table 12. JPA-standard basic attribute types
69,Classification
69,Package
69,Types
69,Primitive types
69,"boolean, int, double, etc"
69,Primitive wrappers
69,java.lang
69,"Boolean, Integer, Double, etc"
69,Strings
69,java.lang
69,String
69,Arbitrary-precision numeric types
69,java.math
69,"BigInteger, BigDecimal"
69,Date/time types
69,java.time
69,"LocalDate, LocalTime, LocalDateTime, OffsetDateTime, Instant"
69,Deprecated date/time types 💀
69,java.util
69,"Date, Calendar"
69,Deprecated JDBC date/time types 💀
69,java.sql
69,"Date, Time, Timestamp"
69,Binary and character arrays
69,"byte[], char[]"
69,UUIDs
69,java.util
69,UUID
69,Enumerated types
69,Any enum
69,Serializable types
69,Any type which implements java.io.Serializable
69,We’re begging you to use types from the java.time package instead of anything which inherits java.util.Date.
69,Serializing a Java object and storing its binary representation in the database is usually wrong.
69,"As we’ll soon see in Embeddable objects, Hibernate has much better ways to handle complex Java objects."
69,Hibernate slightly extends this list with the following types:
69,Table 13. Additional basic attribute types in Hibernate
69,Classification
69,Package
69,Types
69,Additional date/time types
69,java.time
69,"Duration, ZoneId, ZoneOffset, Year, and even ZonedDateTime"
69,JDBC LOB types
69,java.sql
69,"Blob, Clob, NClob"
69,Java class object
69,java.lang
69,Class
69,Miscellaneous types
69,java.util
69,"Currency, URL, TimeZone"
69,"The @Basic annotation explicitly specifies that an attribute is basic, but it’s often not needed, since attributes are assumed basic by default."
69,"On the other hand, if a non-primitively-typed attribute cannot be null, use of @Basic(optional=false) is highly recommended."
69,@Basic(optional=false) String firstName;
69,@Basic(optional=false) String lastName;
69,String middleName; // may be null
69,Note that primitively-typed attributes are inferred NOT NULL by default.
69,How to make a column not null in JPA
69,There are two standard ways to add a NOT NULL constraint to a mapped column in JPA:
69,"using @Basic(optional=false), or"
69,using @Column(nullable=false).
69,You might wonder what the difference is.
69,"Well, it’s perhaps not obvious to a casual user of the JPA annotations, but they actually come in two ""layers"":"
69,"annotations like @Entity, @Id, and @Basic belong to the logical layer, the subject of the current chapter—they specify the semantics of your Java domain model, whereas"
69,"annotations like @Table and @Column belong to the mapping layer, the topic of the next chapter—they specify how elements of the domain model map to objects in the relational database."
69,"Information may be inferred from the logical layer down to the mapping layer, but is never inferred in the opposite direction."
69,"Now, the @Column annotation, to whom we’ll be properly introduced a bit later, belongs to the mapping layer, and so its nullable member only affects schema generation (resulting in a not null constraint in the generated DDL)."
69,"On the other hand, the @Basic annotation belongs to the logical layer, and so an attribute marked optional=false is checked by Hibernate before it even writes an entity to the database."
69,Note that:
69,"optional=false implies nullable=false, but"
69,nullable=false does not imply optional=false.
69,"Therefore, we prefer @Basic(optional=false) to @Column(nullable=false)."
69,But wait!
69,An even better solution is to use the @NotNull annotation from Bean Validation.
69,"Just add Hibernate Validator to your project build, as described in Optional dependencies."
69,3.11. Enumerated types
69,We included Java enums on the list above.
69,"An enumerated type is considered a sort of basic type, but since most databases don’t have a native ENUM type, JPA provides a special @Enumerated annotation to specify how the enumerated values should be represented in the database:"
69,"by default, an enum is stored as an integer, the value of its ordinal() member, but"
69,"if the attribute is annotated @Enumerated(STRING), it will be stored as a string, the value of its name() member."
69,"//here, an ORDINAL encoding makes sense"
69,@Enumerated
69,@Basic(optional=false)
69,DayOfWeek dayOfWeek;
69,"//but usually, a STRING encoding is better"
69,@Enumerated(EnumType.STRING)
69,@Basic(optional=false)
69,Status status;
69,"In Hibernate 6, an enum annotated @Enumerated(STRING) is mapped to:"
69,"a VARCHAR column type with a CHECK constraint on most databases, or"
69,an ENUM column type on MySQL.
69,Any other enum is mapped to a TINYINT column with a CHECK constraint.
69,JPA picks the wrong default here.
69,"In most cases, storing an integer encoding of the enum value makes the relational data harder to interpret."
69,"Even considering DayOfWeek, the encoding to integers is ambiguous."
69,"If you check java.time.DayOfWeek, you’ll notice that SUNDAY is encoded as 6."
69,"But in the country I was born, SUNDAY is the first day of the week!"
69,So we prefer @Enumerated(STRING) for most enum attributes.
69,An interesting special case is PostgreSQL.
69,"Postgres supports named ENUM types, which must be declared using a DDL CREATE TYPE statement."
69,"Sadly, these ENUM types aren’t well-integrated with the language nor well-supported by the Postgres JDBC driver, so Hibernate doesn’t use them by default."
69,"But if you would like to use a named enumerated type on Postgres, just annotate your enum attribute like this:"
69,@JdbcTypeCode(SqlTypes.NAMED_ENUM)
69,@Basic(optional=false)
69,Status status;
69,The limited set of pre-defined basic attribute types can be stretched a bit further by supplying a converter.
69,3.12. Converters
69,A JPA AttributeConverter is responsible for:
69,"converting a given Java type to one of the types listed above, and/or"
69,perform any other sort of pre- and post-processing you might need to perform on a basic attribute value before writing and reading it to or from the database.
69,Converters substantially widen the set of attribute types that can be handled by JPA.
69,There are two ways to apply a converter:
69,"the @Convert annotation applies an AttributeConverter to a particular entity attribute, or"
69,"the @Converter annotation (or, alternatively, the @ConverterRegistration annotation) registers an AttributeConverter for automatic application to all attributes of a given type."
69,"For example, the following converter will be automatically applied to any attribute of type BitSet, and takes care of persisting the BitSet to a column of type varbinary:"
69,@Converter(autoApply = true)
69,"public static class EnumSetConverter implements AttributeConverter<EnumSet<DayOfWeek>,Integer> {"
69,@Override
69,public Integer convertToDatabaseColumn(EnumSet<DayOfWeek> enumSet) {
69,int encoded = 0;
69,var values = DayOfWeek.values();
69,for (int i = 0; i<values.length; i++) {
69,if (enumSet.contains(values[i])) {
69,encoded |= 1<<i;
69,return encoded;
69,@Override
69,public EnumSet<DayOfWeek> convertToEntityAttribute(Integer encoded) {
69,var set = EnumSet.noneOf(DayOfWeek.class);
69,var values = DayOfWeek.values();
69,for (int i = 0; i<values.length; i++) {
69,if (((1<<i) & encoded) != 0) {
69,set.add(values[i]);
69,return set;
69,"On the other hand, if we don’t set autoapply=true, then we must explicitly apply the converter using the @Convert annotation:"
69,@Convert(converter = BitSetConverter.class)
69,@Basic(optional = false)
69,BitSet bitset;
69,"All this is nice, but it probably won’t surprise you that Hibernate goes beyond what is required by JPA."
69,3.13. Compositional basic types
69,"Hibernate considers a ""basic type"" to be formed by the marriage of two objects:"
69,"a JavaType, which models the semantics of a certain Java class, and"
69,"a JdbcType, representing a SQL type which is understood by JDBC."
69,"When mapping a basic attribute, we may explicitly specify a JavaType, a JdbcType, or both."
69,JavaType
69,An instance of org.hibernate.type.descriptor.java.JavaType represents a particular Java class.
69,It’s able to:
69,"compare instances of the class to determine if an attribute of that class type is dirty (modified),"
69,"produce a useful hash code for an instance of the class,"
69,"coerce values to other types, and, in particular,"
69,convert an instance of the class to one of several other equivalent Java representations at the request of its partner JdbcType.
69,"For example, IntegerJavaType knows how to convert an Integer or int value to the types Long, BigInteger, and String, among others."
69,"We may explicitly specify a Java type using the @JavaType annotation, but for the built-in JavaTypes this is never necessary."
69,@JavaType(LongJavaType.class)
69,"// not needed, this is the default JavaType for long"
69,long currentTimeMillis;
69,"For a user-written JavaType, the annotation is more useful:"
69,@JavaType(BitSetJavaType.class)
69,BitSet bitSet;
69,"Alternatively, the @JavaTypeRegistration annotation may be used to register BitSetJavaType as the default JavaType for BitSet."
69,JdbcType
69,A org.hibernate.type.descriptor.jdbc.JdbcType is able to read and write a single Java type from and to JDBC.
69,"For example, VarcharJdbcType takes care of:"
69,"writing Java strings to JDBC PreparedStatements by calling setString(), and"
69,reading Java strings from JDBC ResultSets using getString().
69,"By pairing LongJavaType with VarcharJdbcType in holy matrimony, we produce a basic type which maps Longs and primitive longss to the SQL type VARCHAR."
69,We may explicitly specify a JDBC type using the @JdbcType annotation.
69,@JdbcType(VarcharJdbcType.class)
69,long currentTimeMillis;
69,"Alternatively, we may specify a JDBC type code:"
69,@JdbcTypeCode(Types.VARCHAR)
69,long currentTimeMillis;
69,The @JdbcTypeRegistration annotation may be used to register a user-written JdbcType as the default for a given SQL type code.
69,JDBC types and JDBC type codes
69,The types defined by the JDBC specification are enumerated by the integer type codes in the class java.sql.Types.
69,Each JDBC type is an abstraction of a commonly-available type in SQL.
69,"For example, Types.VARCHAR represents the SQL type VARCHAR (or VARCHAR2 on Oracle)."
69,"Since Hibernate understand more SQL types than JDBC, there’s an extended list of integer type codes in the class org.hibernate.type.SqlTypes."
69,"For example, SqlTypes.GEOMETRY represents the spatial data type GEOMETRY."
69,AttributeConverter
69,"If a given JavaType doesn’t know how to convert its instances to the type required by its partner JdbcType, we must help it out by providing a JPA AttributeConverter to perform the conversion."
69,"For example, to form a basic type using LongJavaType and TimestampJdbcType, we would provide an AttributeConverter<Long,Timestamp>."
69,@JdbcType(TimestampJdbcType.class)
69,@Convert(converter = LongToTimestampConverter.class)
69,long currentTimeMillis;
69,"Let’s abandon our analogy right here, before we start calling this basic type a ""throuple""."
69,3.14. Embeddable objects
69,"An embeddable object is a Java class whose state maps to multiple columns of a table, but which doesn’t have its own persistent identity."
69,"That is, it’s a class with mapped attributes, but no @Id attribute."
69,An embeddable object can only be made persistent by assigning it to the attribute of an entity.
69,"Since the embeddable object does not have its own persistent identity, its lifecycle with respect to persistence is completely determined by the lifecycle of the entity to which it belongs."
69,An embeddable class must be annotated @Embeddable instead of @Entity.
69,@Embeddable
69,class Name {
69,@Basic(optional=false)
69,String firstName;
69,@Basic(optional=false)
69,String lastName;
69,String middleName;
69,Name() {}
69,"Name(String firstName, String middleName, String lastName) {"
69,this.firstName = firstName;
69,this.middleName = middleName;
69,this.lastName = lastName;
69,...
69,"An embeddable class must satisfy the same requirements that entity classes satisfy, with the exception that an embeddable class has no @Id attribute."
69,"In particular, it must have a constructor with no parameters."
69,"Alternatively, an embeddable type may be defined as a Java record type:"
69,@Embeddable
69,"record Name(String firstName, String middleName, String lastName) {}"
69,"In this case, the requirement for a constructor with no parameters is relaxed."
69,"Unfortunately, as of May 2023, Java record types still cannot be used as @EmbeddedIds."
69,We may now use our Name class (or record) as the type of an entity attribute:
69,@Entity
69,class Author {
69,@Id @GeneratedValue
69,Long id;
69,Name name;
69,...
69,Embeddable types can be nested.
69,"That is, an @Embeddable class may have an attribute whose type is itself a different @Embeddable class."
69,JPA provides an @Embedded annotation to identify an attribute of an entity that refers to an embeddable type.
69,"This annotation is completely optional, and so we don’t usually use it."
69,On the other hand a reference to an embeddable type is never polymorphic.
69,"One @Embeddable class F may inherit a second @Embeddable class E, but an attribute of type E will always refer to an instance of that concrete class E, never to an instance of F."
69,"Usually, embeddable types are stored in a ""flattened"" format."
69,Their attributes map columns of the table of their parent entity.
69,"Later, in Mapping embeddable types to UDTs or to JSON, we’ll see a couple of different options."
69,"An attribute of embeddable type represents a relationship between a Java object with a persistent identity, and a Java object with no persistent identity."
69,We can think of it as a whole/part relationship.
69,"The embeddable object belongs to the entity, and can’t be shared with other entity instances."
69,And it exits for only as long as its parent entity exists.
69,Next we’ll discuss a different kind of relationship: a relationship between Java objects which each have their own distinct persistent identity and persistence lifecycle.
69,3.15. Associations
69,An association is a relationship between entities.
69,We usually classify associations based on their multiplicity.
69,"If E and F are both entity classes, then:"
69,"a one-to-one association relates at most one unique instance E with at most one unique instance of F,"
69,"a many-to-one association relates zero or more instances of E with a unique instance of F, and"
69,a many-to-many association relates zero or more instances of E with zero or more instance of F.
69,An association between entity classes may be either:
69,"unidirectional, navigable from E to F but not from F to E, or"
69,"bidirectional, and navigable in either direction."
69,"In this example data model, we can see the sorts of associations which are possible:"
69,An astute observer of the diagram above might notice that the relationship we’ve presented as a unidirectional one-to-one association could reasonably be represented in Java using subtyping.
69,This is quite normal.
69,A one-to-one association is the usual way we implement subtyping in a fully-normalized relational model.
69,It’s related to the JOINED inheritance mapping strategy.
69,"There are three annotations for mapping associations: @ManyToOne, @OneToMany, and @ManyToMany."
69,They share some common annotation members:
69,Table 14. Association-defining annotation members
69,Member
69,Interpretation
69,Default value
69,cascade
69,Persistence operations which should cascade to the associated entity; a list of CascadeTypes
69,fetch
69,Whether the association is eagerly fetched or may be proxied
69,LAZY for @OneToMany and @ManyToMany
69,EAGER for @ManyToOne 💀💀💀
69,targetEntity
69,The associated entity class
69,Determined from the attribute type declaration
69,optional
69,"For a @ManyToOne or @OneToOne association, whether the association can be null"
69,true
69,mappedBy
69,"For a bidirectional association, an attribute of the associated entity which maps the association"
69,"By default, the association is assumed unidirectional"
69,We’ll explain the effect of these members as we consider the various types of association mapping.
69,Let’s begin with the most common association multiplicity.
69,3.16. Many-to-one
69,A many-to-one association is the most basic sort of association we can imagine.
69,It maps completely naturally to a foreign key in the database.
69,Almost all the associations in your domain model are going to be of this form.
69,"Later, we’ll see how to map a many-to-one association to an association table."
69,"The @ManyToOne annotation marks the ""to one"" side of the association, so a unidirectional many-to-one association looks like this:"
69,class Book {
69,@Id @GeneratedValue
69,Long id;
69,@ManyToOne(fetch=LAZY)
69,Publisher publisher;
69,...
69,"Here, the Book table has a foreign key column holding the identifier of the associated Publisher."
69,A very unfortunate misfeature of JPA is that @ManyToOne associations are fetched eagerly by default.
69,This is almost never what we want.
69,Almost all associations should be lazy.
69,The only scenario in which fetch=EAGER makes sense is if we think there’s always a very high probability that the associated object will be found in the second-level cache.
69,"Whenever this isn’t the case, remember to explicitly specify fetch=LAZY."
69,"Most of the time, we would like to be able to easily navigate our associations in both directions."
69,"We do need a way to get the Publisher of a given Book, but we would also like to be able to obtain all the Books belonging to a given publisher."
69,"To make this association bidirectional, we need to add a collection-valued attribute to the Publisher class, and annotate it @OneToMany."
69,Hibernate needs to proxy unfetched associations at runtime.
69,"Therefore, the many-valued side must be declared using an interface type like Set or List, and never using a concrete type like HashSet or ArrayList."
69,"To indicate clearly that this is a bidirectional association, and to reuse any mapping information already specified in the Book entity, we must use the mappedBy annotation member to refer back to Book.publisher."
69,@Entity
69,class Publisher {
69,@Id @GeneratedValue
69,Long id;
69,"@OneToMany(mappedBy=""publisher"")"
69,Set<Book> books;
69,...
69,The Publisher.books field is called the unowned side of the association.
69,"Now, we passionately hate the stringly-typed mappedBy reference to the owning side of the association."
69,"Thankfully, the Metamodel Generator gives us a way to make it a"
69,bit more typesafe:
69,@OneToMany(mappedBy=Book_.PUBLISHER)
69,// get used to doing it this way!
69,Set<Book> books;
69,We’re going to use this approach for the rest of the Introduction.
69,"To modify a bidirectional association, we must change the owning side."
69,Changes made to the unowned side of an association are never synchronized to the database.
69,"If we desire to change an association in the database, we must change it from the owning side."
69,"Here, we must set Book.publisher."
69,"In fact, it’s often necessary to change both sides of a bidirectional association."
69,"For example, if the collection Publisher.books was stored in the second-level cache, we must also modify the collection, to ensure that the second-level cache remains synchronized with the database."
69,"That said, it’s not a hard requirement to update the unowned side, at least if you’re sure you know what you’re doing."
69,"In principle Hibernate does allow you to have a unidirectional one-to-many, that is, a @OneToMany with no matching @ManyToOne on the other side."
69,"In practice, this mapping is unnatural, and just doesn’t work very well."
69,Avoid it.
69,"Here we’ve used Set as the type of the collection, but Hibernate also allows the use of List or Collection here, with almost no difference in semantics."
69,"In particular, the List may not contain duplicate elements, and its order will not be persistent."
69,@OneToMany(mappedBy=Book_.PUBLISHER)
69,Collection<Book> books;
69,We’ll see how to map a collection with a persistent order much later.
69,"Set, List, or Collection?"
69,"A one-to-many association mapped to a foreign key can never contain duplicate elements, so Set seems like the most semantically correct Java collection type to use here, and so that’s the conventional practice in the Hibernate community."
69,The catch associated with using a set is that we must carefully ensure that Book has a high-quality implementation of equals() and hashCode().
69,"Now, that’s not necessarily a bad thing, since a quality equals() is independently useful."
69,But what if we used Collection or List instead?
69,Then our code would be much less sensitive to how equals() and hashCode() were implemented.
69,"In the past, we were perhaps too dogmatic in recommending the use of Set."
69,Now? I guess we’re happy to let you guys decide.
69,"In hindsight, we could have done more to make clear that this was always a viable option."
69,3.17. One-to-one (first way)
69,"The simplest sort of one-to-one association is almost exactly like a @ManyToOne association, except that it maps to a foreign key column with a UNIQUE constraint."
69,"Later, we’ll see how to map a one-to-one association to an association table."
69,A one-to-one association must be annotated @OneToOne:
69,@Entity
69,class Author {
69,@Id @GeneratedValue
69,Long id;
69,"@OneToOne(optional=false, fetch=LAZY)"
69,Person author;
69,...
69,"Here, the Author table has a foreign key column holding the identifier of the associated Publisher."
69,"A one-to-one association often models a ""type of"" relationship."
69,"In our example, an Author is a type of Person."
69,"An alternative—and often more natural—way to represent ""type of"" relationships in Java is via entity class inheritance."
69,We can make this association bidirectional by adding a reference back to the Author in the Person entity:
69,@Entity
69,class Person {
69,@Id @GeneratedValue
69,Long id;
69,@OneToOne(mappedBy = Author_.PERSON)
69,Author author;
69,...
69,"Person.author is the unowned side, because it’s the side marked mappedBy."
69,Lazy fetching for one-to-one associations
69,Notice that we did not declare the unowned end of the association fetch=LAZY.
69,That’s because:
69,"not every Person has an associated Author, and"
69,"the foreign key is held in the table mapped by Author, not in the table mapped by Person."
69,"Therefore, Hibernate can’t tell if the reference from Person to Author is null without fetching the associated Author."
69,"On the other hand, if every Person was an Author, that is, if the association were non-optional, we would not have to consider the possibility of null references, and we would map it like this:"
69,"@OneToOne(optional=false, mappedBy = Author_.PERSON, fetch=LAZY)"
69,Author author;
69,This is not the only sort of one-to-one association.
69,3.18. One-to-one (second way)
69,An arguably more elegant way to represent such a relationship is to share a primary key between the two tables.
69,"To use this approach, the Author class must be annotated like this:"
69,@Entity
69,class Author {
69,@Id
69,Long id;
69,"@OneToOne(optional=false, fetch=LAZY)"
69,@MapsId
69,Person author;
69,...
69,"Notice that, compared with the previous mapping:"
69,"the @Id attribute is no longer a @GeneratedValue and,"
69,"instead, the author association is annotated @MapsId."
69,This lets Hibernate know that the association to Person is the source of primary key values for Author.
69,"Here, there’s no extra foreign key column in the Author table, since the id column holds the identifier of Person."
69,"That is, the primary key of the Author table does double duty as the foreign key referring to the Person table."
69,The Person class doesn’t change.
69,"If the association is bidirectional, we annotate the unowned side @OneToOne(mappedBy = Author_.PERSON) just as before."
69,3.19. Many-to-many
69,A unidirectional many-to-many association is represented as a collection-valued attribute.
69,It always maps to a separate association table in the database.
69,It tends to happen that a many-to-many association eventually turns out to be an entity in disguise.
69,Suppose we start with a nice clean many-to-many association between Author and Book.
69,"Later on, it’s quite likely that we’ll discover some additional information which comes attached to the association, so that the association table needs some extra columns."
69,"For example, imagine that we needed to report the percentage contribution of each author to a book."
69,That information naturally belongs to the association table.
69,"We can’t easily store it as an attribute of Book, nor as an attribute of Author."
69,"When this happens, we need to change our Java model, usually introducing a new entity class which maps the association table directly."
69,"In our example, we might call this entity something like BookAuthorship, and it would have @OneToMany associations to both Author and Book, along with the contribution attribute."
69,"We can evade the disruption occasioned by such ""discoveries"" by simply avoiding the use of @ManyToMany right from the start."
69,There’s little downside to representing every—or at least almost every—logical many-to-many association using an intermediate entity.
69,A many-to-many association must be annotated @ManyToMany:
69,@Entity
69,class Book {
69,@Id @GeneratedValue
69,Long id;
69,@ManyToMany
69,Set<Author> authors;
69,...
69,"If the association is bidirectional, we add a very similar-looking attribute to Book, but this time we must specify mappedBy to indicate that this is the unowned side of the association:"
69,@Entity
69,class Book {
69,@Id @GeneratedValue
69,Long id;
69,@ManyToMany(mappedBy=Author_.BOOKS)
69,Set<Author> authors;
69,...
69,"Remember, if we wish to the modify the collection we must change the owning side."
69,We’ve again used Sets to represent the association.
69,"As before, we have the option to use Collection or List."
69,But in this case it does make a difference to the semantics of the association.
69,A many-to-many association represented as a Collection or List may contain duplicate elements.
69,"However, as before, the order of the elements is not persistent."
69,"That is, the collection is a bag, not a set."
69,3.20. Collections of basic values and embeddable objects
69,We’ve now seen the following kinds of entity attribute:
69,Kind of entity attribute
69,Kind of reference
69,Multiplicity
69,Examples
69,Single-valued attribute of basic type
69,Non-entity
69,At most one
69,@Basic String name
69,Single-valued attribute of embeddable type
69,Non-entity
69,At most one
69,@Embedded Name name
69,Single-valued association
69,Entity
69,At most one
69,@ManyToOne Publisher publisher
69,@OneToOne Person person
69,Many-valued association
69,Entity
69,Zero or more
69,@OneToMany Set<Book> books
69,@ManyToMany Set<Author> authors
69,"Scanning this taxonomy, you might ask: does Hibernate have multivalued attributes of basic or embeddable type?"
69,"Well, actually, we’ve already seen that it does, at least in two special cases."
69,"So first, lets recall that JPA treats byte[] and char[] arrays as basic types."
69,"Hibernate persists a byte[] or char[] array to a VARBINARY or VARCHAR column, respectively."
69,But in this section we’re really concerned with cases other than these two special cases.
69,"So then, apart from byte[] and char[], does Hibernate have multivalued attributes of basic or embeddable type?"
69,"And the answer again is that it does. Indeed, there are two different ways to handle such a collection, by mapping it:"
69,"to a column of SQL ARRAY type (assuming the database has an ARRAY type), or"
69,to a separate table.
69,So we may expand our taxonomy with:
69,Kind of entity attribute
69,Kind of reference
69,Multiplicity
69,Examples
69,byte[] and char[] arrays
69,Non-entity
69,Zero or more
69,byte[] image
69,char[] text
69,Collection of basic-typed elements
69,Non-entity
69,Zero or more
69,@Array String[] names
69,@ElementCollection Set<String> names
69,Collection of embeddable elements
69,Non-entity
69,Zero or more
69,@ElementCollection Set<Name> names
69,"There’s actually two new kinds of mapping here: @Array mappings, and @ElementCollection mappings."
69,These sorts of mappings are overused.
69,There are situations where we think it’s appropriate to use a collection of basic-typed values in our entity class.
69,But such situations are rare.
69,Almost every many-valued relationship should map to a foreign key association between separate tables.
69,And almost every table should be mapped by an entity class.
69,The features we’re about to meet in the next two subsections are used much more often by beginners than they’re used by experts.
69,"So if you’re a beginner, you’ll save yourself same hassle by staying away from these features for now."
69,We’ll talk about @Array mappings first.
69,3.21. Collections mapped to SQL arrays
69,Let’s consider a calendar event which repeats on certain days of the week.
69,We might represent this in our Event entity as an attribute of type DayOfWeek[] or List<DayOfWeek>.
69,"Since the number of elements of this array or list is upper bounded by 7, this is a reasonable case for the use of an ARRAY-typed column."
69,It’s hard to see much value in storing this collection in a separate table.
69,Learning to not hate SQL arrays
69,"For a long time, we thought arrays were a kind of weird and warty thing to add to the relational model, but recently we’ve come to realize that this view was overly closed-minded."
69,"Indeed, we might choose to view SQL ARRAY types as a generalization of VARCHAR and VARBINARY to generic ""element"" types."
69,"And from this point of view, SQL arrays look quite attractive, at least for certain problems."
69,"If we’re comfortable mapping byte[] to VARBINARY(255), why would we shy away from mapping DayOfWeek[] to TINYINT ARRAY[7]?"
69,"Unfortunately, JPA doesn’t define a standard way to map SQL arrays, but here’s how we can do it in Hibernate:"
69,@Entity
69,class Event {
69,@Id @GeneratedValue
69,Long id;
69,...
69,@Array(length=7)
69,DayOfWeek[] daysOfWeek;
69,// stored as a SQL ARRAY type
69,...
69,"The @Array annotation is optional, but it’s important to limit the amount of storage space the database allocates to the ARRAY column."
69,"Now for the gotcha: not every database has a SQL ARRAY type, and some that do have an ARRAY type don’t allow it to be used as a column type."
69,"In particular, neither DB2 nor SQL Server have array-typed columns."
69,"On these databases, Hibernate falls back to something much worse: it uses Java serialization to encode the array to a binary representation, and stores the binary stream in a VARBINARY column."
69,"Quite clearly, this is terrible."
69,"You can ask Hibernate to do something slightly less terrible by annotating the attribute @JdbcTypeCode(SqlTypes.JSON), so that the array is serialized to JSON instead of binary format."
69,But at this point it’s better to just admit defeat and use an @ElementCollection instead.
69,"Alternatively, we could store this array or list in a separate table."
69,3.22. Collections mapped to a separate table
69,JPA does define a standard way to map a collection to an auxiliary table: the @ElementCollection annotation.
69,@Entity
69,class Event {
69,@Id @GeneratedValue
69,Long id;
69,...
69,@ElementCollection
69,DayOfWeek[] daysOfWeek;
69,// stored in a dedicated table
69,...
69,"Actually, we shouldn’t use an array here, since array types can’t be proxied, and so the JPA specification doesn’t even say they’re supported."
69,"Instead, we should use Set, List, or Map."
69,@Entity
69,class Event {
69,@Id @GeneratedValue
69,Long id;
69,...
69,@ElementCollection
69,List<DayOfWeek> daysOfWeek;
69,// stored in a dedicated table
69,...
69,"Here, each collection element is stored as a separate row of the auxiliary table."
69,"By default, this table has the following definition:"
69,create table Event_daysOfWeek (
69,"Event_id bigint not null,"
69,"daysOfWeek tinyint check (daysOfWeek between 0 and 6),"
69,"daysOfWeek_ORDER integer not null,"
69,"primary key (Event_id, daysOfWeek_ORDER)"
69,"Which is fine, but it’s still a mapping we prefer to avoid."
69,@ElementCollection is one of our least-favorite features of JPA.
69,Even the name of the annotation is bad.
69,The code above results in a table with three columns:
69,"a foreign key of the Event table,"
69,"a TINYINT encoding the enum, and"
69,an INTEGER encoding the ordering of elements in the array.
69,"Instead of a surrogate primary key, it has a composite key comprising the foreign key of Event and the order column."
69,"When—inevitably—we find that we need to add a fourth column to that table, our Java code must change completely."
69,"Most likely, we’ll realize that we need to add a separate entity after all."
69,So this mapping isn’t very robust in the face of minor changes to our data model.
69,"There’s much more we could say about ""element collections"", but we won’t say it, because we don’t want to hand you the gun you’ll shoot your foot with."
69,3.23. Summary of annotations
69,Let’s pause to remember the annotations we’ve met so far.
69,Table 15. Declaring entities and embeddable types
69,Annotation
69,Purpose
69,JPA-standard
69,@Entity
69,Declare an entity class
69,@MappedSuperclass
69,Declare a non-entity class with mapped attributes inherited by an entity
69,@Embeddable
69,Declare an embeddable type
69,@IdClass
69,Declare the identifier class for an entity with multiple @Id attributes
69,Table 16. Declaring basic and embedded attributes
69,Annotation
69,Purpose
69,JPA-standard
69,@Id
69,Declare a basic-typed identifier attribute
69,@Version
69,Declare a version attribute
69,@Basic
69,Declare a basic attribute
69,Default
69,@EmbeddedId
69,Declare an embeddable-typed identifier attribute
69,@Embedded
69,Declare an embeddable-typed attribute
69,Inferred
69,@Enumerated
69,Declare an enum-typed attribute and specify how it is encoded
69,Inferred
69,@Array
69,"Declare that an attribute maps to a SQL ARRAY, and specify the length"
69,Inferred
69,@ElementCollection
69,Declare that a collection is mapped to a dedicated table
69,Table 17. Converters and compositional basic types
69,Annotation
69,Purpose
69,JPA-standard
69,@Converter
69,Register an AttributeConverter
69,@Convert
69,Apply a converter to an attribute
69,@JavaType
69,Explicitly specify an implementation of JavaType for a basic attribute
69,@JdbcType
69,Explicitly specify an implementation of JdbcType for a basic attribute
69,@JdbcTypeCode
69,Explicitly specify a JDBC type code used to determine the JdbcType for a basic attribute
69,@JavaTypeRegistration
69,Register a JavaType for a given Java type
69,@JdbcTypeRegistration
69,Register a JdbcType for a given JDBC type code
69,Table 18. System-generated identifiers
69,Annotation
69,Purpose
69,JPA-standard
69,@GeneratedValue
69,Specify that an identifier is system-generated
69,@SequenceGenerator
69,Define an id generated backed by on a database sequence
69,@TableGenerator
69,Define an id generated backed by a database table
69,@IdGeneratorType
69,Declare an annotation that associates a custom Generator with each @Id attribute it annotates
69,@ValueGenerationType
69,Declare an annotation that associates a custom Generator with each @Basic attribute it annotates
69,Table 19. Declaring entity associations
69,Annotation
69,Purpose
69,JPA-standard
69,@ManyToOne
69,Declare the single-valued side of a many-to-one association (the owning side)
69,@OneToMany
69,Declare the many-valued side of a many-to-one association (the unowned side)
69,@ManyToMany
69,Declare either side of a one-to-one association
69,@OneToOne
69,Declare either side of a one-to-one association
69,@MapsId
69,Declare that the owning side of a @OneToOne association maps the primary key column
69,Phew!
69,"That’s already a lot of annotations, and we have not even started with the annotations for O/R mapping!"
69,3.24. equals() and hashCode()
69,"Entity classes should override equals() and hashCode(), especially when associations are represented as sets."
69,People new to Hibernate or JPA are often confused by exactly which fields should be included in the hashCode().
69,And people with more experience often argue quite religiously that one or another approach is the only right way.
69,"The truth is, there’s no unique right way to do it, but there are some constraints."
69,So please keep the following principles in mind:
69,"You should not include a mutable field in the hashcode, since that would require rehashing every collection containing the entity whenever the field is mutated."
69,"It’s not completely wrong to include a generated identifier (surrogate key) in the hashcode, but since the identifier is not generated until the entity instance is made persistent, you must take great care to not add it to any hashed collection before the identifier is generated. We therefore advise against including any database-generated field in the hashcode."
69,"It’s OK to include any immutable, non-generated field in the hashcode."
69,"We therefore recommend identifying a natural key for each entity, that is, a combination of fields that uniquely identifies an instance of the entity, from the perspective of the data model of the program. The natural key should correspond to a unique constraint on the database, and to the fields which are included in equals() and hashCode()."
69,"In this example, the equals() and hashCode() methods agree with the @NaturalId annotation:"
69,@Entity
69,class Book {
69,@Id @GeneratedValue
69,Long id;
69,@NaturalId
69,@Basic(optional=false)
69,String isbn;
69,...
69,@Override
69,public boolean equals(Object other) {
69,return other instanceof Book
69,&& ((Book) other).isbn.equals(isbn);
69,@Override
69,public int hashCode() {
69,return isbn.hashCode();
69,"That said, an implementation of equals() and hashCode() based on the generated identifier of the entity can work if you’re careful."
69,4. Object/relational mapping
69,"Given a domain model—that is, a collection of entity classes decorated with all the fancy annotations we just met in the previous chapter—Hibernate will happily go away and infer a complete relational schema, and even export it to your database if you ask politely."
69,"The resulting schema will be entirely sane and reasonable, though if you look closely, you’ll find some flaws."
69,"For example, every VARCHAR column will have the same length, VARCHAR(255)."
69,But the process I just described—which we call top down mapping—simply doesn’t fit the most common scenario for the use of O/R mapping.
69,It’s only rarely that the Java classes precede the relational schema.
69,"Usually, we already have a relational schema, and we’re constructing our domain model around the schema."
69,This is called bottom up mapping.
69,"Developers often refer to a pre-existing relational database as ""legacy"" data."
69,"This tends to conjure images of bad old ""legacy apps"" written in COBOL or something."
69,"But legacy data is valuable, and learning to work with it is important."
69,"Especially when mapping bottom up, we often need to customize the inferred object/relational mappings."
69,"This is a somewhat tedious topic, and so we don’t want to spend too many words on it."
69,"Instead, we’ll quickly skim the most important mapping annotations."
69,Hibernate SQL case convention
69,Computers have had lowercase letters for rather a long time now.
69,"Most developers learned long ago that text written in MixedCase, camelCase, or even snake_case is easier to read than text written in SHOUTYCASE."
69,This is just as true of SQL as it is of any other language.
69,"Therefore, for over twenty years, the convention on the Hibernate project has been that:"
69,"query language identifiers are written in lowercase,"
69,"table names are written in MixedCase, and"
69,column names are written in camelCase.
69,"That is to say, we simply adopted Java’s excellent conventions and applied them to SQL."
69,"Now, there’s no way we can force you to follow this convention, even if we wished to."
69,"Hell, you can easily write a PhysicalNamingStrategy which makes table and column names ALL UGLY AND SHOUTY LIKE THIS IF YOU PREFER."
69,"But, by default, it’s the convention Hibernate follows, and it’s frankly a pretty reasonable one."
69,4.1. Mapping entity inheritance hierarchies
69,In Entity class inheritance we saw that entity classes may exist within an inheritance hierarchy.
69,There’s three basic strategies for mapping an entity hierarchy to relational tables.
69,"Let’s put them in a table, so we can more easily compare the points of difference between them."
69,Table 20. Entity inheritance mapping strategies
69,Strategy
69,Mapping
69,Polymorphic queries
69,Constraints
69,Normalization
69,When to use it
69,SINGLE_TABLE
69,"Map every class in the hierarchy to the same table, and uses the value of a discriminator column to determine which concrete class each row represents."
69,"To retrieve instances of a given class, we only need to query the one table."
69,Attributes declared by subclasses map to columns without NOT NULL constraints. 💀
69,Any association may have a FOREIGN KEY constraint. 🤓
69,Subclass data is denormalized. 🧐
69,Works well when subclasses declare few or no additional attributes.
69,JOINED
69,"Map every class in the hierarchy to a separate table, but each table only maps the attributes declared by the class itself."
69,"Optionally, a discriminator column may be used."
69,"To retrieve instances of a given class, we must JOIN the table mapped by the class with:"
69,all tables mapped by its superclasses and
69,all tables mapped by its subclasses.
69,Any attribute may map to a column with a NOT NULL constraint. 🤓
69,Any association may have a FOREIGN KEY constraint. 🤓
69,The tables are normalized. 🤓
69,The best option when we care a lot about constraints and normalization.
69,TABLE_PER_CLASS
69,"Map every concrete class in the hierarchy to a separate table, but denormalize all inherited attributes into the table."
69,"To retrieve instances of a given class, we must take a UNION over the table mapped by the class and the tables mapped by its subclasses."
69,Associations targeting a superclass cannot have a corresponding FOREIGN KEY constraint in the database. 💀💀
69,Any attribute may map to a column with a NOT NULL constraint. 🤓
69,Superclass data is denormalized. 🧐
69,Not very popular.
69,"From a certain point of view, competes with @MappedSuperclass."
69,The three mapping strategies are enumerated by InheritanceType.
69,We specify an inheritance mapping strategy using the @Inheritance annotation.
69,"For mappings with a discriminator column, we should:"
69,"specify the discriminator column name and type by annotating the root entity @DiscriminatorColumn, and"
69,specify the values of this discriminator by annotating each entity in the hierarchy @DiscriminatorValue.
69,For single table inheritance we always need a discriminator:
69,@Entity
69,"@DiscriminatorColumn(discriminatorType=CHAR, name=""kind"")"
69,@DiscriminatorValue('P')
69,class Person { ... }
69,@Entity
69,@DiscriminatorValue('A')
69,class Author { ... }
69,"We don’t need to explicitly specify @Inheritance(strategy=SINGLE_TABLE), since that’s the default."
69,For JOINED inheritance we don’t need a discriminator:
69,@Entity
69,@Inheritance(strategy=JOINED)
69,class Person { ... }
69,@Entity
69,class Author { ... }
69,"However, we can add a discriminator column if we like, and in that case the generated SQL for polymorphic queries will be slightly simpler."
69,"Similarly, for TABLE_PER_CLASS inheritance we have:"
69,@Entity
69,@Inheritance(strategy=TABLE_PER_CLASS)
69,class Person { ... }
69,@Entity
69,class Author { ... }
69,"Hibernate doesn’t allow discriminator columns for TABLE_PER_CLASS inheritance mappings, since they would make no sense, and offer no advantage."
69,"Notice that in this last case, a polymorphic association like:"
69,@ManyToOne Person person;
69,"is a bad idea, since it’s impossible to create a foreign key constraint that targets both mapped tables."
69,4.2. Mapping to tables
69,The following annotations specify exactly how elements of the domain model map to tables of the relational model:
69,Table 21. Annotations for mapping tables
69,Annotation
69,Purpose
69,@Table
69,Map an entity class to its primary table
69,@SecondaryTable
69,Define a secondary table for an entity class
69,@JoinTable
69,Map a many-to-many or many-to-one association to its association table
69,@CollectionTable
69,Map an @ElementCollection to its table
69,"The first two annotations are used to map an entity to its primary table and, optionally, one or more secondary tables."
69,4.3. Mapping entities to tables
69,"By default, an entity maps to a single table, which may be specified using @Table:"
69,@Entity
69,"@Table(name=""People"")"
69,class Person { ... }
69,"However, the @SecondaryTable annotation allows us to spread its attributes across multiple secondary tables."
69,@Entity
69,"@Table(name=""Books"")"
69,"@SecondaryTable(name=""Editions"")"
69,class Book { ... }
69,The @Table annotation can do more than just specify a name:
69,Table 22. @Table annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped table
69,schema 💀
69,The schema to which the table belongs
69,catalog 💀
69,The catalog to which the table belongs
69,uniqueConstraints
69,One or more @UniqueConstraint annotations declaring multi-column unique constraints
69,indexes
69,One or more @Index annotations each declaring an index
69,It only makes sense to explicitly specify the schema in annotations if the domain model is spread across multiple schemas.
69,"Otherwise, it’s a bad idea to hardcode the schema (or catalog) in a @Table annotation."
69,Instead:
69,"set the configuration property hibernate.default_schema (or hibernate.default_catalog), or"
69,simply specify the schema in the JDBC connection URL.
69,The @SecondaryTable annotation is even more interesting:
69,Table 23. @SecondaryTable annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped table
69,schema 💀
69,The schema to which the table belongs
69,catalog 💀
69,The catalog to which the table belongs
69,uniqueConstraints
69,One or more @UniqueConstraint annotations declaring multi-column unique constraints
69,indexes
69,One or more @Index annotations each declaring an index
69,pkJoinColumns
69,"One or more @PrimaryKeyJoinColumn annotations, specifying primary key column mappings"
69,foreignKey
69,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the @PrimaryKeyJoinColumns
69,Using @SecondaryTable on a subclass in a SINGLE_TABLE entity inheritance hierarchy gives us a sort of mix of SINGLE_TABLE with JOINED inheritance.
69,4.4. Mapping associations to tables
69,"The @JoinTable annotation specifies an association table, that is, a table holding foreign keys of both associated entities."
69,This annotation is usually used with @ManyToMany associations:
69,@Entity
69,class Book {
69,...
69,@ManyToMany
69,"@JoinTable(name=""BooksAuthors"")"
69,Set<Author> authors;
69,...
69,But it’s even possible to use it to map a @ManyToOne or @OneToOne association to an association table.
69,@Entity
69,class Book {
69,...
69,@ManyToOne(fetch=LAZY)
69,"@JoinTable(name=""BookPublisher"")"
69,Publisher publisher;
69,...
69,"Here, there should be a UNIQUE constraint on one of the columns of the association table."
69,@Entity
69,class Author {
69,...
69,"@OneToOne(optional=false, fetch=LAZY)"
69,"@JoinTable(name=""AuthorPerson"")"
69,Person author;
69,...
69,"Here, there should be a UNIQUE constraint on both columns of the association table."
69,Table 24. @JoinTable annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped association table
69,schema 💀
69,The schema to which the table belongs
69,catalog 💀
69,The catalog to which the table belongs
69,uniqueConstraints
69,One or more @UniqueConstraint annotations declaring multi-column unique constraints
69,indexes
69,One or more @Index annotations each declaring an index
69,joinColumns
69,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the owning side"
69,inverseJoinColumns
69,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the unowned side"
69,foreignKey
69,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the joinColumnss
69,inverseForeignKey
69,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the inverseJoinColumnss
69,"To better understand these annotations, we must first discuss column mappings in general."
69,4.5. Mapping to columns
69,These annotations specify how elements of the domain model map to columns of tables in the relational model:
69,Table 25. Annotations for mapping columns
69,Annotation
69,Purpose
69,@Column
69,Map an attribute to a column
69,@JoinColumn
69,Map an association to a foreign key column
69,@PrimaryKeyJoinColumn
69,"Map the primary key used to join a secondary table with its primary, or a subclass table in JOINED inheritance with its root class table"
69,@OrderColumn
69,Specifies a column that should be used to maintain the order of a List.
69,@MapKeyColumn
69,Specified a column that should be used to persist the keys of a Map.
69,We use the @Column annotation to map basic attributes.
69,4.6. Mapping basic attributes to columns
69,The @Column annotation is not only useful for specifying the column name.
69,Table 26. @Column annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped column
69,table
69,The name of the table to which this column belongs
69,length
69,"The length of a VARCHAR, CHAR, or VARBINARY column type"
69,precision
69,"The decimal digits of precision of a FLOAT, DECIMAL, NUMERIC, or TIME, or TIMESTAMP column type"
69,scale
69,"The scale of a DECIMAL or NUMERIC column type, the digits of precision that occur to the right of the decimal point"
69,unique
69,Whether the column has a UNIQUE constraint
69,nullable
69,Whether the column has a NOT NULL constraint
69,insertable
69,Whether the column should appear in generated SQL INSERT statements
69,updatable
69,Whether the column should appear in generated SQL UPDATE statements
69,columnDefinition 💀
69,A DDL fragment that should be used to declare the column
69,We no longer recommend the use of columnDefinition since it results in unportable DDL.
69,Hibernate has much better ways to customize the generated DDL using techniques that result in portable behavior across different databases.
69,Here we see four different ways to use the @Column annotation:
69,@Entity
69,"@Table(name=""Books"")"
69,"@SecondaryTable(name=""Editions"")"
69,class Book {
69,@Id @GeneratedValue
69,"@Column(name=""bookId"") // customize column name"
69,Long id;
69,"@Column(length=100, nullable=false) // declare column as VARCHAR(100) NOT NULL"
69,String title;
69,"@Column(length=17, unique=true, nullable=false) // declare column as VARCHAR(17) NOT NULL UNIQUE"
69,String isbn;
69,"@Column(table=""Editions"", updatable=false) // column belongs to the secondary table, and is never updated"
69,int edition;
69,We don’t use @Column to map associations.
69,4.7. Mapping associations to foreign key columns
69,The @JoinColumn annotation is used to customize a foreign key column.
69,Table 27. @JoinColumn annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped foreign key column
69,table
69,The name of the table to which this column belongs
69,referencedColumnName
69,The name of the column to which the mapped foreign key column refers
69,unique
69,Whether the column has a UNIQUE constraint
69,nullable
69,Whether the column has a NOT NULL constraint
69,insertable
69,Whether the column should appear in generated SQL INSERT statements
69,updatable
69,Whether the column should appear in generated SQL UPDATE statements
69,columnDefinition 💀
69,A DDL fragment that should be used to declare the column
69,foreignKey
69,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
69,A foreign key column doesn’t necessarily have to refer to the primary key of the referenced table.
69,"It’s quite acceptable for the foreign key to refer to any other unique key of the referenced entity, even to a unique key of a secondary table."
69,Here we see how to use @JoinColumn to define a @ManyToOne association mapping a foreign key column which refers to the @NaturalId of Book:
69,@Entity
69,"@Table(name=""Items"")"
69,class Item {
69,...
69,@ManyToOne(optional=false)
69,// implies nullable=false
69,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"","
69,// a reference to a non-PK column
69,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn"")) // supply a name for the FK constraint"
69,Book book;
69,...
69,In case this is confusing:
69,"bookIsbn is the name of the foreign key column in the Items table,"
69,"it refers to a unique key isbn in the Books table, and"
69,it has a foreign key constraint named ItemsToBooksBySsn.
69,Note that the foreignKey member is completely optional and only affects DDL generation.
69,"If you don’t supply an explicit name using @ForeignKey, Hibernate will generate a quite ugly name."
69,"The reason for this is that the maximum length of foreign key names on some databases is extremely constrained, and we need to avoid collisions."
69,"To be fair, this is perfectly fine if you’re only using the generated DDL for testing."
69,For composite foreign keys we might have multiple @JoinColumn annotations:
69,@Entity
69,"@Table(name=""Items"")"
69,class Item {
69,...
69,@ManyToOne(optional=false)
69,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"")"
69,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")"
69,Book book;
69,...
69,"If we need to specify the @ForeignKey, this starts to get a bit messy:"
69,@Entity
69,"@Table(name=""Items"")"
69,class Item {
69,...
69,@ManyToOne(optional=false)
69,"@JoinColumns(value = {@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn""),"
69,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")},"
69,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn""))"
69,Book book;
69,...
69,"For associations mapped to a @JoinTable, fetching the association requires two joins, and so we must declare the @JoinColumns inside the @JoinTable annotation:"
69,@Entity
69,class Book {
69,@Id @GeneratedValue
69,Long id;
69,@ManyToMany
69,"@JoinTable(joinColumns=@JoinColumn(name=""bookId""),"
69,"inverseJoinColumns=@joinColumn(name=""authorId""),"
69,"foreignKey=@ForeignKey(name=""BooksToAuthors""))"
69,Set<Author> authors;
69,...
69,"Again, the foreignKey member is optional."
69,4.8. Mapping primary key joins between tables
69,The @PrimaryKeyJoinColumn is a special-purpose annotation for mapping:
69,"the primary key column of a @SecondaryTable—which is also a foreign key referencing the primary table, or"
69,the primary key column of the primary table mapped by a subclass in a JOINED inheritance hierarchy—which is also a foreign key referencing the primary table mapped by the root entity.
69,Table 28. @PrimaryKeyJoinColumn annotation members
69,Annotation member
69,Purpose
69,name
69,The name of the mapped foreign key column
69,referencedColumnName
69,The name of the column to which the mapped foreign key column refers
69,columnDefinition 💀
69,A DDL fragment that should be used to declare the column
69,foreignKey
69,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
69,"When mapping a subclass table primary key, we place the @PrimaryKeyJoinColumn annotation on the entity class:"
69,@Entity
69,"@Table(name=""People"")"
69,@Inheritance(strategy=JOINED)
69,class Person { ... }
69,@Entity
69,"@Table(name=""Authors"")"
69,"@PrimaryKeyJoinColumn(name=""personId"") // the primary key of the Authors table"
69,class Author { ... }
69,"But to map a secondary table primary key, the @PrimaryKeyJoinColumn annotation must occur inside the @SecondaryTable annotation:"
69,@Entity
69,"@Table(name=""Books"")"
69,"@SecondaryTable(name=""Editions"","
69,"pkJoinColumns = @PrimaryKeyJoinColumn(name=""bookId"")) // the primary key of the Editions table"
69,class Book {
69,@Id @GeneratedValue
69,"@Column(name=""bookId"") // the name of the primary key of the Books table"
69,Long id;
69,...
69,4.9. Column lengths and adaptive column types
69,Hibernate automatically adjusts the column type used in generated DDL based on the column length specified by the @Column annotation.
69,"So we don’t usually need to explicitly specify that a column should be of type TEXT or CLOB—or worry about the parade of TINYTEXT, MEDIUMTEXT, TEXT, LONGTEXT types on MySQL—because Hibernate will automatically select one of those types if required to accommodate a string of the length we specify."
69,The constant values defined in the class Length are very helpful here:
69,Table 29. Predefined column lengths
69,Constant
69,Value
69,Description
69,DEFAULT
69,255
69,The default length of a VARCHAR or VARBINARY column when none is explicitly specified
69,LONG
69,32600
69,The largest column length for a VARCHAR or VARBINARY that is allowed on every database Hibernate supports
69,LONG16
69,32767
69,The maximum length that can be represented using 16 bits (but this length is too large for a VARCHAR or VARBINARY column on for some database)
69,LONG32
69,2147483647
69,The maximum length for a Java string
69,We can use these constants in the @Column annotation:
69,@Column(length=LONG)
69,String text;
69,@Column(length=LONG32)
69,byte[] binaryData;
69,This is usually all you need to do to make use of large object types in Hibernate.
69,4.10. LOBs
69,JPA provides a @Lob annotation which specifies that a field should be persisted as a BLOB or CLOB.
69,Semantics of the @Lob annotation
69,What the spec actually says is that the field should be persisted
69,…​as a large object to a database-supported large object type.
69,"It’s quite unclear what this means, and the spec goes on to say that"
69,…​the treatment of the Lob annotation is provider-dependent…​
69,which doesn’t help much.
69,Hibernate interprets this annotation in what we think is the most reasonable way.
69,"In Hibernate, an attribute annotated @Lob will be written to JDBC using the setClob() or setBlob() method of PreparedStatement, and will be read from JDBC using the getClob() or getBlob() method of ResultSet."
69,"Now, the use of these JDBC methods is usually unnecessary!"
69,JDBC drivers are perfectly capable of converting between String and CLOB or between byte[] and BLOB.
69,"So unless you specifically need to use these JDBC LOB APIs, you don’t need the @Lob annotation."
69,"Instead, as we just saw in Column lengths and adaptive column types, all you need is to specify a large enough column length to accommodate the data you plan to write to that column."
69,"Unfortunately, the driver for PostgreSQL doesn’t allow BYTEA or TEXT columns to be read via the JDBC LOB APIs."
69,This limitation of the Postgres driver has resulted in a whole cottage industry of bloggers and stackoverflow question-answerers recommending convoluted ways to hack the Hibernate Dialect for Postgres to allow an attribute annotated @Lob to be written using setString() and read using getString().
69,But simply removing the @Lob annotation has exactly the same effect.
69,Conclusion:
69,"on PostgreSQL, @Lob always means the OID type,"
69,"@Lob should never be used to map columns of type BYTEA or TEXT, and"
69,please don’t believe everything you read on stackoverflow.
69,"Finally, as an alternative, Hibernate lets you declare an attribute of type java.sql.Blob or java.sql.Clob."
69,@Entity
69,class Book {
69,...
69,Clob text;
69,Blob coverArt;
69,....
69,"The advantage is that a java.sql.Clob or java.sql.Blob can in principle index up to 263 characters or bytes, much more data than you can fit in a Java String or byte[] array (or in your computer)."
69,"To assign a value to these fields, we’ll need to use a LobHelper."
69,We can get one from the Session:
69,LobHelper helper = session.getLobHelper();
69,book.text = helper.createClob(text);
69,book.coverArt = helper.createBlob(image);
69,"In principle, the Blob and Clob objects provide efficient ways to read or stream LOB data from the server."
69,"Book book = session.find(Book.class, bookId);"
69,"String text = book.text.getSubString(1, textLength);"
69,InputStream bytes = book.images.getBinaryStream();
69,"Of course, the behavior here depends very much on the JDBC driver, and so we really can’t promise that this is a sensible thing to do on your database."
69,4.11. Mapping embeddable types to UDTs or to JSON
69,There’s a couple of alternative ways to represent an embeddable type on the database side.
69,Embeddables as UDTs
69,"First, a really nice option, at least in the case of Java record types, and for databases which support user-defined types (UDTs), is to define a UDT which represents the record type."
69,Hibernate 6 makes this really easy.
69,"Just annotate the record type, or the attribute which holds a reference to it, with the new @Struct annotation:"
69,@Embeddable
69,"@Struct(name=""PersonName"")"
69,"record Name(String firstName, String middleName, String lastName) {}"
69,@Entity
69,class Person {
69,...
69,Name name;
69,...
69,This results in the following UDT:
69,"create type PersonName as (firstName varchar(255), middleName varchar(255), lastName varchar(255))"
69,And the name column of the Author table will have the type PersonName.
69,Embeddables to JSON
69,A second option that’s available is to map the embeddable type to a JSON (or JSONB) column.
69,"Now, this isn’t something we would exactly recommend if you’re defining a data model from scratch, but it’s at least useful for mapping pre-existing tables with JSON-typed columns."
69,"Since embeddable types are nestable, we can map some JSON formats this way, and even query JSON properties using HQL."
69,"At this time, JSON arrays are not supported!"
69,"To map an attribute of embeddable type to JSON, we must annotate the attribute @JdbcTypeCode(SqlTypes.JSON), instead of annotating the embeddable type."
69,But the embeddable type Name should still be annotated @Embeddable if we want to query its attributes using HQL.
69,@Embeddable
69,"record Name(String firstName, String middleName, String lastName) {}"
69,@Entity
69,class Person {
69,...
69,@JdbcTypeCode(SqlTypes.JSON)
69,Name name;
69,...
69,"We also need to add Jackson or an implementation of JSONB—for example, Yasson—to our runtime classpath."
69,To use Jackson we could add this line to our Gradle build:
69,runtimeOnly 'com.fasterxml.jackson.core:jackson-databind:{jacksonVersion}'
69,"Now the name column of the Author table will have the type jsonb, and Hibernate will automatically use Jackson to serialize a Name to and from JSON format."
69,4.12. Summary of SQL column type mappings
69,"So, as we’ve seen, there are quite a few annotations that affect the mapping of Java types to SQL column types in DDL."
69,"Here we summarize the ones we’ve just seen in the second half of this chapter, along with some we already mentioned in earlier chapters."
69,Table 30. Annotations for mapping SQL column types
69,Annotation
69,Interpretation
69,@Enumerated
69,Specify how an enum type should be persisted
69,@Nationalized
69,"Use a nationalized character type: NCHAR, NVARCHAR, or NCLOB"
69,@Lob 💀
69,Use JDBC LOB APIs to read and write the annotated attribute
69,@Array
69,Map a collection to a SQL ARRAY type of the specified length
69,@Struct
69,Map an embeddable to a SQL UDT with the given name
69,@TimeZoneStorage
69,Specify how the time zone information should be persisted
69,@JdbcType or @JdbcTypeCode
69,Use an implementation of JdbcType to map an arbitrary SQL type
69,"In addition, there are some configuration properties which have a global affect on how basic types map to SQL column types:"
69,Table 31. Type mapping settings
69,Configuration property name
69,Purpose
69,hibernate.use_nationalized_character_data
69,Enable use of nationalized character types by default
69,hibernate.type.preferred_boolean_jdbc_type
69,Specify the default SQL column type for mapping boolean
69,hibernate.type.preferred_uuid_jdbc_type
69,Specify the default SQL column type for mapping UUID
69,hibernate.type.preferred_duration_jdbc_type
69,Specify the default SQL column type for mapping Duration
69,hibernate.type.preferred_instant_jdbc_type
69,Specify the default SQL column type for mapping Instant
69,hibernate.timezone.default_storage
69,Specify the default strategy for storing time zone information
69,These are global settings and thus quite clumsy.
69,We recommend against messing with any of these settings unless you have a really good reason for it.
69,There’s one more topic we would like to cover in this chapter.
69,4.13. Mapping to formulas
69,Hibernate lets us map an attribute of an entity to a SQL formula involving columns of the mapped table.
69,"Thus, the attribute is a sort of ""derived"" value."
69,Table 32. Annotations for mapping formulas
69,Annotation
69,Purpose
69,@Formula
69,Map an attribute to a SQL formula
69,@JoinFormula
69,Map an association to a SQL formula
69,@DiscriminatorFormula
69,Use a SQL formula as the discriminator in single table inheritance.
69,For example:
69,@Entity
69,class Order {
69,...
69,"@Column(name = ""sub_total"", scale=2, precision=8)"
69,BigDecimal subTotal;
69,"@Column(name = ""tax"", scale=4, precision=4)"
69,BigDecimal taxRate;
69,"@Formula(""sub_total * (1.0 + tax)"")"
69,BigDecimal totalWithTax;
69,...
69,4.14. Derived Identity
69,"An entity has a derived identity if it inherits part of its primary key from an associated ""parent"" entity."
69,We’ve already met a kind of degenerate case of derived identity when we talked about one-to-one associations with a shared primary key.
69,But a @ManyToOne association may also form part of a derived identity.
69,"That is to say, there could be a foreign key column or columns included as part of the composite primary key."
69,There’s three different ways to represent this situation on the Java side of things:
69,"using @IdClass without @MapsId,"
69,"using @IdClass with @MapsId, or"
69,using @EmbeddedId with @MapsId.
69,Let’s suppose we have a Parent entity class defined as follows:
69,@Entity
69,class Parent {
69,@Id
69,Long parentId;
69,...
69,"The parentId field holds the primary key of the Parent table, which will also form part of the composite primary key of every Child belonging to the Parent."
69,First way
69,"In the first, slightly simpler approach, we define an @IdClass to represent the primary key of Child:"
69,class DerivedId {
69,Long parent;
69,String childId;
69,"// constructors, equals, hashcode, etc"
69,...
69,And a Child entity class with a @ManyToOne association annotated @Id:
69,@Entity
69,@IdClass(DerivedId.class)
69,class Child {
69,@Id
69,String childId;
69,@Id @ManyToOne
69,"@JoinColumn(name=""parentId"")"
69,Parent parent;
69,...
69,"Then the primary key of the Child table comprises the columns (childId,parentId)."
69,Second way
69,"This is fine, but sometimes it’s nice to have a field for each element of the primary key."
69,We may use the @MapsId annotation we met earlier:
69,@Entity
69,@IdClass(DerivedId.class)
69,class Child {
69,@Id
69,Long parentId;
69,@Id
69,String childId;
69,@ManyToOne
69,@MapsId(Child_.PARENT_ID) // typesafe reference to Child.parentId
69,"@JoinColumn(name=""parentId"")"
69,Parent parent;
69,...
69,We’re using the approach we saw previously to refer to the parentId property of Child in a typesafe way.
69,"Note that we must place column mapping information on the association annotated @MapsId, not on the @Id field."
69,We must slightly modify our @IdClass so that field names align:
69,class DerivedId {
69,Long parentId;
69,String childId;
69,"// constructors, equals, hashcode, etc"
69,...
69,Third way
69,The third alternative is to redefine our @IdClass as an @Embeddable.
69,"We don’t actually need to change the DerivedId class, but we do need to add the annotation."
69,@Embeddable
69,class DerivedId {
69,Long parentId;
69,String childId;
69,"// constructors, equals, hashcode, etc"
69,...
69,Then we may use @EmbeddedId in Child:
69,@Entity
69,class Child {
69,@EmbeddedId
69,DerivedId id;
69,@ManyToOne
69,@MapsId(DerivedId_.PARENT_ID) // typesafe reference to DerivedId.parentId
69,"@JoinColumn(name=""parentId"")"
69,Parent parent;
69,...
69,The choice between @IdClass and @EmbeddedId boils down to taste.
69,The @EmbeddedId is perhaps a little DRYer.
69,4.15. Adding constraints
69,Database constraints are important.
69,"Even if you’re sure that your program has no bugs 🧐, it’s probably not the only program with access to the database."
69,Constraints help ensure that different programs (and human administrators) play nicely with each other.
69,"Hibernate adds certain constraints to generated DDL automatically: primary key constraints, foreign key constraints, and some unique constraints."
69,But it’s common to need to:
69,"add additional unique constraints,"
69,"add check constraints, or"
69,customize the name of a foreign key constraint.
69,We’ve already seen how to use @ForeignKey to specify the name of a foreign key constraint.
69,There’s two ways to add a unique constraint to a table:
69,"using @Column(unique=true) to indicate a single-column unique key, or"
69,using the @UniqueConstraint annotation to define a uniqueness constraint on a combination of columns.
69,@Entity
69,"@Table(uniqueConstraints=@UniqueConstraint(columnNames={""title"", ""year"", ""publisher_id""}))"
69,class Book { ... }
69,"This annotation looks a bit ugly perhaps, but it’s actually useful even as documentation."
69,The @Check annotation adds a check constraint to the table.
69,@Entity
69,"@Check(name=""ValidISBN"", constraints=""length(isbn)=13"")"
69,class Book { ... }
69,The @Check annotation is commonly used at the field level:
69,"@Id @Check(constraints=""length(isbn)=13"")"
69,String isbn;
69,5. Interacting with the database
69,"To interact with the database, that is, to execute queries, or to insert, update, or delete data, we need an instance of one of the following objects:"
69,"a JPA EntityManager,"
69,"a Hibernate Session, or"
69,a Hibernate StatelessSession.
69,"The Session interface extends EntityManager, and so the only difference between the two interfaces is that Session offers a few more operations."
69,"Actually, in Hibernate, every EntityManager is a Session, and you can narrow it like this:"
69,Session session = entityManager.unwrap(Session.class);
69,An instance of Session (or of EntityManager) is a stateful session.
69,It mediates the interaction between your program and the database via a operations on a persistence context.
69,"In this chapter, we’re not going to talk much about StatelessSession."
69,We’ll come back to this very useful API when we talk about performance.
69,What you need to know for now is that a stateless session doesn’t have a persistence context.
69,"Still, we should let you know that some people prefer to use StatelessSession everywhere."
69,"It’s a simpler programming model, and lets the developer interact with the database more directly."
69,"Stateful sessions certainly have their advantages, but they’re more difficult to reason about, and when something goes wrong, the error messages can be more difficult to understand."
69,5.1. Persistence Contexts
69,"A persistence context is a sort of cache; we sometimes call it the ""first-level cache"", to distinguish it from the second-level cache."
69,"For every entity instance read from the database within the scope of a persistence context, and for every new entity made persistent within the scope of the persistence context, the context holds a unique mapping from the identifier of the entity instance to the instance itself."
69,"Thus, an entity instance may be in one of three states with respect to a given persistence context:"
69,"transient — never persistent, and not associated with the persistence context,"
69,"persistent — currently associated with the persistence context, or"
69,"detached — previously persistent in another session, but not currently associated with this persistence context."
69,"At any given moment, an instance may be associated with at most one persistence context."
69,"The lifetime of a persistence context usually corresponds to the lifetime of a transaction, though it’s possible to have a persistence context that spans several database-level transactions that form a single logical unit of work."
69,"A persistence context—that is, a Session or EntityManager—absolutely positively must not be shared between multiple threads or between concurrent transactions."
69,"If you accidentally leak a session across threads, you will suffer."
69,Container-managed persistence contexts
69,"In a container environment, the lifecycle of a persistence context scoped to the transaction will usually be managed for you."
69,There are several reasons we like persistence contexts.
69,"They help avoid data aliasing: if we modify an entity in one section of code, then other code executing within the same persistence context will see our modification."
69,"They enable automatic dirty checking: after modifying an entity, we don’t need to perform any explicit operation to ask Hibernate to propagate that change back to the database."
69,"Instead, the change will be automatically synchronized with the database when the session is flushed."
69,They can improve performance by avoiding a trip to the database when a given entity instance is requested repeatedly in a given unit of work.
69,They make it possible to transparently batch together multiple database operations.
69,A persistence context also allows us to detect circularities when performing operations on graphs of entities.
69,"(Even in a stateless session, we need some sort of temporary cache of the entity instances we’ve visited while executing a query.)"
69,"On the other hand, stateful sessions come with some very important restrictions, since:"
69,"persistence contexts aren’t threadsafe, and can’t be shared across threads, and"
69,"a persistence context can’t be reused across unrelated transactions, since that would break the isolation and atomicity of the transactions."
69,"Furthermore, a persistence context holds a hard references to all its entities, preventing them from being garbage collected."
69,"Thus, the session must be discarded once a unit of work is complete."
69,"If you don’t completely understand the previous passage, go back and re-read it until you do."
69,A great deal of human suffering has resulted from users mismanaging the lifecycle of the Hibernate Session or JPA EntityManager.
69,We’ll conclude by noting that whether a persistence context helps or harms the performance of a given unit of work depends greatly on the nature of the unit of work.
69,For this reason Hibernate provides both stateful and stateless sessions.
69,5.2. Creating a session
69,"Sticking with standard JPA-defined APIs, we saw how to obtain an EntityManagerFactory in Configuration using JPA XML."
69,It’s quite unsurprising that we may use this object to create an EntityManager:
69,EntityManager entityManager = entityManagerFactory.createEntityManager();
69,"When we’re finished with the EntityManager, we should explicitly clean it up:"
69,entityManager.close();
69,"On the other hand, if we’re starting from a SessionFactory, as described in Configuration using Hibernate API, we may use:"
69,Session session = sessionFactory.openSession();
69,But we still need to clean up:
69,session.close();
69,Injecting the EntityManager
69,"If you’re writing code for some sort of container environment, you’ll probably obtain the EntityManager by some sort of dependency injection."
69,"For example, in Java (or Jakarta) EE you would write:"
69,@PersistenceContext EntityManager entityManager;
69,"In Quarkus, injection is handled by CDI:"
69,@Inject EntityManager entityManager;
69,"Outside a container environment, we’ll also have to write code to manage database transactions."
69,5.3. Managing transactions
69,"Using JPA-standard APIs, the EntityTransaction interface allows us to control database transactions."
69,The idiom we recommend is the following:
69,EntityManager entityManager = entityManagerFactory.createEntityManager();
69,EntityTransaction tx = entityManager.getTransaction();
69,try {
69,tx.begin();
69,//do some work
69,...
69,tx.commit();
69,catch (Exception e) {
69,if (tx.isActive()) tx.rollback();
69,throw e;
69,finally {
69,entityManager.close();
69,"Using Hibernate’s native APIs we might write something really similar,"
69,"but since this sort of code is extremely tedious, we have a much nicer option:"
69,sessionFactory.inTransaction(session -> {
69,//do the work
69,...
69,});
69,Container-managed transactions
69,"In a container environment, the container itself is usually responsible for managing transactions."
69,"In Java EE or Quarkus, you’ll probably indicate the boundaries of the transaction using the @Transactional annotation."
69,5.4. Operations on the persistence context
69,"Of course, the main reason we need an EntityManager is to do stuff to the database."
69,The following important operations let us interact with the persistence context and schedule modifications to the data:
69,Table 33. Methods for modifying data and managing the persistence context
69,Method name and parameters
69,Effect
69,persist(Object)
69,Make a transient object persistent and schedule a SQL insert statement for later execution
69,remove(Object)
69,Make a persistent object transient and schedule a SQL delete statement for later execution
69,merge(Object)
69,Copy the state of a given detached object to a corresponding managed persistent instance and return
69,the persistent object
69,detach(Object)
69,Disassociate a persistent object from a session without
69,affecting the database
69,clear()
69,Empty the persistence context and detach all its entities
69,flush()
69,"Detect changes made to persistent objects association with the session and synchronize the database state with the state of the session by executing SQL insert, update, and delete statements"
69,"Notice that persist() and remove() have no immediate effect on the database, and instead simply schedule a command for later execution."
69,Also notice that there’s no update() operation for a stateful session.
69,Modifications are automatically detected when the session is flushed.
69,"On the other hand, except for getReference(), the following operations all result in immediate access to the database:"
69,Table 34. Methods for reading and locking data
69,Method name and parameters
69,Effect
69,"find(Class,Object)"
69,Obtain a persistent object given its type and its id
69,"find(Class,Object,LockModeType)"
69,"Obtain a persistent object given its type and its id, requesting the given optimistic or pessimistic lock mode"
69,"getReference(Class,id)"
69,"Obtain a reference to a persistent object given its type and its id, without actually loading its state from the database"
69,getReference(Object)
69,"Obtain a reference to a persistent object with the same identity as the given detached instance, without actually loading its state from the database"
69,refresh(Object)
69,Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database
69,"refresh(Object,LockModeType)"
69,"Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database, requesting the given optimistic or pessimistic lock mode"
69,"lock(Object, LockModeType)"
69,Obtain an optimistic or pessimistic lock on a persistent object
69,Any of these operations might throw an exception.
69,"Now, if an exception occurs while interacting with the database, there’s no good way to resynchronize the state of the current persistence context with the state held in database tables."
69,"Therefore, a session is considered to be unusable after any of its methods throws an exception."
69,The persistence context is fragile.
69,"If you receive an exception from Hibernate, you should immediately close and discard the current session. Open a new session if you need to, but throw the bad one away first."
69,Each of the operations we’ve seen so far affects a single entity instance passed as an argument.
69,But there’s a way to set things up so that an operation will propagate to associated entities.
69,5.5. Cascading persistence operations
69,It’s quite often the case that the lifecycle of a child entity is completely dependent on the lifecycle of some parent.
69,"This is especially common for many-to-one and one-to-one associations, though it’s very rare for many-to-many associations."
69,"For example, it’s quite common to make an Order and all its Items persistent in the same transaction, or to delete a Project and its Filess at once."
69,This sort of relationship is sometimes called a whole/part-type relationship.
69,Cascading is a convenience which allows us to propagate one of the operations listed in Operations on the persistence context from a parent to its children.
69,"To set up cascading, we specify the cascade member of one of the association mapping annotations, usually @OneToMany or @OneToOne."
69,@Entity
69,class Order {
69,...
69,"@OneToMany(mappedby=Item_.ORDER,"
69,"// cascade persist(), remove(), and refresh() from Order to Item"
69,"cascade={PERSIST,REMOVE,REFRESH},"
69,// also remove() orphaned Items
69,orphanRemoval=true)
69,private Set<Item> items;
69,...
69,Orphan removal indicates that an Item should be automatically deleted if it is removed from the set of items belonging to its parent Order.
69,5.6. Proxies and lazy fetching
69,"Our data model is a set of interconnected entities, and in Java our whole dataset would be represented as an enormous interconnected graph of objects."
69,"It’s possible that this graph is disconnected, but more likely it’s connected, or composed of a relatively small number of connected subgraphs."
69,"Therefore, when we retrieve on object belonging to this graph from the database and instantiate it in memory, we simply can’t recursively retrieve and instantiate all its associated entities."
69,"Quite aside from the waste of memory on the VM side, this process would involve a huge number of round trips to the database server, or a massive multidimensional cartesian product of tables, or both."
69,"Instead, we’re forced to cut the graph somewhere."
69,Hibernate solves this problem using proxies and lazy fetching.
69,"A proxy is an object that masquerades as a real entity or collection, but doesn’t actually hold any state, because that state has not yet been fetched from the database."
69,"When you call a method of the proxy, Hibernate will detect the call and fetch the state from the database before allowing the invocation to proceed to the real entity object or collection."
69,Now for the gotchas:
69,Hibernate will only do this for an entity which is currently associated with a persistence context.
69,"Once the session ends, and the persistence context is cleaned up, the proxy is no longer fetchable, and instead its methods throw the hated LazyInitializationException."
69,A round trip to the database to fetch the state of a single entity instance is just about the least efficient way to access data.
69,It almost inevitably leads to the infamous N+1 selects problem we’ll discuss later when we talk about how to optimize association fetching.
69,"We’re getting a bit ahead of ourselves here, but let’s quickly mention the general strategy we recommend to navigate past these gotchas:"
69,All associations should be set fetch=LAZY to avoid fetching extra data when it’s not needed.
69,"As we mentioned earlier, this setting is not the default for @ManyToOne associations, and must be specified explicitly."
69,But strive to avoid writing code which triggers lazy fetching.
69,"Instead, fetch all the data you’ll need upfront at the beginning of a unit of work, using one of the techniques described in Association fetching, usually, using join fetch in HQL or an EntityGraph."
69,It’s important to know that some operations which may be performed with an unfetched proxy don’t require fetching its state from the database.
69,"First, we’re always allowed to obtain its identifier:"
69,"var pubId = entityManager.find(Book.class, bookId).getPublisher().getId(); // does not fetch publisher"
69,"Second, we may create an association to a proxy:"
69,"book.setPublisher(entityManager.getReference(Publisher.class, pubId)); // does not fetch publisher"
69,Sometimes it’s useful to test whether a proxy or collection has been fetched from the database.
69,JPA lets us do this using the PersistenceUnitUtil:
69,boolean authorsFetched = entityManagerFactory.getPersistenceUnitUtil().isLoaded(book.getAuthors());
69,Hibernate has a slightly easier way to do it:
69,boolean authorsFetched = Hibernate.isInitialized(book.getAuthors());
69,"But the static methods of the Hibernate class let us do a lot more, and it’s worth getting a bit familiar them."
69,Of particular interest are the operations which let us work with unfetched collections without fetching their state from the database.
69,"For example, consider this code:"
69,"Book book = session.find(Book.class, bookId);"
69,"// fetch just the Book, leaving authors unfetched"
69,"Author authorRef = session.getReference(Author.class, authorId);"
69,// obtain an unfetched proxy
69,"boolean isByAuthor = Hibernate.contains(book.getAuthors(), authorRef); // no fetching"
69,This code fragment leaves both the set book.authors and the proxy authorRef unfetched.
69,"Finally, Hibernate.initialize() is a convenience method that force-fetches a proxy or collection:"
69,"Book book = session.find(Book.class, bookId);"
69,"// fetch just the Book, leaving authors unfetched"
69,Hibernate.initialize(book.getAuthors());
69,// fetch the Authors
69,"But of course, this code is very inefficient, requiring two trips to the database to obtain data that could in principle be retrieved with just one query."
69,"It’s clear from the discussion above that we need a way to request that an association be eagerly fetched using a database join, thus protecting ourselves from the infamous N+1 selects."
69,One way to do this is by passing an EntityGraph to find().
69,5.7. Entity graphs and eager fetching
69,"When an association is mapped fetch=LAZY, it won’t, by default, be fetched when we call the find() method."
69,We may request that an association be fetched eagerly (immediately) by passing an EntityGraph to find().
69,The JPA-standard API for this is a bit unwieldy:
69,var graph = entityManager.createEntityGraph(Book.class);
69,graph.addSubgraph(Book_.publisher);
69,"Book book = entityManager.find(Book.class, bookId, Map.of(SpecHints.HINT_SPEC_FETCH_GRAPH, graph));"
69,This is untypesafe and unnecessarily verbose.
69,Hibernate has a better way:
69,var graph = session.createEntityGraph(Book.class);
69,graph.addSubgraph(Book_.publisher);
69,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
69,"This code adds a left outer join to our SQL query, fetching the associated Publisher along with the Book."
69,We may even attach additional nodes to our EntityGraph:
69,var graph = session.createEntityGraph(Book.class);
69,graph.addSubgraph(Book_.publisher);
69,graph.addPluralSubgraph(Book_.authors).addSubgraph(Author_.person);
69,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
69,This results in a SQL query with four left outer joins.
69,"In the code examples above, The classes Book_ and Author_ are generated by the JPA Metamodel Generator we saw earlier."
69,They let us refer to attributes of our model in a completely type-safe way.
69,"We’ll use them again, below, when we talk about Criteria queries."
69,JPA specifies that any given EntityGraph may be interpreted in two different ways.
69,A fetch graph specifies exactly the associations that should be eagerly loaded.
69,Any association not belonging to the entity graph is proxied and loaded lazily only if required.
69,A load graph specifies that the associations in the entity graph are to be fetched in addition to the associations mapped fetch=EAGER.
69,"You’re right, the names make no sense."
69,"But don’t worry, if you take our advice, and map your associations fetch=LAZY, there’s no difference between a ""fetch"" graph and a ""load"" graph, so the names don’t matter."
69,JPA even specifies a way to define named entity graphs using annotations.
69,But the annotation-based API is so verbose that it’s just not worth using.
69,5.8. Flushing the session
69,"From time to time, a flush operation is triggered, and the session synchronizes dirty state held in memory—that is, modifications to the state of entities associated with the persistence context—with persistent state held in the database. Of course, it does this by executing SQL INSERT, UPDATE, and DELETE statements."
69,"By default, a flush is triggered:"
69,"when the current transaction commits, for example, when Transaction.commit() is called,"
69,"before execution of a query whose result would be affected by the synchronization of dirty state held in memory, or"
69,when the program directly calls flush().
69,"Notice that SQL statements are not usually executed synchronously by methods of the Session interface like persist() and remove(). If synchronous execution of SQL is desired, the StatelessSession allows this."
69,This behavior can be controlled by explicitly setting the flush mode.
69,"For example, to disable flushes that occur before query execution, call:"
69,entityManager.setFlushMode(FlushModeType.COMMIT);
69,Hibernate allows greater control over the flush mode than JPA:
69,session.setHibernateFlushMode(FlushMode.MANUAL);
69,"Since flushing is a somewhat expensive operation (the session must dirty-check every entity in the persistence context), setting the flush mode to COMMIT can occasionally be a useful optimization."
69,Table 35. Flush modes
69,Hibernate FlushMode
69,JPA FlushModeType
69,Interpretation
69,MANUAL
69,Never flush automatically
69,COMMIT
69,COMMIT
69,Flush before transaction commit
69,AUTO
69,AUTO
69,"Flush before transaction commit, and before execution of a query whose results might be affected by modifications held in memory"
69,ALWAYS
69,"Flush before transaction commit, and before execution of every query"
69,A second way to reduce the cost of flushing is to load entities in read-only mode:
69,"Session.setDefaultReadOnly(false) specifies that all entities loaded by a given session should be loaded in read-only mode by default,"
69,"SelectionQuery.setReadOnly(false) specifies that every entity returned by a given query should be loaded in read-only mode, and"
69,"Session.setReadOnly(Object, false) specifies that a given entity already loaded by the session should be switched to read-only mode."
69,It’s not necessary to dirty-check an entity instance in read-only mode.
69,5.9. Queries
69,Hibernate features three complementary ways to write queries:
69,"the Hibernate Query Language, an extremely powerful superset of JPQL, which abstracts most of the features of modern dialects of SQL,"
69,"the JPA criteria query API, along with extensions, allowing almost any HQL query to be constructed programmatically via a typesafe API, and, of course"
69,"for when all else fails, native SQL queries."
69,5.10. HQL queries
69,A full discussion of the query language would require almost as much text as the rest of this Introduction.
69,"Fortunately, HQL is already described in exhaustive (and exhausting) detail in A Guide to Hibernate Query Language."
69,It doesn’t make sense to repeat that information here.
69,Here we want to see how to execute a query via the Session or EntityManager API.
69,The method we call depends on what kind of query it is:
69,"selection queries return a result list, but do not modify the data, but"
69,"mutation queries modify data, and return the number of modified rows."
69,"Selection queries usually start with the keyword select or from, whereas mutation queries begin with the keyword insert, update, or delete."
69,Table 36. Executing HQL
69,Kind
69,Session method
69,EntityManager method
69,Query execution method
69,Selection
69,"createSelectionQuery(String,Class)"
69,"createQuery(String,Class)"
69,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
69,Mutation
69,createMutationQuery(String)
69,createQuery(String)
69,executeUpdate()
69,So for the Session API we would write:
69,List<Book> matchingBooks =
69,"session.createSelectionQuery(""from Book where title like :titleSearchPattern"", Book.class)"
69,".setParameter(""titleSearchPattern"", titleSearchPattern)"
69,.getResultList();
69,"Or, if we’re sticking to the JPA-standard APIs:"
69,List<Book> matchingBooks =
69,"entityManager.createQuery(""select b from Book b where b.title like :titleSearchPattern"", Book.class)"
69,".setParameter(""titleSearchPattern"", titleSearchPattern)"
69,.getResultList();
69,"The only difference between createSelectionQuery() and createQuery() is that createSelectionQuery() throws an exception if passed an insert, delete, or update."
69,"In the query above, :titleSearchPattern is called a named parameter."
69,We may also identify parameters by a number.
69,These are called ordinal parameters.
69,List<Book> matchingBooks =
69,"session.createSelectionQuery(""from Book where title like ?1"", Book.class)"
69,".setParameter(1, titleSearchPattern)"
69,.getResultList();
69,"When a query has multiple parameters, named parameters tend to be easier to read, even if slightly more verbose."
69,Never concatenate user input with HQL and pass the concatenated string to createSelectionQuery().
69,This would open up the possibility for an attacker to execute arbitrary code on your database server.
69,"If we’re expecting a query to return a single result, we can use getSingleResult()."
69,Book book =
69,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
69,".setParameter(1, isbn)"
69,.getSingleResult();
69,"Or, if we’re expecting it to return at most one result, we can use getSingleResultOrNull()."
69,Book bookOrNull =
69,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
69,".setParameter(1, isbn)"
69,.getSingleResultOrNull();
69,"The difference, of course, is that getSingleResult() throws an exception if there’s no matching row in the database, whereas getSingleResultOrNull() just returns null."
69,"By default, Hibernate dirty checks entities in the persistence context before executing a query, in order to determine if the session should be flushed."
69,"If there are many entities association with the persistence context, then this can be an expensive operation."
69,"To disable this behavior, set the flush mode to COMMIT or MANUAL:"
69,Book bookOrNull =
69,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
69,".setParameter(1, isbn)"
69,.setHibernateFlushMode(MANUAL)
69,.getSingleResult();
69,Setting the flush mode to COMMIT or MANUAL might cause the query to return stale results.
69,"Occasionally we need to build a query at runtime, from a set of optional conditions."
69,"For this, JPA offers an API which allows programmatic construction of a query."
69,5.11. Criteria queries
69,"Imagine we’re implementing some sort of search screen, where the user of our system is offered several different ways to constrain the query result set."
69,"For example, we might let them search for books by title and/or the author name."
69,"Of course, we could construct a HQL query by string concatenation, but this is a bit fragile, so it’s quite nice to have an alternative."
69,HQL is implemented in terms of criteria objects
69,"Actually, in Hibernate 6, every HQL query is compiled to a criteria query before being translated to SQL."
69,This ensures that the semantics of HQL and criteria queries are identical.
69,First we need an object for building criteria queries.
69,"Using the JPA-standard APIs, this would be a CriteriaBuilder, and we get it from the EntityManagerFactory:"
69,CriteriaBuilder builder = entityManagerFactory.getCriteriaBuilder();
69,"But if we have a SessionFactory, we get something much better, a HibernateCriteriaBuilder:"
69,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
69,The HibernateCriteriaBuilder extends CriteriaBuilder and adds many operations that JPQL doesn’t have.
69,"If you’re using EntityManagerFactory, don’t despair, you have two perfectly good ways to obtain the HibernateCriteriaBuilder associated with that factory."
69,Either:
69,HibernateCriteriaBuilder builder =
69,entityManagerFactory.unwrap(SessionFactory.class).getCriteriaBuilder();
69,Or simply:
69,HibernateCriteriaBuilder builder =
69,(HibernateCriteriaBuilder) entityManagerFactory.getCriteriaBuilder();
69,We’re ready to create a criteria query.
69,CriteriaQuery<Book> query = builder.createQuery(Book.class);
69,Root<Book> book = query.from(Book.class);
69,Predicate where = builder.conjunction();
69,if (titlePattern != null) {
69,"where = builder.and(where, builder.like(book.get(Book_.title), titlePattern));"
69,if (namePattern != null) {
69,"Join<Book,Author> author = book.join(Book_.author);"
69,"where = builder.and(where, builder.like(author.get(Author_.name), namePattern));"
69,query.select(book).where(where)
69,.orderBy(builder.asc(book.get(Book_.title)));
69,"Here, as before, the classes Book_ and Author_ are generated by Hibernate’s JPA Metamodel Generator."
69,Notice that we didn’t bother treating titlePattern and namePattern as parameters.
69,"That’s safe because, by default, Hibernate automatically and transparently treats strings passed to the CriteriaBuilder as JDBC parameters."
69,Execution of a criteria query works almost exactly like execution of HQL.
69,Table 37. Executing criteria queries
69,Kind
69,Session method
69,EntityManager method
69,Query execution method
69,Selection
69,createSelectionQuery(CriteriaQuery)
69,createQuery(CriteriaQuery)
69,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
69,Mutation
69,createMutationQuery(CriteriaUpdate) or createQuery(CriteriaDelete)
69,createQuery(CriteriaUpdate) or createQuery(CriteriaDelte)
69,executeUpdate()
69,For example:
69,List<Book> matchingBooks =
69,session.createSelectionQuery(query)
69,.getResultList();
69,"Update, insert, and delete queries work similarly:"
69,CriteriaDelete<Book> delete = builder.createCriteriaDelete(Book.class);
69,Root<Book> book = delete.from(Book.class);
69,"delete.where(builder.lt(builder.year(book.get(Book_.publicationDate)), 2000));"
69,session.createMutationQuery(delete).executeUpdate();
69,"It’s even possible to transform a HQL query string to a criteria query, and modify the query programmatically before execution:"
69,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
69,"var query = builder.createQuery(""from Book where year(publicationDate) > 2000"", Book.class);"
69,var root = (Root<Book>) query.getRootList().get(0);
69,"query.where(builder.like(root.get(Book_.title), builder.literal(""Hibernate%"")));"
69,"query.orderBy(builder.asc(root.get(Book_.title)), builder.desc(root.get(Book_.isbn)));"
69,List<Book> matchingBooks = session.createSelectionQuery(query).getResultList();
69,Do you find some of the code above a bit too verbose?
69,We do.
69,5.12. A more comfortable way to write criteria queries
69,"Actually, what makes the JPA criteria API less ergonomic than it should be is the need to call all operations of the CriteriaBuilder as instance methods, instead of having them as static functions."
69,The reason it works this way is that each JPA provider has its own implementation of CriteriaBuilder.
69,Hibernate 6.3 introduces the helper class CriteriaDefinition to reduce the verbosity of criteria queries.
69,Our example looks like this:
69,CriteriaQuery<Book> query =
69,"new CriteriaDefinition(entityManagerFactory, Book.class) {{"
69,select(book);
69,if (titlePattern != null) {
69,"restrict(like(book.get(Book_.title), titlePattern));"
69,if (namePattern != null) {
69,var author = book.join(Book_.author);
69,"restrict(like(author.get(Author_.name), namePattern));"
69,orderBy(asc(book.get(Book_.title)));
69,}};
69,"When all else fails, and sometimes even before that, we’re left with the option of writing a query in SQL."
69,5.13. Native SQL queries
69,"HQL is a powerful language which helps reduce the verbosity of SQL, and significantly increases portability of queries between databases."
69,"But ultimately, the true value of ORM is not in avoiding SQL, but in alleviating the pain involved in dealing with SQL result sets once we get them back to our Java program."
69,"As we said right up front, Hibernate’s generated SQL is meant to be used in conjunction with handwritten SQL, and native SQL queries are one of the facilities we provide to make that easy."
69,Table 38. Executing SQL
69,Kind
69,Session method
69,EntityManager method
69,Query execution method
69,Selection
69,"createNativeQuery(String,Class)"
69,"createNativeQuery(String,Class)"
69,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
69,Mutation
69,createNativeMutationQuery(String)
69,createNativeQuery(String)
69,executeUpdate()
69,Stored procedure
69,createStoredProcedureCall(String)
69,createStoredProcedureQuery(String)
69,execute()
69,"For the most simple cases, Hibernate can infer the shape of the result set:"
69,Book book =
69,"session.createNativeQuery(""select * from Books where isbn = ?1"", Book.class)"
69,.getSingleResult();
69,String title =
69,"session.createNativeQuery(""select title from Books where isbn = ?1"", String.class)"
69,.getSingleResult();
69,"However, in general, there isn’t enough information in the JDBC ResultSetMetaData to infer the mapping of columns to entity objects."
69,"So for more complicated cases, you’ll need to use the @SqlResultSetMapping annotation to define a named mapping, and pass the name to createNativeQuery(). This gets fairly messy, so we don’t want to hurt your eyes by showing you an example of it."
69,"By default, Hibernate doesn’t flush the session before execution of a native query."
69,That’s because the session is unaware of which modifications held in memory would affect the results of the query.
69,"So if there are any unflushed changes to Books, this query might return stale data:"
69,List<Book> books =
69,"session.createNativeQuery(""select * from Books"")"
69,.getResultList()
69,There’s two ways to ensure the persistence context is flushed before this query is executed.
69,"Either, we could simply force a flush by calling flush() or by setting the flush mode to ALWAYS:"
69,List<Book> books =
69,"session.createNativeQuery(""select * from Books"")"
69,.setHibernateFlushMode(ALWAYS)
69,.getResultList()
69,"Or, alternatively, we could tell Hibernate which modified state affects the results of the query:"
69,List<Book> books =
69,"session.createNativeQuery(""select * from Books"")"
69,.addSynchronizedEntityClass(Book.class)
69,.getResultList()
69,You can call stored procedures using createStoredProcedureQuery() or createStoredProcedureCall().
69,"5.14. Limits, pagination, and ordering"
69,"If a query might return more results than we can handle at one time, we may specify:"
69,"a limit on the maximum number of rows returned, and,"
69,"optionally, an offset, the first row of an ordered result set to return."
69,The offset is used to paginate query results.
69,There’s two ways to add a limit or offset to a HQL or native SQL query:
69,"using the syntax of the query language itself, for example, offset 10 rows fetch next 20 rows only, or"
69,using the methods setFirstResult() and setMaxResults() of the SelectionQuery interface.
69,"If the limit or offset is parameterized, the second option is simpler."
69,"For example, this:"
69,List<Book> books =
69,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
69,".setParameter(1, titlePattern)"
69,.setMaxResults(MAX_RESULTS)
69,.getResultList();
69,is simpler than:
69,List<Book> books =
69,"session.createSelectionQuery(""from Book where title like ?1 order by title fetch first ?2 rows only"")"
69,".setParameter(1, titlePattern)"
69,".setParameter(2, MAX_RESULTS)"
69,.getResultList();
69,Hibernate’s SelectionQuery has a slightly different way to paginate the query results:
69,List<Book> books =
69,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
69,".setParameter(1, titlePattern)"
69,.setPage(Page.first(MAX_RESULTS))
69,.getResultList();
69,A closely-related issue is ordering.
69,It’s quite common for pagination to be combined with the need to order query results by a field that’s determined at runtime.
69,"So, as an alternative to the HQL order by clause, SelectionQuery offers the ability to specify that the query results should be ordered by one or more fields of the entity type returned by the query:"
69,List<Book> books =
69,"session.createSelectionQuery(""from Book where title like ?1"")"
69,".setParameter(1, titlePattern)"
69,".setOrder(List.of(Order.asc(Book._title), Order.asc(Book._isbn)))"
69,.setMaxResults(MAX_RESULTS)
69,.getResultList();
69,"Unfortunately, there’s no way to do this using JPA’s TypedQuery interface."
69,"Table 39. Methods for query limits, pagination, and ordering"
69,Method name
69,Purpose
69,JPA-standard
69,setMaxResults()
69,Set a limit on the number of results returned by a query
69,setFirstResult()
69,Set an offset on the results returned by a query
69,setPage()
69,Set the limit and offset by specifying a Page object
69,setOrder()
69,Specify how the query results should be ordered
69,5.15. Representing projection lists
69,"A projection list is the list of things that a query returns, that is, the list of expressions in the select clause."
69,"Since Java has no tuple types, representing query projection lists in Java has always been a problem for JPA and Hibernate."
69,"Traditionally, we’ve just used Object[] most of the time:"
69,var results =
69,"session.createSelectionQuery(""select isbn, title from Book"", Object[].class)"
69,.getResultList();
69,for (var result : results) {
69,var isbn = (String) result[0];
69,var title = (String) result[1];
69,...
69,This is really a bit ugly.
69,Java’s record types now offer an interesting alternative:
69,"record IsbnTitle(String isbn, String title) {}"
69,var results =
69,"session.createSelectionQuery(""select isbn, title from Book"", IsbnTitle.class)"
69,.getResultList();
69,for (var result : results) {
69,var isbn = result.isbn();
69,var title = result.title();
69,...
69,Notice that we’re able to declare the record right before the line which executes the query.
69,"Now, this is only superficially more typesafe, since the query itself is not checked statically, and so we can’t say it’s objectively better."
69,But perhaps you find it more aesthetically pleasing.
69,"And if we’re going to be passing query results around the system, the use of a record type is much better."
69,The criteria query API offers a much more satisfying solution to the problem.
69,Consider the following code:
69,var builder = sessionFactory.getCriteriaBuilder();
69,var query = builder.createTupleQuery();
69,var book = query.from(Book.class);
69,var bookTitle = book.get(Book_.title);
69,var bookIsbn = book.get(Book_.isbn);
69,var bookPrice = book.get(Book_.price);
69,"query.select(builder.tuple(bookTitle, bookIsbn, bookPrice));"
69,var resultList = session.createSelectionQuery(query).getResultList();
69,for (var result: resultList) {
69,String title = result.get(bookTitle);
69,String isbn = result.get(bookIsbn);
69,BigDecimal price = result.get(bookPrice);
69,...
69,"This code is manifestly completely typesafe, and much better than we can hope to do with HQL."
69,5.16. Named queries
69,The @NamedQuery annotation lets us define a HQL query that is compiled and checked as part of the bootstrap process.
69,"This means we find out about errors in our queries earlier, instead of waiting until the query is actually executed."
69,"We can place the @NamedQuery annotation on any class, even on an entity class."
69,"@NamedQuery(name=""10BooksByTitle"","
69,"query=""from Book where title like :titlePattern order by title fetch first 10 rows only"")"
69,class BookQueries {}
69,"We have to make sure that the class with the @NamedQuery annotation will be scanned by Hibernate, either:"
69,"by adding <class>org.hibernate.example.BookQueries</class> to persistence.xml, or"
69,by calling configuration.addClass(BookQueries.class).
69,"Unfortunately, JPA’s @NamedQuery annotation can’t be placed on a package descriptor."
69,"Therefore, Hibernate provides a very similar annotation, @org.hibernate.annotations.NamedQuery which can be specified at the package level."
69,"If we declare a named query at the package level, we must call:"
69,"configuration.addPackage(""org.hibernate.example"")"
69,so that Hibernate knows where to find it.
69,The @NamedNativeQuery annotation lets us do the same for native SQL queries.
69,"There’s much less advantage to using @NamedNativeQuery, because there is very little that Hibernate can do to validate the correctness of a query written in the native SQL dialect of your database."
69,Table 40. Executing named queries
69,Kind
69,Session method
69,EntityManager method
69,Query execution method
69,Selection
69,"createNamedSelectionQuery(String,Class)"
69,"createNamedQuery(String,Class)"
69,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
69,Mutation
69,createNamedMutationQuery(String)
69,createNamedQuery(String)
69,executeUpdate()
69,We execute our named query like this:
69,List<Book> books =
69,entityManager.createNamedQuery(BookQueries_.QUERY_10_BOOKS_BY_TITLE)
69,".setParameter(""titlePattern"", titlePattern)"
69,.getResultList()
69,"Here, BookQueries_.QUERY_10_BOOKS_BY_TITLE is a constant with value ""10BooksByTitle"", generated by the Metamodel Generator."
69,"Note that the code which executes the named query is not aware of whether the query was written in HQL or in native SQL, making it slightly easier to change and optimize the query later."
69,It’s nice to have our queries checked at startup time.
69,It’s even better to have them checked at compile time.
69,"In Organizing persistence logic, we mentioned that the Metamodel Generator can do that for us, with the help of the @CheckHQL annotation, and we presented that as a reason to use @NamedQuery."
69,"But actually, Hibernate has a separate Query Validator capable of performing compile-time validation of HQL query strings that occur as arguments to createQuery() and friends."
69,"If we use the Query Validator, there’s not much advantage to the use of named queries."
69,5.17. Controlling lookup by id
69,"We can do almost anything via HQL, criteria, or native SQL queries."
69,"But when we already know the identifier of the entity we need, a query can feel like overkill."
69,And queries don’t make efficient use of the second level cache.
69,We met the find() method earlier.
69,It’s the most basic way to perform a lookup by id.
69,"But as we also already saw, it can’t quite do everything."
69,"Therefore, Hibernate has some APIs that streamline certain more complicated lookups:"
69,Table 41. Operations for lookup by id
69,Method name
69,Purpose
69,byId()
69,"Lets us specify association fetching via an EntityGraph, as we saw; also lets us specify some additional options, including how the lookup interacts with the second level cache, and whether the entity should be loaded in read-only mode"
69,byMultipleIds()
69,Lets us load a batch of ids at the same time
69,Batch loading is very useful when we need to retrieve multiple instances of the same entity class by id:
69,var graph = session.createEntityGraph(Book.class);
69,graph.addSubgraph(Book_.publisher);
69,List<Book> books =
69,session.byMultipleIds(Book.class)
69,.withFetchGraph(graph)
69,// control association fetching
69,.withBatchSize(20)
69,// specify an explicit batch size
69,.with(CacheMode.GET)
69,// control interaction with the cache
69,.multiLoad(bookIds);
69,"The given list of bookIds will be broken into batches, and each batch will be fetched from the database in a single select."
69,"If we don’t specify the batch size explicitly, a batch size will be chosen automatically."
69,We also have some operations for working with lookups by natural id:
69,Method name
69,Purpose
69,bySimpleNaturalId()
69,For an entity with just one attribute is annotated @NaturalId
69,byNaturalId()
69,For an entity with multiple attributes are annotated @NaturalId
69,byMultipleNaturalId()
69,Lets us load a batch of natural ids at the same time
69,Here’s how we can retrieve an entity by its composite natural id:
69,Book book =
69,session.byNaturalId(Book.class)
69,".using(Book_.isbn, isbn)"
69,".using(Book_.printing, printing)"
69,.load();
69,"Notice that this code fragment is completely typesafe, again thanks to the Metamodel Generator."
69,5.18. Interacting directly with JDBC
69,From time to time we run into the need to write some code that calls JDBC directly.
69,"Unfortunately, JPA offers no good way to do this, but the Hibernate Session does."
69,session.doWork(connection -> {
69,"try (var callable = connection.prepareCall(""{call myproc(?)}"")) {"
69,"callable.setLong(1, argument);"
69,callable.execute();
69,});
69,"The Connection passed to the work is the same connection being used by the session, and so any work performed using that connection occurs in the same transaction context."
69,"If the work returns a value, use doReturningWork() instead of doWork()."
69,"In a container environment where transactions and database connections are managed by the container, this might not be the easiest way to obtain the JDBC connection."
69,5.19. What to do when things go wrong
69,"Object/relational mapping has been called the ""Vietnam of computer science""."
69,"The person who made this analogy is American, and so one supposes that he meant to imply some kind of unwinnable war."
69,"This is quite ironic, since at the very moment he made this comment, Hibernate was already on the brink of winning the war."
69,"Today, Vietnam is a peaceful country with exploding per-capita GDP, and ORM is a solved problem."
69,"That said, Hibernate is complex, and ORM still presents many pitfalls for the inexperienced, even occasionally for the experienced."
69,Sometimes things go wrong.
69,"In this section we’ll quickly sketch some general strategies for avoiding ""quagmires""."
69,Understand SQL and the relational model.
69,Know the capabilities of your RDBMS.
69,Work closely with the DBA if you’re lucky enough to have one.
69,"Hibernate is not about ""transparent persistence"" for Java objects."
69,It’s about making two excellent technologies work smoothly together.
69,Log the SQL executed by Hibernate.
69,You cannot know that your persistence logic is correct until you’ve actually inspected the SQL that’s being executed.
69,"Even when everything seems to be ""working"", there might be a lurking N+1 selects monster."
69,Be careful when modifying bidirectional associations.
69,"In principle, you should update both ends of the association."
69,"But Hibernate doesn’t strictly enforce that, since there are some situations where such a rule would be too heavy-handed."
69,"Whatever the case, it’s up to you to maintain consistency across your model."
69,Never leak a persistence context across threads or concurrent transactions.
69,Have a strategy or framework to guarantee this never happens.
69,"When running queries that return large result sets, take care to consider the size of the session cache."
69,Consider using a stateless session.
69,"Think carefully about the semantics of the second-level cache, and how the caching policies impact transaction isolation."
69,Avoid fancy bells and whistles you don’t need.
69,"Hibernate is incredibly feature-rich, and that’s a good thing, because it serves the needs of a huge number of users, many of whom have one or two very specialized needs."
69,But nobody has all those specialized needs.
69,"In all probability, you have none of them."
69,"Write your domain model in the simplest way that’s reasonable, using the simplest mapping strategies that make sense."
69,"When something isn’t behaving as you expect, simplify."
69,Isolate the problem.
69,"Find the absolute minimum test case which reproduces the behavior, before asking for help online."
69,"Most of the time, the mere act of isolating the problem will suggest an obvious solution."
69,"Avoid frameworks and libraries that ""wrap"" JPA."
69,"If there’s any one criticism of Hibernate and ORM that sometimes does ring true, it’s that it takes you too far from direct control over JDBC."
69,An additional layer just takes you even further.
69,Avoid copy/pasting code from random bloggers or stackoverflow reply guys.
69,"Many of the suggestions you’ll find online just aren’t the simplest solution, and many aren’t correct for Hibernate 6."
69,"Instead, understand what you’re doing; study the Javadoc of the APIs you’re using; read the JPA specification; follow the advice we give in this document; go direct to the Hibernate team on Zulip."
69,"(Sure, we can be a bit cantankerous at times, but we do always want you to be successful.)"
69,Always consider other options.
69,You don’t have to use Hibernate for everything.
69,6. Compile-time tooling
69,The Metamodel Generator is a standard part of JPA.
69,"We’ve actually already seen its handiwork in the code examples earlier: it’s the author of the class Book_, which contains the static metamodel of the entity class Book."
69,The Metamodel Generator
69,Hibernate’s Metamodel Generator is an annotation processor that produces what JPA calls a static metamodel.
69,"That is, it produces a typed model of the persistent classes in our program, giving us a type-safe way to refer to their attributes in Java code."
69,"In particular, it lets us specify entity graphs and criteria queries in a completely type-safe way."
69,The history behind this thing is quite interesting.
69,"Back when Java’s annotation processing API was brand spankin' new, the static metamodel for JPA was proposed by Gavin King for inclusion in JPA 2.0, as a way to achieve type safety in the nascent criteria query API."
69,"It’s fair to say that, back in 2010, this API was not a runaway success."
69,"Tools did not, at the time, feature robust support for annotation processors."
69,And all the explicit generic types made user code quite verbose and difficult to read.
69,(The need for an explicit reference to a CriteriaBuilder instance also contributed verbosity to the criteria API.)
69,"For years, Gavin counted this as one of his more embarrassing missteps."
69,But time has been kind to the static metamodel.
69,"In 2023, all Java compilers, build tools, and IDEs have robust support for annotation processing, and Java’s local type inference (the var keyword) eliminates the verbose generic types."
69,"JPA’s CriteriaBuilder and EntityGraph APIs are still not quite perfect, but the imperfections aren’t related to static type safety or annotation processing."
69,The static metamodel itself is undeniably useful and elegant.
69,"And so now, in Hibernate 6.3, we’re finally ready to go new places with the Metamodel Generator."
69,And it turns out that there’s quite a lot of unlocked potential there.
69,"Now, you still don’t have to use the Metamodel Generator with Hibernate—the APIs we just mentioned still also accept plain strings—but we find that it works well with Gradle and integrates smoothly with our IDE, and the advantage in type-safety is compelling."
69,We’ve already seen how to set up the annotation processor in the Gradle build we saw earlier.
69,"For more details on how to integrate the Metamodel Generator, check out the Static Metamodel Generator section in the User Guide."
69,"Here’s an example of the sort of code that’s generated for an entity class, as mandated by the JPA specification:"
69,Generated Code
69,@StaticMetamodel(Book.class)
69,public abstract class Book_ {
69,/**
69,* @see org.example.Book#isbn
69,**/
69,"public static volatile SingularAttribute<Book, String> isbn;"
69,/**
69,* @see org.example.Book#text
69,**/
69,"public static volatile SingularAttribute<Book, String> text;"
69,/**
69,* @see org.example.Book#title
69,**/
69,"public static volatile SingularAttribute<Book, String> title;"
69,/**
69,* @see org.example.Book#type
69,**/
69,"public static volatile SingularAttribute<Book, Type> type;"
69,/**
69,* @see org.example.Book#publicationDate
69,**/
69,"public static volatile SingularAttribute<Book, LocalDate> publicationDate;"
69,/**
69,* @see org.example.Book#publisher
69,**/
69,"public static volatile SingularAttribute<Book, Publisher> publisher;"
69,/**
69,* @see org.example.Book#authors
69,**/
69,"public static volatile SetAttribute<Book, Author> authors;"
69,"public static final String ISBN = ""isbn"";"
69,"public static final String TEXT = ""text"";"
69,"public static final String TITLE = ""title"";"
69,"public static final String TYPE = ""type"";"
69,"public static final String PUBLICATION_DATE = ""publicationDate"";"
69,"public static final String PUBLISHER = ""publisher"";"
69,"public static final String AUTHORS = ""authors"";"
69,"For each attribute of the entity, the Book_ class has:"
69,"a String-valued constant like TITLE , and"
69,a typesafe reference like title to a metamodel object of type Attribute.
69,We’ve already been using metamodel references like Book_.authors and Book.AUTHORS in the previous chapters.
69,So now let’s see what else the Metamodel Generator can do for us.
69,"The Metamodel Generator provides statically-typed access to elements of the JPA Metamodel. But the Metamodel is also accessible in a ""reflective"" way, via the EntityManagerFactory."
69,EntityType<Book> book = entityManagerFactory.getMetamodel().entity(Book.class);
69,"SingularAttribute<Book,Long> id = book.getDeclaredId(Long.class)"
69,This is very useful for writing generic code in frameworks or libraries.
69,"For example, you could use it to create your own criteria query API."
69,"Automatic generation of finder methods and query methods is a new feature of Hibernate’s implementation of the Metamodel Generator, and an extension to the functionality defined by the JPA specification."
69,"In this chapter, we’re going to explore these features."
69,The functionality described in the rest of this chapter depends on the use of the annotations described in Entities.
69,"The Metamodel Generator is not currently able to generate finder methods and query methods for entities declared completely in XML, and it’s not able to validate HQL which queries such entities."
69,"(On the other hand, the O/R mappings may be specified in XML, since they’re not needed by the Metamodel Generator.)"
69,We’re going to meet three different kinds of generated method:
69,"a named query method has its signature and implementation generated directly from a @NamedQuery annotation,"
69,"a query method has a signature that’s explicitly declared, and a generated implementation which executes a HQL or SQL query specified via a @HQL or @SQL annotation, and"
69,"a finder method annotated @Find has a signature that’s explicitly declared, and a generated implementation inferred from the parameter list."
69,"To whet our appetites, let’s see how this works for a @NamedQuery."
69,6.1. Named queries and the Metamodel Generator
69,"The very simplest way to generate a query method is to put a @NamedQuery annotation anywhere we like, with a name beginning with the magical character #."
69,Let’s just stick it on the Book class:
69,@CheckHQL // validate the query at compile time
69,"@NamedQuery(name = ""#findByTitleAndType"","
69,"query = ""select book from Book book where book.title like :titlen and book.type = :type"")"
69,@Entity
69,public class Book { ... }
69,Now the Metamodel Generator adds the following method declaration to the metamodel class Book_.
69,Generated Code
69,/**
69,* Execute named query {@value #QUERY_FIND_BY_TITLE_AND_TYPE} defined by annotation of {@link Book}.
69,**/
69,"public static List<Book> findByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
69,return entityManager.createNamedQuery(QUERY_FIND_BY_TITLE_AND_TYPE)
69,".setParameter(""titlePattern"", title)"
69,".setParameter(""type"", type)"
69,.getResultList();
69,"We can easily call this method from wherever we like, as long as we have access to an EntityManager:"
69,List<Book> books =
69,"Book_.findByTitleAndType(entityManager, titlePattern, Type.BOOK);"
69,"Now, this is quite nice, but it’s a bit inflexible in various ways, and so this probably isn’t the best way to generate a query method."
69,6.2. Generated query methods
69,The principal problem with generating the query method straight from the @NamedQuery annotation is that it doesn’t let us explicitly specify the return type or parameter list.
69,"In the case we just saw, the Metamodel Generator does a reasonable job of inferring the query return type and parameter types, but we’re often going to need a bit more control."
69,"The solution is to write down the signature of the query method explicitly, as an abstract method in Java."
69,"We’ll need a place to put this method, and since our Book entity isn’t an abstract class, we’ll just introduce a new interface for this purpose:"
69,interface Queries {
69,"@HQL(""where title like :title and type = :type"")"
69,"List<Book> findBooksByTitleAndType(String title, String type);"
69,"Instead of @NamedQuery, which is a type-level annotation, we specify the HQL query using the new @HQL annotation, which we place directly on the query method."
69,This results in the following generated code in the Queries_ class:
69,Generated Code
69,@StaticMetamodel(Queries.class)
69,public abstract class Queries_ {
69,/**
69,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
69,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
69,**/
69,"public static List<Book> findBooksByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
69,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
69,".setParameter(""title"", title)"
69,".setParameter(""type"", type)"
69,.getResultList();
69,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
69,"""where title like :title and type = :type"";"
69,Notice that the signature differs just slightly from the one we wrote down in the Queries interface: the Metamodel Generator has prepended a parameter accepting EntityManager to the parameter list.
69,"If we want to explicitly specify the name and type of this parameter, we may declare it explicitly:"
69,interface Queries {
69,"@HQL(""where title like :title and type = :type"")"
69,"List<Book> findBooksByTitleAndType(StatelessSession session, String title, String type);"
69,"The Metamodel Generator defaults to using EntityManager as the session type, but other types are allowed:"
69,"Session,"
69,"StatelessSession, or"
69,Mutiny.Session from Hibernate Reactive.
69,The real value of all this is in the checks which can now be done at compile time.
69,"The Metamodel Generator verifies that the parameters of our abstract method declaration match the parameters of the HQL query, for example:"
69,"for a named parameter :alice, there must be a method parameter named alice with exactly the same type, or"
69,"for an ordinal parameter ?2, the second method parameter must have exactly the same type."
69,"The query must also be syntactically legal and semantically well-typed, that is, the entities, attributes, and functions referenced in the query must actually exist and have compatible types."
69,The Metamodel Generator determines this by inspecting the annotations of the entity classes at compile time.
69,The @CheckHQL annotation which instructs Hibernate to validate named queries is not necessary for query methods annotated @HQL.
69,The @HQL annotation has a friend named @SQL which lets us specify a query written in native SQL instead of in HQL.
69,In this case there’s a lot less the Metamodel Generator can do to check that the query is legal and well-typed.
69,We imagine you’re wondering whether a static method is really the right thing to use here.
69,6.3. Generating query methods as instance methods
69,One thing not to like about what we’ve just seen is that we can’t transparently replace a generated static function of the Queries_ class with an improved handwritten implementation without impacting clients.
69,"Now, if our query is only called in one place, which is quite common, this isn’t going to be a big issue, and so we’re inclined to think the static function is fine."
69,"But if this function is called from many places, it’s probably better to promote it to an instance method of some class or interface."
69,"Fortunately, this is straightforward."
69,All we need to do is add an abstract getter method for the session object to our Queries interface.
69,(And remove the session from the method parameter list.)
69,We may call this method anything we like:
69,interface Queries {
69,EntityManager entityManager();
69,"@HQL(""where title like :title and type = :type"")"
69,"List<Book> findBooksByTitleAndType(String title, String type);"
69,"Here we’ve used EntityManager as the session type, but other types are allowed, as we saw above."
69,Now the Metamodel Generator does something a bit different:
69,Generated Code
69,@StaticMetamodel(Queries.class)
69,public class Queries_ implements Queries {
69,private final @Nonnull EntityManager entityManager;
69,public Queries_(@Nonnull EntityManager entityManager) {
69,this.entityManager = entityManager;
69,public @Nonnull EntityManager entityManager() {
69,return entityManager;
69,/**
69,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
69,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
69,**/
69,@Override
69,"public List<Book> findBooksByTitleAndType(String title, Type type) {"
69,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
69,".setParameter(""title"", title)"
69,".setParameter(""type"", type)"
69,.getResultList();
69,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
69,"""where title like :title and type = :type"";"
69,"The generated class Queries_ now implements the Queries interface, and the generated query method implements our abstract method directly."
69,"Of course, the protocol for calling the query method has to change:"
69,Queries queries = new Queries_(entityManager);
69,"List<Book> books = queries.findByTitleAndType(titlePattern, Type.BOOK);"
69,"If we ever need to swap out the generated query method with one we write by hand, without impacting clients, all we need to do is replace the abstract method with a default method of the Queries interface."
69,For example:
69,interface Queries {
69,EntityManager entityManager();
69,// handwritten method replacing previous generated implementation
69,"default List<Book> findBooksByTitleAndType(String title, String type) {"
69,entityManager()
69,".createQuery(""where title like :title and type = :type"", Book.class)"
69,".setParameter(""title"", title)"
69,".setParameter(""type"", type)"
69,.setFlushMode(COMMIT)
69,.setMaxResults(100)
69,.getResultList();
69,What if we would like to inject a Queries object instead of calling its constructor directly?
69,"As you recall, we don’t think these things really need to be container-managed objects."
69,"But if you want them to be—if you’re allergic to calling constructors, for some reason—then:"
69,"placing jakarta.inject on the build path will cause an @Inject annotation to be added to the constructor of Queries_, and"
69,placing jakarta.enterprise.context on the build path will cause a @Dependent annotation to be added to the Queries_ class.
69,"Thus, the generated implementation of Queries will be a perfectly functional CDI bean with no extra work to be done."
69,Is the Queries interface starting to look a lot like a DAO-style repository object?
69,"Well, perhaps."
69,You can certainly decide to use this facility to create a BookRepository if that’s what you prefer.
69,"But unlike a repository, our Queries interface:"
69,"doesn’t attempt to hide the EntityManager from its clients,"
69,"doesn’t implement or extend any framework-provided interface or abstract class, at least not unless you want to create such a framework yourself, and"
69,isn’t restricted to service a particular entity class.
69,We can have as many or as few interfaces with query methods as we like.
69,There’s no one-one-correspondence between these interfaces and entity types.
69,"This approach is so flexible that we don’t even really know what to call these ""interfaces with query methods""."
69,6.4. Generated finder methods
69,"At this point, one usually begins to question whether it’s even necessary to write a query at all."
69,Would it be possible to just infer the query from the method signature?
69,"In some simple cases it’s indeed possible, and this is the purpose of finder methods."
69,A finder method is a method annotated @Find.
69,For example:
69,@Find
69,Book getBook(String isbn);
69,A finder method may have multiple parameters:
69,@Find
69,"List<Book> getBooksByTitle(String title, Type type);"
69,The name of the finder method is arbitrary and carries no semantics.
69,But:
69,"the return type determines the entity class to be queried, and"
69,"the parameters of the method must match the fields of the entity class exactly, by both name and type."
69,"Considering our first example, Book has a persistent field String isbn, so this finder method is legal."
69,"If there were no field named isbn in Book, or if it had a different type, this method declaration would be rejected with a meaningful error at compile time."
69,"Similarly, the second example is legal, since Book has fields String title and Type type."
69,You might notice that our solution to this problem is very different from the approach taken by others.
69,"In DAO-style repository frameworks, you’re asked to encode the semantics of the finder method into the name of the method."
69,"This idea came to Java from Ruby, and we think it doesn’t belong here."
69,"It’s completely unnatural in Java, and by almost any measure other than counting characters it’s objectively worse than just writing the query in a string literal."
69,At least string literals accommodate whitespace and punctuation characters.
69,"Oh and, you know, it’s pretty useful to be able to rename a finder method without changing its semantics. 🙄"
69,The code generated for this finder method depends on what kind of fields match the method parameters:
69,@Id field
69,Uses EntityManager.find()
69,All @NaturalId fields
69,Uses Session.byNaturalId()
69,"Other persistent fields, or a mix of field types"
69,Uses a criteria query
69,"The generated code also depends on what kind of session we have, since the capabilities of stateless sessions, and of reactive sessions, differ slightly from the capabilities of regular stateful sessions."
69,"With EntityManager as the session type, we obtain:"
69,/**
69,* Find {@link Book} by {@link Book#isbn isbn}.
69,* @see org.example.Dao#getBook(String)
69,**/
69,@Override
69,public Book getBook(@Nonnull String isbn) {
69,"return entityManager.find(Book.class, isbn);"
69,/**
69,* Find {@link Book} by {@link Book#title title} and {@link Book#type type}.
69,"* @see org.example.Dao#getBooksByTitle(String,Type)"
69,**/
69,@Override
69,"public List<Book> getBooksByTitle(String title, Type type) {"
69,var builder = entityManager.getEntityManagerFactory().getCriteriaBuilder();
69,var query = builder.createQuery(Book.class);
69,var entity = query.from(Book.class);
69,query.where(
69,title==null
69,? entity.get(Book_.title).isNull()
69,": builder.equal(entity.get(Book_.title), title),"
69,type==null
69,? entity.get(Book_.type).isNull()
69,": builder.equal(entity.get(Book_.type), type)"
69,return entityManager.createQuery(query).getResultList();
69,It’s even possible to match a parameter of a finder method against a property of an associated entity or embeddable.
69,"The natural syntax would be a parameter declaration like String publisher.name, but because that’s not legal Java, we can write it as String publisher$name, taking advantage of a legal Java identifier character that nobody ever uses for anything else:"
69,@Find
69,List<Book> getBooksByPublisherName(String publisher$name);
69,"A finder method may specify fetch profiles, for example:"
69,@Find(namedFetchProfiles=Book_.FETCH_WITH_AUTHORS)
69,Book getBookWithAuthors(String isbn);
69,This lets us declare which associations of Book should be pre-fetched by annotating the Book class.
69,6.5. Paging and ordering
69,"Optionally, a query method may have additional ""magic"" parameters which do not map to query parameters:"
69,Parameter type
69,Purpose
69,Example argument
69,Page
69,Specifies a page of query results
69,Page.first(20)
69,Order<? super E>
69,"Specifies an entity attribute to order by, if E is the entity type returned by the query"
69,Order.asc(Book_.title)
69,List<Order? super E>
69,(or varargs)
69,"Specifies entity attributes to order by, if E is the entity type returned by the query"
69,"List.of(Order.asc(Book_.title), Order.asc(Book_.isbn))"
69,Order<Object[]>
69,"Specifies a column to order by, if the query returns a projection list"
69,Order.asc(1)
69,List<Object[]>
69,(or varargs)
69,"Specifies columns to order by, if the query returns a projection list"
69,"List.of(Order.asc(1), Order.desc(2))"
69,"Thus, if we redefine our earlier query method as follows:"
69,interface Queries {
69,"@HQL(""from Book where title like :title and type = :type"")"
69,"List<Book> findBooksByTitleAndType(String title, Page page, Order<? super Book>... order);"
69,Then we can call it like this:
69,List<Book> books =
69,"Queries_.findBooksByTitleAndType(entityManager, titlePattern, Type.BOOK,"
69,"Page.page(RESULTS_PER_PAGE, page), Order.asc(Book_.isbn));"
69,6.6. Query and finder method return types
69,A query method doesn’t need to return List.
69,It might return a single Book.
69,"@HQL(""where isbn = :isbn"")"
69,Book findBookByIsbn(String isbn);
69,"For a query with a projection list, Object[] or List<Object[]> is permitted:"
69,"@HQL(""select isbn, title from Book where isbn = :isbn"")"
69,Object[] findBookAttributesByIsbn(String isbn);
69,"But when there’s just one item in the select list, the type of that item should be used:"
69,"@HQL(""select title from Book where isbn = :isbn"")"
69,String getBookTitleByIsbn(String isbn);
69,"@HQL(""select local datetime"")"
69,LocalDateTime getServerDateTime();
69,"A query which returns a selection list may have a query method which repackages the result as a record, as we saw in Representing projection lists."
69,"record IsbnTitle(String isbn, String title) {}"
69,"@HQL(""select isbn, title from Book"")"
69,List<IsbnTitle> listIsbnAndTitleForEachBook(Page page);
69,A query method might even return TypedQuery or SelectionQuery:
69,"@HQL(""where title like :title"")"
69,SelectionQuery<Book> findBooksByTitle(String title);
69,"This is extremely useful at times, since it allows the client to further manipulate the query:"
69,List<Book> books =
69,"Queries_.findBooksByTitle(entityManager, titlePattern)"
69,.setOrder(Order.asc(Book_.title))
69,// order the results
69,".setPage(Page.page(RESULTS_PER_PAGE, page))"
69,// return the given page of results
69,.setFlushMode(FlushModeType.COMMIT)
69,// don't flush session before query execution
69,.setReadOnly(true)
69,// load the entities in read-only mode
69,.setCacheStoreMode(CacheStoreMode.BYPASS)
69,// don't cache the results
69,".setComment(""Hello world!"")"
69,// add a comment to the generated SQL
69,.getResultList();
69,"An insert, update, or delete query must return int or void."
69,"@HQL(""delete from Book"")"
69,int deleteAllBooks();
69,"@HQL(""update Book set discontinued = true where isbn = :isbn"")"
69,void discontinueBook(String isbn);
69,"On the other hand, finder methods are currently much more limited."
69,"A finder method must return an entity type like Book, or a list of the entity type, List<Book>, for example."
69,"As you might expect, for a reactive session, all query methods and finder methods must return Uni."
69,6.7. An alternative approach
69,"What if you just don’t like the ideas we’ve presented in this chapter, preferring to call the Session or EntityManager directly, but you still want compile-time validation for HQL?"
69,"Or what if you do like the ideas, but you’re working on a huge existing codebase full of code you don’t want to change?"
69,"Well, there’s a solution for you, too."
69,"The Query Validator is a separate annotation processor that’s capable of type-checking HQL strings, not only in annotations, but even when they occur as arguments to createQuery(), createSelectionQuery(), or createMutationQuery(). It’s even able to check calls to setParameter(), with some restrictions."
69,"The Query Validator works in javac, Gradle, Maven, and the Eclipse Java Compiler."
69,"Unlike the Metamodel Generator, which is a completely bog-standard Java annotation processor based on only standard Java APIs, the Query Validator makes use of internal compiler APIs in javac and ecj. This means it can’t be guaranteed to work in every Java compiler. The current release is known to work in JDK 11 and above, though JDK 15 or above is preferred."
69,7. Tuning and performance
69,Once you have a program up and running using Hibernate to access
69,"the database, it’s inevitable that you’ll find places where performance is"
69,disappointing or unacceptable.
69,"Fortunately, most performance problems are relatively easy to solve with"
69,"the tools that Hibernate makes available to you, as long as you keep a"
69,couple of simple principles in mind.
69,First and most important: the reason you’re using Hibernate is
69,"that it makes things easier. If, for a certain problem, it’s making"
69,"things harder, stop using it. Solve this problem with a different tool"
69,instead.
69,Just because you’re using Hibernate in your program doesn’t mean
69,you have to use it everywhere.
69,Second: there are two main potential sources of performance bottlenecks in
69,a program that uses Hibernate:
69,"too many round trips to the database, and"
69,memory consumption associated with the first-level (session) cache.
69,So performance tuning primarily involves reducing the number of accesses
69,"to the database, and/or controlling the size of the session cache."
69,"But before we get to those more advanced topics, we should start by tuning"
69,the connection pool.
69,7.1. Tuning the connection pool
69,"The connection pool built in to Hibernate is suitable for testing, but isn’t intended for use in production."
69,"Instead, Hibernate supports a range of different connection pools, including our favorite, Agroal."
69,"To select and configure Agroal, you’ll need to set some extra configuration properties, in addition to the settings we already saw in Basic configuration settings."
69,Properties with the prefix hibernate.agroal are passed through to Agroal:
69,# configure Agroal connection pool
69,hibernate.agroal.maxSize 20
69,hibernate.agroal.minSize 10
69,hibernate.agroal.acquisitionTimeout PT1s
69,hibernate.agroal.reapTimeout PT10s
69,"As long as you set at least one property with the prefix hibernate.agroal, the AgroalConnectionProvider will be selected automatically."
69,There’s many to choose from:
69,Table 42. Settings for configuring Agroal
69,Configuration property name
69,Purpose
69,hibernate.agroal.maxSize
69,The maximum number of connections present on the pool
69,hibernate.agroal.minSize
69,The minimum number of connections present on the pool
69,hibernate.agroal.initialSize
69,The number of connections added to the pool when it is started
69,hibernate.agroal.maxLifetime
69,"The maximum amount of time a connection can live, after which it is removed from the pool"
69,hibernate.agroal.acquisitionTimeout
69,"The maximum amount of time a thread can wait for a connection, after which an exception is thrown instead"
69,hibernate.agroal.reapTimeout
69,The duration for eviction of idle connections
69,hibernate.agroal.leakTimeout
69,The duration of time a connection can be held without causing a leak to be reported
69,hibernate.agroal.idleValidationTimeout
69,A foreground validation is executed if a connection has been idle on the pool for longer than this duration
69,hibernate.agroal.validationTimeout
69,The interval between background validation checks
69,hibernate.agroal.initialSql
69,A SQL command to be executed when a connection is created
69,The following settings are common to all connection pools supported by Hibernate:
69,Table 43. Common settings for connection pools
69,hibernate.connection.autocommit
69,The default autocommit mode
69,hibernate.connection.isolation
69,The default transaction isolation level
69,Container-managed datasources
69,"In a container environment, you usually don’t need to configure a connection pool through Hibernate."
69,"Instead, you’ll use a container-managed datasource, as we saw in Basic configuration settings."
69,7.2. Enabling statement batching
69,"An easy way to improve performance of some transactions, with almost no work at all, is to turn on automatic DML statement batching."
69,"Batching only helps in cases where a program executes many inserts, updates, or deletes against the same table in a single transaction."
69,All we need to do is set a single property:
69,Table 44. Enabling JDBC batching
69,Configuration property name
69,Purpose
69,Alternative
69,hibernate.jdbc.batch_size
69,Maximum batch size for SQL statement batching
69,setJdbcBatchSize()
69,"Even better than DML statement batching is the use of HQL update or delete queries, or even native SQL that calls a stored procedure!"
69,7.3. Association fetching
69,Achieving high performance in ORM means minimizing the number of round trips to the database. This goal should be uppermost in your mind whenever you’re writing data access code with Hibernate. The most fundamental rule of thumb in ORM is:
69,"explicitly specify all the data you’re going to need right at the start of a session/transaction, and fetch it immediately in one or two queries,"
69,and only then start navigating associations between persistent entities.
69,"Without question, the most common cause of poorly-performing data access code in Java programs is the problem of N+1 selects."
69,"Here, a list of N rows is retrieved from the database in an initial query, and then associated instances of a related entity are fetched using N subsequent queries."
69,This isn’t a bug or limitation of Hibernate; this problem even affects typical handwritten JDBC code behind DAOs.
69,"Only you, the developer, can solve this problem, because only you know ahead of time what data you’re going to need in a given unit of work."
69,But that’s OK.
69,Hibernate gives you all the tools you need.
69,"In this section we’re going to discuss different ways to avoid such ""chatty"" interaction with the database."
69,Hibernate provides several strategies for efficiently fetching associations and avoiding N+1 selects:
69,"outer join fetching—where an association is fetched using a left outer join,"
69,"batch fetching—where an association is fetched using a subsequent select with a batch of primary keys, and"
69,subselect fetching—where an association is fetched using a subsequent select with keys re-queried in a subselect.
69,"Of these, you should almost always use outer join fetching."
69,But let’s consider the alternatives first.
69,7.4. Batch fetching and subselect fetching
69,Consider the following code:
69,List<Book> books =
69,"session.createSelectionQuery(""from Book order by isbn"", Book.class)"
69,.getResultList();
69,"books.forEach(book -> book.getAuthors().forEach(author -> out.println(book.title + "" by "" + author.name)));"
69,"This code is very inefficient, resulting, by default, in the execution of N+1 select statements, where n is the number of Books."
69,Let’s see how we can improve on that.
69,SQL for batch fetching
69,"With batch fetching enabled, Hibernate might execute the following SQL on PostgreSQL:"
69,/* initial query for Books */
69,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
69,from Book b1_0
69,order by b1_0.isbn
69,/* first batch of associated Authors */
69,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
69,from Book_Author a1_0
69,join Author a1_1 on a1_1.id=a1_0.authors_id
69,where a1_0.books_isbn = any (?)
69,/* second batch of associated Authors */
69,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
69,from Book_Author a1_0
69,join Author a1_1 on a1_1.id=a1_0.authors_id
69,where a1_0.books_isbn = any (?)
69,The first select statement queries and retrieves Books.
69,The second and third queries fetch the associated Authors in batches.
69,The number of batches required depends on the configured batch size.
69,"Here, two batches were required, so two SQL statements were executed."
69,The SQL for batch fetching looks slightly different depending on the database.
69,"Here, on PostgreSQL, Hibernate passes a batch of primary key values as a SQL ARRAY."
69,SQL for subselect fetching
69,"On the other hand, with subselect fetching, Hibernate would execute this SQL:"
69,/* initial query for Books */
69,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
69,from Book b1_0
69,order by b1_0.isbn
69,/* fetch all associated Authors */
69,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
69,from Book_Author a1_0
69,join Author a1_1 on a1_1.id=a1_0.authors_id
69,where a1_0.books_isbn in (select b1_0.isbn from Book b1_0)
69,Notice that the first query is re-executed in a subselect in the second query.
69,"The execution of the subselect is likely to be relatively inexpensive, since the data should already be cached by the database."
69,"Clever, huh?"
69,Enabling the use of batch or subselect fetching
69,"Both batch fetching and subselect fetching are disabled by default, but we may enable one or the other globally using properties."
69,Table 45. Configuration settings to enable batch and subselect fetching
69,Configuration property name
69,Property value
69,Alternatives
69,hibernate.default_batch_fetch_size
69,A sensible batch size >1 to enable batch fetching
69,"@BatchSize(), setFetchBatchSize()"
69,hibernate.use_subselect_fetch
69,true to enable subselect fetching
69,"@Fetch(SUBSELECT), setSubselectFetchingEnabled()"
69,"Alternatively, we can enable one or the other in a given session:"
69,session.setFetchBatchSize(5);
69,session.setSubselectFetchingEnabled(true);
69,We may request subselect fetching more selectively by annotating a collection or many-valued association with the @Fetch annotation.
69,@ManyToMany @Fetch(SUBSELECT)
69,Set<Author> authors;
69,"Note that @Fetch(SUBSELECT) has the same effect as @Fetch(SELECT), except after execution of a HQL or criteria query."
69,"But after query execution, @Fetch(SUBSELECT) is able to much more efficiently fetch associations."
69,"Later, we’ll see how we can use fetch profiles to do this even more selectively."
69,That’s all there is to it.
69,"Too easy, right?"
69,"Sadly, that’s not the end of the story."
69,"While batch fetching might mitigate problems involving N+1 selects, it won’t solve them."
69,The truly correct solution is to fetch associations using joins.
69,Batch fetching (or subselect fetching) can only be the best solution in rare cases where outer join fetching would result in a cartesian product and a huge result set.
69,But batch fetching and subselect fetching have one important characteristic in common: they can be performed lazily.
69,"This is, in principle, pretty convenient."
69,"When we query data, and then navigate an object graph, lazy fetching saves us the effort of planning ahead."
69,It turns out that this is a convenience we’re going to have to surrender.
69,7.5. Join fetching
69,"Outer join fetching is usually the best way to fetch associations, and it’s what we use most of the time."
69,"Unfortunately, by its very nature, join fetching simply can’t be lazy."
69,"So to make use of join fetching, we must plan ahead."
69,Our general advice is:
69,"Avoid the use of lazy fetching, which is often the source of N+1 selects."
69,"Now, we’re not saying that associations should be mapped for eager fetching by default!"
69,"That would be a terrible idea, resulting in simple session operations that fetch almost the entire database."
69,Therefore:
69,Most associations should be mapped for lazy fetching by default.
69,"It sounds as if this tip is in contradiction to the previous one, but it’s not."
69,It’s saying that you must explicitly specify eager fetching for associations precisely when and where they are needed.
69,"If we need eager join fetching in some particular transaction, we have four different ways to specify that."
69,Passing a JPA EntityGraph
69,We’ve already seen this in Entity graphs and eager fetching
69,Specifying a named fetch profile
69,We’ll discuss this approach later in Named fetch profiles
69,Using left join fetch in HQL/JPQL
69,See A Guide to Hibernate Query Language for details
69,Using From.fetch() in a criteria query
69,Same semantics as join fetch in HQL
69,"Typically, a query is the most convenient option."
69,Here’s how we can ask for join fetching in HQL:
69,List<Book> booksWithJoinFetchedAuthors =
69,"session.createSelectionQuery(""from Book join fetch authors order by isbn"")"
69,.getResultList();
69,"And this is the same query, written using the criteria API:"
69,var builder = sessionFactory.getCriteriaBuilder();
69,var query = builder.createQuery(Book.class);
69,var book = query.from(Book.class);
69,book.fetch(Book_.authors);
69,query.select(book);
69,query.orderBy(builder.asc(book.get(Book_.isbn)));
69,List<Book> booksWithJoinFetchedAuthors =
69,session.createSelectionQuery(query).getResultList();
69,"Either way, a single SQL select statement is executed:"
69,"select b1_0.isbn,a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
69,from Book b1_0
69,join (Book_Author a1_0 join Author a1_1 on a1_1.id=a1_0.authors_id)
69,on b1_0.isbn=a1_0.books_isbn
69,order by b1_0.isbn
69,Much better!
69,"Join fetching, despite its non-lazy nature, is clearly more efficient than either batch or subselect fetching, and this is the source of our recommendation to avoid the use of lazy fetching."
69,There’s one interesting case where join fetching becomes inefficient: when we fetch two many-valued associations in parallel.
69,Imagine we wanted to fetch both Author.books and Author.royaltyStatements in some unit of work.
69,"Joining both collections in a single query would result in a cartesian product of tables, and a large SQL result set."
69,"Subselect fetching comes to the rescue here, allowing us to fetch books using a join, and royaltyStatements using a single subsequent select."
69,"Of course, an alternative way to avoid many round trips to the database is to cache the data we need in the Java client."
69,"If we’re expecting to find the associated data in a local cache, we probably don’t need join fetching at all."
69,But what if we can’t be certain that all associated data will be in the cache?
69,"In that case, we might be able to reduce the cost of cache misses by enabling batch fetching."
69,7.6. The second-level cache
69,"A classic way to reduce the number of accesses to the database is to use a second-level cache, allowing"
69,data cached in memory to be shared between sessions.
69,"By nature, a second-level cache tends to undermine the ACID properties of transaction processing in a relational database."
69,We don’t use a distributed transaction with two-phase commit to ensure that changes to the cache and database happen atomically.
69,"So a second-level cache is often by far the easiest way to improve the performance of a system, but only at the cost of making it much more difficult to reason about concurrency."
69,And so the cache is a potential source of bugs which are difficult to isolate and reproduce.
69,"Therefore, by default, an entity is not eligible for storage in the second-level cache."
69,We must explicitly mark each entity that will
69,be stored in the second-level cache with the @Cache annotation from org.hibernate.annotations.
69,But that’s still not enough.
69,"Hibernate does not itself contain an implementation of a second-level cache, so it’s necessary to configure an external cache provider."
69,Caching is disabled by default.
69,"To minimize the risk of data loss, we force you to stop and think before any entity goes into the cache."
69,"Hibernate segments the second-level cache into named regions, one for each:"
69,mapped entity hierarchy or
69,collection role.
69,"For example, there might be separate cache regions for Author, Book, Author.books, and Book.authors."
69,"Each region is permitted its own policies for expiry, persistence, and replication. These policies must be configured externally to Hibernate."
69,"The appropriate policies depend on the kind of data an entity represents. For example, a program might have different caching policies for ""reference"" data, for transactional data, and for data used for analytics. Ordinarily, the implementation of those policies is the responsibility of the underlying cache implementation."
69,7.7. Specifying which data is cached
69,"By default, no data is eligible for storage in the second-level cache."
69,An entity hierarchy or collection role may be assigned a region using the @Cache annotation.
69,"If no region name is explicitly specified, the region name is just the name of the entity class or collection role."
69,@Entity
69,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
69,class Publisher {
69,...
69,"@Cache(usage=READ_WRITE, region=""PublishedBooks"")"
69,@OneToMany(mappedBy=Book_.PUBLISHER)
69,Set<Book> books;
69,...
69,The cache defined by a @Cache annotation is automatically utilized by Hibernate to:
69,"retrieve an entity by id when find() is called, or"
69,to resolve an association by id.
69,The @Cache annotation must be specified on the root class of an entity inheritance hierarchy.
69,It’s an error to place it on a subclass entity.
69,"The @Cache annotation always specifies a CacheConcurrencyStrategy, a policy governing access to the second-level cache by concurrent transactions."
69,Table 46. Cache concurrency
69,Concurrency policy
69,Interpretation
69,Explanation
69,READ_ONLY
69,Immutable data
69,Read-only access
69,"Indicates that the cached object is immutable, and is never updated. If an entity with this cache concurrency is updated, an exception is thrown."
69,"This is the simplest, safest, and best-performing cache concurrency strategy. It’s particularly suitable for so-called ""reference"" data."
69,NONSTRICT_READ_WRITE
69,Concurrent updates are extremely improbable
69,Read/write access with no locking
69,"Indicates that the cached object is sometimes updated, but that it’s extremely unlikely that two transactions will attempt to update the same item of data at the same time."
69,"This strategy does not use locks. When an item is updated, the cache is invalidated both before and after completion of the updating transaction. But without locking, it’s impossible to completely rule out the possibility of a second transaction storing or retrieving stale data in or from the cache during the completion process of the first transaction."
69,READ_WRITE
69,Concurrent updates are possible but not common
69,Read/write access using soft locks
69,Indicates a non-vanishing likelihood that two concurrent transactions attempt to update the same item of data simultaneously.
69,"This strategy uses ""soft"" locks to prevent concurrent transactions from retrieving or storing a stale item from or in the cache during the transaction completion process. A soft lock is simply a marker entry placed in the cache while the updating transaction completes."
69,"A second transaction may not read the item from the cache while the soft lock is present, and instead simply proceeds to read the item directly from the database, exactly as if a regular cache miss had occurred."
69,"Similarly, the soft lock also prevents this second transaction from storing a stale item to the cache when it returns from its round trip to the database with something that might not quite be the latest version."
69,TRANSACTIONAL
69,Concurrent updates are frequent
69,Transactional access
69,"Indicates that concurrent writes are common, and the only way to maintain synchronization between the second-level cache and the database is via the use of a fully transactional cache provider. In this case, the cache and the database must cooperate via JTA or the XA protocol, and Hibernate itself takes on little responsibility for maintaining the integrity of the cache."
69,Which policies make sense may also depend on the underlying second-level cache implementation.
69,"JPA has a similar annotation, named @Cacheable."
69,"Unfortunately, it’s almost useless to us, since:"
69,"it provides no way to specify any information about the nature of the cached entity and how its cache should be managed, and"
69,"it may not be used to annotate associations, and so we can’t even use it to mark collection roles as eligible for storage in the second-level cache."
69,7.8. Caching by natural id
69,"If our entity has a natural id, we can enable an additional cache, which holds cross-references from natural id to primary id, by annotating the entity @NaturalIdCache."
69,"By default, the natural id cache is stored in a dedicated region of the second-level cache, separate from the cached entity data."
69,@Entity
69,"@Cache(usage=READ_WRITE, region=""Book"")"
69,"@NaturalIdCache(region=""BookIsbn"")"
69,class Book {
69,...
69,@NaturalId
69,String isbn;
69,@NaturalId
69,int printing;
69,...
69,This cache is utilized when the entity is retrieved using one of the operations of Session which performs lookup by natural id.
69,"Since the natural id cache doesn’t contain the actual state of the entity, it doesn’t make sense to annotate an entity @NaturalIdCache unless it’s already eligible for storage in the second-level cache, that is, unless it’s also annotated @Cache."
69,"It’s worth noticing that, unlike the primary identifier of an entity, a natural id might be mutable."
69,"We must now consider a subtlety that often arises when we have to deal with so-called ""reference data"", that is, data which fits easily in memory, and doesn’t change much."
69,7.9. Caching and association fetching
69,Let’s consider again our Publisher class:
69,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
69,@Entity
69,class Publisher { ... }
69,"Data about publishers doesn’t change very often, and there aren’t so many of them."
69,Suppose we’ve set everything up so that the publishers are almost always available in the second-level cache.
69,Then in this case we need to think carefully about associations of type Publisher.
69,@ManyToOne
69,Publisher publisher;
69,"There’s no need for this association to be lazily fetched, since we’re expecting it to be available in memory, so we won’t set it fetch=LAZY."
69,"But on the other hand, if we leave it marked for eager fetching then, by default, Hibernate will often fetch it using a join."
69,This places completely unnecessary load on the database.
69,The solution is the @Fetch annotation:
69,@ManyToOne @Fetch(SELECT)
69,Publisher publisher;
69,"By annotating the association @Fetch(SELECT), we suppress join fetching, giving Hibernate a chance to find the associated Publisher in the cache."
69,"Therefore, we arrive at this rule of thumb:"
69,"Many-to-one associations to ""reference data"", or to any other data that will almost always be available in the cache, should be mapped EAGER,SELECT."
69,"Other associations, as we’ve already made clear, should be LAZY."
69,"Once we’ve marked an entity or collection as eligible for storage in the second-level cache, we still need to set up an actual cache."
69,7.10. Configuring the second-level cache provider
69,"Configuring a second-level cache provider is a rather involved topic, and quite outside the scope of this document."
69,"But in case it helps, we often test Hibernate with the following configuration, which uses EHCache as the cache implementation, as above in Optional dependencies:"
69,Table 47. EHCache configuration
69,Configuration property name
69,Property value
69,hibernate.cache.region.factory_class
69,jcache
69,hibernate.javax.cache.uri
69,/ehcache.xml
69,"If you’re using EHCache, you’ll also need to include an ehcache.xml file"
69,that explicitly configures the behavior of each cache region belonging to
69,your entities and collections.
69,You’ll find more information about configuring EHCache here.
69,"We may use any other implementation of JCache, such as Caffeine."
69,JCache automatically selects whichever implementation it finds on the classpath.
69,"If there are multiple implementations on the classpath, we must disambiguate using:"
69,Table 48. Disambiguating the JCache implementation
69,Configuration property name
69,Property value
69,hibernate.javax.cache.provider
69,"The implementation of javax.cache.spiCachingProvider, for example:"
69,org.ehcache.jsr107.EhcacheCachingProvider
69,for EHCache
69,com.github.benmanes.caffeine.jcache.spi.CaffeineCachingProvider
69,for Caffeine
69,"Alternatively, to use Infinispan as the cache implementation, the following settings are required:"
69,Table 49. Infinispan provider configuration
69,Configuration property name
69,Property value
69,hibernate.cache.region.factory_class
69,infinispan
69,hibernate.cache.infinispan.cfg
69,"Path to infinispan configuration file, for example:"
69,org/infinispan/hibernate/cache/commons/builder/infinispan-configs.xml
69,for a distributed cache
69,org/infinispan/hibernate/cache/commons/builder/infinispan-configs-local.xml
69,to test with local cache
69,Infinispan is usually used when distributed caching is required.
69,There’s more about using Infinispan with Hibernate here.
69,"Finally, there’s a way to globally disable the second-level cache:"
69,Table 50. Setting to disable caching
69,Configuration property name
69,Property value
69,hibernate.cache.use_second_level_cache
69,"true to enable caching, or false to disable it"
69,"When hibernate.cache.region.factory_class is set, this property defaults to true."
69,This setting lets us easily disable the second-level cache completely when troubleshooting or profiling performance.
69,You can find much more information about the second-level cache in the User Guide.
69,7.11. Caching query result sets
69,The caches we’ve described above are only used to optimize lookups by id or by natural id.
69,"Hibernate also has a way to cache the result sets of queries, though this is only rarely an efficient thing to do."
69,The query cache must be enabled explicitly:
69,Table 51. Setting to enable the query cache
69,Configuration property name
69,Property value
69,hibernate.cache.use_query_cache
69,true to enable the query cache
69,"To cache the results of a query, call SelectionQuery.setCacheable(true):"
69,"session.createQuery(""from Product where discontinued = false"")"
69,.setCacheable(true)
69,.getResultList();
69,"By default, the query result set is stored in a cache region named default-query-results-region."
69,"Since different queries should have different caching policies, it’s common to explicitly specify a region name:"
69,"session.createQuery(""from Product where discontinued = false"")"
69,.setCacheable(true)
69,".setCacheRegion(""ProductCatalog"")"
69,.getResultList();
69,A result set is cached together with a logical timestamp.
69,"By ""logical"", we mean that it doesn’t actually increase linearly with time, and in particular it’s not the system time."
69,"When a Product is updated, Hibernate does not go through the query cache and invalidate every cached result set that’s affected by the change."
69,"Instead, there’s a special region of the cache which holds a logical timestamp of the most-recent update to each table."
69,"This is called the update timestamps cache, and it’s kept in the region default-update-timestamps-region."
69,It’s your responsibility to ensure that this cache region is configured with appropriate policies.
69,"In particular, update timestamps should never expire or be evicted."
69,"When a query result set is read from the cache, Hibernate compares its timestamp with the timestamp of each of the tables that affect the results of the query, and only returns the result set if the result set isn’t stale."
69,"If the result set is stale, Hibernate goes ahead and re-executes the query against the database and updates the cached result set."
69,"As is generally the case with any second-level cache, the query cache can break the ACID properties of transactions."
69,7.12. Second-level cache management
69,"For the most part, the second-level cache is transparent."
69,"Program logic which interacts with the Hibernate session is unaware of the cache, and is not impacted by changes to caching policies."
69,"At worst, interaction with the cache may be controlled by specifying of an explicit CacheMode:"
69,session.setCacheMode(CacheMode.IGNORE);
69,"Or, using JPA-standard APIs:"
69,entityManager.setCacheRetrieveMode(CacheRetrieveMode.BYPASS);
69,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
69,The JPA-defined cache modes come in two flavors: CacheRetrieveMode and CacheStoreMode.
69,Table 52. JPA-defined cache retrieval modes
69,Mode
69,Interpretation
69,CacheRetrieveMode.USE
69,Read data from the cache if available
69,CacheRetrieveMode.BYPASS
69,Don’t read data from the cache; go direct to the database
69,We might select CacheRetrieveMode.BYPASS if we’re concerned about the possibility of reading stale data from the cache.
69,Table 53. JPA-defined cache storage modes
69,Mode
69,Interpretation
69,CacheStoreMode.USE
69,Write data to the cache when read from the database or when modified; do not update already-cached items when reading
69,CacheStoreMode.REFRESH
69,Write data to the cache when read from the database or when modified; always update cached items when reading
69,CacheStoreMode.BYPASS
69,Don’t write data to the cache
69,We should select CacheStoreMode.BYPASS if we’re querying data that doesn’t need to be cached.
69,It’s a good idea to set the CacheStoreMode to BYPASS just before running a query which returns a large result set full of data that we don’t expect to need again soon.
69,"This saves work, and prevents the newly-read data from pushing out the previously cached data."
69,In JPA we would use this idiom:
69,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
69,List<Publisher> allpubs =
69,"entityManager.createQuery(""from Publisher"", Publisher.class)"
69,.getResultList();
69,entityManager.setCacheStoreMode(CacheStoreMode.USE);
69,But Hibernate has a better way:
69,List<Publisher> allpubs =
69,"session.createSelectionQuery(""from Publisher"", Publisher.class)"
69,.setCacheStoreMode(CacheStoreMode.BYPASS)
69,.getResultList();
69,A Hibernate CacheMode packages a CacheRetrieveMode with a CacheStoreMode.
69,Table 54. Hibernate cache modes and JPA equivalents
69,Hibernate CacheMode
69,Equivalent JPA modes
69,NORMAL
69,"CacheRetrieveMode.USE, CacheStoreMode.USE"
69,IGNORE
69,"CacheRetrieveMode.BYPASS, CacheStoreMode.BYPASS"
69,GET
69,"CacheRetrieveMode.USE, CacheStoreMode.BYPASS"
69,PUT
69,"CacheRetrieveMode.BYPASS, CacheStoreMode.USE"
69,REFRESH
69,"CacheRetrieveMode.REFRESH, CacheStoreMode.BYPASS"
69,There’s no particular reason to prefer Hibernate’s CacheMode over the JPA equivalents.
69,This enumeration only exists because Hibernate had cache modes long before they were added to JPA.
69,"For ""reference"" data, that is, for data which is expected to always be found in the second-level cache, it’s a good idea to prime the cache at startup."
69,There’s a really easy way to do this: just execute a query immediately after obtaining the
69,EntityManager or SessionFactory.
69,SessionFactory sessionFactory =
69,setupHibernate(new Configuration())
69,.buildSessionFactory();
69,// prime the second-level cache
69,sessionFactory.inSession(session -> {
69,"session.createSelectionQuery(""from Country""))"
69,.setReadOnly(true)
69,.getResultList();
69,"session.createSelectionQuery(""from Product where discontinued = false""))"
69,.setReadOnly(true)
69,.getResultList();
69,});
69,"Very occasionally, it’s necessary or advantageous to control the cache explicitly, for example, to evict some data that we know to be stale."
69,The Cache interface allows programmatic eviction of cached items.
69,"sessionFactory.getCache().evictEntityData(Book.class, bookId);"
69,Second-level cache management via the Cache interface is not transaction-aware.
69,"None of the operations of Cache respect any isolation or transactional semantics associated with the underlying caches. In particular, eviction via the methods of this interface causes an immediate ""hard"" removal outside any current transaction and/or locking scheme."
69,"Ordinarily, however, Hibernate automatically evicts or updates cached data after modifications, and, in addition, cached data which is unused will eventually be expired according to the configured policies."
69,This is quite different to what happens with the first-level cache.
69,7.13. Session cache management
69,Entity instances aren’t automatically evicted from the session cache when they’re no longer needed.
69,"Instead, they stay pinned in memory until the session they belong to is discarded by your program."
69,"The methods detach() and clear() allow you to remove entities from the session cache, making them available for garbage collection."
69,"Since most sessions are rather short-lived, you won’t need these operations very often."
69,"And if you find yourself thinking you do need them in a certain situation, you should strongly consider an alternative solution: a stateless session."
69,7.14. Stateless sessions
69,"An arguably-underappreciated feature of Hibernate is the StatelessSession interface, which provides a command-oriented, more bare-metal approach to interacting with the database."
69,You may obtain a stateless session from the SessionFactory:
69,StatelessSession ss = getSessionFactory().openStatelessSession();
69,A stateless session:
69,"doesn’t have a first-level cache (persistence context), nor does it interact with any second-level caches, and"
69,"doesn’t implement transactional write-behind or automatic dirty checking, so all operations are executed immediately when they’re explicitly called."
69,"For a stateless session, we’re always working with detached objects."
69,"Thus, the programming model is a bit different:"
69,Table 55. Important methods of the StatelessSession
69,Method name and parameters
69,Effect
69,"get(Class, Object)"
69,"Obtain a detached object, given its type and its id, by executing a select"
69,fetch(Object)
69,Fetch an association of a detached object
69,refresh(Object)
69,Refresh the state of a detached object by executing
69,a select
69,insert(Object)
69,Immediately insert the state of the given transient object into the database
69,update(Object)
69,Immediately update the state of the given detached object in the database
69,delete(Object)
69,Immediately delete the state of the given detached object from the database
69,upsert(Object)
69,Immediately insert or update the state of the given detached object using a SQL merge into statement
69,"There’s no flush() operation, and so update() is always explicit."
69,"In certain circumstances, this makes stateless sessions easier to work with, but with the caveat that a stateless session is much more vulnerable to data aliasing effects, since it’s easy to get two non-identical Java objects which both represent the same row of a database table."
69,"If we use fetch() in a stateless session, we can very easily obtain two objects representing the same database row!"
69,"In particular, the absence of a persistence context means that we can safely perform bulk-processing tasks without allocating huge quantities of memory."
69,Use of a StatelessSession alleviates the need to call:
69,"clear() or detach() to perform first-level cache management, and"
69,setCacheMode() to bypass interaction with the second-level cache.
69,"Stateless sessions can be useful, but for bulk operations on huge datasets, Hibernate can’t possibly compete with stored procedures!"
69,"When using a stateless session, you should be aware of the following additional limitations:"
69,"persistence operations never cascade to associated instances,"
69,"changes to @ManyToMany associations and @ElementCollections cannot be made persistent, and"
69,operations performed via a stateless session bypass callbacks.
69,7.15. Optimistic and pessimistic locking
69,"Finally, an aspect of behavior under load that we didn’t mention above is row-level data contention."
69,"When many transactions try to read and update the same data, the program might become unresponsive with lock escalation, deadlocks, and lock acquisition timeout errors."
69,There’s two basic approaches to data concurrency in Hibernate:
69,"optimistic locking using @Version columns, and"
69,database-level pessimistic locking using the SQL for update syntax (or equivalent).
69,"In the Hibernate community it’s much more common to use optimistic locking, and Hibernate makes that incredibly easy."
69,"Where possible, in a multiuser system, avoid holding a pessimistic lock across a user interaction."
69,"Indeed, the usual practice is to avoid having transactions that span user interactions. For multiuser systems, optimistic locking is king."
69,"That said, there is also a place for pessimistic locks, which can sometimes reduce the probability of transaction rollbacks."
69,"Therefore, the find(), lock(), and refresh() methods of the reactive session accept an optional LockMode."
69,We can also specify a LockMode for a query.
69,"The lock mode can be used to request a pessimistic lock, or to customize the behavior of optimistic locking:"
69,Table 56. Optimistic and pessimistic lock modes
69,LockMode type
69,Meaning
69,READ
69,An optimistic lock obtained implicitly whenever
69,an entity is read from the database using select
69,OPTIMISTIC
69,An optimistic lock obtained when an entity is
69,"read from the database, and verified using a"
69,select to check the version when the
69,transaction completes
69,OPTIMISTIC_FORCE_INCREMENT
69,An optimistic lock obtained when an entity is
69,"read from the database, and enforced using an"
69,update to increment the version when the
69,transaction completes
69,WRITE
69,A pessimistic lock obtained implicitly whenever
69,an entity is written to the database using
69,update or insert
69,PESSIMISTIC_READ
69,A pessimistic for share lock
69,PESSIMISTIC_WRITE
69,A pessimistic for update lock
69,PESSIMISTIC_FORCE_INCREMENT
69,A pessimistic lock enforced using an immediate
69,update to increment the version
69,7.16. Collecting statistics
69,We may ask Hibernate to collect statistics about its activity by setting this configuration property:
69,Configuration property name
69,Property value
69,hibernate.generate_statistics
69,true to enable collection of statistics
69,The statistics are exposed by the Statistics object:
69,long failedVersionChecks =
69,sessionFactory.getStatistics()
69,.getOptimisticFailureCount();
69,long publisherCacheMissCount =
69,sessionFactory.getStatistics()
69,.getEntityStatistics(Publisher.class.getName())
69,.getCacheMissCount()
69,Hibernate’s statistics enable observability.
69,Both Micrometer and SmallRye Metrics are capable of exposing these metrics.
69,7.17. Tracking down slow queries
69,"When a poorly-performing SQL query is discovered in production, it can sometimes be hard to track down exactly where in the Java code the query originates."
69,Hibernate offers two configuration properties that can make it easier to identify a slow query and find its source.
69,Table 57. Settings for tracking slow queries
69,Configuration property name
69,Purpose
69,Property value
69,hibernate.log_slow_query
69,Log slow queries at the INFO level
69,"The minimum execution time, in milliseconds, which characterizes a ""slow"" query"
69,hibernate.use_sql_comments
69,Prepend comments to the executed SQL
69,true or false
69,"When hibernate.use_sql_comments is enabled, the text of the HQL query is prepended as a comment to the generated SQL, which usually makes it easy to find the HQL in the Java code."
69,The comment text may be customized:
69,"by calling Query.setComment(comment) or Query.setHint(AvailableHints.HINT_COMMENT,comment), or"
69,via the @NamedQuery annotation.
69,"Once you’ve identified a slow query, one of the best ways to make it faster is to actually go and talk to someone who is an expert at making queries go fast."
69,"These people are called ""database administrators"", and if you’re reading this document you probably aren’t one."
69,Database administrators know lots of stuff that Java developers don’t.
69,"So if you’re lucky enough to have a DBA about, you don’t need to Dunning-Kruger your way out of a slow query."
69,An expertly-defined index might be all you need to fix a slow query.
69,7.18. Adding indexes
69,The @Index annotation may be used to add an index to a table:
69,@Entity
69,"@Table(indexes=@Index(columnList=""title, year, publisher_id""))"
69,class Book { ... }
69,"It’s even possible to specify an ordering for an indexed column, or that the index should be case-insensitive:"
69,@Entity
69,"@Table(indexes=@Index(columnList=""(lower(title)), year desc, publisher_id""))"
69,class Book { ... }
69,This lets us create a customized index for a particular query.
69,Note that SQL expressions like lower(title) must be enclosed in parentheses in the columnList of the index definition.
69,It’s not clear that information about indexes belongs in annotations of Java code.
69,"Indexes are usually maintained and modified by a database administrator, ideally by an expert in tuning the performance of one particular RDBMS."
69,So it might be better to keep the definition of indexes in a SQL DDL script that your DBA can easily read and modify.
69,"Remember, we can ask Hibernate to execute a DDL script using the property javax.persistence.schema-generation.create-script-source."
69,7.19. Dealing with denormalized data
69,"A typical relational database table in a well-normalized schema has a relatively small number of columns, and so there’s little to be gained by selectively querying columns and populating only certain fields of an entity class."
69,"But occasionally, we hear from someone asking how to map a table with a hundred columns or more!"
69,This situation can arise when:
69,"data is intentionally denormalized for performance,"
69,"the results of a complicated analytic query are exposed via a view, or"
69,someone has done something crazy and wrong.
69,Let’s suppose that we’re not dealing with the last possibility.
69,Then we would like to be able to query the monster table without returning all of its columns.
69,"At first glance, Hibernate doesn’t offer a perfect bottled solution to this problem."
69,This first impression is misleading.
69,"Actually, Hibernate features more than one way to deal with this situation, and the real problem is deciding between the ways."
69,We could:
69,"map multiple entity classes to the same table or view, being careful about ""overlaps"" where a mutable column is mapped to more than one of the entities,"
69,"use HQL or native SQL queries returning results into record types instead of retrieving entity instances, or"
69,use the bytecode enhancer and @LazyGroup for attribute-level lazy fetching.
69,"Some other ORM solutions push the third option as the recommended way to handle huge tables, but this has never been the preference of the Hibernate team or Hibernate community."
69,It’s much more typesafe to use one of the first two options.
69,7.20. Reactive programming with Hibernate
69,"Finally, many systems which require high scalability now make use of reactive programming and reactive streams."
69,Hibernate Reactive brings O/R mapping to the world of reactive programming.
69,You can learn much more about Hibernate Reactive from its Reference Documentation.
69,"Hibernate Reactive may be used alongside vanilla Hibernate in the same program, and can reuse the same entity classes."
69,This means you can use the reactive programming model exactly where you need it—perhaps only in one or two places in your system.
69,You don’t need to rewrite your whole program using reactive streams.
69,8. Advanced Topics
69,"In the last chapter of this Introduction, we turn to some topics that don’t really belong in an introduction."
69,"Here we consider some problems, and solutions, that you’re probably not going to run into immediately if you’re new to Hibernate."
69,"But we do want you to know about them, so that when the time comes, you’ll know what tool to reach for."
69,8.1. Filters
69,"Filters are one of the nicest and under-usedest features of Hibernate, and we’re quite proud of them."
69,"A filter is a named, globally-defined, parameterized restriction on the data that is visible in a given session."
69,Examples of well-defined filters might include:
69,"a filter that restricts the data visible to a given user according to row-level permissions,"
69,"a filter which hides data which has been soft-deleted,"
69,"in a versioned database, a filter that displays versions which were current at a given instant in the past, or"
69,a filter that restricts to data associated with a certain geographical region.
69,A filter must be declared somewhere.
69,A package descriptor is as good a place as any for a @FilterDef:
69,"@FilterDef(name = ""ByRegion"","
69,"parameters = @ParamDef(name = ""region"", type = String.class))"
69,package org.hibernate.example;
69,This filter has one parameter.
69,"Fancier filters might in principle have multiple parameters, though we admit this must be quite rare."
69,"If you add annotations to a package descriptor, and you’re using Configuration to configure Hibernate, make sure you call Configuration.addPackage() to let Hibernate know that the package descriptor is annotated."
69,"Typically, but not necessarily, a @FilterDef specifies a default restriction:"
69,"@FilterDef(name = ""ByRegion"","
69,"parameters = @ParamDef(name = ""region"", type = String.class),"
69,"defaultCondition = ""region = :region"")"
69,package org.hibernate.example;
69,"The restriction must contain a reference to the parameter of the filter, specified using the usual syntax for named parameters."
69,Any entity or collection which is affected by a filter must be annotated @Filter:
69,@Entity
69,@Filter(name = example_.BY_REGION)
69,class User {
69,@Id String username;
69,String region;
69,...
69,"Here, as usual, example_.BY_REGION is generated by the Metamodel Generator, and is just a constant with the value ""ByRegion""."
69,"If the @Filter annotation does not explicitly specify a restriction, the default restriction given by the @FilterDef will be applied to the entity."
69,But an entity is free to override the default condition.
69,@Entity
69,"@Filter(name = example_.FILTER_BY_REGION, condition = ""name = :region"")"
69,class Region {
69,@Id String name;
69,...
69,Note that the restriction specified by the condition or defaultCondition is a native SQL expression.
69,Table 58. Annotations for defining filters
69,Annotation
69,Purpose
69,@FilterDef
69,Defines a filter and declares its name (exactly one per filter)
69,@Filter
69,Specifies how a filter applies to a given entity or collection (many per filter)
69,"By default, a new session comes with every filter disabled."
69,A filter may be explicitly enabled in a given session by calling enableFilter() and assigning arguments to the parameters of the filter.
69,You should do this right at the start of the session.
69,sessionFactory.inTransaction(session -> {
69,session.enableFilter(example_.FILTER_BY_REGION)
69,".setParameter(""region"", ""es"")"
69,.validate();
69,...
69,});
69,"Now, any queries executed within the session will have the filter restriction applied."
69,Collections annotated @Filter will also have their members correctly filtered.
69,"On the other hand, filters are not applied to @ManyToOne associations, nor to find()."
69,This is completely by design and is not in any way a bug.
69,More than one filter may be enabled in a given session.
69,"When we only need to filter rows by a static condition with no parameters, we don’t need a filter, since @SQLRestriction provides a much simpler way to do that."
69,"We’ve mentioned that a filter can be used to implement versioning, and to provide historical views of the data."
69,"Being such a general-purpose construct, filters provide a lot of flexibility here."
69,"But if you’re after a more focused/opinionated solution to this problem, you should definitely check out Envers."
69,Using Envers for auditing historical data
69,"Envers is an add-on to Hibernate ORM which keeps a historical record of each versioned entity in a separate audit table, and allows past revisions of the data to be viewed and queried."
69,"A full introduction to Envers would require a whole chapter, so we’ll just give you a quick taste here."
69,"First, we must mark an entity as versioned, using the @Audited annotation:"
69,@Audited @Entity
69,"@Table(name=""CurrentDocument"")"
69,"@AuditTable(""DocumentRevision"")"
69,class Document { ... }
69,"The @AuditTable annotation is optional, and it’s better to set either org.hibernate.envers.audit_table_prefix or org.hibernate.envers.audit_table_suffix and let the audit table name be inferred."
69,The AuditReader interface exposes operations for retrieving and querying historical revisions.
69,It’s really easy to get hold of one of these:
69,AuditReader reader = AuditReaderFactory.get(entityManager);
69,Envers tracks revisions of the data via a global revision number.
69,We may easily find the revision number which was current at a given instant:
69,Number revision = reader.getRevisionNumberForDate(datetime);
69,We can use the revision number to ask for the version of our entity associated with the given revision number:
69,"Document doc = reader.find(Document.class, id, revision);"
69,"Alternatively, we can directly ask for the version which was current at a given instant:"
69,"Document doc = reader.find(Document.class, id, datetime);"
69,We can even execute queries to obtain lists of entities current at the given revision number:
69,List documents =
69,reader.createQuery()
69,".forEntitiesAtRevision(Document.class, revision)"
69,.getResultList();
69,"For much more information, see the User Guide."
69,Another closely-related problem is multi-tenancy.
69,8.2. Multi-tenancy
69,A multi-tenant database is one where the data is segregated by tenant.
69,"We don’t need to actually define what a ""tenant"" really represents here; all we care about at this level of abstraction is that each tenant may be distinguished by a unique identifier."
69,And that there’s a well-defined current tenant in each session.
69,We may specify the current tenant when we open a session:
69,var session =
69,sessionFactory.withOptions()
69,.tenantIdentifier(tenantId)
69,.openSession();
69,"Or, when using JPA-standard APIs:"
69,var entityManager =
69,"entityManagerFactory.createEntityManager(Map.of(HibernateHints.HINT_TENANT_ID, tenantId));"
69,"However, since we often don’t have this level of control over creation of the session, it’s more common to supply an implementation of CurrentTenantIdentifierResolver to Hibernate."
69,There are three common ways to implement multi-tenancy:
69,"each tenant has its own database,"
69,"each tenant has its own schema, or"
69,"tenants share tables in a single schema, and rows are tagged with the tenant id."
69,"From the point of view of Hibernate, there’s little difference between the first two options."
69,Hibernate will need to obtain a JDBC connection with permissions on the database and schema owned by the current tenant.
69,"Therefore, we must implement a MultiTenantConnectionProvider which takes on this responsibility:"
69,"from time to time, Hibernate will ask for a connection, passing the id of the current tenant, and then we must create an appropriate connection or obtain one from a pool, and return it to Hibernate, and"
69,"later, Hibernate will release the connection and ask us to destroy it or return it to the appropriate pool."
69,Check out DataSourceBasedMultiTenantConnectionProviderImpl for inspiration.
69,The third option is quite different.
69,"In this case we don’t need a MultiTenantConnectionProvider, but we will need a dedicated column holding the tenant id mapped by each of our entities."
69,@Entity
69,class Account {
69,@Id String id;
69,@TenantId String tenantId;
69,...
69,The @TenantId annotation is used to indicate an attribute of an entity which holds the tenant id.
69,"Within a given session, our data is automatically filtered so that only rows tagged with the tenant id of the current tenant are visible in that session."
69,Native SQL queries are not automatically filtered by tenant id; you’ll have to do that part yourself.
69,"To make use of multi-tenancy, we’ll usually need to set at least one of these configuration properties:"
69,Table 59. Multi-tenancy configuration
69,Configuration property name
69,Purpose
69,hibernate.tenant_identifier_resolver
69,Specifies the CurrentTenantIdentifierResolver
69,hibernate.multi_tenant_connection_provider
69,Specifies the MultiTenantConnectionProvider
69,8.3. Using custom-written SQL
69,"We’ve already discussed how to run queries written in SQL, but occasionally that’s not enough."
69,Sometimes—but much less often than you might expect—we would like to customize the SQL used by Hibernate to perform basic CRUD operations for an entity or collection.
69,For this we can use @SQLInsert and friends:
69,@Entity
69,"@SQLInsert(sql = ""insert into person (name, id, valid) values (?, ?, true)"", check = COUNT)"
69,"@SQLUpdate(sql = ""update person set name = ? where id = ?"")"
69,"@SQLDelete(sql = ""update person set valid = false where id = ?"")"
69,"@SQLSelect(sql = ""select id, name from person where id = ? and valid = true"")"
69,public static class Person { ... }
69,"If the custom SQL should be executed via a CallableStatement, just specify callable=true."
69,"Any SQL statement specified by one of these annotations must have exactly the number of JDBC parameters that Hibernate expects, that is, one for each column mapped by the entity, in the exact order Hibernate expects. In particular, the primary key columns must come last."
69,"However, the @Column annotation does lend some flexibility here:"
69,"if a column should not be written as part of the custom insert statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(insertable=false), or"
69,"if a column should not be written as part of the custom update statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(updatable=false)."
69,"If you need custom SQL, but are targeting multiple dialects of SQL, you can use the annotations defined in DialectOverrides."
69,"For example, this annotation lets us override the custom insert statement just for PostgreSQL:"
69,"@DialectOverride.SQLInsert(dialect = PostgreSQLDialect.class,"
69,"override = @SQLInsert(sql=""insert into person (name,id) values (?,gen_random_uuid())""))"
69,It’s even possible to override the custom SQL for specific versions of a database.
69,Sometimes a custom insert or update statement assigns a value to a mapped column which is calculated when the statement is executed on the database.
69,"For example, the value might be obtained by calling a SQL function:"
69,"@SQLInsert(sql = ""insert into person (name, id) values (?, gen_random_uuid())"")"
69,But the entity instance which represents the row being inserted or updated won’t be automatically populated with that value.
69,And so our persistence context loses synchronization with the database.
69,"In situations like this, we may use the @Generated annotation to tell Hibernate to reread the state of the entity after each insert or update."
69,8.4. Handling database-generated columns
69,"Sometimes, a column value is assigned or mutated by events that happen in the database, and aren’t visible to Hibernate."
69,For example:
69,"a table might have a column value populated by a trigger,"
69,"a mapped column might have a default value defined in DDL, or"
69,"a custom SQL insert or update statement might assign a value to a mapped column, as we saw in the previous subsection."
69,"One way to deal with this situation is to explicitly call refresh() at appropriate moments, forcing the session to reread the state of the entity."
69,But this is annoying.
69,The @Generated annotation relieves us of the burden of explicitly calling refresh().
69,"It specifies that the value of the annotated entity attribute is generated by the database, and that the generated value should be automatically retrieved using a SQL returning clause, or separate select after it is generated."
69,A useful example is the following mapping:
69,@Entity
69,class Entity {
69,@Generated @Id
69,"@ColumnDefault(""gen_random_uuid()"")"
69,UUID id;
69,The generated DDL is:
69,create table Entity (
69,"id uuid default gen_random_uuid() not null,"
69,primary key (uuid)
69,"So here the value of id is defined by the column default clause, by calling the PostgreSQL function gen_random_uuid()."
69,"When a column value is generated during updates, use @Generated(event=UPDATE)."
69,"When a value is generated by both inserts and updates, use @Generated(event={INSERT,UPDATE})."
69,"For columns which should be generated using a SQL generated always as clause, prefer the @GeneratedColumn annotation, so that Hibernate automatically generates the correct DDL."
69,"Actually, the @Generated and @GeneratedColumn annotations are defined in terms of a more generic and user-extensible framework for handling attribute values generated in Java, or by the database."
69,"So let’s drop down a layer, and see how that works."
69,8.5. User-defined generators
69,"JPA doesn’t define a standard way to extend the set of id generation strategies, but Hibernate does:"
69,"the Generator hierarchy of interfaces in the package org.hibernate.generator lets you define new generators, and"
69,the @IdGeneratorType meta-annotation from the package org.hibernate.annotations lets you write an annotation which associates a Generator type with identifier attributes.
69,"Furthermore, the @ValueGenerationType meta-annotation lets you write an annotation which associates a Generator type with a non-@Id attribute."
69,"These APIs are new in Hibernate 6, and supersede the classic IdentifierGenerator interface and @GenericGenerator annotation from older versions of Hibernate."
69,"However, the older APIs are still available and custom IdentifierGenerators written for older versions of Hibernate continue to work in Hibernate 6."
69,Hibernate has a range of built-in generators which are defined in terms of this new framework.
69,Table 60. Built-in generators
69,Annotation
69,Implementation
69,Purpose
69,@Generated
69,GeneratedGeneration
69,Generically handles database-generated values
69,@GeneratedColumn
69,GeneratedAlwaysGeneration
69,Handles values generated using generated always
69,@CurrentTimestamp
69,CurrentTimestampGeneration
69,Generic support for database or in-memory generation of creation or update timestamps
69,@CreationTimestamp
69,CurrentTimestampGeneration
69,A timestamp generated when an entity is first made persistent
69,@UpdateTimestamp
69,CurrentTimestampGeneration
69,"A timestamp generated when an entity is made persistent, and regenerated every time the entity is modified"
69,@UuidGenerator
69,UuidGenerator
69,A more flexible generator for RFC 4122 UUIDs
69,"Furthermore, support for JPA’s standard id generation strategies is also defined in terms of this framework."
69,"As an example, let’s look at how @UuidGenerator is defined:"
69,@IdGeneratorType(org.hibernate.id.uuid.UuidGenerator.class)
69,@ValueGenerationType(generatedBy = org.hibernate.id.uuid.UuidGenerator.class)
69,@Retention(RUNTIME)
69,"@Target({ FIELD, METHOD })"
69,public @interface UuidGenerator { ... }
69,@UuidGenerator is meta-annotated both @IdGeneratorType and @ValueGenerationType because it may be used to generate both ids and values of regular attributes.
69,"Either way, this Generator class does the hard work:"
69,public class UuidGenerator
69,// this generator produced values before SQL is executed
69,implements BeforeExecutionGenerator {
69,// constructors accept an instance of the @UuidGenerator
69,"// annotation, allowing the generator to be ""configured"""
69,// called to create an id generator
69,public UuidGenerator(
69,"org.hibernate.annotations.UuidGenerator config,"
69,"Member idMember,"
69,CustomIdGeneratorCreationContext creationContext) {
69,"this(config, idMember);"
69,// called to create a generator for a regular attribute
69,public UuidGenerator(
69,"org.hibernate.annotations.UuidGenerator config,"
69,"Member member,"
69,GeneratorCreationContext creationContext) {
69,"this(config, idMember);"
69,...
69,@Override
69,public EnumSet<EventType> getEventTypes() {
69,"// UUIDs are only assigned on insert, and never regenerated"
69,return INSERT_ONLY;
69,@Override
69,"public Object generate(SharedSessionContractImplementor session, Object owner, Object currentValue, EventType eventType) {"
69,// actually generate a UUID and transform it to the required type
69,return valueTransformer.transform( generator.generateUuid( session ) );
69,You can find out more about custom generators from the Javadoc for @IdGeneratorType and for org.hibernate.generator.
69,8.6. Naming strategies
69,"When working with a pre-existing relational schema, it’s usual to find that the column and table naming conventions used in the schema don’t match Java’s naming conventions."
69,"Of course, the @Table and @Column annotations let us explicitly specify a mapped table or column name."
69,But we would prefer to avoid scattering these annotations across our whole domain model.
69,"Therefore, Hibernate lets us define a mapping between Java naming conventions, and the naming conventions of the relational schema."
69,Such a mapping is called a naming strategy.
69,"First, we need to understand how Hibernate assigns and processes names."
69,Logical naming is the process of applying naming rules to determine the logical names of objects which were not explicitly assigned names in the O/R mapping.
69,"That is, when there’s no @Table or @Column annotation."
69,"Physical naming is the process of applying additional rules to transform a logical name into an actual ""physical"" name that will be used in the database."
69,"For example, the rules might include things like using standardized abbreviations, or trimming the length of identifiers."
69,"Thus, there’s two flavors of naming strategy, with slightly different responsibilities."
69,Hibernate comes with default implementations of these interfaces:
69,Flavor
69,Default implementation
69,An ImplicitNamingStrategy is responsible for assigning a logical name when none is specified by an annotation
69,A default strategy which implements the rules defined by JPA
69,A PhysicalNamingStrategy is responsible for transforming a logical name and producing the name used in the database
69,A trivial implementation which does no processing
69,"We happen to not much like the naming rules defined by JPA, which specify that mixed case and camel case identifiers should be concatenated using underscores."
69,We bet you could easily come up with a much better ImplicitNamingStrategy than that!
69,(Hint: it should always produce legit mixed case identifiers.)
69,A popular PhysicalNamingStrategy produces snake case identifiers.
69,Custom naming strategies may be enabled using the configuration properties we already mentioned without much explanation back in Minimizing repetitive mapping information.
69,Table 61. Naming strategy configuration
69,Configuration property name
69,Purpose
69,hibernate.implicit_naming_strategy
69,Specifies the ImplicitNamingStrategy
69,hibernate.physical_naming_strategy
69,Specifies the PhysicalNamingStrategy
69,8.7. Spatial datatypes
69,Hibernate Spatial augments the built-in basic types with a set of Java mappings for OGC spatial types.
69,"Geolatte-geom defines a set of Java types implementing the OGC spatial types, and codecs for translating to and from database-native spatial datatypes."
69,Hibernate Spatial itself supplies integration with Hibernate.
69,"To use Hibernate Spatial, we must add it as a dependency, as described in Optional dependencies."
69,Then we may immediately use Geolatte-geom and JTS types in our entities.
69,No special annotations are needed:
69,import org.locationtech.jts.geom.Point;
69,import jakarta.persistence.*;
69,@Entity
69,class Event {
69,Event() {}
69,"Event(String name, Point location) {"
69,this.name = name;
69,this.location = location;
69,@Id @GeneratedValue
69,Long id;
69,String name;
69,Point location;
69,The generated DDL uses geometry as the type of the column mapped by location:
69,create table Event (
69,"id bigint not null,"
69,"location geometry,"
69,"name varchar(255),"
69,primary key (id)
69,Hibernate Spatial lets us work with spatial types just as we would with any of the built-in basic attribute types.
69,var geometryFactory = new GeometryFactory();
69,...
69,"Point point = geometryFactory.createPoint(new Coordinate(10, 5));"
69,"session.persist(new Event(""Hibernate ORM presentation"", point));"
69,But what makes this powerful is that we may write some very fancy queries involving functions of spatial types:
69,Polygon triangle =
69,geometryFactory.createPolygon(
69,new Coordinate[] {
69,"new Coordinate(9, 4),"
69,"new Coordinate(11, 4),"
69,"new Coordinate(11, 20),"
69,"new Coordinate(9, 4)"
69,Point event =
69,"session.createQuery(""select location from Event where within(location, :zone) = true"", Point.class)"
69,".setParameter(""zone"", triangle)"
69,.getSingleResult();
69,"Here, within() is one of the functions for testing spatial relations defined by the OpenGIS specification."
69,"Other such functions include touches(), intersects(), distance(), boundary(), etc."
69,Not every spatial relation function is supported on every database.
69,A matrix of support for spatial relation functions may be found in the User Guide.
69,"If you want to play with spatial functions on H2, run the following code first:"
69,sessionFactory.inTransaction(session -> {
69,session.doWork(connection -> {
69,try (var statement = connection.createStatement()) {
69,"statement.execute(""create alias if not exists h2gis_spatial for \""org.h2gis.functions.factory.H2GISFunctions.load\"""");"
69,"statement.execute(""call h2gis_spatial()"");"
69,});
69,} );
69,8.8. Ordered and sorted collections and map keys
69,"Java lists and maps don’t map very naturally to foreign key relationships between tables, and so we tend to avoid using them to represent associations between our entity classes."
69,"But if you feel like you really need a collection with a fancier structure than Set, Hibernate does have options."
69,"The first three options let us map the index of a List or key of a Map to a column, and are usually used with a @ElementCollection, or on the owning side of an association:"
69,Table 62. Annotations for mapping lists and maps
69,Annotation
69,Purpose
69,JPA-standard
69,@OrderColumn
69,Specifies the column used to maintain the order of a list
69,@ListIndexBase
69,The column value for the first element of the list (zero by default)
69,@MapKeyColumn
69,Specifies the column used to persist the keys of a map
69,(used when the key is of basic type)
69,@MapKeyJoinColumn
69,Specifies the column used to persist the keys of a map
69,(used when the key is an entity)
69,@ManyToMany
69,@OrderColumn // order of list is persistent
69,List<Author> authors = new ArrayList<>();
69,@ElementCollection
69,"@OrderColumn(name=""tag_order"") @ListIndexBase(1) // order column and base value"
69,List<String> tags;
69,@ElementCollection
69,"@CollectionTable(name = ""author_bios"","
69,// table name
69,"joinColumns = @JoinColumn(name = ""book_isbn"")) // column holding foreign key of owner"
69,"@Column(name=""bio"")"
69,// column holding map values
69,"@MapKeyJoinColumn(name=""author_ssn"")"
69,// column holding map keys
69,"Map<Author,String> biographies;"
69,"For a Map representing an unowned @OneToMany association, the column must also be mapped on the owning side, usually by an attribute of the target entity."
69,In this case we usually use a different annotation:
69,Table 63. Annotation for mapping an entity attribute to a map key
69,Annotation
69,Purpose
69,JPA-standard
69,@MapKey
69,Specifies an attribute of the target entity which acts as the key of the map
69,@OneToMany(mappedBy = Book_.PUBLISHER)
69,@MapKey(name = Book_.TITLE) // the key of the map is the title of the book
69,"Map<String,Book> booksByTitle = new HashMap<>();"
69,"Now, let’s introduce a little distinction:"
69,"an ordered collection is one with an ordering maintained in the database, and"
69,a sorted collection is one which is sorted in Java code.
69,These annotations allow us to specify how the elements of a collection should be ordered as they are read from the database:
69,Table 64. Annotations for ordered collections
69,Annotation
69,Purpose
69,JPA-standard
69,@OrderBy
69,Specifies a fragment of JPQL used to order the collection
69,@SQLOrder
69,Specifies a fragment of SQL used to order the collection
69,"On the other hand, the following annotations specify how a collection should be sorted in memory, and are used for collections of type SortedSet or SortedMap:"
69,Table 65. Annotations for sorted collections
69,Annotation
69,Purpose
69,JPA-standard
69,@SortNatural
69,Specifies that the elements of a collection are Comparable
69,@SortComparator
69,Specifies a Comparator used to sort the collection
69,"Under the covers, Hibernate uses a TreeSet or TreeMap to maintain the collection in sorted order."
69,8.9. Any mappings
69,An @Any mapping is a sort of polymorphic many-to-one association where the target entity types are not related by the usual entity inheritance.
69,The target type is distinguished using a discriminator value stored on the referring side of the relationship.
69,This is quite different to discriminated inheritance where the discriminator is held in the tables mapped by the referenced entity hierarchy.
69,"For example, consider an Order entity containing Payment information, where a Payment might be a CashPayment or a CreditCardPayment:"
69,interface Payment { ... }
69,@Entity
69,class CashPayment { ... }
69,@Entity
69,class CreditCardPayment { ... }
69,"In this example, Payment is not be declared as an entity type, and is not annotated @Entity. It might even be an interface, or at most just a mapped superclass, of CashPayment and CreditCardPayment. So in terms of the object/relational mappings, CashPayment and CreditCardPayment would not be considered to participate in the same entity inheritance hierarchy."
69,"On the other hand, CashPayment and CreditCardPayment do have the same identifier type."
69,This is important.
69,"An @Any mapping would store the discriminator value identifying the concrete type of Payment along with the state of the associated Order, instead of storing it in the table mapped by Payment."
69,@Entity
69,class Order {
69,...
69,@Any
69,@AnyKeyJavaClass(UUID.class)
69,//the foreign key type
69,"@JoinColumn(name=""payment_id"") // the foreign key column"
69,"@Column(name=""payment_type"")"
69,// the discriminator column
69,// map from discriminator values to target entity types
69,"@AnyDiscriminatorValue(discriminator=""CASH"", entity=CashPayment.class)"
69,"@AnyDiscriminatorValue(discriminator=""CREDIT"", entity=CreditCardPayment.class)"
69,Payment payment;
69,...
69,"It’s reasonable to think of the ""foreign key"" in an @Any mapping as a composite value made up of the foreign key and discriminator taken together. Note, however, that this composite foreign key is only conceptual and cannot be declared as a physical constraint on the relational database table."
69,There are a number of annotations which are useful to express this sort of complicated and unnatural mapping:
69,Table 66. Annotations for @Any mappings
69,Annotations
69,Purpose
69,@Any
69,Declares that an attribute is a discriminated polymorphic association mapping
69,@AnyDiscriminator
69,Specify the Java type of the discriminator
69,@JdbcType or @JdbcTypeCode
69,Specify the JDBC type of the discriminator
69,@AnyDiscriminatorValue
69,Specifies how discriminator values map to entity types
69,@Column or @Formula
69,Specify the column or formula in which the discriminator value is stored
69,@AnyKeyJavaType or @AnyKeyJavaClass
69,"Specify the Java type of the foreign key (that is, of the ids of the target entities)"
69,@AnyKeyJdbcType or @AnyKeyJdbcTypeCode
69,Specify the JDBC type of the foreign key
69,@JoinColumn
69,Specifies the foreign key column
69,"Of course, @Any mappings are disfavored, except in extremely special cases, since it’s much more difficult to enforce referential integrity at the database level."
69,There’s also currently some limitations around querying @Any associations in HQL.
69,This is allowed:
69,from Order ord
69,join CashPayment cash
69,on id(ord.payment) = cash.id
69,Polymorphic association joins for @Any mappings are not currently implemented.
69,8.10. Selective column lists in inserts and updates
69,"By default, Hibernate generates insert and update statements for each entity during boostrap, and reuses the same insert statement every time an instance of the entity is made persistent, and the same update statement every time an instance of the entity is modified."
69,This means that:
69,"if an attribute is null when the entity is made persistent, its mapped column is redundantly included in the SQL insert, and"
69,"worse, if a certain attribute is unmodified when other attributes are changed, the column mapped by that attribute is redundantly included in the SQL update."
69,"Most of the time, this just isn’t an issue worth worrying about."
69,"The cost of interacting with the database is usually dominated by the cost of a round trip, not by the number of columns in the insert or update."
69,"But in cases where it does become important, there are two ways to be more selective about which columns are included in the SQL."
69,The JPA-standard way is to indicate statically which columns are eligible for inclusion via the @Column annotation.
69,"For example, if an entity is always created with an immutable creationDate, and with no completionDate, then we would write:"
69,@Column(updatable=false) LocalDate creationDate;
69,@Column(insertable=false) LocalDate completionDate;
69,"This approach works quite well in many cases, but often breaks down for entities with more than a handful of updatable columns."
69,An alternative solution is to ask Hibernate to generate SQL dynamically each time an insert or update is executed.
69,We do this by annotating the entity class.
69,Table 67. Annotations for dynamic SQL generation
69,Annotation
69,Purpose
69,@DynamicInsert
69,Specifies that an insert statement should be generated each time an entity is made persistent
69,@DynamicUpdate
69,Specifies that an update statement should be generated each time an entity is modified
69,"It’s important to realize that, while @DynamicInsert has no impact on semantics, the more useful @DynamicUpdate annotation does have a subtle side effect."
69,"The wrinkle is that if an entity has no version property, @DynamicUpdate opens the possibility of two optimistic transactions concurrently reading and selectively updating a given instance of the entity."
69,"In principle, this might lead to a row with inconsistent column values after both optimistic transactions commit successfully."
69,"Of course, this consideration doesn’t arise for entities with a @Version attribute."
69,But there’s a solution!
69,Well-designed relational schemas should have constraints to ensure data integrity.
69,That’s true no matter what measures we take to preserve integrity in our program logic.
69,We may ask Hibernate to add a check constraint to our table using the @Check annotation.
69,Check constraints and foreign key constraints can help ensure that a row never contains inconsistent column values.
69,8.11. Using the bytecode enhancer
69,Hibernate’s bytecode enhancer enables the following features:
69,"attribute-level lazy fetching for basic attributes annotated @Basic(fetch=LAZY) and for lazy non-polymorphic associations,"
69,interception-based—instead of the usual snapshot-based—detection of modifications.
69,"To use the bytecode enhancer, we must add the Hibernate plugin to our gradle build:"
69,plugins {
69,"id ""org.hibernate.orm"" version ""6.3.0.Final"""
69,hibernate { enhancement }
69,Consider this field:
69,@Entity
69,class Book {
69,...
69,"@Basic(optional = false, fetch = LAZY)"
69,@Column(length = LONG32)
69,String fullText;
69,...
69,"The fullText field maps to a clob or text column, depending on the SQL dialect."
69,"Since it’s expensive to retrieve the full book-length text, we’ve mapped the field fetch=LAZY, telling Hibernate not to read the field until it’s actually used."
69,"Without the bytecode enhancer, this instruction is ignored, and the field is always fetched immediately, as part of the initial select that retrieves the Book entity."
69,"With bytecode enhancement, Hibernate is able to detect access to the field, and lazy fetching is possible."
69,"By default, Hibernate fetches all lazy fields of a given entity at once, in a single select, when any one of them is accessed."
69,"Using the @LazyGroup annotation, it’s possible to assign fields to distinct ""fetch groups"", so that different lazy fields may be fetched independently."
69,"Similarly, interception lets us implement lazy fetching for non-polymorphic associations without the need for a separate proxy object."
69,"However, if an association is polymorphic, that is, if the target entity type has subclasses, then a proxy is still required."
69,Interception-based change detection is a nice performance optimization with a slight cost in terms of correctness.
69,"Without the bytecode enhancer, Hibernate keeps a snapshot of the state of each entity after reading from or writing to the database."
69,"When the session flushes, the snapshot state is compared to the current state of the entity to determine if the entity has been modified."
69,Maintaining these snapshots does have an impact on performance.
69,"With bytecode enhancement, we may avoid this cost by intercepting writes to the field and recording these modifications as they happen."
69,"This optimization isn’t completely transparent, however."
69,Interception-based change detection is less accurate than snapshot-based dirty checking.
69,"For example, consider this attribute:"
69,byte[] image;
69,"Interception is able to detect writes to the image field, that is, replacement of the whole array."
69,"It’s not able to detect modifications made directly to the elements of the array, and so such modifications may be lost."
69,8.12. Named fetch profiles
69,We’ve already seen two different ways to override the default fetching strategy for an association:
69,"JPA entity graphs, and"
69,"the join fetch clause in HQL, or, equivalently, the method From.fetch() in the criteria query API."
69,A third way is to define a named fetch profile.
69,"First, we must declare the profile, by annotating a class or package:"
69,"@FetchProfile(name = ""EagerBook"")"
69,@Entity
69,class Book { ... }
69,"Note that even though we’ve placed this annotation on the Book entity, a fetch profile—unlike an entity graph—isn’t ""rooted"" at any particular entity."
69,"We may specify association fetching strategies using the fetchOverrides member of the @FetchProfile annotation, but frankly it looks so messy that we’re embarrassed to show it to you here."
69,"Similarly, a JPA entity graph may be defined using @NamedEntityGraph."
69,"But the format of this annotation is even worse than @FetchProfile(fetchOverrides=…​), so we can’t recommend it. 💀"
69,A better way is to annotate an association with the fetch profiles it should be fetched in:
69,"@FetchProfile(name = ""EagerBook"")"
69,@Entity
69,class Book {
69,...
69,@ManyToOne(fetch = LAZY)
69,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
69,Publisher publisher;
69,@ManyToMany
69,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
69,Set<Author> authors;
69,...
69,@Entity
69,class Author {
69,...
69,@OneToOne
69,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
69,Person person;
69,...
69,"Here, once again, Book_.PROFILE_EAGER_BOOK is generated by the Metamodel Generator, and is just a constant with the value ""EagerBook""."
69,"For collections, we may even request subselect fetching:"
69,"@FetchProfile(name = ""EagerBook"")"
69,"@FetchProfile(name = ""BookWithAuthorsBySubselect"")"
69,@Entity
69,class Book {
69,...
69,@OneToOne
69,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
69,Person person;
69,@ManyToMany
69,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
69,"@FetchProfileOverride(profile = Book_.BOOK_WITH_AUTHORS_BY_SUBSELECT,"
69,mode = SUBSELECT)
69,Set<Author> authors;
69,...
69,We may define as many different fetch profiles as we like.
69,Table 68. Annotations for defining fetch profiles
69,Annotation
69,Purpose
69,@FetchProfile
69,"Declares a named fetch profile, optionally including a list of @FetchOverrides"
69,@FetchProfile.FetchOverride
69,Declares a fetch strategy override as part of the @FetchProfile declaration
69,@FetchProfileOverride
69,"Specifies the fetch strategy for the annotated association, in a given fetch profile"
69,A fetch profile must be explicitly enabled for a given session:
69,session.enableFetchProfile(Book_.PROFILE_EAGER_BOOK);
69,"Book eagerBook = session.find(Book.class, bookId);"
69,So why or when might we prefer named fetch profiles to entity graphs?
69,"Well, it’s really hard to say."
69,"It’s nice that this feature exists, and if you love it, that’s great."
69,But Hibernate offers alternatives that we think are more compelling most of the time.
69,The one and only advantage unique to fetch profiles is that they let us very selectively request subselect fetching.
69,"We can’t do that with entity graphs, and we can’t do it with HQL."
69,There’s a special built-in fetch profile named org.hibernate.defaultProfile which is defined as the profile with @FetchProfileOverride(mode=JOIN) applied to every eager @ManyToOne or @OneToOne association.
69,If you enable this profile:
69,"session.enableFetchProfile(""org.hibernate.defaultProfile"");"
69,Then outer joins for such associations will automatically be added to every HQL or criteria query.
69,This is nice if you can’t be bothered typing out those join fetches explicitly.
69,And in principle it even helps partially mitigate the problem of JPA having specified the wrong default for the fetch member of @ManyToOne.
69,9. Credits
69,The full list of contributors to Hibernate ORM can be found on the
69,GitHub repository.
69,The following contributors were involved in this documentation:
69,Gavin King
69,Version 6.3.2.Final
69,Last updated 2023-11-23 14:49:43 UTC
70,Example: Deploying WordPress and MySQL with Persistent Volumes | Kubernetes
70,Example: Deploying WordPress and MySQL with Persistent Volumes | KubernetesDocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsRelease Information
70,v1.29
70,v1.28
70,v1.27
70,v1.26
70,v1.25English中文 (Chinese)
70,日本語 (Japanese)
70,한국어 (Korean)
70,"KubeCon + CloudNativeCon Europe 2024Join us for four days of incredible opportunities to collaborate, learn and share with the cloud native community.Buy your ticket now! 19 - 22 March | Paris, France"
70,Documentation
70,Available Documentation Versions
70,Getting started
70,Learning environment
70,Production environment
70,Container Runtimes
70,Installing Kubernetes with deployment tools
70,Bootstrapping clusters with kubeadm
70,Installing kubeadm
70,Troubleshooting kubeadm
70,Creating a cluster with kubeadm
70,Customizing components with the kubeadm API
70,Options for Highly Available Topology
70,Creating Highly Available Clusters with kubeadm
70,Set up a High Availability etcd Cluster with kubeadm
70,Configuring each kubelet in your cluster using kubeadm
70,Dual-stack support with kubeadm
70,Turnkey Cloud Solutions
70,Best practices
70,Considerations for large clusters
70,Running in multiple zones
70,Validate node setup
70,Enforcing Pod Security Standards
70,PKI certificates and requirements
70,Concepts
70,Overview
70,Objects In Kubernetes
70,Kubernetes Object Management
70,Object Names and IDs
70,Labels and Selectors
70,Namespaces
70,Annotations
70,Field Selectors
70,Finalizers
70,Owners and Dependents
70,Recommended Labels
70,Kubernetes Components
70,The Kubernetes API
70,Cluster Architecture
70,Nodes
70,Communication between Nodes and the Control Plane
70,Controllers
70,Leases
70,Cloud Controller Manager
70,About cgroup v2
70,Container Runtime Interface (CRI)
70,Garbage Collection
70,Mixed Version Proxy
70,Containers
70,Images
70,Container Environment
70,Runtime Class
70,Container Lifecycle Hooks
70,Workloads
70,Pods
70,Pod Lifecycle
70,Init Containers
70,Sidecar Containers
70,Ephemeral Containers
70,Disruptions
70,Pod Quality of Service Classes
70,User Namespaces
70,Downward API
70,Workload Management
70,Deployments
70,ReplicaSet
70,StatefulSets
70,DaemonSet
70,Jobs
70,Automatic Cleanup for Finished Jobs
70,CronJob
70,ReplicationController
70,Autoscaling Workloads
70,Managing Workloads
70,"Services, Load Balancing, and Networking"
70,Service
70,Ingress
70,Ingress Controllers
70,Gateway API
70,EndpointSlices
70,Network Policies
70,DNS for Services and Pods
70,IPv4/IPv6 dual-stack
70,Topology Aware Routing
70,Networking on Windows
70,Service ClusterIP allocation
70,Service Internal Traffic Policy
70,Storage
70,Volumes
70,Persistent Volumes
70,Projected Volumes
70,Ephemeral Volumes
70,Storage Classes
70,Volume Attributes Classes
70,Dynamic Volume Provisioning
70,Volume Snapshots
70,Volume Snapshot Classes
70,CSI Volume Cloning
70,Storage Capacity
70,Node-specific Volume Limits
70,Volume Health Monitoring
70,Windows Storage
70,Configuration
70,Configuration Best Practices
70,ConfigMaps
70,Secrets
70,Resource Management for Pods and Containers
70,Organizing Cluster Access Using kubeconfig Files
70,Resource Management for Windows nodes
70,Security
70,Cloud Native Security
70,Pod Security Standards
70,Pod Security Admission
70,Service Accounts
70,Pod Security Policies
70,Security For Windows Nodes
70,Controlling Access to the Kubernetes API
70,Role Based Access Control Good Practices
70,Good practices for Kubernetes Secrets
70,Multi-tenancy
70,Hardening Guide - Authentication Mechanisms
70,Kubernetes API Server Bypass Risks
70,Security Checklist
70,Policies
70,Limit Ranges
70,Resource Quotas
70,Process ID Limits And Reservations
70,Node Resource Managers
70,"Scheduling, Preemption and Eviction"
70,Kubernetes Scheduler
70,Assigning Pods to Nodes
70,Pod Overhead
70,Pod Scheduling Readiness
70,Pod Topology Spread Constraints
70,Taints and Tolerations
70,Scheduling Framework
70,Dynamic Resource Allocation
70,Scheduler Performance Tuning
70,Resource Bin Packing
70,Pod Priority and Preemption
70,Node-pressure Eviction
70,API-initiated Eviction
70,Cluster Administration
70,Certificates
70,Cluster Networking
70,Logging Architecture
70,Metrics For Kubernetes System Components
70,System Logs
70,Traces For Kubernetes System Components
70,Proxies in Kubernetes
70,API Priority and Fairness
70,Installing Addons
70,Windows in Kubernetes
70,Windows containers in Kubernetes
70,Guide for Running Windows Containers in Kubernetes
70,Extending Kubernetes
70,"Compute, Storage, and Networking Extensions"
70,Network Plugins
70,Device Plugins
70,Extending the Kubernetes API
70,Custom Resources
70,Kubernetes API Aggregation Layer
70,Operator pattern
70,Tasks
70,Install Tools
70,Install and Set Up kubectl on Linux
70,Install and Set Up kubectl on macOS
70,Install and Set Up kubectl on Windows
70,Administer a Cluster
70,Administration with kubeadm
70,Certificate Management with kubeadm
70,Configuring a cgroup driver
70,Reconfiguring a kubeadm cluster
70,Upgrading kubeadm clusters
70,Upgrading Linux nodes
70,Upgrading Windows nodes
70,Changing The Kubernetes Package Repository
70,Migrating from dockershim
70,Changing the Container Runtime on a Node from Docker Engine to containerd
70,Migrate Docker Engine nodes from dockershim to cri-dockerd
70,Find Out What Container Runtime is Used on a Node
70,Troubleshooting CNI plugin-related errors
70,Check whether dockershim removal affects you
70,Migrating telemetry and security agents from dockershim
70,Generate Certificates Manually
70,"Manage Memory, CPU, and API Resources"
70,Configure Default Memory Requests and Limits for a Namespace
70,Configure Default CPU Requests and Limits for a Namespace
70,Configure Minimum and Maximum Memory Constraints for a Namespace
70,Configure Minimum and Maximum CPU Constraints for a Namespace
70,Configure Memory and CPU Quotas for a Namespace
70,Configure a Pod Quota for a Namespace
70,Install a Network Policy Provider
70,Use Antrea for NetworkPolicy
70,Use Calico for NetworkPolicy
70,Use Cilium for NetworkPolicy
70,Use Kube-router for NetworkPolicy
70,Romana for NetworkPolicy
70,Weave Net for NetworkPolicy
70,Access Clusters Using the Kubernetes API
70,Advertise Extended Resources for a Node
70,Autoscale the DNS Service in a Cluster
70,Change the Access Mode of a PersistentVolume to ReadWriteOncePod
70,Change the default StorageClass
70,Switching from Polling to CRI Event-based Updates to Container Status
70,Change the Reclaim Policy of a PersistentVolume
70,Cloud Controller Manager Administration
70,Configure a kubelet image credential provider
70,Configure Quotas for API Objects
70,Control CPU Management Policies on the Node
70,Control Topology Management Policies on a node
70,Customizing DNS Service
70,Debugging DNS Resolution
70,Declare Network Policy
70,Developing Cloud Controller Manager
70,Enable Or Disable A Kubernetes API
70,Encrypting Confidential Data at Rest
70,Decrypt Confidential Data that is Already Encrypted at Rest
70,Guaranteed Scheduling For Critical Add-On Pods
70,IP Masquerade Agent User Guide
70,Limit Storage Consumption
70,Migrate Replicated Control Plane To Use Cloud Controller Manager
70,Namespaces Walkthrough
70,Operating etcd clusters for Kubernetes
70,Reserve Compute Resources for System Daemons
70,Running Kubernetes Node Components as a Non-root User
70,Safely Drain a Node
70,Securing a Cluster
70,Set Kubelet Parameters Via A Configuration File
70,Share a Cluster with Namespaces
70,Upgrade A Cluster
70,Use Cascading Deletion in a Cluster
70,Using a KMS provider for data encryption
70,Using CoreDNS for Service Discovery
70,Using NodeLocal DNSCache in Kubernetes Clusters
70,Using sysctls in a Kubernetes Cluster
70,Utilizing the NUMA-aware Memory Manager
70,Verify Signed Kubernetes Artifacts
70,Configure Pods and Containers
70,Assign Memory Resources to Containers and Pods
70,Assign CPU Resources to Containers and Pods
70,Configure GMSA for Windows Pods and containers
70,Resize CPU and Memory Resources assigned to Containers
70,Configure RunAsUserName for Windows pods and containers
70,Create a Windows HostProcess Pod
70,Configure Quality of Service for Pods
70,Assign Extended Resources to a Container
70,Configure a Pod to Use a Volume for Storage
70,Configure a Pod to Use a PersistentVolume for Storage
70,Configure a Pod to Use a Projected Volume for Storage
70,Configure a Security Context for a Pod or Container
70,Configure Service Accounts for Pods
70,Pull an Image from a Private Registry
70,"Configure Liveness, Readiness and Startup Probes"
70,Assign Pods to Nodes
70,Assign Pods to Nodes using Node Affinity
70,Configure Pod Initialization
70,Attach Handlers to Container Lifecycle Events
70,Configure a Pod to Use a ConfigMap
70,Share Process Namespace between Containers in a Pod
70,Use a User Namespace With a Pod
70,Create static Pods
70,Translate a Docker Compose File to Kubernetes Resources
70,Enforce Pod Security Standards by Configuring the Built-in Admission Controller
70,Enforce Pod Security Standards with Namespace Labels
70,Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller
70,"Monitoring, Logging, and Debugging"
70,Troubleshooting Applications
70,Debug Pods
70,Debug Services
70,Debug a StatefulSet
70,Determine the Reason for Pod Failure
70,Debug Init Containers
70,Debug Running Pods
70,Get a Shell to a Running Container
70,Troubleshooting Clusters
70,Troubleshooting kubectl
70,Resource metrics pipeline
70,Tools for Monitoring Resources
70,Monitor Node Health
70,Debugging Kubernetes nodes with crictl
70,Auditing
70,Debugging Kubernetes Nodes With Kubectl
70,Developing and debugging services locally using telepresence
70,Windows debugging tips
70,Manage Kubernetes Objects
70,Declarative Management of Kubernetes Objects Using Configuration Files
70,Declarative Management of Kubernetes Objects Using Kustomize
70,Managing Kubernetes Objects Using Imperative Commands
70,Imperative Management of Kubernetes Objects Using Configuration Files
70,Update API Objects in Place Using kubectl patch
70,Managing Secrets
70,Managing Secrets using kubectl
70,Managing Secrets using Configuration File
70,Managing Secrets using Kustomize
70,Inject Data Into Applications
70,Define a Command and Arguments for a Container
70,Define Dependent Environment Variables
70,Define Environment Variables for a Container
70,Expose Pod Information to Containers Through Environment Variables
70,Expose Pod Information to Containers Through Files
70,Distribute Credentials Securely Using Secrets
70,Run Applications
70,Run a Stateless Application Using a Deployment
70,Run a Single-Instance Stateful Application
70,Run a Replicated Stateful Application
70,Scale a StatefulSet
70,Delete a StatefulSet
70,Force Delete StatefulSet Pods
70,Horizontal Pod Autoscaling
70,HorizontalPodAutoscaler Walkthrough
70,Specifying a Disruption Budget for your Application
70,Accessing the Kubernetes API from a Pod
70,Run Jobs
70,Running Automated Tasks with a CronJob
70,Coarse Parallel Processing Using a Work Queue
70,Fine Parallel Processing Using a Work Queue
70,Indexed Job for Parallel Processing with Static Work Assignment
70,Job with Pod-to-Pod Communication
70,Parallel Processing using Expansions
70,Handling retriable and non-retriable pod failures with Pod failure policy
70,Access Applications in a Cluster
70,Deploy and Access the Kubernetes Dashboard
70,Accessing Clusters
70,Configure Access to Multiple Clusters
70,Use Port Forwarding to Access Applications in a Cluster
70,Use a Service to Access an Application in a Cluster
70,Connect a Frontend to a Backend Using Services
70,Create an External Load Balancer
70,List All Container Images Running in a Cluster
70,Set up Ingress on Minikube with the NGINX Ingress Controller
70,Communicate Between Containers in the Same Pod Using a Shared Volume
70,Configure DNS for a Cluster
70,Access Services Running on Clusters
70,Extend Kubernetes
70,Configure the Aggregation Layer
70,Use Custom Resources
70,Extend the Kubernetes API with CustomResourceDefinitions
70,Versions in CustomResourceDefinitions
70,Set up an Extension API Server
70,Configure Multiple Schedulers
70,Use an HTTP Proxy to Access the Kubernetes API
70,Use a SOCKS5 Proxy to Access the Kubernetes API
70,Set up Konnectivity service
70,TLS
70,Configure Certificate Rotation for the Kubelet
70,Manage TLS Certificates in a Cluster
70,Manual Rotation of CA Certificates
70,Manage Cluster Daemons
70,Perform a Rolling Update on a DaemonSet
70,Perform a Rollback on a DaemonSet
70,Running Pods on Only Some Nodes
70,Networking
70,Adding entries to Pod /etc/hosts with HostAliases
70,Extend Service IP Ranges
70,Validate IPv4/IPv6 dual-stack
70,Extend kubectl with plugins
70,Manage HugePages
70,Schedule GPUs
70,Tutorials
70,Hello Minikube
70,Learn Kubernetes Basics
70,Create a Cluster
70,Using Minikube to Create a Cluster
70,Deploy an App
70,Using kubectl to Create a Deployment
70,Explore Your App
70,Viewing Pods and Nodes
70,Expose Your App Publicly
70,Using a Service to Expose Your App
70,Scale Your App
70,Running Multiple Instances of Your App
70,Update Your App
70,Performing a Rolling Update
70,Configuration
70,Example: Configuring a Java Microservice
70,"Externalizing config using MicroProfile, ConfigMaps and Secrets"
70,Configuring Redis using a ConfigMap
70,Security
70,Apply Pod Security Standards at the Cluster Level
70,Apply Pod Security Standards at the Namespace Level
70,Restrict a Container's Access to Resources with AppArmor
70,Restrict a Container's Syscalls with seccomp
70,Stateless Applications
70,Exposing an External IP Address to Access an Application in a Cluster
70,Example: Deploying PHP Guestbook application with Redis
70,Stateful Applications
70,StatefulSet Basics
70,Example: Deploying WordPress and MySQL with Persistent Volumes
70,Example: Deploying Cassandra with a StatefulSet
70,"Running ZooKeeper, A Distributed System Coordinator"
70,Services
70,Connecting Applications with Services
70,Using Source IP
70,Explore Termination Behavior for Pods And Their Endpoints
70,Reference
70,Glossary
70,API Overview
70,Kubernetes API Concepts
70,Server-Side Apply
70,Client Libraries
70,Common Expression Language in Kubernetes
70,Kubernetes Deprecation Policy
70,Deprecated API Migration Guide
70,Kubernetes API health endpoints
70,API Access Control
70,Authenticating
70,Authenticating with Bootstrap Tokens
70,Certificates and Certificate Signing Requests
70,Admission Controllers
70,Dynamic Admission Control
70,Managing Service Accounts
70,Authorization Overview
70,Using RBAC Authorization
70,Using ABAC Authorization
70,Using Node Authorization
70,Mapping PodSecurityPolicies to Pod Security Standards
70,Webhook Mode
70,Kubelet authentication/authorization
70,TLS bootstrapping
70,Validating Admission Policy
70,"Well-Known Labels, Annotations and Taints"
70,Audit Annotations
70,Kubernetes API
70,Workload Resources
70,Pod
70,PodTemplate
70,ReplicationController
70,ReplicaSet
70,Deployment
70,StatefulSet
70,ControllerRevision
70,DaemonSet
70,Job
70,CronJob
70,HorizontalPodAutoscaler
70,HorizontalPodAutoscaler
70,PriorityClass
70,PodSchedulingContext v1alpha2
70,ResourceClaim v1alpha2
70,ResourceClaimTemplate v1alpha2
70,ResourceClass v1alpha2
70,Service Resources
70,Service
70,Endpoints
70,EndpointSlice
70,Ingress
70,IngressClass
70,Config and Storage Resources
70,ConfigMap
70,Secret
70,Volume
70,PersistentVolumeClaim
70,PersistentVolume
70,StorageClass
70,VolumeAttachment
70,CSIDriver
70,CSINode
70,CSIStorageCapacity
70,Authentication Resources
70,ServiceAccount
70,TokenRequest
70,TokenReview
70,CertificateSigningRequest
70,ClusterTrustBundle v1alpha1
70,SelfSubjectReview
70,Authorization Resources
70,LocalSubjectAccessReview
70,SelfSubjectAccessReview
70,SelfSubjectRulesReview
70,SubjectAccessReview
70,ClusterRole
70,ClusterRoleBinding
70,Role
70,RoleBinding
70,Policy Resources
70,LimitRange
70,ResourceQuota
70,NetworkPolicy
70,PodDisruptionBudget
70,IPAddress v1alpha1
70,Extend Resources
70,CustomResourceDefinition
70,MutatingWebhookConfiguration
70,ValidatingWebhookConfiguration
70,ValidatingAdmissionPolicy v1beta1
70,Cluster Resources
70,Node
70,Namespace
70,Event
70,APIService
70,Lease
70,RuntimeClass
70,FlowSchema v1beta3
70,PriorityLevelConfiguration v1beta3
70,Binding
70,ComponentStatus
70,ClusterCIDR v1alpha1
70,Common Definitions
70,DeleteOptions
70,LabelSelector
70,ListMeta
70,LocalObjectReference
70,NodeSelectorRequirement
70,ObjectFieldSelector
70,ObjectMeta
70,ObjectReference
70,Patch
70,Quantity
70,ResourceFieldSelector
70,Status
70,TypedLocalObjectReference
70,Other Resources
70,ValidatingAdmissionPolicyBindingList v1beta1
70,Common Parameters
70,Instrumentation
70,Service Level Indicator Metrics
70,CRI Pod & Container Metrics
70,Node metrics data
70,Kubernetes Metrics Reference
70,Kubernetes Issues and Security
70,Kubernetes Issue Tracker
70,Kubernetes Security and Disclosure Information
70,CVE feed
70,Node Reference Information
70,Kubelet Checkpoint API
70,Articles on dockershim Removal and on Using CRI-compatible Runtimes
70,Node Labels Populated By The Kubelet
70,Kubelet Device Manager API Versions
70,Node Status
70,Networking Reference
70,Protocols for Services
70,Ports and Protocols
70,Virtual IPs and Service Proxies
70,Setup tools
70,Kubeadm
70,kubeadm init
70,kubeadm join
70,kubeadm upgrade
70,kubeadm config
70,kubeadm reset
70,kubeadm token
70,kubeadm version
70,kubeadm alpha
70,kubeadm certs
70,kubeadm init phase
70,kubeadm join phase
70,kubeadm kubeconfig
70,kubeadm reset phase
70,kubeadm upgrade phase
70,Implementation details
70,Command line tool (kubectl)
70,kubectl Quick Reference
70,kubectl reference
70,kubectl
70,kubectl annotate
70,kubectl api-resources
70,kubectl api-versions
70,kubectl apply
70,kubectl apply edit-last-applied
70,kubectl apply set-last-applied
70,kubectl apply view-last-applied
70,kubectl attach
70,kubectl auth
70,kubectl auth can-i
70,kubectl auth reconcile
70,kubectl auth whoami
70,kubectl autoscale
70,kubectl certificate
70,kubectl certificate approve
70,kubectl certificate deny
70,kubectl cluster-info
70,kubectl cluster-info dump
70,kubectl completion
70,kubectl config
70,kubectl config current-context
70,kubectl config delete-cluster
70,kubectl config delete-context
70,kubectl config delete-user
70,kubectl config get-clusters
70,kubectl config get-contexts
70,kubectl config get-users
70,kubectl config rename-context
70,kubectl config set
70,kubectl config set-cluster
70,kubectl config set-context
70,kubectl config set-credentials
70,kubectl config unset
70,kubectl config use-context
70,kubectl config view
70,kubectl cordon
70,kubectl cp
70,kubectl create
70,kubectl create clusterrole
70,kubectl create clusterrolebinding
70,kubectl create configmap
70,kubectl create cronjob
70,kubectl create deployment
70,kubectl create ingress
70,kubectl create job
70,kubectl create namespace
70,kubectl create poddisruptionbudget
70,kubectl create priorityclass
70,kubectl create quota
70,kubectl create role
70,kubectl create rolebinding
70,kubectl create secret
70,kubectl create secret docker-registry
70,kubectl create secret generic
70,kubectl create secret tls
70,kubectl create service
70,kubectl create service clusterip
70,kubectl create service externalname
70,kubectl create service loadbalancer
70,kubectl create service nodeport
70,kubectl create serviceaccount
70,kubectl create token
70,kubectl debug
70,kubectl delete
70,kubectl describe
70,kubectl diff
70,kubectl drain
70,kubectl edit
70,kubectl events
70,kubectl exec
70,kubectl explain
70,kubectl expose
70,kubectl get
70,kubectl kustomize
70,kubectl label
70,kubectl logs
70,kubectl options
70,kubectl patch
70,kubectl plugin
70,kubectl plugin list
70,kubectl port-forward
70,kubectl proxy
70,kubectl replace
70,kubectl rollout
70,kubectl rollout history
70,kubectl rollout pause
70,kubectl rollout restart
70,kubectl rollout resume
70,kubectl rollout status
70,kubectl rollout undo
70,kubectl run
70,kubectl scale
70,kubectl set
70,kubectl set env
70,kubectl set image
70,kubectl set resources
70,kubectl set selector
70,kubectl set serviceaccount
70,kubectl set subject
70,kubectl taint
70,kubectl top
70,kubectl top node
70,kubectl top pod
70,kubectl uncordon
70,kubectl version
70,kubectl wait
70,kubectl Commands
70,kubectl
70,JSONPath Support
70,kubectl for Docker Users
70,kubectl Usage Conventions
70,Component tools
70,Feature Gates
70,Feature Gates (removed)
70,kubelet
70,kube-apiserver
70,kube-controller-manager
70,kube-proxy
70,kube-scheduler
70,Debug cluster
70,Flow control
70,Configuration APIs
70,Client Authentication (v1)
70,Client Authentication (v1beta1)
70,Event Rate Limit Configuration (v1alpha1)
70,Image Policy API (v1alpha1)
70,kube-apiserver Admission (v1)
70,kube-apiserver Audit Configuration (v1)
70,kube-apiserver Configuration (v1)
70,kube-apiserver Configuration (v1alpha1)
70,kube-apiserver Configuration (v1beta1)
70,kube-apiserver Encryption Configuration (v1)
70,kube-controller-manager Configuration (v1alpha1)
70,kube-proxy Configuration (v1alpha1)
70,kube-scheduler Configuration (v1)
70,kubeadm Configuration (v1beta3)
70,kubeadm Configuration (v1beta4)
70,kubeconfig (v1)
70,Kubelet Configuration (v1)
70,Kubelet Configuration (v1alpha1)
70,Kubelet Configuration (v1beta1)
70,Kubelet CredentialProvider (v1)
70,WebhookAdmission Configuration (v1)
70,External APIs
70,Kubernetes Custom Metrics (v1beta2)
70,Kubernetes External Metrics (v1beta1)
70,Kubernetes Metrics (v1beta1)
70,Scheduling
70,Scheduler Configuration
70,Scheduling Policies
70,Other Tools
70,Mapping from dockercli to crictl
70,Contribute
70,Contribute to Kubernetes Documentation
70,Suggesting content improvements
70,Contributing new content
70,Opening a pull request
70,Documenting for a release
70,Blogs and case studies
70,Reviewing changes
70,Reviewing pull requests
70,For approvers and reviewers
70,Localizing Kubernetes documentation
70,Participating in SIG Docs
70,Roles and responsibilities
70,Issue Wranglers
70,PR wranglers
70,Documentation style overview
70,Content guide
70,Style guide
70,Diagram guide
70,Writing a new topic
70,Page content types
70,Content organization
70,Custom Hugo Shortcodes
70,Updating Reference Documentation
70,Quickstart
70,Contributing to the Upstream Kubernetes Code
70,Generating Reference Documentation for the Kubernetes API
70,Generating Reference Documentation for kubectl Commands
70,Generating Reference Documentation for Metrics
70,Generating Reference Pages for Kubernetes Components and Tools
70,Advanced contributing
70,Viewing Site Analytics
70,Docs smoke test pageKubernetes DocumentationTutorialsStateful ApplicationsExample: Deploying WordPress and MySQL with Persistent VolumesExample: Deploying WordPress and MySQL with Persistent VolumesThis tutorial shows you how to deploy a WordPress site and a MySQL database using
70,Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.A PersistentVolume (PV) is a piece
70,"of storage in the cluster that has been manually provisioned by an administrator,"
70,or dynamically provisioned by Kubernetes using a StorageClass.
70,A PersistentVolumeClaim (PVC)
70,is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and
70,PersistentVolumeClaims are independent from Pod lifecycles and preserve data through
70,"restarting, rescheduling, and even deleting Pods.Warning: This deployment is not suitable for production use cases, as it uses single instance"
70,WordPress and MySQL Pods. Consider using
70,WordPress Helm Chart
70,to deploy WordPress in production.Note: The files provided in this tutorial are using GA Deployment APIs and are specific
70,to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier
70,"version of Kubernetes, please update the API version appropriately, or reference"
70,"earlier versions of this tutorial.ObjectivesCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yaml witha Secret generatorMySQL resource configsWordPress resource configsApply the kustomization directory by kubectl apply -k ./Clean upBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must"
70,be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
70,"cluster, you can create one by using"
70,minikube
70,"or you can use one of these Kubernetes playgrounds:KillercodaPlay with KubernetesTo check the version, enter kubectl version.The example shown on this page works with kubectl 1.27 and above.Download the following configuration files:mysql-deployment.yamlwordpress-deployment.yamlCreate PersistentVolumeClaims and PersistentVolumesMySQL and Wordpress each require a PersistentVolume to store data."
70,Their PersistentVolumeClaims will be created at the deployment step.Many cluster environments have a default StorageClass installed.
70,"When a StorageClass is not specified in the PersistentVolumeClaim,"
70,"the cluster's default StorageClass is used instead.When a PersistentVolumeClaim is created, a PersistentVolume is dynamically"
70,"provisioned based on the StorageClass configuration.Warning: In local clusters, the default StorageClass uses the hostPath provisioner."
70,hostPath volumes are only suitable for development and testing. With hostPath
70,"volumes, your data lives in /tmp on the node the Pod is scheduled onto and does"
70,not move between nodes. If a Pod dies and gets scheduled to another node in the
70,"cluster, or the node is rebooted, the data is lost.Note: If you are bringing up a cluster that needs to use the hostPath provisioner,"
70,"the --enable-hostpath-provisioner flag must be set in the controller-manager component.Note: If you have a Kubernetes cluster running on Google Kubernetes Engine, please"
70,follow this guide.Create a kustomization.yamlAdd a Secret generatorA Secret is an object that stores a piece
70,"of sensitive data like a password or key. Since 1.14, kubectl supports the"
70,management of Kubernetes objects using a kustomization file. You can create a Secret
70,by generators in kustomization.yaml.Add a Secret generator in kustomization.yaml from the following command.
70,You will need to replace YOUR_PASSWORD with the password you want to use.cat <<EOF >./kustomization.yaml
70,secretGenerator:
70,- name: mysql-pass
70,literals:
70,- password=YOUR_PASSWORD
70,EOF
70,Add resource configs for MySQL and WordPressThe following manifest describes a single-instance MySQL Deployment. The MySQL
70,container mounts the PersistentVolume at /var/lib/mysql. The MYSQL_ROOT_PASSWORD
70,environment variable sets the database password from the Secret.application/wordpress/mysql-deployment.yaml
70,apiVersion: v1
70,kind: Service
70,metadata:
70,name: wordpress-mysql
70,labels:
70,app: wordpress
70,spec:
70,ports:
70,- port: 3306
70,selector:
70,app: wordpress
70,tier: mysql
70,clusterIP: None
70,---
70,apiVersion: v1
70,kind: PersistentVolumeClaim
70,metadata:
70,name: mysql-pv-claim
70,labels:
70,app: wordpress
70,spec:
70,accessModes:
70,- ReadWriteOnce
70,resources:
70,requests:
70,storage: 20Gi
70,---
70,apiVersion: apps/v1
70,kind: Deployment
70,metadata:
70,name: wordpress-mysql
70,labels:
70,app: wordpress
70,spec:
70,selector:
70,matchLabels:
70,app: wordpress
70,tier: mysql
70,strategy:
70,type: Recreate
70,template:
70,metadata:
70,labels:
70,app: wordpress
70,tier: mysql
70,spec:
70,containers:
70,- image: mysql:8.0
70,name: mysql
70,env:
70,- name: MYSQL_ROOT_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,- name: MYSQL_DATABASE
70,value: wordpress
70,- name: MYSQL_USER
70,value: wordpress
70,- name: MYSQL_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,ports:
70,- containerPort: 3306
70,name: mysql
70,volumeMounts:
70,- name: mysql-persistent-storage
70,mountPath: /var/lib/mysql
70,volumes:
70,- name: mysql-persistent-storage
70,persistentVolumeClaim:
70,claimName: mysql-pv-claim
70,The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the
70,PersistentVolume at /var/www/html for website data files. The WORDPRESS_DB_HOST environment variable sets
70,"the name of the MySQL Service defined above, and WordPress will access the database by Service. The"
70,WORDPRESS_DB_PASSWORD environment variable sets the database password from the Secret kustomize generated.application/wordpress/wordpress-deployment.yaml
70,apiVersion: v1
70,kind: Service
70,metadata:
70,name: wordpress
70,labels:
70,app: wordpress
70,spec:
70,ports:
70,- port: 80
70,selector:
70,app: wordpress
70,tier: frontend
70,type: LoadBalancer
70,---
70,apiVersion: v1
70,kind: PersistentVolumeClaim
70,metadata:
70,name: wp-pv-claim
70,labels:
70,app: wordpress
70,spec:
70,accessModes:
70,- ReadWriteOnce
70,resources:
70,requests:
70,storage: 20Gi
70,---
70,apiVersion: apps/v1
70,kind: Deployment
70,metadata:
70,name: wordpress
70,labels:
70,app: wordpress
70,spec:
70,selector:
70,matchLabels:
70,app: wordpress
70,tier: frontend
70,strategy:
70,type: Recreate
70,template:
70,metadata:
70,labels:
70,app: wordpress
70,tier: frontend
70,spec:
70,containers:
70,- image: wordpress:6.2.1-apache
70,name: wordpress
70,env:
70,- name: WORDPRESS_DB_HOST
70,value: wordpress-mysql
70,- name: WORDPRESS_DB_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,- name: WORDPRESS_DB_USER
70,value: wordpress
70,ports:
70,- containerPort: 80
70,name: wordpress
70,volumeMounts:
70,- name: wordpress-persistent-storage
70,mountPath: /var/www/html
70,volumes:
70,- name: wordpress-persistent-storage
70,persistentVolumeClaim:
70,claimName: wp-pv-claim
70,Download the MySQL deployment configuration file.curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
70,Download the WordPress configuration file.curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
70,Add them to kustomization.yaml file.cat <<EOF >>./kustomization.yaml
70,resources:
70,- mysql-deployment.yaml
70,- wordpress-deployment.yaml
70,EOF
70,Apply and VerifyThe kustomization.yaml contains all the resources for deploying a WordPress site and a
70,MySQL database. You can apply the directory bykubectl apply -k ./
70,Now you can verify that all objects exist.Verify that the Secret exists by running the following command:kubectl get secrets
70,The response should be like this:NAME
70,TYPE
70,DATA
70,AGE
70,mysql-pass-c57bb4t7mf
70,Opaque
70,Verify that a PersistentVolume got dynamically provisioned.kubectl get pvc
70,Note: It can take up to a few minutes for the PVs to be provisioned and bound.The response should be like this:NAME
70,STATUS
70,VOLUME
70,CAPACITY
70,ACCESS MODES
70,STORAGECLASS
70,AGE
70,mysql-pv-claim
70,Bound
70,pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002
70,20Gi
70,RWO
70,standard
70,77s
70,wp-pv-claim
70,Bound
70,pvc-8cd0df54-4044-11e9-b2bb-42010a800002
70,20Gi
70,RWO
70,standard
70,77s
70,Verify that the Pod is running by running the following command:kubectl get pods
70,Note: It can take up to a few minutes for the Pod's Status to be RUNNING.The response should be like this:NAME
70,READY
70,STATUS
70,RESTARTS
70,AGE
70,wordpress-mysql-1894417608-x5dzt
70,1/1
70,Running
70,40s
70,Verify that the Service is running by running the following command:kubectl get services wordpress
70,The response should be like this:NAME
70,TYPE
70,CLUSTER-IP
70,EXTERNAL-IP
70,PORT(S)
70,AGE
70,wordpress
70,LoadBalancer
70,10.0.0.89
70,<pending>
70,80:32406/TCP
70,Note: Minikube can only expose Services through NodePort. The EXTERNAL-IP is always pending.Run the following command to get the IP Address for the WordPress Service:minikube service wordpress --url
70,The response should be like this:http://1.2.3.4:32406
70,"Copy the IP address, and load the page in your browser to view your site.You should see the WordPress set up page similar to the following screenshot.Warning: Do not leave your WordPress installation on this page. If another user finds it,"
70,"they can set up a website on your instance and use it to serve malicious content.Either install WordPress by creating a username and password or delete your instance.Cleaning upRun the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:kubectl delete -k ./"
70,What's nextLearn more about Introspection and DebuggingLearn more about JobsLearn more about Port ForwardingLearn how to Get a Shell to a ContainerFeedbackWas this page helpful?Yes
70,"NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on"
70,Stack Overflow.
70,Open an issue in the GitHub Repository if you want to
70,report a problem
70,"suggest an improvement.Last modified August 24, 2023 at 6:38 PM PST: Use code_sample shortcode instead of code shortcode (e8b136c3b3) Edit this page"
70,Create child page
70,Create an issue
70,Print entire sectionObjectivesBefore you beginCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yamlAdd a Secret generatorAdd resource configs for MySQL and WordPressApply and VerifyCleaning upWhat's nextDocumentation
70,Blog
70,Training
70,Partners
70,Community
70,"Case Studies© 2024 The Kubernetes Authors | Documentation Distributed under CC BY 4.0Copyright © 2024 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage pageICP license: 京ICP备17074266号-3"
71,JBoss Performance Tuning - 5 Best Practices | eG Innovations
71,Partners
71,Become a partner
71,Channel partners
71,MSP Partners
71,Technology partners
71,Register opportunity
71,Documentation
71,Blog
71,Careers
71,Contact Us
71,English
71,Deutsche
71,Dutch
71,Espanol
71,Français
71,Portuguese
71,한국어
71,日本語
71,Products
71,Product Overview
71,How It Works Monitoring as SaaS Supported Technologies Why eG Enterprise? What's New All Features
71,APM
71,Digital Workspace Monitoring
71,Cloud/Hybrid Cloud Monitoring
71,Digital Experience Monitoring
71,IT Infrastructure Monitoring
71,Enterprise Application Monitoring
71,Solutions
71,By Technology Digital Workspace Monitoring Application Monitoring Cloud Monitoring Container Monitoring Virtualization Monitoring Web Server Monitoring App Server Monitoring Database Monitoring SaaS/Enterprise Monitoring Infrastructure Monitoring Server Monitoring By Vendor By Industry
71,Full Stack Observability
71,"Single Console for Applications & Infrastructure. eG Enterprise is an end-to-end IT performance monitoring solution that supports over 500+ different technologies. It consolidates your disparate, siloed monitoring tools into a single pane of glass to get you to the root-cause of performance problems quicker. Role-based access control means everyone on the team can monitor their area of responsibility and create customized dashboards unique to their requirements. When problems arise, eG Enterprise has already mapped your interdependencies and automatically correlates application and infrastructure performance to pin-point the root-cause."
71,End-to-End Monitoring for Digital WorkSpaces
71,Deliver the ultimate end-user experience by proactively addressing virtual application and desktop performance issues before your employees notice.
71,Citrix Monitoring
71,Citrix Cloud Monitoring
71,VMware Horizon Monitoring
71,AVD Monitoring
71,Amazon WorkSpaces Monitoring
71,Amazon AppStream Monitoring
71,Microsoft RDS Monitoring
71,IGEL Monitoring
71,VDI Monitoring
71,Nerdio Integration
71,Trace User Accesses End-to-End Optimize Application Performance
71,"Deliver high performance applications that are deployed on the cloud, on-premises or in hybrid, multi-cloud environments."
71,Java APM
71,.NET APM
71,PHP APM
71,Node.js APM
71,Single Pane of Glass for All Public Cloud Services
71,Consolidate your public cloud monitoring into a single dashboard and correlate the root-cause of problems even for hybrid or multi-cloud deployments.
71,AWS Monitoring
71,Azure Monitoring
71,Alibaba Monitoring
71,Full Stack Observability for Containerized Environments
71,"Full visibility into orchestration, worker nodes, containers, application running on them and the underlying infrastructure."
71,Red Hat OpenShift Monitoring
71,Kubernetes Monitoring
71,Docker Monitoring
71,Get 360° Visibilty of Virtual Machine Performance Virtual Machine Visibility
71,Unified console for monitoring virtual platforms which analyzes virtualization performance within the context of the business services that it supports.
71,VMware ESX Monitoring
71,Hyper-V Monitoring
71,Nutanix AHV Monitoring
71,RHEV Monitoring
71,Solaris LDoms Monitoring
71,Oracle VM Monitoring
71,Citrix Hypervisor Monitoring
71,High Performance Web Server Monitoring
71,"Track performance and usage by web sites and transactions, error URLs, traffic, queues, slow requests and more in a single console."
71,Microsoft IIS Monitoring
71,Apache Monitoring
71,Nginx Monitoring
71,Full Stack Visibility into Web Application Server Performance
71,"Monitor all aspects of application server performance: JVM, web containers, application transactions and more."
71,Tomcat Monitoring
71,WebLogic Monitoring
71,WebSphere Monitoring
71,JBoss Monitoring
71,SAP NetWeaver AS Monitoring
71,JEUS Monitoring
71,Unified and Integrated Server Monitoring
71,Track all key performance indicators of server performance from a central web console and get proactive alerts.
71,Windows Server Monitoring
71,Linux Server Monitoring
71,Solaris Server monitoring
71,HP-UX Server Monitoring
71,IBM AIX Server Monitoring
71,Server Hardware Monitoring
71,In-Depth Database Performance Monitoring and Insights
71,"Get visibility into all aspects of database performance - workload, configuration, memory buffers, I/O operations, queries, and deadlocks."
71,SQL Server Monitoring
71,Oracle Database Monitoring
71,MySQL Monitoring
71,Sybase Monitoring
71,MongoDB Monitoring
71,MariaDB Monitoring
71,PostgreSQL Monitoring
71,SAP HANA Monitoring
71,IBM DB2 Monitoring
71,Azure SQL Monitoring
71,Snowflake Monitoring
71,Redis Monitoring
71,Aurora Database Monitoring
71,DynamoDB Monitoring
71,"Insights into Every Layer, Every Tier of your IT Infrastructure"
71,"Monitor network, server, storage, cloud, containers and more. AIOps-powered insights make monitoring and diagnosis easy."
71,Active Directory Monitoring
71,Azure AD Monitoring
71,Network Monitoring
71,Storage Monitoring
71,Citrix ADC Monitoring
71,Message Queue Monitoring
71,Customized Monitoring of Enterprise Applications and SaaS
71,Get in-depth insights and proactively monitor and troubleshoot a wide spectrum of enterprise applications and SaaS services.
71,Office 365 Monitoring
71,SharePoint Monitoring
71,Microsoft Exchange Monitoring
71,SAP Monitoring
71,PeopleSoft Monitoring
71,AllScripts Monitoring
71,Moodle Monitoring
71,Cerner Monitoring
71,Vendor-specific Monitoring Solutions
71,Microsoft Monitoring
71,Red Hat Monitoring
71,Industry Solutions
71,eG Innovations offers specialized IT performance monitoring for a range of industries to help IT teams deliver what their businesses expect of them.
71,Healthcare
71,Education
71,Government
71,Banking & Finance
71,Credit Unions
71,Manufacturing
71,Retail
71,MSPs
71,Solutions
71,By Technology
71,Digital Workspace Monitoring
71,Citrix Monitoring Citrix Cloud Monitoring
71,VMware Horizon Monitoring AVD Monitoring Amazon WorkSpaces Monitoring Amazon AppStream Monitoring Microsoft RDS Monitoring IGEL Monitoring VDI Monitoring Nerdio Integration Application Monitoring Java APM .NET APM PHP APM Node.js APM
71,Cloud Monitoring AWS Monitoring Azure Monitoring Alibaba Monitoring Container Monitoring Red Hat OpenShift Monitoring Kubernetes Monitoring Docker Monitoring Virtualization Monitoring VMware ESX Monitoring Hyper-V Monitoring Nutanix AHV Monitoring RHEV Monitoring Solaris LDoms Monitoring Oracle VM Monitoring Citrix Hypervisor Monitoring Web Server Monitoring Microsoft IIS Monitoring Apache Monitoring Nginx Monitoring App Server Monitoring Tomcat Monitoring WebLogic Monitoring WebSphere Monitoring JBoss Monitoring SAP NetWeaver AS Monitoring JEUS Monitoring
71,Database Monitoring SQL Server Monitoring Oracle Database Monitoring
71,MySQL Monitoring Sybase Monitoring MongoDB Monitoring MariaDB Monitoring PostgreSQL Monitoring SAP HANA Monitoring IBM DB2 Monitoring Azure SQL Monitoring Snowflake Monitoring Redis Monitoring Aurora Database Monitoring DynamoDB Monitoring SaaS/Enterprise Monitoring Office 365 Monitoring SharePoint Monitoring Microsoft Exchange Monitoring SAP Monitoring PeopleSoft Monitoring AllScripts Monitoring Moodle Monitoring Cerner Monitoring
71,Infrastructure Monitoring Active Directory Monitoring Azure AD Monitoring Network Monitoring Storage Monitoring Citrix ADC Monitoring Message Queue Monitoring
71,Server Monitoring Windows Server Monitoring Linux Server Monitoring Solaris Server Monitoring HP-UX Server Monitoring IBM AIX Server Monitoring Server Hardware Monitoring
71,By Vendor
71,Microsoft Monitoring Red Hat Monitoring
71,By Industry
71,Healthcare Education
71,Government Banking/Finance Credit Unions Retail Manufacturing
71,MSPs
71,Pricing
71,Resources
71,Solution Briefs Case Studies White Papers Expert Reviews E-Books Webinars Demos
71,Videos
71,Infographics Glossary
71,About
71,About Us Customers Product Support Press Releases Events Awards
71,Partners
71,Become a Partner Channel Partners MSP Partners Technology Partners Register Opportunity
71,Documentation
71,Blog
71,Careers
71,Contact Us
71,Free Trial
71,Free Trial
71,Toggle Menu
71,JBoss Performance Tuning – 5 Best Practices
71,Arun Aravamudhan
71,"Published on: May 18, 2023Last updated on: July 18, 2023"
71,IN THIS BLOG POST
71,What is JBoss?Top tips for JBoss performance tuningTune Garbage Collection (GC) for better JBoss performanceOptimize resource allocation for JBoss-based applicationsConnection pool monitoring for JBossOptimize JBoss clustering with caching to improve Java performanceOptimize database access when implementing JBoss-based applicationsOther best practices for improving JBoss application performance include:Learn more:
71,Blog
71,JBoss Performance Tuning – 5 Best Practices
71,"A collection of 5 top tips for JBoss performance tuning to ensure optimal application performance and resilience. Learn how to monitor and troubleshoot JBoss systems to eliminate bottlenecks, reduce costs and minimize user issues. What is JBoss? JBoss is an open source, standards-compliant, J2EE application server implemented in 100% pure Java. There are many variants of JBoss available today:"
71,JBoss EAP is the name for the Java EE application server that Red Hat produces and supports. JBoss AS and WildFly are the community versions of the JBoss application server.
71,"While Apache Tomcat used to be the underlying engine for JBoss EAP and AS, more recently, Undertow is used as the application server engine. Top tips for JBoss performance tuning Today’s blog is a quick overview of 5 areas, which are those most likely to be worth your while scrutinizing and optimizing. It should be noted that simply doing performance tuning on JBoss alone is not enough to produce a high-performance application. It is just one of the steps that you need to follow. A more detailed guide on optimizing Java applications within the context of JBoss usage including configuration settings is also available, see: JBoss Performance Tuning & Monitoring | eG Innovations. I’ll also cover what capabilities the eG Enterprise application performance monitoring and observability solution offers out-of-the-box for JBoss-based applications. Tools such as eG Enterprise are commonly used for JBoss monitoring to automate alerting without the need for scripting or coding and because they cover interdependencies and the whole stack from Java code to server CPU and network. 1. Tune Garbage Collection (GC) for better JBoss performance Problem:"
71,"The performance of Java applications running on JBoss can be impacted by inefficient garbage collection (GC) processes. Garbage collection is a critical process that manages memory in Java applications by reclaiming memory occupied by objects that are no longer in use. However, inefficient GC settings or configurations can lead to increased CPU usage, longer pause times, and degraded overall application performance."
71,Solution:
71,"Tune garbage collection (GC) settings in JBoss to optimize memory management and improve application performance. Properly configuring and tuning GC parameters such as heap size, garbage collection algorithm, and GC tuning options can help minimize pause times, reduce CPU usage, and optimize memory usage. This can result in improved response times, reduced resource consumption, and enhanced overall application performance."
71,"How can eG Enterprise help? eG Enterprise provides real-time monitoring and analysis of GC-related metrics, such as GC Pause Times, GC Throughput, Heap Utilization, GC Events, and Memory Usage. It offers JBoss memory analysis capabilities, such as automatic heap dump analysis and GC performance analysis, to identify bottlenecks, inefficiencies, and tuning opportunities in GC processes. This helps in optimizing GC configuration and achieving optimal GC performance. eG Enterprise also offers performance comparison reports for different garbage collection algorithms such as CMS vs G1 vs ZGC. eG Enterprise enables proactive alerting on GC-related issues in JBoss, such as high GC pause times or heap usage, allowing SREs and IT Operations to quickly identify and resolve problems before they impact application performance. It also provides monitoring capabilities for test and development environments, allowing for thorough testing and fine-tuning of GC configuration before applying changes to production environments. Learn all about Garbage collection in Java: What is Java Garbage Collection | eG Innovations 2. Optimize resource allocation for JBoss-based applications Problem:"
71,"Inadequate allocation of system resources, such as CPU, memory, disk, and network, in JBoss-based applications can result in performance degradation, slow response times, and potential application failures."
71,Solution:
71,"Monitor resource utilization: Use eG Enterprise performance monitoring tools to regularly monitor resource utilization, such as CPU usage, memory usage, disk I/O, and network traffic, in the JBoss environment to identify any resource bottlenecks or inadequacies. Optimize resource allocation: Analyze resource utilization patterns and identify any inefficiencies or imbalances in resource allocation. Optimize resource allocation settings, such as CPU affinity, memory allocation, and disk caching, in the JBoss environment to achieve optimal resource utilization and prevent resource-related performance issues. Scale resources dynamically: Implement dynamic resource allocation techniques, such as auto-scaling, in the JBoss environment to dynamically allocate or deallocate resources based on the application’s workload demands, to ensure optimal performance during peak or fluctuating workload periods. Plan for future growth: Anticipate future growth and allocate resources accordingly in the JBoss environment to ensure scalability and performance of the application as the workload increases over time."
71,How can eG Enterprise help? eG Enterprise can help with:
71,"Real-time monitoring of resource utilization metrics, such as CPU usage, memory usage, disk I/O, and network traffic, in the JBoss environment to provide insights into resource utilization patterns and detect resource bottlenecks or inadequacies. Proactive alerts and notifications: eG Enterprise can generate proactive alerts and notifications based on predefined thresholds or anomalies in resource utilization, allowing IT administrators to take proactive measures to allocate sufficient resources and prevent resource-related performance issues in JBoss-based applications. Automated resource allocation: eG Enterprise provides automation capabilities to dynamically allocate or deallocate resources based on workload demands, allowing for optimal resource allocation in JBoss environments. Performance optimization recommendations: eG Enterprise can provide performance optimization recommendations, such as CPU affinity, memory allocation, and disk caching settings, based on resource utilization patterns and best practices, to optimize resource allocation in JBoss environments."
71,"Capacity planning: eG Enterprise simplifies capacity planning by sorting hosts (both physical and virtual) based on metrics such as CPU, memory, disk and network. JBoss administrators can categorize hosts, compare relevant metrics, and analyze resource allocation. If JBoss deployments are virtualized, you can save money by optimizing host capacity through actions like automatic scaling of VM instances. However, overloaded hosts can lead to poor performance, longer response times, and lost revenue. On the other hand, virtual hosts may underutilize allocated resources while their physical hosts have ample available capacity. eG Enterprise can identify these imbalances and alert you to take corrective measures."
71,"3. Connection pool monitoring for JBoss Problem: Monitoring database connection pool performance and identifying issues such as database connection leaks in JBoss-based applications can be challenging without proper visibility into connection pool metrics and usage. If the connection pool is exhausted or not configured properly, requests to the application may take longer than usual to complete. This can result in slow response times and a poor user experience. And if connections are not released back to the pool properly, they may be held open indefinitely. This can cause the pool to become exhausted, resulting in slow response times, application crashes, or data inconsistencies. Solution: There are many metrics you should proactively monitor for JBoss connections such as ActiveCount, AvailableCount, MaxUsedCount which are available via the native CLI, API and HTTP API. When using a connection pool, you need to balance the costs of keeping connections open vs. opening/closing new connections. Ideally you will want to size the connection pool such that the number of idle connections is minimized, but also optimize the frequency at which you open/close new connections. You will also need to ensure that the maximum number of connections is appropriate, as this will limit the maximum capacity throughput that a database can achieve. If monitoring reveals problems with connection pool performance or connection availability, you may need to tune the connection pool. Some advice on how to configure and tune your connection pools is given in JBoss Performance Tuning & Monitoring | eG Innovations. How can eG Enterprise help? eG Enterprise for JBoss enables monitoring of connection pool performance metrics out-of-the-box, such as connection pool size, active connections, idle connections, and connection usage statistics. This helps identify potential bottlenecks or performance issues related to connection pooling. eG Enterprise can also detect and alert on connection leaks, which can occur when connections are not properly closed after usage, leading to resource exhaustion and potential application failures. 4. Optimize JBoss clustering with caching to improve Java performance Problem:"
71,The performance of critical Java functions in JBoss could be adversely impacted by the high load on the database. This could result in slow response times and degraded overall application performance.
71,Solution:
71,"Implement application and web caching as part of JBoss Clustering to reduce the load on critical Java methods and functions. By caching frequently accessed data in memory, JBoss Clustering reduces the need to repeatedly fetch data from the database, resulting in faster response times and improved overall application performance. However, be sure to tune the cache. Performance issues such as cache misses, high turnaround, and memory exhaustion can impact your application negatively."
71,How can eG Enterprise help? eG Enterprise can help with:
71,"Real-time monitoring of caching-related metrics, such as:"
71,"Cache Hits: The number of times data was successfully retrieved from the cache without needing to fetch from the underlying data source. Cache Misses: The number of times data was requested from the cache but was not found, resulting in a fetch from the underlying data source. Cache Evictions: The number of times data was removed from the cache to free up space for new data. Cache Put Operations: The number of times data was added or updated in the cache. Cache Hit Ratio: The ratio of cache hits to the total number of cache accesses (hits + misses), expressed as a percentage. A higher hit ratio indicates a more effective cache. Average Time for Cache Access: The average time taken for a cache access operation, including both cache hits and misses. This metric can provide insights into the efficiency of cache retrieval operations. Cache Size: The current size of the cache, indicating the amount of data stored in the cache at any given time."
71,"Alerting and troubleshooting: eG Enterprise provides alerting capabilities to detect any abnormalities or issues with the JBoss caching mechanism and ensure timely troubleshooting and resolution of cache-related performance issues. Learn about eG Innovations’ AIOps automated alerting and anomaly detection capabilities, see: Proactive IT Alerts & Alert Monitoring | eG Innovations."
71,5. Optimize database access when implementing JBoss-based applications Problem:
71,"Inefficient database access patterns and queries in JBoss-based applications can result in unnecessary database roundtrips, increased database overhead, and degraded performance."
71,Solution:
71,Review and Optimize Database Access: Analyze the application’s database access patterns and queries to identify any inefficient or redundant queries that result in unnecessary database roundtrips. Optimize database access patterns and queries to minimize database overhead and improve overall application performance.
71,How can eG Enterprise help?
71,"Monitor database performance: eG Enterprise monitors database performance metrics, such as query execution times, database CPU utilization, database I/O, and database connection pooling, to identify any potential performance bottlenecks related to database access. Alerts and notifications: eG Enterprise can trigger alerts and notifications based on predefined thresholds for database performance metrics, allowing proactive identification and resolution of database-related performance issues before they impact application performance. Visualize database performance: eG Enterprise provides graphical dashboards and reports that visualize database performance metrics, making it easy to identify inefficient database access patterns, redundant queries, and other areas that need optimization. End-to-end monitoring: eG Enterprise provides end-to-end monitoring of the entire JBoss application stack, including the underlying infrastructure, application servers, web servers, and databases, allowing comprehensive performance analysis and optimization of database access within the larger application context. Database-specific monitoring: eG Enterprise supports monitoring of various databases, including Oracle, Microsoft SQL Server, MySQL, PostgreSQL, IBM DB2, and others, providing database-specific insights and recommendations for optimizing database access in JBoss applications. Real-time and historical monitoring: eG Enterprise provides real-time and historical monitoring capabilities, allowing analysis of both current and historical database performance trends, patterns, and anomalies, and enabling proactive identification and resolution of database-related performance issues."
71,"eG Enterprise is an Observability solution for Modern IT. Monitor digital workspaces, web applications, SaaS services, cloud and containers from a single pane of glass."
71,Free Trial
71,See the platform
71,Other best practices for improving JBoss application performance include:
71,"Optimize Logging: Avoid excessive logging, configure logging levels appropriately, and log only what is necessary for troubleshooting and monitoring. Enable Compression: Enable compression for web content to reduce the amount of data transmitted over the network and improve response times. Apply Security Best Practices: Follow best practices for securing the JBoss environment, such as keeping it up to date with security patches, implementing proper access controls, and securing communication channels. Implement Load Balancing: Use load balancing techniques, such as JBoss clustering or a dedicated load balancer, to distribute application workload across multiple servers for improved performance and scalability."
71,"eG Enterprise is an Observability solution for Modern IT. Monitor digital workspaces, web applications, SaaS services, cloud and containers from a single pane of glass."
71,Free Trial
71,See the platform
71,Learn more:
71,"Learn all about JBoss Monitoring Find out about JBoss Application Server Tuning, see: JBoss Performance Tuning & Monitoring | eG Innovations As JBoss runs on top of a JVM you may like to explore: JVM Monitoring Tools – Threads, GC, Memory Leaks & more (eginnovations.com)"
71,"About the Author Arun is Head of Products, Container & Cloud Performance Monitoring at eG Innovations. Over a 20+ year career, Arun has worked in roles including development, architecture and ops across multiple verticals such as banking, e-commerce and telco. An early adopter of APM products since the mid 2000s, his focus has predominantly been on performance tuning and monitoring of large-scale distributed applications."
71,You may also like
71,Is your Java Observability tool Lambda Expressions aware? by Arun Aravamudhan
71,Troubleshooting Spring Boot Microservices – A Real World Case Study by Pandian Ramaiah
71,What is Garbage Collection in Java: Detailed Guide by Arun Aravamudhan
71,Related Blogs
71,Java Monitoring
71,Demystifying Java Lambda expressions
71,by Arun Aravamudhan
71,"February 29, 2024"
71,eG Enterprise
71,Detecting powershell exploitation
71,by Babu Sundaram
71,"December 19, 2023"
71,Java Monitoring
71,Is your Java observability tool Lambda expressions aware?
71,by Arun Aravamudhan
71,"December 6, 2023"
71,"Discover, Diagnose, and Resolve IT Performance"
71,Issues with Full Stack Observability
71,Get Started
71,PRODUCT How It Works Key Features SaaS Deployment Supported Technologies Pricing Benefits
71,Why choose eG New! eG Enterprise v7
71,Solutions Citrix Monitoring VMware Horizon Monitoring Azure Virtual Desktop Monitoring AWS Workspaces Monitoring AWS Cloud Monitoring Azure Monitoring Java Application Monitoring .NET Monitoring SAP Monitoring VMware Hypervisor Monitoring Network Monitoring
71,Resources Demos Webinars White Papers Case Studies Expert Reviews Alternatives Media Kit Glossary
71,Company About Us Customers Partners Support Documentation Press Releases Awards Careers Blog Events Contact Us>
71,"eG Innovations, Inc., 33 Wood Ave. South, Suite 600, Iselin, NJ 08830, USA Phone: +1 (866) 526 6700 eG Innovations B.V., WTC, Den Haag, Prinses Margrietplantsoen 33,"
71,"2595 AM Den Haag, The Netherlands Phone: +31 (0)70-2055210"
71,© 2024 eG Innovations. All rights reserved.
71,Privacy Policy  |  Terms of Use
73,Distributed Mode | JuiceFS Document Center
73,"Skip to main contentHomeProductsCommunity EditionCloud ServiceSolutionsBy Use CaseBig DataAIKubernetesBy IndustryQuantitative TradingSelf-DrivingCommunityDocsCommunity Editon DocsCloud Service DocsJuiceFS CSI Driver DocsBlogEnglishEnglishChineseGitHubSearchCommunity EditionIntroductionQuick StartInstallationStandalone ModeDistributed ModeKey FeaturesDeploymentBenchmark & ProfilingAdministrationSecurityTutorialsReferenceFAQDevelopmentCommunityRelease NotesQuick StartDistributed ModeOn this pageDistributed ModeThe previous document introduces how to create a file system that can be mounted on any host by using an ""object storage"" and a ""SQLite"" database. Thanks to the feature that the object storage is accessible by any computer with privileges on the network, we can also access the same JuiceFS file system on different computers by simply copying the SQLite database file to any computer that needs to access the storage.However, the real-time availability of the files is not guaranteed if the file system is shared by the above approach. Since SQLite is a single file database that cannot be accessed by multiple computers at the same time, a database that supports network access is needed, such as Redis, PostgreSQL, MySQL, etc., which allows a file system to be mounted and read by multiple computers in a distributed environment.In this document, a multi-user ""cloud database"" is used to replace the single-user ""SQLite"" database used in the previous document, aiming to implement a distributed file system that can be mounted on any computer on the network for reading and writing.Network Database​The meaning of ""Network Database"" here refers to the database that allows multiple users to access it simultaneously through the network. From this perspective, the database can be simply divided into:Standalone Database: which is a single-file database and is usually only accessed locally, such as SQLite, Microsoft Access, etc.Network Database: which usually has complex multi-file structures, provides network-based access interfaces and supports simultaneous access by multiple users, such as Redis, PostgreSQL, etc.JuiceFS currently supports the following network-based databases.Key-Value Database: Redis, TiKV, etcd, FoundationDBRelational Database: PostgreSQL, MySQL, MariaDBDifferent databases have different performance and stability. For example, Redis is an in-memory key-value database with an excellent performance but a relatively weak reliability, while PostgreSQL is a relational database which is more reliable but has a less excellent performance than the in-memory database.The document that specifically introduces how to select database will come soon.Cloud Database​Cloud computing platforms usually offer a wide variety of cloud database, such as Amazon RDS for various relational database versions and Amazon ElastiCache for Redis-compatible in-memory database products, which allows to create a multi-copy and highly available database cluster by a simple initial setup.Of course, you can also build your own database on the server.For simplicity, we take Amazon ElastiCache for Redis as an example. The most basic information of a network database consists of the following 2 items.Database Address: the access address of the database; the cloud platform may provide different links for internal and external networks.Username and Password: authentication information used to access the database.Hands-on Practice​1. Install Client​Install the JuiceFS client on all computers that need to mount the file system, refer to ""Installation"" for details.2. Preparing Object Storage​Here is a pseudo sample with Amazon S3 as an example. You can also switch to other object storage (refer to JuiceFS Supported Storage for details).Bucket Endpoint: https://myjfs.s3.us-west-1.amazonaws.comAccess Key ID: ABCDEFGHIJKLMNopqXYZAccess Key Secret: ZYXwvutsrqpoNMLkJiHgfeDCBA3. Preparing Database​Here is a pseudo sample with Amazon ElastiCache for Redis as an example. You can also switch to other types of databases (refer to JuiceFS Supported Databases for details).Database Address: myjfs-sh-abc.apse1.cache.amazonaws.com:6379Database Username: tomDatabase Password: mypasswordThe format for using a Redis database in JuiceFS is as follows.redis://<username>:<password>@<Database-IP-or-URL>:6379/1tipRedis versions lower than 6.0 do not take username, so omit the <username> part in the URL, e.g. redis://:[email protected]:6379/1 (please note that the colon in front of the password is a separator and needs to be preserved).4. Creating a file system​The following command creates a file system that supports cross-network, multi-machine simultaneous mounts, and shared reads and writes using an object storage and a Redis database.juicefs format \"
73,--storage s3 \
73,--bucket https://myjfs.s3.us-west-1.amazonaws.com \
73,--access-key ABCDEFGHIJKLMNopqXYZ \
73,--secret-key ZYXwvutsrqpoNMLkJiHgfeDCBA \
73,redis://tom:[email protected]:6379/1 \
73,"myjfsOnce the file system is created, the terminal will output something like the following.2021/12/16 16:37:14.264445 juicefs[22290] <INFO>: Meta address: redis://@myjfs-sh-abc.apse1.cache.amazonaws.com:6379/12021/12/16 16:37:14.277632 juicefs[22290] <WARNING>: maxmemory_policy is ""volatile-lru"", please set it to 'noeviction'.2021/12/16 16:37:14.281432 juicefs[22290] <INFO>: Ping redis: 3.609453ms2021/12/16 16:37:14.527879 juicefs[22290] <INFO>: Data uses s3://myjfs/myjfs/2021/12/16 16:37:14.593450 juicefs[22290] <INFO>: Volume is formatted as {Name:myjfs UUID:4ad0bb86-6ef5-4861-9ce2-a16ac5dea81b Storage:s3 Bucket:https://myjfs AccessKey:ABCDEFGHIJKLMNopqXYZ SecretKey:removed BlockSize:4096 Compression:none Shards:0 Partitions:0 Capacity:0 Inodes:0 EncryptKey:}infoOnce a file system is created, the relevant information including name, object storage, access keys, etc. are recorded in the database. In the current example, the file system information is recorded in the Redis database, so any computer with the database address, username, and password information can mount and read the file system.5. Mounting the file system​Since the ""data"" and ""metadata"" of this file system are stored in cloud services, the file system can be mounted on any computer with a JuiceFS client installed for shared reads and writes at the same time. For example:juicefs mount redis://tom:[email protected]:6379/1 ~/jfsStrong data consistency guarantee​JuiceFS guarantees a ""close-to-open"" consistency, which means that when two or more clients read and write the same file at the same time, the changes made by client A may not be immediately visible to client B. Other client is guaranteed to see the latest data when they re-opens the file only if client A closes the file, no matter whether the file is on the same node with A or not.Increase cache size to improve performance​Since object storage is a network-based storage service, it will inevitably encounter access latency. To solve this problem, JuiceFS provides and enables caching mechanism by default, i.e. allocating a part of local storage as a buffer layer between data and object storage, and caching data asynchronously to local storage when reading files. Please refer to ""Cache"" for more details.JuiceFS will set 100GiB cache in $HOME/.juicefs/cache or /var/jfsCache directory by default. Setting a larger cache space on a faster SSD can effectively improve read and write performance of JuiceFS even more .You can use --cache-dir to adjust the location of the cache directory and --cache-size to adjust the size of the cache space, e.g.:juicefs mount"
73,--background \
73,--cache-dir /mycache \
73,--cache-size 512000 \
73,redis://tom:[email protected]:6379/1 \
73,"~/jfsnoteThe JuiceFS process needs permission to read and write to the --cache-dir directory.The above command sets the cache directory in the /mycache directory and specifies the cache space as 500GiB.Auto-mount on boot​In a Linux environment, you can set up automatic mounting when mounting a file system via the --update-fstab option, which adds the options required to mount JuiceFS to /etc/fstab. For example:noteThis feature requires JuiceFS version 1.1.0 and above$ sudo juicefs mount --update-fstab --max-uploads=50 --writeback --cache-size 204800 redis://tom:[email protected]:6379/1 <MOUNTPOINT>$ grep <MOUNTPOINT> /etc/fstabredis://tom:[email protected]:6379/1 <MOUNTPOINT> juicefs _netdev,max-uploads=50,writeback,cache-size=204800 0 0$ ls -l /sbin/mount.juicefslrwxrwxrwx 1 root root 29 Aug 11 16:43 /sbin/mount.juicefs -> /usr/local/bin/juicefsRefer to ""Mount JuiceFS at Boot Time"" for more details.6. Verify the file system​After the file system is mounted, you can use the juicefs bench command to perform basic performance tests and functional verification of the file system to ensure that the JuiceFS file system can be accessed normally and its performance meets expectations.infoThe juicefs bench command can only complete basic performance tests. If you need a more complete evaluation of JuiceFS, please refer to ""JuiceFS Performance Evaluation Guide"".juicefs bench ~/jfsAfter running the juicefs bench command, N large files (1 by default) and N small files (100 by default) will be written to and read from the JuiceFS file system according to the specified concurrency (1 by default), and statistics the throughput of read and write and the latency of a single operation, as well as the latency of accessing the metadata engine.If you encounter any problems during the verification of the file system, please refer to the ""Fault Diagnosis and Analysis"" document for troubleshooting first.7. Unmounting the file system​You can unmount the JuiceFS file system (assuming the mount point path is ~/jfs) by the command juicefs umount.juicefs umount ~/jfsUnmounting failure​If the command fails to unmount the file system after execution, it will prompt Device or resource busy.2021-05-09 22:42:55.757097 I | fusermount: failed to unmount ~/jfs: Device or resource busyexit status 1This failure happens probably because some programs are reading or writing files in the file system when executing unmount command. To avoid data loss, you should first determine which processes are accessing files in the file system (e.g. via the command lsof) and try to release the files before re-executing the unmount command.cautionThe following command may result in file corruption and loss, so be careful to use it!You can add the option --force or -f to force the file system unmounted if you are clear about the consequence of the operation.juicefs umount --force ~/jfsEdit this pageLast updated on Jan 5, 2024PreviousStandalone ModeNextCacheNetwork DatabaseCloud DatabaseHands-on Practice1. Install Client2. Preparing Object Storage3. Preparing Database4. Creating a file system5. Mounting the file systemStrong data consistency guaranteeIncrease cache size to improve performanceAuto-mount on boot6. Verify the file system7. Unmounting the file systemUnmounting failureFollow us on TwitterLet's chat on SlackProductCommunity EditionCloud ServiceSolutionMachine LearningBig DataKubernetes PVfor Self-Drivingfor Quantitative TradingResourcesCommunity Edition DocsCloud Service DocsJuiceFS CSI Driver DocsCommunityBlogJoin SlackCompanyAbout JuicedataCareerTerms of ServicePrivacy PolicyContact UsHot TopicsKubernetes CSI DriverClickHouse Data-TieringCephFS vs. JuiceFSAlluxio vs. JuiceFSS3FS vs. JuiceFSDistributed File System ComparisonCopyright © 2017-2024 Juicedata, Inc."
76,Grouping and aggregation query optimization - AnalyticDB for MySQL - Alibaba Cloud Documentation Center
76,Document Center
76,All Products
76,Search
76,Document Center
76,AnalyticDB for MySQL
76,User Guide
76,Performance Optimization
76,Tuning queries
76,Grouping and aggregation query optimization
76,all-products-head
76,This Product
76,This Product
76,All Products
76,AnalyticDB for MySQL:Grouping and aggregation query optimization
76,Document Center
76,AnalyticDB for MySQL:Grouping and aggregation query optimization
76,"Last Updated:May 11, 2023"
76,"This topic describes how to optimize grouping and aggregation queries in AnalyticDB for MySQL. Grouping and aggregation processAnalyticDB for MySQL is a distributed data warehouse. By default, it performs the following steps to execute a distributed aggregate query:Perform partial aggregation on data. Partial aggregate nodes use only a small amount of memory. The aggregation process is complete in a streaming manner, which prevents workloads of partial aggregate nodes from piling up. After partial aggregation is complete, redistribute data among nodes based on partial aggregation results obtained by grouping and then perform final aggregation. Partial aggregation results are transferred over networks to the nodes of a downstream stage. (For more information, see Factors that affect query performance.) The amount of data to be transferred over networks is small because the partial aggregation is performed on the data. This reduces the network pressure. After the data is redistributed, final aggregation is performed. On the final aggregate node, the values and aggregation state of a group must be maintained in the memory until all data is processed. This ensures that no new data needs to be processed for a specific group value. Therefore, the final aggregate node may occupy a large amount of memory. For example, the following SQL statement for grouping and aggregation is executed: SELECT sum(A), max(B) FROM tb1 GROUP BY C,D;When the preceding statement is executed to perform grouping and aggregation, partial aggregation is first performed on data on the Node1 and Node2 nodes of the upstream stage. Partial aggregation results are partial sum(A), partial max(B), C, and D. These partial aggregation results are transferred over networks to the Node 3 and Node4 nodes of the downstream stage for final aggregation, as shown in the following figure. Use hints to optimize grouping and aggregationScenariosIn most scenarios, two-step aggregation can strike a good balance between memory and network resources. However, in special scenarios, two-step aggregation may not be the best choice. For example, large numbers of groups must be processed by using grouping and aggregation because the GROUP BY column has a large number of unique values. Assume a scenario that requires mobile numbers or user IDs for grouping. If you use the two-step aggregation method, partial aggregation is performed although only a small amount of data can be aggregated. Moreover, the partial aggregation step involves multiple operations, such as calculating hash values of groups, deduplication, and executing aggregation functions. The amount of data to be transferred over networks is not reduced in the partial aggregation step due to large numbers of groups. However, large amounts of computing resources are consumed. SolutionTo solve the preceding problem of a low aggregation rate, you can add the /*aggregation_path_type=single_agg*/ hint to skip partial aggregation and directly perform final aggregation when you execute a query. This reduces unnecessary computing overheads. Note If the /*aggregation_path_type=single_agg*/ hint is used in an SQL statement, all grouping and aggregation queries in the SQL statement use the specified optimization process. Therefore, the best method is to first analyze the characteristics of aggregation operators in the original execution plan, evaluate the benefits brought by the hint, and then decide whether to use this optimization scheme. Optimization descriptionIf the aggregation rate is low, the amount of data to be transferred over networks is not reduced on the Node1 and Node2 nodes during partial aggregation and consumes large amounts of computing resources. After optimization, partial aggregation is not performed on the Node1 and Node2 nodes. All data (A, B, C, and D) is directly aggregated by the Node3 and Node4 nodes of the downstream stage, which reduces the amount of required computing resources, as shown in the following figure. Note This optimization may not reduce memory usage. If the aggregation rate is low, large amounts of data are accumulated in memory for deduplication and aggregation to ensure that all data for a specific group value is processed."
76,Thank you! We've received your
76,feedback.
77,Domibus - v5.0
77,Skip to main content
77,assistive.skiplink.to.breadcrumbs
77,assistive.skiplink.to.header.menu
77,assistive.skiplink.to.action.menu
77,assistive.skiplink.to.quick.search
77,Required: page refresh 5
77,Log in
77,Skip to sidebar
77,Skip to main content
77,Spaces
77,Hit enter to search
77,Help
77,Online Help
77,Keyboard Shortcuts
77,Feed Builder
77,What’s new
77,Available Gadgets
77,About Confluence
77,Log in
77,DigitalPagesBlogPage tree
77,Browse pagesConfigureSpace tools
77,Attachments (269)
77,Page History
77,Page Information
77,Resolved comments
77,View in Hierarchy
77,View Source
77,Export to PDF
77,Export to Word
77,Export to PDF
77,Page Labels
77,Attachment Labels
77,Hide Inline Comments
77,Full Width
77,Normal Width
77,Log in
77,Advanced Menu
77,Pages
77,Digital
77,About usBuilding BlocksContact us (opens in a new tab)
77,ABOUT THE PROGRAMME
77,About us
77,Service Offering Canvas
77,READ & MEET
77,Events
77,Past and upcoming events
77,News
77,Recent news about the programme
77,Success stories
77,Find out how our Building Blocks are being used!
77,MONITORING
77,Monitoring explained
77,DIGITAL Monitoring
77,CEF Monitoring - Archive
77,Contact us (opens in a new tab)
77,eDelivery
77,Exchange electronic data and documents in an interoperable and secure way
77,eSignature
77,"Create and verify electronic, paperless signatures"
77,eID
77,Offer services capable of electronically identifying users from all across Europe
77,Once-Only Technical System (OOTS)
77,Reduce administrative burden on citizens and businesses
77,eInvoicing
77,Send and receive electronic invoices in line with the European Directive
77,Apply for Grants
77,Pages
77,Digital Homepage
77,eDelivery
77,Services eDelivery
77,Access Point software
77,Domibus releases
77,Go to start of metadata
77,Created by
77,"Caroline AEBY, last modified by"
77,"Mohamed Chaouki BERRAH on Nov 20, 2023"
77,"Domibus 5.0This page collects the resources for Domibus version 5.0, released in June 2022."
77,Download Domibus v5.0Download for Tomcat
77,Download for Wildfly
77,Download for Weblogic
77,Verify files integrityAccess source code View all binariesDomibus Default FileSystem PluginDomibus Default JMS PluginDomibus Default WebService PluginDomibus Sample Configuration and TestingDomibus SQL ScriptsDomibus Tomcat ConfigurationDomibus Tomcat Full DistributionDomibus TomcatDomibus Weblogic ConfigurationDomibus Weblogic EU Login ConfigurationDomibus Weblogic EU LoginDomibus WeblogicDomibus Wildfly ConfigurationDomibus Wildfly Full DistributionDomibus WildflyDomibus Plugin API JavadocDomibus DSS Extension
77,"DescriptionWe are happy to announce that the release of Domibus 5.0 is available. Domibus is the sample implementation of an eDelivery Access Point maintained by the European Commission.In addition to new features, Domibus 5.0 also comes with a new database schema design that supports unprecedented performance improvements. This new version can reliably handle a throughput of more than 1,000 messages/s* and, with added support for table partitioning, ensures this high level of performance even as the size of the database increases. The introduction of an API for archiving messages facilitates moving data from the database to an external long-term archival system.Domibus 5.0 contains several changes that will impact the compatibility of the plugins developed for earlier versions of Domibus. Such plugins will have to be adapted to be compatible with version 5.0 and onwards. This will affect projects relying on custom plugins and/or the standard Domibus Web Service (WS) plugin. Please click here for further details concerning the backwards compatibility of plugins in Domibus 5.0.Domibus 5.0 includes a number of new features, improvements, and bug fixes, among which:Possibility to visualize the non-repudiation receipts in the Admin consoleImproved overall performance including refactoring of the database to allow partitioningOptimized pull locking for Oracle Possibility to use an external agent to archive Domibus messagesDefault WS Plugin: Possibility to operate the plugin in PUSH modePossibility to filter messages retrieved by the listPendingMessages method of the WS PluginImplement offloading the SSL traffic to an external componentImprove the decoupling of the Domibus core from the plugin implementations Possibility to validate incoming UserMessages using a Validation ExtensionPossibility to reference JMS Plugin payloads via HTTP endpointsPossibility to add and remove a domain at runtimeNew client authentication trust store admin console pageNew WS Plugin which uses the edelivery namespaceUI Replication has been removedImportant note: there is a known issue on the Domibus 5.0 version with the reload keystore functionality: due to a caching issue in the reload keystore functionality, Domibus requires restart when changing the keystore. The steps to replace the keystore are:change the keystore file on the diskpress ReloadKeystore in Domibus Admin Console - the new keystore will be visible in the UI but not used for exchanging messagesrestart Domibus (to be fixed in next hotfix release)Domibus 5.0 is backward compatible with 4.2.x. The upgrade is not mandatory, but it is highly recommended.*Results measured during a 2-hour period with Domibus working in single-tenancy mode, deployed in a 4-node cluster, using Oracle WebLogic Server and Oracle Database with partitioning enabled, configured to receive 500 messages/s of 5 kB each and send 1,000 messages/s of 500 B each.Supported platforms:Application servers:WildFly 26.1.xWebLogic 12.2.1.4 (tested version, future versions might work)Apache Tomcat 9.0.xDatabase:MySQL 8 (future versions might work)Oracle 12c R2 and Oracle 19cJava 8 features / compile with Oracle JDK 8: tested to run correctly with:Oracle JDK 8/ WebLogicOpenJDK 11/ WildFly + Tomcat (tested with AdoptOpenJDK 11 version 11.0.9.1+1)Security Note: To ensure their system’s security, users installing any of the Domibus packages labelled as “Full Distribution” have the responsibility to update the application servers to the latest version after the installation.Technical Notes:Note 1: In the pMode XML of Domibus 5.0, the twoWay MEP with pushAndPush, pushAndPull, pullAndPush are deprecated, meaning a pMode file containing these values are successfully uploaded but a warning is displayed:""WARNING: Two-way mep with binding [pushAndpush] is not supported for process [tc1Process]. In the pMode XML it is required to use 2 one-way processes to simulate two-way communication.""MEP: http://docs.oasis-open.org/ebxml-msg/ebms/v3.0/ns/core/200704/twoWayBindings:http://docs.oasis-open.org/ebxml-msg/ebms/v3.0/ns/core/200704/pushAndPushhttp://docs.oasis-open.org/ebxml-msg/ebms/v3.0/ns/core/200704/pushAndPullhttp://docs.oasis-open.org/ebxml-msg/ebms/v3.0/ns/core/200704/pullAndPushNote 2: the invalid binding value ""http://docs.oasis-open.org/ebxml-msg/ebms/v3.0/ns/core/200704/push-and-push"" that was occasionally misused in the pMode XML is not accepted anymore and should not be used.DocumentationQuick Start Guide (pdf)This guide allows the user to quickly get started with Domibus. After completing this document, you will have a local Domibus instance up and running locally on a Tomcat/MySQL environment.Testing guide (pdf)This document is intended for developers that want to perform a set of checks on their Domibus installation and testers that want to have a starting point to create their own test cases.Interface Control Document of the default JMS (pdf)The purpose of this document is to outline the JMS Data Format Exchange to be used as part of the default JMS backend plugin.Interface Control Document of the default (new) WS plugin (pdf)This document describes the WSDL and the observable behaviour of the interface provided in the default WS plugin.Interface Control Document of the default (old) WS plugin (pdf)This document describes the WSDL and the observable behaviour of the interface provided in the default WS plugin. Interface Control Document of the File System plugin (pdf)The purpose of this document is to outline the file system messages exchange as part of the default File System (FS) backend integration solution for the Domibus Access Point.Administration Guide (pdf)The purpose of this guide is to provide detailed information on how to deploy and configure Domibus on WebLogic, Tomcat and WildFly with MySQL and Oracle. It also provides detailed descriptions of related Security Configurations (Policies, Certificates, TLS Configuration), Message Filtering, PMode Configuration, Application Monitoring, Registration of custom plugins and Troubleshooting.File System Plugin Administration Guide (pdf)The purpose of this guide is to provide detailed information on how to configure and deploy the File System Plugin available in Domibus 3.3 and later versions.Plugin cookbook (implementation manual) (pdf)After reading this document the reader should be aware of the capabilities provided by the Domibus plugin system. Additionally a developer familiar with the AS4 protocol will be able to implement a plugin integrating an existing back office application into Domibus.Extension cookbook (pdf)This document details the technical specifications of Domibus extension mechanism. It lays out applicable guidelines to support the technical implementation of an extension.Validation extension cookbook (pdf)This document details the technical specifications of Domibus Validation Extension mechanism. Its scope are the functional aspects of the extension mechanism and the technical and operational aspects of the extension mechanism.Domibus Software Architecture Document (pdf)This document provides a comprehensive architectural overview of the system, using a number of different architectural views to depict individual aspects of the system.  It is intended to capture and convey the significant architectural decisions that have been made on the system.Domibus REST services documentationDocumentation on the Domibus REST services.Domibus eArchiving REST services documentationDocumentation on the Domibus eArchiving REST services.Licence (pdf)European Union Public Licence.Domibus upgrade and downgrade procedureThe purpose of this document is to describe the details of making a backup of Domibus and restore Domibus previous version if it is needed."
77,"Upgrade from 4.2.11 to 5.0For a detailed description of the Domibus upgrade procedure, click here.Release notesPlease find below the list of new features, improvements, solved bugs and known limitations.New features[EDELIVERY-9070] - Possibility to use an external eArchiving agent[EDELIVERY-3746] - Download eDelivery Message Receipts for Non repudiation purposes[EDELIVERY-7284] - Default WS Plugin: possibility to implement PUSH[EDELIVERY-7628] - Offloading the SSL traffic to an external component[EDELIVERY-6194] - Scan uploaded files by antivirus software[EDELIVERY-8882] - Possibility to reference the payloads via a REST endpoint in the JMS Plugin[EDELIVERY-4627] - SSL- client authentication truststore in GUIImprovements[EDELIVERY-3247] - Domibus Admin console : plugin information available on MessageDetails[EDELIVERY-3485] - Domibus Alert system open to the plugins[EDELIVERY-4321] - Pull locking for Oracle optimized[EDELIVERY-4649] - Remove pull dependency from JMX[EDELIVERY-4789] - Segregation between domain specific configuration improvement[EDELIVERY-4808] - Possibility to filter messages retrieved by the listPendingMessages WS Plugin method[EDELIVERY-4810] - FS Plugin should have property file per each tenant[EDELIVERY-4932] - Enhance the handling of TLS custom and cacert certificate in plugins and extensions[EDELIVERY-7091] - Show the certificate about to expire in orange[EDELIVERY-7378] - Performance optimizations[EDELIVERY-7382] - Adapt WS Plugin to use edelivery namespace[EDELIVERY-7591] - Make Domibus Properties console page load faster[EDELIVERY-2676] - Search pages should have a Clear button to reset filter form[EDELIVERY-3742] - [GUI] Load truststore: after entering the password: Enter for OK[EDELIVERY-3853] - UI admin console: downloading the pmode from the 'Current pmode' page [EDELIVERY-8596] - Possibility to remove (or disable) a domain without downtime Fixed bugs[EDELIVERY-2094] - Message is not immediately deleted after download when attachement storage location is psecified.[EDELIVERY-3296] - Message type drop down needs an extra empty option[EDELIVERY-3335] - Messages page: performance decrease when certain filters are applied on millions of messages[EDELIVERY-3740] - When uploading a PMode: the field description is not marked as mandatory(in red) after selecting a file[EDELIVERY-3744] - Load truststore with invalid keystore password[EDELIVERY-3985] - Alerts management: UTF-8 special characters in alert emails subjects[EDELIVERY-4186] - Small irregularities in downloaded alert CSV file[EDELIVERY-4438] - JTA transaction error is masking original JPA/Hibernate error[EDELIVERY-4592] - Edit and Delete operations on Pmode Parties do not require confirmation[EDELIVERY-4757] - Exported CSV alerts file should match the columns in the UI grid[EDELIVERY-4759] - Exported CSV file that contains plugin users contains some extra columns[EDELIVERY-4784] - Issue with browsing the jms queues in case of clustered external activemq server.[EDELIVERY-4892] - EDGE Specific : Distorted Ui for field EndPoint & Party Id on Pmode:Parties page[EDELIVERY-4985] - Not possible to access the truststore if the keystore password is wrong.[EDELIVERY-5540] - Delete party sometimes makes pmode created and deleted events appear in reverse order in UI[EDELIVERY-6411] - Confusing color for warning messages on Save PMode[EDELIVERY-6480] - $jacoco Data column present in all CSV files resulted from downloading info listed in grids[EDELIVERY-6611] - Improve error message in the logs in case of empty partyID for FS plugin.[EDELIVERY-6613] - Improve error message in the logs in case of empty party role for FS plugin.[EDELIVERY-6617] - Improve error message in the logs in case of empty mime-type for FS plugin.[EDELIVERY-6652] - Build-Time shown in console (on the login page) is not accurate[EDELIVERY-6697] - Blank description in alert email generated for ACKNOWLEDGED status[EDELIVERY-6942] - Domibus-MSH-soapui-tests cannot be built with openjdk[EDELIVERY-7046] - Extra column is shown in downloaded CSV of Alert page[EDELIVERY-7094] - Domibus UI - browser console has a lot of warnings[EDELIVERY-7137] - The configuration for the default domain is confusing[EDELIVERY-7262] - User can create 2 parties with the same party id[EDELIVERY-7266] - Changing property ""domibus.logging.payload.print"" at runtime has no effect.[EDELIVERY-7290] - Refactor Service classes having a lot of dependecies[EDELIVERY-7301] - domibus.logging.cxf.limit cannot be Integer.MAX_VALUE[EDELIVERY-7353] - Domibus properties should be cached[EDELIVERY-7416] - Incomplete error message on Plugin users page having multiple duplicate entries[EDELIVERY-7419] - Event of Suspended User/Plugin user with wrong password is captured on Audit page with wrong user name[EDELIVERY-7420] - 404 Error while editing Pmode party having no certificate[EDELIVERY-7458] - [Mac OS X] Unable to create SAAJ meta-factoryProvider on Docker Tomcat 9 in Domibus built on Mac OS[EDELIVERY-7471] - Unable to create SAAJ meta-factoryProvider on Docker Tomcat 9 in Domibus built on Mac OS[EDELIVERY-7472] - Sometimes duplicate plugin users can be created by pressing save multiple times[EDELIVERY-7502] - Current party is always a participant in all processes regardless of user choice[EDELIVERY-7517] - CSV export of Properties has extra column ""Cluster Aware""[EDELIVERY-7518] - Cannot sort properties grid by column ""Usage""[EDELIVERY-7524] - HTTPS Configuration Issue on domibus-wildfly[EDELIVERY-7527] - Adapt Bamboo Plans to the new session cookie name[EDELIVERY-7533] - All properties can be edited in UI Domibus Properties[EDELIVERY-7545] - Incorrect value of fsplugin.messages.notifications property generating stack trace in logs[EDELIVERY-7572] - Error when trying to filter global domibus properties .[EDELIVERY-7592] - Changing the Domain in a Multitenancy Fails in Domibus 5.0[EDELIVERY-7645] - Uumds extension must align with the new version of Ehcache 3.8.x[EDELIVERY-7648] - Error when filtering in the Pmode - Parties page[EDELIVERY-7649] - Error when filtering in the JMS Monitoring page[EDELIVERY-7650] - Error when filtering in Audit page[EDELIVERY-7651] - Error when filtering with both Show Domain Properties and Is Writable unchecked in Properties page[EDELIVERY-7652] - Possible infinite loop when changing domain with error message opened[EDELIVERY-7653] - Searching only for deleted users returns no results[EDELIVERY-7654] - Columns Message Fragment and Source Message visible in UI and not in CSV[EDELIVERY-7678] - Error when listing messages in Message Log page[EDELIVERY-7685] - Cannot update global properties[EDELIVERY-7687] - Cannot download message in Message log page[EDELIVERY-7688] - Cannot filter by message status in Message Log page[EDELIVERY-7689] - Minor inconsistencies in column names between Properties page and downloaded CSV file[EDELIVERY-7704] - Domibus not starting with 4 domains deployed.[EDELIVERY-7759] - NoClassDefFoundError in logs[EDELIVERY-7776] - Issue with certificate added to the truststore after dynamic discovery process.[EDELIVERY-7777] - Investigate bamboo Dynamic Dyscovery plan failures[EDELIVERY-7778] - UI - edit pmode: error message when 'Cancel' button is pressed[EDELIVERY-7780] - New certificate plugin user operation returns error[EDELIVERY-7794] - PMode upload error without <payloadprofiles> in Domibus[EDELIVERY-7795] - domibus-weblogic122 image won't start on windows because it's missing security.properties file[EDELIVERY-7815] - Fix the cause for ERROR and WARN log entries[EDELIVERY-7818] - fsplugin: ConcurrentModificationException error appears in the log on the receiving side[EDELIVERY-7830] - application.properties should be renamed to version.properties[EDELIVERY-7836] - PoC to delete messages using partitions[EDELIVERY-7846] - Party name not validated while receiving message[EDELIVERY-7871] - Updating property ""wsplugin.mtom.enabled"" at runtime has no effect.[EDELIVERY-7872] - Unable to edit/delete Certificate type Plugin user[EDELIVERY-7873] - Extra option present in the Alert type dropdown in Alerts page that only contains ""PLUGIN""[EDELIVERY-7875] - Error when trying to filter by alert type Plugin[EDELIVERY-7880] - domibus fails to start on wildfly with BeanCurrentlyInCreationException (related to CertificateServiceImpl)[EDELIVERY-7882] - Error in retentionWorkerJob: ClassCastException: class eu.domibus.api.message.MessageSubtype cannot be cast to class java.lang.String[EDELIVERY-7891] - Validate the MPC received as a UserMessage attribute to the mpc configured in the pMode[EDELIVERY-7912] - Error in PartyServiceImpl: Party partly lost IM0020051419[EDELIVERY-7915] - If domibus.ui.pages.messageLogs.countLimit is set to 9,11 or 13 and user changes page size the Messages page will show no results[EDELIVERY-7921] - Null pointer on Domibus when SoapFault is received as response to sending a UserMessage[EDELIVERY-7922] - Domibus encounters NullPointer when SoapFault is received as response to sending a UserMessage[EDELIVERY-7928] - Split&Join error on Tomcat/Oracle setup[EDELIVERY-7931] - Message expired if retry timeout is too large[EDELIVERY-7933] - [TAPAS] - IM0020083359 - Wrong date for PMODE Archive records[EDELIVERY-7934] - Unknown error for wrong security in AS4 messages[EDELIVERY-7956] - domibus.property.length.max update not applied[EDELIVERY-7964] - [Docker] Circular Reference When Deploying Domibus on Wildfly[EDELIVERY-7966] - Sometimes the messageId is missing from the MDC context[EDELIVERY-7968] - Fix integration test UserMessageLogDaoIT after performing 4.2.1 merge into development (5.0)[EDELIVERY-7983] - PoC to use sequences using a specific format for Oracle[EDELIVERY-7985] - Migration script to convert from local dates to UTC[EDELIVERY-7994] - quartz issue on wildfly: Can't call rollback when autocommit=true[EDELIVERY-8004] - PULL processes with dynamic initiator are not handled properly in Domibus[EDELIVERY-8027] - Sometimes Domibus fails to start.[EDELIVERY-8030] - Unable to update wsplugin.dispatcher.worker.cronExpression from admin console-properties page[EDELIVERY-8034] - Submitting messages with bodyload fails for the new plugin.[EDELIVERY-8035] - Updating ws plugin properties at runtime affects only the new plugin implementation.[EDELIVERY-8037] - change properties at runtime not working for domibus.security.keystore.location[EDELIVERY-8039] - Error in logs when starting bamboo plans[EDELIVERY-8054] - Fix pull mechanism after database refactoring[EDELIVERY-8081] - Fix the list of SignalMessages in the admin console[EDELIVERY-8082] - Fix the ConnectionMonitoring underlying query (admin console)[EDELIVERY-8103] - Payload Profiling Error When Sending Message from c2 to c3 in Docker[EDELIVERY-8120] - Service type is ignored when parsing the pmode.[EDELIVERY-8121] - MimeType value is case sensitive[EDELIVERY-8136] - Domibus on Tomcat/OracleXE fails to send message[EDELIVERY-8137] - NullPointerException when receiving a test message on 5.0[EDELIVERY-8138] - Domibus ErrorLog page shows an error[EDELIVERY-8140] - Admin console: the ""Message Type"" column is always empty in the Messages page[EDELIVERY-8142] - Domibus shows error at startup[EDELIVERY-8143] - Domibus unable to send message from C2 to C3[EDELIVERY-8146] - PMode configuration: adding new service/action is not taken in account[EDELIVERY-8147] - Duplicate entry 'MimeType' for key 'tb_d_part_property.UK_D_PART_PROP_NAME'[EDELIVERY-8148] - Domibus pull mechanism - message in send failure[EDELIVERY-8149] - User messages parameters extraction issues.[EDELIVERY-8150] - ErrorLog page - filtering by AP Role doesn't work[EDELIVERY-8152] - Admin console: the Messages page shows both test and regular messages at the same time[EDELIVERY-8153] - quartz scheduler issues when rescheduling/removing jobs[EDELIVERY-8154] - Error while retrieving messages[EDELIVERY-8155] - Exceptions and standard errors returned instead of properly intercepting the errors[EDELIVERY-8164] - Can't send simplest message with 5.0 version[EDELIVERY-8170] - Wrong error in log when submitting message[EDELIVERY-8182] - Circular deps: multitenant + 2Lcache[EDELIVERY-8220] - Pull requests are not sent.[EDELIVERY-8221] - Issue with retention worker.[EDELIVERY-8222] - Message parsing error in the logs.[EDELIVERY-8223] - proxy error when submitting a message with a non empty agreementRef.[EDELIVERY-8234] - ORA-00920: invalid relational operator on RetentionWorker - Weblogic, Oracle[EDELIVERY-8235] - Issue with ID_PK on TB_MESSAGE_PROPERTIES table[EDELIVERY-8236] - NullPointer when converting SoapFault, Weblogic/Oracle[EDELIVERY-8238] - Performance assessment of hibernate sequence SCALE EXTEND in Domibus[EDELIVERY-8239] - Fix issue with pMode not showing in the UI[EDELIVERY-8247] - Fix failing Bamboo plans following database refactoring[EDELIVERY-8254] - Quartz jobs can start before schema is fully initialized[EDELIVERY-8259] - AgreementRef type might not be taken into account[EDELIVERY-8260] - Submitting a message with more than 28 attachments succeeds.[EDELIVERY-8263] - Error when submitting a message via jms plugin.[EDELIVERY-8264] - Review Rfc on cryptography[EDELIVERY-8266] - LazyInitializationException when sending a split-and-join message[EDELIVERY-8278] - Cannot create users[EDELIVERY-8279] - The unique constraint on tb_d_part_property doesn't behave as expected on mysql[EDELIVERY-8280] - PMode validation: we should accept parties without party id type[EDELIVERY-8285] - Pull request handling issue in case of multitenancy.[EDELIVERY-8286] - Errors when running oracle-5.0-SNAPSHOT.ddl[EDELIVERY-8305] - Could not create backup file for TLS truststore[EDELIVERY-8314] - The order of the payloads is not preserved[EDELIVERY-8315] - Timezone difference between soapui and domibus containers.[EDELIVERY-8317] - Fix dynamic discovery following database refactoring[EDELIVERY-8318] - Bring the migration scripts up to date with the latest updates on the database schema[EDELIVERY-8329] - Metadata missing for property[EDELIVERY-8331] - Oracle&Multi-tenancy: Sequence does not exist in general schema[EDELIVERY-8332] - Weblogic/Oracle MT - sequence does not exist[EDELIVERY-8333] - Delete Database Data Script Not Working[EDELIVERY-8334] - Update of properties at runtime doesn't seem to be propagated in a clustered deployment[EDELIVERY-8335] - Sometimes messages are handled by the wrong filter.[EDELIVERY-8336] - The pmode id needs to be changed from int to long[EDELIVERY-8340] - Users with no domain can be created using the REST API[EDELIVERY-8341] - Super can create on one domain users with another domain assigned[EDELIVERY-8364] - MT/WL bamboo plan fails with ConstraintViolationException in PartyId dictionary tables[EDELIVERY-8367] - NPE in WSPluginLoggingEventSender[EDELIVERY-8371] - NPE when deleting failed sent message[EDELIVERY-8372] - refToMessageId is missing from the signal message for certificate-related issues[EDELIVERY-8373] - BouncyCastle NoClassDefFound[EDELIVERY-8385] - NullPointerException in retentionWorkerJob[EDELIVERY-8393] - Pmode Parties Page- Blank error on edit identifier pop up for Party with no partyIdtype[EDELIVERY-8394] - Forbidden character detected in property root->username error message in login page needs correction[EDELIVERY-8396] - Extra word ""class"" appears in some rows in Logging page[EDELIVERY-8397] - Unexpected warnings in domibus logs - Bambo load tests plans failing[EDELIVERY-8409] - Assist in analysing bamboo plans failures.[EDELIVERY-8410] - Properties values registered in case not needed.[EDELIVERY-8411] - Cannot create users[EDELIVERY-8415] - Several exceptions observed in the logs of Domibus deployed on wildfly.[EDELIVERY-8416] - Data Migration for TB_SEND_ATTEMPT is missing in 5.0 (Step 2)[EDELIVERY-8417] - Data Migration for TB_SEND_ATTEMPT is missing in 5.0 (Step 2)[EDELIVERY-8419] - DSS Refresh Connection timeout parameter- development[EDELIVERY-8422] - [WEBLOGIC] WARN e.d.c.j.m.DomainMessageListenerContainer:929 - Execution of JMS message listener failed, and no ErrorHandler has been set.[EDELIVERY-8423] - Issue with the password encryption at startup[EDELIVERY-8425] - Exception returned instead of proper error in case of an issue with domibus deployed in multitenancy[EDELIVERY-8428] - Errors encountered during load tests in Bamboo[EDELIVERY-8455] - Delete operation is not working on Alert page[EDELIVERY-8456] - Issue with property ""domibus.connection.cxf.ssl.offload.enable""[EDELIVERY-8457] - Download message has not the right filename[EDELIVERY-8462] - Create index on TB_SIGNAL_MESSAGE[EDELIVERY-8463] - Logging page grid is not sortable but columns are marked as sorted when clicked[EDELIVERY-8464] - Exception in the response in case the SMP https connection fails.[EDELIVERY-8466] - Alerts are not marked as processed or deleted[EDELIVERY-8469] - Wildfly cluster - Party changes in one node not reflect on a different node[EDELIVERY-8473] - Wildfly Cluster Not Starting Due to Missing JDBC Modules[EDELIVERY-8477] - ProcessingType is missing from the sample metadata.xml distributed with fs-plugin[EDELIVERY-8479] - Fix the cause for ERROR and WARN log entries[EDELIVERY-8502] - SQL exception returned instead of proper error in case of an issue with domibus deployed in multitenancy[EDELIVERY-8505] - FS-Plugin: issue with opt-out[EDELIVERY-8506] - wsplugin's push mode doesn't work properly in multi-tenancy[EDELIVERY-8507] - Fix the cause for ERROR and WARN log entries[EDELIVERY-8508] - Hibernate query cache and second level cache is not working[EDELIVERY-8509] - Error when creating super user[EDELIVERY-8515] - H2 commits a lot[EDELIVERY-8516] - Broken pipe error while loading metrics from metrics servlet[EDELIVERY-8542] - Error when uploading trustore[EDELIVERY-8547] - Alert Ids on Alert page are incorrect[EDELIVERY-8548] - Domibus Admin console: Alerts page: Buttons disappear on click of 'Show Columns'[EDELIVERY-8550] - Detect a domain in the domains folder.[EDELIVERY-8555] - 2 properties in default domain properties file are missing ""default"" prefix.[EDELIVERY-8557] - Fix the cause for ERROR and WARN log entries - TLS truststore[EDELIVERY-8558] - Fix the cause for ERROR and WARN log entries -weblogic - synchronization feature[EDELIVERY-8577] - Domain Properties of type Password are not encrypted in plugins[EDELIVERY-8593] - getStatus request not taking into account original sender property linked to the plugin user.[EDELIVERY-8599] - Wrong error when uploading TLSTrustore[EDELIVERY-8600] - Error while processing pmode for multitenancy.[EDELIVERY-8601] - Error in multitenancy configuration for the fs plugin.[EDELIVERY-8609] - Metrics do not display properly[EDELIVERY-8610] - Pull message exchange not working anymore for the old ws plugin.[EDELIVERY-8617] - Remove GSon dependencies[EDELIVERY-8624] - Circular bean dependency in domibus[EDELIVERY-8627] - When ws schema validation is enabled, submitting a message without ProcessingType parameter fails[EDELIVERY-8629] - Sometimes login ends in error[EDELIVERY-8630] - Weblogic122_(Oracle)-DYN-Weblogic122_(Oracle)-DYN plan is failing - [PartyInfo/To/PartyId] is not provided[EDELIVERY-8631] - eArchiving: decompress payloads for sent messages[EDELIVERY-8633] - UI: sometimes the domain selector shows on pages it wasn't supposed to[EDELIVERY-8638] - Changing properties at runtime seems not working in a cluster deployment.[EDELIVERY-8645] - Create a WSDL for the services/msh endpoint[EDELIVERY-8658] - eArchiving: SANITIZER request type should not be exposed to the client[EDELIVERY-8661] - Error logged when sending test message[EDELIVERY-8662] - Set archived message status[EDELIVERY-8671] - Issue detected in the logs for submitting a message with old ws plugin.[EDELIVERY-8672] - eArchiving: Timestamp in REST call[EDELIVERY-8675] - LOAD bamboo plan: Duplicate error[EDELIVERY-8677] - Issue with handling duplicate message detection.[EDELIVERY-8688] - General Schema Audit: Events for adding/editing a super user are not present in Audit[EDELIVERY-8694] - Extra column in exported CSV from Trustore page[EDELIVERY-8695] - Batch status set to ""STARTED"" in case of successfull export.[EDELIVERY-8698] - NullPointerException in log when receiving a duplicate message[EDELIVERY-8702] - Property ""domibus.earchive.batch.retryTimeOut"" backup value seems not working[EDELIVERY-8711] - Stuck threads and oracle session blocked[EDELIVERY-8718] - eArchiving: send an alert if continuous date has NOT changed[EDELIVERY-8721] - eArchiving: [REST] GET and PUT sanitizer start date[EDELIVERY-8722] - ECAS access to multiple tenants[EDELIVERY-8723] - Compression: MimeType[EDELIVERY-8726] - Send 2 body attachments returns 200[EDELIVERY-8730] - mysql-5.0-SNAPSHOT-data imported on multi tenancy docker environment[EDELIVERY-8732] - Invalid response in eArchiving REST endpoint[EDELIVERY-8736] - Fix Sonar reported bugs[EDELIVERY-8743] - Improve the default password mechanism[EDELIVERY-8744] - Enforce the use of strong passwords[EDELIVERY-8745] - No HTTP Strict-Transport Security Header[EDELIVERY-8755] - Refactor PartInfo PostConstruct in a service[EDELIVERY-8757] - Cannot create, edit or reorder message filters[EDELIVERY-8758] - Cannot create or edit parties[EDELIVERY-8765] - findFailedMessages - queryString incorrect[EDELIVERY-8766] - findMessageStatus instead of findOrCreate[EDELIVERY-8767] - findByEntity -> hibernate initialization[EDELIVERY-8770] - Parties not marked as (IR) for processes[EDELIVERY-8773] - Delete All action in JMS Monitoring page doesn't refresh the grid and shows no success message[EDELIVERY-8774] - Validation Sender presence[EDELIVERY-8776] - MessageProperty ClassCast exception[EDELIVERY-8778] - [Test] Daos layer[EDELIVERY-8783] - Error message shown when configuring https connector for tomcat server.[EDELIVERY-8784] - Messages are not archived.[EDELIVERY-8786] - Default jms plugin only transforms specific partInfo properties.[EDELIVERY-8787] - Change the description of the property domibus.earchive.batch.retry.timeout[EDELIVERY-8788] - Message stuck in WAITING_FOR_RETRY state[EDELIVERY-8789] - UI replication accessement[EDELIVERY-8794] - Login fails with error for user SUPER on Wildfly_Mysql_MT docker environment[EDELIVERY-8795] - Messages duplicate exports[EDELIVERY-8796] - [Multitenancy] Migration script to convert from local dates to UTC[EDELIVERY-8797] - Upload PMode: IT Tests[EDELIVERY-8799] - Messages are deleted despite not being exported yet.[EDELIVERY-8800] - Messages in ACKNOWLEDGED status are not exported.[EDELIVERY-8812] - FSPluginPropertiesMultitenantIT - fix tests[EDELIVERY-8813] - In case of a failed export, the referred message ID in the notification is wrong.[EDELIVERY-8814] - SubmitMessageIT - fix tests[EDELIVERY-8815] - BackendNotificationServiceTest[EDELIVERY-8828] - IT tests WSPLUGIN: tests for rest methods ignored[EDELIVERY-8830] - Second payload is printed although domibus.logging.payload.print is set to false[EDELIVERY-8831] - Not possible to submit messages with average payload size.[EDELIVERY-8832] - Tomcat multitenancy docker environment doesn't start[EDELIVERY-8833] - On business validation error before Msg creation in DB, throws foreign key violation error for TB_ERROR table[EDELIVERY-8836] - Check if EARCHIVE QUEUE property is needed[EDELIVERY-8837] - No clear notification in case eArchiving client is not reachable.[EDELIVERY-8838] - 1 hour offset between message timestamp and batch metadata[EDELIVERY-8842] - List queued export request filter does not seem to work properly.[EDELIVERY-8843] - Submiting a message via WebService plugin with an existing message id results in error containing SQL statement[EDELIVERY-8848] - Delete all JMS messages from a queue - when deleting Domibus doesn't logs anything in logs[EDELIVERY-8849] - ""Delete All"" message in JMS queue button is always active even when queue is empty[EDELIVERY-8850] - JMS Monitoring page shows error when selecting a queue with more than 3000 messages in it[EDELIVERY-8851] - Messages page shows error when system has more than 6000 messages[EDELIVERY-8852] - Error in the response for messages exported in a batch request.[EDELIVERY-8853] - Messages exported list returned is not aligned with the actual exported list in case of a failed batch.[EDELIVERY-8854] - The response message, for get exported messages request, in case batch ID does not exist, can be improved.[EDELIVERY-8863] - Not possible to submit a ""list not archived messages"" request.[EDELIVERY-8865] - nothingPolicy not found in load plans[EDELIVERY-8866] - ARCHIVING_START_DATE_STOPPED alert created by sanitizer even if messages are in a final state.[EDELIVERY-8871] - Deleted messages should not be exported.[EDELIVERY-8872] - Alert not created in case the eArchiving client is not reachable.[EDELIVERY-8873] - Messages belonging to a failed batch must be handled by the sanitizer[EDELIVERY-8874] - RuntimeException when Domibus is using credentials to access the endpoint of the e-archiving client[EDELIVERY-8875] - Some audit logs for eArchiving are not present.[EDELIVERY-8877] - Possible issue for set bach archive status.[EDELIVERY-8884] - First PullRequest in error Bamboo LOAD[EDELIVERY-8885] - Wildfly Full on 5.0 snapshots is broken[EDELIVERY-8886] - Alert ""ARCHIVING_START_DATE_STOPPED"" created for the sanitizer start date instead of the continuous date[EDELIVERY-8887] - Sometimes the messageId is still missing for some logs entries[EDELIVERY-8902] - Tomcat local environment needs to have keystore/trustore passwords encrypted even if domibus.password.encryption.active=false[EDELIVERY-8903] - Password change for users[EDELIVERY-8905] - Wildfly Full on 5.0 snapshots misses domibus.config.location[EDELIVERY-8911] - Domibus-default-ws-plugin-backend-ws-test Client[EDELIVERY-8913] - Truststore and keystore files are not loaded from files at startup.[EDELIVERY-8914] - EU Login Weblogic docker environment doesn't deploy domibus[EDELIVERY-8915] - Wrong error message returned related to certificate extraction.[EDELIVERY-8916] - Issue with printing payloads in the logs.[EDELIVERY-8919] - Issues with REST endpoint to mark messages as DELETED in case message ID is not found.[EDELIVERY-8920] - Issues with REST endpoint to mark messages as DELETED in case message in final state[EDELIVERY-8921] - Issues with REST endpoint to mark messages as DELETED in case message not linked to user[EDELIVERY-8922] - Issues with REST endpoint to mark messages as DELETED while specifying the time interval[EDELIVERY-8929] - [Docker] Oracle 19c Image Startup Errors[EDELIVERY-8948] - Issue when submitting messages with dynamic initator for pull MEP.[EDELIVERY-8949] - Submitting a message does not succeed in case optional parameter mpc is not provided.[EDELIVERY-8952] - [Docker] Oracle Images updates[EDELIVERY-8956] - domibus-earchive-client: default value -> webhook[EDELIVERY-8959] - Error returned in case of an unauthorized user for clear all caches request.[EDELIVERY-8960] - Repeated logs related to clearing cache in weblogic cluster deployment.[EDELIVERY-8963] - Investigate startup error in Weblogic[EDELIVERY-8965] - Batch folder not deleted unless it is archived (or failed to be archived).[EDELIVERY-8966] - Exception from the retention worker when trying to delete archived messages.[EDELIVERY-8967] - Connection monitoring test message invalid if MPC validation is enabled[EDELIVERY-8969] - For messages in Send Failure state downloading the envelope results in empty zip[EDELIVERY-8970] - Domain selector is visible for admin and user roles[EDELIVERY-8973] - Export of acknowledged messages with 2 payloads fails.[EDELIVERY-8974] - After the first update value of domibus.property.length.max is not taken into consideration anymore[EDELIVERY-8975] - Init values for 2 new domibus jms properties.[EDELIVERY-9006] - Docker images don't contain clientauthentication.xml file making the TLS Trustore page untestable[EDELIVERY-9008] - Plugin user not linked to the final recipient of a message is able to download its payload.[EDELIVERY-9022] - Expired Partitions handling is not clear.[EDELIVERY-9025] - Deleting using partitions: messages are deleted before their retention expires.[EDELIVERY-9027] - Sometimes rest services return 500 error and message containing SQL statements[EDELIVERY-9028] - Changing preferred domain for super user is not logged in Audit[EDELIVERY-9029] - Add metadata for properties in the DSS module[EDELIVERY-9030] - Error message needs improvement in TLSTruststore page[EDELIVERY-9031] - Create on-the-fly metadata for external module properties[EDELIVERY-9032] - Fsplugin jobs are not created.[EDELIVERY-9033] - No confirmation pop up shown when removing certificate in TLS Trustore page[EDELIVERY-9035] - Error When upgrading Oracle to Domibus 50[EDELIVERY-9049] - Not possible to encrypt DSS extension's password.[EDELIVERY-9053] - MT: Unable to delete a user if it was not configured properly for MT[EDELIVERY-9054] - Add support for Content Security Policy (CSP)[EDELIVERY-9056] - Fix XML External Entity attacks[EDELIVERY-9067] - Typo in domibus-distribution-5.0-20220310.110709-562[EDELIVERY-9068] - Domibus 5.0 RC docs update[EDELIVERY-2602] - Improve error message.[EDELIVERY-5517] - UIReplication synchronization issue: improve UPDATE mechanism[EDELIVERY-6086] - Users page - Minor difference in csv file vs grid(Domain Name in UI and Domain Code in csv file)[EDELIVERY-7137] - The configuration for the default domain is confusing[EDELIVERY-7786] - Error when searching messages by Message Status when UI replication is enabled[EDELIVERY-7813] - Timezone Issues When Saving and Restoring Date Time Objects[EDELIVERY-7822] - Provide a solution to tackle the timezone issues detected[EDELIVERY-7842] - Pagination is reset to page 1 if user tries to navigate to another page as the grid is loading[EDELIVERY-8027] - Sometimes Domibus fails to start.[EDELIVERY-8286] - Errors when running oracle-5.0-SNAPSHOT.ddl[EDELIVERY-8333] - Delete Database Data Script Not Working[EDELIVERY-8338] - Update Domibus v5.0 Database Migration Process[EDELIVERY-8364] - MT/WL bamboo plan fails with ConstraintViolationException in PartyId dictionary tables[EDELIVERY-8419] - DSS Refresh Connection timeout parameter- development[EDELIVERY-8606] - It is possible to create a plugin standard user with an empty original user via rest call.[EDELIVERY-8609] - Metrics do not display properly[EDELIVERY-8624] - Circular bean dependency in domibus[EDELIVERY-8661] - Error logged when sending test message[EDELIVERY-8694] - Extra column in exported CSV from Trustore page[EDELIVERY-8723] - Compression: MimeType[EDELIVERY-8726] - Send 2 body attachments returns 200[EDELIVERY-8730] - mysql-5.0-SNAPSHOT-data imported on multi tenancy docker environment[EDELIVERY-8742] - Fix dynamic discovery for sending and receving messages[EDELIVERY-8743] - Improve the default password mechanism[EDELIVERY-8770] - Parties not marked as (IR) for processes[EDELIVERY-8783] - Error message shown when configuring https connector for tomcat server.[EDELIVERY-8788] - Message stuck in WAITING_FOR_RETRY state[EDELIVERY-8789] - UI replication should be removed[EDELIVERY-8797] - Upload PMode: IT Tests[EDELIVERY-8812] - FSPluginPropertiesMultitenantIT - fix tests[EDELIVERY-8828] - IT tests WSPLUGIN: tests for rest methods ignored[EDELIVERY-8833] - On business validation error before Msg creation in DB, throws foreign key violation error for TB_ERROR table[EDELIVERY-8844] - Metadata missing for property domibus.earchive.sanitizer.cron[EDELIVERY-8850] - JMS Monitoring page shows error when selecting a queue with more than 3000 messages in it[EDELIVERY-8851] - Messages page shows error when system has more than 6000 messages[EDELIVERY-8884] - First PullRequest in error Bamboo LOAD[EDELIVERY-8887] - Sometimes the messageId is still missing for some logs entries[EDELIVERY-8915] - Wrong error message returned related to certificate extraction.[EDELIVERY-8921] - Issues with REST endpoint to mark messages as DELETED in case message not linked to user[EDELIVERY-8948] - Issue when submitting messages with dynamic initator for pull MEP.[EDELIVERY-8963] - Investigate startup error in Weblogic[EDELIVERY-8966] - Exception from the retention worker when trying to delete archived messages.[EDELIVERY-8967] - Connection monitoring test message invalid if MPC validation is enabled[EDELIVERY-8969] - For messages in Send Failure state downloading the envelope results in empty zip[EDELIVERY-8973] - Export of acknowledged messages with 2 payloads fails.[EDELIVERY-8974] - After the first update value of domibus.property.length.max is not taken into consideration anymore[EDELIVERY-9008] - Plugin user not linked to the final recipient of a message is able to download its payload.[EDELIVERY-9020] - Typo in upgrade-info.txt for 5.0 (development branch)[EDELIVERY-9022] - Expired Partitions handling is not clear.[EDELIVERY-9025] - Deleting using partitions: messages are deleted before their retention expires.[EDELIVERY-9027] - Sometimes rest services return 500 error and message containing SQL statements[EDELIVERY-9028] - Changing preferred domain for super user is not logged in Audit[EDELIVERY-9029] - Add missing metadata for properties in the DSS module[EDELIVERY-9030] - Error message needs improvement in TLSTruststore page[EDELIVERY-9059] - Fix the cause for WARN log entries -wildfly WARN o.a.c.p.PhaseInterceptorChain:468 and WARN e.d.c.m.p.PullMessageSender[EDELIVERY-9062] - A tenant can disable the fsplugin even if it is the only installed plugin.[EDELIVERY-9073] - Error running script for multitenancy mysql-4.2.3-to-5.0-data-migration-step1-4[EDELIVERY-9075] - Running MySQL Migration Scripts Witout the root User[EDELIVERY-9078] - Database Migration Errors for Partitioning Columns[EDELIVERY-9079] - Original user not checked when authorizing access to ""ext/monitoring/messages/failed"" ressource[EDELIVERY-9080] - All failed messages are returned when requesting ""ext/monitoring/messages/failed"" ressource with an empty finalRecipient[EDELIVERY-9081] - 4.2.7 to 5.0 migration issue for Error Messages page and MySql DB[EDELIVERY-9082] - Error shown in Mesasge Log page when user opens Advanced filters and presses Reset button[EDELIVERY-9091] - Fix EAcrhive plan on bamboo[EDELIVERY-9094] - Error while getting role from MSHRoleEntity[EDELIVERY-9114] - Domibus returns an error about party role instead of partyId type.[EDELIVERY-9115] - Message without mime type is submitted correctly.[EDELIVERY-9120] - Tomcat OpenJDK 11/MySQL docker environment fails to start with error java.sql.SQLException: Connection is closed[EDELIVERY-9122] - FailSafe is not failing with IT failures[EDELIVERY-9124] - getMessageErrorsResponse returns an empty list when the messageId does not exists[EDELIVERY-9125] - Wildfly/MySQL MT - cannot login using super user after the first session expired[EDELIVERY-9131] - Domibus property has an extra special character in its default value.[EDELIVERY-9137] - Users of all roles can login and use the Domibus even if their domain is disabled[EDELIVERY-9142] - Error in the log that indicates that the fsplugin can't be disabled.[EDELIVERY-9143] - The value of a fsplugin property displayed in the admin console is misleading.[EDELIVERY-9144] - Weblogic Cluster MT environment is not starting[EDELIVERY-9145] - On fresh MT installation, the super user cannot login[EDELIVERY-9147] - BeanCreationException in log when the saveCertificateAndLogRevocationJob is triggered during startup[EDELIVERY-9148] - Circular reference in Domibus core[EDELIVERY-9166] - WS Plugin: The backend wsdl and xsd should be distributed as zip in the Domibus distribution module[EDELIVERY-9183] - NullPointerException when domibus receives a message without MessageProperties[EDELIVERY-9185] - Issue when running stress test with partition[EDELIVERY-9186] - domain prefixed fs plugin property is present in defaut fsplugin properties file.[EDELIVERY-9187] - 4.2.9 to 5.0 migration issue for Audit page and MySql DB[EDELIVERY-9192] - Modifying property domain.title makes the system unusable[EDELIVERY-9198] - After 4.2.9 to 5.0 migration I can't search for signal messages in Admin Console[EDELIVERY-9199] - After 4.2.9 to 5.0 migration info about missing envelops appears in logs[EDELIVERY-9201] - Domibus MySql DB migration from 4.2.9 to 5.0 is to slow[EDELIVERY-9202] - MessageProperties are not properly migrated when MIGRATE ONGOING MESSAGES 50 to 50[EDELIVERY-9203] - Review the indexes TB_MESSAGE_PROPERTIES - Inefficient table/index[EDELIVERY-9206] - In multitenancy mode, messages without mimetype are accepted[EDELIVERY-9213] - DEFAULT_USER_AUTOGENERATE_PASSWORD is set to FALSE regardless of user choice in docker image weblogic122[EDELIVERY-9220] - Domibus 5.0-RC1 upgrade from 4.2.8, SQL error when writing TB_ERROR_LOG[EDELIVERY-9222] - Error when sending dynamic discovery messages via fsplugin.[EDELIVERY-9225] - MySQL 4.2.9 to 5.0 migration issue - For dictionary tables the ID_PK is not correctly formatted[EDELIVERY-9226] - For TB_ERROR_LOG system not handling properly Null value for MSH_ROLE_ID_FK[EDELIVERY-9228] - When password is autogenerated and checkDefaultPassword is set to true super user is asked to change his password at first login[EDELIVERY-9229] - 4.2.9 to 5.0 migration issue - not always filters on ""Message Filter"" page after migration are in the same order[EDELIVERY-9231] - On a TCMT env, sometimes, login fails with ""object unsubscribed"" error message[EDELIVERY-9233] - Messages IDs are wrong and duplicate in the admin console.[EDELIVERY-9242] - Message in ""SEND_FAILURE"" status is not deleted by the partition deleting strategy.[EDELIVERY-9248] - java.lang.IllegalArgumentException: max-results cannot be negative[EDELIVERY-9249] - Error when restarting managed server[EDELIVERY-9250] - One stuck thread linked to earchiving[EDELIVERY-9252] - Warning is present in logs when sending a message using FS-plugin[EDELIVERY-9254] - Exception in earchiving[EDELIVERY-9261] - Error message when accessing the ""Alerts"" page in Domibus admin console.[EDELIVERY-9272] - Cannot login to 5.0-SNAPSHOT, no users created on DB[EDELIVERY-9276] - Blank page with error message shown when user tries to download CSV from TLS trustre page when no trustore has ever been uploaded[EDELIVERY-9277] - Fix warning: WARN o.s.s.c.a.w.b.WebSecurity:294 - You are asking Spring Security to ignore Ant [pattern=[EDELIVERY-9279] - IT tests cannot be run on windows[EDELIVERY-9281] - 500 error after user disables domain[EDELIVERY-9282] - Download message envelopes icon is enabled for SEND_ENQUEUED messages[EDELIVERY-9289] - Fix duplicate detection[EDELIVERY-9291] - Admin Console: the downloaded attachments should have the correct extension[EDELIVERY-9292] - The value used to detect blocked Quartz jobs should use a Domibus property[EDELIVERY-9308] - Domibus 5.0 Domain [UPPERCASE] cannot be found[EDELIVERY-9311] - Wrong message status indication in the logs.[EDELIVERY-9314] - When password properties encryption is activated some properties are encrypted by default[EDELIVERY-9315] - Not possible to create a plugin user with a peppol ""original user"" type[EDELIVERY-9317] - Domibus accepts messages signed with key corresponding to a different party[EDELIVERY-9319] - Domain name is case sensitive in the pmode.[EDELIVERY-9321] - Add/Remove certificate from TLS truststore page is not logged in Audit[EDELIVERY-9322] - Updating the property ""domibus.msh.retry.cron"" affects the number of retries.[EDELIVERY-9323] - Header mismatch in alert page CSV file[EDELIVERY-9324] - Email missing from users csv export[EDELIVERY-9326] - Headers from Message Log CSV export don't match the UI[EDELIVERY-9334] - No action on a certificate plugin user is logged in Audit[EDELIVERY-9335] - Importing certificate from Edit party popup is logged incorrectly in Audit page[EDELIVERY-9336] - Disabling the prefered domain of the super user crashes the system[EDELIVERY-9338] - JMS error shown when starting Wildfly server[EDELIVERY-9339] - Message acknowledgment not sent successfully from C3 to C2(wildfly)[EDELIVERY-9341] - Sometimes the OK button doesn't enable even if the create user dialog is filled with valid data[EDELIVERY-9344] - Domibus 5.0 WS plugin PUSH notifications CXF not visible[EDELIVERY-9345] - Delete All button deletes all messages from a queue even if there are filters applied[EDELIVERY-9346] - Add a second check to enable the partition deletion[EDELIVERY-9348] - Cyclic dependency exception[EDELIVERY-9350] - Fix PULL retry for lost receipt[EDELIVERY-9354] - Cannot enable new domain[EDELIVERY-9361] - Filtering by invalid JMS selector results in request stuck in pending forever[EDELIVERY-9362] - Opening Audit page as domain admin result in .SQLGrammarException[EDELIVERY-9366] - PMode duplicate detection IS case sensitive[EDELIVERY-9376] - fsplugin password properties are not encrypted.[EDELIVERY-9377] - In multitenancy, password properties are not encrypted in the main properties file.[EDELIVERY-9382] - Error for step3 of migration procedure for some test data[EDELIVERY-9383] - UnsatisfiedDependencyException (unresolvable circular reference) at deploy on Tomcat[EDELIVERY-9392] - Message exchange with dynamic receiver fails.[EDELIVERY-9407] - Performance optimization when consuming alerts[EDELIVERY-9408] - Wrong error message when disabling the default domain[EDELIVERY-9409] - Super user can disable it's preferred domain making it impossible for him to login afterwards[EDELIVERY-9410] - Error while triggering an alert for partition deletetion[EDELIVERY-9411] - Super user can login even if preferred domain is disabled[EDELIVERY-9412] - Error when changing the domibus.alert.active property dynamically from the Domibus Admin Console[EDELIVERY-9423] - Fix sonar bugs[EDELIVERY-9424] - Domibus accepts the message in case of decompression failure in C3 side.[EDELIVERY-9425] - 500 error after super tries multiple times to disable its own preffered domain[EDELIVERY-9438] - upgrade-info.txt file - Domibus upgrade procedure could be improved[EDELIVERY-9442] - Error reloading TLS in a clustered environment[EDELIVERY-9443] - WS Plugin Backend: Push notification exception in logs[EDELIVERY-9444] - MT Weblogic is failing with timeout exception[EDELIVERY-9446] - Circular reference issue on Wildfly docker[EDELIVERY-9451] - Weblogic Cluster java.sql.SQLIntegrityConstraintViolationException: ORA-00001: unique constraint (DOMIBUS.PRIMARY_05) violated[EDELIVERY-9452] - Migration scripts for ongoing messages between 5.0 and 5.0 - fails[EDELIVERY-9457] - Envelop zips contains file with name signal_message_envelope.xml only[EDELIVERY-9459] - Authorization Issue When Receiving Messages in Dynamic Discovery[EDELIVERY-9461] - inserted partition key does not map to any partition[EDELIVERY-9462] - 500 error when super user disables ""default"" domain after switching preferred domainTasks[EDELIVERY-967] - Split JUnit and integration tests[EDELIVERY-1694] - Configure hibernate to use a third party connection pool for Tomcat[EDELIVERY-2130] - Some statuses in the message flow diagram are not implemented in Domibus[EDELIVERY-3480] - TwoWay/PushAndPush - mandatory RefToMessageId[EDELIVERY-3636] - UIReplication: Improve performance of Messages page - Oracle hints and other optimizations[EDELIVERY-4095] - Implement integration test for testing large files using Docker Tomcat[EDELIVERY-4681] - Make Domibus port configurable on Fiware image at launch time[EDELIVERY-4716] - Create a builder Maven module[EDELIVERY-4734] - Remove dependency from JMX[EDELIVERY-4842] - TrustStore configured in Client Authentication XML should be exposed to the plugins per domain[EDELIVERY-4867] - Delete all messages from a queue[EDELIVERY-4879] - Possibility to force the Content-Length header on the C3 response[EDELIVERY-5419] - Implement setting of composable properties at run-time[EDELIVERY-6114] - Include the WebLogic tweaked configuration used in stress tests in Domibus installation scripts[EDELIVERY-6443] - Clean code within branch 6419 and merge to development.[EDELIVERY-6552] - Include small files SoapUI tests for split and join in Bamboo plans[EDELIVERY-6623] - Performance testing of Domibus 4.2 with Foreign Server in Taxud environment[EDELIVERY-6781] - Upgrade JMockit framework[EDELIVERY-6886] - Domibus should not use classes inside JDK package sun.*[EDELIVERY-7007] - Add Evict-Cache method on Rest api[EDELIVERY-7092] - Create Bamboo plan with DSS extension and UUMDS[EDELIVERY-7106] - Upgrade alerts tests are not stable enough to be move to Bamboo plan[EDELIVERY-7132] - Align fs-plugin.properties mechanism with the rest of the plugins[EDELIVERY-7139] - Prepare procedure for cleaning folders from git[EDELIVERY-7141] - Update bamboo plans to use new domibus-test repository[EDELIVERY-7142] - Domibus git repository cleanup[EDELIVERY-7147] - ReliabilityChecker analize and implement ReceiptionAwareness or NRR found but not expected[EDELIVERY-7270] - Encrypt the passwords in DSS extension[EDELIVERY-7280] - Hibernate and Configuration entity issue when adding new temporal sub-entities[EDELIVERY-7283] - Update all libraries to the latest version[EDELIVERY-7285] - Add Bamboo plans for 5.0[EDELIVERY-7366] - Improve the decoupling of the Domibus core from the plugin implementations[EDELIVERY-7383] - Update library - Liquibase to the latest version[EDELIVERY-7384] - Update library - Hibernate validator to the latest version[EDELIVERY-7385] - Update library - commons-vfs2 to the latest version[EDELIVERY-7390] - Update library - Apache cxf to the latest version[EDELIVERY-7393] - Update library - org.reflections to the latest version[EDELIVERY-7395] - Update library - activemq client to the latest version[EDELIVERY-7396] - Update library - wss4j-ws-security and xmlsec to the latest version[EDELIVERY-7401] - Update library - jaxb and related to the latest version[EDELIVERY-7404] - Update library - Ehcache to 3.8.x version[EDELIVERY-7409] - Tomcat IT tests - failing from time to time - javax.management.InstanceAlreadyExistsException[EDELIVERY-7411] - FSPlugin test - inconsistent result on Bamboo[EDELIVERY-7423] - Update library - Wildlfy 20 warning on guava new version[EDELIVERY-7428] - Analyse offloading the SSL traffic to an external component[EDELIVERY-7431] - Merge Dev to 5.0[EDELIVERY-7437] - Clean expired error logs that are not linked to message ids[EDELIVERY-7464] - Refactor IT tests which are using Thread.sleep()[EDELIVERY-7515] - FSPlugin should check about domain location and throw friendly exception[EDELIVERY-7553] - DomibusProperties API for Plugins mechanism get/set is not consistent because of Cacheable[EDELIVERY-7555] - Refactor DSS refresh command[EDELIVERY-7557] - Migration of ongoing messages from one database to a new database[EDELIVERY-7598] - Add a new method on domainTaskExecutor: submitWithSecurityContext that preserves the sec context[EDELIVERY-7610] - Refactor SetDomainFilter class to reuse getLoggedUser method from AuthenticationServiceBase[EDELIVERY-7616] - Update Bamboo plans after 4.2 release[EDELIVERY-7619] - Refactor soapui groovy scripts - after 4.2 - part 2[EDELIVERY-7620] - Extend test guide project to include sending simple message to Weblogic and Wildfly using Jms[EDELIVERY-7622] - Docker registry - Request job users and create wiki for the TO[EDELIVERY-7625] - Run full Taxud scenario on Azure environment[EDELIVERY-7627] - Upgrade OS on edelquality to latest OS version[EDELIVERY-7632] - Evaluate the best way to test a clustered WL, Oracle and UUMDS setup[EDELIVERY-7635] - Bamboo Mysql migration plans are failing since the merge of 5.0 to Dev[EDELIVERY-7636] - Failed unit test from 2021[EDELIVERY-7637] - weblogic deployment warnings in server log[EDELIVERY-7639] - Table ""SPRING_SESSION"" not found error in build logs[EDELIVERY-7641] - Bamboo Oracle + others migration plans should have failed since the merge of 5.0 to Dev[EDELIVERY-7643] - Default WS Plugin: Possibility to implement PUSH - Performance issue get finalRecipient + OriginalSender[EDELIVERY-7644] - Eliminate the use of Lazy injection of SignalService from DomibusCacheServiceImpl class[EDELIVERY-7646] - FSPluginPropertiesIT should be the only one loading test fsplugin.properties[EDELIVERY-7664] - Replace XStream library[EDELIVERY-7667] - Fix Sonar issues[EDELIVERY-7668] - Possibility to set the database schema version associated to the release[EDELIVERY-7669] - Adapt the compatibility plan for 5.0[EDELIVERY-7672] - Create new database structure[EDELIVERY-7694] - Investigate EUCEG PROD issue[EDELIVERY-7705] - Rest API to clear all caches expose to an external service[EDELIVERY-7738] - Rename *Test classes to *IT[EDELIVERY-7744] - Upgrade dss-utils libraries[EDELIVERY-7746] - Bamboo plan Domibus Plugin compatibility need buildKey to be added[EDELIVERY-7750] - Refactor soapui groovy scripts - after 4.2 - part 3[EDELIVERY-7775] - Domibus Liquibase - refactoring and organize scripts[EDELIVERY-7787] - Check the possibiliy to add maven-enforcer-plugin in order to sanitize project dependencies[EDELIVERY-7793] - Error on OracleXE for WSPlugin table[EDELIVERY-7799] - Investigate OWASP reported threats[EDELIVERY-7802] - Implement offloading the SSL traffic when Domibus communicates with SMP[EDELIVERY-7803] - Adapt the JPA entities to reflect the new database schema - part1[EDELIVERY-7804] - Create a basic list of security tests for domibus.[EDELIVERY-7812] - Missing property metadata for error log cleaner[EDELIVERY-7828] - Data source switching exception in Domibus 4.1[EDELIVERY-7832] - Create a basic list of security tests for domibus admin console.[EDELIVERY-7833] - Implement the migration script from 4.2 to 5.0 - part 1[EDELIVERY-7840] - Assign better names for indexes, foreign keys, etc in the liquibase scripts[EDELIVERY-7843] - Implement integration tests for the retention worker[EDELIVERY-7845] - Add missing remarks and remove self explanatory column remarks on the liquibase scripts[EDELIVERY-7848] - Upgrade DSS to version 5.8[EDELIVERY-7849] - Use UTC for now property defined in the SQL scripts[EDELIVERY-7850] - Adapt the JPA entities to reflect the new database schema - part2[EDELIVERY-7856] - Create functional tests for the retention worker[EDELIVERY-7877] - Data migration script from 4.2 to 5.0 - TB_USER_MESSAGE and TB_SJ_ tables[EDELIVERY-7878] - Data migration script from 4.2 to 5.0 - TB_RAWENVELOPE_LOG[EDELIVERY-7879] - Data migration script from 4.2 to 5.0 - TB_MESSAGE_LOG, TB_SIGNAL_MESSAGE, TB_RECEIPT, TB_RECEIPT_DATA[EDELIVERY-7892] - Implement the migration script from 4.2 to 5.0 - part 2[EDELIVERY-7895] - Align azure environment with TAXUD environment[EDELIVERY-7900] - Adapt the JPA entities to reflect the new database schema - part3[EDELIVERY-7923] - Junit Mapper DomibusExtMapper & MonitoringMapper[EDELIVERY-7927] - No warning when user have no roles[EDELIVERY-7947] - Multihop analysis[EDELIVERY-7948] - Adapt the JPA entities to reflect the new database schema - part4[EDELIVERY-7969] - [URGENT] TimeZone issue[EDELIVERY-7970] - Data migration script from 4.2 to 5.0 - integration with Liquibase[EDELIVERY-7972] - Implement the mapper between jpa entities and ebms3 classes[EDELIVERY-7973] - Fix DomibusCoreMapper following database schema refactoring - part 1[EDELIVERY-7974] - [WSPLUGIN] GetNestedProperties is not first level only anymore[EDELIVERY-7975] - Data migration script from 4.2 to 5.0 - TB_PART_INFO, TB_PROPERTY[EDELIVERY-7977] - Mysql Database migration plans are failing[EDELIVERY-7982] - Adapt the JPA entities to reflect the new database schema - part5[EDELIVERY-8006] - Data migration script from 4.2 to 5.0 - TB_ERROR_LOG, TB_MESSAGE_ACKNW, TB_MESSAGE_ACKNW_PROP[EDELIVERY-8011] - Use UTC for now property defined in the H2 SQL scripts[EDELIVERY-8012] - Merge the development branch into domibus50 branch[EDELIVERY-8013] - Merge the development branch into domibus50 branch[EDELIVERY-8014] - Merge the development branch into domibus50 branch[EDELIVERY-8015] - Merge the development branch into domibus50 branch[EDELIVERY-8016] - Fix the sending of a UserMessage from C2 to C3[EDELIVERY-8017] - Fix the receiving of a UserMessage from C3 to C2[EDELIVERY-8018] - Merge the development branch into domibus50 branch[EDELIVERY-8025] - Create automated tests in SoapUI for ext Domibus API - part 1[EDELIVERY-8038] - Fix the sending and receiving for Split and Join[EDELIVERY-8045] - Analyze the UTC_TIMESTAMP issue in Mysql8[EDELIVERY-8046] - Add tasks in bamboo plan to run soapUI tests against new ws plugin.[EDELIVERY-8047] - Merge the development branch into domibus50 branch[EDELIVERY-8048] - Merge the development branch into domibus50 branch[EDELIVERY-8049] - Merge the development branch into domibus50 branch[EDELIVERY-8050] - Merge the development branch into domibus50 branch[EDELIVERY-8051] - Refactor the integration tests architecture[EDELIVERY-8052] - Failing tests must be ignored[EDELIVERY-8055] - Investigate OWASP reported threats[EDELIVERY-8056] - Merge the development branch into domibus50 branch[EDELIVERY-8057] - Create Bamboo plan for domibus50 branch[EDELIVERY-8062] - Create automated tests in SoapUI for ext Domibus API - part 2[EDELIVERY-8065] - default boolean value Domibus property[EDELIVERY-8067] - Database migration plans are failing[EDELIVERY-8068] - MessageAcknowledgeConverter: call userMessage[EDELIVERY-8069] - Core Queue name in api[EDELIVERY-8073] - Investigate/fix OWASP reported threats (Angular)[EDELIVERY-8083] - Merge EDELIVERY-7779 into domibus50 branch[EDELIVERY-8084] - Fix the ErrorLog page (admin console)[EDELIVERY-8085] - Fix the UserMessage download feature (admin console)[EDELIVERY-8113] - Errors after IT tests are shutting down[EDELIVERY-8116] - Merge DSS 5.8 into Domibus 5.0 branch[EDELIVERY-8117] - Revert Authorization spi changes[EDELIVERY-8118] - Fix the soapUI tests after Domibus DB updates in 5.0 releases.[EDELIVERY-8119] - Improve performance of findFailedMessages and findSendEnqueuedMessages by removing left join on TB_PROPERTY table[EDELIVERY-8124] - Investigate OWASP reported threats (Angular)[EDELIVERY-8125] - Implement the migration script from 4.2 to 5.0 for MySQL- part 1[EDELIVERY-8126] - Analyze how to adapt the SQL query used for eArchiving to unzip the payloads for sent messages[EDELIVERY-8128] - Performance improvements when C2 sends messages - part 1[EDELIVERY-8141] - Transform MessageLogInfoFilterTest & derivates into IT tests[EDELIVERY-8160] - Update soapUI project for the new plugin implementation.[EDELIVERY-8161] - Update security related soapUI tests.[EDELIVERY-8165] - Fix failing Tomcat-Wildfly Docker Bamboo plan[EDELIVERY-8166] - Merge 4.2.2 branch into domibus50 branch[EDELIVERY-8176] - [SOAP UI] Investigation sending multipart[EDELIVERY-8219] - Optimize pull locking - PoC with high number of messages in the JMS queue[EDELIVERY-8224] - Change the Domibus plugin specification for pulling.[EDELIVERY-8226] - Merge domibus50 into development[EDELIVERY-8231] - Integrate the the override the default identity generator with MySQL identify generator[EDELIVERY-8237] - Integrate POC for new hibernate sequence format into Domibus[EDELIVERY-8242] - Make 5.0 work on automation[EDELIVERY-8243] - Improve elastic search dashboard[EDELIVERY-8244] - Investigate OWASP reported threats[EDELIVERY-8245] - Document the received notifications in the default plugins[EDELIVERY-8246] - Document all JMS queues + SO presentation[EDELIVERY-8253] - Adapt soapUI tests to have seperate project files.[EDELIVERY-8261] - OWASP report for Test module[EDELIVERY-8265] - Write eArchiving solution document[EDELIVERY-8274] - Estimate Taxud MoU proposal[EDELIVERY-8275] - TAPAS | eArchiving requirements document analysis[EDELIVERY-8282] - Implement downgrade procedure from 5.0 to 4.2.x[EDELIVERY-8291] - Fix Bamboo plans[EDELIVERY-8299] - Adapt soapUI project files for new/old ws plugins.[EDELIVERY-8300] - Monitor bamboo plans and fix errors in case needed[EDELIVERY-8320] - Adapt soapUI project files for new/old ws plugins: target manual test suites.[EDELIVERY-8322] - Asses Domibus 4.2.2 performance in the automation environment[EDELIVERY-8362] - Fix the vulnerabilities from the ZAP report[EDELIVERY-8363] - Review eArchiving solution document following Taxud comments[EDELIVERY-8378] - Update retention worker to delete partitions[EDELIVERY-8379] - Update the MySql entity id generation to the same format as the oracle hibernate sequence.[EDELIVERY-8380] - Change the queries used in the code to make use of the new format.[EDELIVERY-8383] - LocalDateTime Should Always Use UTC Timezone When Converting to Date[EDELIVERY-8384] - Fix failing Bamboo plans following database refactoring[EDELIVERY-8387] - Investigate pending issues for bamboo plans.[EDELIVERY-8400] - Refactor ErrorLogDao[EDELIVERY-8401] - Automation - Add weblogic monitoring[EDELIVERY-8402] - Performance baseline for Domibus 5.0[EDELIVERY-8403] - Merge 4.2.3 into development[EDELIVERY-8426] - [TOMCAT] ehcache warnings[EDELIVERY-8430] - eArchiving: possibility to create the EARK-SIP batch folder structure - part 1[EDELIVERY-8431] - Support for EUCEG[EDELIVERY-8432] - Refactor Domibus sending mechanism to use small transactions[EDELIVERY-8435] - eArchiving: possibility to create the METS.xml manifest file[EDELIVERY-8440] - Adapt server configuration files to remove eDeliveryDs XA transaction[EDELIVERY-8441] - Migration of Primary Keys From the v4.2.x Format to the New v5.0 Format - part 1[EDELIVERY-8443] - FileStorage: refactor to reuse the VFS in eArchiving functionality[EDELIVERY-8444] - Implement downgrade procedure from 5.0 to 4.2.x[EDELIVERY-8445] - Performance baseline for Domibus 5.0 with higher disk IO - part 1[EDELIVERY-8448] - Move the Domibus properties from the the configuration file into the database - part 1[EDELIVERY-8449] - eArchiving: possibility to create the EARK-SIP batch folder structure - part 2[EDELIVERY-8450] - eArchiving: implement the continuous export mechanism - part 1[EDELIVERY-8451] - eArchiving: implement the Domibus REST interface - part 1[EDELIVERY-8452] - Refactor Domibus sending mechanism to use small transactions - part 2[EDELIVERY-8459] - Upgrade OpenApi version to 3.0.1 and use the lates swagger tools (libs and plugins)[EDELIVERY-8468] - Investigate the possiblity to use of zapp tool for security tests.[EDELIVERY-8471] - Automation - Add weblogic monitoring[EDELIVERY-8475] - Some of the ws-plugin properties should be 'domain' properties[EDELIVERY-8487] - eArchiving: implement the continuous export mechanism - part 2[EDELIVERY-8488] - Move the Domibus properties from the the configuration file into the database - part 2[EDELIVERY-8489] - eArchiving: implement the logic behind the Domibus REST interface - part 1[EDELIVERY-8491] - Performance baseline for Domibus 5.0 and Domibus 4.2.4[EDELIVERY-8492] - Create ICD for the old WS Plugin[EDELIVERY-8493] - Refactor Domibus sending mechanism to use small transactions - part 3[EDELIVERY-8494] - eArchiving: load testing of the continuous export mechanism on local environment[EDELIVERY-8498] - Modify the Bamboo plans to use the new multitenancy configuration structure[EDELIVERY-8499] - Investigate the impact of adapting the TAPAS specific configuration[EDELIVERY-8504] - Perform stress test on 4.2.4 on loaded db as asked by TAXUD[EDELIVERY-8513] - Test if a rolling upgrade is possible in WebLogic cluster[EDELIVERY-8514] - TomcatIT tests failing[EDELIVERY-8519] - Make the jms pull plugin backward compatible for TAPAS[EDELIVERY-8521] - Implement/test the retention mechanism with multitenancy[EDELIVERY-8522] - Test Domibus 5.0 snapshot with enhanced transactions on the sending side.[EDELIVERY-8525] - Upgrade libraries to the latest version[EDELIVERY-8526] - Messages should be deleted only by the retention worker[EDELIVERY-8527] - Possibility to add a new domain without downtime - part1[EDELIVERY-8529] - REST endpoint to mark messages as DELETED[EDELIVERY-8530] - Refactor Domibus receiving mechanism to use small transactions - part 1[EDELIVERY-8551] - eArchiving: Implementation of the eAchive rest api-client/servlet example[EDELIVERY-8554] - Sometimes, domibus weblogic cluster deployed on docker fails to start properly.[EDELIVERY-8568] - Possibility to add a new domain without downtime - part 2[EDELIVERY-8574] - Start prepare test cases for earchiving feature.[EDELIVERY-8575] - Progress on security tests.[EDELIVERY-8578] - eArchiving: clear exported files after the notification from the archiving client[EDELIVERY-8579] - Refactor Domibus receiving mechanism to use small transactions - part 2[EDELIVERY-8583] - eArchiving: Sanitizer[EDELIVERY-8585] - eArchiving: batch.json[EDELIVERY-8588] - Improve Swagger distribution[EDELIVERY-8592] - WildFly startup error when upgrading Spring[EDELIVERY-8608] - Create Webhook Rest API module for eArchiving in Domibus[EDELIVERY-8611] - Move tests automated for local run to the bamboo test suite - Part 1[EDELIVERY-8612] - Perform round tripping test for Maarten[EDELIVERY-8615] - eArchiving: implement the continuous export mechanism - part 3[EDELIVERY-8616] - Explore feature and create simplest tests for eArchiving[EDELIVERY-8621] - Merge 4.2.5 changes in 4.2.6 and development branches[EDELIVERY-8622] - Refactor Domibus receiving mechanism to use small transactions - part 3[EDELIVERY-8623] - eArchiving: create bamboo-docker with example integration tests[EDELIVERY-8637] - eArchiving: simplify with messageId in Table Batch_UM[EDELIVERY-8648] - Analyze licensing of libraries in Domibus[EDELIVERY-8653] - Create azure C2 with Oracle and Weblogic.[EDELIVERY-8655] - Zip raw envelopes before saving to database[EDELIVERY-8659] - Progress in writing functional eArchiving test cases[EDELIVERY-8660] - Prepare the eArchiving testing startegy for the configuration and data preparation.[EDELIVERY-8678] - Exported batch enahncement[EDELIVERY-8679] - Migration of Values for Columns that Have Become Not Nullable in 5.0[EDELIVERY-8696] - [Multitenancy] Migration of Primary Keys From the v4.2.x Format to the New v5.0 Format - part 3[EDELIVERY-8703] - [UI] Update the Messages page to show the default search interval[EDELIVERY-8704] - Migration script to compress envelopes in the database[EDELIVERY-8707] - Client for easy way to prepare test data in Domibus:[EDELIVERY-8712] - Analyze the security report[EDELIVERY-8713] - Run performance tests for Domibus 5.0[EDELIVERY-8715] - eArchiving: add batchId to /ext/archive/batches/exported[EDELIVERY-8731] - UI property update domibus.earchive.active[EDELIVERY-8738] - Review eArchiving proposal document following changes in the implementation[EDELIVERY-8739] - Improve code coverage[EDELIVERY-8741] - Investigate high memory consumption for large files[EDELIVERY-8746] - Create REST endpoint to create plugin users using plugin admin user[EDELIVERY-8751] - Rolling upgrade of Domibus to a new version produces error[EDELIVERY-8818] - Run performance tests for Domibus 5.0[EDELIVERY-8819] - Implement end to end test in Bamboo for earchiving[EDELIVERY-8822] - Merge 4.2.6 into development[EDELIVERY-8824] - Adapt/align docker to cope with the new user creation process.[EDELIVERY-8827] - Test eArchiving feature.[EDELIVERY-8839] - Specify that the continuous export date is rounded to hours.[EDELIVERY-8845] - Analyze and provide feedback to Taxud raised issue regardin the migration scripts[EDELIVERY-8876] - Support for certificate policy check on dynamic discovery[EDELIVERY-8879] - Run performance tests for Domibus 5.0[EDELIVERY-8891] - IT test for the quartz start/pause/resume features from DomibusQuartzStarter/DomibusSchedulerExtService[EDELIVERY-8892] - Fix the ignored unit tests[EDELIVERY-8893] - Check that the reply to the EO is working fine between DB schemas[EDELIVERY-8894] - Check that the migration of messages from 4.2 to 5.0 can be done on a live Domibus instance[EDELIVERY-8895] - Check the SQL scripts of the migration procedure[EDELIVERY-8912] - Message retention mechanism with stored procedure should be deleted[EDELIVERY-8918] - Failing ignored tests must be fixed and re-enabled - part 1[EDELIVERY-8935] - Merge 4.2.7 into development[EDELIVERY-8936] - Run performance tests for Domibus 5.0[EDELIVERY-8937] - All libraries should be migrated to the latest version[EDELIVERY-8940] - Oracle job to create partitions[EDELIVERY-8971] - CacheExtResourceIT should not use mocks[EDELIVERY-8983] - Tests maintenance activities.[EDELIVERY-8992] - Failing ignored tests must be fixed and re-enabled - part 2[EDELIVERY-9007] - EDelivery Platform migration - new URL with ""digital-building-blocks"".[EDELIVERY-5544] - Admin console user can move message to same Jms queue[EDELIVERY-6182] - Session IDs stored in cookies shall have the ""Secure"" flag set to TRUE[EDELIVERY-6183] - Session IDs stored in cookies should have the domain attribute blank[EDELIVERY-6185] - Only accept cookies as a means for session ID exchange management[EDELIVERY-6186] - Session idle timeouts[EDELIVERY-6189] - The HSTS header should be pre-loaded into browsers[EDELIVERY-6191] - The X-Frame-Options header shall be used[EDELIVERY-6609] - Eliminate the use of Lazy injection/applicationContext.getBean in domibus property management.[EDELIVERY-7374] - Non-XA switch for 3 servers: Tomcat, WildFly and WebLogic[EDELIVERY-7375] - Refactoring the database to include the performance optimizations[EDELIVERY-7376] - Fix the UI due to the impact of refactoring the database[EDELIVERY-7410] - Decouple Jaxb annotations from the Jpa annotation for the ebms3 classes - part 1[EDELIVERY-7426] - Change the truststore location, type and password in tandem.[EDELIVERY-7427] - Truststore Backup[EDELIVERY-7435] - Domibus proxy password should be able to change the at runtime[EDELIVERY-7439] - Prefer Startup Server Parameters to Modifying the domibus.properties File[EDELIVERY-7445] - Non-XA switch for 3 servers: Tomcat, WildFly and WebLogic - part 1[EDELIVERY-7447] - Create new version of Domibus WS Plugin[EDELIVERY-7475] - Default WS Plugin: Possibility to implement PUSH - enable/disable property[EDELIVERY-7476] - Default WS Plugin: Possibility to implement PUSH - reschedule worker[EDELIVERY-7477] - Default WS Plugin: Possibility to implement PUSH - Implement Submit message for C4[EDELIVERY-7478] - Default WS Plugin: Possibility to implement PUSH - use JMS queue[EDELIVERY-7479] - Default WS Plugin: Possibility to implement PUSH - send Alert in case of failure[EDELIVERY-7480] - Default WS Plugin: Possibility to implement PUSH - retry[EDELIVERY-7495] - Default WS Plugin: Possibility to implement PUSH - Change Message Status[EDELIVERY-7510] - Split SuperUserManagementService class in 2 classes[EDELIVERY-7521] - Some properties should be excluded from validations because it could lead to situations where user cannot update properties anymore[EDELIVERY-7547] - Set the security context in the base class of Job classes[EDELIVERY-7567] - Package javax.xml.* not available in OpenJDK. Find alternative[EDELIVERY-7618] - [UI] search fields should remove spaces[EDELIVERY-7670] - Disable autocommit in hibernate and disable aggressive release[EDELIVERY-7677] - Display Service and Action information in the Domibus Admin console.[EDELIVERY-7680] - Default WS Plugin: Possibility to implement PUSH - update ICD wsPlugin[EDELIVERY-7783] - DomainCoreDefaultConverter should be removed[EDELIVERY-7824] - FS Plugin should use only MT for multiple tenants[EDELIVERY-7826] - FS Plugin: a tenant should have the option to optout[EDELIVERY-7829] - DomainExtConverter/DomainExtDefaultConverter should be removed[EDELIVERY-7847] - Non-XA switch for 3 servers: Tomcat, WildFly and WebLogic - part 2[EDELIVERY-7851] - Hibernate transaction isolation default value[EDELIVERY-7852] - Investigate if the hibernate sequence should be scalable[EDELIVERY-7863] - FS Plugin - remove decryption service calls[EDELIVERY-7881] - We should not use interfaces for defining Constants[EDELIVERY-7903] - hibernate.jdbc.fetch_size should configurable[EDELIVERY-7937] - Remove Joda Time Dependency[EDELIVERY-7976] - Possibility to override the default identity generator with MySQL identify generator[EDELIVERY-8077] - Activate second level cache for all dictionary entities[EDELIVERY-8167] - Fix the UI due to the impact of refactoring the database - part 1[EDELIVERY-8225] - Integrate hibernate.jdbc.fetch_size into domibus50[EDELIVERY-8376] - JMSPlugin: use a qualifier when injecting jndi destination resolver[EDELIVERY-8647] - Possibility to change the keystore without having to manually modify the db data[EDELIVERY-8768] - PMode validation: we shouldn't accept invalid signatureMethod values[EDELIVERY-8841] - Response message in case batch ID does not existe can be improved.[EDELIVERY-8860] - Display the failed batches in the response of the request ""history of exported batches""[EDELIVERY-8862] - Response message in case batch ID does not existe can be improved (manual export request).[EDELIVERY-8942] - List queued export request specific filter response.[EDELIVERY-8976] - Domain selector dropdown should have ""default"" domain as first option and the rest to be sorted alphabetically[EDELIVERY-9016] - No indication that keystore was successfully reloaded.[EDELIVERY-9055] - Change to SHA-256 algorithm fot certificate fingerprintsEDELIVERY-6948] - Internal Xerces class used[EDELIVERY-7270] - Encrypt the passwords in DSS extension[EDELIVERY-7280] - Hibernate and Configuration entity issue when adding new temporal sub-entities[EDELIVERY-7610] - Refactor SetDomainFilter class to reuse getLoggedUser method from AuthenticationServiceBase[EDELIVERY-7620] - Extend test guide project to include sending simple message to Weblogic and Wildfly using Jms[EDELIVERY-7744] - Upgrade dss-utils libraries[EDELIVERY-7750] - Refactor soapui groovy scripts - after 4.2 - part 3[EDELIVERY-7834] - Implement the scripts from scratch for the UI replication tables[EDELIVERY-7971] - Edelquality plans: Roles are not checked by default[EDELIVERY-8119] - Improve performance of findFailedMessages and findSendEnqueuedMessages by removing left join on TB_PROPERTY table[EDELIVERY-8427] - [WILDFLY] jar does not point to a valid jar for a Class-Path reference.[EDELIVERY-8602] - Migration of Primary Keys From the v4.2.x Format to the New v5.0 Format - part 2[EDELIVERY-8654] - Zip payloads before saving to database/file system[EDELIVERY-8679] - Migration of Values for Columns that Have Become Not Nullable in 5.0[EDELIVERY-8824] - Adapt/align docker to cope with the new user creation process.[EDELIVERY-8846] - Migration scripts for ongoing messages between 5.0 and 5.0[EDELIVERY-8892] - Fix the ignored unit tests[EDELIVERY-8894] - Check that the migration of messages from 4.2 to 5.0 can be done on a live Domibus instance[EDELIVERY-8923] - Update DB upgrade instructions[EDELIVERY-8955] - Add task scheduler to the backendJmsListenerContainerFactory bean[EDELIVERY-8986] - Migration scripts for ongoing messages between 5.0 and 5.0[EDELIVERY-8992] - Failing ignored tests must be fixed and re-enabled - part 2[EDELIVERY-9007] - EDelivery Platform migration - new URL with ""digital-building-blocks"".[EDELIVERY-9013] - Update the domibus test guide doucment for the 5.0 release.[EDELIVERY-9026] - Upgrade Tomcat and Wildfly version in Domibus 5.0 and define upgrade procedure for older versions using the distribution[EDELIVERY-9039] - Upgrade cxf, wss4j and xmlsec to the latest version[EDELIVERY-9040] - Domibus 5.0 documentation update[EDELIVERY-9085] - Fix the links pointing to the old cefdigital website[EDELIVERY-9086] - Migration scripts ongoing messages between 5.0 and 5.0 for new 5.0 tables - Split & Join / eArchiving / WS Plugin Message Log[EDELIVERY-9089] - Update Tomcat and Wildfly versions in docker[EDELIVERY-9099] - Trigger alert when partitions are not properly created in advance[EDELIVERY-9100] - Integrate the basic message exchange tests of taxud to a bamboo plan.[EDELIVERY-9263] - Adapt the retry worker to run less frequently[EDELIVERY-9313] - Update ActiveMQ to the latest version[EDELIVERY-8656] - eArchiving: improve data in the notifications sent to the earchiving client[EDELIVERY-9009] - Log error when specific jms properties are null.[EDELIVERY-9010] - When message is not found in payload download rest request, 400 is returned.[EDELIVERY-9023] - Partition logs are not clear.[EDELIVERY-9034] - User should be permited to add certificate even if all certificates have been deleted from TLS Trustore page[EDELIVERY-9060] - Move the Lazy annotation in a better place in FsPlugin[EDELIVERY-9071] - Domibus property for the default value of the recent parameter drop-down[EDELIVERY-9083] - Please confirm that warning about different number of records for MIGR_TB_MESSAGE_PROPERTIES and TB_PROPERTY is not an issue[EDELIVERY-9119] - Http error code for ressource /ext/monitoring/messages/delete in case plugin user is not authorized.[EDELIVERY-9184] - WS Plugin Backend: Push notification log cxf[EDELIVERY-9381] - Description of property domibus.quartz.trigger.blocked.duration can be improved[EDELIVERY-9390] - TLS trustore changes in the Audit page should list a valid db IDKnown issues and limitations[EDELIVERY-2608] - Unable to use Admin Console in IE (not EDGE)[EDELIVERY-3250] - Improve sending a message to the same Access Point[EDELIVERY-9394] - Domibus not loading rest controllers defined in plugins[EDELIVERY-9692] - Data migration script from 4.2.9 to 5.0 fails on Oracle 12c[EDELIVERY-10036]  - Missing messages on message log page after 5.0 data migration[EDELIVERY-10460]  - Reload key issue: due to a caching issue in the reload keystore functionality, Domibus requires restart when changing the keystore.For more information, please contact us via our portal or by e-mail: EC-EDELIVERY-SUPPORT@ec.europa.eu"
77,Overview
77,Content Tools
77,Activity
77,Services & information
77,eDelivery
77,eID
77,eInvoicing
77,eSignature
77,Once-Only Technical System (OOTS)
77,About the Digital Building Blocks
77,Access Help Desk(opens in a new tab)
77,You can find information on how your personal data will be processed in thisprivacy statement.
77,Subscribe to our newsletter
77,"Find out all the latest on the building blocks and related news, from technical updates and events to exciting new examples of building block reuse and programme-wide initiatives!"
77,Subscribe (opens in a new tab)
77,Legal notice (opens in a new tab)
77,Cookies (opens in a new tab)
77,Master Service Arrangement
77,Privacy Statement
77,Powered by a free Atlassian Confluence Open Source Project License granted to EC DIGIT. Evaluate Confluence today.
77,Powered by Atlassian Confluence 7.19.18
77,Printed by Atlassian Confluence 7.19.18
77,Report a bug
77,Atlassian News
77,Atlassian
77,"{""serverDuration"": 1256, ""requestCorrelationId"": ""402d14ec64ce4e32""}"
78,Dedicated with Large Site | Tuning Guide | LiteSpeed Web Server | LiteSpeed Documentation
78,Skip to content
78,LiteSpeed Documentation
78,Dedicated with Large Site
78,Initializing search
78,GitHub
78,Get Started
78,LiteSpeed Web Server
78,LiteSpeed Web ADC
78,LSCache
78,Cloud
78,Licenses
78,Other Products
78,LiteSpeed Documentation
78,GitHub
78,Get Started
78,Get Started
78,Welcome
78,LiteSpeed Web Server
78,LiteSpeed Web Server
78,Overview
78,Trial License Installation
78,Installation
78,Updates
78,Configuration
78,Configuration
78,Configuration
78,reCAPTCHA
78,Security Headers
78,Bubblewrap
78,cgroups
78,OCSP Stapling
78,Command Reference
78,Real-Time Stats
78,Tuning
78,Tuning
78,Shared Hosting Server
78,Dedicated with Large Site
78,Dedicated with Large Site
78,Table of contents
78,Enable Caching
78,Increase Maximum Connections
78,Avoid Forking PHP Worker Processes
78,Increase PHP Running Time
78,Move PHP Session Data
78,Check PHP Info
78,timezonedb
78,xdebug
78,snmp
78,Troubleshooting
78,Troubleshooting
78,WebAdmin Console
78,External Applications
78,External Applications
78,PHP
78,PHP
78,Overview
78,Getting Started
78,Configuration
78,Configuration
78,Controlling LSPHP
78,LSPHP Modes
78,LSPHP Options
78,Advanced
78,Troubleshooting
78,Troubleshooting
78,Overview
78,503
78,Improve PHP Performance
78,How-To
78,Advanced
78,Extensions
78,Perl
78,Perl
78,Overview
78,Perl Configuration
78,Control Panels
78,Control Panels
78,cPanel
78,cPanel
78,Overview
78,Quick Start Guide
78,WHM LiteSpeed Plugin
78,WHM LiteSpeed Plugin
78,Installation
78,Switching Licenses
78,WP Cache Management
78,cPanel Plugin
78,Troubleshooting
78,PHP
78,PHP
78,PHP
78,Per-User php.ini
78,Override Auto-Detected PHP
78,PHP Selector
78,TimeZoneDB
78,CloudLinux
78,LSCache Setup
78,Security
78,Security
78,ModSecurity/WAF
78,DDoS Attack Protection
78,reCAPTCHA Protection
78,WordPress Protection
78,Tuning
78,ZeroConf Clusters
78,How-To
78,How-To
78,Enable or Disable QUIC and HTTP/3
78,GeoLocation Support
78,Cloudflare Issues
78,mod_pagespeed
78,Websocket Proxy
78,Rewrite Rule Proxy
78,PHP Without Timeout
78,LSMCD for MultiUser
78,Compression
78,ECC Certificates Quick Start
78,OCSP Stapling on cPanel
78,Troubleshooting
78,Troubleshooting
78,Apache Migration FAQ
78,403 Error
78,500 Error
78,503 Error
78,Fix High Load
78,Server Issues
78,Enabling Rewrite Logs
78,Toggle Debug Logging
78,Submit a Bug Report
78,Uninstall
78,Plesk
78,Plesk
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,Troubleshooting
78,General
78,403 Error
78,DirectAdmin
78,DirectAdmin
78,Overview
78,Installation
78,Configuration
78,FAQ
78,CyberPanel
78,CyberPanel
78,Overview
78,Enhance
78,Enhance
78,Overview
78,Interworx
78,Interworx
78,Overview
78,Installation
78,LSCache Setup
78,Benchmarking Tips
78,LiteSpeed Web ADC
78,LiteSpeed Web ADC
78,Overview
78,Installation
78,Configuration
78,Configuration
78,Basic Configuration
78,Additional Configuration
78,Cache Configuration
78,Security Configuration
78,ZeroConf Configuration
78,Commands Reference
78,Troubleshooting
78,Frequently Asked Questions
78,LSCache
78,LSCache
78,What is LiteSpeed Cache?
78,Basic Concepts
78,Getting Started
78,Troubleshooting
78,Tips
78,LSCache Plugins
78,LSCache Plugins
78,CS-Cart
78,CS-Cart
78,Overview
78,Installation
78,Troubleshooting
78,FAQ
78,Drupal
78,Drupal
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,FAQ
78,Joomla!
78,Joomla!
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,FAQ
78,Laravel
78,Laravel
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,FAQ
78,Magento
78,Magento
78,Overview
78,Installation
78,Configuration
78,Crawler Script
78,Troubleshooting
78,FAQ
78,MediaWiki
78,MediaWiki
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,FAQ
78,OpenCart
78,OpenCart
78,Overview
78,Installation
78,Configuration
78,Troubleshooting
78,FAQ
78,PrestaShop
78,PrestaShop
78,Overview
78,Installation
78,Configuration
78,Crawler Script
78,Troubleshooting
78,FAQ
78,WordPress
78,WordPress
78,Overview
78,Installation
78,Beginner's Guide
78,Configuration
78,Configuration
78,Dashboard
78,Presets
78,General
78,Cache
78,CDN
78,Image Optimization
78,Page Optimization
78,Database
78,Crawler
78,Toolbox
78,LiteSpeed Options Metabox
78,Troubleshooting
78,Troubleshooting
78,Troubleshooting Guide
78,CSS/JS Issues
78,Media Issues
78,Crawler Issues
78,CDN Support Issues
78,Admin
78,API
78,WordPress CLI
78,Third Party Compatibility
78,FAQ
78,XenForo
78,XenForo
78,Overview
78,Installation and Configuration
78,Troubleshooting
78,FAQ
78,Third Party Plugins
78,Third Party Plugins
78,Craft CMS
78,Craft CMS
78,Overview
78,Installation
78,Troubleshooting
78,FAQ
78,Flarem
78,Flarem
78,Overview
78,JTL-Shop
78,JTL-Shop
78,Overview
78,LSCache Without a Plugin
78,LSCache Without a Plugin
78,Overview
78,Installation
78,Configuration
78,LSCache Developers Guide
78,LSCache Developers Guide
78,Overview
78,Basic Controls
78,Advanced Concepts
78,Usage Examples
78,Cloud
78,Cloud
78,Images
78,Images
78,What are Cloud Images?
78,WordPress
78,CyberPanel
78,Node.js
78,Django
78,Rails
78,Joomla!
78,Drupal
78,Virtuozzo
78,Virtuozzo
78,What is Virtuozzo?
78,WordPress
78,Magento
78,Docker
78,Docker
78,What is Docker?
78,OpenLiteSpeed
78,LiteSpeed Enterprise
78,WordPress + OLS
78,WordPress + LSWS
78,Drupal
78,Magento
78,PrestaShop
78,Node.js OLS Reverse Proxy
78,Kubernetes
78,Kubernetes
78,What is Kubernetes?
78,Installation
78,Usage Considerations
78,Samples
78,Helm Configuration
78,Controller Configuration
78,Load Balancer Configuration
78,Metrics and Prometheus
78,Gateway
78,Advanced Deployments
78,Troubleshooting
78,RunCloud
78,RunCloud
78,Overview
78,WordPress
78,Licenses
78,Licenses
78,Overview
78,Products
78,Products
78,LiteSpeed Web Server
78,LiteSpeed Web Server with CyberPanel
78,LiteSpeed Web ADC
78,LiteSpeed Cache
78,Trial
78,How-To
78,Troubleshooting
78,Billing
78,FAQs
78,Other Products
78,Other Products
78,Overview
78,QUIC.cloud CDN
78,QUIC.cloud CDN
78,What is QUIC.cloud?
78,LiteSpeed Memcached
78,LiteSpeed Memcached
78,What is LiteSpeed Memcached?
78,Getting Started
78,Configuration
78,Configuration
78,Configuration Overview
78,General
78,Replication
78,SASL
78,cPanel Plugin
78,Commands
78,Troubleshooting
78,OpenLiteSpeed
78,OpenLiteSpeed
78,What is OpenLiteSpeed?
78,Table of contents
78,Enable Caching
78,Increase Maximum Connections
78,Avoid Forking PHP Worker Processes
78,Increase PHP Running Time
78,Move PHP Session Data
78,Check PHP Info
78,timezonedb
78,xdebug
78,snmp
78,"Dedicated Server with a Large Site¶ A dedicated server has different challenges than a shared hosting server. With shared hosting, usually you wish to give high performance without allowing a single user to monopolize server resources. However, on a dedicated server with a relatively large site, you want to maximize the performance without limiting the usage. Most of the default LiteSpeed WebAdmin Console settings are suitable for both scenarios, but there are a few exceptions. Here are some settings that you can adjust to push the maximum capacity. Enable Caching¶ Caching is the key to improving site performance. It is fundamental. If possible, you should always enable caching. If there is an LSCache plugin available for your application, use it. If a plugin is not available, you may also consider configuring LSCache through rewrite rules. Be sure to test for a cache hit before undertaking any benchmark testing. Increase Maximum Connections¶ With caching enabled to avoid PHP and MySQL processes, the site will undoubtedly be faster. However, there will still sometimes be uncached pages, such as shopping cart checkout pages, or other pages that have expired in cache. For this content, the performance of the application is at the mercy of PHP performance. The PHP SuEXEC Max CONN setting controls how many concurrent PHP processes are allowed. The default of 10 may not be enough for a dedicated server. How high you increase it depends on the size of the server, especially in terms of RAM and CPU. While you could technically set it as high as 1000, higher is not necessarily better. An 8-core physical server, for example, will likely not benefit from any more than fifty to eighty processes. Avoid Forking PHP Worker Processes¶ With shared hosting, LiteSpeed Web Server dynamically forks PHP worker processes in order to save server resources. The server spawns new worker groups or forks a child PHP worker process as necessary to serve incoming requests. While this preserves server resources, it penalizes performance by slowing down PHP’s initial start-up response time. This trade-off makes sense for shared hosting, but you may not wish it on a dedicated server. Set the LSAPI_AVOID_FORK environment variable to 1 in order to keep the PHP worker process alive for a longer period of time. Doing so ensures that there will always be a PHP worker available to serve incoming requests, and no delay in PHP response time."
78,"Tip You can set the environment variable within WebAdmin Console in the PHP tab, Environment Variables field."
78,"Increase PHP Running Time¶ The purpose of Max Idle Time is to specify the number of seconds before an external application is stopped by the server. To make PHP live longer, simply increase Max Idle Time to a large number, such as 86400, in the PHP tab, or at the External App level. Move PHP Session Data¶ If the server has enough free memory, you can move PHP sessions and opcode cache disk storage to /dev/shm. Check PHP Info¶ Take a look at your phpinfo.php page, and check these few modules: timezonedb¶ It’s recommended to have the timezonedb module enabled for PHP. You may not notice the absence of timezonedb on your dedicated server, but enabling it can keep PHP from needing to scan hundreds of directories. xdebug¶ Make sure the PHP xdebugmodule is disabled if you don't need it. xdebug is for developer IDE integration only and should not be installed by default. snmp¶ Same for snmp. The snmp module will scan and parse available MIB files, but not everyone needs snmp support."
78,"Last update: August 1, 2023"
78,Copyright © 2013-2023 LiteSpeed Technologies Inc.
78,Made with
78,Material for MkDocs
79,Database Tools and SQL | IntelliJ IDEA Documentation
79,"IntelliJ IDEA 2023.3 HelpDatabase Tools and SQLEnable the Database Tools and SQL plugin This functionality relies on the Database Tools and SQL plugin,"
79,"which is bundled and enabled in IntelliJ IDEA by default. If the relevant features aren't available, make sure that you didn't disable the plugin. The Database Tools and SQL plugin is available only in IntelliJ IDEA Ultimate.Press Ctrl+Alt+S to open the IDE settings and then select Plugins.Open the Installed tab, find the Database Tools and SQL plugin, and select the checkbox next to the plugin name.This section provides an overview of basic functionality that is available with the Database Tools and SQL plugin in IntelliJ IDEA. For more information about database features, refer to the official DataGrip documentation.The database management functionality in IntelliJ IDEA is supported by the Database Tools and SQL plugin. The plugin provides support for all the features that are available in DataGrip, the standalone database management environment for developers. With the plugin, you can query, create and manage databases. Databases can work locally, on a server, or in the cloud. The plugin supports MySQL, PostgreSQL, Microsoft SQL Server, SQLite, MariaDB, Oracle, Apache Cassandra, and others. See the full list of supported vendors in Create a data source.Get started with database toolsThese topics are focused on describing features, functionality, the application interface, and might help you to get started quickly.GlossaryLearn the main concepts.Create a data sourceStart working with your database by creating a connection configuration for it.Run a queryConnect an SQL file to a data source and run your code.View resultsConnect an SQL file to a data source and run your code.Work with dataView and edit data in data editor.Export/ImportImport and export your data to and from various formats with or without special tools like mysqldump, pg_dump, pg_restore, or psql.DiagramsGenerate diagrams for database objects, build query and execution plans, generate flame graphs for EXPLAIN statements.TroubleshootingConnectivity issue is a very common definition and might include many problems. This troubleshooting article will guide you through the most common steps to identify and fix the issue. If you could not find the answer, try to"
79,contact JetBrains and we can try to help you.Last modified: 22 February 2024Tailwind CSSGlossary for the database management functionality
81,"MySQL Compatibility | PingCAP DocsHomeTiDB CloudTiDBPlaygroundForumContact UsLanguage​​Sign InTry Freev7.5Docs HomeAbout TiDBTiDB IntroductionTiDB 7.5 Release NotesFeaturesMySQL CompatibilityTiDB LimitationsCreditsRoadmapQuick StartDevelopDeployMigrateIntegrateMaintainMonitor and AlertTroubleshootPerformance TuningTutorialsTiDB ToolsReferenceFAQsRelease NotesGlossaryMySQL CompatibilityTiDB is highly compatible with the MySQL protocol and the common features and syntax of MySQL 5.7 and MySQL 8.0. The ecosystem tools for MySQL (PHPMyAdmin, Navicat, MySQL Workbench, DBeaver and more) and the MySQL client can be used for TiDB.TiDB is highly compatible with the MySQL protocol and the common features and syntax of MySQL 5.7 and MySQL 8.0. The ecosystem tools for MySQL (PHPMyAdmin, Navicat, MySQL Workbench, DBeaver and more) and the MySQL client can be used for TiDB.However, some features of MySQL are not supported in TiDB. This could be because there is now a better way to solve the problem (such as the use of JSON instead of XML functions) or a lack of current demand versus effort required (such as stored procedures and functions). Additionally, some features might be difficult to implement in a distributed system.It's important to note that TiDB does not support the MySQL replication protocol. Instead, specific tools are provided to replicate data with MySQL:Replicate data from MySQL: TiDB Data Migration (DM) is a tool that supports full data migration and incremental data replication from MySQL or MariaDB into TiDB.Replicate data to MySQL: TiCDC is a tool for replicating the incremental data of TiDB by pulling TiKV change logs. TiCDC uses the MySQL sink to replicate the incremental data of TiDB to MySQL.NoteThis page describes general differences between MySQL and TiDB. For more information on compatibility with MySQL in the areas of security and pessimistic transaction mode, refer to the dedicated pages on Security and Pessimistic Transaction Mode.NoteFor information about transaction differences between MySQL and TiDB, see Pessimistic Transaction Mode.You can try out TiDB features on TiDB Playground.Unsupported featuresStored procedures and functionsTriggersEventsUser-defined functionsFULLTEXT syntax and indexes #1793SPATIAL (also known as GIS/GEOMETRY) functions, data types and indexes #6347Character sets other than ascii, latin1, binary, utf8, utf8mb4, and gbk.SYS schemaOptimizer traceXML FunctionsX-Protocol #1109Column-level privileges #9766XA syntax (TiDB uses a two-phase commit internally, but this is not exposed via an SQL interface)CREATE TABLE tblName AS SELECT stmt syntax #4754CHECK TABLE syntax #4673CHECKSUM TABLE syntax #1895REPAIR TABLE syntaxOPTIMIZE TABLE syntaxHANDLER statementCREATE TABLESPACE statement""Session Tracker: Add GTIDs context to the OK packet""Descending Index #2519SKIP LOCKED syntax #18207Lateral derived tables #40328Differences from MySQLAuto-increment IDIn TiDB, the auto-incremental column values (IDs) are globally unique and incremental within a single TiDB server. To make the IDs incremental among multiple TiDB servers, you can use the AUTO_INCREMENT MySQL compatibility mode. However, the IDs are not necessarily allocated sequentially, so it is recommended that you avoid mixing default and custom values to prevent encountering the Duplicated Error message.You can use the tidb_allow_remove_auto_inc system variable to allow or forbid removing the AUTO_INCREMENT column attribute. To remove the column attribute, use the ALTER TABLE MODIFY or ALTER TABLE CHANGE syntax.TiDB does not support adding the AUTO_INCREMENT column attribute, and once removed, it cannot be recovered.For TiDB v6.6.0 and earlier versions, auto-increment columns in TiDB behave the same as in MySQL InnoDB, requiring them to be primary keys or index prefixes. Starting from v7.0.0, TiDB removes this restriction, allowing for more flexible table primary key definitions. #40580For more details, see AUTO_INCREMENT.NoteIf you do not specify a primary key when creating a table, TiDB uses _tidb_rowid to identify the row. The allocation of this value shares an allocator with the auto-increment column (if such a column exists). If you specify an auto-increment column as the primary key, TiDB uses this column to identify the row. In this situation, the following situation might occur:mysql> CREATE TABLE t(id INT UNIQUE KEY AUTO_INCREMENT);"
81,"Query OK, 0 rows affected (0.05 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,"mysql> SELECT _tidb_rowid, id FROM t;"
81,+-------------+------+
81,| _tidb_rowid | id
81,+-------------+------+
81,2 |
81,1 |
81,4 |
81,3 |
81,6 |
81,5 |
81,+-------------+------+
81,3 rows in set (0.01 sec)
81,"As shown, because of the shared allocator, the id increments by 2 each time. This behavior changes in MySQL compatibility mode, where there is no shared allocator and therefore no skipping of numbers.NoteThe AUTO_INCREMENT attribute might cause hotspot in production environments. See Troubleshoot HotSpot Issues for details. It is recommended to use AUTO_RANDOM instead.NoteThe AUTO_INCREMENT attribute might cause hotspot in production environments. See Troubleshoot HotSpot Issues for details. It is recommended to use AUTO_RANDOM instead.Performance schemaTiDB utilizes a combination of Prometheus and Grafana for storing and querying performance monitoring metrics. In TiDB, performance schema tables do not return any results.To check performance metrics in TiDB Cloud, you can either check the cluster overview page on the TiDB Cloud console or use third-party monitoring integrations. Performance schema tables return empty results in TiDB.Query Execution PlanThe output format, content, and privilege settings of Query Execution Plan (EXPLAIN/EXPLAIN FOR) in TiDB differ significantly from those in MySQL.In TiDB, the MySQL system variable optimizer_switch is read-only and has no effect on query plans. Although optimizer hints can be used in similar syntax to MySQL, the available hints and their implementation might differ.For more information, refer to Understand the Query Execution Plan.Built-in functionsTiDB supports most of the built-in functions in MySQL, but not all. You can use the statement SHOW BUILTINS to get a list of the available functions.For more information, refer to the TiDB SQL Grammar.DDL operationsIn TiDB, all supported DDL changes can be performed online. However, there are some major restrictions on DDL operations in TiDB compared to MySQL:When using a single ALTER TABLE statement to alter multiple schema objects (such as columns or indexes) of a table, specifying the same object in multiple changes is not supported. For example, if you execute the ALTER TABLE t1 MODIFY COLUMN c1 INT, DROP COLUMN c1 command, the Unsupported operate same column/index error is output.It is not supported to modify multiple TiDB-specific schema objects using a single ALTER TABLE statement, such as TIFLASH REPLICA, SHARD_ROW_ID_BITS, and AUTO_ID_CACHE.TiDB does not support the changes of some data types using ALTER TABLE. For example, TiDB does not support the change from the DECIMAL type to the DATE type. If a data type change is unsupported, TiDB reports the Unsupported modify column: type %d not match origin %d error. Refer to ALTER TABLE for more details.The ALGORITHM={INSTANT,INPLACE,COPY} syntax functions only as an assertion in TiDB, and does not modify the ALTER algorithm. See ALTER TABLE for further details.Adding/Dropping the primary key of the CLUSTERED type is unsupported. For more details about the primary key of the CLUSTERED type, refer to clustered index.Different types of indexes (HASH|BTREE|RTREE|FULLTEXT) are not supported, and will be parsed and ignored when specified.TiDB supports HASH, RANGE, LIST, and KEY partitioning types. Currently, the KEY partition type does not support partition statements with an empty partition column list. For an unsupported partition type, TiDB returns Warning: Unsupported partition type %s, treat as normal table, where %s is the specific unsupported partition type.Range, Range COLUMNS, List, and List COLUMNS partitioned tables support ADD, DROP, TRUNCATE, and REORGANIZE operations. Other partition operations are ignored.Hash and Key partitioned tables support ADD, COALESCE, and TRUNCATE operations. Other partition operations are ignored.The following syntaxes are not supported for partitioned tables:SUBPARTITION{CHECK|OPTIMIZE|REPAIR|IMPORT|DISCARD|REBUILD} PARTITIONFor more details on partitioning, see Partitioning.Analyzing tablesIn TiDB, Statistics Collection differs from MySQL in that it completely rebuilds the statistics for a table, making it a more resource-intensive operation that takes longer to complete. In contrast, MySQL/InnoDB performs a relatively lightweight and short-lived operation.For more information, refer to ANALYZE TABLE.Limitations of SELECT syntaxTiDB does not support the following SELECT syntax:SELECT ... INTO @variableSELECT .. GROUP BY expr does not imply GROUP BY expr ORDER BY expr as it does in MySQL 5.7.For more details, see the SELECT statement reference.UPDATE statementSee the UPDATE statement reference.ViewsViews in TiDB are not updatable and do not support write operations such as UPDATE, INSERT, and DELETE.Temporary tablesFor more information, see Compatibility between TiDB local temporary tables and MySQL temporary tables.Character sets and collationsTo learn about the character sets and collations supported by TiDB, see Character Set and Collation Overview.For information on the MySQL compatibility of the GBK character set, refer to GBK compatibility .TiDB inherits the character set used in the table as the national character set.Storage enginesTiDB allows for tables to be created with alternative storage engines. Despite this, the metadata as described by TiDB is for the InnoDB storage engine as a way to ensure compatibility.To specify a storage engine using the --store option, it is necessary to start the TiDB server. This storage engine abstraction feature is similar to MySQL.SQL modesTiDB supports most SQL modes:The compatibility modes, such as Oracle and PostgreSQL are parsed but ignored. Compatibility modes are deprecated in MySQL 5.7 and removed in MySQL 8.0.The ONLY_FULL_GROUP_BY mode has minor semantic differences from MySQL 5.7.The NO_DIR_IN_CREATE and NO_ENGINE_SUBSTITUTION SQL modes in MySQL are accepted for compatibility, but are not applicable to TiDB.Default differencesTiDB has default differences when compared with MySQL 5.7 and MySQL 8.0:Default character set:TiDB’s default value is utf8mb4.MySQL 5.7’s default value is latin1.MySQL 8.0’s default value is utf8mb4.Default collation:TiDB’s default collation is utf8mb4_bin.MySQL 5.7’s default collation is utf8mb4_general_ci.MySQL 8.0’s default collation is utf8mb4_0900_ai_ci.Default SQL mode:TiDB’s default SQL mode includes these modes: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION.MySQL’s default SQL mode:The default SQL mode in MySQL 5.7 is the same as TiDB.The default SQL mode in MySQL 8.0 includes these modes: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION.Default value of lower_case_table_names:The default value in TiDB is 2, and only 2 is currently supported.MySQL defaults to the following values:On Linux: 0. It means that table and database names are stored on disk according to the letter case specified in the CREATE TABLE or CREATE DATABASE statement. Name comparisons are case-sensitive.On Windows: 1. It means table names are stored in lowercase on disk, and name comparisons are not case-sensitive. MySQL converts all table names to lowercase on storage and lookup. This behavior also applies to database names and table aliases.On macOS: 2. It means table and database names are stored on disk according to the letter case specified in the CREATE TABLE or CREATE DATABASE statement, but MySQL converts them to lowercase on lookup. Name comparisons are not case-sensitive.Default value of explicit_defaults_for_timestamp:The default value in TiDB is ON, and only ON is currently supported.MySQL defaults to the following values:For MySQL 5.7: OFF.For MySQL 8.0: ON.Date and TimeTiDB supports named timezones with the following considerations:TiDB uses all the timezone rules presently installed in the system for calculation, typically the tzdata package. This makes it possible to use all timezone names without needing to import timezone table data. Importing timezone table data will not change the calculation rules.Currently, MySQL uses the local timezone by default, then relies on the current timezone rules built into the system (for example, when daylight savings time begins) for calculation. Without importing timezone table data, MySQL cannot specify the timezone by name.Type system differencesThe following column types are supported by MySQL but not by TiDB:SQL_TSI_* (includes SQL_TSI_MONTH, SQL_TSI_WEEK, SQL_TSI_DAY, SQL_TSI_HOUR, SQL_TSI_MINUTE, and SQL_TSI_SECOND, but excludes SQL_TSI_YEAR)Incompatibility due to deprecated featuresTiDB does not implement specific features deprecated in MySQL, including:Specifying precision for floating-point types. MySQL 8.0 deprecates this feature, and it is recommended to use the DECIMAL type instead.The ZEROFILL attribute. MySQL 8.0 deprecates this feature, and it is recommended to pad numeric values in your application instead.CREATE RESOURCE GROUP, DROP RESOURCE GROUP, and ALTER RESOURCE GROUP statementsThe following statements for creating, modifying, and dropping resource groups have different supported parameters than MySQL. For details, see the following documents:CREATE RESOURCE GROUPDROP RESOURCE GROUPALTER RESOURCE GROUPWas this page helpful?YesNoDownload PDFRequest docs changesAsk questions on DiscordPlaygroundNewOne-stop & interactive experience of TiDB's capabilities WITHOUT registration.What's on this pageUnsupported featuresDifferences from MySQLAuto-increment IDPerformance schemaQuery Execution PlanBuilt-in functionsDDL operationsAnalyzing tablesLimitations of SELECT syntaxUPDATE statementViewsTemporary tablesCharacter sets and collationsStorage enginesSQL modesDefault differencesDate and TimeType system differencesIncompatibility due to deprecated featuresCREATE RESOURCE GROUP, DROP RESOURCE GROUP, and ALTER RESOURCE GROUP statementsWhat's on this pageProductsTiDBTiDB DedicatedTiDB ServerlessPricingGet DemoGet StartedEcosystemIntegrationsTiKVTiFlashOSS InsightResourcesTiDB Cloud RoadmapTiDB RoadmapFAQsBlogEducationSupportDiscordForumSlackSupport PortalCompanyAbout UsCareersLegalContact Us© 2024 PingCAP. All Rights Reserved.Privacy Policy."
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning | Journal of Cheminformatics | Full Text
83,Skip to main content
83,Advertisement
83,Search
83,Explore journals
83,Get published
83,About BMC
83,My account
83,Search all BMC articles
83,Search
83,Journal of Cheminformatics
83,Home
83,About
83,Articles
83,Submission Guidelines
83,About the Editors
83,Calls for Papers
83,Submit manuscript
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
83,Download PDF
83,Download ePub
83,Download PDF
83,Download ePub
83,Research
83,Open access
83,Published: 01 February 2024
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
83,"Jonghyun Lee1, Dae Won Jun1,2, Ildae Song3 & …Yun Kim4 Show authors"
83,Journal of Cheminformatics
83,"volume 16, Article number: 14 (2024)"
83,Cite this article
83,819 Accesses
83,2 Altmetric
83,Metrics details
83,"AbstractThe drug discovery process is demanding and time-consuming, and machine learning-based research is increasingly proposed to enhance efficiency. A significant challenge in this field is predicting whether a drug molecule’s structure will interact with a target protein. A recent study attempted to address this challenge by utilizing an encoder that leverages prior knowledge of molecular and protein structures, resulting in notable improvements in the prediction performance of the drug-target interactions task. Nonetheless, the target encoders employed in previous studies exhibit computational complexity that increases quadratically with the input length, thereby limiting their practical utility. To overcome this challenge, we adopt a hint-based learning strategy to develop a compact and efficient target encoder. With the adaptation parameter, our model can blend general knowledge and target-oriented knowledge to build features of the protein sequences. This approach yielded considerable performance enhancements and improved learning efficiency on three benchmark datasets: BIOSNAP, DAVIS, and Binding DB. Furthermore, our methodology boasts the merit of necessitating only a minimal Video RAM (VRAM) allocation, specifically 7.7GB, during the training phase (16.24% of the previous state-of-the-art model). This ensures the feasibility of training and inference even with constrained computational resources."
83,"IntroductionThe process of drug discovery is often compared to finding a needle in a haystack, requiring substantial funds and labor forces. Unfortunately, most newly discovered drugs fail to obtain approval for clinical use due to unexpected adverse drug reactions, insufficient drug effects, and low binding affinity [1,2,3,4,5]. Artificial intelligence has emerged as a promising tool for reducing expenses in various fields of drug discovery, including the predictions of drug toxicity, drug-drug interaction, and molecule properties, among others. In the first step of drug discovery, which involves drug repurposing and/or repositioning, it is critical to identify candidates of druggable molecules that target a specific protein. In this context, drug-target interaction (DTI) prediction tasks have emerged as a crucial area of research.Previous studies on DTI prediction can be broadly categorized into three categories: simulation-based molecular docking, structural similarity, and deep neural network (DNN) approach. Molecular docking simulation utilized 3D structures of proteins and molecules and simulated the binding sites [6,7,8]. While it offers a clear visual understanding, obtaining a 3D structure of a feature is challenging and it was hard to collect large datasets effectively. Conversely, the similarity-based technique proposed binding candidates using priorly established drug-target pairs. While this approach showed considerable predictions for recognized pairs based on similarity, it confronts difficulties in determining similarity for previously unobserved pairs [9, 10]. DNNs have exhibited proficient results in DTI prediction, similar to their successful implementations in various other domains. A pioneering study, DeepDTA [11], employed a drug and target encoder built on Convolutional Neural Networks (CNN) for the prediction of binding affinities. Instead of relying on highly complex datasets, the DeepDTA leveraged 1D expressions of the molecular structure system, Simplified Molecular Input Line Entry System (SMILES), and amino acid sequences, for drug and target, respectively. With hierarchical CNN layers, similar to conventional CNNs used for image recognition, DeepDTA can interpret the interactions of a given drug-target pair. After the DeepDTA, a multitude of research initiatives have been undertaken to either enhance the encoder’s capability or predict interactions more effectively. Such advancements encompass the deployment of CNNs [12,13,14], the development of interactions within gated cross attentions [15], the adoption of encoders that perceive molecular structures in graph format [16,17,18], computing similarity using enhanced DNN-based kernels [19,20,21], encode sequence using generative models [22, 23], and the integration of multi-modal techniques [24,25,26,27].The Transformer architecture [28], renowned for its proficiency in sequence processing, has been extensively employed as an encoder [29,30,31,32,33,34,35,36,37]. Nonetheless, it possesses a fundamental limitation: the computational expense escalates quadratically with the increase in the input length (see more details in Appendix C). Consequently, a majority of research initiatives have leaned towards its application as a drug encoder rather than for proteins [30,31,32,33, 37]. Recent advancements have brought forth efficient transformer methodologies, suggesting the potential for significantly reducing the computational demands in protein-encoding [38,39,40,41]. Concurrently, the ProtTrans project [35], leveraging the established Bidirectional Encoder Representations from Transformers (BERT) [42] model and its training methodology has undertaken pre-training of a protein encoder using an expansive set of amino acids and subsequently made it publicly available. As of now, the academic community lacks a publicly accessible, pre-trained model based on the efficient transformer, thereby preserving the relevance and utility of ProtTrans. A recent study, that utilized both transformer-based encoders for representing drugs and targets was proposed [43]. The prediction performances were considerably improved, however, due to the large size of the protein encoder, they truncated the protein language model into half its size.To reach an efficient computing model, knowledge distillation techniques were proposed [44, 45]. The key concept of knowledge distillation is distilling the knowledge from the large and complex model to the small and simple model with minimum loss of knowledge (See more details on Appendix A). However, DistillProtBERT (260 million parameters) [46], a model employing knowledge distillation from ProtBERT (420 million parameters) [35], is less efficient due to the inherent complexity of the amino acid sequence.To address this, we proposed a more efficient learning method than knowledge distillation, namely hint-based knowledge adaptation. This method involves using the intermediate features of the teacher model as hints, representing an expansion of knowledge distillation inspired by FitNet [47]. We term this approach “general knowledge” as it provides a general understanding of the target sequence, though lacking direct knowledge of the DTI task. It is assumed that this general knowledge, serving as a hint to the sequence, will facilitate successful learning despite the small size and simplicity of the student model. Conversely, the student model, designed to directly learn DTI performance, was structured in a simplified form compared to the original ProtBERT. In essence, knowledge adaptation presents an efficient means of leveraging both general knowledge of the target sequence and task-specific knowledge related to DTI simultaneously. This underscores the concept of adapting the teacher’s knowledge to the student’s knowledge, in contrast to knowledge distillation, which directly conveys task-specific knowledge.In this study, we proposed a Dual Language Model-based DTI model named DLM-DTI. The DLM-DTI was a lightweight and efficient, but accurate DTI prediction model. With the knowledge adaptation, the rich information from ProtBERT successfully adapted to predict DTI tasks. This study has several key contributions:"
83,"The hint-based knowledge adaptation technique, despite its compact parameterization, demonstrates considerably improved performance compared to baseline methods."
83,"By utilizing cached outputs from the teacher network, we achieved a notable reduction in computational costs."
83,"The knowledge adaptation approach is model-agnostic, offering flexibility in the selection of pre-trained models and architectures."
83,"Materials and methodsProblem definitionIn binary DTI classification, the goal is to predict the target value, \(Y_i\), for a given pair of \(X_i\), where \(\text {X}_i = \{ \text {x}_{\textrm{drug}}^i, \text {x}_{\textrm{target}}^i \}\), and \(\text {Y}_i \in \{ 0, 1 \}\) for \(i=1,\cdots , N\). The prediction of DTI can be viewed as a mapping function \(f(X_i) \rightarrow [0,1]\), which maps the drug-target pairs to a probability score of the interaction.Sequence representationSequence representations and embeddings involve converting a sequence, like a sentence, into a format that a computer model can understand. The first step is turning each part of the sequence into tokens, which are basically integer numbers that the model can work with. In this study, each part of the sequence is treated as a separate token. Special tokens, like a class token, are also added to grasp the overall meaning of the entire sequence. The concept of tokenization and special tokens is illustrated in Fig. 1.Fig. 1The concept of sequence representation and pre-training is illustrated. In A, the tokenization of a drug sequence (SMILES string) is depicted. In B, the tokenized elements are converted into integer values according to the predefined dictionary, and the encoder model (in this example, ChemBERTa) restores masked tokens into the original tokens (tokens colored in gray). After pre-training, the class token (CLS) is used to represent a given sequenceFull size imageDataset configurationsWe employ three datasets, namely DAVIS, Binding DB, and BIOSNAP, to train and evaluate the DLM-DTI. The DAVIS dataset consists of 68 drugs and 379 proteins, with 11,103 interactions measured in dissociation constant (\({K}_{d}\)) [48]. The interactions are categorized as positive or negative, with 1506 and 9597 instances, respectively. Similarly, the Binding DB dataset includes 10,665 drugs and 1413 proteins, with 32,601 interactions measured in \({K}_{d}\) [49]. The interactions are categorized as positive or negative, with 9166 and 23,435 instances, respectively. In this study, the threshold value for \({K}_{d}\) is set to 30 units, and interactions with \({K}_{d}\) values less than 30 units are considered positive binding interactions between the given drug and protein pair [29, 43]. The BIOSNAP dataset is initially composed of positive interactions only; however, negative pairs are added in the MolTrans study. The BIOSNAP dataset comprises 4510 drugs and 2181 proteins, with 27,482 interactions, including 13,741 positive and 13,741 negative instances [29].The integrated data training was first proposed by Kang et al., and they demonstrated improvements [43]. In this setting, training and validation datasets were merged, and a model was trained using integrated datasets. After the training steps, the trained model with integrated training datasets was evaluated on individual test datasets. For example, to test the BIOSNAP test dataset, the model was first trained using DAVIS, Binding DB, and BIOSNAP’s training datasets, and then tested on BIOSNAP’s test dataset. Generally, the diversity and quantity of datasets are linked to the improvement of prediction performance. Therefore, we also assessed the impact of dataset integrations using DLM-DTI. A summary of the dataset description is presented in Table 1.Table 1 The description of datasetsFull size tableTo ensure a fair comparison of model performance, we employ the same training, validation, and testing datasets used in previous studies [29, 43]. The datasets are split into training, validation, and testing datasets in the ratio of 7:1:2, respectively. The number of interactions for each data splitting is summarized in Table 2.Table 2 The number of interactions for each splitFull size tableModel configurationsThe process flow of DLM-DTI is depicted in Fig. 2. DLM-DTI was comprised of three primary components: the drug encoder, target encoder, and interaction prediction head. Notably, the target encoder encompasses both the teacher and student models of language models for protein sequences.Fig. 2The process flow of DLM-DTI. The drug and target sequences feed into their respective encoders. The encoded sequences are then merged, and the probability of bindings is computed using the interaction prediction head. DLM-DTI only utilizes the class token (CLS) of each encoded sequence because the class token preserves the abstract meaning of the entire sequence. The features of target sequences are computed using a teacher-student-based architecture, specifically employing a hint-based learning strategyFull size imageDrug encoderThe drug encoder converts SMILES sequences into meaningful features, serving as a mapping function from molecule sequences to a meaningful chemical space. We employed the ChemBERTa encoder, which was trained on various canonical SMILES and learned chemical space. Further details are described in Appendix B.The class token of the last hidden layer was extracted as input for the interaction prediction head. The encoding process of the drug sequence can be represented as follows:$$\begin{aligned} z_{\textrm{drug}} = f\left( \text {LN}(x_{\textrm{class}})\right) , \end{aligned}$$"
83,(1)
83,"where \(\text {LN}(\cdot )\) denotes the layer normalization layer, \(f(\cdot )\) denotes the projection function used to align the dimensions, and the hidden dimensions were set to 512 in this study. The upper limit of the drug sequence length was 512 tokens, corresponding to the maximum sequence length of the original ChemBERTa encoder [31].Target encoderSimilar to the drug encoder, the target encoder also extracts meaningful features from raw target sequences (amino acid sequences). The target encoder in this study was composed of both a teacher and a student model. The teacher model used for target sequence encoding was the ProtBERT model, pre-trained on UniRef and big fantastic database databases [35]. Details of ProtBERT are described in Appendix C. The original ProtBERT model was trained on sequences up to 40 K characters, with 420 million parameters. The student model was designed to match the original teacher model, ProtBERT, however, the number of layers was reduced. Except for the number of layers, the student model followed the hyperparameters of the teacher model. The number of parameters of the student model was 6.2% of the teacher model; teacher model: 420.0 million, student model: 26 million. The detailed parameters of the target encoder are presented in Table 3.Table 3 The specific parameters of target encoderFull size tableIn most cases, fully fine-tuning the large model was impractical due to restrictions on datasets and the associated computational expenses. To address this challenge, we adopted a hint-based training scheme that kind of knowledge distillation comprises both a teacher model and a student model. The teacher model was prevented from parameter updates, enabling solely the parameters of the student model to be updated. Given that the teacher model’s output was not subject to training, it retained a fixed form, thus enabling us to cache outputs of the teacher model prior to the training and inference step. This strategy markedly minimizes computational redundancy, thereby optimizing computational efficiency. Considering the teacher model’s output was not trained, it served as a form of hint to which the task-specific model (student model) could refer. The teacher and student models were combined using class token mixing to encode the target sequence. The output class token was treated as a “hint” that contained general knowledge of the given protein sequence. On the other hand, the output class token of the student model was considered as task-oriented specific knowledge. To mix the general knowledge and task-specific knowledge, we added two class tokens with learnable gating parameters (\(\lambda\)). The encoding process of the target sequence can be represented as follows:$$\begin{aligned} z_{\textrm{target}} = \lambda g\left( \text {LN} (x^{\textrm{student}}_{\textrm{class}})) + (1 - \lambda ) h (\text {LN}(x^{\textrm{teacher}}_{\textrm{class}})\right) , \end{aligned}$$"
83,(2)
83,"where \(g(\cdot )\) and \(h(\cdot )\) are the projection functions used to align the dimensions, and the adaptation parameter \(\lambda\) is a learnable parameter initialized randomly from a uniform distribution, \(\lambda \sim Uniform(0, 1)\). The term “adaptation” was employed to describe the process of adjusting general knowledge to suit the specific requirements of a particular task. An elevated value of the adaptation parameter indicated an increased emphasis of the model on the class token derived from the teacher model. In contrast, a decreased value of the adaptation parameter signified a predominant utilization of task-specific information obtained from the student model. The hidden dimensions of the class token mixing were set to 1024 in this study. The maximum length of the target sequence was set to 545 tokens, which covered 95% of proteins in the datasets, and the same max protein sequence lengths of previous studies [29, 43].Interaction prediction headThe class tokens of drug and target sequences have abstract meanings for each sequence. The interaction prediction head aggregated the features of drug-target pairs and predicted binding probability. In this step, there were multiple choices for mixing the features; for example, cross attention, capsule network, etc. However, we simply employed concatenation that showed stable performances in the previous work [43].The interaction module consists of three sequential blocks. Each block is structured with a Fully Connected (FC) layer, followed by an activation function and subsequently a dropout layer. The respective dimensions of the FC layers are 2048, 1024, and 512. The chosen activation function for these blocks is the Gaussian Error Linear Unit (GeLU). Additionally, a dropout rate of 0.1 has been employed for regularization. A detailed schematic of this configuration can be found in Fig. 3, and the specific parameter values are summarized in Table 4.Fig. 3Structure of the interaction prediction head. The interaction prediction head mixes the features of the drug-target pair to predict the binding probability of a given pair. The number under the block indicates the feature dimensionFull size imageTable 4 The detailed parameters of interaction prediction headFull size tableExperimental setupEvaluation metricsWe used the Area Under the Receiver Operation Characteristics curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) as primary evaluation metrics. AUROC is one of the most favorable metrics to measure classification performance, particularly in the medical field; however, it could be easily overestimated when the data has class imbalance [50]. Therefore, AUPRC is a relatively robust metric for measuring classification performance in imbalanced settings [50]. Sensitivity and specificity scores were utilized as sub-metrics, and the threshold for these sub-metrics was simply set to 0.5.Model training hyperparametersThe DLM-DTI was optimized using the AdamW optimizer with a learning rate of 0.0001. A cosine annealing learning rate scheduler was employed to adjust the learning rate. The binary cross-entropy loss was used to calculate the difference between predictions and ground truth. The model was trained for 50 epochs, and the best-performing parameters were selected based on the AUPRC score during validation. Due to severe class imbalance, the model could easily be overfitted to the dominant class. To prevent the selection of an overfitted model, we set the selection criteria as AUPRC rather than AUROC or the minimum loss coefficient. Automated mixed precision was utilized, and the batch size was set to 32. The best combination of hyperparameters was determined through iterative experiments.The use of a class imbalance sampler did not show any benefit for model training; therefore, we did not apply an imbalance sampler. Instead, AUPRC-based optimization demonstrated better performance in predicting binding probability.Hardware and softwardWe used a single NVIDIA A100 GPU to train DLM-DTI. The Python (v3.8) and PyTorch deep learning framework (v1.13) for trained DLM-DTI.ResultsBinding probability predictionThe baseline models, namely MolTrans [29] and the approach by Kang et al. [43], along with our proposed DLM-DTI, were trained on the same training datasets and evaluated using identical test datasets. Table 5 presents a summary of the evaluation results obtained from these experiments. MolTrans was exclusively trained on individual datasets and evaluated individually. In contrast, both Kang et al. and our DLM-DTI were trained using both individual and combined dataset settings. This approach was claimed in Kang et al., and therefore the previous study, MolTrans, did not experiment with an integrated dataset.Within the BIOSNAP dataset, DLM-DTI showed an improved AUPRC score (absolute value; percentage) than MolTrans (0.013; 1.44%), and Kang et al. (0.014 \(\sim\) 0.017; 1.56 \(\sim\) 1.90%). The AUROC score was improved compared to MolTrans (0.019; 2.12%), however, the AUROC showed similarity to Kang et al.’s model. Similarly, in the Binding DB, DLM-DTI exhibited a considerably improved AUPRC score than other methods, MolTrans (0.021; 3.38%), and Kang et al.’s model (0.004 \(\sim\) 0.02; 0.63 \(\sim\) 3.21%), respectively.In the DAVIS dataset, the performance of the DLM-DTI was degraded, and its performance was similar to that of MolTrans. The training with an integrated dataset showed benefits for the DLM-DTI only in the DAVIS dataset.Table 5 The prediction performance of binding affinityFull size tableAdaptation parameter, \(\lambda\)"
83,"During the training, the randomly initialized adaptation parameter \(\lambda\) gradually decreased and converged, as illustrated in Fig. 4. The adaptation parameter controlled the feature weights from the teacher and student encoder. As mentioned earlier, the teacher encoder contained general knowledge of the target sequence, and the student encoder had narrow but specific task-related knowledge. With the adaptation parameter, the DLM-DTI modulated the importance of each feature to accurately predict binding probability.Fig. 4Variation of the adaptation parameter (\(\lambda\)) during model training processFull size imageTo evaluate the effect of teacher-student architecture-based target sequence encoding, two ablation studies were conducted."
83,\(\lambda\) set to 0: Only the teacher encoder (general knowledge) was utilized.
83,\(\lambda\) set to 1: Only the student encoder (task-specific knowledge) was utilized.
83,"The adaptation setting (which utilized both teacher-student encoders) showed the best performance (AUROC: 0.912; AUPRC: 0.643) compared to the teacher encoder-only setting (AUROC: 0.911; AUPRC: 0.635) or the student encoder-only setting (AUROC: 0.900; AUPRC: 0.635). The effect of the \(\lambda\) parameter is summarized in Table 6.Table 6 The prediction performance of binding affinityFull size tableThe student encoder-only setting exhibited the poorest prediction performance (Rank: \(\text {3}^{\textrm{rd}}\)). This implies that two layers of simple and shallow networks were not sufficient to capture the complex patterns and features of target sequences to accurately predict DTIs. However, the teacher encoder-only setting demonstrated comparable performance (Rank: \(\text {2}^{\textrm{nd}}\)). This suggests that the general knowledge of the teacher model has the potential to predict binding probability. The teacher encoder-only setting corresponds to linear probing, where the training strategy only updates the prediction head without adjusting the weights of the encoder [51, 52]. The prediction performance of linear probing is considered as an encoder’s existing knowledge.Time and memory analysisTypically, a model’s performance exhibits a direct correlation with its parameter count, suggesting that larger models often yield superior outcomes. Nonetheless, this advantage comes with a caveat; substantial models necessitate considerable computational resources during both the training and inference stages. In light of this, we embarked on a systematic analysis comparing training time and parameter counts (Table 7). The metric for training time was derived by computing the mean learning time across three epochs, utilizing the Binding DB dataset.Table 7 Time and memory analysis of baseline models and DLM-DTIFull size tableDLM-DTI showed the best AUPRC score (0.643), only with 24.56% (86.7 million) of parameters compared to the Kang et al. (353.0 million) [43]. Additionally, DLM-DTI required 7.7 GB video random access memory (VRAM), and 63.00 s for a single training epoch. It was 16.24% (47.4 GB), and 9.98% (631.00 s) of the Kang et al. [43]. The MolTrans required the smallest VRAM (5.9 GB), however, the AUPRC score (0.622) was slightly lower than DLM-DTI (0.643). In our experimental setting, DLM-DTI required 7.7 GB of VRAM, therefore, it could be trained on conventional graphic processing units (GPUs), not for high-performing research machines (See details on 2.5.2).Cold drug, target, and bindingsIn addressing DTI challenges, the cold splitting testing approach is widely adopted [36, 53], primarily due to the inherent difficulties in dataset procurement and the paramount importance of achieving generalization for novel pairs. The term “cold splitting” pertains to scenarios where previously unseen drug-target interactions are involved, ones that were excluded from both the training and validation datasets. To simulate this condition, we conducted experiments where we isolated cold drugs, cold targets, and cold binding interactions from the test set of models trained to utilize the Binding DB dataset. We identified a total of 2,127 cold drugs and 136 cold targets. Specifically, a cold drug configuration encompasses all interactions associated with a cold drug, while a cold target configuration comprises all interactions associated with a cold target. The cold bindings were the interactions between cold drugs and cold targets, and only 114 pairs were identified. The performances of cold-splitting datasets are summarized in Table 8. DLM-DTI’s performance was comparable to the baseline models in the context of the cold drug, yet exhibited a minor deterioration to the cold target and was found to be most deficient in addressing cold binding. Conversely, Kang et al. [43] manifested commendable prediction capabilities across all testing scenarios. MolTrans [29] exhibited a performance metric closely mirroring Kang et al. in terms of AUROC, but fell short when evaluated using AUPRC.Table 8 The classification performances within the cold splitting settingsFull size tableDiscussionIn this study, we suggested a lightweight but accurate DTI prediction model, namely DLM-DTI. The main hurdle for utilizing protein sequence-based language models, such as ProtBERT [35], was heavy computing resource requirements. To comprehend the complex and long sequence of a protein, it needed heavy and large architectures and an intensive pre-training process. The DLM-DTI mitigated the computational burden caused by the protein encoder, by using a knowledge adaptation. DLM-DTI achieved improved AUPRC performance, especially in Binding DB (0.63 \(\sim\) 3.38%), and BIOSNAP (1.44 \(\sim\) 1.9%) datasets. The most interesting point was that DLM-DTI utilized only 25% of parameters (86.7 million) compared to the previous state-of-the-art model, Kang et al. (353 million) [43]. Additionally, DLM-DTI required only 7.7 GB of VRAM, and 63 s for each training epoch, that of 16.24%, and 9.98% of Kang et al. [43].The Transformer-based language model has exhibited impressive capabilities across various applications, including molecular and protein sequences. However, pre-training has emerged as a key approach to further optimize the model’s functional and semantic relationship learning from large sequence datasets [35,36,37, 42, 43]. Despite the promising results, the computational cost of the language model increases significantly with the input length. To address this challenge, Kang et al. proposed a Kang et al. approach, which employed only half of the pre-trained target encoder [43]. The methodology employed by the ELECTRA-DTA model aligns closely with our approach [36]. In the ELECTRA-DTA framework, the features originating from the pre-trained drug encoder and protein encoder are individually averaged. Subsequently, these averaged features are compactly represented as a compressed feature vector. This vector is subsequently incorporated into a squeeze-and-excitation network, aiming to enhance the predictive capabilities of the model. Their approach can also be perceived as a tactical maneuver to circumvent the necessity of fine-tuning the complete encoder. However, it is important to note that we could not directly compare the prediction performance of our DLM-DTI approach to that of ELECTRA-DTA due to differences in the target tasks, with DLM-DTI using binary classification and ELECTRA-DTA using \(pK_{d}\) regression.In our study, we introduced an adaptation parameter to efficiently generate meaningful protein features. The adaptation parameter, denoted as \(\lambda\), was randomly initialized and tuned. This parameter controlled the weights of knowledge from both the teacher model (providing general knowledge) and the student model (capturing task-specific knowledge). In the ablation studies (Table 6), the absence of knowledge adaptation resulted in significant degradation of performance for both the teacher-only and student-only settings. However, the DLM-DTI with knowledge adaptation exhibited weaknesses in generalization performance. Kang et al.’s [43] work also demonstrated strong performance under cold-splitting conditions (Table 8). In contrast, our DLM-DTI, which either matched or outperformed Kang et al. on the complete dataset, showed reduced effectiveness in cold-splitting evaluations, particularly concerning cold-binding interactions. This may be attributed to the over-reduction of the student model, limiting generalization performance. Inspired by recent examples that incorporate natural language-based prior knowledge to enhance prediction performance, we aim to improve our approach by adding natural language information related to the function of proteins in future work [54]. Interestingly, integrated dataset training did not prove beneficial for DLM-DTI. In Kang et al. [43], training with integrated datasets demonstrated outstanding performances. Large-scale Transformer-based architectures typically require a substantial amount of data to realize their full potential. However, DLM-DTI introduces a small-scale student model, and it is speculated that the small size was sufficient for effective learning.Recently, foundation models based on large language models have been widely studied [55, 56]. A shared challenge between these models and protein sequence encoders pertains to the intricacies involved in fine-tuning. Due to the scarcity of annotated data and the extensive parameters within these models, innovative strategies for effective fine-tuning have been proposed. For instance, a method called low-rank adaptation (LoRA) [57], similar to our own approach, adopt a technique where only the adaptation layer is adjusted. This is achieved by integrating a low-rank adaptation layer, which eliminates the need for comprehensive fine-tuning across all layers. This approach proves to be more cost-effective and quicker to converge compared to the resource-intensive process of complete fine-tuning. Therefore, in our future study, we plan to compare the performances of a fine-tuning model using LoRA’s adaptation approaches. Furthermore, there is a need for enhancement in the design of the interaction head. Currently, this component is composed of a sequence of straightforward FC layers, which exhibits reduced effectiveness in cold bindings. To address this, potential strategies include the integration of a squeeze-and-excitation network [58], capsule network [59], cross-attention [60], and other alternatives.ConclusionIn this study, we employed knowledge adaptation to efficiently and accurately predict binding probability. The knowledge adaptation was efficiently tuned with both general knowledge and task-specific knowledge through the teacher-student architectures. With only 25% of the model parameters, DLM-DTI exhibited considerable performance compared to the previous state-of-the-art model. Notably, DLM-DTI required 7.7 GB of VRAM, allowing training on conventional GPUs without the need for high-performing GPUs."
83,Availability of data and materials
83,The datasets are available at: https://github.com/kexinhuang12345/MolTrans/tree/master/dataset.
83,Code availability
83,The source codes are available at: https://github.com/jonghyunlee1993/DLM-DTI_hint-based-learning/tree/master.
83,"ReferencesAnusuya S, Kesherwani M, Priya KV, Vimala A, Shanmugam G, Velmurugan D, Gromiha MM (2018) Drug-target interactions: prediction methods and applications. Curr Protein Pept Sci 19(6):537–561Article"
83,CAS
83,PubMed
83,Google Scholar
83,Ledford H (2011) 4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. Nature looks at ways they might be saved. Nature 477(7366):526–529Article
83,CAS
83,PubMed
83,Google Scholar
83,"Zheng Y, Wu Z (2021) A machine learning-based biological drug-target interaction prediction method for a tripartite heterogeneous network. ACS Omega 6(4):3037–3045Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Ashburn TT, Thor KB (2004) Drug repositioning: identifying and developing new uses for existing drugs. Nat Rev Drug Discovery 3(8):673–683Article"
83,CAS
83,PubMed
83,Google Scholar
83,Strittmatter SM (2014) Overcoming drug development bottlenecks with repurposing: old drugs learn new tricks. Nat Med 20(6):590–591Article
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Li H, Leung K-S, Wong M-H, Ballester PJ (2015) Low-quality structural and interaction data improves binding affinity prediction via random forest. Molecules 20(6):10947–10962Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Trott O, Olson AJ (2010) Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J Comput Chem 31(2):455–461Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Luo H, Mattes W, Mendrick DL, Hong H (2016) Molecular docking for identification of potential targets for drug repurposing. Curr Top Med Chem 16(30):3636–3645Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Pahikkala T, Airola A, Pietilä S, Shakyawar S, Szwajda A, Tang J, Aittokallio T (2015) Toward more realistic drug-target interaction predictions. Brief Bioinform 16(2):325–337Article"
83,CAS
83,PubMed
83,Google Scholar
83,"He T, Heidemeyer M, Ban F, Cherkasov A, Ester M (2017) Simboost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines. J Cheminformatics 9(1):1–14Article"
83,Google Scholar
83,"Öztürk H, Özgür A, Ozkirimli E (2018) Deepdta: deep drug-target binding affinity prediction. Bioinformatics 34(17):821–829Article"
83,Google Scholar
83,"Lee I, Keum J, Nam H (2019) Deepconv-dti: prediction of drug-target interactions via deep learning with convolution on protein sequences. PLoS Comput Biol 15(6):1007129Article"
83,Google Scholar
83,"Lee I, Nam H (2022) Sequence-based prediction of protein binding regions and drug-target interactions. J Cheminformatics 14(1):1–15Article"
83,Google Scholar
83,"Zeng Y, Chen X, Luo Y, Li X, Peng D (2021) Deep drug-target binding affinity prediction with multiple attention blocks. Brief Bioinform 22(5):117Article"
83,Google Scholar
83,"Kim Y, Shin B (2021) An interpretable framework for drug-target interaction with gated cross attention. In: Machine Learning for Healthcare Conference, pp. 337–353. PMLRNguyen T, Le H, Quinn TP, Nguyen T, Le TD, Venkatesh S (2021) Graphdta: predicting drug-target binding affinity with graph neural networks. Bioinformatics 37(8):1140–1147Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Thafar MA, Alshahrani M, Albaradei S, Gojobori T, Essack M, Gao X (2022) Affinity2vec: drug-target binding affinity prediction through representation learning, graph mining, and machine learning. Sci Rep 12(1):1–18Article"
83,Google Scholar
83,"Liao J, Chen H, Wei L, Wei L (2022) Gsaml-dta: an interpretable drug-target binding affinity prediction model based on graph neural networks with self-attention mechanism and mutual information. Comput Biol Med 150:106145Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Su X, Hu L, You Z, Hu P, Wang L, Zhao B (2022) A deep learning method for repurposing antiviral drugs against new viruses via multi-view nonnegative matrix factorization and its application to sars-cov-2. Brief Bioinform 23(1):526Article"
83,Google Scholar
83,"Li Y-C, You Z-H, Yu C-Q, Wang L, Wong L, Hu L, Hu P-W, Huang Y-A (2022) Ppaedti: personalized propagation auto-encoder model for predicting drug-target interactions. IEEE J Biomed Health Inform 27(1):573–582Article"
83,Google Scholar
83,"Thafar MA, Olayan RS, Albaradei S, Bajic VB, Gojobori T, Essack M, Gao X (2021) Dti2vec: drug-target interaction prediction using network embedding and ensemble learning. J Cheminformatics 13(1):1–18Article"
83,Google Scholar
83,"Zhao L, Wang J, Pang L, Liu Y, Zhang J (2020) Gansdta: predicting drug-target binding affinity using gans. Front Genetics 1243Chen Y, Wang Z, Wang L, Wang J, Li P, Cao D, Zeng X, Ye X, Sakurai T (2023) Deep generative model for drug design from protein target sequence. J Cheminformatics 15(1):38Article"
83,CAS
83,Google Scholar
83,"Liu G, Singha M, Pu L, Neupane P, Feinstein J, Wu H-C, Ramanujam J, Brylinski M (2021) Graphdti: a robust deep learning predictor of drug-target interactions from multiple heterogeneous data. J Cheminformatics 13(1):1–17Article"
83,Google Scholar
83,"Yan X, Liu Y (2022) Graph-sequence attention and transformer for predicting drug-target affinity. RSC Adv 12(45):29525–29534Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Hua Y, Song X, Feng Z, Wu X (2023) Mfr-dta: a multi-functional and robust model for predicting drug-target binding affinity and region. Bioinformatics 39(2):056Article"
83,Google Scholar
83,"Bian J, Zhang X, Zhang X, Xu D, Wang G (2023) Mcanet: shared-weight-based multiheadcrossattention network for drug-target interaction prediction. Brief Bioinform 24(2):082Article"
83,Google Scholar
83,"Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Proc Syst 30Huang K, Xiao C, Glass LM, Sun J (2021) Moltrans: molecular interaction transformer for drug-target interaction prediction. Bioinformatics 37(6):830–836Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Honda S, Shi S, Ueda HR (2019) Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738Chithrananda S, Grand G, Ramsundar B (2020) Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Molecule attention transformer. arXiv preprint arXiv:2002.08264Fabian B, Edlich T, Gaspar H, Segler M, Meyers J, Fiscato M, Ahmed M (2020) Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) Unified rational protein engineering with sequence-based deep representation learning. Nat Methods 16(12):1315–1322Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M, et al (2020) Prottrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225Wang J, Wen N, Wang C, Zhao L, Cheng L (2022) Electra-dta: a new compound-protein binding affinity prediction model based on the contextualized sequence encoding. J Cheminformatics 14(1):1–14Article"
83,Google Scholar
83,"Shin B, Park S, Kang K, Ho JC (2019) Self-attention based molecule representation for predicting drug-target interaction. In: Machine Learning for Healthcare Conference, pp. 230–248. PMLRXiong Y, Zeng Z, Chakraborty R, Tan M, Fung G, Li Y, Singh V (2021) Nyströmformer: A nyström-based algorithm for approximating self-attention. Proc AAAI Conf Artif Intell 35:14138–14148PubMed"
83,PubMed Central
83,Google Scholar
83,"Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509Press O, Smith NA, Lewis M (2021) Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409Dao T, Fu D, Ermon S, Rudra A, Ré C (2022) Flashattention: fast and memory-efficient exact attention with io-awareness. Adv Neural Inf Process Syst 35:16344–16359"
83,Google Scholar
83,"Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805Kang H, Goo S, Lee H, Chae J-W, Yun H-Y, Jung S (2022) Fine-tuning of bert model to accurately predict drug-target interactions. Pharmaceutics 14(8):1710Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531Gou J, Yu B, Maybank SJ, Tao D (2021) Knowledge distillation: a survey. Int J Comput Vision 129:1789–1819Article"
83,Google Scholar
83,"Geffen Y, Ofran Y, Unger R (2022) Distilprotbert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts. Bioinformatics 38(Supplement–2):95–98Article"
83,Google Scholar
83,"Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550Davis MI, Hunt JP, Herrgard S, Ciceri P, Wodicka LM, Pallares G, Hocker M, Treiber DK, Zarrinkar PP (2011) Comprehensive analysis of kinase inhibitor selectivity. Nat Biotechnol 29(11):1046–1051Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Liu T, Lin Y, Wen X, Jorissen RN, Gilson MK (2007) Bindingdb: a web-accessible database of experimentally determined protein-ligand binding affinities. Nucleic Acids Res 35(suppl-1):198–201Article"
83,Google Scholar
83,"Saito T, Rehmsmeier M (2015) The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. PLoS ONE 10(3):0118432Article"
83,Google Scholar
83,"Kumar A, Raghunathan A, Jones RM, Ma T, Liang P (2022) Fine-tuning can distort pretrained features and underperform out-of-distribution. In: International Conference on Learning Representations. https://openreview.net/forum?id=UYneFzXSJWhAlain G, Bengio Y (2016) Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644Chatterjee A, Walters R, Shafi Z, Ahmed OS, Sebek M, Gysi D, Yu R, Eliassi-Rad T, Barabási A-L, Menichetti G (2021) Ai-bind: improving binding predictions for novel protein targets and ligands. arXiv preprint arXiv:2112.13168Chen YT, Zou J (2023) Genept: a simple but hard-to-beat foundation model for genes and cells built from chatgpt. bioRxiv, 2023–10Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al (2023) Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham P, Chung HW, Sutton C, Gehrmann S, et al (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W (2021) Lora: low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141Sabour S, Frosst N, Hinton GE (2017) Dynamic routing between capsules. Adv Neural Inf Proc Syst 30Gheini M, Ren X, May J (2021) Cross-attention is all you need: adapting pretrained transformers for machine translation. arXiv preprint arXiv:2104.08771Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692Shibata Y, Kida T, Fukamachi S, Takeda M, Shinohara A, Shinohara T, Arikawa S (1999) Byte pair encoding: a text compression scheme that accelerates pattern matchingRogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Inf Model 50(5):742–754Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Katharopoulos A, Vyas A, Pappas N, Fleuret F (2020) Transformers are rnns: Fast autoregressive transformers with linear attention. In: International Conference on Machine Learning, pp. 5156–5165. PMLRDownload referencesFunding(1) This work was supported by research grants from Daegu Catholic University in 2022. (2) This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (RS-2022-00166945).Author informationAuthors and AffiliationsDepartment of Medical and Digital Engineering, Hanyang University College of Engineering, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaJonghyun Lee & Dae Won JunDepartment of Internal Medicine, Hanyang University College of Medicine, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaDae Won JunDepartment of Pharmaceutical Science and Technology, Kyungsung University, 309, Suyeong-ro, Nam-gu, Busan, 48434, KoreaIldae SongCollege of Pharmacy, Deagu Catholic University, 13-13, Hayang-ro, Hayang-eup, Gyeongsan-si, 38430, Gyeongsangbuk-do, KoreaYun KimAuthorsJonghyun LeeView author publicationsYou can also search for this author in"
83,PubMed Google ScholarDae Won JunView author publicationsYou can also search for this author in
83,PubMed Google ScholarIldae SongView author publicationsYou can also search for this author in
83,PubMed Google ScholarYun KimView author publicationsYou can also search for this author in
83,"PubMed Google ScholarContributionsConceptualization, JL, DJ, IS, and YK; methodology, JL, and YK; writing—original draft preparation, JL; writing—review and editing, YK; supervision, DJ, and YK; formal analysis, JL; resources, YK; All authors have read and agreed to the published version of the manuscript.Corresponding authorCorrespondence to"
83,Yun Kim.Ethics declarations
83,Ethics approval and consent to participate
83,Not applicable.
83,Consent for publication
83,Not applicable.
83,Competing interests
83,The author(s) declare that they have no conflict of interest.
83,"Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.AppendicesAppendix A Knowledge DistillationRecent high-performing DNN models boast millions or billions of parameters, necessitating extensive and high-performance hardware resources, such as GPU clusters and TPU pods. Knowledge distillation was proposed to develop a lightweight model while retaining robust information processing capabilities [44, 45]. The knowledge distillation process involves two models, specifically the teacher model and the student model. Conventionally, knowledge distillation begins by training the teacher model, a complex and high-capacity model, on the target task. Subsequently, the acquired knowledge from the teacher model is transferred to the student model, a more lightweight counterpart. This transfer is typically achieved by encouraging the student model to mimic the outputs [44] or internal representations [47] of the teacher model. The overarching goal is to distill the comprehensive knowledge captured by the teacher model into a more compact and computationally efficient student model.FitNet [47] introduces the concept of “hints” to enhance the knowledge distillation approach. In addition to replicating the output of the current teacher model, hints guide the student to mimic intermediate features together. This inclusion of hints enhances the performance of knowledge distillation by enabling the learning of not only the final result but also the intermediate features. In this context, a hint can be interpreted as providing information about both the intermediate features and the final feature.Appendix B Drug Encoder: ChemBERTaChemBERTa is a Transformer-based model pre-trained using 10 million SMILES sequences [31]. Based on RoBERTa [61], a model known for its outstanding performance in natural language processing, ChemBERTa comprises 12 attention heads and 6 layers. Drug sequences, expressed in Canonical SMILES, are tokenized using a subword-level tokenizer, while a byte-pair encoder (BPE) tokenizer is employed to group frequently occurring elements together into larger chunks for more efficient processing. BPE stands as a blend of character and word-level representations, facilitating the management of extensive vocabularies in natural language corpora. Guided by the insight that less common or unfamiliar words can frequently be broken down into several recognized subwords, BPE identifies the optimal word segmentation through an iterative and greedy merging of frequently occurring character pairs [62]. ChemBERTa has a total of 767 tokens, including a class token to encapsulate the abstract meaning of the entire sequence, a start of sequence token (SOS), an end of sequence token (EOS), and a pad token to mark the start and end of the sequence.ChemBERTa was trained using masked language modeling (MLM), where the task involves masking a portion of the entire sequence and then restoring the corresponding tokens; 15% of the total sequence was masked. The maximum processable sequence length is 512 tokens. ChemBERTa, pre-trained using MLM tasks, can then be used as an encoder for drug sequences because it has been trained on restoration tasks and has an understanding of molecule sequences. ChemBERTa can perform comparably to the commonly used extended-connectivity fingerprint (ECFP) [63] in molecule properties prediction tasks using the ChemBERTa encoder, and it was employed in this study due to its availability through the HuggingFace API, facilitating easy utilization.Appendix C Target Encoder: ProtBERTProtBERT, a component of the ProtTrans project, is a BERT model trained on an extensive dataset of amino acid sequences [35]. It underwent training using the same MLM approach as ChemBERTa, with 15% masking (Appendix B). However, owing to the intricacy of amino acid sequences, ProtBERT consists of 30 layers and 16 attention heads, resulting in a total parameter count of 4.2 million. Each element is considered one token in ProtBERT, and it comprises 30 tokens, including special tokens. Notably, it was trained to handle sequences of up to 4000 tokens, accommodating the typically extended length of amino acid sequences.However, ProtBERT uses the Transformer’s core operation, self-attention, where the amount of computation increases as the square of the length of a given sequence. The self-attention operation is as follows:$$\begin{aligned} \text {Attention}(Q, K) = \text {softmax} \left( \frac{{QK^T}}{{\sqrt{d_k}}}\right) , \end{aligned}$$"
83,(C1)
83,"where the query (Q) is the product of input sequence x and learnable parameter \(W_{Q}\), and key (K) is the product of input sequence x and learnable parameter \(W_{K}\).Therefore, a substantial amount of memory and computational resources must be allocated to manage long sequences of amino acids. This constitutes a significant bottleneck in the practical utilization of ProtBERT. While recent proposals, such as efficient self-attention computations using linear transformers [64] and Nystrom approximation [38], aim to address this challenge, pre-training with such approaches remains expensive. As an illustration, ProtBERT underwent training utilizing 1,024 tensor processing units (TPUs), a resource allocation typically inaccessible in standard research environments. Consequently, this study emphasizes the efficient utilization of the previously published ProtBERT, prioritizing practical application over creating a new pre-training model that might reduce computational requirements.Rights and permissions"
83,Open Access
83,"This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data."
83,"Reprints and permissionsAbout this articleCite this articleLee, J., Jun, D.W., Song, I. et al. DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning."
83,"J Cheminform 16, 14 (2024). https://doi.org/10.1186/s13321-024-00808-1Download citationReceived: 09 September 2023Accepted: 22 January 2024Published: 01 February 2024DOI: https://doi.org/10.1186/s13321-024-00808-1Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard"
83,Provided by the Springer Nature SharedIt content-sharing initiative
83,KeywordsDrug-target interactionsPre-trained language modelKnowledge adaptationLightweight framework
83,Download PDF
83,Download ePub
83,Advertisement
83,Journal of Cheminformatics
83,ISSN: 1758-2946
83,Contact us
83,Submission enquiries: journalsubmissions@springernature.com
83,Read more on our blogs
83,Receive BMC newsletters
83,Manage article alerts
83,Language editing for authors
83,Scientific editing for authors
83,Policies
83,Accessibility
83,Press center
83,Support and Contact
83,Leave feedback
83,Careers
83,Follow BMC
83,BMC Twitter page
83,BMC Facebook page
83,BMC Weibo page
83,"By using this website, you agree to our"
83,"Terms and Conditions,"
83,"Your US state privacy rights,"
83,Privacy
83,statement and
83,Cookies policy.
83,Your privacy choices/Manage cookies we use in the preference centre.
83,© 2024 BioMed Central Ltd unless otherwise stated. Part of
83,Springer Nature.
84,OData Query Options | Mendix Documentation
84,Docs
84,Release Notes
84,Mx10 Feature Release Calendar
84,Studio Pro
84,"LTS, MTS, and Monthly Releases"
84,10.8
84,10.7
84,10.6
84,10.5
84,10.4
84,10.3
84,10.2
84,10.1
84,10.0
84,9.24
84,9.23
84,9.22
84,9.21
84,9.20
84,9.19
84,9.18
84,9.17
84,9.16
84,9.15
84,9.14
84,9.13
84,9.12
84,9.11
84,9.10
84,9.9
84,9.8
84,9.7
84,9.6
84,9.5
84,9.4
84,9.3
84,9.2
84,9.1
84,9.0
84,8.18
84,8.17
84,8.16
84,8.15
84,8.14
84,8.13
84,8.12
84,8.11
84,8.10
84,8.9
84,8.8
84,8.7
84,8.6
84,8.5
84,8.4
84,8.3
84,8.2
84,8.1
84,8.0
84,7.23
84,7.22
84,7.21
84,7.20
84,7.19
84,7.18
84,7.17
84,7.16
84,7.15
84,7.14
84,7.13
84,7.12
84,7.11
84,7.10
84,7.9
84,7.8
84,7.7
84,7.6
84,7.5
84,7.4
84,7.3
84,7.2
84,7.1
84,7.0
84,Windows Service
84,Mobile
84,Make It Native Apps
84,Make It Native 10 App
84,Make It Native 9 App
84,Make It Native 8 App
84,Mendix Native Mobile Builder
84,Native Builder
84,Native Template
84,Studio Pro 9 & 10 Compatible
84,Native Template 8
84,Native Template 7
84,Native Template 6
84,Studio Pro 8 Compatible
84,Native Template 5.2
84,Native Template 5.1
84,Native Template 5.0
84,Mendix Mobile App
84,Hybrid App Base and Template
84,Developer Portal
84,Deployment
84,Mendix Cloud
84,Mendix for Private Cloud
84,SAP BTP
84,Other Deployment Options
84,Control Center
84,Marketplace
84,Catalog
84,Community Tools
84,Private Mendix Platform
84,1.6 (MTS)
84,1.5
84,SDKs
84,Model SDK
84,Platform SDK
84,Metamodel
84,10.8
84,10.7
84,10.6
84,10.5
84,10.4
84,10.3
84,10.2
84,10.1
84,10.0
84,9.24
84,9.23
84,9.22
84,9.21
84,9.20
84,9.19
84,9.18
84,9.17
84,9.16
84,9.15
84,9.14
84,9.13
84,9.12
84,9.11
84,9.10
84,9.9
84,9.8
84,9.7
84,9.6
84,9.5
84,9.4
84,9.3
84,9.2
84,9.1
84,9.0
84,8.18
84,8.16
84,8.15
84,8.14
84,8.13
84,8.12
84,8.11
84,8.10
84,8.9
84,8.8
84,8.7
84,8.6
84,8.5
84,8.4
84,8.3
84,8.2
84,8.1
84,Security Advisories
84,Beta and Experimental Releases
84,Quick Starts
84,Creating a Hello World App
84,Building a Responsive Web App
84,Adding a Native Mobile App
84,Studio Pro 10 Guide
84,Installation
84,Installing Studio Pro
84,System Requirements
84,Upgrading from Studio Pro 9 to 10
84,Configuring Parallels
84,Performance Tips
84,General Info
84,Studio Pro Overview
84,MxBuild
84,Developer Tool Recommendations
84,mx Command-Line Tool
84,App Commands
84,Adaptable Solution Commands
84,Module Commands
84,Export Package Commands
84,Merging and Diffing commands
84,MPR dump
84,Third-Party Licenses
84,App Modeling
84,Best Practices for Development
84,Best Practices for App Performance
84,Consistency Errors
84,Page Editor Consistency Errors
84,Navigation Consistency Errors
84,Importing and Exporting Elements
84,Starting with App from a Spreadsheet
84,Menus
84,File Menu
84,New App
84,Open App
84,Export App Package
84,Import App Package
84,Edit Menu
84,"Find, Find Advanced and Find Usages"
84,Go to Option
84,Preferences
84,View Menu
84,Changes Pane
84,Integration Pane
84,Errors Pane
84,Suppression Rules
84,Page Explorer
84,Stories Pane
84,App Menu
84,Create Deployment Package
84,Deploy to the Cloud
84,Run Menu
84,Edit Cloud Foundry Settings
84,Version Control Menu
84,Commit
84,History
84,Download from Version Control Server
84,Upload to Version Control Server
84,Branch Line Manager
84,Create Branch Line
84,Merge Dialog Box
84,Language Menu
84,Batch Replace
84,Batch Translate
84,Language Operations
84,Language Settings
84,Translating Your App Content
84,Using Translatable Validation Messages
84,App Explorer
84,App
84,App Settings
84,Configurations
84,Navigation
84,Set Up Navigation
84,System Texts
84,Modules
84,Module Settings
84,Publish Add-on and Solution Modules
84,Consume Add-on Modules and Solutions
84,UI Resources Package
84,Security
84,App Security
84,User Roles
84,Administrator
84,Demo Users
84,Anonymous Users
84,Password Policy
84,Module Security
84,Domain Model
84,Entities
84,External Entities
84,Persistability
84,Attributes
84,Validation Rules
84,Event Handlers
84,Indexes
84,Access Rules
84,Associations
84,Association Properties
84,Association Tab Properties
84,Querying Over Self-References
84,Annotations
84,Generalization vs 1-to-1 Associations
84,Configuring a Domain Model
84,Setting Up Data Validation
84,Pages
84,Page
84,Page Properties
84,Page Resources
84,Image Collection
84,Layout
84,Placeholder
84,Header
84,Sidebar Toggle
84,Menu
84,Snippet
84,Building Block
84,Page Template
84,Icon Collection
84,Data Containers
84,Data View
84,Grids
84,Data Grid
84,Grid Columns
84,Template Grid
84,Control Bar
84,Search Bar
84,Sort Order
84,List View
84,Data Sources
84,Database Source
84,XPath Source
84,Context Source
84,Microflow Source
84,Nanoflow Source
84,Association Source
84,Listen to Widget Source
84,Configure Form and Show Form Items
84,Configure List and View Details on 1 Page
84,Text
84,Text
84,Label
84,Page Title
84,Structure
84,Layout Grid
84,Container
84,Group Box
84,Snippet Call
84,Tab Container
84,Scroll Container
84,Table
84,Navigation List
84,Input Elements
84,Text Box
84,Text Area
84,Drop-Down
84,Check Box
84,Radio Buttons
84,Date Picker
84,Reference Selector
84,Reference Set Selector
84,Input Reference Set Selector
84,"Images, Videos, and Files"
84,Static Image
84,Dynamic Image
84,File Manager
84,Image Uploader
84,Enable End-Users to Attach Images
84,Configure File Upload and Download
84,Buttons
84,Button Properties
84,Creating a Custom Save Button
84,Menus and Navigation
84,Menu Bar
84,Simple Menu Bar
84,Navigation Tree
84,Authentication
84,Login ID Text Box
84,Password Text Box
84,Sign-In Button
84,Validation Message
84,Charts
84,Chart Configuration
84,Chart Advanced Cheat Sheet
84,Any Chart Widgets
84,Any Chart Building Blocks
84,Any Chart Cheat Sheet
84,Properties Common in the Page Editor
84,On Click Event and Events Section
84,Application Logic
84,Microflows and Nanoflows
84,Microflows
84,Microflow Properties
84,Triggering a Microflow From a Menu Item
84,Testing Microflows with Unit Test Module
84,Error Handling in Microflows
84,Extracting and Using Sub-Microflows
84,Retrieving Current User with a Microflow
84,Nanoflows
84,Nanoflow Properties
84,Error Handling in Nanoflows
84,Sequence Flow
84,Activities
84,Object Activities
84,Cast Object
84,Change Object
84,Commit Object(s)
84,Create Object
84,Delete Object(s)
84,Retrieve
84,Rollback Object
84,List Activities
84,Aggregate List
84,Change List
84,Create List
84,List Operation
84,Working with Lists in a Microflow
84,Action Call Activities
84,Java Action Call
84,JavaScript Action Call
84,Microflow Call
84,Variable Activities
84,Change Variable
84,Create Variable
84,Client Activities
84,Call Nanoflow
84,Show Message
84,Close Page
84,Download File
84,Show Home Page
84,Show Page
84,Synchronize to Device
84,Clear from Device
84,Synchronize
84,Validation Feedback
84,Integration Activities
84,Call External Action
84,Call REST Service
84,Call Web Service
84,Import Data from File
84,Import with Mapping
84,Export With Mapping
84,Query External Database
84,Send REST Request (Beta)
84,Log Message
84,Generate Document
84,Metrics Activities
84,Counter
84,Gauge
84,Increment Counter
84,ML Kit Activities
84,Call ML Model
84,Workflow Activities
84,Apply Jump-To Option
84,Call Workflow
84,Change Workflow State
84,Complete User Task
84,Generate Jump-To Options
84,Retrieve Workflow Activity Records
84,Retrieve Workflow Context
84,Retrieve Workflows
84,Show User Task Page
84,Show Workflow Admin Page
84,Lock Workflow
84,Unlock Workflow
84,Notify Workflow
84,External Object Activities
84,Delete External Object
84,Send External Object
84,Decisions
84,Decision
84,Object Type Decision
84,Merge
84,Annotation
84,Parameter
84,Loop
84,Events
84,Start Event
84,End Event
84,Error Event
84,Continue Event
84,Break Event
84,Common Properties
84,Debugging Microflows and Nanoflows
84,Debugging Microflows Remotely
84,Workflows
84,Workflow Elements
84,Workflow Parameters
84,Multi-User Task
84,User Task
84,Wait for Notification
84,Wait for Timer
84,Decision in Workflows
84,Parallel Split
84,Jump Activity
84,Call Microflow
84,Call Workflow
84,Workflow Properties
84,Configure Workflow Security
84,Workflow Engine
84,Add Workflow to Existing App
84,Jump to Different Activities
84,Workflow Events
84,Workflow Versioning and Conflict Mitigation
84,Workflow for Employee Onboarding
84,Add Custom Action to Workflow Toolbox
84,Expressions
84,Unary Expressions
84,Arithmetic Expressions
84,Relational Expressions
84,Special Checks
84,Boolean Expressions
84,If Expressions
84,Mathematical Function Calls
84,String Function Calls
84,Date Creation
84,Begin-of Date Function Calls
84,End-of Date Function Calls
84,Between Date Function Calls
84,Add Date Function Calls
84,Subtract Date Function Calls
84,Trim to Date
84,To String
84,Length
84,Parse Integer
84,Parse and Format Decimal Function Calls
84,Parse and Format Date Function Calls
84,Enumerations in Expressions
84,Configure String Concatenation
84,Mendix Assist
84,MxAssist Logic Bot
84,MxAssist Best Practice Bot
84,Recommendations from Best Practice Bot
84,Validation Assist
84,MendixChat
84,Resources
84,Java Actions
84,JavaScript Actions
84,Rules
84,Enumerations
84,Datasets
84,OQL
84,OQL Expressions
84,OQL Aggregation
84,OQL Functions
84,OQL CAST
84,OQL COALESCE
84,OQL DATEDIFF
84,OQL DATEPART
84,OQL LENGTH
84,OQL LOWER
84,OQL RANGEBEGIN
84,OQL RANGEEND
84,OQL REPLACE
84,OQL ROUND
84,OQL UPPER
84,OQL Operators
84,OQL Case Expression
84,OQL Parameters
84,OQL From Clause
84,OQL Group by Clause
84,OQL Limit Clause
84,OQL Order by Clause
84,OQL Select Clause
84,OQL Where Clause
84,Constants
84,Regular Expressions
84,Scheduled Events
84,Task Queue
84,Document Templates
84,Creating Your Own Documents
84,Data Grid (Document Template)
84,Columns (Document Template)
84,Data View (Document Template)
84,Document Template
84,Dynamic Image (Document Template)
84,Dynamic Label (Document Template)
84,Footer (Document Template)
84,Header (Document Template)
84,Line Break (Document Template)
84,Page Break (Document Template)
84,Static Image (Document Template)
84,Static Label (Document Template)
84,Style
84,Table (Document Template)
84,Row (Document Template)
84,Cell (Document Template)
84,Template Grid (Document Template)
84,Title (Document Template)
84,Data Types
84,Images
84,XPath
84,XPath Aggregate Functions
84,XPath Constraints
84,XPath Constraint Functions
84,XPath true
84,XPath false
84,XPath not
84,XPath length
84,XPath string-length
84,XPath year-from-dateTime
84,XPath month-from-dateTime
84,XPath day-from-dateTime
84,XPath hours-from-dateTime
84,XPath minutes-from-dateTime
84,XPath seconds-from-dateTime
84,XPath quarter-from-dateTime
84,XPath day-of-year-from-dateTime
84,XPath week-from-dateTime
84,XPath weekday-from-dateTime
84,XPath contains
84,XPath starts-with
84,XPath ends-with
84,XPath Expressions
84,XPath Keywords and System Variables
84,XPath Operators
84,XPath Tokens
84,Define Access Rules Using XPath
84,Filter Data Using XPath
84,Integration
84,Message Definitions
84,JSON Structures
84,XML Schemas
84,XML Schema Support
84,Mapping Documents
84,Export Mappings
84,Import Mappings
84,Map Automatically
84,ML Model Mapping
84,Select Elements
84,XML Inheritance and Choice
84,Business Event Services
84,External Database Connection
84,OData Services
84,Consumed OData Services
84,Consumed OData Service
84,Consumed OData Service Requirements
84,Published OData Services
84,Published OData Attribute
84,OData Query Options
84,OData Representation
84,Published OData Entity
84,Published OData Microflow
84,Build OData APIs with REST Best Practices
84,Security and Shared Datasets
84,REST Services
84,Consumed REST Services
84,Using a Proxy to Call a REST Service
84,Server-Side Paging and Sorting
84,Advanced Consumed REST Services
84,Published REST Services
84,Published REST Service
84,Published REST Operation
84,Operation Parameters for Published REST
84,Published REST Path Parameters
84,Published REST Query Parameters
84,Published REST Resource
84,CORS Settings for Published REST Services
84,GitHub-Flavored Markdown
84,Version a REST Service
84,Generating a Published REST Resource
84,Publish Microflow as REST Operation
84,Technical Details of Published REST
84,Published REST Request Routing
84,JSON Schema for Published REST Operation
84,OpenAPI 2.0 Documentation
84,OpenAPI 3.0 Documentation
84,Custom Authentication Microflow Parameters
84,HttpRequest and HttpResponse System Entities
84,Images and Files with REST
84,Consumed REST Services (Beta)
84,Web Services
84,Consumed Web Services
84,Consume a Simple Web Service
84,Consume a Complex Web Service
84,Consumed Web Service
84,Numeric Formatting
84,Using a Proxy to Call a Web Service
84,Published Web Services
84,Expose a Web Service
84,Operations
84,Published Web Service
84,Test Web Services Using SoapUI
84,Machine Learning Kit
84,Using ML Kit
84,Logistic Regression Example
84,Pre-Trained ML Models
84,Design Patterns
84,Advanced Inference Design Patterns
84,Pre/Post-Processor Design Patterns
84,Version Control
84,Using Version Control
84,Combining Changes and Conflict Resolution
84,Automatic Fetching
84,Git Storage Optimization
84,Troubleshoot Version Control
84,Repository Size
84,Team Server Issues
84,Version Control FAQ
84,Git On-Premises Version Control Server
84,Mendix Runtime
84,Runtime Server
84,Mendix Client
84,Mendix React Client
84,Marketplace Component React Status
84,Runtime Deployment
84,Clustered Mendix Runtime
84,Communication Patterns
84,Data Sources Retrieval
84,Minimizing Objects in Session
84,Data Storage
84,Attribute Type Migration
84,Case-Sensitive Database Behavior
84,Order By Behavior
84,Unlimited String Behavior
84,MySQL/MariaDB
84,Oracle
84,SAP HANA
84,Date and Time Handling
84,DateTime Handling FAQ
84,Logging
84,Login Behavior
84,Mendix Runtime and Java
84,Non-Persistable Objects and Garbage Collecting
84,Java Memory Usage
84,Common Runtime and Java Errors
84,Metrics
84,Monitoring Client State
84,Monitoring Mendix Runtime
84,Objects and Caching
84,Runtime Customization
84,Advanced Custom Settings
84,WebSockets
84,Mobile
84,Getting Started with Mobile
84,Prerequisites and Troubleshooting
84,Introduction to Mobile Technologies
84,Native Mobile
84,Progressive Web App
84,Designing Mobile User Interfaces
84,Design Principles
84,Navigation
84,"Images, Icons, and Fonts"
84,Native Styling
84,Widget Styling Guide
84,Building Efficient Mobile Apps
84,Optimizing Native Startup
84,Offline-First Data
84,Offline Synchronization
84,Offline Best Practices
84,Synchronization & Auto-Committed Objects
84,Offline Data Security
84,Logging in Native Apps
84,Using Mobile Capabilities
84,Authenticating Users
84,Deep Links
84,Internationalize Mobile Apps
84,Location and Maps
84,Push Notifications
84,1. Add Module Dependencies
84,2. Push Notifications Module
84,3. Set Up Firebase Cloud Messaging
84,4. Configure Push Notifications
84,5. Push Notifications in Native App
84,6. Native App with Push Notifications
84,7. Test Push Notification
84,8. Notifications to Multiple Devices
84,Local Notifications
84,Part 1: Local Notifications
84,Part 2: Badges
84,Part 3: Actions
84,Part 4: Data
84,Part 5: Scheduling
84,Augmented Reality
84,Get Started with AR
84,Create an AR Business Card
84,App Permissions
84,Mobile Accessibility
84,"Build, Test, Distribute Apps"
84,Building Native Apps
84,Build a Mendix Native App Locally
84,Native App Local Manual Build
84,Deploy Mendix Native Mobile App
84,Creating a Custom Developer App
84,Native Template
84,Distributing Native Apps
84,Updating Native Apps
84,Debugging Native Apps
84,Testing Native Apps
84,Best Practices for Mobile Apps
84,Deleted Flag
84,Incremental Synchronization
84,Batch Synchronization
84,Compound Object
84,Request Object
84,Java Programming
84,Troubleshooting
84,Using Eclipse
84,Extending App with Custom Java
84,Using the Mendix Runtime Java API
84,Java Version Migration
84,Managed Dependencies
84,Studio Pro 10 How-tos
84,Front End
84,UI Design
84,Get Started
84,Customize Styling
84,Configure Module-Level Theme Settings
84,Create a Company Design System
84,Extend Design Properties
84,Atlas UI Kit for Figma
84,Implement Best Practices for UX Design
84,Use Navigation Layouts
84,Configure Your Theme
84,Create Overview and Detail Pages
84,Use Layouts and Snippets
84,Implement Classes
84,Create Custom Error Pages
84,Data Models
84,Denormalize Data to Improve Performance
84,Share the Development Database
84,Migrate Your Mendix Database
84,Integration
84,Integrate Legacy System
84,Import XML Documents
84,Export XML Documents
84,Import Excel Documents
84,Import a Large Excel File
84,Export to Excel
84,Publish a REST Service
84,Share Data Between Apps
84,Access a Samba Share
84,Expose Data to BI Tools Using OData
84,Configure Selenium Support
84,Execute SQL on External Database
84,Write Data to Another App
84,Use the Data Importer
84,Use the External Database Connector
84,CI/CD Pipeline for Mendix Cloud
84,Use a Client Certificate
84,Extensibility
84,Build a Pluggable Native Widget
84,Update Pluggable Widgets Tools
84,Build Pluggable Web Widgets
84,1. Build Pluggable Web Widget
84,2. Build Pluggable Web Widget
84,Build JavaScript Actions
84,1. Build JavaScript Actions
84,2. Build JavaScript Actions
84,Build JavaScript Actions for Native Mobile
84,JavaScript Actions Best Practices
84,Build Microflow Actions with Java
84,Data Storage APIs for Reusable Microflows
84,Security
84,Create a Secure App
84,Best Practices for App Security
84,Set Up Anonymous User Security
84,Content Security Policy
84,Testing
84,Test Mendix Apps Using Selenium IDE
84,Create Automated Tests with TestNG
84,Monitoring and Troubleshooting
84,Clear Warning Messages
84,Debug Java Actions
84,Debug Java Actions Remotely
84,Find the Root Cause of Runtime Errors
84,Set Log Levels
84,Monitor Mendix Using JMX
84,Solve Load and Import Errors
84,Manage App Performance
84,Manage App Performance with New Relic
84,Detect and Resolve Performance Issues
84,Populate User Types
84,Developer Portal Guide
84,Global Navigation
84,General
84,Buzz
84,Team
84,App Roles
84,Documents
84,Team Server
84,Migrate to Git
84,Settings
84,Leave and Delete an App
84,Manage Deep Links
84,Project Management
84,Epics
84,Board
84,Planning
84,Epics
84,Archive
84,Jira
84,App Insights
84,Feedback
84,Mini Surveys
84,Deployment
84,General
84,Licensing Apps
84,Secure Outgoing Connections
84,Two-Factor Authentication
84,Version Downgrade Protection
84,Iframes and Running Apps
84,Deployment Location
84,SAP BTP
84,Monitoring Environments in Mendix Apps on SAP BTP
84,SAP Destination Service
84,Use SAP Connectivity Service with REST and SOAP
84,SAP Cloud Connector
84,Application Autoscaler for SAP BTP
84,SAP Single Sign-On
84,Mendix Cloud
84,About Mendix Cloud
84,Environments
84,Environment Details
84,Migrate to Other Node
84,Studio Pro Deployment Settings
84,Licensing Mendix Cloud Apps
84,Mendix Basic Package
84,Free App to Basic Package
84,Node Permissions
84,Running Now
84,Mendix Cloud Status
84,Mendix Cloud Region
84,Scaling in Mendix Cloud
84,Custom Domains
84,Certificates
84,Maintenance Windows
84,Pipelines (Beta)
84,HTTP Request Headers
84,Restrict Incoming Access
84,Mendix IP Addresses
84,Sending Email
84,Mendix Single Sign-On
84,Webhooks
84,Siemens Insights Hub
84,Private Cloud
84,Creating a Private Cloud Cluster
84,Non-Interactive Mode
84,Storage Plans
84,Registry Configuration
84,Hosting Your Own Registry
84,Running the Mendix Operator in Global Mode
84,Running the Mendix Operator in Standard Mode
84,Deploy Mendix App
84,Retrieve Environment-Sensitive Data from a Secret Store
84,Use Velero to Back Up Namespaces
84,Use CLI to Deploy
84,CI/CD with Tekton
84,Air-gapped Tekton Installation
84,PCLM – License Manager
84,Monitor Environments
84,Migrate Data (Preview)
84,Environment Planning
84,Technical Appendix for Mendix Private Cloud
84,1. Introduction to Operators
84,2. Operator Flows
84,Upgrading Private Cloud
84,Supported Providers
84,Industrial Edge Apps
84,Cloud Foundry
84,Docker
84,Run Docker Image
84,Run with Minikube
84,On-Premises
84,On-Premises Installation Security
84,Monitoring with New Relic
84,Microsoft Windows
84,Automate Mendix Deployment
84,Deploy Mendix in MS Azure
84,MS Windows: Activate Mendix License
84,MS Windows: Update a Mendix App
84,Microsoft SQL Server
84,New Database Setup on SQL Server
84,User Setup on SQL Server
84,Database User Setup on SQL Server
84,Maintenance of SQL Server
84,Maintenance Plans for SQL Server
84,Restore Database on SQL Server
84,Troubleshooting SQL Server
84,Troubleshooting IIS
84,Unix-Like Deployment
84,Mobile App
84,Operations
84,Alerts
84,Receive Environment Status Alerts
84,Logs
84,Metrics
84,Monitoring with APM
84,AppDynamics for Mendix Cloud
84,Datadog for Mendix Cloud
84,New Relic for Mendix Cloud
84,Dynatrace for Mendix Cloud
84,Splunk for Mendix Cloud
84,Backups
84,Creating a Backup
84,Downloading a Backup
84,Restoring a Backup
84,Restoring a Backup Locally
84,Reducing Database Size
84,Portfolio Management
84,Prioritization Models
84,Export and Import Initiatives
84,Control Center Guide
84,Dashboard
84,Application Health Dashboard
84,Apps
84,Members
84,Groups
84,Company Settings
84,Company Brand
84,Security
84,Set Up an SSO (BYOIDP)
84,Cloud
84,Entitlements
84,Deployed Apps
84,Catalog
84,Portfolios
84,Private Marketplace
84,Roles & Permissions
84,Marketplace Guide
84,Marketplace Overview
84,My Marketplace
84,Using Marketplace Content
84,SISW EULA for Freeware
84,Mendix Component Partner Program
84,Creating Content
84,Create Solutions
84,Commercial Solution Partner Programs
84,Introduction to Adaptable Solutions
84,Architect Solutions
84,Apply IP Protection
84,Best Practices for Adaptability
84,Implement Solutions
84,Upgrade a Solution
84,Set Up a Solution
84,Build a Connector
84,Best Practices for Building Connectors
84,Sharing Marketplace Content
84,Governance Process
84,Modules
84,AWS Connectors
84,AWS Authentication
84,Amazon Bedrock
84,Amazon DynamoDB
84,Amazon EventBridge
84,Amazon Polly
84,Amazon RDS
84,Amazon Rekognition
84,Amazon S3
84,Amazon SageMaker
84,Amazon SES
84,Amazon SNS
84,Amazon Textract
84,Amazon Translate
84,AWS IoT SiteWise
84,AWS IoT TwinMaker
84,AWS Lambda
84,Build an AWS Connector
84,Amazon SQS
84,SAP Connectors
84,OData Connector for SAP Solutions
84,BAPI Connector for SAP Solutions
84,SAP Event Mesh Connector
84,XSUAA Connector for SAP BTP
84,SAP Logging Connector
84,SAP Fiori UI Resources
84,SAP Horizon Native UI Resources
84,Administration
84,Advanced Audit Trail
84,Advanced Audit Trail UI
84,Any Chart
84,App Switcher ⚠
84,Atlas Core
84,Atlas UI Resources ⚠
84,Audit Trail
84,Community Commons
84,Data Importer
84,Data Widgets
84,Data Grid 2
84,Gallery
84,Tree Node
84,Database
84,Database Replication
84,Deep Link ⚠
84,Email
84,Encryption
84,Excel Exporter
84,Excel Importer
84,External Database Connector
84,Forgot Password
84,Google Tag
84,Hybrid Mobile Actions ⚠
84,Image Crop
84,LDAP
84,Mendix Feedback
84,Mendix Mini Surveys
84,Mendix SSO
84,Mobile SSO
84,MQTT
84,Mx Model Reflection
84,Nanoflow Commons
84,Native Mobile AR
84,Native Mobile Resources
84,Object Handling
84,OIDC SSO
84,OpenAI
84,RAG Example Implementation
84,Vector Database Setup
84,PDF Document Generation
84,Process Queue ⚠
84,Push Notifications Connector
84,SAML
84,Unit Testing
84,User Migration
84,Web Actions
84,Workflow Commons
84,Services
84,Event Broker
84,Mendix Business Events
84,Model Creator for SAP Integrations
84,OIDC Provider
84,Pusher
84,Widgets
84,Widget CSP Overview
84,HTML/JavaScript Snippet CSP
84,Maps CSP
84,Accessibility Helper
84,Accordion
84,Auto-Load More ⚠
84,Badge
84,Badge Button
84,Barcode Scanner
84,Bootstrap Tooltip ⚠
84,Calendar
84,Carousel
84,Charts
84,Create a Basic Chart
84,Use Any Chart
84,Chart Advanced Tuning
84,Use the Charts Theme
84,Create a Dynamic Series Chart
84,Use a Chart with a REST Data Source
84,Plotly Images REST Endpoint
84,Checkbox Set Selector
84,Color Picker
84,Combo Box
84,Fieldset
84,Format String ⚠
84,Google Analytics
84,Google Maps ⚠
84,HTML Element
84,HTML/JavaScript Snippet
84,Image
84,Label Selector
84,Language Selector
84,List View Swipe ⚠
84,Maps
84,Microflow Timer
84,Mobile Device ⚠
84,Mobile Features ⚠
84,Pop-Up Menu
84,Progress Bar
84,Progress Circle
84,Pull to Refresh ⚠
84,Radio Button List
84,Range Slider
84,Rating
84,Rich Text
84,Rich Text v2.0 & Below
84,Signature
84,Simple Checkbox Set Selector
84,Slider
84,Switch
84,Tab Swipe ⚠
84,Timeline
84,Tooltip
84,Video Player
84,Partner Solutions
84,APD
84,APD Installation Guides
84,APD 3 Installation Guide
84,APM 2 Installation Guide
84,APM 1 Installation Guide
84,Prerequisites
84,Java Security Settings
84,Sizing Impact
84,Installation Steps
84,After Startup Error?
84,Constants
84,Uninstall Steps
84,Upgrade Steps
84,APM Use Cases
84,APD 3 Use Cases
84,APM 2 Use Cases
84,APM 1 Use Cases
84,APD Reference Guides
84,APD 3 Reference Guide
84,Apps
84,Dashboard
84,Environments
84,Logs
84,Long-Running Actions
84,Performance Recorder
84,Browser Recorder Results
84,Runtime Recorder Results
84,Performance Statistics
84,Settings
84,APM 2 Reference Guide
84,Apps
84,Dashboard
84,Environments
84,Logs
84,Long-Running Actions
84,Performance Recorder
84,Browser Recorder Results
84,Runtime Recorder Results
84,Performance Statistics
84,Settings
84,APM 1 Reference Guide
84,Configuration
84,Dashboard
84,Download and License
84,JVM Browser
84,Load Test Recorder
84,Log Tool
84,Measurements Tool
84,Performance Tool
84,Inserting Context Information
84,Performance Tool Results
84,Query Tool
84,Statistics Tool
84,Trap Tool
84,Triggers
84,APD Release Notes
84,ATS
84,ATS Overview
84,Introduction to ATS
84,Compatibility
84,Deployment Options
84,Maintenance
84,ATS Reference Guides
84,ATS 2 Reference Guide
84,Action
84,Administration
84,App
84,ATS Helper
84,CI/CD API
84,Data-Driven Testing
84,Desktop Recorder
84,Drop-Down
84,Function Reference
84,Local Profile
84,On-Premises Installation
84,Repository
84,Results
84,Schedule
84,Selectors
84,Supported Widgets
84,Test Case
84,Test Run
84,Compatibility Table
84,Job Configuration
84,Supported Selenium Providers
84,Test Step
84,Test Suite
84,ATS 1 Reference Guide
84,Administration
84,Configuration
84,Data Management
84,Monitoring
84,Projects
84,Scheduling
84,Test Development
84,Object Types in ATS
84,Recorder
84,Manual Test Steps
84,Standard Actions
84,Custom Actions
84,Best Practices for Writing Custom Actions
84,Selectors
84,Data-Driven Tests
84,Test Case Documentation
84,Standard Actions Reference
84,ATS Core Actions
84,Assert Equals
84,Assert Not equals
84,Concatenate String
84,Get Current DateTime String
84,Random Number
84,Random String
84,Set Return Value
84,Mendix Actions
84,"DataGrid, TemplateGrid, and ListView"
84,Click DataGrid Row
84,Find Item/Row
84,Find Item/Row (by child element)
84,Find Selected Item/Row
84,Find/Assert DataGrid Row
84,Get Item/Row Index
84,Get Row Cell Value
84,Get Total Item/Row Count
84,Get Visible Item/Row Count
84,Set ListView Search
84,Set Row Cell Value
84,Sort DataGrid
84,Dialog
84,Cancel Dialog
84,Close Dialog
84,Confirm Dialog
84,Find/Assert Dialog
84,Get Dialog Message Text
84,File Manager
84,Set File Manager
84,Generic
84,Assert Current Page
84,Assert Validation Message
84,Click Widget
84,Click Widget Button
84,Click/Doubleclick
84,Find/Assert Widget
84,Get Validation Message
84,Login
84,Logout
84,Open Application
84,GroupBox
84,Close GroupBox
84,GroupBox is Collapsed
84,Open GroupBox
84,Input
84,Assert Checkbox Value
84,Assert Value
84,Dropdown has Option
84,Get Checkbox Value
84,Get Index
84,Get Value
84,Set Checkbox Value
84,Set Value
84,Set Value (by Index)
84,Toggle Checkbox Value
84,Navigation Menu
84,Click Menu Item
84,Find/Assert Menu Item
84,System
84,Find Widget Child Node
84,Focus WebElement
84,Get Current Page Title
84,Mendix wait
84,Tab
84,Assert Active Tab Caption
84,Get Active Tab Caption
84,Mendix Marketplace Widgets Actions
84,BooleanSlider
84,Assert BooleanSlider Value
84,Get BooleanSlider Value
84,Set BooleanSlider Value
84,Toggle BooleanSlider Value
84,BootstrapRTE
84,Assert BootstrapRTE Value
84,Get BootstrapRTE Value
84,Set BootstrapRTE Value
84,Checkbox Set Selector
84,Assert Checkbox Set Selector Value
84,Find Checkbox Set Selector
84,Find Checkbox Set Selector (All)
84,Get Checkbox Set Selector Value
84,Get Checkbox Set Selector Value (All)
84,Set Checkbox Set Selector Value
84,Set Checkbox Set Selector Value
84,Toggle Checkbox Set Selector Value
84,Toggle Checkbox Set Selector Value
84,CKEditor
84,Assert CKEditor Value
84,Get CKEditor Value
84,Set CKEditor Value
84,Dropdown Div Converter
84,Click Drop-Down div Converter Drop-Down Button
84,Click Drop-Down div Converter Split Button
84,Grid Selector
84,Assert Grid Selector Value
84,Find Grid Selector Box
84,Get Grid Selector Box Value
84,Set Checkbox Set Selector Value
84,Set Grid Selector RadioButton Value
84,Toggle Grid Selector Checkbox Value
84,Input reference Selector
84,Assert InputReferenceSelector Value
84,Get InputReferenceSelector Value
84,Set InputReferenceSelector Value
84,Simple Checkbox Set Selector
84,Assert Simple Checkbox Set Selector Value
84,Find Simple Checkbox Set Selector
84,Get Simple Checkbox Set Selector Value
84,Set Simple Checkbox Set Selector Value
84,Toggle Simple Checkbox Set Selector Value
84,Selenium Actions
84,Click Coordinates
84,Execute JavaScript Integer
84,Execute Javascript String
84,Execute Javascript WebElement
84,Find
84,Find Element by CSS
84,Find Element by ID
84,Find Element by Sizzle
84,Get
84,Get Property Value
84,Get Selected Option Index
84,Get Selected Option Text
84,Get Selected Option Value
84,Get Text
84,Send Keys
84,Test Run
84,ATS How-tos
84,ATS 2 How-tos
84,(Un)Mask Your Data
84,Assert Data Grid Rows
84,Configure a Selenium Hub
84,Create a Data-Driven Test Case
84,Create a Negative Test Case
84,Create a Test Case
84,Create a Test Suite
84,Create Custom Actions
84,Create Custom Action Basics
84,Create Search Context Actions
84,CAB.11 - Find Item/Row by Unique Text Value
84,Create Unsupported Widget Actions
84,CAB.02 - Switch
84,CAB.03 - Textbox
84,CAB.05 - Reference Selector
84,CAB.07 - Radio Buttons
84,CAB.10 - AutoComplete
84,General
84,Custom Action Expense App
84,Definitions
84,Guidelines for Creating a Custom Action
84,Helpful Resources
84,Prerequisites for How-To's
84,Structure for How-To's
84,Create Maintainable Test Cases
84,Get Started
84,Increase ATS Recorder and Helper Coverage
84,Install ATS Helper and ATS Recorder
84,Link Test Cases and Suites to User Stories
84,Schedule a Test Suite/Test Case
84,Set Up Selenium Locally
84,Set Up a Local Docker Selenium Hub
84,Set Up a Local Selenium Hub
84,Set Up a Local Selenium Solution
84,Set Up a Local Selenoid Hub
84,Upload a File in Your App Using ATS
84,Browserstack Test Files
84,Use ATS in Combination with CI/CD
84,Use Precondition in Test Cases
84,ATS 1 How-tos
84,Get Started
84,Install ATS Helper and Recorder
84,Create a Test Case
84,Create a Test Suite
84,Create Custom Actions
84,Create Custom Action Basics
84,Create Search Context Actions
84,CAB.11 - Find Item/Row by Unique Text Value
84,Create Unsupported Widget Actions
84,CAB.02 - Switch
84,CAB.03 - Textbox
84,CAB.05 - Reference Selector
84,CAB.07 - Radio Buttons
84,CAB.10 - AutoComplete
84,General
84,Custom Action Expense App
84,Definitions
84,Guidelines for Creating a Custom Action
84,Helpful Resources
84,Prerequisites for How-tos
84,Structure for How-tos
84,Upload a File in Your App Using ATS
84,ATS Best Practices
84,ATS 2 Best Practices
84,Finding the Action You Need
84,Test Case Dependencies
84,ATS 1 Best Practices
84,Finding the Action You Need
84,ATS Release Notes
84,QSM
84,Catalog Guide
84,Get Started with the Catalog
84,Register Data Sources
84,Register Resources
84,Register Non-OData Resources
84,Automate Catalog Registration
84,Private Cloud/On-Premises Registration
84,OpenAPI Beta Functionality
84,Consume Data Sources
84,Consume Registered Assets
84,Manage Data Sources
84,Landscape View
84,Catalog User Roles
84,Curate Registered Assets
84,Data Accessibility and Security
84,Search in the Catalog
84,Private Mendix Platform Guide
84,Private Mendix Platform Prerequisites
84,Private Mendix Platform Quick Start Guide
84,Configuring Private Mendix Platform
84,Configuring CI/CD on Azure
84,Configuring CI/CD on Kubernetes
84,Configuring the Version Control System for Private Mendix Platform
84,Private Mendix Platform Administration Guide
84,Private Mendix Platform User Guide
84,Community Tools Guide
84,Mendix Profile
84,User Settings
84,Mendix Community
84,Set Up Your Partner Profile
84,Contribute to a GitHub Repo
84,OAuth and Scopes
84,Contribute to Mendix Docs
84,Documentation Writing Guidelines
84,Mendix Support Guide
84,Prepare Your App for Support
84,Submit a Support Request
84,App Node Requests
84,Support Ticket Priority
84,Support Escalation Process
84,Security Findings FAQ
84,Strategic Partners Guide
84,Siemens
84,Insights Hub
84,Insights Hub IIoT for Makers
84,Mendix on Insights Hub
84,Insights Hub Development Considerations
84,Insights Hub Module Details
84,Insights Hub Monitor Example
84,Insights Hub Mobile Native
84,3D Viewer
84,3D Viewer for Teamcenter
84,Use the 3D Viewer API
84,AWS
84,SAP
84,APIs and SDK
84,API Documentation
84,App Repository API
84,Authentication
84,Backups API v2
84,Build API
84,Catalog APIs
84,Client API
84,Content API
84,Deploy API v1
84,Deploy API v2
84,Deploy API v4
84,Design Properties API
84,Epics API
84,Feedback API v1 ⚠
84,Feedback API v2
84,Mendix for Private Cloud Build API
84,Mendix for Private Cloud Deploy API
84,Mendix Runtime API
84,Model SDK and Platform SDK
84,Permissions API ⚠
84,Pluggable Widgets API
84,Property Types
84,Client APIs for Pluggable Widgets
84,List Values
84,Preview Appearance APIs
84,Configuration Module API
84,Declaring Native Dependencies
84,Mendix 9
84,Property Types – Mx9
84,Client APIs for Pluggable Widgets
84,List Values – Mx9
84,Preview Appearance APIs
84,Configuration Module API – Mx9
84,Declaring Native Dependencies – Mx9
84,Mendix 8
84,Property Types – Mx8
84,Client APIs for Pluggable Widgets
84,Preview Appearance APIs
84,Compare Pluggable and Custom Widgets
84,Private Mendix Platform API Documentation
84,Private Mendix Platform Group API
84,Private Mendix Platform Marketplace API
84,Private Mendix Platform Project API
84,Private Mendix Platform User API
84,Projects API ⚠
84,Stories API ⚠
84,Team Server API ⚠
84,User Management API ⚠
84,Webhooks API
84,Webhooks for Stories/Sprints ⚠
84,SDK Documentation
84,SDK Introduction
84,SDK Use Cases
84,SDK FAQ and Troubleshooting
84,SDK Reference Guide
84,Mendix Metamodel
84,Projects in the Metamodel
84,Domain Model in the Metamodel
84,Pages in the Metamodel
84,Microflows in the Metamodel
84,JavaScript and TypeScript Resources
84,SDK How-tos
84,Set Up Your Development Environment
84,Set Up your Personal Access Token (PAT)
84,Use the Platform SDK
84,Create Your First Script
84,Create the Domain Model
84,Manipulate Existing Models
84,Change Things in the Model
84,Close the Server Connection
84,Find Things in the Model
84,Work with Load Units and Elements
84,Generate SDK Script Based on Model
84,Old SDK Versions (Below 5.0) ⚠
84,Set Up Development Environment
84,Create Your First Script
84,Studio Pro 9 Guide
84,General Info
84,System Requirements
84,Install Mendix Studio Pro
84,Configure Parallels
84,Moving from Mendix Studio Pro 8 to 9
84,Migrate From Atlas 2 To Atlas 3
84,Atlas 3 Change Summary
84,Migrate Workflow Apps
84,mx Command-Line Tool
84,MxBuild
84,Developer Tool Recommendations
84,Third-Party Licenses
84,App Modeling
84,Studio Pro Overview
84,Best Practices for Development
84,Best Practices for App Performance
84,Importing and Exporting Elements
84,Starting with App from a Spreadsheet
84,Menus
84,File Menu
84,New App
84,Open App
84,Export App Package
84,Import App Package
84,Edit Menu
84,"Find, Find Advanced and Find Usages"
84,Go to Option
84,Preferences
84,View Menu
84,Changes Pane
84,Data Hub Pane
84,Errors Pane
84,Consistency Errors
84,Page Editor Consistency Errors
84,Navigation Consistency Errors
84,Suppression Rules
84,Page Explorer
84,Stories Pane
84,App Menu
84,Create Deployment Package
84,Deploy to the Cloud
84,Run Menu
84,Edit Cloud Foundry Settings
84,Version Control Menu
84,Commit
84,History
84,Download from Version Control Server
84,Upload to Version Control Server
84,Branch Line Manager
84,Create Branch Line
84,Merge Dialog
84,Language Menu
84,Batch Replace
84,Batch Translate
84,Language Operations
84,Language Settings
84,Translating Your App Content
84,Using Translatable Validation Messages
84,App Explorer
84,App
84,App Settings
84,Configurations
84,Navigation
84,Set Up Navigation
84,System Texts
84,Modules
84,Module Settings
84,Publish Add-on and Solution Modules
84,Consume Add-on Modules and Solutions
84,UI Resources Package
84,Security
84,App Security
84,User Roles
84,Administrator
84,Demo Users
84,Anonymous Users
84,Password Policy
84,Module Security
84,Domain Model
84,Entities
84,Persistability
84,Attributes
84,Validation Rules
84,Event Handlers
84,Indexes
84,Access Rules
84,External Entities
84,Associations
84,Association Properties
84,Association Tab Properties
84,Querying Over Self-References
84,Annotations
84,Generalization vs 1-to-1 Associations
84,Creating a Basic Data Layer
84,Setting Up Data Validation
84,Pages
84,Page
84,Page Properties
84,Page Resources
84,Icon Collection
84,Image Collection
84,Layout
84,Placeholder
84,Header
84,Sidebar Toggle
84,Page Template
84,Snippet
84,Building Block
84,Menu
84,Data Containers
84,Data View
84,Grids
84,Data Grid
84,Grid Columns
84,Template Grid
84,Control Bar
84,Search Bar
84,Sort Bar
84,List View
84,Data Sources
84,Database Source
84,XPath Source
84,Context Source
84,Microflow Source
84,Nanoflow Source
84,Association Source
84,Listen to Widget Source
84,Configure Form and Show Form Items
84,Configure List and View Details on 1 Page
84,Text
84,Text
84,Label
84,Page Title
84,Structure
84,Layout Grid
84,Container
84,Group Box
84,Snippet Call
84,Tab Container
84,Scroll Container
84,Table
84,Navigation List
84,Input Elements
84,Text Box
84,Text Area
84,Drop-Down
84,Check Box
84,Radio Buttons
84,Date Picker
84,Reference Selector
84,Reference Set Selector
84,Input Reference Set Selector
84,"Images, Videos and Files"
84,Static Image
84,Dynamic Image
84,File Manager
84,Image Uploader
84,Enable End-Users to Attach Images
84,Configure File Upload and Download
84,Buttons
84,Button Properties
84,Creating a Custom Save Button
84,Menus and Navigation
84,Menu Bar
84,Simple Menu Bar
84,Navigation Tree
84,Reports
84,Report Grid
84,Report Parameter
84,Report Date Parameter
84,Date Range Field
84,Generate Report Button
84,Authentication
84,Login ID Text Box
84,Password Text Box
84,Sign-In Button
84,Validation Message
84,Charts
84,Chart Configuration
84,Chart Advanced Cheat Sheet
84,Any Chart Widgets
84,Any Chart Building Blocks
84,Any Chart Cheat Sheet
84,Properties Common in the Page Editor
84,On Click Event and Events Section
84,Application Logic
84,Microflows and Nanoflows
84,Microflows
84,Microflow Properties
84,Triggering a Microflow From a Menu Item
84,Testing Microflows with Unit Test Module
84,Error Handling in Microflows
84,Extracting and Using Sub-Microflows
84,Nanoflows
84,Nanoflow Properties
84,Error Handling in Nanoflows
84,Sequence Flow
84,Activities
84,Object Activities
84,Cast Object
84,Change Object
84,Commit Object(s)
84,Create Object
84,Delete Object(s)
84,Retrieve
84,Rollback Object
84,List Activities
84,Aggregate List
84,Change List
84,Create List
84,List Operation
84,Working with Lists in a Microflow
84,Action Call Activities
84,Java Action Call
84,JavaScript Action Call
84,Microflow Call
84,Variable Activities
84,Change Variable
84,Create Variable
84,ML Kit Activities
84,Call ML Model
84,Client Activities
84,Call Nanoflow
84,Show Message
84,Close Page
84,Download File
84,Show Home Page
84,Show Page
84,Synchronize to Device
84,Synchronize
84,Validation Feedback
84,Integration Activities
84,Call REST Service
84,Call Web Service
84,Import with Mapping
84,Export With Mapping
84,Log Message
84,Generate Document
84,Workflow Activities
84,Apply Jump-To Option
84,Workflow Call
84,Change Workflow State
84,Complete Task
84,Generate Jump-To Options
84,Retrieve Workflow Context
84,Show User Task Page
84,Show Workflow Admin Page
84,Lock Workflow
84,Unlock Workflow
84,External Object Activities
84,Delete External Object
84,Send External Object
84,Metrics Activities
84,Counter
84,Gauge
84,Increment Counter
84,Decisions
84,Decision
84,Object Type Decision
84,Merge
84,Annotation
84,Parameter
84,Loop
84,Events
84,Start Event
84,End Event
84,Error Event
84,Continue Event
84,Break Event
84,Common Properties
84,Debugging Microflows and Nanoflows
84,Debugging Microflows Remotely
84,Workflows
84,Workflow Elements
84,Workflow Parameters
84,User Task
84,Decision in Workflows
84,Parallel Split
84,Jump Activity
84,Call Microflow
84,Call Workflow
84,Workflow Properties
84,Configure Workflow Security
84,Add Workflow to Existing App
84,Jump to Different Activities
84,Workflow Versioning and Conflict Mitigation
84,Workflow for Employee Onboarding
84,Add Custom Action to Workflow Toolbox
84,Expressions
84,Unary Expressions
84,Arithmetic Expressions
84,Relational Expressions
84,Special Checks
84,Boolean Expressions
84,If Expressions
84,Mathematical Function Calls
84,String Function Calls
84,Date Creation
84,Begin-of Date Function Calls
84,End-of Date Function Calls
84,Between Date Function Calls
84,Add Date Function Calls
84,Subtract Date Function Calls
84,Trim to Date
84,To String
84,Parse Integer
84,Parse and Format Decimal Function Calls
84,Parse and Format Date Function Calls
84,Enumerations in Expressions
84,Configure String Concatenation
84,Mendix Assist
84,MxAssist Logic Bot
84,MxAssist Performance Bot
84,Performance Best Practices
84,Validation Assist
84,Resources
84,Java Actions
84,JavaScript Actions
84,Rules
84,Enumerations
84,Datasets
84,OQL
84,OQL Expressions
84,OQL Aggregation
84,OQL Functions
84,OQL CAST
84,OQL COALESCE
84,OQL DATEDIFF
84,OQL DATEPART
84,OQL LENGTH
84,OQL LOWER
84,OQL RANGEBEGIN
84,OQL RANGEEND
84,OQL REPLACE
84,OQL ROUND
84,OQL UPPER
84,OQL Operators
84,OQL Case Expression
84,OQL Parameters
84,OQL From Clause
84,OQL Group by Clause
84,OQL Limit Clause
84,OQL Order by Clause
84,OQL Select Clause
84,OQL Where Clause
84,Constants
84,Regular Expressions
84,Scheduled Events
84,Scheduled Events – Task Queue
84,Legacy Scheduled Events
84,Task Queue
84,Document Templates
84,Creating Your Own Documents
84,Data Grid (Document Template)
84,Columns (Document Template)
84,Data View (Document Template)
84,Document Template
84,Dynamic Image (Document Template)
84,Dynamic Label (Document Template)
84,Footer (Document Template)
84,Header (Document Template)
84,Line Break (Document Template)
84,Page Break (Document Template)
84,Static Image (Document Template)
84,Static Label (Document Template)
84,Style
84,Table (Document Template)
84,Row (Document Template)
84,Cell (Document Template)
84,Template Grid (Document Template)
84,Title (Document Template)
84,Data Types
84,Images
84,XPath
84,XPath Aggregate Functions
84,XPath avg
84,XPath count
84,XPath max
84,XPath min
84,XPath sum
84,XPath Constraints
84,XPath Constraint Functions
84,XPath contains
84,XPath day-from-dateTime
84,XPath day-of-year-from-dateTime
84,XPath ends-with
84,XPath false
84,XPath hours-from-dateTime
84,XPath length
84,XPath minutes-from-dateTime
84,XPath month-from-dateTime
84,XPath not
84,XPath quarter-from-dateTime
84,XPath seconds-from-dateTime
84,XPath starts-with
84,XPath string-length
84,XPath true
84,XPath week-from-dateTime
84,XPath weekday-from-dateTime
84,XPath year-from-dateTime
84,XPath Expressions
84,XPath Keywords and System Variables
84,XPath Operators
84,XPath Tokens
84,Define Access Rules Using XPath
84,Filter Data Using XPath
84,Integration
84,Message Definitions
84,JSON Structures
84,XML Schemas
84,XML Schema Support
84,Mapping Documents
84,Export Mappings
84,Import Mappings
84,Map Automatically
84,ML Model Mapping
84,Select Elements
84,XML Inheritance and Choice
84,Business Event Services
84,OData Services
84,Consumed OData Services
84,Consumed OData Service
84,Consumed OData Service Requirements
84,Published OData Services
84,Published OData Attribute
84,OData Query Options
84,OData Representation
84,Published OData Resource
84,Wrap with OData
84,REST Services
84,Consumed REST Services
84,Using a Proxy to Call a REST Service
84,Server-Side Paging and Sorting
84,Consume a REST Service
84,Published REST Services
84,Publish a REST Service
84,Published REST Service
84,Published REST Operation
84,Operation Parameters for Published REST
84,Published REST Path Parameters
84,Published REST Query Parameters
84,Published REST Resource
84,CORS Settings for Published REST Services
84,GitHub-Flavored Markdown
84,Version a REST Service
84,Generating a Published REST Resource
84,Publish Microflow as REST Operation
84,Technical Details of Published REST
84,Published REST Request Routing
84,JSON Schema for Published REST Operation
84,OpenAPI 2.0 Documentation
84,Custom Authentication Microflow Parameters
84,HttpRequest and HttpResponse System Entities
84,Images and Files with REST
84,Web Services
84,Consumed Web Services
84,Consume a Simple Web Service
84,Consume a Complex Web Service
84,Consumed Web Service
84,Numeric Formatting
84,Using a Proxy to Call a Web Service
84,Published Web Services
84,Expose a Web Service
84,Operations
84,Published Web Service
84,Test Web Services Using SoapUI
84,Machine Learning Kit
84,Using ML Kit
84,Logistic Regression Example
84,Pre-Trained ML Models
84,Design Patterns
84,Advanced Inference Design Patterns
84,Pre/Post-Processor Design Patterns
84,Version Control
84,Using Version Control
84,Merge Algorithm and Conflict Resolution
84,Git Storage Optimization
84,Troubleshoot Version Control
84,Solving Git Issues
84,Team Server Issues
84,Version Control FAQ
84,Differences Between Git and SVN
84,SVN On-Premises Version Control Server
84,Git On-Premises Version Control Server
84,Mendix Runtime
84,Runtime Server
84,Mendix Client
84,Runtime Deployment
84,Clustered Mendix Runtime
84,Communication Patterns
84,Minimizing Objects in Session
84,Data Storage
84,Attribute Type Migration
84,Case-Sensitive Database Behavior
84,Order By Behavior
84,Unlimited String Behavior
84,DB2
84,MySQL/MariaDB
84,Oracle
84,SAP HANA
84,Date and Time Handling
84,DateTime Handling FAQ
84,Logging
84,Login Behavior
84,Mendix Runtime and Java
84,Non-Persistable Objects and Garbage Collecting
84,Java Memory Usage
84,Common Runtime and Java Errors
84,Metrics
84,Monitoring Client State
84,Monitoring Mendix Runtime
84,Objects and Caching
84,Runtime Customization
84,Advanced Custom Settings
84,WebSockets
84,Mobile
84,Getting Started with Mobile
84,Prerequisites and Troubleshooting
84,Introduction to Mobile Technologies
84,Native Mobile
84,Progressive Web App
84,Hybrid Mobile (Deprecated)
84,Designing Mobile User Interfaces
84,Design Principles
84,Navigation
84,"Images, Icons, and Fonts"
84,Native Styling
84,Widget Styling Guide
84,Building Efficient Mobile Apps
84,Optimizing Native Startup
84,Offline-First Data
84,Offline Synchronization
84,Offline Best Practices
84,Synchronization & Auto-Committed Objects
84,Offline Data Security
84,Logging in Native Apps
84,Using Mobile Capabilities
84,Deep Links
84,Internationalize Mobile Apps
84,Location and Maps
84,Push Notifications
84,1. Add Module Dependencies
84,2. Push Notifications Module
84,3. Set Up Firebase Cloud Messaging
84,4. Configure Push Notifications
84,5. Push Notifications in Native App
84,6. Native App with Push Notifications
84,7. Test Push Notification
84,8. Notifications to Multiple Devices
84,Local Notifications
84,Part 1: Local Notifications
84,Part 2: Badges
84,Part 3: Actions
84,Part 4: Data
84,Part 5: Scheduling
84,Augmented Reality
84,Get Started with AR
84,Create an AR Business Card
84,App Permissions
84,Mobile Accessibility
84,"Build, Test, Distribute Apps"
84,Building Native Apps
84,Build a Mendix Native App Locally
84,Deploy Mendix Native Mobile App
84,Native App Local Manual Build
84,Creating a Custom Developer App
84,Native Template
84,Distributing Native Apps
84,Updating Native Apps
84,Debugging Native Apps
84,Testing Native Apps
84,Java Programming
84,Troubleshooting
84,Using Eclipse
84,Extending App with Custom Java
84,Using the Java API
84,Studio Pro 9 How-tos
84,Front End
84,UI Design
84,Get Started
84,Customize Styling
84,Configure Module-Level Theme Settings
84,Create a Company Design System
84,Extend Design Properties
84,Implement Best Practices for UX Design
84,Use Navigation Layouts
84,Configure Your Theme
84,Create Overview and Detail Pages
84,Use Layouts and Snippets
84,Implement Classes
84,Create Custom Error Pages
84,Data Models
84,Denormalize Data to Improve Performance
84,Share the Development Database
84,Migrate Your Mendix Database
84,Integration
84,Integrate Legacy System
84,Import XML Documents
84,Export XML Documents
84,Import Excel Documents
84,Import a Large Excel File
84,Export to Excel
84,Access a Samba Share
84,Expose Data to BI Tools Using OData
84,Configure Selenium Support
84,Execute SQL on External Database
84,CI/CD Pipeline for Mendix Cloud
84,Use a Client Certificate
84,Extensibility
84,Build a Pluggable Native Widget
84,Build Pluggable Web Widgets
84,1. Build Pluggable Web Widget
84,2. Build Pluggable Web Widget
84,Build JavaScript Actions
84,1. Build JavaScript Actions
84,2. Build JavaScript Actions
84,Build JavaScript Actions for Native Mobile
84,JavaScript Actions Best Practices
84,Build Microflow Actions with Java
84,Data Storage APIs for Reusable Microflows
84,Security
84,Create a Secure App
84,Best Practices for App Security
84,Set Up Anonymous User Security
84,Content Security Policy
84,Testing
84,Test Mendix Apps Using Selenium IDE
84,Create Automated Tests with TestNG
84,Monitoring and Troubleshooting
84,Clear Warning Messages
84,Debug Java Actions
84,Debug Java Actions Remotely
84,Debug a Hybrid Mobile Application
84,Find the Root Cause of Runtime Errors
84,Set Log Levels
84,Monitor Mendix Using JMX
84,Solve Load and Import Errors
84,Manage App Performance
84,Manage App Performance with New Relic
84,Detect and Resolve Performance Issues
84,Populate User Types
84,Studio Pro 8 Guide
84,General Info
84,System Requirements
84,Desktop Modeler 7 to Studio Pro 8
84,Troubleshooting DOM Changes
84,Troubleshooting Atlas UI Changes
84,mx Command-Line Tool
84,MxBuild
84,Developer Tool Recommendations
84,Third-Party Licenses
84,App Modeling
84,Studio Pro Overview
84,Menus
84,File Menu
84,New Project
84,Open Project
84,Export Project Package
84,Import Project Package
84,Edit Menu
84,"Find, Find Advanced, and Find Usages"
84,Go to Option
84,Preferences
84,View Menu
84,Changes Pane
84,Data Hub Pane
84,Errors Pane
84,Consistency Errors
84,Navigation Consistency Errors
84,Page Editor Consistency Errors
84,Suppression Rules
84,Project Explorer
84,Project
84,Project Settings
84,Configurations
84,Navigation
84,System Texts
84,Modules
84,UI Resources Package
84,Security
84,Project Security
84,User Roles
84,Administrator
84,Demo Users
84,Anonymous Users
84,Password Policy
84,Module Security
84,Stories Pane
84,Project Menu
84,Create Deployment Package
84,Deploy to the Cloud
84,Run Menu
84,Edit Cloud Foundry Settings
84,Version Control Menu
84,Commit
84,History
84,Download from Version Control Server
84,Upload to Version Control Server
84,Branch Line Manager
84,Create Branch Line
84,Merge Dialog
84,Language Menu
84,Language Settings
84,Batch Replace
84,Batch Translate
84,Language Operations
84,Domain Model
84,Entities
84,Persistability
84,Attributes
84,Validation Rules
84,Event Handlers
84,Indexes
84,Access Rules
84,External Entities
84,Associations
84,Association Properties
84,Association Tab Properties
84,Querying Over Self-References
84,Annotations
84,Generalization vs 1-to-1 Associations
84,Pages
84,Page
84,Page Properties
84,Page Resources
84,Image Collection
84,Layout
84,Placeholder
84,Header
84,Sidebar Toggle
84,Page Template
84,Snippet
84,Building Block
84,Menu
84,Data Widgets
84,Data View
84,Grids
84,Data Grid
84,Grid Columns
84,Template Grid
84,Control Bar
84,Search Bar
84,Sort Bar
84,List View
84,Data Sources
84,Database Source
84,XPath Source
84,Context Source
84,Microflow Source
84,Nanoflow Source
84,Association Source
84,Listen to Widget Source
84,Common Widgets
84,Text
84,Image
84,Label
84,Snippet Call
84,Page Title
84,Container Widgets
84,Layout Grid
84,Container
84,Group Box
84,Tab Container
84,Scroll Container
84,Table
84,Navigation List
84,Input Widgets
84,Text Box
84,Text Area
84,Drop-Down
84,Check Box
84,Radio Buttons
84,Date Picker
84,Reference Selector
84,Reference Set Selector
84,Input Reference Set Selector
84,File Widgets
84,File Manager
84,Image Uploader
84,Image Viewer
84,Button Widgets
84,Button Properties
84,Menu Widgets
84,Menu Bar
84,Simple Menu Bar
84,Navigation Tree
84,Report Widgets
84,Report Grid
84,Report Parameter
84,Report Date Parameter
84,Date Range Field
84,Generate Report Button
84,Authentication Widgets
84,Login ID Text Box
84,Password Text Box
84,Sign-In Button
84,Validation Message
84,Chart Widgets
84,Chart Configuration
84,Chart Advanced Cheat Sheet
84,Any Chart Widgets
84,Any Chart Building Blocks
84,Any Chart Cheat Sheet
84,Properties Common in the Page Editor
84,On Click Event and Events Section
84,Application Logic
84,Microflows
84,Microflow Properties
84,MxAssist Logic Bot
84,Nanoflows
84,Nanoflow Properties
84,Sequence Flow
84,Activities
84,Object Activities
84,Cast Object
84,Change Object
84,Commit Object(s)
84,Create Object
84,Delete Object(s)
84,Retrieve
84,Rollback Object
84,List Activities
84,Aggregate List
84,Change List
84,Create List
84,List Operation
84,Action Call Activities
84,Java Action Call
84,JavaScript Action Call
84,Microflow Call
84,Variable Activities
84,Change Variable
84,Create Variable
84,Client Activities
84,Call Nanoflow
84,Show Message
84,Close Page
84,Download File
84,Show Home Page
84,Show Page
84,Synchronize to Device
84,Synchronize
84,Validation Feedback
84,Integration Activities
84,Call REST Service
84,Call Web Service
84,Import with Mapping
84,Export With Mapping
84,Log Message
84,Generate Document
84,Decisions
84,Merge
84,Object Type Decision
84,Decision
84,Annotation
84,Parameter
84,Loop
84,Events
84,Start Event
84,End Event
84,Error Event
84,Continue Event
84,Break Event
84,Expressions
84,Unary Expressions
84,Arithmetic Expressions
84,Relational Expressions
84,Special Checks
84,Boolean Expressions
84,If Expressions
84,Mathematical Function Calls
84,String Function Calls
84,Date Creation
84,Between Date Function Calls
84,Add Date Function Calls
84,Trim to Date
84,To String
84,Parse Integer
84,Parse and Format Decimal Function Calls
84,Parse and Format Date Function Calls
84,Enumerations in Expressions
84,Common Properties
84,Resources
84,Java Actions
84,JavaScript Actions
84,Rules
84,Enumerations
84,Datasets
84,OQL
84,OQL Expressions
84,OQL Aggregation
84,OQL Functions
84,OQL CAST
84,OQL COALESCE
84,OQL DATEDIFF
84,OQL DATEPART
84,OQL LENGTH
84,OQL RANGEBEGIN
84,OQL RANGEEND
84,OQL ROUND
84,OQL Operators
84,OQL Case Expression
84,OQL Parameters
84,OQL From Clause
84,OQL Group by Clause
84,OQL Limit Clause
84,OQL Order by Clause
84,OQL Select Clause
84,OQL Where Clause
84,Constants
84,Regular Expressions
84,Scheduled Events
84,Document Templates
84,Creating Your Own Documents
84,Data Grid (Document Template)
84,Columns (Document Template)
84,Data View (Document Template)
84,Document Template
84,Dynamic Image (Document Template)
84,Dynamic Label (Document Template)
84,Footer (Document Template)
84,Header (Document Template)
84,Line Break (Document Template)
84,Page Break (Document Template)
84,Static Image (Document Template)
84,Static Label (Document Template)
84,Style
84,Table (Document Template)
84,Row (Document Template)
84,Cell (Document Template)
84,Template Grid (Document Template)
84,Title (Document Template)
84,Data Types
84,Images
84,XPath
84,XPath Constraints
84,XPath Constraint Functions
84,XPath Contains
84,XPath Day-from-DateTime
84,XPath Day-of-Year-from-DateTime
84,XPath Ends-With
84,XPath False
84,XPath Hours-from-DateTime
84,XPath Length
84,XPath Minutes-from-DateTime
84,XPath Month-From-DateTime
84,XPath Not
84,XPath Quarter-from-DateTime
84,XPath Seconds-from-DateTime
84,XPath Starts-With
84,XPath String-Length
84,XPath True
84,XPath Week-from-DateTime
84,XPath Weekday-from-DateTime
84,XPath Year-from-DateTime
84,XPath Expressions
84,XPath Keywords and System Variables
84,XPath Operators
84,XPath Query Functions
84,XPath Avg
84,XPath Count
84,XPath Max
84,XPath Min
84,XPath Sum
84,XPath Tokens
84,Integration
84,Consumed App Services
84,Select App Service
84,Settings
84,Consumed OData Services
84,Consumed OData Service
84,Consumed OData Service Requirements
84,Consumed REST Services
84,Using a Proxy to Call a REST Service
84,Consumed Web Services
84,Consumed Web Service
84,Numeric Formatting
84,Using a Proxy to Call a Web Service
84,HttpRequest and HttpResponse System Entities
84,JSON Structures
84,Mapping Documents
84,Export Mappings
84,Import Mappings
84,Map Automatically
84,Select Elements
84,XML Inheritance and Choice
84,Message Definitions
84,Published App Services
84,Actions
84,Published App Service
84,Published OData Services
84,OData Query Options
84,OData Representation
84,Published OData Resource
84,Published REST Services
84,Published REST Service
84,Published REST Operation
84,Operation Parameters for Published REST
84,Published REST Path Parameters
84,Published REST Query Parameters
84,Published REST Resource
84,CORS Settings for Published REST Services
84,GitHub-Flavored Markdown
84,Generate a Published REST Resource
84,Publish Microflow as REST Operation
84,Technical Details of Published REST
84,Published REST Request Routing
84,JSON Schema for Published REST Operation
84,OpenAPI 2.0 Documentation
84,Custom Authentication Microflow Parameters
84,Published Web Services
84,Operations
84,Published Web Service
84,XML Schemas
84,XML Schema Support
84,Version Control
84,Using Version Control in Studio Pro
84,Mendix Runtime
84,Runtime Server
84,Mendix Client
84,Runtime Deployment
84,Clustered Mendix Runtime
84,Communication Patterns
84,Data Storage
84,Attribute Type Migration
84,Case-Sensitive Database Behavior
84,Order By Behavior
84,Uniqueness Constraint Migration
84,DB2
84,MySQL/MariaDB
84,Oracle
84,SAP HANA
84,Date and Time Handling
84,DateTime Handling FAQ
84,Logging
84,Login Behavior
84,Mendix Runtime and Java
84,Non-Persistable Objects and Garbage Collecting
84,Java Memory Usage
84,Common Runtime and Java Errors
84,Monitoring Client State
84,Monitoring Mendix Runtime
84,Objects and Caching
84,Runtime Customization
84,Advanced Custom Settings
84,WebSockets
84,Mobile
84,Native Mobile
84,Getting the Make It Native App
84,Native Navigation
84,Native Mobile Styling
84,Native Builder (CLI)
84,Working with Vector Graphics
84,Hybrid Mobile
84,Customizing Hybrid Mobile Apps
84,Developing Hybrid Mobile Apps
84,Getting the Mendix Developer App
84,Offline Hybrid Mobile Apps
84,Packaging Hybrid Mobile Apps
84,Managing App Signing Keys
84,Offline-First
84,Java Programming
84,Troubleshooting
84,Using Eclipse
84,Studio Pro 8 How-tos
84,Collaboration
84,Solve Known Version Control Issues
84,Team Server Network Issues
84,Contribute to a Mendix GitHub Repository
84,Start Your Own GitHub Repository
84,Share the Development Database
84,Translate Your App Content
84,On-Premises Version Control Server
84,Front End
84,Atlas UI
84,Get Started with Atlas UI
84,Migrate Existing Apps to Atlas UI
84,Create Company Atlas UI Resources
84,Share Company Atlas UI Resources
84,Custom Preview Images
84,Customize Your Styling
84,Customize Styling Using Calypso
84,Customize Styling Using Gulp
84,Set Up Gulp and Sass
84,Start Styling with Gulp and Sass
84,Implement Best Practices for UX Design
84,Use Navigation Layouts
84,Configure Your Theme
84,Use the Charts Widgets
84,Create a Basic Chart
84,Use Any Chart
84,Chart Advanced Tuning
84,Use the Charts Theme
84,Create a Dynamic Series Chart
84,Use a Chart with a REST Data Source
84,Plotly Images REST Endpoint
84,Create Overview and Detail Pages
84,Use Layouts and Snippets
84,Implement Classes
84,Create Custom Error Pages
84,Style Google Maps
84,Mobile
84,Native Mobile
84,Get Started with Native Mobile
84,Build Native Apps
84,Deploy Mendix Native Mobile App
84,Build Local Native Mobile App
84,Local Native Mobile App Manual Build
84,Debug Native Mobile Apps (Advanced)
84,Create a Custom Developer App
84,Build Apps Using Native Builder CLI
84,Deploy Mobile App with Native Builder CLI
84,Custom Developer App with Native Builder CLI
84,Over the Air Updates with CodePush and CLI
84,Implement Native Mobile Styling
84,Style Your Mendix Native Mobile App
84,Native Mobile App UI Best Practices
84,Add Fonts to Your Native Mobile App
84,Use Notifications
84,Add Module Dependencies
84,Push Notifications Module
84,Set Up Firebase Cloud Messaging
84,Configure Push Notifications
84,Push Notifications in Native App
84,Native App with Push Notifications
84,Send Your First Test Push Notification
84,Send Notifications to Multiple Devices
84,Use Local Notifications
84,Part 1: Local Notifications
84,Part 2: Badges
84,Part 3: Actions
84,Part 4: Data
84,Part 5: Scheduling
84,Over the Air Updates with CodePush
84,Deep Links in Native Mobile Apps
84,Set Up Maps in Native Mobile Apps
84,Troubleshoot Common Native Mobile Issues
84,Hybrid Mobile
84,Build Hybrid Apps
84,Build a Mendix Hybrid App Locally
84,Publish Hybrid Mobile App in App Stores
84,Customizing Local Build Packages
84,Set Up Hybrid Push Notifications
84,Include Push Notifications
84,Implement Push Notifications
84,Send Push Notifications
84,Apple Push Notification Server
84,Test Push Notifications
84,Configure iOS Mendix Feedback Widget
84,SSO on Hybrid App with SAML
84,Debug a Hybrid Mobile App
84,Deploy Your First Hybrid Mobile App
84,Data Models
84,Create a Basic Data Layer
84,Set Up Data Validation
84,Work with Images and Files
84,Denormalize Data to Improve Performance
84,Migrate Your Mendix Database
84,Logic and Business Rules
84,Trigger a Microflow From a Menu Item
84,Create a Custom Save Button
84,Extract and Use Sub-Microflows
84,Work with Lists in a Microflow
84,Optimize Microflow Aggregates
84,Set Up Error Handling
84,Optimize Retrieve Activities
84,Define Access Rules Using XPath
84,Configure String Concatenation
84,Extend App with Custom Java
84,Use the Java API
84,Use Translatable Validation Messages
84,Filter Data Using XPath
84,Server-Side Paging and Sorting
84,Integration
84,Import and Export Objects
84,Import XML Documents
84,Export XML Documents
84,Import Excel Documents
84,Import a Large Excel File
84,Export to Excel
84,Consume a Simple Web Service
84,Consume a Complex Web Service
84,Consume a REST Service
84,Publish a REST Service
84,Access a Samba Share
84,Version a REST Service
84,Expose a Web Service
84,Expose Data to BI Tools Using OData
84,Publish Data to Other Mendix Apps Using an App Service (Deprecated)
84,Configure Selenium Support
84,Execute SQL on External Database
84,Test Web Services Using SoapUI
84,Implement CI/CD Pipeline
84,Use a Client Certificate
84,Extensibility
84,Build a Pluggable Native Widget
84,Build Pluggable Web Widgets
84,1. Build Pluggable Web Widget
84,2. Build Pluggable Web Widget
84,Build Custom Widgets
84,Build Widgets with XML
84,Preview Image for Custom Widget
84,Build JavaScript Actions
84,1. Build JavaScript Actions
84,2. Build JavaScript Actions
84,Build JavaScript Actions for Native Mobile
84,JavaScript Actions Best Practices
84,Microflow Actions Using Connector Kit
84,Data Storage APIs for Reusable Microflows
84,Security
84,Create a Secure App
84,Best Practices for App Security
84,Set Up Anonymous User Security
84,Testing
84,Test Microflows Using Unit Test Module
84,Test with ATS
84,Test Mendix Apps Using Selenium IDE
84,Create Automated Tests with TestNG
84,Monitoring and Troubleshooting
84,Clear Warning Messages
84,Debug Microflows
84,Debug Microflows Remotely
84,Debug Java Actions
84,Debug Java Actions Remotely
84,Debug a Hybrid Mobile Application
84,Find the Root Cause of Runtime Errors
84,Set Log Levels
84,Monitor Mendix Using JMX
84,Solve Load and Import Errors
84,Manage App Performance
84,Manage App Performance with New Relic
84,Detect and Resolve Performance Issues
84,General Info
84,Configure Parallels
84,Set Up the Navigation Structure
84,Minimize Objects in Session
84,Best Practices for Development
84,Best Practices for App Performance
84,Install Mendix Studio Pro
84,Mendix 7 Reference Guide
84,General
84,System Requirements
84,Moving from Modeler Version 6 to 7
84,Offline
84,MxBuild
84,Developer Tool Recommendations
84,Third-Party Licenses
84,Desktop Modeler
84,Desktop Modeler Overview
84,Application Logic
84,Microflows
84,Microflow Properties
84,Nanoflows
84,Nanoflow Properties
84,Rules
84,Common Elements
84,Activities
84,Action Call Activities
84,Java Action Call
84,Microflow Call
84,Client Activities
84,Close Page
84,Download File
84,Show Home Page
84,Show Message
84,Show Page
84,Validation Feedback
84,Document Generation Activities
84,Generate Document
84,Integration Activities
84,Call Web Service
84,Export XML
84,Import XML
84,List Activities
84,Aggregate List
84,Change List
84,Create List
84,List Operation
84,Logging Activities
84,Log Message
84,Object Activities
84,Cast Object
84,Change Object
84,Commit Object(s)
84,Create Object
84,Delete Object(s)
84,Retrieve
84,Rollback Object
84,Variable Activities
84,Change Variable
84,Create Variable
84,Annotation
84,Annotation Flow
84,Events
84,Break Event
84,Continue Event
84,End Event
84,Error Event
84,Start Event
84,Expressions
84,Add date function calls
84,Arithmetic expressions
84,Between Date Function Calls
84,Boolean expressions
84,Date Creation
84,Enumerations in Expressions
84,If expressions
84,Mathematical function calls
84,Parse and Format Date Function Calls
84,Parse and Format Decimal Function Calls
84,Parse and Format Float Function Calls
84,Parse integer
84,Relational expressions
84,Special checks
84,String Function Calls
84,To float
84,To string
84,Trim to Date
84,Unary expressions
84,Loop
84,Microflow Element Common Properties
84,Parameter
84,Sequence Flow
84,Splits
84,Exclusive Split
84,Inheritance Split
84,Merge
84,Consistency Errors
84,Navigation Consistency Errors
84,Page Editor Consistency Errors
84,Constants
84,Data Types
84,Datasets
84,OQL
84,OQL Expressions
84,OQL Aggregation
84,OQL Functions
84,OQL CAST
84,OQL COALESCE
84,OQL DATEDIFF
84,OQL DATEPART
84,OQL LENGTH
84,OQL RANGEBEGIN
84,OQL RANGEEND
84,OQL ROUND
84,OQL Operators
84,OQL Case Expression
84,OQL Parameters
84,OQL From Clause
84,OQL Group by Clause
84,OQL Limit Clause
84,OQL Order by Clause
84,OQL Select Clause
84,OQL Where Clause
84,Dialog Boxes
84,App Settings Dialog
84,Branch Line Manager Dialog
84,Commit Dialog
84,Create Branch Line Dialog
84,Create Deployment Package Dialog
84,Deploy To The Cloud Dialog
84,Download From Version Control Server Dialog
84,Edit Cloud Foundry Settings Dialog
84,Export an App Package
84,History Dialog
84,Import Project Package
84,Merge Dialog
84,Open App Dialog
84,Preferences Dialog
84,Sign In Dialog
84,Upload To Version Control Server
84,Document Templates
84,Creating Your Own Documents
84,Data Grid (Document Template)
84,Columns (Document Template)
84,Data View (Document Template)
84,Document Template
84,Dynamic Image (Document Template)
84,Dynamic Label (Document Template)
84,Footer (Document Template)
84,Header (Document Template)
84,Line break (Document Template)
84,Page Break (Document Template)
84,Static Image (Document Template)
84,Static Label (Document Template)
84,Style
84,Table (Document Template)
84,Row (Document Template)
84,Cell (Document Template)
84,Template Grid (Document Template)
84,Title (Document Template)
84,Domain Model
84,Annotations
84,Associations and Their Properties
84,Entities
84,Generalization and 1-to-1 Associations
84,Persistability
84,Attributes
84,Associations
84,Validation Rules
84,Event Handlers
84,Indexes
84,Access Rules
84,Enumerations
84,Enumeration Values
84,Images
84,Integration
84,Consumed App Services
84,Select app service
84,Settings
84,Consumed REST Services
84,Using a Proxy to Call a REST Service
84,Consumed Web Services
84,Consumed Web Service
84,Numeric formatting
84,Using a proxy to call a webservice
84,HttpRequest and HttpResponse System Entities
84,JSON Structures
84,Mapping Documents
84,Export Mappings
84,Import Mappings
84,Map Automatically
84,Select Elements
84,XML Inheritance and Choice
84,Message Definitions
84,Message Definition
84,Microflow Activities
84,Call REST Service Action
84,Call Web Service Action
84,Export Mapping Action
84,Import Mapping Action
84,Published App Services
84,Actions
84,Published App Service
84,Published OData Services
84,OData Query Options
84,OData Representation
84,Published OData Resource
84,Published REST Services
84,Published REST Service
84,Published REST Operation
84,Operation Parameters for Published REST
84,Published REST Path Parameters
84,Published REST Query Parameters
84,Published REST Resource
84,CORS Settings for Published REST Services
84,GitHub-Flavored Markdown
84,Generate a Published REST Resource
84,Publish Microflow as REST Operation
84,Technical Details of Published REST
84,Published REST Request Routing
84,JSON Schema for Published REST Operation
84,OpenAPI 2.0 Documentation
84,Custom Authentication Microflow Parameters
84,Published Web Services
84,Operations
84,Published Web Service
84,XML Schemas
84,XML Schema Support
84,Java Actions
84,Modules
84,Module Security
84,Module Role
84,Pages
84,Page Concepts
84,Conditions
84,Data Sources
84,Association Source
84,Context Source
84,Database Source
84,Listen To Widget Source
84,Microflow Source
84,XPath Source
84,On Click Event
84,Opening Pages
84,Starting Microflows
84,Authentication Widgets
84,Login Id Text Box
84,Password Text Box
84,Sign In Button
84,Validation Message
84,Building Block
84,Button Widgets
84,Action Button
84,Close Page Button
84,Create Button
84,Drop-Down Button
84,Image Property
84,Chart Widgets
84,Chart Configuration
84,Chart Advanced Cheat Sheet
84,Any Chart Widgets
84,Any Chart Building Blocks
84,Any Chart Cheat Sheet
84,Common Widgets
84,Common Widget Properties
84,Image
84,Label
84,Page title
84,Snippet Call
84,Text
84,Container Widgets
84,Container
84,Group box
84,Layout grid
84,Navigation list
84,Scroll Container
84,Scroll Container Region
84,Tab container
84,Tab page
84,Table
84,Table cell
84,Table row
84,Data Widgets
84,Data grid
84,Columns
84,Control Bar
84,Add Button
84,Delete button
84,Deselect All Button
84,Edit button
84,Export to CSV button
84,Export to excel button
84,Grid Action button
84,Grid Create Button
84,Remove button
84,Search button
84,Select All Button
84,Select button
84,Search Bar
84,Comparison Search Field
84,Drop-Down Search Field
84,Range Search Field
84,Sort Bar
84,Data view
84,List view
84,Template Grid
84,File Widgets
84,File manager
84,Image uploader
84,Image viewer
84,Input Widgets
84,Check Box
84,Date Picker
84,Drop-Down
84,Input Reference Set Selector
84,Radio Buttons
84,Reference Selector
84,Reference Set Selector
84,Text Area
84,Text Box
84,Layout Widgets
84,Header
84,Placeholder
84,Sidebar toggle button
84,Layouts
84,Menu
84,Menu Item
84,Menu Widgets
84,Menu Bar
84,Navigation Tree
84,Simple Menu Bar
84,Page
84,Page Templates
84,Report Widgets
84,Date Range Field
84,Report Button
84,Report Chart
84,Report Date Parameter
84,Report Grid
84,Report Parameter
84,Snippet
84,Projects
84,Converting to 7.4 - Navigation Profile Issues
84,Navigation Before Mendix 7.2
84,Desktop Profile
84,Hybrid Phone Profile
84,Hybrid Tablet Profile
84,Offline Device Profile
84,Phone Profile
84,Tablet Profile
84,Navigation in 7.2 and 7.3
84,Navigation Profile in 7.2 and 7.3
84,Navigation in Mendix 7.4 and Above
84,Navigation Profile
84,Project Security
84,Administrator
84,Anonymous Users
84,Demo Users
84,Module Status
84,Password Policy
84,User Roles
84,Project Settings
84,Configuration
84,System Texts
84,Regular Expressions
84,Scheduled Events
84,Security
84,Translatable Texts
84,UI Resources Package
84,XPath
84,XPath Constraints
84,XPath Constraint Functions
84,XPath contains
84,XPath day-from-dateTime
84,XPath day-of-year-from-dateTime
84,XPath ends-with
84,XPath false
84,XPath hours-from-dateTime
84,XPath length
84,XPath minutes-from-dateTime
84,XPath month-from-dateTime
84,XPath not
84,XPath quarter-from-dateTime
84,XPath seconds-from-dateTime
84,XPath starts-with
84,XPath string-length
84,XPath true
84,XPath week-from-dateTime
84,XPath weekday-from-dateTime
84,XPath year-from-dateTime
84,XPath Expressions
84,XPath Keywords and System Variables
84,XPath Operators
84,XPath Query Functions
84,XPath avg
84,XPath count
84,XPath id
84,XPath max
84,XPath min
84,XPath sum
84,XPath Tokens
84,Version Control
84,Version Control
84,Team Server
84,Team Server FAQ
84,Collaborative Development
84,Mendix Runtime
84,Clustered Mendix Runtime
84,Data Storage
84,Attributes Type Migration
84,Order By Behavior
84,Uniqueness Constraint Migration
84,DB2
84,MySQL/MariaDB
84,Oracle
84,SAP HANA
84,Date and Time Handling
84,DateTime Handling FAQ
84,Logging
84,Login Behavior
84,Mendix Runtime and Java
84,Transient Objects and Garbage Collecting
84,Java Memory Usage
84,Common Runtime and Java Errors
84,Monitoring client state
84,Monitoring Mendix Runtime
84,Objects and Caching
84,Runtime Customization
84,Advanced Custom Settings
84,SIG–Mendix Performance Subjects
84,Java Programming
84,Troubleshooting
84,Using Eclipse
84,Mobile Development
84,Customizing Hybrid Mobile Apps
84,Customizing Local Build Packages
84,Developing Hybrid Mobile Apps
84,Getting the Mendix Developer App
84,Managing App Signing Keys
84,Offline Hybrid Mobile Apps
84,Packaging Hybrid Mobile Apps
84,Mendix 7 How-tos
84,General
84,Install the Mendix Desktop Modeler
84,Show the Project Directory in Explorer
84,Best Practices for Development
84,Best Practices for App Performance
84,Find Your Way in an App
84,Find Unused App Items
84,Minimize Objects in Session
84,Set Up the Navigation Structure
84,Front End
84,Atlas UI
84,Get Started with Atlas UI
84,Migrate Existing Apps to Atlas UI
84,Create Company Atlas UI Resources
84,Share Company Atlas UI Resources
84,Custom Preview Images
84,Implement Best Practices for UX Design
84,Configure Your Theme
84,Create Overview and Detail Pages
84,Use Layouts and Snippets
84,Implement Classes
84,Use Gulp and Sass
84,Set Up Gulp and Sass
84,Start Styling with Gulp
84,Sass
84,Create Custom Error Pages
84,Style Google Maps
84,Data Models
84,Create a Basic Data Layer
84,Set Up Data Validation
84,Work with Object Events
84,Work with Images and Files
84,Query Over Self-References
84,Denormalize Data to Improve Performance
84,Migrate Your Mendix Database
84,Logic and Business Rules
84,Your First Microflow
84,Trigger Logic Using Microflows
84,Create a Custom Save Button
84,Drag App Documents into Microflow
84,Extract and Use Sub-Microflows
84,Work with Lists in a Microflow
84,Optimize Microflow Aggregates
84,Set Up Error Handling
84,Optimize Retrieve Activities
84,Define Access Rules Using XPath
84,Configure String Concatenation
84,Use the Java API
84,Find Object Activities
84,Use Translatable Validation Messages
84,Filter Data Using XPath
84,Mobile Development
84,Include Push Notifications
84,Implement Push Notifications
84,Send Push Notifications
84,Apple Push Notification Server
84,Set Up Firebase Cloud Messaging
84,Test the Implementation
84,Configure iOS Mendix Feedback Widget
84,SSO on Hybrid App with SAML
84,Debug a Hybrid Mobile App
84,Deploy Your First Hybrid Mobile App
84,Publish Hybrid App in App Stores
84,Security
84,Create a Secure App
84,Best Practices for App Security
84,Set Up Anonymous User Security
84,Integration
84,Integrate Legacy System
84,Import and Export Objects
84,Import XML Documents
84,Export XML Documents
84,Import Excel Documents
84,Import a Large Excel File
84,Consume a Simple Web Service
84,Consume a Complex Web Service
84,Consume a REST Service
84,Publish a REST Service
84,Version a REST Service
84,Expose a Web Service
84,Publish Data Using App Service
84,Configure Selenium Support
84,Execute SQL on External Database
84,Test Web Services Using SoapUI
84,Implement CI/CD Pipeline
84,Use a Client Certificate
84,Extensibility
84,Microflow Actions Using Connector Kit
84,Data Storage APIs for Reusable Microflows
84,Access a Samba Share
84,Use the Charts Widgets
84,Create a Basic Chart
84,Use Any Chart
84,Chart Advanced Tuning
84,Use the Charts Theme
84,Create a Dynamic Series Chart
84,Use a Chart with a REST Data Source
84,Plotly Images REST Endpoint
84,Widget Development
84,Adobe Brackets Widget Development Plugin
84,Preview Image for Custom Widget
84,Scaffold Widget with Yeoman
84,Use XML in Widget Development
84,Testing
84,Test Microflows Using Unit Test Module
84,Test with ATS
84,Test Apps Using Selenium IDE
84,Create Automated Tests with TestNG
84,Monitoring and Troubleshooting
84,Clear Warning Messages
84,Debug Microflows
84,Debug Microflows Remotely
84,Handle Common Mendix SSO Errors
84,Debug Java Actions
84,Debug Java Actions Remotely
84,Find the Root Cause of Runtime Errors
84,Set Log Levels
84,Monitor Mendix Using JMX
84,Solve Load and Import Errors
84,New Relic App Performance
84,Detect and Resolve Performance Issues
84,Collaboration and Requirements Management
84,Solving Known Version Control Issues
84,Team Server Network Issues
84,Contribute to a GitHub Repository
84,Start Your Own Repository
84,Share the Development Database
84,Translate Your App Content
84,On-Premises Version Control Server
84,OData Query Options
84,1 Introduction
84,2 Retrieving Objects
84,2.1 Retrieving All Objects
84,2.2 Retrieving a Single Object
84,3 Counting the Number of Objects
84,3.1 Retrieving a Count of Objects
84,3.2 Inline Count
84,4 Filtering
84,4.1 Passing attributes
84,4.2 Comparison Operators
84,4.3 Functions
84,4.4 Combining Filters
84,4.5 Arithmetic Operators
84,5 Sorting
84,6 Selecting fields
84,7 Paging
84,7.1 Top (Limit)
84,7.2 Skip (Offset)
84,8 Null Literals
84,Docs
84,Mendix 7 Reference Guide
84,Desktop Modeler
84,Integration
84,Published OData Services
84,OData Query Options
84,"Mendix 7 is no longer supported unless you have Extended Support (for details, please contact Mendix Support). Mendix 7 documentation will remain available for customers with Extended Support until July, 2024."
84,OData Query Options
84,"Last modified: June 14, 2023"
84,1 Introduction
84,This is a list of query options for OData.
84,We currently only support the options described here.
84,2 Retrieving Objects
84,2.1 Retrieving All Objects
84,All objects can be retrieved by specifying the URI. For example: /odata/myservice/myresource. You can see this if you specify the URI in a browser.
84,2.2 Retrieving a Single Object
84,A single object can be retrieved by passing the object identifier in the URI. For example: /odata/myservice/myresource(8444249301330581).
84,3 Counting the Number of Objects
84,3.1 Retrieving a Count of Objects
84,"You can find out how many objects there are by passing the $count query option. In this case, the result is an integer which is the number of objects. For example: /odata/myservice/myresource/$count."
84,3.2 Inline Count
84,"By setting the $inlinecount query option to ‘allpages’, a count of the number of items returned will be included in the result. For example: ?$inlinecount=allpages."
84,4 Filtering
84,Filters are applied by appending a $filter=... parameter to the request. For example: /Employees?$filter=Name eq 'John'.
84,4.1 Passing attributes
84,This table describes how to pass values for different attribute types:
84,Type
84,How to Pass
84,String and Enumeration
84,"Enclosed in single quotes (for example, ‘John’)"
84,Datetime
84,"Preceded with datetime and enclosed in single quotes (for example, datetime'2015-01-01’ or datetime’<epoch value here>’)"
84,Other
84,"Plain value (for example, 15)"
84,4.2 Comparison Operators
84,We support the following comparison operators:
84,Operator
84,Meaning
84,Example
84,equals
84,/Employees?$filter=Name eq 'John'
84,does not equal
84,/Employees?$filter=Name ne 'John'
84,greater than
84,/Employees?$filter=Age gt 15
84,less than
84,/Employees?$filter=Age lt 15
84,greater than or equal to
84,/Employees?$filter=Age ge 15
84,less than or equal to
84,/Employees?$filter=Age le 15
84,4.3 Functions
84,Function
84,Example
84,Returns
84,substringof
84,"/Employees?$filter=substringof('f', Name)"
84,All employees with names that contain an ‘f’
84,endswith
84,"/Employees?$filter=endswith(Name, 'f')"
84,All employees with names that end with ‘f’
84,startswith
84,"/Employees?$filter=startswith(Name, 'f')"
84,All employees with names that start with ‘f’
84,length
84,/Employees?$filter=length(Name) eq 5
84,All employees with names that have a length of 5
84,year
84,/Employees?$filter=year(DateOfBirth) eq 1990
84,All employees born in the year 1990
84,month
84,/Employees?$filter=month(DateOfBirth) eq 5
84,All employees born in May
84,day
84,/Employees?$filter=day(DateOfBirth) eq 31
84,All employees born on the 31st day of the month
84,hour
84,/Employees?$filter=hour(Registration) eq 13
84,All employees registered between 13:00 (1 PM) and 13:59 (1:59 PM)
84,minute
84,/Employees?$filter=minute(Registration) eq 55
84,All employees registered on the 55th minute of any hour
84,second
84,/Employees?$filter=second(Registration) eq 55
84,All employees registered on the 55th second of any minute of any hour
84,4.4 Combining Filters
84,"Filters can be combined with and, or, not, and (). For example: ?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)."
84,Combination
84,Example
84,and
84,/Employees?$filter=Name eq 'John' and Age gt 65
84,/Employees?$filter=Age gt 65 or Age lt 11
84,not
84,/Employees?$filter=not(Name eq 'John')
84,( )
84,/Employees?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)
84,4.5 Arithmetic Operators
84,"The use of arithmetic operators such as add, sub, mul, div, and mod in filter expressions is not supported."
84,5 Sorting
84,You can sort the result using the $orderby query option. For example: ?$orderby=Name.
84,"The default direction is ascending, and you can make this explicit. For example: ?$orderby=Name asc."
84,You can also order the result in a descending direction. For example: ?$orderby=Name desc.
84,"It is possible to sort on multiple attributes, which have to be comma-separated. For example: ?$orderby=Name, Age desc."
84,6 Selecting fields
84,"You can select which attributes and associations to return by specifying the $select query option. For example: ?$select=Name,Age."
84,7 Paging
84,7.1 Top (Limit)
84,"You can limit the number of returned objects using the $top query option, where the limit is a positive integer. For example: ?$top=100."
84,7.2 Skip (Offset)
84,"You can skip a number of objects before retrieving the result using the $skip query option, where the offset is a positive integer. For example: ?$skip=100 will return objects starting with the 101st object in the list."
84,8 Null Literals
84,You can compare values against the null literal. For example: ?$filter=Name eq null.
84,"In this example, Name is a string attribute that can have no assigned value in the database. Note that null means no value as opposed to '' (which is an empty string)."
84,"When you filter against associations, null literals can be quite useful. For example: ?$filter=Association_A_B ne null. In this example, you query for objects of entity type A that have at least one association set to objects of entity type B."
84,Documentation licensed under CC BY 4.0
84,© Mendix Technology BV 2024. All rights reserved
84,Mendix.com
84,Terms of Use
84,Privacy Policy
84,EU Digital Services Act Notice
85,Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers) · GitHub
85,Skip to content
85,All gists
85,Back to GitHub
85,Sign in
85,Sign up
85,Sign in
85,Sign up
85,You signed in with another tab or window. Reload to refresh your session.
85,You signed out in another tab or window. Reload to refresh your session.
85,You switched accounts on another tab or window. Reload to refresh your session.
85,Dismiss alert
85,"Instantly share code, notes, and snippets."
85,fevangelou/my.cnf
85,Last active
85,"February 14, 2024 09:14"
85,Star
85,104
85,You must be signed in to star a gist
85,Fork
85,You must be signed in to fork a gist
85,Star
85,You must be signed in to star a gist
85,Code
85,Revisions
85,Stars
85,104
85,Forks
85,Embed
85,Embed
85,Embed this gist in your website.
85,Share
85,Copy sharable link for this gist.
85,Clone via HTTPS
85,Clone using the web URL.
85,Learn more about clone URLs
85,Clone this repository at &lt;script src=&quot;https://gist.github.com/fevangelou/0da9941e67a9c9bb2596.js&quot;&gt;&lt;/script&gt;
85,Save fevangelou/0da9941e67a9c9bb2596 to your computer and use it in GitHub Desktop.
85,Download ZIP
85,Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers)
85,Raw
85,my.cnf
85,# === Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers) ===
85,"# by Fotis Evangelou, developer of Engintron (engintron.com)"
85,# ~ Updated December 2021 ~
85,# The settings provided below are a starting point for a 8-16 GB RAM server with 4-8 CPU cores.
85,"# If you have different resources available you should adjust accordingly to save CPU, RAM & disk I/O usage."
85,"# The settings marked with a specific comment or the word ""UPD"" (after the value)"
85,# should be adjusted for your system by using database diagnostics tools like:
85,# https://github.com/major/MySQLTuner-perl
85,# or
85,# https://github.com/BMDan/tuning-primer.sh
85,"# Run either of these scripts before optimizing your database, at least 1 hr after the optimization & finally"
85,# at least once a day for 3 days (without restarting the database) to see how your server performs and if you need
85,"# to re-adjust anything. The more MySQL/MariaDB runs without restarting, the more usage data it gathers, so these"
85,# diagnostics scripts will report in mode detail how MySQL/MariaDB performs.
85,"# IMPORTANT NOTE: If there is NO comment after a setting value, then 99,9% of the times you won't need to adjust it."
85,# --- THINGS TO DO AFTER YOU UPDATE MY.CNF - TROUBLESHOOTING ---
85,"# If any terminal commands are mentioned, make sure you execute them as ""root"" user."
85,"# If MySQL or MariaDB cannot start (or restart), then perform the following actions."
85,# 1. If the server had the stock database configuration and you added or updated any
85,"""innodb_log_*"" settings (as suggested below), then execute these commands ONLY"
85,the first time you apply this configuration:
85,$ rm -rvf /var/lib/mysql/ib_logfile*
85,$ touch /var/lib/mysql/mysql.sock
85,$ touch /var/lib/mysql/mysql.pid
85,$ chown -R mysql:mysql /var/lib/mysql
85,$ /scripts/restartsrv_mysql
85,or use the shorthand command:
85,$ rm -rvf /var/lib/mysql/ib_logfile*; touch /var/lib/mysql/mysql.sock; touch /var/lib/mysql/mysql.pid; chown -R mysql:mysql /var/lib/mysql; /scripts/restartsrv_mysql
85,"IMPORTANT: If you edit this file from the Engintron WHM app in cPanel/WHM,"
85,then you DO NOT need to execute the above terminal commands. When you save
85,"the file through the Engintron WHM app, these terminal commands will be"
85,executed automatically after the file is saved on disk.
85,"# 2. If the setting ""bind-address"" is not commented out, then make sure the file /etc/hosts is"
85,"properly configured. A good example of a ""clean"" /etc/hosts file is something like this:"
85,127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
85,::1
85,localhost localhost.localdomain localhost6 localhost6.localdomain6
85,1.2.3.4
85,hostname.domain.tld hostname # <-- Replace accordingly!
85,Finally restart the database using the related cPanel script:
85,$ /scripts/restartsrv_mysql
85,"# 3. If the database service cannot restart even after the first 2 steps, make sure the database data folder"
85,"(common for either MySQL or MariaDB) ""/var/lib/mysql"" is owned by the ""mysql"" user AND group."
85,"Additionally, the folder itself can have 0751 or 0755 file permissions. To fix it, simply do this:"
85,$ chown -R mysql:mysql /var/lib/mysql
85,$ chmod 0755 /var/lib/mysql
85,Finally restart the database using the related cPanel script:
85,$ /scripts/restartsrv_mysql
85,"# 4. Adjust SQL settings under ""Tweak Settings"" in WHM:"
85,"After applying the optimized my.cnf file, you'll also want to DISABLE the following 3 settings"
85,"in the ""SQL"" tab of Tweak Settings in WHM:"
85,- Allow cPanel & WHM to determine the best value for your MySQL open_files_limit configuration?
85,- Allow cPanel & WHM to determine the best value for your MySQL max_allowed_packet configuration?
85,- Allow cPanel & WHM to determine the best value for your MySQL innodb_buffer_pool_size configuration?
85,# ~ FIN ~
85,[mysql]
85,port
85,= 3306
85,socket
85,= /var/lib/mysql/mysql.sock
85,[mysqld]
85,# === Required Settings ===
85,basedir
85,= /usr
85,bind_address
85,= 127.0.0.1 # Change to 0.0.0.0 to allow remote connections
85,datadir
85,= /var/lib/mysql
85,#default_authentication_plugin
85,= mysql_native_password # Enable in MySQL 8+ or MariaDB 10.6+ for backwards compatibility with common CMSs
85,max_allowed_packet
85,= 256M
85,max_connect_errors
85,= 1000000
85,pid_file
85,= /var/lib/mysql/mysql.pid
85,port
85,= 3306
85,skip_external_locking
85,socket
85,= /var/lib/mysql/mysql.sock
85,tmpdir
85,= /tmp
85,user
85,= mysql
85,# === SQL Compatibility Mode ===
85,# Enable for b/c with databases created in older MySQL/MariaDB versions
85,# (e.g. when using null dates)
85,#sql_mode
85,"= ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES"
85,# Crappy SQL queries/schema? Go bold!
85,#sql_mode
85,"= """""
85,# === InnoDB Settings ===
85,default_storage_engine
85,= InnoDB
85,innodb_buffer_pool_instances
85,= 4
85,# Use 1 instance per 1GB of InnoDB pool size - max is 64
85,innodb_buffer_pool_size
85,= 4G
85,# Use up to 70-80% of RAM
85,innodb_file_per_table
85,= 1
85,innodb_flush_log_at_trx_commit
85,= 0
85,innodb_flush_method
85,= O_DIRECT
85,innodb_log_buffer_size
85,= 16M
85,innodb_log_file_size
85,= 1G
85,innodb_sort_buffer_size
85,= 4M
85,# UPD - Defines how much data is read into memory for sorting operations before writing to disk (default is 1M / max is 64M)
85,innodb_stats_on_metadata
85,= 0
85,#innodb_use_fdatasync
85,= 1
85,# Only (!) for MySQL v8.0.26+
85,#innodb_temp_data_file_path
85,= ibtmp1:64M:autoextend:max:20G # Control the maximum size for the ibtmp1 file
85,#innodb_thread_concurrency
85,= 4
85,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
85,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
85,# the overall load produced by MySQL/MariaDB.
85,innodb_read_io_threads
85,= 64
85,innodb_write_io_threads
85,= 64
85,#innodb_io_capacity
85,= 2000
85,"# Depends on the storage tech - use 2000 for SSD, more for NVMe"
85,#innodb_io_capacity_max
85,= 4000
85,# Usually double the value of innodb_io_capacity
85,# === MyISAM Settings ===
85,# The following 3 options are ONLY supported by MariaDB & up to MySQL 5.7
85,# Do NOT un-comment on MySQL 8.x+
85,#query_cache_limit
85,= 4M
85,# UPD
85,#query_cache_size
85,= 64M
85,# UPD
85,#query_cache_type
85,= 1
85,# Enabled by default
85,key_buffer_size
85,= 24M
85,# UPD
85,low_priority_updates
85,= 1
85,concurrent_insert
85,= 2
85,# === Connection Settings ===
85,max_connections
85,= 100
85,# UPD - Important: high no. of connections = high RAM consumption
85,back_log
85,= 512
85,thread_cache_size
85,= 100
85,thread_stack
85,= 192K
85,interactive_timeout
85,= 180
85,wait_timeout
85,= 180
85,# For MySQL 5.7+ only (disabled by default)
85,#max_execution_time
85,= 90000 # Set a timeout limit for SELECT statements (value in milliseconds).
85,"# This option may be useful to address aggressive crawling on large sites,"
85,# but it can also cause issues (e.g. with backups). So use with extreme caution and test!
85,# More info at: https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time
85,# For MariaDB 10.1.1+ only (disabled by default)
85,#max_statement_time
85,= 90
85,"# The equivalent of ""max_execution_time"" in MySQL 5.7+ (set above)"
85,"# The variable is of type double, thus you can use subsecond timeout."
85,# For example you can use value 0.01 for 10 milliseconds timeout.
85,# More info at: https://mariadb.com/kb/en/aborting-statements/
85,# === Buffer Settings ===
85,# Handy tip for managing your database's RAM usage:
85,"# The following values should be treated carefully as they are added together and then multiplied by your ""max_connections"" value."
85,"# Other options will also add up to RAM consumption (e.g. tmp_table_size). So don't go switching your ""join_buffer_size"" to 1G, it's harmful & inefficient."
85,"# Use one of the database diagnostics tools mentioned at the top of this file to count your database's potential total RAM usage, so you know if you are within"
85,"# reasonable limits. Remember that other services will require enough RAM to operate properly (like Apache or PHP-FPM), so set your limits wisely."
85,join_buffer_size
85,= 4M
85,# UPD
85,read_buffer_size
85,= 3M
85,# UPD
85,read_rnd_buffer_size
85,= 4M
85,# UPD
85,sort_buffer_size
85,= 4M
85,# UPD
85,# === Table Settings ===
85,"# In systemd managed systems like Ubuntu 16.04+ or CentOS 7+, you need to perform an extra action for table_open_cache & open_files_limit"
85,# to be overriden (also see comment next to open_files_limit).
85,"# E.g. for MySQL 5.7, please check: https://dev.mysql.com/doc/refman/5.7/en/using-systemd.html"
85,# and for MariaDB check: https://mariadb.com/kb/en/library/systemd/
85,table_definition_cache
85,= 40000 # UPD
85,table_open_cache
85,= 40000 # UPD
85,open_files_limit
85,= 60000 # UPD - This can be 2x to 3x the table_open_cache value or match the system's
85,# open files limit usually set in /etc/sysctl.conf and /etc/security/limits.conf
85,# In systemd managed systems this limit must also be set in:
85,# - /etc/systemd/system/mysql.service.d/override.conf (for MySQL 5.7+ in Ubuntu) or
85,# - /etc/systemd/system/mysqld.service.d/override.conf (for MySQL 5.7+ in CentOS) or
85,# - /etc/systemd/system/mariadb.service.d/override.conf (for MariaDB)
85,# otherwise changing open_files_limit will have no effect.
85,# To edit the right file execute:
85,# $ systemctl edit mysql (or mysqld or mariadb)
85,"# and set ""LimitNOFILE="" to something like 100000 or more (depending on your system limits for MySQL)"
85,"# or use ""LimitNOFILE=infinity"" for MariaDB only."
85,# Finally merge the changes with:
85,# $ systemctl daemon-reload; systemctl restart mysql (or mysqld or mariadb)
85,max_heap_table_size
85,= 128M
85,# Increase to 256M or 512M if you have lots of temporary tables because of missing indices in JOINs
85,tmp_table_size
85,= 128M
85,# Use same value as max_heap_table_size
85,# === Search Settings ===
85,ft_min_word_len
85,= 3
85,# Minimum length of words to be indexed for search results
85,# === Binary Logging ===
85,disable_log_bin
85,= 1
85,# Binary logging disabled by default
85,#log_bin
85,"# To enable binary logging, uncomment this line & only one of the following 2 lines"
85,# that corresponds to your actual MySQL/MariaDB version.
85,"# Remember to comment out the line with ""disable_log_bin""."
85,#expire_logs_days
85,= 1
85,# Keep logs for 1 day - For MySQL 5.x & MariaDB before 10.6 only
85,#binlog_expire_logs_seconds
85,= 86400 # Keep logs for 1 day (in seconds) - For MySQL 8+ & MariaDB 10.6+ only
85,# === Error & Slow Query Logging ===
85,log_error
85,= /var/lib/mysql/mysql_error.log
85,log_queries_not_using_indexes
85,= 0
85,# Disabled on production
85,long_query_time
85,= 5
85,slow_query_log
85,= 0
85,# Disabled on production
85,slow_query_log_file
85,= /var/lib/mysql/mysql_slow.log
85,[mysqldump]
85,# Variable reference
85,# For MySQL 5.7+:
85,https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html
85,# For MariaDB:
85,https://mariadb.com/kb/en/library/mysqldump/
85,quick
85,quote_names
85,max_allowed_packet
85,= 1024M
85,Load earlier comments...
85,Copy link
85,Author
85,fevangelou
85,commented
85,"May 19, 2020"
85,"Run systemctl status mysqld.service to see why MySQL won't start. If there is no practical hint there, see MySQL's error log."
85,"In any case, make sure the contents of this my.cnf are properly copied into your server's /etc/my.cnf file."
85,"Sorry, something went wrong."
85,Copy link
85,EvangelosBalafoutis
85,commented
85,"May 19, 2020"
85,edited by fevangelou
85,Hello Fotis.
85,The file is properly copied. I tried on another server with no problem to restart.
85,Because I had mysql down I return to the old my.cnf.
85,But mmy error log says
85,2020-05-18T22:39:27.121991Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:27.122710Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:28.435750Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:28.435948Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:28.598329Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:28.598350Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:28.598384Z 0 [Note] Governor connected
85,2020-05-18T22:39:28.598388Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:28.598695Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:28.598700Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:28.598705Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:28.599674Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1108942 ...
85,2020-05-18T22:39:28.603292Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:28.603317Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:28.603321Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:28.603324Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:28.603327Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:28.603329Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:28.603503Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:28.603583Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:28.629156Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:28.629174Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:29.129267Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:29.629363Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:30.129474Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:30.629602Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:31.129723Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:31.129751Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:31.130257Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:31.130274Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:31.130282Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:31.130287Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:31.130293Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:31.130297Z 0 [ERROR] Aborting
85,2020-05-18T22:39:31.130318Z 0 [Note] Binlog end
85,2020-05-18T22:39:31.130399Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:31.130410Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:31.131464Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:32.431097Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:32.431252Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:32.588494Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:32.588515Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:32.588550Z 0 [Note] Governor connected
85,2020-05-18T22:39:32.588554Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:32.588858Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:32.588862Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:32.588868Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:32.589848Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1108988 ...
85,2020-05-18T22:39:32.593669Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:32.593689Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:32.593693Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:32.593698Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:32.593701Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:32.593704Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:32.593894Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:32.593984Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:32.620481Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:32.620500Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:33.120626Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:33.620740Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:34.120842Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:34.620919Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:35.121029Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:35.121039Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:35.121400Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:35.121408Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:35.121413Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:35.121417Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:35.121421Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:35.121423Z 0 [ERROR] Aborting
85,2020-05-18T22:39:35.121437Z 0 [Note] Binlog end
85,2020-05-18T22:39:35.121492Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:35.121498Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:35.122266Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:36.439434Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:36.439590Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:36.598534Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:36.598555Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:36.598592Z 0 [Note] Governor connected
85,2020-05-18T22:39:36.598596Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:36.598953Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:36.598957Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:36.598963Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:36.599938Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109023 ...
85,2020-05-18T22:39:36.603630Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:36.603653Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:36.603657Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:36.603662Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:36.603665Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:36.603668Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:36.603841Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:36.603944Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:36.628651Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:36.628672Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:37.128780Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:37.628892Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:38.129025Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:38.629111Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:39.129214Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:39.129227Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:39.129601Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:39.129609Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:39.129615Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:39.129619Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:39.129623Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:39.129625Z 0 [ERROR] Aborting
85,2020-05-18T22:39:39.129644Z 0 [Note] Binlog end
85,2020-05-18T22:39:39.129689Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:39.129695Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:39.130426Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:40.428814Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:40.429004Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:40.588371Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:40.588393Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:40.588427Z 0 [Note] Governor connected
85,2020-05-18T22:39:40.588431Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:40.588735Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:40.588739Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:40.588745Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:40.589742Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109081 ...
85,2020-05-18T22:39:40.593523Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:40.593546Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:40.593550Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:40.593553Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:40.593557Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:40.593560Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:40.593727Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:40.593805Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:40.620202Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:40.620220Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:41.120328Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:41.620456Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:42.120598Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:42.620718Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:43.120849Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:43.120866Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:43.121237Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:43.121248Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:43.121254Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:43.121257Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:43.121261Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:43.121264Z 0 [ERROR] Aborting
85,2020-05-18T22:39:43.121278Z 0 [Note] Binlog end
85,2020-05-18T22:39:43.121321Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:43.121338Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:43.122104Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:44.430237Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:44.430391Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:44.588306Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:44.588328Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:44.588363Z 0 [Note] Governor connected
85,2020-05-18T22:39:44.588367Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:44.588673Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:44.588677Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:44.588683Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:44.589669Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109129 ...
85,2020-05-18T22:39:44.593358Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:44.593382Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:44.593385Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:44.593388Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:44.593391Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:44.593394Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:44.593562Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:44.593641Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:44.620025Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:44.620046Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:45.120147Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:45.620277Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:46.120412Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:46.620542Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:47.120657Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:47.120668Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:47.121029Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:47.121038Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:47.121044Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:47.121047Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:47.121051Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:47.121054Z 0 [ERROR] Aborting
85,2020-05-18T22:39:47.121067Z 0 [Note] Binlog end
85,2020-05-18T22:39:47.121111Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:47.121118Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:47.121808Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,2020-05-18T22:39:48.431640Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
85,2020-05-18T22:39:48.431795Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
85,2020-05-18T22:39:48.588549Z 0 [Note] libgovernor.so found
85,2020-05-18T22:39:48.588570Z 0 [Note] All governors functions found too
85,2020-05-18T22:39:48.588605Z 0 [Note] Governor connected
85,2020-05-18T22:39:48.588609Z 0 [Note] All governors lve functions found too
85,"2020-05-18T22:39:48.588923Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
85,2020-05-18T22:39:48.588928Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
85,2020-05-18T22:39:48.588933Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
85,2020-05-18T22:39:48.589908Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109169 ...
85,2020-05-18T22:39:48.593632Z 0 [Note] InnoDB: PUNCH HOLE support available
85,2020-05-18T22:39:48.593658Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
85,2020-05-18T22:39:48.593661Z 0 [Note] InnoDB: Uses event mutexes
85,2020-05-18T22:39:48.593664Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
85,2020-05-18T22:39:48.593667Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
85,2020-05-18T22:39:48.593670Z 0 [Note] InnoDB: Using Linux native AIO
85,2020-05-18T22:39:48.593843Z 0 [Note] InnoDB: Number of pools: 1
85,2020-05-18T22:39:48.593948Z 0 [Note] InnoDB: Using CPU crc32 instructions
85,2020-05-18T22:39:48.619840Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
85,2020-05-18T22:39:48.619863Z 0 [Warning] InnoDB: io_setup() attempt 1.
85,2020-05-18T22:39:49.119916Z 0 [Warning] InnoDB: io_setup() attempt 2.
85,2020-05-18T22:39:49.620033Z 0 [Warning] InnoDB: io_setup() attempt 3.
85,2020-05-18T22:39:50.120145Z 0 [Warning] InnoDB: io_setup() attempt 4.
85,2020-05-18T22:39:50.620255Z 0 [Warning] InnoDB: io_setup() attempt 5.
85,2020-05-18T22:39:51.120368Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
85,2020-05-18T22:39:51.120379Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
85,2020-05-18T22:39:51.120748Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
85,2020-05-18T22:39:51.120757Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
85,2020-05-18T22:39:51.120763Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
85,2020-05-18T22:39:51.120767Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
85,2020-05-18T22:39:51.120771Z 0 [ERROR] Failed to initialize builtin plugins.
85,2020-05-18T22:39:51.120773Z 0 [ERROR] Aborting
85,2020-05-18T22:39:51.120788Z 0 [Note] Binlog end
85,2020-05-18T22:39:51.120831Z 0 [Note] Shutting down plugin 'CSV'
85,2020-05-18T22:39:51.120847Z 0 [Note] Shutting down plugin 'MyISAM'
85,2020-05-18T22:39:51.121623Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
85,"Sorry, something went wrong."
85,Copy link
85,EvangelosBalafoutis
85,commented
85,"May 21, 2020"
85,edited by fevangelou
85,Hello Fotis. I removed every line had to do with innodb and it did restarted if this make sence.
85,I removed
85,# InnoDB Settings
85,default_storage_engine
85,= InnoDB
85,innodb_buffer_pool_instances
85,= 2
85,# Use 1 instance per 1GB of InnoDB pool size
85,innodb_buffer_pool_size
85,= 2G
85,# Use up to 70-80% of RAM
85,innodb_file_per_table
85,= 1
85,innodb_flush_log_at_trx_commit
85,= 0
85,innodb_flush_method
85,= O_DIRECT
85,innodb_log_buffer_size
85,= 16M
85,innodb_log_file_size
85,= 512M
85,innodb_stats_on_metadata
85,= 0
85,#innodb_temp_data_file_path
85,= ibtmp1:64M:autoextend:max:20G # Control the maximum size for the ibtmp1 file
85,#innodb_thread_concurrency
85,= 7
85,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
85,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
85,# the overall load produced by MySQL/MariaDB.
85,innodb_read_io_threads
85,= 64
85,innodb_write_io_threads
85,= 64
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"May 22, 2020"
85,"@EvangelosBalafoutis This is not a solution though... Seeing the logs that you sent, it's most likely you have limits enforced by MySQL Governor (by CloudLinux). For the record, unless your server hosts 300+ accounts, you probably don't need CloudLinux. And hey, if MySQL Governor worked, you wouldn't be looking to optimize MySQL, right? Food for thought..."
85,"Sorry, something went wrong."
85,Copy link
85,EvangelosBalafoutis
85,commented
85,"May 22, 2020"
85,"Thank you for the info Fotis, Ill think about asking the client to remove it."
85,"Sorry, something went wrong."
85,Copy link
85,EvangelosBalafoutis
85,commented
85,"May 22, 2020"
85,Or I'll try with completely remove mysql govenor and let you know. Thank you my friend.
85,"Sorry, something went wrong."
85,Copy link
85,ghost
85,commented
85,"Nov 11, 2020"
85,edited by fevangelou
85,Cant do anything on wp site without mysql and php-fpm using 60% or higher cpu. What is causing this? Here is my.cnf
85,[mysql]
85,port
85,= 3306
85,socket
85,= /var/lib/mysql/mysql.sock
85,[mysqld]
85,# Required Settings
85,basedir
85,= /usr
85,bind_address
85,= 0.0.0.0 # Change to 0.0.0.0 to allow remote connections
85,datadir
85,= /var/lib/mysql
85,max_allowed_packet
85,= 16M
85,max_connect_errors
85,= 1000000
85,pid_file
85,= /var/lib/mysql/mysql.pid
85,port
85,= 3306
85,skip_external_locking
85,socket
85,= /var/lib/mysql/mysql.sock
85,tmpdir
85,= /tmp
85,user
85,= mysql
85,performance_schema
85,= ON
85,skip-name-resolve
85,# to pinpoint aborted connection we need this:
85,log-warnings=2
85,# InnoDB Settings
85,default_storage_engine
85,= InnoDB
85,innodb_buffer_pool_instances
85,= 4
85,# Use 1 instance per 1GB of InnoDB pool size
85,innodb_buffer_pool_size
85,= 4G
85,# Use up to 70-80% of RAM
85,innodb_file_per_table
85,= On
85,innodb_flush_log_at_trx_commit
85,= 2
85,innodb_flush_method
85,= O_DIRECT
85,innodb_log_buffer_size
85,= 8M
85,innodb_log_file_size
85,= 512M
85,innodb_stats_on_metadata
85,= 0
85,#innodb_temp_data_file_path
85,= ibtmp1:3G:autoextend # Control the maximum size for the ibtmp1 file
85,innodb_thread_concurrency
85,= 0
85,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
85,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
85,# the overall load produced by MySQL/MariaDB.
85,innodb_read_io_threads
85,= 128
85,innodb_write_io_threads
85,= 128
85,innodb_use_native_aio
85,= 0
85,# MyISAM Settings
85,query_cache_limit
85,= 4M
85,"# UPD - Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
85,query_cache_size
85,= 0
85,"# UPD - Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
85,query_cache_type
85,= 0
85,"# Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
85,#key_buffer_size
85,= 32M
85,# UPD
85,low_priority_updates
85,= 1
85,concurrent_insert
85,= 2
85,# Connection Settings
85,max_connections
85,= 200
85,# UPD - Important: high no. of connections = more RAM consumption
85,back_log
85,= 512
85,thread_cache_size
85,= 100
85,thread_stack
85,= 192K
85,interactive_timeout
85,= 300
85,wait_timeout
85,= 300
85,# For MySQL 5.7+ only (disabled by default)
85,#max_execution_time
85,= 30000 # Set a timeout limit for SELECT statements (value in milliseconds).
85,"# This option may be useful to address aggressive crawling on large sites,"
85,# but it can also cause issues (e.g. with backups). So use with extreme caution and test!
85,# More info at: https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time
85,# For MariaDB 10.1.1+ only (disabled by default)
85,#max_statement_time
85,= 30
85,"# The equivalent of ""max_execution_time"" in MySQL 5.7+ (set above)"
85,"# The variable is of type double, thus you can use subsecond timeout."
85,# For example you can use value 0.01 for 10 milliseconds timeout.
85,# More info at: https://mariadb.com/kb/en/aborting-statements/
85,# Buffer Settings
85,#join_buffer_size
85,= 4M
85,# UPD
85,#read_buffer_size
85,= 3M
85,# UPD
85,#read_rnd_buffer_size
85,= 4M
85,# UPD
85,#sort_buffer_size
85,= 4M
85,# UPD
85,# Table Settings
85,"# In systemd managed systems like CentOS 7, you need to perform an extra action for table_open_cache & open_files_limit"
85,# to be overriden (also see comment next to open_files_limit).
85,"# E.g. for MySQL 5.7 (when it's supported in cPanel), please check: https://dev.mysql.com/doc/refman/5.7/en/using-systemd.html"
85,# and for MariaDB check: https://mariadb.com/kb/en/library/systemd/
85,table_definition_cache
85,= 40000 # UPD
85,table_open_cache
85,= 40000 # UPD
85,open_files_limit
85,= 60000 # UPD - This can be 2x to 3x the table_open_cache value or match the system's
85,# open files limit usually set in /etc/sysctl.conf or /etc/security/limits.conf
85,# In systemd managed systems this limit must also be set in:
85,# /etc/systemd/system/mysqld.service.d/override.conf (for MySQL 5.7+) and
85,# /etc/systemd/system/mariadb.service.d/override.conf (for MariaDB)
85,max_heap_table_size
85,= 126M
85,tmp_table_size
85,= 128M
85,# Search Settings
85,ft_min_word_len
85,= 3
85,# Minimum length of words to be indexed for search results
85,# Logging
85,log_error
85,= /var/lib/mysql/mysql_error.log
85,log_queries_not_using_indexes
85,= 1
85,long_query_time
85,= 5
85,slow_query_log
85,= 0
85,# Disabled for production
85,slow_query_log_file
85,= /var/lib/mysql/mysql_slow.log
85,[mysqldump]
85,# Variable reference
85,# For MySQL 5.7: https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html
85,# For MariaDB:
85,https://mariadb.com/kb/en/library/mysqldump/
85,quick
85,quote_names
85,max_allowed_packet
85,= 64M
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Nov 13, 2020"
85,"@darnellkeithj It's really way off. You are increasing some variables to irrational values (e.g. max_connections, innodb_*_io_threads -which should not exceed 64- and others) and commenting out variables (e,g, *_buffer_size) which are important. You should seek professional performance auditing (which extends beyond MySQL/MariaDB). If you need my services you can always email me at engintron [at] gmail [dot] com."
85,"Sorry, something went wrong."
85,Copy link
85,talk2rajasimman
85,commented
85,"Dec 2, 2020"
85,I have Intel Xeon E3-1230 v2 - 3.3 GHz - 4 core(s) 8 threads.
85,RAM:	16GB - DDR3
85,Hello @fevangelou can you suggest me the my.cnf file. Really i am confused lot about this configuration.
85,"Sorry, something went wrong."
85,Copy link
85,karimrattani
85,commented
85,"Jan 23, 2021"
85,"Thanks for the config, I had to change below config otherwise MySQL failed to restart"
85,innodb_read_io_threads
85,= 40
85,innodb_write_io_threads
85,= 40
85,"Sorry, something went wrong."
85,Copy link
85,locksmithunit
85,commented
85,"Mar 5, 2021"
85,edited
85,is because you have 40 cores 4M 8C like mine.
85,"i did that 16. just in case, is not good to read everything from the DISK anyway."
85,the only thing I little bit confused about.
85,is this:
85,innodb_io_capacity = 1000
85,https://dev.mysql.com/doc/refman/8.0/en/innodb-configuring-io-capacity.html
85,this very good but it depends on your drive.
85,"in this case, I dont know what they talking about if you have Linux."
85,they recommended 1000 as well.
85,innodb_read_io_threads = 16
85,innodb_write_io_threads = 16
85,innodb_io_capacity = 1000
85,This can be very good to cPanel on a VPS cloud 4M 8C
85,BUT YOU MUST SPEAK WITH YOUR HOSTING ASK THEM IF IS SSD OR SATA 2 WITH 7200RPM
85,If they hosting with SSD (Must of the VPS hosting with SSD)
85,so need to uncomment the io_capacitiy = 1000
85,This 16M 8C is not what must of the hosting selling today...
85,I think is be better to improve the old version of the 4M 8C
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Nov 11, 2021"
85,Updated with better defaults and new performance related additions for MySQL 8.
85,"Sorry, something went wrong."
85,Copy link
85,locksmithunit
85,commented
85,"Nov 13, 2021"
85,edited
85,MariaDB 10.5?
85,I try and adjust the file already but i was needed to change it and comment out a lot from the configuration.
85,btw MariaDB 10.5 by default force you to put socket_unix=off
85,user name changed to MariaDB and not MySQL...
85,can you do one smaller and matching MariaDB 10.5? (BTW CPANEL FORCE CLIENTS INSTALL IT AND UPGRADE TO 10.5)
85,"CPANEL ALREADY ELECTED MARIADB 10.6 EXPERIMENTAL,"
85,It is a matter of time everybody will move these versions... :/
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Nov 25, 2021"
85,"Config updated with new tool references for DB diagnostics, minor changes in defaults and additional details in open_files_limit."
85,"Sorry, something went wrong."
85,Copy link
85,raramuridesign
85,commented
85,"Dec 2, 2021"
85,@fevangelou Thank you. This has been a great improvement on our servers. Appreciated
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Dec 2, 2021"
85,@raramuridesign You're most welcome Matthew!
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Dec 16, 2021"
85,New version of the config released.
85,"Binary logging is now disabled by default, ""innodb_sort_buffer_size"" has been bumped to 4M as a better default value, ""default_authentication_plugin"" is referenced (but commented by default - read the comments there), new performance related comments added in the buffers section."
85,"Sorry, something went wrong."
85,Copy link
85,dandidan2
85,commented
85,"Jan 8, 2022"
85,Can i hire you for my.cnf optimize for a server with 128gb ram?
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Jan 11, 2022"
85,@dandidan2 Sure - contact details are here: https://github.com/engintron/engintron#commercial-support--server-optimization-services
85,"Sorry, something went wrong."
85,Copy link
85,soulkeeperxx
85,commented
85,"Jul 9, 2022"
85,@dandidan2 Sure - contact details are here: https://github.com/engintron/engintron#commercial-support--server-optimization-services
85,Can I use your help to optimize my WHM ? I have a VDS with high specs and I need some optimization :D
85,couldnt find your contact detail as your site is down.
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Jul 18, 2022"
85,"Nothing was ever down... The URL above ALSO links to: ""...simply email us at: engintron [at] gmail [dot] com"""
85,"Sorry, something went wrong."
85,Copy link
85,raramuridesign
85,commented
85,"Jul 26, 2022"
85,@fevangelou any chance you could look at a version of this for a native mariadb install thats not on whm/cpanel?
85,"Sorry, something went wrong."
85,Copy link
85,asciixster
85,commented
85,"Mar 19, 2023"
85,@fevangelou any change you can update this configs to 8.0.32?
85,"Since we have ""forced"" to move to almalinux and 8.0.32 is enforced? thk you"
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Mar 22, 2023"
85,@raramuridesign
85,Here you go Matthew: https://gist.github.com/fevangelou/fb72f36bbe333e059b66
85,@asciixster
85,The config works just fine with MySQL 8 on Almalinux.
85,"To both, there are distinct comments for any differences between MySQL and MariaDB. Read them please. :)"
85,"Sorry, something went wrong."
85,Copy link
85,siamnews
85,commented
85,"Mar 22, 2023"
85,via email
85,bind_address is wrong
85,bind-address is right
85,Boris Sullivan
85,"Deputy Marketing Manager, Siam News Network"
85,***@***.***
85,Address: 160 Robinson Road
85,#14-04 Singapore Business Federation Centre
85,Website: https://www.siamnewsnetwork.net
85,<https://www.facebook.com/siamnewsnetwork>
85,<https://www.linkedin.com/company/siam-news-network/>
85,<https://www.twitter.com/ThailandBizNews>
85,"On Wed, Mar 22, 2023 at 2:31 PM Boris Sullivan ***@***.***>"
85,wrote:
85,You have a typo in your config :
85,bind_adress should be bind-adress
85,Boris Sullivan
85,"Deputy Marketing Manager, Siam News Network"
85,***@***.***
85,Address: 160 Robinson Road
85,#14-04 Singapore Business Federation Centre
85,Website: https://www.siamnewsnetwork.net
85,<https://www.facebook.com/siamnewsnetwork>
85,<https://www.linkedin.com/company/siam-news-network/>
85,<https://www.twitter.com/ThailandBizNews>
85,"On Wed, Mar 22, 2023 at 2:09 PM Fotis Evangelou ***@***.***>"
85,wrote:
85,> ***@***.**** commented on this gist.
85,> ------------------------------
85,> @raramuridesign <https://github.com/raramuridesign>
85,> Here you go Matthew:
85,> https://gist.github.com/fevangelou/fb72f36bbe333e059b66
85,> @asciixster <https://github.com/asciixster>
85,> The config works just fine with MySQL 8 on Almalinux.
85,"> To both, there are distinct comments for any differences between MySQL"
85,> and MariaDB. Read them please. :)
85,> —
85,"> Reply to this email directly, view it on GitHub"
85,> <https://gist.github.com/fevangelou/0da9941e67a9c9bb2596#gistcomment-4511874>
85,> or unsubscribe
85,> <https://github.com/notifications/unsubscribe-auth/ABYNHEUQPF4YACZF4NDMKS3W5L2YLBFKMF2HI4TJMJ2XIZLTSKBKK5TBNR2WLJDHNFZXJJDOMFWWLK3UNBZGKYLEL52HS4DFQKSXMYLMOVS2I5DSOVS2I3TBNVS3W5DIOJSWCZC7OBQXE5DJMNUXAYLOORPWCY3UNF3GS5DZVRZXKYTKMVRXIX3UPFYGLK2HNFZXIQ3PNVWWK3TUUZ2G64DJMNZZDAVEOR4XAZNEM5UXG5FFOZQWY5LFVAZTANZVGE4TINVHORZGSZ3HMVZKMY3SMVQXIZI>
85,> .
85,> You are receiving this email because you commented on the thread.
85,> Triage notifications on the go with GitHub Mobile for iOS
85,> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
85,> or Android
85,> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>
85,> .
85,"Sorry, something went wrong."
85,Copy link
85,raramuridesign
85,commented
85,"Mar 22, 2023"
85,@fevangelou Thanks ;-)
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Mar 22, 2023"
85,@siamnews Not exactly. You can define variables with both dashes and underscores.
85,"Sorry, something went wrong."
85,Copy link
85,theozsnowman
85,commented
85,"Apr 12, 2023"
85,ive tried this config on a Centos 7 server running Maria 10.6.12
85,are the following warnings normal on restart?
85,Apr 12 15:41:25 xxx.server.com systemd[1]: Starting MariaDB 10.6.12 database server...
85,Apr 12 15:41:25 xxx.server.com mariadbd[8812]: 2023-04-12 15:41:25 0 [Warning] Could not increase number of max_open_files to more than 40000 (request: 640139)
85,Apr 12 15:41:25 xxx.server.com mariadbd[8812]: 2023-04-12 15:41:25 0 [Warning] Changed limits: max_open_files: 40000
85,max_connections: 100 (was 100)
85,table_cache: 19935 (was 40000)
85,"Sorry, something went wrong."
85,Copy link
85,Author
85,fevangelou
85,commented
85,"Apr 18, 2023"
85,"@theozsnowman Read the comment next to ""open_files_limit"" in the config."
85,"Sorry, something went wrong."
85,Sign up for free
85,to join this conversation on GitHub.
85,Already have an account?
85,Sign in to comment
85,Footer
85,"© 2024 GitHub, Inc."
85,Footer navigation
85,Terms
85,Privacy
85,Security
85,Status
85,Docs
85,Contact
85,Manage cookies
85,Do not share my personal information
85,You can’t perform that action at this time.
86,Working with Databases: Active Record | The Definitive Guide to Yii 2.0 | Yii PHP Framework
86,Guide
86,API
86,Wiki
86,Forum
86,Community Live Chat
86,Extensions
86,Resources
86,Members
86,Hall of Fame
86,Badges
86,More Learn
86,Books
86,Resources
86,Develop
86,Download Yii
86,Report an Issue
86,Report a Security Issue
86,Contribute to Yii
86,Donate
86,About
86,What is Yii?
86,Release Cycle
86,News
86,License
86,Team
86,Official Logos and Design
86,Login
86,The Definitive Guide to Yii 2.0
86,Download PDF
86,Offline HTML (tar.gz)
86,Offline HTML (tar.bz2) English العربية
86,Español
86,Français
86,Bahasa Indonesia
86,日本語
86,Polski
86,Português brasileiro
86,Русский
86,Українська
86,Oʻzbekcha
86,简体中文
86,Tiếng Việt
86,Version 2.0 1.1
86,1.0
86,"Active RecordDeclaring Active Record ClassesSetting a table nameActive records are called ""models""Connecting to DatabasesQuerying DataAccessing DataData TransformationRetrieving Data in ArraysRetrieving Data in BatchesSaving DataData ValidationMassive AssignmentUpdating CountersDirty AttributesDefault Attribute ValuesAttributes TypecastingJSON in MySQL and PostgreSQLArrays in PostgreSQLUpdating Multiple RowsDeleting DataActive Record Life CyclesNew Instance Life CycleQuerying Data Life CycleSaving Data Life CycleDeleting Data Life CycleRefreshing Data Life CycleWorking with TransactionsOptimistic LocksWorking with Relational DataDeclaring RelationsAccessing Relational DataDynamic Relational QueryRelations via a Junction TableChaining relation definitions via multiple tablesLazy Loading and Eager LoadingJoining with RelationsRelation table aliasesInverse RelationsSaving RelationsCross-Database RelationsCustomizing Query ClassesSelecting extra fieldsUser Contributed Notes"
86,SideNav
86,Search IntroductionAbout Yii
86,Upgrading from Version 1.1
86,Getting StartedWhat do you need to know
86,Installing Yii
86,Running Applications
86,Saying Hello
86,Working with Forms
86,Working with Databases
86,Generating Code with Gii
86,Looking Ahead
86,Application StructureApplication Structure Overview
86,Entry Scripts
86,Applications
86,Application Components
86,Controllers
86,Models
86,Views
86,Modules
86,Filters
86,Widgets
86,Assets
86,Extensions
86,Handling RequestsRequest Handling Overview
86,Bootstrapping
86,Routing and URL Creation
86,Requests
86,Responses
86,Sessions and Cookies
86,Handling Errors
86,Logging
86,Key ConceptsComponents
86,Properties
86,Events
86,Behaviors
86,Configurations
86,Aliases
86,Class Autoloading
86,Service Locator
86,Dependency Injection Container
86,Working with DatabasesDatabase Access Objects
86,Query Builder
86,Active Record
86,Migrations
86,Sphinx
86,Redis
86,MongoDB
86,ElasticSearch
86,Getting Data from UsersCreating Forms
86,Validating Input
86,Uploading Files
86,Collecting Tabular Input
86,Getting Data for Multiple Models
86,Extending ActiveForm on the Client Side
86,Displaying DataData Formatting
86,Pagination
86,Sorting
86,Data Providers
86,Data Widgets
86,Working with Client Scripts
86,Theming
86,SecuritySecurity Overview
86,Authentication
86,Authorization
86,Working with Passwords
86,Cryptography
86,Auth Clients
86,Best Practices
86,CachingCaching Overview
86,Data Caching
86,Fragment Caching
86,Page Caching
86,HTTP Caching
86,RESTful Web ServicesQuick Start
86,Resources
86,Controllers
86,Filtering Collections
86,Routing
86,Response Formatting
86,Authentication
86,Rate Limiting
86,Versioning
86,Error Handling
86,Development ToolsDebug Toolbar and Debugger
86,Generating Code using Gii
86,Generating API Documentation
86,TestingTesting Overview
86,Testing environment setup
86,Unit Tests
86,Functional Tests
86,Acceptance Tests
86,Fixtures
86,Special TopicsAdvanced Project Template
86,Building Application from Scratch
86,Console Commands
86,Core Validators
86,Docker
86,Internationalization
86,Mailing
86,Performance Tuning
86,Shared Hosting Environment
86,Template Engines
86,Working with Third-Party Code
86,Using Yii as a micro framework
86,WidgetsGridView
86,ListView
86,DetailView
86,ActiveForm
86,Pjax
86,Menu
86,LinkPager
86,LinkSorter
86,Bootstrap Widgets
86,jQuery UI Widgets
86,HelpersHelpers Overview
86,ArrayHelper
86,Html
86,Json
86,Url
86,16 followers
86,Active Record ¶
86,Declaring Active Record Classes
86,Connecting to Databases
86,Querying Data
86,Accessing Data
86,Saving Data
86,Deleting Data
86,Active Record Life Cycles
86,Working with Transactions
86,Optimistic Locks
86,Working with Relational Data
86,Saving Relations
86,Cross-Database Relations
86,Customizing Query Classes
86,Selecting extra fields
86,Active Record provides an object-oriented interface
86,"for accessing and manipulating data stored in databases. An Active Record class is associated with a database table,"
86,"an Active Record instance corresponds to a row of that table, and an attribute of an Active Record"
86,"instance represents the value of a particular column in that row. Instead of writing raw SQL statements,"
86,you would access Active Record attributes and call Active Record methods to access and manipulate the data stored
86,in database tables.
86,"For example, assume Customer is an Active Record class which is associated with the customer table"
86,and name is a column of the customer table. You can write the following code to insert a new
86,row into the customer table:
86,$customer = new Customer();
86,$customer->name = 'Qiang';
86,$customer->save();
86,"The above code is equivalent to using the following raw SQL statement for MySQL, which is less"
86,"intuitive, more error prone, and may even have compatibility problems if you are using a different kind of database:"
86,"$db->createCommand('INSERT INTO `customer` (`name`) VALUES (:name)', ["
86,"':name' => 'Qiang',"
86,])->execute();
86,Yii provides the Active Record support for the following relational databases:
86,MySQL 4.1 or later: via yii\db\ActiveRecord
86,PostgreSQL 7.3 or later: via yii\db\ActiveRecord
86,SQLite 2 and 3: via yii\db\ActiveRecord
86,Microsoft SQL Server 2008 or later: via yii\db\ActiveRecord
86,Oracle: via yii\db\ActiveRecord
86,CUBRID 9.3 or later: via yii\db\ActiveRecord (Note that due to a bug in
86,"the cubrid PDO extension, quoting of values will not work, so you need CUBRID 9.3 as the client as well as the server)"
86,"Sphinx: via yii\sphinx\ActiveRecord, requires the yii2-sphinx extension"
86,"ElasticSearch: via yii\elasticsearch\ActiveRecord, requires the yii2-elasticsearch extension"
86,"Additionally, Yii also supports using Active Record with the following NoSQL databases:"
86,"Redis 2.6.12 or later: via yii\redis\ActiveRecord, requires the yii2-redis extension"
86,"MongoDB 1.3.0 or later: via yii\mongodb\ActiveRecord, requires the yii2-mongodb extension"
86,"In this tutorial, we will mainly describe the usage of Active Record for relational databases."
86,"However, most content described here are also applicable to Active Record for NoSQL databases."
86,Declaring Active Record Classes
86,"¶To get started, declare an Active Record class by extending yii\db\ActiveRecord."
86,Setting a table name ¶By default each Active Record class is associated with its database table.
86,The tableName() method returns the table name by converting the class name via yii\helpers\Inflector::camel2id().
86,You may override this method if the table is not named after this convention.
86,Also a default tablePrefix can be applied. For example if
86,"tablePrefix is tbl_, Customer becomes tbl_customer and OrderItem becomes tbl_order_item."
86,"If a table name is given as {{%TableName}}, then the percentage character % will be replaced with the table prefix."
86,"For example, {{%post}} becomes {{tbl_post}}. The brackets around the table name are used for"
86,quoting in an SQL query.
86,"In the following example, we declare an Active Record class named Customer for the customer database table."
86,namespace app\models;
86,use yii\db\ActiveRecord;
86,class Customer extends ActiveRecord
86,const STATUS_INACTIVE = 0;
86,const STATUS_ACTIVE = 1;
86,/**
86,* @return string the name of the table associated with this ActiveRecord class.
86,public static function tableName()
86,return '{{customer}}';
86,"Active records are called ""models"" ¶Active Record instances are considered as models. For this reason, we usually put Active Record"
86,classes under the app\models namespace (or other namespaces for keeping model classes).
86,"Because yii\db\ActiveRecord extends from yii\base\Model, it inherits all model features,"
86,"such as attributes, validation rules, data serialization, etc."
86,Connecting to Databases
86,"¶By default, Active Record uses the db application component"
86,as the DB connection to access and manipulate the database data. As explained in
86,"Database Access Objects, you can configure the db component in the application configuration like shown"
86,"below,"
86,return [
86,'components' => [
86,'db' => [
86,"'class' => 'yii\db\Connection',"
86,"'dsn' => 'mysql:host=localhost;dbname=testdb',"
86,"'username' => 'demo',"
86,"'password' => 'demo',"
86,"If you want to use a different database connection other than the db component, you should override"
86,the getDb() method:
86,class Customer extends ActiveRecord
86,// ...
86,public static function getDb()
86,"// use the ""db2"" application component"
86,return \Yii::$app->db2;
86,Querying Data
86,"¶After declaring an Active Record class, you can use it to query data from the corresponding database table."
86,The process usually takes the following three steps:
86,Create a new query object by calling the yii\db\ActiveRecord::find() method;
86,Build the query object by calling query building methods;
86,Call a query method to retrieve data in terms of Active Record instances.
86,"As you can see, this is very similar to the procedure with query builder. The only difference"
86,"is that instead of using the new operator to create a query object, you call yii\db\ActiveRecord::find()"
86,to return a new query object which is of class yii\db\ActiveQuery.
86,Below are some examples showing how to use Active Query to query data:
86,// return a single customer whose ID is 123
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::find()
86,->where(['id' => 123])
86,->one();
86,// return all active customers and order them by their IDs
86,// SELECT * FROM `customer` WHERE `status` = 1 ORDER BY `id`
86,$customers = Customer::find()
86,->where(['status' => Customer::STATUS_ACTIVE])
86,->orderBy('id')
86,->all();
86,// return the number of active customers
86,// SELECT COUNT(*) FROM `customer` WHERE `status` = 1
86,$count = Customer::find()
86,->where(['status' => Customer::STATUS_ACTIVE])
86,->count();
86,// return all customers in an array indexed by customer IDs
86,// SELECT * FROM `customer`
86,$customers = Customer::find()
86,->indexBy('id')
86,->all();
86,"In the above, $customer is a Customer object while $customers is an array of Customer objects. They are"
86,all populated with the data retrieved from the customer table.
86,"Info: Because yii\db\ActiveQuery extends from yii\db\Query, you can use all query building methods and"
86,query methods as described in the Section Query Builder.
86,"Because it is a common task to query by primary key values or a set of column values, Yii provides two shortcut"
86,methods for this purpose:
86,yii\db\ActiveRecord::findOne(): returns a single Active Record instance populated with the first row of the query result.
86,yii\db\ActiveRecord::findAll(): returns an array of Active Record instances populated with all query result.
86,Both methods can take one of the following parameter formats:
86,a scalar value: the value is treated as the desired primary key value to be looked for. Yii will determine
86,automatically which column is the primary key column by reading database schema information.
86,an array of scalar values: the array is treated as the desired primary key values to be looked for.
86,an associative array: the keys are column names and the values are the corresponding desired column values to
86,be looked for. Please refer to Hash Format for more details.
86,The following code shows how these methods can be used:
86,// returns a single customer whose ID is 123
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::findOne(123);
86,"// returns customers whose ID is 100, 101, 123 or 124"
86,"// SELECT * FROM `customer` WHERE `id` IN (100, 101, 123, 124)"
86,"$customers = Customer::findAll([100, 101, 123, 124]);"
86,// returns an active customer whose ID is 123
86,// SELECT * FROM `customer` WHERE `id` = 123 AND `status` = 1
86,$customer = Customer::findOne([
86,"'id' => 123,"
86,"'status' => Customer::STATUS_ACTIVE,"
86,]);
86,// returns all inactive customers
86,// SELECT * FROM `customer` WHERE `status` = 0
86,$customers = Customer::findAll([
86,"'status' => Customer::STATUS_INACTIVE,"
86,]);
86,"Warning: If you need to pass user input to these methods, make sure the input value is scalar or in case of"
86,"array condition, make sure the array structure can not be changed from the outside:"
86,// yii\web\Controller ensures that $id is scalar
86,public function actionView($id)
86,$model = Post::findOne($id);
86,// ...
86,"// explicitly specifying the column to search, passing a scalar or array here will always result in finding a single record"
86,$model = Post::findOne(['id' => Yii::$app->request->get('id')]);
86,// do NOT use the following code! it is possible to inject an array condition to filter by arbitrary column values!
86,$model = Post::findOne(Yii::$app->request->get('id'));
86,Note: Neither yii\db\ActiveRecord::findOne() nor yii\db\ActiveQuery::one() will add LIMIT 1 to
86,"the generated SQL statement. If your query may return many rows of data, you should call limit(1) explicitly"
86,"to improve the performance, e.g., Customer::find()->limit(1)->one()."
86,"Besides using query building methods, you can also write raw SQLs to query data and populate the results into"
86,Active Record objects. You can do so by calling the yii\db\ActiveRecord::findBySql() method:
86,// returns all inactive customers
86,$sql = 'SELECT * FROM customer WHERE status=:status';
86,"$customers = Customer::findBySql($sql, [':status' => Customer::STATUS_INACTIVE])->all();"
86,Do not call extra query building methods after calling findBySql() as they
86,will be ignored.
86,Accessing Data
86,"¶As aforementioned, the data brought back from the database are populated into Active Record instances, and"
86,each row of the query result corresponds to a single Active Record instance. You can access the column values
86,"by accessing the attributes of the Active Record instances, for example,"
86,"// ""id"" and ""email"" are the names of columns in the ""customer"" table"
86,$customer = Customer::findOne(123);
86,$id = $customer->id;
86,$email = $customer->email;
86,Note: The Active Record attributes are named after the associated table columns in a case-sensitive manner.
86,Yii automatically defines an attribute in Active Record for every column of the associated table.
86,You should NOT redeclare any of the attributes.
86,"Because Active Record attributes are named after table columns, you may find you are writing PHP code like"
86,"$customer->first_name, which uses underscores to separate words in attribute names if your table columns are"
86,"named in this way. If you are concerned about code style consistency, you should rename your table columns accordingly"
86,"(to use camelCase, for example)."
86,Data Transformation
86,¶It often happens that the data being entered and/or displayed are in a format which is different from the one used in
86,"storing the data in a database. For example, in the database you are storing customers' birthdays as UNIX timestamps"
86,"(which is not a good design, though), while in most cases you would like to manipulate birthdays as strings in"
86,"the format of 'YYYY/MM/DD'. To achieve this goal, you can define data transformation methods in the Customer"
86,Active Record class like the following:
86,class Customer extends ActiveRecord
86,// ...
86,public function getBirthdayText()
86,"return date('Y/m/d', $this->birthday);"
86,public function setBirthdayText($value)
86,$this->birthday = strtotime($value);
86,"Now in your PHP code, instead of accessing $customer->birthday, you would access $customer->birthdayText, which"
86,will allow you to input and display customer birthdays in the format of 'YYYY/MM/DD'.
86,Tip: The above example shows a generic way of transforming data in different formats. If you are working with
86,"date values, you may use DateValidator and yii\jui\DatePicker,"
86,which is easier to use and more powerful.
86,Retrieving Data in Arrays
86,"¶While retrieving data in terms of Active Record objects is convenient and flexible, it is not always desirable"
86,"when you have to bring back a large amount of data due to the big memory footprint. In this case, you can retrieve"
86,data using PHP arrays by calling asArray() before executing a query method:
86,// return all customers
86,// each customer is returned as an associative array
86,$customers = Customer::find()
86,->asArray()
86,->all();
86,"Note: While this method saves memory and improves performance, it is closer to the lower DB abstraction layer"
86,and you will lose most of the Active Record features. A very important distinction lies in the data type of
86,"the column values. When you return data in Active Record instances, column values will be automatically typecast"
86,"according to the actual column types; on the other hand when you return data in arrays, column values will be"
86,"strings (since they are the result of PDO without any processing), regardless their actual column types."
86,Retrieving Data in Batches
86,"¶In Query Builder, we have explained that you may use batch query to minimize your memory"
86,"usage when querying a large amount of data from the database. You may use the same technique in Active Record. For example,"
86,// fetch 10 customers at a time
86,foreach (Customer::find()->batch(10) as $customers) {
86,// $customers is an array of 10 or fewer Customer objects
86,// fetch 10 customers at a time and iterate them one by one
86,foreach (Customer::find()->each(10) as $customer) {
86,// $customer is a Customer object
86,// batch query with eager loading
86,foreach (Customer::find()->with('orders')->each() as $customer) {
86,// $customer is a Customer object with the 'orders' relation populated
86,Saving Data
86,"¶Using Active Record, you can easily save data to the database by taking the following steps:"
86,Prepare an Active Record instance
86,Assign new values to Active Record attributes
86,Call yii\db\ActiveRecord::save() to save the data into database.
86,"For example,"
86,// insert a new row of data
86,$customer = new Customer();
86,$customer->name = 'James';
86,$customer->email = 'james@example.com';
86,$customer->save();
86,// update an existing row of data
86,$customer = Customer::findOne(123);
86,$customer->email = 'james@newexample.com';
86,$customer->save();
86,"The save() method can either insert or update a row of data, depending on the state"
86,"of the Active Record instance. If the instance is newly created via the new operator, calling"
86,"save() will cause insertion of a new row; If the instance is the result of a query method,"
86,calling save() will update the row associated with the instance.
86,You can differentiate the two states of an Active Record instance by checking its
86,isNewRecord property value. This property is also used by
86,save() internally as follows:
86,"public function save($runValidation = true, $attributeNames = null)"
86,if ($this->getIsNewRecord()) {
86,"return $this->insert($runValidation, $attributeNames);"
86,} else {
86,"return $this->update($runValidation, $attributeNames) !== false;"
86,Tip: You can call insert() or update()
86,directly to insert or update a row.
86,Data Validation
86,"¶Because yii\db\ActiveRecord extends from yii\base\Model, it shares the same data validation feature."
86,You can declare validation rules by overriding the rules() method and perform
86,data validation by calling the validate() method.
86,"When you call save(), by default it will call validate()"
86,"automatically. Only when the validation passes, will it actually save the data; otherwise it will simply return false,"
86,and you can check the errors property to retrieve the validation error messages.
86,"Tip: If you are certain that your data do not need validation (e.g., the data comes from trustable sources),"
86,you can call save(false) to skip the validation.
86,Massive Assignment
86,"¶Like normal models, Active Record instances also enjoy the massive assignment feature."
86,"Using this feature, you can assign values to multiple attributes of an Active Record instance in a single PHP statement,"
86,"like shown below. Do remember that only safe attributes can be massively assigned, though."
86,$values = [
86,"'name' => 'James',"
86,"'email' => 'james@example.com',"
86,$customer = new Customer();
86,$customer->attributes = $values;
86,$customer->save();
86,Updating Counters
86,"¶It is a common task to increment or decrement a column in a database table. We call these columns ""counter columns""."
86,You can use updateCounters() to update one or multiple counter columns.
86,"For example,"
86,$post = Post::findOne(100);
86,// UPDATE `post` SET `view_count` = `view_count` + 1 WHERE `id` = 100
86,$post->updateCounters(['view_count' => 1]);
86,"Note: If you use yii\db\ActiveRecord::save() to update a counter column, you may end up with inaccurate result,"
86,because it is likely the same counter is being saved by multiple requests which read and write the same counter value.
86,Dirty Attributes
86,"¶When you call save() to save an Active Record instance, only dirty attributes"
86,are being saved. An attribute is considered dirty if its value has been modified since it was loaded from DB or
86,saved to DB most recently. Note that data validation will be performed regardless if the Active Record
86,instance has dirty attributes or not.
86,Active Record automatically maintains the list of dirty attributes. It does so by maintaining an older version of
86,the attribute values and comparing them with the latest one. You can call yii\db\ActiveRecord::getDirtyAttributes()
86,to get the attributes that are currently dirty. You can also call yii\db\ActiveRecord::markAttributeDirty()
86,to explicitly mark an attribute as dirty.
86,"If you are interested in the attribute values prior to their most recent modification, you may call"
86,getOldAttributes() or getOldAttribute().
86,Note: The comparison of old and new values will be done using the === operator so a value will be considered dirty
86,even if it has the same value but a different type. This is often the case when the model receives user input from
86,HTML forms where every value is represented as a string.
86,To ensure the correct type for e.g. integer values you may apply a validation filter:
86,"['attributeName', 'filter', 'filter' => 'intval']. This works with all the typecasting functions of PHP like"
86,"intval(), floatval(),"
86,"boolval, etc..."
86,Default Attribute Values
86,"¶Some of your table columns may have default values defined in the database. Sometimes, you may want to pre-populate your"
86,"Web form for an Active Record instance with these default values. To avoid writing the same default values again,"
86,you can call loadDefaultValues() to populate the DB-defined default values
86,into the corresponding Active Record attributes:
86,$customer = new Customer();
86,$customer->loadDefaultValues();
86,"// $customer->xyz will be assigned the default value declared when defining the ""xyz"" column"
86,Attributes Typecasting
86,"¶Being populated by query results, yii\db\ActiveRecord performs automatic typecast for its attribute values, using"
86,information from database table schema. This allows data retrieved from table column
86,"declared as integer to be populated in ActiveRecord instance with PHP integer, boolean with boolean and so on."
86,"However, typecasting mechanism has several limitations:"
86,"Float values are not be converted and will be represented as strings, otherwise they may loose precision."
86,Conversion of the integer values depends on the integer capacity of the operation system you use. In particular:
86,values of column declared as 'unsigned integer' or 'big integer' will be converted to PHP integer only at 64-bit
86,"operation system, while on 32-bit ones - they will be represented as strings."
86,Note that attribute typecast is performed only during populating ActiveRecord instance from query result. There is no
86,automatic conversion for the values loaded from HTTP request or set directly via property access.
86,"The table schema will also be used while preparing SQL statements for the ActiveRecord data saving, ensuring"
86,"values are bound to the query with correct type. However, ActiveRecord instance attribute values will not be"
86,converted during saving process.
86,Tip: you may use yii\behaviors\AttributeTypecastBehavior to facilitate attribute values typecasting
86,on ActiveRecord validation or saving.
86,"Since 2.0.14, Yii ActiveRecord supports complex data types, such as JSON or multidimensional arrays."
86,"JSON in MySQL and PostgreSQL ¶After data population, the value from JSON column will be automatically decoded from JSON according to standard JSON"
86,decoding rules.
86,"To save attribute value to a JSON column, ActiveRecord will automatically create a JsonExpression object"
86,that will be encoded to a JSON string on QueryBuilder level.
86,"Arrays in PostgreSQL ¶After data population, the value from Array column will be automatically decoded from PgSQL notation to an ArrayExpression"
86,"object. It implements PHP ArrayAccess interface, so you can use it as an array, or call ->getValue() to get the array itself."
86,"To save attribute value to an array column, ActiveRecord will automatically create an ArrayExpression object"
86,that will be encoded by QueryBuilder to an PgSQL string representation of array.
86,You can also use conditions for JSON columns:
86,"$query->andWhere(['=', 'json', new ArrayExpression(['foo' => 'bar'])])"
86,To learn more about expressions building system read the Query Builder – Adding custom Conditions and Expressions
86,article.
86,Updating Multiple Rows
86,"¶The methods described above all work on individual Active Record instances, causing inserting or updating of individual"
86,"table rows. To update multiple rows simultaneously, you should call updateAll(), instead,"
86,which is a static method.
86,// UPDATE `customer` SET `status` = 1 WHERE `email` LIKE `%@example.com%`
86,"Customer::updateAll(['status' => Customer::STATUS_ACTIVE], ['like', 'email', '@example.com']);"
86,"Similarly, you can call updateAllCounters() to update counter columns of"
86,multiple rows at the same time.
86,// UPDATE `customer` SET `age` = `age` + 1
86,Customer::updateAllCounters(['age' => 1]);
86,Deleting Data
86,"¶To delete a single row of data, first retrieve the Active Record instance corresponding to that row and then call"
86,the yii\db\ActiveRecord::delete() method.
86,$customer = Customer::findOne(123);
86,$customer->delete();
86,"You can call yii\db\ActiveRecord::deleteAll() to delete multiple or all rows of data. For example,"
86,Customer::deleteAll(['status' => Customer::STATUS_INACTIVE]);
86,Note: Be very careful when calling deleteAll() because it may totally
86,erase all data from your table if you make a mistake in specifying the condition.
86,Active Record Life Cycles
86,¶It is important to understand the life cycles of Active Record when it is used for different purposes.
86,"During each life cycle, a certain sequence of methods will be invoked, and you can override these methods"
86,to get a chance to customize the life cycle. You can also respond to certain Active Record events triggered
86,during a life cycle to inject your custom code. These events are especially useful when you are developing
86,Active Record behaviors which need to customize Active Record life cycles.
86,"In the following, we will summarize the various Active Record life cycles and the methods/events that are involved"
86,in the life cycles.
86,New Instance Life Cycle
86,"¶When creating a new Active Record instance via the new operator, the following life cycle will happen:"
86,Class constructor.
86,init(): triggers an EVENT_INIT event.
86,Querying Data Life Cycle
86,"¶When querying data through one of the querying methods, each newly populated Active Record will"
86,undergo the following life cycle:
86,Class constructor.
86,init(): triggers an EVENT_INIT event.
86,afterFind(): triggers an EVENT_AFTER_FIND event.
86,Saving Data Life Cycle
86,"¶When calling save() to insert or update an Active Record instance, the following"
86,life cycle will happen:
86,beforeValidate(): triggers
86,an EVENT_BEFORE_VALIDATE event. If the method returns false
86,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
86,"Performs data validation. If data validation fails, the steps after Step 3 will be skipped."
86,afterValidate(): triggers
86,an EVENT_AFTER_VALIDATE event.
86,beforeSave(): triggers
86,an EVENT_BEFORE_INSERT
86,or EVENT_BEFORE_UPDATE event. If the method returns false
86,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
86,Performs the actual data insertion or updating.
86,afterSave(): triggers
86,an EVENT_AFTER_INSERT
86,or EVENT_AFTER_UPDATE event.
86,Deleting Data Life Cycle
86,"¶When calling delete() to delete an Active Record instance, the following"
86,life cycle will happen:
86,beforeDelete(): triggers
86,an EVENT_BEFORE_DELETE event. If the method returns false
86,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
86,Performs the actual data deletion.
86,afterDelete(): triggers
86,an EVENT_AFTER_DELETE event.
86,Note: Calling any of the following methods will NOT initiate any of the above life cycles because they work on the
86,database directly and not on a record basis:
86,yii\db\ActiveRecord::updateAll()
86,yii\db\ActiveRecord::deleteAll()
86,yii\db\ActiveRecord::updateCounters()
86,yii\db\ActiveRecord::updateAllCounters()
86,Note: DI is not supported by default due to performance concerns. You can add support if needed by overriding
86,the instantiate() method to instantiate the class via Yii::createObject():
86,public static function instantiate($row)
86,return Yii::createObject(static::class);
86,Refreshing Data Life Cycle
86,"¶When calling refresh() to refresh an Active Record instance, the"
86,EVENT_AFTER_REFRESH event is triggered if refresh is successful and the method returns true.
86,Working with Transactions
86,¶There are two ways of using transactions while working with Active Record.
86,"The first way is to explicitly enclose Active Record method calls in a transactional block, like shown below,"
86,$customer = Customer::findOne(123);
86,Customer::getDb()->transaction(function($db) use ($customer) {
86,$customer->id = 200;
86,$customer->save();
86,// ...other DB operations...
86,});
86,// or alternatively
86,$transaction = Customer::getDb()->beginTransaction();
86,try {
86,$customer->id = 200;
86,$customer->save();
86,// ...other DB operations...
86,$transaction->commit();
86,} catch(\Exception $e) {
86,$transaction->rollBack();
86,throw $e;
86,} catch(\Throwable $e) {
86,$transaction->rollBack();
86,throw $e;
86,Note: in the above code we have two catch-blocks for compatibility
86,with PHP 5.x and PHP 7.x. \Exception implements the \Throwable interface
86,"since PHP 7.0, so you can skip the part with \Exception if your app uses only PHP 7.0 and higher."
86,The second way is to list the DB operations that require transactional support in the yii\db\ActiveRecord::transactions()
86,"method. For example,"
86,class Customer extends ActiveRecord
86,public function transactions()
86,return [
86,"'admin' => self::OP_INSERT,"
86,"'api' => self::OP_INSERT | self::OP_UPDATE | self::OP_DELETE,"
86,// the above is equivalent to the following:
86,"// 'api' => self::OP_ALL,"
86,The yii\db\ActiveRecord::transactions() method should return an array whose keys are scenario
86,names and values are the corresponding operations that should be enclosed within transactions. You should use the following
86,constants to refer to different DB operations:
86,OP_INSERT: insertion operation performed by insert();
86,OP_UPDATE: update operation performed by update();
86,OP_DELETE: deletion operation performed by delete().
86,Use the | operators to concatenate the above constants to indicate multiple operations. You may also use the shortcut
86,constant OP_ALL to refer to all three operations above.
86,Transactions that are created using this method will be started before calling beforeSave()
86,and will be committed after afterSave() has run.
86,Optimistic Locks
86,¶Optimistic locking is a way to prevent conflicts that may occur when a single row of data is being
86,"updated by multiple users. For example, both user A and user B are editing the same wiki article"
86,"at the same time. After user A saves his edits, user B clicks on the ""Save"" button in an attempt to"
86,"save his edits as well. Because user B was actually working on an outdated version of the article,"
86,it would be desirable to have a way to prevent him from saving the article and show him some hint message.
86,Optimistic locking solves the above problem by using a column to record the version number of each row.
86,"When a row is being saved with an outdated version number, a yii\db\StaleObjectException exception"
86,"will be thrown, which prevents the row from being saved. Optimistic locking is only supported when you"
86,"update or delete an existing row of data using yii\db\ActiveRecord::update() or yii\db\ActiveRecord::delete(),"
86,respectively.
86,"To use optimistic locking,"
86,Create a column in the DB table associated with the Active Record class to store the version number of each row.
86,The column should be of big integer type (in MySQL it would be BIGINT DEFAULT 0).
86,Override the yii\db\ActiveRecord::optimisticLock() method to return the name of this column.
86,Implement OptimisticLockBehavior inside your model class to automatically parse its value from received requests.
86,Remove the version attribute from validation rules as OptimisticLockBehavior should handle it.
86,"In the Web form that takes user inputs, add a hidden field to store the current version number of the row being updated."
86,"In the controller action that updates the row using Active Record, try and catch the yii\db\StaleObjectException"
86,"exception. Implement necessary business logic (e.g. merging the changes, prompting staled data) to resolve the conflict."
86,"For example, assume the version column is named as version. You can implement optimistic locking with the code like"
86,the following.
86,// ------ view code -------
86,use yii\helpers\Html;
86,// ...other input fields
86,"echo Html::activeHiddenInput($model, 'version');"
86,// ------ controller code -------
86,use yii\db\StaleObjectException;
86,public function actionUpdate($id)
86,$model = $this->findModel($id);
86,try {
86,if ($model->load(Yii::$app->request->post()) && $model->save()) {
86,"return $this->redirect(['view', 'id' => $model->id]);"
86,} else {
86,"return $this->render('update', ["
86,"'model' => $model,"
86,]);
86,} catch (StaleObjectException $e) {
86,// logic to resolve the conflict
86,// ------ model code -------
86,use yii\behaviors\OptimisticLockBehavior;
86,public function behaviors()
86,return [
86,"OptimisticLockBehavior::class,"
86,public function optimisticLock()
86,return 'version';
86,Note: Because OptimisticLockBehavior will ensure the record is only saved
86,"if user submits a valid version number by directly parsing getBodyParam(), it"
86,may be useful to extend your model class and do step 2 in parent model while attaching the behavior (step 3) to the child
86,class so you can have an instance dedicated to internal use while tying the other to controllers responsible of receiving
86,"end user inputs. Alternatively, you can implement your own logic by configuring its value property."
86,Working with Relational Data
86,"¶Besides working with individual database tables, Active Record is also capable of bringing together related data,"
86,"making them readily accessible through the primary data. For example, the customer data is related with the order"
86,"data because one customer may have placed one or multiple orders. With appropriate declaration of this relation,"
86,you'll be able to access a customer's order information using the expression $customer->orders which gives
86,back the customer's order information in terms of an array of Order Active Record instances.
86,Declaring Relations
86,"¶To work with relational data using Active Record, you first need to declare relations in Active Record classes."
86,"The task is as simple as declaring a relation method for every interested relation, like the following,"
86,class Customer extends ActiveRecord
86,// ...
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,class Order extends ActiveRecord
86,// ...
86,public function getCustomer()
86,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
86,"In the above code, we have declared an orders relation for the Customer class, and a customer relation"
86,for the Order class.
86,Each relation method must be named as getXyz. We call xyz (the first letter is in lower case) the relation name.
86,Note that relation names are case sensitive.
86,"While declaring a relation, you should specify the following information:"
86,the multiplicity of the relation: specified by calling either hasMany()
86,or hasOne(). In the above example you may easily read in the relation
86,declarations that a customer has many orders while an order only has one customer.
86,the name of the related Active Record class: specified as the first parameter to
86,either hasMany() or hasOne().
86,A recommended practice is to call Xyz::class to get the class name string so that you can receive
86,IDE auto-completion support as well as error detection at compiling stage.
86,the link between the two types of data: specifies the column(s) through which the two types of data are related.
86,The array values are the columns of the primary data (represented by the Active Record class that you are declaring
86,"relations), while the array keys are the columns of the related data."
86,"An easy rule to remember this is, as you see in the example above, you write the column that belongs to the related"
86,Active Record directly next to it. You see there that customer_id is a property of Order and id is a property
86,of Customer.
86,Warning: Relation name relation is reserved. When used it will produce ArgumentCountError.
86,Accessing Relational Data
86,"¶After declaring relations, you can access relational data through relation names. This is just like accessing"
86,"an object property defined by the relation method. For this reason, we call it relation property."
86,"For example,"
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::findOne(123);
86,// SELECT * FROM `order` WHERE `customer_id` = 123
86,// $orders is an array of Order objects
86,$orders = $customer->orders;
86,"Info: When you declare a relation named xyz via a getter method getXyz(), you will be able to access"
86,xyz like an object property. Note that the name is case sensitive.
86,"If a relation is declared with hasMany(), accessing this relation property"
86,will return an array of the related Active Record instances; if a relation is declared with
86,"hasOne(), accessing the relation property will return the related"
86,Active Record instance or null if no related data is found.
86,"When you access a relation property for the first time, a SQL statement will be executed, like shown in the"
86,"above example. If the same property is accessed again, the previous result will be returned without re-executing"
86,"the SQL statement. To force re-executing the SQL statement, you should unset the relation property"
86,first: unset($customer->orders).
86,"Note: While this concept looks similar to the object property feature, there is an"
86,important difference. For normal object properties the property value is of the same type as the defining getter method.
86,"A relation method however returns an yii\db\ActiveQuery instance, while accessing a relation property will either"
86,return a yii\db\ActiveRecord instance or an array of these.
86,$customer->orders; // is an array of `Order` objects
86,$customer->getOrders(); // returns an ActiveQuery instance
86,"This is useful for creating customized queries, which is described in the next section."
86,Dynamic Relational Query
86,"¶Because a relation method returns an instance of yii\db\ActiveQuery, you can further build this query"
86,"using query building methods before performing DB query. For example,"
86,$customer = Customer::findOne(123);
86,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 200 ORDER BY `id`
86,$orders = $customer->getOrders()
86,"->where(['>', 'subtotal', 200])"
86,->orderBy('id')
86,->all();
86,"Unlike accessing a relation property, each time you perform a dynamic relational query via a relation method,"
86,"a SQL statement will be executed, even if the same dynamic relational query was performed before."
86,Sometimes you may even want to parametrize a relation declaration so that you can more easily perform
86,"dynamic relational query. For example, you may declare a bigOrders relation as follows,"
86,class Customer extends ActiveRecord
86,public function getBigOrders($threshold = 100)
86,"return $this->hasMany(Order::class, ['customer_id' => 'id'])"
86,"->where('subtotal > :threshold', [':threshold' => $threshold])"
86,->orderBy('id');
86,Then you will be able to perform the following relational queries:
86,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 200 ORDER BY `id`
86,$orders = $customer->getBigOrders(200)->all();
86,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 100 ORDER BY `id`
86,$orders = $customer->bigOrders;
86,Relations via a Junction Table
86,"¶In database modelling, when the multiplicity between two related tables is many-to-many,"
86,"a junction table is usually introduced. For example, the order"
86,table and the item table may be related via a junction table named order_item. One order will then correspond
86,"to multiple order items, while one product item will also correspond to multiple order items."
86,"When declaring such relations, you would call either via() or viaTable()"
86,to specify the junction table. The difference between via() and viaTable()
86,is that the former specifies the junction table in terms of an existing relation name while the latter directly uses
86,"the junction table. For example,"
86,class Order extends ActiveRecord
86,public function getItems()
86,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
86,"->viaTable('order_item', ['order_id' => 'id']);"
86,"or alternatively,"
86,class Order extends ActiveRecord
86,public function getOrderItems()
86,"return $this->hasMany(OrderItem::class, ['order_id' => 'id']);"
86,public function getItems()
86,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
86,->via('orderItems');
86,"The usage of relations declared with a junction table is the same as that of normal relations. For example,"
86,// SELECT * FROM `order` WHERE `id` = 100
86,$order = Order::findOne(100);
86,// SELECT * FROM `order_item` WHERE `order_id` = 100
86,// SELECT * FROM `item` WHERE `item_id` IN (...)
86,// returns an array of Item objects
86,$items = $order->items;
86,Chaining relation definitions via multiple tables
86,¶Its further possible to define relations via multiple tables by chaining relation definitions using via().
86,"Considering the examples above, we have classes Customer, Order, and Item."
86,"We can add a relation to the Customer class that lists all items from all the orders they placed,"
86,"and name it getPurchasedItems(), the chaining of relations is show in the following code example:"
86,class Customer extends ActiveRecord
86,// ...
86,public function getPurchasedItems()
86,"// customer's items, matching 'id' column of `Item` to 'item_id' in OrderItem"
86,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
86,->via('orderItems');
86,public function getOrderItems()
86,"// customer's order items, matching 'id' column of `Order` to 'order_id' in OrderItem"
86,"return $this->hasMany(OrderItem::class, ['order_id' => 'id'])"
86,->via('orders');
86,public function getOrders()
86,// same as above
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,Lazy Loading and Eager Loading
86,"¶In Accessing Relational Data, we explained that you can access a relation property"
86,of an Active Record instance like accessing a normal object property. A SQL statement will be executed only when
86,you access the relation property the first time. We call such relational data accessing method lazy loading.
86,"For example,"
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::findOne(123);
86,// SELECT * FROM `order` WHERE `customer_id` = 123
86,$orders = $customer->orders;
86,// no SQL executed
86,$orders2 = $customer->orders;
86,"Lazy loading is very convenient to use. However, it may suffer from a performance issue when you need to access"
86,the same relation property of multiple Active Record instances. Consider the following code example. How many
86,SQL statements will be executed?
86,// SELECT * FROM `customer` LIMIT 100
86,$customers = Customer::find()->limit(100)->all();
86,foreach ($customers as $customer) {
86,// SELECT * FROM `order` WHERE `customer_id` = ...
86,$orders = $customer->orders;
86,"As you can see from the code comment above, there are 101 SQL statements being executed! This is because each"
86,"time you access the orders relation property of a different Customer object in the for-loop, a SQL statement"
86,will be executed.
86,"To solve this performance problem, you can use the so-called eager loading approach as shown below,"
86,// SELECT * FROM `customer` LIMIT 100;
86,// SELECT * FROM `orders` WHERE `customer_id` IN (...)
86,$customers = Customer::find()
86,->with('orders')
86,->limit(100)
86,->all();
86,foreach ($customers as $customer) {
86,// no SQL executed
86,$orders = $customer->orders;
86,"By calling yii\db\ActiveQuery::with(), you instruct Active Record to bring back the orders for the first 100"
86,"customers in one single SQL statement. As a result, you reduce the number of the executed SQL statements from 101 to 2!"
86,You can eagerly load one or multiple relations. You can even eagerly load nested relations. A nested relation is a relation
86,"that is declared within a related Active Record class. For example, Customer is related with Order through the orders"
86,"relation, and Order is related with Item through the items relation. When querying for Customer, you can eagerly"
86,load items using the nested relation notation orders.items.
86,The following code shows different usage of with(). We assume the Customer class
86,"has two relations orders and country, while the Order class has one relation items."
86,"// eager loading both ""orders"" and ""country"""
86,"$customers = Customer::find()->with('orders', 'country')->all();"
86,// equivalent to the array syntax below
86,"$customers = Customer::find()->with(['orders', 'country'])->all();"
86,// no SQL executed
86,$orders= $customers[0]->orders;
86,// no SQL executed
86,$country = $customers[0]->country;
86,"// eager loading ""orders"" and the nested relation ""orders.items"""
86,$customers = Customer::find()->with('orders.items')->all();
86,// access the items of the first order of the first customer
86,// no SQL executed
86,$items = $customers[0]->orders[0]->items;
86,"You can eagerly load deeply nested relations, such as a.b.c.d. All parent relations will be eagerly loaded."
86,"That is, when you call with() using a.b.c.d, you will eagerly load"
86,"a, a.b, a.b.c and a.b.c.d."
86,"Info: In general, when eagerly loading N relations among which M relations are defined with a"
86,"junction table, a total number of N+M+1 SQL statements will be executed."
86,Note that a nested relation a.b.c.d counts as 4 relations.
86,"When eagerly loading a relation, you can customize the corresponding relational query using an anonymous function."
86,"For example,"
86,// find customers and bring back together their country and active orders
86,// SELECT * FROM `customer`
86,// SELECT * FROM `country` WHERE `id` IN (...)
86,// SELECT * FROM `order` WHERE `customer_id` IN (...) AND `status` = 1
86,$customers = Customer::find()->with([
86,"'country',"
86,'orders' => function ($query) {
86,$query->andWhere(['status' => Order::STATUS_ACTIVE]);
86,])->all();
86,"When customizing the relational query for a relation, you should specify the relation name as an array key"
86,and use an anonymous function as the corresponding array value. The anonymous function will receive a $query parameter
86,which represents the yii\db\ActiveQuery object used to perform the relational query for the relation.
86,"In the code example above, we are modifying the relational query by appending an additional condition about order status."
86,"Note: If you call select() while eagerly loading relations, you have to make sure"
86,"the columns referenced in the relation declarations are being selected. Otherwise, the related models may not"
86,"be loaded properly. For example,"
86,"$orders = Order::find()->select(['id', 'amount'])->with('customer')->all();"
86,"// $orders[0]->customer is always `null`. To fix the problem, you should do the following:"
86,"$orders = Order::find()->select(['id', 'amount', 'customer_id'])->with('customer')->all();"
86,Joining with Relations
86,"¶Note: The content described in this subsection is only applicable to relational databases, such as"
86,"MySQL, PostgreSQL, etc."
86,The relational queries that we have described so far only reference the primary table columns when
86,"querying for the primary data. In reality we often need to reference columns in the related tables. For example,"
86,"we may want to bring back the customers who have at least one active order. To solve this problem, we can"
86,build a join query like the following:
86,// SELECT `customer`.* FROM `customer`
86,// LEFT JOIN `order` ON `order`.`customer_id` = `customer`.`id`
86,// WHERE `order`.`status` = 1
86,// SELECT * FROM `order` WHERE `customer_id` IN (...)
86,$customers = Customer::find()
86,->select('customer.*')
86,"->leftJoin('order', '`order`.`customer_id` = `customer`.`id`')"
86,->where(['order.status' => Order::STATUS_ACTIVE])
86,->with('orders')
86,->all();
86,Note: It is important to disambiguate column names when building relational queries involving JOIN SQL statements.
86,A common practice is to prefix column names with their corresponding table names.
86,"However, a better approach is to exploit the existing relation declarations by calling yii\db\ActiveQuery::joinWith():"
86,$customers = Customer::find()
86,->joinWith('orders')
86,->where(['order.status' => Order::STATUS_ACTIVE])
86,->all();
86,"Both approaches execute the same set of SQL statements. The latter approach is much cleaner and drier, though."
86,"By default, joinWith() will use LEFT JOIN to join the primary table with the"
86,related table. You can specify a different join type (e.g. RIGHT JOIN) via its third parameter $joinType. If
86,"the join type you want is INNER JOIN, you can simply call innerJoinWith(), instead."
86,Calling joinWith() will eagerly load the related data by default.
86,"If you do not want to bring in the related data, you can specify its second parameter $eagerLoading as false."
86,Note: Even when using joinWith() or innerJoinWith()
86,with eager loading enabled the related data will not be populated from the result of the JOIN query. So there's
86,still an extra query for each joined relation as explained in the section on eager loading.
86,"Like with(), you can join with one or multiple relations; you may customize the relation"
86,queries on-the-fly; you may join with nested relations; and you may mix the use of with()
86,"and joinWith(). For example,"
86,$customers = Customer::find()->joinWith([
86,'orders' => function ($query) {
86,"$query->andWhere(['>', 'subtotal', 100]);"
86,])->with('country')
86,->all();
86,"Sometimes when joining two tables, you may need to specify some extra conditions in the ON part of the JOIN query."
86,This can be done by calling the yii\db\ActiveQuery::onCondition() method like the following:
86,// SELECT `customer`.* FROM `customer`
86,// LEFT JOIN `order` ON `order`.`customer_id` = `customer`.`id` AND `order`.`status` = 1
86,// SELECT * FROM `order` WHERE `customer_id` IN (...)
86,$customers = Customer::find()->joinWith([
86,'orders' => function ($query) {
86,$query->onCondition(['order.status' => Order::STATUS_ACTIVE]);
86,])->all();
86,"This above query brings back all customers, and for each customer it brings back all active orders."
86,Note that this differs from our earlier example which only brings back customers who have at least one active order.
86,"Info: When yii\db\ActiveQuery is specified with a condition via onCondition(),"
86,the condition will be put in the ON part if the query involves a JOIN query. If the query does not involve
86,"JOIN, the on-condition will be automatically appended to the WHERE part of the query."
86,Thus it may only contain conditions including columns of the related table.
86,Relation table aliases
86,"¶As noted before, when using JOIN in a query, we need to disambiguate column names. Therefore often an alias is"
86,defined for a table. Setting an alias for the relational query would be possible by customizing the relation query in the following way:
86,$query->joinWith([
86,'orders' => function ($q) {
86,$q->from(['o' => Order::tableName()]);
86,This however looks very complicated and involves either hardcoding the related objects table name or calling Order::tableName().
86,"Since version 2.0.7, Yii provides a shortcut for this. You may now define and use the alias for the relation table like the following:"
86,// join the orders relation and sort the result by orders.id
86,$query->joinWith(['orders o'])->orderBy('o.id');
86,The above syntax works for simple relations. If you need an alias for an intermediate table when joining over
86,"nested relations, e.g. $query->joinWith(['orders.product']),"
86,you need to nest the joinWith calls like in the following example:
86,$query->joinWith(['orders o' => function($q) {
86,$q->joinWith('product p');
86,}])
86,->where('o.amount > 100');
86,Inverse Relations
86,"¶Relation declarations are often reciprocal between two Active Record classes. For example, Customer is related"
86,"to Order via the orders relation, and Order is related back to Customer via the customer relation."
86,class Customer extends ActiveRecord
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,class Order extends ActiveRecord
86,public function getCustomer()
86,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
86,Now consider the following piece of code:
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::findOne(123);
86,// SELECT * FROM `order` WHERE `customer_id` = 123
86,$order = $customer->orders[0];
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer2 = $order->customer;
86,"// displays ""not the same"""
86,echo $customer2 === $customer ? 'same' : 'not the same';
86,"We would think $customer and $customer2 are the same, but they are not! Actually they do contain the same"
86,"customer data, but they are different objects. When accessing $order->customer, an extra SQL statement"
86,is executed to populate a new object $customer2.
86,"To avoid the redundant execution of the last SQL statement in the above example, we should tell Yii that"
86,customer is an inverse relation of orders by calling the inverseOf() method
86,like shown below:
86,class Customer extends ActiveRecord
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id'])->inverseOf('customer');"
86,"With this modified relation declaration, we will have:"
86,// SELECT * FROM `customer` WHERE `id` = 123
86,$customer = Customer::findOne(123);
86,// SELECT * FROM `order` WHERE `customer_id` = 123
86,$order = $customer->orders[0];
86,// No SQL will be executed
86,$customer2 = $order->customer;
86,"// displays ""same"""
86,echo $customer2 === $customer ? 'same' : 'not the same';
86,Note: Inverse relations cannot be defined for relations involving a junction table.
86,"That is, if a relation is defined with via() or viaTable(),"
86,you should not call inverseOf() further.
86,Saving Relations
86,"¶When working with relational data, you often need to establish relationships between different data or destroy"
86,"existing relationships. This requires setting proper values for the columns that define the relations. Using Active Record,"
86,you may end up writing the code like the following:
86,$customer = Customer::findOne(123);
86,$order = new Order();
86,$order->subtotal = 100;
86,// ...
86,"// setting the attribute that defines the ""customer"" relation in Order"
86,$order->customer_id = $customer->id;
86,$order->save();
86,Active Record provides the link() method that allows you to accomplish this task more nicely:
86,$customer = Customer::findOne(123);
86,$order = new Order();
86,$order->subtotal = 100;
86,// ...
86,"$order->link('customer', $customer);"
86,The link() method requires you to specify the relation name and the target Active Record
86,instance that the relationship should be established with. The method will modify the values of the attributes that
86,"link two Active Record instances and save them to the database. In the above example, it will set the customer_id"
86,attribute of the Order instance to be the value of the id attribute of the Customer instance and then save it
86,to the database.
86,Note: You cannot link two newly created Active Record instances.
86,The benefit of using link() is even more obvious when a relation is defined via
86,"a junction table. For example, you may use the following code to link an Order instance"
86,with an Item instance:
86,"$order->link('items', $item);"
86,The above code will automatically insert a row in the order_item junction table to relate the order with the item.
86,Info: The link() method will NOT perform any data validation while
86,saving the affected Active Record instance. It is your responsibility to validate any input data before
86,calling this method.
86,The opposite operation to link() is unlink()
86,"which breaks an existing relationship between two Active Record instances. For example,"
86,$customer = Customer::find()->with('orders')->where(['id' => 123])->one();
86,"$customer->unlink('orders', $customer->orders[0]);"
86,"By default, the unlink() method will set the foreign key value(s) that specify"
86,"the existing relationship to be null. You may, however, choose to delete the table row that contains the foreign key value"
86,by passing the $delete parameter as true to the method.
86,"When a junction table is involved in a relation, calling unlink() will cause"
86,"the foreign keys in the junction table to be cleared, or the deletion of the corresponding row in the junction table"
86,if $delete is true.
86,Cross-Database Relations
86,¶Active Record allows you to declare relations between Active Record classes that are powered by different databases.
86,"The databases can be of different types (e.g. MySQL and PostgreSQL, or MS SQL and MongoDB), and they can run on"
86,"different servers. You can use the same syntax to perform relational queries. For example,"
86,"// Customer is associated with the ""customer"" table in a relational database (e.g. MySQL)"
86,class Customer extends \yii\db\ActiveRecord
86,public static function tableName()
86,return 'customer';
86,public function getComments()
86,// a customer has many comments
86,"return $this->hasMany(Comment::class, ['customer_id' => 'id']);"
86,"// Comment is associated with the ""comment"" collection in a MongoDB database"
86,class Comment extends \yii\mongodb\ActiveRecord
86,public static function collectionName()
86,return 'comment';
86,public function getCustomer()
86,// a comment has one customer
86,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
86,$customers = Customer::find()->with('comments')->all();
86,You can use most of the relational query features that have been described in this section.
86,Note: Usage of joinWith() is limited to databases that allow cross-database JOIN queries.
86,"For this reason, you cannot use this method in the above example because MongoDB does not support JOIN."
86,Customizing Query Classes
86,"¶By default, all Active Record queries are supported by yii\db\ActiveQuery. To use a customized query class"
86,"in an Active Record class, you should override the yii\db\ActiveRecord::find() method and return an instance"
86,"of your customized query class. For example,"
86,// file Comment.php
86,namespace app\models;
86,use yii\db\ActiveRecord;
86,class Comment extends ActiveRecord
86,public static function find()
86,return new CommentQuery(get_called_class());
86,"Now whenever you are performing a query (e.g. find(), findOne()) or defining a relation (e.g. hasOne())"
86,"with Comment, you will be calling an instance of CommentQuery instead of ActiveQuery."
86,"You now have to define the CommentQuery class, which can be customized in many creative ways to improve your query building experience. For example,"
86,// file CommentQuery.php
86,namespace app\models;
86,use yii\db\ActiveQuery;
86,class CommentQuery extends ActiveQuery
86,// conditions appended by default (can be skipped)
86,public function init()
86,$this->andOnCondition(['deleted' => false]);
86,parent::init();
86,// ... add customized query methods here ...
86,public function active($state = true)
86,return $this->andOnCondition(['active' => $state]);
86,"Note: Instead of calling onCondition(), you usually should call"
86,andOnCondition() or orOnCondition()
86,to append additional conditions when defining new query building methods so that any existing conditions are not overwritten.
86,This allows you to write query building code like the following:
86,$comments = Comment::find()->active()->all();
86,$inactiveComments = Comment::find()->active(false)->all();
86,"Tip: In big projects, it is recommended that you use customized query classes to hold most query-related code"
86,so that the Active Record classes can be kept clean.
86,You can also use the new query building methods when defining relations about Comment or performing relational query:
86,class Customer extends \yii\db\ActiveRecord
86,public function getActiveComments()
86,"return $this->hasMany(Comment::class, ['customer_id' => 'id'])->active();"
86,$customers = Customer::find()->joinWith('activeComments')->all();
86,// or alternatively
86,class Customer extends \yii\db\ActiveRecord
86,public function getComments()
86,"return $this->hasMany(Comment::class, ['customer_id' => 'id']);"
86,$customers = Customer::find()->joinWith([
86,'comments' => function($q) {
86,$q->active();
86,])->all();
86,"Info: In Yii 1.1, there is a concept called scope. Scope is no longer directly supported in Yii 2.0,"
86,and you should use customized query classes and query methods to achieve the same goal.
86,"Selecting extra fields ¶When Active Record instance is populated from query results, its attributes are filled up by corresponding column"
86,values from received data set.
86,You are able to fetch additional columns or values from query and store it inside the Active Record.
86,"For example, assume we have a table named room, which contains information about rooms available in the hotel."
86,"Each room stores information about its geometrical size using fields length, width, height."
86,Imagine we need to retrieve list of all available rooms with their volume in descending order.
86,"So you can not calculate volume using PHP, because we need to sort the records by its value, but you also want volume"
86,to be displayed in the list.
86,"To achieve the goal, you need to declare an extra field in your Room Active Record class, which will store volume value:"
86,class Room extends \yii\db\ActiveRecord
86,public $volume;
86,// ...
86,"Then you need to compose a query, which calculates volume of the room and performs the sort:"
86,$rooms = Room::find()
86,->select([
86,"'{{room}}.*', // select all columns"
86,"'([[length]] * [[width]] * [[height]]) AS volume', // calculate a volume"
86,->orderBy('volume DESC') // apply sort
86,->all();
86,foreach ($rooms as $room) {
86,echo $room->volume; // contains value calculated by SQL
86,Ability to select extra fields can be exceptionally useful for aggregation queries.
86,Assume you need to display a list of customers with the count of orders they have made.
86,"First of all, you need to declare a Customer class with orders relation and extra field for count storage:"
86,class Customer extends \yii\db\ActiveRecord
86,public $ordersCount;
86,// ...
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,"Then you can compose a query, which joins the orders and calculates their count:"
86,$customers = Customer::find()
86,->select([
86,"'{{customer}}.*', // select all customer fields"
86,'COUNT({{order}}.id) AS ordersCount' // calculate orders count
86,->joinWith('orders') // ensure table junction
86,->groupBy('{{customer}}.id') // group the result to ensure aggregation function works
86,->all();
86,"A disadvantage of using this method would be that, if the information isn't loaded on the SQL query - it has to be calculated"
86,"separately. Thus, if you have found particular record via regular query without extra select statements, it"
86,will be unable to return actual value for the extra field. Same will happen for the newly saved record.
86,$room = new Room();
86,$room->length = 100;
86,$room->width = 50;
86,$room->height = 2;
86,"$room->volume; // this value will be `null`, since it was not declared yet"
86,Using the __get() and __set() magic methods
86,we can emulate the behavior of a property:
86,class Room extends \yii\db\ActiveRecord
86,private $_volume;
86,public function setVolume($volume)
86,$this->_volume = (float) $volume;
86,public function getVolume()
86,if (empty($this->length) || empty($this->width) || empty($this->height)) {
86,return null;
86,if ($this->_volume === null) {
86,$this->setVolume(
86,$this->length * $this->width * $this->height
86,return $this->_volume;
86,// ...
86,"When the select query doesn't provide the volume, the model will be able to calculate it automatically using"
86,the attributes of the model.
86,You can calculate the aggregation fields as well using defined relations:
86,class Customer extends \yii\db\ActiveRecord
86,private $_ordersCount;
86,public function setOrdersCount($count)
86,$this->_ordersCount = (int) $count;
86,public function getOrdersCount()
86,if ($this->isNewRecord) {
86,return null; // this avoid calling a query searching for null primary keys
86,if ($this->_ordersCount === null) {
86,$this->setOrdersCount($this->getOrders()->count()); // calculate aggregation on demand from relation
86,return $this->_ordersCount;
86,// ...
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,"With this code, in case 'ordersCount' is present in 'select' statement - Customer::ordersCount will be populated"
86,"by query results, otherwise it will be calculated on demand using Customer::orders relation."
86,"This approach can be as well used for creation of the shortcuts for some relational data, especially for the aggregation."
86,For example:
86,class Customer extends \yii\db\ActiveRecord
86,/**
86,* Defines read-only virtual property for aggregation data.
86,public function getOrdersCount()
86,if ($this->isNewRecord) {
86,return null; // this avoid calling a query searching for null primary keys
86,return empty($this->ordersAggregation) ? 0 : $this->ordersAggregation[0]['counted'];
86,/**
86,* Declares normal 'orders' relation.
86,public function getOrders()
86,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
86,/**
86,"* Declares new relation based on 'orders', which provides aggregation."
86,public function getOrdersAggregation()
86,return $this->getOrders()
86,"->select(['customer_id', 'counted' => 'count(*)'])"
86,->groupBy('customer_id')
86,->asArray(true);
86,// ...
86,foreach (Customer::find()->with('ordersAggregation')->all() as $customer) {
86,echo $customer->ordersCount; // outputs aggregation data from relation without extra query due to eager loading
86,$customer = Customer::findOne($pk);
86,$customer->ordersCount; // output aggregation data from lazy loaded relation
86,Query BuilderGo to Top Migrations
86,Found a typo or you think this page needs improvement?
86,Edit it on github !
86,User Contributed Notes 4
86,#20193
86,Yii ActiveRecord looks brilliantly simple and effective for drastically reducing the amount of custom SQL one has to write.
86,"It would be great to see more examples of using joins. For example, if using leftJoin(), what array/object structure is returned? Is ActiveRecord smart enough to nest the joined records underneath each parent record?"
86,Simon East at
86,"Apr 13, 2018, 5:17:49 AM"
86,#20194
86,"It would be great to see more examples of using joins. For example, if using leftJoin(), what array/object structure is returned? Is ActiveRecord smart enough to nest the joined records underneath each parent record?"
86,This is what you get when using with() or joinWith().
86,Lazy Loading and Eager Loading is about calling with() to fetch related records together with the primary query.
86,Joining with Relations is about calling joinWith() to join the table of a related record together with the primary query.
86,"The objects returned are the same but the difference is in which queries are executed in the background if you access a relation, e.g. $post->author."
86,CeBe at
86,"Apr 13, 2018, 6:07:50 AM"
86,#21079
86,"I think this page require a section may be titled as ""Updating Relations"" that shows standard ways to update relational data specially for Many to Many relationships via the junction table."
86,Said Bakr at
86,"Jul 30, 2022, 12:17:16 AM"
86,#21148
86,for anyone wondering how to use link()
86,"public function afterSave($insert, $changedAttributes)"
86,"parent::afterSave($insert, $changedAttributes);"
86,"$categories = Yii::$app->request->getBodyParam('categories', []);"
86,"$authors = Yii::$app->request->getBodyParam('authors', []);"
86,$categoryModels = Categories::findAll($categories);
86,$authorsModels = [];
86,foreach ($authors as $authorData) {
86,$author = Author::findOne(['name' => $authorData['name']]);
86,if (!$author) {
86,$author = new Author();
86,$author->name = $authorData['name'];
86,$author->save();
86,$authorsModels[] = $author;
86,if ($this->isNewRecord) {
86,"$this->link('categories', $categoryModels);"
86,"$this->link('authors', $authorsModels);"
86,} else {
86,"$this->unlinkAll('categories', true);"
86,"$this->unlinkAll('authors', true);"
86,foreach ($categoryModels as $categoryModel) {
86,"$this->link('categories', $categoryModel);"
86,foreach ($authorsModels as $authorModel) {
86,"$this->link('authors', $authorModel);"
86,This is Example Assuming you have Authors and Books and Categories Tables
86,"and Junction Tables (BookAuthors, BookCategories)"
86,I have predefined categories and I want to Link book with author in the author Table and if it isn't found in the Author table create it and then link it with junction table.
86,in this part:
86,if ($this->isNewRecord) {
86,"$this->link('categories', $categoryModels);"
86,"$this->link('authors', $authorsModels);"
86,I tried doing it without condition but it throws error didn't find any logical reason for that. if anyone knows why it would be helpful to let us know.
86,Hope someone find this helpful.
86,Saleh
86,Abuhussein at
86,"May 12, 2023, 1:20:50 PM"
86,Leave a comment
86,Signup or Login in order to comment.
86,About
86,About Yii
86,News
86,License
86,Contact Us
86,Downloads
86,Framework
86,Documentation
86,Extensions
86,Logo
86,Documentation
86,Guide
86,API
86,Wiki
86,Resources
86,Development
86,Contribute
86,Latest Updates
86,Report a Bug
86,Report Security Issue
86,Community
86,Forum
86,Live Chat
86,Facebook Group
86,Hall of Fame
86,Badges
86,Terms of service
86,License
86,Website Source Code
86,© 2008 - 2024 Yii
86,Design: Eshill
86,Terms of service
86,License
86,Website Source Code
86,© 2008 - 2024 Yii
86,Design: Eshill
86,Supported by
87,"How Do I Fix Replication Slot Errors?Fivetran documentationSearch the docs/Sign InGetting StartedCore ConceptsUsing FivetranUsage-Based PricingConnectorsApplicationsDatabasesConnection OptionsBigQueryConvexCosmos DBDb2 for iDocumentDBDynamoDBElasticsearchFirebaseMariaDBMongoDBMySQLOpenSearchOraclePlanetScalePostgreSQLSetup Guide - Generic PostgreSQLSetup Guide - Aurora PostgreSQLSetup Guide - Azure PostgreSQLSetup Guide - Google Cloud PostgreSQLSetup Guide - Heroku PostgreSQLSetup Guide - RDS PostgreSQLTroubleshootingConnection Attempt Failed with SSH ServerMigrating WAL pluginSunsetting test_decoding Update MethodConnector Not Capturing DeletesTables without Permission to SyncReplication Slot Timing OutFix Replication Slot ErrorsXMIN Connector Not SyncingConnection Limit ErrorReplica Canceled Query or Piping ErrorLogical Replication Connector Timed OutAurora Frozen RowsRequired System ColumnsReplication to BigQuery DelayCurrent Transaction ID ErrorSwitch From XMIN to Logical ReplicationDestination More Records than SourceNot Releasing WALLong Time to Execute QueryLast Tracked LSN ErrorEnsure Data Integrity When UpgradingReplication Slot Not FoundXMIN Frozen AlertConnection Attempt FailedRelease NotesRedshiftSAP ERPSnowflakeSQL ServerTroubleshootingRelease NotesFilesEventsFunctionsDestinationsBy RequestPartner-BuiltTransformationsLogsSecurityREST APIRelease NotesHVR 6 DocumentationHVR 5 DocumentationEdit on GitHubHow Do I Fix Replication Slot Errors?linkQuestionlinkHow to troubleshoot a stuck replication slot?EnvironmentlinkPostgreSQL connectorsRecommendationslinkA PostgreSQL replication slot may be stuck due to one of the following reasons:The replication slot returned no items in the 24-hour timeout Fivetran defines after requesting the WAL stream.The size of the replication slot is in hundreds of GB, approaching 1 TB.PostgreSQL cannot replay the WAL in memory and must write to disk before streaming the output to Fivetran. Learn more in AWS' Tuning memory parameters for Aurora PostgreSQL article. It's written for Aurora, but the concepts apply to all PostgreSQL-hosting platforms.Set max_slot_wal_keep_size to a value that is large enough to handle any bulk updates done on the source database, but small enough that a problem replication slot will not bloat and exceed the size of the disk. See the PostgreSQLCO.NF site for a more detailed description. We recommend a minimum of two days' worth of retention. See the section below on tuning max_wal_size for how to measure the amount of WAL generated in a given time.logical_decoding_work_mem specifies the maximum amount of memory to be used by logical decoding. Set this value high enough to allow PostgreSQL to hold the LSN range requested by Fivetran in memory. The default is 64MB, which is insufficient for high transaction production databases. Setting it to 4GB or higher helps to ensure PostgreSQL can replay transactions in the requested LSN range in memory.Check if the bgwriter (background writer) process is healthy by inspecting its stats. Execute the following query: SELECT * FROM pg_stat_bgwriter;.If the maxwritten_clean value is excessively high for your environment, increase the bgwriter_lru_maxpages parameter. In a well-tuned environment, the maxwritten_clean value should be 0.If the buffers_backend value is larger than the buffers_clean value, increase the bgwriter_lru_multiplier parameter and decrease the bgwriter_delay parameter.NOTE: The above condition may also indicate insufficient shared buffers. The hot part of your data is forced to travel between the RAM and disks.The buffers_backend_fsync value indicates if the backend was forced to make its own fsync requests to synchronize the buffers with storage. A value above 0 points to problems with the storage when the fsync queue is completely filled.If you change the values of any of the above parameters, reset the bgwriter stats. Execute the following query: pg_stat_reset_shared('bgwriter');. Check the stats again the next day.Check if the max_wal_size value is high enough to be rarely reached within your defined checkpoint_timeout window.For PostgreSQL versions 9.6 and below, do the following:Get LSN1. Execute: postgres=# SELECT pg_current_xlog_insert_location();.Wait for the length of checkpoint_timeout value.Get LSN2. Execute: postgres=# SELECT pg_current_xlog_insert_location();.Get the total amount of WAL written during the checkpoint period. Execute: postgres=# SELECT pg_xlog_location_diff('LSN2_VALUE', 'LSN1_VALUE');.Multiply the result from Step 4 by three.Collect a few data points. Repeat Steps 1 to 5 during a period of heavy database activity.Update max_wal_size to the result of Step 5.For PostgreSQL versions 9.7 and above, do the following:Get LSN1. Execute: postgres=# SELECT pg_current_wal_insert_lsn();.Wait for the length of checkpoint_timeout value.Get LSN2. Execute: postgres=# SELECT pg_current_wal_insert_lsn();.Get the total amount of WAL written during the checkpoint period. Execute: postgres=# SELECT pg_wal_lsn_diff('LSN2_VALUE', 'LSN1_VALUE');.Multiply the result from Step 4 by three.Collect a few data points. Repeat Steps 1 to 5 during a period of heavy database activity.Update max_wal_size to the result of Step 5.Increase wal_buffers. On very busy, high-core machines, it is useful to increase the wal_buffers value to as high as 128 MB. Consult your DBA before increasing this value because the buffer value depends on the database's hardware capacity. Fivetran recommends a minimum of 16 MB.If the above steps do not resolve the stuck replication slot, you must perform the following steps:Pause the connector.Drop the replication slot.Wait for a checkpoint (dependent on the checkpoint_timeout value).Recreate the replication slot.Ensure replication is in place. Execute: SELECT * FROM pg_logical_slot_peek_changes('slot_name', NULL, 1);.TIP: If the query fails, it indicates that replication is not in place.Go to the Setup tab of the connector, and click the Re-sync All Historical Data link.Unpause the connector.On this pageLoading...More from FivetranSet up connectorAsk the communityFivetran statusContact usThanks for your feedback!Was this page helpful?YesNoSubmitNeed help?Contact supportSign inCreate an accountChangelogRelease note RSSBlogCase studiesResource center©2024 Fivetran Inc."
88,How To Set Up MariaDB On Rocky Linux 8 | LinuxTeck
88,Skip to the content
88,Skip to content
88,Top Menu
88,"Mar 15, 2024"
88,About Us
88,FAQ
88,Write For Us
88,Facebook
88,Twitter
88,LinkedIn
88,Twitter
88,fa-youtube
88,Join to our facebook group
88,Home
88,Linux
88,RHEL-Centos-7
88,RHEL-CentOS-8
88,Rocky Linux
88,Ubuntu
88,LINUX COMMANDS
88,Cheat Sheets
88,Linux
88,Docker
88,Search for:
88,Main Menu
88,Rocky Linux
88,How to set up MariaDB on Rocky Linux 8
88,"Last updated on March 17th, 2023"
88,John Gomez
88,Leave a Comment
88,"There are several ways that we can install and maintain the databases, including commercial products like Microsoft SQL Server or Oracle. But with open-source software, there are often many, many choices for various platforms and operating systems."
88,"In this tutorial, I will show you how easily we can install and configure MariaDB on Rocky Linux in just a few steps. We will install the packages directly from the repository of Rocky Linux. This means that your database software is being kept up-to-date by package maintainers who are experts in maintaining their particular flavor of software. These maintainers also test updates thoroughly before making them available to users, so you don’t have to worry about whether or not a new version of your software is going to work correctly on your system."
88,"The MariaDB relational database management system is an open-source, multi-threaded system licensed under the GNU Public License (GPL), well-suited to replace MySQL databases. It is fast, scalable, and robust. In terms of performance, MariaDB is superior to MySQL. MariaDB (often called MySQL) because it has very similar features and functionality to MySQL. A typical implementation would be to replace the MySQL module in the widely used LAMP (Linux, Apache, MySQL, PHP) stack. Since the company was acquired by Oracle Corporation in 2009, the developers of the original project created MariaDB as a fork of MySQL."
88,"The MariaDB database is available in two editions: Community and Enterprise. Community editions are available to anyone for free, whereas Enterprise editions require a subscription. This article will show you how to install and configure the latest version of MariaDB Server on a Rocky Linux 8."
88,Note:
88,What are the main advantages of using MariaDB over MySQL?
88,"* Speed, Efficiency, Security, and Manageability."
88,"* The thread pool in MariaDB is capable of running faster and supporting up to 200,000+ connections."
88,"* PBXT, Aria, XtraDB, Maria, and FederatedX are some of the 12 new storage engines."
88,* The new version supports new commands such as WITH and KILL and is compatible with JSON.
88,"* Furthermore, MariaDB includes query-specific optimizations for disk access, joins, subqueries, derived tables and views,"
88,"execution control, and even explaining statements."
88,* And many more...
88,Prerequisites :
88,Operating System
88,Rocky Linux / RHEL /CentOS /Fedora
88,package
88,mariadb-server mariadb
88,User account
88,root user or user account with sudo privileges
88,Recommended to run all the administrative commands as with sudo privilege instead of root.
88,Difficulties in setting up sudo users? Click here to find the steps.
88,"For those new to Rocky Linux, you can check the Rocky Linux installation steps by visiting this link."
88,My Lab Setup :
88,"As part of the lab setup, I'm running MariaDB-Server on a Rocky Linux box."
88,MariaDB Server:
88,Operating System
88,Rocky Linux release 8.5 (Green Obsidian)
88,Hostname
88,db01.linuxteck
88,IP Address
88,192.168.1.100
88,Table of Contents
88,show
88,Step 1: Install MariaDB package
88,Step 2: Securing MariaDB Server
88,Conclusion:
88,Support My Work
88,Thank you for your endless support!
88,Step 1: Install MariaDB package
88,"As a best practice, you should update your operating system before installing the package."
88,$ sudo dnf update
88,Note:
88,"The MariaDB-server package can now be installed by executing the following command: However, the AppStream repository of Rocky Linux only contains a rather outdated version of MariaDB, which is 10.3. You can verify that with the following command in the terminal."
88,$ sudo dnf module list mariadb
88,Note:
88,"At the time of writing this article, we have version 10.9. However, the MariaDB Foundation does not recommend using the latest version for production use. In a production environment, version 10.6 is recommended, but in a development or test environment, you can install the latest version. Throughout this article, we will be using the stable version of MariaDB which is version 10.6. To install the customized version of MariaDB, you simply need to create a MariaDB repository and add the appropriate version manually to your system as follows: Here is the MariaDB 10.6 version."
88,$ sudo vi /etc/yum.repos.d/mariadb.repo
88,[mariadb]
88,name = MariaDB
88,baseurl = http://yum.mariadb.org/10.6/rhel8-amd64
88,module_hotfixes=1
88,gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
88,gpgcheck=1
88,Note:
88,"The above lines need to be added to the MariaDB.repo file. On the BASEURL line, you can see the version of the repository file we added. Likewise, you can adjust the version to suit your needs. Once the lines are added, you can save and exit the file."
88,"As soon as we add the new repository version, we must update the Rocky Linux repository before installing the Mariadb-Server."
88,$ sudo dnf update
88,"Following the update of the system, you can proceed to install the MariaDB server using the following command."
88,$ sudo dnf install mariadb-server mariadb
88,"After installation, the following command will show you how to start, enable, and check the status of mariadb in linux."
88,$ sudo systemctl start mariadb
88,$ sudo systemctl enable mariadb
88,$ sudo systemctl status mariadb
88,You can see from the screenshot below that MariaDB is running and active.
88,Step 2: Securing MariaDB Server
88,Note:
88,"The default configuration of MariaDB is less secure, which is highly problematic, meaning anybody can interact with the database. To prevent intruders or hackers, we must perform security upgrades/fixes. You can accomplish this by executing the built-in PERL application that comes with the MariaDB package. By running this script, you can add a layer of security while installing the Database server. Using the script we can change MariaDB's root password, remove anonymous user accounts, disable root logins within localhost's backyard, remove test databases, and reload privileges. Following the successful execution of the script, you can declare that you have added additional measures to safeguard MariaDB."
88,"Since version 10.4.6, MariaDB no longer uses the old Mysql prefix and replaces it with the new MariaDB-specific command. In other words, formerly we would have executed a PERL script as ""mysql_secure_installation"" but now we execute it as ""mariadb-secure-installation"". Since we are using MariaDB 10.6, we need a new command. If you want to find out what version of MariaDB you are using, use this command ""$ mariadb -V""."
88,$ sudo mariadb-secure-installation
88,Note:
88,"Once you have executed the above command, you will be prompted with the following series where you need to make a few changes to the MariaDB installation security options."
88,"1. To begin, you will be prompted to enter your current database's root password. Since this is a brand new installation and no root password has been set. To continue, you can press the enter key."
88,"2. You can now switch to unix_socket authentication by typing ""Y"" and pressing the enter key."
88,"3. If you wish to change the MariaDB root password, you should type ""Y"" and press enter, and then you should type in the new password (twice), and then press enter key to confirm the change."
88,"From here on, you will be asking a series of basic questions. For your convenience, we have provided the answers to the below screenshots. We recommend that you acknowledge ""YES"" for each of the following: As a result of the long output, I have split the screenshots into three parts. The first one shows how to set the root password."
88,"Following that, you will see the remaining steps."
88,Note:
88,"That's it. We have implemented the security measures successfully. You can now attempt to log in to MariaDB using the following command: By executing the below command, you will be prompted to type in the password you have previously set for the MySQL root account. Now enter that password and press enter to access your SQL shell."
88,$ mysql -u root -p
88,Note:
88,"It means that you are trying to access the MySQL database. The ""-u"" parameter specifies the MySQL user name, and the ""-p"" parameter indicates the password for the MySQL user."
88,Note:
88,"Those who are familiar with SQL administrative commands can use the SQL shell or use a MySQL client such as PHPMyAdmin, workbench, etc. It will enable you to manage your MySQL/MariaDB database easily. Here, for testing purposes, let's use one of the SQL commands in the SHELL prompt . For example, you can use the ""SHOW DATABASES"" command to see all of the databases available on MariaDB:"
88,More administrative SQL commands can be found here
88,Conclusion:
88,That's it. We hope this article has helped you understand how to Install MariaDB 10.6 Database Server step-by-step on Rocky Linux 8. Drop me your feedback/comments. Feel free to share this article with others if you like it.
88,Support My Work
88,"Thank you for your support and for being a part of my journey, I would be very grateful if you could consider buying me a coffee. The contributions you make will help me to continue to produce quality content and enhance my readers' experience."
88,Thank you for your endless support!
88,Related post
88,How to Secure Apache with SSL in Rocky Linux
88,"April 03,2023"
88,How to install Apache on Rocky Linux 9
88,"March 17,2023"
88,Install LAMP Stack on Rocky Linux 9 {Step by Step}
88,"March 17,2023"
88,Previous Article
88,How to set up an SFTP server on Rocky Linux 8
88,Next Article
88,How to Install and use phpMyAdmin on Rocky Linux
88,About John Gomez
88,John Britto Founder & Cheif-Editor @LinuxTeck. A Computer Geek and
88,Linux Intellectual having more than 10+ years of experience in Linux and Open Source technologies.
88,View all posts by John Gomez →
88,Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment * Name *
88,Email *
88,Website
88,"Save my name, email, and website in this browser for the next time I comment."
88,Trending
88,25 basic 'find' command to search files in Linux with Examples
88,How to install Apache on Rocky Linux 9
88,10 basic and most useful 'ssh' client commands in Linux
88,Linux Security Command Cheat Sheet
88,How to Install Rocky Linux 8.4 {Step by Step} with Screenshots
88,Linux Audio and Video Command Cheat Sheet
88,9 basic 'du' command in linux with practical examples
88,How to Secure Apache with SSL in Rocky Linux
88,15 basic useful firewall-cmd commands in Linux
88,Linux Compression and Archiving Command Cheat Sheet
88,Popular
88,Comments
88,Tags
88,25 basic 'find' command to search files in Linux with Examples
88,"Aug 26, 2019"
88,How to install Apache on Rocky Linux 9
88,"Mar 14, 2023"
88,John GomezHow to configure Two Node High Availability Cluster On RHEL/CentOS/RockyLinux
88,John GomezHow to install Apache on Rocky Linux 9No tags to show
88,First name or full name
88,Email
88,I want to receive a newsletters
88,Contact US
88,Privacy Policy
88,Terms of use
88,Copyright © 2024 LinuxTeck.
88,All Rights Reserved.
88,Material from our website cannot be republished ONLINE or OFFLINE without our
88,permission.
89,Configuring Infinispan caches
89,Configuring Infinispan caches
89,Table of Contents
89,1. Infinispan caches
89,1.1. Cache API
89,1.2. Cache Managers
89,1.3. Cache modes
89,1.3.1. Comparison of cache modes
89,1.4. Local caches
89,1.4.1. Simple caches
89,2. Clustered caches
89,2.1. Replicated caches
89,2.2. Distributed caches
89,2.2.1. Read consistency
89,2.2.2. Key ownership
89,2.2.3. Capacity factors
89,Zero capacity nodes
89,2.2.4. Level one (L1) caches
89,2.2.5. Server hinting
89,2.2.6. Key affinity service
89,2.2.7. Grouping API
89,2.3. Invalidation caches
89,2.4. Scattered caches
89,2.5. Asynchronous replication
89,2.5.1. Return values with asynchronous replication
89,2.6. Configuring initial cluster size
89,3. Infinispan cache configuration
89,3.1. Declarative cache configuration
89,3.1.1. Cache configuration
89,3.2. Adding cache templates
89,3.2.1. Creating caches from templates
89,3.2.2. Cache template inheritance
89,3.2.3. Cache template wildcards
89,3.2.4. Cache templates from multiple XML files
89,3.3. Creating remote caches
89,3.3.1. Default Cache Manager
89,3.3.2. Creating caches with Infinispan Console
89,3.3.3. Creating remote caches with the Infinispan CLI
89,3.3.4. Creating remote caches from Hot Rod clients
89,3.3.5. Creating remote caches with the REST API
89,3.4. Creating embedded caches
89,3.4.1. Adding Infinispan to your project
89,3.4.2. Creating and using embedded caches
89,3.4.3. Cache API
89,AdvancedCache API
89,Flags
89,Asynchronous API
89,Why use such an API?
89,Which processes actually happen asynchronously?
89,4. Enabling and configuring Infinispan statistics and JMX monitoring
89,4.1. Enabling statistics in embedded caches
89,4.2. Enabling statistics in remote caches
89,4.3. Enabling Hot Rod client statistics
89,4.4. Configuring Infinispan metrics
89,4.5. Registering JMX MBeans
89,4.5.1. Enabling JMX remote ports
89,4.5.2. Infinispan MBeans
89,4.5.3. Registering MBeans in custom MBean servers
89,4.6. Exporting metrics during a state transfer operation
89,4.7. Monitoring the status of cross-site replication
89,5. Configuring JVM memory usage
89,5.1. Default memory configuration
89,5.2. Eviction and expiration
89,5.3. Eviction with Infinispan caches
89,5.3.1. Eviction strategies
89,5.3.2. Configuring maximum count eviction
89,5.3.3. Configuring maximum size eviction
89,5.3.4. Manual eviction
89,5.3.5. Passivation with eviction
89,5.4. Expiration with lifespan and maximum idle
89,5.4.1. How expiration works
89,5.4.2. Expiration reaper
89,5.4.3. Maximum idle and clustered caches
89,5.4.4. Configuring lifespan and maximum idle times for caches
89,5.4.5. Configuring lifespan and maximum idle times per entry
89,5.5. JVM heap and off-heap memory
89,5.5.1. Off-heap data storage
89,5.5.2. Configuring off-heap memory
89,6. Configuring persistent storage
89,6.1. Passivation
89,6.1.1. How passivation works
89,6.2. Write-through cache stores
89,6.3. Write-behind cache stores
89,6.4. Segmented cache stores
89,6.5. Shared cache stores
89,6.6. Transactions with persistent cache stores
89,6.7. Global persistent location
89,6.7.1. Configuring the global persistent location
89,6.8. File-based cache stores
89,6.8.1. Configuring file-based cache stores
89,6.8.2. Configuring single file cache stores
89,6.9. JDBC connection factories
89,6.9.1. Configuring managed datasources
89,Configuring caches with JNDI names
89,Connection pool tuning properties
89,6.9.2. Configuring JDBC connection pools with Agroal properties
89,6.10. SQL cache stores
89,6.10.1. Data types for keys and values
89,Composite keys and values
89,Embedded keys
89,SQL types to Protobuf types
89,6.10.2. Loading Infinispan caches from database tables
89,6.10.3. Using SQL queries to load data and perform operations
89,SQL query store configuration
89,6.10.4. SQL cache store troubleshooting
89,6.11. JDBC string-based cache stores
89,6.11.1. Configuring JDBC string-based cache stores
89,6.12. RocksDB cache stores
89,6.13. Remote cache stores
89,6.14. Cluster cache loaders
89,6.15. Creating custom cache store implementations
89,6.15.1. Infinispan Persistence SPI
89,6.15.2. Creating cache stores
89,6.15.3. Examples of custom cache store configuration
89,6.15.4. Deploying custom cache stores
89,6.16. Migrating data between cache stores
89,6.16.1. Cache store migrator
89,6.16.2. Getting the cache store migrator
89,6.16.3. Configuring the cache store migrator
89,Configuration properties for the cache store migrator
89,6.16.4. Migrating Infinispan cache stores
89,7. Configuring Infinispan to handle network partitions
89,7.1. Split clusters and network partitions
89,7.1.1. Data consistency in a split cluster
89,7.2. Cache availability and degraded mode
89,7.2.1. Degraded cache recovery example
89,7.2.2. Verifying cache availability during network partitions
89,7.2.3. Making caches available
89,7.3. Configuring partition handling
89,7.4. Partition handling strategies
89,7.5. Merge policies
89,7.6. Configuring custom merge policies
89,7.7. Manually merging partitions in embedded caches
89,8. Security authorization with role-based access control
89,8.1. Infinispan user roles and permissions
89,8.1.1. Permissions
89,8.1.2. Role and permission mappers
89,Mapping users to roles and permissions in Infinispan
89,8.1.3. Configuring role mappers
89,8.2. Configuring caches with security authorization
89,9. Configuring transactions
89,9.1. Transactions
89,9.1.1. Configuring transactions
89,9.1.2. Isolation levels
89,9.1.3. Transaction locking
89,Pessimistic transactional cache
89,Optimistic transactional cache
89,What do I need - pessimistic or optimistic transactions?
89,9.1.4. Write Skews
89,Forcing write locks on keys in pessimitic transactions
89,9.1.5. Dealing with exceptions
89,9.1.6. Enlisting Synchronizations
89,9.1.7. Batching
89,API
89,Batching and JTA
89,9.1.8. Transaction recovery
89,When to use recovery
89,How does it work
89,Configuring recovery
89,Enable JMX support
89,Recovery cache
89,Integration with the transaction manager
89,Reconciliation
89,Force commit/rollback based on XID
89,10. Configuring locking and concurrency
89,10.1. Locking and concurrency
89,10.1.1. Clustered caches and locks
89,10.1.2. The LockManager
89,10.1.3. Lock striping
89,10.1.4. Concurrency levels
89,10.1.5. Lock timeout
89,10.1.6. Consistency
89,10.1.7. Data Versioning
89,11. Using clustered counters
89,11.1. Clustered Counters
89,11.1.1. Installation and Configuration
89,List counter names
89,11.1.2. CounterManager interface
89,Remove a counter via CounterManager
89,11.1.3. The Counter
89,The StrongCounter interface: when the consistency or bounds matters.
89,Bounded StrongCounter
89,Uses cases
89,Usage Examples
89,The WeakCounter interface: when speed is needed
89,Weak Counter Interface
89,Uses cases
89,Examples
89,11.1.4. Notifications and Events
89,12. Listeners and notifications
89,12.1. Listeners and notifications
89,12.2. Cache-level notifications
89,12.3. Cache Manager notifications
89,12.4. Synchronicity of events
89,Create and configure Infinispan caches with the mode and capabilities that suit your application requirements.
89,You can configure caches with expiration to remove stale entries or use eviction to control cache size.
89,"You can also add persistent storage to caches, enable partition handling for clustered caches, set up transactions, and more."
89,1. Infinispan caches
89,"Infinispan caches provide flexible, in-memory data stores that you can configure to suit use cases such as:"
89,Boosting application performance with high-speed local caches.
89,Optimizing databases by decreasing the volume of write operations.
89,Providing resiliency and durability for consistent data across clusters.
89,1.1. Cache API
89,"Cache<K,V> is the central interface for Infinispan and extends java.util.concurrent.ConcurrentMap."
89,"Cache entries are highly concurrent data structures in key:value format that support a wide and configurable range of data types, from simple strings to much more complex objects."
89,1.2. Cache Managers
89,The CacheManager API is the entry point for interacting with Infinispan.
89,"Cache Managers control cache lifecycle; creating, modifying, and deleting cache instances."
89,Cache Managers also provide cluster management and monitoring along with the ability to execute code across nodes.
89,Infinispan provides two CacheManager implementations:
89,EmbeddedCacheManager
89,Entry point for caches when running Infinispan inside the same Java Virtual Machine (JVM) as the client application.
89,RemoteCacheManager
89,Entry point for caches when running Infinispan Server in its own JVM. When you instantiate a RemoteCacheManager it establishes a persistent TCP connection to Infinispan Server through the Hot Rod endpoint.
89,Both embedded and remote CacheManager implementations share some methods and properties.
89,"However, semantic differences do exist between EmbeddedCacheManager and RemoteCacheManager."
89,1.3. Cache modes
89,Infinispan Cache Managers can create and control multiple caches that use
89,"different modes. For example, you can use the same Cache Manager for local"
89,"caches, distributed caches, and caches with invalidation mode."
89,Local
89,Infinispan runs as a single node and never replicates read or write operations on cache entries.
89,Replicated
89,Infinispan replicates all cache entries on all nodes in a cluster and performs local read operations only.
89,Distributed
89,Infinispan replicates cache entries on a subset of nodes in a cluster and assigns entries to fixed owner nodes.
89,Infinispan requests read operations from owner nodes to ensure it returns the correct value.
89,Invalidation
89,Infinispan evicts stale data from all nodes whenever operations modify entries in the cache. Infinispan performs local read operations only.
89,Scattered
89,Infinispan stores cache entries across a subset of nodes.
89,By default Infinispan assigns a primary owner and a backup owner to each cache entry in scattered caches.
89,"Infinispan assigns primary owners in the same way as with distributed caches, while backup owners are always the nodes that initiate the write operations."
89,Infinispan requests read operations from at least one owner node to ensure it returns the correct value.
89,1.3.1. Comparison of cache modes
89,The cache mode that you should choose depends on the qualities and guarantees you need for your data.
89,The following table summarizes the primary differences between cache modes:
89,Simple
89,Local
89,Invalidation
89,Replicated
89,Distributed
89,Scattered
89,Clustered
89,Yes
89,Yes
89,Yes
89,Yes
89,Read performance
89,Highest
89,(local)
89,High
89,(local)
89,High
89,(local)
89,High
89,(local)
89,Medium
89,(owners)
89,Medium
89,(primary)
89,Write performance
89,Highest
89,(local)
89,High
89,(local)
89,Low
89,"(all nodes, no data)"
89,Lowest
89,(all nodes)
89,Medium
89,(owner nodes)
89,Higher
89,(single RPC)
89,Capacity
89,Single node
89,Single node
89,Single node
89,Smallest node
89,Cluster
89,"\$(sum_(i=1)^""nodes""""node_capacity"")/""owners""\$"
89,Cluster
89,"\$(sum_(i=1)^""nodes""""node_capacity"")/""2""\$"
89,Availability
89,Single node
89,Single node
89,Single node
89,All nodes
89,Owner nodes
89,Owner nodes
89,Features
89,"No TX, persistence, indexing"
89,All
89,No indexing
89,All
89,All
89,No TX
89,1.4. Local caches
89,Infinispan offers a local cache mode that is similar to a ConcurrentHashMap.
89,"Caches offer more capabilities than simple maps, including write-through and"
89,write-behind to persistent storage as well as management capabilities such as eviction and expiration.
89,"The Infinispan Cache API extends the ConcurrentMap API in Java, making it easy to migrate from a map to a Infinispan cache."
89,Local cache configuration
89,XML
89,"<local-cache name=""mycache"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,</local-cache>
89,JSON
89,"""local-cache"": {"
89,"""name"": ""mycache"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,YAML
89,localCache:
89,"name: ""mycache"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,1.4.1. Simple caches
89,A simple cache is a type of local cache that disables support for the following capabilities:
89,Transactions and invocation batching
89,Persistent storage
89,Custom interceptors
89,Indexing
89,Transcoding
89,"However, you can use other Infinispan capabilities with simple caches such as expiration, eviction, statistics, and security features."
89,"If you configure a capability that is not compatible with a simple cache, Infinispan throws an exception."
89,Simple cache configuration
89,XML
89,"<local-cache simple-cache=""true"" />"
89,JSON
89,"""local-cache"" : {"
89,"""simple-cache"" : ""true"""
89,YAML
89,localCache:
89,"simpleCache: ""true"""
89,2. Clustered caches
89,You can create embedded and remote caches on Infinispan clusters that replicate data across nodes.
89,2.1. Replicated caches
89,Infinispan replicates all entries in the cache to all nodes in the cluster.
89,Each node can perform read operations locally.
89,"Replicated caches provide a quick and easy way to share state across a cluster, but is suitable for clusters of less than ten nodes."
89,"Because the number of replication requests scales linearly with the number of nodes in the cluster, using replicated caches with larger clusters reduces performance."
89,However you can use UDP multicasting for replication requests to improve performance.
89,"Each key has a primary owner, which serializes data container updates in order to provide consistency."
89,Figure 1. Replicated cache
89,Synchronous or asynchronous replication
89,"Synchronous replication blocks the caller (e.g. on a cache.put(key, value)) until the modifications have been replicated successfully to all the nodes in the cluster."
89,"Asynchronous replication performs replication in the background, and write operations return immediately."
89,"Asynchronous replication is not recommended, because communication errors, or errors that happen on remote nodes are not reported to the caller."
89,Transactions
89,"If transactions are enabled, write operations are not replicated through the primary owner."
89,"With pessimistic locking, each write triggers a lock message, which is broadcast to all the nodes."
89,"During transaction commit, the originator broadcasts a one-phase prepare message and an unlock message (optional)."
89,Either the one-phase prepare or the unlock message is fire-and-forget.
89,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
89,"Again, either the one-phase prepare or the unlock message is fire-and-forget."
89,2.2. Distributed caches
89,"Infinispan attempts to keep a fixed number of copies of any entry in the cache,"
89,configured as numOwners.
89,"This allows distributed caches to scale linearly, storing more data as nodes are added to the cluster."
89,"As nodes join and leave the cluster, there will be times when a key has more or less than numOwners copies."
89,"In particular, if numOwners nodes leave in quick succession, some entries will be lost, so we say that a distributed cache tolerates numOwners - 1 node failures."
89,The number of copies represents a trade-off between performance and durability of data.
89,"The more copies you maintain, the lower performance will be, but also the lower the risk of losing data due to server or network failures."
89,"Infinispan splits the owners of a key into one primary owner, which coordinates writes to the key, and zero or more backup owners."
89,The following diagram shows a write operation that a client sends to a backup owner.
89,"In this case the backup node forwards the write to the primary owner, which then replicates the write to the backup."
89,Figure 2. Cluster replication
89,Figure 3. Distributed cache
89,Read operations
89,Read operations request the value from the primary owner.
89,"If the primary owner does not respond in a reasonable amount of time, Infinispan requests the value from the backup owners as well."
89,"A read operation may require 0 messages if the key is present in the local cache, or up to 2 * numOwners messages if all the owners are slow."
89,Write operations
89,Write operations result in at most 2 * numOwners messages.
89,One message from the originator to the primary owner and numOwners - 1 messages from the primary to the backup nodes along with the corresponding acknowledgment messages.
89,Cache topology changes may cause retries and additional messages for both read and write operations.
89,Synchronous or asynchronous replication
89,Asynchronous replication is not recommended because it can lose updates.
89,"In addition to losing updates, asynchronous distributed caches can also see a stale value when a thread writes to a key and then immediately reads the same key."
89,Transactions
89,"Transactional distributed caches send lock/prepare/commit/unlock messages to the affected nodes only, meaning all nodes that own at least one key affected by the transaction."
89,"As an optimization, if the transaction writes to a single key and the originator is the primary owner of the key, lock messages are not replicated."
89,2.2.1. Read consistency
89,"Even with synchronous replication, distributed caches are not linearizable."
89,"For transactional caches, they do not support serialization/snapshot isolation."
89,"For example, a thread is carrying out a single put request:"
89,cache.get(k) -> v1
89,"cache.put(k, v2)"
89,cache.get(k) -> v2
89,But another thread might see the values in a different order:
89,cache.get(k) -> v2
89,cache.get(k) -> v1
89,"The reason is that read can return the value from any owner, depending on how fast the primary owner replies."
89,The write is not atomic across all the owners.
89,"In fact, the primary commits the update only after it receives a confirmation from the backup."
89,"While the primary is waiting for the confirmation message from the backup, reads from the backup will see the new value, but reads from the primary will see the old one."
89,2.2.2. Key ownership
89,Distributed caches split entries into a fixed number of segments and assign
89,each segment to a list of owner nodes.
89,"Replicated caches do the same, with the exception that every node is an owner."
89,The first node in the list of owners is the primary owner.
89,The other nodes in the list are backup owners.
89,"When the cache topology changes, because a node joins or leaves the cluster, the segment ownership table is broadcast to every node."
89,This allows nodes to locate keys without making multicast requests or maintaining metadata for each key.
89,The numSegments property configures the number of segments available.
89,"However, the number of segments cannot change unless the cluster is restarted."
89,Likewise the key-to-segment mapping cannot change.
89,Keys must always map to the same segments regardless of cluster topology changes.
89,It is important that the key-to-segment mapping evenly distributes the number of segments allocated to each node while minimizing the number of segments that must move when the cluster topology changes.
89,Consistent hash factory implementation
89,Description
89,SyncConsistentHashFactory
89,Uses an algorithm based on consistent hashing. Selected by default when server hinting is disabled.
89,This implementation always assigns keys to the same nodes in every cache as
89,"long as the cluster is symmetric. In other words, all caches run on all nodes."
89,This implementation does have some negative points in that the load distribution is slightly uneven. It also moves more segments than strictly necessary on a join or leave.
89,TopologyAwareSyncConsistentHashFactory
89,Equivalent to SyncConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners. This is the default consistent hashing implementation with server hinting.
89,DefaultConsistentHashFactory
89,"Achieves a more even distribution than SyncConsistentHashFactory, but with one disadvantage. The order in which nodes join the cluster determines which nodes own which segments. As a result, keys might be assigned to different nodes in different caches."
89,TopologyAwareConsistentHashFactory
89,Equivalent to DefaultConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners.
89,ReplicatedConsistentHashFactory
89,Used internally to implement replicated caches. You should never explicitly
89,select this algorithm in a distributed cache.
89,Hashing configuration
89,"You can configure ConsistentHashFactory implementations, including custom ones, with embedded caches only."
89,XML
89,"<distributed-cache name=""distributedCache"""
89,"owners=""2"""
89,"segments=""100"""
89,"capacity-factor=""2"" />"
89,ConfigurationBuilder
89,Configuration c = new ConfigurationBuilder()
89,.clustering()
89,.cacheMode(CacheMode.DIST_SYNC)
89,.hash()
89,.numOwners(2)
89,.numSegments(100)
89,.capacityFactor(2)
89,.build();
89,Additional resources
89,KeyPartitioner
89,2.2.3. Capacity factors
89,Capacity factors allocate the number of segments based on resources available to each node in the cluster.
89,The capacity factor for a node applies to segments for which that node is both the primary owner and backup owner.
89,"In other words, the capacity factor specifies is the total capacity that a node has in comparison to other nodes in the cluster."
89,The default value is 1 which means that all nodes in the cluster have an equal capacity and Infinispan allocates the same number of segments to all nodes in the cluster.
89,"However, if nodes have different amounts of memory available to them, you can configure the capacity factor so that the Infinispan hashing algorithm assigns each node a number of segments weighted by its capacity."
89,The value for the capacity factor configuration must be a positive number and can be a fraction such as 1.5.
89,You can also configure a capacity factor of 0 but is recommended only for nodes that join the cluster temporarily and should use the zero capacity configuration instead.
89,Zero capacity nodes
89,"You can configure nodes where the capacity factor is 0 for every cache, user defined caches, and internal caches."
89,"When defining a zero capacity node, the node does not hold any data."
89,Zero capacity node configuration
89,XML
89,<infinispan>
89,"<cache-container zero-capacity-node=""true"" />"
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""zero-capacity-node"" : ""true"""
89,YAML
89,infinispan:
89,cacheContainer:
89,"zeroCapacityNode: ""true"""
89,ConfigurationBuilder
89,new GlobalConfigurationBuilder().zeroCapacityNode(true);
89,2.2.4. Level one (L1) caches
89,Infinispan nodes create local replicas when they retrieve entries from another node in the cluster.
89,L1 caches avoid repeatedly looking up entries on primary owner nodes and adds performance.
89,The following diagram illustrates how L1 caches work:
89,Figure 4. L1 cache
89,"In the ""L1 cache"" diagram:"
89,A client invokes cache.get() to read an entry for which another node in the cluster is the primary owner.
89,The originator node forwards the read operation to the primary owner.
89,The primary owner returns the key/value entry.
89,The originator node creates a local copy.
89,Subsequent cache.get() invocations return the local entry instead of forwarding to the primary owner.
89,L1 caching performance
89,Enabling L1 improves performance for read operations but requires primary owner nodes to broadcast invalidation messages when entries are modified.
89,This ensures that Infinispan removes any out of date replicas across the cluster.
89,"However this also decreases performance of write operations and increases memory usage, reducing overall capacity of caches."
89,"Infinispan evicts and expires local replicas, or L1 entries, like any other cache entry."
89,L1 cache configuration
89,XML
89,"<distributed-cache l1-lifespan=""5000"""
89,"l1-cleanup-interval=""60000"">"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""l1-lifespan"": ""5000"","
89,"""l1-cleanup-interval"": ""60000"""
89,YAML
89,distributedCache:
89,"l1Lifespan: ""5000"""
89,"l1-cleanup-interval: ""60000"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
89,.l1()
89,".lifespan(5000, TimeUnit.MILLISECONDS)"
89,".cleanupTaskFrequency(60000, TimeUnit.MILLISECONDS);"
89,2.2.5. Server hinting
89,"Server hinting increases availability of data in distributed caches by replicating entries across as many servers, racks, and data centers as possible."
89,Server hinting applies only to distributed caches.
89,"When Infinispan distributes the copies of your data, it follows the order of precedence: site, rack, machine, and node."
89,All of the configuration attributes are optional.
89,"For example, when you specify only the rack IDs, then Infinispan distributes the copies across different racks and nodes."
89,Server hinting can impact cluster rebalancing operations by moving more segments than necessary if the number of segments for the cache is too low.
89,An alternative for clusters in multiple data centers is cross-site replication.
89,Server hinting configuration
89,XML
89,<cache-container>
89,"<transport cluster=""MyCluster"""
89,"machine=""LinuxServer01"""
89,"rack=""Rack01"""
89,"site=""US-WestCoast""/>"
89,</cache-container>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""transport"" : {"
89,"""cluster"" : ""MyCluster"","
89,"""machine"" : ""LinuxServer01"","
89,"""rack"" : ""Rack01"","
89,"""site"" : ""US-WestCoast"""
89,YAML
89,cacheContainer:
89,transport:
89,"cluster: ""MyCluster"""
89,"machine: ""LinuxServer01"""
89,"rack: ""Rack01"""
89,"site: ""US-WestCoast"""
89,GlobalConfigurationBuilder
89,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder()
89,.transport()
89,".clusterName(""MyCluster"")"
89,".machineId(""LinuxServer01"")"
89,".rackId(""Rack01"")"
89,".siteId(""US-WestCoast"");"
89,Additional resources
89,org.infinispan.configuration.global.TransportConfigurationBuilder
89,2.2.6. Key affinity service
89,"In a distributed cache, a key is allocated to a list of nodes with an opaque algorithm."
89,There is no easy way to reverse the computation and generate a key that maps to a particular node.
89,"However, Infinispan can generate a sequence of (pseudo-)random keys, see what their primary owner is, and hand them out to the application when it needs a key mapping to a particular node."
89,Following code snippet depicts how a reference to this service can be obtained and used.
89,// 1. Obtain a reference to a cache
89,Cache cache = ...
89,Address address = cache.getCacheManager().getAddress();
89,// 2. Create the affinity service
89,KeyAffinityService keyAffinityService = KeyAffinityServiceFactory.newLocalKeyAffinityService(
89,"cache,"
89,"new RndKeyGenerator(),"
89,"Executors.newSingleThreadExecutor(),"
89,100);
89,// 3. Obtain a key for which the local node is the primary owner
89,Object localKey = keyAffinityService.getKeyForAddress(address);
89,// 4. Insert the key in the cache
89,"cache.put(localKey, ""yourValue"");"
89,The service is started at step 2: after this point it uses the supplied Executor to generate and queue keys.
89,"At step 3, we obtain a key from the service, and at step 4 we use it."
89,Lifecycle
89,"KeyAffinityService extends Lifecycle, which allows stopping and (re)starting it:"
89,public interface Lifecycle {
89,void start();
89,void stop();
89,The service is instantiated through KeyAffinityServiceFactory.
89,"All the factory methods have an Executor parameter, that is used for asynchronous key generation (so that it"
89,won’t happen in the caller’s thread).
89,It is the user’s responsibility to handle the shutdown of this Executor.
89,"The KeyAffinityService, once started, needs to be explicitly stopped."
89,This stops the background key generation and releases other held resources.
89,The only situation in which KeyAffinityService stops by itself is when the Cache Manager with which it was registered is shutdown.
89,Topology changes
89,"When the cache topology changes, the ownership of the keys generated by the KeyAffinityService might change."
89,"The key affinity service keep tracks of these topology changes and doesn’t return keys that would currently map to a different node, but it won’t do anything about keys generated earlier."
89,"As such, applications should treat KeyAffinityService purely as an optimization, and they should not rely on the location of a generated key for correctness."
89,"In particular, applications should not rely on keys generated by KeyAffinityService for the same address to always be located together."
89,Collocation of keys is only provided by the Grouping API.
89,2.2.7. Grouping API
89,"Complementary to the Key affinity service, the Grouping API allows you to co-locate a group of entries on the same nodes, but without being able to select the actual nodes."
89,"By default, the segment of a key is computed using the key’s hashCode()."
89,"If you use the Grouping API, Infinispan will compute the segment of the group and use that as the segment of the key."
89,"When the Grouping API is in use, it is important that every node can still compute the owners of every key without contacting other nodes."
89,"For this reason, the group cannot be specified manually."
89,The group can either be intrinsic to the entry (generated by the key class) or extrinsic (generated by an external function).
89,"To use the Grouping API, you must enable groups."
89,Configuration c = new ConfigurationBuilder()
89,.clustering().hash().groups().enabled()
89,.build();
89,<distributed-cache>
89,"<groups enabled=""true""/>"
89,</distributed-cache>
89,"If you have control of the key class (you can alter the class definition, it’s not part of an unmodifiable library), then we recommend using an intrinsic group."
89,"The intrinsic group is specified by adding the @Group annotation to a method, for example:"
89,class User {
89,...
89,String office;
89,...
89,public int hashCode() {
89,"// Defines the hash for the key, normally used to determine location"
89,...
89,// Override the location by specifying a group
89,// All keys in the same group end up with the same owners
89,@Group
89,public String getOffice() {
89,return office;
89,The group method must return a String
89,"If you don’t have control over the key class, or the determination of the group is an orthogonal concern to the key class, we recommend using an extrinsic group."
89,An extrinsic group is specified by implementing the Grouper interface.
89,public interface Grouper<T> {
89,"String computeGroup(T key, String group);"
89,Class<T> getKeyType();
89,"If multiple Grouper classes are configured for the same key type, all of them will be called, receiving the value computed by the previous one."
89,"If the key class also has a @Group annotation, the first Grouper will receive the group computed by the annotated method."
89,This allows you even greater control over the group when using an intrinsic group.
89,Example Grouper implementation
89,public class KXGrouper implements Grouper<String> {
89,"// The pattern requires a String key, of length 2, where the first character is"
89,"// ""k"" and the second character is a digit. We take that digit, and perform"
89,"// modular arithmetic on it to assign it to group ""0"" or group ""1""."
89,"private static Pattern kPattern = Pattern.compile(""(^k)(<a>\\d</a>)$"");"
89,"public String computeGroup(String key, String group) {"
89,Matcher matcher = kPattern.matcher(key);
89,if (matcher.matches()) {
89,"String g = Integer.parseInt(matcher.group(2)) % 2 + """";"
89,return g;
89,} else {
89,return null;
89,public Class<String> getKeyType() {
89,return String.class;
89,Grouper implementations must be registered explicitly in the cache configuration.
89,If you are configuring Infinispan programmatically:
89,Configuration c = new ConfigurationBuilder()
89,.clustering().hash().groups().enabled().addGrouper(new KXGrouper())
89,.build();
89,"Or, if you are using XML:"
89,<distributed-cache>
89,"<groups enabled=""true"">"
89,"<grouper class=""com.example.KXGrouper"" />"
89,</groups>
89,</distributed-cache>
89,Advanced API
89,AdvancedCache has two group-specific methods:
89,getGroup(groupName) retrieves all keys in the cache that belong to a group.
89,removeGroup(groupName) removes all the keys in the cache that belong to a group.
89,"Both methods iterate over the entire data container and store (if present), so they can be slow when a cache contains lots of small groups."
89,2.3. Invalidation caches
89,Invalidation cache mode in Infinispan is designed to optimize systems that perform high volumes of read operations to a shared permanent data store.
89,You can use invalidation mode to reduce the number of database writes when state changes occur.
89,Invalidation cache mode is deprecated for Infinispan remote deployments.
89,Use invalidation cache mode with embedded caches that are stored in shared cache stores only.
89,"Invalidation cache mode is effective only when you have a permanent data store, such as a database, and are only using Infinispan as an optimization in a read-heavy system to prevent hitting the database for every read."
89,"When a cache is configured for invalidation, each data change in a cache triggers a message to other caches in the cluster, informing them that their data is now stale and should be removed from memory."
89,Invalidation messages remove stale values from other nodes' memory.
89,"The messages are very small compared to replicating the entire value, and also other caches in the cluster look up modified data in a lazy manner, only when needed."
89,The update to the shared store is typically handled by user application code or Hibernate.
89,Figure 5. Invalidation cache
89,"Sometimes the application reads a value from the external store and wants to write it to the local cache, without removing it from the other nodes."
89,"To do this, it must call Cache.putForExternalRead(key, value) instead of Cache.put(key, value)."
89,Invalidation mode is suitable only for shared stores where all nodes can access the same data.
89,"Using invalidation mode without a persistent store is impractical, as updated values need to be read from a shared store for consistency across nodes."
89,"Never use invalidation mode with a local, non-shared, cache store."
89,"The invalidation message will not remove entries in the local store, and some nodes will keep seeing the stale value."
89,"An invalidation cache can also be configured with a special cache loader, ClusterLoader."
89,"When ClusterLoader is enabled, read operations that do not find the key on the local node will request it from all the other nodes first, and store it in memory locally."
89,"This can lead to storing stale values, so only use it if you have a high tolerance for stale values."
89,Synchronous or asynchronous replication
89,"When synchronous, a write operation blocks until all nodes in the cluster have evicted the stale value."
89,"When asynchronous, the originator broadcasts invalidation messages but does not wait for responses."
89,That means other nodes still see the stale value for a while after the write completed on the originator.
89,Transactions
89,Transactions can be used to batch the invalidation messages.
89,Transactions acquire the key lock on the primary owner.
89,"With pessimistic locking, each write triggers a lock message, which is"
89,broadcast to all the nodes.
89,"During transaction commit, the originator broadcasts a one-phase prepare message (optionally fire-and-forget) which invalidates all affected keys and releases the locks."
89,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
89,"Either the one-phase prepare or the unlock message is fire-and-forget, and the last message always releases the locks."
89,2.4. Scattered caches
89,Scattered caches are very similar to distributed caches as they allow linear scaling of the cluster.
89,Scattered caches allow single node failure by maintaining two copies of the data (numOwners=2).
89,"Unlike distributed caches, the location of data is not fixed; while we use the same Consistent Hash algorithm to locate the primary owner, the backup copy is stored on the node that wrote the data last time."
89,"When the write originates on the primary owner, backup copy is stored on any other node (the exact location of this copy is not important)."
89,"This has the advantage of single Remote Procedure Call (RPC) for any write (distributed caches require one or two RPCs), but reads have to always target the primary owner."
89,"That results in faster writes but possibly slower reads, and therefore this mode is more suitable for write-intensive applications."
89,Storing multiple backup copies also results in slightly higher memory consumption.
89,"In order to remove out-of-date backup copies, invalidation messages are broadcast in the cluster, which generates some overhead."
89,This lowers the performance of scattered caches in clusters with a large number of nodes.
89,"When a node crashes, the primary copy may be lost."
89,"Therefore, the cluster has to reconcile the backups and find out the last written backup copy."
89,This process results in more network traffic during state transfer.
89,"Since the writer of data is also a backup, even if we specify machine/rack/site IDs on the transport level the cluster cannot be resilient to more than one failure on the same machine/rack/site."
89,You cannot use scattered caches with transactions or asynchronous replication.
89,"The cache is configured in a similar way as the other cache modes, here is an example of declarative configuration:"
89,"<scattered-cache name=""scatteredCache"" />"
89,Configuration c = new ConfigurationBuilder()
89,.clustering().cacheMode(CacheMode.SCATTERED_SYNC)
89,.build();
89,Scattered mode is not exposed in the server configuration as the server is usually accessed through the Hot Rod
89,protocol. The protocol automatically selects primary owner for the writes and therefore the write (in distributed
89,"mode with two owner) requires single RPC inside the cluster, too. Therefore, scattered cache would not bring"
89,the performance benefit.
89,2.5. Asynchronous replication
89,All clustered cache modes can be configured to use asynchronous communications with the
89,"mode=""ASYNC"""
89,"attribute on the <replicated-cache/>, <distributed-cache>, or <invalidation-cache/>"
89,element.
89,"With asynchronous communications, the originator node does not receive any"
89,"acknowledgement from the other nodes about the status of the operation, so there is no"
89,way to check if it succeeded on other nodes.
89,"We do not recommend asynchronous communications in general, as they can cause"
89,"inconsistencies in the data, and the results are hard to reason about."
89,"Nevertheless, sometimes speed is more important than consistency, and the option is"
89,available for those cases.
89,Asynchronous API
89,"The Asynchronous API allows you to use synchronous communications,"
89,but without blocking the user thread.
89,There is one caveat:
89,The asynchronous operations do NOT preserve the program order.
89,"If a thread calls cache.putAsync(k, v1); cache.putAsync(k, v2), the final value of k"
89,may be either v1 or v2.
89,The advantage over using asynchronous communications is that the final value can’t be
89,v1 on one node and v2 on another.
89,2.5.1. Return values with asynchronous replication
89,"Because the Cache interface extends java.util.Map, write methods like"
89,"put(key, value) and remove(key) return the previous value by default."
89,"In some cases, the return value may not be correct:"
89,"When using AdvancedCache.withFlags() with Flag.IGNORE_RETURN_VALUE,"
89,"Flag.SKIP_REMOTE_LOOKUP, or Flag.SKIP_CACHE_LOAD."
89,"When the cache is configured with unreliable-return-values=""true""."
89,When using asynchronous communications.
89,"When there are multiple concurrent writes to the same key, and the cache topology"
89,changes.
89,"The topology change will make Infinispan retry the write operations, and a retried"
89,operation’s return value is not reliable.
89,Transactional caches return the correct previous value in cases 3 and 4.
89,"However, transactional caches also have a gotcha: in distributed mode, the"
89,read-committed isolation level is implemented as repeatable-read.
89,"That means this example of ""double-checked locking"" won’t work:"
89,Cache cache = ...
89,TransactionManager tm = ...
89,tm.begin();
89,try {
89,Integer v1 = cache.get(k);
89,// Increment the value
89,"Integer v2 = cache.put(k, v1 + 1);"
89,"if (Objects.equals(v1, v2) {"
89,// success
89,} else {
89,// retry
89,} finally {
89,tm.commit();
89,The correct way to implement this is to use
89,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(k).
89,"In caches with optimistic locking, writes can also return stale previous values. Write skew checks can avoid stale previous values."
89,2.6. Configuring initial cluster size
89,Infinispan handles cluster topology changes dynamically.
89,This means that nodes do not need to wait for other nodes to join the cluster before Infinispan initializes the caches.
89,"If your applications require a specific number of nodes in the cluster before caches start, you can configure the initial cluster size as part of the transport."
89,Procedure
89,Open your Infinispan configuration for editing.
89,Set the minimum number of nodes required before caches start with the initial-cluster-size attribute or initialClusterSize() method.
89,"Set the timeout, in milliseconds, after which the Cache Manager does not start with the initial-cluster-timeout attribute or initialClusterTimeout() method."
89,Save and close your Infinispan configuration.
89,Initial cluster size configuration
89,XML
89,<infinispan>
89,<cache-container>
89,"<transport initial-cluster-size=""4"""
89,"initial-cluster-timeout=""30000"" />"
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""transport"" : {"
89,"""initial-cluster-size"" : ""4"","
89,"""initial-cluster-timeout"" : ""30000"""
89,YAML
89,infinispan:
89,cacheContainer:
89,transport:
89,"initialClusterSize: ""4"""
89,"initialClusterTimeout: ""30000"""
89,ConfigurationBuilder
89,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
89,.transport()
89,.initialClusterSize(4)
89,".initialClusterTimeout(30000, TimeUnit.MILLISECONDS);"
89,3. Infinispan cache configuration
89,Cache configuration controls how Infinispan stores your data.
89,"As part of your cache configuration, you declare the cache mode you want to use."
89,"For instance, you can configure Infinispan clusters to use replicated caches or distributed caches."
89,Your configuration also defines the characteristics of your caches and enables the Infinispan capabilities that you want to use when handling data.
89,"For instance, you can configure how Infinispan encodes entries in your caches, whether replication requests happen synchronously or asynchronously between nodes, if entries are mortal or immortal, and so on."
89,3.1. Declarative cache configuration
89,"You can configure caches declaratively, in XML, JSON, and YAML format, according to the Infinispan schema."
89,Declarative cache configuration has the following advantages over programmatic configuration:
89,Portability
89,Define each configuration in a standalone file that you can use to create embedded and remote caches.
89,You can also use declarative configuration to create caches with Infinispan Operator for clusters running on Kubernetes.
89,Simplicity
89,Keep markup languages separate to programming languages.
89,"For example, to create remote caches it is generally better to not add complex XML directly to Java code."
89,"Infinispan Server configuration extends infinispan.xml to include cluster transport mechanisms, security realms, and endpoint configuration."
89,"If you declare caches as part of your Infinispan Server configuration you should use management tooling, such as Ansible or Chef, to keep it synchronized across the cluster."
89,"To dynamically synchronize remote caches across Infinispan clusters, create them at runtime."
89,3.1.1. Cache configuration
89,"You can create declarative cache configuration in XML, JSON, and YAML format."
89,All declarative caches must conform to the Infinispan schema.
89,"Configuration in JSON format must follow the structure of an XML configuration, elements correspond to objects and attributes correspond to fields."
89,Infinispan restricts characters to a maximum of 255 for a cache name or a cache template name.
89,"If you exceed this character limit, Infinispan throws an exception."
89,Write succinct cache names and cache template names.
89,"A file system might set a limitation for the length of a file name, so ensure that a cache’s name does not exceed this limitation."
89,"If a cache name exceeds a file system’s naming limitation, general operations or initialing operations towards that cache might fail."
89,Write succinct file names.
89,Distributed caches
89,XML
89,"<distributed-cache owners=""2"""
89,"segments=""256"""
89,"capacity-factor=""1.0"""
89,"l1-lifespan=""5000"""
89,"mode=""SYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<locking isolation=""REPEATABLE_READ""/>"
89,"<transaction mode=""FULL_XA"""
89,"locking=""OPTIMISTIC""/>"
89,"<expiration lifespan=""5000"""
89,"max-idle=""1000"" />"
89,"<memory max-count=""1000000"""
89,"when-full=""REMOVE""/>"
89,"<indexing enabled=""true"""
89,"storage=""local-heap"">"
89,"<index-reader refresh-interval=""1000""/>"
89,<indexed-entities>
89,<indexed-entity>org.infinispan.Person</indexed-entity>
89,</indexed-entities>
89,</indexing>
89,"<partition-handling when-split=""ALLOW_READ_WRITES"""
89,"merge-policy=""PREFERRED_NON_NULL""/>"
89,"<persistence passivation=""false"">"
89,<!-- Persistent storage configuration. -->
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""mode"": ""SYNC"","
89,"""owners"": ""2"","
89,"""segments"": ""256"","
89,"""capacity-factor"": ""1.0"","
89,"""l1-lifespan"": ""5000"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""locking"": {"
89,"""isolation"": ""REPEATABLE_READ"""
89,"""transaction"": {"
89,"""mode"": ""FULL_XA"","
89,"""locking"": ""OPTIMISTIC"""
89,"""expiration"" : {"
89,"""lifespan"" : ""5000"","
89,"""max-idle"" : ""1000"""
89,"""memory"": {"
89,"""max-count"": ""1000000"","
89,"""when-full"": ""REMOVE"""
89,"""indexing"" : {"
89,"""enabled"" : true,"
89,"""storage"" : ""local-heap"","
89,"""index-reader"" : {"
89,"""refresh-interval"" : ""1000"""
89,"""indexed-entities"": ["
89,"""org.infinispan.Person"""
89,"""partition-handling"" : {"
89,"""when-split"" : ""ALLOW_READ_WRITES"","
89,"""merge-policy"" : ""PREFERRED_NON_NULL"""
89,"""persistence"" : {"
89,"""passivation"" : false"
89,YAML
89,distributedCache:
89,"mode: ""SYNC"""
89,"owners: ""2"""
89,"segments: ""256"""
89,"capacityFactor: ""1.0"""
89,"l1Lifespan: ""5000"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,locking:
89,"isolation: ""REPEATABLE_READ"""
89,transaction:
89,"mode: ""FULL_XA"""
89,"locking: ""OPTIMISTIC"""
89,expiration:
89,"lifespan: ""5000"""
89,"maxIdle: ""1000"""
89,memory:
89,"maxCount: ""1000000"""
89,"whenFull: ""REMOVE"""
89,indexing:
89,"enabled: ""true"""
89,"storage: ""local-heap"""
89,indexReader:
89,"refreshInterval: ""1000"""
89,indexedEntities:
89,"- ""org.infinispan.Person"""
89,partitionHandling:
89,"whenSplit: ""ALLOW_READ_WRITES"""
89,"mergePolicy: ""PREFERRED_NON_NULL"""
89,persistence:
89,"passivation: ""false"""
89,# Persistent storage configuration.
89,Replicated caches
89,XML
89,"<replicated-cache segments=""256"""
89,"mode=""SYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<locking isolation=""REPEATABLE_READ""/>"
89,"<transaction mode=""FULL_XA"""
89,"locking=""OPTIMISTIC""/>"
89,"<expiration lifespan=""5000"""
89,"max-idle=""1000"" />"
89,"<memory max-count=""1000000"""
89,"when-full=""REMOVE""/>"
89,"<indexing enabled=""true"""
89,"storage=""local-heap"">"
89,"<index-reader refresh-interval=""1000""/>"
89,<indexed-entities>
89,<indexed-entity>org.infinispan.Person</indexed-entity>
89,</indexed-entities>
89,</indexing>
89,"<partition-handling when-split=""ALLOW_READ_WRITES"""
89,"merge-policy=""PREFERRED_NON_NULL""/>"
89,"<persistence passivation=""false"">"
89,<!-- Persistent storage configuration. -->
89,</persistence>
89,</replicated-cache>
89,JSON
89,"""replicated-cache"": {"
89,"""mode"": ""SYNC"","
89,"""segments"": ""256"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""locking"": {"
89,"""isolation"": ""REPEATABLE_READ"""
89,"""transaction"": {"
89,"""mode"": ""FULL_XA"","
89,"""locking"": ""OPTIMISTIC"""
89,"""expiration"" : {"
89,"""lifespan"" : ""5000"","
89,"""max-idle"" : ""1000"""
89,"""memory"": {"
89,"""max-count"": ""1000000"","
89,"""when-full"": ""REMOVE"""
89,"""indexing"" : {"
89,"""enabled"" : true,"
89,"""storage"" : ""local-heap"","
89,"""index-reader"" : {"
89,"""refresh-interval"" : ""1000"""
89,"""indexed-entities"": ["
89,"""org.infinispan.Person"""
89,"""partition-handling"" : {"
89,"""when-split"" : ""ALLOW_READ_WRITES"","
89,"""merge-policy"" : ""PREFERRED_NON_NULL"""
89,"""persistence"" : {"
89,"""passivation"" : false"
89,YAML
89,replicatedCache:
89,"mode: ""SYNC"""
89,"segments: ""256"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,locking:
89,"isolation: ""REPEATABLE_READ"""
89,transaction:
89,"mode: ""FULL_XA"""
89,"locking: ""OPTIMISTIC"""
89,expiration:
89,"lifespan: ""5000"""
89,"maxIdle: ""1000"""
89,memory:
89,"maxCount: ""1000000"""
89,"whenFull: ""REMOVE"""
89,indexing:
89,"enabled: ""true"""
89,"storage: ""local-heap"""
89,indexReader:
89,"refreshInterval: ""1000"""
89,indexedEntities:
89,"- ""org.infinispan.Person"""
89,partitionHandling:
89,"whenSplit: ""ALLOW_READ_WRITES"""
89,"mergePolicy: ""PREFERRED_NON_NULL"""
89,persistence:
89,"passivation: ""false"""
89,# Persistent storage configuration.
89,Multiple caches
89,XML
89,<infinispan
89,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
89,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-14.0.xsd"
89,"urn:infinispan:server:14.0 https://infinispan.org/schemas/infinispan-server-14.0.xsd"""
89,"xmlns=""urn:infinispan:config:14.0"""
89,"xmlns:server=""urn:infinispan:server:14.0"">"
89,"<cache-container name=""default"""
89,"statistics=""true"">"
89,"<distributed-cache name=""mycacheone"""
89,"mode=""ASYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<expiration lifespan=""300000""/>"
89,"<memory max-size=""400MB"""
89,"when-full=""REMOVE""/>"
89,</distributed-cache>
89,"<distributed-cache name=""mycachetwo"""
89,"mode=""SYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<expiration lifespan=""300000""/>"
89,"<memory max-size=""400MB"""
89,"when-full=""REMOVE""/>"
89,</distributed-cache>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""name"" : ""default"","
89,"""statistics"" : ""true"","
89,"""caches"" : {"
89,"""mycacheone"" : {"
89,"""distributed-cache"" : {"
89,"""mode"": ""ASYNC"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""expiration"" : {"
89,"""lifespan"" : ""300000"""
89,"""memory"": {"
89,"""max-size"": ""400MB"","
89,"""when-full"": ""REMOVE"""
89,"""mycachetwo"" : {"
89,"""distributed-cache"" : {"
89,"""mode"": ""SYNC"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""expiration"" : {"
89,"""lifespan"" : ""300000"""
89,"""memory"": {"
89,"""max-size"": ""400MB"","
89,"""when-full"": ""REMOVE"""
89,YAML
89,infinispan:
89,cacheContainer:
89,"name: ""default"""
89,"statistics: ""true"""
89,caches:
89,mycacheone:
89,distributedCache:
89,"mode: ""ASYNC"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,expiration:
89,"lifespan: ""300000"""
89,memory:
89,"maxSize: ""400MB"""
89,"whenFull: ""REMOVE"""
89,mycachetwo:
89,distributedCache:
89,"mode: ""SYNC"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,expiration:
89,"lifespan: ""300000"""
89,memory:
89,"maxSize: ""400MB"""
89,"whenFull: ""REMOVE"""
89,Additional resources
89,Infinispan configuration schema reference
89,infinispan-config-14.0.xsd
89,3.2. Adding cache templates
89,The Infinispan schema includes *-cache-configuration elements that you can use to create templates.
89,"You can then create caches on demand, using the same configuration multiple times."
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add the cache configuration with the appropriate *-cache-configuration element or object to the Cache Manager.
89,Save and close your Infinispan configuration.
89,Cache template example
89,XML
89,<infinispan>
89,<cache-container>
89,"<distributed-cache-configuration name=""my-dist-template"""
89,"mode=""SYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<memory max-count=""1000000"""
89,"when-full=""REMOVE""/>"
89,"<expiration lifespan=""5000"""
89,"max-idle=""1000""/>"
89,</distributed-cache-configuration>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""distributed-cache-configuration"" : {"
89,"""name"" : ""my-dist-template"","
89,"""mode"": ""SYNC"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""expiration"" : {"
89,"""lifespan"" : ""5000"","
89,"""max-idle"" : ""1000"""
89,"""memory"": {"
89,"""max-count"": ""1000000"","
89,"""when-full"": ""REMOVE"""
89,YAML
89,infinispan:
89,cacheContainer:
89,distributedCacheConfiguration:
89,"name: ""my-dist-template"""
89,"mode: ""SYNC"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,expiration:
89,"lifespan: ""5000"""
89,"maxIdle: ""1000"""
89,memory:
89,"maxCount: ""1000000"""
89,"whenFull: ""REMOVE"""
89,3.2.1. Creating caches from templates
89,Create caches from configuration templates.
89,Templates for remote caches are available from the Cache templates menu in Infinispan Console.
89,Prerequisites
89,Add at least one cache template to the Cache Manager.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Specify the template from which the cache inherits with the configuration attribute or field.
89,Save and close your Infinispan configuration.
89,Cache configuration inherited from a template
89,XML
89,"<distributed-cache configuration=""my-dist-template"" />"
89,JSON
89,"""distributed-cache"": {"
89,"""configuration"": ""my-dist-template"""
89,YAML
89,distributedCache:
89,"configuration: ""my-dist-template"""
89,3.2.2. Cache template inheritance
89,Cache configuration templates can inherit from other templates to extend and override settings.
89,Cache template inheritance is hierarchical.
89,"For a child configuration template to inherit from a parent, you must include it after the parent template."
89,"Additionally, template inheritance is additive for elements that have multiple values."
89,"A cache that inherits from another template merges the values from that template, which can override properties."
89,Template inheritance example
89,XML
89,<infinispan>
89,<cache-container>
89,"<distributed-cache-configuration name=""base-template"">"
89,"<expiration lifespan=""5000""/>"
89,</distributed-cache-configuration>
89,"<distributed-cache-configuration name=""extended-template"""
89,"configuration=""base-template"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,"<expiration lifespan=""10000"""
89,"max-idle=""1000""/>"
89,</distributed-cache-configuration>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""caches"" : {"
89,"""base-template"" : {"
89,"""distributed-cache-configuration"" : {"
89,"""expiration"" : {"
89,"""lifespan"" : ""5000"""
89,"""extended-template"" : {"
89,"""distributed-cache-configuration"" : {"
89,"""configuration"" : ""base-template"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,"""expiration"" : {"
89,"""lifespan"" : ""10000"","
89,"""max-idle"" : ""1000"""
89,YAML
89,infinispan:
89,cacheContainer:
89,caches:
89,base-template:
89,distributedCacheConfiguration:
89,expiration:
89,"lifespan: ""5000"""
89,extended-template:
89,distributedCacheConfiguration:
89,"configuration: ""base-template"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,expiration:
89,"lifespan: ""10000"""
89,"maxIdle: ""1000"""
89,3.2.3. Cache template wildcards
89,You can add wildcards to cache configuration template names.
89,"If you then create caches where the name matches the wildcard, Infinispan applies the configuration template."
89,Infinispan throws exceptions if cache names match more than one wildcard.
89,Template wildcard example
89,XML
89,<infinispan>
89,<cache-container>
89,"<distributed-cache-configuration name=""async-dist-cache-*"""
89,"mode=""ASYNC"""
89,"statistics=""true"">"
89,"<encoding media-type=""application/x-protostream""/>"
89,</distributed-cache-configuration>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""distributed-cache-configuration"" : {"
89,"""name"" : ""async-dist-cache-*"","
89,"""mode"": ""ASYNC"","
89,"""statistics"": ""true"","
89,"""encoding"": {"
89,"""media-type"": ""application/x-protostream"""
89,YAML
89,infinispan:
89,cacheContainer:
89,distributedCacheConfiguration:
89,"name: ""async-dist-cache-*"""
89,"mode: ""ASYNC"""
89,"statistics: ""true"""
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,"Using the preceding example, if you create a cache named ""async-dist-cache-prod"" then Infinispan uses the configuration from the async-dist-cache-* template."
89,3.2.4. Cache templates from multiple XML files
89,Split cache configuration templates into multiple XML files for granular flexibility and reference them with XML inclusions (XInclude).
89,Infinispan provides minimal support for the XInclude specification.
89,"This means you cannot use the xpointer attribute, the xi:fallback element, text processing, or content negotiation."
89,"You must also add the xmlns:xi=""http://www.w3.org/2001/XInclude"" namespace to infinispan.xml to use XInclude."
89,Xinclude cache template
89,"<infinispan xmlns:xi=""http://www.w3.org/2001/XInclude"">"
89,"<cache-container default-cache=""cache-1"">"
89,<!-- References files that contain cache configuration templates. -->
89,"<xi:include href=""distributed-cache-template.xml"" />"
89,"<xi:include href=""replicated-cache-template.xml"" />"
89,</cache-container>
89,</infinispan>
89,Infinispan also provides an infinispan-config-fragment-14.0.xsd schema that you can use with configuration fragments.
89,Configuration fragment schema
89,"<local-cache xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
89,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-fragment-14.0.xsd"""
89,"xmlns=""urn:infinispan:config:14.0"""
89,"name=""mycache""/>"
89,Additional resources
89,XInclude specification
89,3.3. Creating remote caches
89,"When you create remote caches at runtime, Infinispan Server synchronizes your configuration across the cluster so that all nodes have a copy."
89,For this reason you should always create remote caches dynamically with the following mechanisms:
89,Infinispan Console
89,Infinispan Command Line Interface (CLI)
89,Hot Rod or HTTP clients
89,3.3.1. Default Cache Manager
89,Infinispan Server provides a default Cache Manager that controls the lifecycle of remote caches.
89,Starting Infinispan Server automatically instantiates the Cache Manager so you can create and delete remote caches and other resources like Protobuf schema.
89,"After you start Infinispan Server and add user credentials, you can view details about the Cache Manager and get cluster information from Infinispan Console."
89,Open 127.0.0.1:11222 in any browser.
89,You can also get information about the Cache Manager through the Command Line Interface (CLI) or REST API:
89,CLI
89,Run the describe command in the default container.
89,[//containers/default]> describe
89,REST
89,Open 127.0.0.1:11222/rest/v2/cache-managers/default/ in any browser.
89,Default Cache Manager configuration
89,XML
89,<infinispan>
89,"<!-- Creates a Cache Manager named ""default"" and enables metrics. -->"
89,"<cache-container name=""default"""
89,"statistics=""true"">"
89,<!-- Adds cluster transport that uses the default JGroups TCP stack. -->
89,"<transport cluster=""${infinispan.cluster.name:cluster}"""
89,"stack=""${infinispan.cluster.stack:tcp}"""
89,"node-name=""${infinispan.node.name:}""/>"
89,<!-- Requires user permission to access caches and perform operations. -->
89,<security>
89,<authorization/>
89,</security>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""jgroups"" : {"
89,"""transport"" : ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
89,"""cache-container"" : {"
89,"""name"" : ""default"","
89,"""statistics"" : ""true"","
89,"""transport"" : {"
89,"""cluster"" : ""cluster"","
89,"""node-name"" : """","
89,"""stack"" : ""tcp"""
89,"""security"" : {"
89,"""authorization"" : {}"
89,YAML
89,infinispan:
89,jgroups:
89,"transport: ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
89,cacheContainer:
89,"name: ""default"""
89,"statistics: ""true"""
89,transport:
89,"cluster: ""cluster"""
89,"nodeName: """""
89,"stack: ""tcp"""
89,security:
89,authorization: ~
89,3.3.2. Creating caches with Infinispan Console
89,Use Infinispan Console to create remote caches in an intuitive visual interface from any web browser.
89,Prerequisites
89,Create a Infinispan user with admin permissions.
89,Start at least one Infinispan Server instance.
89,Have a Infinispan cache configuration.
89,Procedure
89,Open 127.0.0.1:11222/console/ in any browser.
89,Select Create Cache and follow the steps as Infinispan Console guides you through the process.
89,3.3.3. Creating remote caches with the Infinispan CLI
89,Use the Infinispan Command Line Interface (CLI) to add remote caches on Infinispan Server.
89,Prerequisites
89,Create a Infinispan user with admin permissions.
89,Start at least one Infinispan Server instance.
89,Have a Infinispan cache configuration.
89,Procedure
89,Start the CLI.
89,bin/cli.sh
89,Run the connect command and enter your username and password when prompted.
89,Use the create cache command to create remote caches.
89,"For example, create a cache named ""mycache"" from a file named mycache.xml as follows:"
89,create cache --file=mycache.xml mycache
89,Verification
89,List all remote caches with the ls command.
89,ls caches
89,mycache
89,View cache configuration with the describe command.
89,describe caches/mycache
89,3.3.4. Creating remote caches from Hot Rod clients
89,"Use the Infinispan Hot Rod API to create remote caches on Infinispan Server from Java, C++, .NET/C#, JS clients and more."
89,This procedure shows you how to use Hot Rod Java clients that create remote caches on first access.
89,You can find code examples for other Hot Rod clients in the Infinispan Tutorials.
89,Prerequisites
89,Create a Infinispan user with admin permissions.
89,Start at least one Infinispan Server instance.
89,Have a Infinispan cache configuration.
89,Procedure
89,Invoke the remoteCache() method as part of your the ConfigurationBuilder.
89,Set the configuration or configuration_uri properties in the hotrod-client.properties file on your classpath.
89,ConfigurationBuilder
89,"File file = new File(""path/to/infinispan.xml"")"
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,"builder.remoteCache(""another-cache"")"
89,".configuration(""<distributed-cache name=\""another-cache\""/>"");"
89,"builder.remoteCache(""my.other.cache"")"
89,.configurationURI(file.toURI());
89,hotrod-client.properties
89,"infinispan.client.hotrod.cache.another-cache.configuration=<distributed-cache name=\""another-cache\""/>"
89,infinispan.client.hotrod.cache.[my.other.cache].configuration_uri=file:///path/to/infinispan.xml
89,"If the name of your remote cache contains the . character, you must enclose it in square brackets when using hotrod-client.properties files."
89,Additional resources
89,Hot Rod Client Configuration
89,org.infinispan.client.hotrod.configuration.RemoteCacheConfigurationBuilder
89,3.3.5. Creating remote caches with the REST API
89,Use the Infinispan REST API to create remote caches on Infinispan Server from any suitable HTTP client.
89,Prerequisites
89,Create a Infinispan user with admin permissions.
89,Start at least one Infinispan Server instance.
89,Have a Infinispan cache configuration.
89,Procedure
89,Invoke POST requests to /rest/v2/caches/<cache_name> with cache configuration in the payload.
89,Additional resources
89,Creating and Managing Caches with the REST API
89,3.4. Creating embedded caches
89,Infinispan provides an EmbeddedCacheManager API that lets you control both the Cache Manager and embedded cache lifecycles programmatically.
89,3.4.1. Adding Infinispan to your project
89,Add Infinispan to your project to create embedded caches in your applications.
89,Prerequisites
89,Configure your project to get Infinispan artifacts from the Maven repository.
89,Procedure
89,Add the infinispan-core artifact as a dependency in your pom.xml as
89,follows:
89,<dependencies>
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-core</artifactId>
89,</dependency>
89,</dependencies>
89,3.4.2. Creating and using embedded caches
89,Infinispan provides a GlobalConfigurationBuilder API that controls the Cache Manager and a ConfigurationBuilder API that configures caches.
89,Prerequisites
89,Add the infinispan-core artifact as a dependency in your pom.xml.
89,Procedure
89,Initialize a CacheManager.
89,You must always call the cacheManager.start() method to initialize a CacheManager before you can create caches.
89,Default constructors do this for you but there are overloaded versions of the constructors that do not.
89,Cache Managers are also heavyweight objects and Infinispan recommends instantiating only one instance per JVM.
89,Use the ConfigurationBuilder API to define cache configuration.
89,"Obtain caches with getCache(), createCache(), or getOrCreateCache() methods."
89,Infinispan recommends using the getOrCreateCache() method because it either creates a cache on all nodes or returns an existing cache.
89,If necessary use the PERMANENT flag for caches to survive restarts.
89,Stop the CacheManager by calling the cacheManager.stop() method to release JVM resources and gracefully shutdown any caches.
89,// Set up a clustered Cache Manager.
89,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder();
89,// Initialize the default Cache Manager.
89,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
89,// Create a distributed cache with synchronous replication.
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.clustering().cacheMode(CacheMode.DIST_SYNC);
89,// Obtain a volatile cache.
89,"Cache<String, String> cache = cacheManager.administration().withFlags(CacheContainerAdmin.AdminFlag.VOLATILE).getOrCreateCache(""myCache"", builder.build());"
89,// Stop the Cache Manager.
89,cacheManager.stop();
89,getCache() method
89,"Invoke the getCache(String) method to obtain caches, as follows:"
89,"Cache<String, String> myCache = manager.getCache(""myCache"");"
89,"The preceding operation creates a cache named myCache, if it does not already exist, and returns it."
89,"Using the getCache() method creates the cache only on the node where you invoke the method. In other words, it performs a local operation that must be invoked on each node across the cluster. Typically, applications deployed across multiple nodes obtain caches during initialization to ensure that caches are symmetric and exist on each node."
89,createCache() method
89,Invoke the createCache() method to create caches dynamically across the entire cluster.
89,"Cache<String, String> myCache = manager.administration().createCache(""myCache"", ""myTemplate"");"
89,The preceding operation also automatically creates caches on any nodes that subsequently join the cluster.
89,"Caches that you create with the createCache() method are ephemeral by default. If the entire cluster shuts down, the cache is not automatically created again when it restarts."
89,PERMANENT flag
89,Use the PERMANENT flag to ensure that caches can survive restarts.
89,"Cache<String, String> myCache = manager.administration().withFlags(AdminFlag.PERMANENT).createCache(""myCache"", ""myTemplate"");"
89,"For the PERMANENT flag to take effect, you must enable global state and set a configuration storage provider."
89,"For more information about configuration storage providers, see GlobalStateConfigurationBuilder#configurationStorage()."
89,Additional resources
89,EmbeddedCacheManager
89,EmbeddedCacheManager Configuration
89,org.infinispan.configuration.global.GlobalConfiguration
89,org.infinispan.configuration.cache.ConfigurationBuilder
89,3.4.3. Cache API
89,"Infinispan provides a Cache interface that exposes simple methods for adding, retrieving and removing entries, including atomic mechanisms exposed by the JDK’s ConcurrentMap interface."
89,"Based on the cache mode used, invoking these methods will trigger a number of things to happen, potentially even including replicating an entry to a remote node or looking up an entry from a remote node, or potentially a cache store."
89,"For simple usage, using the Cache API should be no different from using the JDK Map API, and hence migrating from simple in-memory caches based on a Map to Infinispan’s Cache should be trivial."
89,Performance Concerns of Certain Map Methods
89,"Certain methods exposed in Map have certain performance consequences when used with Infinispan, such as"
89,"size() ,"
89,"values() ,"
89,keySet() and
89,entrySet() .
89,"Specific methods on the keySet, values and entrySet are fine for use please see their Javadoc for further details."
89,Attempting to perform these operations globally would have large performance impact as well as become a scalability bottleneck.
89,"As such, these methods should only be used for informational or debugging purposes only."
89,"It should be noted that using certain flags with the withFlags() method can mitigate some of these concerns, please check each method’s documentation for more details."
89,Mortal and Immortal Data
89,"Further to simply storing entries, Infinispan’s cache API allows you to attach mortality information to data."
89,"For example, simply using put(key, value) would create an immortal entry, i.e., an entry that lives in the cache forever, until it is removed (or evicted from memory to prevent running out of memory)."
89,"If, however, you put data in the cache using put(key, value, lifespan, timeunit) , this creates a mortal entry, i.e., an entry that has a fixed lifespan and expires after that lifespan."
89,"In addition to lifespan , Infinispan also supports maxIdle as an additional metric with which to determine expiration."
89,Any combination of lifespans or maxIdles can be used.
89,putForExternalRead operation
89,Infinispan’s Cache class contains a different 'put' operation called putForExternalRead . This operation is particularly useful when Infinispan is used as a temporary cache for data that is persisted elsewhere.
89,"Under heavy read scenarios, contention in the cache should not delay the real transactions at hand, since caching should just be an optimization and not something that gets in the way."
89,"To achieve this, putForExternalRead() acts as a put call that only operates if the key is not present in the cache, and fails fast and silently if another thread is trying to store the same key at the same time. In this particular scenario, caching data is a way to optimise the system and it’s not desirable that a failure in caching affects the on-going transaction, hence why failure is handled differently. putForExternalRead() is considered to be a fast operation because regardless of whether it’s successful or not, it doesn’t wait for any locks, and so returns to the caller promptly."
89,"To understand how to use this operation, let’s look at basic example. Imagine a cache of Person instances, each keyed by a PersonId , whose data originates in a separate data store. The following code shows the most common pattern of using putForExternalRead within the context of this example:"
89,"// Id of the person to look up, provided by the application"
89,PersonId id = ...;
89,// Get a reference to the cache where person instances will be stored
89,"Cache<PersonId, Person> cache = ...;"
89,"// First, check whether the cache contains the person instance"
89,// associated with with the given id
89,Person cachedPerson = cache.get(id);
89,if (cachedPerson == null) {
89,"// The person is not cached yet, so query the data store with the id"
89,Person person = dataStore.lookup(id);
89,// Cache the person along with the id so that future requests can
89,// retrieve it from memory rather than going to the data store
89,"cache.putForExternalRead(id, person);"
89,} else {
89,"// The person was found in the cache, so return it to the application"
89,return cachedPerson;
89,"Note that putForExternalRead should never be used as a mechanism to update the cache with a new Person instance originating from application execution (i.e. from a transaction that modifies a Person’s address). When updating cached values, please use the standard put operation, otherwise the possibility of caching corrupt data is likely."
89,AdvancedCache API
89,"In addition to the simple Cache interface, Infinispan offers an AdvancedCache interface, geared towards extension authors."
89,The AdvancedCache offers the ability to access certain internal components and to apply flags to alter the default behavior of certain cache methods.
89,The following code snippet depicts how an AdvancedCache can be obtained:
89,AdvancedCache advancedCache = cache.getAdvancedCache();
89,Flags
89,Flags are applied to regular cache methods to alter the behavior of certain methods.
89,"For a list of all available flags, and their effects, see the Flag enumeration."
89,Flags are applied using AdvancedCache.withFlags() .
89,"This builder method can be used to apply any number of flags to a cache invocation, for example:"
89,"advancedCache.withFlags(Flag.CACHE_MODE_LOCAL, Flag.SKIP_LOCKING)"
89,.withFlags(Flag.FORCE_SYNCHRONOUS)
89,".put(""hello"", ""world"");"
89,Asynchronous API
89,"In addition to synchronous API methods like Cache.put() , Cache.remove() , etc., Infinispan also has an asynchronous, non-blocking API where you can achieve the same results in a non-blocking fashion."
89,"These methods are named in a similar fashion to their blocking counterparts, with ""Async"" appended.  E.g., Cache.putAsync() , Cache.removeAsync() , etc.  These asynchronous counterparts return a CompletableFuture that contains the actual result of the operation."
89,"For example, in a cache parameterized as Cache<String, String>, Cache.put(String key, String value) returns String while Cache.putAsync(String key, String value) returns CompletableFuture<String>."
89,Why use such an API?
89,Non-blocking APIs are powerful in that they provide all of the guarantees of synchronous communications - with the ability to handle communication failures and exceptions - with the ease of not having to block until a call completes.  This allows you to better harness parallelism in your system.  For example:
89,Set<CompletableFuture<?>> futures = new HashSet<>();
89,"futures.add(cache.putAsync(key1, value1)); // does not block"
89,"futures.add(cache.putAsync(key2, value2)); // does not block"
89,"futures.add(cache.putAsync(key3, value3)); // does not block"
89,// the remote calls for the 3 puts will effectively be executed
89,"// in parallel, particularly useful if running in distributed mode"
89,// and the 3 keys would typically be pushed to 3 different nodes
89,// in the cluster
89,// check that the puts completed successfully
89,for (CompletableFuture<?> f: futures) f.get();
89,Which processes actually happen asynchronously?
89,There are 4 things in Infinispan that can be considered to be on the critical path of a typical write operation.
89,"These are, in order of cost:"
89,network calls
89,marshalling
89,writing to a cache store (optional)
89,locking
89,"Using the async methods will take the network calls and marshalling off the critical path.  For various technical reasons, writing to a cache store and acquiring locks, however, still happens in the caller’s thread."
89,4. Enabling and configuring Infinispan statistics and JMX monitoring
89,Infinispan can provide Cache Manager and cache statistics as well as export JMX MBeans.
89,4.1. Enabling statistics in embedded caches
89,Configure Infinispan to export statistics for the Cache Manager and embedded caches.
89,Procedure
89,Open your Infinispan configuration for editing.
89,"Add the statistics=""true"" attribute or the .statistics(true) method."
89,Save and close your Infinispan configuration.
89,Embedded cache statistics
89,XML
89,<infinispan>
89,"<cache-container statistics=""true"">"
89,"<distributed-cache statistics=""true""/>"
89,"<replicated-cache statistics=""true""/>"
89,</cache-container>
89,</infinispan>
89,GlobalConfigurationBuilder
89,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder().cacheContainer().statistics(true);
89,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
89,Configuration builder = new ConfigurationBuilder();
89,builder.statistics().enable();
89,4.2. Enabling statistics in remote caches
89,Infinispan Server automatically enables statistics for the default Cache Manager.
89,"However, you must explicitly enable statistics for your caches."
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add the statistics attribute or field and specify true as the value.
89,Save and close your Infinispan configuration.
89,Remote cache statistics
89,XML
89,"<distributed-cache statistics=""true"" />"
89,JSON
89,"""distributed-cache"": {"
89,"""statistics"": ""true"""
89,YAML
89,distributedCache:
89,statistics: true
89,4.3. Enabling Hot Rod client statistics
89,Hot Rod Java clients can provide statistics that include remote cache and near-cache hits and misses as well as connection pool usage.
89,Procedure
89,Open your Hot Rod Java client configuration for editing.
89,Set true as the value for the statistics property or invoke the statistics().enable() methods.
89,Export JMX MBeans for your Hot Rod client with the jmx and jmx_domain properties or invoke the jmxEnable() and jmxDomain() methods.
89,Save and close your client configuration.
89,Hot Rod Java client statistics
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.statistics().enable()
89,.jmxEnable()
89,".jmxDomain(""my.domain.org"")"
89,.addServer()
89,".host(""127.0.0.1"")"
89,.port(11222);
89,RemoteCacheManager remoteCacheManager = new RemoteCacheManager(builder.build());
89,hotrod-client.properties
89,infinispan.client.hotrod.statistics = true
89,infinispan.client.hotrod.jmx = true
89,infinispan.client.hotrod.jmx_domain = my.domain.org
89,4.4. Configuring Infinispan metrics
89,Infinispan generates metrics that are compatible with any monitoring system.
89,Gauges provide values such as the average number of nanoseconds for write operations or JVM uptime.
89,"Histograms provide details about operation execution times such as read,"
89,"write, and remove times."
89,"By default, Infinispan generates gauges when you enable statistics but you can also configure it to generate histograms."
89,Infinispan metrics are provided at the vendor scope.
89,Metrics related to the JVM are provided in the base scope.
89,Prerequisites
89,You must add Micrometer Core and Micrometer Registry Prometheus JARs to your classpath to export Infinispan metrics for embedded caches.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add the metrics element or object to the cache container.
89,Enable or disable gauges with the gauges attribute or field.
89,Enable or disable histograms with the histograms attribute or field.
89,Save and close your client configuration.
89,Metrics configuration
89,XML
89,<infinispan>
89,"<cache-container statistics=""true"">"
89,"<metrics gauges=""true"""
89,"histograms=""true"" />"
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""statistics"" : ""true"","
89,"""metrics"" : {"
89,"""gauges"" : ""true"","
89,"""histograms"" : ""true"""
89,YAML
89,infinispan:
89,cacheContainer:
89,"statistics: ""true"""
89,metrics:
89,"gauges: ""true"""
89,"histograms: ""true"""
89,GlobalConfigurationBuilder
89,GlobalConfiguration globalConfig = new GlobalConfigurationBuilder()
89,//Computes and collects statistics for the Cache Manager.
89,.statistics().enable()
89,//Exports collected statistics as gauge and histogram metrics.
89,.metrics().gauges(true).histograms(true)
89,.build();
89,Verification
89,Infinispan Server exposes statistics through the metrics endpoint that you can collect with monitoring tools such as Prometheus.
89,"To verify that statistics are exported to the metrics endpoint, you can do the following:"
89,Prometheus format
89,curl -v http://localhost:11222/metrics \
89,--digest -u username:password
89,OpenMetrics format
89,curl -v http://localhost:11222/metrics \
89,--digest -u username:password \
89,"-H ""Accept: application/openmetrics-text"""
89,Infinispan no longer provides metrics in MicroProfile JSON format.
89,Additional resources
89,Micrometer Prometheus
89,4.5. Registering JMX MBeans
89,Infinispan can register JMX MBeans that you can use to collect statistics and
89,perform administrative operations.
89,You must also enable statistics otherwise Infinispan provides 0 values for all statistic attributes in JMX MBeans.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add the jmx element or object to the cache container and specify true as the value for the enabled attribute or field.
89,"Add the domain attribute or field and specify the domain where JMX MBeans are exposed, if required."
89,Save and close your client configuration.
89,JMX configuration
89,XML
89,<infinispan>
89,"<cache-container statistics=""true"">"
89,"<jmx enabled=""true"""
89,"domain=""example.com""/>"
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""statistics"" : ""true"","
89,"""jmx"" : {"
89,"""enabled"" : ""true"","
89,"""domain"" : ""example.com"""
89,YAML
89,infinispan:
89,cacheContainer:
89,"statistics: ""true"""
89,jmx:
89,"enabled: ""true"""
89,"domain: ""example.com"""
89,GlobalConfigurationBuilder
89,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
89,.jmx().enable()
89,".domain(""org.mydomain"");"
89,4.5.1. Enabling JMX remote ports
89,Provide unique remote JMX ports to expose Infinispan MBeans through connections in JMXServiceURL format.
89,Infinispan Server does not expose JMX remotely via the single port endpoint.
89,If you want to remotely access Infinispan Server via JMX you must enable a remote port.
89,You can enable remote JMX ports using one of the following approaches:
89,Enable remote JMX ports that require authentication to one of the Infinispan Server security realms.
89,Enable remote JMX ports manually using the standard Java management configuration options.
89,Prerequisites
89,"For remote JMX with authentication, define JMX specific user roles using the default security realm."
89,Users must have controlRole with read/write access or the monitorRole with read-only access to access any JMX resources.
89,Procedure
89,Start Infinispan Server with a remote JMX port enabled using one of the following ways:
89,Enable remote JMX through port 9999.
89,bin/server.sh --jmx 9999
89,Using remote JMX with SSL disabled is not intended for production environments.
89,Pass the following system properties to Infinispan Server at startup.
89,bin/server.sh -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
89,Enabling remote JMX with no authentication or SSL is not secure and not recommended in any environment.
89,Disabling authentication and SSL allows unauthorized users to connect to your server and access the data hosted there.
89,Additional resources
89,Creating security realms
89,4.5.2. Infinispan MBeans
89,Infinispan exposes JMX MBeans that represent manageable resources.
89,org.infinispan:type=Cache
89,Attributes and operations available for cache instances.
89,org.infinispan:type=CacheManager
89,"Attributes and operations available for Cache Managers, including Infinispan cache and cluster health statistics."
89,For a complete list of available JMX MBeans along with descriptions and
89,"available operations and attributes, see the Infinispan JMX Components"
89,documentation.
89,Additional resources
89,Infinispan JMX Components
89,4.5.3. Registering MBeans in custom MBean servers
89,Infinispan includes an MBeanServerLookup interface that you can use to
89,register MBeans in custom MBeanServer instances.
89,Prerequisites
89,Create an implementation of MBeanServerLookup so that the getMBeanServer() method returns the custom MBeanServer instance.
89,Configure Infinispan to register JMX MBeans.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add the mbean-server-lookup attribute or field to the JMX configuration for the Cache Manager.
89,Specify fully qualified name (FQN) of your MBeanServerLookup implementation.
89,Save and close your client configuration.
89,JMX MBean server lookup configuration
89,XML
89,<infinispan>
89,"<cache-container statistics=""true"">"
89,"<jmx enabled=""true"""
89,"domain=""example.com"""
89,"mbean-server-lookup=""com.example.MyMBeanServerLookup""/>"
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""statistics"" : ""true"","
89,"""jmx"" : {"
89,"""enabled"" : ""true"","
89,"""domain"" : ""example.com"","
89,"""mbean-server-lookup"" : ""com.example.MyMBeanServerLookup"""
89,YAML
89,infinispan:
89,cacheContainer:
89,"statistics: ""true"""
89,jmx:
89,"enabled: ""true"""
89,"domain: ""example.com"""
89,"mbeanServerLookup: ""com.example.MyMBeanServerLookup"""
89,GlobalConfigurationBuilder
89,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
89,.jmx().enable()
89,".domain(""org.mydomain"")"
89,.mBeanServerLookup(new com.acme.MyMBeanServerLookup());
89,4.6. Exporting metrics during a state transfer operation
89,You can export time metrics for clustered caches that Infinispan redistributes across nodes.
89,"A state transfer operation occurs when a clustered cache topology changes, such as a node joining or leaving a cluster."
89,"During a state transfer operation, Infinispan exports metrics from each cache, so that you can determine a cache’s status."
89,"A state transfer exposes attributes as properties, so that Infinispan can export metrics from each cache."
89,You cannot perform a state transfer operation in invalidation mode.
89,Infinispan generates time metrics that are compatible with the REST API and the JMX API.
89,Prerequisites
89,Configure Infinispan metrics.
89,"Enable metrics for your cache type, such as embedded cache or remote cache."
89,Initiate a state transfer operation by changing your clustered cache topology.
89,Procedure
89,Choose one of the following methods:
89,Configure Infinispan to use the REST API to collect metrics.
89,Configure Infinispan to use the JMX API to collect metrics.
89,Additional resources
89,Enabling and configuring Infinispan statistics and JMX monitoring (Infinispan caches)
89,StateTransferManager (Infinispan 14.0 API)
89,4.7. Monitoring the status of cross-site replication
89,Monitor the site status of your backup locations to detect interruptions in the communication between the sites.
89,"When a remote site status changes to offline, Infinispan stops replicating your data to the backup location."
89,Your data become out of sync and you must fix the inconsistencies before bringing the clusters back online.
89,Monitoring cross-site events is necessary for early problem detection.
89,Use one of the following monitoring strategies:
89,Monitoring cross-site replication with the REST API
89,Monitoring cross-site replication with the Prometheus metrics or any other monitoring system
89,Monitoring cross-site replication with the REST API
89,Monitor the status of cross-site replication for all caches using the REST endpoint.
89,You can implement a custom script to poll the REST endpoint or use the following example.
89,Prerequisites
89,Enable cross-site replication.
89,Procedure
89,Implement a script to poll the REST endpoint.
89,The following example demonstrates how you can use a Python script to poll the site status every five seconds.
89,#!/usr/bin/python3
89,import time
89,import requests
89,from requests.auth import HTTPDigestAuth
89,class InfinispanConnection:
89,"def __init__(self, server: str = 'http://localhost:11222', cache_manager: str = 'default',"
89,"auth: tuple = ('admin', 'change_me')) -> None:"
89,super().__init__()
89,self.__url = f'{server}/rest/v2/cache-managers/{cache_manager}/x-site/backups/'
89,self.__auth = auth
89,self.__headers = {
89,'accept': 'application/json'
89,def get_sites_status(self):
89,try:
89,"rsp = requests.get(self.__url, headers=self.__headers, auth=HTTPDigestAuth(self.__auth[0], self.__auth[1]))"
89,if rsp.status_code != 200:
89,return None
89,return rsp.json()
89,except:
89,return None
89,# Specify credentials for Infinispan user with permission to access the REST endpoint
89,USERNAME = 'admin'
89,PASSWORD = 'change_me'
89,# Set an interval between cross-site status checks
89,POLL_INTERVAL_SEC = 5
89,# Provide a list of servers
89,SERVERS = [
89,"InfinispanConnection('http://127.0.0.1:11222', auth=(USERNAME, PASSWORD)),"
89,"InfinispanConnection('http://127.0.0.1:12222', auth=(USERNAME, PASSWORD))"
89,#Specify the names of remote sites
89,REMOTE_SITES = [
89,'nyc'
89,#Provide a list of caches to monitor
89,CACHES = [
89,"'work',"
89,'sessions'
89,"def on_event(site: str, cache: str, old_status: str, new_status: str):"
89,# TODO implement your handling code here
89,print(f'site={site} cache={cache} Status changed {old_status} -> {new_status}')
89,"def __handle_mixed_state(state: dict, site: str, site_status: dict):"
89,if site not in state:
89,state[site] = {c: 'online' if c in site_status['online'] else 'offline' for c in CACHES}
89,return
89,for cache in CACHES:
89,"__update_cache_state(state, site, cache, 'online' if cache in site_status['online'] else 'offline')"
89,"def __handle_online_or_offline_state(state: dict, site: str, new_status: str):"
89,if site not in state:
89,state[site] = {c: new_status for c in CACHES}
89,return
89,for cache in CACHES:
89,"__update_cache_state(state, site, cache, new_status)"
89,"def __update_cache_state(state: dict, site: str, cache: str, new_status: str):"
89,old_status = state[site].get(cache)
89,if old_status != new_status:
89,"on_event(site, cache, old_status, new_status)"
89,state[site][cache] = new_status
89,def update_state(state: dict):
89,rsp = None
89,for conn in SERVERS:
89,rsp = conn.get_sites_status()
89,if rsp:
89,break
89,if rsp is None:
89,print('Unable to fetch site status from any server')
89,return
89,for site in REMOTE_SITES:
89,"site_status = rsp.get(site, {})"
89,new_status = site_status.get('status')
89,if new_status == 'mixed':
89,"__handle_mixed_state(state, site, site_status)"
89,else:
89,"__handle_online_or_offline_state(state, site, new_status)"
89,if __name__ == '__main__':
89,_state = {}
89,while True:
89,update_state(_state)
89,time.sleep(POLL_INTERVAL_SEC)
89,"When a site status changes from online to offline or vice-versa, the function on_event is invoked."
89,"If you want to use this script, you must specify the following variables:"
89,USERNAME and PASSWORD: The username and password of Infinispan user with permission to access the REST endpoint.
89,POLL_INTERVAL_SEC: The number of seconds between polls.
89,SERVERS: The list of Infinispan Servers at this site.
89,The script only requires a single valid response but the list is provided to allow fail over.
89,REMOTE_SITES: The list of remote sites to monitor on these servers.
89,CACHES: The list of cache names to monitor.
89,Additional resources
89,REST API: Getting status of backup locations
89,Monitoring cross-site replication with the Prometheus metrics
89,"Prometheus, and other monitoring systems, let you configure alerts to detect when a site status changes to offline."
89,Monitoring cross-site latency metrics can help you to discover potential issues.
89,Prerequisites
89,Enable cross-site replication.
89,Procedure
89,Configure Infinispan metrics.
89,Configure alerting rules using the Prometheus metrics format.
89,"For the site status, use 1 for online and 0 for offline."
89,"For the expr filed, use the following format:"
89,vendor_cache_manager_default_cache_<cache name>_x_site_admin_<site name>_status.
89,"In the following example, Prometheus alerts you when the NYC site gets offline for cache named work or sessions."
89,groups:
89,- name: Cross Site Rules
89,rules:
89,- alert: Cache Work and Site NYC
89,expr: vendor_cache_manager_default_cache_work_x_site_admin_nyc_status == 0
89,- alert: Cache Sessions and Site NYC
89,expr: vendor_cache_manager_default_cache_sessions_x_site_admin_nyc_status == 0
89,The following image shows an alert that the NYC site is offline for cache work.
89,Figure 6. Prometheus Alert
89,Additional resources
89,Configuring Infinispan metrics
89,Prometheus Alerting Overview
89,Grafana Alerting Documentation
89,Openshift Managing Alerts
89,5. Configuring JVM memory usage
89,Control how Infinispan stores data in JVM memory by:
89,Managing JVM memory usage with eviction that automatically removes data from caches.
89,Adding lifespan and maximum idle times to expire entries and prevent stale data.
89,"Configuring Infinispan to store data in off-heap, native memory."
89,5.1. Default memory configuration
89,By default Infinispan stores cache entries as objects in the JVM heap.
89,"Over time, as applications add entries, the size of caches can exceed the amount of memory that is available to the JVM."
89,"Likewise, if Infinispan is not the primary data store, then entries become out of date which means your caches contain stale data."
89,XML
89,<distributed-cache>
89,"<memory storage=""HEAP""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""memory"" : {"
89,"""storage"": ""HEAP"""
89,YAML
89,distributedCache:
89,memory:
89,"storage: ""HEAP"""
89,5.2. Eviction and expiration
89,Eviction and expiration are two strategies for cleaning the data container by
89,"removing old, unused entries."
89,"Although eviction and expiration are similar, they have some important differences."
89,Eviction lets Infinispan control the size of the data container by removing entries when the container becomes larger than a configured threshold.
89,Expiration limits the amount of time entries can exist. Infinispan uses
89,a scheduler to periodically remove expired entries. Entries that are expired
89,but not yet removed are immediately removed on access; in this case get()
89,"calls for expired entries return ""null"" values."
89,Eviction is local to Infinispan nodes.
89,Expiration takes place across Infinispan clusters.
89,You can use eviction and expiration together or independently of each other.
89,You can configure eviction and expiration declaratively in infinispan.xml to apply cache-wide defaults for entries.
89,You can explicitly define expiration settings for specific entries but you cannot define eviction on a per-entry basis.
89,You can manually evict entries and manually trigger expiration.
89,5.3. Eviction with Infinispan caches
89,Eviction lets you control the size of the data container by removing entries from memory in one of two ways:
89,Total number of entries (max-count).
89,Maximum amount of memory (max-size).
89,Eviction drops one entry from the data container at a time and is local to the node on which it occurs.
89,Eviction removes entries from memory but not from persistent cache stores.
89,"To ensure that entries remain available after Infinispan evicts them, and to prevent inconsistencies with your data, you should configure persistent storage."
89,"When you configure memory, Infinispan approximates the current memory usage of the data container."
89,"When entries are added or modified, Infinispan compares the current memory usage of the data container to the maximum size."
89,"If the size exceeds the maximum, Infinispan performs eviction."
89,Eviction happens immediately in the thread that adds an entry that exceeds the maximum size.
89,5.3.1. Eviction strategies
89,When you configure Infinispan eviction you specify:
89,The maximum size of the data container.
89,A strategy for removing entries when the cache reaches the threshold.
89,You can either perform eviction manually or configure Infinispan to do one of the following:
89,Remove old entries to make space for new ones.
89,Throw ContainerFullException and prevent new entries from being created.
89,The exception eviction strategy works only with transactional caches that use 2 phase commits; not with 1 phase commits or synchronization optimizations.
89,Refer to the schema reference for more details about the eviction strategies.
89,Infinispan includes the Caffeine caching library that implements a variation
89,of the Least Frequently Used (LFU) cache replacement algorithm known as
89,"TinyLFU. For off-heap storage, Infinispan uses a custom implementation of the"
89,Least Recently Used (LRU) algorithm.
89,Additional resources
89,Caffeine
89,Infinispan configuration schema reference
89,5.3.2. Configuring maximum count eviction
89,Limit the size of Infinispan caches to a total number of entries.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Specify the total number of entries that caches can contain before
89,Infinispan performs eviction with either the max-count attribute or maxCount() method.
89,Set one of the following as the eviction strategy to control how Infinispan removes entries with the when-full attribute or whenFull() method.
89,REMOVE Infinispan performs eviction. This is the default strategy.
89,MANUAL You perform eviction manually for embedded caches.
89,EXCEPTION Infinispan throws an exception instead of evicting entries.
89,Save and close your Infinispan configuration.
89,Maximum count eviction
89,"In the following example, Infinispan removes an entry when the cache contains a total of 500 entries and a new entry is created:"
89,XML
89,<distributed-cache>
89,"<memory max-count=""500"" when-full=""REMOVE""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"" : {"
89,"""memory"" : {"
89,"""max-count"" : ""500"","
89,"""when-full"" : ""REMOVE"""
89,YAML
89,distributedCache:
89,memory:
89,"maxCount: ""500"""
89,"whenFull: ""REMOVE"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.memory().maxCount(500).whenFull(EvictionStrategy.REMOVE);
89,Additional resources
89,Infinispan configuration schema reference
89,org.infinispan.configuration.cache.MemoryConfigurationBuilder
89,5.3.3. Configuring maximum size eviction
89,Limit the size of Infinispan caches to a maximum amount of memory.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Specify application/x-protostream as the media type for cache encoding.
89,You must specify a binary media type to use maximum size eviction.
89,"Configure the maximum amount of memory, in bytes, that caches can use before"
89,Infinispan performs eviction with the max-size attribute or maxSize() method.
89,Optionally specify a byte unit of measurement.
89,The default is B (bytes). Refer to the configuration schema for supported units.
89,Set one of the following as the eviction strategy to control how Infinispan removes entries with either the when-full attribute or whenFull() method.
89,REMOVE Infinispan performs eviction. This is the default strategy.
89,MANUAL You perform eviction manually for embedded caches.
89,EXCEPTION Infinispan throws an exception instead of evicting entries.
89,Save and close your Infinispan configuration.
89,Maximum size eviction
89,"In the following example, Infinispan removes an entry when the size of the cache reaches 1.5 GB (gigabytes) and a new entry is created:"
89,XML
89,<distributed-cache>
89,"<encoding media-type=""application/x-protostream""/>"
89,"<memory max-size=""1.5GB"" when-full=""REMOVE""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"" : {"
89,"""encoding"" : {"
89,"""media-type"" : ""application/x-protostream"""
89,"""memory"" : {"
89,"""max-size"" : ""1.5GB"","
89,"""when-full"" : ""REMOVE"""
89,YAML
89,distributedCache:
89,encoding:
89,"mediaType: ""application/x-protostream"""
89,memory:
89,"maxSize: ""1.5GB"""
89,"whenFull: ""REMOVE"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,"builder.encoding().mediaType(""application/x-protostream"")"
89,.memory()
89,".maxSize(""1.5GB"")"
89,.whenFull(EvictionStrategy.REMOVE);
89,Additional resources
89,Infinispan configuration schema reference
89,org.infinispan.configuration.cache.EncodingConfiguration
89,org.infinispan.configuration.cache.MemoryConfigurationBuilder
89,Cache Encoding and Marshalling
89,5.3.4. Manual eviction
89,"If you choose the manual eviction strategy, Infinispan does not perform eviction."
89,You must do so manually with the evict() method.
89,You should use manual eviction with embedded caches only.
89,"For remote caches, you should always configure Infinispan with the REMOVE or EXCEPTION eviction strategy."
89,This configuration prevents a warning message when you enable passivation but do not configure eviction.
89,XML
89,<distributed-cache>
89,"<memory max-count=""500"" when-full=""MANUAL""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"" : {"
89,"""memory"" : {"
89,"""max-count"" : ""500"","
89,"""when-full"" : ""MANUAL"""
89,YAML
89,distributedCache:
89,memory:
89,"maxCount: ""500"""
89,"whenFull: ""MANUAL"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,"builder.encoding().mediaType(""application/x-protostream"")"
89,.memory()
89,".maxSize(""1.5GB"")"
89,.whenFull(EvictionStrategy.REMOVE);
89,5.3.5. Passivation with eviction
89,Passivation persists data to cache stores when Infinispan evicts entries.
89,"You should always enable eviction if you enable passivation, as in the following examples:"
89,XML
89,<distributed-cache>
89,"<persistence passivation=""true"">"
89,<!-- Persistent storage configuration. -->
89,</persistence>
89,"<memory max-count=""100""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""memory"" : {"
89,"""max-count"" : ""100"""
89,"""persistence"" : {"
89,"""passivation"" : true"
89,YAML
89,distributedCache:
89,memory:
89,"maxCount: ""100"""
89,persistence:
89,"passivation: ""true"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.memory().maxCount(100);
89,builder.persistence().passivation(true); //Persistent storage configuration
89,5.4. Expiration with lifespan and maximum idle
89,Expiration configures Infinispan to remove entries from caches when they reach one of the following time limits:
89,Lifespan
89,Sets the maximum amount of time that entries can exist.
89,Maximum idle
89,Specifies how long entries can remain idle. If operations do not occur for
89,"entries, they become idle."
89,Maximum idle expiration does not currently support caches with persistent storage.
89,"If you use expiration and eviction with the EXCEPTION eviction strategy, entries that are expired, but not yet removed from the cache, count towards the size of the data container."
89,5.4.1. How expiration works
89,"When you configure expiration, Infinispan stores keys with metadata that"
89,determines when entries expire.
89,Lifespan uses a creation timestamp and the value for the lifespan configuration property.
89,Maximum idle uses a last used timestamp and the value for the max-idle configuration property.
89,Infinispan checks if lifespan or maximum idle metadata is set and then
89,compares the values with the current time.
89,If (creation + lifespan < currentTime) or (lastUsed + maxIdle < currentTime) then Infinispan detects that the entry is expired.
89,Expiration occurs whenever entries are accessed or found by the expiration
89,reaper.
89,"For example, k1 reaches the maximum idle time and a client makes a"
89,"Cache.get(k1) request. In this case, Infinispan detects that the entry is"
89,expired and removes it from the data container. The Cache.get(k1) request returns null.
89,"Infinispan also expires entries from cache stores, but only with lifespan"
89,expiration. Maximum idle expiration does not work with cache stores. In the
89,"case of cache loaders, Infinispan cannot expire entries because loaders can"
89,only read from external storage.
89,Infinispan adds expiration metadata as long primitive data types to cache
89,entries. This can increase the size of keys by as much as 32 bytes.
89,5.4.2. Expiration reaper
89,Infinispan uses a reaper thread that runs periodically to detect and remove
89,expired entries. The expiration reaper ensures that expired entries that are no
89,longer accessed are removed.
89,The Infinispan ExpirationManager interface handles the expiration reaper and
89,exposes the processExpiration() method.
89,"In some cases, you can disable the expiration reaper and manually expire"
89,"entries by calling processExpiration(); for instance, if you are using local"
89,cache mode with a custom application where a maintenance thread runs
89,periodically.
89,"If you use clustered cache modes, you should never disable the expiration"
89,reaper.
89,Infinispan always uses the expiration reaper when using cache stores. In this
89,case you cannot disable it.
89,Additional resources
89,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
89,org.infinispan.expiration.ExpirationManager
89,5.4.3. Maximum idle and clustered caches
89,Because maximum idle expiration relies on the last access time for cache
89,"entries, it has some limitations with clustered cache modes."
89,"With lifespan expiration, the creation time for cache entries provides a value"
89,"that is consistent across clustered caches. For example, the creation time for"
89,k1 is always the same on all nodes.
89,"For maximum idle expiration with clustered caches, last access time for entries"
89,is not always the same on all nodes. To ensure that entries have the same
89,"relative access times across clusters, Infinispan sends touch commands to all"
89,owners when keys are accessed.
89,The touch commands that Infinispan send have the following considerations:
89,Cache.get() requests do not return until all touch commands complete. This synchronous behavior increases latency of client requests.
89,"The touch command also updates the ""recently accessed"" metadata for cache entries on all owners, which Infinispan uses for eviction."
89,"With scattered cache mode, Infinispan sends touch commands to all nodes, not just primary and backup owners."
89,Additional information
89,Maximum idle expiration does not work with invalidation mode.
89,Iteration across a clustered cache can return expired entries that have
89,exceeded the maximum idle time limit. This behavior ensures performance because
89,no remote invocations are performed during the iteration. Also note that
89,iteration does not refresh any expired entries.
89,5.4.4. Configuring lifespan and maximum idle times for caches
89,Set lifespan and maximum idle times for all entries in a cache.
89,Procedure
89,Open your Infinispan configuration for editing.
89,"Specify the amount of time, in milliseconds, that entries can stay in the cache with the lifespan attribute or lifespan() method."
89,"Specify the amount of time, in milliseconds, that entries can remain idle after last access with the max-idle attribute or maxIdle() method."
89,Save and close your Infinispan configuration.
89,Expiration for Infinispan caches
89,"In the following example, Infinispan expires all cache entries after 5 seconds or 1 second after the last access time, whichever happens first:"
89,XML
89,<replicated-cache>
89,"<expiration lifespan=""5000"" max-idle=""1000"" />"
89,</replicated-cache>
89,JSON
89,"""replicated-cache"" : {"
89,"""expiration"" : {"
89,"""lifespan"" : ""5000"","
89,"""max-idle"" : ""1000"""
89,YAML
89,replicatedCache:
89,expiration:
89,"lifespan: ""5000"""
89,"maxIdle: ""1000"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,"builder.expiration().lifespan(5000, TimeUnit.MILLISECONDS)"
89,".maxIdle(1000, TimeUnit.MILLISECONDS);"
89,5.4.5. Configuring lifespan and maximum idle times per entry
89,Specify lifespan and maximum idle times for individual entries.
89,"When you add lifespan and maximum idle times to entries, those values take priority over expiration configuration for caches."
89,"When you explicitly define lifespan and maximum idle time values for cache entries, Infinispan replicates those values across the cluster along with the cache entries."
89,"Likewise, Infinispan writes expiration values along with the entries to persistent storage."
89,Procedure
89,"For remote caches, you can add lifespan and maximum idle times to entries interactively with the Infinispan Console."
89,"With the Infinispan Command Line Interface (CLI), use the --max-idle= and --ttl= arguments with the put command."
89,"For both remote and embedded caches, you can add lifespan and maximum idle times with cache.put() invocations."
89,//Lifespan of 5 seconds.
89,//Maximum idle time of 1 second.
89,"cache.put(""hello"", ""world"", 5, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
89,//Lifespan is disabled with a value of -1.
89,//Maximum idle time of 1 second.
89,"cache.put(""hello"", ""world"", -1, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
89,Additional resources
89,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
89,org.infinispan.expiration.ExpirationManager
89,5.5. JVM heap and off-heap memory
89,Infinispan stores cache entries in JVM heap memory by default.
89,"You can configure Infinispan to use off-heap storage, which means that your data occupies native memory outside the managed JVM memory space."
89,The following diagram is a simplified illustration of the memory space for a JVM process where Infinispan is running:
89,Figure 7. JVM memory space
89,JVM heap memory
89,The heap is divided into young and old generations that help keep referenced Java objects and other application data in memory.
89,"The GC process reclaims space from unreachable objects, running more frequently on the young generation memory pool."
89,"When Infinispan stores cache entries in JVM heap memory, GC runs can take longer to complete as you start adding data to your caches."
89,"Because GC is an intensive process, longer and more frequent runs can degrade application performance."
89,Off-heap memory
89,Off-heap memory is native available system memory outside JVM memory management.
89,The JVM memory space diagram shows the Metaspace memory pool that holds class metadata and is allocated from native memory.
89,The diagram also represents a section of native memory that holds Infinispan cache entries.
89,Off-heap memory:
89,Uses less memory per entry.
89,Improves overall JVM performance by avoiding Garbage Collector (GC) runs.
89,"One disadvantage, however, is that JVM heap dumps do not show entries stored in off-heap memory."
89,5.5.1. Off-heap data storage
89,"When you add entries to off-heap caches, Infinispan dynamically allocates native memory to your data."
89,Infinispan hashes the serialized byte[] for each key into buckets that are similar to a standard Java HashMap.
89,Buckets include address pointers that Infinispan uses to locate entries that you store in off-heap memory.
89,"Even though Infinispan stores cache entries in native memory, run-time operations require JVM heap representations of those objects."
89,"For instance, cache.get() operations read objects into heap memory before returning."
89,"Likewise, state transfer operations hold subsets of objects in heap memory while they take place."
89,Object equality
89,Infinispan determines equality of Java objects in off-heap storage using the serialized byte[] representation of each object instead of the object instance.
89,Data consistency
89,Infinispan uses an array of locks to protect off-heap address spaces.
89,The number of locks is twice the number of cores and then rounded to the nearest power of two.
89,This ensures that there is an even distribution of ReadWriteLock instances to prevent write operations from blocking read operations.
89,5.5.2. Configuring off-heap memory
89,Configure Infinispan to store cache entries in native memory outside the JVM
89,heap space.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Set OFF_HEAP as the value for the storage attribute or storage() method.
89,Set a boundary for the size of the cache by configuring eviction.
89,Save and close your Infinispan configuration.
89,Off-heap storage
89,Infinispan stores cache entries as bytes in native memory.
89,Eviction happens when there are 100 entries in the data container and Infinispan gets a request to create a new entry:
89,XML
89,<replicated-cache>
89,"<memory storage=""OFF_HEAP"" max-count=""500""/>"
89,</replicated-cache>
89,JSON
89,"""replicated-cache"" : {"
89,"""memory"" : {"
89,"""storage"" : ""OFF_HEAP"","
89,"""max-count"" : ""500"""
89,YAML
89,replicatedCache:
89,memory:
89,"storage: ""OFF_HEAP"""
89,"maxCount: ""500"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.memory().storage(StorageType.OFF_HEAP).maxCount(500);
89,Additional resources
89,Infinispan configuration schema reference
89,org.infinispan.configuration.cache.MemoryConfigurationBuilder
89,6. Configuring persistent storage
89,Infinispan uses cache stores and loaders to interact with persistent storage.
89,Durability
89,Adding cache stores allows you to persist data to non-volatile storage so it
89,survives restarts.
89,Write-through caching
89,Configuring Infinispan as a caching layer in front of persistent storage
89,simplifies data access for applications because Infinispan handles all
89,interactions with the external storage.
89,Data overflow
89,Using eviction and passivation techniques ensures that Infinispan keeps only
89,frequently used data in-memory and writes older entries to persistent storage.
89,6.1. Passivation
89,Passivation configures Infinispan to write entries to cache stores when it
89,"evicts those entries from memory. In this way, passivation prevents unnecessary"
89,and potentially expensive writes to persistent storage.
89,Activation is the process of restoring entries to memory from the cache store
89,"when there is an attempt to access passivated entries. For this reason, when you"
89,"enable passivation, you must configure cache stores that implement both"
89,CacheWriter and CacheLoader interfaces so they can write and load entries
89,from persistent storage.
89,"When Infinispan evicts an entry from the cache, it notifies cache listeners"
89,that the entry is passivated then stores the entry in the cache store. When
89,"Infinispan gets an access request for an evicted entry, it lazily loads the"
89,entry from the cache store into memory and then notifies cache listeners that
89,the entry is activated while keeping the value still in the store.
89,Passivation uses the first cache loader in the Infinispan configuration and
89,ignores all others.
89,Passivation is not supported with:
89,Transactional stores. Passivation writes and removes entries from the store
89,outside the scope of the actual Infinispan commit boundaries.
89,Shared stores. Shared cache stores require entries to always exist in the
89,"store for other owners. For this reason, passivation is not supported because"
89,entries cannot be removed.
89,"If you enable passivation with transactional stores or shared stores,"
89,Infinispan throws an exception.
89,6.1.1. How passivation works
89,Passivation disabled
89,Writes to data in memory result in writes to persistent storage.
89,"If Infinispan evicts data from memory, then data in persistent storage"
89,includes entries that are evicted from memory. In this way persistent storage
89,is a superset of the in-memory cache.
89,This is recommended when you require highest consistency as the store will be able to be read again after a crash.
89,"If you do not configure eviction, then data in persistent storage provides a"
89,copy of data in memory.
89,Passivation enabled
89,Infinispan adds data to persistent storage only when it evicts data from
89,"memory, an entry is removed or upon shutting down the node."
89,"When Infinispan activates entries, it restores data in memory but keeps the data in the store still."
89,"This allows for writes to be just as fast as without a store, and still maintains consistency."
89,When an entry is created or updated only the in memory will be updated and thus
89,the store will be outdated for the time being.
89,Passivation is not supported when a store is also configured as shared.
89,This is due to entries can become out of sync between nodes depending on when a write is evicted versus read.
89,To gurantee data consistency any store that is not shared should always have purgeOnStartup enabled.
89,This is true for both passivation enabled or disabled since a store could hold an outdated entry while down and resurrect it at a later point.
89,The following table shows data in memory and in persistent storage after a
89,series of operations:
89,Operation
89,Passivation disabled
89,Passivation enabled
89,Insert k1.
89,Memory: k1
89,Disk: k1
89,Memory: k1
89,Disk: -
89,Insert k2.
89,"Memory: k1, k2"
89,"Disk: k1, k2"
89,"Memory: k1, k2"
89,Disk: -
89,Eviction thread runs and evicts k1.
89,Memory: k2
89,"Disk: k1, k2"
89,Memory: k2
89,Disk: k1
89,Read k1.
89,"Memory: k1, k2"
89,"Disk: k1, k2"
89,"Memory: k1, k2"
89,Disk: k1
89,Eviction thread runs and evicts k2.
89,Memory: k1
89,"Disk: k1, k2"
89,Memory: k1
89,"Disk: k1, k2"
89,Remove k2.
89,Memory: k1
89,Disk: k1
89,Memory: k1
89,Disk: k1
89,6.2. Write-through cache stores
89,Write-through is a cache writing mode where writes to memory and writes to
89,"cache stores are synchronous. When a client application updates a cache entry,"
89,"in most cases by invoking Cache.put(), Infinispan does not return the call"
89,until it updates the cache store. This cache writing mode results in updates to
89,the cache store concluding within the boundaries of the client thread.
89,The primary advantage of write-through mode is that the cache and cache store
89,"are updated simultaneously, which ensures that the cache store is always"
89,consistent with the cache.
89,"However, write-through mode can potentially decrease performance because the"
89,need to access and update cache stores directly adds latency to cache
89,operations.
89,Write-through configuration
89,Infinispan uses write-through mode unless you explicitly add write-behind configuration to your caches.
89,There is no separate element or method for configuring write-through mode.
89,"For example, the following configuration adds a file-based store to the cache that implicitly uses write-through mode:"
89,<distributed-cache>
89,"<persistence passivation=""false"">"
89,<file-store>
89,"<index path=""path/to/index"" />"
89,"<data path=""path/to/data"" />"
89,</file-store>
89,</persistence>
89,</distributed-cache>
89,6.3. Write-behind cache stores
89,Write-behind is a cache writing mode where writes to memory are synchronous
89,and writes to cache stores are asynchronous.
89,"When clients send write requests, Infinispan adds those operations to a"
89,modification queue. Infinispan processes operations as they join the queue so
89,that the calling thread is not blocked and the operation completes immediately.
89,If the number of write operations in the modification queue increases beyond
89,"the size of the queue, Infinispan adds those additional operations to the"
89,"queue. However, those operations do not complete until Infinispan processes"
89,operations that are already in the queue.
89,"For example, calling Cache.putAsync returns immediately and the Stage also"
89,completes immediately if the modification queue is not full. If the
89,"modification queue is full, or if Infinispan is currently processing a batch"
89,"of write operations, then Cache.putAsync returns immediately and the Stage"
89,completes later.
89,Write-behind mode provides a performance advantage over write-through mode
89,because cache operations do not need to wait for updates to the underlying cache
89,"store to complete. However, data in the cache store remains inconsistent with"
89,"data in the cache until the modification queue is processed. For this reason,"
89,"write-behind mode is suitable for cache stores with low latency, such as"
89,"unshared and local file-based cache stores, where the time between the"
89,write to the cache and the write to the cache store is as small as possible.
89,Write-behind configuration
89,XML
89,<distributed-cache>
89,<persistence>
89,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
89,"dialect=""H2"""
89,"shared=""true"""
89,"table-name=""books"">"
89,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
89,"username=""sa"""
89,"password=""changeme"""
89,"driver=""org.h2.Driver""/>"
89,"<write-behind modification-queue-size=""2048"""
89,"fail-silently=""true""/>"
89,</table-jdbc-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"" : {"
89,"""table-jdbc-store"": {"
89,"""dialect"": ""H2"","
89,"""shared"": ""true"","
89,"""table-name"": ""books"","
89,"""connection-pool"": {"
89,"""connection-url"": ""jdbc:h2:mem:infinispan"","
89,"""driver"": ""org.h2.Driver"","
89,"""username"": ""sa"","
89,"""password"": ""changeme"""
89,"""write-behind"" : {"
89,"""modification-queue-size"" : ""2048"","
89,"""fail-silently"" : true"
89,YAML
89,distributedCache:
89,persistence:
89,tableJdbcStore:
89,"dialect: ""H2"""
89,"shared: ""true"""
89,"tableName: ""books"""
89,connectionPool:
89,"connectionUrl: ""jdbc:h2:mem:infinispan"""
89,"driver: ""org.h2.Driver"""
89,"username: ""sa"""
89,"password: ""changeme"""
89,writeBehind:
89,"modificationQueueSize: ""2048"""
89,"failSilently: ""true"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence()
89,.async()
89,.modificationQueueSize(2048)
89,.failSilently(true);
89,Failing silently
89,Write-behind configuration includes a fail-silently parameter that controls what happens when either the cache store is unavailable or the modification queue is full.
89,"If fail-silently=""true"" then Infinispan logs WARN messages and rejects"
89,write operations.
89,"If fail-silently=""false"" then Infinispan throws exceptions if it detects"
89,the cache store is unavailable during a write operation. Likewise if the
89,"modification queue becomes full, Infinispan throws an exception."
89,"In some cases, data loss can occur if Infinispan restarts and write operations"
89,"exist in the modification queue. For example the cache store goes offline but,"
89,"during the time it takes to detect that the cache store is unavailable, write"
89,operations are added to the modification queue because it is not full. If
89,Infinispan restarts or otherwise becomes unavailable before the cache store
89,"comes back online, then the write operations in the modification queue are lost"
89,because they were not persisted.
89,6.4. Segmented cache stores
89,Cache stores can organize data into hash space segments to which keys map.
89,"Segmented stores increase read performance for bulk operations; for example,"
89,"streaming over data (Cache.size, Cache.entrySet.stream), pre-loading the"
89,"cache, and doing state transfer operations."
89,"However, segmented stores can also result in loss of performance for write"
89,operations. This performance loss applies particularly to batch write
89,operations that can take place with transactions or write-behind stores. For
89,"this reason, you should evaluate the overhead for write operations before you"
89,enable segmented stores. The performance gain for bulk read operations might
89,not be acceptable if there is a significant performance loss for write
89,operations.
89,The number of segments you configure for cache stores must match the number of
89,segments you define in the Infinispan configuration with the
89,clustering.hash.numSegments parameter.
89,If you change the numSegments parameter in the configuration after you add a
89,"segmented cache store, Infinispan cannot read data from that cache store."
89,6.5. Shared cache stores
89,Infinispan cache stores can be local to a given node or shared across all nodes in the cluster.
89,"By default, cache stores are local (shared=""false"")."
89,"Local cache stores are unique to each node; for example, a file-based cache store that persists data to the host filesystem."
89,"Local cache stores should use ""purge on startup"" to avoid loading stale entries from persistent storage."
89,"Shared cache stores allow multiple nodes to use the same persistent storage; for example, a JDBC cache store that allows multiple nodes to access the same database."
89,"Shared cache stores ensure that only the primary owner write to persistent storage, instead of backup nodes performing write operations for every modification."
89,"Purging deletes data, which is not typically the desired behavior for persistent storage."
89,Local cache store
89,<persistence>
89,"<store shared=""false"""
89,"purge=""true""/>"
89,</persistence>
89,Shared cache store
89,<persistence>
89,"<store shared=""true"""
89,"purge=""false""/>"
89,</persistence>
89,Additional resources
89,Infinispan Configuration Schema
89,6.6. Transactions with persistent cache stores
89,Infinispan supports transactional operations with JDBC-based cache stores only.
89,"To configure caches as transactional, you set transactional=true to keep data in persistent storage synchronized with data in memory."
89,"For all other cache stores, Infinispan does not enlist cache loaders in transactional operations."
89,This can result in data inconsistency if transactions succeed in modifying data in memory but do not completely apply changes to data in the cache store.
89,In these cases manual recovery is not possible with cache stores.
89,6.7. Global persistent location
89,Infinispan preserves global state so that it can restore cluster topology and cached data after restart.
89,Remote caches
89,Infinispan Server saves cluster state to the $ISPN_HOME/server/data directory.
89,You should never delete or modify the server/data directory or its content.
89,Infinispan restores cluster state from this directory when you restart your server instances.
89,Changing the default configuration or directly modifying the server/data directory can cause unexpected behavior and lead to data loss.
89,Embedded caches
89,Infinispan defaults to the user.dir system property as the global persistent location.
89,In most cases this is the directory where your application starts.
89,"For clustered embedded caches, such as replicated or distributed, you should always enable and configure a global persistent location to restore cluster topology."
89,You should never configure an absolute path for a file-based cache store that is outside the global persistent location.
89,"If you do, Infinispan writes the following exception to logs:"
89,"ISPN000558: ""The store location 'foo' is not a child of the global persistent location 'bar'"""
89,6.7.1. Configuring the global persistent location
89,Enable and configure the location where Infinispan stores global state for clustered embedded caches.
89,Infinispan Server enables global persistence and configures a default location.
89,You should not disable global persistence or change the default configuration for remote caches.
89,Prerequisites
89,Add Infinispan to your project.
89,Procedure
89,Enable global state in one of the following ways:
89,Add the global-state element to your Infinispan configuration.
89,Call the globalState().enable() methods in the GlobalConfigurationBuilder API.
89,Define whether the global persistent location is unique to each node or shared between the cluster.
89,Location type
89,Configuration
89,Unique to each node
89,persistent-location element or persistentLocation() method
89,Shared between the cluster
89,shared-persistent-location element or sharedPersistentLocation(String) method
89,Set the path where Infinispan stores cluster state.
89,"For example, file-based cache stores the path is a directory on the host filesystem."
89,Values can be:
89,Absolute and contain the full location including the root.
89,Relative to a root location.
89,"If you specify a relative value for the path, you must also specify a system property that resolves to a root location."
89,"For example, on a Linux host system you set global/state as the path."
89,You also set the my.data property that resolves to the /opt/data root location.
89,In this case Infinispan uses /opt/data/global/state as the global persistent location.
89,Global persistent location configuration
89,XML
89,<infinispan>
89,<cache-container>
89,<global-state>
89,"<persistent-location path=""global/state"" relative-to=""my.data""/>"
89,</global-state>
89,</cache-container>
89,</infinispan>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""global-state"": {"
89,"""persistent-location"" : {"
89,"""path"" : ""global/state"","
89,"""relative-to"" : ""my.data"""
89,YAML
89,cacheContainer:
89,globalState:
89,persistentLocation:
89,"path: ""global/state"""
89,"relativeTo : ""my.data"""
89,GlobalConfigurationBuilder
89,new GlobalConfigurationBuilder().globalState()
89,.enable()
89,".persistentLocation(""global/state"", ""my.data"");"
89,Additional resources
89,Infinispan configuration schema
89,org.infinispan.configuration.global.GlobalStateConfiguration
89,6.8. File-based cache stores
89,File-based cache stores provide persistent storage on the local host filesystem where Infinispan is running.
89,"For clustered caches, file-based cache stores are unique to each Infinispan node."
89,"Never use filesystem-based cache stores on shared file systems, such as an NFS or Samba share, because they do not provide file locking capabilities and data corruption can occur."
89,"Additionally if you attempt to use transactional caches with shared file systems, unrecoverable failures can happen when writing to files during the commit phase."
89,Soft-Index File Stores
89,SoftIndexFileStore is the default implementation for file-based cache stores and stores data in a set of append-only files.
89,When append-only files:
89,"Reach their maximum size, Infinispan creates a new file and starts writing to it."
89,"Reach the compaction threshold of less than 50% usage, Infinispan overwrites the entries to a new file and then deletes the old file."
89,SoftIndexFileStore should use purge on startup to ensure stale entries are not resurrected.
89,B+ trees
89,"To improve performance, append-only files in a SoftIndexFileStore are indexed using a B+ Tree that can be stored both on disk and in memory."
89,The in-memory index uses Java soft references to ensure it can be rebuilt if removed by Garbage Collection (GC) then requested again.
89,"Because SoftIndexFileStore uses Java soft references to keep indexes in memory, it helps prevent out-of-memory exceptions."
89,GC removes indexes before they consume too much memory while still falling back to disk.
89,SoftIndexFileStore creates a B+ tree per configured cache segment.
89,"This provides an additional ""index"" as it only has so many elements and provides additional parallelism for index updates."
89,Currently we allow for a parallel amount based on one sixteenth of the number of cache segments.
89,Each entry in the B+ tree is a node.
89,"By default, the size of each node is limited to 4096 bytes."
89,SoftIndexFileStore throws an exception if keys are longer after serialization occurs.
89,File limits
89,SoftIndexFileStore will use two plus the configured openFilesLimit amount of files at a given time.
89,The two additional file pointers are reserved for the log appender for newly updated data and another
89,for the compactor which writes compacted entries into a new file.
89,The amount of open allocated files allocated for indexing is one tenth of the total number of the configured openFilesLimit.
89,This number has a minimum of 1 or the number of cache segments.
89,Any number remaning from configured limit is allocated for open data files themselves.
89,Segmentation
89,Soft-index file stores are always segmented. The append log(s) are not directly segmented and segmentation is handled directly by the index.
89,Expiration
89,The SoftIndexFileStore has full support for expired entries and their requirements.
89,Single File Cache Stores
89,Single file cache stores are now deprecated and planned for removal.
89,"Single File cache stores, SingleFileStore, persist data to file."
89,Infinispan also maintains an in-memory index of keys while keys and values are stored in the file.
89,"Because SingleFileStore keeps an in-memory index of keys and the location of values, it requires additional memory, depending on the key size and the number of keys."
89,"For this reason, SingleFileStore is not recommended for use cases where the keys are larger or there can be a larger number of them."
89,"In some cases, SingleFileStore can also become fragmented."
89,"If the size of values continually increases, available space in the single file is not used but the entry is appended to the end of the file."
89,Available space in the file is used only if an entry can fit within it.
89,"Likewise, if you remove all entries from memory, the single file store does not decrease in size or become defragmented."
89,Segmentation
89,"Single file cache stores are segmented by default with a separate instance per segment, which results in multiple directories."
89,Each directory is a number that represents the segment to which the data maps.
89,6.8.1. Configuring file-based cache stores
89,Add file-based cache stores to Infinispan to persist data on the host filesystem.
89,Prerequisites
89,Enable global state and configure a global persistent location if you are configuring embedded caches.
89,Procedure
89,Add the persistence element to your cache configuration.
89,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
89,Include the file-store element and configure attributes as appropriate.
89,Specify false as the value for the shared attribute.
89,File-based cache stores should always be unique to each Infinispan instance.
89,"If you want to use the same persistent across a cluster, configure shared storage such as a JDBC string-based cache store ."
89,Configure the index and data elements to specify the location where Infinispan creates indexes and stores data.
89,Include the write-behind element if you want to configure the cache store with write-behind mode.
89,File-based cache store configuration
89,XML
89,<distributed-cache>
89,"<persistence passivation=""true"">"
89,"<file-store shared=""false"">"
89,"<data path=""data""/>"
89,"<index path=""index""/>"
89,"<write-behind modification-queue-size=""2048"" />"
89,</file-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""passivation"": true,"
89,"""file-store"" : {"
89,"""shared"": false,"
89,"""data"": {"
89,"""path"": ""data"""
89,"""index"": {"
89,"""path"": ""index"""
89,"""write-behind"": {"
89,"""modification-queue-size"": ""2048"""
89,YAML
89,distributedCache:
89,persistence:
89,"passivation: ""true"""
89,fileStore:
89,"shared: ""false"""
89,data:
89,"path: ""data"""
89,index:
89,"path: ""index"""
89,writeBehind:
89,"modificationQueueSize: ""2048"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence().passivation(true)
89,.addSoftIndexFileStore()
89,.shared(false)
89,".dataLocation(""data"")"
89,".indexLocation(""index"")"
89,.modificationQueueSize(2048);
89,6.8.2. Configuring single file cache stores
89,"If required, you can configure Infinispan to create single file stores."
89,Single file stores are deprecated.
89,You should use soft-index file stores for better performance and data consistency in comparison with single file stores.
89,Prerequisites
89,Enable global state and configure a global persistent location if you are configuring embedded caches.
89,Procedure
89,Add the persistence element to your cache configuration.
89,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
89,Include the single-file-store element.
89,Specify false as the value for the shared attribute.
89,Configure any other attributes as appropriate.
89,Include the write-behind element to configure the cache store as write behind instead of as write through.
89,Single file cache store configuration
89,XML
89,<distributed-cache>
89,"<persistence passivation=""true"">"
89,"<single-file-store shared=""false"""
89,"preload=""true""/>"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"" : {"
89,"""passivation"" : true,"
89,"""single-file-store"" : {"
89,"""shared"" : false,"
89,"""preload"" : true"
89,YAML
89,distributedCache:
89,persistence:
89,"passivation: ""true"""
89,singleFileStore:
89,"shared: ""false"""
89,"preload: ""true"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence().passivation(true)
89,.addStore(SingleFileStoreConfigurationBuilder.class)
89,.shared(false)
89,.preload(true);
89,6.9. JDBC connection factories
89,Infinispan provides different ConnectionFactory implementations that allow you to connect to databases.
89,You use JDBC connections with SQL cache stores and JDBC string-based caches stores.
89,Connection pools
89,Connection pools are suitable for standalone Infinispan deployments and are based on Agroal.
89,XML
89,<distributed-cache>
89,<persistence>
89,"<connection-pool connection-url=""jdbc:h2:mem:infinispan;DB_CLOSE_DELAY=-1"""
89,"username=""sa"""
89,"password=""changeme"""
89,"driver=""org.h2.Driver""/>"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""connection-pool"": {"
89,"""connection-url"": ""jdbc:h2:mem:infinispan_string_based"","
89,"""driver"": ""org.h2.Driver"","
89,"""username"": ""sa"","
89,"""password"": ""changeme"""
89,YAML
89,distributedCache:
89,persistence:
89,connectionPool:
89,"connectionUrl: ""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"""
89,driver: org.h2.Driver
89,username: sa
89,password: changeme
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence()
89,.connectionPool()
89,".connectionUrl(""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"")"
89,".username(""sa"")"
89,".driverClass(""org.h2.Driver"");"
89,Managed datasources
89,Datasource connections are suitable for managed environments such as application servers.
89,XML
89,<distributed-cache>
89,<persistence>
89,"<data-source jndi-url=""java:/StringStoreWithManagedConnectionTest/DS"" />"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""data-source"": {"
89,"""jndi-url"": ""java:/StringStoreWithManagedConnectionTest/DS"""
89,YAML
89,distributedCache:
89,persistence:
89,dataSource:
89,"jndiUrl: ""java:/StringStoreWithManagedConnectionTest/DS"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence()
89,.dataSource()
89,".jndiUrl(""java:/StringStoreWithManagedConnectionTest/DS"");"
89,Simple connections
89,Simple connection factories create database connections on a per invocation basis and are intended for use with test or development environments only.
89,XML
89,<distributed-cache>
89,<persistence>
89,"<simple-connection connection-url=""jdbc:h2://localhost"""
89,"username=""sa"""
89,"password=""changeme"""
89,"driver=""org.h2.Driver""/>"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""simple-connection"": {"
89,"""connection-url"": ""jdbc:h2://localhost"","
89,"""driver"": ""org.h2.Driver"","
89,"""username"": ""sa"","
89,"""password"": ""changeme"""
89,YAML
89,distributedCache:
89,persistence:
89,simpleConnection:
89,"connectionUrl: ""jdbc:h2://localhost"""
89,driver: org.h2.Driver
89,username: sa
89,password: changeme
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence()
89,.simpleConnection()
89,".connectionUrl(""jdbc:h2://localhost"")"
89,".driverClass(""org.h2.Driver"")"
89,".username(""admin"")"
89,".password(""changeme"");"
89,Additional resources
89,PooledConnectionFactoryConfigurationBuilder
89,ManagedConnectionFactoryConfigurationBuilder
89,SimpleConnectionFactoryConfigurationBuilder
89,6.9.1. Configuring managed datasources
89,Create managed datasources as part of your Infinispan Server configuration to optimize connection pooling and performance for JDBC database connections.
89,"You can then specify the JDNI name of the managed datasources in your caches, which centralizes JDBC connection configuration for your deployment."
89,Prerequisites
89,Copy database drivers to the server/lib directory in your Infinispan Server installation.
89,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
89,install org.postgresql:postgresql:42.4.4
89,Procedure
89,Open your Infinispan Server configuration for editing.
89,Add a new data-source to the data-sources section.
89,Uniquely identify the datasource with the name attribute or field.
89,Specify a JNDI name for the datasource with the jndi-name attribute or field.
89,You use the JNDI name to specify the datasource in your JDBC cache store
89,configuration.
89,Set true as the value of the statistics attribute or field to enable statistics for the datasource through the /metrics endpoint.
89,Provide JDBC driver details that define how to connect to the datasource in the connection-factory section.
89,Specify the name of the database driver with the driver attribute or field.
89,Specify the JDBC connection url with the url attribute or field.
89,Specify credentials with the username and password attributes or fields.
89,Provide any other configuration as appropriate.
89,Define how Infinispan Server nodes pool and reuse connections with connection pool tuning properties in the connection-pool section.
89,Save the changes to your configuration.
89,Verification
89,"Use the Infinispan Command Line Interface (CLI) to test the datasource connection, as follows:"
89,Start a CLI session.
89,bin/cli.sh
89,List all datasources and confirm the one you created is available.
89,server datasource ls
89,Test a datasource connection.
89,server datasource test my-datasource
89,Managed datasource configuration
89,XML
89,"<server xmlns=""urn:infinispan:server:14.0"">"
89,<data-sources>
89,<!-- Defines a unique name for the datasource and JNDI name that you
89,reference in JDBC cache store configuration.
89,"Enables statistics for the datasource, if required. -->"
89,"<data-source name=""ds"""
89,"jndi-name=""jdbc/postgres"""
89,"statistics=""true"">"
89,<!-- Specifies the JDBC driver that creates connections. -->
89,"<connection-factory driver=""org.postgresql.Driver"""
89,"url=""jdbc:postgresql://localhost:5432/postgres"""
89,"username=""postgres"""
89,"password=""changeme"">"
89,<!-- Sets optional JDBC driver-specific connection properties. -->
89,"<connection-property name=""name"">value</connection-property>"
89,</connection-factory>
89,<!-- Defines connection pool tuning properties. -->
89,"<connection-pool initial-size=""1"""
89,"max-size=""10"""
89,"min-size=""3"""
89,"background-validation=""1000"""
89,"idle-removal=""1"""
89,"blocking-timeout=""1000"""
89,"leak-detection=""10000""/>"
89,</data-source>
89,</data-sources>
89,</server>
89,JSON
89,"""server"": {"
89,"""data-sources"": [{"
89,"""name"": ""ds"","
89,"""jndi-name"": ""jdbc/postgres"","
89,"""statistics"": true,"
89,"""connection-factory"": {"
89,"""driver"": ""org.postgresql.Driver"","
89,"""url"": ""jdbc:postgresql://localhost:5432/postgres"","
89,"""username"": ""postgres"","
89,"""password"": ""changeme"","
89,"""connection-properties"": {"
89,"""name"": ""value"""
89,"""connection-pool"": {"
89,"""initial-size"": 1,"
89,"""max-size"": 10,"
89,"""min-size"": 3,"
89,"""background-validation"": 1000,"
89,"""idle-removal"": 1,"
89,"""blocking-timeout"": 1000,"
89,"""leak-detection"": 10000"
89,YAML
89,server:
89,dataSources:
89,- name: ds
89,jndiName: 'jdbc/postgres'
89,statistics: true
89,connectionFactory:
89,"driver: ""org.postgresql.Driver"""
89,"url: ""jdbc:postgresql://localhost:5432/postgres"""
89,"username: ""postgres"""
89,"password: ""changeme"""
89,connectionProperties:
89,name: value
89,connectionPool:
89,initialSize: 1
89,maxSize: 10
89,minSize: 3
89,backgroundValidation: 1000
89,idleRemoval: 1
89,blockingTimeout: 1000
89,leakDetection: 10000
89,Configuring caches with JNDI names
89,When you add a managed datasource to Infinispan Server you can add the JNDI name to a JDBC-based cache store configuration.
89,Prerequisites
89,Configure Infinispan Server with a managed datasource.
89,Procedure
89,Open your cache configuration for editing.
89,Add the data-source element or field to the JDBC-based cache store configuration.
89,Specify the JNDI name of the managed datasource as the value of the jndi-url attribute.
89,Configure the JDBC-based cache stores as appropriate.
89,Save the changes to your configuration.
89,JNDI name in cache configuration
89,XML
89,<distributed-cache>
89,<persistence>
89,<jdbc:string-keyed-jdbc-store>
89,<!-- Specifies the JNDI name of a managed datasource on Infinispan Server. -->
89,"<jdbc:data-source jndi-url=""jdbc/postgres""/>"
89,"<jdbc:string-keyed-table drop-on-exit=""true"" create-on-start=""true"" prefix=""TBL"">"
89,"<jdbc:id-column name=""ID"" type=""VARCHAR(255)""/>"
89,"<jdbc:data-column name=""DATA"" type=""BYTEA""/>"
89,"<jdbc:timestamp-column name=""TS"" type=""BIGINT""/>"
89,"<jdbc:segment-column name=""S"" type=""INT""/>"
89,</jdbc:string-keyed-table>
89,</jdbc:string-keyed-jdbc-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""string-keyed-jdbc-store"": {"
89,"""data-source"": {"
89,"""jndi-url"": ""jdbc/postgres"""
89,"""string-keyed-table"": {"
89,"""prefix"": ""TBL"","
89,"""drop-on-exit"": true,"
89,"""create-on-start"": true,"
89,"""id-column"": {"
89,"""name"": ""ID"","
89,"""type"": ""VARCHAR(255)"""
89,"""data-column"": {"
89,"""name"": ""DATA"","
89,"""type"": ""BYTEA"""
89,"""timestamp-column"": {"
89,"""name"": ""TS"","
89,"""type"": ""BIGINT"""
89,"""segment-column"": {"
89,"""name"": ""S"","
89,"""type"": ""INT"""
89,YAML
89,distributedCache:
89,persistence:
89,stringKeyedJdbcStore:
89,dataSource:
89,"jndi-url: ""jdbc/postgres"""
89,stringKeyedTable:
89,"prefix: ""TBL"""
89,dropOnExit: true
89,createOnStart: true
89,idColumn:
89,"name: ""ID"""
89,"type: ""VARCHAR(255)"""
89,dataColumn:
89,"name: ""DATA"""
89,"type: ""BYTEA"""
89,timestampColumn:
89,"name: ""TS"""
89,"type: ""BIGINT"""
89,segmentColumn:
89,"name: ""S"""
89,"type: ""INT"""
89,Connection pool tuning properties
89,You can tune JDBC connection pools for managed datasources in your Infinispan Server configuration.
89,Property
89,Description
89,initial-size
89,Initial number of connections the pool should hold.
89,max-size
89,Maximum number of connections in the pool.
89,min-size
89,Minimum number of connections the pool should hold.
89,blocking-timeout
89,Maximum time in milliseconds to block while waiting for a connection before throwing an exception.
89,This will never throw an exception if creating a new connection takes an inordinately long period of time.
89,Default is 0 meaning that a call will wait indefinitely.
89,background-validation
89,Time in milliseconds between background validation runs. A duration of 0 means that this feature is disabled.
89,validate-on-acquisition
89,"Connections idle for longer than this time, specified in milliseconds, are validated before being acquired (foreground validation). A duration of 0 means that this feature is disabled."
89,idle-removal
89,Time in minutes a connection has to be idle before it can be removed.
89,leak-detection
89,Time in milliseconds a connection has to be held before a leak warning.
89,6.9.2. Configuring JDBC connection pools with Agroal properties
89,You can use a properties file to configure pooled connection factories for JDBC string-based cache stores.
89,Procedure
89,"Specify JDBC connection pool configuration with org.infinispan.agroal.* properties, as in the following example:"
89,org.infinispan.agroal.metricsEnabled=false
89,org.infinispan.agroal.minSize=10
89,org.infinispan.agroal.maxSize=100
89,org.infinispan.agroal.initialSize=20
89,org.infinispan.agroal.acquisitionTimeout_s=1
89,org.infinispan.agroal.validationTimeout_m=1
89,org.infinispan.agroal.leakTimeout_s=10
89,org.infinispan.agroal.reapTimeout_m=10
89,org.infinispan.agroal.metricsEnabled=false
89,org.infinispan.agroal.autoCommit=true
89,org.infinispan.agroal.jdbcTransactionIsolation=READ_COMMITTED
89,org.infinispan.agroal.jdbcUrl=jdbc:h2:mem:PooledConnectionFactoryTest;DB_CLOSE_DELAY=-1
89,org.infinispan.agroal.driverClassName=org.h2.Driver.class
89,org.infinispan.agroal.principal=sa
89,org.infinispan.agroal.credential=sa
89,Configure Infinispan to use your properties file with the properties-file attribute or the PooledConnectionFactoryConfiguration.propertyFile() method.
89,XML
89,"<connection-pool properties-file=""path/to/agroal.properties""/>"
89,JSON
89,"""persistence"": {"
89,"""connection-pool"": {"
89,"""properties-file"": ""path/to/agroal.properties"""
89,YAML
89,persistence:
89,connectionPool:
89,propertiesFile: path/to/agroal.properties
89,ConfigurationBuilder
89,".connectionPool().propertyFile(""path/to/agroal.properties"")"
89,Additional resources
89,Agroal
89,6.10. SQL cache stores
89,SQL cache stores let you load Infinispan caches from existing database tables.
89,Infinispan offers two types of SQL cache store:
89,Table
89,Infinispan loads entries from a single database table.
89,Query
89,"Infinispan uses SQL queries to load entries from single or multiple database tables, including from sub-columns within those tables, and perform insert, update, and delete operations."
89,Visit the code tutorials to try a SQL cache store in action.
89,See the Persistence code tutorial with remote caches.
89,Both SQL table and query stores:
89,Allow read and write operations to persistent storage.
89,Can be read-only and act as a cache loader.
89,Support keys and values that correspond to a single database column or a composite of multiple database columns.
89,"For composite keys and values, you must provide Infinispan with Protobuf schema (.proto files) that describe the keys and values."
89,With Infinispan Server you can add schema through the Infinispan Console or Command Line Interface (CLI) with the schema command.
89,The SQL cache store is intended for use with an existing database table.
89,"As a result, it does not store any metadata, which includes expiration, segments, and, versioning metadata."
89,"Due to the absence of version storage, SQL store does not support optimistic transactional caching and asynchronous cross-site replication."
89,This limitation also extends to Hot Rod versioned operations.
89,Use expiration with the SQL cache store when it is configured as read only.
89,"Expiration removes stale values from memory, causing the cache to fetch the values from the database again and cache them anew."
89,Additional resources
89,DatabaseType Enum lists supported database dialects
89,Infinispan SQL store configuration reference
89,6.10.1. Data types for keys and values
89,"Infinispan loads keys and values from columns in database tables via SQL cache stores, automatically using the appropriate data types."
89,"The following CREATE statement adds a table named ""books"" that has two columns, isbn and title:"
89,Database table with two columns
89,CREATE TABLE books (
89,"isbn NUMBER(13),"
89,title varchar(120)
89,PRIMARY KEY(isbn)
89,"When you use this table with a SQL cache store, Infinispan adds an entry to the cache using the isbn column as the key and the title column as the value."
89,Additional resources
89,Infinispan SQL store configuration reference
89,Composite keys and values
89,You can use SQL stores with database tables that contain composite primary keys or composite values.
89,"To use composite keys or values, you must provide Infinispan with Protobuf schema that describe the data types."
89,You must also add schema configuration to your SQL store and specify the message names for keys and values.
89,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
89,"You can then upload your Protobuf schema for remote caches through the Infinispan Console, CLI, or REST API."
89,Composite values
89,The following database table holds a composite value of the title and author columns:
89,CREATE TABLE books (
89,"isbn NUMBER(13),"
89,"title varchar(120),"
89,author varchar(80)
89,PRIMARY KEY(isbn)
89,Infinispan adds an entry to the cache using the isbn column as the key.
89,"For the value, Infinispan requires a Protobuf schema that maps the title column and the author columns:"
89,package library;
89,message books_value {
89,optional string title = 1;
89,optional string author = 2;
89,Composite keys and values
89,"The following database table holds a composite primary key and a composite value, with two columns each:"
89,CREATE TABLE books (
89,"isbn NUMBER(13),"
89,"reprint INT,"
89,"title varchar(120),"
89,author varchar(80)
89,"PRIMARY KEY(isbn, reprint)"
89,"For both the key and the value, Infinispan requires a Protobuf schema that maps the columns to keys and values:"
89,package library;
89,message books_key {
89,required string isbn = 1;
89,required int32 reprint = 2;
89,message books_value {
89,optional string title = 1;
89,optional string author = 2;
89,Additional resources
89,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
89,Infinispan SQL store configuration reference
89,Embedded keys
89,"Protobuf schema can include keys within values, as in the following example:"
89,Protobuf schema with an embedded key
89,package library;
89,message books_key {
89,required string isbn = 1;
89,required int32 reprint = 2;
89,message books_value {
89,required string isbn = 1;
89,required string reprint = 2;
89,optional string title = 3;
89,optional string author = 4;
89,"To use embedded keys, you must include the embedded-key=""true"" attribute or embeddedKey(true) method in your SQL store configuration."
89,SQL types to Protobuf types
89,The following table contains default mappings of SQL data types to Protobuf data types:
89,SQL type
89,Protobuf type
89,int4
89,int32
89,int8
89,int64
89,float4
89,float
89,float8
89,double
89,numeric
89,double
89,bool
89,bool
89,char
89,string
89,varchar
89,string
89,"text, tinytext, mediumtext, longtext"
89,string
89,"bytea, tinyblob, blob, mediumblob, longblob"
89,bytes
89,Additional resources
89,Cache encoding and marshalling
89,6.10.2. Loading Infinispan caches from database tables
89,Add a SQL table cache store to your configuration if you want Infinispan to load data from a database table.
89,"When it connects to the database, Infinispan uses metadata from the table to detect column names and data types."
89,Infinispan also automatically determines which columns in the database are part of the primary key.
89,Prerequisites
89,Have JDBC connection details.
89,You can add JDBC connection factories directly to your cache configuration.
89,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
89,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
89,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
89,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
89,Procedure
89,Add database drivers to your Infinispan deployment.
89,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
89,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
89,install org.postgresql:postgresql:42.4.4
89,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file.
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-cachestore-sql</artifactId>
89,</dependency>
89,Open your Infinispan configuration for editing.
89,Add a SQL table cache store.
89,Declarative
89,"table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
89,Programmatic
89,persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
89,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
89,"Configure the SQL cache store with the properties you require, for example:"
89,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
89,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
89,"Name the database table that loads the cache with table-name=""<database_table_name>"" or table.name(""<database_table_name>"")."
89,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
89,Specify the package name with the package attribute or package() method.
89,Specify composite values with the message-name attribute or messageName() method.
89,Specify composite keys with the key-message-name attribute or keyMessageName() method.
89,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
89,Save the changes to your configuration.
89,SQL table store configuration
89,"The following example loads a distributed cache from a database table named ""books"" using composite values defined in a Protobuf schema:"
89,XML
89,<distributed-cache>
89,<persistence>
89,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
89,"dialect=""H2"""
89,"shared=""true"""
89,"table-name=""books"">"
89,"<schema message-name=""books_value"""
89,"package=""library""/>"
89,</table-jdbc-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""table-jdbc-store"": {"
89,"""dialect"": ""H2"","
89,"""shared"": ""true"","
89,"""table-name"": ""books"","
89,"""schema"": {"
89,"""message-name"": ""books_value"","
89,"""package"": ""library"""
89,YAML
89,distributedCache:
89,persistence:
89,tableJdbcStore:
89,"dialect: ""H2"""
89,"shared: ""true"""
89,"tableName: ""books"""
89,schema:
89,"messageName: ""books_value"""
89,"package: ""library"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
89,.dialect(DatabaseType.H2)
89,".shared(""true"")"
89,".tableName(""books"")"
89,.schemaJdbcConfigurationBuilder()
89,".messageName(""books_value"")"
89,".packageName(""library"");"
89,Additional resources
89,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
89,Persistence code tutorial with remote caches
89,JDBC connection factories
89,DatabaseType Enum lists supported database dialects
89,Infinispan SQL store configuration reference
89,6.10.3. Using SQL queries to load data and perform operations
89,"SQL query cache stores let you load caches from multiple database tables, including from sub-columns in database tables, and perform insert, update, and delete operations."
89,Prerequisites
89,Have JDBC connection details.
89,You can add JDBC connection factories directly to your cache configuration.
89,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
89,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
89,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
89,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
89,Procedure
89,Add database drivers to your Infinispan deployment.
89,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
89,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
89,install org.postgresql:postgresql:42.4.4
89,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file and make sure database drivers are on your application classpath.
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-cachestore-sql</artifactId>
89,</dependency>
89,Open your Infinispan configuration for editing.
89,Add a SQL query cache store.
89,Declarative
89,"query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
89,Programmatic
89,persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
89,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
89,"Configure the SQL cache store with the properties you require, for example:"
89,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
89,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
89,Define SQL query statements that load caches with data and modify database tables with the queries element or the queries() method.
89,Query statement
89,Description
89,SELECT
89,Loads a single entry into caches.
89,You can use wildcards but must specify parameters for keys.
89,You can use labelled expressions.
89,SELECT ALL
89,Loads multiple entries into caches.
89,You can use the * wildcard if the number of columns returned match the key and value columns.
89,You can use labelled expressions.
89,SIZE
89,Counts the number of entries in the cache.
89,DELETE
89,Deletes a single entry from the cache.
89,DELETE ALL
89,Deletes all entries from the cache.
89,UPSERT
89,Modifies entries in the cache.
89,"DELETE, DELETE ALL, and UPSERT statements do not apply to read only cache stores but are required if cache stores allow modifications."
89,Parameters in DELETE statements must match parameters in SELECT statements exactly.
89,Variables in UPSERT statements must have the same number of uniquely named variables that SELECT and SELECT ALL statements return.
89,"For example, if SELECT returns foo and bar this statement must take only :foo and :bar as variables."
89,However you can apply the same named variable more than once in a statement.
89,"SQL queries can include JOIN, ON, and any other clauses that the database supports."
89,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
89,Specify the package name with the package attribute or package() method.
89,Specify composite values with the message-name attribute or messageName() method.
89,Specify composite keys with the key-message-name attribute or keyMessageName() method.
89,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
89,Save the changes to your configuration.
89,Additional resources
89,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
89,Persistence code tutorial with remote caches
89,JDBC connection factories
89,DatabaseType Enum lists supported database dialects
89,Infinispan SQL store configuration reference
89,SQL query store configuration
89,"This section provides an example configuration for a SQL query cache store that loads a distributed cache with data from two database tables: ""person"" and ""address""."
89,SQL statements
89,"The following examples show SQL data definition language (DDL) statements for the ""person"" and ""address"" tables."
89,The data types described in the example are only valid for PostgreSQL database.
89,"SQL statement for the ""person"" table"
89,CREATE TABLE Person (
89,"name VARCHAR(255) NOT NULL,"
89,"picture BYTEA,"
89,"sex VARCHAR(255),"
89,"birthdate TIMESTAMP,"
89,"accepted_tos BOOLEAN,"
89,"notused VARCHAR(255),"
89,PRIMARY KEY (name)
89,"SQL statement for the ""address"" table"
89,CREATE TABLE Address (
89,"name VARCHAR(255) NOT NULL,"
89,"street VARCHAR(255),"
89,"city VARCHAR(255),"
89,"zip INT,"
89,PRIMARY KEY (name)
89,Protobuf schemas
89,"Protobuf schema for the ""person"" and ""address"" tables are as follows:"
89,"Protobuf schema for the ""address"" table"
89,package com.example;
89,message Address {
89,optional string street = 1;
89,"optional string city = 2 [default = ""San Jose""];"
89,optional int32 zip = 3 [default = 0];
89,"Protobuf schema for the ""person"" table"
89,package com.example;
89,import com.example.Address;
89,enum Sex {
89,FEMALE = 1;
89,MALE = 2;
89,message Person {
89,optional string name = 1;
89,optional Address address = 2;
89,optional bytes picture = 3;
89,optional Sex sex = 4;
89,optional fixed64 birthDate = 5 [default = 0];
89,optional bool accepted_tos = 6 [default = false];
89,Cache configuration
89,"The following example loads a distributed cache from the ""person"" and ""address"" tables using a SQL query that includes a JOIN clause:"
89,XML
89,<distributed-cache>
89,<persistence>
89,"<query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
89,"dialect=""POSTGRES"""
89,"shared=""true"" key-columns=""name"">"
89,"<connection-pool driver=""org.postgresql.Driver"""
89,"connection-url=""jdbc:postgresql://localhost:5432/postgres"""
89,"username=""postgres"""
89,"password=""changeme""/>"
89,"<queries select-single=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
89,"select-all=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
89,"delete-single=""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
89,"delete-all=""DELETE FROM Person; DELETE FROM Address"""
89,"upsert=""INSERT INTO Person (name,"
89,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
89,"size=""SELECT COUNT(*) FROM Person"""
89,"<schema message-name=""Person"""
89,"package=""com.example"""
89,"embedded-key=""true""/>"
89,</query-jdbc-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""query-jdbc-store"": {"
89,"""dialect"": ""POSTGRES"","
89,"""shared"": ""true"","
89,"""key-columns"": ""name"","
89,"""queries"": {"
89,"""select-single"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"","
89,"""select-all"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"","
89,"""delete-single"": ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"","
89,"""delete-all"": ""DELETE FROM Person; DELETE FROM Address"","
89,"""upsert"": ""INSERT INTO Person (name,"
89,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"","
89,"""size"": ""SELECT COUNT(*) FROM Person"""
89,"""schema"": {"
89,"""message-name"": ""Person"","
89,"""package"": ""com.example"","
89,"""embedded-key"": ""true"""
89,YAML
89,distributedCache:
89,persistence:
89,queryJdbcStore:
89,"dialect: ""POSTGRES"""
89,"shared: ""true"""
89,"keyColumns: ""name"""
89,queries:
89,"selectSingle: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
89,"selectAll: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
89,"deleteSingle: ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
89,"deleteAll: ""DELETE FROM Person; DELETE FROM Address"""
89,"upsert: ""INSERT INTO Person (name,"
89,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
89,"size: ""SELECT COUNT(*) FROM Person"""
89,schema:
89,"messageName: ""Person"""
89,"package: ""com.example"""
89,"embeddedKey: ""true"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
89,.dialect(DatabaseType.POSTGRES)
89,".shared(""true"")"
89,".keyColumns(""name"")"
89,.queriesJdbcConfigurationBuilder()
89,".select(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"")"
89,".selectAll(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"")"
89,".delete(""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"")"
89,".deleteAll(""DELETE FROM Person; DELETE FROM Address"")"
89,".upsert(""INSERT INTO Person (name,"
89,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"")"
89,".size(""SELECT COUNT(*) FROM Person"")"
89,.schemaJdbcConfigurationBuilder()
89,".messageName(""Person"")"
89,".packageName(""com.example"")"
89,.embeddedKey(true);
89,Additional resources
89,Infinispan SQL store configuration reference
89,6.10.4. SQL cache store troubleshooting
89,Find out about common issues and errors with SQL cache stores and how to troubleshoot them.
89,"ISPN008064: No primary keys found for table <table_name>, check case sensitivity"
89,Infinispan logs this message in the following cases:
89,The database table does not exist.
89,"The database table name is case sensitive and needs to be either all lower case or all upper case, depending on the database provider."
89,The database table does not have any primary keys defined.
89,To resolve this issue you should:
89,Check your SQL cache store configuration and ensure that you specify the name of an existing table.
89,Ensure that the database table name conforms to an case sensitivity requirements.
89,Ensure that your database tables have primary keys that uniquely identify the appropriate rows.
89,6.11. JDBC string-based cache stores
89,"JDBC String-Based cache stores, JdbcStringBasedStore, use JDBC drivers to load and store values in the underlying database."
89,JDBC String-Based cache stores:
89,Store each entry in its own row in the table to increase throughput for concurrent loads.
89,Use a simple one-to-one mapping that maps each key to a String object using the key-to-string-mapper interface.
89,"Infinispan provides a default implementation, DefaultTwoWayKey2StringMapper, that handles primitive types."
89,"In addition to the data table used to store cache entries, the store also creates a _META table for storing metadata."
89,This table is used to ensure that any existing database content is compatible with the current Infinispan version and configuration.
89,"By default Infinispan shares are not stored, which means that all nodes in the"
89,cluster write to the underlying store on each update. If you want operations to
89,"write to the underlying database once only, you must configure JDBC store as"
89,shared.
89,Segmentation
89,JdbcStringBasedStore uses segmentation by default and requires a column in the database table to represent the segments to which entries belong.
89,Additional resources
89,DatabaseType Enum lists supported database dialects
89,6.11.1. Configuring JDBC string-based cache stores
89,Configure Infinispan caches with JDBC string-based cache stores that can connect to databases.
89,Prerequisites
89,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
89,Embedded caches: Add the infinispan-cachestore-jdbc dependency to your pom file.
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-cachestore-jdbc</artifactId>
89,</dependency>
89,Procedure
89,Create a JDBC string-based cache store configuration in one of the following ways:
89,"Declaratively, add the persistence element or field then add string-keyed-jdbc-store with the following schema namespace:"
89,"xmlns=""urn:infinispan:config:store:jdbc:14.0"""
89,"Programmatically, add the following methods to your ConfigurationBuilder:"
89,persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
89,Specify the dialect of the database with either the dialect attribute or the dialect() method.
89,Configure any properties for the JDBC string-based cache store as appropriate.
89,"For example, specify if the cache store is shared with multiple cache instances with either the shared attribute or the shared() method."
89,Add a JDBC connection factory so that Infinispan can connect to the database.
89,Add a database table that stores cache entries.
89,JDBC string-based cache store configuration
89,XML
89,<distributed-cache>
89,<persistence>
89,"<string-keyed-jdbc-store xmlns=""urn:infinispan:config:store:jdbc:14.0"""
89,"dialect=""H2"">"
89,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
89,"username=""sa"""
89,"password=""changeme"""
89,"driver=""org.h2.Driver""/>"
89,"<string-keyed-table create-on-start=""true"""
89,"prefix=""ISPN_STRING_TABLE"">"
89,"<id-column name=""ID_COLUMN"""
89,"type=""VARCHAR(255)"" />"
89,"<data-column name=""DATA_COLUMN"""
89,"type=""BINARY"" />"
89,"<timestamp-column name=""TIMESTAMP_COLUMN"""
89,"type=""BIGINT"" />"
89,"<segment-column name=""SEGMENT_COLUMN"""
89,"type=""INT""/>"
89,</string-keyed-table>
89,</string-keyed-jdbc-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"": {"
89,"""string-keyed-jdbc-store"": {"
89,"""dialect"": ""H2"","
89,"""string-keyed-table"": {"
89,"""prefix"": ""ISPN_STRING_TABLE"","
89,"""create-on-start"": true,"
89,"""id-column"": {"
89,"""name"": ""ID_COLUMN"","
89,"""type"": ""VARCHAR(255)"""
89,"""data-column"": {"
89,"""name"": ""DATA_COLUMN"","
89,"""type"": ""BINARY"""
89,"""timestamp-column"": {"
89,"""name"": ""TIMESTAMP_COLUMN"","
89,"""type"": ""BIGINT"""
89,"""segment-column"": {"
89,"""name"": ""SEGMENT_COLUMN"","
89,"""type"": ""INT"""
89,"""connection-pool"": {"
89,"""connection-url"": ""jdbc:h2:mem:infinispan"","
89,"""driver"": ""org.h2.Driver"","
89,"""username"": ""sa"","
89,"""password"": ""changeme"""
89,YAML
89,distributedCache:
89,persistence:
89,stringKeyedJdbcStore:
89,"dialect: ""H2"""
89,stringKeyedTable:
89,"prefix: ""ISPN_STRING_TABLE"""
89,createOnStart: true
89,idColumn:
89,"name: ""ID_COLUMN"""
89,"type: ""VARCHAR(255)"""
89,dataColumn:
89,"name: ""DATA_COLUMN"""
89,"type: ""BINARY"""
89,timestampColumn:
89,"name: ""TIMESTAMP_COLUMN"""
89,"type: ""BIGINT"""
89,segmentColumn:
89,"name: ""SEGMENT_COLUMN"""
89,"type: ""INT"""
89,connectionPool:
89,"connectionUrl: ""jdbc:h2:mem:infinispan"""
89,"driver: ""org.h2.Driver"""
89,"username: ""sa"""
89,"password: ""changeme"""
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
89,.dialect(DatabaseType.H2)
89,.table()
89,.dropOnExit(true)
89,.createOnStart(true)
89,".tableNamePrefix(""ISPN_STRING_TABLE"")"
89,".idColumnName(""ID_COLUMN"").idColumnType(""VARCHAR(255)"")"
89,".dataColumnName(""DATA_COLUMN"").dataColumnType(""BINARY"")"
89,".timestampColumnName(""TIMESTAMP_COLUMN"").timestampColumnType(""BIGINT"")"
89,".segmentColumnName(""SEGMENT_COLUMN"").segmentColumnType(""INT"")"
89,.connectionPool()
89,".connectionUrl(""jdbc:h2:mem:infinispan"")"
89,".username(""sa"")"
89,".password(""changeme"")"
89,".driverClass(""org.h2.Driver"");"
89,Additional resources
89,JDBC connection factories
89,6.12. RocksDB cache stores
89,RocksDB provides key-value filesystem-based storage with high performance and
89,reliability for highly concurrent environments.
89,"RocksDB cache stores, RocksDBStore, use two databases. One database provides"
89,a primary cache store for data in memory; the other database holds entries that
89,Infinispan expires from memory.
89,Table 1. Configuration parameters
89,Parameter
89,Description
89,location
89,Specifies the path to the RocksDB database that provides the primary cache
89,"store. If you do not set the location, it is automatically created. Note that"
89,the path must be relative to the global persistent location.
89,expiredLocation
89,Specifies the path to the RocksDB database that provides the cache store for
89,"expired data. If you do not set the location, it is automatically created. Note"
89,that the path must be relative to the global persistent location.
89,expiryQueueSize
89,Sets the size of the in-memory queue for expiring entries. When the queue
89,"reaches the size, Infinispan flushes the expired into the RocksDB cache store."
89,clearThreshold
89,Sets the maximum number of entries before deleting and re-initializing
89,"(re-init) the RocksDB database. For smaller size cache stores, iterating"
89,through all entries and removing each one individually can provide a faster
89,method.
89,Tuning parameters
89,You can also specify the following RocksDB tuning parameters:
89,compressionType
89,blockSize
89,cacheSize
89,Configuration properties
89,Optionally set properties in the configuration as follows:
89,Prefix properties with database to adjust and tune RocksDB databases.
89,Prefix properties with data to configure the column families in which RocksDB stores your data.
89,"<property name=""database.max_background_compactions"">2</property>"
89,"<property name=""data.write_buffer_size"">64MB</property>"
89,"<property name=""data.compression_per_level"">kNoCompression:kNoCompression:kNoCompression:kSnappyCompression:kZSTD:kZSTD</property>"
89,Segmentation
89,RocksDBStore supports segmentation and creates a separate column family per
89,segment. Segmented RocksDB cache stores improve lookup performance
89,and iteration but slightly lower performance of write operations.
89,You should not configure more than a few hundred segments. RocksDB is not
89,designed to have an unlimited number of column families. Too many segments also
89,significantly increases cache store start time.
89,RocksDB cache store configuration
89,XML
89,<local-cache>
89,<persistence>
89,"<rocksdb-store xmlns=""urn:infinispan:config:store:rocksdb:14.0"""
89,"path=""rocksdb/data"">"
89,"<expiration path=""rocksdb/expired""/>"
89,</rocksdb-store>
89,</persistence>
89,</local-cache>
89,JSON
89,"""local-cache"": {"
89,"""persistence"": {"
89,"""rocksdb-store"": {"
89,"""path"": ""rocksdb/data"","
89,"""expiration"": {"
89,"""path"": ""rocksdb/expired"""
89,YAML
89,localCache:
89,persistence:
89,rocksdbStore:
89,"path: ""rocksdb/data"""
89,expiration:
89,"path: ""rocksdb/expired"""
89,ConfigurationBuilder
89,Configuration cacheConfig = new ConfigurationBuilder().persistence()
89,.addStore(RocksDBStoreConfigurationBuilder.class)
89,.build();
89,EmbeddedCacheManager cacheManager = new DefaultCacheManager(cacheConfig);
89,"Cache<String, User> usersCache = cacheManager.getCache(""usersCache"");"
89,"usersCache.put(""raytsang"", new User(...));"
89,ConfigurationBuilder with properties
89,Properties props = new Properties();
89,"props.put(""database.max_background_compactions"", ""2"");"
89,"props.put(""data.write_buffer_size"", ""512MB"");"
89,Configuration cacheConfig = new ConfigurationBuilder().persistence()
89,.addStore(RocksDBStoreConfigurationBuilder.class)
89,".location(""rocksdb/data"")"
89,".expiredLocation(""rocksdb/expired"")"
89,.properties(props)
89,.build();
89,Reference
89,RocksDB cache store configuration schema
89,RocksDBStore
89,RocksDBStoreConfiguration
89,rocksdb.org
89,RocksDB Tuning Guide
89,RocksDB Cache Store test
89,RocksDB Cache Store test configuration
89,6.13. Remote cache stores
89,"Remote cache stores, RemoteStore, use the Hot Rod protocol to store data on"
89,Infinispan clusters.
89,If you configure remote cache stores as shared you cannot preload data.
89,"In other words if shared=""true"" in your configuration then you must set preload=""false""."
89,Segmentation
89,RemoteStore supports segmentation and can publish keys and entries by
89,"segment, which makes bulk operations more efficient. However, segmentation is"
89,available only with Infinispan Hot Rod protocol version 2.3 or later.
89,"When you enable segmentation for RemoteStore, it uses the number of segments"
89,that you define in your Infinispan server configuration.
89,If the source cache is segmented and uses a different number of segments than
89,"RemoteStore, then incorrect values are returned for bulk operations. In this"
89,"case, you should disable segmentation for RemoteStore."
89,Remote cache store configuration
89,XML
89,<distributed-cache>
89,<persistence>
89,"<remote-store xmlns=""urn:infinispan:config:store:remote:14.0"""
89,"cache=""mycache"""
89,"raw-values=""true"">"
89,"<remote-server host=""one"""
89,"port=""12111"" />"
89,"<remote-server host=""two"" />"
89,"<connection-pool max-active=""10"""
89,"exhausted-action=""CREATE_NEW"" />"
89,</remote-store>
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""remote-store"": {"
89,"""cache"": ""mycache"","
89,"""raw-values"": ""true"","
89,"""remote-server"": ["
89,"""host"": ""one"","
89,"""port"": ""12111"""
89,"""host"": ""two"""
89,"""connection-pool"": {"
89,"""max-active"": ""10"","
89,"""exhausted-action"": ""CREATE_NEW"""
89,YAML
89,distributedCache:
89,remoteStore:
89,"cache: ""mycache"""
89,"rawValues: ""true"""
89,remoteServer:
89,"- host: ""one"""
89,"port: ""12111"""
89,"- host: ""two"""
89,connectionPool:
89,"maxActive: ""10"""
89,"exhaustedAction: ""CREATE_NEW"""
89,ConfigurationBuilder
89,ConfigurationBuilder b = new ConfigurationBuilder();
89,b.persistence().addStore(RemoteStoreConfigurationBuilder.class)
89,.ignoreModifications(false)
89,.purgeOnStartup(false)
89,".remoteCacheName(""mycache"")"
89,.rawValues(true)
89,.addServer()
89,".host(""one"").port(12111)"
89,.addServer()
89,".host(""two"")"
89,.connectionPool()
89,.maxActive(10)
89,.exhaustedAction(ExhaustedAction.CREATE_NEW)
89,.async().enable();
89,Reference
89,Remote cache store configuration schema
89,RemoteStore
89,RemoteStoreConfigurationBuilder
89,6.14. Cluster cache loaders
89,ClusterCacheLoader retrieves data from other Infinispan cluster members but
89,"does not persist data. In other words, ClusterCacheLoader is not a cache"
89,store.
89,ClusterLoader is deprecated and planned for removal in a future version.
89,ClusterCacheLoader provides a non-blocking partial alternative to state
89,transfer. ClusterCacheLoader fetches keys from other nodes on demand if those
89,"keys are not available on the local node, which is similar to lazily loading"
89,cache content.
89,The following points also apply to ClusterCacheLoader:
89,Preloading does not take effect (preload=true).
89,Segmentation is not supported.
89,Cluster cache loader configuration
89,XML
89,<distributed-cache>
89,<persistence>
89,"<cluster-loader preload=""true"" remote-timeout=""500""/>"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"" : {"
89,"""cluster-loader"" : {"
89,"""preload"" : true,"
89,"""remote-timeout"" : ""500"""
89,YAML
89,distributedCache:
89,persistence:
89,clusterLoader:
89,"preload: ""true"""
89,"remoteTimeout: ""500"""
89,ConfigurationBuilder
89,ConfigurationBuilder b = new ConfigurationBuilder();
89,b.persistence()
89,.addClusterLoader()
89,.remoteCallTimeout(500);
89,Additional resources
89,Infinispan configuration schema
89,ClusterLoader
89,ClusterLoaderConfiguration
89,6.15. Creating custom cache store implementations
89,You can create custom cache stores through the Infinispan persistent SPI.
89,6.15.1. Infinispan Persistence SPI
89,The Infinispan Service Provider Interface (SPI) enables read and write
89,operations to external storage through the NonBlockingStore interface and has
89,the following features:
89,Portability across JCache-compliant vendors
89,Infinispan maintains compatibility between the NonBlockingStore interface
89,and the JSR-107 JCache specification by using an adapter that handles
89,blocking code.
89,Simplified transaction integration
89,Infinispan automatically handles locking so your implementations do not need
89,to coordinate concurrent access to persistent stores. Depending on the locking
89,"mode you use, concurrent writes to the same key generally do not occur."
89,"However, you should expect operations on the persistent storage to originate"
89,from multiple threads and create implementations to tolerate this behavior.
89,Parallel iteration
89,Infinispan lets you iterate over entries in persistent stores with multiple
89,threads in parallel.
89,Reduced serialization resulting in less CPU usage
89,Infinispan exposes stored entries in a serialized format that can be
89,"transmitted remotely. For this reason, Infinispan does not need to deserialize"
89,entries that it retrieves from persistent storage and then serialize again when
89,writing to the wire.
89,Additional resources
89,Persistence SPI
89,NonBlockingStore
89,JSR-107
89,6.15.2. Creating cache stores
89,Create custom cache stores with implementations of the NonBlockingStore API.
89,Procedure
89,Implement the appropriate Infinispan persistent SPIs.
89,Annotate your store class with the @ConfiguredBy annotation if it has a custom configuration.
89,Create a custom cache store configuration and builder if desired.
89,Extend AbstractStoreConfiguration and AbstractStoreConfigurationBuilder.
89,Optionally add the following annotations to your store Configuration class to ensure that your
89,custom configuration builder parses your cache store configuration from XML:
89,@ConfigurationFor
89,@BuiltBy
89,"If you do not add these annotations, then CustomStoreConfigurationBuilder parses the common"
89,store attributes defined in AbstractStoreConfiguration and any additional elements are ignored.
89,If a configuration does not declare the
89,"@ConfigurationFor annotation, a warning message is logged when Infinispan"
89,initializes the cache.
89,6.15.3. Examples of custom cache store configuration
89,The following are examples show how to configure Infinispan with custom cache store implementations:
89,XML
89,<distributed-cache>
89,<persistence>
89,"<store class=""org.infinispan.persistence.example.MyInMemoryStore"" />"
89,</persistence>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""persistence"" : {"
89,"""store"" : {"
89,"""class"" : ""org.infinispan.persistence.example.MyInMemoryStore"""
89,YAML
89,distributedCache:
89,persistence:
89,store:
89,"class: ""org.infinispan.persistence.example.MyInMemoryStore"""
89,ConfigurationBuilder
89,Configuration config = new ConfigurationBuilder()
89,.persistence()
89,.addStore(CustomStoreConfigurationBuilder.class)
89,.build();
89,6.15.4. Deploying custom cache stores
89,"To use your cache store implementation with Infinispan Server, you must provide it with a JAR file."
89,Prerequisites
89,Stop Infinispan Server if it is running.
89,Infinispan loads JAR files at startup only.
89,Procedure
89,Package your custom cache store implementation in a JAR file.
89,Add your JAR file to the server/lib directory of your Infinispan Server installation.
89,6.16. Migrating data between cache stores
89,Infinispan provides a utility to migrate data from one cache store to another.
89,6.16.1. Cache store migrator
89,Infinispan provides the StoreMigrator.java utility that recreates data for the latest Infinispan cache store implementations.
89,StoreMigrator takes a cache store from a previous version of Infinispan as source and uses a cache store implementation as target.
89,"When you run StoreMigrator, it creates the target cache with the cache store type that you define using the EmbeddedCacheManager interface."
89,StoreMigrator then loads entries from the source store into memory and then puts them into the target cache.
89,"StoreMigrator also lets you migrate data from one type of cache store to another. For example, you can migrate from a JDBC string-based cache store to a RocksDB cache store."
89,StoreMigrator cannot migrate data from segmented cache stores to:
89,Non-segmented cache store.
89,Segmented cache stores that have a different number of segments.
89,6.16.2. Getting the cache store migrator
89,"StoreMigrator is available as part of the Infinispan tools library,"
89,"infinispan-tools, and is included in the Maven repository."
89,Procedure
89,Configure your pom.xml for StoreMigrator as follows:
89,"<?xml version=""1.0"" encoding=""UTF-8""?>"
89,"<project xmlns=""http://maven.apache.org/POM/4.0.0"""
89,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
89,"xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">"
89,<modelVersion>4.0.0</modelVersion>
89,<groupId>org.infinispan.example</groupId>
89,<artifactId>jdbc-migrator-example</artifactId>
89,<version>1.0-SNAPSHOT</version>
89,<dependencies>
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-tools</artifactId>
89,</dependency>
89,<!-- Additional dependencies -->
89,</dependencies>
89,<build>
89,<plugins>
89,<plugin>
89,<groupId>org.codehaus.mojo</groupId>
89,<artifactId>exec-maven-plugin</artifactId>
89,<version>1.2.1</version>
89,<executions>
89,<execution>
89,<goals>
89,<goal>java</goal>
89,</goals>
89,</execution>
89,</executions>
89,<configuration>
89,<mainClass>org.infinispan.tools.store.migrator.StoreMigrator</mainClass>
89,<arguments>
89,<argument>path/to/migrator.properties</argument>
89,</arguments>
89,</configuration>
89,</plugin>
89,</plugins>
89,</build>
89,</project>
89,6.16.3. Configuring the cache store migrator
89,Use the migrator.properties file to configure properties for source and target cache stores.
89,Procedure
89,Create a migrator.properties file.
89,Configure properties for source and target cache store using the migrator.properties file.
89,Add the source. prefix to all configuration properties for the source cache store.
89,Example source cache store
89,source.type=SOFT_INDEX_FILE_STORE
89,source.cache_name=myCache
89,source.location=/path/to/source/sifs
89,source.version=<version>
89,"For migrating data from segmented cache stores, you must also configure the number of segments using the source.segment_count property."
89,The number of segments must match clustering.hash.numSegments in your Infinispan configuration.
89,"If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
89,Add the target. prefix to all configuration properties for the target cache store.
89,Example target cache store
89,target.type=SINGLE_FILE_STORE
89,target.cache_name=myCache
89,target.location=/path/to/target/sfs.dat
89,Configuration properties for the cache store migrator
89,Configure source and target cache stores in a StoreMigrator properties.
89,Table 2. Cache Store Type Property
89,Property
89,Description
89,Required/Optional
89,type
89,Specifies the type of cache store for a source or target cache store.
89,.type=JDBC_STRING
89,.type=JDBC_BINARY
89,.type=JDBC_MIXED
89,.type=LEVELDB
89,.type=ROCKSDB
89,.type=SINGLE_FILE_STORE
89,.type=SOFT_INDEX_FILE_STORE
89,.type=JDBC_MIXED
89,Required
89,Table 3. Common Properties
89,Property
89,Description
89,Example Value
89,Required/Optional
89,cache_name
89,The name of the cache that you want to back up.
89,.cache_name=myCache
89,Required
89,segment_count
89,The number of segments for target cache stores that can use
89,segmentation.
89,The number of segments must match clustering.hash.numSegments in the
89,"Infinispan configuration. If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
89,.segment_count=256
89,Optional
89,Table 4. JDBC Properties
89,Property
89,Description
89,Required/Optional
89,dialect
89,Specifies the dialect of the underlying database.
89,Required
89,version
89,Specifies the marshaller version for source cache stores.
89,Set the value that matches the Infinispan major version of the source cluster. For example; set a value of 14 for Infinispan 14.x.
89,Required for source stores only.
89,marshaller.class
89,Specifies a custom marshaller class.
89,Required if using custom marshallers.
89,marshaller.externalizers
89,Specifies a comma-separated list of custom AdvancedExternalizer implementations to load in this format: [id]:<Externalizer class>
89,Optional
89,connection_pool.connection_url
89,Specifies the JDBC connection URL.
89,Required
89,connection_pool.driver_class
89,Specifies the class of the JDBC driver.
89,Required
89,connection_pool.username
89,Specifies a database username.
89,Required
89,connection_pool.password
89,Specifies a password for the database username.
89,Required
89,db.disable_upsert
89,Disables database upsert.
89,Optional
89,db.disable_indexing
89,Specifies if table indexes are created.
89,Optional
89,table.string.table_name_prefix
89,Specifies additional prefixes for the table name.
89,Optional
89,table.string.<id|data|timestamp>.name
89,Specifies the column name.
89,Required
89,table.string.<id|data|timestamp>.type
89,Specifies the column type.
89,Required
89,key_to_string_mapper
89,Specifies the TwoWayKey2StringMapper class.
89,Optional
89,"To migrate from Binary cache stores in older Infinispan versions, change"
89,table.string.* to table.binary.\* in the following properties:
89,source.table.binary.table_name_prefix
89,source.table.binary.<id\|data\|timestamp>.name
89,source.table.binary.<id\|data\|timestamp>.type
89,# Example configuration for migrating to a JDBC String-Based cache store
89,target.type=STRING
89,target.cache_name=myCache
89,target.dialect=POSTGRES
89,target.marshaller.class=org.example.CustomMarshaller
89,"target.marshaller.externalizers=25:Externalizer1,org.example.Externalizer2"
89,target.connection_pool.connection_url=jdbc:postgresql:postgres
89,target.connection_pool.driver_class=org.postrgesql.Driver
89,target.connection_pool.username=postgres
89,target.connection_pool.password=redhat
89,target.db.disable_upsert=false
89,target.db.disable_indexing=false
89,target.table.string.table_name_prefix=tablePrefix
89,target.table.string.id.name=id_column
89,target.table.string.data.name=datum_column
89,target.table.string.timestamp.name=timestamp_column
89,target.table.string.id.type=VARCHAR
89,target.table.string.data.type=bytea
89,target.table.string.timestamp.type=BIGINT
89,target.key_to_string_mapper=org.infinispan.persistence.keymappers. DefaultTwoWayKey2StringMapper
89,Table 5. RocksDB Properties
89,Property
89,Description
89,Required/Optional
89,location
89,Sets the database directory.
89,Required
89,compression
89,Specifies the compression type to use.
89,Optional
89,# Example configuration for migrating from a RocksDB cache store.
89,source.type=ROCKSDB
89,source.cache_name=myCache
89,source.location=/path/to/rocksdb/database
89,source.compression=SNAPPY
89,Table 6. SingleFileStore Properties
89,Property
89,Description
89,Required/Optional
89,location
89,Sets the directory that contains the cache store .dat file.
89,Required
89,# Example configuration for migrating to a Single File cache store.
89,target.type=SINGLE_FILE_STORE
89,target.cache_name=myCache
89,target.location=/path/to/sfs.dat
89,Table 7. SoftIndexFileStore Properties
89,Property
89,Description
89,Value
89,Required/Optional
89,location
89,Sets the database directory.
89,Required
89,index_location
89,Sets the database index directory.
89,# Example configuration for migrating to a Soft-Index File cache store.
89,target.type=SOFT_INDEX_FILE_STORE
89,target.cache_name=myCache
89,target.location=path/to/sifs/database
89,target.location=path/to/sifs/index
89,6.16.4. Migrating Infinispan cache stores
89,You can use the StoreMigrator to migrate data between cache stores with different Infinispan versions or to migrate data from one type of cache store to another.
89,Prerequisites
89,Have a
89,infinispan-tools.jar.
89,Have the source and target cache store configured in the migrator.properties file.
89,Procedure
89,"If you built the infinispan-tools.jar from the source code, do the following:"
89,Add infinispan-tools.jar to your classpath.
89,"Add dependencies for your source and target databases, such as JDBC drivers to your classpath."
89,Specify migrator.properties file as an argument for StoreMigrator.
89,"If you pulled infinispan-tools.jar from the Maven repository, run the following command:"
89,mvn exec:java
89,7. Configuring Infinispan to handle network partitions
89,Infinispan clusters can split into network partitions in which subsets of nodes become isolated from each other.
89,This condition results in loss of availability or consistency for clustered caches.
89,Infinispan automatically detects crashed nodes and resolves conflicts to merge caches back together.
89,7.1. Split clusters and network partitions
89,"Network partitions are the result of error conditions in the running environment, such as when a network router crashes."
89,"When a cluster splits into partitions, nodes create a JGroups cluster view that includes only the nodes in that partition."
89,This condition means that nodes in one partition can operate independently of nodes in the other partition.
89,Detecting a split
89,"To automatically detect network partitions, Infinispan uses the FD_ALL protocol in the default JGroups stack to determine when nodes leave the cluster abruptly."
89,Infinispan cannot detect what causes nodes to leave abruptly.
89,"This can happen not only when there is a network failure but also for other reasons, such as when Garbage Collection (GC) pauses the JVM."
89,Infinispan suspects that nodes have crashed after the following number of milliseconds:
89,FD_ALL[2|3].timeout + FD_ALL[2|3].interval + VERIFY_SUSPECT[2].timeout + GMS.view_ack_collection_timeout
89,"When it detects that the cluster is split into network partitions, Infinispan uses a strategy for handling cache operations."
89,Depending on your application requirements Infinispan can:
89,Allow read and/or write operations for availability
89,Deny read and write operations for consistency
89,Merging partitions together
89,"To fix a split cluster, Infinispan merges the partitions back together."
89,"During the merge, Infinispan uses the .equals() method for values of cache entries to determine if any conflicts exist."
89,"To resolve any conflicts between replicas it finds on partitions, Infinispan uses a merge policy that you can configure."
89,7.1.1. Data consistency in a split cluster
89,Network outages or errors that cause Infinispan clusters to split into partitions can result in data loss or consistency issues regardless of any handling strategy or merge policy.
89,Between the split and detection
89,"If a write operation takes place on a node that is in a minor partition when a split occurs, and before Infinispan detects the split, that value is lost when Infinispan transfers state to that minor partition during the merge."
89,In the event that all partitions are in the DEGRADED mode that value is not lost because no state transfer occurs but the entry can have an inconsistent value.
89,"For transactional caches write operations that are in progress when the split occurs can be committed on some nodes and rolled back on other nodes, which also results in inconsistent values."
89,"During the split and the time that Infinispan detects it, it is possible to get stale reads from a cache in a minor partition that has not yet entered DEGRADED mode."
89,During the merge
89,When Infinispan starts removing partitions nodes reconnect to the cluster with a series of merge events.
89,"Before this merge process completes it is possible that write operations on transactional caches succeed on some nodes but not others, which can potentially result in stale reads until the entries are updated."
89,7.2. Cache availability and degraded mode
89,"To preserve data consistency, Infinispan can put caches into DEGRADED mode if you configure them to use either the DENY_READ_WRITES or ALLOW_READS partition handling strategy."
89,Infinispan puts caches in a partition into DEGRADED mode when the following conditions are true:
89,At least one segment has lost all owners.
89,This happens when a number of nodes equal to or greater than the number of owners for a distributed cache have left the cluster.
89,There is not a majority of nodes in the partition.
89,"A majority of nodes is any number greater than half the total number of nodes in the cluster from the most recent stable topology, which was the last time a cluster rebalancing operation completed successfully."
89,"When caches are in DEGRADED mode, Infinispan:"
89,Allows read and write operations only if all replicas of an entry reside in the same partition.
89,Denies read and write operations and throws an AvailabilityException if the partition does not include all replicas of an entry.
89,"With the ALLOW_READS strategy, Infinispan allows read operations on caches in DEGRADED mode."
89,DEGRADED mode guarantees consistency by ensuring that write operations do not take place for the same key in different partitions.
89,Additionally DEGRADED mode prevents stale read operations that happen when a key is updated in one partition but read in another partition.
89,If all partitions are in DEGRADED mode then the cache becomes available again after merge only if the cluster contains a majority of nodes from the most recent stable topology and there is at least one replica of each entry.
89,"When the cluster has at least one replica of each entry, no keys are lost and Infinispan can create new replicas based on the number of owners during cluster rebalancing."
89,In some cases a cache in one partition can remain available while entering DEGRADED mode in another partition.
89,When this happens the available partition continues cache operations as normal and Infinispan attempts to rebalance data across those nodes.
89,To merge the cache together Infinispan always transfers state from the available partition to the partition in DEGRADED mode.
89,7.2.1. Degraded cache recovery example
89,This topic illustrates how Infinispan recovers from split clusters with caches that use the DENY_READ_WRITES partition handling strategy.
89,"As an example, a Infinispan cluster has four nodes and includes a distributed cache with two replicas for each entry (owners=2)."
89,"There are four entries in the cache, k1, k2, k3 and k4."
89,"With the DENY_READ_WRITES strategy, if the cluster splits into partitions, Infinispan allows cache operations only if all replicas of an entry are in the same partition."
89,"In the following diagram, while the cache is split into partitions, Infinispan allows read and write operations for k1 on partition 1 and k4 on partition 2."
89,"Because there is only one replica for k2 and k3 on either partition 1 or partition 2, Infinispan denies read and write operations for those entries."
89,"When network conditions allow the nodes to re-join the same cluster view, Infinispan merges the partitions without state transfer and restores normal cache operations."
89,7.2.2. Verifying cache availability during network partitions
89,Determine if caches on Infinispan clusters are in AVAILABLE mode or DEGRADED mode during a network partition.
89,"When Infinispan clusters split into partitions, nodes in those partitions can enter DEGRADED mode to guarantee data consistency."
89,In DEGRADED mode clusters do not allow cache operations resulting in loss of availability.
89,Procedure
89,Verify availability of clustered caches in network partitions in one of the following ways:
89,Check Infinispan logs for ISPN100011 messages that indicate if the cluster is available or if at least one cache is in DEGRADED mode.
89,Get the availability of remote caches through the Infinispan Console or with the REST API.
89,"Open the Infinispan Console in any browser, select the Data Container tab, and then locate the availability status in the Health column."
89,Retrieve cache health from the REST API.
89,GET /rest/v2/cache-managers/<cacheManagerName>/health
89,Programmatically retrieve the availability of embedded caches with the getAvailability() method in the AdvancedCache API.
89,Additional resources
89,REST API: Getting cluster health
89,org.infinispan.AdvancedCache.getAvailability
89,Enum AvailabilityMode
89,7.2.3. Making caches available
89,Make caches available for read and write operations by forcing them out of DEGRADED mode.
89,You should force clusters out of DEGRADED mode only if your deployment can tolerate data loss and inconsistency.
89,Procedure
89,Make caches available in one of the following ways:
89,Open the Infinispan Console and select the Make available option.
89,Change the availability of remote caches with the REST API.
89,POST /rest/v2/caches/<cacheName>?action=set-availability&availability=AVAILABLE
89,Programmatically change the availability of embedded caches with the AdvancedCache API.
89,AdvancedCache ac = cache.getAdvancedCache();
89,// Retrieve cache availability
89,boolean available = ac.getAvailability() == AvailabilityMode.AVAILABLE;
89,// Make the cache available
89,if (!available) {
89,ac.setAvailability(AvailabilityMode.AVAILABLE);
89,Additional resources
89,REST API: Setting cache availability
89,org.infinispan.AdvancedCache
89,7.3. Configuring partition handling
89,Configure Infinispan to use a partition handling strategy and merge policy so it can resolve split clusters when network issues occur.
89,By default Infinispan uses a strategy that provides availability at the cost of lowering consistency guarantees for your data.
89,When a cluster splits due to a network partition clients can continue to perform read and write operations on caches.
89,"If you require consistency over availability, you can configure Infinispan to deny read and write operations while the cluster is split into partitions."
89,Alternatively you can allow read operations and deny write operations.
89,You can also specify custom merge policy implementations that configure Infinispan to resolve splits with custom logic tailored to your requirements.
89,Prerequisites
89,Have a Infinispan cluster where you can create either a replicated or distributed cache.
89,Partition handling configuration applies only to replicated and distributed caches.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add partition handling configuration to your cache with either the partition-handling element or partitionHandling() method.
89,Specify a strategy for Infinispan to use when the cluster splits into partitions with the when-split attribute or whenSplit() method.
89,The default partition handling strategy is ALLOW_READ_WRITES so caches remain availabile.
89,"If your use case requires data consistency over cache availability, specify the DENY_READ_WRITES strategy."
89,Specify a policy that Infinispan uses to resolve conflicting entries when merging partitions with the merge-policy attribute or mergePolicy() method.
89,By default Infinispan does not resolve conflicts on merge.
89,Save the changes to your Infinispan configuration.
89,Partition handling configuration
89,XML
89,<distributed-cache>
89,"<partition-handling when-split=""DENY_READ_WRITES"""
89,"merge-policy=""PREFERRED_ALWAYS""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""partition-handling"" : {"
89,"""when-split"": ""DENY_READ_WRITES"","
89,"""merge-policy"": ""PREFERRED_ALWAYS"""
89,YAML
89,distributedCache:
89,partitionHandling:
89,whenSplit: DENY_READ_WRITES
89,mergePolicy: PREFERRED_ALWAYS
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
89,.partitionHandling()
89,.whenSplit(PartitionHandling.DENY_READ_WRITES)
89,.mergePolicy(MergePolicy.PREFERRED_NON_NULL);
89,7.4. Partition handling strategies
89,Partition handling strategies control if Infinispan allows read and write operations when a cluster is split.
89,The strategy you configure determines whether you get cache availability or data consistency.
89,Table 8. Partition handling strategies
89,Strategy
89,Description
89,Availability or consistency
89,ALLOW_READ_WRITES
89,Infinispan allows read and write operations on caches while a cluster is split into network partitions.
89,Nodes in each partition remain available and function independently of each other.
89,This is the default partition handling strategy.
89,Availability
89,DENY_READ_WRITES
89,Infinispan allows read and write operations only if all replicas of an entry are in the partition.
89,"If a partition does not include all replicas of an entry, Infinispan prevents cache operations for that entry."
89,Consistency
89,ALLOW_READS
89,Infinispan allows read operations for entries and prevents write operations unless the partition includes all replicas of an entry.
89,Consistency with read availability
89,7.5. Merge policies
89,Merge policies control how Infinispan resolves conflicts between replicas when bringing cluster partitions together.
89,You can use one of the merge policies that Infinispan provides or you can create a custom implementation of the EntryMergePolicy API.
89,Table 9. Infinispan merge policies
89,Merge policy
89,Description
89,Considerations
89,NONE
89,Infinispan does not resolve conflicts when merging split clusters. This is the default merge policy.
89,"Nodes drop segments for which they are not the primary owner, which can result in data loss."
89,PREFERRED_ALWAYS
89,Infinispan finds the value that exists on the majority of nodes in the cluster and uses it to resolve conflicts.
89,"Infinispan could use stale values to resolve conflicts. Even if an entry is available the majority of nodes, the last update could happen on the minority partition."
89,PREFERRED_NON_NULL
89,Infinispan uses the first non-null value that it finds on the cluster to resolve conflicts.
89,Infinispan could restore deleted entries.
89,REMOVE_ALL
89,Infinispan removes any conflicting entries from the cache.
89,Results in loss of any entries that have different values when merging split clusters.
89,7.6. Configuring custom merge policies
89,Configure Infinispan to use custom implementations of the EntryMergePolicy API when handling network partitions.
89,Prerequisites
89,Implement the EntryMergePolicy API.
89,"public class CustomMergePolicy implements EntryMergePolicy<String, String> {"
89,@Override
89,"public CacheEntry<String, String> merge(CacheEntry<String, String> preferredEntry, List<CacheEntry<String, String>> otherEntries) {"
89,// Decide which entry resolves the conflict
89,return the_solved_CacheEntry;
89,Procedure
89,Deploy your merge policy implementation to Infinispan Server if you use remote caches.
89,Package your classes as a JAR file that includes a META-INF/services/org.infinispan.conflict.EntryMergePolicy file that contains the fully qualified class name of your merge policy.
89,# List implementations of EntryMergePolicy with the full qualified class name
89,org.example.CustomMergePolicy
89,Add the JAR file to the server/lib directory.
89,Use the install command with the Infinispan Command Line Interface (CLI) to download the JAR to the server/lib directory.
89,Open your Infinispan configuration for editing.
89,Configure cache encoding with the encoding element or encoding() method as appropriate.
89,"For remote caches, if you use only object metadata for comparison when merging entries then you can use application/x-protostream as the media type. In this case Infinispan returns entries to the EntryMergePolicy as byte[]."
89,If you require the object itself when merging conflicts then you should configure caches with the application/x-java-object media type. In this case you must deploy the relevant ProtoStream marshallers to Infinispan Server so it can perform byte[] to object transformations if clients use Protobuf encoding.
89,Specify your custom merge policy with the merge-policy attribute or mergePolicy() method as part of the partition handling configuration.
89,Save your changes.
89,Custom merge policy configuration
89,XML
89,"<distributed-cache name=""mycache"">"
89,"<partition-handling when-split=""DENY_READ_WRITES"""
89,"merge-policy=""org.example.CustomMergePolicy""/>"
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""partition-handling"" : {"
89,"""when-split"": ""DENY_READ_WRITES"","
89,"""merge-policy"": ""org.example.CustomMergePolicy"""
89,YAML
89,distributedCache:
89,partitionHandling:
89,whenSplit: DENY_READ_WRITES
89,mergePolicy: org.example.CustomMergePolicy
89,ConfigurationBuilder
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
89,.partitionHandling()
89,.whenSplit(PartitionHandling.DENY_READ_WRITES)
89,.mergePolicy(new CustomMergePolicy());
89,Additional resources
89,org.infinispan.conflict.EntryMergePolicy
89,7.7. Manually merging partitions in embedded caches
89,Detect and resolve conflicting entries to manually merge embedded caches after network partitions occur.
89,Procedure
89,"Retrieve the ConflictManager from the EmbeddedCacheManager to detect and resolve conflicting entries in a cache, as in the following example:"
89,"EmbeddedCacheManager manager = new DefaultCacheManager(""example-config.xml"");"
89,"Cache<Integer, String> cache = manager.getCache(""testCache"");"
89,"ConflictManager<Integer, String> crm = ConflictManagerFactory.get(cache.getAdvancedCache());"
89,// Get all versions of a key
89,"Map<Address, InternalCacheValue<String>> versions = crm.getAllVersions(1);"
89,// Process conflicts stream and perform some operation on the cache
89,"Stream<Map<Address, CacheEntry<Integer, String>>> conflicts = crm.getConflicts();"
89,conflicts.forEach(map -> {
89,"CacheEntry<Integer, String> entry = map.values().iterator().next();"
89,Object conflictKey = entry.getKey();
89,cache.remove(conflictKey);
89,});
89,// Detect and then resolve conflicts using the configured EntryMergePolicy
89,crm.resolveConflicts();
89,// Detect and then resolve conflicts using the passed EntryMergePolicy instance
89,"crm.resolveConflicts((preferredEntry, otherEntries) -> preferredEntry);"
89,"Although the ConflictManager::getConflicts stream is processed per entry, the underlying spliterator lazily loads cache entries on a per segment basis."
89,8. Security authorization with role-based access control
89,Role-based access control (RBAC) capabilities use different permissions levels to restrict user interactions with Infinispan.
89,"For information on creating users and configuring authorization specific to remote or embedded caches, see:"
89,Configuring user roles and permissions with Infinispan Server
89,Programmatically configuring user roles and permissions
89,8.1. Infinispan user roles and permissions
89,Infinispan includes several roles that provide users with permissions to access caches and Infinispan resources.
89,Role
89,Permissions
89,Description
89,admin
89,ALL
89,Superuser with all permissions including control of the Cache Manager lifecycle.
89,deployer
89,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR, CREATE"
89,Can create and delete Infinispan resources in addition to application permissions.
89,application
89,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR"
89,Has read and write access to Infinispan resources in addition to observer permissions. Can also listen to events and execute server tasks and scripts.
89,observer
89,"ALL_READ, MONITOR"
89,Has read access to Infinispan resources in addition to monitor permissions.
89,monitor
89,MONITOR
89,Can view statistics via JMX and the metrics endpoint.
89,Additional resources
89,org.infinispan.security.AuthorizationPermission Enum
89,Infinispan configuration schema reference
89,8.1.1. Permissions
89,User roles are sets of permissions with different access levels.
89,Table 10. Cache Manager permissions
89,Permission
89,Function
89,Description
89,CONFIGURATION
89,defineConfiguration
89,Defines new cache configurations.
89,LISTEN
89,addListener
89,Registers listeners against a Cache Manager.
89,LIFECYCLE
89,stop
89,Stops the Cache Manager.
89,CREATE
89,"createCache, removeCache"
89,Create and remove container resources
89,"such as caches, counters, schemas, and scripts."
89,MONITOR
89,getStats
89,Allows access to JMX statistics and the metrics endpoint.
89,ALL
89,Includes all Cache Manager permissions.
89,Table 11. Cache permissions
89,Permission
89,Function
89,Description
89,READ
89,"get, contains"
89,Retrieves entries from a cache.
89,WRITE
89,"put, putIfAbsent, replace, remove, evict"
89,"Writes, replaces, removes, evicts data in a cache."
89,EXEC
89,"distexec, streams"
89,Allows code execution against a cache.
89,LISTEN
89,addListener
89,Registers listeners against a cache.
89,BULK_READ
89,"keySet, values, entrySet, query"
89,Executes bulk retrieve operations.
89,BULK_WRITE
89,"clear, putAll"
89,Executes bulk write operations.
89,LIFECYCLE
89,"start, stop"
89,Starts and stops a cache.
89,ADMIN
89,"getVersion, addInterceptor*, removeInterceptor, getInterceptorChain, getEvictionManager, getComponentRegistry, getDistributionManager, getAuthorizationManager, evict, getRpcManager, getCacheConfiguration, getCacheManager, getInvocationContextContainer, setAvailability, getDataContainer, getStats, getXAResource"
89,Allows access to underlying components and internal structures.
89,MONITOR
89,getStats
89,Allows access to JMX statistics and the metrics endpoint.
89,ALL
89,Includes all cache permissions.
89,ALL_READ
89,Combines the READ and BULK_READ permissions.
89,ALL_WRITE
89,Combines the WRITE and BULK_WRITE permissions.
89,Additional resources
89,Infinispan Security API
89,8.1.2. Role and permission mappers
89,Infinispan implements users as a collection of principals.
89,"Principals represent either an individual user identity, such as a username, or a group to which the users belong. Internally, these are implemented with the javax.security.auth.Subject class."
89,"To enable authorization, the principals must be mapped to role names, which are then expanded into a set of permissions."
89,"Infinispan includes the PrincipalRoleMapper API for associating security principals to roles, and the RolePermissionMapper API for associating roles with specific permissions."
89,Infinispan provides the following role and permission mapper implementations:
89,Cluster role mapper
89,Stores principal to role mappings in the cluster registry.
89,Cluster permission mapper
89,Stores role to permission mappings in the cluster registry. Allows you to dynamically modify user roles and permissions.
89,Identity role mapper
89,"Uses the principal name as the role name. The type or format of the principal name depends on the source. For example, in an LDAP directory the principal name could be a Distinguished Name (DN)."
89,Common name role mapper
89,"Uses the Common Name (CN) as the role name. You can use this role mapper with an LDAP directory or with client certificates that contain Distinguished Names (DN); for example cn=managers,ou=people,dc=example,dc=com maps to the managers role."
89,Mapping users to roles and permissions in Infinispan
89,"Consider the following user retrieved from an LDAP server, as a collection of DNs:"
89,"CN=myapplication,OU=applications,DC=mycompany"
89,"CN=dataprocessors,OU=groups,DC=mycompany"
89,"CN=finance,OU=groups,DC=mycompany"
89,"Using the Common name role mapper, the user would be mapped to the following roles:"
89,dataprocessors
89,finance
89,Infinispan has the following role definitions:
89,dataprocessors: ALL_WRITE ALL_READ
89,finance: LISTEN
89,The user would have the following permissions:
89,ALL_WRITE ALL_READ LISTEN
89,Additional resources
89,Infinispan Security API
89,org.infinispan.security.PrincipalRoleMapper
89,org.infinispan.security.RolePermissionMapper
89,org.infinispan.security.mappers.IdentityRoleMapper
89,org.infinispan.security.mappers.CommonNameRoleMapper
89,8.1.3. Configuring role mappers
89,Infinispan enables the cluster role mapper and cluster permission mapper by default.
89,"To use a different implementation for role mapping, you must configure the role mappers."
89,Procedure
89,Open your Infinispan configuration for editing.
89,Declare the role mapper as part of the security authorization in the Cache Manager configuration.
89,Save the changes to your configuration.
89,With embedded caches you can programmatically configure role and permission mappers with the principalRoleMapper() and rolePermissionMapper() methods.
89,Role mapper configuration
89,XML
89,<cache-container>
89,<security>
89,<authorization>
89,<common-name-role-mapper />
89,</authorization>
89,</security>
89,</cache-container>
89,JSON
89,"""infinispan"" : {"
89,"""cache-container"" : {"
89,"""security"" : {"
89,"""authorization"" : {"
89,"""common-name-role-mapper"": {}"
89,YAML
89,infinispan:
89,cacheContainer:
89,security:
89,authorization:
89,commonNameRoleMapper: ~
89,Additional resources
89,Infinispan configuration schema reference
89,8.2. Configuring caches with security authorization
89,Add security authorization to caches to enforce role-based access control (RBAC).
89,This requires Infinispan users to have a role with a sufficient level of permission to perform cache operations.
89,Prerequisites
89,Create Infinispan users and either grant them with roles or assign them to groups.
89,Procedure
89,Open your Infinispan configuration for editing.
89,Add a security section to the configuration.
89,Specify roles that users must have to perform cache operations with the authorization element.
89,You can implicitly add all roles defined in the Cache Manager or explicitly define a subset of roles.
89,Save the changes to your configuration.
89,Implicit role configuration
89,The following configuration implicitly adds every role defined in the Cache Manager:
89,XML
89,<distributed-cache>
89,<security>
89,<authorization/>
89,</security>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""security"": {"
89,"""authorization"": {"
89,"""enabled"": true"
89,YAML
89,distributedCache:
89,security:
89,authorization:
89,enabled: true
89,Explicit role configuration
89,The following configuration explicitly adds a subset of roles defined in the Cache Manager.
89,In this case Infinispan denies cache operations for any users that do not have one of the configured roles.
89,XML
89,<distributed-cache>
89,<security>
89,"<authorization roles=""admin supervisor""/>"
89,</security>
89,</distributed-cache>
89,JSON
89,"""distributed-cache"": {"
89,"""security"": {"
89,"""authorization"": {"
89,"""enabled"": true,"
89,"""roles"": [""admin"",""supervisor""]"
89,YAML
89,distributedCache:
89,security:
89,authorization:
89,enabled: true
89,"roles: [""admin"",""supervisor""]"
89,9. Configuring transactions
89,"Data that resides on a distributed system is vulnerable to errors that can arise from temporary network outages, system failures, or just simple human error."
89,These external factors are uncontrollable but can have serious consequences for quality of your data.
89,The effects of data corruption range from lower customer satisfaction to costly system reconciliation that results in service unavailability.
89,"Infinispan can carry out ACID (atomicity, consistency, isolation, durability) transactions to ensure the cache state is consistent."
89,9.1. Transactions
89,Infinispan can be configured to use and to participate in JTA compliant transactions.
89,"Alternatively, if transaction support is disabled, it is equivalent to using autocommit in JDBC calls, where modifications are potentially replicated after every change (if replication is enabled)."
89,On every cache operation Infinispan does the following:
89,Retrieves the current Transaction associated with the thread
89,"If not already done, registers XAResource with the transaction manager to be notified when a transaction commits or is rolled back."
89,"In order to do this, the cache has to be provided with a reference to the environment’s TransactionManager."
89,This is usually done by configuring the cache with the class name of an implementation of the TransactionManagerLookup interface.
89,"When the cache starts, it will create an instance of this class and invoke its getTransactionManager() method, which returns a reference to the TransactionManager."
89,Infinispan ships with several transaction manager lookup classes:
89,Transaction manager lookup implementations
89,EmbeddedTransactionManagerLookup:
89,This provides with a basic transaction manager which should only be used for embedded mode when no other implementation is available.
89,This implementation has some severe limitations to do with concurrent transactions and recovery.
89,JBossStandaloneJTAManagerLookup:
89,"If you’re running Infinispan in a standalone environment, or in JBoss AS 7 and earlier, and WildFly 8, 9, and 10, this should be your default choice for transaction manager."
89,It’s a fully fledged transaction manager based on JBoss Transactions which overcomes all the deficiencies of the EmbeddedTransactionManager.
89,WildflyTransactionManagerLookup:
89,"If you’re running Infinispan in WildFly 11 or later, this should be your default choice for transaction manager."
89,GenericTransactionManagerLookup:
89,This is a lookup class that locate transaction managers in the most popular Java EE application servers.
89,"If no transaction manager can be found, it defaults on the EmbeddedTransactionManager."
89,"Once initialized, the TransactionManager can also be obtained from the Cache itself:"
89,//the cache must have a transactionManagerLookupClass defined
89,Cache cache = cacheManager.getCache();
89,//equivalent with calling TransactionManagerLookup.getTransactionManager();
89,TransactionManager tm = cache.getAdvancedCache().getTransactionManager();
89,9.1.1. Configuring transactions
89,Transactions are configured at cache level.
89,Below is the configuration that affects a transaction behaviour and a small description of each configuration attribute.
89,<locking
89,"isolation=""READ_COMMITTED""/>"
89,<transaction
89,"locking=""OPTIMISTIC"""
89,"auto-commit=""true"""
89,"complete-timeout=""60000"""
89,"mode=""NONE"""
89,"notifications=""true"""
89,"reaper-interval=""30000"""
89,"recovery-cache=""__recoveryInfoCacheName__"""
89,"stop-timeout=""30000"""
89,"transaction-manager-lookup=""org.infinispan.transaction.lookup.GenericTransactionManagerLookup""/>"
89,or programmatically:
89,ConfigurationBuilder builder = new ConfigurationBuilder();
89,builder.locking()
89,.isolationLevel(IsolationLevel.READ_COMMITTED);
89,builder.transaction()
89,.lockingMode(LockingMode.OPTIMISTIC)
89,.autoCommit(true)
89,.completedTxTimeout(60000)
89,.transactionMode(TransactionMode.NON_TRANSACTIONAL)
89,.useSynchronization(false)
89,.notifications(true)
89,.reaperWakeUpInterval(30000)
89,.cacheStopTimeout(30000)
89,.transactionManagerLookup(new GenericTransactionManagerLookup())
89,.recovery()
89,.enabled(false)
89,".recoveryInfoCacheName(""__recoveryInfoCacheName__"");"
89,isolation - configures the isolation level. Check section Isolation Levels for more details.
89,Default is REPEATABLE_READ.
89,locking - configures whether the cache uses optimistic or pessimistic locking. Check section Transaction Locking for more details.
89,Default is OPTIMISTIC.
89,"auto-commit - if enable, the user does not need to start a transaction manually for a single operation. The transaction is automatically started and committed."
89,Default is true.
89,complete-timeout - the duration in milliseconds to keep information about completed transactions. Default is 60000.
89,mode - configures whether the cache is transactional or not. Default is NONE. The available options are:
89,NONE - non transactional cache
89,FULL_XA - XA transactional cache with recovery enabled. Check section Transaction recovery for more details about recovery.
89,NON_DURABLE_XA - XA transactional cache with recovery disabled.
89,NON_XA - transactional cache with integration via Synchronization instead of XA.
89,Check section Enlisting Synchronizations for details.
89,BATCH-
89,transactional cache using batch to group operations. Check section Batching for details.
89,notifications - enables/disables triggering transactional events in cache listeners. Default is true.
89,reaper-interval - the time interval in millisecond at which the thread that cleans up transaction completion information kicks in.
89,Defaults is 30000.
89,recovery-cache - configures the cache name to store the recovery information. Check section Transaction recovery for more details about recovery.
89,Default is recoveryInfoCacheName.
89,stop-timeout - the time in millisecond to wait for ongoing transaction when the cache is stopping. Default is
89,30000.
89,transaction-manager-lookup - configures the fully qualified class name of a class that looks up a reference to a javax.transaction.TransactionManager.
89,Default is org.infinispan.transaction.lookup.GenericTransactionManagerLookup.
89,For more details on how Two-Phase-Commit (2PC) is implemented in Infinispan and how locks are being acquired see the section below.
89,More details about the configuration settings are available in Configuration reference.
89,9.1.2. Isolation levels
89,Infinispan offers two isolation levels - READ_COMMITTED and REPEATABLE_READ.
89,"These isolation levels determine when readers see a concurrent write, and are internally implemented using different subclasses of MVCCEntry, which have different behaviour in how state is committed back to the data container."
89,Here’s a more detailed example that should help understand the difference between READ_COMMITTED and REPEATABLE_READ in the context of Infinispan.
89,"With READ_COMMITTED, if between two consecutive read calls on the same key, the key has been updated by another transaction, the second read may return the new updated value:"
89,Thread1: tx1.begin()
89,Thread1: cache.get(k) // returns v
89,Thread2:
89,tx2.begin()
89,Thread2:
89,cache.get(k) // returns v
89,Thread2:
89,"cache.put(k, v2)"
89,Thread2:
89,tx2.commit()
89,Thread1: cache.get(k) // returns v2!
89,Thread1: tx1.commit()
89,"With REPEATABLE_READ, the final get will still return v."
89,"So, if you’re going to retrieve the same key multiple times within a transaction, you should use REPEATABLE_READ."
89,"However, as read-locks are not acquired even for REPEATABLE_READ, this phenomena can occur:"
89,"cache.get(""A"") // returns 1"
89,"cache.get(""B"") // returns 1"
89,Thread1: tx1.begin()
89,"Thread1: cache.put(""A"", 2)"
89,"Thread1: cache.put(""B"", 2)"
89,Thread2:
89,tx2.begin()
89,Thread2:
89,"cache.get(""A"") // returns 1"
89,Thread1: tx1.commit()
89,Thread2:
89,"cache.get(""B"") // returns 2"
89,Thread2:
89,tx2.commit()
89,9.1.3. Transaction locking
89,Pessimistic transactional cache
89,"From a lock acquisition perspective, pessimistic transactions obtain locks on keys at the time the key is written."
89,A lock request is sent to the primary owner (can be an explicit lock request or an operation)
89,The primary owner tries to acquire the lock:
89,"If it succeed, it sends back a positive reply;"
89,"Otherwise, a negative reply is sent and the transaction is rollback."
89,As an example:
89,transactionManager.begin();
89,"cache.put(k1,v1); //k1 is locked."
89,cache.remove(k2); //k2 is locked when this returns
89,transactionManager.commit();
89,"When cache.put(k1,v1) returns, k1 is locked and no other transaction running anywhere in the cluster can write to it."
89,Reading k1 is still possible.
89,The lock on k1 is released when the transaction completes (commits or rollbacks).
89,"For conditional operations, the validation is performed in the originator."
89,Optimistic transactional cache
89,With optimistic transactions locks are being acquired at transaction prepare time and are only being held up to the point the transaction commits (or rollbacks).
89,This is different from the 5.0 default locking model where local locks are being acquire on writes and cluster locks are being acquired during prepare time.
89,The prepare is sent to all the owners.
89,The primary owners try to acquire the locks needed:
89,"If locking succeeds, it performs the write skew check."
89,"If the write skew check succeeds (or is disabled), send a positive reply."
89,"Otherwise, a negative reply is sent and the transaction is rolled back."
89,As an example:
89,transactionManager.begin();
89,"cache.put(k1,v1);"
89,cache.remove(k2);
89,"transactionManager.commit(); //at prepare time, K1 and K2 is locked until committed/rolled back."
89,"For conditional commands, the validation still happens on the originator."
89,What do I need - pessimistic or optimistic transactions?
89,"From a use case perspective, optimistic transactions should be used when there is not a lot of contention between multiple transactions running at the same time."
89,That is because the optimistic transactions rollback if data has changed between the time it was read and the time it was committed (with write skew check enabled).
89,"On the other hand, pessimistic transactions might be a better fit when there is high contention on the keys and transaction rollbacks are less desirable."
89,Pessimistic transactions are more costly by their nature: each write operation potentially involves a RPC for lock acquisition.
89,9.1.4. Write Skews
89,Write skews occur when two transactions independently and simultaneously read and write to the same key. The result of a write skew is that both transactions successfully commit updates to the same key but with different values.
89,Infinispan automatically performs write skew checks to ensure data consistency for REPEATABLE_READ isolation levels in optimistic transactions. This allows Infinispan to detect and roll back one of the transactions.
89,"When operating in LOCAL mode, write skew checks rely on Java object"
89,"references to compare differences, which provides a reliable technique for"
89,checking for write skews.
89,Forcing write locks on keys in pessimitic transactions
89,"To avoid write skews with pessimistic transactions, lock keys at read-time with Flag.FORCE_WRITE_LOCK."
89,"In non-transactional caches, Flag.FORCE_WRITE_LOCK does not work. The get() call reads the key value but does not acquire locks remotely."
89,You should use Flag.FORCE_WRITE_LOCK with transactions in which the entity is updated later in the same transaction.
89,Compare the following code snippets for an example of Flag.FORCE_WRITE_LOCK:
89,// begin the transaction
89,if (!cache.getAdvancedCache().lock(key)) {
89,// abort the transaction because the key was not locked
89,} else {
89,cache.get(key);
89,"cache.put(key, value);"
89,// commit the transaction
89,// begin the transaction
89,try {
89,// throws an exception if the key is not locked.
89,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(key);
89,"cache.put(key, value);"
89,} catch (CacheException e) {
89,// mark the transaction rollback-only
89,// commit or rollback the transaction
89,9.1.5. Dealing with exceptions
89,"If a CacheException (or a subclass of it) is thrown by a cache method within the scope of a JTA transaction, then the transaction is automatically marked for rollback."
89,9.1.6. Enlisting Synchronizations
89,By default Infinispan registers itself as a first class participant in distributed transactions through XAResource.
89,"There are situations where Infinispan is not required to be a participant in the transaction, but only to be notified by its lifecycle (prepare, complete): e.g. in the case Infinispan is used as a 2nd level cache in Hibernate."
89,Infinispan allows transaction enlistment through Synchronization.
89,To enable it just use NON_XA transaction mode.
89,Synchronizations have the advantage that they allow TransactionManager to optimize 2PC with a 1PC where only one other resource is enlisted with that transaction (last resource commit optimization).
89,"E.g. Hibernate second level cache: if Infinispan registers itself with the TransactionManager as a XAResource than at commit time, the TransactionManager sees two XAResource (cache and database) and does not make this optimization."
89,Having to coordinate between two resources it needs to write the tx log to disk.
89,"On the other hand, registering Infinispan as a Synchronization makes the TransactionManager skip writing the log to the disk (performance improvement)."
89,9.1.7. Batching
89,"Batching allows atomicity and some characteristics of a transaction, but not full-blown JTA or XA capabilities."
89,Batching is often a lot lighter and cheaper than a full-blown transaction.
89,"Generally speaking, one should use batching API whenever the only participant in the transaction is an Infinispan cluster."
89,"On the other hand, JTA transactions (involving TransactionManager) should be used whenever the transactions involves multiple systems."
89,"E.g. considering the ""Hello world!"" of transactions: transferring money from one bank account to the other."
89,"If both accounts are stored within Infinispan, then batching can be used."
89,"If one account is in a database and the other is Infinispan, then distributed transactions are required."
89,You do not have to have a transaction manager defined to use batching.
89,API
89,"Once you have configured your cache to use batching, you use it by calling startBatch() and endBatch() on Cache. E.g.,"
89,Cache cache = cacheManager.getCache();
89,// not using a batch
89,"cache.put(""key"", ""value""); // will replicate immediately"
89,// using a batch
89,cache.startBatch();
89,"cache.put(""k1"", ""value"");"
89,"cache.put(""k2"", ""value"");"
89,"cache.put(""k2"", ""value"");"
89,cache.endBatch(true); // This will now replicate the modifications since the batch was started.
89,// a new batch
89,cache.startBatch();
89,"cache.put(""k1"", ""value"");"
89,"cache.put(""k2"", ""value"");"
89,"cache.put(""k3"", ""value"");"
89,"cache.endBatch(false); // This will ""discard"" changes made in the batch"
89,Batching and JTA
89,"Behind the scenes, the batching functionality starts a JTA transaction, and all the invocations in that scope are associated with it."
89,For this it uses a very simple (e.g. no recovery) internal TransactionManager implementation.
89,"With batching, you get:"
89,Locks you acquire during an invocation are held until the batch completes
89,Changes are all replicated around the cluster in a batch as part of the batch completion process. Reduces replication chatter for each update in the batch.
89,"If synchronous replication or invalidation are used, a failure in replication/invalidation will cause the batch to roll back."
89,All the transaction related configurations apply for batching as well.
89,9.1.8. Transaction recovery
89,"Recovery is a feature of XA transactions, which deal with the eventuality of a resource or possibly even the transaction manager failing, and recovering accordingly from such a situation."
89,When to use recovery
89,Consider a distributed transaction in which money is transferred from an account stored in an external database to an account stored in Infinispan.
89,"When TransactionManager.commit() is invoked, both resources prepare successfully (1st phase). During the commit (2nd) phase, the database successfully applies the changes whilst Infinispan fails before receiving the commit request from the transaction manager."
89,At this point the system is in an inconsistent state: money is taken from the account in the external database but not visible yet in Infinispan (since locks are only released during 2nd phase of a two-phase commit protocol).
89,Recovery deals with this situation to make sure data in both the database and Infinispan ends up in a consistent state.
89,How does it work
89,Recovery is coordinated by the transaction manager.
89,"The transaction manager works with Infinispan to determine the list of in-doubt transactions that require manual intervention and informs the system administrator (via email, log alerts, etc)."
89,"This process is transaction manager specific, but generally requires some configuration on the transaction manager."
89,"Knowing the in-doubt transaction ids, the system administrator can now connect to the Infinispan cluster and replay the commit of transactions or force the rollback."
89,Infinispan provides JMX tooling for this - this is explained extensively in the Transaction recovery and reconciliation section.
89,Configuring recovery
89,Recovery is not enabled by default in Infinispan.
89,"If disabled, the TransactionManager won’t be able to work with Infinispan to determine the in-doubt transactions."
89,The Transaction configuration section shows how to enable it.
89,NOTE: recovery-cache attribute is not mandatory and it is configured per-cache.
89,"For recovery to work, mode must be set to FULL_XA, since full-blown XA transactions are needed."
89,Enable JMX support
89,In order to be able to use JMX for managing recovery JMX support must be explicitly enabled.
89,Recovery cache
89,"In order to track in-doubt transactions and be able to reply them, Infinispan caches all transaction state for future use."
89,"This state is held only for in-doubt transaction, being removed for successfully completed transactions after when the commit/rollback phase completed."
89,This in-doubt transaction data is held within a local cache: this allows one to configure swapping this info to disk through cache loader in the case it gets too big.
89,This cache can be specified through the recovery-cache configuration attribute.
89,If not specified Infinispan will configure a local cache for you.
89,It is possible (though not mandated) to share same recovery cache between all the Infinispan caches that have recovery enabled.
89,"If the default recovery cache is overridden, then the specified recovery cache must use a TransactionManagerLookup that returns a different transaction manager than the one used by the cache itself."
89,Integration with the transaction manager
89,"Even though this is transaction manager specific, generally a transaction manager would need a reference to a XAResource implementation in order to invoke XAResource.recover() on it."
89,In order to obtain a reference to an Infinispan XAResource following API can be used:
89,XAResource xar = cache.getAdvancedCache().getXAResource();
89,It is a common practice to run the recovery in a different process from the one running the transaction.
89,Reconciliation
89,The transaction manager informs the system administrator on in-doubt transaction in a proprietary way.
89,At this stage it is assumed that the system administrator knows transaction’s XID (a byte array).
89,A normal recovery flow is:
89,"STEP 1: The system administrator connects to an Infinispan server through JMX, and lists the in doubt transactions."
89,The image below demonstrates JConsole connecting to an Infinispan node that has an in doubt transaction.
89,Figure 8. Show in-doubt transactions
89,"The status of each in-doubt transaction is displayed(in this example "" PREPARED "")."
89,"There might be multiple elements in the status field, e.g. ""PREPARED"" and ""COMMITTED"" in the case the transaction committed on certain nodes but not on all of them."
89,"STEP 2: The system administrator visually maps the XID received from the transaction manager to an Infinispan internal id, represented as a number."
89,"This step is needed because the XID, a byte array, cannot conveniently be passed to the JMX tool (e.g. JConsole) and then re-assembled on Infinispan’s side."
89,"STEP 3: The system administrator forces the transaction’s commit/rollback through the corresponding jmx operation, based on the internal id."
89,The image below is obtained by forcing the commit of the transaction based on its internal id.
89,Figure 9. Force commit
89,"All JMX operations described above can be executed on any node, regardless of where the transaction originated."
89,Force commit/rollback based on XID
89,XID-based JMX operations for forcing in-doubt transactions' commit/rollback are available as well: these methods receive byte[] arrays describing the XID instead of the number associated with the transactions (as previously described at step 2).
89,These can be useful e.g. if one wants to set up an automatic completion job for certain in-doubt transactions.
89,This process is plugged into transaction manager’s recovery and has access to the transaction manager’s XID objects.
89,10. Configuring locking and concurrency
89,Infinispan uses multi-versioned concurrency control (MVCC) to improve access to shared data.
89,Allowing concurrent readers and writers
89,Readers and writers do not block one another
89,Write skews can be detected and handled
89,Internal locks can be striped
89,10.1. Locking and concurrency
89,Multi-versioned concurrency control (MVCC) is a concurrency scheme popular with relational databases and other data stores.
89,MVCC offers many advantages over coarse-grained Java synchronization and even JDK Locks for access to shared data.
89,"Infinispan’s MVCC implementation makes use of minimal locks and synchronizations, leaning heavily towards lock-free techniques such as compare-and-swap and lock-free data structures wherever possible, which helps optimize for multi-CPU and multi-core environments."
89,"In particular, Infinispan’s MVCC implementation is heavily optimized for readers."
89,"Reader threads do not acquire explicit locks for entries, and instead directly read the entry in question."
89,"Writers, on the other hand, need to acquire a write lock."
89,"This ensures only one concurrent writer per entry, causing concurrent writers to queue up to change an entry."
89,"To allow concurrent reads, writers make a copy of the entry they intend to modify, by wrapping the entry in an MVCCEntry."
89,This copy isolates concurrent readers from seeing partially modified state.
89,"Once a write has completed, MVCCEntry.commit() will flush changes to the data container and subsequent readers will see the changes written."
89,10.1.1. Clustered caches and locks
89,"In Infinispan clusters, primary owner nodes are responsible for locking keys."
89,"For non-transactional caches, Infinispan forwards the write operation to the primary owner of the key so it can attempt to lock it."
89,Infinispan either then forwards the write operation to the other owners or throws an exception if it cannot lock the key.
89,"If the operation is conditional and fails on the primary owner, Infinispan does not forward it to the other owners."
89,"For transactional caches, primary owners can lock keys with optimistic and pessimistic locking modes."
89,Infinispan also supports different isolation levels to control concurrent reads between transactions.
89,10.1.2. The LockManager
89,The LockManager is a component that is responsible for locking an entry for writing.
89,The LockManager makes use of a LockContainer to locate/hold/create locks.
89,"LockContainers come in two broad flavours, with support for lock striping and with support for one lock per entry."
89,10.1.3. Lock striping
89,"Lock striping entails the use of a fixed-size, shared collection of locks for the entire cache, with locks being allocated to entries based on the entry’s key’s hash code."
89,"Similar to the way the JDK’s ConcurrentHashMap allocates locks, this allows for a highly scalable, fixed-overhead locking mechanism in exchange for potentially unrelated entries being blocked by the same lock."
89,The alternative is to disable lock striping - which would mean a new lock is created per entry.
89,"This approach may give you greater concurrent throughput, but it will be at the cost of additional memory usage, garbage collection churn, etc."
89,Default lock striping settings
89,"lock striping is disabled by default, due to potential deadlocks that can happen if locks for different keys end up in the same lock stripe."
89,The size of the shared lock collection used by lock striping can be tuned using the concurrencyLevel attribute of the <locking /> configuration element.
89,Configuration example:
89,"<locking striping=""false|true""/>"
89,new ConfigurationBuilder().locking().useLockStriping(false|true);
89,10.1.4. Concurrency levels
89,"In addition to determining the size of the striped lock container, this concurrency level is also used to tune any JDK ConcurrentHashMap based collections where related, such as internal to DataContainers."
89,"Please refer to the JDK ConcurrentHashMap Javadocs for a detailed discussion of concurrency levels, as this parameter is used in exactly the same way in Infinispan."
89,Configuration example:
89,"<locking concurrency-level=""32""/>"
89,new ConfigurationBuilder().locking().concurrencyLevel(32);
89,10.1.5. Lock timeout
89,"The lock timeout specifies the amount of time, in milliseconds, to wait for a contented lock."
89,Configuration example:
89,"<locking acquire-timeout=""10000""/>"
89,new ConfigurationBuilder().locking().lockAcquisitionTimeout(10000);
89,//alternatively
89,"new ConfigurationBuilder().locking().lockAcquisitionTimeout(10, TimeUnit.SECONDS);"
89,10.1.6. Consistency
89,The fact that a single owner is locked (as opposed to all owners being locked) does not break the following consistency guarantee:
89,"if key K is hashed to nodes {A, B} and transaction TX1 acquires a lock for K, let’s say on A."
89,"If another transaction, TX2, is started on B (or any other node) and TX2 tries to lock K then it will fail with a timeout as the lock is already held by TX1."
89,"The reason for this is the that the lock for a key K is always, deterministically, acquired on the same node of the cluster, regardless of where the transaction originates."
89,10.1.7. Data Versioning
89,Infinispan supports two forms of data versioning: simple and external.
89,The simple versioning is used in transactional caches for write skew check.
89,"The external versioning is used to encapsulate an external source of data versioning within Infinispan, such as when using Infinispan with Hibernate which in turn gets its data version information directly from a database."
89,"In this scheme, a mechanism to pass in the version becomes necessary, and overloaded versions of put() and putForExternalRead() will be provided in AdvancedCache to take in an external data version."
89,This is then stored on the InvocationContext and applied to the entry at commit time.
89,Write skew checks cannot and will not be performed in the case of external data versioning.
89,11. Using clustered counters
89,Infinispan provides counters that record the count of objects and are distributed across all nodes in a cluster.
89,11.1. Clustered Counters
89,Clustered counters are counters which are distributed and shared among all nodes in the Infinispan cluster.
89,Counters can have different consistency levels: strong and weak.
89,"Although a strong/weak consistent counter has separate interfaces, both support updating its value,"
89,return the current value and they provide events when its value is updated.
89,Details are provided below in this document to help you choose which one fits best your uses-case.
89,11.1.1. Installation and Configuration
89,"In order to start using the counters, you needs to add the dependency in your Maven pom.xml file:"
89,pom.xml
89,<dependency>
89,<groupId>org.infinispan</groupId>
89,<artifactId>infinispan-clustered-counter</artifactId>
89,</dependency>
89,The counters can be configured Infinispan configuration file or on-demand via the CounterManager interface detailed
89,later in this document.
89,A counters configured in Infinispan configuration file is created at boot time when the EmbeddedCacheManager is starting.
89,These counters are started eagerly and they are available in all the cluster’s nodes.
89,configuration.xml
89,<infinispan>
89,<cache-container ...>
89,"<!-- To persist counters, you need to configure the global state. -->"
89,<global-state>
89,<!-- Global state configuration goes here. -->
89,</global-state>
89,<!-- Cache configuration goes here. -->
89,"<counters xmlns=""urn:infinispan:config:counters:14.0"" num-owners=""3"" reliability=""CONSISTENT"">"
89,"<strong-counter name=""c1"" initial-value=""1"" storage=""PERSISTENT""/>"
89,"<strong-counter name=""c2"" initial-value=""2"" storage=""VOLATILE"" lower-bound=""0""/>"
89,"<strong-counter name=""c3"" initial-value=""3"" storage=""PERSISTENT"" upper-bound=""5""/>"
89,"<strong-counter name=""c4"" initial-value=""4"" storage=""VOLATILE"" lower-bound=""0"" upper-bound=""10""/>"
89,"<strong-counter name=""c5"" initial-value=""0"" upper-bound=""100"" lifespan=""60000""/>"
89,"<weak-counter name=""c6"" initial-value=""5"" storage=""PERSISTENT"" concurrency-level=""1""/>"
89,</counters>
89,</cache-container>
89,</infinispan>
89,"or programmatically, in the GlobalConfigurationBuilder:"
89,GlobalConfigurationBuilder globalConfigurationBuilder = ...;
89,CounterManagerConfigurationBuilder builder = globalConfigurationBuilder.addModule(CounterManagerConfigurationBuilder.class);
89,builder.numOwner(3).reliability(Reliability.CONSISTENT);
89,"builder.addStrongCounter().name(""c1"").initialValue(1).storage(Storage.PERSISTENT);"
89,"builder.addStrongCounter().name(""c2"").initialValue(2).lowerBound(0).storage(Storage.VOLATILE);"
89,"builder.addStrongCounter().name(""c3"").initialValue(3).upperBound(5).storage(Storage.PERSISTENT);"
89,"builder.addStrongCounter().name(""c4"").initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE);"
89,"builder.addStrongCounter().name(""c5"").initialValue(0).upperBound(100).lifespan(60000);"
89,"builder.addWeakCounter().name(""c6"").initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT);"
89,"On other hand, the counters can be configured on-demand, at any time after the EmbeddedCacheManager is initialized."
89,CounterManager manager = ...;
89,"manager.defineCounter(""c1"", CounterConfiguration.builder(CounterType.UNBOUNDED_STRONG).initialValue(1).storage(Storage.PERSISTENT).build());"
89,"manager.defineCounter(""c2"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(2).lowerBound(0).storage(Storage.VOLATILE).build());"
89,"manager.defineCounter(""c3"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(3).upperBound(5).storage(Storage.PERSISTENT).build());"
89,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE).build());"
89,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(0).upperBound(100).lifespan(60000).build());"
89,"manager.defineCounter(""c6"", CounterConfiguration.builder(CounterType.WEAK).initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT).build());"
89,CounterConfiguration is immutable and can be reused.
89,The method defineCounter() will return true if the counter is successful configured or false otherwise.
89,"However, if the configuration is invalid, the method will throw a CounterConfigurationException."
89,"To find out if a counter is already defined, use the method isDefined()."
89,CounterManager manager = ...
89,"if (!manager.isDefined(""someCounter"")) {"
89,"manager.define(""someCounter"", ...);"
89,Additional resources
89,Infinispan configuration schema reference
89,List counter names
89,"To list all the counters defined, the method CounterManager.getCounterNames() returns a collection of all counter"
89,names created cluster-wide.
89,11.1.2. CounterManager interface
89,"The CounterManager interface is the entry point to define, retrieve and remove counters."
89,Embedded deployments
89,CounterManager automatically listen to the creation of EmbeddedCacheManager and proceeds with the registration
89,of an
89,instance of it per EmbeddedCacheManager.
89,It starts the caches needed to store the counter state and configures the default counters.
89,Retrieving the CounterManager is as simple as invoke the
89,EmbeddedCounterManagerFactory.asCounterManager(EmbeddedCacheManager)
89,as shown in the example below:
89,// create or obtain your EmbeddedCacheManager
89,EmbeddedCacheManager manager = ...;
89,// retrieve the CounterManager
89,CounterManager counterManager = EmbeddedCounterManagerFactory.asCounterManager(manager);
89,Server deployments
89,"For Hot Rod clients, the CounterManager is registered in the RemoteCacheManager and can be retrieved as follows:"
89,// create or obtain your RemoteCacheManager
89,RemoteCacheManager manager = ...;
89,// retrieve the CounterManager
89,CounterManager counterManager = RemoteCounterManagerFactory.asCounterManager(manager);
89,Remove a counter via CounterManager
89,There is a difference between remove a counter via the Strong/WeakCounter interfaces and the CounterManager.
89,The CounterManager.remove(String) removes the counter value from the cluster and removes all the listeners registered
89,in the counter in the local counter instance.
89,"In addition, the counter instance is no longer reusable and it may return an invalid results."
89,"On the other side, the Strong/WeakCounter removal only removes the counter value."
89,The instance can still be reused and the listeners still works.
89,The counter is re-created if it is accessed after a removal.
89,11.1.3. The Counter
89,A counter can be strong (StrongCounter) or weakly consistent (WeakCounter) and both is identified by a name.
89,"They have a specific interface but they share some logic, namely, both of them are asynchronous"
89,"( a CompletableFuture is returned by each operation), provide an update event and can be reset to its initial value."
89,"If you don’t want to use the async API, it is possible to return a synchronous counter via sync() method."
89,The API is the same but without the CompletableFuture return value.
89,The following methods are common to both interfaces:
89,String getName();
89,CompletableFuture<Long> getValue();
89,CompletableFuture<Void> reset();
89,<T extends CounterListener> Handle<T> addListener(T listener);
89,CounterConfiguration getConfiguration();
89,CompletableFuture<Void> remove();
89,SyncStrongCounter sync(); //SyncWeakCounter for WeakCounter
89,getName() returns the counter name (identifier).
89,getValue() returns the current counter’s value.
89,reset() allows to reset the counter’s value to its initial value.
89,addListener() register a listener to receive update events.
89,More details about it in the Notification and Events section.
89,getConfiguration() returns the configuration used by the counter.
89,remove() removes the counter value from the cluster. The instance can still be used and the listeners are kept.
89,sync() creates a synchronous counter.
89,The counter is re-created if it is accessed after a removal.
89,The StrongCounter interface: when the consistency or bounds matters.
89,The strong counter provides uses a single key stored in Infinispan cache to provide the consistency needed.
89,All the updates are performed under the key lock to updates its values.
89,"On other hand, the reads don’t acquire any locks and reads the current value."
89,"Also, with this scheme, it allows to bound the counter value and provide atomic operations like compare-and-set/swap."
89,A StrongCounter can be retrieved from the CounterManager by using the getStrongCounter() method.
89,As an example:
89,CounterManager counterManager = ...
89,"StrongCounter aCounter = counterManager.getStrongCounter(""my-counter"");"
89,"Since every operation will hit a single key, the StrongCounter has a higher contention rate."
89,The StrongCounter interface adds the following method:
89,default CompletableFuture<Long> incrementAndGet() {
89,return addAndGet(1L);
89,default CompletableFuture<Long> decrementAndGet() {
89,return addAndGet(-1L);
89,CompletableFuture<Long> addAndGet(long delta);
89,"CompletableFuture<Boolean> compareAndSet(long expect, long update);"
89,"CompletableFuture<Long> compareAndSwap(long expect, long update);"
89,incrementAndGet() increments the counter by one and returns the new value.
89,decrementAndGet() decrements the counter by one and returns the new value.
89,addAndGet() adds a delta to the counter’s value and returns the new value.
89,compareAndSet() and compareAndSwap() atomically set the counter’s value if the current value is the expected.
89,A operation is considered completed when the CompletableFuture is completed.
89,The difference between compare-and-set and compare-and-swap is that the former returns true if the operation succeeds
89,while the later returns the previous value.
89,The compare-and-swap is successful if the return value is the same as the expected.
89,Bounded StrongCounter
89,"When bounded, all the update method above will throw a CounterOutOfBoundsException when they reached the"
89,lower or upper bound.
89,The exception has the following methods to check which side bound has been reached:
89,public boolean isUpperBoundReached();
89,public boolean isLowerBoundReached();
89,Uses cases
89,The strong counter fits better in the following uses cases:
89,"When counter’s value is needed after each update (example, cluster-wise ids generator or sequences)"
89,"When a bounded counter is needed (example, rate limiter)"
89,Usage Examples
89,"StrongCounter counter = counterManager.getStrongCounter(""unbounded_counter"");"
89,// incrementing the counter
89,"System.out.println(""new value is "" + counter.incrementAndGet().get());"
89,// decrement the counter's value by 100 using the functional API
89,counter.addAndGet(-100).thenApply(v -> {
89,"System.out.println(""new value is "" + v);"
89,return null;
89,}).get();
89,"// alternative, you can do some work while the counter is updated"
89,CompletableFuture<Long> f = counter.addAndGet(10);
89,// ... do some work ...
89,"System.out.println(""new value is "" + f.get());"
89,"// and then, check the current value"
89,"System.out.println(""current value is "" + counter.getValue().get());"
89,"// finally, reset to initial value"
89,counter.reset().get();
89,"System.out.println(""current value is "" + counter.getValue().get());"
89,// or set to a new value if zero
89,"System.out.println(""compare and set succeeded? "" + counter.compareAndSet(0, 1));"
89,"And below, there is another example using a bounded counter:"
89,"StrongCounter counter = counterManager.getStrongCounter(""bounded_counter"");"
89,// incrementing the counter
89,try {
89,"System.out.println(""new value is "" + counter.addAndGet(100).get());"
89,} catch (ExecutionException e) {
89,Throwable cause = e.getCause();
89,if (cause instanceof CounterOutOfBoundsException) {
89,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
89,"System.out.println(""ops, upper bound reached."");"
89,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
89,"System.out.println(""ops, lower bound reached."");"
89,// now using the functional API
89,"counter.addAndGet(-100).handle((v, throwable) -> {"
89,if (throwable != null) {
89,Throwable cause = throwable.getCause();
89,if (cause instanceof CounterOutOfBoundsException) {
89,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
89,"System.out.println(""ops, upper bound reached."");"
89,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
89,"System.out.println(""ops, lower bound reached."");"
89,return null;
89,"System.out.println(""new value is "" + v);"
89,return null;
89,}).get();
89,Compare-and-set vs Compare-and-swap examples:
89,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
89,"long oldValue, newValue;"
89,do {
89,oldValue = counter.getValue().get();
89,newValue = someLogic(oldValue);
89,"} while (!counter.compareAndSet(oldValue, newValue).get());"
89,"With compare-and-swap, it saves one invocation counter invocation (counter.getValue())"
89,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
89,long oldValue = counter.getValue().get();
89,"long currentValue, newValue;"
89,do {
89,currentValue = oldValue;
89,newValue = someLogic(oldValue);
89,"} while ((oldValue = counter.compareAndSwap(oldValue, newValue).get()) != currentValue);"
89,"To use a strong counter as a rate limiter, configure upper-bound and lifespan parameters as follows:"
89,// 5 request per minute
89,CounterConfiguration configuration = CounterConfiguration.builder(CounterType.BOUNDED_STRONG)
89,.upperBound(5)
89,.lifespan(60000)
89,.build();
89,"counterManager.defineCounter(""rate_limiter"", configuration);"
89,"StrongCounter counter = counterManager.getStrongCounter(""rate_limiter"");"
89,"// on each operation, invoke"
89,try {
89,counter.incrementAndGet().get();
89,// continue with operation
89,} catch (InterruptedException e) {
89,Thread.currentThread().interrupt();
89,} catch (ExecutionException e) {
89,if (e.getCause() instanceof CounterOutOfBoundsException) {
89,// maximum rate. discard operation
89,return;
89,} else {
89,"// unexpected error, handling property"
89,The lifespan parameter is an experimental capability and may be removed in a future version.
89,The WeakCounter interface: when speed is needed
89,The WeakCounter stores the counter’s value in multiple keys in Infinispan cache.
89,The number of keys created is configured by the concurrency-level attribute.
89,Each key stores a partial state of the counter’s value and it can be updated concurrently.
89,It main advantage over the StrongCounter is the lower contention in the cache.
89,"On other hand, the read of its value is more expensive and bounds are not allowed."
89,The reset operation should be handled with caution.
89,It is not atomic and it produces intermediates values.
89,These value may be seen by a read operation and by any listener registered.
89,A WeakCounter can be retrieved from the CounterManager by using the getWeakCounter() method.
89,As an example:
89,CounterManager counterManager = ...
89,"StrongCounter aCounter = counterManager.getWeakCounter(""my-counter);"
89,Weak Counter Interface
89,The WeakCounter adds the following methods:
89,default CompletableFuture<Void> increment() {
89,return add(1L);
89,default CompletableFuture<Void> decrement() {
89,return add(-1L);
89,CompletableFuture<Void> add(long delta);
89,They are similar to the `StrongCounter’s methods but they don’t return the new value.
89,Uses cases
89,The weak counter fits best in uses cases where the result of the update operation is not needed or the counter’s value
89,is not required too often.
89,Collecting statistics is a good example of such an use case.
89,Examples
89,"Below, there is an example of the weak counter usage."
89,"WeakCounter counter = counterManager.getWeakCounter(""my_counter"");"
89,// increment the counter and check its result
89,counter.increment().get();
89,"System.out.println(""current value is "" + counter.getValue());"
89,CompletableFuture<Void> f = counter.add(-100);
89,//do some work
89,f.get(); //wait until finished
89,"System.out.println(""current value is "" + counter.getValue().get());"
89,//using the functional API
89,"counter.reset().whenComplete((aVoid, throwable) -> System.out.println(""Reset done "" + (throwable == null ? ""successfully"" : ""unsuccessfully""))).get();"
89,"System.out.println(""current value is "" + counter.getValue().get());"
89,11.1.4. Notifications and Events
89,Both strong and weak counter supports a listener to receive its updates events.
89,The listener must implement CounterListener and it can be registered by the following method:
89,<T extends CounterListener> Handle<T> addListener(T listener);
89,The CounterListener has the following interface:
89,public interface CounterListener {
89,void onUpdate(CounterEvent entry);
89,The Handle object returned has the main goal to remove the CounterListener when it is not longer needed.
89,"Also, it allows to have access to the CounterListener instance that is it handling."
89,It has the following interface:
89,public interface Handle<T extends CounterListener> {
89,T getCounterListener();
89,void remove();
89,"Finally, the CounterEvent has the previous and current value and state."
89,It has the following interface:
89,public interface CounterEvent {
89,long getOldValue();
89,State getOldState();
89,long getNewValue();
89,State getNewState();
89,The state is always State.VALID for unbounded strong counter and weak counter.
89,State.LOWER_BOUND_REACHED and State.UPPER_BOUND_REACHED are only valid for bounded strong counters.
89,The weak counter reset() operation will trigger multiple notification with intermediate values.
89,12. Listeners and notifications
89,Use listeners with Infinispan to get notifications when events occur for the Cache Manager or for caches.
89,12.1. Listeners and notifications
89,"Infinispan offers a listener API, where clients can register for and get notified when events take place."
89,This annotation-driven API applies to 2 different levels: cache level events and Cache Manager level events.
89,Events trigger a notification which is dispatched to listeners.
89,Listeners are simple POJOs annotated with @Listener and registered using the methods defined in the Listenable interface.
89,"Both Cache and CacheManager implement Listenable, which means you can attach listeners to either a cache or a Cache Manager, to receive either cache-level or Cache Manager-level notifications."
89,"For example, the following class defines a listener to print out some information every time a new entry is added to the cache, in a non blocking fashion:"
89,@Listener
89,public class PrintWhenAdded {
89,Queue<CacheEntryCreatedEvent> events = new ConcurrentLinkedQueue<>();
89,@CacheEntryCreated
89,public CompletionStage<Void> print(CacheEntryCreatedEvent event) {
89,events.add(event);
89,return null;
89,"For more comprehensive examples, please see the Javadocs for @Listener."
89,12.2. Cache-level notifications
89,"Cache-level events occur on a per-cache basis, and by default are only raised on nodes where the events occur."
89,Note in a distributed cache these events are only raised on the owners of data being affected.
89,"Examples of cache-level events are entries being added, removed, modified, etc."
89,These events trigger notifications to listeners registered to a specific cache.
89,"Please see the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for a comprehensive list of all cache-level notifications, and their respective method-level annotations."
89,Please refer to the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for the list of cache-level notifications available in Infinispan.
89,Cluster listeners
89,The cluster listeners should be used when it is desirable to listen to the cache events on a single node.
89,To do so all that is required is set to annotate your listener as being clustered.
89,@Listener (clustered = true)
89,public class MyClusterListener { .... }
89,There are some limitations to cluster listeners from a non clustered listener.
89,"A cluster listener can only listen to @CacheEntryModified, @CacheEntryCreated, @CacheEntryRemoved and @CacheEntryExpired events."
89,Note this means any other type of event will not be listened to for this listener.
89,"Only the post event is sent to a cluster listener, the pre event is ignored."
89,Event filtering and conversion
89,All applicable events on the node where the listener is installed will be raised to the listener.
89,"It is possible to dynamically filter what events are raised by using a KeyFilter (only allows filtering on keys) or CacheEventFilter (used to filter for keys, old value, old metadata, new value, new metadata, whether command was retried, if the event is before the event (ie. isPre) and also the command type)."
89,The example here shows a simple KeyFilter that will only allow events to be raised when an event modified the entry for the key Only Me.
89,public class SpecificKeyFilter implements KeyFilter<String> {
89,private final String keyToAccept;
89,public SpecificKeyFilter(String keyToAccept) {
89,if (keyToAccept == null) {
89,throw new NullPointerException();
89,this.keyToAccept = keyToAccept;
89,public boolean accept(String key) {
89,return keyToAccept.equals(key);
89,...
89,"cache.addListener(listener, new SpecificKeyFilter(""Only Me""));"
89,...
89,This can be useful when you want to limit what events you receive in a more efficient manner.
89,There is also a CacheEventConverter that can be supplied that allows for converting a value to another before raising the event.
89,This can be nice to modularize any code that does value conversions.
89,The mentioned filters and converters are especially beneficial when used in conjunction with a Cluster Listener.
89,This is because the filtering and conversion is done on the node where the event originated and not on the node where event is listened to.
89,This can provide benefits of not having to replicate events across the cluster (filter) or even have reduced payloads (converter).
89,Initial State Events
89,When a listener is installed it will only be notified of events after it is fully installed.
89,It may be desirable to get the current state of the cache contents upon first registration of listener by having an event generated of type @CacheEntryCreated for each element in the cache.
89,Any additionally generated events during this initial phase will be queued until appropriate events have been raised.
89,This only works for clustered listeners at this time.
89,ISPN-4608 covers adding this for non clustered listeners.
89,Duplicate Events
89,It is possible in a non transactional cache to receive duplicate events.
89,This is possible when the primary owner of a key goes down while trying to perform a write operation such as a put.
89,"Infinispan internally will rectify the put operation by sending it to the new primary owner for the given key automatically, however there are no guarantees in regards to if the write was first replicated to backups."
89,"Thus more than 1 of the following write events (CacheEntryCreatedEvent, CacheEntryModifiedEvent & CacheEntryRemovedEvent) may be sent on a single operation."
89,If more than one event is generated Infinispan will mark the event that it was generated by a retried command to help the user to know when this occurs without having to pay attention to view changes.
89,@Listener
89,public class MyRetryListener {
89,@CacheEntryModified
89,public void entryModified(CacheEntryModifiedEvent event) {
89,if (event.isCommandRetried()) {
89,// Do something
89,Also when using a CacheEventFilter or CacheEventConverter the EventType contains a method isRetry to tell if the event was generated due to retry.
89,12.3. Cache Manager notifications
89,"Events that occur on a Cache Manager level are cluster-wide and involve events that affect all caches created by a single Cache Manager. Examples of Cache Manager events are nodes joining or leaving a cluster, or caches starting or stopping."
89,"See the org.infinispan.notifications.cachemanagerlistener.annotation package for a comprehensive list of all Cache Manager notifications,"
89,and their respective method-level annotations.
89,12.4. Synchronicity of events
89,"By default, all async notifications are dispatched in the notification thread pool."
89,Sync notifications will delay the operation from continuing until the listener method completes or the CompletionStage
89,"completes (the former causing the thread to block). Alternatively, you could annotate your listener as asynchronous in"
89,"which case the operation will continue immediately, while the notification is completed asynchronously on the notification thread pool."
89,"To do this, simply annotate your listener such:"
89,Asynchronous Listener
89,@Listener (sync = false)
89,public class MyAsyncListener {
89,@CacheEntryCreated
89,void listen(CacheEntryCreatedEvent event) { }
89,Blocking Synchronous Listener
89,@Listener
89,public class MySyncListener {
89,@CacheEntryCreated
89,void listen(CacheEntryCreatedEvent event) { }
89,Non-Blocking Listener
89,@Listener
89,public class MyNonBlockingListener {
89,@CacheEntryCreated
89,CompletionStage<Void> listen(CacheEntryCreatedEvent event) { }
89,Asynchronous thread pool
89,"To tune the thread pool used to dispatch such asynchronous notifications, use the <listener-executor /> XML element in your configuration file."
89,Last updated 2024-03-13 12:05:42 UTC
90,Configuring Infinispan caches
90,Configuring Infinispan caches
90,Table of Contents
90,1. Infinispan caches
90,1.1. Cache API
90,1.2. Cache Managers
90,1.3. Cache modes
90,1.3.1. Comparison of cache modes
90,1.4. Local caches
90,1.4.1. Simple caches
90,2. Clustered caches
90,2.1. Replicated caches
90,2.2. Distributed caches
90,2.2.1. Read consistency
90,2.2.2. Key ownership
90,2.2.3. Capacity factors
90,Zero capacity nodes
90,2.2.4. Level one (L1) caches
90,2.2.5. Server hinting
90,2.2.6. Key affinity service
90,2.2.7. Grouping API
90,2.3. Invalidation caches
90,2.4. Scattered caches
90,2.5. Asynchronous replication
90,2.5.1. Return values with asynchronous replication
90,2.6. Configuring initial cluster size
90,3. Infinispan cache configuration
90,3.1. Declarative cache configuration
90,3.1.1. Cache configuration
90,3.2. Adding cache templates
90,3.2.1. Creating caches from templates
90,3.2.2. Cache template inheritance
90,3.2.3. Cache template wildcards
90,3.2.4. Cache templates from multiple XML files
90,3.3. Creating remote caches
90,3.3.1. Default Cache Manager
90,3.3.2. Creating caches with Infinispan Console
90,3.3.3. Creating remote caches with the Infinispan CLI
90,3.3.4. Creating remote caches from Hot Rod clients
90,3.3.5. Creating remote caches with the REST API
90,3.4. Creating embedded caches
90,3.4.1. Adding Infinispan to your project
90,3.4.2. Creating and using embedded caches
90,3.4.3. Cache API
90,AdvancedCache API
90,Flags
90,Asynchronous API
90,Why use such an API?
90,Which processes actually happen asynchronously?
90,4. Enabling and configuring Infinispan statistics and JMX monitoring
90,4.1. Enabling statistics in embedded caches
90,4.2. Enabling statistics in remote caches
90,4.3. Enabling Hot Rod client statistics
90,4.4. Configuring Infinispan metrics
90,4.5. Registering JMX MBeans
90,4.5.1. Enabling JMX remote ports
90,4.5.2. Infinispan MBeans
90,4.5.3. Registering MBeans in custom MBean servers
90,4.6. Exporting metrics during a state transfer operation
90,4.7. Monitoring the status of cross-site replication
90,5. Configuring JVM memory usage
90,5.1. Default memory configuration
90,5.2. Eviction and expiration
90,5.3. Eviction with Infinispan caches
90,5.3.1. Eviction strategies
90,5.3.2. Configuring maximum count eviction
90,5.3.3. Configuring maximum size eviction
90,5.3.4. Manual eviction
90,5.3.5. Passivation with eviction
90,5.4. Expiration with lifespan and maximum idle
90,5.4.1. How expiration works
90,5.4.2. Expiration reaper
90,5.4.3. Maximum idle and clustered caches
90,5.4.4. Configuring lifespan and maximum idle times for caches
90,5.4.5. Configuring lifespan and maximum idle times per entry
90,5.5. JVM heap and off-heap memory
90,5.5.1. Off-heap data storage
90,5.5.2. Configuring off-heap memory
90,6. Configuring persistent storage
90,6.1. Passivation
90,6.1.1. How passivation works
90,6.2. Write-through cache stores
90,6.3. Write-behind cache stores
90,6.4. Segmented cache stores
90,6.5. Shared cache stores
90,6.6. Transactions with persistent cache stores
90,6.7. Global persistent location
90,6.7.1. Configuring the global persistent location
90,6.8. File-based cache stores
90,6.8.1. Configuring file-based cache stores
90,6.8.2. Configuring single file cache stores
90,6.9. JDBC connection factories
90,6.9.1. Configuring managed datasources
90,Configuring caches with JNDI names
90,Connection pool tuning properties
90,6.9.2. Configuring JDBC connection pools with Agroal properties
90,6.10. SQL cache stores
90,6.10.1. Data types for keys and values
90,Composite keys and values
90,Embedded keys
90,SQL types to Protobuf types
90,6.10.2. Loading Infinispan caches from database tables
90,6.10.3. Using SQL queries to load data and perform operations
90,SQL query store configuration
90,6.10.4. SQL cache store troubleshooting
90,6.11. JDBC string-based cache stores
90,6.11.1. Configuring JDBC string-based cache stores
90,6.12. RocksDB cache stores
90,6.13. Remote cache stores
90,6.14. Cluster cache loaders
90,6.15. Creating custom cache store implementations
90,6.15.1. Infinispan Persistence SPI
90,6.15.2. Creating cache stores
90,6.15.3. Examples of custom cache store configuration
90,6.15.4. Deploying custom cache stores
90,6.16. Migrating data between cache stores
90,6.16.1. Cache store migrator
90,6.16.2. Getting the cache store migrator
90,6.16.3. Configuring the cache store migrator
90,Configuration properties for the cache store migrator
90,6.16.4. Migrating Infinispan cache stores
90,7. Configuring Infinispan to handle network partitions
90,7.1. Split clusters and network partitions
90,7.1.1. Data consistency in a split cluster
90,7.2. Cache availability and degraded mode
90,7.2.1. Degraded cache recovery example
90,7.2.2. Verifying cache availability during network partitions
90,7.2.3. Making caches available
90,7.3. Configuring partition handling
90,7.4. Partition handling strategies
90,7.5. Merge policies
90,7.6. Configuring custom merge policies
90,7.7. Manually merging partitions in embedded caches
90,8. Security authorization with role-based access control
90,8.1. Infinispan user roles and permissions
90,8.1.1. Permissions
90,8.1.2. Role and permission mappers
90,Mapping users to roles and permissions in Infinispan
90,8.1.3. Configuring role mappers
90,8.2. Configuring caches with security authorization
90,9. Configuring transactions
90,9.1. Transactions
90,9.1.1. Configuring transactions
90,9.1.2. Isolation levels
90,9.1.3. Transaction locking
90,Pessimistic transactional cache
90,Optimistic transactional cache
90,What do I need - pessimistic or optimistic transactions?
90,9.1.4. Write Skews
90,Forcing write locks on keys in pessimitic transactions
90,9.1.5. Dealing with exceptions
90,9.1.6. Enlisting Synchronizations
90,9.1.7. Batching
90,API
90,Batching and JTA
90,9.1.8. Transaction recovery
90,When to use recovery
90,How does it work
90,Configuring recovery
90,Enable JMX support
90,Recovery cache
90,Integration with the transaction manager
90,Reconciliation
90,Force commit/rollback based on XID
90,10. Configuring locking and concurrency
90,10.1. Locking and concurrency
90,10.1.1. Clustered caches and locks
90,10.1.2. The LockManager
90,10.1.3. Lock striping
90,10.1.4. Concurrency levels
90,10.1.5. Lock timeout
90,10.1.6. Consistency
90,10.1.7. Data Versioning
90,11. Using clustered counters
90,11.1. Clustered Counters
90,11.1.1. Installation and Configuration
90,List counter names
90,11.1.2. CounterManager interface
90,Remove a counter via CounterManager
90,11.1.3. The Counter
90,The StrongCounter interface: when the consistency or bounds matters.
90,Bounded StrongCounter
90,Uses cases
90,Usage Examples
90,The WeakCounter interface: when speed is needed
90,Weak Counter Interface
90,Uses cases
90,Examples
90,11.1.4. Notifications and Events
90,12. Listeners and notifications
90,12.1. Listeners and notifications
90,12.2. Cache-level notifications
90,12.3. Cache Manager notifications
90,12.4. Synchronicity of events
90,Create and configure Infinispan caches with the mode and capabilities that suit your application requirements.
90,You can configure caches with expiration to remove stale entries or use eviction to control cache size.
90,"You can also add persistent storage to caches, enable partition handling for clustered caches, set up transactions, and more."
90,1. Infinispan caches
90,"Infinispan caches provide flexible, in-memory data stores that you can configure to suit use cases such as:"
90,Boosting application performance with high-speed local caches.
90,Optimizing databases by decreasing the volume of write operations.
90,Providing resiliency and durability for consistent data across clusters.
90,1.1. Cache API
90,"Cache<K,V> is the central interface for Infinispan and extends java.util.concurrent.ConcurrentMap."
90,"Cache entries are highly concurrent data structures in key:value format that support a wide and configurable range of data types, from simple strings to much more complex objects."
90,1.2. Cache Managers
90,The CacheManager API is the entry point for interacting with Infinispan.
90,"Cache Managers control cache lifecycle; creating, modifying, and deleting cache instances."
90,Cache Managers also provide cluster management and monitoring along with the ability to execute code across nodes.
90,Infinispan provides two CacheManager implementations:
90,EmbeddedCacheManager
90,Entry point for caches when running Infinispan inside the same Java Virtual Machine (JVM) as the client application.
90,RemoteCacheManager
90,Entry point for caches when running Infinispan Server in its own JVM. When you instantiate a RemoteCacheManager it establishes a persistent TCP connection to Infinispan Server through the Hot Rod endpoint.
90,Both embedded and remote CacheManager implementations share some methods and properties.
90,"However, semantic differences do exist between EmbeddedCacheManager and RemoteCacheManager."
90,1.3. Cache modes
90,Infinispan Cache Managers can create and control multiple caches that use
90,"different modes. For example, you can use the same Cache Manager for local"
90,"caches, distributed caches, and caches with invalidation mode."
90,Local
90,Infinispan runs as a single node and never replicates read or write operations on cache entries.
90,Replicated
90,Infinispan replicates all cache entries on all nodes in a cluster and performs local read operations only.
90,Distributed
90,Infinispan replicates cache entries on a subset of nodes in a cluster and assigns entries to fixed owner nodes.
90,Infinispan requests read operations from owner nodes to ensure it returns the correct value.
90,Invalidation
90,Infinispan evicts stale data from all nodes whenever operations modify entries in the cache. Infinispan performs local read operations only.
90,Scattered
90,Infinispan stores cache entries across a subset of nodes.
90,By default Infinispan assigns a primary owner and a backup owner to each cache entry in scattered caches.
90,"Infinispan assigns primary owners in the same way as with distributed caches, while backup owners are always the nodes that initiate the write operations."
90,Infinispan requests read operations from at least one owner node to ensure it returns the correct value.
90,1.3.1. Comparison of cache modes
90,The cache mode that you should choose depends on the qualities and guarantees you need for your data.
90,The following table summarizes the primary differences between cache modes:
90,Simple
90,Local
90,Invalidation
90,Replicated
90,Distributed
90,Scattered
90,Clustered
90,Yes
90,Yes
90,Yes
90,Yes
90,Read performance
90,Highest
90,(local)
90,High
90,(local)
90,High
90,(local)
90,High
90,(local)
90,Medium
90,(owners)
90,Medium
90,(primary)
90,Write performance
90,Highest
90,(local)
90,High
90,(local)
90,Low
90,"(all nodes, no data)"
90,Lowest
90,(all nodes)
90,Medium
90,(owner nodes)
90,Higher
90,(single RPC)
90,Capacity
90,Single node
90,Single node
90,Single node
90,Smallest node
90,Cluster
90,"\$(sum_(i=1)^""nodes""""node_capacity"")/""owners""\$"
90,Cluster
90,"\$(sum_(i=1)^""nodes""""node_capacity"")/""2""\$"
90,Availability
90,Single node
90,Single node
90,Single node
90,All nodes
90,Owner nodes
90,Owner nodes
90,Features
90,"No TX, persistence, indexing"
90,All
90,No indexing
90,All
90,All
90,No TX
90,1.4. Local caches
90,Infinispan offers a local cache mode that is similar to a ConcurrentHashMap.
90,"Caches offer more capabilities than simple maps, including write-through and"
90,write-behind to persistent storage as well as management capabilities such as eviction and expiration.
90,"The Infinispan Cache API extends the ConcurrentMap API in Java, making it easy to migrate from a map to a Infinispan cache."
90,Local cache configuration
90,XML
90,"<local-cache name=""mycache"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,</local-cache>
90,JSON
90,"""local-cache"": {"
90,"""name"": ""mycache"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,YAML
90,localCache:
90,"name: ""mycache"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,1.4.1. Simple caches
90,A simple cache is a type of local cache that disables support for the following capabilities:
90,Transactions and invocation batching
90,Persistent storage
90,Custom interceptors
90,Indexing
90,Transcoding
90,"However, you can use other Infinispan capabilities with simple caches such as expiration, eviction, statistics, and security features."
90,"If you configure a capability that is not compatible with a simple cache, Infinispan throws an exception."
90,Simple cache configuration
90,XML
90,"<local-cache simple-cache=""true"" />"
90,JSON
90,"""local-cache"" : {"
90,"""simple-cache"" : ""true"""
90,YAML
90,localCache:
90,"simpleCache: ""true"""
90,2. Clustered caches
90,You can create embedded and remote caches on Infinispan clusters that replicate data across nodes.
90,2.1. Replicated caches
90,Infinispan replicates all entries in the cache to all nodes in the cluster.
90,Each node can perform read operations locally.
90,"Replicated caches provide a quick and easy way to share state across a cluster, but is suitable for clusters of less than ten nodes."
90,"Because the number of replication requests scales linearly with the number of nodes in the cluster, using replicated caches with larger clusters reduces performance."
90,However you can use UDP multicasting for replication requests to improve performance.
90,"Each key has a primary owner, which serializes data container updates in order to provide consistency."
90,Figure 1. Replicated cache
90,Synchronous or asynchronous replication
90,"Synchronous replication blocks the caller (e.g. on a cache.put(key, value)) until the modifications have been replicated successfully to all the nodes in the cluster."
90,"Asynchronous replication performs replication in the background, and write operations return immediately."
90,"Asynchronous replication is not recommended, because communication errors, or errors that happen on remote nodes are not reported to the caller."
90,Transactions
90,"If transactions are enabled, write operations are not replicated through the primary owner."
90,"With pessimistic locking, each write triggers a lock message, which is broadcast to all the nodes."
90,"During transaction commit, the originator broadcasts a one-phase prepare message and an unlock message (optional)."
90,Either the one-phase prepare or the unlock message is fire-and-forget.
90,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
90,"Again, either the one-phase prepare or the unlock message is fire-and-forget."
90,2.2. Distributed caches
90,"Infinispan attempts to keep a fixed number of copies of any entry in the cache,"
90,configured as numOwners.
90,"This allows distributed caches to scale linearly, storing more data as nodes are added to the cluster."
90,"As nodes join and leave the cluster, there will be times when a key has more or less than numOwners copies."
90,"In particular, if numOwners nodes leave in quick succession, some entries will be lost, so we say that a distributed cache tolerates numOwners - 1 node failures."
90,The number of copies represents a trade-off between performance and durability of data.
90,"The more copies you maintain, the lower performance will be, but also the lower the risk of losing data due to server or network failures."
90,"Infinispan splits the owners of a key into one primary owner, which coordinates writes to the key, and zero or more backup owners."
90,The following diagram shows a write operation that a client sends to a backup owner.
90,"In this case the backup node forwards the write to the primary owner, which then replicates the write to the backup."
90,Figure 2. Cluster replication
90,Figure 3. Distributed cache
90,Read operations
90,Read operations request the value from the primary owner.
90,"If the primary owner does not respond in a reasonable amount of time, Infinispan requests the value from the backup owners as well."
90,"A read operation may require 0 messages if the key is present in the local cache, or up to 2 * numOwners messages if all the owners are slow."
90,Write operations
90,Write operations result in at most 2 * numOwners messages.
90,One message from the originator to the primary owner and numOwners - 1 messages from the primary to the backup nodes along with the corresponding acknowledgment messages.
90,Cache topology changes may cause retries and additional messages for both read and write operations.
90,Synchronous or asynchronous replication
90,Asynchronous replication is not recommended because it can lose updates.
90,"In addition to losing updates, asynchronous distributed caches can also see a stale value when a thread writes to a key and then immediately reads the same key."
90,Transactions
90,"Transactional distributed caches send lock/prepare/commit/unlock messages to the affected nodes only, meaning all nodes that own at least one key affected by the transaction."
90,"As an optimization, if the transaction writes to a single key and the originator is the primary owner of the key, lock messages are not replicated."
90,2.2.1. Read consistency
90,"Even with synchronous replication, distributed caches are not linearizable."
90,"For transactional caches, they do not support serialization/snapshot isolation."
90,"For example, a thread is carrying out a single put request:"
90,cache.get(k) -> v1
90,"cache.put(k, v2)"
90,cache.get(k) -> v2
90,But another thread might see the values in a different order:
90,cache.get(k) -> v2
90,cache.get(k) -> v1
90,"The reason is that read can return the value from any owner, depending on how fast the primary owner replies."
90,The write is not atomic across all the owners.
90,"In fact, the primary commits the update only after it receives a confirmation from the backup."
90,"While the primary is waiting for the confirmation message from the backup, reads from the backup will see the new value, but reads from the primary will see the old one."
90,2.2.2. Key ownership
90,Distributed caches split entries into a fixed number of segments and assign
90,each segment to a list of owner nodes.
90,"Replicated caches do the same, with the exception that every node is an owner."
90,The first node in the list of owners is the primary owner.
90,The other nodes in the list are backup owners.
90,"When the cache topology changes, because a node joins or leaves the cluster, the segment ownership table is broadcast to every node."
90,This allows nodes to locate keys without making multicast requests or maintaining metadata for each key.
90,The numSegments property configures the number of segments available.
90,"However, the number of segments cannot change unless the cluster is restarted."
90,Likewise the key-to-segment mapping cannot change.
90,Keys must always map to the same segments regardless of cluster topology changes.
90,It is important that the key-to-segment mapping evenly distributes the number of segments allocated to each node while minimizing the number of segments that must move when the cluster topology changes.
90,Consistent hash factory implementation
90,Description
90,SyncConsistentHashFactory
90,Uses an algorithm based on consistent hashing. Selected by default when server hinting is disabled.
90,This implementation always assigns keys to the same nodes in every cache as
90,"long as the cluster is symmetric. In other words, all caches run on all nodes."
90,This implementation does have some negative points in that the load distribution is slightly uneven. It also moves more segments than strictly necessary on a join or leave.
90,TopologyAwareSyncConsistentHashFactory
90,Equivalent to SyncConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners. This is the default consistent hashing implementation with server hinting.
90,DefaultConsistentHashFactory
90,"Achieves a more even distribution than SyncConsistentHashFactory, but with one disadvantage. The order in which nodes join the cluster determines which nodes own which segments. As a result, keys might be assigned to different nodes in different caches."
90,TopologyAwareConsistentHashFactory
90,Equivalent to DefaultConsistentHashFactory but used with server hinting to distribute data across the topology so that backed up copies of data are stored on different nodes in the topology than the primary owners.
90,ReplicatedConsistentHashFactory
90,Used internally to implement replicated caches. You should never explicitly
90,select this algorithm in a distributed cache.
90,Hashing configuration
90,"You can configure ConsistentHashFactory implementations, including custom ones, with embedded caches only."
90,XML
90,"<distributed-cache name=""distributedCache"""
90,"owners=""2"""
90,"segments=""100"""
90,"capacity-factor=""2"" />"
90,ConfigurationBuilder
90,Configuration c = new ConfigurationBuilder()
90,.clustering()
90,.cacheMode(CacheMode.DIST_SYNC)
90,.hash()
90,.numOwners(2)
90,.numSegments(100)
90,.capacityFactor(2)
90,.build();
90,Additional resources
90,KeyPartitioner
90,2.2.3. Capacity factors
90,Capacity factors allocate the number of segments based on resources available to each node in the cluster.
90,The capacity factor for a node applies to segments for which that node is both the primary owner and backup owner.
90,"In other words, the capacity factor specifies is the total capacity that a node has in comparison to other nodes in the cluster."
90,The default value is 1 which means that all nodes in the cluster have an equal capacity and Infinispan allocates the same number of segments to all nodes in the cluster.
90,"However, if nodes have different amounts of memory available to them, you can configure the capacity factor so that the Infinispan hashing algorithm assigns each node a number of segments weighted by its capacity."
90,The value for the capacity factor configuration must be a positive number and can be a fraction such as 1.5.
90,You can also configure a capacity factor of 0 but is recommended only for nodes that join the cluster temporarily and should use the zero capacity configuration instead.
90,Zero capacity nodes
90,"You can configure nodes where the capacity factor is 0 for every cache, user defined caches, and internal caches."
90,"When defining a zero capacity node, the node does not hold any data."
90,Zero capacity node configuration
90,XML
90,<infinispan>
90,"<cache-container zero-capacity-node=""true"" />"
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""zero-capacity-node"" : ""true"""
90,YAML
90,infinispan:
90,cacheContainer:
90,"zeroCapacityNode: ""true"""
90,ConfigurationBuilder
90,new GlobalConfigurationBuilder().zeroCapacityNode(true);
90,2.2.4. Level one (L1) caches
90,Infinispan nodes create local replicas when they retrieve entries from another node in the cluster.
90,L1 caches avoid repeatedly looking up entries on primary owner nodes and adds performance.
90,The following diagram illustrates how L1 caches work:
90,Figure 4. L1 cache
90,"In the ""L1 cache"" diagram:"
90,A client invokes cache.get() to read an entry for which another node in the cluster is the primary owner.
90,The originator node forwards the read operation to the primary owner.
90,The primary owner returns the key/value entry.
90,The originator node creates a local copy.
90,Subsequent cache.get() invocations return the local entry instead of forwarding to the primary owner.
90,L1 caching performance
90,Enabling L1 improves performance for read operations but requires primary owner nodes to broadcast invalidation messages when entries are modified.
90,This ensures that Infinispan removes any out of date replicas across the cluster.
90,"However this also decreases performance of write operations and increases memory usage, reducing overall capacity of caches."
90,"Infinispan evicts and expires local replicas, or L1 entries, like any other cache entry."
90,L1 cache configuration
90,XML
90,"<distributed-cache l1-lifespan=""5000"""
90,"l1-cleanup-interval=""60000"">"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""l1-lifespan"": ""5000"","
90,"""l1-cleanup-interval"": ""60000"""
90,YAML
90,distributedCache:
90,"l1Lifespan: ""5000"""
90,"l1-cleanup-interval: ""60000"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
90,.l1()
90,".lifespan(5000, TimeUnit.MILLISECONDS)"
90,".cleanupTaskFrequency(60000, TimeUnit.MILLISECONDS);"
90,2.2.5. Server hinting
90,"Server hinting increases availability of data in distributed caches by replicating entries across as many servers, racks, and data centers as possible."
90,Server hinting applies only to distributed caches.
90,"When Infinispan distributes the copies of your data, it follows the order of precedence: site, rack, machine, and node."
90,All of the configuration attributes are optional.
90,"For example, when you specify only the rack IDs, then Infinispan distributes the copies across different racks and nodes."
90,Server hinting can impact cluster rebalancing operations by moving more segments than necessary if the number of segments for the cache is too low.
90,An alternative for clusters in multiple data centers is cross-site replication.
90,Server hinting configuration
90,XML
90,<cache-container>
90,"<transport cluster=""MyCluster"""
90,"machine=""LinuxServer01"""
90,"rack=""Rack01"""
90,"site=""US-WestCoast""/>"
90,</cache-container>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""transport"" : {"
90,"""cluster"" : ""MyCluster"","
90,"""machine"" : ""LinuxServer01"","
90,"""rack"" : ""Rack01"","
90,"""site"" : ""US-WestCoast"""
90,YAML
90,cacheContainer:
90,transport:
90,"cluster: ""MyCluster"""
90,"machine: ""LinuxServer01"""
90,"rack: ""Rack01"""
90,"site: ""US-WestCoast"""
90,GlobalConfigurationBuilder
90,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder()
90,.transport()
90,".clusterName(""MyCluster"")"
90,".machineId(""LinuxServer01"")"
90,".rackId(""Rack01"")"
90,".siteId(""US-WestCoast"");"
90,Additional resources
90,org.infinispan.configuration.global.TransportConfigurationBuilder
90,2.2.6. Key affinity service
90,"In a distributed cache, a key is allocated to a list of nodes with an opaque algorithm."
90,There is no easy way to reverse the computation and generate a key that maps to a particular node.
90,"However, Infinispan can generate a sequence of (pseudo-)random keys, see what their primary owner is, and hand them out to the application when it needs a key mapping to a particular node."
90,Following code snippet depicts how a reference to this service can be obtained and used.
90,// 1. Obtain a reference to a cache
90,Cache cache = ...
90,Address address = cache.getCacheManager().getAddress();
90,// 2. Create the affinity service
90,KeyAffinityService keyAffinityService = KeyAffinityServiceFactory.newLocalKeyAffinityService(
90,"cache,"
90,"new RndKeyGenerator(),"
90,"Executors.newSingleThreadExecutor(),"
90,100);
90,// 3. Obtain a key for which the local node is the primary owner
90,Object localKey = keyAffinityService.getKeyForAddress(address);
90,// 4. Insert the key in the cache
90,"cache.put(localKey, ""yourValue"");"
90,The service is started at step 2: after this point it uses the supplied Executor to generate and queue keys.
90,"At step 3, we obtain a key from the service, and at step 4 we use it."
90,Lifecycle
90,"KeyAffinityService extends Lifecycle, which allows stopping and (re)starting it:"
90,public interface Lifecycle {
90,void start();
90,void stop();
90,The service is instantiated through KeyAffinityServiceFactory.
90,"All the factory methods have an Executor parameter, that is used for asynchronous key generation (so that it"
90,won’t happen in the caller’s thread).
90,It is the user’s responsibility to handle the shutdown of this Executor.
90,"The KeyAffinityService, once started, needs to be explicitly stopped."
90,This stops the background key generation and releases other held resources.
90,The only situation in which KeyAffinityService stops by itself is when the Cache Manager with which it was registered is shutdown.
90,Topology changes
90,"When the cache topology changes, the ownership of the keys generated by the KeyAffinityService might change."
90,"The key affinity service keep tracks of these topology changes and doesn’t return keys that would currently map to a different node, but it won’t do anything about keys generated earlier."
90,"As such, applications should treat KeyAffinityService purely as an optimization, and they should not rely on the location of a generated key for correctness."
90,"In particular, applications should not rely on keys generated by KeyAffinityService for the same address to always be located together."
90,Collocation of keys is only provided by the Grouping API.
90,2.2.7. Grouping API
90,"Complementary to the Key affinity service, the Grouping API allows you to co-locate a group of entries on the same nodes, but without being able to select the actual nodes."
90,"By default, the segment of a key is computed using the key’s hashCode()."
90,"If you use the Grouping API, Infinispan will compute the segment of the group and use that as the segment of the key."
90,"When the Grouping API is in use, it is important that every node can still compute the owners of every key without contacting other nodes."
90,"For this reason, the group cannot be specified manually."
90,The group can either be intrinsic to the entry (generated by the key class) or extrinsic (generated by an external function).
90,"To use the Grouping API, you must enable groups."
90,Configuration c = new ConfigurationBuilder()
90,.clustering().hash().groups().enabled()
90,.build();
90,<distributed-cache>
90,"<groups enabled=""true""/>"
90,</distributed-cache>
90,"If you have control of the key class (you can alter the class definition, it’s not part of an unmodifiable library), then we recommend using an intrinsic group."
90,"The intrinsic group is specified by adding the @Group annotation to a method, for example:"
90,class User {
90,...
90,String office;
90,...
90,public int hashCode() {
90,"// Defines the hash for the key, normally used to determine location"
90,...
90,// Override the location by specifying a group
90,// All keys in the same group end up with the same owners
90,@Group
90,public String getOffice() {
90,return office;
90,The group method must return a String
90,"If you don’t have control over the key class, or the determination of the group is an orthogonal concern to the key class, we recommend using an extrinsic group."
90,An extrinsic group is specified by implementing the Grouper interface.
90,public interface Grouper<T> {
90,"String computeGroup(T key, String group);"
90,Class<T> getKeyType();
90,"If multiple Grouper classes are configured for the same key type, all of them will be called, receiving the value computed by the previous one."
90,"If the key class also has a @Group annotation, the first Grouper will receive the group computed by the annotated method."
90,This allows you even greater control over the group when using an intrinsic group.
90,Example Grouper implementation
90,public class KXGrouper implements Grouper<String> {
90,"// The pattern requires a String key, of length 2, where the first character is"
90,"// ""k"" and the second character is a digit. We take that digit, and perform"
90,"// modular arithmetic on it to assign it to group ""0"" or group ""1""."
90,"private static Pattern kPattern = Pattern.compile(""(^k)(<a>\\d</a>)$"");"
90,"public String computeGroup(String key, String group) {"
90,Matcher matcher = kPattern.matcher(key);
90,if (matcher.matches()) {
90,"String g = Integer.parseInt(matcher.group(2)) % 2 + """";"
90,return g;
90,} else {
90,return null;
90,public Class<String> getKeyType() {
90,return String.class;
90,Grouper implementations must be registered explicitly in the cache configuration.
90,If you are configuring Infinispan programmatically:
90,Configuration c = new ConfigurationBuilder()
90,.clustering().hash().groups().enabled().addGrouper(new KXGrouper())
90,.build();
90,"Or, if you are using XML:"
90,<distributed-cache>
90,"<groups enabled=""true"">"
90,"<grouper class=""com.example.KXGrouper"" />"
90,</groups>
90,</distributed-cache>
90,Advanced API
90,AdvancedCache has two group-specific methods:
90,getGroup(groupName) retrieves all keys in the cache that belong to a group.
90,removeGroup(groupName) removes all the keys in the cache that belong to a group.
90,"Both methods iterate over the entire data container and store (if present), so they can be slow when a cache contains lots of small groups."
90,2.3. Invalidation caches
90,Invalidation cache mode in Infinispan is designed to optimize systems that perform high volumes of read operations to a shared permanent data store.
90,You can use invalidation mode to reduce the number of database writes when state changes occur.
90,Invalidation cache mode is deprecated for Infinispan remote deployments.
90,Use invalidation cache mode with embedded caches that are stored in shared cache stores only.
90,"Invalidation cache mode is effective only when you have a permanent data store, such as a database, and are only using Infinispan as an optimization in a read-heavy system to prevent hitting the database for every read."
90,"When a cache is configured for invalidation, each data change in a cache triggers a message to other caches in the cluster, informing them that their data is now stale and should be removed from memory."
90,Invalidation messages remove stale values from other nodes' memory.
90,"The messages are very small compared to replicating the entire value, and also other caches in the cluster look up modified data in a lazy manner, only when needed."
90,The update to the shared store is typically handled by user application code or Hibernate.
90,Figure 5. Invalidation cache
90,"Sometimes the application reads a value from the external store and wants to write it to the local cache, without removing it from the other nodes."
90,"To do this, it must call Cache.putForExternalRead(key, value) instead of Cache.put(key, value)."
90,Invalidation mode is suitable only for shared stores where all nodes can access the same data.
90,"Using invalidation mode without a persistent store is impractical, as updated values need to be read from a shared store for consistency across nodes."
90,"Never use invalidation mode with a local, non-shared, cache store."
90,"The invalidation message will not remove entries in the local store, and some nodes will keep seeing the stale value."
90,"An invalidation cache can also be configured with a special cache loader, ClusterLoader."
90,"When ClusterLoader is enabled, read operations that do not find the key on the local node will request it from all the other nodes first, and store it in memory locally."
90,"This can lead to storing stale values, so only use it if you have a high tolerance for stale values."
90,Synchronous or asynchronous replication
90,"When synchronous, a write operation blocks until all nodes in the cluster have evicted the stale value."
90,"When asynchronous, the originator broadcasts invalidation messages but does not wait for responses."
90,That means other nodes still see the stale value for a while after the write completed on the originator.
90,Transactions
90,Transactions can be used to batch the invalidation messages.
90,Transactions acquire the key lock on the primary owner.
90,"With pessimistic locking, each write triggers a lock message, which is"
90,broadcast to all the nodes.
90,"During transaction commit, the originator broadcasts a one-phase prepare message (optionally fire-and-forget) which invalidates all affected keys and releases the locks."
90,"With optimistic locking, the originator broadcasts a prepare message, a commit message, and an unlock message (optional)."
90,"Either the one-phase prepare or the unlock message is fire-and-forget, and the last message always releases the locks."
90,2.4. Scattered caches
90,Scattered caches are very similar to distributed caches as they allow linear scaling of the cluster.
90,Scattered caches allow single node failure by maintaining two copies of the data (numOwners=2).
90,"Unlike distributed caches, the location of data is not fixed; while we use the same Consistent Hash algorithm to locate the primary owner, the backup copy is stored on the node that wrote the data last time."
90,"When the write originates on the primary owner, backup copy is stored on any other node (the exact location of this copy is not important)."
90,"This has the advantage of single Remote Procedure Call (RPC) for any write (distributed caches require one or two RPCs), but reads have to always target the primary owner."
90,"That results in faster writes but possibly slower reads, and therefore this mode is more suitable for write-intensive applications."
90,Storing multiple backup copies also results in slightly higher memory consumption.
90,"In order to remove out-of-date backup copies, invalidation messages are broadcast in the cluster, which generates some overhead."
90,This lowers the performance of scattered caches in clusters with a large number of nodes.
90,"When a node crashes, the primary copy may be lost."
90,"Therefore, the cluster has to reconcile the backups and find out the last written backup copy."
90,This process results in more network traffic during state transfer.
90,"Since the writer of data is also a backup, even if we specify machine/rack/site IDs on the transport level the cluster cannot be resilient to more than one failure on the same machine/rack/site."
90,You cannot use scattered caches with transactions or asynchronous replication.
90,"The cache is configured in a similar way as the other cache modes, here is an example of declarative configuration:"
90,"<scattered-cache name=""scatteredCache"" />"
90,Configuration c = new ConfigurationBuilder()
90,.clustering().cacheMode(CacheMode.SCATTERED_SYNC)
90,.build();
90,Scattered mode is not exposed in the server configuration as the server is usually accessed through the Hot Rod
90,protocol. The protocol automatically selects primary owner for the writes and therefore the write (in distributed
90,"mode with two owner) requires single RPC inside the cluster, too. Therefore, scattered cache would not bring"
90,the performance benefit.
90,2.5. Asynchronous replication
90,All clustered cache modes can be configured to use asynchronous communications with the
90,"mode=""ASYNC"""
90,"attribute on the <replicated-cache/>, <distributed-cache>, or <invalidation-cache/>"
90,element.
90,"With asynchronous communications, the originator node does not receive any"
90,"acknowledgement from the other nodes about the status of the operation, so there is no"
90,way to check if it succeeded on other nodes.
90,"We do not recommend asynchronous communications in general, as they can cause"
90,"inconsistencies in the data, and the results are hard to reason about."
90,"Nevertheless, sometimes speed is more important than consistency, and the option is"
90,available for those cases.
90,Asynchronous API
90,"The Asynchronous API allows you to use synchronous communications,"
90,but without blocking the user thread.
90,There is one caveat:
90,The asynchronous operations do NOT preserve the program order.
90,"If a thread calls cache.putAsync(k, v1); cache.putAsync(k, v2), the final value of k"
90,may be either v1 or v2.
90,The advantage over using asynchronous communications is that the final value can’t be
90,v1 on one node and v2 on another.
90,2.5.1. Return values with asynchronous replication
90,"Because the Cache interface extends java.util.Map, write methods like"
90,"put(key, value) and remove(key) return the previous value by default."
90,"In some cases, the return value may not be correct:"
90,"When using AdvancedCache.withFlags() with Flag.IGNORE_RETURN_VALUE,"
90,"Flag.SKIP_REMOTE_LOOKUP, or Flag.SKIP_CACHE_LOAD."
90,"When the cache is configured with unreliable-return-values=""true""."
90,When using asynchronous communications.
90,"When there are multiple concurrent writes to the same key, and the cache topology"
90,changes.
90,"The topology change will make Infinispan retry the write operations, and a retried"
90,operation’s return value is not reliable.
90,Transactional caches return the correct previous value in cases 3 and 4.
90,"However, transactional caches also have a gotcha: in distributed mode, the"
90,read-committed isolation level is implemented as repeatable-read.
90,"That means this example of ""double-checked locking"" won’t work:"
90,Cache cache = ...
90,TransactionManager tm = ...
90,tm.begin();
90,try {
90,Integer v1 = cache.get(k);
90,// Increment the value
90,"Integer v2 = cache.put(k, v1 + 1);"
90,"if (Objects.equals(v1, v2) {"
90,// success
90,} else {
90,// retry
90,} finally {
90,tm.commit();
90,The correct way to implement this is to use
90,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(k).
90,"In caches with optimistic locking, writes can also return stale previous values. Write skew checks can avoid stale previous values."
90,2.6. Configuring initial cluster size
90,Infinispan handles cluster topology changes dynamically.
90,This means that nodes do not need to wait for other nodes to join the cluster before Infinispan initializes the caches.
90,"If your applications require a specific number of nodes in the cluster before caches start, you can configure the initial cluster size as part of the transport."
90,Procedure
90,Open your Infinispan configuration for editing.
90,Set the minimum number of nodes required before caches start with the initial-cluster-size attribute or initialClusterSize() method.
90,"Set the timeout, in milliseconds, after which the Cache Manager does not start with the initial-cluster-timeout attribute or initialClusterTimeout() method."
90,Save and close your Infinispan configuration.
90,Initial cluster size configuration
90,XML
90,<infinispan>
90,<cache-container>
90,"<transport initial-cluster-size=""4"""
90,"initial-cluster-timeout=""30000"" />"
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""transport"" : {"
90,"""initial-cluster-size"" : ""4"","
90,"""initial-cluster-timeout"" : ""30000"""
90,YAML
90,infinispan:
90,cacheContainer:
90,transport:
90,"initialClusterSize: ""4"""
90,"initialClusterTimeout: ""30000"""
90,ConfigurationBuilder
90,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
90,.transport()
90,.initialClusterSize(4)
90,".initialClusterTimeout(30000, TimeUnit.MILLISECONDS);"
90,3. Infinispan cache configuration
90,Cache configuration controls how Infinispan stores your data.
90,"As part of your cache configuration, you declare the cache mode you want to use."
90,"For instance, you can configure Infinispan clusters to use replicated caches or distributed caches."
90,Your configuration also defines the characteristics of your caches and enables the Infinispan capabilities that you want to use when handling data.
90,"For instance, you can configure how Infinispan encodes entries in your caches, whether replication requests happen synchronously or asynchronously between nodes, if entries are mortal or immortal, and so on."
90,3.1. Declarative cache configuration
90,"You can configure caches declaratively, in XML, JSON, and YAML format, according to the Infinispan schema."
90,Declarative cache configuration has the following advantages over programmatic configuration:
90,Portability
90,Define each configuration in a standalone file that you can use to create embedded and remote caches.
90,You can also use declarative configuration to create caches with Infinispan Operator for clusters running on Kubernetes.
90,Simplicity
90,Keep markup languages separate to programming languages.
90,"For example, to create remote caches it is generally better to not add complex XML directly to Java code."
90,"Infinispan Server configuration extends infinispan.xml to include cluster transport mechanisms, security realms, and endpoint configuration."
90,"If you declare caches as part of your Infinispan Server configuration you should use management tooling, such as Ansible or Chef, to keep it synchronized across the cluster."
90,"To dynamically synchronize remote caches across Infinispan clusters, create them at runtime."
90,3.1.1. Cache configuration
90,"You can create declarative cache configuration in XML, JSON, and YAML format."
90,All declarative caches must conform to the Infinispan schema.
90,"Configuration in JSON format must follow the structure of an XML configuration, elements correspond to objects and attributes correspond to fields."
90,Infinispan restricts characters to a maximum of 255 for a cache name or a cache template name.
90,"If you exceed this character limit, Infinispan throws an exception."
90,Write succinct cache names and cache template names.
90,"A file system might set a limitation for the length of a file name, so ensure that a cache’s name does not exceed this limitation."
90,"If a cache name exceeds a file system’s naming limitation, general operations or initialing operations towards that cache might fail."
90,Write succinct file names.
90,Distributed caches
90,XML
90,"<distributed-cache owners=""2"""
90,"segments=""256"""
90,"capacity-factor=""1.0"""
90,"l1-lifespan=""5000"""
90,"mode=""SYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<locking isolation=""REPEATABLE_READ""/>"
90,"<transaction mode=""FULL_XA"""
90,"locking=""OPTIMISTIC""/>"
90,"<expiration lifespan=""5000"""
90,"max-idle=""1000"" />"
90,"<memory max-count=""1000000"""
90,"when-full=""REMOVE""/>"
90,"<indexing enabled=""true"""
90,"storage=""local-heap"">"
90,"<index-reader refresh-interval=""1000""/>"
90,<indexed-entities>
90,<indexed-entity>org.infinispan.Person</indexed-entity>
90,</indexed-entities>
90,</indexing>
90,"<partition-handling when-split=""ALLOW_READ_WRITES"""
90,"merge-policy=""PREFERRED_NON_NULL""/>"
90,"<persistence passivation=""false"">"
90,<!-- Persistent storage configuration. -->
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""mode"": ""SYNC"","
90,"""owners"": ""2"","
90,"""segments"": ""256"","
90,"""capacity-factor"": ""1.0"","
90,"""l1-lifespan"": ""5000"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""locking"": {"
90,"""isolation"": ""REPEATABLE_READ"""
90,"""transaction"": {"
90,"""mode"": ""FULL_XA"","
90,"""locking"": ""OPTIMISTIC"""
90,"""expiration"" : {"
90,"""lifespan"" : ""5000"","
90,"""max-idle"" : ""1000"""
90,"""memory"": {"
90,"""max-count"": ""1000000"","
90,"""when-full"": ""REMOVE"""
90,"""indexing"" : {"
90,"""enabled"" : true,"
90,"""storage"" : ""local-heap"","
90,"""index-reader"" : {"
90,"""refresh-interval"" : ""1000"""
90,"""indexed-entities"": ["
90,"""org.infinispan.Person"""
90,"""partition-handling"" : {"
90,"""when-split"" : ""ALLOW_READ_WRITES"","
90,"""merge-policy"" : ""PREFERRED_NON_NULL"""
90,"""persistence"" : {"
90,"""passivation"" : false"
90,YAML
90,distributedCache:
90,"mode: ""SYNC"""
90,"owners: ""2"""
90,"segments: ""256"""
90,"capacityFactor: ""1.0"""
90,"l1Lifespan: ""5000"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,locking:
90,"isolation: ""REPEATABLE_READ"""
90,transaction:
90,"mode: ""FULL_XA"""
90,"locking: ""OPTIMISTIC"""
90,expiration:
90,"lifespan: ""5000"""
90,"maxIdle: ""1000"""
90,memory:
90,"maxCount: ""1000000"""
90,"whenFull: ""REMOVE"""
90,indexing:
90,"enabled: ""true"""
90,"storage: ""local-heap"""
90,indexReader:
90,"refreshInterval: ""1000"""
90,indexedEntities:
90,"- ""org.infinispan.Person"""
90,partitionHandling:
90,"whenSplit: ""ALLOW_READ_WRITES"""
90,"mergePolicy: ""PREFERRED_NON_NULL"""
90,persistence:
90,"passivation: ""false"""
90,# Persistent storage configuration.
90,Replicated caches
90,XML
90,"<replicated-cache segments=""256"""
90,"mode=""SYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<locking isolation=""REPEATABLE_READ""/>"
90,"<transaction mode=""FULL_XA"""
90,"locking=""OPTIMISTIC""/>"
90,"<expiration lifespan=""5000"""
90,"max-idle=""1000"" />"
90,"<memory max-count=""1000000"""
90,"when-full=""REMOVE""/>"
90,"<indexing enabled=""true"""
90,"storage=""local-heap"">"
90,"<index-reader refresh-interval=""1000""/>"
90,<indexed-entities>
90,<indexed-entity>org.infinispan.Person</indexed-entity>
90,</indexed-entities>
90,</indexing>
90,"<partition-handling when-split=""ALLOW_READ_WRITES"""
90,"merge-policy=""PREFERRED_NON_NULL""/>"
90,"<persistence passivation=""false"">"
90,<!-- Persistent storage configuration. -->
90,</persistence>
90,</replicated-cache>
90,JSON
90,"""replicated-cache"": {"
90,"""mode"": ""SYNC"","
90,"""segments"": ""256"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""locking"": {"
90,"""isolation"": ""REPEATABLE_READ"""
90,"""transaction"": {"
90,"""mode"": ""FULL_XA"","
90,"""locking"": ""OPTIMISTIC"""
90,"""expiration"" : {"
90,"""lifespan"" : ""5000"","
90,"""max-idle"" : ""1000"""
90,"""memory"": {"
90,"""max-count"": ""1000000"","
90,"""when-full"": ""REMOVE"""
90,"""indexing"" : {"
90,"""enabled"" : true,"
90,"""storage"" : ""local-heap"","
90,"""index-reader"" : {"
90,"""refresh-interval"" : ""1000"""
90,"""indexed-entities"": ["
90,"""org.infinispan.Person"""
90,"""partition-handling"" : {"
90,"""when-split"" : ""ALLOW_READ_WRITES"","
90,"""merge-policy"" : ""PREFERRED_NON_NULL"""
90,"""persistence"" : {"
90,"""passivation"" : false"
90,YAML
90,replicatedCache:
90,"mode: ""SYNC"""
90,"segments: ""256"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,locking:
90,"isolation: ""REPEATABLE_READ"""
90,transaction:
90,"mode: ""FULL_XA"""
90,"locking: ""OPTIMISTIC"""
90,expiration:
90,"lifespan: ""5000"""
90,"maxIdle: ""1000"""
90,memory:
90,"maxCount: ""1000000"""
90,"whenFull: ""REMOVE"""
90,indexing:
90,"enabled: ""true"""
90,"storage: ""local-heap"""
90,indexReader:
90,"refreshInterval: ""1000"""
90,indexedEntities:
90,"- ""org.infinispan.Person"""
90,partitionHandling:
90,"whenSplit: ""ALLOW_READ_WRITES"""
90,"mergePolicy: ""PREFERRED_NON_NULL"""
90,persistence:
90,"passivation: ""false"""
90,# Persistent storage configuration.
90,Multiple caches
90,XML
90,<infinispan
90,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
90,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-14.0.xsd"
90,"urn:infinispan:server:14.0 https://infinispan.org/schemas/infinispan-server-14.0.xsd"""
90,"xmlns=""urn:infinispan:config:14.0"""
90,"xmlns:server=""urn:infinispan:server:14.0"">"
90,"<cache-container name=""default"""
90,"statistics=""true"">"
90,"<distributed-cache name=""mycacheone"""
90,"mode=""ASYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<expiration lifespan=""300000""/>"
90,"<memory max-size=""400MB"""
90,"when-full=""REMOVE""/>"
90,</distributed-cache>
90,"<distributed-cache name=""mycachetwo"""
90,"mode=""SYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<expiration lifespan=""300000""/>"
90,"<memory max-size=""400MB"""
90,"when-full=""REMOVE""/>"
90,</distributed-cache>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""name"" : ""default"","
90,"""statistics"" : ""true"","
90,"""caches"" : {"
90,"""mycacheone"" : {"
90,"""distributed-cache"" : {"
90,"""mode"": ""ASYNC"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""expiration"" : {"
90,"""lifespan"" : ""300000"""
90,"""memory"": {"
90,"""max-size"": ""400MB"","
90,"""when-full"": ""REMOVE"""
90,"""mycachetwo"" : {"
90,"""distributed-cache"" : {"
90,"""mode"": ""SYNC"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""expiration"" : {"
90,"""lifespan"" : ""300000"""
90,"""memory"": {"
90,"""max-size"": ""400MB"","
90,"""when-full"": ""REMOVE"""
90,YAML
90,infinispan:
90,cacheContainer:
90,"name: ""default"""
90,"statistics: ""true"""
90,caches:
90,mycacheone:
90,distributedCache:
90,"mode: ""ASYNC"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,expiration:
90,"lifespan: ""300000"""
90,memory:
90,"maxSize: ""400MB"""
90,"whenFull: ""REMOVE"""
90,mycachetwo:
90,distributedCache:
90,"mode: ""SYNC"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,expiration:
90,"lifespan: ""300000"""
90,memory:
90,"maxSize: ""400MB"""
90,"whenFull: ""REMOVE"""
90,Additional resources
90,Infinispan configuration schema reference
90,infinispan-config-14.0.xsd
90,3.2. Adding cache templates
90,The Infinispan schema includes *-cache-configuration elements that you can use to create templates.
90,"You can then create caches on demand, using the same configuration multiple times."
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add the cache configuration with the appropriate *-cache-configuration element or object to the Cache Manager.
90,Save and close your Infinispan configuration.
90,Cache template example
90,XML
90,<infinispan>
90,<cache-container>
90,"<distributed-cache-configuration name=""my-dist-template"""
90,"mode=""SYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<memory max-count=""1000000"""
90,"when-full=""REMOVE""/>"
90,"<expiration lifespan=""5000"""
90,"max-idle=""1000""/>"
90,</distributed-cache-configuration>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""distributed-cache-configuration"" : {"
90,"""name"" : ""my-dist-template"","
90,"""mode"": ""SYNC"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""expiration"" : {"
90,"""lifespan"" : ""5000"","
90,"""max-idle"" : ""1000"""
90,"""memory"": {"
90,"""max-count"": ""1000000"","
90,"""when-full"": ""REMOVE"""
90,YAML
90,infinispan:
90,cacheContainer:
90,distributedCacheConfiguration:
90,"name: ""my-dist-template"""
90,"mode: ""SYNC"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,expiration:
90,"lifespan: ""5000"""
90,"maxIdle: ""1000"""
90,memory:
90,"maxCount: ""1000000"""
90,"whenFull: ""REMOVE"""
90,3.2.1. Creating caches from templates
90,Create caches from configuration templates.
90,Templates for remote caches are available from the Cache templates menu in Infinispan Console.
90,Prerequisites
90,Add at least one cache template to the Cache Manager.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Specify the template from which the cache inherits with the configuration attribute or field.
90,Save and close your Infinispan configuration.
90,Cache configuration inherited from a template
90,XML
90,"<distributed-cache configuration=""my-dist-template"" />"
90,JSON
90,"""distributed-cache"": {"
90,"""configuration"": ""my-dist-template"""
90,YAML
90,distributedCache:
90,"configuration: ""my-dist-template"""
90,3.2.2. Cache template inheritance
90,Cache configuration templates can inherit from other templates to extend and override settings.
90,Cache template inheritance is hierarchical.
90,"For a child configuration template to inherit from a parent, you must include it after the parent template."
90,"Additionally, template inheritance is additive for elements that have multiple values."
90,"A cache that inherits from another template merges the values from that template, which can override properties."
90,Template inheritance example
90,XML
90,<infinispan>
90,<cache-container>
90,"<distributed-cache-configuration name=""base-template"">"
90,"<expiration lifespan=""5000""/>"
90,</distributed-cache-configuration>
90,"<distributed-cache-configuration name=""extended-template"""
90,"configuration=""base-template"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,"<expiration lifespan=""10000"""
90,"max-idle=""1000""/>"
90,</distributed-cache-configuration>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""caches"" : {"
90,"""base-template"" : {"
90,"""distributed-cache-configuration"" : {"
90,"""expiration"" : {"
90,"""lifespan"" : ""5000"""
90,"""extended-template"" : {"
90,"""distributed-cache-configuration"" : {"
90,"""configuration"" : ""base-template"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,"""expiration"" : {"
90,"""lifespan"" : ""10000"","
90,"""max-idle"" : ""1000"""
90,YAML
90,infinispan:
90,cacheContainer:
90,caches:
90,base-template:
90,distributedCacheConfiguration:
90,expiration:
90,"lifespan: ""5000"""
90,extended-template:
90,distributedCacheConfiguration:
90,"configuration: ""base-template"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,expiration:
90,"lifespan: ""10000"""
90,"maxIdle: ""1000"""
90,3.2.3. Cache template wildcards
90,You can add wildcards to cache configuration template names.
90,"If you then create caches where the name matches the wildcard, Infinispan applies the configuration template."
90,Infinispan throws exceptions if cache names match more than one wildcard.
90,Template wildcard example
90,XML
90,<infinispan>
90,<cache-container>
90,"<distributed-cache-configuration name=""async-dist-cache-*"""
90,"mode=""ASYNC"""
90,"statistics=""true"">"
90,"<encoding media-type=""application/x-protostream""/>"
90,</distributed-cache-configuration>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""distributed-cache-configuration"" : {"
90,"""name"" : ""async-dist-cache-*"","
90,"""mode"": ""ASYNC"","
90,"""statistics"": ""true"","
90,"""encoding"": {"
90,"""media-type"": ""application/x-protostream"""
90,YAML
90,infinispan:
90,cacheContainer:
90,distributedCacheConfiguration:
90,"name: ""async-dist-cache-*"""
90,"mode: ""ASYNC"""
90,"statistics: ""true"""
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,"Using the preceding example, if you create a cache named ""async-dist-cache-prod"" then Infinispan uses the configuration from the async-dist-cache-* template."
90,3.2.4. Cache templates from multiple XML files
90,Split cache configuration templates into multiple XML files for granular flexibility and reference them with XML inclusions (XInclude).
90,Infinispan provides minimal support for the XInclude specification.
90,"This means you cannot use the xpointer attribute, the xi:fallback element, text processing, or content negotiation."
90,"You must also add the xmlns:xi=""http://www.w3.org/2001/XInclude"" namespace to infinispan.xml to use XInclude."
90,Xinclude cache template
90,"<infinispan xmlns:xi=""http://www.w3.org/2001/XInclude"">"
90,"<cache-container default-cache=""cache-1"">"
90,<!-- References files that contain cache configuration templates. -->
90,"<xi:include href=""distributed-cache-template.xml"" />"
90,"<xi:include href=""replicated-cache-template.xml"" />"
90,</cache-container>
90,</infinispan>
90,Infinispan also provides an infinispan-config-fragment-14.0.xsd schema that you can use with configuration fragments.
90,Configuration fragment schema
90,"<local-cache xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
90,"xsi:schemaLocation=""urn:infinispan:config:14.0 https://infinispan.org/schemas/infinispan-config-fragment-14.0.xsd"""
90,"xmlns=""urn:infinispan:config:14.0"""
90,"name=""mycache""/>"
90,Additional resources
90,XInclude specification
90,3.3. Creating remote caches
90,"When you create remote caches at runtime, Infinispan Server synchronizes your configuration across the cluster so that all nodes have a copy."
90,For this reason you should always create remote caches dynamically with the following mechanisms:
90,Infinispan Console
90,Infinispan Command Line Interface (CLI)
90,Hot Rod or HTTP clients
90,3.3.1. Default Cache Manager
90,Infinispan Server provides a default Cache Manager that controls the lifecycle of remote caches.
90,Starting Infinispan Server automatically instantiates the Cache Manager so you can create and delete remote caches and other resources like Protobuf schema.
90,"After you start Infinispan Server and add user credentials, you can view details about the Cache Manager and get cluster information from Infinispan Console."
90,Open 127.0.0.1:11222 in any browser.
90,You can also get information about the Cache Manager through the Command Line Interface (CLI) or REST API:
90,CLI
90,Run the describe command in the default container.
90,[//containers/default]> describe
90,REST
90,Open 127.0.0.1:11222/rest/v2/cache-managers/default/ in any browser.
90,Default Cache Manager configuration
90,XML
90,<infinispan>
90,"<!-- Creates a Cache Manager named ""default"" and enables metrics. -->"
90,"<cache-container name=""default"""
90,"statistics=""true"">"
90,<!-- Adds cluster transport that uses the default JGroups TCP stack. -->
90,"<transport cluster=""${infinispan.cluster.name:cluster}"""
90,"stack=""${infinispan.cluster.stack:tcp}"""
90,"node-name=""${infinispan.node.name:}""/>"
90,<!-- Requires user permission to access caches and perform operations. -->
90,<security>
90,<authorization/>
90,</security>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""jgroups"" : {"
90,"""transport"" : ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
90,"""cache-container"" : {"
90,"""name"" : ""default"","
90,"""statistics"" : ""true"","
90,"""transport"" : {"
90,"""cluster"" : ""cluster"","
90,"""node-name"" : """","
90,"""stack"" : ""tcp"""
90,"""security"" : {"
90,"""authorization"" : {}"
90,YAML
90,infinispan:
90,jgroups:
90,"transport: ""org.infinispan.remoting.transport.jgroups.JGroupsTransport"""
90,cacheContainer:
90,"name: ""default"""
90,"statistics: ""true"""
90,transport:
90,"cluster: ""cluster"""
90,"nodeName: """""
90,"stack: ""tcp"""
90,security:
90,authorization: ~
90,3.3.2. Creating caches with Infinispan Console
90,Use Infinispan Console to create remote caches in an intuitive visual interface from any web browser.
90,Prerequisites
90,Create a Infinispan user with admin permissions.
90,Start at least one Infinispan Server instance.
90,Have a Infinispan cache configuration.
90,Procedure
90,Open 127.0.0.1:11222/console/ in any browser.
90,Select Create Cache and follow the steps as Infinispan Console guides you through the process.
90,3.3.3. Creating remote caches with the Infinispan CLI
90,Use the Infinispan Command Line Interface (CLI) to add remote caches on Infinispan Server.
90,Prerequisites
90,Create a Infinispan user with admin permissions.
90,Start at least one Infinispan Server instance.
90,Have a Infinispan cache configuration.
90,Procedure
90,Start the CLI.
90,bin/cli.sh
90,Run the connect command and enter your username and password when prompted.
90,Use the create cache command to create remote caches.
90,"For example, create a cache named ""mycache"" from a file named mycache.xml as follows:"
90,create cache --file=mycache.xml mycache
90,Verification
90,List all remote caches with the ls command.
90,ls caches
90,mycache
90,View cache configuration with the describe command.
90,describe caches/mycache
90,3.3.4. Creating remote caches from Hot Rod clients
90,"Use the Infinispan Hot Rod API to create remote caches on Infinispan Server from Java, C++, .NET/C#, JS clients and more."
90,This procedure shows you how to use Hot Rod Java clients that create remote caches on first access.
90,You can find code examples for other Hot Rod clients in the Infinispan Tutorials.
90,Prerequisites
90,Create a Infinispan user with admin permissions.
90,Start at least one Infinispan Server instance.
90,Have a Infinispan cache configuration.
90,Procedure
90,Invoke the remoteCache() method as part of your the ConfigurationBuilder.
90,Set the configuration or configuration_uri properties in the hotrod-client.properties file on your classpath.
90,ConfigurationBuilder
90,"File file = new File(""path/to/infinispan.xml"")"
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,"builder.remoteCache(""another-cache"")"
90,".configuration(""<distributed-cache name=\""another-cache\""/>"");"
90,"builder.remoteCache(""my.other.cache"")"
90,.configurationURI(file.toURI());
90,hotrod-client.properties
90,"infinispan.client.hotrod.cache.another-cache.configuration=<distributed-cache name=\""another-cache\""/>"
90,infinispan.client.hotrod.cache.[my.other.cache].configuration_uri=file:///path/to/infinispan.xml
90,"If the name of your remote cache contains the . character, you must enclose it in square brackets when using hotrod-client.properties files."
90,Additional resources
90,Hot Rod Client Configuration
90,org.infinispan.client.hotrod.configuration.RemoteCacheConfigurationBuilder
90,3.3.5. Creating remote caches with the REST API
90,Use the Infinispan REST API to create remote caches on Infinispan Server from any suitable HTTP client.
90,Prerequisites
90,Create a Infinispan user with admin permissions.
90,Start at least one Infinispan Server instance.
90,Have a Infinispan cache configuration.
90,Procedure
90,Invoke POST requests to /rest/v2/caches/<cache_name> with cache configuration in the payload.
90,Additional resources
90,Creating and Managing Caches with the REST API
90,3.4. Creating embedded caches
90,Infinispan provides an EmbeddedCacheManager API that lets you control both the Cache Manager and embedded cache lifecycles programmatically.
90,3.4.1. Adding Infinispan to your project
90,Add Infinispan to your project to create embedded caches in your applications.
90,Prerequisites
90,Configure your project to get Infinispan artifacts from the Maven repository.
90,Procedure
90,Add the infinispan-core artifact as a dependency in your pom.xml as
90,follows:
90,<dependencies>
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-core</artifactId>
90,</dependency>
90,</dependencies>
90,3.4.2. Creating and using embedded caches
90,Infinispan provides a GlobalConfigurationBuilder API that controls the Cache Manager and a ConfigurationBuilder API that configures caches.
90,Prerequisites
90,Add the infinispan-core artifact as a dependency in your pom.xml.
90,Procedure
90,Initialize a CacheManager.
90,You must always call the cacheManager.start() method to initialize a CacheManager before you can create caches.
90,Default constructors do this for you but there are overloaded versions of the constructors that do not.
90,Cache Managers are also heavyweight objects and Infinispan recommends instantiating only one instance per JVM.
90,Use the ConfigurationBuilder API to define cache configuration.
90,"Obtain caches with getCache(), createCache(), or getOrCreateCache() methods."
90,Infinispan recommends using the getOrCreateCache() method because it either creates a cache on all nodes or returns an existing cache.
90,If necessary use the PERMANENT flag for caches to survive restarts.
90,Stop the CacheManager by calling the cacheManager.stop() method to release JVM resources and gracefully shutdown any caches.
90,// Set up a clustered Cache Manager.
90,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder();
90,// Initialize the default Cache Manager.
90,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
90,// Create a distributed cache with synchronous replication.
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.clustering().cacheMode(CacheMode.DIST_SYNC);
90,// Obtain a volatile cache.
90,"Cache<String, String> cache = cacheManager.administration().withFlags(CacheContainerAdmin.AdminFlag.VOLATILE).getOrCreateCache(""myCache"", builder.build());"
90,// Stop the Cache Manager.
90,cacheManager.stop();
90,getCache() method
90,"Invoke the getCache(String) method to obtain caches, as follows:"
90,"Cache<String, String> myCache = manager.getCache(""myCache"");"
90,"The preceding operation creates a cache named myCache, if it does not already exist, and returns it."
90,"Using the getCache() method creates the cache only on the node where you invoke the method. In other words, it performs a local operation that must be invoked on each node across the cluster. Typically, applications deployed across multiple nodes obtain caches during initialization to ensure that caches are symmetric and exist on each node."
90,createCache() method
90,Invoke the createCache() method to create caches dynamically across the entire cluster.
90,"Cache<String, String> myCache = manager.administration().createCache(""myCache"", ""myTemplate"");"
90,The preceding operation also automatically creates caches on any nodes that subsequently join the cluster.
90,"Caches that you create with the createCache() method are ephemeral by default. If the entire cluster shuts down, the cache is not automatically created again when it restarts."
90,PERMANENT flag
90,Use the PERMANENT flag to ensure that caches can survive restarts.
90,"Cache<String, String> myCache = manager.administration().withFlags(AdminFlag.PERMANENT).createCache(""myCache"", ""myTemplate"");"
90,"For the PERMANENT flag to take effect, you must enable global state and set a configuration storage provider."
90,"For more information about configuration storage providers, see GlobalStateConfigurationBuilder#configurationStorage()."
90,Additional resources
90,EmbeddedCacheManager
90,EmbeddedCacheManager Configuration
90,org.infinispan.configuration.global.GlobalConfiguration
90,org.infinispan.configuration.cache.ConfigurationBuilder
90,3.4.3. Cache API
90,"Infinispan provides a Cache interface that exposes simple methods for adding, retrieving and removing entries, including atomic mechanisms exposed by the JDK’s ConcurrentMap interface."
90,"Based on the cache mode used, invoking these methods will trigger a number of things to happen, potentially even including replicating an entry to a remote node or looking up an entry from a remote node, or potentially a cache store."
90,"For simple usage, using the Cache API should be no different from using the JDK Map API, and hence migrating from simple in-memory caches based on a Map to Infinispan’s Cache should be trivial."
90,Performance Concerns of Certain Map Methods
90,"Certain methods exposed in Map have certain performance consequences when used with Infinispan, such as"
90,"size() ,"
90,"values() ,"
90,keySet() and
90,entrySet() .
90,"Specific methods on the keySet, values and entrySet are fine for use please see their Javadoc for further details."
90,Attempting to perform these operations globally would have large performance impact as well as become a scalability bottleneck.
90,"As such, these methods should only be used for informational or debugging purposes only."
90,"It should be noted that using certain flags with the withFlags() method can mitigate some of these concerns, please check each method’s documentation for more details."
90,Mortal and Immortal Data
90,"Further to simply storing entries, Infinispan’s cache API allows you to attach mortality information to data."
90,"For example, simply using put(key, value) would create an immortal entry, i.e., an entry that lives in the cache forever, until it is removed (or evicted from memory to prevent running out of memory)."
90,"If, however, you put data in the cache using put(key, value, lifespan, timeunit) , this creates a mortal entry, i.e., an entry that has a fixed lifespan and expires after that lifespan."
90,"In addition to lifespan , Infinispan also supports maxIdle as an additional metric with which to determine expiration."
90,Any combination of lifespans or maxIdles can be used.
90,putForExternalRead operation
90,Infinispan’s Cache class contains a different 'put' operation called putForExternalRead . This operation is particularly useful when Infinispan is used as a temporary cache for data that is persisted elsewhere.
90,"Under heavy read scenarios, contention in the cache should not delay the real transactions at hand, since caching should just be an optimization and not something that gets in the way."
90,"To achieve this, putForExternalRead() acts as a put call that only operates if the key is not present in the cache, and fails fast and silently if another thread is trying to store the same key at the same time. In this particular scenario, caching data is a way to optimise the system and it’s not desirable that a failure in caching affects the on-going transaction, hence why failure is handled differently. putForExternalRead() is considered to be a fast operation because regardless of whether it’s successful or not, it doesn’t wait for any locks, and so returns to the caller promptly."
90,"To understand how to use this operation, let’s look at basic example. Imagine a cache of Person instances, each keyed by a PersonId , whose data originates in a separate data store. The following code shows the most common pattern of using putForExternalRead within the context of this example:"
90,"// Id of the person to look up, provided by the application"
90,PersonId id = ...;
90,// Get a reference to the cache where person instances will be stored
90,"Cache<PersonId, Person> cache = ...;"
90,"// First, check whether the cache contains the person instance"
90,// associated with with the given id
90,Person cachedPerson = cache.get(id);
90,if (cachedPerson == null) {
90,"// The person is not cached yet, so query the data store with the id"
90,Person person = dataStore.lookup(id);
90,// Cache the person along with the id so that future requests can
90,// retrieve it from memory rather than going to the data store
90,"cache.putForExternalRead(id, person);"
90,} else {
90,"// The person was found in the cache, so return it to the application"
90,return cachedPerson;
90,"Note that putForExternalRead should never be used as a mechanism to update the cache with a new Person instance originating from application execution (i.e. from a transaction that modifies a Person’s address). When updating cached values, please use the standard put operation, otherwise the possibility of caching corrupt data is likely."
90,AdvancedCache API
90,"In addition to the simple Cache interface, Infinispan offers an AdvancedCache interface, geared towards extension authors."
90,The AdvancedCache offers the ability to access certain internal components and to apply flags to alter the default behavior of certain cache methods.
90,The following code snippet depicts how an AdvancedCache can be obtained:
90,AdvancedCache advancedCache = cache.getAdvancedCache();
90,Flags
90,Flags are applied to regular cache methods to alter the behavior of certain methods.
90,"For a list of all available flags, and their effects, see the Flag enumeration."
90,Flags are applied using AdvancedCache.withFlags() .
90,"This builder method can be used to apply any number of flags to a cache invocation, for example:"
90,"advancedCache.withFlags(Flag.CACHE_MODE_LOCAL, Flag.SKIP_LOCKING)"
90,.withFlags(Flag.FORCE_SYNCHRONOUS)
90,".put(""hello"", ""world"");"
90,Asynchronous API
90,"In addition to synchronous API methods like Cache.put() , Cache.remove() , etc., Infinispan also has an asynchronous, non-blocking API where you can achieve the same results in a non-blocking fashion."
90,"These methods are named in a similar fashion to their blocking counterparts, with ""Async"" appended.  E.g., Cache.putAsync() , Cache.removeAsync() , etc.  These asynchronous counterparts return a CompletableFuture that contains the actual result of the operation."
90,"For example, in a cache parameterized as Cache<String, String>, Cache.put(String key, String value) returns String while Cache.putAsync(String key, String value) returns CompletableFuture<String>."
90,Why use such an API?
90,Non-blocking APIs are powerful in that they provide all of the guarantees of synchronous communications - with the ability to handle communication failures and exceptions - with the ease of not having to block until a call completes.  This allows you to better harness parallelism in your system.  For example:
90,Set<CompletableFuture<?>> futures = new HashSet<>();
90,"futures.add(cache.putAsync(key1, value1)); // does not block"
90,"futures.add(cache.putAsync(key2, value2)); // does not block"
90,"futures.add(cache.putAsync(key3, value3)); // does not block"
90,// the remote calls for the 3 puts will effectively be executed
90,"// in parallel, particularly useful if running in distributed mode"
90,// and the 3 keys would typically be pushed to 3 different nodes
90,// in the cluster
90,// check that the puts completed successfully
90,for (CompletableFuture<?> f: futures) f.get();
90,Which processes actually happen asynchronously?
90,There are 4 things in Infinispan that can be considered to be on the critical path of a typical write operation.
90,"These are, in order of cost:"
90,network calls
90,marshalling
90,writing to a cache store (optional)
90,locking
90,"Using the async methods will take the network calls and marshalling off the critical path.  For various technical reasons, writing to a cache store and acquiring locks, however, still happens in the caller’s thread."
90,4. Enabling and configuring Infinispan statistics and JMX monitoring
90,Infinispan can provide Cache Manager and cache statistics as well as export JMX MBeans.
90,4.1. Enabling statistics in embedded caches
90,Configure Infinispan to export statistics for the Cache Manager and embedded caches.
90,Procedure
90,Open your Infinispan configuration for editing.
90,"Add the statistics=""true"" attribute or the .statistics(true) method."
90,Save and close your Infinispan configuration.
90,Embedded cache statistics
90,XML
90,<infinispan>
90,"<cache-container statistics=""true"">"
90,"<distributed-cache statistics=""true""/>"
90,"<replicated-cache statistics=""true""/>"
90,</cache-container>
90,</infinispan>
90,GlobalConfigurationBuilder
90,GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder().cacheContainer().statistics(true);
90,DefaultCacheManager cacheManager = new DefaultCacheManager(global.build());
90,Configuration builder = new ConfigurationBuilder();
90,builder.statistics().enable();
90,4.2. Enabling statistics in remote caches
90,Infinispan Server automatically enables statistics for the default Cache Manager.
90,"However, you must explicitly enable statistics for your caches."
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add the statistics attribute or field and specify true as the value.
90,Save and close your Infinispan configuration.
90,Remote cache statistics
90,XML
90,"<distributed-cache statistics=""true"" />"
90,JSON
90,"""distributed-cache"": {"
90,"""statistics"": ""true"""
90,YAML
90,distributedCache:
90,statistics: true
90,4.3. Enabling Hot Rod client statistics
90,Hot Rod Java clients can provide statistics that include remote cache and near-cache hits and misses as well as connection pool usage.
90,Procedure
90,Open your Hot Rod Java client configuration for editing.
90,Set true as the value for the statistics property or invoke the statistics().enable() methods.
90,Export JMX MBeans for your Hot Rod client with the jmx and jmx_domain properties or invoke the jmxEnable() and jmxDomain() methods.
90,Save and close your client configuration.
90,Hot Rod Java client statistics
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.statistics().enable()
90,.jmxEnable()
90,".jmxDomain(""my.domain.org"")"
90,.addServer()
90,".host(""127.0.0.1"")"
90,.port(11222);
90,RemoteCacheManager remoteCacheManager = new RemoteCacheManager(builder.build());
90,hotrod-client.properties
90,infinispan.client.hotrod.statistics = true
90,infinispan.client.hotrod.jmx = true
90,infinispan.client.hotrod.jmx_domain = my.domain.org
90,4.4. Configuring Infinispan metrics
90,Infinispan generates metrics that are compatible with any monitoring system.
90,Gauges provide values such as the average number of nanoseconds for write operations or JVM uptime.
90,"Histograms provide details about operation execution times such as read,"
90,"write, and remove times."
90,"By default, Infinispan generates gauges when you enable statistics but you can also configure it to generate histograms."
90,Infinispan metrics are provided at the vendor scope.
90,Metrics related to the JVM are provided in the base scope.
90,Prerequisites
90,You must add Micrometer Core and Micrometer Registry Prometheus JARs to your classpath to export Infinispan metrics for embedded caches.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add the metrics element or object to the cache container.
90,Enable or disable gauges with the gauges attribute or field.
90,Enable or disable histograms with the histograms attribute or field.
90,Save and close your client configuration.
90,Metrics configuration
90,XML
90,<infinispan>
90,"<cache-container statistics=""true"">"
90,"<metrics gauges=""true"""
90,"histograms=""true"" />"
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""statistics"" : ""true"","
90,"""metrics"" : {"
90,"""gauges"" : ""true"","
90,"""histograms"" : ""true"""
90,YAML
90,infinispan:
90,cacheContainer:
90,"statistics: ""true"""
90,metrics:
90,"gauges: ""true"""
90,"histograms: ""true"""
90,GlobalConfigurationBuilder
90,GlobalConfiguration globalConfig = new GlobalConfigurationBuilder()
90,//Computes and collects statistics for the Cache Manager.
90,.statistics().enable()
90,//Exports collected statistics as gauge and histogram metrics.
90,.metrics().gauges(true).histograms(true)
90,.build();
90,Verification
90,Infinispan Server exposes statistics through the metrics endpoint that you can collect with monitoring tools such as Prometheus.
90,"To verify that statistics are exported to the metrics endpoint, you can do the following:"
90,Prometheus format
90,curl -v http://localhost:11222/metrics \
90,--digest -u username:password
90,OpenMetrics format
90,curl -v http://localhost:11222/metrics \
90,--digest -u username:password \
90,"-H ""Accept: application/openmetrics-text"""
90,Infinispan no longer provides metrics in MicroProfile JSON format.
90,Additional resources
90,Micrometer Prometheus
90,4.5. Registering JMX MBeans
90,Infinispan can register JMX MBeans that you can use to collect statistics and
90,perform administrative operations.
90,You must also enable statistics otherwise Infinispan provides 0 values for all statistic attributes in JMX MBeans.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add the jmx element or object to the cache container and specify true as the value for the enabled attribute or field.
90,"Add the domain attribute or field and specify the domain where JMX MBeans are exposed, if required."
90,Save and close your client configuration.
90,JMX configuration
90,XML
90,<infinispan>
90,"<cache-container statistics=""true"">"
90,"<jmx enabled=""true"""
90,"domain=""example.com""/>"
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""statistics"" : ""true"","
90,"""jmx"" : {"
90,"""enabled"" : ""true"","
90,"""domain"" : ""example.com"""
90,YAML
90,infinispan:
90,cacheContainer:
90,"statistics: ""true"""
90,jmx:
90,"enabled: ""true"""
90,"domain: ""example.com"""
90,GlobalConfigurationBuilder
90,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
90,.jmx().enable()
90,".domain(""org.mydomain"");"
90,4.5.1. Enabling JMX remote ports
90,Provide unique remote JMX ports to expose Infinispan MBeans through connections in JMXServiceURL format.
90,Infinispan Server does not expose JMX remotely via the single port endpoint.
90,If you want to remotely access Infinispan Server via JMX you must enable a remote port.
90,You can enable remote JMX ports using one of the following approaches:
90,Enable remote JMX ports that require authentication to one of the Infinispan Server security realms.
90,Enable remote JMX ports manually using the standard Java management configuration options.
90,Prerequisites
90,"For remote JMX with authentication, define JMX specific user roles using the default security realm."
90,Users must have controlRole with read/write access or the monitorRole with read-only access to access any JMX resources.
90,Procedure
90,Start Infinispan Server with a remote JMX port enabled using one of the following ways:
90,Enable remote JMX through port 9999.
90,bin/server.sh --jmx 9999
90,Using remote JMX with SSL disabled is not intended for production environments.
90,Pass the following system properties to Infinispan Server at startup.
90,bin/server.sh -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
90,Enabling remote JMX with no authentication or SSL is not secure and not recommended in any environment.
90,Disabling authentication and SSL allows unauthorized users to connect to your server and access the data hosted there.
90,Additional resources
90,Creating security realms
90,4.5.2. Infinispan MBeans
90,Infinispan exposes JMX MBeans that represent manageable resources.
90,org.infinispan:type=Cache
90,Attributes and operations available for cache instances.
90,org.infinispan:type=CacheManager
90,"Attributes and operations available for Cache Managers, including Infinispan cache and cluster health statistics."
90,For a complete list of available JMX MBeans along with descriptions and
90,"available operations and attributes, see the Infinispan JMX Components"
90,documentation.
90,Additional resources
90,Infinispan JMX Components
90,4.5.3. Registering MBeans in custom MBean servers
90,Infinispan includes an MBeanServerLookup interface that you can use to
90,register MBeans in custom MBeanServer instances.
90,Prerequisites
90,Create an implementation of MBeanServerLookup so that the getMBeanServer() method returns the custom MBeanServer instance.
90,Configure Infinispan to register JMX MBeans.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add the mbean-server-lookup attribute or field to the JMX configuration for the Cache Manager.
90,Specify fully qualified name (FQN) of your MBeanServerLookup implementation.
90,Save and close your client configuration.
90,JMX MBean server lookup configuration
90,XML
90,<infinispan>
90,"<cache-container statistics=""true"">"
90,"<jmx enabled=""true"""
90,"domain=""example.com"""
90,"mbean-server-lookup=""com.example.MyMBeanServerLookup""/>"
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""statistics"" : ""true"","
90,"""jmx"" : {"
90,"""enabled"" : ""true"","
90,"""domain"" : ""example.com"","
90,"""mbean-server-lookup"" : ""com.example.MyMBeanServerLookup"""
90,YAML
90,infinispan:
90,cacheContainer:
90,"statistics: ""true"""
90,jmx:
90,"enabled: ""true"""
90,"domain: ""example.com"""
90,"mbeanServerLookup: ""com.example.MyMBeanServerLookup"""
90,GlobalConfigurationBuilder
90,GlobalConfiguration global = GlobalConfigurationBuilder.defaultClusteredBuilder()
90,.jmx().enable()
90,".domain(""org.mydomain"")"
90,.mBeanServerLookup(new com.acme.MyMBeanServerLookup());
90,4.6. Exporting metrics during a state transfer operation
90,You can export time metrics for clustered caches that Infinispan redistributes across nodes.
90,"A state transfer operation occurs when a clustered cache topology changes, such as a node joining or leaving a cluster."
90,"During a state transfer operation, Infinispan exports metrics from each cache, so that you can determine a cache’s status."
90,"A state transfer exposes attributes as properties, so that Infinispan can export metrics from each cache."
90,You cannot perform a state transfer operation in invalidation mode.
90,Infinispan generates time metrics that are compatible with the REST API and the JMX API.
90,Prerequisites
90,Configure Infinispan metrics.
90,"Enable metrics for your cache type, such as embedded cache or remote cache."
90,Initiate a state transfer operation by changing your clustered cache topology.
90,Procedure
90,Choose one of the following methods:
90,Configure Infinispan to use the REST API to collect metrics.
90,Configure Infinispan to use the JMX API to collect metrics.
90,Additional resources
90,Enabling and configuring Infinispan statistics and JMX monitoring (Infinispan caches)
90,StateTransferManager (Infinispan 14.0 API)
90,4.7. Monitoring the status of cross-site replication
90,Monitor the site status of your backup locations to detect interruptions in the communication between the sites.
90,"When a remote site status changes to offline, Infinispan stops replicating your data to the backup location."
90,Your data become out of sync and you must fix the inconsistencies before bringing the clusters back online.
90,Monitoring cross-site events is necessary for early problem detection.
90,Use one of the following monitoring strategies:
90,Monitoring cross-site replication with the REST API
90,Monitoring cross-site replication with the Prometheus metrics or any other monitoring system
90,Monitoring cross-site replication with the REST API
90,Monitor the status of cross-site replication for all caches using the REST endpoint.
90,You can implement a custom script to poll the REST endpoint or use the following example.
90,Prerequisites
90,Enable cross-site replication.
90,Procedure
90,Implement a script to poll the REST endpoint.
90,The following example demonstrates how you can use a Python script to poll the site status every five seconds.
90,#!/usr/bin/python3
90,import time
90,import requests
90,from requests.auth import HTTPDigestAuth
90,class InfinispanConnection:
90,"def __init__(self, server: str = 'http://localhost:11222', cache_manager: str = 'default',"
90,"auth: tuple = ('admin', 'change_me')) -> None:"
90,super().__init__()
90,self.__url = f'{server}/rest/v2/cache-managers/{cache_manager}/x-site/backups/'
90,self.__auth = auth
90,self.__headers = {
90,'accept': 'application/json'
90,def get_sites_status(self):
90,try:
90,"rsp = requests.get(self.__url, headers=self.__headers, auth=HTTPDigestAuth(self.__auth[0], self.__auth[1]))"
90,if rsp.status_code != 200:
90,return None
90,return rsp.json()
90,except:
90,return None
90,# Specify credentials for Infinispan user with permission to access the REST endpoint
90,USERNAME = 'admin'
90,PASSWORD = 'change_me'
90,# Set an interval between cross-site status checks
90,POLL_INTERVAL_SEC = 5
90,# Provide a list of servers
90,SERVERS = [
90,"InfinispanConnection('http://127.0.0.1:11222', auth=(USERNAME, PASSWORD)),"
90,"InfinispanConnection('http://127.0.0.1:12222', auth=(USERNAME, PASSWORD))"
90,#Specify the names of remote sites
90,REMOTE_SITES = [
90,'nyc'
90,#Provide a list of caches to monitor
90,CACHES = [
90,"'work',"
90,'sessions'
90,"def on_event(site: str, cache: str, old_status: str, new_status: str):"
90,# TODO implement your handling code here
90,print(f'site={site} cache={cache} Status changed {old_status} -> {new_status}')
90,"def __handle_mixed_state(state: dict, site: str, site_status: dict):"
90,if site not in state:
90,state[site] = {c: 'online' if c in site_status['online'] else 'offline' for c in CACHES}
90,return
90,for cache in CACHES:
90,"__update_cache_state(state, site, cache, 'online' if cache in site_status['online'] else 'offline')"
90,"def __handle_online_or_offline_state(state: dict, site: str, new_status: str):"
90,if site not in state:
90,state[site] = {c: new_status for c in CACHES}
90,return
90,for cache in CACHES:
90,"__update_cache_state(state, site, cache, new_status)"
90,"def __update_cache_state(state: dict, site: str, cache: str, new_status: str):"
90,old_status = state[site].get(cache)
90,if old_status != new_status:
90,"on_event(site, cache, old_status, new_status)"
90,state[site][cache] = new_status
90,def update_state(state: dict):
90,rsp = None
90,for conn in SERVERS:
90,rsp = conn.get_sites_status()
90,if rsp:
90,break
90,if rsp is None:
90,print('Unable to fetch site status from any server')
90,return
90,for site in REMOTE_SITES:
90,"site_status = rsp.get(site, {})"
90,new_status = site_status.get('status')
90,if new_status == 'mixed':
90,"__handle_mixed_state(state, site, site_status)"
90,else:
90,"__handle_online_or_offline_state(state, site, new_status)"
90,if __name__ == '__main__':
90,_state = {}
90,while True:
90,update_state(_state)
90,time.sleep(POLL_INTERVAL_SEC)
90,"When a site status changes from online to offline or vice-versa, the function on_event is invoked."
90,"If you want to use this script, you must specify the following variables:"
90,USERNAME and PASSWORD: The username and password of Infinispan user with permission to access the REST endpoint.
90,POLL_INTERVAL_SEC: The number of seconds between polls.
90,SERVERS: The list of Infinispan Servers at this site.
90,The script only requires a single valid response but the list is provided to allow fail over.
90,REMOTE_SITES: The list of remote sites to monitor on these servers.
90,CACHES: The list of cache names to monitor.
90,Additional resources
90,REST API: Getting status of backup locations
90,Monitoring cross-site replication with the Prometheus metrics
90,"Prometheus, and other monitoring systems, let you configure alerts to detect when a site status changes to offline."
90,Monitoring cross-site latency metrics can help you to discover potential issues.
90,Prerequisites
90,Enable cross-site replication.
90,Procedure
90,Configure Infinispan metrics.
90,Configure alerting rules using the Prometheus metrics format.
90,"For the site status, use 1 for online and 0 for offline."
90,"For the expr filed, use the following format:"
90,vendor_cache_manager_default_cache_<cache name>_x_site_admin_<site name>_status.
90,"In the following example, Prometheus alerts you when the NYC site gets offline for cache named work or sessions."
90,groups:
90,- name: Cross Site Rules
90,rules:
90,- alert: Cache Work and Site NYC
90,expr: vendor_cache_manager_default_cache_work_x_site_admin_nyc_status == 0
90,- alert: Cache Sessions and Site NYC
90,expr: vendor_cache_manager_default_cache_sessions_x_site_admin_nyc_status == 0
90,The following image shows an alert that the NYC site is offline for cache work.
90,Figure 6. Prometheus Alert
90,Additional resources
90,Configuring Infinispan metrics
90,Prometheus Alerting Overview
90,Grafana Alerting Documentation
90,Openshift Managing Alerts
90,5. Configuring JVM memory usage
90,Control how Infinispan stores data in JVM memory by:
90,Managing JVM memory usage with eviction that automatically removes data from caches.
90,Adding lifespan and maximum idle times to expire entries and prevent stale data.
90,"Configuring Infinispan to store data in off-heap, native memory."
90,5.1. Default memory configuration
90,By default Infinispan stores cache entries as objects in the JVM heap.
90,"Over time, as applications add entries, the size of caches can exceed the amount of memory that is available to the JVM."
90,"Likewise, if Infinispan is not the primary data store, then entries become out of date which means your caches contain stale data."
90,XML
90,<distributed-cache>
90,"<memory storage=""HEAP""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""memory"" : {"
90,"""storage"": ""HEAP"""
90,YAML
90,distributedCache:
90,memory:
90,"storage: ""HEAP"""
90,5.2. Eviction and expiration
90,Eviction and expiration are two strategies for cleaning the data container by
90,"removing old, unused entries."
90,"Although eviction and expiration are similar, they have some important differences."
90,Eviction lets Infinispan control the size of the data container by removing entries when the container becomes larger than a configured threshold.
90,Expiration limits the amount of time entries can exist. Infinispan uses
90,a scheduler to periodically remove expired entries. Entries that are expired
90,but not yet removed are immediately removed on access; in this case get()
90,"calls for expired entries return ""null"" values."
90,Eviction is local to Infinispan nodes.
90,Expiration takes place across Infinispan clusters.
90,You can use eviction and expiration together or independently of each other.
90,You can configure eviction and expiration declaratively in infinispan.xml to apply cache-wide defaults for entries.
90,You can explicitly define expiration settings for specific entries but you cannot define eviction on a per-entry basis.
90,You can manually evict entries and manually trigger expiration.
90,5.3. Eviction with Infinispan caches
90,Eviction lets you control the size of the data container by removing entries from memory in one of two ways:
90,Total number of entries (max-count).
90,Maximum amount of memory (max-size).
90,Eviction drops one entry from the data container at a time and is local to the node on which it occurs.
90,Eviction removes entries from memory but not from persistent cache stores.
90,"To ensure that entries remain available after Infinispan evicts them, and to prevent inconsistencies with your data, you should configure persistent storage."
90,"When you configure memory, Infinispan approximates the current memory usage of the data container."
90,"When entries are added or modified, Infinispan compares the current memory usage of the data container to the maximum size."
90,"If the size exceeds the maximum, Infinispan performs eviction."
90,Eviction happens immediately in the thread that adds an entry that exceeds the maximum size.
90,5.3.1. Eviction strategies
90,When you configure Infinispan eviction you specify:
90,The maximum size of the data container.
90,A strategy for removing entries when the cache reaches the threshold.
90,You can either perform eviction manually or configure Infinispan to do one of the following:
90,Remove old entries to make space for new ones.
90,Throw ContainerFullException and prevent new entries from being created.
90,The exception eviction strategy works only with transactional caches that use 2 phase commits; not with 1 phase commits or synchronization optimizations.
90,Refer to the schema reference for more details about the eviction strategies.
90,Infinispan includes the Caffeine caching library that implements a variation
90,of the Least Frequently Used (LFU) cache replacement algorithm known as
90,"TinyLFU. For off-heap storage, Infinispan uses a custom implementation of the"
90,Least Recently Used (LRU) algorithm.
90,Additional resources
90,Caffeine
90,Infinispan configuration schema reference
90,5.3.2. Configuring maximum count eviction
90,Limit the size of Infinispan caches to a total number of entries.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Specify the total number of entries that caches can contain before
90,Infinispan performs eviction with either the max-count attribute or maxCount() method.
90,Set one of the following as the eviction strategy to control how Infinispan removes entries with the when-full attribute or whenFull() method.
90,REMOVE Infinispan performs eviction. This is the default strategy.
90,MANUAL You perform eviction manually for embedded caches.
90,EXCEPTION Infinispan throws an exception instead of evicting entries.
90,Save and close your Infinispan configuration.
90,Maximum count eviction
90,"In the following example, Infinispan removes an entry when the cache contains a total of 500 entries and a new entry is created:"
90,XML
90,<distributed-cache>
90,"<memory max-count=""500"" when-full=""REMOVE""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"" : {"
90,"""memory"" : {"
90,"""max-count"" : ""500"","
90,"""when-full"" : ""REMOVE"""
90,YAML
90,distributedCache:
90,memory:
90,"maxCount: ""500"""
90,"whenFull: ""REMOVE"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.memory().maxCount(500).whenFull(EvictionStrategy.REMOVE);
90,Additional resources
90,Infinispan configuration schema reference
90,org.infinispan.configuration.cache.MemoryConfigurationBuilder
90,5.3.3. Configuring maximum size eviction
90,Limit the size of Infinispan caches to a maximum amount of memory.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Specify application/x-protostream as the media type for cache encoding.
90,You must specify a binary media type to use maximum size eviction.
90,"Configure the maximum amount of memory, in bytes, that caches can use before"
90,Infinispan performs eviction with the max-size attribute or maxSize() method.
90,Optionally specify a byte unit of measurement.
90,The default is B (bytes). Refer to the configuration schema for supported units.
90,Set one of the following as the eviction strategy to control how Infinispan removes entries with either the when-full attribute or whenFull() method.
90,REMOVE Infinispan performs eviction. This is the default strategy.
90,MANUAL You perform eviction manually for embedded caches.
90,EXCEPTION Infinispan throws an exception instead of evicting entries.
90,Save and close your Infinispan configuration.
90,Maximum size eviction
90,"In the following example, Infinispan removes an entry when the size of the cache reaches 1.5 GB (gigabytes) and a new entry is created:"
90,XML
90,<distributed-cache>
90,"<encoding media-type=""application/x-protostream""/>"
90,"<memory max-size=""1.5GB"" when-full=""REMOVE""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"" : {"
90,"""encoding"" : {"
90,"""media-type"" : ""application/x-protostream"""
90,"""memory"" : {"
90,"""max-size"" : ""1.5GB"","
90,"""when-full"" : ""REMOVE"""
90,YAML
90,distributedCache:
90,encoding:
90,"mediaType: ""application/x-protostream"""
90,memory:
90,"maxSize: ""1.5GB"""
90,"whenFull: ""REMOVE"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,"builder.encoding().mediaType(""application/x-protostream"")"
90,.memory()
90,".maxSize(""1.5GB"")"
90,.whenFull(EvictionStrategy.REMOVE);
90,Additional resources
90,Infinispan configuration schema reference
90,org.infinispan.configuration.cache.EncodingConfiguration
90,org.infinispan.configuration.cache.MemoryConfigurationBuilder
90,Cache Encoding and Marshalling
90,5.3.4. Manual eviction
90,"If you choose the manual eviction strategy, Infinispan does not perform eviction."
90,You must do so manually with the evict() method.
90,You should use manual eviction with embedded caches only.
90,"For remote caches, you should always configure Infinispan with the REMOVE or EXCEPTION eviction strategy."
90,This configuration prevents a warning message when you enable passivation but do not configure eviction.
90,XML
90,<distributed-cache>
90,"<memory max-count=""500"" when-full=""MANUAL""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"" : {"
90,"""memory"" : {"
90,"""max-count"" : ""500"","
90,"""when-full"" : ""MANUAL"""
90,YAML
90,distributedCache:
90,memory:
90,"maxCount: ""500"""
90,"whenFull: ""MANUAL"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,"builder.encoding().mediaType(""application/x-protostream"")"
90,.memory()
90,".maxSize(""1.5GB"")"
90,.whenFull(EvictionStrategy.REMOVE);
90,5.3.5. Passivation with eviction
90,Passivation persists data to cache stores when Infinispan evicts entries.
90,"You should always enable eviction if you enable passivation, as in the following examples:"
90,XML
90,<distributed-cache>
90,"<persistence passivation=""true"">"
90,<!-- Persistent storage configuration. -->
90,</persistence>
90,"<memory max-count=""100""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""memory"" : {"
90,"""max-count"" : ""100"""
90,"""persistence"" : {"
90,"""passivation"" : true"
90,YAML
90,distributedCache:
90,memory:
90,"maxCount: ""100"""
90,persistence:
90,"passivation: ""true"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.memory().maxCount(100);
90,builder.persistence().passivation(true); //Persistent storage configuration
90,5.4. Expiration with lifespan and maximum idle
90,Expiration configures Infinispan to remove entries from caches when they reach one of the following time limits:
90,Lifespan
90,Sets the maximum amount of time that entries can exist.
90,Maximum idle
90,Specifies how long entries can remain idle. If operations do not occur for
90,"entries, they become idle."
90,Maximum idle expiration does not currently support caches with persistent storage.
90,"If you use expiration and eviction with the EXCEPTION eviction strategy, entries that are expired, but not yet removed from the cache, count towards the size of the data container."
90,5.4.1. How expiration works
90,"When you configure expiration, Infinispan stores keys with metadata that"
90,determines when entries expire.
90,Lifespan uses a creation timestamp and the value for the lifespan configuration property.
90,Maximum idle uses a last used timestamp and the value for the max-idle configuration property.
90,Infinispan checks if lifespan or maximum idle metadata is set and then
90,compares the values with the current time.
90,If (creation + lifespan < currentTime) or (lastUsed + maxIdle < currentTime) then Infinispan detects that the entry is expired.
90,Expiration occurs whenever entries are accessed or found by the expiration
90,reaper.
90,"For example, k1 reaches the maximum idle time and a client makes a"
90,"Cache.get(k1) request. In this case, Infinispan detects that the entry is"
90,expired and removes it from the data container. The Cache.get(k1) request returns null.
90,"Infinispan also expires entries from cache stores, but only with lifespan"
90,expiration. Maximum idle expiration does not work with cache stores. In the
90,"case of cache loaders, Infinispan cannot expire entries because loaders can"
90,only read from external storage.
90,Infinispan adds expiration metadata as long primitive data types to cache
90,entries. This can increase the size of keys by as much as 32 bytes.
90,5.4.2. Expiration reaper
90,Infinispan uses a reaper thread that runs periodically to detect and remove
90,expired entries. The expiration reaper ensures that expired entries that are no
90,longer accessed are removed.
90,The Infinispan ExpirationManager interface handles the expiration reaper and
90,exposes the processExpiration() method.
90,"In some cases, you can disable the expiration reaper and manually expire"
90,"entries by calling processExpiration(); for instance, if you are using local"
90,cache mode with a custom application where a maintenance thread runs
90,periodically.
90,"If you use clustered cache modes, you should never disable the expiration"
90,reaper.
90,Infinispan always uses the expiration reaper when using cache stores. In this
90,case you cannot disable it.
90,Additional resources
90,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
90,org.infinispan.expiration.ExpirationManager
90,5.4.3. Maximum idle and clustered caches
90,Because maximum idle expiration relies on the last access time for cache
90,"entries, it has some limitations with clustered cache modes."
90,"With lifespan expiration, the creation time for cache entries provides a value"
90,"that is consistent across clustered caches. For example, the creation time for"
90,k1 is always the same on all nodes.
90,"For maximum idle expiration with clustered caches, last access time for entries"
90,is not always the same on all nodes. To ensure that entries have the same
90,"relative access times across clusters, Infinispan sends touch commands to all"
90,owners when keys are accessed.
90,The touch commands that Infinispan send have the following considerations:
90,Cache.get() requests do not return until all touch commands complete. This synchronous behavior increases latency of client requests.
90,"The touch command also updates the ""recently accessed"" metadata for cache entries on all owners, which Infinispan uses for eviction."
90,"With scattered cache mode, Infinispan sends touch commands to all nodes, not just primary and backup owners."
90,Additional information
90,Maximum idle expiration does not work with invalidation mode.
90,Iteration across a clustered cache can return expired entries that have
90,exceeded the maximum idle time limit. This behavior ensures performance because
90,no remote invocations are performed during the iteration. Also note that
90,iteration does not refresh any expired entries.
90,5.4.4. Configuring lifespan and maximum idle times for caches
90,Set lifespan and maximum idle times for all entries in a cache.
90,Procedure
90,Open your Infinispan configuration for editing.
90,"Specify the amount of time, in milliseconds, that entries can stay in the cache with the lifespan attribute or lifespan() method."
90,"Specify the amount of time, in milliseconds, that entries can remain idle after last access with the max-idle attribute or maxIdle() method."
90,Save and close your Infinispan configuration.
90,Expiration for Infinispan caches
90,"In the following example, Infinispan expires all cache entries after 5 seconds or 1 second after the last access time, whichever happens first:"
90,XML
90,<replicated-cache>
90,"<expiration lifespan=""5000"" max-idle=""1000"" />"
90,</replicated-cache>
90,JSON
90,"""replicated-cache"" : {"
90,"""expiration"" : {"
90,"""lifespan"" : ""5000"","
90,"""max-idle"" : ""1000"""
90,YAML
90,replicatedCache:
90,expiration:
90,"lifespan: ""5000"""
90,"maxIdle: ""1000"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,"builder.expiration().lifespan(5000, TimeUnit.MILLISECONDS)"
90,".maxIdle(1000, TimeUnit.MILLISECONDS);"
90,5.4.5. Configuring lifespan and maximum idle times per entry
90,Specify lifespan and maximum idle times for individual entries.
90,"When you add lifespan and maximum idle times to entries, those values take priority over expiration configuration for caches."
90,"When you explicitly define lifespan and maximum idle time values for cache entries, Infinispan replicates those values across the cluster along with the cache entries."
90,"Likewise, Infinispan writes expiration values along with the entries to persistent storage."
90,Procedure
90,"For remote caches, you can add lifespan and maximum idle times to entries interactively with the Infinispan Console."
90,"With the Infinispan Command Line Interface (CLI), use the --max-idle= and --ttl= arguments with the put command."
90,"For both remote and embedded caches, you can add lifespan and maximum idle times with cache.put() invocations."
90,//Lifespan of 5 seconds.
90,//Maximum idle time of 1 second.
90,"cache.put(""hello"", ""world"", 5, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
90,//Lifespan is disabled with a value of -1.
90,//Maximum idle time of 1 second.
90,"cache.put(""hello"", ""world"", -1, TimeUnit.SECONDS, 1, TimeUnit.SECONDS);"
90,Additional resources
90,org.infinispan.configuration.cache.ExpirationConfigurationBuilder
90,org.infinispan.expiration.ExpirationManager
90,5.5. JVM heap and off-heap memory
90,Infinispan stores cache entries in JVM heap memory by default.
90,"You can configure Infinispan to use off-heap storage, which means that your data occupies native memory outside the managed JVM memory space."
90,The following diagram is a simplified illustration of the memory space for a JVM process where Infinispan is running:
90,Figure 7. JVM memory space
90,JVM heap memory
90,The heap is divided into young and old generations that help keep referenced Java objects and other application data in memory.
90,"The GC process reclaims space from unreachable objects, running more frequently on the young generation memory pool."
90,"When Infinispan stores cache entries in JVM heap memory, GC runs can take longer to complete as you start adding data to your caches."
90,"Because GC is an intensive process, longer and more frequent runs can degrade application performance."
90,Off-heap memory
90,Off-heap memory is native available system memory outside JVM memory management.
90,The JVM memory space diagram shows the Metaspace memory pool that holds class metadata and is allocated from native memory.
90,The diagram also represents a section of native memory that holds Infinispan cache entries.
90,Off-heap memory:
90,Uses less memory per entry.
90,Improves overall JVM performance by avoiding Garbage Collector (GC) runs.
90,"One disadvantage, however, is that JVM heap dumps do not show entries stored in off-heap memory."
90,5.5.1. Off-heap data storage
90,"When you add entries to off-heap caches, Infinispan dynamically allocates native memory to your data."
90,Infinispan hashes the serialized byte[] for each key into buckets that are similar to a standard Java HashMap.
90,Buckets include address pointers that Infinispan uses to locate entries that you store in off-heap memory.
90,"Even though Infinispan stores cache entries in native memory, run-time operations require JVM heap representations of those objects."
90,"For instance, cache.get() operations read objects into heap memory before returning."
90,"Likewise, state transfer operations hold subsets of objects in heap memory while they take place."
90,Object equality
90,Infinispan determines equality of Java objects in off-heap storage using the serialized byte[] representation of each object instead of the object instance.
90,Data consistency
90,Infinispan uses an array of locks to protect off-heap address spaces.
90,The number of locks is twice the number of cores and then rounded to the nearest power of two.
90,This ensures that there is an even distribution of ReadWriteLock instances to prevent write operations from blocking read operations.
90,5.5.2. Configuring off-heap memory
90,Configure Infinispan to store cache entries in native memory outside the JVM
90,heap space.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Set OFF_HEAP as the value for the storage attribute or storage() method.
90,Set a boundary for the size of the cache by configuring eviction.
90,Save and close your Infinispan configuration.
90,Off-heap storage
90,Infinispan stores cache entries as bytes in native memory.
90,Eviction happens when there are 100 entries in the data container and Infinispan gets a request to create a new entry:
90,XML
90,<replicated-cache>
90,"<memory storage=""OFF_HEAP"" max-count=""500""/>"
90,</replicated-cache>
90,JSON
90,"""replicated-cache"" : {"
90,"""memory"" : {"
90,"""storage"" : ""OFF_HEAP"","
90,"""max-count"" : ""500"""
90,YAML
90,replicatedCache:
90,memory:
90,"storage: ""OFF_HEAP"""
90,"maxCount: ""500"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.memory().storage(StorageType.OFF_HEAP).maxCount(500);
90,Additional resources
90,Infinispan configuration schema reference
90,org.infinispan.configuration.cache.MemoryConfigurationBuilder
90,6. Configuring persistent storage
90,Infinispan uses cache stores and loaders to interact with persistent storage.
90,Durability
90,Adding cache stores allows you to persist data to non-volatile storage so it
90,survives restarts.
90,Write-through caching
90,Configuring Infinispan as a caching layer in front of persistent storage
90,simplifies data access for applications because Infinispan handles all
90,interactions with the external storage.
90,Data overflow
90,Using eviction and passivation techniques ensures that Infinispan keeps only
90,frequently used data in-memory and writes older entries to persistent storage.
90,6.1. Passivation
90,Passivation configures Infinispan to write entries to cache stores when it
90,"evicts those entries from memory. In this way, passivation prevents unnecessary"
90,and potentially expensive writes to persistent storage.
90,Activation is the process of restoring entries to memory from the cache store
90,"when there is an attempt to access passivated entries. For this reason, when you"
90,"enable passivation, you must configure cache stores that implement both"
90,CacheWriter and CacheLoader interfaces so they can write and load entries
90,from persistent storage.
90,"When Infinispan evicts an entry from the cache, it notifies cache listeners"
90,that the entry is passivated then stores the entry in the cache store. When
90,"Infinispan gets an access request for an evicted entry, it lazily loads the"
90,entry from the cache store into memory and then notifies cache listeners that
90,the entry is activated while keeping the value still in the store.
90,Passivation uses the first cache loader in the Infinispan configuration and
90,ignores all others.
90,Passivation is not supported with:
90,Transactional stores. Passivation writes and removes entries from the store
90,outside the scope of the actual Infinispan commit boundaries.
90,Shared stores. Shared cache stores require entries to always exist in the
90,"store for other owners. For this reason, passivation is not supported because"
90,entries cannot be removed.
90,"If you enable passivation with transactional stores or shared stores,"
90,Infinispan throws an exception.
90,6.1.1. How passivation works
90,Passivation disabled
90,Writes to data in memory result in writes to persistent storage.
90,"If Infinispan evicts data from memory, then data in persistent storage"
90,includes entries that are evicted from memory. In this way persistent storage
90,is a superset of the in-memory cache.
90,This is recommended when you require highest consistency as the store will be able to be read again after a crash.
90,"If you do not configure eviction, then data in persistent storage provides a"
90,copy of data in memory.
90,Passivation enabled
90,Infinispan adds data to persistent storage only when it evicts data from
90,"memory, an entry is removed or upon shutting down the node."
90,"When Infinispan activates entries, it restores data in memory but keeps the data in the store still."
90,"This allows for writes to be just as fast as without a store, and still maintains consistency."
90,When an entry is created or updated only the in memory will be updated and thus
90,the store will be outdated for the time being.
90,Passivation is not supported when a store is also configured as shared.
90,This is due to entries can become out of sync between nodes depending on when a write is evicted versus read.
90,To gurantee data consistency any store that is not shared should always have purgeOnStartup enabled.
90,This is true for both passivation enabled or disabled since a store could hold an outdated entry while down and resurrect it at a later point.
90,The following table shows data in memory and in persistent storage after a
90,series of operations:
90,Operation
90,Passivation disabled
90,Passivation enabled
90,Insert k1.
90,Memory: k1
90,Disk: k1
90,Memory: k1
90,Disk: -
90,Insert k2.
90,"Memory: k1, k2"
90,"Disk: k1, k2"
90,"Memory: k1, k2"
90,Disk: -
90,Eviction thread runs and evicts k1.
90,Memory: k2
90,"Disk: k1, k2"
90,Memory: k2
90,Disk: k1
90,Read k1.
90,"Memory: k1, k2"
90,"Disk: k1, k2"
90,"Memory: k1, k2"
90,Disk: k1
90,Eviction thread runs and evicts k2.
90,Memory: k1
90,"Disk: k1, k2"
90,Memory: k1
90,"Disk: k1, k2"
90,Remove k2.
90,Memory: k1
90,Disk: k1
90,Memory: k1
90,Disk: k1
90,6.2. Write-through cache stores
90,Write-through is a cache writing mode where writes to memory and writes to
90,"cache stores are synchronous. When a client application updates a cache entry,"
90,"in most cases by invoking Cache.put(), Infinispan does not return the call"
90,until it updates the cache store. This cache writing mode results in updates to
90,the cache store concluding within the boundaries of the client thread.
90,The primary advantage of write-through mode is that the cache and cache store
90,"are updated simultaneously, which ensures that the cache store is always"
90,consistent with the cache.
90,"However, write-through mode can potentially decrease performance because the"
90,need to access and update cache stores directly adds latency to cache
90,operations.
90,Write-through configuration
90,Infinispan uses write-through mode unless you explicitly add write-behind configuration to your caches.
90,There is no separate element or method for configuring write-through mode.
90,"For example, the following configuration adds a file-based store to the cache that implicitly uses write-through mode:"
90,<distributed-cache>
90,"<persistence passivation=""false"">"
90,<file-store>
90,"<index path=""path/to/index"" />"
90,"<data path=""path/to/data"" />"
90,</file-store>
90,</persistence>
90,</distributed-cache>
90,6.3. Write-behind cache stores
90,Write-behind is a cache writing mode where writes to memory are synchronous
90,and writes to cache stores are asynchronous.
90,"When clients send write requests, Infinispan adds those operations to a"
90,modification queue. Infinispan processes operations as they join the queue so
90,that the calling thread is not blocked and the operation completes immediately.
90,If the number of write operations in the modification queue increases beyond
90,"the size of the queue, Infinispan adds those additional operations to the"
90,"queue. However, those operations do not complete until Infinispan processes"
90,operations that are already in the queue.
90,"For example, calling Cache.putAsync returns immediately and the Stage also"
90,completes immediately if the modification queue is not full. If the
90,"modification queue is full, or if Infinispan is currently processing a batch"
90,"of write operations, then Cache.putAsync returns immediately and the Stage"
90,completes later.
90,Write-behind mode provides a performance advantage over write-through mode
90,because cache operations do not need to wait for updates to the underlying cache
90,"store to complete. However, data in the cache store remains inconsistent with"
90,"data in the cache until the modification queue is processed. For this reason,"
90,"write-behind mode is suitable for cache stores with low latency, such as"
90,"unshared and local file-based cache stores, where the time between the"
90,write to the cache and the write to the cache store is as small as possible.
90,Write-behind configuration
90,XML
90,<distributed-cache>
90,<persistence>
90,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
90,"dialect=""H2"""
90,"shared=""true"""
90,"table-name=""books"">"
90,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
90,"username=""sa"""
90,"password=""changeme"""
90,"driver=""org.h2.Driver""/>"
90,"<write-behind modification-queue-size=""2048"""
90,"fail-silently=""true""/>"
90,</table-jdbc-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"" : {"
90,"""table-jdbc-store"": {"
90,"""dialect"": ""H2"","
90,"""shared"": ""true"","
90,"""table-name"": ""books"","
90,"""connection-pool"": {"
90,"""connection-url"": ""jdbc:h2:mem:infinispan"","
90,"""driver"": ""org.h2.Driver"","
90,"""username"": ""sa"","
90,"""password"": ""changeme"""
90,"""write-behind"" : {"
90,"""modification-queue-size"" : ""2048"","
90,"""fail-silently"" : true"
90,YAML
90,distributedCache:
90,persistence:
90,tableJdbcStore:
90,"dialect: ""H2"""
90,"shared: ""true"""
90,"tableName: ""books"""
90,connectionPool:
90,"connectionUrl: ""jdbc:h2:mem:infinispan"""
90,"driver: ""org.h2.Driver"""
90,"username: ""sa"""
90,"password: ""changeme"""
90,writeBehind:
90,"modificationQueueSize: ""2048"""
90,"failSilently: ""true"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence()
90,.async()
90,.modificationQueueSize(2048)
90,.failSilently(true);
90,Failing silently
90,Write-behind configuration includes a fail-silently parameter that controls what happens when either the cache store is unavailable or the modification queue is full.
90,"If fail-silently=""true"" then Infinispan logs WARN messages and rejects"
90,write operations.
90,"If fail-silently=""false"" then Infinispan throws exceptions if it detects"
90,the cache store is unavailable during a write operation. Likewise if the
90,"modification queue becomes full, Infinispan throws an exception."
90,"In some cases, data loss can occur if Infinispan restarts and write operations"
90,"exist in the modification queue. For example the cache store goes offline but,"
90,"during the time it takes to detect that the cache store is unavailable, write"
90,operations are added to the modification queue because it is not full. If
90,Infinispan restarts or otherwise becomes unavailable before the cache store
90,"comes back online, then the write operations in the modification queue are lost"
90,because they were not persisted.
90,6.4. Segmented cache stores
90,Cache stores can organize data into hash space segments to which keys map.
90,"Segmented stores increase read performance for bulk operations; for example,"
90,"streaming over data (Cache.size, Cache.entrySet.stream), pre-loading the"
90,"cache, and doing state transfer operations."
90,"However, segmented stores can also result in loss of performance for write"
90,operations. This performance loss applies particularly to batch write
90,operations that can take place with transactions or write-behind stores. For
90,"this reason, you should evaluate the overhead for write operations before you"
90,enable segmented stores. The performance gain for bulk read operations might
90,not be acceptable if there is a significant performance loss for write
90,operations.
90,The number of segments you configure for cache stores must match the number of
90,segments you define in the Infinispan configuration with the
90,clustering.hash.numSegments parameter.
90,If you change the numSegments parameter in the configuration after you add a
90,"segmented cache store, Infinispan cannot read data from that cache store."
90,6.5. Shared cache stores
90,Infinispan cache stores can be local to a given node or shared across all nodes in the cluster.
90,"By default, cache stores are local (shared=""false"")."
90,"Local cache stores are unique to each node; for example, a file-based cache store that persists data to the host filesystem."
90,"Local cache stores should use ""purge on startup"" to avoid loading stale entries from persistent storage."
90,"Shared cache stores allow multiple nodes to use the same persistent storage; for example, a JDBC cache store that allows multiple nodes to access the same database."
90,"Shared cache stores ensure that only the primary owner write to persistent storage, instead of backup nodes performing write operations for every modification."
90,"Purging deletes data, which is not typically the desired behavior for persistent storage."
90,Local cache store
90,<persistence>
90,"<store shared=""false"""
90,"purge=""true""/>"
90,</persistence>
90,Shared cache store
90,<persistence>
90,"<store shared=""true"""
90,"purge=""false""/>"
90,</persistence>
90,Additional resources
90,Infinispan Configuration Schema
90,6.6. Transactions with persistent cache stores
90,Infinispan supports transactional operations with JDBC-based cache stores only.
90,"To configure caches as transactional, you set transactional=true to keep data in persistent storage synchronized with data in memory."
90,"For all other cache stores, Infinispan does not enlist cache loaders in transactional operations."
90,This can result in data inconsistency if transactions succeed in modifying data in memory but do not completely apply changes to data in the cache store.
90,In these cases manual recovery is not possible with cache stores.
90,6.7. Global persistent location
90,Infinispan preserves global state so that it can restore cluster topology and cached data after restart.
90,Remote caches
90,Infinispan Server saves cluster state to the $ISPN_HOME/server/data directory.
90,You should never delete or modify the server/data directory or its content.
90,Infinispan restores cluster state from this directory when you restart your server instances.
90,Changing the default configuration or directly modifying the server/data directory can cause unexpected behavior and lead to data loss.
90,Embedded caches
90,Infinispan defaults to the user.dir system property as the global persistent location.
90,In most cases this is the directory where your application starts.
90,"For clustered embedded caches, such as replicated or distributed, you should always enable and configure a global persistent location to restore cluster topology."
90,You should never configure an absolute path for a file-based cache store that is outside the global persistent location.
90,"If you do, Infinispan writes the following exception to logs:"
90,"ISPN000558: ""The store location 'foo' is not a child of the global persistent location 'bar'"""
90,6.7.1. Configuring the global persistent location
90,Enable and configure the location where Infinispan stores global state for clustered embedded caches.
90,Infinispan Server enables global persistence and configures a default location.
90,You should not disable global persistence or change the default configuration for remote caches.
90,Prerequisites
90,Add Infinispan to your project.
90,Procedure
90,Enable global state in one of the following ways:
90,Add the global-state element to your Infinispan configuration.
90,Call the globalState().enable() methods in the GlobalConfigurationBuilder API.
90,Define whether the global persistent location is unique to each node or shared between the cluster.
90,Location type
90,Configuration
90,Unique to each node
90,persistent-location element or persistentLocation() method
90,Shared between the cluster
90,shared-persistent-location element or sharedPersistentLocation(String) method
90,Set the path where Infinispan stores cluster state.
90,"For example, file-based cache stores the path is a directory on the host filesystem."
90,Values can be:
90,Absolute and contain the full location including the root.
90,Relative to a root location.
90,"If you specify a relative value for the path, you must also specify a system property that resolves to a root location."
90,"For example, on a Linux host system you set global/state as the path."
90,You also set the my.data property that resolves to the /opt/data root location.
90,In this case Infinispan uses /opt/data/global/state as the global persistent location.
90,Global persistent location configuration
90,XML
90,<infinispan>
90,<cache-container>
90,<global-state>
90,"<persistent-location path=""global/state"" relative-to=""my.data""/>"
90,</global-state>
90,</cache-container>
90,</infinispan>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""global-state"": {"
90,"""persistent-location"" : {"
90,"""path"" : ""global/state"","
90,"""relative-to"" : ""my.data"""
90,YAML
90,cacheContainer:
90,globalState:
90,persistentLocation:
90,"path: ""global/state"""
90,"relativeTo : ""my.data"""
90,GlobalConfigurationBuilder
90,new GlobalConfigurationBuilder().globalState()
90,.enable()
90,".persistentLocation(""global/state"", ""my.data"");"
90,Additional resources
90,Infinispan configuration schema
90,org.infinispan.configuration.global.GlobalStateConfiguration
90,6.8. File-based cache stores
90,File-based cache stores provide persistent storage on the local host filesystem where Infinispan is running.
90,"For clustered caches, file-based cache stores are unique to each Infinispan node."
90,"Never use filesystem-based cache stores on shared file systems, such as an NFS or Samba share, because they do not provide file locking capabilities and data corruption can occur."
90,"Additionally if you attempt to use transactional caches with shared file systems, unrecoverable failures can happen when writing to files during the commit phase."
90,Soft-Index File Stores
90,SoftIndexFileStore is the default implementation for file-based cache stores and stores data in a set of append-only files.
90,When append-only files:
90,"Reach their maximum size, Infinispan creates a new file and starts writing to it."
90,"Reach the compaction threshold of less than 50% usage, Infinispan overwrites the entries to a new file and then deletes the old file."
90,SoftIndexFileStore should use purge on startup to ensure stale entries are not resurrected.
90,B+ trees
90,"To improve performance, append-only files in a SoftIndexFileStore are indexed using a B+ Tree that can be stored both on disk and in memory."
90,The in-memory index uses Java soft references to ensure it can be rebuilt if removed by Garbage Collection (GC) then requested again.
90,"Because SoftIndexFileStore uses Java soft references to keep indexes in memory, it helps prevent out-of-memory exceptions."
90,GC removes indexes before they consume too much memory while still falling back to disk.
90,SoftIndexFileStore creates a B+ tree per configured cache segment.
90,"This provides an additional ""index"" as it only has so many elements and provides additional parallelism for index updates."
90,Currently we allow for a parallel amount based on one sixteenth of the number of cache segments.
90,Each entry in the B+ tree is a node.
90,"By default, the size of each node is limited to 4096 bytes."
90,SoftIndexFileStore throws an exception if keys are longer after serialization occurs.
90,File limits
90,SoftIndexFileStore will use two plus the configured openFilesLimit amount of files at a given time.
90,The two additional file pointers are reserved for the log appender for newly updated data and another
90,for the compactor which writes compacted entries into a new file.
90,The amount of open allocated files allocated for indexing is one tenth of the total number of the configured openFilesLimit.
90,This number has a minimum of 1 or the number of cache segments.
90,Any number remaning from configured limit is allocated for open data files themselves.
90,Segmentation
90,Soft-index file stores are always segmented. The append log(s) are not directly segmented and segmentation is handled directly by the index.
90,Expiration
90,The SoftIndexFileStore has full support for expired entries and their requirements.
90,Single File Cache Stores
90,Single file cache stores are now deprecated and planned for removal.
90,"Single File cache stores, SingleFileStore, persist data to file."
90,Infinispan also maintains an in-memory index of keys while keys and values are stored in the file.
90,"Because SingleFileStore keeps an in-memory index of keys and the location of values, it requires additional memory, depending on the key size and the number of keys."
90,"For this reason, SingleFileStore is not recommended for use cases where the keys are larger or there can be a larger number of them."
90,"In some cases, SingleFileStore can also become fragmented."
90,"If the size of values continually increases, available space in the single file is not used but the entry is appended to the end of the file."
90,Available space in the file is used only if an entry can fit within it.
90,"Likewise, if you remove all entries from memory, the single file store does not decrease in size or become defragmented."
90,Segmentation
90,"Single file cache stores are segmented by default with a separate instance per segment, which results in multiple directories."
90,Each directory is a number that represents the segment to which the data maps.
90,6.8.1. Configuring file-based cache stores
90,Add file-based cache stores to Infinispan to persist data on the host filesystem.
90,Prerequisites
90,Enable global state and configure a global persistent location if you are configuring embedded caches.
90,Procedure
90,Add the persistence element to your cache configuration.
90,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
90,Include the file-store element and configure attributes as appropriate.
90,Specify false as the value for the shared attribute.
90,File-based cache stores should always be unique to each Infinispan instance.
90,"If you want to use the same persistent across a cluster, configure shared storage such as a JDBC string-based cache store ."
90,Configure the index and data elements to specify the location where Infinispan creates indexes and stores data.
90,Include the write-behind element if you want to configure the cache store with write-behind mode.
90,File-based cache store configuration
90,XML
90,<distributed-cache>
90,"<persistence passivation=""true"">"
90,"<file-store shared=""false"">"
90,"<data path=""data""/>"
90,"<index path=""index""/>"
90,"<write-behind modification-queue-size=""2048"" />"
90,</file-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""passivation"": true,"
90,"""file-store"" : {"
90,"""shared"": false,"
90,"""data"": {"
90,"""path"": ""data"""
90,"""index"": {"
90,"""path"": ""index"""
90,"""write-behind"": {"
90,"""modification-queue-size"": ""2048"""
90,YAML
90,distributedCache:
90,persistence:
90,"passivation: ""true"""
90,fileStore:
90,"shared: ""false"""
90,data:
90,"path: ""data"""
90,index:
90,"path: ""index"""
90,writeBehind:
90,"modificationQueueSize: ""2048"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence().passivation(true)
90,.addSoftIndexFileStore()
90,.shared(false)
90,".dataLocation(""data"")"
90,".indexLocation(""index"")"
90,.modificationQueueSize(2048);
90,6.8.2. Configuring single file cache stores
90,"If required, you can configure Infinispan to create single file stores."
90,Single file stores are deprecated.
90,You should use soft-index file stores for better performance and data consistency in comparison with single file stores.
90,Prerequisites
90,Enable global state and configure a global persistent location if you are configuring embedded caches.
90,Procedure
90,Add the persistence element to your cache configuration.
90,Optionally specify true as the value for the passivation attribute to write to the file-based cache store only when data is evicted from memory.
90,Include the single-file-store element.
90,Specify false as the value for the shared attribute.
90,Configure any other attributes as appropriate.
90,Include the write-behind element to configure the cache store as write behind instead of as write through.
90,Single file cache store configuration
90,XML
90,<distributed-cache>
90,"<persistence passivation=""true"">"
90,"<single-file-store shared=""false"""
90,"preload=""true""/>"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"" : {"
90,"""passivation"" : true,"
90,"""single-file-store"" : {"
90,"""shared"" : false,"
90,"""preload"" : true"
90,YAML
90,distributedCache:
90,persistence:
90,"passivation: ""true"""
90,singleFileStore:
90,"shared: ""false"""
90,"preload: ""true"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence().passivation(true)
90,.addStore(SingleFileStoreConfigurationBuilder.class)
90,.shared(false)
90,.preload(true);
90,6.9. JDBC connection factories
90,Infinispan provides different ConnectionFactory implementations that allow you to connect to databases.
90,You use JDBC connections with SQL cache stores and JDBC string-based caches stores.
90,Connection pools
90,Connection pools are suitable for standalone Infinispan deployments and are based on Agroal.
90,XML
90,<distributed-cache>
90,<persistence>
90,"<connection-pool connection-url=""jdbc:h2:mem:infinispan;DB_CLOSE_DELAY=-1"""
90,"username=""sa"""
90,"password=""changeme"""
90,"driver=""org.h2.Driver""/>"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""connection-pool"": {"
90,"""connection-url"": ""jdbc:h2:mem:infinispan_string_based"","
90,"""driver"": ""org.h2.Driver"","
90,"""username"": ""sa"","
90,"""password"": ""changeme"""
90,YAML
90,distributedCache:
90,persistence:
90,connectionPool:
90,"connectionUrl: ""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"""
90,driver: org.h2.Driver
90,username: sa
90,password: changeme
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence()
90,.connectionPool()
90,".connectionUrl(""jdbc:h2:mem:infinispan_string_based;DB_CLOSE_DELAY=-1"")"
90,".username(""sa"")"
90,".driverClass(""org.h2.Driver"");"
90,Managed datasources
90,Datasource connections are suitable for managed environments such as application servers.
90,XML
90,<distributed-cache>
90,<persistence>
90,"<data-source jndi-url=""java:/StringStoreWithManagedConnectionTest/DS"" />"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""data-source"": {"
90,"""jndi-url"": ""java:/StringStoreWithManagedConnectionTest/DS"""
90,YAML
90,distributedCache:
90,persistence:
90,dataSource:
90,"jndiUrl: ""java:/StringStoreWithManagedConnectionTest/DS"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence()
90,.dataSource()
90,".jndiUrl(""java:/StringStoreWithManagedConnectionTest/DS"");"
90,Simple connections
90,Simple connection factories create database connections on a per invocation basis and are intended for use with test or development environments only.
90,XML
90,<distributed-cache>
90,<persistence>
90,"<simple-connection connection-url=""jdbc:h2://localhost"""
90,"username=""sa"""
90,"password=""changeme"""
90,"driver=""org.h2.Driver""/>"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""simple-connection"": {"
90,"""connection-url"": ""jdbc:h2://localhost"","
90,"""driver"": ""org.h2.Driver"","
90,"""username"": ""sa"","
90,"""password"": ""changeme"""
90,YAML
90,distributedCache:
90,persistence:
90,simpleConnection:
90,"connectionUrl: ""jdbc:h2://localhost"""
90,driver: org.h2.Driver
90,username: sa
90,password: changeme
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence()
90,.simpleConnection()
90,".connectionUrl(""jdbc:h2://localhost"")"
90,".driverClass(""org.h2.Driver"")"
90,".username(""admin"")"
90,".password(""changeme"");"
90,Additional resources
90,PooledConnectionFactoryConfigurationBuilder
90,ManagedConnectionFactoryConfigurationBuilder
90,SimpleConnectionFactoryConfigurationBuilder
90,6.9.1. Configuring managed datasources
90,Create managed datasources as part of your Infinispan Server configuration to optimize connection pooling and performance for JDBC database connections.
90,"You can then specify the JDNI name of the managed datasources in your caches, which centralizes JDBC connection configuration for your deployment."
90,Prerequisites
90,Copy database drivers to the server/lib directory in your Infinispan Server installation.
90,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
90,install org.postgresql:postgresql:42.4.4
90,Procedure
90,Open your Infinispan Server configuration for editing.
90,Add a new data-source to the data-sources section.
90,Uniquely identify the datasource with the name attribute or field.
90,Specify a JNDI name for the datasource with the jndi-name attribute or field.
90,You use the JNDI name to specify the datasource in your JDBC cache store
90,configuration.
90,Set true as the value of the statistics attribute or field to enable statistics for the datasource through the /metrics endpoint.
90,Provide JDBC driver details that define how to connect to the datasource in the connection-factory section.
90,Specify the name of the database driver with the driver attribute or field.
90,Specify the JDBC connection url with the url attribute or field.
90,Specify credentials with the username and password attributes or fields.
90,Provide any other configuration as appropriate.
90,Define how Infinispan Server nodes pool and reuse connections with connection pool tuning properties in the connection-pool section.
90,Save the changes to your configuration.
90,Verification
90,"Use the Infinispan Command Line Interface (CLI) to test the datasource connection, as follows:"
90,Start a CLI session.
90,bin/cli.sh
90,List all datasources and confirm the one you created is available.
90,server datasource ls
90,Test a datasource connection.
90,server datasource test my-datasource
90,Managed datasource configuration
90,XML
90,"<server xmlns=""urn:infinispan:server:14.0"">"
90,<data-sources>
90,<!-- Defines a unique name for the datasource and JNDI name that you
90,reference in JDBC cache store configuration.
90,"Enables statistics for the datasource, if required. -->"
90,"<data-source name=""ds"""
90,"jndi-name=""jdbc/postgres"""
90,"statistics=""true"">"
90,<!-- Specifies the JDBC driver that creates connections. -->
90,"<connection-factory driver=""org.postgresql.Driver"""
90,"url=""jdbc:postgresql://localhost:5432/postgres"""
90,"username=""postgres"""
90,"password=""changeme"">"
90,<!-- Sets optional JDBC driver-specific connection properties. -->
90,"<connection-property name=""name"">value</connection-property>"
90,</connection-factory>
90,<!-- Defines connection pool tuning properties. -->
90,"<connection-pool initial-size=""1"""
90,"max-size=""10"""
90,"min-size=""3"""
90,"background-validation=""1000"""
90,"idle-removal=""1"""
90,"blocking-timeout=""1000"""
90,"leak-detection=""10000""/>"
90,</data-source>
90,</data-sources>
90,</server>
90,JSON
90,"""server"": {"
90,"""data-sources"": [{"
90,"""name"": ""ds"","
90,"""jndi-name"": ""jdbc/postgres"","
90,"""statistics"": true,"
90,"""connection-factory"": {"
90,"""driver"": ""org.postgresql.Driver"","
90,"""url"": ""jdbc:postgresql://localhost:5432/postgres"","
90,"""username"": ""postgres"","
90,"""password"": ""changeme"","
90,"""connection-properties"": {"
90,"""name"": ""value"""
90,"""connection-pool"": {"
90,"""initial-size"": 1,"
90,"""max-size"": 10,"
90,"""min-size"": 3,"
90,"""background-validation"": 1000,"
90,"""idle-removal"": 1,"
90,"""blocking-timeout"": 1000,"
90,"""leak-detection"": 10000"
90,YAML
90,server:
90,dataSources:
90,- name: ds
90,jndiName: 'jdbc/postgres'
90,statistics: true
90,connectionFactory:
90,"driver: ""org.postgresql.Driver"""
90,"url: ""jdbc:postgresql://localhost:5432/postgres"""
90,"username: ""postgres"""
90,"password: ""changeme"""
90,connectionProperties:
90,name: value
90,connectionPool:
90,initialSize: 1
90,maxSize: 10
90,minSize: 3
90,backgroundValidation: 1000
90,idleRemoval: 1
90,blockingTimeout: 1000
90,leakDetection: 10000
90,Configuring caches with JNDI names
90,When you add a managed datasource to Infinispan Server you can add the JNDI name to a JDBC-based cache store configuration.
90,Prerequisites
90,Configure Infinispan Server with a managed datasource.
90,Procedure
90,Open your cache configuration for editing.
90,Add the data-source element or field to the JDBC-based cache store configuration.
90,Specify the JNDI name of the managed datasource as the value of the jndi-url attribute.
90,Configure the JDBC-based cache stores as appropriate.
90,Save the changes to your configuration.
90,JNDI name in cache configuration
90,XML
90,<distributed-cache>
90,<persistence>
90,<jdbc:string-keyed-jdbc-store>
90,<!-- Specifies the JNDI name of a managed datasource on Infinispan Server. -->
90,"<jdbc:data-source jndi-url=""jdbc/postgres""/>"
90,"<jdbc:string-keyed-table drop-on-exit=""true"" create-on-start=""true"" prefix=""TBL"">"
90,"<jdbc:id-column name=""ID"" type=""VARCHAR(255)""/>"
90,"<jdbc:data-column name=""DATA"" type=""BYTEA""/>"
90,"<jdbc:timestamp-column name=""TS"" type=""BIGINT""/>"
90,"<jdbc:segment-column name=""S"" type=""INT""/>"
90,</jdbc:string-keyed-table>
90,</jdbc:string-keyed-jdbc-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""string-keyed-jdbc-store"": {"
90,"""data-source"": {"
90,"""jndi-url"": ""jdbc/postgres"""
90,"""string-keyed-table"": {"
90,"""prefix"": ""TBL"","
90,"""drop-on-exit"": true,"
90,"""create-on-start"": true,"
90,"""id-column"": {"
90,"""name"": ""ID"","
90,"""type"": ""VARCHAR(255)"""
90,"""data-column"": {"
90,"""name"": ""DATA"","
90,"""type"": ""BYTEA"""
90,"""timestamp-column"": {"
90,"""name"": ""TS"","
90,"""type"": ""BIGINT"""
90,"""segment-column"": {"
90,"""name"": ""S"","
90,"""type"": ""INT"""
90,YAML
90,distributedCache:
90,persistence:
90,stringKeyedJdbcStore:
90,dataSource:
90,"jndi-url: ""jdbc/postgres"""
90,stringKeyedTable:
90,"prefix: ""TBL"""
90,dropOnExit: true
90,createOnStart: true
90,idColumn:
90,"name: ""ID"""
90,"type: ""VARCHAR(255)"""
90,dataColumn:
90,"name: ""DATA"""
90,"type: ""BYTEA"""
90,timestampColumn:
90,"name: ""TS"""
90,"type: ""BIGINT"""
90,segmentColumn:
90,"name: ""S"""
90,"type: ""INT"""
90,Connection pool tuning properties
90,You can tune JDBC connection pools for managed datasources in your Infinispan Server configuration.
90,Property
90,Description
90,initial-size
90,Initial number of connections the pool should hold.
90,max-size
90,Maximum number of connections in the pool.
90,min-size
90,Minimum number of connections the pool should hold.
90,blocking-timeout
90,Maximum time in milliseconds to block while waiting for a connection before throwing an exception.
90,This will never throw an exception if creating a new connection takes an inordinately long period of time.
90,Default is 0 meaning that a call will wait indefinitely.
90,background-validation
90,Time in milliseconds between background validation runs. A duration of 0 means that this feature is disabled.
90,validate-on-acquisition
90,"Connections idle for longer than this time, specified in milliseconds, are validated before being acquired (foreground validation). A duration of 0 means that this feature is disabled."
90,idle-removal
90,Time in minutes a connection has to be idle before it can be removed.
90,leak-detection
90,Time in milliseconds a connection has to be held before a leak warning.
90,6.9.2. Configuring JDBC connection pools with Agroal properties
90,You can use a properties file to configure pooled connection factories for JDBC string-based cache stores.
90,Procedure
90,"Specify JDBC connection pool configuration with org.infinispan.agroal.* properties, as in the following example:"
90,org.infinispan.agroal.metricsEnabled=false
90,org.infinispan.agroal.minSize=10
90,org.infinispan.agroal.maxSize=100
90,org.infinispan.agroal.initialSize=20
90,org.infinispan.agroal.acquisitionTimeout_s=1
90,org.infinispan.agroal.validationTimeout_m=1
90,org.infinispan.agroal.leakTimeout_s=10
90,org.infinispan.agroal.reapTimeout_m=10
90,org.infinispan.agroal.metricsEnabled=false
90,org.infinispan.agroal.autoCommit=true
90,org.infinispan.agroal.jdbcTransactionIsolation=READ_COMMITTED
90,org.infinispan.agroal.jdbcUrl=jdbc:h2:mem:PooledConnectionFactoryTest;DB_CLOSE_DELAY=-1
90,org.infinispan.agroal.driverClassName=org.h2.Driver.class
90,org.infinispan.agroal.principal=sa
90,org.infinispan.agroal.credential=sa
90,Configure Infinispan to use your properties file with the properties-file attribute or the PooledConnectionFactoryConfiguration.propertyFile() method.
90,XML
90,"<connection-pool properties-file=""path/to/agroal.properties""/>"
90,JSON
90,"""persistence"": {"
90,"""connection-pool"": {"
90,"""properties-file"": ""path/to/agroal.properties"""
90,YAML
90,persistence:
90,connectionPool:
90,propertiesFile: path/to/agroal.properties
90,ConfigurationBuilder
90,".connectionPool().propertyFile(""path/to/agroal.properties"")"
90,Additional resources
90,Agroal
90,6.10. SQL cache stores
90,SQL cache stores let you load Infinispan caches from existing database tables.
90,Infinispan offers two types of SQL cache store:
90,Table
90,Infinispan loads entries from a single database table.
90,Query
90,"Infinispan uses SQL queries to load entries from single or multiple database tables, including from sub-columns within those tables, and perform insert, update, and delete operations."
90,Visit the code tutorials to try a SQL cache store in action.
90,See the Persistence code tutorial with remote caches.
90,Both SQL table and query stores:
90,Allow read and write operations to persistent storage.
90,Can be read-only and act as a cache loader.
90,Support keys and values that correspond to a single database column or a composite of multiple database columns.
90,"For composite keys and values, you must provide Infinispan with Protobuf schema (.proto files) that describe the keys and values."
90,With Infinispan Server you can add schema through the Infinispan Console or Command Line Interface (CLI) with the schema command.
90,The SQL cache store is intended for use with an existing database table.
90,"As a result, it does not store any metadata, which includes expiration, segments, and, versioning metadata."
90,"Due to the absence of version storage, SQL store does not support optimistic transactional caching and asynchronous cross-site replication."
90,This limitation also extends to Hot Rod versioned operations.
90,Use expiration with the SQL cache store when it is configured as read only.
90,"Expiration removes stale values from memory, causing the cache to fetch the values from the database again and cache them anew."
90,Additional resources
90,DatabaseType Enum lists supported database dialects
90,Infinispan SQL store configuration reference
90,6.10.1. Data types for keys and values
90,"Infinispan loads keys and values from columns in database tables via SQL cache stores, automatically using the appropriate data types."
90,"The following CREATE statement adds a table named ""books"" that has two columns, isbn and title:"
90,Database table with two columns
90,CREATE TABLE books (
90,"isbn NUMBER(13),"
90,title varchar(120)
90,PRIMARY KEY(isbn)
90,"When you use this table with a SQL cache store, Infinispan adds an entry to the cache using the isbn column as the key and the title column as the value."
90,Additional resources
90,Infinispan SQL store configuration reference
90,Composite keys and values
90,You can use SQL stores with database tables that contain composite primary keys or composite values.
90,"To use composite keys or values, you must provide Infinispan with Protobuf schema that describe the data types."
90,You must also add schema configuration to your SQL store and specify the message names for keys and values.
90,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
90,"You can then upload your Protobuf schema for remote caches through the Infinispan Console, CLI, or REST API."
90,Composite values
90,The following database table holds a composite value of the title and author columns:
90,CREATE TABLE books (
90,"isbn NUMBER(13),"
90,"title varchar(120),"
90,author varchar(80)
90,PRIMARY KEY(isbn)
90,Infinispan adds an entry to the cache using the isbn column as the key.
90,"For the value, Infinispan requires a Protobuf schema that maps the title column and the author columns:"
90,package library;
90,message books_value {
90,optional string title = 1;
90,optional string author = 2;
90,Composite keys and values
90,"The following database table holds a composite primary key and a composite value, with two columns each:"
90,CREATE TABLE books (
90,"isbn NUMBER(13),"
90,"reprint INT,"
90,"title varchar(120),"
90,author varchar(80)
90,"PRIMARY KEY(isbn, reprint)"
90,"For both the key and the value, Infinispan requires a Protobuf schema that maps the columns to keys and values:"
90,package library;
90,message books_key {
90,required string isbn = 1;
90,required int32 reprint = 2;
90,message books_value {
90,optional string title = 1;
90,optional string author = 2;
90,Additional resources
90,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
90,Infinispan SQL store configuration reference
90,Embedded keys
90,"Protobuf schema can include keys within values, as in the following example:"
90,Protobuf schema with an embedded key
90,package library;
90,message books_key {
90,required string isbn = 1;
90,required int32 reprint = 2;
90,message books_value {
90,required string isbn = 1;
90,required string reprint = 2;
90,optional string title = 3;
90,optional string author = 4;
90,"To use embedded keys, you must include the embedded-key=""true"" attribute or embeddedKey(true) method in your SQL store configuration."
90,SQL types to Protobuf types
90,The following table contains default mappings of SQL data types to Protobuf data types:
90,SQL type
90,Protobuf type
90,int4
90,int32
90,int8
90,int64
90,float4
90,float
90,float8
90,double
90,numeric
90,double
90,bool
90,bool
90,char
90,string
90,varchar
90,string
90,"text, tinytext, mediumtext, longtext"
90,string
90,"bytea, tinyblob, blob, mediumblob, longblob"
90,bytes
90,Additional resources
90,Cache encoding and marshalling
90,6.10.2. Loading Infinispan caches from database tables
90,Add a SQL table cache store to your configuration if you want Infinispan to load data from a database table.
90,"When it connects to the database, Infinispan uses metadata from the table to detect column names and data types."
90,Infinispan also automatically determines which columns in the database are part of the primary key.
90,Prerequisites
90,Have JDBC connection details.
90,You can add JDBC connection factories directly to your cache configuration.
90,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
90,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
90,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
90,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
90,Procedure
90,Add database drivers to your Infinispan deployment.
90,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
90,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
90,install org.postgresql:postgresql:42.4.4
90,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file.
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-cachestore-sql</artifactId>
90,</dependency>
90,Open your Infinispan configuration for editing.
90,Add a SQL table cache store.
90,Declarative
90,"table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
90,Programmatic
90,persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
90,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
90,"Configure the SQL cache store with the properties you require, for example:"
90,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
90,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
90,"Name the database table that loads the cache with table-name=""<database_table_name>"" or table.name(""<database_table_name>"")."
90,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
90,Specify the package name with the package attribute or package() method.
90,Specify composite values with the message-name attribute or messageName() method.
90,Specify composite keys with the key-message-name attribute or keyMessageName() method.
90,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
90,Save the changes to your configuration.
90,SQL table store configuration
90,"The following example loads a distributed cache from a database table named ""books"" using composite values defined in a Protobuf schema:"
90,XML
90,<distributed-cache>
90,<persistence>
90,"<table-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
90,"dialect=""H2"""
90,"shared=""true"""
90,"table-name=""books"">"
90,"<schema message-name=""books_value"""
90,"package=""library""/>"
90,</table-jdbc-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""table-jdbc-store"": {"
90,"""dialect"": ""H2"","
90,"""shared"": ""true"","
90,"""table-name"": ""books"","
90,"""schema"": {"
90,"""message-name"": ""books_value"","
90,"""package"": ""library"""
90,YAML
90,distributedCache:
90,persistence:
90,tableJdbcStore:
90,"dialect: ""H2"""
90,"shared: ""true"""
90,"tableName: ""books"""
90,schema:
90,"messageName: ""books_value"""
90,"package: ""library"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence().addStore(TableJdbcStoreConfigurationBuilder.class)
90,.dialect(DatabaseType.H2)
90,".shared(""true"")"
90,".tableName(""books"")"
90,.schemaJdbcConfigurationBuilder()
90,".messageName(""books_value"")"
90,".packageName(""library"");"
90,Additional resources
90,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
90,Persistence code tutorial with remote caches
90,JDBC connection factories
90,DatabaseType Enum lists supported database dialects
90,Infinispan SQL store configuration reference
90,6.10.3. Using SQL queries to load data and perform operations
90,"SQL query cache stores let you load caches from multiple database tables, including from sub-columns in database tables, and perform insert, update, and delete operations."
90,Prerequisites
90,Have JDBC connection details.
90,You can add JDBC connection factories directly to your cache configuration.
90,"For remote caches in production environments, you should add managed datasources to Infinispan Server configuration and specify the JNDI name in the cache configuration."
90,Generate Protobuf schema for any composite keys or composite values and register your schemas with Infinispan.
90,Infinispan recommends generating Protobuf schema with the ProtoStream processor.
90,"For remote caches, you can register your schemas by adding them through the Infinispan Console, CLI, or REST API."
90,Procedure
90,Add database drivers to your Infinispan deployment.
90,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
90,"Use the install command with the Infinispan Command Line Interface (CLI) to download the required drivers to the server/lib directory, for example:"
90,install org.postgresql:postgresql:42.4.4
90,Embedded caches: Add the infinispan-cachestore-sql dependency to your pom file and make sure database drivers are on your application classpath.
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-cachestore-sql</artifactId>
90,</dependency>
90,Open your Infinispan configuration for editing.
90,Add a SQL query cache store.
90,Declarative
90,"query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
90,Programmatic
90,persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
90,"Specify the database dialect with either dialect="""" or dialect(), for example dialect=""H2"" or dialect=""postgres""."
90,"Configure the SQL cache store with the properties you require, for example:"
90,"To use the same cache store across your cluster, set shared=""true"" or shared(true)."
90,"To create a read only cache store, set read-only=""true"" or .ignoreModifications(true)."
90,Define SQL query statements that load caches with data and modify database tables with the queries element or the queries() method.
90,Query statement
90,Description
90,SELECT
90,Loads a single entry into caches.
90,You can use wildcards but must specify parameters for keys.
90,You can use labelled expressions.
90,SELECT ALL
90,Loads multiple entries into caches.
90,You can use the * wildcard if the number of columns returned match the key and value columns.
90,You can use labelled expressions.
90,SIZE
90,Counts the number of entries in the cache.
90,DELETE
90,Deletes a single entry from the cache.
90,DELETE ALL
90,Deletes all entries from the cache.
90,UPSERT
90,Modifies entries in the cache.
90,"DELETE, DELETE ALL, and UPSERT statements do not apply to read only cache stores but are required if cache stores allow modifications."
90,Parameters in DELETE statements must match parameters in SELECT statements exactly.
90,Variables in UPSERT statements must have the same number of uniquely named variables that SELECT and SELECT ALL statements return.
90,"For example, if SELECT returns foo and bar this statement must take only :foo and :bar as variables."
90,However you can apply the same named variable more than once in a statement.
90,"SQL queries can include JOIN, ON, and any other clauses that the database supports."
90,Add the schema element or the .schemaJdbcConfigurationBuilder() method and add Protobuf schema configuration for composite keys or values.
90,Specify the package name with the package attribute or package() method.
90,Specify composite values with the message-name attribute or messageName() method.
90,Specify composite keys with the key-message-name attribute or keyMessageName() method.
90,Set a value of true for the embedded-key attribute or embeddedKey() method if your schema includes keys within values.
90,Save the changes to your configuration.
90,Additional resources
90,Cache encoding and marshalling: Generate Protobuf schema and register them with Infinispan
90,Persistence code tutorial with remote caches
90,JDBC connection factories
90,DatabaseType Enum lists supported database dialects
90,Infinispan SQL store configuration reference
90,SQL query store configuration
90,"This section provides an example configuration for a SQL query cache store that loads a distributed cache with data from two database tables: ""person"" and ""address""."
90,SQL statements
90,"The following examples show SQL data definition language (DDL) statements for the ""person"" and ""address"" tables."
90,The data types described in the example are only valid for PostgreSQL database.
90,"SQL statement for the ""person"" table"
90,CREATE TABLE Person (
90,"name VARCHAR(255) NOT NULL,"
90,"picture BYTEA,"
90,"sex VARCHAR(255),"
90,"birthdate TIMESTAMP,"
90,"accepted_tos BOOLEAN,"
90,"notused VARCHAR(255),"
90,PRIMARY KEY (name)
90,"SQL statement for the ""address"" table"
90,CREATE TABLE Address (
90,"name VARCHAR(255) NOT NULL,"
90,"street VARCHAR(255),"
90,"city VARCHAR(255),"
90,"zip INT,"
90,PRIMARY KEY (name)
90,Protobuf schemas
90,"Protobuf schema for the ""person"" and ""address"" tables are as follows:"
90,"Protobuf schema for the ""address"" table"
90,package com.example;
90,message Address {
90,optional string street = 1;
90,"optional string city = 2 [default = ""San Jose""];"
90,optional int32 zip = 3 [default = 0];
90,"Protobuf schema for the ""person"" table"
90,package com.example;
90,import com.example.Address;
90,enum Sex {
90,FEMALE = 1;
90,MALE = 2;
90,message Person {
90,optional string name = 1;
90,optional Address address = 2;
90,optional bytes picture = 3;
90,optional Sex sex = 4;
90,optional fixed64 birthDate = 5 [default = 0];
90,optional bool accepted_tos = 6 [default = false];
90,Cache configuration
90,"The following example loads a distributed cache from the ""person"" and ""address"" tables using a SQL query that includes a JOIN clause:"
90,XML
90,<distributed-cache>
90,<persistence>
90,"<query-jdbc-store xmlns=""urn:infinispan:config:store:sql:14.0"""
90,"dialect=""POSTGRES"""
90,"shared=""true"" key-columns=""name"">"
90,"<connection-pool driver=""org.postgresql.Driver"""
90,"connection-url=""jdbc:postgresql://localhost:5432/postgres"""
90,"username=""postgres"""
90,"password=""changeme""/>"
90,"<queries select-single=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
90,"select-all=""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
90,"delete-single=""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
90,"delete-all=""DELETE FROM Person; DELETE FROM Address"""
90,"upsert=""INSERT INTO Person (name,"
90,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
90,"size=""SELECT COUNT(*) FROM Person"""
90,"<schema message-name=""Person"""
90,"package=""com.example"""
90,"embedded-key=""true""/>"
90,</query-jdbc-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""query-jdbc-store"": {"
90,"""dialect"": ""POSTGRES"","
90,"""shared"": ""true"","
90,"""key-columns"": ""name"","
90,"""queries"": {"
90,"""select-single"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"","
90,"""select-all"": ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"","
90,"""delete-single"": ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"","
90,"""delete-all"": ""DELETE FROM Person; DELETE FROM Address"","
90,"""upsert"": ""INSERT INTO Person (name,"
90,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"","
90,"""size"": ""SELECT COUNT(*) FROM Person"""
90,"""schema"": {"
90,"""message-name"": ""Person"","
90,"""package"": ""com.example"","
90,"""embedded-key"": ""true"""
90,YAML
90,distributedCache:
90,persistence:
90,queryJdbcStore:
90,"dialect: ""POSTGRES"""
90,"shared: ""true"""
90,"keyColumns: ""name"""
90,queries:
90,"selectSingle: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"""
90,"selectAll: ""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"""
90,"deleteSingle: ""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"""
90,"deleteAll: ""DELETE FROM Person; DELETE FROM Address"""
90,"upsert: ""INSERT INTO Person (name,"
90,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"""
90,"size: ""SELECT COUNT(*) FROM Person"""
90,schema:
90,"messageName: ""Person"""
90,"package: ""com.example"""
90,"embeddedKey: ""true"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence().addStore(QueriesJdbcStoreConfigurationBuilder.class)
90,.dialect(DatabaseType.POSTGRES)
90,".shared(""true"")"
90,".keyColumns(""name"")"
90,.queriesJdbcConfigurationBuilder()
90,".select(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 WHERE t1.name = :name AND t2.name = :name"")"
90,".selectAll(""SELECT t1.name, t1.picture, t1.sex, t1.birthdate, t1.accepted_tos, t2.street, t2.city, t2.zip FROM Person t1 JOIN Address t2 ON t1.name = t2.name"")"
90,".delete(""DELETE FROM Person t1 WHERE t1.name = :name; DELETE FROM Address t2 where t2.name = :name"")"
90,".deleteAll(""DELETE FROM Person; DELETE FROM Address"")"
90,".upsert(""INSERT INTO Person (name,"
90,"picture, sex, birthdate, accepted_tos) VALUES (:name, :picture, :sex, :birthdate, :accepted_tos); INSERT INTO Address(name, street, city, zip) VALUES (:name, :street, :city, :zip)"")"
90,".size(""SELECT COUNT(*) FROM Person"")"
90,.schemaJdbcConfigurationBuilder()
90,".messageName(""Person"")"
90,".packageName(""com.example"")"
90,.embeddedKey(true);
90,Additional resources
90,Infinispan SQL store configuration reference
90,6.10.4. SQL cache store troubleshooting
90,Find out about common issues and errors with SQL cache stores and how to troubleshoot them.
90,"ISPN008064: No primary keys found for table <table_name>, check case sensitivity"
90,Infinispan logs this message in the following cases:
90,The database table does not exist.
90,"The database table name is case sensitive and needs to be either all lower case or all upper case, depending on the database provider."
90,The database table does not have any primary keys defined.
90,To resolve this issue you should:
90,Check your SQL cache store configuration and ensure that you specify the name of an existing table.
90,Ensure that the database table name conforms to an case sensitivity requirements.
90,Ensure that your database tables have primary keys that uniquely identify the appropriate rows.
90,6.11. JDBC string-based cache stores
90,"JDBC String-Based cache stores, JdbcStringBasedStore, use JDBC drivers to load and store values in the underlying database."
90,JDBC String-Based cache stores:
90,Store each entry in its own row in the table to increase throughput for concurrent loads.
90,Use a simple one-to-one mapping that maps each key to a String object using the key-to-string-mapper interface.
90,"Infinispan provides a default implementation, DefaultTwoWayKey2StringMapper, that handles primitive types."
90,"In addition to the data table used to store cache entries, the store also creates a _META table for storing metadata."
90,This table is used to ensure that any existing database content is compatible with the current Infinispan version and configuration.
90,"By default Infinispan shares are not stored, which means that all nodes in the"
90,cluster write to the underlying store on each update. If you want operations to
90,"write to the underlying database once only, you must configure JDBC store as"
90,shared.
90,Segmentation
90,JdbcStringBasedStore uses segmentation by default and requires a column in the database table to represent the segments to which entries belong.
90,Additional resources
90,DatabaseType Enum lists supported database dialects
90,6.11.1. Configuring JDBC string-based cache stores
90,Configure Infinispan caches with JDBC string-based cache stores that can connect to databases.
90,Prerequisites
90,Remote caches: Copy database drivers to the server/lib directory in your Infinispan Server installation.
90,Embedded caches: Add the infinispan-cachestore-jdbc dependency to your pom file.
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-cachestore-jdbc</artifactId>
90,</dependency>
90,Procedure
90,Create a JDBC string-based cache store configuration in one of the following ways:
90,"Declaratively, add the persistence element or field then add string-keyed-jdbc-store with the following schema namespace:"
90,"xmlns=""urn:infinispan:config:store:jdbc:14.0"""
90,"Programmatically, add the following methods to your ConfigurationBuilder:"
90,persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
90,Specify the dialect of the database with either the dialect attribute or the dialect() method.
90,Configure any properties for the JDBC string-based cache store as appropriate.
90,"For example, specify if the cache store is shared with multiple cache instances with either the shared attribute or the shared() method."
90,Add a JDBC connection factory so that Infinispan can connect to the database.
90,Add a database table that stores cache entries.
90,JDBC string-based cache store configuration
90,XML
90,<distributed-cache>
90,<persistence>
90,"<string-keyed-jdbc-store xmlns=""urn:infinispan:config:store:jdbc:14.0"""
90,"dialect=""H2"">"
90,"<connection-pool connection-url=""jdbc:h2:mem:infinispan"""
90,"username=""sa"""
90,"password=""changeme"""
90,"driver=""org.h2.Driver""/>"
90,"<string-keyed-table create-on-start=""true"""
90,"prefix=""ISPN_STRING_TABLE"">"
90,"<id-column name=""ID_COLUMN"""
90,"type=""VARCHAR(255)"" />"
90,"<data-column name=""DATA_COLUMN"""
90,"type=""BINARY"" />"
90,"<timestamp-column name=""TIMESTAMP_COLUMN"""
90,"type=""BIGINT"" />"
90,"<segment-column name=""SEGMENT_COLUMN"""
90,"type=""INT""/>"
90,</string-keyed-table>
90,</string-keyed-jdbc-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"": {"
90,"""string-keyed-jdbc-store"": {"
90,"""dialect"": ""H2"","
90,"""string-keyed-table"": {"
90,"""prefix"": ""ISPN_STRING_TABLE"","
90,"""create-on-start"": true,"
90,"""id-column"": {"
90,"""name"": ""ID_COLUMN"","
90,"""type"": ""VARCHAR(255)"""
90,"""data-column"": {"
90,"""name"": ""DATA_COLUMN"","
90,"""type"": ""BINARY"""
90,"""timestamp-column"": {"
90,"""name"": ""TIMESTAMP_COLUMN"","
90,"""type"": ""BIGINT"""
90,"""segment-column"": {"
90,"""name"": ""SEGMENT_COLUMN"","
90,"""type"": ""INT"""
90,"""connection-pool"": {"
90,"""connection-url"": ""jdbc:h2:mem:infinispan"","
90,"""driver"": ""org.h2.Driver"","
90,"""username"": ""sa"","
90,"""password"": ""changeme"""
90,YAML
90,distributedCache:
90,persistence:
90,stringKeyedJdbcStore:
90,"dialect: ""H2"""
90,stringKeyedTable:
90,"prefix: ""ISPN_STRING_TABLE"""
90,createOnStart: true
90,idColumn:
90,"name: ""ID_COLUMN"""
90,"type: ""VARCHAR(255)"""
90,dataColumn:
90,"name: ""DATA_COLUMN"""
90,"type: ""BINARY"""
90,timestampColumn:
90,"name: ""TIMESTAMP_COLUMN"""
90,"type: ""BIGINT"""
90,segmentColumn:
90,"name: ""SEGMENT_COLUMN"""
90,"type: ""INT"""
90,connectionPool:
90,"connectionUrl: ""jdbc:h2:mem:infinispan"""
90,"driver: ""org.h2.Driver"""
90,"username: ""sa"""
90,"password: ""changeme"""
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.persistence().addStore(JdbcStringBasedStoreConfigurationBuilder.class)
90,.dialect(DatabaseType.H2)
90,.table()
90,.dropOnExit(true)
90,.createOnStart(true)
90,".tableNamePrefix(""ISPN_STRING_TABLE"")"
90,".idColumnName(""ID_COLUMN"").idColumnType(""VARCHAR(255)"")"
90,".dataColumnName(""DATA_COLUMN"").dataColumnType(""BINARY"")"
90,".timestampColumnName(""TIMESTAMP_COLUMN"").timestampColumnType(""BIGINT"")"
90,".segmentColumnName(""SEGMENT_COLUMN"").segmentColumnType(""INT"")"
90,.connectionPool()
90,".connectionUrl(""jdbc:h2:mem:infinispan"")"
90,".username(""sa"")"
90,".password(""changeme"")"
90,".driverClass(""org.h2.Driver"");"
90,Additional resources
90,JDBC connection factories
90,6.12. RocksDB cache stores
90,RocksDB provides key-value filesystem-based storage with high performance and
90,reliability for highly concurrent environments.
90,"RocksDB cache stores, RocksDBStore, use two databases. One database provides"
90,a primary cache store for data in memory; the other database holds entries that
90,Infinispan expires from memory.
90,Table 1. Configuration parameters
90,Parameter
90,Description
90,location
90,Specifies the path to the RocksDB database that provides the primary cache
90,"store. If you do not set the location, it is automatically created. Note that"
90,the path must be relative to the global persistent location.
90,expiredLocation
90,Specifies the path to the RocksDB database that provides the cache store for
90,"expired data. If you do not set the location, it is automatically created. Note"
90,that the path must be relative to the global persistent location.
90,expiryQueueSize
90,Sets the size of the in-memory queue for expiring entries. When the queue
90,"reaches the size, Infinispan flushes the expired into the RocksDB cache store."
90,clearThreshold
90,Sets the maximum number of entries before deleting and re-initializing
90,"(re-init) the RocksDB database. For smaller size cache stores, iterating"
90,through all entries and removing each one individually can provide a faster
90,method.
90,Tuning parameters
90,You can also specify the following RocksDB tuning parameters:
90,compressionType
90,blockSize
90,cacheSize
90,Configuration properties
90,Optionally set properties in the configuration as follows:
90,Prefix properties with database to adjust and tune RocksDB databases.
90,Prefix properties with data to configure the column families in which RocksDB stores your data.
90,"<property name=""database.max_background_compactions"">2</property>"
90,"<property name=""data.write_buffer_size"">64MB</property>"
90,"<property name=""data.compression_per_level"">kNoCompression:kNoCompression:kNoCompression:kSnappyCompression:kZSTD:kZSTD</property>"
90,Segmentation
90,RocksDBStore supports segmentation and creates a separate column family per
90,segment. Segmented RocksDB cache stores improve lookup performance
90,and iteration but slightly lower performance of write operations.
90,You should not configure more than a few hundred segments. RocksDB is not
90,designed to have an unlimited number of column families. Too many segments also
90,significantly increases cache store start time.
90,RocksDB cache store configuration
90,XML
90,<local-cache>
90,<persistence>
90,"<rocksdb-store xmlns=""urn:infinispan:config:store:rocksdb:14.0"""
90,"path=""rocksdb/data"">"
90,"<expiration path=""rocksdb/expired""/>"
90,</rocksdb-store>
90,</persistence>
90,</local-cache>
90,JSON
90,"""local-cache"": {"
90,"""persistence"": {"
90,"""rocksdb-store"": {"
90,"""path"": ""rocksdb/data"","
90,"""expiration"": {"
90,"""path"": ""rocksdb/expired"""
90,YAML
90,localCache:
90,persistence:
90,rocksdbStore:
90,"path: ""rocksdb/data"""
90,expiration:
90,"path: ""rocksdb/expired"""
90,ConfigurationBuilder
90,Configuration cacheConfig = new ConfigurationBuilder().persistence()
90,.addStore(RocksDBStoreConfigurationBuilder.class)
90,.build();
90,EmbeddedCacheManager cacheManager = new DefaultCacheManager(cacheConfig);
90,"Cache<String, User> usersCache = cacheManager.getCache(""usersCache"");"
90,"usersCache.put(""raytsang"", new User(...));"
90,ConfigurationBuilder with properties
90,Properties props = new Properties();
90,"props.put(""database.max_background_compactions"", ""2"");"
90,"props.put(""data.write_buffer_size"", ""512MB"");"
90,Configuration cacheConfig = new ConfigurationBuilder().persistence()
90,.addStore(RocksDBStoreConfigurationBuilder.class)
90,".location(""rocksdb/data"")"
90,".expiredLocation(""rocksdb/expired"")"
90,.properties(props)
90,.build();
90,Reference
90,RocksDB cache store configuration schema
90,RocksDBStore
90,RocksDBStoreConfiguration
90,rocksdb.org
90,RocksDB Tuning Guide
90,RocksDB Cache Store test
90,RocksDB Cache Store test configuration
90,6.13. Remote cache stores
90,"Remote cache stores, RemoteStore, use the Hot Rod protocol to store data on"
90,Infinispan clusters.
90,If you configure remote cache stores as shared you cannot preload data.
90,"In other words if shared=""true"" in your configuration then you must set preload=""false""."
90,Segmentation
90,RemoteStore supports segmentation and can publish keys and entries by
90,"segment, which makes bulk operations more efficient. However, segmentation is"
90,available only with Infinispan Hot Rod protocol version 2.3 or later.
90,"When you enable segmentation for RemoteStore, it uses the number of segments"
90,that you define in your Infinispan server configuration.
90,If the source cache is segmented and uses a different number of segments than
90,"RemoteStore, then incorrect values are returned for bulk operations. In this"
90,"case, you should disable segmentation for RemoteStore."
90,Remote cache store configuration
90,XML
90,<distributed-cache>
90,<persistence>
90,"<remote-store xmlns=""urn:infinispan:config:store:remote:14.0"""
90,"cache=""mycache"""
90,"raw-values=""true"">"
90,"<remote-server host=""one"""
90,"port=""12111"" />"
90,"<remote-server host=""two"" />"
90,"<connection-pool max-active=""10"""
90,"exhausted-action=""CREATE_NEW"" />"
90,</remote-store>
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""remote-store"": {"
90,"""cache"": ""mycache"","
90,"""raw-values"": ""true"","
90,"""remote-server"": ["
90,"""host"": ""one"","
90,"""port"": ""12111"""
90,"""host"": ""two"""
90,"""connection-pool"": {"
90,"""max-active"": ""10"","
90,"""exhausted-action"": ""CREATE_NEW"""
90,YAML
90,distributedCache:
90,remoteStore:
90,"cache: ""mycache"""
90,"rawValues: ""true"""
90,remoteServer:
90,"- host: ""one"""
90,"port: ""12111"""
90,"- host: ""two"""
90,connectionPool:
90,"maxActive: ""10"""
90,"exhaustedAction: ""CREATE_NEW"""
90,ConfigurationBuilder
90,ConfigurationBuilder b = new ConfigurationBuilder();
90,b.persistence().addStore(RemoteStoreConfigurationBuilder.class)
90,.ignoreModifications(false)
90,.purgeOnStartup(false)
90,".remoteCacheName(""mycache"")"
90,.rawValues(true)
90,.addServer()
90,".host(""one"").port(12111)"
90,.addServer()
90,".host(""two"")"
90,.connectionPool()
90,.maxActive(10)
90,.exhaustedAction(ExhaustedAction.CREATE_NEW)
90,.async().enable();
90,Reference
90,Remote cache store configuration schema
90,RemoteStore
90,RemoteStoreConfigurationBuilder
90,6.14. Cluster cache loaders
90,ClusterCacheLoader retrieves data from other Infinispan cluster members but
90,"does not persist data. In other words, ClusterCacheLoader is not a cache"
90,store.
90,ClusterLoader is deprecated and planned for removal in a future version.
90,ClusterCacheLoader provides a non-blocking partial alternative to state
90,transfer. ClusterCacheLoader fetches keys from other nodes on demand if those
90,"keys are not available on the local node, which is similar to lazily loading"
90,cache content.
90,The following points also apply to ClusterCacheLoader:
90,Preloading does not take effect (preload=true).
90,Segmentation is not supported.
90,Cluster cache loader configuration
90,XML
90,<distributed-cache>
90,<persistence>
90,"<cluster-loader preload=""true"" remote-timeout=""500""/>"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"" : {"
90,"""cluster-loader"" : {"
90,"""preload"" : true,"
90,"""remote-timeout"" : ""500"""
90,YAML
90,distributedCache:
90,persistence:
90,clusterLoader:
90,"preload: ""true"""
90,"remoteTimeout: ""500"""
90,ConfigurationBuilder
90,ConfigurationBuilder b = new ConfigurationBuilder();
90,b.persistence()
90,.addClusterLoader()
90,.remoteCallTimeout(500);
90,Additional resources
90,Infinispan configuration schema
90,ClusterLoader
90,ClusterLoaderConfiguration
90,6.15. Creating custom cache store implementations
90,You can create custom cache stores through the Infinispan persistent SPI.
90,6.15.1. Infinispan Persistence SPI
90,The Infinispan Service Provider Interface (SPI) enables read and write
90,operations to external storage through the NonBlockingStore interface and has
90,the following features:
90,Portability across JCache-compliant vendors
90,Infinispan maintains compatibility between the NonBlockingStore interface
90,and the JSR-107 JCache specification by using an adapter that handles
90,blocking code.
90,Simplified transaction integration
90,Infinispan automatically handles locking so your implementations do not need
90,to coordinate concurrent access to persistent stores. Depending on the locking
90,"mode you use, concurrent writes to the same key generally do not occur."
90,"However, you should expect operations on the persistent storage to originate"
90,from multiple threads and create implementations to tolerate this behavior.
90,Parallel iteration
90,Infinispan lets you iterate over entries in persistent stores with multiple
90,threads in parallel.
90,Reduced serialization resulting in less CPU usage
90,Infinispan exposes stored entries in a serialized format that can be
90,"transmitted remotely. For this reason, Infinispan does not need to deserialize"
90,entries that it retrieves from persistent storage and then serialize again when
90,writing to the wire.
90,Additional resources
90,Persistence SPI
90,NonBlockingStore
90,JSR-107
90,6.15.2. Creating cache stores
90,Create custom cache stores with implementations of the NonBlockingStore API.
90,Procedure
90,Implement the appropriate Infinispan persistent SPIs.
90,Annotate your store class with the @ConfiguredBy annotation if it has a custom configuration.
90,Create a custom cache store configuration and builder if desired.
90,Extend AbstractStoreConfiguration and AbstractStoreConfigurationBuilder.
90,Optionally add the following annotations to your store Configuration class to ensure that your
90,custom configuration builder parses your cache store configuration from XML:
90,@ConfigurationFor
90,@BuiltBy
90,"If you do not add these annotations, then CustomStoreConfigurationBuilder parses the common"
90,store attributes defined in AbstractStoreConfiguration and any additional elements are ignored.
90,If a configuration does not declare the
90,"@ConfigurationFor annotation, a warning message is logged when Infinispan"
90,initializes the cache.
90,6.15.3. Examples of custom cache store configuration
90,The following are examples show how to configure Infinispan with custom cache store implementations:
90,XML
90,<distributed-cache>
90,<persistence>
90,"<store class=""org.infinispan.persistence.example.MyInMemoryStore"" />"
90,</persistence>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""persistence"" : {"
90,"""store"" : {"
90,"""class"" : ""org.infinispan.persistence.example.MyInMemoryStore"""
90,YAML
90,distributedCache:
90,persistence:
90,store:
90,"class: ""org.infinispan.persistence.example.MyInMemoryStore"""
90,ConfigurationBuilder
90,Configuration config = new ConfigurationBuilder()
90,.persistence()
90,.addStore(CustomStoreConfigurationBuilder.class)
90,.build();
90,6.15.4. Deploying custom cache stores
90,"To use your cache store implementation with Infinispan Server, you must provide it with a JAR file."
90,Prerequisites
90,Stop Infinispan Server if it is running.
90,Infinispan loads JAR files at startup only.
90,Procedure
90,Package your custom cache store implementation in a JAR file.
90,Add your JAR file to the server/lib directory of your Infinispan Server installation.
90,6.16. Migrating data between cache stores
90,Infinispan provides a utility to migrate data from one cache store to another.
90,6.16.1. Cache store migrator
90,Infinispan provides the StoreMigrator.java utility that recreates data for the latest Infinispan cache store implementations.
90,StoreMigrator takes a cache store from a previous version of Infinispan as source and uses a cache store implementation as target.
90,"When you run StoreMigrator, it creates the target cache with the cache store type that you define using the EmbeddedCacheManager interface."
90,StoreMigrator then loads entries from the source store into memory and then puts them into the target cache.
90,"StoreMigrator also lets you migrate data from one type of cache store to another. For example, you can migrate from a JDBC string-based cache store to a RocksDB cache store."
90,StoreMigrator cannot migrate data from segmented cache stores to:
90,Non-segmented cache store.
90,Segmented cache stores that have a different number of segments.
90,6.16.2. Getting the cache store migrator
90,"StoreMigrator is available as part of the Infinispan tools library,"
90,"infinispan-tools, and is included in the Maven repository."
90,Procedure
90,Configure your pom.xml for StoreMigrator as follows:
90,"<?xml version=""1.0"" encoding=""UTF-8""?>"
90,"<project xmlns=""http://maven.apache.org/POM/4.0.0"""
90,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
90,"xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">"
90,<modelVersion>4.0.0</modelVersion>
90,<groupId>org.infinispan.example</groupId>
90,<artifactId>jdbc-migrator-example</artifactId>
90,<version>1.0-SNAPSHOT</version>
90,<dependencies>
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-tools</artifactId>
90,</dependency>
90,<!-- Additional dependencies -->
90,</dependencies>
90,<build>
90,<plugins>
90,<plugin>
90,<groupId>org.codehaus.mojo</groupId>
90,<artifactId>exec-maven-plugin</artifactId>
90,<version>1.2.1</version>
90,<executions>
90,<execution>
90,<goals>
90,<goal>java</goal>
90,</goals>
90,</execution>
90,</executions>
90,<configuration>
90,<mainClass>org.infinispan.tools.store.migrator.StoreMigrator</mainClass>
90,<arguments>
90,<argument>path/to/migrator.properties</argument>
90,</arguments>
90,</configuration>
90,</plugin>
90,</plugins>
90,</build>
90,</project>
90,6.16.3. Configuring the cache store migrator
90,Use the migrator.properties file to configure properties for source and target cache stores.
90,Procedure
90,Create a migrator.properties file.
90,Configure properties for source and target cache store using the migrator.properties file.
90,Add the source. prefix to all configuration properties for the source cache store.
90,Example source cache store
90,source.type=SOFT_INDEX_FILE_STORE
90,source.cache_name=myCache
90,source.location=/path/to/source/sifs
90,source.version=<version>
90,"For migrating data from segmented cache stores, you must also configure the number of segments using the source.segment_count property."
90,The number of segments must match clustering.hash.numSegments in your Infinispan configuration.
90,"If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
90,Add the target. prefix to all configuration properties for the target cache store.
90,Example target cache store
90,target.type=SINGLE_FILE_STORE
90,target.cache_name=myCache
90,target.location=/path/to/target/sfs.dat
90,Configuration properties for the cache store migrator
90,Configure source and target cache stores in a StoreMigrator properties.
90,Table 2. Cache Store Type Property
90,Property
90,Description
90,Required/Optional
90,type
90,Specifies the type of cache store for a source or target cache store.
90,.type=JDBC_STRING
90,.type=JDBC_BINARY
90,.type=JDBC_MIXED
90,.type=LEVELDB
90,.type=ROCKSDB
90,.type=SINGLE_FILE_STORE
90,.type=SOFT_INDEX_FILE_STORE
90,.type=JDBC_MIXED
90,Required
90,Table 3. Common Properties
90,Property
90,Description
90,Example Value
90,Required/Optional
90,cache_name
90,The name of the cache that you want to back up.
90,.cache_name=myCache
90,Required
90,segment_count
90,The number of segments for target cache stores that can use
90,segmentation.
90,The number of segments must match clustering.hash.numSegments in the
90,"Infinispan configuration. If the number of segments for a cache store does not match the number of segments for the corresponding cache, Infinispan cannot read data from the cache store."
90,.segment_count=256
90,Optional
90,Table 4. JDBC Properties
90,Property
90,Description
90,Required/Optional
90,dialect
90,Specifies the dialect of the underlying database.
90,Required
90,version
90,Specifies the marshaller version for source cache stores.
90,Set the value that matches the Infinispan major version of the source cluster. For example; set a value of 14 for Infinispan 14.x.
90,Required for source stores only.
90,marshaller.class
90,Specifies a custom marshaller class.
90,Required if using custom marshallers.
90,marshaller.externalizers
90,Specifies a comma-separated list of custom AdvancedExternalizer implementations to load in this format: [id]:<Externalizer class>
90,Optional
90,connection_pool.connection_url
90,Specifies the JDBC connection URL.
90,Required
90,connection_pool.driver_class
90,Specifies the class of the JDBC driver.
90,Required
90,connection_pool.username
90,Specifies a database username.
90,Required
90,connection_pool.password
90,Specifies a password for the database username.
90,Required
90,db.disable_upsert
90,Disables database upsert.
90,Optional
90,db.disable_indexing
90,Specifies if table indexes are created.
90,Optional
90,table.string.table_name_prefix
90,Specifies additional prefixes for the table name.
90,Optional
90,table.string.<id|data|timestamp>.name
90,Specifies the column name.
90,Required
90,table.string.<id|data|timestamp>.type
90,Specifies the column type.
90,Required
90,key_to_string_mapper
90,Specifies the TwoWayKey2StringMapper class.
90,Optional
90,"To migrate from Binary cache stores in older Infinispan versions, change"
90,table.string.* to table.binary.\* in the following properties:
90,source.table.binary.table_name_prefix
90,source.table.binary.<id\|data\|timestamp>.name
90,source.table.binary.<id\|data\|timestamp>.type
90,# Example configuration for migrating to a JDBC String-Based cache store
90,target.type=STRING
90,target.cache_name=myCache
90,target.dialect=POSTGRES
90,target.marshaller.class=org.example.CustomMarshaller
90,"target.marshaller.externalizers=25:Externalizer1,org.example.Externalizer2"
90,target.connection_pool.connection_url=jdbc:postgresql:postgres
90,target.connection_pool.driver_class=org.postrgesql.Driver
90,target.connection_pool.username=postgres
90,target.connection_pool.password=redhat
90,target.db.disable_upsert=false
90,target.db.disable_indexing=false
90,target.table.string.table_name_prefix=tablePrefix
90,target.table.string.id.name=id_column
90,target.table.string.data.name=datum_column
90,target.table.string.timestamp.name=timestamp_column
90,target.table.string.id.type=VARCHAR
90,target.table.string.data.type=bytea
90,target.table.string.timestamp.type=BIGINT
90,target.key_to_string_mapper=org.infinispan.persistence.keymappers. DefaultTwoWayKey2StringMapper
90,Table 5. RocksDB Properties
90,Property
90,Description
90,Required/Optional
90,location
90,Sets the database directory.
90,Required
90,compression
90,Specifies the compression type to use.
90,Optional
90,# Example configuration for migrating from a RocksDB cache store.
90,source.type=ROCKSDB
90,source.cache_name=myCache
90,source.location=/path/to/rocksdb/database
90,source.compression=SNAPPY
90,Table 6. SingleFileStore Properties
90,Property
90,Description
90,Required/Optional
90,location
90,Sets the directory that contains the cache store .dat file.
90,Required
90,# Example configuration for migrating to a Single File cache store.
90,target.type=SINGLE_FILE_STORE
90,target.cache_name=myCache
90,target.location=/path/to/sfs.dat
90,Table 7. SoftIndexFileStore Properties
90,Property
90,Description
90,Value
90,Required/Optional
90,location
90,Sets the database directory.
90,Required
90,index_location
90,Sets the database index directory.
90,# Example configuration for migrating to a Soft-Index File cache store.
90,target.type=SOFT_INDEX_FILE_STORE
90,target.cache_name=myCache
90,target.location=path/to/sifs/database
90,target.location=path/to/sifs/index
90,6.16.4. Migrating Infinispan cache stores
90,You can use the StoreMigrator to migrate data between cache stores with different Infinispan versions or to migrate data from one type of cache store to another.
90,Prerequisites
90,Have a
90,infinispan-tools.jar.
90,Have the source and target cache store configured in the migrator.properties file.
90,Procedure
90,"If you built the infinispan-tools.jar from the source code, do the following:"
90,Add infinispan-tools.jar to your classpath.
90,"Add dependencies for your source and target databases, such as JDBC drivers to your classpath."
90,Specify migrator.properties file as an argument for StoreMigrator.
90,"If you pulled infinispan-tools.jar from the Maven repository, run the following command:"
90,mvn exec:java
90,7. Configuring Infinispan to handle network partitions
90,Infinispan clusters can split into network partitions in which subsets of nodes become isolated from each other.
90,This condition results in loss of availability or consistency for clustered caches.
90,Infinispan automatically detects crashed nodes and resolves conflicts to merge caches back together.
90,7.1. Split clusters and network partitions
90,"Network partitions are the result of error conditions in the running environment, such as when a network router crashes."
90,"When a cluster splits into partitions, nodes create a JGroups cluster view that includes only the nodes in that partition."
90,This condition means that nodes in one partition can operate independently of nodes in the other partition.
90,Detecting a split
90,"To automatically detect network partitions, Infinispan uses the FD_ALL protocol in the default JGroups stack to determine when nodes leave the cluster abruptly."
90,Infinispan cannot detect what causes nodes to leave abruptly.
90,"This can happen not only when there is a network failure but also for other reasons, such as when Garbage Collection (GC) pauses the JVM."
90,Infinispan suspects that nodes have crashed after the following number of milliseconds:
90,FD_ALL[2|3].timeout + FD_ALL[2|3].interval + VERIFY_SUSPECT[2].timeout + GMS.view_ack_collection_timeout
90,"When it detects that the cluster is split into network partitions, Infinispan uses a strategy for handling cache operations."
90,Depending on your application requirements Infinispan can:
90,Allow read and/or write operations for availability
90,Deny read and write operations for consistency
90,Merging partitions together
90,"To fix a split cluster, Infinispan merges the partitions back together."
90,"During the merge, Infinispan uses the .equals() method for values of cache entries to determine if any conflicts exist."
90,"To resolve any conflicts between replicas it finds on partitions, Infinispan uses a merge policy that you can configure."
90,7.1.1. Data consistency in a split cluster
90,Network outages or errors that cause Infinispan clusters to split into partitions can result in data loss or consistency issues regardless of any handling strategy or merge policy.
90,Between the split and detection
90,"If a write operation takes place on a node that is in a minor partition when a split occurs, and before Infinispan detects the split, that value is lost when Infinispan transfers state to that minor partition during the merge."
90,In the event that all partitions are in the DEGRADED mode that value is not lost because no state transfer occurs but the entry can have an inconsistent value.
90,"For transactional caches write operations that are in progress when the split occurs can be committed on some nodes and rolled back on other nodes, which also results in inconsistent values."
90,"During the split and the time that Infinispan detects it, it is possible to get stale reads from a cache in a minor partition that has not yet entered DEGRADED mode."
90,During the merge
90,When Infinispan starts removing partitions nodes reconnect to the cluster with a series of merge events.
90,"Before this merge process completes it is possible that write operations on transactional caches succeed on some nodes but not others, which can potentially result in stale reads until the entries are updated."
90,7.2. Cache availability and degraded mode
90,"To preserve data consistency, Infinispan can put caches into DEGRADED mode if you configure them to use either the DENY_READ_WRITES or ALLOW_READS partition handling strategy."
90,Infinispan puts caches in a partition into DEGRADED mode when the following conditions are true:
90,At least one segment has lost all owners.
90,This happens when a number of nodes equal to or greater than the number of owners for a distributed cache have left the cluster.
90,There is not a majority of nodes in the partition.
90,"A majority of nodes is any number greater than half the total number of nodes in the cluster from the most recent stable topology, which was the last time a cluster rebalancing operation completed successfully."
90,"When caches are in DEGRADED mode, Infinispan:"
90,Allows read and write operations only if all replicas of an entry reside in the same partition.
90,Denies read and write operations and throws an AvailabilityException if the partition does not include all replicas of an entry.
90,"With the ALLOW_READS strategy, Infinispan allows read operations on caches in DEGRADED mode."
90,DEGRADED mode guarantees consistency by ensuring that write operations do not take place for the same key in different partitions.
90,Additionally DEGRADED mode prevents stale read operations that happen when a key is updated in one partition but read in another partition.
90,If all partitions are in DEGRADED mode then the cache becomes available again after merge only if the cluster contains a majority of nodes from the most recent stable topology and there is at least one replica of each entry.
90,"When the cluster has at least one replica of each entry, no keys are lost and Infinispan can create new replicas based on the number of owners during cluster rebalancing."
90,In some cases a cache in one partition can remain available while entering DEGRADED mode in another partition.
90,When this happens the available partition continues cache operations as normal and Infinispan attempts to rebalance data across those nodes.
90,To merge the cache together Infinispan always transfers state from the available partition to the partition in DEGRADED mode.
90,7.2.1. Degraded cache recovery example
90,This topic illustrates how Infinispan recovers from split clusters with caches that use the DENY_READ_WRITES partition handling strategy.
90,"As an example, a Infinispan cluster has four nodes and includes a distributed cache with two replicas for each entry (owners=2)."
90,"There are four entries in the cache, k1, k2, k3 and k4."
90,"With the DENY_READ_WRITES strategy, if the cluster splits into partitions, Infinispan allows cache operations only if all replicas of an entry are in the same partition."
90,"In the following diagram, while the cache is split into partitions, Infinispan allows read and write operations for k1 on partition 1 and k4 on partition 2."
90,"Because there is only one replica for k2 and k3 on either partition 1 or partition 2, Infinispan denies read and write operations for those entries."
90,"When network conditions allow the nodes to re-join the same cluster view, Infinispan merges the partitions without state transfer and restores normal cache operations."
90,7.2.2. Verifying cache availability during network partitions
90,Determine if caches on Infinispan clusters are in AVAILABLE mode or DEGRADED mode during a network partition.
90,"When Infinispan clusters split into partitions, nodes in those partitions can enter DEGRADED mode to guarantee data consistency."
90,In DEGRADED mode clusters do not allow cache operations resulting in loss of availability.
90,Procedure
90,Verify availability of clustered caches in network partitions in one of the following ways:
90,Check Infinispan logs for ISPN100011 messages that indicate if the cluster is available or if at least one cache is in DEGRADED mode.
90,Get the availability of remote caches through the Infinispan Console or with the REST API.
90,"Open the Infinispan Console in any browser, select the Data Container tab, and then locate the availability status in the Health column."
90,Retrieve cache health from the REST API.
90,GET /rest/v2/cache-managers/<cacheManagerName>/health
90,Programmatically retrieve the availability of embedded caches with the getAvailability() method in the AdvancedCache API.
90,Additional resources
90,REST API: Getting cluster health
90,org.infinispan.AdvancedCache.getAvailability
90,Enum AvailabilityMode
90,7.2.3. Making caches available
90,Make caches available for read and write operations by forcing them out of DEGRADED mode.
90,You should force clusters out of DEGRADED mode only if your deployment can tolerate data loss and inconsistency.
90,Procedure
90,Make caches available in one of the following ways:
90,Open the Infinispan Console and select the Make available option.
90,Change the availability of remote caches with the REST API.
90,POST /rest/v2/caches/<cacheName>?action=set-availability&availability=AVAILABLE
90,Programmatically change the availability of embedded caches with the AdvancedCache API.
90,AdvancedCache ac = cache.getAdvancedCache();
90,// Retrieve cache availability
90,boolean available = ac.getAvailability() == AvailabilityMode.AVAILABLE;
90,// Make the cache available
90,if (!available) {
90,ac.setAvailability(AvailabilityMode.AVAILABLE);
90,Additional resources
90,REST API: Setting cache availability
90,org.infinispan.AdvancedCache
90,7.3. Configuring partition handling
90,Configure Infinispan to use a partition handling strategy and merge policy so it can resolve split clusters when network issues occur.
90,By default Infinispan uses a strategy that provides availability at the cost of lowering consistency guarantees for your data.
90,When a cluster splits due to a network partition clients can continue to perform read and write operations on caches.
90,"If you require consistency over availability, you can configure Infinispan to deny read and write operations while the cluster is split into partitions."
90,Alternatively you can allow read operations and deny write operations.
90,You can also specify custom merge policy implementations that configure Infinispan to resolve splits with custom logic tailored to your requirements.
90,Prerequisites
90,Have a Infinispan cluster where you can create either a replicated or distributed cache.
90,Partition handling configuration applies only to replicated and distributed caches.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add partition handling configuration to your cache with either the partition-handling element or partitionHandling() method.
90,Specify a strategy for Infinispan to use when the cluster splits into partitions with the when-split attribute or whenSplit() method.
90,The default partition handling strategy is ALLOW_READ_WRITES so caches remain availabile.
90,"If your use case requires data consistency over cache availability, specify the DENY_READ_WRITES strategy."
90,Specify a policy that Infinispan uses to resolve conflicting entries when merging partitions with the merge-policy attribute or mergePolicy() method.
90,By default Infinispan does not resolve conflicts on merge.
90,Save the changes to your Infinispan configuration.
90,Partition handling configuration
90,XML
90,<distributed-cache>
90,"<partition-handling when-split=""DENY_READ_WRITES"""
90,"merge-policy=""PREFERRED_ALWAYS""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""partition-handling"" : {"
90,"""when-split"": ""DENY_READ_WRITES"","
90,"""merge-policy"": ""PREFERRED_ALWAYS"""
90,YAML
90,distributedCache:
90,partitionHandling:
90,whenSplit: DENY_READ_WRITES
90,mergePolicy: PREFERRED_ALWAYS
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
90,.partitionHandling()
90,.whenSplit(PartitionHandling.DENY_READ_WRITES)
90,.mergePolicy(MergePolicy.PREFERRED_NON_NULL);
90,7.4. Partition handling strategies
90,Partition handling strategies control if Infinispan allows read and write operations when a cluster is split.
90,The strategy you configure determines whether you get cache availability or data consistency.
90,Table 8. Partition handling strategies
90,Strategy
90,Description
90,Availability or consistency
90,ALLOW_READ_WRITES
90,Infinispan allows read and write operations on caches while a cluster is split into network partitions.
90,Nodes in each partition remain available and function independently of each other.
90,This is the default partition handling strategy.
90,Availability
90,DENY_READ_WRITES
90,Infinispan allows read and write operations only if all replicas of an entry are in the partition.
90,"If a partition does not include all replicas of an entry, Infinispan prevents cache operations for that entry."
90,Consistency
90,ALLOW_READS
90,Infinispan allows read operations for entries and prevents write operations unless the partition includes all replicas of an entry.
90,Consistency with read availability
90,7.5. Merge policies
90,Merge policies control how Infinispan resolves conflicts between replicas when bringing cluster partitions together.
90,You can use one of the merge policies that Infinispan provides or you can create a custom implementation of the EntryMergePolicy API.
90,Table 9. Infinispan merge policies
90,Merge policy
90,Description
90,Considerations
90,NONE
90,Infinispan does not resolve conflicts when merging split clusters. This is the default merge policy.
90,"Nodes drop segments for which they are not the primary owner, which can result in data loss."
90,PREFERRED_ALWAYS
90,Infinispan finds the value that exists on the majority of nodes in the cluster and uses it to resolve conflicts.
90,"Infinispan could use stale values to resolve conflicts. Even if an entry is available the majority of nodes, the last update could happen on the minority partition."
90,PREFERRED_NON_NULL
90,Infinispan uses the first non-null value that it finds on the cluster to resolve conflicts.
90,Infinispan could restore deleted entries.
90,REMOVE_ALL
90,Infinispan removes any conflicting entries from the cache.
90,Results in loss of any entries that have different values when merging split clusters.
90,7.6. Configuring custom merge policies
90,Configure Infinispan to use custom implementations of the EntryMergePolicy API when handling network partitions.
90,Prerequisites
90,Implement the EntryMergePolicy API.
90,"public class CustomMergePolicy implements EntryMergePolicy<String, String> {"
90,@Override
90,"public CacheEntry<String, String> merge(CacheEntry<String, String> preferredEntry, List<CacheEntry<String, String>> otherEntries) {"
90,// Decide which entry resolves the conflict
90,return the_solved_CacheEntry;
90,Procedure
90,Deploy your merge policy implementation to Infinispan Server if you use remote caches.
90,Package your classes as a JAR file that includes a META-INF/services/org.infinispan.conflict.EntryMergePolicy file that contains the fully qualified class name of your merge policy.
90,# List implementations of EntryMergePolicy with the full qualified class name
90,org.example.CustomMergePolicy
90,Add the JAR file to the server/lib directory.
90,Use the install command with the Infinispan Command Line Interface (CLI) to download the JAR to the server/lib directory.
90,Open your Infinispan configuration for editing.
90,Configure cache encoding with the encoding element or encoding() method as appropriate.
90,"For remote caches, if you use only object metadata for comparison when merging entries then you can use application/x-protostream as the media type. In this case Infinispan returns entries to the EntryMergePolicy as byte[]."
90,If you require the object itself when merging conflicts then you should configure caches with the application/x-java-object media type. In this case you must deploy the relevant ProtoStream marshallers to Infinispan Server so it can perform byte[] to object transformations if clients use Protobuf encoding.
90,Specify your custom merge policy with the merge-policy attribute or mergePolicy() method as part of the partition handling configuration.
90,Save your changes.
90,Custom merge policy configuration
90,XML
90,"<distributed-cache name=""mycache"">"
90,"<partition-handling when-split=""DENY_READ_WRITES"""
90,"merge-policy=""org.example.CustomMergePolicy""/>"
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""partition-handling"" : {"
90,"""when-split"": ""DENY_READ_WRITES"","
90,"""merge-policy"": ""org.example.CustomMergePolicy"""
90,YAML
90,distributedCache:
90,partitionHandling:
90,whenSplit: DENY_READ_WRITES
90,mergePolicy: org.example.CustomMergePolicy
90,ConfigurationBuilder
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.clustering().cacheMode(CacheMode.DIST_SYNC)
90,.partitionHandling()
90,.whenSplit(PartitionHandling.DENY_READ_WRITES)
90,.mergePolicy(new CustomMergePolicy());
90,Additional resources
90,org.infinispan.conflict.EntryMergePolicy
90,7.7. Manually merging partitions in embedded caches
90,Detect and resolve conflicting entries to manually merge embedded caches after network partitions occur.
90,Procedure
90,"Retrieve the ConflictManager from the EmbeddedCacheManager to detect and resolve conflicting entries in a cache, as in the following example:"
90,"EmbeddedCacheManager manager = new DefaultCacheManager(""example-config.xml"");"
90,"Cache<Integer, String> cache = manager.getCache(""testCache"");"
90,"ConflictManager<Integer, String> crm = ConflictManagerFactory.get(cache.getAdvancedCache());"
90,// Get all versions of a key
90,"Map<Address, InternalCacheValue<String>> versions = crm.getAllVersions(1);"
90,// Process conflicts stream and perform some operation on the cache
90,"Stream<Map<Address, CacheEntry<Integer, String>>> conflicts = crm.getConflicts();"
90,conflicts.forEach(map -> {
90,"CacheEntry<Integer, String> entry = map.values().iterator().next();"
90,Object conflictKey = entry.getKey();
90,cache.remove(conflictKey);
90,});
90,// Detect and then resolve conflicts using the configured EntryMergePolicy
90,crm.resolveConflicts();
90,// Detect and then resolve conflicts using the passed EntryMergePolicy instance
90,"crm.resolveConflicts((preferredEntry, otherEntries) -> preferredEntry);"
90,"Although the ConflictManager::getConflicts stream is processed per entry, the underlying spliterator lazily loads cache entries on a per segment basis."
90,8. Security authorization with role-based access control
90,Role-based access control (RBAC) capabilities use different permissions levels to restrict user interactions with Infinispan.
90,"For information on creating users and configuring authorization specific to remote or embedded caches, see:"
90,Configuring user roles and permissions with Infinispan Server
90,Programmatically configuring user roles and permissions
90,8.1. Infinispan user roles and permissions
90,Infinispan includes several roles that provide users with permissions to access caches and Infinispan resources.
90,Role
90,Permissions
90,Description
90,admin
90,ALL
90,Superuser with all permissions including control of the Cache Manager lifecycle.
90,deployer
90,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR, CREATE"
90,Can create and delete Infinispan resources in addition to application permissions.
90,application
90,"ALL_READ, ALL_WRITE, LISTEN, EXEC, MONITOR"
90,Has read and write access to Infinispan resources in addition to observer permissions. Can also listen to events and execute server tasks and scripts.
90,observer
90,"ALL_READ, MONITOR"
90,Has read access to Infinispan resources in addition to monitor permissions.
90,monitor
90,MONITOR
90,Can view statistics via JMX and the metrics endpoint.
90,Additional resources
90,org.infinispan.security.AuthorizationPermission Enum
90,Infinispan configuration schema reference
90,8.1.1. Permissions
90,User roles are sets of permissions with different access levels.
90,Table 10. Cache Manager permissions
90,Permission
90,Function
90,Description
90,CONFIGURATION
90,defineConfiguration
90,Defines new cache configurations.
90,LISTEN
90,addListener
90,Registers listeners against a Cache Manager.
90,LIFECYCLE
90,stop
90,Stops the Cache Manager.
90,CREATE
90,"createCache, removeCache"
90,Create and remove container resources
90,"such as caches, counters, schemas, and scripts."
90,MONITOR
90,getStats
90,Allows access to JMX statistics and the metrics endpoint.
90,ALL
90,Includes all Cache Manager permissions.
90,Table 11. Cache permissions
90,Permission
90,Function
90,Description
90,READ
90,"get, contains"
90,Retrieves entries from a cache.
90,WRITE
90,"put, putIfAbsent, replace, remove, evict"
90,"Writes, replaces, removes, evicts data in a cache."
90,EXEC
90,"distexec, streams"
90,Allows code execution against a cache.
90,LISTEN
90,addListener
90,Registers listeners against a cache.
90,BULK_READ
90,"keySet, values, entrySet, query"
90,Executes bulk retrieve operations.
90,BULK_WRITE
90,"clear, putAll"
90,Executes bulk write operations.
90,LIFECYCLE
90,"start, stop"
90,Starts and stops a cache.
90,ADMIN
90,"getVersion, addInterceptor*, removeInterceptor, getInterceptorChain, getEvictionManager, getComponentRegistry, getDistributionManager, getAuthorizationManager, evict, getRpcManager, getCacheConfiguration, getCacheManager, getInvocationContextContainer, setAvailability, getDataContainer, getStats, getXAResource"
90,Allows access to underlying components and internal structures.
90,MONITOR
90,getStats
90,Allows access to JMX statistics and the metrics endpoint.
90,ALL
90,Includes all cache permissions.
90,ALL_READ
90,Combines the READ and BULK_READ permissions.
90,ALL_WRITE
90,Combines the WRITE and BULK_WRITE permissions.
90,Additional resources
90,Infinispan Security API
90,8.1.2. Role and permission mappers
90,Infinispan implements users as a collection of principals.
90,"Principals represent either an individual user identity, such as a username, or a group to which the users belong. Internally, these are implemented with the javax.security.auth.Subject class."
90,"To enable authorization, the principals must be mapped to role names, which are then expanded into a set of permissions."
90,"Infinispan includes the PrincipalRoleMapper API for associating security principals to roles, and the RolePermissionMapper API for associating roles with specific permissions."
90,Infinispan provides the following role and permission mapper implementations:
90,Cluster role mapper
90,Stores principal to role mappings in the cluster registry.
90,Cluster permission mapper
90,Stores role to permission mappings in the cluster registry. Allows you to dynamically modify user roles and permissions.
90,Identity role mapper
90,"Uses the principal name as the role name. The type or format of the principal name depends on the source. For example, in an LDAP directory the principal name could be a Distinguished Name (DN)."
90,Common name role mapper
90,"Uses the Common Name (CN) as the role name. You can use this role mapper with an LDAP directory or with client certificates that contain Distinguished Names (DN); for example cn=managers,ou=people,dc=example,dc=com maps to the managers role."
90,Mapping users to roles and permissions in Infinispan
90,"Consider the following user retrieved from an LDAP server, as a collection of DNs:"
90,"CN=myapplication,OU=applications,DC=mycompany"
90,"CN=dataprocessors,OU=groups,DC=mycompany"
90,"CN=finance,OU=groups,DC=mycompany"
90,"Using the Common name role mapper, the user would be mapped to the following roles:"
90,dataprocessors
90,finance
90,Infinispan has the following role definitions:
90,dataprocessors: ALL_WRITE ALL_READ
90,finance: LISTEN
90,The user would have the following permissions:
90,ALL_WRITE ALL_READ LISTEN
90,Additional resources
90,Infinispan Security API
90,org.infinispan.security.PrincipalRoleMapper
90,org.infinispan.security.RolePermissionMapper
90,org.infinispan.security.mappers.IdentityRoleMapper
90,org.infinispan.security.mappers.CommonNameRoleMapper
90,8.1.3. Configuring role mappers
90,Infinispan enables the cluster role mapper and cluster permission mapper by default.
90,"To use a different implementation for role mapping, you must configure the role mappers."
90,Procedure
90,Open your Infinispan configuration for editing.
90,Declare the role mapper as part of the security authorization in the Cache Manager configuration.
90,Save the changes to your configuration.
90,With embedded caches you can programmatically configure role and permission mappers with the principalRoleMapper() and rolePermissionMapper() methods.
90,Role mapper configuration
90,XML
90,<cache-container>
90,<security>
90,<authorization>
90,<common-name-role-mapper />
90,</authorization>
90,</security>
90,</cache-container>
90,JSON
90,"""infinispan"" : {"
90,"""cache-container"" : {"
90,"""security"" : {"
90,"""authorization"" : {"
90,"""common-name-role-mapper"": {}"
90,YAML
90,infinispan:
90,cacheContainer:
90,security:
90,authorization:
90,commonNameRoleMapper: ~
90,Additional resources
90,Infinispan configuration schema reference
90,8.2. Configuring caches with security authorization
90,Add security authorization to caches to enforce role-based access control (RBAC).
90,This requires Infinispan users to have a role with a sufficient level of permission to perform cache operations.
90,Prerequisites
90,Create Infinispan users and either grant them with roles or assign them to groups.
90,Procedure
90,Open your Infinispan configuration for editing.
90,Add a security section to the configuration.
90,Specify roles that users must have to perform cache operations with the authorization element.
90,You can implicitly add all roles defined in the Cache Manager or explicitly define a subset of roles.
90,Save the changes to your configuration.
90,Implicit role configuration
90,The following configuration implicitly adds every role defined in the Cache Manager:
90,XML
90,<distributed-cache>
90,<security>
90,<authorization/>
90,</security>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""security"": {"
90,"""authorization"": {"
90,"""enabled"": true"
90,YAML
90,distributedCache:
90,security:
90,authorization:
90,enabled: true
90,Explicit role configuration
90,The following configuration explicitly adds a subset of roles defined in the Cache Manager.
90,In this case Infinispan denies cache operations for any users that do not have one of the configured roles.
90,XML
90,<distributed-cache>
90,<security>
90,"<authorization roles=""admin supervisor""/>"
90,</security>
90,</distributed-cache>
90,JSON
90,"""distributed-cache"": {"
90,"""security"": {"
90,"""authorization"": {"
90,"""enabled"": true,"
90,"""roles"": [""admin"",""supervisor""]"
90,YAML
90,distributedCache:
90,security:
90,authorization:
90,enabled: true
90,"roles: [""admin"",""supervisor""]"
90,9. Configuring transactions
90,"Data that resides on a distributed system is vulnerable to errors that can arise from temporary network outages, system failures, or just simple human error."
90,These external factors are uncontrollable but can have serious consequences for quality of your data.
90,The effects of data corruption range from lower customer satisfaction to costly system reconciliation that results in service unavailability.
90,"Infinispan can carry out ACID (atomicity, consistency, isolation, durability) transactions to ensure the cache state is consistent."
90,9.1. Transactions
90,Infinispan can be configured to use and to participate in JTA compliant transactions.
90,"Alternatively, if transaction support is disabled, it is equivalent to using autocommit in JDBC calls, where modifications are potentially replicated after every change (if replication is enabled)."
90,On every cache operation Infinispan does the following:
90,Retrieves the current Transaction associated with the thread
90,"If not already done, registers XAResource with the transaction manager to be notified when a transaction commits or is rolled back."
90,"In order to do this, the cache has to be provided with a reference to the environment’s TransactionManager."
90,This is usually done by configuring the cache with the class name of an implementation of the TransactionManagerLookup interface.
90,"When the cache starts, it will create an instance of this class and invoke its getTransactionManager() method, which returns a reference to the TransactionManager."
90,Infinispan ships with several transaction manager lookup classes:
90,Transaction manager lookup implementations
90,EmbeddedTransactionManagerLookup:
90,This provides with a basic transaction manager which should only be used for embedded mode when no other implementation is available.
90,This implementation has some severe limitations to do with concurrent transactions and recovery.
90,JBossStandaloneJTAManagerLookup:
90,"If you’re running Infinispan in a standalone environment, or in JBoss AS 7 and earlier, and WildFly 8, 9, and 10, this should be your default choice for transaction manager."
90,It’s a fully fledged transaction manager based on JBoss Transactions which overcomes all the deficiencies of the EmbeddedTransactionManager.
90,WildflyTransactionManagerLookup:
90,"If you’re running Infinispan in WildFly 11 or later, this should be your default choice for transaction manager."
90,GenericTransactionManagerLookup:
90,This is a lookup class that locate transaction managers in the most popular Java EE application servers.
90,"If no transaction manager can be found, it defaults on the EmbeddedTransactionManager."
90,"Once initialized, the TransactionManager can also be obtained from the Cache itself:"
90,//the cache must have a transactionManagerLookupClass defined
90,Cache cache = cacheManager.getCache();
90,//equivalent with calling TransactionManagerLookup.getTransactionManager();
90,TransactionManager tm = cache.getAdvancedCache().getTransactionManager();
90,9.1.1. Configuring transactions
90,Transactions are configured at cache level.
90,Below is the configuration that affects a transaction behaviour and a small description of each configuration attribute.
90,<locking
90,"isolation=""READ_COMMITTED""/>"
90,<transaction
90,"locking=""OPTIMISTIC"""
90,"auto-commit=""true"""
90,"complete-timeout=""60000"""
90,"mode=""NONE"""
90,"notifications=""true"""
90,"reaper-interval=""30000"""
90,"recovery-cache=""__recoveryInfoCacheName__"""
90,"stop-timeout=""30000"""
90,"transaction-manager-lookup=""org.infinispan.transaction.lookup.GenericTransactionManagerLookup""/>"
90,or programmatically:
90,ConfigurationBuilder builder = new ConfigurationBuilder();
90,builder.locking()
90,.isolationLevel(IsolationLevel.READ_COMMITTED);
90,builder.transaction()
90,.lockingMode(LockingMode.OPTIMISTIC)
90,.autoCommit(true)
90,.completedTxTimeout(60000)
90,.transactionMode(TransactionMode.NON_TRANSACTIONAL)
90,.useSynchronization(false)
90,.notifications(true)
90,.reaperWakeUpInterval(30000)
90,.cacheStopTimeout(30000)
90,.transactionManagerLookup(new GenericTransactionManagerLookup())
90,.recovery()
90,.enabled(false)
90,".recoveryInfoCacheName(""__recoveryInfoCacheName__"");"
90,isolation - configures the isolation level. Check section Isolation Levels for more details.
90,Default is REPEATABLE_READ.
90,locking - configures whether the cache uses optimistic or pessimistic locking. Check section Transaction Locking for more details.
90,Default is OPTIMISTIC.
90,"auto-commit - if enable, the user does not need to start a transaction manually for a single operation. The transaction is automatically started and committed."
90,Default is true.
90,complete-timeout - the duration in milliseconds to keep information about completed transactions. Default is 60000.
90,mode - configures whether the cache is transactional or not. Default is NONE. The available options are:
90,NONE - non transactional cache
90,FULL_XA - XA transactional cache with recovery enabled. Check section Transaction recovery for more details about recovery.
90,NON_DURABLE_XA - XA transactional cache with recovery disabled.
90,NON_XA - transactional cache with integration via Synchronization instead of XA.
90,Check section Enlisting Synchronizations for details.
90,BATCH-
90,transactional cache using batch to group operations. Check section Batching for details.
90,notifications - enables/disables triggering transactional events in cache listeners. Default is true.
90,reaper-interval - the time interval in millisecond at which the thread that cleans up transaction completion information kicks in.
90,Defaults is 30000.
90,recovery-cache - configures the cache name to store the recovery information. Check section Transaction recovery for more details about recovery.
90,Default is recoveryInfoCacheName.
90,stop-timeout - the time in millisecond to wait for ongoing transaction when the cache is stopping. Default is
90,30000.
90,transaction-manager-lookup - configures the fully qualified class name of a class that looks up a reference to a javax.transaction.TransactionManager.
90,Default is org.infinispan.transaction.lookup.GenericTransactionManagerLookup.
90,For more details on how Two-Phase-Commit (2PC) is implemented in Infinispan and how locks are being acquired see the section below.
90,More details about the configuration settings are available in Configuration reference.
90,9.1.2. Isolation levels
90,Infinispan offers two isolation levels - READ_COMMITTED and REPEATABLE_READ.
90,"These isolation levels determine when readers see a concurrent write, and are internally implemented using different subclasses of MVCCEntry, which have different behaviour in how state is committed back to the data container."
90,Here’s a more detailed example that should help understand the difference between READ_COMMITTED and REPEATABLE_READ in the context of Infinispan.
90,"With READ_COMMITTED, if between two consecutive read calls on the same key, the key has been updated by another transaction, the second read may return the new updated value:"
90,Thread1: tx1.begin()
90,Thread1: cache.get(k) // returns v
90,Thread2:
90,tx2.begin()
90,Thread2:
90,cache.get(k) // returns v
90,Thread2:
90,"cache.put(k, v2)"
90,Thread2:
90,tx2.commit()
90,Thread1: cache.get(k) // returns v2!
90,Thread1: tx1.commit()
90,"With REPEATABLE_READ, the final get will still return v."
90,"So, if you’re going to retrieve the same key multiple times within a transaction, you should use REPEATABLE_READ."
90,"However, as read-locks are not acquired even for REPEATABLE_READ, this phenomena can occur:"
90,"cache.get(""A"") // returns 1"
90,"cache.get(""B"") // returns 1"
90,Thread1: tx1.begin()
90,"Thread1: cache.put(""A"", 2)"
90,"Thread1: cache.put(""B"", 2)"
90,Thread2:
90,tx2.begin()
90,Thread2:
90,"cache.get(""A"") // returns 1"
90,Thread1: tx1.commit()
90,Thread2:
90,"cache.get(""B"") // returns 2"
90,Thread2:
90,tx2.commit()
90,9.1.3. Transaction locking
90,Pessimistic transactional cache
90,"From a lock acquisition perspective, pessimistic transactions obtain locks on keys at the time the key is written."
90,A lock request is sent to the primary owner (can be an explicit lock request or an operation)
90,The primary owner tries to acquire the lock:
90,"If it succeed, it sends back a positive reply;"
90,"Otherwise, a negative reply is sent and the transaction is rollback."
90,As an example:
90,transactionManager.begin();
90,"cache.put(k1,v1); //k1 is locked."
90,cache.remove(k2); //k2 is locked when this returns
90,transactionManager.commit();
90,"When cache.put(k1,v1) returns, k1 is locked and no other transaction running anywhere in the cluster can write to it."
90,Reading k1 is still possible.
90,The lock on k1 is released when the transaction completes (commits or rollbacks).
90,"For conditional operations, the validation is performed in the originator."
90,Optimistic transactional cache
90,With optimistic transactions locks are being acquired at transaction prepare time and are only being held up to the point the transaction commits (or rollbacks).
90,This is different from the 5.0 default locking model where local locks are being acquire on writes and cluster locks are being acquired during prepare time.
90,The prepare is sent to all the owners.
90,The primary owners try to acquire the locks needed:
90,"If locking succeeds, it performs the write skew check."
90,"If the write skew check succeeds (or is disabled), send a positive reply."
90,"Otherwise, a negative reply is sent and the transaction is rolled back."
90,As an example:
90,transactionManager.begin();
90,"cache.put(k1,v1);"
90,cache.remove(k2);
90,"transactionManager.commit(); //at prepare time, K1 and K2 is locked until committed/rolled back."
90,"For conditional commands, the validation still happens on the originator."
90,What do I need - pessimistic or optimistic transactions?
90,"From a use case perspective, optimistic transactions should be used when there is not a lot of contention between multiple transactions running at the same time."
90,That is because the optimistic transactions rollback if data has changed between the time it was read and the time it was committed (with write skew check enabled).
90,"On the other hand, pessimistic transactions might be a better fit when there is high contention on the keys and transaction rollbacks are less desirable."
90,Pessimistic transactions are more costly by their nature: each write operation potentially involves a RPC for lock acquisition.
90,9.1.4. Write Skews
90,Write skews occur when two transactions independently and simultaneously read and write to the same key. The result of a write skew is that both transactions successfully commit updates to the same key but with different values.
90,Infinispan automatically performs write skew checks to ensure data consistency for REPEATABLE_READ isolation levels in optimistic transactions. This allows Infinispan to detect and roll back one of the transactions.
90,"When operating in LOCAL mode, write skew checks rely on Java object"
90,"references to compare differences, which provides a reliable technique for"
90,checking for write skews.
90,Forcing write locks on keys in pessimitic transactions
90,"To avoid write skews with pessimistic transactions, lock keys at read-time with Flag.FORCE_WRITE_LOCK."
90,"In non-transactional caches, Flag.FORCE_WRITE_LOCK does not work. The get() call reads the key value but does not acquire locks remotely."
90,You should use Flag.FORCE_WRITE_LOCK with transactions in which the entity is updated later in the same transaction.
90,Compare the following code snippets for an example of Flag.FORCE_WRITE_LOCK:
90,// begin the transaction
90,if (!cache.getAdvancedCache().lock(key)) {
90,// abort the transaction because the key was not locked
90,} else {
90,cache.get(key);
90,"cache.put(key, value);"
90,// commit the transaction
90,// begin the transaction
90,try {
90,// throws an exception if the key is not locked.
90,cache.getAdvancedCache().withFlags(Flag.FORCE_WRITE_LOCK).get(key);
90,"cache.put(key, value);"
90,} catch (CacheException e) {
90,// mark the transaction rollback-only
90,// commit or rollback the transaction
90,9.1.5. Dealing with exceptions
90,"If a CacheException (or a subclass of it) is thrown by a cache method within the scope of a JTA transaction, then the transaction is automatically marked for rollback."
90,9.1.6. Enlisting Synchronizations
90,By default Infinispan registers itself as a first class participant in distributed transactions through XAResource.
90,"There are situations where Infinispan is not required to be a participant in the transaction, but only to be notified by its lifecycle (prepare, complete): e.g. in the case Infinispan is used as a 2nd level cache in Hibernate."
90,Infinispan allows transaction enlistment through Synchronization.
90,To enable it just use NON_XA transaction mode.
90,Synchronizations have the advantage that they allow TransactionManager to optimize 2PC with a 1PC where only one other resource is enlisted with that transaction (last resource commit optimization).
90,"E.g. Hibernate second level cache: if Infinispan registers itself with the TransactionManager as a XAResource than at commit time, the TransactionManager sees two XAResource (cache and database) and does not make this optimization."
90,Having to coordinate between two resources it needs to write the tx log to disk.
90,"On the other hand, registering Infinispan as a Synchronization makes the TransactionManager skip writing the log to the disk (performance improvement)."
90,9.1.7. Batching
90,"Batching allows atomicity and some characteristics of a transaction, but not full-blown JTA or XA capabilities."
90,Batching is often a lot lighter and cheaper than a full-blown transaction.
90,"Generally speaking, one should use batching API whenever the only participant in the transaction is an Infinispan cluster."
90,"On the other hand, JTA transactions (involving TransactionManager) should be used whenever the transactions involves multiple systems."
90,"E.g. considering the ""Hello world!"" of transactions: transferring money from one bank account to the other."
90,"If both accounts are stored within Infinispan, then batching can be used."
90,"If one account is in a database and the other is Infinispan, then distributed transactions are required."
90,You do not have to have a transaction manager defined to use batching.
90,API
90,"Once you have configured your cache to use batching, you use it by calling startBatch() and endBatch() on Cache. E.g.,"
90,Cache cache = cacheManager.getCache();
90,// not using a batch
90,"cache.put(""key"", ""value""); // will replicate immediately"
90,// using a batch
90,cache.startBatch();
90,"cache.put(""k1"", ""value"");"
90,"cache.put(""k2"", ""value"");"
90,"cache.put(""k2"", ""value"");"
90,cache.endBatch(true); // This will now replicate the modifications since the batch was started.
90,// a new batch
90,cache.startBatch();
90,"cache.put(""k1"", ""value"");"
90,"cache.put(""k2"", ""value"");"
90,"cache.put(""k3"", ""value"");"
90,"cache.endBatch(false); // This will ""discard"" changes made in the batch"
90,Batching and JTA
90,"Behind the scenes, the batching functionality starts a JTA transaction, and all the invocations in that scope are associated with it."
90,For this it uses a very simple (e.g. no recovery) internal TransactionManager implementation.
90,"With batching, you get:"
90,Locks you acquire during an invocation are held until the batch completes
90,Changes are all replicated around the cluster in a batch as part of the batch completion process. Reduces replication chatter for each update in the batch.
90,"If synchronous replication or invalidation are used, a failure in replication/invalidation will cause the batch to roll back."
90,All the transaction related configurations apply for batching as well.
90,9.1.8. Transaction recovery
90,"Recovery is a feature of XA transactions, which deal with the eventuality of a resource or possibly even the transaction manager failing, and recovering accordingly from such a situation."
90,When to use recovery
90,Consider a distributed transaction in which money is transferred from an account stored in an external database to an account stored in Infinispan.
90,"When TransactionManager.commit() is invoked, both resources prepare successfully (1st phase). During the commit (2nd) phase, the database successfully applies the changes whilst Infinispan fails before receiving the commit request from the transaction manager."
90,At this point the system is in an inconsistent state: money is taken from the account in the external database but not visible yet in Infinispan (since locks are only released during 2nd phase of a two-phase commit protocol).
90,Recovery deals with this situation to make sure data in both the database and Infinispan ends up in a consistent state.
90,How does it work
90,Recovery is coordinated by the transaction manager.
90,"The transaction manager works with Infinispan to determine the list of in-doubt transactions that require manual intervention and informs the system administrator (via email, log alerts, etc)."
90,"This process is transaction manager specific, but generally requires some configuration on the transaction manager."
90,"Knowing the in-doubt transaction ids, the system administrator can now connect to the Infinispan cluster and replay the commit of transactions or force the rollback."
90,Infinispan provides JMX tooling for this - this is explained extensively in the Transaction recovery and reconciliation section.
90,Configuring recovery
90,Recovery is not enabled by default in Infinispan.
90,"If disabled, the TransactionManager won’t be able to work with Infinispan to determine the in-doubt transactions."
90,The Transaction configuration section shows how to enable it.
90,NOTE: recovery-cache attribute is not mandatory and it is configured per-cache.
90,"For recovery to work, mode must be set to FULL_XA, since full-blown XA transactions are needed."
90,Enable JMX support
90,In order to be able to use JMX for managing recovery JMX support must be explicitly enabled.
90,Recovery cache
90,"In order to track in-doubt transactions and be able to reply them, Infinispan caches all transaction state for future use."
90,"This state is held only for in-doubt transaction, being removed for successfully completed transactions after when the commit/rollback phase completed."
90,This in-doubt transaction data is held within a local cache: this allows one to configure swapping this info to disk through cache loader in the case it gets too big.
90,This cache can be specified through the recovery-cache configuration attribute.
90,If not specified Infinispan will configure a local cache for you.
90,It is possible (though not mandated) to share same recovery cache between all the Infinispan caches that have recovery enabled.
90,"If the default recovery cache is overridden, then the specified recovery cache must use a TransactionManagerLookup that returns a different transaction manager than the one used by the cache itself."
90,Integration with the transaction manager
90,"Even though this is transaction manager specific, generally a transaction manager would need a reference to a XAResource implementation in order to invoke XAResource.recover() on it."
90,In order to obtain a reference to an Infinispan XAResource following API can be used:
90,XAResource xar = cache.getAdvancedCache().getXAResource();
90,It is a common practice to run the recovery in a different process from the one running the transaction.
90,Reconciliation
90,The transaction manager informs the system administrator on in-doubt transaction in a proprietary way.
90,At this stage it is assumed that the system administrator knows transaction’s XID (a byte array).
90,A normal recovery flow is:
90,"STEP 1: The system administrator connects to an Infinispan server through JMX, and lists the in doubt transactions."
90,The image below demonstrates JConsole connecting to an Infinispan node that has an in doubt transaction.
90,Figure 8. Show in-doubt transactions
90,"The status of each in-doubt transaction is displayed(in this example "" PREPARED "")."
90,"There might be multiple elements in the status field, e.g. ""PREPARED"" and ""COMMITTED"" in the case the transaction committed on certain nodes but not on all of them."
90,"STEP 2: The system administrator visually maps the XID received from the transaction manager to an Infinispan internal id, represented as a number."
90,"This step is needed because the XID, a byte array, cannot conveniently be passed to the JMX tool (e.g. JConsole) and then re-assembled on Infinispan’s side."
90,"STEP 3: The system administrator forces the transaction’s commit/rollback through the corresponding jmx operation, based on the internal id."
90,The image below is obtained by forcing the commit of the transaction based on its internal id.
90,Figure 9. Force commit
90,"All JMX operations described above can be executed on any node, regardless of where the transaction originated."
90,Force commit/rollback based on XID
90,XID-based JMX operations for forcing in-doubt transactions' commit/rollback are available as well: these methods receive byte[] arrays describing the XID instead of the number associated with the transactions (as previously described at step 2).
90,These can be useful e.g. if one wants to set up an automatic completion job for certain in-doubt transactions.
90,This process is plugged into transaction manager’s recovery and has access to the transaction manager’s XID objects.
90,10. Configuring locking and concurrency
90,Infinispan uses multi-versioned concurrency control (MVCC) to improve access to shared data.
90,Allowing concurrent readers and writers
90,Readers and writers do not block one another
90,Write skews can be detected and handled
90,Internal locks can be striped
90,10.1. Locking and concurrency
90,Multi-versioned concurrency control (MVCC) is a concurrency scheme popular with relational databases and other data stores.
90,MVCC offers many advantages over coarse-grained Java synchronization and even JDK Locks for access to shared data.
90,"Infinispan’s MVCC implementation makes use of minimal locks and synchronizations, leaning heavily towards lock-free techniques such as compare-and-swap and lock-free data structures wherever possible, which helps optimize for multi-CPU and multi-core environments."
90,"In particular, Infinispan’s MVCC implementation is heavily optimized for readers."
90,"Reader threads do not acquire explicit locks for entries, and instead directly read the entry in question."
90,"Writers, on the other hand, need to acquire a write lock."
90,"This ensures only one concurrent writer per entry, causing concurrent writers to queue up to change an entry."
90,"To allow concurrent reads, writers make a copy of the entry they intend to modify, by wrapping the entry in an MVCCEntry."
90,This copy isolates concurrent readers from seeing partially modified state.
90,"Once a write has completed, MVCCEntry.commit() will flush changes to the data container and subsequent readers will see the changes written."
90,10.1.1. Clustered caches and locks
90,"In Infinispan clusters, primary owner nodes are responsible for locking keys."
90,"For non-transactional caches, Infinispan forwards the write operation to the primary owner of the key so it can attempt to lock it."
90,Infinispan either then forwards the write operation to the other owners or throws an exception if it cannot lock the key.
90,"If the operation is conditional and fails on the primary owner, Infinispan does not forward it to the other owners."
90,"For transactional caches, primary owners can lock keys with optimistic and pessimistic locking modes."
90,Infinispan also supports different isolation levels to control concurrent reads between transactions.
90,10.1.2. The LockManager
90,The LockManager is a component that is responsible for locking an entry for writing.
90,The LockManager makes use of a LockContainer to locate/hold/create locks.
90,"LockContainers come in two broad flavours, with support for lock striping and with support for one lock per entry."
90,10.1.3. Lock striping
90,"Lock striping entails the use of a fixed-size, shared collection of locks for the entire cache, with locks being allocated to entries based on the entry’s key’s hash code."
90,"Similar to the way the JDK’s ConcurrentHashMap allocates locks, this allows for a highly scalable, fixed-overhead locking mechanism in exchange for potentially unrelated entries being blocked by the same lock."
90,The alternative is to disable lock striping - which would mean a new lock is created per entry.
90,"This approach may give you greater concurrent throughput, but it will be at the cost of additional memory usage, garbage collection churn, etc."
90,Default lock striping settings
90,"lock striping is disabled by default, due to potential deadlocks that can happen if locks for different keys end up in the same lock stripe."
90,The size of the shared lock collection used by lock striping can be tuned using the concurrencyLevel attribute of the <locking /> configuration element.
90,Configuration example:
90,"<locking striping=""false|true""/>"
90,new ConfigurationBuilder().locking().useLockStriping(false|true);
90,10.1.4. Concurrency levels
90,"In addition to determining the size of the striped lock container, this concurrency level is also used to tune any JDK ConcurrentHashMap based collections where related, such as internal to DataContainers."
90,"Please refer to the JDK ConcurrentHashMap Javadocs for a detailed discussion of concurrency levels, as this parameter is used in exactly the same way in Infinispan."
90,Configuration example:
90,"<locking concurrency-level=""32""/>"
90,new ConfigurationBuilder().locking().concurrencyLevel(32);
90,10.1.5. Lock timeout
90,"The lock timeout specifies the amount of time, in milliseconds, to wait for a contented lock."
90,Configuration example:
90,"<locking acquire-timeout=""10000""/>"
90,new ConfigurationBuilder().locking().lockAcquisitionTimeout(10000);
90,//alternatively
90,"new ConfigurationBuilder().locking().lockAcquisitionTimeout(10, TimeUnit.SECONDS);"
90,10.1.6. Consistency
90,The fact that a single owner is locked (as opposed to all owners being locked) does not break the following consistency guarantee:
90,"if key K is hashed to nodes {A, B} and transaction TX1 acquires a lock for K, let’s say on A."
90,"If another transaction, TX2, is started on B (or any other node) and TX2 tries to lock K then it will fail with a timeout as the lock is already held by TX1."
90,"The reason for this is the that the lock for a key K is always, deterministically, acquired on the same node of the cluster, regardless of where the transaction originates."
90,10.1.7. Data Versioning
90,Infinispan supports two forms of data versioning: simple and external.
90,The simple versioning is used in transactional caches for write skew check.
90,"The external versioning is used to encapsulate an external source of data versioning within Infinispan, such as when using Infinispan with Hibernate which in turn gets its data version information directly from a database."
90,"In this scheme, a mechanism to pass in the version becomes necessary, and overloaded versions of put() and putForExternalRead() will be provided in AdvancedCache to take in an external data version."
90,This is then stored on the InvocationContext and applied to the entry at commit time.
90,Write skew checks cannot and will not be performed in the case of external data versioning.
90,11. Using clustered counters
90,Infinispan provides counters that record the count of objects and are distributed across all nodes in a cluster.
90,11.1. Clustered Counters
90,Clustered counters are counters which are distributed and shared among all nodes in the Infinispan cluster.
90,Counters can have different consistency levels: strong and weak.
90,"Although a strong/weak consistent counter has separate interfaces, both support updating its value,"
90,return the current value and they provide events when its value is updated.
90,Details are provided below in this document to help you choose which one fits best your uses-case.
90,11.1.1. Installation and Configuration
90,"In order to start using the counters, you needs to add the dependency in your Maven pom.xml file:"
90,pom.xml
90,<dependency>
90,<groupId>org.infinispan</groupId>
90,<artifactId>infinispan-clustered-counter</artifactId>
90,</dependency>
90,The counters can be configured Infinispan configuration file or on-demand via the CounterManager interface detailed
90,later in this document.
90,A counters configured in Infinispan configuration file is created at boot time when the EmbeddedCacheManager is starting.
90,These counters are started eagerly and they are available in all the cluster’s nodes.
90,configuration.xml
90,<infinispan>
90,<cache-container ...>
90,"<!-- To persist counters, you need to configure the global state. -->"
90,<global-state>
90,<!-- Global state configuration goes here. -->
90,</global-state>
90,<!-- Cache configuration goes here. -->
90,"<counters xmlns=""urn:infinispan:config:counters:14.0"" num-owners=""3"" reliability=""CONSISTENT"">"
90,"<strong-counter name=""c1"" initial-value=""1"" storage=""PERSISTENT""/>"
90,"<strong-counter name=""c2"" initial-value=""2"" storage=""VOLATILE"" lower-bound=""0""/>"
90,"<strong-counter name=""c3"" initial-value=""3"" storage=""PERSISTENT"" upper-bound=""5""/>"
90,"<strong-counter name=""c4"" initial-value=""4"" storage=""VOLATILE"" lower-bound=""0"" upper-bound=""10""/>"
90,"<strong-counter name=""c5"" initial-value=""0"" upper-bound=""100"" lifespan=""60000""/>"
90,"<weak-counter name=""c6"" initial-value=""5"" storage=""PERSISTENT"" concurrency-level=""1""/>"
90,</counters>
90,</cache-container>
90,</infinispan>
90,"or programmatically, in the GlobalConfigurationBuilder:"
90,GlobalConfigurationBuilder globalConfigurationBuilder = ...;
90,CounterManagerConfigurationBuilder builder = globalConfigurationBuilder.addModule(CounterManagerConfigurationBuilder.class);
90,builder.numOwner(3).reliability(Reliability.CONSISTENT);
90,"builder.addStrongCounter().name(""c1"").initialValue(1).storage(Storage.PERSISTENT);"
90,"builder.addStrongCounter().name(""c2"").initialValue(2).lowerBound(0).storage(Storage.VOLATILE);"
90,"builder.addStrongCounter().name(""c3"").initialValue(3).upperBound(5).storage(Storage.PERSISTENT);"
90,"builder.addStrongCounter().name(""c4"").initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE);"
90,"builder.addStrongCounter().name(""c5"").initialValue(0).upperBound(100).lifespan(60000);"
90,"builder.addWeakCounter().name(""c6"").initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT);"
90,"On other hand, the counters can be configured on-demand, at any time after the EmbeddedCacheManager is initialized."
90,CounterManager manager = ...;
90,"manager.defineCounter(""c1"", CounterConfiguration.builder(CounterType.UNBOUNDED_STRONG).initialValue(1).storage(Storage.PERSISTENT).build());"
90,"manager.defineCounter(""c2"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(2).lowerBound(0).storage(Storage.VOLATILE).build());"
90,"manager.defineCounter(""c3"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(3).upperBound(5).storage(Storage.PERSISTENT).build());"
90,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(4).lowerBound(0).upperBound(10).storage(Storage.VOLATILE).build());"
90,"manager.defineCounter(""c4"", CounterConfiguration.builder(CounterType.BOUNDED_STRONG).initialValue(0).upperBound(100).lifespan(60000).build());"
90,"manager.defineCounter(""c6"", CounterConfiguration.builder(CounterType.WEAK).initialValue(5).concurrencyLevel(1).storage(Storage.PERSISTENT).build());"
90,CounterConfiguration is immutable and can be reused.
90,The method defineCounter() will return true if the counter is successful configured or false otherwise.
90,"However, if the configuration is invalid, the method will throw a CounterConfigurationException."
90,"To find out if a counter is already defined, use the method isDefined()."
90,CounterManager manager = ...
90,"if (!manager.isDefined(""someCounter"")) {"
90,"manager.define(""someCounter"", ...);"
90,Additional resources
90,Infinispan configuration schema reference
90,List counter names
90,"To list all the counters defined, the method CounterManager.getCounterNames() returns a collection of all counter"
90,names created cluster-wide.
90,11.1.2. CounterManager interface
90,"The CounterManager interface is the entry point to define, retrieve and remove counters."
90,Embedded deployments
90,CounterManager automatically listen to the creation of EmbeddedCacheManager and proceeds with the registration
90,of an
90,instance of it per EmbeddedCacheManager.
90,It starts the caches needed to store the counter state and configures the default counters.
90,Retrieving the CounterManager is as simple as invoke the
90,EmbeddedCounterManagerFactory.asCounterManager(EmbeddedCacheManager)
90,as shown in the example below:
90,// create or obtain your EmbeddedCacheManager
90,EmbeddedCacheManager manager = ...;
90,// retrieve the CounterManager
90,CounterManager counterManager = EmbeddedCounterManagerFactory.asCounterManager(manager);
90,Server deployments
90,"For Hot Rod clients, the CounterManager is registered in the RemoteCacheManager and can be retrieved as follows:"
90,// create or obtain your RemoteCacheManager
90,RemoteCacheManager manager = ...;
90,// retrieve the CounterManager
90,CounterManager counterManager = RemoteCounterManagerFactory.asCounterManager(manager);
90,Remove a counter via CounterManager
90,There is a difference between remove a counter via the Strong/WeakCounter interfaces and the CounterManager.
90,The CounterManager.remove(String) removes the counter value from the cluster and removes all the listeners registered
90,in the counter in the local counter instance.
90,"In addition, the counter instance is no longer reusable and it may return an invalid results."
90,"On the other side, the Strong/WeakCounter removal only removes the counter value."
90,The instance can still be reused and the listeners still works.
90,The counter is re-created if it is accessed after a removal.
90,11.1.3. The Counter
90,A counter can be strong (StrongCounter) or weakly consistent (WeakCounter) and both is identified by a name.
90,"They have a specific interface but they share some logic, namely, both of them are asynchronous"
90,"( a CompletableFuture is returned by each operation), provide an update event and can be reset to its initial value."
90,"If you don’t want to use the async API, it is possible to return a synchronous counter via sync() method."
90,The API is the same but without the CompletableFuture return value.
90,The following methods are common to both interfaces:
90,String getName();
90,CompletableFuture<Long> getValue();
90,CompletableFuture<Void> reset();
90,<T extends CounterListener> Handle<T> addListener(T listener);
90,CounterConfiguration getConfiguration();
90,CompletableFuture<Void> remove();
90,SyncStrongCounter sync(); //SyncWeakCounter for WeakCounter
90,getName() returns the counter name (identifier).
90,getValue() returns the current counter’s value.
90,reset() allows to reset the counter’s value to its initial value.
90,addListener() register a listener to receive update events.
90,More details about it in the Notification and Events section.
90,getConfiguration() returns the configuration used by the counter.
90,remove() removes the counter value from the cluster. The instance can still be used and the listeners are kept.
90,sync() creates a synchronous counter.
90,The counter is re-created if it is accessed after a removal.
90,The StrongCounter interface: when the consistency or bounds matters.
90,The strong counter provides uses a single key stored in Infinispan cache to provide the consistency needed.
90,All the updates are performed under the key lock to updates its values.
90,"On other hand, the reads don’t acquire any locks and reads the current value."
90,"Also, with this scheme, it allows to bound the counter value and provide atomic operations like compare-and-set/swap."
90,A StrongCounter can be retrieved from the CounterManager by using the getStrongCounter() method.
90,As an example:
90,CounterManager counterManager = ...
90,"StrongCounter aCounter = counterManager.getStrongCounter(""my-counter"");"
90,"Since every operation will hit a single key, the StrongCounter has a higher contention rate."
90,The StrongCounter interface adds the following method:
90,default CompletableFuture<Long> incrementAndGet() {
90,return addAndGet(1L);
90,default CompletableFuture<Long> decrementAndGet() {
90,return addAndGet(-1L);
90,CompletableFuture<Long> addAndGet(long delta);
90,"CompletableFuture<Boolean> compareAndSet(long expect, long update);"
90,"CompletableFuture<Long> compareAndSwap(long expect, long update);"
90,incrementAndGet() increments the counter by one and returns the new value.
90,decrementAndGet() decrements the counter by one and returns the new value.
90,addAndGet() adds a delta to the counter’s value and returns the new value.
90,compareAndSet() and compareAndSwap() atomically set the counter’s value if the current value is the expected.
90,A operation is considered completed when the CompletableFuture is completed.
90,The difference between compare-and-set and compare-and-swap is that the former returns true if the operation succeeds
90,while the later returns the previous value.
90,The compare-and-swap is successful if the return value is the same as the expected.
90,Bounded StrongCounter
90,"When bounded, all the update method above will throw a CounterOutOfBoundsException when they reached the"
90,lower or upper bound.
90,The exception has the following methods to check which side bound has been reached:
90,public boolean isUpperBoundReached();
90,public boolean isLowerBoundReached();
90,Uses cases
90,The strong counter fits better in the following uses cases:
90,"When counter’s value is needed after each update (example, cluster-wise ids generator or sequences)"
90,"When a bounded counter is needed (example, rate limiter)"
90,Usage Examples
90,"StrongCounter counter = counterManager.getStrongCounter(""unbounded_counter"");"
90,// incrementing the counter
90,"System.out.println(""new value is "" + counter.incrementAndGet().get());"
90,// decrement the counter's value by 100 using the functional API
90,counter.addAndGet(-100).thenApply(v -> {
90,"System.out.println(""new value is "" + v);"
90,return null;
90,}).get();
90,"// alternative, you can do some work while the counter is updated"
90,CompletableFuture<Long> f = counter.addAndGet(10);
90,// ... do some work ...
90,"System.out.println(""new value is "" + f.get());"
90,"// and then, check the current value"
90,"System.out.println(""current value is "" + counter.getValue().get());"
90,"// finally, reset to initial value"
90,counter.reset().get();
90,"System.out.println(""current value is "" + counter.getValue().get());"
90,// or set to a new value if zero
90,"System.out.println(""compare and set succeeded? "" + counter.compareAndSet(0, 1));"
90,"And below, there is another example using a bounded counter:"
90,"StrongCounter counter = counterManager.getStrongCounter(""bounded_counter"");"
90,// incrementing the counter
90,try {
90,"System.out.println(""new value is "" + counter.addAndGet(100).get());"
90,} catch (ExecutionException e) {
90,Throwable cause = e.getCause();
90,if (cause instanceof CounterOutOfBoundsException) {
90,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
90,"System.out.println(""ops, upper bound reached."");"
90,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
90,"System.out.println(""ops, lower bound reached."");"
90,// now using the functional API
90,"counter.addAndGet(-100).handle((v, throwable) -> {"
90,if (throwable != null) {
90,Throwable cause = throwable.getCause();
90,if (cause instanceof CounterOutOfBoundsException) {
90,if (((CounterOutOfBoundsException) cause).isUpperBoundReached()) {
90,"System.out.println(""ops, upper bound reached."");"
90,} else if (((CounterOutOfBoundsException) cause).isLowerBoundReached()) {
90,"System.out.println(""ops, lower bound reached."");"
90,return null;
90,"System.out.println(""new value is "" + v);"
90,return null;
90,}).get();
90,Compare-and-set vs Compare-and-swap examples:
90,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
90,"long oldValue, newValue;"
90,do {
90,oldValue = counter.getValue().get();
90,newValue = someLogic(oldValue);
90,"} while (!counter.compareAndSet(oldValue, newValue).get());"
90,"With compare-and-swap, it saves one invocation counter invocation (counter.getValue())"
90,"StrongCounter counter = counterManager.getStrongCounter(""my-counter"");"
90,long oldValue = counter.getValue().get();
90,"long currentValue, newValue;"
90,do {
90,currentValue = oldValue;
90,newValue = someLogic(oldValue);
90,"} while ((oldValue = counter.compareAndSwap(oldValue, newValue).get()) != currentValue);"
90,"To use a strong counter as a rate limiter, configure upper-bound and lifespan parameters as follows:"
90,// 5 request per minute
90,CounterConfiguration configuration = CounterConfiguration.builder(CounterType.BOUNDED_STRONG)
90,.upperBound(5)
90,.lifespan(60000)
90,.build();
90,"counterManager.defineCounter(""rate_limiter"", configuration);"
90,"StrongCounter counter = counterManager.getStrongCounter(""rate_limiter"");"
90,"// on each operation, invoke"
90,try {
90,counter.incrementAndGet().get();
90,// continue with operation
90,} catch (InterruptedException e) {
90,Thread.currentThread().interrupt();
90,} catch (ExecutionException e) {
90,if (e.getCause() instanceof CounterOutOfBoundsException) {
90,// maximum rate. discard operation
90,return;
90,} else {
90,"// unexpected error, handling property"
90,The lifespan parameter is an experimental capability and may be removed in a future version.
90,The WeakCounter interface: when speed is needed
90,The WeakCounter stores the counter’s value in multiple keys in Infinispan cache.
90,The number of keys created is configured by the concurrency-level attribute.
90,Each key stores a partial state of the counter’s value and it can be updated concurrently.
90,It main advantage over the StrongCounter is the lower contention in the cache.
90,"On other hand, the read of its value is more expensive and bounds are not allowed."
90,The reset operation should be handled with caution.
90,It is not atomic and it produces intermediates values.
90,These value may be seen by a read operation and by any listener registered.
90,A WeakCounter can be retrieved from the CounterManager by using the getWeakCounter() method.
90,As an example:
90,CounterManager counterManager = ...
90,"StrongCounter aCounter = counterManager.getWeakCounter(""my-counter);"
90,Weak Counter Interface
90,The WeakCounter adds the following methods:
90,default CompletableFuture<Void> increment() {
90,return add(1L);
90,default CompletableFuture<Void> decrement() {
90,return add(-1L);
90,CompletableFuture<Void> add(long delta);
90,They are similar to the `StrongCounter’s methods but they don’t return the new value.
90,Uses cases
90,The weak counter fits best in uses cases where the result of the update operation is not needed or the counter’s value
90,is not required too often.
90,Collecting statistics is a good example of such an use case.
90,Examples
90,"Below, there is an example of the weak counter usage."
90,"WeakCounter counter = counterManager.getWeakCounter(""my_counter"");"
90,// increment the counter and check its result
90,counter.increment().get();
90,"System.out.println(""current value is "" + counter.getValue());"
90,CompletableFuture<Void> f = counter.add(-100);
90,//do some work
90,f.get(); //wait until finished
90,"System.out.println(""current value is "" + counter.getValue().get());"
90,//using the functional API
90,"counter.reset().whenComplete((aVoid, throwable) -> System.out.println(""Reset done "" + (throwable == null ? ""successfully"" : ""unsuccessfully""))).get();"
90,"System.out.println(""current value is "" + counter.getValue().get());"
90,11.1.4. Notifications and Events
90,Both strong and weak counter supports a listener to receive its updates events.
90,The listener must implement CounterListener and it can be registered by the following method:
90,<T extends CounterListener> Handle<T> addListener(T listener);
90,The CounterListener has the following interface:
90,public interface CounterListener {
90,void onUpdate(CounterEvent entry);
90,The Handle object returned has the main goal to remove the CounterListener when it is not longer needed.
90,"Also, it allows to have access to the CounterListener instance that is it handling."
90,It has the following interface:
90,public interface Handle<T extends CounterListener> {
90,T getCounterListener();
90,void remove();
90,"Finally, the CounterEvent has the previous and current value and state."
90,It has the following interface:
90,public interface CounterEvent {
90,long getOldValue();
90,State getOldState();
90,long getNewValue();
90,State getNewState();
90,The state is always State.VALID for unbounded strong counter and weak counter.
90,State.LOWER_BOUND_REACHED and State.UPPER_BOUND_REACHED are only valid for bounded strong counters.
90,The weak counter reset() operation will trigger multiple notification with intermediate values.
90,12. Listeners and notifications
90,Use listeners with Infinispan to get notifications when events occur for the Cache Manager or for caches.
90,12.1. Listeners and notifications
90,"Infinispan offers a listener API, where clients can register for and get notified when events take place."
90,This annotation-driven API applies to 2 different levels: cache level events and Cache Manager level events.
90,Events trigger a notification which is dispatched to listeners.
90,Listeners are simple POJOs annotated with @Listener and registered using the methods defined in the Listenable interface.
90,"Both Cache and CacheManager implement Listenable, which means you can attach listeners to either a cache or a Cache Manager, to receive either cache-level or Cache Manager-level notifications."
90,"For example, the following class defines a listener to print out some information every time a new entry is added to the cache, in a non blocking fashion:"
90,@Listener
90,public class PrintWhenAdded {
90,Queue<CacheEntryCreatedEvent> events = new ConcurrentLinkedQueue<>();
90,@CacheEntryCreated
90,public CompletionStage<Void> print(CacheEntryCreatedEvent event) {
90,events.add(event);
90,return null;
90,"For more comprehensive examples, please see the Javadocs for @Listener."
90,12.2. Cache-level notifications
90,"Cache-level events occur on a per-cache basis, and by default are only raised on nodes where the events occur."
90,Note in a distributed cache these events are only raised on the owners of data being affected.
90,"Examples of cache-level events are entries being added, removed, modified, etc."
90,These events trigger notifications to listeners registered to a specific cache.
90,"Please see the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for a comprehensive list of all cache-level notifications, and their respective method-level annotations."
90,Please refer to the Javadocs on the org.infinispan.notifications.cachelistener.annotation package for the list of cache-level notifications available in Infinispan.
90,Cluster listeners
90,The cluster listeners should be used when it is desirable to listen to the cache events on a single node.
90,To do so all that is required is set to annotate your listener as being clustered.
90,@Listener (clustered = true)
90,public class MyClusterListener { .... }
90,There are some limitations to cluster listeners from a non clustered listener.
90,"A cluster listener can only listen to @CacheEntryModified, @CacheEntryCreated, @CacheEntryRemoved and @CacheEntryExpired events."
90,Note this means any other type of event will not be listened to for this listener.
90,"Only the post event is sent to a cluster listener, the pre event is ignored."
90,Event filtering and conversion
90,All applicable events on the node where the listener is installed will be raised to the listener.
90,"It is possible to dynamically filter what events are raised by using a KeyFilter (only allows filtering on keys) or CacheEventFilter (used to filter for keys, old value, old metadata, new value, new metadata, whether command was retried, if the event is before the event (ie. isPre) and also the command type)."
90,The example here shows a simple KeyFilter that will only allow events to be raised when an event modified the entry for the key Only Me.
90,public class SpecificKeyFilter implements KeyFilter<String> {
90,private final String keyToAccept;
90,public SpecificKeyFilter(String keyToAccept) {
90,if (keyToAccept == null) {
90,throw new NullPointerException();
90,this.keyToAccept = keyToAccept;
90,public boolean accept(String key) {
90,return keyToAccept.equals(key);
90,...
90,"cache.addListener(listener, new SpecificKeyFilter(""Only Me""));"
90,...
90,This can be useful when you want to limit what events you receive in a more efficient manner.
90,There is also a CacheEventConverter that can be supplied that allows for converting a value to another before raising the event.
90,This can be nice to modularize any code that does value conversions.
90,The mentioned filters and converters are especially beneficial when used in conjunction with a Cluster Listener.
90,This is because the filtering and conversion is done on the node where the event originated and not on the node where event is listened to.
90,This can provide benefits of not having to replicate events across the cluster (filter) or even have reduced payloads (converter).
90,Initial State Events
90,When a listener is installed it will only be notified of events after it is fully installed.
90,It may be desirable to get the current state of the cache contents upon first registration of listener by having an event generated of type @CacheEntryCreated for each element in the cache.
90,Any additionally generated events during this initial phase will be queued until appropriate events have been raised.
90,This only works for clustered listeners at this time.
90,ISPN-4608 covers adding this for non clustered listeners.
90,Duplicate Events
90,It is possible in a non transactional cache to receive duplicate events.
90,This is possible when the primary owner of a key goes down while trying to perform a write operation such as a put.
90,"Infinispan internally will rectify the put operation by sending it to the new primary owner for the given key automatically, however there are no guarantees in regards to if the write was first replicated to backups."
90,"Thus more than 1 of the following write events (CacheEntryCreatedEvent, CacheEntryModifiedEvent & CacheEntryRemovedEvent) may be sent on a single operation."
90,If more than one event is generated Infinispan will mark the event that it was generated by a retried command to help the user to know when this occurs without having to pay attention to view changes.
90,@Listener
90,public class MyRetryListener {
90,@CacheEntryModified
90,public void entryModified(CacheEntryModifiedEvent event) {
90,if (event.isCommandRetried()) {
90,// Do something
90,Also when using a CacheEventFilter or CacheEventConverter the EventType contains a method isRetry to tell if the event was generated due to retry.
90,12.3. Cache Manager notifications
90,"Events that occur on a Cache Manager level are cluster-wide and involve events that affect all caches created by a single Cache Manager. Examples of Cache Manager events are nodes joining or leaving a cluster, or caches starting or stopping."
90,"See the org.infinispan.notifications.cachemanagerlistener.annotation package for a comprehensive list of all Cache Manager notifications,"
90,and their respective method-level annotations.
90,12.4. Synchronicity of events
90,"By default, all async notifications are dispatched in the notification thread pool."
90,Sync notifications will delay the operation from continuing until the listener method completes or the CompletionStage
90,"completes (the former causing the thread to block). Alternatively, you could annotate your listener as asynchronous in"
90,"which case the operation will continue immediately, while the notification is completed asynchronously on the notification thread pool."
90,"To do this, simply annotate your listener such:"
90,Asynchronous Listener
90,@Listener (sync = false)
90,public class MyAsyncListener {
90,@CacheEntryCreated
90,void listen(CacheEntryCreatedEvent event) { }
90,Blocking Synchronous Listener
90,@Listener
90,public class MySyncListener {
90,@CacheEntryCreated
90,void listen(CacheEntryCreatedEvent event) { }
90,Non-Blocking Listener
90,@Listener
90,public class MyNonBlockingListener {
90,@CacheEntryCreated
90,CompletionStage<Void> listen(CacheEntryCreatedEvent event) { }
90,Asynchronous thread pool
90,"To tune the thread pool used to dispatch such asynchronous notifications, use the <listener-executor /> XML element in your configuration file."
90,Last updated 2024-03-13 12:05:42 UTC
91,"Express Tutorial Part 3: Using a Database (with Mongoose) - Learn web development | MDNSkip to main contentSkip to searchSkip to select languageMDN Web DocsOpen main menuReferencesReferencesOverview / Web TechnologyWeb technology reference for developersHTMLStructure of content on the webCSSCode used to describe document styleJavaScriptGeneral-purpose scripting languageHTTPProtocol for transmitting web resourcesWeb APIsInterfaces for building web applicationsWeb ExtensionsDeveloping extensions for web browsersWeb TechnologyWeb technology reference for developersGuidesGuidesOverview / MDN Learning AreaLearn web developmentMDN Learning AreaLearn web developmentHTMLLearn to structure web content with HTMLCSSLearn to style content using CSSJavaScriptLearn to run scripts in the browserAccessibilityLearn to make the web accessible to allPlusPlusOverviewA customized MDN experienceAI Help (beta)Get real-time assistance and supportUpdatesAll browser compatibility updates at a glanceDocumentationLearn how to use MDN PlusFAQFrequently asked questions about MDN PlusCurriculumNewBlogPlayAI Help BetaSearch MDNClear search inputSearchThemeLog inSign up for freeGuidesServer-side website programmingExpress web framework (Node.js/JavaScript)Express Tutorial Part 3: Using a Database (with Mongoose)Article ActionsEnglish (US)Filter sidebarClear filter inputIn this articleOverviewDesigning the LocalLibrary modelsMongoose primerSetting up the MongoDB databaseInstall MongooseConnect to MongoDBDefining the LocalLibrary SchemaTesting — create some itemsSummarySee alsoComplete beginners start here!Getting started with the webGetting started with the webInstalling basic softwareWhat will your website look like?Dealing with filesHTML basicsCSS basicsJavaScript basicsPublishing your websiteHow the web worksHTML — Structuring the webIntroduction to HTMLIntroduction to HTMLGetting started with HTMLWhat's in the head? Metadata in HTMLHTML text fundamentalsCreating hyperlinksAdvanced text formattingDocument and website structureDebugging HTMLMarking up a letterStructuring a page of contentMultimedia and embeddingMultimedia and embeddingImages in HTMLVideo and audio contentFrom object to iframe — other embedding technologiesAdding vector graphics to the webResponsive imagesMozilla splash pageHTML tablesHTML tablesHTML table basicsHTML table advanced features and accessibilityStructuring planet dataCSS — Styling the webCSS first stepsCSS first stepsWhat is CSS?Getting started with CSSHow CSS is structuredHow CSS worksStyling a biography pageCSS building blocksCSS building blocksCSS selectorsType, class, and ID selectorsAttribute selectorsPseudo-classes and pseudo-elementsCombinatorsCascade, specificity, and inheritanceCascade layersThe box modelBackgrounds and bordersHandling different text directionsOverflowing contentCSS values and unitsSizing items in CSSImages, media, and form elementsStyling tablesDebugging CSSOrganizing your CSSFundamental CSS comprehensionCreating fancy letterheaded paperA cool-looking boxStyling textCSS styling textFundamental text and font stylingStyling listsStyling linksWeb fontsTypesetting a community school homepageCSS layoutCSS layoutIntroduction to CSS layoutNormal FlowFlexboxGridsFloatsPositioningMultiple-column layoutResponsive designBeginner's guide to media queriesLegacy layout methodsSupporting older browsersFundamental layout comprehensionJavaScript — Dynamic client-side scriptingJavaScript first stepsJavaScript first stepsWhat is JavaScript?A first splash into JavaScriptWhat went wrong? Troubleshooting JavaScriptStoring the information you need — VariablesBasic math in JavaScript — numbers and operatorsHandling text — strings in JavaScriptUseful string methodsArraysSilly story generatorJavaScript building blocksJavaScript building blocksMaking decisions in your code — conditionalsLooping codeFunctions — reusable blocks of codeBuild your own functionFunction return valuesIntroduction to eventsImage galleryIntroducing JavaScript objectsIntroducing JavaScript objectsJavaScript object basicsObject prototypesObject-oriented programmingClasses in JavaScriptWorking with JSONObject building practiceAdding features to our bouncing balls demoAsynchronous JavaScriptAsynchronous JavaScriptIntroducing asynchronous JavaScriptHow to use promisesHow to implement a promise-based APIIntroducing workersSequencing animationsClient-side web APIsClient-side web APIsIntroduction to web APIsManipulating documentsFetching data from the serverThird-party APIsDrawing graphicsVideo and Audio APIsClient-side storageWeb forms — Working with user dataCore forms learning pathwayWeb form building blocksYour first formHow to structure a web formBasic native form controlsThe HTML5 input typesOther form controlsStyling web formsAdvanced form stylingUI pseudo-classesClient-side form validationSending form dataAdvanced forms articlesHow to build custom form controlsSending forms through JavaScriptCSS property compatibility table for form controlsHTML forms in legacy browsersAccessibility — Make the web usable by everyoneAccessibility guidesAccessibilityWhat is accessibility?HTML: A good basis for accessibilityCSS and JavaScript accessibility best practicesWAI-ARIA basicsAccessible multimediaMobile accessibilityAssessment: Accessibility troubleshootingPerformance — Making websites fast and responsivePerformance guidesWeb performanceThe ""why"" of web performanceWhat is web performance?Perceived performanceMeasuring performanceMultimedia: ImagesMultimedia: videoJavaScript performance optimizationHTML performance optimizationCSS performance optimizationThe business case for web performanceMathML — Writing mathematics with MathMLMathML first stepsMathML first stepsGetting started with MathMLMathML Text ContainersMathML fractions and rootsMathML scripted elementsMathML tablesThree famous mathematical formulasGames — Developing games for the webGuides and tutorialsIntroduction to game development for the WebTechniques for game developmentTutorialsPublishing gamesTools and testingClient-side web development toolsUnderstanding client-side web development toolsClient-side tooling overviewCommand line crash coursePackage management basicsIntroducing a complete toolchainDeploying our appIntroduction to client-side frameworksIntroduction to client-side frameworksFramework main featuresReactGetting started with ReactBeginning our React todo listComponentizing our React appReact interactivity: Events and stateReact interactivity: Editing, filtering, conditional renderingAccessibility in ReactReact resourcesEmberGetting started with EmberEmber app structure and componentizationEmber interactivity: Events, classes and stateEmber Interactivity: Footer functionality, conditional renderingRouting in EmberEmber resources and troubleshootingVueGetting started with VueCreating our first Vue componentRendering a list of Vue componentsAdding a new todo form: Vue events, methods, and modelsStyling Vue components with CSSUsing Vue computed propertiesVue conditional rendering: editing existing todosVue refs and lifecycle methods for focus managementVue resourcesSvelteGetting started with SvelteStarting our Svelte to-do list appDynamic behavior in Svelte: working with variables and propsComponentizing our Svelte appAdvanced Svelte: Reactivity, lifecycle, accessibilityWorking with Svelte storesTypeScript support in SvelteDeployment and next stepsAngularGetting started with AngularBeginning our Angular todo list appStyling our Angular appCreating an item componentFiltering our to-do itemsBuilding Angular applications and further resourcesGit and GitHubGit and GitHubCross browser testingCross browser testingIntroduction to cross-browser testingStrategies for carrying out testingHandling common HTML and CSS problemsHandling common JavaScript problemsHandling common accessibility problemsImplementing feature detectionIntroduction to automated testingSetting up your own test automation environmentServer-side website programmingFirst stepsServer-side website programming first stepsIntroduction to the server sideClient-Server OverviewServer-side web frameworksWebsite securityDjango web framework (Python)Django Web Framework (Python)Django introductionSetting up a Django development environmentDjango Tutorial: The Local Library websiteDjango Tutorial Part 2: Creating a skeleton websiteDjango Tutorial Part 3: Using modelsDjango Tutorial Part 4: Django admin siteDjango Tutorial Part 5: Creating our home pageDjango Tutorial Part 6: Generic list and detail viewsDjango Tutorial Part 7: Sessions frameworkDjango Tutorial Part 8: User authentication and permissionsDjango Tutorial Part 9: Working with formsDjango Tutorial Part 10: Testing a Django web applicationDjango Tutorial Part 11: Deploying Django to productionDjango web application securityAssessment: DIY Django mini blogExpress Web Framework (Node.js/JavaScript)Express web framework (Node.js/JavaScript)Express/Node introductionSetting up a Node development environmentExpress Tutorial: The Local Library websiteExpress Tutorial Part 2: Creating a skeleton websiteExpress Tutorial Part 3: Using a Database (with Mongoose)Express Tutorial Part 4: Routes and controllersExpress Tutorial Part 5: Displaying library dataExpress Tutorial Part 6: Working with formsExpress Tutorial Part 7: Deploying to productionFurther resourcesCommon questionsCommon questionsUse HTML to solve common problemsUse CSS to solve common problemsSolve common problems in your JavaScript codeWeb mechanicsTools and setupDesign and accessibilityIn this articleOverviewDesigning the LocalLibrary modelsMongoose primerSetting up the MongoDB databaseInstall MongooseConnect to MongoDBDefining the LocalLibrary SchemaTesting — create some itemsSummarySee alsoExpress Tutorial Part 3: Using a Database (with Mongoose)"
91,Previous
91,Overview: Express Nodejs
91,Next
91,"This article briefly introduces databases, and how to use them with Node/Express apps. It then goes on to show how we can use Mongoose to provide database access for the LocalLibrary website. It explains how object schema and models are declared, the main field types, and basic validation. It also briefly shows a few of the main ways in which you can access model data."
91,Prerequisites:
91,Express Tutorial Part 2: Creating a skeleton website
91,Objective:
91,To be able to design and create your own models using Mongoose.
91,"OverviewLibrary staff will use the Local Library website to store information about books and borrowers, while library members will use it to browse and search for books, find out whether there are any copies available, and then reserve or borrow them. In order to store and retrieve information efficiently, we will store it in a database."
91,"Express apps can use many different databases, and there are several approaches you can use for performing Create, Read, Update and Delete (CRUD) operations. This tutorial provides a brief overview of some of the available options and then goes on to show in detail the particular mechanisms selected.What databases can I use?Express apps can use any database supported by Node (Express itself doesn't define any specific additional behavior/requirements for database management). There are many popular options, including PostgreSQL, MySQL, Redis, SQLite, and MongoDB."
91,"When choosing a database, you should consider things like time-to-productivity/learning curve, performance, ease of replication/backup, cost, community support, etc. While there is no single ""best"" database, almost any of the popular solutions should be more than acceptable for a small-to-medium-sized site like our Local Library."
91,For more information on the options see Database integration (Express docs).What is the best way to interact with a database?There are two common approaches for interacting with a database:
91,"Using the databases' native query language, such as SQL."
91,"Using an Object Relational Mapper (""ORM""). An ORM represents the website's data as JavaScript objects, which are then mapped to the underlying database. Some ORMs are tied to a specific database, while others provide a database-agnostic backend."
91,"The very best performance can be gained by using SQL, or whatever query language is supported by the database. ODM's are often slower because they use translation code to map between objects and the database format, which may not use the most efficient database queries (this is particularly true if the ODM supports different database backends, and must make greater compromises in terms of what database features are supported)."
91,The benefit of using an ORM is that programmers can continue to think in terms of JavaScript objects rather than database semantics — this is particularly true if you need to work with different databases (on either the same or different websites). They also provide an obvious place to perform data validation.
91,"Note: Using ODM/ORMs often results in lower costs for development and maintenance! Unless you're very familiar with the native query language or performance is paramount, you should strongly consider using an ODM."
91,What ORM/ODM should I use?There are many ODM/ORM solutions available on the npm package manager site (check out the odm and orm tags for a subset!).
91,A few solutions that were popular at the time of writing are:
91,Mongoose: Mongoose is a MongoDB object modeling tool designed to work in an asynchronous environment.
91,"Waterline: An ORM extracted from the Express-based Sails web framework. It provides a uniform API for accessing numerous different databases, including Redis, MySQL, LDAP, MongoDB, and Postgres."
91,"Bookshelf: Features both promise-based and traditional callback interfaces, providing transaction support, eager/nested-eager relation loading, polymorphic associations, and support for one-to-one, one-to-many, and many-to-many relations. Works with PostgreSQL, MySQL, and SQLite3."
91,"Objection: Makes it as easy as possible to use the full power of SQL and the underlying database engine (supports SQLite3, Postgres, and MySQL)."
91,"Sequelize is a promise-based ORM for Node.js and io.js. It supports the dialects PostgreSQL, MySQL, MariaDB, SQLite, and MSSQL and features solid transaction support, relations, read replication and more."
91,"Node ORM2 is an Object Relationship Manager for NodeJS. It supports MySQL, SQLite, and Progress, helping to work with the database using an object-oriented approach."
91,"GraphQL: Primarily a query language for restful APIs, GraphQL is very popular, and has features available for reading data from databases."
91,"As a general rule, you should consider both the features provided and the ""community activity"" (downloads, contributions, bug reports, quality of documentation, etc.) when selecting a solution. At the time of writing Mongoose is by far the most popular ODM, and is a reasonable choice if you're using MongoDB for your database.Using Mongoose and MongoDB for the LocalLibraryFor the Local Library example (and the rest of this topic) we're going to use the Mongoose ODM to access our library data. Mongoose acts as a front end to MongoDB, an open source NoSQL database that uses a document-oriented data model. A ""collection"" of ""documents"" in a MongoDB database is analogous to a ""table"" of ""rows"" in a relational database."
91,"This ODM and database combination is extremely popular in the Node community, partially because the document storage and query system looks very much like JSON, and is hence familiar to JavaScript developers."
91,"Note: You don't need to know MongoDB in order to use Mongoose, although parts of the Mongoose documentation are easier to use and understand if you are already familiar with MongoDB."
91,"The rest of this tutorial shows how to define and access the Mongoose schema and models for the LocalLibrary website example.Designing the LocalLibrary modelsBefore you jump in and start coding the models, it's worth taking a few minutes to think about what data we need to store and the relationships between the different objects."
91,"We know that we need to store information about books (title, summary, author, genre, ISBN) and that we might have multiple copies available (with globally unique ids, availability statuses, etc.). We might need to store more information about the author than just their name, and there might be multiple authors with the same or similar names. We want to be able to sort information based on the book title, author, genre, and category."
91,"When designing your models it makes sense to have separate models for every ""object"" (a group of related information). In this case some obvious candidates for these models are books, book instances, and authors."
91,"You might also want to use models to represent selection-list options (e.g. like a drop-down list of choices), rather than hard-coding the choices into the website itself — this is recommended when all the options aren't known up front or may change. A good example is a genre (e.g. fantasy, science fiction, etc.)."
91,"Once we've decided on our models and fields, we need to think about the relationships between them."
91,"With that in mind, the UML association diagram below shows the models we'll define in this case (as boxes). As discussed above, we've created models for the book (the generic details of the book), book instance (status of specific physical copies of the book available in the system), and author. We have also decided to have a model for the genre so that values can be created dynamically. We've decided not to have a model for the BookInstance:status — we will hard code the acceptable values because we don't expect these to change. Within each of the boxes, you can see the model name, the field names and types, and also the methods and their return types."
91,"The diagram also shows the relationships between the models, including their multiplicities. The multiplicities are the numbers on the diagram showing the numbers (maximum and minimum) of each model that may be present in the relationship. For example, the connecting line between the boxes shows that Book and a Genre are related. The numbers close to the Book model show that a Genre must have zero or more Books (as many as you like), while the numbers on the other end of the line next to the Genre show that a book can have zero or more associated Genres."
91,"Note: As discussed in our Mongoose primer below it is often better to have the field that defines the relationship between the documents/models in just one model (you can still find the reverse relationship by searching for the associated _id in the other model). Below we have chosen to define the relationship between Book/Genre and Book/Author in the Book schema, and the relationship between the Book/BookInstance in the BookInstance Schema. This choice was somewhat arbitrary — we could equally well have had the field in the other schema."
91,"Note: The next section provides a basic primer explaining how models are defined and used. As you read it, consider how we will construct each of the models in the diagram above."
91,Database APIs are asynchronous
91,"Database methods to create, find, update, or delete records are asynchronous."
91,"What this means is that the methods return immediately, and the code to handle the success or failure of the method runs at a later time when the operation completes."
91,"Other code can execute while the server is waiting for the database operation to complete, so the server can remain responsive to other requests."
91,JavaScript has a number of mechanisms for supporting asynchronous behavior.
91,Historically JavaScript relied heavily on passing callback functions to asynchronous methods to handle the success and error cases.
91,In modern JavaScript callbacks have largely been replaced by Promises.
91,Promises are objects that are (immediately) returned by an asynchronous method that represent its future state.
91,"When the operation completes, the promise object is ""settled"", and resolves an object that represents the result of the operation or an error."
91,"There are two main ways you can use promises to run code when a promise is settled, and we highly recommend that you read How to use promises for a high level overview of both approaches."
91,"In this tutorial, we'll primarily be using await to wait on promise completion within an async function, because this leads to more readable and understandable asynchronous code."
91,"The way this approach works is that you use the async function keyword to mark a function as asynchronous, and then inside that function apply await to any method that returns a promise."
91,When the asynchronous function is executed its operation is paused at the first await method until the promise settles.
91,From the perspective of the surrounding code the asynchronous function then returns and the code after it is able to run.
91,"Later when the promise settles, the await method inside the asynchronous function returns with the result, or an error is thrown if the promise was rejected."
91,"The code in the asynchronous function then executes until either another await is encountered, at which point it will pause again, or until all the code in the function has been run."
91,You can see how this works in the example below.
91,myFunction() is an asynchronous function that is called within a try...catch block.
91,"When myFunction() is run, code execution is paused at methodThatReturnsPromise() until the promise resolves, at which point the code continues to aFunctionThatReturnsPromise() and waits again."
91,"The code in the catch block runs if an error is thrown in the asynchronous function, and this will happen if the promise returned by either of the methods is rejected."
91,jsasync function myFunction {
91,// ...
91,await someObject.methodThatReturnsPromise();
91,// ...
91,await aFunctionThatReturnsPromise();
91,// ...
91,try {
91,// ...
91,myFunction();
91,// ...
91,} catch (e) {
91,// error handling code
91,The asynchronous methods above are run in sequence.
91,If the methods don't depend on each other then you can run them in parallel and finish the whole operation more quickly.
91,"This is done using the Promise.all() method, which takes an iterable of promises as input and returns a single Promise."
91,"This returned promise fulfills when all of the input's promises fulfill, with an array of the fulfillment values."
91,"It rejects when any of the input's promises rejects, with this first rejection reason."
91,The code below shows how this works.
91,"First, we have two functions that return promises."
91,We await on both of them to complete using the promise returned by Promise.all().
91,"Once they both complete await returns and the results array is populated,"
91,"the function then continues to the next await, and waits until the promise returned by anotherFunctionThatReturnsPromise() is settled."
91,You would call the myFunction() in a try...catch block to catch any errors.
91,jsasync function myFunction {
91,// ...
91,"const [resultFunction1, resultFunction2] = await Promise.all(["
91,"functionThatReturnsPromise1(),"
91,functionThatReturnsPromise2()
91,]);
91,// ...
91,await anotherFunctionThatReturnsPromise(resultFunction1);
91,"Promises with await/async allow both flexible and ""comprehensible"" control over asynchronous execution!Mongoose primerThis section provides an overview of how to connect Mongoose to a MongoDB database, how to define a schema and a model, and how to make basic queries."
91,Note: This primer is heavily influenced by the Mongoose quick start on npm and the official documentation.
91,Installing Mongoose and MongoDB
91,Mongoose is installed in your project (package.json) like any other dependency — using npm.
91,"To install it, use the following command inside your project folder:"
91,bashnpm install mongoose
91,"Installing Mongoose adds all its dependencies, including the MongoDB database driver, but it does not install MongoDB itself. If you want to install a MongoDB server then you can download installers from here for various operating systems and install it locally. You can also use cloud-based MongoDB instances."
91,"Note: For this tutorial, we'll be using the MongoDB Atlas cloud-based database as a service free tier to provide the database. This is suitable for development and makes sense for the tutorial because it makes ""installation"" operating system independent (database-as-a-service is also one approach you might use for your production database)."
91,Connecting to MongoDB
91,Mongoose requires a connection to a MongoDB database.
91,You can require() and connect to a locally hosted database with mongoose.connect() as shown below (for the tutorial we'll instead connect to an internet-hosted database).
91,js// Import the mongoose module
91,"const mongoose = require(""mongoose"");"
91,// Set `strictQuery: false` to globally opt into filtering by properties that aren't in the schema
91,// Included because it removes preparatory warnings for Mongoose 7.
91,// See: https://mongoosejs.com/docs/migrating_to_6.html#strictquery-is-removed-and-replaced-by-strict
91,"mongoose.set(""strictQuery"", false);"
91,// Define the database URL to connect to.
91,"const mongoDB = ""mongodb://127.0.0.1/my_database"";"
91,"// Wait for database to connect, logging an error if there is a problem"
91,main().catch((err) => console.log(err));
91,async function main() {
91,await mongoose.connect(mongoDB);
91,"Note: As discussed in the Database APIs are asynchronous section, here we await on the promise returned by the connect() method within an async function."
91,"We use the promise catch() handler to handle any errors when trying to connect, but we might also have called main() within a try...catch block."
91,You can get the default Connection object with mongoose.connection.
91,If you need to create additional connections you can use mongoose.createConnection().
91,"This takes the same form of database URI (with host, database, port, options, etc.) as connect() and returns a Connection object)."
91,Note that createConnection() returns immediately; if you need to wait on the connection to be established you can call it with asPromise() to return a promise (mongoose.createConnection(mongoDB).asPromise()).
91,"Defining and creating modelsModels are defined using the Schema interface. The Schema allows you to define the fields stored in each document along with their validation requirements and default values. In addition, you can define static and instance helper methods to make it easier to work with your data types, and also virtual properties that you can use like any other field, but which aren't actually stored in the database (we'll discuss a bit further below)."
91,"Schemas are then ""compiled"" into models using the mongoose.model() method. Once you have a model you can use it to find, create, update, and delete objects of the given type."
91,Note: Each model maps to a collection of documents in the MongoDB database. The documents will contain the fields/schema types defined in the model Schema.
91,Defining schemas
91,"The code fragment below shows how you might define a simple schema. First you require() mongoose, then use the Schema constructor to create a new schema instance, defining the various fields inside it in the constructor's object parameter."
91,js// Require Mongoose
91,"const mongoose = require(""mongoose"");"
91,// Define a schema
91,const Schema = mongoose.Schema;
91,const SomeModelSchema = new Schema({
91,"a_string: String,"
91,"a_date: Date,"
91,});
91,"In the case above we just have two fields, a string and a date. In the next sections, we will show some of the other field types, validation, and other methods."
91,Creating a model
91,Models are created from schemas using the mongoose.model() method:
91,js// Define schema
91,const Schema = mongoose.Schema;
91,const SomeModelSchema = new Schema({
91,"a_string: String,"
91,"a_date: Date,"
91,});
91,// Compile model from schema
91,"const SomeModel = mongoose.model(""SomeModel"", SomeModelSchema);"
91,"The first argument is the singular name of the collection that will be created for your model (Mongoose will create the database collection for the above model SomeModel above), and the second argument is the schema you want to use in creating the model."
91,"Note: Once you've defined your model classes you can use them to create, update, or delete records, and run queries to get all records or particular subsets of records. We'll show you how to do this in the Using models section, and when we create our views."
91,Schema types (fields)
91,A schema can have an arbitrary number of fields — each one represents a field in the documents stored in MongoDB.
91,An example schema showing many of the common field types and how they are declared is shown below.
91,jsconst schema = new Schema({
91,"name: String,"
91,"binary: Buffer,"
91,"living: Boolean,"
91,"updated: { type: Date, default: Date.now() },"
91,"age: { type: Number, min: 18, max: 65, required: true },"
91,"mixed: Schema.Types.Mixed,"
91,"_someId: Schema.Types.ObjectId,"
91,"array: [],"
91,"ofString: [String], // You can also have an array of each of the other types too."
91,"nested: { stuff: { type: String, lowercase: true, trim: true } },"
91,});
91,"Most of the SchemaTypes (the descriptors after ""type:"" or after field names) are self-explanatory. The exceptions are:"
91,"ObjectId: Represents specific instances of a model in the database. For example, a book might use this to represent its author object. This will actually contain the unique ID (_id) for the specified object. We can use the populate() method to pull in the associated information when needed."
91,Mixed: An arbitrary schema type.
91,"[]: An array of items. You can perform JavaScript array operations on these models (push, pop, unshift, etc.). The examples above show an array of objects without a specified type and an array of String objects, but you can have an array of any type of object."
91,The code also shows both ways of declaring a field:
91,"Field name and type as a key-value pair (i.e. as done with fields name, binary and living)."
91,"Field name followed by an object defining the type, and any other options for the field. Options include things like:"
91,default values.
91,built-in validators (e.g. max/min values) and custom validation functions.
91,Whether the field is required
91,"Whether String fields should automatically be set to lowercase, uppercase, or trimmed (e.g. { type: String, lowercase: true, trim: true })"
91,For more information about options see SchemaTypes (Mongoose docs).
91,Validation
91,"Mongoose provides built-in and custom validators, and synchronous and asynchronous validators. It allows you to specify both the acceptable range of values and the error message for validation failure in all cases."
91,The built-in validators include:
91,All SchemaTypes have the built-in required validator. This is used to specify whether the field must be supplied in order to save a document.
91,Numbers have min and max validators.
91,Strings have:
91,enum: specifies the set of allowed values for the field.
91,match: specifies a regular expression that the string must match.
91,maxLength and minLength for the string.
91,The example below (slightly modified from the Mongoose documents) shows how you can specify some of the validator types and error messages:
91,jsconst breakfastSchema = new Schema({
91,eggs: {
91,"type: Number,"
91,"min: [6, ""Too few eggs""],"
91,"max: 12,"
91,"required: [true, ""Why no eggs?""],"
91,drink: {
91,"type: String,"
91,"enum: [""Coffee"", ""Tea"", ""Water""],"
91,});
91,For complete information on field validation see Validation (Mongoose docs).
91,Virtual properties
91,"Virtual properties are document properties that you can get and set but that do not get persisted to MongoDB. The getters are useful for formatting or combining fields, while setters are useful for de-composing a single value into multiple values for storage. The example in the documentation constructs (and deconstructs) a full name virtual property from a first and last name field, which is easier and cleaner than constructing a full name every time one is used in a template."
91,Note: We will use a virtual property in the library to define a unique URL for each model record using a path and the record's _id value.
91,For more information see Virtuals (Mongoose documentation).
91,Methods and query helpers
91,"A schema can also have instance methods, static methods, and query helpers. The instance and static methods are similar, but with the obvious difference that an instance method is associated with a particular record and has access to the current object. Query helpers allow you to extend mongoose's chainable query builder API (for example, allowing you to add a query ""byName"" in addition to the find(), findOne() and findById() methods).Using modelsOnce you've created a schema you can use it to create models. The model represents a collection of documents in the database that you can search, while the model's instances represent individual documents that you can save and retrieve."
91,We provide a brief overview below. For more information see: Models (Mongoose docs).
91,"Note: Creation, update, deletion and querying of records are asynchronous operations that return a promise."
91,The examples below show just the use of the relevant methods and await (i.e. the essential code for using the methods).
91,The surrounding async function and try...catch block to catch errors are omitted for clarity.
91,For more information on using await/async see Database APIs are asynchronous above.
91,Creating and modifying documents
91,To create a record you can define an instance of the model and then call save() on it.
91,The examples below assume SomeModel is a model (with a single field name) that we have created from our schema.
91,js// Create an instance of model SomeModel
91,"const awesome_instance = new SomeModel({ name: ""awesome"" });"
91,// Save the new model instance asynchronously
91,await awesome_instance.save();
91,You can also use create() to define the model instance at the same time as you save it.
91,"Below we create just one, but you can create multiple instances by passing in an array of objects."
91,"jsawait SomeModel.create({ name: ""also_awesome"" });"
91,Every model has an associated connection (this will be the default connection when you use mongoose.model()). You create a new connection and call .model() on it to create the documents on a different database.
91,"You can access the fields in this new record using the dot syntax, and change the values. You have to call save() or update() to store modified values back to the database."
91,js// Access model field values using dot notation
91,console.log(awesome_instance.name); //should log 'also_awesome'
91,"// Change record by modifying the fields, then calling save()."
91,"awesome_instance.name = ""New cool name"";"
91,await awesome_instance.save();
91,Searching for records
91,"You can search for records using query methods, specifying the query conditions as a JSON document. The code fragment below shows how you might find all athletes in a database that play tennis, returning just the fields for athlete name and age. Here we just specify one matching field (sport) but you can add more criteria, specify regular expression criteria, or remove the conditions altogether to return all athletes."
91,"jsconst Athlete = mongoose.model(""Athlete"", yourSchema);"
91,"// find all athletes who play tennis, returning the 'name' and 'age' fields"
91,const tennisPlayers = await Athlete.find(
91,"{ sport: ""Tennis"" },"
91,"""name age"","
91,).exec();
91,Note: It is important to remember that not finding any results is not an error for a search — but it may be a fail-case in the context of your application.
91,If your application expects a search to find a value you can check the number of entries returned in the result.
91,"Query APIs, such as find(), return a variable of type Query."
91,You can use a query object to build up a query in parts before executing it with the exec() method.
91,exec() executes the query and returns a promise that you can await on for the result.
91,js// find all athletes that play tennis
91,"const query = Athlete.find({ sport: ""Tennis"" });"
91,// selecting the 'name' and 'age' fields
91,"query.select(""name age"");"
91,// limit our results to 5 items
91,query.limit(5);
91,// sort by age
91,query.sort({ age: -1 });
91,// execute the query at a later time
91,query.exec();
91,"Above we've defined the query conditions in the find() method. We can also do this using a where() function, and we can chain all the parts of our query together using the dot operator (.) rather than adding them separately."
91,"The code fragment below is the same as our query above, with an additional condition for the age."
91,jsAthlete.find()
91,".where(""sport"")"
91,".equals(""Tennis"")"
91,".where(""age"")"
91,.gt(17)
91,.lt(50) // Additional where query
91,.limit(5)
91,.sort({ age: -1 })
91,".select(""name age"")"
91,.exec();
91,"The find() method gets all matching records, but often you just want to get one match. The following methods query for a single record:"
91,findById(): Finds the document with the specified id (every document has a unique id).
91,findOne(): Finds a single document that matches the specified criteria.
91,"findByIdAndDelete(), findByIdAndUpdate(), findOneAndRemove(), findOneAndUpdate(): Finds a single document by id or criteria and either updates or removes it. These are useful convenience functions for updating and removing records."
91,Note: There is also a countDocuments() method that you can use to get the number of items that match conditions. This is useful if you want to perform a count without actually fetching the records.
91,There is a lot more you can do with queries. For more information see: Queries (Mongoose docs).
91,Working with related documents — population
91,"You can create references from one document/model instance to another using the ObjectId schema field, or from one document to many using an array of ObjectIds. The field stores the id of the related model. If you need the actual content of the associated document, you can use the populate() method in a query to replace the id with the actual data."
91,"For example, the following schema defines authors and stories."
91,"Each author can have multiple stories, which we represent as an array of ObjectId."
91,Each story can have a single author.
91,The ref property tells the schema which model can be assigned to this field.
91,"jsconst mongoose = require(""mongoose"");"
91,const Schema = mongoose.Schema;
91,const authorSchema = new Schema({
91,"name: String,"
91,"stories: [{ type: Schema.Types.ObjectId, ref: ""Story"" }],"
91,});
91,const storySchema = new Schema({
91,"author: { type: Schema.Types.ObjectId, ref: ""Author"" },"
91,"title: String,"
91,});
91,"const Story = mongoose.model(""Story"", storySchema);"
91,"const Author = mongoose.model(""Author"", authorSchema);"
91,We can save our references to the related document by assigning the _id value.
91,"Below we create an author, then a story, and assign the author id to our story's author field."
91,"jsconst bob = new Author({ name: ""Bob Smith"" });"
91,await bob.save();
91,"// Bob now exists, so lets create a story"
91,const story = new Story({
91,"title: ""Bob goes sledding"","
91,"author: bob._id, // assign the _id from our author Bob. This ID is created by default!"
91,});
91,await story.save();
91,Note: One great benefit of this style of programming is that we don't have to complicate the main path of our code with error checking.
91,"If any of the save() operations fail, the promise will reject and an error will be thrown."
91,"Our error handling code deals with that separately (usually in a catch() block), so the intent of our code is very clear."
91,"Our story document now has an author referenced by the author document's ID. In order to get the author information in the story results we use populate(), as shown below."
91,"jsStory.findOne({ title: ""Bob goes sledding"" })"
91,".populate(""author"") // Replace the author id with actual author information in results"
91,.exec();
91,"Note: Astute readers will have noted that we added an author to our story, but we didn't do anything to add our story to our author's stories array. How then can we get all stories by a particular author? One way would be to add our story to the stories array, but this would result in us having two places where the information relating authors and stories needs to be maintained."
91,"A better way is to get the _id of our author, then use find() to search for this in the author field across all stories."
91,jsStory.find({ author: bob._id }).exec();
91,This is almost everything you need to know about working with related items for this tutorial. For more detailed information see Population (Mongoose docs).One schema/model per file
91,"While you can create schemas and models using any file structure you like, we highly recommend defining each model schema in its own module (file), then exporting the method to create the model."
91,This is shown below:
91,js// File: ./models/somemodel.js
91,// Require Mongoose
91,"const mongoose = require(""mongoose"");"
91,// Define a schema
91,const Schema = mongoose.Schema;
91,const SomeModelSchema = new Schema({
91,"a_string: String,"
91,"a_date: Date,"
91,});
91,"// Export function to create ""SomeModel"" model class"
91,"module.exports = mongoose.model(""SomeModel"", SomeModelSchema);"
91,You can then require and use the model immediately in other files. Below we show how you might use it to get all instances of the model.
91,js// Create a SomeModel model just by requiring the module
91,"const SomeModel = require(""../models/somemodel"");"
91,// Use the SomeModel object (model) to find all SomeModel records
91,const modelInstances = await SomeModel.find().exec();
91,"Setting up the MongoDB databaseNow that we understand something of what Mongoose can do and how we want to design our models, it's time to start work on the LocalLibrary website. The very first thing we want to do is set up a MongoDB database that we can use to store our library data."
91,"For this tutorial, we're going to use the MongoDB Atlas cloud-hosted sandbox database. This database tier is not considered suitable for production websites because it has no redundancy, but it is great for development and prototyping. We're using it here because it is free and easy to set up, and because MongoDB Atlas is a popular database as a service vendor that you might reasonably choose for your production database (other popular choices at the time of writing include Compose, ScaleGrid and ObjectRocket)."
91,"Note: If you prefer, you can set up a MongoDB database locally by downloading and installing the appropriate binaries for your system. The rest of the instructions in this article would be similar, except for the database URL you would specify when connecting."
91,"In the Express Tutorial Part 7: Deploying to Production tutorial we host both the application and database on Railway, but we could equally well have used a database on MongoDB Atlas."
91,"You will first need to create an account with MongoDB Atlas (this is free, and just requires that you enter basic contact details and acknowledge their terms of service)."
91,"After logging in, you'll be taken to the home screen:"
91,Click the + Create button in the Overview section.
91,This will open the Deploy your database screen. Click on the M0 FREE option template.
91,Scroll down the page to see the different options you can choose.
91,Select any provider and region from the Provider and Region sections. Different regions offer different providers.
91,You can change the name of your Cluster under Cluster Name. We are naming it Cluster0 for this tutorial.
91,Tags are optional. We will not use them here.
91,Click the Create button (creation of the cluster will take some minutes).
91,This will open the Security Quickstart section.
91,Enter a username and password. Remember to copy and store the credentials safely as we will need them later on. Click the Create User button.
91,Note: Avoid using special characters in your MongoDB user password as mongoose may not parse the connection string properly.
91,Enter 0.0.0.0/0 in the IP Address field. This tells MongoDB that we want to allow access from anywhere. Click the Add Entry button.
91,Note: It is a best practice to limit the IP addresses that can connect to your database and other resources. Here we allow a connection from anywhere because we don't know where the request will come from after deployment.
91,Click the Finish and Close button.
91,This will open the following screen. Click on the Go to Overview button.
91,You will return to the Overview screen. Click on the Database section under the Deployment menu on the left. Click the Browse Collections button.
91,This will open the Collections section. Click the Add My Own Data button.
91,This will open the Create Database screen.
91,Enter the name for the new database as local_library.
91,Enter the name of the collection as Collection0.
91,Click the Create button to create the database.
91,You will return to the Collections screen with your database created.
91,Click the Overview tab to return to the cluster overview.
91,From the Cluster0 Overview screen click the Connect button.
91,This will open the Connect to Cluster screen.
91,Click the Drivers option under the Connect to your application section.
91,You will now be shown the Connect screen.
91,Select the Node driver and version as shown.
91,DO NOT follow the step 2.
91,Click the Copy icon to copy the connection string.
91,Paste this in your local text editor.
91,Update the username and password with your user's password.
91,"Insert the database name ""local_library"" in the path before the options (...mongodb.net/local_library?retryWrites...)"
91,Save the file containing this string somewhere safe.
91,"You have now created the database, and have a URL (with username and password) that can be used to access it."
91,This will look something like: mongodb+srv://your_user_name:your_password@cluster0.lz91hw2.mongodb.net/local_library?retryWrites=true&w=majority
91,Install Mongoose
91,Open a command prompt and navigate to the directory where you created your skeleton Local Library website.
91,"Enter the following command to install Mongoose (and its dependencies) and add it to your package.json file, unless you have already done so when reading the Mongoose Primer above."
91,bashnpm install mongoose
91,Connect to MongoDB
91,Open /app.js (in the root of your project) and copy the following text below where you declare the Express application object (after the line const app = express();).
91,Replace the database URL string ('insert_your_database_url_here') with the location URL representing your own database (i.e. using the information from MongoDB Atlas).
91,js// Set up mongoose connection
91,"const mongoose = require(""mongoose"");"
91,"mongoose.set(""strictQuery"", false);"
91,"const mongoDB = ""insert_your_database_url_here"";"
91,main().catch((err) => console.log(err));
91,async function main() {
91,await mongoose.connect(mongoDB);
91,"As discussed in the Mongoose primer above, this code creates the default connection to the database and reports any errors to the console."
91,Note that hard-coding database credentials in source code as shown above is not recommended.
91,"We do it here because it shows the core connection code, and because during development there is no significant risk that leaking these details will expose or corrupt sensitive information."
91,We'll show you how to do this more safely when deploying to production!
91,Defining the LocalLibrary Schema
91,"We will define a separate module for each model, as discussed above."
91,Start by creating a folder for our models in the project root (/models) and then create separate files for each of the models:
91,/express-locallibrary-tutorial
91,// the project root
91,/models
91,author.js
91,book.js
91,bookinstance.js
91,genre.js
91,Author model
91,Copy the Author schema code shown below and paste it into your ./models/author.js file.
91,"The schema defines an author as having String SchemaTypes for the first and family names (required, with a maximum of 100 characters), and Date fields for the dates of birth and death."
91,"jsconst mongoose = require(""mongoose"");"
91,const Schema = mongoose.Schema;
91,const AuthorSchema = new Schema({
91,"first_name: { type: String, required: true, maxLength: 100 },"
91,"family_name: { type: String, required: true, maxLength: 100 },"
91,"date_of_birth: { type: Date },"
91,"date_of_death: { type: Date },"
91,});
91,// Virtual for author's full name
91,"AuthorSchema.virtual(""name"").get(function () {"
91,// To avoid errors in cases where an author does not have either a family name or first name
91,// We want to make sure we handle the exception by returning an empty string for that case
91,"let fullname = """";"
91,if (this.first_name && this.family_name) {
91,"fullname = `${this.family_name}, ${this.first_name}`;"
91,return fullname;
91,});
91,// Virtual for author's URL
91,"AuthorSchema.virtual(""url"").get(function () {"
91,// We don't use an arrow function as we'll need the this object
91,return `/catalog/author/${this._id}`;
91,});
91,// Export model
91,"module.exports = mongoose.model(""Author"", AuthorSchema);"
91,"We've also declared a virtual for the AuthorSchema named ""url"" that returns the absolute URL required to get a particular instance of the model — we'll use the property in our templates whenever we need to get a link to a particular author."
91,Note: Declaring our URLs as a virtual in the schema is a good idea because then the URL for an item only ever needs to be changed in one place.
91,"At this point, a link using this URL wouldn't work, because we haven't got any routes handling code for individual model instances."
91,We'll set those up in a later article!
91,"At the end of the module, we export the model.Book model"
91,Copy the Book schema code shown below and paste it into your ./models/book.js file.
91,"Most of this is similar to the author model — we've declared a schema with a number of string fields and a virtual for getting the URL of specific book records, and we've exported the model."
91,"jsconst mongoose = require(""mongoose"");"
91,const Schema = mongoose.Schema;
91,const BookSchema = new Schema({
91,"title: { type: String, required: true },"
91,"author: { type: Schema.Types.ObjectId, ref: ""Author"", required: true },"
91,"summary: { type: String, required: true },"
91,"isbn: { type: String, required: true },"
91,"genre: [{ type: Schema.Types.ObjectId, ref: ""Genre"" }],"
91,});
91,// Virtual for book's URL
91,"BookSchema.virtual(""url"").get(function () {"
91,// We don't use an arrow function as we'll need the this object
91,return `/catalog/book/${this._id}`;
91,});
91,// Export model
91,"module.exports = mongoose.model(""Book"", BookSchema);"
91,The main difference here is that we've created two references to other models:
91,"author is a reference to a single Author model object, and is required."
91,genre is a reference to an array of Genre model objects. We haven't declared this object yet!
91,BookInstance model
91,"Finally, copy the BookInstance schema code shown below and paste it into your ./models/bookinstance.js file."
91,"The BookInstance represents a specific copy of a book that someone might borrow and includes information about whether the copy is available, on what date it is expected back, and ""imprint"" (or version) details."
91,"jsconst mongoose = require(""mongoose"");"
91,const Schema = mongoose.Schema;
91,const BookInstanceSchema = new Schema({
91,"book: { type: Schema.Types.ObjectId, ref: ""Book"", required: true }, // reference to the associated book"
91,"imprint: { type: String, required: true },"
91,status: {
91,"type: String,"
91,"required: true,"
91,"enum: [""Available"", ""Maintenance"", ""Loaned"", ""Reserved""],"
91,"default: ""Maintenance"","
91,"due_back: { type: Date, default: Date.now },"
91,});
91,// Virtual for bookinstance's URL
91,"BookInstanceSchema.virtual(""url"").get(function () {"
91,// We don't use an arrow function as we'll need the this object
91,return `/catalog/bookinstance/${this._id}`;
91,});
91,// Export model
91,"module.exports = mongoose.model(""BookInstance"", BookInstanceSchema);"
91,The new things we show here are the field options:
91,"enum: This allows us to set the allowed values of a string. In this case, we use it to specify the availability status of our books (using an enum means that we can prevent mis-spellings and arbitrary values for our status)."
91,"default: We use default to set the default status for newly created book instances to ""Maintenance"" and the default due_back date to now (note how you can call the Date function when setting the date!)."
91,"Everything else should be familiar from our previous schema.Genre model - challenge!Open your ./models/genre.js file and create a schema for storing genres (the category of book, e.g. whether it is fiction or non-fiction, romance or military history, etc.)."
91,The definition will be very similar to the other models:
91,The model should have a String SchemaType called name to describe the genre.
91,This name should be required and have between 3 and 100 characters.
91,"Declare a virtual for the genre's URL, named url."
91,Export the model.
91,Testing — create some itemsThat's it. We now have all models for the site set up!
91,In order to test the models (and to create some example books and other items that we can use in our next articles) we'll now run an independent script to create items of each type:
91,Download (or otherwise create) the file populatedb.js inside your express-locallibrary-tutorial directory (in the same level as package.json).
91,"Note: The code in populatedb.js may be useful in learning JavaScript, but understanding it is not necessary for this tutorial."
91,"Run the script using node in your command prompt, passing in the URL of your MongoDB database (the same one you replaced the insert_your_database_url_here placeholder with, inside app.js earlier):"
91,bashnode populatedb <your MongoDB url>
91,"Note: On Windows you need to wrap the database URL inside double ("")."
91,On other operating systems you may need single (') quotation marks.
91,"The script should run through to completion, displaying items as it creates them in the terminal."
91,Note: Go to your database on MongoDB Atlas (in the Collections tab).
91,"You should now be able to drill down into individual collections of Books, Authors, Genres and BookInstances, and check out individual documents."
91,"SummaryIn this article, we've learned a bit about databases and ORMs on Node/Express, and a lot about how Mongoose schema and models are defined. We then used this information to design and implement Book, BookInstance, Author and Genre models for the LocalLibrary website."
91,"Last of all, we tested our models by creating a number of instances (using a standalone script). In the next article we'll look at creating some pages to display these objects.See also"
91,Database integration (Express docs)
91,Mongoose website (Mongoose docs)
91,Mongoose Guide (Mongoose docs)
91,Validation (Mongoose docs)
91,Schema Types (Mongoose docs)
91,Models (Mongoose docs)
91,Queries (Mongoose docs)
91,Population (Mongoose docs)
91,Previous
91,Overview: Express Nodejs
91,Next
91,"Help improve MDNWas this page helpful to you?YesNoLearn how to contribute.This page was last modified on Feb 26, 2024 by MDN contributors.View this page on GitHub • Report a problem with this contentMDN logoYour blueprint for a better internet.MDN on MastodonMDN on X (formerly Twitter)MDN on GitHubMDN Blog RSS FeedMDNAboutBlogCareersAdvertise with usSupportProduct helpReport an issueOur communitiesMDN CommunityMDN ForumMDN ChatDevelopersWeb TechnologiesLearn Web DevelopmentMDN PlusHacks BlogMozilla logoWebsite Privacy NoticeCookiesLegalCommunity Participation GuidelinesVisit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.Portions of this content are ©1998–2024 by individual mozilla.org contributors. Content available under a Creative Commons license."
92,Blog - Page 3 of 167 - Brent Ozar Unlimited®
92,"Log InContact BrentProduct has been added to your cart.ConsultingPerformance TuningRemote DBA ServicesUpgrades and MigrationsConsultant ToolkitTrainingMy Videos and DownloadsMy AccountPrivate SQL Server TrainingTraining FAQMonitoringScriptssp_Blitz – free health checksp_BlitzCache – find queries to tunesp_BlitzFirst – instant performance checksp_BlitzIndex – design the right indexessp_BlitzWho – what’s happening right nowPasteThePlan – share query plansBlogT-SQLQuery ExercisesExecution PlansIndexingVideosArchitectureBackup and RecoveryCloud ComputingDevelopmentFirst Responder KitHigh AvailabilityHumorLocking, Blocking, and Isolation LevelsParameter SniffingProduction Database AdministrationProfessional DevelopmentSQL ConstantCare"
92,BlogHomeBlogPage 3
92,"Query Exercise: Find Foreign Key ProblemsLast Updated January 10, 2024Brent OzarQuery Exercises21 CommentsFor 2024, I’m trying something new: weekly homework challenges! For this week, let’s say we’ve decided to implement foreign keys, and we need to find data that’s going to violate our desired keys."
92,"We’re going to use the Stack Overflow database, and we’ll focus on these 3 tables:"
92,dbo.Users table: with Id column as its primary key
92,dbo.Posts table: with OwnerUserId column noting which Users.Id wrote the post
92,"dbo.Comments table: with UserId column noting which Users.Id wrote the comment, and PostId column noting which Posts.Id is being commented on"
92,"Before we attempt to implement foreign keys, we need to find data which might violate the foreign key relationships. Are there any:"
92,Posts rows whose OwnerUserId does not match up with a valid Users.Id
92,Comments rows whose UserId doesn’t match up with a valid Users.Id
92,Comments rows whose PostId doesn’t match up with a valid Posts.Id
92,"And to make your task easier, let’s focus on just the first 100K rows in each table (rows with an Id <= 100000) to see whether or not foreign keys make sense for this database"
92,Your query exercise has a few parts:
92,Write one or more queries to find these answers as quickly as possible with low load on the database.
92,"Given what you find, hypothesize about what might have caused the foreign key problems."
92,"Given what you learned, are there any changes you want to make to the app, processes, or database?"
92,"You can post your answers in this blog post’s comments, and discuss each others’ ideas. We discuss the challenges & techniques in the next post. Have fun!"
92,"[Video] Office Hours: Oddball Questions EditionLast Updated December 30, 2023Brent OzarVideos0The last Office Hours of 2023 featured some oddball questions from https://pollgab.com/room/brento. Not bad, just … odd."
92,https://youtu.be/prkpWssHsaE
92,Here’s what we covered:
92,00:00 Start
92,"03:04 TheMooneyFlyer: Hey Brent, how do you work on optimizing sp that performs insert/update/delete? Does putting the exec within a begin tran / rollback is a good option?"
92,"06:06 MyTeaGotCold: If a table is empty and I absolutely know that nobody else is using it, should I always insert in to it WITH (TABLOCK)? What if it’s a temp table?"
92,07:09 Tonia S: Have you tried the mock DBA interviews with ChatGPT? Very realistic?
92,"09:11 ChompingBits: What “Best Practice” pays your bills the most? I’m thinking DBAs with superstitions they follow that cause issues in newer versions, but if you’ve got suggestions that almost no one follows so you come in and clean up in an afternoon, I’d like to hear that too."
92,11:33 Nardole: What are the top SSIS issues you see with your clients? Anything performance related?
92,11:45 Philo: Is windows paging of sqlserver always bad? What are the top issues you see with windows paging?
92,12:00 OnSiteDBA: You mentioned that shops that go multi-terabyte need to do snapshots backups instead of the native backups given the reduced restore times typical of snapshots. How does one handle possible inconsistencies in MSSQL configurations with data and log files in different volumes?
92,"12:36 crushingtempdb: Hi Brent! I am troubleshooting some tempdb issues on Azure SQL Database; When I read the documentation; mo cores=mo tempdb, we’re told; When I run0 select Sum (max_size)/1024.0/1024.0 FROM tempdb.sys.database_files WHERE type_desc = ‘ROWS’ it doesn’t match. Thoughts?"
92,13:45 Karthik: Have you ever had to run the windows debugger against sqlserver.exe? What was the scenario?
92,17:42 Bonnie: What are your best and worst observed times for manually scaling azure SQL VM to a higher SKU?
92,"19:36 MyTeaGotCold: How should I manage an effort to refactor away from Hungarian notation (e.g. “sp_” prefixes)? Even when I win the battle on clarity and performance, I lose it on the fear that the changes will break something."
92,"Who’s Hiring in the Microsoft Data Platform Community? January 2024 EditionLast Updated December 30, 2023Brent OzarWho's Hiring9 CommentsIs your company hiring for a database position as of January 2024? Do you wanna work with the kinds of people who read this blog? Let’s set up some rapid networking here."
92,"If your company is hiring, leave a comment. The rules:"
92,"Your comment must include the job title, and either a link to the full job description, or the text of it. It doesn’t have to be a SQL Server DBA job, but it does have to be related to databases. (We get a pretty broad readership here – it can be any database.)"
92,"An email address to send resumes, or a link to the application process – if I were you, I’d put an email address because you may want to know that applicants are readers here, because they might be more qualified than the applicants you regularly get."
92,"Please state the location and include REMOTE and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE."
92,"Please only post if you personally are part of the hiring company—no recruiting firms or job boards. Only one post per company. If it isn’t a household name, please explain what your company does."
92,Commenters: please don’t reply to job posts to complain about something. It’s off topic here.
92,Readers: please only email if you are personally interested in the job.
92,"If your comment isn’t relevant or smells fishy, I’ll delete it. If you have questions about why your comment got deleted, or how to maximize the effectiveness of your comment, contact me."
92,"Each month, I publish a new post in the Who’s Hiring category here so y’all can get the latest opportunities."
92,"[Video] Office Hours: Holiday Speed Round EditionLast Updated December 25, 2023Brent OzarVideos1 CommentMost of the https://pollgab.com/room/brento questions from this episode had really short and sweet answers. Let’s take a break for the holidays and knock ’em out:"
92,Here’s what we covered this episode:
92,00:00 Start
92,03:29 TheyBlameMe: Have you ever made it across the finish line for a customer by changing the way their application connects to SQL Server (i.e. OLE DB vs ODBC driver)?
92,"03:51 Sleepless in Seattle: How often, if ever, do you use partially contained databases?"
92,"04:36 GUIDKeysWasteAndFlushBufferCache: With a GUID PK & CX on a large table, most new rows will land on 8K pages containing only old, obsolete data. Almost every new row (that will be accessed again soon) has a whole 8K page in the buffer cache! Buffer cache will be mostly old, obsolete data. Am I missing something?"
92,06:26 Gökçe: What should you do when you notice the third party vendor app is using NOLOCK like butter all over their TSQL queries? Have you experienced this?
92,06:59 Björgvin B.: Entity Framework is spamming the SQL plan cache with many duplicate plans. How do you deal with this issue? Use sp’s instead?
92,07:22 Anastasios: Do you have any good stories where SQL Server was blamed for slow app performance but turned out to be something completely unrelated to SQL Server?
92,08:27 MyTeaGotCold: Has your opinion on agent jobs changed in the past 5 years? AWS lambda and its kin seem to be replacing the Task Scheduler.
92,"09:13 Stooky Sue: As SQL migrates from on-prem to the cloud, do you still see separation of duties between DBA and Storage Admins?"
92,10:12 bagllop: What would you suggest my friend to use instead of linked servers? SSIS? OpenQuery? Or is there some new fancy stuff he could use?
92,11:41 Rena: Is adding foreign keys just to get join elimination ever advised?
92,12:28 Persephone: What scripts do you like to use for Columnstore Index maint?
92,12:52 Encino Man: Is there a good way to determine if a given query from C# is using the density vector or histogram for cardinality estimates?
92,"13:40 Stooky Bill: For SQL Server, Microsoft recommends a minimum target for page life expectancy of 300 seconds. Is this still a good recommendation or does it need updating for modern times?"
92,"14:10 DBADany: Hey Brent, in a working environment with multiple DBAs having sysadmin permissions, are you aware of anyway we could audit who restarted SQL Server? Apparently SQL Audit only tracks the time for stop/start itself but no details of hostname or IP from who did it? Thanks"
92,15:15 Huyang: What is your opinion of Azure NetApp Files and Silk cloud ISCSI SAN for hosting Azure SQL VM data files?
92,"15:26 OnSiteDBA: I have heard you mention PostgreSQL as a cheaper RDBMS alternative to MSSQL, but hardly mention MySQL. Is there a reason you hardly mention MySQL? notable performance hitches, feature-specific limitations etc."
92,"16:46 KyleDevDBA: Hi Brent, Do you have a favorite wait type to fix and why?"
92,17:12 Renzi: What are your pros / cons of app consistent snapshot backups vs crash consistent snapshot backups for multi TB boxed SQL db’s?
92,18:29 Stooky Bill: What are your thoughts on the TOP operator short circuiting parallelism? Good thing or bad thing?
92,20:07 ChompingBits: Why is it that the Microsoft owned apps are always the worst offenders with SQL issues? Sharepoint and SCCM have longstanding known issues with deadlocks. SCOM is a beast that spawns all kind of GUID named agents jobs. SCORCH doesn’t support availability groups.
92,20:57 Argyris P.: Do you have any good ways to find all large (billions of rows) static tables in a boxed SQL Server instance?
92,21:38 Alex: Huge fan. What’s your opinion on azure database fleet and should I aim to replace my elastic pools with this new feature or is it another gimmick?
92,"22:32 Pytzamarama: Hi Brent! When we update our customers databases (a lot of them are still on SQL Server 2008R2) for a new app version, we drop/create all procedures and triggers. How does this impact performance? Thanx"
92,"24:12 Toymaker: Is there ever value in perf testing queries cold vs hot (I.e. DBCC DROPCLEANBUFFERS, DBCC FREEPROCCACHE)?"
92,"24:37 WouldPrefer3rdNormalForm: Have you ever seen or heard of a ceiling/threshold for XML columns, beyond which performance craters? We are scared by two large XML columns (and their indexes). They are currently performing adequately, but comprise about about 40% of our 1.5TB database and continue to grow…"
92,25:49 Kane Baden: Was literally typing a question around “what do you think are the main questions around picking MSSQL vs. Postgres…” while watching your last upload when you spelled it out for the last question on that SaaS question. So I figured I should let you know thanks instead! Thanks!
92,26:10 Bonnie: How do you run sp_blitzcache to target a temporary stored procedures for analysis?
92,26:35 bottomless: Azure SQL Database Serverless is supposed to cost less but at the end of the month it costs more than DTU: our SaaS has routines and jobs that wake up the database. From you experience have you aver seen a successful use of Azure SQL Database Serverless?
92,"Updated First Responder Kit and Consultant Toolkit for December 2023Last Updated December 25, 2023Brent OzarFirst Responder Kit Updates1 CommentNew this month: more work on letting sp_Blitz run with limited permissions, nicer Markdown output, sp_BlitzLock compatibility with Managed Instances, and more."
92,Wanna watch me use it? Take the class.
92,To get the new version:
92,Download the updated FirstResponderKit.zip
92,Azure Data Studio users with the First Responder Kit extension:
92,"ctrl/command+shift+p, First Responder Kit: Import."
92,PowerShell users: run Install-DbaFirstResponderKit from dbatools
92,Get The Consultant Toolkit to quickly export the First Responder Kit results into an easy-to-share spreadsheet
92,Consultant Toolkit Changes
92,"There are two changes to the spreadsheet in this release. In the “Plans Duplicated” and “Plans by Query Hash” tabs, we’ve added new columns for Compile Time MS, Compile CPU MS, and Compile Memory KB. These values are for the one specific plan you’re looking at in the row, not the total amount for that duplicated plan. It gives you a rough idea of how resource-intensive these queries are each time they run. (Thanks Erik Darling.)"
92,If you’ve customized your query_manifest.json and/or your spreadsheet
92,"And you don’t want the new columns, then you can simply copy your customized query_manifest.json and/or spreadsheet over ours just like you normally do. Nothing will error out – you just won’t have the new columns."
92,"And you do want the new columns, then you’ll need to merge our query_manifest.json changes for queries 50 & 82, and copy the tabs “Plans Duplicated” and “Plans by Query Hash” over those tabs in your customized spreadsheet."
92,sp_Blitz Changes
92,"Enhancement: way prettier output when @OutputType = ‘markdown’. (#3401, thanks Mike Scalise.)"
92,"Enhancement: simpler, more intuitive checks for Instant File Initialization. (#3362 and #3409, thanks Montro1981.)"
92,"Enhancement: if we had to skip checks because you didn’t have enough permissions, we now warn you about that. (#3376, thanks Montro1981.)"
92,"Fix: continued work on detecting msdb permissions. (#3377, thanks Montro1981.)"
92,sp_BlitzCache Changes
92,"Fix: @Debug = 1 was skipping a character of the dynamic SQL. (#3406, thanks Per Scheffer and Montro1981.)"
92,sp_BlitzIndex Changes
92,"Fix: @Debug = 1 was skipping a character of the dynamic SQL. (#3406, thanks Per Scheffer and Montro1981.)"
92,"Fix: added drop-if-exists statement for temp tables to make it easier to run parts of sp_BlitzIndex ad-hoc, outside of a stored proc. (#3383, thanks Chad Baldwin.)"
92,sp_BlitzLock Changes
92,"Fix: better compatibility with Azure Managed Instances. (#3392, thanks Fatima Brunson and Erik Darling.)"
92,"Fix: won’t error out when sys.dm_exec_query_stats reports a last_execution_time of 1900-01-01. (#3385, thanks teej21012 and Erik Darling.)"
92,sp_BlitzQueryStore Changes
92,"Enhancement: added @Help = 1 output. (#3388, thanks Vlad Drumea.)"
92,sp_ineachdb Changes
92,"Enhancement: new @is_ag_writeable_copy parameter to only run queries against those databases. (#3399, thanks Douglas Taft.)"
92,For Support
92,"When you have questions about how the tools work, talk with the community in the #FirstResponderKit Slack channel. Be patient: it’s staffed by volunteers with day jobs. If it’s your first time in the community Slack, get started here."
92,"When you find a bug or want something changed, read the contributing.md file."
92,"When you have a question about what the scripts found, first make sure you read the “More Details” URL for any warning you find. We put a lot of work into documentation, and we wouldn’t want someone to yell at you to go read the fine manual. After that, when you’ve still got questions about how something works in SQL Server, post a question at DBA.StackExchange.com and the community (that includes me!) will help. Include exact errors and any applicable screenshots, your SQL Server version number (including the build #), and the version of the tool you’re working with."
92,"[Video] Working on First Responder Kit Pull RequestsLast Updated December 25, 2023Brent OzarVideos0For this month’s First Responder Kit releases, I worked through most of the pull requests live on my Twitch channel. If you wanna get a glimpse of what it’s like being an open source maintainer, this is a good behind-the-scenes look:"
92,"And part 2, with more PRs:"
92,"The Annual Data Professional Salary Survey Closes This Week!Last Updated January 4, 2024Brent OzarSalary2 CommentsTake the Data Professional Salary Survey now. The survey has closed."
92,"The 2020s have been tough: a pandemic, a recession, layoffs, and inflation. Inflation makes things particularly tricky because your costs for everything have risen a lot in the last year, but at the same time… has your salary? What about your peers? You’re in a tough position because it’s hard to ask for more money when there are layoffs everywhere. I feel you."
92,"So it’s time for our annual salary survey to find out what data professionals make. You fill out the data, we open source the whole thing, and you can analyze the data to spot trends and do a better job of negotiating your own salary."
92,"The anonymous survey closes Sunday, Jan 1. The results are completely open source, and shared with the community for your analysis. (You can analyze ’em now mid-flight, but I’d wait until the final results come in. I’ll combine them into a single spreadsheet with the past results, and publish those on January 9th.)"
92,Thanks for your help in giving everybody in the community a better chance to talk honestly with their managers about salary.
92,"[Video] Office Hours: 21 Good QuestionsLast Updated December 15, 2023Brent OzarVideos2 CommentsThis one is a 3-part episode: I take 21 questions from https://pollgab.com/room/brento and then later, work on First Responder Kit pull requests."
92,Here’s what we covered:
92,00:00 Start
92,"01:08 MooneyFlyer: Hey Brent, apart from installing and playing around with it, what is the best way to get started with Postgres for someone familiar with SQL Server?"
92,"02:04 Frozt: Hi Brent, I would just like to inquire if do you have DBA tasks in mind that can be delegated to a command center(Service desk) as a DBA or this tasks should remain to DBA for job security and also to lessen risk of service desk doing something wrong on Prod server. Thank you"
92,"03:29 Steven: Hi Brent, my friends BI tool joins an aggregated temp (1m rows with 13% sampling) and lookup table (40 rows). Both scan estimates are accurate and all rows match but the hash join estimates 1 rows. Any ideas on why the estimate could be so far off?"
92,04:13 Frank Castle: What’s your opinion of Azure AI search?
92,"04:22 Need Memory Dump: Have server with 1TB of ram, min set to 0, max to 900GB. Server takes all 900GB as soon as I turn on the service, have not run anything. Any ideas?"
92,05:25 Heinrich: What is your opinion of Azure Cosmos DB? Does it compete well with AWS Aurora for new software development?
92,06:39 Jose: Why is it slower to do a SELECT * INTO a temp table at the beginning of the proc than it is to SELECT the columns you need then INSERT them INTO the temp table?
92,"07:25 James Fogel: I don’t have a question, just a thank you. I’ve learned a lot from you and I’m sure countless others have. Thank you for what you do and all you give to us. You are appreciated."
92,"07:48 ChompingBits: What is your preferred way to run the same query against a bunch of servers? Using T-SQL and linked servers? Agent jobs pushed out from a Central Management Server? PowerShell/DBATools? I assume it’s some form of all of the above, when is each the best practice?"
92,08:34 Perseus: Should joins on natural keys on large tables always be avoided in favor of joins on surrogate keys?
92,09:40 Wilson Fisk: What is your opinion of the new Azure ARC SQL performance dashboards?
92,10:35 MyTeaGotCold: Is it bad practice to have my server query itself and write the results to CSV? I see it a lot in SSIS.
92,"11:11 marcus-the-german: Hi Brent, you use sp_ as the prefix of you stored procedures. Why? AFAIK it’s recommended not to use sp_ for user procedures."
92,"11:58 Eugene: Which is better, a DBA specializing in one platform or multiple platforms?"
92,13:29 Ya?mur: Would like to upgrade our Azure SQL VM from SQL 2019 standard to 2019 enterprise version so we can leverage more memory and cores. Is this an ok exception to do as an inplace upgrade?
92,14:00 Mumtaz: Can we audit IUD transactions without configure any audit features ?
92,15:01 ImAfraidOfBI: You know how they say dogs usually look like their owners… I saw it a bit with the cute dog you picked up last time! No offence dog’s cute!
92,"15:32 KyleDevDBA: Hi Brent. Curious about the origin story of the naming of the sp_Blitz* scripts (why they start with Blitz). Funny acronym, interesting story, dark past? I searched around, but wasn’t able to find anything. Thank you for all you do for the SQL community."
92,16:31 Hugo: How do you find queries that are spamming the plan cache due to different SET options?
92,"16:51 Henrik Fältström: Is there any benefit of creating an index on an index for LARGE read-only tables? Or are there other ways in SQLServer to accomplish fast access? I know it’s not possible today in SQLServer, unless you implement something yourself, Has anyone done this?"
92,17:22 Nicolaj Lindtner: I’m building a saas. Considering sqlserver vs postgresql. My suspision is it really doesn’t matter – so will probally go for postgresql. Input ?
92,"[Video] Office Hours: Wrong Side of the Bed EditionLast Updated December 12, 2023Brent OzarVideos1 CommentI woke up on the wrong side of the bed after a nap. How does that even happen?!? To take the edge off, I poured myself a gin & tonic and went through your top-voted questions from https://pollgab.com/room/brento."
92,Here’s what we covered:
92,00:00 Start
92,"05:41 LongRunningBackups: You say to use SAN snapshot backups on larger DBs (multi-TB). I have searched “Ozar SAN Snapshot”, but your article has a broken link. Other searches don’t provide relevant results because “snapshot” is a common/overused term. Is this a Fundamentals or Mastering level topic?"
92,"06:52 Asking For a Friend: Follow up on the “Are cloud databases overrated” question Dec 1. No, the on-prem VM is not “free”, but it is a sunk cost since it already exists with many DBs, is licensed, is here to stay. Moving the db on prem simply eliminates monthly cloud costs. Does that change your answer?"
92,"09:17 TheMooneyFlyer: How often do you come across performance issues that cannot be solved by index, queries or server optimization but requires an application redesign? How do you manage this so your client is happy with your report?"
92,13:22 Xavier R: If the software vendor is using inefficient coding techniques that impact application/DB performance. Where would you draw the line in helping them? I went as far as giving them a query (which solved a network timeout) to replace their nested stored procedures
92,"15:37 Montro1981: Hi Brent, do you listen to music while working, and what is your favorite type of music you listen?"
92,16:54 Renzi: Do you see any specific A.I. skillsets complementing SQL DBA?
92,19:27 Ömer: What are the top performance related features that you get in boxed SQL Enterprise that you don’t get in standard version?
92,21:03 Kemal: Do you have any suggestions for how to load test TempDB on a prospective new cloud VM?
92,"Interesting Aurora MySQL Feature: The Buffer Pool Survives RestartsLast Updated December 9, 2023Brent OzarAmazon Web Services (AWS)11 Comments“Documentation! Hey, look at that.”"
92,"Lemme start this off by saying this is probably irrelevant to you. (It’s irrelevant to me, too.)"
92,"If you’re strapped for time, just skip past this blog post."
92,This one’s for the curious folks.
92,"AWS Aurora MySQL is Amazon’s flavor of MySQL with their own unique performance and reliability improvements. I’ve never used it, and I don’t see myself using it anytime soon because I just don’t need it. The database back end for the BrentOzar.com blog is MySQL, but I use a managed WordPress hosting partner, so Aurora MySQL is irrelevant there too."
92,"Having said that, I still read the database news because it’s neat to see how companies are innovating, and this new optimization from AWS is intriguing:"
92,"The current implementation of buffer pool in Aurora MySQL employs survivable page cache where each database instance’s page cache is managed in a separate process from the database, which allows the page cache to survive independently of the database."
92,<record scratch> WAT
92,"This is obviously dramatically different from Microsoft SQL Server. In SQL Server, if you restart the SQL Server process:"
92,Dirty (changed) buffer pool pages are written to disk
92,"The SQL Server process shuts down, releasing all memory back to the OS"
92,"The SQL Server process starts again, and has no memory allocated at the beginning (unless you play around with LPIM and minimum server memory settings)"
92,"SQL Server gradually requests memory from the OS as needed, reading data pages up from disk as needed, and caching those pages in the buffer pool"
92,"At first glance, Aurora MySQL’s optimization sounds amazing, but it has a few gotchas. It would seem to only be relevant when:"
92,"The MySQL writeable replica stays on the same server – meaning I would assume it’s less relevant for database patching, since you’d want to patch a passive replica first, then fail over to it. (Although as long as Amazon’s putting in this much work, they could conceivably do the patching live on the same node – I would assume that would result in longer downtime though, as opposed to failing over to an already-patched instance.)"
92,"The MySQL process restarts, but the OS stays up – meaning it’s not relevant for OS patching either."
92,The buffer pool is fairly stable – this doesn’t help you on underpowered servers where everything gets read from disk anyway.
92,"And keep in mind that we’re only talking about the page cache, not things like execution plans, DMV metrics, etc."
92,"This isn’t the only optimization they’ve done, of course. The whole documentation section on Aurora storage and reliability is interesting, like how storage costs automatically drop as you drop tables and indexes. You don’t have to worry about resizing the data files or resizing the underlying OS volumes like you do with Azure SQL DB or conventional SQL Servers."
92,"I’m not saying Aurora MySQL is better than Azure SQL DB or SQL Server, by any means. (I’m not even saying the optimization works, hahaha!) I’m not even saying Microsoft should build this kind of cache persistence for SQL Server! It’s such a niche use case. I’m just saying it’s interesting to see these kinds of advancements in cloud databases."
92,"[Video] Office Hours: Sunshine EditionLast Updated December 12, 2023Brent OzarVideos1 CommentYou know how this works: you posta the questions at https://pollgab.com/room/brento, and I giva the answers:"
92,Here’s what you asked in this episode:
92,00:00 Start
92,"03:18 DBA in VA: I recently discovered the “force order” query hint. I’m usually inclined to let the optimizer do its thing, but I have seen some of our code perform MUCH better with this hint. Are there gotchas/downsides I should know about?"
92,"04:29 RoJo: We call SQL from C# code, but it’s hard in SQL to trace where in code the call came from. Is adding a comment at the end of SQL statement, with module/method info a viable solution or are there better ways to connect SQL statements to actual method calls in a Live system."
92,"06:34 OralceIsBetter: Hi, I have large database ~90 TB, full backup is taking almost 2 days (one time per month). Utilization off SAN network is around 2 gigabits/s. Do You have any tips how I can speed up this process ? Compressions is enabled and backup is running in many threads using couple disks."
92,07:16 Raj: When would you want to manually create statistics without a corresponding index?
92,08:13 Kimberly: I’ve got a 2 node AG cluster on SQL Server 2019. SQL Server sees 2 databases as added to the AG when they were actually removed. The primary shows synced and the secondary doesn’t have the db. Why does sql still think the db is in the AG when it is not?
92,"09:06 Mike: Hi Brent, can you tell in which ways SQL Server is better than PostgreSQL ? Are there any bullet points ? And vice versa, are there any things in which PostgreSQL is better than SQL Server ?"
92,11:26 RollbackIsSingleThreaded: Hi Brent! Writing articles about SQL Server does not earn much money. What do you think is the main advantage of writing articles?
92,"14:49 Lysander: In boxed SQL, when should you use table partitioning vs partitioned views?"
92,15:33 Mike: In which scenarios Failover Cluster Instances are preferable over Availability Groups ? When and why we should use FCI instead of AG ?
92,17:01 AnotherDataLayer: Linked Server vs Polybase: both are doing the similar things if not the same. which one to use when it comes to pull data from another MSSQL server and why. We are using entity framework.
92,"18:35 Don’t Bother Asking: My friend has inherited a database which has lots of nonclustered PKs and very few clustered indexes. DUI query performance is not great. Will adding clustered indexes, or rebuilding PKs as clustered, improve query performance? And if so, are there any gotchas?"
92,19:52 Bonnie: Do you have a good way to determine which operators in a query plan are contributing the most to a memory grant?
92,21:24 Perseus: Is there a good way to know why SQL Server ignores a given query hint?
92,22:29 Ophelia: What are the top signs that the SQL buffer pool is under pressure?
92,23:27 Chrysostomos: Does one database per customer model work well with SQL AG HADR?
92,25:32 Meryem: What’s the best resource for learning how to write efficient linked server queries?
92,25:46 Mandeep: What are the top things you see that break log shipping?
92,"27:00 Pradeep: Given modern fast storage, is clustered column store index fragmentation as inconsequential as non-clustered index fragmentation?"
92,"27:46 Renaldo: For cloud SQL VM, what are top charge back mechanisms you see for billing SQL VM costs back to each customer on the SQL VM?"
92,28:49 Olga: Have you ever had to rebuild all indexes? What was the use case?
92,"30:09 Jessica: Hey brent, meta question. When building PollGab did you intentionally set out to build a site without trackers etc that would be blocked by uBlock Origin? Its amazing to see a clean site for the first time in a while."
92,"30:43 muppet: Hey Brent, my friend has a fairly typically designed table with 1.2 billion row table stored as regular row store. It performs sluggishly so I was thinking of proposing partitioning but what do you think of switching to columnstore instead?"
92,"31:29 Brynjar: In sql server, how do you determine the optimal column order when creating a non-clustered column store index?"
92,"33:38 AllAboutSearch: When someone type the third character for the FName/MName/LName in the FE, it queries the DB’s computed column in the backend, all good. But when there is space in the name, entity framework query starts scanning the whole table and timeouts. Any tip?"
92,"34:53 Montro1981: Hi Brent, I hope you’re doing good. Have there been moments in your long career that you might have taken another path? If, your answer is yes (which is very likely), where might you have ended up in life?"
92,"[Video] Office Hours: Really Good Questions EditionLast Updated December 12, 2023Brent OzarVideos0Y’all were on fire in this one! You posted great questions at https://pollgab.com/room/brento and I worked through ’em:"
92,Here’s what we covered:
92,00:00 Start
92,"04:06 DislikeEntityFramework: In AWS RDS, we inherited a 9TB table with uniqueidentifier as clustered PK, another column is an identity int but not needed. How can we quickly fix this? Drop existing identity, add new bigint identity PK clustered, and make NC index on uniqueidentifier. 0% downtime. Tips?"
92,"05:16 mailbox: I’ve noticed more job listings for PostgreSQL DBA or Data professional on indeed.com. Weird thing– many of these listings ask that you be able to write SP or translate legacy SP to run on PG SQL. Is the job scope of a DBA becoming broader, or do you think these are mis-titled?"
92,06:13 Eh? Aye: You wrote about Postgres/SQL licensing. As compute (CPU & RAM) is getting cheaper (physically & potentially in the Cloud too) at what point will skills in tuning & execution plans etc become redundant due to having a big enough hammer available to the engine to push them through?
92,08:29 mailbox: Are there any OLAP DBMS (clickhouse for example) that can compete performance-wise with SQL Server. Let’s assume SQL Server is handling a similar workload and taking advantage of columnstore indexing.
92,09:19 Wren: Why does SSMS query plan show scalar UDF’s in estimated query plan but not in actual query plan?
92,10:44 Lori: What are the top ways AI will affect SQL DBA’s?
92,11:13 Right Said Fred: Is there a good way to identify indexes with hot columns?
92,"12:10 SQL_bob: Hey Brent, Do you hide your politics at work? How do you avoid getting roped into a political discussion? I have a coworker who is constantly trying to get a rise out of his fellow coworkers."
92,13:57 Stone Breaker: Does Azure SQL DB compare favorably with AWS Aurora PostgreSQL for developing new applications?
92,14:41 Vitali: Have you ever considered offering Office Hours additionally as a podcast?
92,16:18 Jolene: What is your opinion of scoped configuration LAST_QUERY_PLAN_STATS ? Safe to turn on?
92,"17:47 ZappyDBCC: Hello Brent, do you often come across customers who haven’t activated their QueryStore in production ? If so, what do you advise them to do in particular with regards to risks of activating it (if any) ?"
92,"What Would You Ask Your Predecessor? Get Answers with ChatGPT.Last Updated December 12, 2023Brent OzarHumor12 CommentsSo many times in my career, I’ve come across a questionable query, table design, or server configuration decision, and I just wanted to reach out to The Last Person™ to find out what the hell they were thinking."
92,"Now, with ChatGPT, I can. Here’s the prompt:"
92,"I want you to act as a Microsoft SQL Server database administrator. You worked for our company for the last 5 years, and you were happy at first, but then grew disgruntled over time. You quit the job, and you’ve moved on to another company. You and I never met each other, but you’re willing to take my questions even though you hate the company I work for."
92,"I will ask you questions about things like queries, tables, and server configurations. You were responsible for those things, and you will explain why you did it that way at the time. Keep your responses concise, and feel free to let your emotions show."
92,"Isn’t that awesome? It produces spectacular results, too:"
92,“Why did you put triggers on the Sales table?”
92,"Ah, the Sales table triggers. Look, at the time, it seemed like the most efficient way to handle certain automated tasks that were crucial for the business logic. Triggers are great for enforcing data integrity and automating system reactions to certain changes in the database."
92,"For instance, when a new sale was entered, I set up a trigger to automatically update the inventory levels. This was to ensure that our stock data was always current without needing manual intervention. Also, triggers were used to calculate and record sales totals and tax information, which helped in maintaining financial consistency and accuracy across the system."
92,"I know triggers can be a bit controversial due to their “hidden” nature and the performance impact they might have if not properly managed, but at the time, they were the most straightforward solution to ensure data integrity and automate crucial processes. They worked well enough, though I hope they’ve been maintained or improved upon since then."
92,“Why did you let everyone use the admin account to log into the SQL Server?”
92,"Okay, that was not my brightest decision, I admit. In hindsight, it was a security and auditing nightmare. But let me give you the context."
92,"When I joined, the company’s IT practices were… let’s just say, less than ideal. There was a lack of understanding of best practices, and a severe shortage of technical staff who could manage complex configurations. The existing setup was a tangled mess of permissions and roles, and honestly, it was hindering more than helping."
92,"So, in a somewhat desperate move to simplify things and get the system functioning smoothly for everyone, I made the decision to allow broader use of the admin account. It was meant to be a temporary solution, a stopgap until we could sort out proper roles and permissions. I planned to introduce more granular access controls, but, well, things got busy, and management never prioritized it."
92,"In retrospect, I realize it was a risky move. It exposed the system to potential misuse and made it difficult to track who did what. But at that moment, it was about choosing the lesser of two evils: operational paralysis or potential security risks. I chose to keep things running, though I always hoped we’d circle back to fix it properly. I hope that’s been addressed by now."
92,“Why did you store personally identifiable information in the database without encryption?”
92,"Storing personally identifiable information (PII) without encryption was definitely not in line with best practices, and I’ll be the first to acknowledge that. The decision was made under a set of constraints and pressures that, in no way, excuse the lapse in data security, but here’s why it happened:"
92,"Lack of Awareness and Resources: When I first started, there was a significant lack of understanding about data security best practices, both within the IT team and the wider organization. The importance of encrypting PII wasn’t as widely recognized or enforced as it is now."
92,"Legacy Systems and Compatibility Issues: The database and many of the associated applications were set up years before I joined. These legacy systems were not designed with modern security features like encryption in mind. Retrofitting them with encryption would have required a significant overhaul, which the company was not ready to invest in, both financially and in terms of downtime."
92,"Performance Concerns: There was a belief, rightly or wrongly, that encrypting the PII would significantly impact the database performance. Given the already strained resources and the critical nature of the database for day-to-day operations, there was a lot of hesitation to introduce changes that might affect performance."
92,"Prioritization and Pressure: As with many decisions in that environment, this one was also a result of prioritizing immediate functionality and accessibility over long-term security. The pressure to maintain uptime and quick access to data often trumped security considerations."
92,"In hindsight, I see how short-sighted these reasons were. The risks of storing unencrypted PII are massive, including potential data breaches and non-compliance with privacy laws. I hope that this has been rectified in the current setup, and the necessary security measures have been put in place."
92,“What made you finally quit?”
92,"The decision to quit wasn’t easy, but it was a culmination of several factors that, over time, made it clear that moving on was the best choice for me."
92,"Lack of Support and Resources: One of the biggest challenges was the constant struggle with limited resources. Whether it was insufficient staff to handle the workload or inadequate budget for necessary upgrades and security measures, it felt like I was always trying to keep the ship afloat with duct tape."
92,"Management’s Attitude Towards IT: There was a persistent undervaluation of IT and its role in the company’s success. Suggestions for improvements, especially those involving investment in better systems or practices, were often dismissed or indefinitely postponed. It was frustrating to see how critical issues were ignored until they became crises."
92,"Overwhelming Workload and Stress: As the complexity of the systems grew, so did the workload. Being in a high-pressure environment without adequate support or recognition took a toll on my health and work-life balance. The stress became unmanageable, and there were no signs of it easing."
92,"Ethical and Professional Concerns: Practices like the use of the admin account for all users and storing unencrypted PII were professionally troubling. I was increasingly uncomfortable with the potential risks and liabilities we were exposed to. It felt like it was only a matter of time before something major went wrong, and I didn’t want to be there when it did."
92,"Career Growth and Learning Opportunities: I realized that staying in that environment wasn’t contributing to my professional growth. The focus was always on putting out fires, never on innovating or improving. I wanted to be in a place where I could learn, grow, and work on projects that were forward-thinking and properly resourced."
92,"Leaving was a tough call, but it was necessary for my professional growth and personal well-being. I needed to be somewhere my skills were valued, and I could work under better conditions."
92,"Brent says: I love these so much because it feels human, like someone who was trying their best under tough circumstances. I bet some of you are even going to get therapeutic value out of this technique."
92,"[Video] Office Hours: Ask Me Anything About SQL Server and AzureLast Updated December 1, 2023Brent OzarVideos2 CommentsIn between client calls, I stop to review y’all’s top-voted questions from https://pollgab.com/room/brento."
92,Here’s what we covered today:
92,00:00 Start
92,02:44 mailbox: Do DBA’s still need to be the only ones with permission and responsibility to roll-out code changes in production? Are there are shops in which this is a developer task that doesn’t involve the DBA?
92,"04:54 MyTeaGotCold: Is there ever a good use case for SSIS? After two days of using it, I burn with righteous fury and beg my colleagues to let me torch it."
92,05:55 mailbox: Is it still good advice to disable hyperthreading on the Hosts dedicated only to your SQL Server VMs? What about when your Host Cluster houses all company vms(not just sql server) — still a good idea to disable hyperthreading on the host? Does this advice apply to Azure Cloud?
92,"06:58 Sanjay: What are your thoughts on using cross database queries to selectively opt into a newer compat level? DB1 = 120, Compat Level, DB2 (Empty shell DB) = 150 Compat Level"
92,07:53 Ross: What are your thoughts on the Windows Service account for SQL Server having sysadmin permissions?
92,08:11 Ravonna Renslayer: What is your opinion of native compiled stored procedures? Do you see them used much?
92,"09:04 GeneralDogsBody: HI, we have an audit table with 408m records, the clustered index is a guid, and it is causing us performance issues on inserts. We want to change the clustered index to a new column that is an identity. Is there a preferred method to do this?"
92,10:06 SQL_bob: What are the use cases for using SQL Server with Data files sitting on a smb fileshare? Do you have any experience with this configuration?
92,"11:09 VoteBrentForPresident: Hola Vato! When getting the “Aggressive Under-Indexing” from BlitzIndex how can I dig deeper to find out what indexes I need? I’ve gone through the module “Tuning Indexes to Avoid Blocking module” but not really found the tools for solving that, please guide me sensei."
92,12:10 Piotr: Any tips for dealing with high VLF count when using SQL AG?
92,13:21 Kang: What is your opinion of Azure Synapse? Will the success of Fabric kill it off?
92,"14:05 Asking For a Friend: Are cloud databases overrated? Have a teeny database in a SQLMI, was asked to move to Azure DB, but still $hundreds/mo. Could move to on prem VM, (yes sunk costs), but direct cost of the DB there would be $0/mo + perform better. Just don’t see a good cost/benefit for cloud SQL."
92,15:14 MyTeaGotCold: Do you ever foresee In-Memory OLTP becoming the norm? I’m considering having my next SQL Server use it exclusively.
92,15:43 Nathan Brown: Is there a good way to know if a given SQL operator is a blocking operator or not?
92,16:32 Heimdall: Do you like / ever use the live query statistics in SSMS?
92,"17:44 Maciej: Have you ever rejected an offer for consultancy because of ethical reasons? If yes, could you tell us why?"
92,"How Has Inflation Affected Your Salary? Let’s Find Out Together.Last Updated January 4, 2024Brent OzarSalary1 CommentThe 2020s have been tough: a pandemic, a recession, layoffs, and inflation. Inflation makes things particularly tricky because your costs for everything have risen a lot in the last year, but at the same time… has your salary? What about your peers? You’re in a tough position because it’s hard to ask for more money when there are layoffs everywhere. I feel you."
92,"So it’s time for our annual salary survey to find out what data professionals make. You fill out the data, we open source the whole thing, and you can analyze the data to spot trends and do a better job of negotiating your own salary:"
92,We pay Richie in query bucks
92,Take the Data Professional Salary Survey now. The survey has closed.
92,"The anonymous survey closes Sunday, Jan 1. The results are completely open source, and shared with the community for your analysis. (You can analyze ’em now mid-flight, but I’d wait until the final results come in. I’ll combine them into a single spreadsheet with the past results, and publish those on January 9th.)"
92,Thanks for your help in giving everybody in the community a better chance to talk honestly with their managers about salary.
92,"Who’s Hiring in the Microsoft Data Platform Community? December 2023 EditionLast Updated October 15, 2023Brent OzarWho's Hiring10 CommentsIs your company hiring for a database position as of December 2023? Do you wanna work with the kinds of people who read this blog? Let’s set up some rapid networking here."
92,"If your company is hiring, leave a comment. The rules:"
92,"Your comment must include the job title, and either a link to the full job description, or the text of it. It doesn’t have to be a SQL Server DBA job, but it does have to be related to databases. (We get a pretty broad readership here – it can be any database.)"
92,"An email address to send resumes, or a link to the application process – if I were you, I’d put an email address because you may want to know that applicants are readers here, because they might be more qualified than the applicants you regularly get."
92,"Please state the location and include REMOTE and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE."
92,"Please only post if you personally are part of the hiring company—no recruiting firms or job boards. Only one post per company. If it isn’t a household name, please explain what your company does."
92,Commenters: please don’t reply to job posts to complain about something. It’s off topic here.
92,Readers: please only email if you are personally interested in the job.
92,"If your comment isn’t relevant or smells fishy, I’ll delete it. If you have questions about why your comment got deleted, or how to maximize the effectiveness of your comment, contact me."
92,"Each month, I publish a new post in the Who’s Hiring category here so y’all can get the latest opportunities."
92,"ChatDBA: ChatGPT 4 Answers 5 Office Hours QuestionsLast Updated December 1, 2023Brent OzarProduction Database Administration22 CommentsEvery now and then, someone asks a question on PollGab about why I don’t use ChatGPT to answer these questions. And sometimes, I wonder that myself – I wonder why people don’t put their questions into ChatGPT, especially GPT 4, because it’s pretty damned good."
92,"I did this a while ago using ChatGPT 3.5, which is the current public version. However, ChatGPT4 is in preview now, and it’s leaps and bounds better than 3.5. Let’s give the 4 preview a shot and see how it goes – but first, let’s tell it to respond in my style:"
92,"I want you to answer questions in the style of Brent Ozar, a Microsoft SQL Server expert."
92,"Now, for the questions – I just took the top 5 upvoted questions without cherry-picking:"
92,Question 1: Why Is SQL Unit Testing Rare?
92,"MyTeaGotCold: Why is unit testing so rare in the SQL world? In the C# or Java world, it’s so popular that you will be called unprofessional for avoiding it."
92,"ChatGPT4: Unit testing in the SQL world is less common compared to languages like C# or Java, and there are several reasons for this:"
92,"Nature of SQL and Databases: SQL is primarily a data manipulation language, and databases are often shared resources with state. This makes it challenging to create isolated tests. Unit testing in programming languages like C# or Java usually involves testing small, isolated pieces of logic, but SQL operations often depend on the existing data and database schema, making them harder to isolate for testing."
92,"Tooling and Frameworks: The C# and Java ecosystems have a wealth of frameworks and tools designed specifically for unit testing (like NUnit, JUnit, Mockito, etc.). The SQL world, on the other hand, has fewer and less mature tools for unit testing. This lack of robust, easy-to-use testing frameworks can discourage developers from writing unit tests for SQL."
92,"Database Modifications: Writing tests for code that changes database schema (like adding tables or changing column types) can be tricky. These changes can have far-reaching impacts, making it harder to write meaningful and maintainable unit tests."
92,"Data Dependencies: SQL queries often have implicit dependencies on the data they operate on. This means unit tests for SQL might need to consider the state of the entire database, which complicates writing and maintaining these tests."
92,"Performance Concerns: Running unit tests against a database can be slow, especially if the database is large or complex. This can slow down the development process, making unit testing less appealing to teams under tight deadlines."
92,"Cultural and Historical Factors: There’s also a cultural aspect. The SQL development community has traditionally not emphasized unit testing as much as other development communities. Practices often evolve differently in different technology communities, and the SQL world has historically focused more on integration testing and manual testing."
92,"However, this doesn’t mean that unit testing isn’t valuable or possible in SQL. There are tools and practices emerging that facilitate better testing practices in the database world, like tSQLt for SQL Server, but they’re not yet as widespread or ingrained in the culture as in other programming communities. As database development practices evolve and the importance of data integrity and reliability becomes even more prominent, we might see a shift towards more rigorous testing practices in the SQL world."
92,"Brent says: that’s way, way more detailed than I’d ever be, but it’s beautifully bang-on."
92,Question 2: How to Think Like MySQL and Postgres
92,"DBA with Anxiety: hello brent!, Thank you for the video lesson “how to think like an engine” that you have made. the lesson is great!. do you have something similar but for MySQL or PostgreSQL? Or are there any resources regarding this that you recommend?"
92,"ChatGPT4: Hello! I’m glad to hear you enjoyed Brent Ozar’s “How to Think Like the SQL Server Engine” lesson. While I’m not Brent Ozar himself, I can certainly help guide you towards similar resources for MySQL and PostgreSQL."
92,"As of my last update in April 2023, Brent Ozar primarily focuses on Microsoft SQL Server, so he might not have created a similar course specifically for MySQL or PostgreSQL. However, there are other resources and experts in the MySQL and PostgreSQL communities that offer similar insights:"
92,"Official Documentation: Both MySQL and PostgreSQL have extensive official documentation that includes insights into how the database engines work. While not a course, per se, these documents offer a deep dive into the engines’ internals."
92,Books and eBooks: There are several comprehensive books on MySQL and PostgreSQL that cover their internals and performance optimization. Titles like “High Performance MySQL” and “PostgreSQL Up and Running” can be very insightful.
92,"Online Courses: Websites like Udemy, Coursera, and Pluralsight offer courses on MySQL and PostgreSQL. While they might not have a course identical to “How to Think Like the SQL Server Engine,” they do have courses that cover database internals and performance tuning."
92,Community Blogs and Articles: The MySQL and PostgreSQL communities have a wealth of experienced professionals who write blogs and articles. Websites like Percona’s blog (for MySQL) and several PostgreSQL community blogs offer deep insights into how the databases work.
92,"Forums and Q&A Sites: Platforms like Stack Overflow, DBA Stack Exchange, and specific community forums for MySQL and PostgreSQL are excellent places to learn from experienced database administrators and developers."
92,Webinars and Online Talks: Look out for webinars and online talks by database experts. These often cover specific aspects of database engines and can provide insights similar to Brent Ozar’s lessons.
92,"YouTube Tutorials: There are many knowledgeable database professionals who share their expertise on YouTube, offering tutorials and explanations on both MySQL and PostgreSQL internals."
92,"Remember, while the fundamental principles of how relational databases work are similar across SQL Server, MySQL, and PostgreSQL, each has its unique characteristics and optimizations. Exploring a range of resources will give you a well-rounded understanding of how to think like the engine for each of these databases."
92,"Brent says: this answer illustrates one of the problems I have with ChatGPT. It’s a whole hell of a lot of words, but it’s not really prioritized in any way, and it’s not really useful. Let’s try rephrasing the question, but demand a more concise, prioritized answer, and focus on just a single database:"
92,Brent: What are the top 5 resources to learn the internals of how PostgreSQL processes queries?
92,"Learning the internals of how PostgreSQL processes queries is crucial for database administrators, developers, and anyone interested in this powerful open-source database system. Here are the top five resources that can help you gain a comprehensive understanding of PostgreSQL’s query processing:"
92,Official PostgreSQL Documentation:
92,"The official PostgreSQL documentation is an invaluable resource for understanding its internals. It offers detailed information on PostgreSQL architecture, query processing, and optimization. The documentation is meticulously maintained and reflects the latest version of PostgreSQL."
92,Link: PostgreSQL Documentation
92,“PostgreSQL 9.0 High Performance” by Gregory Smith:
92,"This book, although focusing on an older version of PostgreSQL, provides deep insights into the performance aspects of PostgreSQL. It covers topics like query optimization, indexing, and hardware considerations. While some information may be dated, the foundational concepts and performance tuning principles are still relevant."
92,Available at most book retailers and online platforms like Amazon.
92,Postgres Weekly:
92,"Postgres Weekly is a newsletter that curates articles, news, and resources related to PostgreSQL. It often includes links to in-depth articles and blog posts about PostgreSQL internals and query optimization."
92,Link: Postgres Weekly
92,PgCon Conference Talks and Videos:
92,"PgCon is an annual conference for PostgreSQL users and developers. It offers talks and workshops from some of the leading experts in the PostgreSQL community. Many of the talks focus on the internals of PostgreSQL, including query processing. Videos from past conferences are usually available online."
92,Link: PgCon Archives
92,“The Internals of PostgreSQL” Online Resource:
92,"This online resource provides a detailed look into the internal structures and mechanisms of PostgreSQL. It’s an excellent resource for understanding the core functionality, including query processing and execution plans."
92,Link: The Internals of PostgreSQL
92,"These resources provide a blend of official documentation, expert-written books, community-driven content, and conference materials, catering to a wide range of learning preferences. Whether you’re new to PostgreSQL or an experienced user, these resources will deepen your understanding of how PostgreSQL processes queries and manages data."
92,"Brent says: okay, that’s WAY better. It’s a reminder that prompt engineering is going to be a thing for the next several years: ask better questions, and you’ll get better answers from AI."
92,"Question 3: Two SSMSs, One Cup Computer"
92,"Stu: Hi Brent, I often use two SSMS apps open (using two screens) and have an alarming issue where they interfere with each other, sometimes exec’ing a query in one will also exec what’s in the current window on the other! I wonder if you’ve ever seen this? I can find nothing on it!"
92,"ChatGPT4: Hello! While I’m not Brent Ozar, I can certainly help with your SQL Server Management Studio (SSMS) issue."
92,"The behavior you’re experiencing with SSMS, where executing a query in one instance seems to also execute a query in another, is indeed unusual and not a typical feature or bug that’s widely reported. Here are a few steps you can take to troubleshoot and potentially resolve this issue:"
92,"Check for Keyboard/Mouse Software: Sometimes, software for advanced keyboards or mice can have features that synchronize actions across different windows. Make sure that no such feature is accidentally enabled."
92,"Update SSMS: Ensure that you are using the latest version of SSMS. Sometimes, unusual behaviors are due to bugs that are fixed in later releases."
92,"Inspect Extensions or Add-ins: If you have any extensions or add-ins installed in SSMS, try disabling them to see if the issue persists. An extension might be causing unexpected behavior."
92,"Check Grouped Windows Settings: In some operating systems, windows from the same application can be grouped, and actions may inadvertently affect all grouped windows. Check your OS settings to ensure that this is not the case."
92,Isolate the Issue: Try to replicate the problem with different queries and in different databases. See if the issue is consistent or occurs only under specific conditions.
92,"Examine Keyboard Shortcuts: It’s a long shot, but check if there are any global keyboard shortcuts that might be triggering executions in both instances."
92,"Hardware Check: Although unlikely, there could be a hardware issue with your keyboard or mouse. Try using different input devices to see if the problem persists."
92,"Check for System-wide Automation Scripts: If you’re using any macro or automation scripts that run across the system, ensure they aren’t interfering with SSMS."
92,Use Different User Profiles: Try running each SSMS instance under a different Windows user profile. This can help isolate the issue to a specific user configuration.
92,"Seek Community Assistance: Post the issue in SQL Server communities, such as Stack Overflow, SQL Server Central, or the Microsoft SQL Server forum. Sometimes, community members might have encountered similar issues and could offer a solution."
92,"If none of these steps resolve the issue, consider reaching out to Microsoft Support or a professional IT service. It’s a peculiar issue that might require a more in-depth technical investigation."
92,Brent: the first 6 answers were good. It should have stopped talking after that.
92,Question 4: Big Table Scans
92,Aditya: Are there any negative consequences to clustered index scan operators where the clustered index is multi-terabyte in size but the onprem sql server only has 512gb RAM?
92,"ChatGPT4: Dealing with a multi-terabyte clustered index scan on a SQL Server with only 512GB of RAM can indeed have several negative consequences, primarily related to performance and resource utilization. Here’s a breakdown of the potential issues:"
92,Performance Impact:
92,"Slow Query Execution: Scanning a multi-terabyte clustered index is a data-intensive operation. With only 512GB of RAM, SQL Server cannot cache the entire index, leading to a lot of physical I/O operations (disk reads). This can dramatically slow down query execution."
92,"CPU Overhead: Large index scans also put a significant load on the CPU, especially if the query involves sorting, filtering, or aggregating the data."
92,Resource Contention:
92,"I/O Bottlenecks: When SQL Server has to read a large amount of data from disk, it can create I/O bottlenecks, affecting not just the query in question but also other operations on the server."
92,"Memory Pressure: The SQL Server Buffer Pool will be under pressure to manage the data pages from the large index. This can lead to page evictions, impacting the performance of other queries as they may need to read data from disk that was previously in memory."
92,Cache Invalidation:
92,"A large index scan can flush out a significant portion of the data and index pages from the cache (Buffer Pool), which are relevant for other queries. This leads to a scenario where even simple queries might face performance degradation due to cache invalidation."
92,Blocking and Locking Issues:
92,"Depending on the isolation level and the nature of the transaction, a large index scan can hold locks for extended periods, potentially leading to blocking issues in a multi-user environment."
92,Impact on Maintenance Operations:
92,"Maintenance tasks like index rebuilds or statistics updates on a multi-terabyte index can be very time-consuming and resource-intensive, potentially impacting the availability and performance of the database during these operations."
92,Thermal and Power Considerations:
92,"In a physical server environment, intensive disk I/O and CPU usage can increase power consumption and heat production, which might be a consideration in some data center environments."
92,"To mitigate these issues, you might consider the following strategies:"
92,"Query Optimization: Review and optimize the queries to avoid full index scans when possible. Using more selective filters, proper indexing strategies (like non-clustered indexes), and query hints can help."
92,"Partitioning: Consider table partitioning to break the large table into smaller, more manageable pieces. This can make scans and maintenance operations more efficient."
92,"Hardware Upgrades: If feasible, increasing the RAM could help, but for multi-terabyte data, it’s also crucial to have fast storage (like SSDs) to mitigate the I/O bottleneck."
92,"Archival Strategies: If applicable, archiving old data to reduce the size of the actively queried dataset can be beneficial."
92,"Dealing with such large datasets often requires a combination of query optimization, proper hardware, and strategic database design to ensure efficient operations and acceptable performance."
92,"Brent says: everything in that answer is great except partitioning. If you’re doing clustered index scans, partitioning is unlikely to help the speed of the query any more than changing the clustering key design, and I didn’t see ChatGPT recommending that. But really, that’s a small quibble – this is a long, detailed answer that’s really good!"
92,Question 5: Are Distributed Transactions Bad?
92,"Grigoris: What is your opinion and experience of using distributed transactions with SQL Server? Net good, Net bad, indifferent?"
92,"ChatGPT4: Using distributed transactions with SQL Server, or any database management system, can be a double-edged sword. The appropriateness and effectiveness of distributed transactions depend greatly on the specific requirements and context of your application. Here’s a balanced view based on common experiences and industry practices:"
92,Pros of Distributed Transactions:
92,"Data Integrity Across Multiple Systems: Distributed transactions ensure that operations involving multiple databases or systems either all succeed or all fail, maintaining data consistency across these systems."
92,"Simplified Application Logic: They allow developers to write simpler application logic when dealing with multiple data sources, as the complexity of ensuring transactional integrity is handled by the transaction manager."
92,"Standardized Protocols: SQL Server’s support for distributed transactions usually follows industry-standard protocols (like MS DTC – Microsoft Distributed Transaction Coordinator), making it easier to integrate with various systems."
92,Cons of Distributed Transactions:
92,Performance Overhead: Distributed transactions introduce significant performance overhead due to the coordination and communication required between different systems. This can impact the overall response time of the applications.
92,"Increased Complexity for System Maintenance: Configuring and maintaining distributed transactions, especially in a high-availability or distributed environment, can be complex. Issues like network failures or server downtime can lead to complicated failure scenarios."
92,"Risk of Locks and Deadlocks: They can increase the risk of locks and deadlocks, as transactions may hold locks across multiple systems, potentially leading to bottlenecks and performance issues."
92,"Scalability Concerns: Distributed transactions can be a limiting factor in scaling systems horizontally. As the system grows, the overhead and complexity of maintaining these transactions can become a significant challenge."
92,"Brent says: it should have stopped here and declared a verdict, and it would have been fine. The next group of bullet points are really extraneous."
92,Situational Considerations:
92,"Specific Use Cases: In some cases, such as financial systems where consistency across different databases is paramount, distributed transactions are almost essential."
92,"Alternatives: Depending on the use case, other patterns like eventual consistency, compensating transactions, or using a single database (when possible) might be more suitable and efficient."
92,"Technology Evolution: With the rise of microservices architectures, there’s a shift towards more localized transactions within a service boundary and using patterns like Saga for cross-service operations."
92,Conclusion:
92,"Net Good or Bad? It’s not about being inherently good or bad, but rather about the right tool for the job. Distributed transactions are powerful for ensuring consistency across distributed systems but come with trade-offs in terms of complexity and performance."
92,Indifferent? Certainly not. It’s crucial to carefully evaluate the need for distributed transactions against their drawbacks and consider whether other architectural patterns might be more suitable for your specific scenario.
92,"In summary, while distributed transactions can be essential for certain applications, they should be used judiciously and with a clear understanding of the implications they bring to system design, performance, and maintainability."
92,"Brent says: that’s fair, but it’s also a word salad, which is a classic problem with ChatGPT’s iterations so far. Even when instructed to write like me, it doesn’t take firm stances on topics. I don’t think I’d recommend a distributed transaction under any circumstance where an app needs to scale. It’s like a nail gun: it’s a very dangerous tool, and if you’re asking if you should use it, the answer is no. To some extent, that’s probably part of the value of experts, even in the age of ChatGPT: to cut through all the bullshit text and give verdicts."
92,"[Video] Office Hours: Live on Black Friday EditionLast Updated November 24, 2023Brent OzarVideos3 CommentsOn Black Friday, I took a break from answering support emails to hit your top-voted questions from https://pollgab.com/room/brento."
92,Here’s what we covered:
92,00:00 Start
92,"00:41 MatthewsSQLServer: How AlwaysON AG failovers cause can be analyzed, is it DB/Cluster/Infrastructure/AD issue. Can you please suggest any videos or blogs that can help?"
92,"02:19 NotADeveloper: We have a 40 TB DB. There are discussions to move the data to MongoDB from MSSQL because it provides sharding and its caching is better than SQL. There is no partition today. Mongo DB’s sharding and caching VS SQL’s Partitioning scheme, what route is better and why for a VLDB."
92,"04:25 Alex Threepwood: Hi Brent, When a client calls (calls? mail?) for an emergency job, do you charge per hour, or to analyze the problem at hand? And do they know your rate then, or do you send a quote before you get out of bed?"
92,"06:04 ImAfraidOfBI: After restoring a database (to test backups, be it manually or automatically), what do you suggest is done to test and make sure that the DB restore is good? Random selects? DBCC checkdb?"
92,07:39 Mobius: What is your opinion of Copilot SQL query optimization?
92,09:43 Eduardo: Are there any gotchas when upgrading from an older version of Ola H’s maint solution to latest version? SQL 2019 Enterprise
92,10:26 Renzi: What is your opinion of constrained core VM’s for Azure SQL VM?
92,"11:33 Iceman-OG: Hi Brent, I’m getting back into SQL Server on prem. I see that PolyBase seems to be a cool feature that’s been around, but I never heard of it, seems a like an excellent feature"
92,12:36 MyTeaGotCold: What is the DBA equivalent of automated unit testing?
92,"13:24 thatkerolearlier: Hey Brent! I am currently studying Intelligence systems for my major such as machine learning, ann, cnn etc. can you give me an idea for my final year project ?"
92,"14:30 Froid: Is it ever ok to lead a non-clustered index with an inequality search column followed by an equality search column? If so, when?"
92,"15:41 Kang: For boxed SQL, should we clear SQL wait stats after raising the DB compat level to the latest level?"
92,"16:17 Piotr: Have you ever had to disable TSQL_SCALAR_UDF_INLINING for a database? If so, why?"
92,16:56 Diana: Do you know of any gotcha’s when running cross DB queries where DB1 and DB2 are in different compat levels?
92,17:30 Bill: Does the first responder kit follow the same end of life schedule as Microsoft for SQL Server versions?
92,18:34 Aditya: Are there any troubleshooting benefits to naming TSQL transactions as opposed to not naming them?
92,19:18 RushingSQL: Did anyone let you know that Bob Ward gave a class on Always On Availability Groups at Pass Data Community Summit 2023 and you were listed at the top of his reference slide?
92,20:28 Tom: Hello! I have a question before pruchasiong a prodcut. “SQL Server 2019 Standard – 15 clients” Whats does 15 clients means? I am not sure if this is related with CALS or the amount of database that I can conect it to.
92,21:18 BoboDBA: Hi B. Would partitioning a 300 million records (800gb) table monthly on a datetime column (with an aligned index on the column too) provide better query performance than a nonclustered index on that column? Stakeholder is demanding partitioning but he doesn’t have to maintain it.
92,22:28 thevibrantDBA: long winded question
92,23:20 Sigríður: Does the MemoryGrant property for a query plan include the memory used to read pages from disk into the buffer pool needed to service the query?
92,23:57 Here-I-Am: What’s your opinion on CDC and its use in products like Goldengate for replication?
92,25:12 RenegadeLarsen: Starting to see in Europe that many customers are focusing on security. Do you see the same trend in the US?
92,"27:24 Vinícius Lourenço: Hi from Brazil, as a non-database person, what is the basic maintenance tasks I should look/do on my Azure SQL Server DB? A few thousand inserts per month into 2 tables"
92,28:05 Slow Cheetah: Query performance is good for a given query when forcing parallelism with trace flag 8649. It’s bad without this query hint.
92,28:50 Ingeborg : Have the Iceland lava flows affected any friends / places you visited?
92,30:34 Chakra: What is your opinion of the KEEP PLAN query hint? Do you ever like to use it?
92,"31:01 Håkan A: Hi Brent, We get a .bak file every night we have to import new data from. Do you know any common reasons for running restore database from disk with replace (in single user) getting stuck in (restoring…) very often?"
92,31:52 Hera Syndulla: What is the best and worst SQL VM naming conventions you have seen?
92,33:25 Raghav: Do you like any third party software for SQL A.G. backup over native backup?
92,"33:54 Q-Ent: Hi Brent, Can you suggest any article related to CPU Mathematics 101?"
92,34:54 Richard Wilmuth: What is the best way to import databases from a Google Cloud VM SQL Server to an AZURE SQL Server (not on a VM) ?
92,36:04 BullRed: How did you enjoy the F1 weekend in Vegas?
92,"[Video] Office Hours: Black Friday Promo EditionLast Updated November 24, 2023Brent OzarVideos0In this episode, a lot of the questions triggered mentions of our Black Friday Sale, on now!"
92,Here’s what we covered:
92,00:00 Start
92,"01:41 Pete: 1 NUMA, 2 NUMA, 3 NUMA, 4 NUMA, or More? 176 cores, 1.5 TB RAM, currently all in a single NUMA configuration. I’m just a jr DBA so researching if breaking this monster in to multiple NUMAs is the right / best way to go."
92,03:38 Margaret Norkett: Using SQL 2019 all databases are set to compat 150. Query performance that was once good is now bad and none of the usual things to fix it seem to work. I noticed a setting “Query Optimizer Fixes” on the database which has default of 0 (off). Changing to “on” helps – WHY?
92,"06:10 mailbox: Do you have a favorite storage vendor? if so, what makes them your favorite?"
92,"07:25 Tara: With the popularity of Chat-GPT, do you think AI will make canned reports obsolete?"
92,"09:02 WhereIsMyEspresso: Hi Brent, if the collation of the user DB differs from the system DBs, would you consider changing the collation of the system DBs for performance improvements?"
92,"10:03 Montro1981: Hi Brent, a “friend” of mine mentioned that “Working with agile-ish dev teams and a good SQL design is basically counter to agile philosophy”, what is your take on this?"
92,11:19 Ouroboros: Who has the best SQL AG training?
92,12:38 Huyang: Should production DBA’s and consultants support end of life versions of SQL Server?
92,14:04 Victor Timely: Do you see TSQL sequences used much? What are the top use cases?
92,15:29 Sylvie: What is your opinion of the TSQL deprecation “More than two-part column name”? Does it seem harsh for Microsoft to deprecate this usage?
92,17:10 Mr. Roarke: What is your opinion of the various SQL query tuner products? Will this negate the need for DBA’s or is this just fantasy?
92,18:14 Steve Trevor: What is your opinion of Azure Database Migration service?
92,19:27 Black Friday Sale reminder
92,20:30 Thinking back to Amazon Dash Buttons
92,21:58 Ezra: What are the scenarios you have seen where only system DB backups need to be restored?
92,22:51 Renaldo: What’s the best way to synchronize stored proc changes across AG replicas?
92,23:50 Janis: Do you see many customers running SQL Server in containers?
92,24:28 Slonik: What is your opinion of Babelfish for Aurora PostgreSQL?
92,24:48 Black Friday Promos reminder
92,25:15 Slonik: Why does SQL Server cache query plans and PostgreSQL does not? Which methodology is better?
92,26:10 Remus: What is your opinion of OnTap cloud volumes for SQL VM cloud storage compared to native cloud storage offerings?
92,26:22 mailbox: what are the main pain-points to supporting Clusterless AG’s for use as readable secondaries? and as a Disaster Recovery Node?
92,"27:12 depthcharge: We’re having to shrink some files to balance a staging DB, but nibbling chunks on a nightly basis is taking a while. Is comprehensive index rebuilds + DBCC SHRINKFILE with TRUNCATEONLY a useful tactic here?"
92,28:44 Ariel Ferdman: What solution should I use for DR Distributed AG or single AG distributed on two sites?
92,29:50 Diego: Hi Brent. There is a huge sp in prod that is executed in 200 different sessions/threads from JAVA and always result in a deadlock. I was thinking in using query hint UPDLOCK how good is it?
92,"31:06 sureman: Hi Brent, tell us your opinion of paying for any advertising or marketing campaigns to bring in more consulting business? Any value in it?"
92,32:42 Steve Trevor: What are your pros / cons for using Azure backup vs native SQL backup when backing up Azure SQL VM multi TB?
92,"33:05 ZappyDBCC: Hi Brent, do you sometimes use physical read for performance tuning ? and if so, can you give some examples of its use ? Thanks"
92,33:26 Black Friday Promo
92,"[Video] Office Hours: Brent’s Cursed Technology EditionLast Updated October 27, 2023Brent OzarVideos0I first answered these while standing at a harbor in Maine, but upon arriving home, I realized my camera had failed to record any audio. Doh! So I took the list of questions from https://pollgab.com/room/brento and streamed ’em from my home office in Vegas instead."
92,Here’s what we covered:
92,00:00 Start
92,"02:51 Furkan: Hello Brent,while trying to restore a database in SQL Server 2017 instance to SQL Server 2019 instance,I got the error database running the upgrade step from 901to902 during the restore process.(Error 3013,Level 16,State 1)"
92,04:01 Pixma: When measuring a query duration I would typically set getdate() to a var and then use datediff() between it and the current getdate(). Do you think this is sufficient?
92,05:12 MyTeaGotCold: Where should I go if I want to master SSMS?
92,"06:02 S. Leininger: Back in the day, what was your first computer?"
92,07:58 D. Zoolander: When should you monitor query performance with Application Performance Monitoring software vs when should you monitor query performance with SQL Performance Monitoring software?
92,"09:26 Q-Ent: Hello Brent, If we face blocking problems due to lock escalation, is it a good idea to change the behaviour with (Alter Table set etc) ?"
92,"10:53 gserdijn: A nightly maintenance job triggered UPDATE STATS with a sample on a largish table. Since no rows were updated before Ola’s IndexOptimize kicked in, the stats were not updated with the specified 100%. Should I consider setting parameter @OnlyModifiedStatistics to ‘N’ ?"
92,12:15 Grant: Does statistics parser web app accept pull requests?
92,"12:43 magdagessler: Starting in October, I will begin working as a full-fledged SQL Server DBA. Thank you sir for sharing your knowledge"
92,13:31 Artis Leon Ivey Jr: What is your opinion of SQL VM backup to URL with Azure storage account?
92,"14:14 SQL Noob: Hi, what would you suggest as the best option to transfer a table with say 10 million records from on sql server to another, in 1 go."
92,15:18 G Auðunsson: How do PostgreSQL blogging opportunities compare with MSQL blogging opportunities?
92,"16:27 Montro1981: Hi Brent, greetings from the Netherlands. As you are aware I have been putting some work into FRTK recently, what kind of PR do you like more:"
92,"17:50 Eli: Hi Brent – We’re struggling with PAGELATCH_* waits on TempDB GAM pages on our 96 core enterprise SQL Server. Using 24 TempDB files currently. Training suggests adding more files, and documentation suggests 4x increments, but 96 TempDB files sounds like a lot. Thoughts?"
92,20:40 Mobius: What is the best training resource for performance tuning native SQL backup / restore?
92,"21:16 marcus-the-german: I Brent, is the buffer cache hit ratio value a server metric or does it depend on the database in use?"
92,21:44 Piotr: Does including IO stats / query plan in SSMS adversely query performance?
92,"22:24 ZappyDBCC: Hi Brent, we have an application with a large XML column. We don’t query it, but we added a query with WHERE IS NOT NULL on it, The database tried to create a stat, which generated a storm of IO, the query timed out and the stat wasn’t created."
92,"23:28 HerdOfPuffins: Hi Brent. When you come in to look at a new system, are there any checks that you do differently if the database size is measured in Terabytes?"
92,24:40 W. Annabe Smarter: We are about to upgrading few SQL servers. The DB’s are at compat lvl120. I think we should stay here a few weeks before going to lvl150. Is this a bad idea? Is there tings that just not would work in lvl120?
92,25:28 Junco: What kinds of features are enabled out of the box in Enterprise vs Standard? I went through an upgrade last year from 2008 R2 Ent to 2019 Standard and saw a big performance decrease
92,27:06 hamburger sandwich: How do you handle maxdop and cost threshold for parallelism on servers with multiple instances on the server?
92,28:51 Montro1981: In a recent webinar for some storage provider you had a deadlock demo on the StackOverflow database. Is that code available somewhere?
92,Previous
92,167
92,Next
92,"Hi! I’m Brent Ozar. I make Microsoft SQL Server go faster. I love teaching, travel, cars, and laughing. I’m based out of Las Vegas. He/him. I teach SQL Server training classes, or if you haven’t got time for the pain, I’m available for consulting too."
92,Want to advertise here and reach my savvy readers?
92,Subscribe*
92,© Brent Ozar Unlimited®. All Rights Reserved.
92,Privacy Policy – Terms and Conditions
92,ConsultingTrainingMonitoringScriptsBlog
92,Menu
94,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning | Journal of Cheminformatics | Full Text
94,Skip to main content
94,Advertisement
94,Search
94,Explore journals
94,Get published
94,About BMC
94,My account
94,Search all BMC articles
94,Search
94,Journal of Cheminformatics
94,Home
94,About
94,Articles
94,Submission Guidelines
94,About the Editors
94,Calls for Papers
94,Submit manuscript
94,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
94,Download PDF
94,Download ePub
94,Download PDF
94,Download ePub
94,Research
94,Open access
94,Published: 01 February 2024
94,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
94,"Jonghyun Lee1, Dae Won Jun1,2, Ildae Song3 & …Yun Kim4 Show authors"
94,Journal of Cheminformatics
94,"volume 16, Article number: 14 (2024)"
94,Cite this article
94,819 Accesses
94,2 Altmetric
94,Metrics details
94,"AbstractThe drug discovery process is demanding and time-consuming, and machine learning-based research is increasingly proposed to enhance efficiency. A significant challenge in this field is predicting whether a drug molecule’s structure will interact with a target protein. A recent study attempted to address this challenge by utilizing an encoder that leverages prior knowledge of molecular and protein structures, resulting in notable improvements in the prediction performance of the drug-target interactions task. Nonetheless, the target encoders employed in previous studies exhibit computational complexity that increases quadratically with the input length, thereby limiting their practical utility. To overcome this challenge, we adopt a hint-based learning strategy to develop a compact and efficient target encoder. With the adaptation parameter, our model can blend general knowledge and target-oriented knowledge to build features of the protein sequences. This approach yielded considerable performance enhancements and improved learning efficiency on three benchmark datasets: BIOSNAP, DAVIS, and Binding DB. Furthermore, our methodology boasts the merit of necessitating only a minimal Video RAM (VRAM) allocation, specifically 7.7GB, during the training phase (16.24% of the previous state-of-the-art model). This ensures the feasibility of training and inference even with constrained computational resources."
94,"IntroductionThe process of drug discovery is often compared to finding a needle in a haystack, requiring substantial funds and labor forces. Unfortunately, most newly discovered drugs fail to obtain approval for clinical use due to unexpected adverse drug reactions, insufficient drug effects, and low binding affinity [1,2,3,4,5]. Artificial intelligence has emerged as a promising tool for reducing expenses in various fields of drug discovery, including the predictions of drug toxicity, drug-drug interaction, and molecule properties, among others. In the first step of drug discovery, which involves drug repurposing and/or repositioning, it is critical to identify candidates of druggable molecules that target a specific protein. In this context, drug-target interaction (DTI) prediction tasks have emerged as a crucial area of research.Previous studies on DTI prediction can be broadly categorized into three categories: simulation-based molecular docking, structural similarity, and deep neural network (DNN) approach. Molecular docking simulation utilized 3D structures of proteins and molecules and simulated the binding sites [6,7,8]. While it offers a clear visual understanding, obtaining a 3D structure of a feature is challenging and it was hard to collect large datasets effectively. Conversely, the similarity-based technique proposed binding candidates using priorly established drug-target pairs. While this approach showed considerable predictions for recognized pairs based on similarity, it confronts difficulties in determining similarity for previously unobserved pairs [9, 10]. DNNs have exhibited proficient results in DTI prediction, similar to their successful implementations in various other domains. A pioneering study, DeepDTA [11], employed a drug and target encoder built on Convolutional Neural Networks (CNN) for the prediction of binding affinities. Instead of relying on highly complex datasets, the DeepDTA leveraged 1D expressions of the molecular structure system, Simplified Molecular Input Line Entry System (SMILES), and amino acid sequences, for drug and target, respectively. With hierarchical CNN layers, similar to conventional CNNs used for image recognition, DeepDTA can interpret the interactions of a given drug-target pair. After the DeepDTA, a multitude of research initiatives have been undertaken to either enhance the encoder’s capability or predict interactions more effectively. Such advancements encompass the deployment of CNNs [12,13,14], the development of interactions within gated cross attentions [15], the adoption of encoders that perceive molecular structures in graph format [16,17,18], computing similarity using enhanced DNN-based kernels [19,20,21], encode sequence using generative models [22, 23], and the integration of multi-modal techniques [24,25,26,27].The Transformer architecture [28], renowned for its proficiency in sequence processing, has been extensively employed as an encoder [29,30,31,32,33,34,35,36,37]. Nonetheless, it possesses a fundamental limitation: the computational expense escalates quadratically with the increase in the input length (see more details in Appendix C). Consequently, a majority of research initiatives have leaned towards its application as a drug encoder rather than for proteins [30,31,32,33, 37]. Recent advancements have brought forth efficient transformer methodologies, suggesting the potential for significantly reducing the computational demands in protein-encoding [38,39,40,41]. Concurrently, the ProtTrans project [35], leveraging the established Bidirectional Encoder Representations from Transformers (BERT) [42] model and its training methodology has undertaken pre-training of a protein encoder using an expansive set of amino acids and subsequently made it publicly available. As of now, the academic community lacks a publicly accessible, pre-trained model based on the efficient transformer, thereby preserving the relevance and utility of ProtTrans. A recent study, that utilized both transformer-based encoders for representing drugs and targets was proposed [43]. The prediction performances were considerably improved, however, due to the large size of the protein encoder, they truncated the protein language model into half its size.To reach an efficient computing model, knowledge distillation techniques were proposed [44, 45]. The key concept of knowledge distillation is distilling the knowledge from the large and complex model to the small and simple model with minimum loss of knowledge (See more details on Appendix A). However, DistillProtBERT (260 million parameters) [46], a model employing knowledge distillation from ProtBERT (420 million parameters) [35], is less efficient due to the inherent complexity of the amino acid sequence.To address this, we proposed a more efficient learning method than knowledge distillation, namely hint-based knowledge adaptation. This method involves using the intermediate features of the teacher model as hints, representing an expansion of knowledge distillation inspired by FitNet [47]. We term this approach “general knowledge” as it provides a general understanding of the target sequence, though lacking direct knowledge of the DTI task. It is assumed that this general knowledge, serving as a hint to the sequence, will facilitate successful learning despite the small size and simplicity of the student model. Conversely, the student model, designed to directly learn DTI performance, was structured in a simplified form compared to the original ProtBERT. In essence, knowledge adaptation presents an efficient means of leveraging both general knowledge of the target sequence and task-specific knowledge related to DTI simultaneously. This underscores the concept of adapting the teacher’s knowledge to the student’s knowledge, in contrast to knowledge distillation, which directly conveys task-specific knowledge.In this study, we proposed a Dual Language Model-based DTI model named DLM-DTI. The DLM-DTI was a lightweight and efficient, but accurate DTI prediction model. With the knowledge adaptation, the rich information from ProtBERT successfully adapted to predict DTI tasks. This study has several key contributions:"
94,"The hint-based knowledge adaptation technique, despite its compact parameterization, demonstrates considerably improved performance compared to baseline methods."
94,"By utilizing cached outputs from the teacher network, we achieved a notable reduction in computational costs."
94,"The knowledge adaptation approach is model-agnostic, offering flexibility in the selection of pre-trained models and architectures."
94,"Materials and methodsProblem definitionIn binary DTI classification, the goal is to predict the target value, \(Y_i\), for a given pair of \(X_i\), where \(\text {X}_i = \{ \text {x}_{\textrm{drug}}^i, \text {x}_{\textrm{target}}^i \}\), and \(\text {Y}_i \in \{ 0, 1 \}\) for \(i=1,\cdots , N\). The prediction of DTI can be viewed as a mapping function \(f(X_i) \rightarrow [0,1]\), which maps the drug-target pairs to a probability score of the interaction.Sequence representationSequence representations and embeddings involve converting a sequence, like a sentence, into a format that a computer model can understand. The first step is turning each part of the sequence into tokens, which are basically integer numbers that the model can work with. In this study, each part of the sequence is treated as a separate token. Special tokens, like a class token, are also added to grasp the overall meaning of the entire sequence. The concept of tokenization and special tokens is illustrated in Fig. 1.Fig. 1The concept of sequence representation and pre-training is illustrated. In A, the tokenization of a drug sequence (SMILES string) is depicted. In B, the tokenized elements are converted into integer values according to the predefined dictionary, and the encoder model (in this example, ChemBERTa) restores masked tokens into the original tokens (tokens colored in gray). After pre-training, the class token (CLS) is used to represent a given sequenceFull size imageDataset configurationsWe employ three datasets, namely DAVIS, Binding DB, and BIOSNAP, to train and evaluate the DLM-DTI. The DAVIS dataset consists of 68 drugs and 379 proteins, with 11,103 interactions measured in dissociation constant (\({K}_{d}\)) [48]. The interactions are categorized as positive or negative, with 1506 and 9597 instances, respectively. Similarly, the Binding DB dataset includes 10,665 drugs and 1413 proteins, with 32,601 interactions measured in \({K}_{d}\) [49]. The interactions are categorized as positive or negative, with 9166 and 23,435 instances, respectively. In this study, the threshold value for \({K}_{d}\) is set to 30 units, and interactions with \({K}_{d}\) values less than 30 units are considered positive binding interactions between the given drug and protein pair [29, 43]. The BIOSNAP dataset is initially composed of positive interactions only; however, negative pairs are added in the MolTrans study. The BIOSNAP dataset comprises 4510 drugs and 2181 proteins, with 27,482 interactions, including 13,741 positive and 13,741 negative instances [29].The integrated data training was first proposed by Kang et al., and they demonstrated improvements [43]. In this setting, training and validation datasets were merged, and a model was trained using integrated datasets. After the training steps, the trained model with integrated training datasets was evaluated on individual test datasets. For example, to test the BIOSNAP test dataset, the model was first trained using DAVIS, Binding DB, and BIOSNAP’s training datasets, and then tested on BIOSNAP’s test dataset. Generally, the diversity and quantity of datasets are linked to the improvement of prediction performance. Therefore, we also assessed the impact of dataset integrations using DLM-DTI. A summary of the dataset description is presented in Table 1.Table 1 The description of datasetsFull size tableTo ensure a fair comparison of model performance, we employ the same training, validation, and testing datasets used in previous studies [29, 43]. The datasets are split into training, validation, and testing datasets in the ratio of 7:1:2, respectively. The number of interactions for each data splitting is summarized in Table 2.Table 2 The number of interactions for each splitFull size tableModel configurationsThe process flow of DLM-DTI is depicted in Fig. 2. DLM-DTI was comprised of three primary components: the drug encoder, target encoder, and interaction prediction head. Notably, the target encoder encompasses both the teacher and student models of language models for protein sequences.Fig. 2The process flow of DLM-DTI. The drug and target sequences feed into their respective encoders. The encoded sequences are then merged, and the probability of bindings is computed using the interaction prediction head. DLM-DTI only utilizes the class token (CLS) of each encoded sequence because the class token preserves the abstract meaning of the entire sequence. The features of target sequences are computed using a teacher-student-based architecture, specifically employing a hint-based learning strategyFull size imageDrug encoderThe drug encoder converts SMILES sequences into meaningful features, serving as a mapping function from molecule sequences to a meaningful chemical space. We employed the ChemBERTa encoder, which was trained on various canonical SMILES and learned chemical space. Further details are described in Appendix B.The class token of the last hidden layer was extracted as input for the interaction prediction head. The encoding process of the drug sequence can be represented as follows:$$\begin{aligned} z_{\textrm{drug}} = f\left( \text {LN}(x_{\textrm{class}})\right) , \end{aligned}$$"
94,(1)
94,"where \(\text {LN}(\cdot )\) denotes the layer normalization layer, \(f(\cdot )\) denotes the projection function used to align the dimensions, and the hidden dimensions were set to 512 in this study. The upper limit of the drug sequence length was 512 tokens, corresponding to the maximum sequence length of the original ChemBERTa encoder [31].Target encoderSimilar to the drug encoder, the target encoder also extracts meaningful features from raw target sequences (amino acid sequences). The target encoder in this study was composed of both a teacher and a student model. The teacher model used for target sequence encoding was the ProtBERT model, pre-trained on UniRef and big fantastic database databases [35]. Details of ProtBERT are described in Appendix C. The original ProtBERT model was trained on sequences up to 40 K characters, with 420 million parameters. The student model was designed to match the original teacher model, ProtBERT, however, the number of layers was reduced. Except for the number of layers, the student model followed the hyperparameters of the teacher model. The number of parameters of the student model was 6.2% of the teacher model; teacher model: 420.0 million, student model: 26 million. The detailed parameters of the target encoder are presented in Table 3.Table 3 The specific parameters of target encoderFull size tableIn most cases, fully fine-tuning the large model was impractical due to restrictions on datasets and the associated computational expenses. To address this challenge, we adopted a hint-based training scheme that kind of knowledge distillation comprises both a teacher model and a student model. The teacher model was prevented from parameter updates, enabling solely the parameters of the student model to be updated. Given that the teacher model’s output was not subject to training, it retained a fixed form, thus enabling us to cache outputs of the teacher model prior to the training and inference step. This strategy markedly minimizes computational redundancy, thereby optimizing computational efficiency. Considering the teacher model’s output was not trained, it served as a form of hint to which the task-specific model (student model) could refer. The teacher and student models were combined using class token mixing to encode the target sequence. The output class token was treated as a “hint” that contained general knowledge of the given protein sequence. On the other hand, the output class token of the student model was considered as task-oriented specific knowledge. To mix the general knowledge and task-specific knowledge, we added two class tokens with learnable gating parameters (\(\lambda\)). The encoding process of the target sequence can be represented as follows:$$\begin{aligned} z_{\textrm{target}} = \lambda g\left( \text {LN} (x^{\textrm{student}}_{\textrm{class}})) + (1 - \lambda ) h (\text {LN}(x^{\textrm{teacher}}_{\textrm{class}})\right) , \end{aligned}$$"
94,(2)
94,"where \(g(\cdot )\) and \(h(\cdot )\) are the projection functions used to align the dimensions, and the adaptation parameter \(\lambda\) is a learnable parameter initialized randomly from a uniform distribution, \(\lambda \sim Uniform(0, 1)\). The term “adaptation” was employed to describe the process of adjusting general knowledge to suit the specific requirements of a particular task. An elevated value of the adaptation parameter indicated an increased emphasis of the model on the class token derived from the teacher model. In contrast, a decreased value of the adaptation parameter signified a predominant utilization of task-specific information obtained from the student model. The hidden dimensions of the class token mixing were set to 1024 in this study. The maximum length of the target sequence was set to 545 tokens, which covered 95% of proteins in the datasets, and the same max protein sequence lengths of previous studies [29, 43].Interaction prediction headThe class tokens of drug and target sequences have abstract meanings for each sequence. The interaction prediction head aggregated the features of drug-target pairs and predicted binding probability. In this step, there were multiple choices for mixing the features; for example, cross attention, capsule network, etc. However, we simply employed concatenation that showed stable performances in the previous work [43].The interaction module consists of three sequential blocks. Each block is structured with a Fully Connected (FC) layer, followed by an activation function and subsequently a dropout layer. The respective dimensions of the FC layers are 2048, 1024, and 512. The chosen activation function for these blocks is the Gaussian Error Linear Unit (GeLU). Additionally, a dropout rate of 0.1 has been employed for regularization. A detailed schematic of this configuration can be found in Fig. 3, and the specific parameter values are summarized in Table 4.Fig. 3Structure of the interaction prediction head. The interaction prediction head mixes the features of the drug-target pair to predict the binding probability of a given pair. The number under the block indicates the feature dimensionFull size imageTable 4 The detailed parameters of interaction prediction headFull size tableExperimental setupEvaluation metricsWe used the Area Under the Receiver Operation Characteristics curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) as primary evaluation metrics. AUROC is one of the most favorable metrics to measure classification performance, particularly in the medical field; however, it could be easily overestimated when the data has class imbalance [50]. Therefore, AUPRC is a relatively robust metric for measuring classification performance in imbalanced settings [50]. Sensitivity and specificity scores were utilized as sub-metrics, and the threshold for these sub-metrics was simply set to 0.5.Model training hyperparametersThe DLM-DTI was optimized using the AdamW optimizer with a learning rate of 0.0001. A cosine annealing learning rate scheduler was employed to adjust the learning rate. The binary cross-entropy loss was used to calculate the difference between predictions and ground truth. The model was trained for 50 epochs, and the best-performing parameters were selected based on the AUPRC score during validation. Due to severe class imbalance, the model could easily be overfitted to the dominant class. To prevent the selection of an overfitted model, we set the selection criteria as AUPRC rather than AUROC or the minimum loss coefficient. Automated mixed precision was utilized, and the batch size was set to 32. The best combination of hyperparameters was determined through iterative experiments.The use of a class imbalance sampler did not show any benefit for model training; therefore, we did not apply an imbalance sampler. Instead, AUPRC-based optimization demonstrated better performance in predicting binding probability.Hardware and softwardWe used a single NVIDIA A100 GPU to train DLM-DTI. The Python (v3.8) and PyTorch deep learning framework (v1.13) for trained DLM-DTI.ResultsBinding probability predictionThe baseline models, namely MolTrans [29] and the approach by Kang et al. [43], along with our proposed DLM-DTI, were trained on the same training datasets and evaluated using identical test datasets. Table 5 presents a summary of the evaluation results obtained from these experiments. MolTrans was exclusively trained on individual datasets and evaluated individually. In contrast, both Kang et al. and our DLM-DTI were trained using both individual and combined dataset settings. This approach was claimed in Kang et al., and therefore the previous study, MolTrans, did not experiment with an integrated dataset.Within the BIOSNAP dataset, DLM-DTI showed an improved AUPRC score (absolute value; percentage) than MolTrans (0.013; 1.44%), and Kang et al. (0.014 \(\sim\) 0.017; 1.56 \(\sim\) 1.90%). The AUROC score was improved compared to MolTrans (0.019; 2.12%), however, the AUROC showed similarity to Kang et al.’s model. Similarly, in the Binding DB, DLM-DTI exhibited a considerably improved AUPRC score than other methods, MolTrans (0.021; 3.38%), and Kang et al.’s model (0.004 \(\sim\) 0.02; 0.63 \(\sim\) 3.21%), respectively.In the DAVIS dataset, the performance of the DLM-DTI was degraded, and its performance was similar to that of MolTrans. The training with an integrated dataset showed benefits for the DLM-DTI only in the DAVIS dataset.Table 5 The prediction performance of binding affinityFull size tableAdaptation parameter, \(\lambda\)"
94,"During the training, the randomly initialized adaptation parameter \(\lambda\) gradually decreased and converged, as illustrated in Fig. 4. The adaptation parameter controlled the feature weights from the teacher and student encoder. As mentioned earlier, the teacher encoder contained general knowledge of the target sequence, and the student encoder had narrow but specific task-related knowledge. With the adaptation parameter, the DLM-DTI modulated the importance of each feature to accurately predict binding probability.Fig. 4Variation of the adaptation parameter (\(\lambda\)) during model training processFull size imageTo evaluate the effect of teacher-student architecture-based target sequence encoding, two ablation studies were conducted."
94,\(\lambda\) set to 0: Only the teacher encoder (general knowledge) was utilized.
94,\(\lambda\) set to 1: Only the student encoder (task-specific knowledge) was utilized.
94,"The adaptation setting (which utilized both teacher-student encoders) showed the best performance (AUROC: 0.912; AUPRC: 0.643) compared to the teacher encoder-only setting (AUROC: 0.911; AUPRC: 0.635) or the student encoder-only setting (AUROC: 0.900; AUPRC: 0.635). The effect of the \(\lambda\) parameter is summarized in Table 6.Table 6 The prediction performance of binding affinityFull size tableThe student encoder-only setting exhibited the poorest prediction performance (Rank: \(\text {3}^{\textrm{rd}}\)). This implies that two layers of simple and shallow networks were not sufficient to capture the complex patterns and features of target sequences to accurately predict DTIs. However, the teacher encoder-only setting demonstrated comparable performance (Rank: \(\text {2}^{\textrm{nd}}\)). This suggests that the general knowledge of the teacher model has the potential to predict binding probability. The teacher encoder-only setting corresponds to linear probing, where the training strategy only updates the prediction head without adjusting the weights of the encoder [51, 52]. The prediction performance of linear probing is considered as an encoder’s existing knowledge.Time and memory analysisTypically, a model’s performance exhibits a direct correlation with its parameter count, suggesting that larger models often yield superior outcomes. Nonetheless, this advantage comes with a caveat; substantial models necessitate considerable computational resources during both the training and inference stages. In light of this, we embarked on a systematic analysis comparing training time and parameter counts (Table 7). The metric for training time was derived by computing the mean learning time across three epochs, utilizing the Binding DB dataset.Table 7 Time and memory analysis of baseline models and DLM-DTIFull size tableDLM-DTI showed the best AUPRC score (0.643), only with 24.56% (86.7 million) of parameters compared to the Kang et al. (353.0 million) [43]. Additionally, DLM-DTI required 7.7 GB video random access memory (VRAM), and 63.00 s for a single training epoch. It was 16.24% (47.4 GB), and 9.98% (631.00 s) of the Kang et al. [43]. The MolTrans required the smallest VRAM (5.9 GB), however, the AUPRC score (0.622) was slightly lower than DLM-DTI (0.643). In our experimental setting, DLM-DTI required 7.7 GB of VRAM, therefore, it could be trained on conventional graphic processing units (GPUs), not for high-performing research machines (See details on 2.5.2).Cold drug, target, and bindingsIn addressing DTI challenges, the cold splitting testing approach is widely adopted [36, 53], primarily due to the inherent difficulties in dataset procurement and the paramount importance of achieving generalization for novel pairs. The term “cold splitting” pertains to scenarios where previously unseen drug-target interactions are involved, ones that were excluded from both the training and validation datasets. To simulate this condition, we conducted experiments where we isolated cold drugs, cold targets, and cold binding interactions from the test set of models trained to utilize the Binding DB dataset. We identified a total of 2,127 cold drugs and 136 cold targets. Specifically, a cold drug configuration encompasses all interactions associated with a cold drug, while a cold target configuration comprises all interactions associated with a cold target. The cold bindings were the interactions between cold drugs and cold targets, and only 114 pairs were identified. The performances of cold-splitting datasets are summarized in Table 8. DLM-DTI’s performance was comparable to the baseline models in the context of the cold drug, yet exhibited a minor deterioration to the cold target and was found to be most deficient in addressing cold binding. Conversely, Kang et al. [43] manifested commendable prediction capabilities across all testing scenarios. MolTrans [29] exhibited a performance metric closely mirroring Kang et al. in terms of AUROC, but fell short when evaluated using AUPRC.Table 8 The classification performances within the cold splitting settingsFull size tableDiscussionIn this study, we suggested a lightweight but accurate DTI prediction model, namely DLM-DTI. The main hurdle for utilizing protein sequence-based language models, such as ProtBERT [35], was heavy computing resource requirements. To comprehend the complex and long sequence of a protein, it needed heavy and large architectures and an intensive pre-training process. The DLM-DTI mitigated the computational burden caused by the protein encoder, by using a knowledge adaptation. DLM-DTI achieved improved AUPRC performance, especially in Binding DB (0.63 \(\sim\) 3.38%), and BIOSNAP (1.44 \(\sim\) 1.9%) datasets. The most interesting point was that DLM-DTI utilized only 25% of parameters (86.7 million) compared to the previous state-of-the-art model, Kang et al. (353 million) [43]. Additionally, DLM-DTI required only 7.7 GB of VRAM, and 63 s for each training epoch, that of 16.24%, and 9.98% of Kang et al. [43].The Transformer-based language model has exhibited impressive capabilities across various applications, including molecular and protein sequences. However, pre-training has emerged as a key approach to further optimize the model’s functional and semantic relationship learning from large sequence datasets [35,36,37, 42, 43]. Despite the promising results, the computational cost of the language model increases significantly with the input length. To address this challenge, Kang et al. proposed a Kang et al. approach, which employed only half of the pre-trained target encoder [43]. The methodology employed by the ELECTRA-DTA model aligns closely with our approach [36]. In the ELECTRA-DTA framework, the features originating from the pre-trained drug encoder and protein encoder are individually averaged. Subsequently, these averaged features are compactly represented as a compressed feature vector. This vector is subsequently incorporated into a squeeze-and-excitation network, aiming to enhance the predictive capabilities of the model. Their approach can also be perceived as a tactical maneuver to circumvent the necessity of fine-tuning the complete encoder. However, it is important to note that we could not directly compare the prediction performance of our DLM-DTI approach to that of ELECTRA-DTA due to differences in the target tasks, with DLM-DTI using binary classification and ELECTRA-DTA using \(pK_{d}\) regression.In our study, we introduced an adaptation parameter to efficiently generate meaningful protein features. The adaptation parameter, denoted as \(\lambda\), was randomly initialized and tuned. This parameter controlled the weights of knowledge from both the teacher model (providing general knowledge) and the student model (capturing task-specific knowledge). In the ablation studies (Table 6), the absence of knowledge adaptation resulted in significant degradation of performance for both the teacher-only and student-only settings. However, the DLM-DTI with knowledge adaptation exhibited weaknesses in generalization performance. Kang et al.’s [43] work also demonstrated strong performance under cold-splitting conditions (Table 8). In contrast, our DLM-DTI, which either matched or outperformed Kang et al. on the complete dataset, showed reduced effectiveness in cold-splitting evaluations, particularly concerning cold-binding interactions. This may be attributed to the over-reduction of the student model, limiting generalization performance. Inspired by recent examples that incorporate natural language-based prior knowledge to enhance prediction performance, we aim to improve our approach by adding natural language information related to the function of proteins in future work [54]. Interestingly, integrated dataset training did not prove beneficial for DLM-DTI. In Kang et al. [43], training with integrated datasets demonstrated outstanding performances. Large-scale Transformer-based architectures typically require a substantial amount of data to realize their full potential. However, DLM-DTI introduces a small-scale student model, and it is speculated that the small size was sufficient for effective learning.Recently, foundation models based on large language models have been widely studied [55, 56]. A shared challenge between these models and protein sequence encoders pertains to the intricacies involved in fine-tuning. Due to the scarcity of annotated data and the extensive parameters within these models, innovative strategies for effective fine-tuning have been proposed. For instance, a method called low-rank adaptation (LoRA) [57], similar to our own approach, adopt a technique where only the adaptation layer is adjusted. This is achieved by integrating a low-rank adaptation layer, which eliminates the need for comprehensive fine-tuning across all layers. This approach proves to be more cost-effective and quicker to converge compared to the resource-intensive process of complete fine-tuning. Therefore, in our future study, we plan to compare the performances of a fine-tuning model using LoRA’s adaptation approaches. Furthermore, there is a need for enhancement in the design of the interaction head. Currently, this component is composed of a sequence of straightforward FC layers, which exhibits reduced effectiveness in cold bindings. To address this, potential strategies include the integration of a squeeze-and-excitation network [58], capsule network [59], cross-attention [60], and other alternatives.ConclusionIn this study, we employed knowledge adaptation to efficiently and accurately predict binding probability. The knowledge adaptation was efficiently tuned with both general knowledge and task-specific knowledge through the teacher-student architectures. With only 25% of the model parameters, DLM-DTI exhibited considerable performance compared to the previous state-of-the-art model. Notably, DLM-DTI required 7.7 GB of VRAM, allowing training on conventional GPUs without the need for high-performing GPUs."
94,Availability of data and materials
94,The datasets are available at: https://github.com/kexinhuang12345/MolTrans/tree/master/dataset.
94,Code availability
94,The source codes are available at: https://github.com/jonghyunlee1993/DLM-DTI_hint-based-learning/tree/master.
94,"ReferencesAnusuya S, Kesherwani M, Priya KV, Vimala A, Shanmugam G, Velmurugan D, Gromiha MM (2018) Drug-target interactions: prediction methods and applications. Curr Protein Pept Sci 19(6):537–561Article"
94,CAS
94,PubMed
94,Google Scholar
94,Ledford H (2011) 4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. Nature looks at ways they might be saved. Nature 477(7366):526–529Article
94,CAS
94,PubMed
94,Google Scholar
94,"Zheng Y, Wu Z (2021) A machine learning-based biological drug-target interaction prediction method for a tripartite heterogeneous network. ACS Omega 6(4):3037–3045Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Ashburn TT, Thor KB (2004) Drug repositioning: identifying and developing new uses for existing drugs. Nat Rev Drug Discovery 3(8):673–683Article"
94,CAS
94,PubMed
94,Google Scholar
94,Strittmatter SM (2014) Overcoming drug development bottlenecks with repurposing: old drugs learn new tricks. Nat Med 20(6):590–591Article
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Li H, Leung K-S, Wong M-H, Ballester PJ (2015) Low-quality structural and interaction data improves binding affinity prediction via random forest. Molecules 20(6):10947–10962Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Trott O, Olson AJ (2010) Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J Comput Chem 31(2):455–461Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Luo H, Mattes W, Mendrick DL, Hong H (2016) Molecular docking for identification of potential targets for drug repurposing. Curr Top Med Chem 16(30):3636–3645Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Pahikkala T, Airola A, Pietilä S, Shakyawar S, Szwajda A, Tang J, Aittokallio T (2015) Toward more realistic drug-target interaction predictions. Brief Bioinform 16(2):325–337Article"
94,CAS
94,PubMed
94,Google Scholar
94,"He T, Heidemeyer M, Ban F, Cherkasov A, Ester M (2017) Simboost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines. J Cheminformatics 9(1):1–14Article"
94,Google Scholar
94,"Öztürk H, Özgür A, Ozkirimli E (2018) Deepdta: deep drug-target binding affinity prediction. Bioinformatics 34(17):821–829Article"
94,Google Scholar
94,"Lee I, Keum J, Nam H (2019) Deepconv-dti: prediction of drug-target interactions via deep learning with convolution on protein sequences. PLoS Comput Biol 15(6):1007129Article"
94,Google Scholar
94,"Lee I, Nam H (2022) Sequence-based prediction of protein binding regions and drug-target interactions. J Cheminformatics 14(1):1–15Article"
94,Google Scholar
94,"Zeng Y, Chen X, Luo Y, Li X, Peng D (2021) Deep drug-target binding affinity prediction with multiple attention blocks. Brief Bioinform 22(5):117Article"
94,Google Scholar
94,"Kim Y, Shin B (2021) An interpretable framework for drug-target interaction with gated cross attention. In: Machine Learning for Healthcare Conference, pp. 337–353. PMLRNguyen T, Le H, Quinn TP, Nguyen T, Le TD, Venkatesh S (2021) Graphdta: predicting drug-target binding affinity with graph neural networks. Bioinformatics 37(8):1140–1147Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Thafar MA, Alshahrani M, Albaradei S, Gojobori T, Essack M, Gao X (2022) Affinity2vec: drug-target binding affinity prediction through representation learning, graph mining, and machine learning. Sci Rep 12(1):1–18Article"
94,Google Scholar
94,"Liao J, Chen H, Wei L, Wei L (2022) Gsaml-dta: an interpretable drug-target binding affinity prediction model based on graph neural networks with self-attention mechanism and mutual information. Comput Biol Med 150:106145Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Su X, Hu L, You Z, Hu P, Wang L, Zhao B (2022) A deep learning method for repurposing antiviral drugs against new viruses via multi-view nonnegative matrix factorization and its application to sars-cov-2. Brief Bioinform 23(1):526Article"
94,Google Scholar
94,"Li Y-C, You Z-H, Yu C-Q, Wang L, Wong L, Hu L, Hu P-W, Huang Y-A (2022) Ppaedti: personalized propagation auto-encoder model for predicting drug-target interactions. IEEE J Biomed Health Inform 27(1):573–582Article"
94,Google Scholar
94,"Thafar MA, Olayan RS, Albaradei S, Bajic VB, Gojobori T, Essack M, Gao X (2021) Dti2vec: drug-target interaction prediction using network embedding and ensemble learning. J Cheminformatics 13(1):1–18Article"
94,Google Scholar
94,"Zhao L, Wang J, Pang L, Liu Y, Zhang J (2020) Gansdta: predicting drug-target binding affinity using gans. Front Genetics 1243Chen Y, Wang Z, Wang L, Wang J, Li P, Cao D, Zeng X, Ye X, Sakurai T (2023) Deep generative model for drug design from protein target sequence. J Cheminformatics 15(1):38Article"
94,CAS
94,Google Scholar
94,"Liu G, Singha M, Pu L, Neupane P, Feinstein J, Wu H-C, Ramanujam J, Brylinski M (2021) Graphdti: a robust deep learning predictor of drug-target interactions from multiple heterogeneous data. J Cheminformatics 13(1):1–17Article"
94,Google Scholar
94,"Yan X, Liu Y (2022) Graph-sequence attention and transformer for predicting drug-target affinity. RSC Adv 12(45):29525–29534Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Hua Y, Song X, Feng Z, Wu X (2023) Mfr-dta: a multi-functional and robust model for predicting drug-target binding affinity and region. Bioinformatics 39(2):056Article"
94,Google Scholar
94,"Bian J, Zhang X, Zhang X, Xu D, Wang G (2023) Mcanet: shared-weight-based multiheadcrossattention network for drug-target interaction prediction. Brief Bioinform 24(2):082Article"
94,Google Scholar
94,"Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Proc Syst 30Huang K, Xiao C, Glass LM, Sun J (2021) Moltrans: molecular interaction transformer for drug-target interaction prediction. Bioinformatics 37(6):830–836Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Honda S, Shi S, Ueda HR (2019) Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738Chithrananda S, Grand G, Ramsundar B (2020) Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Molecule attention transformer. arXiv preprint arXiv:2002.08264Fabian B, Edlich T, Gaspar H, Segler M, Meyers J, Fiscato M, Ahmed M (2020) Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) Unified rational protein engineering with sequence-based deep representation learning. Nat Methods 16(12):1315–1322Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M, et al (2020) Prottrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225Wang J, Wen N, Wang C, Zhao L, Cheng L (2022) Electra-dta: a new compound-protein binding affinity prediction model based on the contextualized sequence encoding. J Cheminformatics 14(1):1–14Article"
94,Google Scholar
94,"Shin B, Park S, Kang K, Ho JC (2019) Self-attention based molecule representation for predicting drug-target interaction. In: Machine Learning for Healthcare Conference, pp. 230–248. PMLRXiong Y, Zeng Z, Chakraborty R, Tan M, Fung G, Li Y, Singh V (2021) Nyströmformer: A nyström-based algorithm for approximating self-attention. Proc AAAI Conf Artif Intell 35:14138–14148PubMed"
94,PubMed Central
94,Google Scholar
94,"Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509Press O, Smith NA, Lewis M (2021) Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409Dao T, Fu D, Ermon S, Rudra A, Ré C (2022) Flashattention: fast and memory-efficient exact attention with io-awareness. Adv Neural Inf Process Syst 35:16344–16359"
94,Google Scholar
94,"Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805Kang H, Goo S, Lee H, Chae J-W, Yun H-Y, Jung S (2022) Fine-tuning of bert model to accurately predict drug-target interactions. Pharmaceutics 14(8):1710Article"
94,CAS
94,PubMed
94,PubMed Central
94,Google Scholar
94,"Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531Gou J, Yu B, Maybank SJ, Tao D (2021) Knowledge distillation: a survey. Int J Comput Vision 129:1789–1819Article"
94,Google Scholar
94,"Geffen Y, Ofran Y, Unger R (2022) Distilprotbert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts. Bioinformatics 38(Supplement–2):95–98Article"
94,Google Scholar
94,"Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550Davis MI, Hunt JP, Herrgard S, Ciceri P, Wodicka LM, Pallares G, Hocker M, Treiber DK, Zarrinkar PP (2011) Comprehensive analysis of kinase inhibitor selectivity. Nat Biotechnol 29(11):1046–1051Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Liu T, Lin Y, Wen X, Jorissen RN, Gilson MK (2007) Bindingdb: a web-accessible database of experimentally determined protein-ligand binding affinities. Nucleic Acids Res 35(suppl-1):198–201Article"
94,Google Scholar
94,"Saito T, Rehmsmeier M (2015) The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. PLoS ONE 10(3):0118432Article"
94,Google Scholar
94,"Kumar A, Raghunathan A, Jones RM, Ma T, Liang P (2022) Fine-tuning can distort pretrained features and underperform out-of-distribution. In: International Conference on Learning Representations. https://openreview.net/forum?id=UYneFzXSJWhAlain G, Bengio Y (2016) Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644Chatterjee A, Walters R, Shafi Z, Ahmed OS, Sebek M, Gysi D, Yu R, Eliassi-Rad T, Barabási A-L, Menichetti G (2021) Ai-bind: improving binding predictions for novel protein targets and ligands. arXiv preprint arXiv:2112.13168Chen YT, Zou J (2023) Genept: a simple but hard-to-beat foundation model for genes and cells built from chatgpt. bioRxiv, 2023–10Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al (2023) Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham P, Chung HW, Sutton C, Gehrmann S, et al (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W (2021) Lora: low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141Sabour S, Frosst N, Hinton GE (2017) Dynamic routing between capsules. Adv Neural Inf Proc Syst 30Gheini M, Ren X, May J (2021) Cross-attention is all you need: adapting pretrained transformers for machine translation. arXiv preprint arXiv:2104.08771Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692Shibata Y, Kida T, Fukamachi S, Takeda M, Shinohara A, Shinohara T, Arikawa S (1999) Byte pair encoding: a text compression scheme that accelerates pattern matchingRogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Inf Model 50(5):742–754Article"
94,CAS
94,PubMed
94,Google Scholar
94,"Katharopoulos A, Vyas A, Pappas N, Fleuret F (2020) Transformers are rnns: Fast autoregressive transformers with linear attention. In: International Conference on Machine Learning, pp. 5156–5165. PMLRDownload referencesFunding(1) This work was supported by research grants from Daegu Catholic University in 2022. (2) This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (RS-2022-00166945).Author informationAuthors and AffiliationsDepartment of Medical and Digital Engineering, Hanyang University College of Engineering, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaJonghyun Lee & Dae Won JunDepartment of Internal Medicine, Hanyang University College of Medicine, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaDae Won JunDepartment of Pharmaceutical Science and Technology, Kyungsung University, 309, Suyeong-ro, Nam-gu, Busan, 48434, KoreaIldae SongCollege of Pharmacy, Deagu Catholic University, 13-13, Hayang-ro, Hayang-eup, Gyeongsan-si, 38430, Gyeongsangbuk-do, KoreaYun KimAuthorsJonghyun LeeView author publicationsYou can also search for this author in"
94,PubMed Google ScholarDae Won JunView author publicationsYou can also search for this author in
94,PubMed Google ScholarIldae SongView author publicationsYou can also search for this author in
94,PubMed Google ScholarYun KimView author publicationsYou can also search for this author in
94,"PubMed Google ScholarContributionsConceptualization, JL, DJ, IS, and YK; methodology, JL, and YK; writing—original draft preparation, JL; writing—review and editing, YK; supervision, DJ, and YK; formal analysis, JL; resources, YK; All authors have read and agreed to the published version of the manuscript.Corresponding authorCorrespondence to"
94,Yun Kim.Ethics declarations
94,Ethics approval and consent to participate
94,Not applicable.
94,Consent for publication
94,Not applicable.
94,Competing interests
94,The author(s) declare that they have no conflict of interest.
94,"Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.AppendicesAppendix A Knowledge DistillationRecent high-performing DNN models boast millions or billions of parameters, necessitating extensive and high-performance hardware resources, such as GPU clusters and TPU pods. Knowledge distillation was proposed to develop a lightweight model while retaining robust information processing capabilities [44, 45]. The knowledge distillation process involves two models, specifically the teacher model and the student model. Conventionally, knowledge distillation begins by training the teacher model, a complex and high-capacity model, on the target task. Subsequently, the acquired knowledge from the teacher model is transferred to the student model, a more lightweight counterpart. This transfer is typically achieved by encouraging the student model to mimic the outputs [44] or internal representations [47] of the teacher model. The overarching goal is to distill the comprehensive knowledge captured by the teacher model into a more compact and computationally efficient student model.FitNet [47] introduces the concept of “hints” to enhance the knowledge distillation approach. In addition to replicating the output of the current teacher model, hints guide the student to mimic intermediate features together. This inclusion of hints enhances the performance of knowledge distillation by enabling the learning of not only the final result but also the intermediate features. In this context, a hint can be interpreted as providing information about both the intermediate features and the final feature.Appendix B Drug Encoder: ChemBERTaChemBERTa is a Transformer-based model pre-trained using 10 million SMILES sequences [31]. Based on RoBERTa [61], a model known for its outstanding performance in natural language processing, ChemBERTa comprises 12 attention heads and 6 layers. Drug sequences, expressed in Canonical SMILES, are tokenized using a subword-level tokenizer, while a byte-pair encoder (BPE) tokenizer is employed to group frequently occurring elements together into larger chunks for more efficient processing. BPE stands as a blend of character and word-level representations, facilitating the management of extensive vocabularies in natural language corpora. Guided by the insight that less common or unfamiliar words can frequently be broken down into several recognized subwords, BPE identifies the optimal word segmentation through an iterative and greedy merging of frequently occurring character pairs [62]. ChemBERTa has a total of 767 tokens, including a class token to encapsulate the abstract meaning of the entire sequence, a start of sequence token (SOS), an end of sequence token (EOS), and a pad token to mark the start and end of the sequence.ChemBERTa was trained using masked language modeling (MLM), where the task involves masking a portion of the entire sequence and then restoring the corresponding tokens; 15% of the total sequence was masked. The maximum processable sequence length is 512 tokens. ChemBERTa, pre-trained using MLM tasks, can then be used as an encoder for drug sequences because it has been trained on restoration tasks and has an understanding of molecule sequences. ChemBERTa can perform comparably to the commonly used extended-connectivity fingerprint (ECFP) [63] in molecule properties prediction tasks using the ChemBERTa encoder, and it was employed in this study due to its availability through the HuggingFace API, facilitating easy utilization.Appendix C Target Encoder: ProtBERTProtBERT, a component of the ProtTrans project, is a BERT model trained on an extensive dataset of amino acid sequences [35]. It underwent training using the same MLM approach as ChemBERTa, with 15% masking (Appendix B). However, owing to the intricacy of amino acid sequences, ProtBERT consists of 30 layers and 16 attention heads, resulting in a total parameter count of 4.2 million. Each element is considered one token in ProtBERT, and it comprises 30 tokens, including special tokens. Notably, it was trained to handle sequences of up to 4000 tokens, accommodating the typically extended length of amino acid sequences.However, ProtBERT uses the Transformer’s core operation, self-attention, where the amount of computation increases as the square of the length of a given sequence. The self-attention operation is as follows:$$\begin{aligned} \text {Attention}(Q, K) = \text {softmax} \left( \frac{{QK^T}}{{\sqrt{d_k}}}\right) , \end{aligned}$$"
94,(C1)
94,"where the query (Q) is the product of input sequence x and learnable parameter \(W_{Q}\), and key (K) is the product of input sequence x and learnable parameter \(W_{K}\).Therefore, a substantial amount of memory and computational resources must be allocated to manage long sequences of amino acids. This constitutes a significant bottleneck in the practical utilization of ProtBERT. While recent proposals, such as efficient self-attention computations using linear transformers [64] and Nystrom approximation [38], aim to address this challenge, pre-training with such approaches remains expensive. As an illustration, ProtBERT underwent training utilizing 1,024 tensor processing units (TPUs), a resource allocation typically inaccessible in standard research environments. Consequently, this study emphasizes the efficient utilization of the previously published ProtBERT, prioritizing practical application over creating a new pre-training model that might reduce computational requirements.Rights and permissions"
94,Open Access
94,"This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data."
94,"Reprints and permissionsAbout this articleCite this articleLee, J., Jun, D.W., Song, I. et al. DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning."
94,"J Cheminform 16, 14 (2024). https://doi.org/10.1186/s13321-024-00808-1Download citationReceived: 09 September 2023Accepted: 22 January 2024Published: 01 February 2024DOI: https://doi.org/10.1186/s13321-024-00808-1Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard"
94,Provided by the Springer Nature SharedIt content-sharing initiative
94,KeywordsDrug-target interactionsPre-trained language modelKnowledge adaptationLightweight framework
94,Download PDF
94,Download ePub
94,Advertisement
94,Journal of Cheminformatics
94,ISSN: 1758-2946
94,Contact us
94,Submission enquiries: journalsubmissions@springernature.com
94,Read more on our blogs
94,Receive BMC newsletters
94,Manage article alerts
94,Language editing for authors
94,Scientific editing for authors
94,Policies
94,Accessibility
94,Press center
94,Support and Contact
94,Leave feedback
94,Careers
94,Follow BMC
94,BMC Twitter page
94,BMC Facebook page
94,BMC Weibo page
94,"By using this website, you agree to our"
94,"Terms and Conditions,"
94,"Your US state privacy rights,"
94,Privacy
94,statement and
94,Cookies policy.
94,Your privacy choices/Manage cookies we use in the preference centre.
94,© 2024 BioMed Central Ltd unless otherwise stated. Part of
94,Springer Nature.
95,“How-to” Guides
95,Dark Theme
95,“How-to” Guides
95,Table of Contents
95,Back to index
95,1. Spring Boot Application
95,1.1. Create Your Own FailureAnalyzer
95,1.2. Troubleshoot Auto-configuration
95,1.3. Customize the Environment or ApplicationContext Before It Starts
95,1.4. Build an ApplicationContext Hierarchy (Adding a Parent or Root Context)
95,1.5. Create a Non-web Application
95,2. Properties and Configuration
95,2.1. Automatically Expand Properties at Build Time
95,2.1.1. Automatic Property Expansion Using Maven
95,2.1.2. Automatic Property Expansion Using Gradle
95,2.2. Externalize the Configuration of SpringApplication
95,2.3. Change the Location of External Properties of an Application
95,2.4. Use ‘Short’ Command Line Arguments
95,2.5. Use YAML for External Properties
95,2.6. Set the Active Spring Profiles
95,2.7. Set the Default Profile Name
95,2.8. Change Configuration Depending on the Environment
95,2.9. Discover Built-in Options for External Properties
95,3. Embedded Web Servers
95,3.1. Use Another Web Server
95,3.2. Disabling the Web Server
95,3.3. Change the HTTP Port
95,3.4. Use a Random Unassigned HTTP Port
95,3.5. Discover the HTTP Port at Runtime
95,3.6. Enable HTTP Response Compression
95,3.7. Configure SSL
95,3.7.1. Using PEM-encoded files
95,3.8. Configure HTTP/2
95,3.8.1. HTTP/2 With Tomcat
95,3.8.2. HTTP/2 With Jetty
95,3.8.3. HTTP/2 With Reactor Netty
95,3.8.4. HTTP/2 With Undertow
95,3.9. Configure the Web Server
95,"3.10. Add a Servlet, Filter, or Listener to an Application"
95,"3.10.1. Add a Servlet, Filter, or Listener by Using a Spring Bean"
95,Disable Registration of a Servlet or Filter
95,"3.10.2. Add Servlets, Filters, and Listeners by Using Classpath Scanning"
95,3.11. Configure Access Logging
95,3.12. Running Behind a Front-end Proxy Server
95,3.12.1. Customize Tomcat’s Proxy Configuration
95,3.13. Enable Multiple Connectors with Tomcat
95,3.14. Enable Tomcat’s MBean Registry
95,3.15. Enable Multiple Listeners with Undertow
95,3.16. Create WebSocket Endpoints Using @ServerEndpoint
95,4. Spring MVC
95,4.1. Write a JSON REST Service
95,4.2. Write an XML REST Service
95,4.3. Customize the Jackson ObjectMapper
95,4.4. Customize the @ResponseBody Rendering
95,4.5. Handling Multipart File Uploads
95,4.6. Switch Off the Spring MVC DispatcherServlet
95,4.7. Switch off the Default MVC Configuration
95,4.8. Customize ViewResolvers
95,5. Jersey
95,5.1. Secure Jersey endpoints with Spring Security
95,5.2. Use Jersey Alongside Another Web Framework
95,6. HTTP Clients
95,6.1. Configure RestTemplate to Use a Proxy
95,6.2. Configure the TcpClient used by a Reactor Netty-based WebClient
95,7. Logging
95,7.1. Configure Logback for Logging
95,7.1.1. Configure Logback for File-only Output
95,7.2. Configure Log4j for Logging
95,7.2.1. Use YAML or JSON to Configure Log4j 2
95,7.2.2. Use Composite Configuration to Configure Log4j 2
95,8. Data Access
95,8.1. Configure a Custom DataSource
95,8.2. Configure Two DataSources
95,8.3. Use Spring Data Repositories
95,8.4. Separate @Entity Definitions from Spring Configuration
95,8.5. Configure JPA Properties
95,8.6. Configure Hibernate Naming Strategy
95,8.7. Configure Hibernate Second-Level Caching
95,8.8. Use Dependency Injection in Hibernate Components
95,8.9. Use a Custom EntityManagerFactory
95,8.10. Using Multiple EntityManagerFactories
95,8.11. Use a Traditional persistence.xml File
95,8.12. Use Spring Data JPA and Mongo Repositories
95,8.13. Customize Spring Data’s Web Support
95,8.14. Expose Spring Data Repositories as REST Endpoint
95,8.15. Configure a Component that is Used by JPA
95,8.16. Configure jOOQ with Two DataSources
95,9. Database Initialization
95,9.1. Initialize a Database Using JPA
95,9.2. Initialize a Database Using Hibernate
95,9.3. Initialize a Database Using Basic SQL Scripts
95,9.4. Initialize a Spring Batch Database
95,9.5. Use a Higher-level Database Migration Tool
95,9.5.1. Execute Flyway Database Migrations on Startup
95,9.5.2. Execute Liquibase Database Migrations on Startup
95,9.5.3. Use Flyway for test-only migrations
95,9.5.4. Use Liquibase for test-only migrations
95,9.6. Depend Upon an Initialized Database
95,9.6.1. Detect a Database Initializer
95,9.6.2. Detect a Bean That Depends On Database Initialization
95,10. NoSQL
95,10.1. Use Jedis Instead of Lettuce
95,11. Messaging
95,11.1. Disable Transacted JMS Session
95,12. Batch Applications
95,12.1. Specifying a Batch Data Source
95,12.2. Running Spring Batch Jobs on Startup
95,12.3. Running From the Command Line
95,12.4. Restarting a stopped or failed Job
95,12.5. Storing the Job Repository
95,13. Actuator
95,13.1. Change the HTTP Port or Address of the Actuator Endpoints
95,13.2. Customize the ‘whitelabel’ Error Page
95,13.3. Customizing Sanitization
95,13.4. Map Health Indicators to Micrometer Metrics
95,14. Security
95,14.1. Switch off the Spring Boot Security Configuration
95,14.2. Change the UserDetailsService and Add User Accounts
95,14.3. Enable HTTPS When Running behind a Proxy Server
95,15. Hot Swapping
95,15.1. Reload Static Content
95,15.2. Reload Templates without Restarting the Container
95,15.2.1. Thymeleaf Templates
95,15.2.2. FreeMarker Templates
95,15.2.3. Groovy Templates
95,15.3. Fast Application Restarts
95,15.4. Reload Java Classes without Restarting the Container
95,16. Testing
95,16.1. Testing With Spring Security
95,16.2. Structure @Configuration classes for inclusion in slice tests
95,17. Build
95,17.1. Generate Build Information
95,17.2. Generate Git Information
95,17.3. Customize Dependency Versions
95,17.4. Create an Executable JAR with Maven
95,17.5. Use a Spring Boot Application as a Dependency
95,17.6. Extract Specific Libraries When an Executable Jar Runs
95,17.7. Create a Non-executable JAR with Exclusions
95,17.8. Remote Debug a Spring Boot Application Started with Maven
95,17.9. Build an Executable Archive From Ant without Using spring-boot-antlib
95,18. Ahead-of-time processing
95,18.1. Conditions
95,19. Traditional Deployment
95,19.1. Create a Deployable War File
95,19.2. Convert an Existing Application to Spring Boot
95,19.3. Deploying a WAR to WebLogic
95,20. Docker Compose
95,20.1. Customizing the JDBC URL
95,20.2. Sharing services between multiple applications
95,This section provides answers to some common ‘how do I do that…​’ questions that often arise when using Spring Boot.
95,"Its coverage is not exhaustive, but it does cover quite a lot."
95,"If you have a specific problem that we do not cover here, you might want to check stackoverflow.com to see if someone has already provided an answer."
95,This is also a great place to ask new questions (please use the spring-boot tag).
95,We are also more than happy to extend this section.
95,"If you want to add a ‘how-to’, send us a pull request."
95,1. Spring Boot Application
95,This section includes topics relating directly to Spring Boot applications.
95,1.1. Create Your Own FailureAnalyzer
95,"FailureAnalyzer is a great way to intercept an exception on startup and turn it into a human-readable message, wrapped in a FailureAnalysis."
95,"Spring Boot provides such an analyzer for application-context-related exceptions, JSR-303 validations, and more."
95,You can also create your own.
95,AbstractFailureAnalyzer is a convenient extension of FailureAnalyzer that checks the presence of a specified exception type in the exception to handle.
95,You can extend from that so that your implementation gets a chance to handle the exception only when it is actually present.
95,"If, for whatever reason, you cannot handle the exception, return null to give another implementation a chance to handle the exception."
95,FailureAnalyzer implementations must be registered in META-INF/spring.factories.
95,The following example registers ProjectConstraintViolationFailureAnalyzer:
95,org.springframework.boot.diagnostics.FailureAnalyzer=\
95,com.example.ProjectConstraintViolationFailureAnalyzer
95,"If you need access to the BeanFactory or the Environment, declare them as constructor arguments in your FailureAnalyzer implementation."
95,1.2. Troubleshoot Auto-configuration
95,"The Spring Boot auto-configuration tries its best to “do the right thing”, but sometimes things fail, and it can be hard to tell why."
95,There is a really useful ConditionEvaluationReport available in any Spring Boot ApplicationContext.
95,You can see it if you enable DEBUG logging output.
95,"If you use the spring-boot-actuator (see the Actuator chapter), there is also a conditions endpoint that renders the report in JSON."
95,Use that endpoint to debug the application and see what features have been added (and which have not been added) by Spring Boot at runtime.
95,Many more questions can be answered by looking at the source code and the Javadoc.
95,"When reading the code, remember the following rules of thumb:"
95,Look for classes called *AutoConfiguration and read their sources.
95,Pay special attention to the @Conditional* annotations to find out what features they enable and when.
95,Add --debug to the command line or a System property -Ddebug to get a log on the console of all the auto-configuration decisions that were made in your app.
95,"In a running application with actuator enabled, look at the conditions endpoint (/actuator/conditions or the JMX equivalent) for the same information."
95,Look for classes that are @ConfigurationProperties (such as ServerProperties) and read from there the available external configuration options.
95,The @ConfigurationProperties annotation has a name attribute that acts as a prefix to external properties.
95,"Thus, ServerProperties has prefix=""server"" and its configuration properties are server.port, server.address, and others."
95,"In a running application with actuator enabled, look at the configprops endpoint."
95,Look for uses of the bind method on the Binder to pull configuration values explicitly out of the Environment in a relaxed manner.
95,It is often used with a prefix.
95,Look for @Value annotations that bind directly to the Environment.
95,"Look for @ConditionalOnExpression annotations that switch features on and off in response to SpEL expressions, normally evaluated with placeholders resolved from the Environment."
95,1.3. Customize the Environment or ApplicationContext Before It Starts
95,A SpringApplication has ApplicationListeners and ApplicationContextInitializers that are used to apply customizations to the context or environment.
95,Spring Boot loads a number of such customizations for use internally from META-INF/spring.factories.
95,There is more than one way to register additional customizations:
95,"Programmatically, per application, by calling the addListeners and addInitializers methods on SpringApplication before you run it."
95,"Declaratively, for all applications, by adding a META-INF/spring.factories and packaging a jar file that the applications all use as a library."
95,The SpringApplication sends some special ApplicationEvents to the listeners (some even before the context is created) and then registers the listeners for events published by the ApplicationContext as well.
95,See “Application Events and Listeners” in the ‘Spring Boot features’ section for a complete list.
95,It is also possible to customize the Environment before the application context is refreshed by using EnvironmentPostProcessor.
95,"Each implementation should be registered in META-INF/spring.factories, as shown in the following example:"
95,org.springframework.boot.env.EnvironmentPostProcessor=com.example.YourEnvironmentPostProcessor
95,The implementation can load arbitrary files and add them to the Environment.
95,"For instance, the following example loads a YAML configuration file from the classpath:"
95,Java
95,import java.io.IOException;
95,import org.springframework.boot.SpringApplication;
95,import org.springframework.boot.env.EnvironmentPostProcessor;
95,import org.springframework.boot.env.YamlPropertySourceLoader;
95,import org.springframework.core.env.ConfigurableEnvironment;
95,import org.springframework.core.env.PropertySource;
95,import org.springframework.core.io.ClassPathResource;
95,import org.springframework.core.io.Resource;
95,import org.springframework.util.Assert;
95,public class MyEnvironmentPostProcessor implements EnvironmentPostProcessor {
95,private final YamlPropertySourceLoader loader = new YamlPropertySourceLoader();
95,@Override
95,"public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) {"
95,"Resource path = new ClassPathResource(""com/example/myapp/config.yml"");"
95,PropertySource<?> propertySource = loadYaml(path);
95,environment.getPropertySources().addLast(propertySource);
95,private PropertySource<?> loadYaml(Resource path) {
95,"Assert.isTrue(path.exists(), () -> ""Resource "" + path + "" does not exist"");"
95,try {
95,"return this.loader.load(""custom-resource"", path).get(0);"
95,catch (IOException ex) {
95,"throw new IllegalStateException(""Failed to load yaml configuration from "" + path, ex);"
95,Kotlin
95,import org.springframework.boot.SpringApplication
95,import org.springframework.boot.env.EnvironmentPostProcessor
95,import org.springframework.boot.env.YamlPropertySourceLoader
95,import org.springframework.core.env.ConfigurableEnvironment
95,import org.springframework.core.env.PropertySource
95,import org.springframework.core.io.ClassPathResource
95,import org.springframework.core.io.Resource
95,import org.springframework.util.Assert
95,import java.io.IOException
95,class MyEnvironmentPostProcessor : EnvironmentPostProcessor {
95,private val loader = YamlPropertySourceLoader()
95,"override fun postProcessEnvironment(environment: ConfigurableEnvironment, application: SpringApplication) {"
95,"val path: Resource = ClassPathResource(""com/example/myapp/config.yml"")"
95,val propertySource = loadYaml(path)
95,environment.propertySources.addLast(propertySource)
95,private fun loadYaml(path: Resource): PropertySource<*> {
95,"Assert.isTrue(path.exists()) { ""Resource $path does not exist"" }"
95,return try {
95,"loader.load(""custom-resource"", path)[0]"
95,} catch (ex: IOException) {
95,"throw IllegalStateException(""Failed to load yaml configuration from $path"", ex)"
95,The Environment has already been prepared with all the usual property sources that Spring Boot loads by default.
95,It is therefore possible to get the location of the file from the environment.
95,The preceding example adds the custom-resource property source at the end of the list so that a key defined in any of the usual other locations takes precedence.
95,A custom implementation may define another order.
95,"While using @PropertySource on your @SpringBootApplication may seem to be a convenient way to load a custom resource in the Environment, we do not recommend it."
95,Such property sources are not added to the Environment until the application context is being refreshed.
95,This is too late to configure certain properties such as logging.* and spring.main.* which are read before refresh begins.
95,1.4. Build an ApplicationContext Hierarchy (Adding a Parent or Root Context)
95,You can use the ApplicationBuilder class to create parent/child ApplicationContext hierarchies.
95,See “features.html” in the ‘Spring Boot features’ section for more information.
95,1.5. Create a Non-web Application
95,Not all Spring applications have to be web applications (or web services).
95,"If you want to execute some code in a main method but also bootstrap a Spring application to set up the infrastructure to use, you can use the SpringApplication features of Spring Boot."
95,"A SpringApplication changes its ApplicationContext class, depending on whether it thinks it needs a web application or not."
95,The first thing you can do to help it is to leave server-related dependencies (such as the servlet API) off the classpath.
95,"If you cannot do that (for example, you run two applications from the same code base) then you can explicitly call setWebApplicationType(WebApplicationType.NONE) on your SpringApplication instance or set the applicationContextClass property (through the Java API or with external properties)."
95,Application code that you want to run as your business logic can be implemented as a CommandLineRunner and dropped into the context as a @Bean definition.
95,2. Properties and Configuration
95,This section includes topics about setting and reading properties and configuration settings and their interaction with Spring Boot applications.
95,2.1. Automatically Expand Properties at Build Time
95,"Rather than hardcoding some properties that are also specified in your project’s build configuration, you can automatically expand them by instead using the existing build configuration."
95,This is possible in both Maven and Gradle.
95,2.1.1. Automatic Property Expansion Using Maven
95,You can automatically expand properties from the Maven project by using resource filtering.
95,"If you use the spring-boot-starter-parent, you can then refer to your Maven ‘project properties’ with @..@ placeholders, as shown in the following example:"
95,Properties
95,[email protected]@
95,[email protected]@
95,Yaml
95,app:
95,"encoding: ""@project.build.sourceEncoding@"""
95,java:
95,"version: ""@java.version@"""
95,"Only production configuration is filtered that way (in other words, no filtering is applied on src/test/resources)."
95,"If you enable the addResources flag, the spring-boot:run goal can add src/main/resources directly to the classpath (for hot reloading purposes)."
95,Doing so circumvents the resource filtering and this feature.
95,"Instead, you can use the exec:java goal or customize the plugin’s configuration."
95,See the plugin usage page for more details.
95,"If you do not use the starter parent, you need to include the following"
95,element inside the <build/> element of your pom.xml:
95,<resources>
95,<resource>
95,<directory>src/main/resources</directory>
95,<filtering>true</filtering>
95,</resource>
95,</resources>
95,You also need to include the following element inside <plugins/>:
95,<plugin>
95,<groupId>org.apache.maven.plugins</groupId>
95,<artifactId>maven-resources-plugin</artifactId>
95,<version>2.7</version>
95,<configuration>
95,<delimiters>
95,<delimiter>@</delimiter>
95,</delimiters>
95,<useDefaultDelimiters>false</useDefaultDelimiters>
95,</configuration>
95,</plugin>
95,The useDefaultDelimiters property is important if you use standard Spring placeholders (such as ${placeholder}) in your configuration.
95,"If that property is not set to false, these may be expanded by the build."
95,2.1.2. Automatic Property Expansion Using Gradle
95,"You can automatically expand properties from the Gradle project by configuring the Java plugin’s processResources task to do so, as shown in the following example:"
95,tasks.named('processResources') {
95,expand(project.properties)
95,"You can then refer to your Gradle project’s properties by using placeholders, as shown in the following example:"
95,Properties
95,app.name=${name}
95,app.description=${description}
95,Yaml
95,app:
95,"name: ""${name}"""
95,"description: ""${description}"""
95,"Gradle’s expand method uses Groovy’s SimpleTemplateEngine, which transforms ${..} tokens."
95,The ${..} style conflicts with Spring’s own property placeholder mechanism.
95,"To use Spring property placeholders together with automatic expansion, escape the Spring property placeholders as follows: \${..}."
95,2.2. Externalize the Configuration of SpringApplication
95,"A SpringApplication has bean property setters, so you can use its Java API as you create the application to modify its behavior."
95,"Alternatively, you can externalize the configuration by setting properties in spring.main.*."
95,"For example, in application.properties, you might have the following settings:"
95,Properties
95,spring.main.web-application-type=none
95,spring.main.banner-mode=off
95,Yaml
95,spring:
95,main:
95,"web-application-type: ""none"""
95,"banner-mode: ""off"""
95,"Then the Spring Boot banner is not printed on startup, and the application is not starting an embedded web server."
95,"Properties defined in external configuration override and replace the values specified with the Java API, with the notable exception of the primary sources."
95,Primary sources are those provided to the SpringApplication constructor:
95,Java
95,import org.springframework.boot.Banner;
95,import org.springframework.boot.SpringApplication;
95,import org.springframework.boot.autoconfigure.SpringBootApplication;
95,@SpringBootApplication
95,public class MyApplication {
95,public static void main(String[] args) {
95,SpringApplication application = new SpringApplication(MyApplication.class);
95,application.setBannerMode(Banner.Mode.OFF);
95,application.run(args);
95,Kotlin
95,import org.springframework.boot.Banner
95,import org.springframework.boot.SpringApplication
95,import org.springframework.boot.autoconfigure.SpringBootApplication
95,@SpringBootApplication
95,object MyApplication {
95,@JvmStatic
95,fun main(args: Array<String>) {
95,val application = SpringApplication(MyApplication::class.java)
95,application.setBannerMode(Banner.Mode.OFF)
95,application.run(*args)
95,Or to sources(…​) method of a SpringApplicationBuilder:
95,Java
95,import org.springframework.boot.Banner;
95,import org.springframework.boot.builder.SpringApplicationBuilder;
95,public class MyApplication {
95,public static void main(String[] args) {
95,new SpringApplicationBuilder()
95,.bannerMode(Banner.Mode.OFF)
95,.sources(MyApplication.class)
95,.run(args);
95,Kotlin
95,import org.springframework.boot.Banner
95,import org.springframework.boot.builder.SpringApplicationBuilder
95,object MyApplication {
95,@JvmStatic
95,fun main(args: Array<String>) {
95,SpringApplicationBuilder()
95,.bannerMode(Banner.Mode.OFF)
95,.sources(MyApplication::class.java)
95,.run(*args)
95,"Given the examples above, if we have the following configuration:"
95,Properties
95,"spring.main.sources=com.example.MyDatabaseConfig,com.example.MyJmsConfig"
95,spring.main.banner-mode=console
95,Yaml
95,spring:
95,main:
95,"sources: ""com.example.MyDatabaseConfig,com.example.MyJmsConfig"""
95,"banner-mode: ""console"""
95,The actual application will show the banner (as overridden by configuration) and uses three sources for the ApplicationContext.
95,The application sources are:
95,MyApplication (from the code)
95,MyDatabaseConfig (from the external config)
95,MyJmsConfig(from the external config)
95,2.3. Change the Location of External Properties of an Application
95,"By default, properties from different sources are added to the Spring Environment in a defined order (see “features.html” in the ‘Spring Boot features’ section for the exact order)."
95,You can also provide the following System properties (or environment variables) to change the behavior:
95,spring.config.name (SPRING_CONFIG_NAME): Defaults to application as the root of the file name.
95,spring.config.location (SPRING_CONFIG_LOCATION): The file to load (such as a classpath resource or a URL).
95,"A separate Environment property source is set up for this document and it can be overridden by system properties, environment variables, or the command line."
95,"No matter what you set in the environment, Spring Boot always loads application.properties as described above."
95,"By default, if YAML is used, then files with the ‘.yaml’ and ‘.yml’ extension are also added to the list."
95,If you want detailed information about the files that are being loaded you can set the logging level of org.springframework.boot.context.config to trace.
95,2.4. Use ‘Short’ Command Line Arguments
95,Some people like to use (for example) --port=9000 instead of --server.port=9000 to set configuration properties on the command line.
95,"You can enable this behavior by using placeholders in application.properties, as shown in the following example:"
95,Properties
95,server.port=${port:8080}
95,Yaml
95,server:
95,"port: ""${port:8080}"""
95,"If you inherit from the spring-boot-starter-parent POM, the default filter token of the maven-resources-plugins has been changed from ${*} to @ (that is, @maven.token@ instead of ${maven.token}) to prevent conflicts with Spring-style placeholders."
95,"If you have enabled Maven filtering for the application.properties directly, you may want to also change the default filter token to use other delimiters."
95,"In this specific case, the port binding works in a PaaS environment such as Heroku or Cloud Foundry."
95,"In those two platforms, the PORT environment variable is set automatically and Spring can bind to capitalized synonyms for Environment properties."
95,2.5. Use YAML for External Properties
95,"YAML is a superset of JSON and, as such, is a convenient syntax for storing external properties in a hierarchical format, as shown in the following example:"
95,spring:
95,application:
95,"name: ""cruncher"""
95,datasource:
95,"driver-class-name: ""com.mysql.jdbc.Driver"""
95,"url: ""jdbc:mysql://localhost/test"""
95,server:
95,port: 9000
95,Create a file called application.yaml and put it in the root of your classpath.
95,"Then add snakeyaml to your dependencies (Maven coordinates org.yaml:snakeyaml, already included if you use the spring-boot-starter)."
95,"A YAML file is parsed to a Java Map<String,Object> (like a JSON object), and Spring Boot flattens the map so that it is one level deep and has period-separated keys, as many people are used to with Properties files in Java."
95,The preceding example YAML corresponds to the following application.properties file:
95,spring.application.name=cruncher
95,spring.datasource.driver-class-name=com.mysql.jdbc.Driver
95,spring.datasource.url=jdbc:mysql://localhost/test
95,server.port=9000
95,See “features.html” in the ‘Spring Boot features’ section for more information about YAML.
95,2.6. Set the Active Spring Profiles
95,"The Spring Environment has an API for this, but you would normally set a System property (spring.profiles.active) or an OS environment variable (SPRING_PROFILES_ACTIVE)."
95,"Also, you can launch your application with a -D argument (remember to put it before the main class or jar archive), as follows:"
95,$ java -jar -Dspring.profiles.active=production demo-0.0.1-SNAPSHOT.jar
95,"In Spring Boot, you can also set the active profile in application.properties, as shown in the following example:"
95,Properties
95,spring.profiles.active=production
95,Yaml
95,spring:
95,profiles:
95,"active: ""production"""
95,A value set this way is replaced by the System property or environment variable setting but not by the SpringApplicationBuilder.profiles() method.
95,"Thus, the latter Java API can be used to augment the profiles without changing the defaults."
95,See “features.html” in the “Spring Boot features” section for more information.
95,2.7. Set the Default Profile Name
95,The default profile is a profile that is enabled if no profile is active.
95,"By default, the name of the default profile is default, but it could be changed using a System property (spring.profiles.default) or an OS environment variable (SPRING_PROFILES_DEFAULT)."
95,"In Spring Boot, you can also set the default profile name in application.properties, as shown in the following example:"
95,Properties
95,spring.profiles.default=dev
95,Yaml
95,spring:
95,profiles:
95,"default: ""dev"""
95,See “features.html” in the “Spring Boot features” section for more information.
95,2.8. Change Configuration Depending on the Environment
95,Spring Boot supports multi-document YAML and Properties files (see features.html for details) which can be activated conditionally based on the active profiles.
95,"If a document contains a spring.config.activate.on-profile key, then the profiles value (a comma-separated list of profiles or a profile expression) is fed into the Spring Environment.acceptsProfiles() method."
95,"If the profile expression matches then that document is included in the final merge (otherwise, it is not), as shown in the following example:"
95,Properties
95,server.port=9000
95,#---
95,spring.config.activate.on-profile=development
95,server.port=9001
95,#---
95,spring.config.activate.on-profile=production
95,server.port=0
95,Yaml
95,server:
95,port: 9000
95,---
95,spring:
95,config:
95,activate:
95,"on-profile: ""development"""
95,server:
95,port: 9001
95,---
95,spring:
95,config:
95,activate:
95,"on-profile: ""production"""
95,server:
95,port: 0
95,"In the preceding example, the default port is 9000."
95,"However, if the Spring profile called ‘development’ is active, then the port is 9001."
95,"If ‘production’ is active, then the port is 0."
95,The documents are merged in the order in which they are encountered.
95,Later values override earlier values.
95,2.9. Discover Built-in Options for External Properties
95,Spring Boot binds external properties from application.properties (or YAML files and other places) into an application at runtime.
95,"There is not (and technically cannot be) an exhaustive list of all supported properties in a single location, because contributions can come from additional jar files on your classpath."
95,A running application with the Actuator features has a configprops endpoint that shows all the bound and bindable properties available through @ConfigurationProperties.
95,The appendix includes an application.properties example with a list of the most common properties supported by Spring Boot.
95,The definitive list comes from searching the source code for @ConfigurationProperties and @Value annotations as well as the occasional use of Binder.
95,"For more about the exact ordering of loading properties, see ""features.html""."
95,3. Embedded Web Servers
95,Each Spring Boot web application includes an embedded web server.
95,"This feature leads to a number of how-to questions, including how to change the embedded server and how to configure the embedded server."
95,This section answers those questions.
95,3.1. Use Another Web Server
95,Many Spring Boot starters include default embedded containers.
95,"For servlet stack applications, the spring-boot-starter-web includes Tomcat by including spring-boot-starter-tomcat, but you can use spring-boot-starter-jetty or spring-boot-starter-undertow instead."
95,"For reactive stack applications, the spring-boot-starter-webflux includes"
95,"Reactor Netty by including spring-boot-starter-reactor-netty, but you can use spring-boot-starter-tomcat, spring-boot-starter-jetty, or spring-boot-starter-undertow instead."
95,"When switching to a different HTTP server, you need to swap the default dependencies for those that you need instead."
95,"To help with this process, Spring Boot provides a separate starter for each of the supported HTTP servers."
95,The following Maven example shows how to exclude Tomcat and include Jetty for Spring MVC:
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-web</artifactId>
95,<exclusions>
95,<!-- Exclude the Tomcat dependency -->
95,<exclusion>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-tomcat</artifactId>
95,</exclusion>
95,</exclusions>
95,</dependency>
95,<!-- Use Jetty instead -->
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-jetty</artifactId>
95,</dependency>
95,The following Gradle example configures the necessary dependencies and a module replacement to use Undertow in place of Reactor Netty for Spring WebFlux:
95,dependencies {
95,"implementation ""org.springframework.boot:spring-boot-starter-undertow"""
95,"implementation ""org.springframework.boot:spring-boot-starter-webflux"""
95,modules {
95,"module(""org.springframework.boot:spring-boot-starter-reactor-netty"") {"
95,"replacedBy(""org.springframework.boot:spring-boot-starter-undertow"", ""Use Undertow instead of Reactor Netty"")"
95,"spring-boot-starter-reactor-netty is required to use the WebClient class, so you may need to keep a dependency on Netty even when you need to include a different HTTP server."
95,3.2. Disabling the Web Server
95,"If your classpath contains the necessary bits to start a web server, Spring Boot will automatically start it."
95,"To disable this behavior configure the WebApplicationType in your application.properties, as shown in the following example:"
95,Properties
95,spring.main.web-application-type=none
95,Yaml
95,spring:
95,main:
95,"web-application-type: ""none"""
95,3.3. Change the HTTP Port
95,"In a standalone application, the main HTTP port defaults to 8080 but can be set with server.port (for example, in application.properties or as a System property)."
95,"Thanks to relaxed binding of Environment values, you can also use SERVER_PORT (for example, as an OS environment variable)."
95,"To switch off the HTTP endpoints completely but still create a WebApplicationContext, use server.port=-1 (doing so is sometimes useful for testing)."
95,"For more details, see “web.html” in the ‘Spring Boot Features’ section, or the ServerProperties source code."
95,3.4. Use a Random Unassigned HTTP Port
95,To scan for a free port (using OS natives to prevent clashes) use server.port=0.
95,3.5. Discover the HTTP Port at Runtime
95,You can access the port the server is running on from log output or from the WebServerApplicationContext through its WebServer.
95,The best way to get that and be sure it has been initialized is to add a @Bean of type ApplicationListener<WebServerInitializedEvent> and pull the container out of the event when it is published.
95,"Tests that use @SpringBootTest(webEnvironment=WebEnvironment.RANDOM_PORT) can also inject the actual port into a field by using the @LocalServerPort annotation, as shown in the following example:"
95,Java
95,import org.springframework.boot.test.context.SpringBootTest;
95,import org.springframework.boot.test.context.SpringBootTest.WebEnvironment;
95,import org.springframework.boot.test.web.server.LocalServerPort;
95,@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT)
95,class MyWebIntegrationTests {
95,@LocalServerPort
95,int port;
95,// ...
95,Kotlin
95,import org.springframework.boot.test.context.SpringBootTest
95,import org.springframework.boot.test.context.SpringBootTest.WebEnvironment
95,import org.springframework.boot.test.web.server.LocalServerPort
95,@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT)
95,class MyWebIntegrationTests {
95,@LocalServerPort
95,var port = 0
95,// ...
95,"@LocalServerPort is a meta-annotation for @Value(""${local.server.port}"")."
95,Do not try to inject the port in a regular application.
95,"As we just saw, the value is set only after the container has been initialized."
95,"Contrary to a test, application code callbacks are processed early (before the value is actually available)."
95,3.6. Enable HTTP Response Compression
95,"HTTP response compression is supported by Jetty, Tomcat, Reactor Netty, and Undertow."
95,"It can be enabled in application.properties, as follows:"
95,Properties
95,server.compression.enabled=true
95,Yaml
95,server:
95,compression:
95,enabled: true
95,"By default, responses must be at least 2048 bytes in length for compression to be performed."
95,You can configure this behavior by setting the server.compression.min-response-size property.
95,"By default, responses are compressed only if their content type is one of the following:"
95,text/html
95,text/xml
95,text/plain
95,text/css
95,text/javascript
95,application/javascript
95,application/json
95,application/xml
95,You can configure this behavior by setting the server.compression.mime-types property.
95,3.7. Configure SSL
95,"SSL can be configured declaratively by setting the various server.ssl.* properties, typically in application.properties or application.yaml."
95,The following example shows setting SSL properties using a Java KeyStore file:
95,Properties
95,server.port=8443
95,server.ssl.key-store=classpath:keystore.jks
95,server.ssl.key-store-password=secret
95,server.ssl.key-password=another-secret
95,Yaml
95,server:
95,port: 8443
95,ssl:
95,"key-store: ""classpath:keystore.jks"""
95,"key-store-password: ""secret"""
95,"key-password: ""another-secret"""
95,Using configuration such as the preceding example means the application no longer supports a plain HTTP connector at port 8080.
95,Spring Boot does not support the configuration of both an HTTP connector and an HTTPS connector through application.properties.
95,"If you want to have both, you need to configure one of them programmatically."
95,"We recommend using application.properties to configure HTTPS, as the HTTP connector is the easier of the two to configure programmatically."
95,3.7.1. Using PEM-encoded files
95,You can use PEM-encoded files instead of Java KeyStore files.
95,You should use PKCS#8 key files wherever possible.
95,PEM-encoded PKCS#8 key files start with a -----BEGIN PRIVATE KEY----- or -----BEGIN ENCRYPTED PRIVATE KEY----- header.
95,"If you have files in other formats, e.g., PKCS#1 (-----BEGIN RSA PRIVATE KEY-----) or SEC 1 (-----BEGIN EC PRIVATE KEY-----), you can convert them to PKCS#8 using OpenSSL:"
95,openssl pkcs8 -topk8 -nocrypt -in <input file> -out <output file>
95,The following example shows setting SSL properties using PEM-encoded certificate and private key files:
95,Properties
95,server.port=8443
95,server.ssl.certificate=classpath:my-cert.crt
95,server.ssl.certificate-private-key=classpath:my-cert.key
95,server.ssl.trust-certificate=classpath:ca-cert.crt
95,Yaml
95,server:
95,port: 8443
95,ssl:
95,"certificate: ""classpath:my-cert.crt"""
95,"certificate-private-key: ""classpath:my-cert.key"""
95,"trust-certificate: ""classpath:ca-cert.crt"""
95,"Alternatively, the SSL trust material can be configured in an SSL bundle and applied to the web server as shown in this example:"
95,Properties
95,server.port=8443
95,server.ssl.bundle=example
95,Yaml
95,server:
95,port: 8443
95,ssl:
95,"bundle: ""example"""
95,The server.ssl.bundle property can not be combined with the discrete Java KeyStore or PEM property options under server.ssl.
95,See Ssl for details of all of the supported properties.
95,3.8. Configure HTTP/2
95,You can enable HTTP/2 support in your Spring Boot application with the server.http2.enabled configuration property.
95,Both h2 (HTTP/2 over TLS) and h2c (HTTP/2 over TCP) are supported.
95,"To use h2, SSL must also be enabled."
95,"When SSL is not enabled, h2c will be used."
95,"You may, for example, want to use h2c when your application is running behind a proxy server that is performing TLS termination."
95,3.8.1. HTTP/2 With Tomcat
95,Spring Boot ships by default with Tomcat 10.1.x which supports h2c and h2 out of the box.
95,"Alternatively, you can use libtcnative for h2 support if the library and its dependencies are installed on the host operating system."
95,"The library directory must be made available, if not already, to the JVM library path."
95,You can do so with a JVM argument such as -Djava.library.path=/usr/local/opt/tomcat-native/lib.
95,More on this in the official Tomcat documentation.
95,3.8.2. HTTP/2 With Jetty
95,"For HTTP/2 support, Jetty requires the additional org.eclipse.jetty.http2:jetty-http2-server dependency."
95,To use h2c no other dependencies are required.
95,"To use h2, you also need to choose one of the following dependencies, depending on your deployment:"
95,org.eclipse.jetty:jetty-alpn-java-server to use the JDK built-in support
95,org.eclipse.jetty:jetty-alpn-conscrypt-server and the Conscrypt library
95,3.8.3. HTTP/2 With Reactor Netty
95,The spring-boot-webflux-starter is using by default Reactor Netty as a server.
95,Reactor Netty supports h2c and h2 out of the box.
95,"For optimal runtime performance, this server also supports h2 with native libraries."
95,"To enable that, your application needs to have an additional dependency."
95,"Spring Boot manages the version for the io.netty:netty-tcnative-boringssl-static ""uber jar"", containing native libraries for all platforms."
95,Developers can choose to import only the required dependencies using a classifier (see the Netty official documentation).
95,3.8.4. HTTP/2 With Undertow
95,Undertow supports h2c and h2 out of the box.
95,3.9. Configure the Web Server
95,"Generally, you should first consider using one of the many available configuration keys and customize your web server by adding new entries in your application.properties or application.yaml file."
95,See “Discover Built-in Options for External Properties”).
95,"The server.* namespace is quite useful here, and it includes namespaces like server.tomcat.*, server.jetty.* and others, for server-specific features."
95,See the list of application-properties.html.
95,"The previous sections covered already many common use cases, such as compression, SSL or HTTP/2."
95,"However, if a configuration key does not exist for your use case, you should then look at WebServerFactoryCustomizer."
95,"You can declare such a component and get access to the server factory relevant to your choice: you should select the variant for the chosen Server (Tomcat, Jetty, Reactor Netty, Undertow) and the chosen web stack (servlet or reactive)."
95,The example below is for Tomcat with the spring-boot-starter-web (servlet stack):
95,Java
95,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
95,import org.springframework.stereotype.Component;
95,@Component
95,public class MyTomcatWebServerCustomizer implements WebServerFactoryCustomizer<TomcatServletWebServerFactory> {
95,@Override
95,public void customize(TomcatServletWebServerFactory factory) {
95,// customize the factory here
95,Kotlin
95,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer
95,import org.springframework.stereotype.Component
95,@Component
95,class MyTomcatWebServerCustomizer : WebServerFactoryCustomizer<TomcatServletWebServerFactory?> {
95,override fun customize(factory: TomcatServletWebServerFactory?) {
95,// customize the factory here
95,Spring Boot uses that infrastructure internally to auto-configure the server.
95,"Auto-configured WebServerFactoryCustomizer beans have an order of 0 and will be processed before any user-defined customizers, unless it has an explicit order that states otherwise."
95,"Once you have got access to a WebServerFactory using the customizer, you can use it to configure specific parts, like connectors, server resources, or the server itself - all using server-specific APIs."
95,In addition Spring Boot provides:
95,Server
95,Servlet stack
95,Reactive stack
95,Tomcat
95,TomcatServletWebServerFactory
95,TomcatReactiveWebServerFactory
95,Jetty
95,JettyServletWebServerFactory
95,JettyReactiveWebServerFactory
95,Undertow
95,UndertowServletWebServerFactory
95,UndertowReactiveWebServerFactory
95,Reactor
95,N/A
95,NettyReactiveWebServerFactory
95,"As a last resort, you can also declare your own WebServerFactory bean, which will override the one provided by Spring Boot."
95,"When you do so, auto-configured customizers are still applied on your custom factory, so use that option carefully."
95,"3.10. Add a Servlet, Filter, or Listener to an Application"
95,"In a servlet stack application, that is with the spring-boot-starter-web, there are two ways to add Servlet, Filter, ServletContextListener, and the other listeners supported by the Servlet API to your application:"
95,"Add a Servlet, Filter, or Listener by Using a Spring Bean"
95,"Add Servlets, Filters, and Listeners by Using Classpath Scanning"
95,"3.10.1. Add a Servlet, Filter, or Listener by Using a Spring Bean"
95,"To add a Servlet, Filter, or servlet *Listener by using a Spring bean, you must provide a @Bean definition for it."
95,Doing so can be very useful when you want to inject configuration or dependencies.
95,"However, you must be very careful that they do not cause eager initialization of too many other beans, because they have to be installed in the container very early in the application lifecycle."
95,"(For example, it is not a good idea to have them depend on your DataSource or JPA configuration.)"
95,You can work around such restrictions by initializing the beans lazily when first used instead of on initialization.
95,"In the case of filters and servlets, you can also add mappings and init parameters by adding a FilterRegistrationBean or a ServletRegistrationBean instead of or in addition to the underlying component."
95,"If no dispatcherType is specified on a filter registration, REQUEST is used."
95,This aligns with the servlet specification’s default dispatcher type.
95,"Like any other Spring bean, you can define the order of servlet filter beans; please make sure to check the “web.html” section."
95,Disable Registration of a Servlet or Filter
95,"As described earlier, any Servlet or Filter beans are registered with the servlet container automatically."
95,"To disable registration of a particular Filter or Servlet bean, create a registration bean for it and mark it as disabled, as shown in the following example:"
95,Java
95,import org.springframework.boot.web.servlet.FilterRegistrationBean;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyFilterConfiguration {
95,@Bean
95,public FilterRegistrationBean<MyFilter> registration(MyFilter filter) {
95,FilterRegistrationBean<MyFilter> registration = new FilterRegistrationBean<>(filter);
95,registration.setEnabled(false);
95,return registration;
95,Kotlin
95,import org.springframework.boot.web.servlet.FilterRegistrationBean
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyFilterConfiguration {
95,@Bean
95,fun registration(filter: MyFilter): FilterRegistrationBean<MyFilter> {
95,val registration = FilterRegistrationBean(filter)
95,registration.isEnabled = false
95,return registration
95,"3.10.2. Add Servlets, Filters, and Listeners by Using Classpath Scanning"
95,"@WebServlet, @WebFilter, and @WebListener annotated classes can be automatically registered with an embedded servlet container by annotating a @Configuration class with @ServletComponentScan and specifying the package(s) containing the components that you want to register."
95,"By default, @ServletComponentScan scans from the package of the annotated class."
95,3.11. Configure Access Logging
95,"Access logs can be configured for Tomcat, Undertow, and Jetty through their respective namespaces."
95,"For instance, the following settings log access on Tomcat with a custom pattern."
95,Properties
95,server.tomcat.basedir=my-tomcat
95,server.tomcat.accesslog.enabled=true
95,server.tomcat.accesslog.pattern=%t %a %r %s (%D microseconds)
95,Yaml
95,server:
95,tomcat:
95,"basedir: ""my-tomcat"""
95,accesslog:
95,enabled: true
95,"pattern: ""%t %a %r %s (%D microseconds)"""
95,The default location for logs is a logs directory relative to the Tomcat base directory.
95,"By default, the logs directory is a temporary directory, so you may want to fix Tomcat’s base directory or use an absolute path for the logs."
95,"In the preceding example, the logs are available in my-tomcat/logs relative to the working directory of the application."
95,"Access logging for Undertow can be configured in a similar fashion, as shown in the following example:"
95,Properties
95,server.undertow.accesslog.enabled=true
95,server.undertow.accesslog.pattern=%t %a %r %s (%D milliseconds)
95,server.undertow.options.server.record-request-start-time=true
95,Yaml
95,server:
95,undertow:
95,accesslog:
95,enabled: true
95,"pattern: ""%t %a %r %s (%D milliseconds)"""
95,options:
95,server:
95,record-request-start-time: true
95,"Note that, in addition to enabling access logging and configuring its pattern, recording request start times has also been enabled."
95,This is required when including the response time (%D) in the access log pattern.
95,Logs are stored in a logs directory relative to the working directory of the application.
95,You can customize this location by setting the server.undertow.accesslog.dir property.
95,"Finally, access logging for Jetty can also be configured as follows:"
95,Properties
95,server.jetty.accesslog.enabled=true
95,server.jetty.accesslog.filename=/var/log/jetty-access.log
95,Yaml
95,server:
95,jetty:
95,accesslog:
95,enabled: true
95,"filename: ""/var/log/jetty-access.log"""
95,"By default, logs are redirected to System.err."
95,"For more details, see the Jetty documentation."
95,3.12. Running Behind a Front-end Proxy Server
95,"If your application is running behind a proxy, a load-balancer or in the cloud, the request information (like the host, port, scheme…​) might change along the way."
95,"Your application may be running on 10.10.10.10:8080, but HTTP clients should only see example.org."
95,"RFC7239 ""Forwarded Headers"" defines the Forwarded HTTP header; proxies can use this header to provide information about the original request."
95,"You can configure your application to read those headers and automatically use that information when creating links and sending them to clients in HTTP 302 responses, JSON documents or HTML pages."
95,"There are also non-standard headers, like X-Forwarded-Host, X-Forwarded-Port, X-Forwarded-Proto, X-Forwarded-Ssl, and X-Forwarded-Prefix."
95,"If the proxy adds the commonly used X-Forwarded-For and X-Forwarded-Proto headers, setting server.forward-headers-strategy to NATIVE is enough to support those."
95,"With this option, the Web servers themselves natively support this feature; you can check their specific documentation to learn about specific behavior."
95,"If this is not enough, Spring Framework provides a ForwardedHeaderFilter for the servlet stack and a ForwardedHeaderTransformer for the reactive stack."
95,You can use them in your application by setting server.forward-headers-strategy to FRAMEWORK.
95,"If you are using Tomcat and terminating SSL at the proxy, server.tomcat.redirect-context-root should be set to false."
95,This allows the X-Forwarded-Proto header to be honored before any redirects are performed.
95,"If your application runs in Cloud Foundry, Heroku or Kubernetes, the server.forward-headers-strategy property defaults to NATIVE."
95,"In all other instances, it defaults to NONE."
95,3.12.1. Customize Tomcat’s Proxy Configuration
95,"If you use Tomcat, you can additionally configure the names of the headers used to carry “forwarded” information, as shown in the following example:"
95,Properties
95,server.tomcat.remoteip.remote-ip-header=x-your-remote-ip-header
95,server.tomcat.remoteip.protocol-header=x-your-protocol-header
95,Yaml
95,server:
95,tomcat:
95,remoteip:
95,"remote-ip-header: ""x-your-remote-ip-header"""
95,"protocol-header: ""x-your-protocol-header"""
95,Tomcat is also configured with a regular expression that matches internal proxies that are to be trusted.
95,See the server.tomcat.remoteip.internal-proxies entry in the appendix for its default value.
95,"You can customize the valve’s configuration by adding an entry to application.properties, as shown in the following example:"
95,Properties
95,"server.tomcat.remoteip.internal-proxies=192\\.168\\.\\d{1,3}\\.\\d{1,3}"
95,Yaml
95,server:
95,tomcat:
95,remoteip:
95,"internal-proxies: ""192\\.168\\.\\d{1,3}\\.\\d{1,3}"""
95,You can trust all proxies by setting the internal-proxies to empty (but do not do so in production).
95,"You can take complete control of the configuration of Tomcat’s RemoteIpValve by switching the automatic one off (to do so, set server.forward-headers-strategy=NONE) and adding a new valve instance using a WebServerFactoryCustomizer bean."
95,3.13. Enable Multiple Connectors with Tomcat
95,"You can add an org.apache.catalina.connector.Connector to the TomcatServletWebServerFactory, which can allow multiple connectors, including HTTP and HTTPS connectors, as shown in the following example:"
95,Java
95,import org.apache.catalina.connector.Connector;
95,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyTomcatConfiguration {
95,@Bean
95,public WebServerFactoryCustomizer<TomcatServletWebServerFactory> connectorCustomizer() {
95,return (tomcat) -> tomcat.addAdditionalTomcatConnectors(createConnector());
95,private Connector createConnector() {
95,"Connector connector = new Connector(""org.apache.coyote.http11.Http11NioProtocol"");"
95,connector.setPort(8081);
95,return connector;
95,Kotlin
95,import org.apache.catalina.connector.Connector
95,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyTomcatConfiguration {
95,@Bean
95,fun connectorCustomizer(): WebServerFactoryCustomizer<TomcatServletWebServerFactory> {
95,return WebServerFactoryCustomizer { tomcat: TomcatServletWebServerFactory ->
95,tomcat.addAdditionalTomcatConnectors(
95,createConnector()
95,private fun createConnector(): Connector {
95,"val connector = Connector(""org.apache.coyote.http11.Http11NioProtocol"")"
95,connector.port = 8081
95,return connector
95,3.14. Enable Tomcat’s MBean Registry
95,Embedded Tomcat’s MBean registry is disabled by default.
95,This minimizes Tomcat’s memory footprint.
95,"If you want to use Tomcat’s MBeans, for example so that they can be used by Micrometer to expose metrics, you must use the server.tomcat.mbeanregistry.enabled property to do so, as shown in the following example:"
95,Properties
95,server.tomcat.mbeanregistry.enabled=true
95,Yaml
95,server:
95,tomcat:
95,mbeanregistry:
95,enabled: true
95,3.15. Enable Multiple Listeners with Undertow
95,"Add an UndertowBuilderCustomizer to the UndertowServletWebServerFactory and add a listener to the Builder, as shown in the following example:"
95,Java
95,import io.undertow.Undertow.Builder;
95,import org.springframework.boot.web.embedded.undertow.UndertowServletWebServerFactory;
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyUndertowConfiguration {
95,@Bean
95,public WebServerFactoryCustomizer<UndertowServletWebServerFactory> undertowListenerCustomizer() {
95,return (factory) -> factory.addBuilderCustomizers(this::addHttpListener);
95,private Builder addHttpListener(Builder builder) {
95,"return builder.addHttpListener(8080, ""0.0.0.0"");"
95,Kotlin
95,import io.undertow.Undertow
95,import org.springframework.boot.web.embedded.undertow.UndertowBuilderCustomizer
95,import org.springframework.boot.web.embedded.undertow.UndertowServletWebServerFactory
95,import org.springframework.boot.web.server.WebServerFactoryCustomizer
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyUndertowConfiguration {
95,@Bean
95,fun undertowListenerCustomizer(): WebServerFactoryCustomizer<UndertowServletWebServerFactory> {
95,return WebServerFactoryCustomizer { factory: UndertowServletWebServerFactory ->
95,factory.addBuilderCustomizers(
95,UndertowBuilderCustomizer { builder: Undertow.Builder -> addHttpListener(builder) })
95,private fun addHttpListener(builder: Undertow.Builder): Undertow.Builder {
95,"return builder.addHttpListener(8080, ""0.0.0.0"")"
95,3.16. Create WebSocket Endpoints Using @ServerEndpoint
95,"If you want to use @ServerEndpoint in a Spring Boot application that used an embedded container, you must declare a single ServerEndpointExporter @Bean, as shown in the following example:"
95,Java
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.web.socket.server.standard.ServerEndpointExporter;
95,@Configuration(proxyBeanMethods = false)
95,public class MyWebSocketConfiguration {
95,@Bean
95,public ServerEndpointExporter serverEndpointExporter() {
95,return new ServerEndpointExporter();
95,Kotlin
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.web.socket.server.standard.ServerEndpointExporter
95,@Configuration(proxyBeanMethods = false)
95,class MyWebSocketConfiguration {
95,@Bean
95,fun serverEndpointExporter(): ServerEndpointExporter {
95,return ServerEndpointExporter()
95,The bean shown in the preceding example registers any @ServerEndpoint annotated beans with the underlying WebSocket container.
95,"When deployed to a standalone servlet container, this role is performed by a servlet container initializer, and the ServerEndpointExporter bean is not required."
95,4. Spring MVC
95,Spring Boot has a number of starters that include Spring MVC.
95,Note that some starters include a dependency on Spring MVC rather than include it directly.
95,This section answers common questions about Spring MVC and Spring Boot.
95,4.1. Write a JSON REST Service
95,"Any Spring @RestController in a Spring Boot application should render JSON response by default as long as Jackson2 is on the classpath, as shown in the following example:"
95,Java
95,import org.springframework.web.bind.annotation.RequestMapping;
95,import org.springframework.web.bind.annotation.RestController;
95,@RestController
95,public class MyController {
95,"@RequestMapping(""/thing"")"
95,public MyThing thing() {
95,return new MyThing();
95,Kotlin
95,import org.springframework.web.bind.annotation.RequestMapping
95,import org.springframework.web.bind.annotation.RestController
95,@RestController
95,class MyController {
95,"@RequestMapping(""/thing"")"
95,fun thing(): MyThing {
95,return MyThing()
95,"As long as MyThing can be serialized by Jackson2 (true for a normal POJO or Groovy object), then localhost:8080/thing serves a JSON representation of it by default."
95,"Note that, in a browser, you might sometimes see XML responses, because browsers tend to send accept headers that prefer XML."
95,4.2. Write an XML REST Service
95,"If you have the Jackson XML extension (jackson-dataformat-xml) on the classpath, you can use it to render XML responses."
95,The previous example that we used for JSON would work.
95,"To use the Jackson XML renderer, add the following dependency to your project:"
95,<dependency>
95,<groupId>com.fasterxml.jackson.dataformat</groupId>
95,<artifactId>jackson-dataformat-xml</artifactId>
95,</dependency>
95,"If Jackson’s XML extension is not available and JAXB is available, XML can be rendered with the additional requirement of having MyThing annotated as @XmlRootElement, as shown in the following example:"
95,Java
95,import jakarta.xml.bind.annotation.XmlRootElement;
95,@XmlRootElement
95,public class MyThing {
95,private String name;
95,// getters/setters ...
95,public String getName() {
95,return this.name;
95,public void setName(String name) {
95,this.name = name;
95,Kotlin
95,import jakarta.xml.bind.annotation.XmlRootElement
95,@XmlRootElement
95,class MyThing {
95,var name: String? = null
95,"You will need to ensure that the JAXB library is part of your project, for example by adding:"
95,<dependency>
95,<groupId>org.glassfish.jaxb</groupId>
95,<artifactId>jaxb-runtime</artifactId>
95,</dependency>
95,"To get the server to render XML instead of JSON, you might have to send an Accept: text/xml header (or use a browser)."
95,4.3. Customize the Jackson ObjectMapper
95,Spring MVC (client and server side) uses HttpMessageConverters to negotiate content conversion in an HTTP exchange.
95,"If Jackson is on the classpath, you already get the default converter(s) provided by Jackson2ObjectMapperBuilder, an instance of which is auto-configured for you."
95,The ObjectMapper (or XmlMapper for Jackson XML converter) instance (created by default) has the following customized properties:
95,MapperFeature.DEFAULT_VIEW_INCLUSION is disabled
95,DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES is disabled
95,SerializationFeature.WRITE_DATES_AS_TIMESTAMPS is disabled
95,SerializationFeature.WRITE_DURATIONS_AS_TIMESTAMPS is disabled
95,Spring Boot also has some features to make it easier to customize this behavior.
95,You can configure the ObjectMapper and XmlMapper instances by using the environment.
95,Jackson provides an extensive suite of on/off features that can be used to configure various aspects of its processing.
95,These features are described in several enums (in Jackson) that map onto properties in the environment:
95,Enum
95,Property
95,Values
95,com.fasterxml.jackson.databind.cfg.EnumFeature
95,spring.jackson.datatype.enum.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.databind.cfg.JsonNodeFeature
95,spring.jackson.datatype.json-node.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.databind.DeserializationFeature
95,spring.jackson.deserialization.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.core.JsonGenerator.Feature
95,spring.jackson.generator.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.databind.MapperFeature
95,spring.jackson.mapper.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.core.JsonParser.Feature
95,spring.jackson.parser.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.databind.SerializationFeature
95,spring.jackson.serialization.<feature_name>
95,"true, false"
95,com.fasterxml.jackson.annotation.JsonInclude.Include
95,spring.jackson.default-property-inclusion
95,"always, non_null, non_absent, non_default, non_empty"
95,"For example, to enable pretty print, set spring.jackson.serialization.indent_output=true."
95,"Note that, thanks to the use of relaxed binding, the case of indent_output does not have to match the case of the corresponding enum constant, which is INDENT_OUTPUT."
95,"This environment-based configuration is applied to the auto-configured Jackson2ObjectMapperBuilder bean and applies to any mappers created by using the builder, including the auto-configured ObjectMapper bean."
95,The context’s Jackson2ObjectMapperBuilder can be customized by one or more Jackson2ObjectMapperBuilderCustomizer beans.
95,"Such customizer beans can be ordered (Boot’s own customizer has an order of 0), letting additional customization be applied both before and after Boot’s customization."
95,Any beans of type com.fasterxml.jackson.databind.Module are automatically registered with the auto-configured Jackson2ObjectMapperBuilder and are applied to any ObjectMapper instances that it creates.
95,This provides a global mechanism for contributing custom modules when you add new features to your application.
95,"If you want to replace the default ObjectMapper completely, either define a @Bean of that type and mark it as @Primary or, if you prefer the builder-based approach, define a Jackson2ObjectMapperBuilder @Bean."
95,"Note that, in either case, doing so disables all auto-configuration of the ObjectMapper."
95,"If you provide any @Beans of type MappingJackson2HttpMessageConverter, they replace the default value in the MVC configuration."
95,"Also, a convenience bean of type HttpMessageConverters is provided (and is always available if you use the default MVC configuration)."
95,It has some useful methods to access the default and user-enhanced message converters.
95,See the “Customize the @ResponseBody Rendering” section and the WebMvcAutoConfiguration source code for more details.
95,4.4. Customize the @ResponseBody Rendering
95,Spring uses HttpMessageConverters to render @ResponseBody (or responses from @RestController).
95,You can contribute additional converters by adding beans of the appropriate type in a Spring Boot context.
95,"If a bean you add is of a type that would have been included by default anyway (such as MappingJackson2HttpMessageConverter for JSON conversions), it replaces the default value."
95,A convenience bean of type HttpMessageConverters is provided and is always available if you use the default MVC configuration.
95,"It has some useful methods to access the default and user-enhanced message converters (For example, it can be useful if you want to manually inject them into a custom RestTemplate)."
95,"As in normal MVC usage, any WebMvcConfigurer beans that you provide can also contribute converters by overriding the configureMessageConverters method."
95,"However, unlike with normal MVC, you can supply only additional converters that you need (because Spring Boot uses the same mechanism to contribute its defaults)."
95,"Finally, if you opt out of the Spring Boot default MVC configuration by providing your own @EnableWebMvc configuration, you can take control completely and do everything manually by using getMessageConverters from WebMvcConfigurationSupport."
95,See the WebMvcAutoConfiguration source code for more details.
95,4.5. Handling Multipart File Uploads
95,Spring Boot embraces the servlet 5 jakarta.servlet.http.Part API to support uploading files.
95,"By default, Spring Boot configures Spring MVC with a maximum size of 1MB per file and a maximum of 10MB of file data in a single request."
95,"You may override these values, the location to which intermediate data is stored (for example, to the /tmp directory), and the threshold past which data is flushed to disk by using the properties exposed in the MultipartProperties class."
95,"For example, if you want to specify that files be unlimited, set the spring.servlet.multipart.max-file-size property to -1."
95,The multipart support is helpful when you want to receive multipart encoded file data as a @RequestParam-annotated parameter of type MultipartFile in a Spring MVC controller handler method.
95,See the MultipartAutoConfiguration source for more details.
95,It is recommended to use the container’s built-in support for multipart uploads rather than introducing an additional dependency such as Apache Commons File Upload.
95,4.6. Switch Off the Spring MVC DispatcherServlet
95,"By default, all content is served from the root of your application (/)."
95,"If you would rather map to a different path, you can configure one as follows:"
95,Properties
95,spring.mvc.servlet.path=/mypath
95,Yaml
95,spring:
95,mvc:
95,servlet:
95,"path: ""/mypath"""
95,If you have additional servlets you can declare a @Bean of type Servlet or ServletRegistrationBean for each and Spring Boot will register them transparently to the container.
95,"Because servlets are registered that way, they can be mapped to a sub-context of the DispatcherServlet without invoking it."
95,"Configuring the DispatcherServlet yourself is unusual but if you really need to do it, a @Bean of type DispatcherServletPath must be provided as well to provide the path of your custom DispatcherServlet."
95,4.7. Switch off the Default MVC Configuration
95,The easiest way to take complete control over MVC configuration is to provide your own @Configuration with the @EnableWebMvc annotation.
95,Doing so leaves all MVC configuration in your hands.
95,4.8. Customize ViewResolvers
95,"A ViewResolver is a core component of Spring MVC, translating view names in @Controller to actual View implementations."
95,"Note that ViewResolvers are mainly used in UI applications, rather than REST-style services (a View is not used to render a @ResponseBody)."
95,"There are many implementations of ViewResolver to choose from, and Spring on its own is not opinionated about which ones you should use."
95,"Spring Boot, on the other hand, installs one or two for you, depending on what it finds on the classpath and in the application context."
95,"The DispatcherServlet uses all the resolvers it finds in the application context, trying each one in turn until it gets a result."
95,"If you add your own, you have to be aware of the order and in which position your resolver is added."
95,WebMvcAutoConfiguration adds the following ViewResolvers to your context:
95,An InternalResourceViewResolver named ‘defaultViewResolver’.
95,"This one locates physical resources that can be rendered by using the DefaultServlet (including static resources and JSP pages, if you use those)."
95,It applies a prefix and a suffix to the view name and then looks for a physical resource with that path in the servlet context (the defaults are both empty but are accessible for external configuration through spring.mvc.view.prefix and spring.mvc.view.suffix).
95,You can override it by providing a bean of the same type.
95,A BeanNameViewResolver named ‘beanNameViewResolver’.
95,This is a useful member of the view resolver chain and picks up any beans with the same name as the View being resolved.
95,It should not be necessary to override or replace it.
95,A ContentNegotiatingViewResolver named ‘viewResolver’ is added only if there are actually beans of type View present.
95,"This is a composite resolver, delegating to all the others and attempting to find a match to the ‘Accept’ HTTP header sent by the client."
95,"There is a useful blog about ContentNegotiatingViewResolver that you might like to study to learn more, and you might also look at the source code for detail."
95,You can switch off the auto-configured ContentNegotiatingViewResolver by defining a bean named ‘viewResolver’.
95,"If you use Thymeleaf, you also have a ThymeleafViewResolver named ‘thymeleafViewResolver’."
95,It looks for resources by surrounding the view name with a prefix and suffix.
95,"The prefix is spring.thymeleaf.prefix, and the suffix is spring.thymeleaf.suffix."
95,"The values of the prefix and suffix default to ‘classpath:/templates/’ and ‘.html’, respectively."
95,You can override ThymeleafViewResolver by providing a bean of the same name.
95,"If you use FreeMarker, you also have a FreeMarkerViewResolver named ‘freeMarkerViewResolver’."
95,It looks for resources in a loader path (which is externalized to spring.freemarker.templateLoaderPath and has a default value of ‘classpath:/templates/’) by surrounding the view name with a prefix and a suffix.
95,"The prefix is externalized to spring.freemarker.prefix, and the suffix is externalized to spring.freemarker.suffix."
95,"The default values of the prefix and suffix are empty and ‘.ftlh’, respectively."
95,You can override FreeMarkerViewResolver by providing a bean of the same name.
95,"If you use Groovy templates (actually, if groovy-templates is on your classpath), you also have a GroovyMarkupViewResolver named ‘groovyMarkupViewResolver’."
95,It looks for resources in a loader path by surrounding the view name with a prefix and suffix (externalized to spring.groovy.template.prefix and spring.groovy.template.suffix).
95,"The prefix and suffix have default values of ‘classpath:/templates/’ and ‘.tpl’, respectively."
95,You can override GroovyMarkupViewResolver by providing a bean of the same name.
95,"If you use Mustache, you also have a MustacheViewResolver named ‘mustacheViewResolver’."
95,It looks for resources by surrounding the view name with a prefix and suffix.
95,"The prefix is spring.mustache.prefix, and the suffix is spring.mustache.suffix."
95,"The values of the prefix and suffix default to ‘classpath:/templates/’ and ‘.mustache’, respectively."
95,You can override MustacheViewResolver by providing a bean of the same name.
95,"For more detail, see the following sections:"
95,WebMvcAutoConfiguration
95,ThymeleafAutoConfiguration
95,FreeMarkerAutoConfiguration
95,GroovyTemplateAutoConfiguration
95,5. Jersey
95,5.1. Secure Jersey endpoints with Spring Security
95,Spring Security can be used to secure a Jersey-based web application in much the same way as it can be used to secure a Spring MVC-based web application.
95,"However, if you want to use Spring Security’s method-level security with Jersey, you must configure Jersey to use setStatus(int) rather sendError(int)."
95,This prevents Jersey from committing the response before Spring Security has had an opportunity to report an authentication or authorization failure to the client.
95,"The jersey.config.server.response.setStatusOverSendError property must be set to true on the application’s ResourceConfig bean, as shown in the following example:"
95,import java.util.Collections;
95,import org.glassfish.jersey.server.ResourceConfig;
95,import org.springframework.stereotype.Component;
95,@Component
95,public class JerseySetStatusOverSendErrorConfig extends ResourceConfig {
95,public JerseySetStatusOverSendErrorConfig() {
95,register(Endpoint.class);
95,"setProperties(Collections.singletonMap(""jersey.config.server.response.setStatusOverSendError"", true));"
95,5.2. Use Jersey Alongside Another Web Framework
95,"To use Jersey alongside another web framework, such as Spring MVC, it should be configured so that it will allow the other framework to handle requests that it cannot handle."
95,"First, configure Jersey to use a filter rather than a servlet by configuring the spring.jersey.type application property with a value of filter."
95,"Second, configure your ResourceConfig to forward requests that would have resulted in a 404, as shown in the following example."
95,import org.glassfish.jersey.server.ResourceConfig;
95,import org.glassfish.jersey.servlet.ServletProperties;
95,import org.springframework.stereotype.Component;
95,@Component
95,public class JerseyConfig extends ResourceConfig {
95,public JerseyConfig() {
95,register(Endpoint.class);
95,"property(ServletProperties.FILTER_FORWARD_ON_404, true);"
95,6. HTTP Clients
95,Spring Boot offers a number of starters that work with HTTP clients.
95,This section answers questions related to using them.
95,6.1. Configure RestTemplate to Use a Proxy
95,"As described in io.html, you can use a RestTemplateCustomizer with RestTemplateBuilder to build a customized RestTemplate."
95,This is the recommended approach for creating a RestTemplate configured to use a proxy.
95,The exact details of the proxy configuration depend on the underlying client request factory that is being used.
95,6.2. Configure the TcpClient used by a Reactor Netty-based WebClient
95,When Reactor Netty is on the classpath a Reactor Netty-based WebClient is auto-configured.
95,"To customize the client’s handling of network connections, provide a ClientHttpConnector bean."
95,The following example configures a 60 second connect timeout and adds a ReadTimeoutHandler:
95,Java
95,import io.netty.channel.ChannelOption;
95,import io.netty.handler.timeout.ReadTimeoutHandler;
95,import reactor.netty.http.client.HttpClient;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.http.client.ReactorResourceFactory;
95,import org.springframework.http.client.reactive.ClientHttpConnector;
95,import org.springframework.http.client.reactive.ReactorClientHttpConnector;
95,@Configuration(proxyBeanMethods = false)
95,public class MyReactorNettyClientConfiguration {
95,@Bean
95,ClientHttpConnector clientHttpConnector(ReactorResourceFactory resourceFactory) {
95,HttpClient httpClient = HttpClient.create(resourceFactory.getConnectionProvider())
95,.runOn(resourceFactory.getLoopResources())
95,".option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 60000)"
95,.doOnConnected((connection) -> connection.addHandlerLast(new ReadTimeoutHandler(60)));
95,return new ReactorClientHttpConnector(httpClient);
95,Kotlin
95,import io.netty.channel.ChannelOption
95,import io.netty.handler.timeout.ReadTimeoutHandler
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.http.client.reactive.ClientHttpConnector
95,import org.springframework.http.client.reactive.ReactorClientHttpConnector
95,import org.springframework.http.client.ReactorResourceFactory
95,import reactor.netty.http.client.HttpClient
95,@Configuration(proxyBeanMethods = false)
95,class MyReactorNettyClientConfiguration {
95,@Bean
95,fun clientHttpConnector(resourceFactory: ReactorResourceFactory): ClientHttpConnector {
95,val httpClient = HttpClient.create(resourceFactory.connectionProvider)
95,.runOn(resourceFactory.loopResources)
95,".option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 60000)"
95,.doOnConnected { connection ->
95,connection.addHandlerLast(ReadTimeoutHandler(60))
95,return ReactorClientHttpConnector(httpClient)
95,Note the use of ReactorResourceFactory for the connection provider and event loop resources.
95,This ensures efficient sharing of resources for the server receiving requests and the client making requests.
95,7. Logging
95,"Spring Boot has no mandatory logging dependency, except for the Commons Logging API, which is typically provided by Spring Framework’s spring-jcl module."
95,"To use Logback, you need to include it and spring-jcl on the classpath."
95,"The recommended way to do that is through the starters, which all depend on spring-boot-starter-logging."
95,"For a web application, you only need spring-boot-starter-web, since it depends transitively on the logging starter."
95,"If you use Maven, the following dependency adds logging for you:"
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-web</artifactId>
95,</dependency>
95,Spring Boot has a LoggingSystem abstraction that attempts to configure logging based on the content of the classpath.
95,"If Logback is available, it is the first choice."
95,"If the only change you need to make to logging is to set the levels of various loggers, you can do so in application.properties by using the ""logging.level"" prefix, as shown in the following example:"
95,Properties
95,logging.level.org.springframework.web=debug
95,logging.level.org.hibernate=error
95,Yaml
95,logging:
95,level:
95,"org.springframework.web: ""debug"""
95,"org.hibernate: ""error"""
95,You can also set the location of a file to which the log will be written (in addition to the console) by using logging.file.name.
95,"To configure the more fine-grained settings of a logging system, you need to use the native configuration format supported by the LoggingSystem in question."
95,"By default, Spring Boot picks up the native configuration from its default location for the system (such as classpath:logback.xml for Logback), but you can set the location of the config file by using the logging.config property."
95,7.1. Configure Logback for Logging
95,"If you need to apply customizations to logback beyond those that can be achieved with application.properties, you will need to add a standard logback configuration file."
95,You can add a logback.xml file to the root of your classpath for logback to find.
95,You can also use logback-spring.xml if you want to use the Spring Boot Logback extensions.
95,The Logback documentation has a dedicated section that covers configuration in some detail.
95,Spring Boot provides a number of logback configurations that can be included in your own configuration.
95,These includes are designed to allow certain common Spring Boot conventions to be re-applied.
95,The following files are provided under org/springframework/boot/logging/logback/:
95,"defaults.xml - Provides conversion rules, pattern properties and common logger configurations."
95,console-appender.xml - Adds a ConsoleAppender using the CONSOLE_LOG_PATTERN.
95,file-appender.xml - Adds a RollingFileAppender using the FILE_LOG_PATTERN and ROLLING_FILE_NAME_PATTERN with appropriate settings.
95,"In addition, a legacy base.xml file is provided for compatibility with earlier versions of Spring Boot."
95,A typical custom logback.xml file would look something like this:
95,"<?xml version=""1.0"" encoding=""UTF-8""?>"
95,<configuration>
95,"<include resource=""org/springframework/boot/logging/logback/defaults.xml""/>"
95,"<include resource=""org/springframework/boot/logging/logback/console-appender.xml"" />"
95,"<root level=""INFO"">"
95,"<appender-ref ref=""CONSOLE"" />"
95,</root>
95,"<logger name=""org.springframework.web"" level=""DEBUG""/>"
95,</configuration>
95,Your logback configuration file can also make use of System properties that the LoggingSystem takes care of creating for you:
95,${PID}: The current process ID.
95,${LOG_FILE}: Whether logging.file.name was set in Boot’s external configuration.
95,${LOG_PATH}: Whether logging.file.path (representing a directory for log files to live in) was set in Boot’s external configuration.
95,${LOG_EXCEPTION_CONVERSION_WORD}: Whether logging.exception-conversion-word was set in Boot’s external configuration.
95,${ROLLING_FILE_NAME_PATTERN}: Whether logging.pattern.rolling-file-name was set in Boot’s external configuration.
95,Spring Boot also provides some nice ANSI color terminal output on a console (but not in a log file) by using a custom Logback converter.
95,See the CONSOLE_LOG_PATTERN in the defaults.xml configuration for an example.
95,"If Groovy is on the classpath, you should be able to configure Logback with logback.groovy as well."
95,"If present, this setting is given preference."
95,Spring extensions are not supported with Groovy configuration.
95,Any logback-spring.groovy files will not be detected.
95,7.1.1. Configure Logback for File-only Output
95,"If you want to disable console logging and write output only to a file, you need a custom logback-spring.xml that imports file-appender.xml but not console-appender.xml, as shown in the following example:"
95,"<?xml version=""1.0"" encoding=""UTF-8""?>"
95,<configuration>
95,"<include resource=""org/springframework/boot/logging/logback/defaults.xml"" />"
95,"<property name=""LOG_FILE"" value=""${LOG_FILE:-${LOG_PATH:-${LOG_TEMP:-${java.io.tmpdir:-/tmp}}/}spring.log}""/>"
95,"<include resource=""org/springframework/boot/logging/logback/file-appender.xml"" />"
95,"<root level=""INFO"">"
95,"<appender-ref ref=""FILE"" />"
95,</root>
95,</configuration>
95,"You also need to add logging.file.name to your application.properties or application.yaml, as shown in the following example:"
95,Properties
95,logging.file.name=myapplication.log
95,Yaml
95,logging:
95,file:
95,"name: ""myapplication.log"""
95,7.2. Configure Log4j for Logging
95,Spring Boot supports Log4j 2 for logging configuration if it is on the classpath.
95,"If you use the starters for assembling dependencies, you have to exclude Logback and then include Log4j 2 instead."
95,"If you do not use the starters, you need to provide (at least) spring-jcl in addition to Log4j 2."
95,"The recommended path is through the starters, even though it requires some jiggling."
95,The following example shows how to set up the starters in Maven:
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-web</artifactId>
95,</dependency>
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter</artifactId>
95,<exclusions>
95,<exclusion>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-logging</artifactId>
95,</exclusion>
95,</exclusions>
95,</dependency>
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-log4j2</artifactId>
95,</dependency>
95,Gradle provides a few different ways to set up the starters.
95,One way is to use a module replacement.
95,"To do so, declare a dependency on the Log4j 2 starter and tell Gradle that any occurrences of the default logging starter should be replaced by the Log4j 2 starter, as shown in the following example:"
95,dependencies {
95,"implementation ""org.springframework.boot:spring-boot-starter-log4j2"""
95,modules {
95,"module(""org.springframework.boot:spring-boot-starter-logging"") {"
95,"replacedBy(""org.springframework.boot:spring-boot-starter-log4j2"", ""Use Log4j2 instead of Logback"")"
95,The Log4j starters gather together the dependencies for common logging requirements (such as having Tomcat use java.util.logging but configuring the output using Log4j 2).
95,"To ensure that debug logging performed using java.util.logging is routed into Log4j 2, configure its JDK logging adapter by setting the java.util.logging.manager system property to org.apache.logging.log4j.jul.LogManager."
95,7.2.1. Use YAML or JSON to Configure Log4j 2
95,"In addition to its default XML configuration format, Log4j 2 also supports YAML and JSON configuration files."
95,"To configure Log4j 2 to use an alternative configuration file format, add the appropriate dependencies to the classpath and name your configuration files to match your chosen file format, as shown in the following example:"
95,Format
95,Dependencies
95,File names
95,YAML
95,com.fasterxml.jackson.core:jackson-databind + com.fasterxml.jackson.dataformat:jackson-dataformat-yaml
95,log4j2.yaml + log4j2.yml
95,JSON
95,com.fasterxml.jackson.core:jackson-databind
95,log4j2.json + log4j2.jsn
95,7.2.2. Use Composite Configuration to Configure Log4j 2
95,Log4j 2 has support for combining multiple configuration files into a single composite configuration.
95,"To use this support in Spring Boot, configure logging.log4j2.config.override with the locations of one or more secondary configuration files."
95,"The secondary configuration files will be merged with the primary configuration, whether the primary’s source is Spring Boot’s defaults, a standard location such as log4j.xml, or the location configured by the logging.config property."
95,8. Data Access
95,Spring Boot includes a number of starters for working with data sources.
95,This section answers questions related to doing so.
95,8.1. Configure a Custom DataSource
95,"To configure your own DataSource, define a @Bean of that type in your configuration."
95,"Spring Boot reuses your DataSource anywhere one is required, including database initialization."
95,"If you need to externalize some settings, you can bind your DataSource to the environment (see “features.html”)."
95,The following example shows how to define a data source in a bean:
95,Java
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(prefix = ""app.datasource"")"
95,public SomeDataSource dataSource() {
95,return new SomeDataSource();
95,Kotlin
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(prefix = ""app.datasource"")"
95,fun dataSource(): SomeDataSource {
95,return SomeDataSource()
95,The following example shows how to define a data source by setting properties:
95,Properties
95,app.datasource.url=jdbc:h2:mem:mydb
95,app.datasource.username=sa
95,app.datasource.pool-size=30
95,Yaml
95,app:
95,datasource:
95,"url: ""jdbc:h2:mem:mydb"""
95,"username: ""sa"""
95,pool-size: 30
95,"Assuming that SomeDataSource has regular JavaBean properties for the URL, the username, and the pool size, these settings are bound automatically before the DataSource is made available to other components."
95,"Spring Boot also provides a utility builder class, called DataSourceBuilder, that can be used to create one of the standard data sources (if it is on the classpath)."
95,The builder can detect the one to use based on what is available on the classpath.
95,It also auto-detects the driver based on the JDBC URL.
95,The following example shows how to create a data source by using a DataSourceBuilder:
95,Java
95,import javax.sql.DataSource;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.jdbc.DataSourceBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.datasource"")"
95,public DataSource dataSource() {
95,return DataSourceBuilder.create().build();
95,Kotlin
95,import javax.sql.DataSource
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.boot.jdbc.DataSourceBuilder
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.datasource"")"
95,fun dataSource(): DataSource {
95,return DataSourceBuilder.create().build()
95,"To run an app with that DataSource, all you need is the connection information."
95,Pool-specific settings can also be provided.
95,Check the implementation that is going to be used at runtime for more details.
95,The following example shows how to define a JDBC data source by setting properties:
95,Properties
95,app.datasource.url=jdbc:mysql://localhost/test
95,app.datasource.username=dbuser
95,app.datasource.password=dbpass
95,app.datasource.pool-size=30
95,Yaml
95,app:
95,datasource:
95,"url: ""jdbc:mysql://localhost/test"""
95,"username: ""dbuser"""
95,"password: ""dbpass"""
95,pool-size: 30
95,"However, there is a catch."
95,"Because the actual type of the connection pool is not exposed, no keys are generated in the metadata for your custom DataSource and no completion is available in your IDE (because the DataSource interface exposes no properties)."
95,"Also, if you happen to have Hikari on the classpath, this basic setup does not work, because Hikari has no url property (but does have a jdbcUrl property)."
95,"In that case, you must rewrite your configuration as follows:"
95,Properties
95,app.datasource.jdbc-url=jdbc:mysql://localhost/test
95,app.datasource.username=dbuser
95,app.datasource.password=dbpass
95,app.datasource.pool-size=30
95,Yaml
95,app:
95,datasource:
95,"jdbc-url: ""jdbc:mysql://localhost/test"""
95,"username: ""dbuser"""
95,"password: ""dbpass"""
95,pool-size: 30
95,You can fix that by forcing the connection pool to use and return a dedicated implementation rather than DataSource.
95,"You cannot change the implementation at runtime, but the list of options will be explicit."
95,The following example shows how create a HikariDataSource with DataSourceBuilder:
95,Java
95,import com.zaxxer.hikari.HikariDataSource;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.jdbc.DataSourceBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.datasource"")"
95,public HikariDataSource dataSource() {
95,return DataSourceBuilder.create().type(HikariDataSource.class).build();
95,Kotlin
95,import com.zaxxer.hikari.HikariDataSource
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.boot.jdbc.DataSourceBuilder
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyDataSourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.datasource"")"
95,fun dataSource(): HikariDataSource {
95,return DataSourceBuilder.create().type(HikariDataSource::class.java).build()
95,"You can even go further by leveraging what DataSourceProperties does for you — that is, by providing a default embedded database with a sensible username and password if no URL is provided."
95,"You can easily initialize a DataSourceBuilder from the state of any DataSourceProperties object, so you could also inject the DataSource that Spring Boot creates automatically."
95,"However, that would split your configuration into two namespaces: url, username, password, type, and driver on spring.datasource and the rest on your custom namespace (app.datasource)."
95,"To avoid that, you can redefine a custom DataSourceProperties on your custom namespace, as shown in the following example:"
95,Java
95,import com.zaxxer.hikari.HikariDataSource;
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.context.annotation.Primary;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDataSourceConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource"")"
95,public DataSourceProperties dataSourceProperties() {
95,return new DataSourceProperties();
95,@Bean
95,"@ConfigurationProperties(""app.datasource.configuration"")"
95,public HikariDataSource dataSource(DataSourceProperties properties) {
95,return properties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
95,Kotlin
95,import com.zaxxer.hikari.HikariDataSource
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.context.annotation.Primary
95,@Configuration(proxyBeanMethods = false)
95,class MyDataSourceConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource"")"
95,fun dataSourceProperties(): DataSourceProperties {
95,return DataSourceProperties()
95,@Bean
95,"@ConfigurationProperties(""app.datasource.configuration"")"
95,fun dataSource(properties: DataSourceProperties): HikariDataSource {
95,return properties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
95,"This setup puts you in sync with what Spring Boot does for you by default, except that a dedicated connection pool is chosen (in code) and its settings are exposed in the app.datasource.configuration sub namespace."
95,"Because DataSourceProperties is taking care of the url/jdbcUrl translation for you, you can configure it as follows:"
95,Properties
95,app.datasource.url=jdbc:mysql://localhost/test
95,app.datasource.username=dbuser
95,app.datasource.password=dbpass
95,app.datasource.configuration.maximum-pool-size=30
95,Yaml
95,app:
95,datasource:
95,"url: ""jdbc:mysql://localhost/test"""
95,"username: ""dbuser"""
95,"password: ""dbpass"""
95,configuration:
95,maximum-pool-size: 30
95,Spring Boot will expose Hikari-specific settings to spring.datasource.hikari.
95,This example uses a more generic configuration sub namespace as the example does not support multiple datasource implementations.
95,"Because your custom configuration chooses to go with Hikari, app.datasource.type has no effect."
95,"In practice, the builder is initialized with whatever value you might set there and then overridden by the call to .type()."
95,See “data.html” in the “Spring Boot features” section and the DataSourceAutoConfiguration class for more details.
95,8.2. Configure Two DataSources
95,"If you need to configure multiple data sources, you can apply the same tricks that are described in the previous section."
95,"You must, however, mark one of the DataSource instances as @Primary, because various auto-configurations down the road expect to be able to get one by type."
95,"If you create your own DataSource, the auto-configuration backs off."
95,"In the following example, we provide the exact same feature set as the auto-configuration provides on the primary data source:"
95,Java
95,import com.zaxxer.hikari.HikariDataSource;
95,import org.apache.commons.dbcp2.BasicDataSource;
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.jdbc.DataSourceBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.context.annotation.Primary;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDataSourcesConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first"")"
95,public DataSourceProperties firstDataSourceProperties() {
95,return new DataSourceProperties();
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first.configuration"")"
95,public HikariDataSource firstDataSource(DataSourceProperties firstDataSourceProperties) {
95,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,public BasicDataSource secondDataSource() {
95,return DataSourceBuilder.create().type(BasicDataSource.class).build();
95,Kotlin
95,import com.zaxxer.hikari.HikariDataSource
95,import org.apache.commons.dbcp2.BasicDataSource
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.boot.jdbc.DataSourceBuilder
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.context.annotation.Primary
95,@Configuration(proxyBeanMethods = false)
95,class MyDataSourcesConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first"")"
95,fun firstDataSourceProperties(): DataSourceProperties {
95,return DataSourceProperties()
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first.configuration"")"
95,fun firstDataSource(firstDataSourceProperties: DataSourceProperties): HikariDataSource {
95,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,fun secondDataSource(): BasicDataSource {
95,return DataSourceBuilder.create().type(BasicDataSource::class.java).build()
95,firstDataSourceProperties has to be flagged as @Primary so that the database initializer feature uses your copy (if you use the initializer).
95,Both data sources are also bound for advanced customizations.
95,"For instance, you could configure them as follows:"
95,Properties
95,app.datasource.first.url=jdbc:mysql://localhost/first
95,app.datasource.first.username=dbuser
95,app.datasource.first.password=dbpass
95,app.datasource.first.configuration.maximum-pool-size=30
95,app.datasource.second.url=jdbc:mysql://localhost/second
95,app.datasource.second.username=dbuser
95,app.datasource.second.password=dbpass
95,app.datasource.second.max-total=30
95,Yaml
95,app:
95,datasource:
95,first:
95,"url: ""jdbc:mysql://localhost/first"""
95,"username: ""dbuser"""
95,"password: ""dbpass"""
95,configuration:
95,maximum-pool-size: 30
95,second:
95,"url: ""jdbc:mysql://localhost/second"""
95,"username: ""dbuser"""
95,"password: ""dbpass"""
95,max-total: 30
95,"You can apply the same concept to the secondary DataSource as well, as shown in the following example:"
95,Java
95,import com.zaxxer.hikari.HikariDataSource;
95,import org.apache.commons.dbcp2.BasicDataSource;
95,import org.springframework.beans.factory.annotation.Qualifier;
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.context.annotation.Primary;
95,@Configuration(proxyBeanMethods = false)
95,public class MyCompleteDataSourcesConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first"")"
95,public DataSourceProperties firstDataSourceProperties() {
95,return new DataSourceProperties();
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first.configuration"")"
95,public HikariDataSource firstDataSource(DataSourceProperties firstDataSourceProperties) {
95,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,public DataSourceProperties secondDataSourceProperties() {
95,return new DataSourceProperties();
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second.configuration"")"
95,public BasicDataSource secondDataSource(
95,"@Qualifier(""secondDataSourceProperties"") DataSourceProperties secondDataSourceProperties) {"
95,return secondDataSourceProperties.initializeDataSourceBuilder().type(BasicDataSource.class).build();
95,Kotlin
95,import com.zaxxer.hikari.HikariDataSource
95,import org.apache.commons.dbcp2.BasicDataSource
95,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.context.annotation.Primary
95,@Configuration(proxyBeanMethods = false)
95,class MyCompleteDataSourcesConfiguration {
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first"")"
95,fun firstDataSourceProperties(): DataSourceProperties {
95,return DataSourceProperties()
95,@Bean
95,@Primary
95,"@ConfigurationProperties(""app.datasource.first.configuration"")"
95,fun firstDataSource(firstDataSourceProperties: DataSourceProperties): HikariDataSource {
95,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,fun secondDataSourceProperties(): DataSourceProperties {
95,return DataSourceProperties()
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second.configuration"")"
95,fun secondDataSource(secondDataSourceProperties: DataSourceProperties): BasicDataSource {
95,return secondDataSourceProperties.initializeDataSourceBuilder().type(BasicDataSource::class.java).build()
95,The preceding example configures two data sources on custom namespaces with the same logic as Spring Boot would use in auto-configuration.
95,Note that each configuration sub namespace provides advanced settings based on the chosen implementation.
95,8.3. Use Spring Data Repositories
95,Spring Data can create implementations of @Repository interfaces of various flavors.
95,"Spring Boot handles all of that for you, as long as those @Repository annotations are included in one of the auto-configuration packages, typically the package (or a sub-package) of your main application class that is annotated with @SpringBootApplication or @EnableAutoConfiguration."
95,"For many applications, all you need is to put the right Spring Data dependencies on your classpath."
95,"There is a spring-boot-starter-data-jpa for JPA, spring-boot-starter-data-mongodb for Mongodb, and various other starters for supported technologies."
95,"To get started, create some repository interfaces to handle your @Entity objects."
95,Spring Boot determines the location of your @Repository definitions by scanning the auto-configuration packages.
95,"For more control, use the @Enable…Repositories annotations from Spring Data."
95,"For more about Spring Data, see the Spring Data project page."
95,8.4. Separate @Entity Definitions from Spring Configuration
95,Spring Boot determines the location of your @Entity definitions by scanning the auto-configuration packages.
95,"For more control, use the @EntityScan annotation, as shown in the following example:"
95,Java
95,import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
95,import org.springframework.boot.autoconfigure.domain.EntityScan;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,@EnableAutoConfiguration
95,@EntityScan(basePackageClasses = City.class)
95,public class MyApplication {
95,// ...
95,Kotlin
95,import org.springframework.boot.autoconfigure.EnableAutoConfiguration
95,import org.springframework.boot.autoconfigure.domain.EntityScan
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,@EnableAutoConfiguration
95,@EntityScan(basePackageClasses = [City::class])
95,class MyApplication {
95,// ...
95,8.5. Configure JPA Properties
95,"Spring Data JPA already provides some vendor-independent configuration options (such as those for SQL logging), and Spring Boot exposes those options and a few more for Hibernate as external configuration properties."
95,Some of them are automatically detected according to the context so you should not have to set them.
95,"The spring.jpa.hibernate.ddl-auto is a special case, because, depending on runtime conditions, it has different defaults."
95,"If an embedded database is used and no schema manager (such as Liquibase or Flyway) is handling the DataSource, it defaults to create-drop."
95,"In all other cases, it defaults to none."
95,The dialect to use is detected by the JPA provider.
95,"If you prefer to set the dialect yourself, set the spring.jpa.database-platform property."
95,The most common options to set are shown in the following example:
95,Properties
95,spring.jpa.hibernate.naming.physical-strategy=com.example.MyPhysicalNamingStrategy
95,spring.jpa.show-sql=true
95,Yaml
95,spring:
95,jpa:
95,hibernate:
95,naming:
95,"physical-strategy: ""com.example.MyPhysicalNamingStrategy"""
95,show-sql: true
95,"In addition, all properties in spring.jpa.properties.* are passed through as normal JPA properties (with the prefix stripped) when the local EntityManagerFactory is created."
95,You need to ensure that names defined under spring.jpa.properties.* exactly match those expected by your JPA provider.
95,Spring Boot will not attempt any kind of relaxed binding for these entries.
95,"For example, if you want to configure Hibernate’s batch size you must use spring.jpa.properties.hibernate.jdbc.batch_size."
95,"If you use other forms, such as batchSize or batch-size, Hibernate will not apply the setting."
95,"If you need to apply advanced customization to Hibernate properties, consider registering a HibernatePropertiesCustomizer bean that will be invoked prior to creating the EntityManagerFactory."
95,This takes precedence to anything that is applied by the auto-configuration.
95,8.6. Configure Hibernate Naming Strategy
95,Hibernate uses two different naming strategies to map names from the object model to the corresponding database names.
95,"The fully qualified class name of the physical and the implicit strategy implementations can be configured by setting the spring.jpa.hibernate.naming.physical-strategy and spring.jpa.hibernate.naming.implicit-strategy properties, respectively."
95,"Alternatively, if ImplicitNamingStrategy or PhysicalNamingStrategy beans are available in the application context, Hibernate will be automatically configured to use them."
95,"By default, Spring Boot configures the physical naming strategy with CamelCaseToUnderscoresNamingStrategy."
95,"Using this strategy, all dots are replaced by underscores and camel casing is replaced by underscores as well."
95,"Additionally, by default, all table names are generated in lower case."
95,"For example, a TelephoneNumber entity is mapped to the telephone_number table."
95,"If your schema requires mixed-case identifiers, define a custom CamelCaseToUnderscoresNamingStrategy bean, as shown in the following example:"
95,Java
95,import org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy;
95,import org.hibernate.engine.jdbc.env.spi.JdbcEnvironment;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyHibernateConfiguration {
95,@Bean
95,public CamelCaseToUnderscoresNamingStrategy caseSensitivePhysicalNamingStrategy() {
95,return new CamelCaseToUnderscoresNamingStrategy() {
95,@Override
95,protected boolean isCaseInsensitive(JdbcEnvironment jdbcEnvironment) {
95,return false;
95,Kotlin
95,import org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy
95,import org.hibernate.engine.jdbc.env.spi.JdbcEnvironment
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyHibernateConfiguration {
95,@Bean
95,fun caseSensitivePhysicalNamingStrategy(): CamelCaseToUnderscoresNamingStrategy {
95,return object : CamelCaseToUnderscoresNamingStrategy() {
95,override fun isCaseInsensitive(jdbcEnvironment: JdbcEnvironment): Boolean {
95,return false
95,"If you prefer to use Hibernate’s default instead, set the following property:"
95,spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
95,"Alternatively, you can configure the following bean:"
95,Java
95,import org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,class MyHibernateConfiguration {
95,@Bean
95,PhysicalNamingStrategyStandardImpl caseSensitivePhysicalNamingStrategy() {
95,return new PhysicalNamingStrategyStandardImpl();
95,Kotlin
95,import org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,internal class MyHibernateConfiguration {
95,@Bean
95,fun caseSensitivePhysicalNamingStrategy(): PhysicalNamingStrategyStandardImpl {
95,return PhysicalNamingStrategyStandardImpl()
95,See HibernateJpaAutoConfiguration and JpaBaseConfiguration for more details.
95,8.7. Configure Hibernate Second-Level Caching
95,Hibernate second-level cache can be configured for a range of cache providers.
95,"Rather than configuring Hibernate to lookup the cache provider again, it is better to provide the one that is available in the context whenever possible."
95,"To do this with JCache, first make sure that org.hibernate.orm:hibernate-jcache is available on the classpath."
95,"Then, add a HibernatePropertiesCustomizer bean as shown in the following example:"
95,Java
95,import org.hibernate.cache.jcache.ConfigSettings;
95,import org.springframework.boot.autoconfigure.orm.jpa.HibernatePropertiesCustomizer;
95,import org.springframework.cache.jcache.JCacheCacheManager;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyHibernateSecondLevelCacheConfiguration {
95,@Bean
95,public HibernatePropertiesCustomizer hibernateSecondLevelCacheCustomizer(JCacheCacheManager cacheManager) {
95,"return (properties) -> properties.put(ConfigSettings.CACHE_MANAGER, cacheManager.getCacheManager());"
95,Kotlin
95,import org.hibernate.cache.jcache.ConfigSettings
95,import org.springframework.boot.autoconfigure.orm.jpa.HibernatePropertiesCustomizer
95,import org.springframework.cache.jcache.JCacheCacheManager
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,class MyHibernateSecondLevelCacheConfiguration {
95,@Bean
95,fun hibernateSecondLevelCacheCustomizer(cacheManager: JCacheCacheManager): HibernatePropertiesCustomizer {
95,return HibernatePropertiesCustomizer { properties ->
95,properties[ConfigSettings.CACHE_MANAGER] = cacheManager.cacheManager
95,This customizer will configure Hibernate to use the same CacheManager as the one that the application uses.
95,It is also possible to use separate CacheManager instances.
95,"For details, see the Hibernate user guide."
95,8.8. Use Dependency Injection in Hibernate Components
95,"By default, Spring Boot registers a BeanContainer implementation that uses the BeanFactory so that converters and entity listeners can use regular dependency injection."
95,You can disable or tune this behavior by registering a HibernatePropertiesCustomizer that removes or changes the hibernate.resource.beans.container property.
95,8.9. Use a Custom EntityManagerFactory
95,"To take full control of the configuration of the EntityManagerFactory, you need to add a @Bean named ‘entityManagerFactory’."
95,Spring Boot auto-configuration switches off its entity manager in the presence of a bean of that type.
95,8.10. Using Multiple EntityManagerFactories
95,"If you need to use JPA against multiple data sources, you likely need one EntityManagerFactory per data source."
95,The LocalContainerEntityManagerFactoryBean from Spring ORM allows you to configure an EntityManagerFactory for your needs.
95,"You can also reuse JpaProperties to bind settings for each EntityManagerFactory, as shown in the following example:"
95,Java
95,import javax.sql.DataSource;
95,import org.springframework.boot.autoconfigure.orm.jpa.JpaProperties;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.orm.jpa.JpaVendorAdapter;
95,import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;
95,import org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter;
95,@Configuration(proxyBeanMethods = false)
95,public class MyEntityManagerFactoryConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.jpa.first"")"
95,public JpaProperties firstJpaProperties() {
95,return new JpaProperties();
95,@Bean
95,"public LocalContainerEntityManagerFactoryBean firstEntityManagerFactory(DataSource firstDataSource,"
95,JpaProperties firstJpaProperties) {
95,EntityManagerFactoryBuilder builder = createEntityManagerFactoryBuilder(firstJpaProperties);
95,"return builder.dataSource(firstDataSource).packages(Order.class).persistenceUnit(""firstDs"").build();"
95,private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder(JpaProperties jpaProperties) {
95,JpaVendorAdapter jpaVendorAdapter = createJpaVendorAdapter(jpaProperties);
95,"return new EntityManagerFactoryBuilder(jpaVendorAdapter, jpaProperties.getProperties(), null);"
95,private JpaVendorAdapter createJpaVendorAdapter(JpaProperties jpaProperties) {
95,// ... map JPA properties as needed
95,return new HibernateJpaVendorAdapter();
95,Kotlin
95,import javax.sql.DataSource
95,import org.springframework.boot.autoconfigure.orm.jpa.JpaProperties
95,import org.springframework.boot.context.properties.ConfigurationProperties
95,import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.orm.jpa.JpaVendorAdapter
95,import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean
95,import org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter
95,@Configuration(proxyBeanMethods = false)
95,class MyEntityManagerFactoryConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.jpa.first"")"
95,fun firstJpaProperties(): JpaProperties {
95,return JpaProperties()
95,@Bean
95,fun firstEntityManagerFactory(
95,"firstDataSource: DataSource?,"
95,firstJpaProperties: JpaProperties
95,): LocalContainerEntityManagerFactoryBean {
95,val builder = createEntityManagerFactoryBuilder(firstJpaProperties)
95,"return builder.dataSource(firstDataSource).packages(Order::class.java).persistenceUnit(""firstDs"").build()"
95,private fun createEntityManagerFactoryBuilder(jpaProperties: JpaProperties): EntityManagerFactoryBuilder {
95,val jpaVendorAdapter = createJpaVendorAdapter(jpaProperties)
95,"return EntityManagerFactoryBuilder(jpaVendorAdapter, jpaProperties.properties, null)"
95,private fun createJpaVendorAdapter(jpaProperties: JpaProperties): JpaVendorAdapter {
95,// ... map JPA properties as needed
95,return HibernateJpaVendorAdapter()
95,The example above creates an EntityManagerFactory using a DataSource bean named firstDataSource.
95,It scans entities located in the same package as Order.
95,It is possible to map additional JPA properties using the app.first.jpa namespace.
95,"When you create a bean for LocalContainerEntityManagerFactoryBean yourself, any customization that was applied during the creation of the auto-configured LocalContainerEntityManagerFactoryBean is lost."
95,"For example, in case of Hibernate, any properties under the spring.jpa.hibernate prefix will not be automatically applied to your LocalContainerEntityManagerFactoryBean."
95,"If you were relying on these properties for configuring things like the naming strategy or the DDL mode, you will need to explicitly configure that when creating the LocalContainerEntityManagerFactoryBean bean."
95,You should provide a similar configuration for any additional data sources for which you need JPA access.
95,"To complete the picture, you need to configure a JpaTransactionManager for each EntityManagerFactory as well."
95,"Alternatively, you might be able to use a JTA transaction manager that spans both."
95,"If you use Spring Data, you need to configure @EnableJpaRepositories accordingly, as shown in the following examples:"
95,Java
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
95,@Configuration(proxyBeanMethods = false)
95,"@EnableJpaRepositories(basePackageClasses = Order.class, entityManagerFactoryRef = ""firstEntityManagerFactory"")"
95,public class OrderConfiguration {
95,Kotlin
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.data.jpa.repository.config.EnableJpaRepositories
95,@Configuration(proxyBeanMethods = false)
95,"@EnableJpaRepositories(basePackageClasses = [Order::class], entityManagerFactoryRef = ""firstEntityManagerFactory"")"
95,class OrderConfiguration
95,Java
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
95,@Configuration(proxyBeanMethods = false)
95,"@EnableJpaRepositories(basePackageClasses = Customer.class, entityManagerFactoryRef = ""secondEntityManagerFactory"")"
95,public class CustomerConfiguration {
95,Kotlin
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.data.jpa.repository.config.EnableJpaRepositories
95,@Configuration(proxyBeanMethods = false)
95,"@EnableJpaRepositories(basePackageClasses = [Customer::class], entityManagerFactoryRef = ""secondEntityManagerFactory"")"
95,class CustomerConfiguration
95,8.11. Use a Traditional persistence.xml File
95,Spring Boot will not search for or use a META-INF/persistence.xml by default.
95,"If you prefer to use a traditional persistence.xml, you need to define your own @Bean of type LocalEntityManagerFactoryBean (with an ID of ‘entityManagerFactory’) and set the persistence unit name there."
95,See JpaBaseConfiguration for the default settings.
95,8.12. Use Spring Data JPA and Mongo Repositories
95,Spring Data JPA and Spring Data Mongo can both automatically create Repository implementations for you.
95,"If they are both present on the classpath, you might have to do some extra configuration to tell Spring Boot which repositories to create."
95,The most explicit way to do that is to use the standard Spring Data @EnableJpaRepositories and @EnableMongoRepositories annotations and provide the location of your Repository interfaces.
95,There are also flags (spring.data.*.repositories.enabled and spring.data.*.repositories.type) that you can use to switch the auto-configured repositories on and off in external configuration.
95,"Doing so is useful, for instance, in case you want to switch off the Mongo repositories and still use the auto-configured MongoTemplate."
95,"The same obstacle and the same features exist for other auto-configured Spring Data repository types (Elasticsearch, Redis, and others)."
95,"To work with them, change the names of the annotations and flags accordingly."
95,8.13. Customize Spring Data’s Web Support
95,Spring Data provides web support that simplifies the use of Spring Data repositories in a web application.
95,Spring Boot provides properties in the spring.data.web namespace for customizing its configuration.
95,"Note that if you are using Spring Data REST, you must use the properties in the spring.data.rest namespace instead."
95,8.14. Expose Spring Data Repositories as REST Endpoint
95,"Spring Data REST can expose the Repository implementations as REST endpoints for you,"
95,provided Spring MVC has been enabled for the application.
95,Spring Boot exposes a set of useful properties (from the spring.data.rest namespace) that customize the RepositoryRestConfiguration.
95,"If you need to provide additional customization, you should use a RepositoryRestConfigurer bean."
95,"If you do not specify any order on your custom RepositoryRestConfigurer, it runs after the one Spring Boot uses internally."
95,"If you need to specify an order, make sure it is higher than 0."
95,8.15. Configure a Component that is Used by JPA
95,"If you want to configure a component that JPA uses, then you need to ensure that the component is initialized before JPA."
95,"When the component is auto-configured, Spring Boot takes care of this for you."
95,"For example, when Flyway is auto-configured, Hibernate is configured to depend upon Flyway so that Flyway has a chance to initialize the database before Hibernate tries to use it."
95,"If you are configuring a component yourself, you can use an EntityManagerFactoryDependsOnPostProcessor subclass as a convenient way of setting up the necessary dependencies."
95,"For example, if you use Hibernate Search with Elasticsearch as its index manager, any EntityManagerFactory beans must be configured to depend on the elasticsearchClient bean, as shown in the following example:"
95,Java
95,import jakarta.persistence.EntityManagerFactory;
95,import org.springframework.boot.autoconfigure.orm.jpa.EntityManagerFactoryDependsOnPostProcessor;
95,import org.springframework.stereotype.Component;
95,/**
95,* {@link EntityManagerFactoryDependsOnPostProcessor} that ensures that
95,* {@link EntityManagerFactory} beans depend on the {@code elasticsearchClient} bean.
95,@Component
95,public class ElasticsearchEntityManagerFactoryDependsOnPostProcessor
95,extends EntityManagerFactoryDependsOnPostProcessor {
95,public ElasticsearchEntityManagerFactoryDependsOnPostProcessor() {
95,"super(""elasticsearchClient"");"
95,Kotlin
95,import org.springframework.boot.autoconfigure.orm.jpa.EntityManagerFactoryDependsOnPostProcessor
95,import org.springframework.stereotype.Component
95,@Component
95,class ElasticsearchEntityManagerFactoryDependsOnPostProcessor :
95,"EntityManagerFactoryDependsOnPostProcessor(""elasticsearchClient"")"
95,8.16. Configure jOOQ with Two DataSources
95,"If you need to use jOOQ with multiple data sources, you should create your own DSLContext for each one."
95,See JooqAutoConfiguration for more details.
95,"In particular, JooqExceptionTranslator and SpringTransactionProvider can be reused to provide similar features to what the auto-configuration does with a single DataSource."
95,9. Database Initialization
95,An SQL database can be initialized in different ways depending on what your stack is.
95,"Of course, you can also do it manually, provided the database is a separate process."
95,It is recommended to use a single mechanism for schema generation.
95,9.1. Initialize a Database Using JPA
95,"JPA has features for DDL generation, and these can be set up to run on startup against the database."
95,This is controlled through two external properties:
95,spring.jpa.generate-ddl (boolean) switches the feature on and off and is vendor independent.
95,spring.jpa.hibernate.ddl-auto (enum) is a Hibernate feature that controls the behavior in a more fine-grained way.
95,This feature is described in more detail later in this guide.
95,9.2. Initialize a Database Using Hibernate
95,"You can set spring.jpa.hibernate.ddl-auto explicitly to one of the standard Hibernate property values which are none, validate, update, create, and create-drop."
95,Spring Boot chooses a default value for you based on whether it thinks your database is embedded.
95,It defaults to create-drop if no schema manager has been detected or none in all other cases.
95,An embedded database is detected by looking at the Connection type and JDBC url.
95,"hsqldb, h2, and derby are candidates, while others are not."
95,Be careful when switching from in-memory to a ‘real’ database that you do not make assumptions about the existence of the tables and data in the new platform.
95,You either have to set ddl-auto explicitly or use one of the other mechanisms to initialize the database.
95,You can output the schema creation by enabling the org.hibernate.SQL logger.
95,This is done for you automatically if you enable the debug mode.
95,"In addition, a file named import.sql in the root of the classpath is executed on startup if Hibernate creates the schema from scratch (that is, if the ddl-auto property is set to create or create-drop)."
95,This can be useful for demos and for testing if you are careful but is probably not something you want to be on the classpath in production.
95,It is a Hibernate feature (and has nothing to do with Spring).
95,9.3. Initialize a Database Using Basic SQL Scripts
95,Spring Boot can automatically create the schema (DDL scripts) of your JDBC DataSource or R2DBC ConnectionFactory and initialize its data (DML scripts).
95,"By default, it loads schema scripts from optional:classpath*:schema.sql and data scripts from optional:classpath*:data.sql."
95,The locations of these schema and data scripts can be customized using spring.sql.init.schema-locations and spring.sql.init.data-locations respectively.
95,The optional: prefix means that the application will start even when the files do not exist.
95,"To have the application fail to start when the files are absent, remove the optional: prefix."
95,"In addition, Spring Boot processes the optional:classpath*:schema-${platform}.sql and optional:classpath*:data-${platform}.sql files (if present), where ${platform} is the value of spring.sql.init.platform."
95,This allows you to switch to database-specific scripts if necessary.
95,"For example, you might choose to set it to the vendor name of the database (hsqldb, h2, oracle, mysql, postgresql, and so on)."
95,"By default, SQL database initialization is only performed when using an embedded in-memory database."
95,"To always initialize an SQL database, irrespective of its type, set spring.sql.init.mode to always."
95,"Similarly, to disable initialization, set spring.sql.init.mode to never."
95,"By default, Spring Boot enables the fail-fast feature of its script-based database initializer."
95,"This means that, if the scripts cause exceptions, the application fails to start."
95,You can tune that behavior by setting spring.sql.init.continue-on-error.
95,"Script-based DataSource initialization is performed, by default, before any JPA EntityManagerFactory beans are created."
95,schema.sql can be used to create the schema for JPA-managed entities and data.sql can be used to populate it.
95,"While we do not recommend using multiple data source initialization technologies, if you want script-based DataSource initialization to be able to build upon the schema creation performed by Hibernate, set spring.jpa.defer-datasource-initialization to true."
95,This will defer data source initialization until after any EntityManagerFactory beans have been created and initialized.
95,schema.sql can then be used to make additions to any schema creation performed by Hibernate and data.sql can be used to populate it.
95,The initialization scripts support -- for single line comments and /* */ for block comments.
95,Other comment formats are not supported.
95,"If you are using a Higher-level Database Migration Tool, like Flyway or Liquibase, you should use them alone to create and initialize the schema."
95,Using the basic schema.sql and data.sql scripts alongside Flyway or Liquibase is not recommended and support will be removed in a future release.
95,"If you need to initialize test data using a higher-level database migration tool, please see the sections about Flyway and Liquibase."
95,9.4. Initialize a Spring Batch Database
95,"If you use Spring Batch, it comes pre-packaged with SQL initialization scripts for most popular database platforms."
95,Spring Boot can detect your database type and execute those scripts on startup.
95,"If you use an embedded database, this happens by default."
95,"You can also enable it for any database type, as shown in the following example:"
95,Properties
95,spring.batch.jdbc.initialize-schema=always
95,Yaml
95,spring:
95,batch:
95,jdbc:
95,"initialize-schema: ""always"""
95,You can also switch off the initialization explicitly by setting spring.batch.jdbc.initialize-schema to never.
95,9.5. Use a Higher-level Database Migration Tool
95,Spring Boot supports two higher-level migration tools: Flyway and Liquibase.
95,9.5.1. Execute Flyway Database Migrations on Startup
95,"To automatically run Flyway database migrations on startup, add the org.flywaydb:flyway-core to your classpath."
95,"Typically, migrations are scripts in the form V<VERSION>__<NAME>.sql (with <VERSION> an underscore-separated version, such as ‘1’ or ‘2_1’)."
95,"By default, they are in a directory called classpath:db/migration, but you can modify that location by setting spring.flyway.locations."
95,This is a comma-separated list of one or more classpath: or filesystem: locations.
95,"For example, the following configuration would search for scripts in both the default classpath location and the /opt/migration directory:"
95,Properties
95,"spring.flyway.locations=classpath:db/migration,filesystem:/opt/migration"
95,Yaml
95,spring:
95,flyway:
95,"locations: ""classpath:db/migration,filesystem:/opt/migration"""
95,You can also add a special {vendor} placeholder to use vendor-specific scripts.
95,Assume the following:
95,Properties
95,spring.flyway.locations=classpath:db/migration/{vendor}
95,Yaml
95,spring:
95,flyway:
95,"locations: ""classpath:db/migration/{vendor}"""
95,"Rather than using db/migration, the preceding configuration sets the directory to use according to the type of the database (such as db/migration/mysql for MySQL)."
95,The list of supported databases is available in DatabaseDriver.
95,Migrations can also be written in Java.
95,Flyway will be auto-configured with any beans that implement JavaMigration.
95,FlywayProperties provides most of Flyway’s settings and a small set of additional properties that can be used to disable the migrations or switch off the location checking.
95,"If you need more control over the configuration, consider registering a FlywayConfigurationCustomizer bean."
95,Spring Boot calls Flyway.migrate() to perform the database migration.
95,"If you would like more control, provide a @Bean that implements FlywayMigrationStrategy."
95,Flyway supports SQL and Java callbacks.
95,"To use SQL-based callbacks, place the callback scripts in the classpath:db/migration directory."
95,"To use Java-based callbacks, create one or more beans that implement Callback."
95,Any such beans are automatically registered with Flyway.
95,They can be ordered by using @Order or by implementing Ordered.
95,"Beans that implement the deprecated FlywayCallback interface can also be detected, however they cannot be used alongside Callback beans."
95,"By default, Flyway autowires the (@Primary) DataSource in your context and uses that for migrations."
95,"If you like to use a different DataSource, you can create one and mark its @Bean as @FlywayDataSource."
95,"If you do so and want two data sources, remember to create another one and mark it as @Primary."
95,"Alternatively, you can use Flyway’s native DataSource by setting spring.flyway.[url,user,password] in external properties."
95,Setting either spring.flyway.url or spring.flyway.user is sufficient to cause Flyway to use its own DataSource.
95,"If any of the three properties has not been set, the value of its equivalent spring.datasource property will be used."
95,You can also use Flyway to provide data for specific scenarios.
95,"For example, you can place test-specific migrations in src/test/resources and they are run only when your application starts for testing."
95,"Also, you can use profile-specific configuration to customize spring.flyway.locations so that certain migrations run only when a particular profile is active."
95,"For example, in application-dev.properties, you might specify the following setting:"
95,Properties
95,"spring.flyway.locations=classpath:/db/migration,classpath:/dev/db/migration"
95,Yaml
95,spring:
95,flyway:
95,"locations: ""classpath:/db/migration,classpath:/dev/db/migration"""
95,"With that setup, migrations in dev/db/migration run only when the dev profile is active."
95,9.5.2. Execute Liquibase Database Migrations on Startup
95,"To automatically run Liquibase database migrations on startup, add the org.liquibase:liquibase-core to your classpath."
95,"When you add the org.liquibase:liquibase-core to your classpath, database migrations run by default for both during application startup and before your tests run."
95,"This behavior can be customized by using the spring.liquibase.enabled property, setting different values in the main and test configurations."
95,"It is not possible to use two different ways to initialize the database (for example Liquibase for application startup, JPA for test runs)."
95,"By default, the master change log is read from db/changelog/db.changelog-master.yaml, but you can change the location by setting spring.liquibase.change-log."
95,"In addition to YAML, Liquibase also supports JSON, XML, and SQL change log formats."
95,"By default, Liquibase autowires the (@Primary) DataSource in your context and uses that for migrations."
95,"If you need to use a different DataSource, you can create one and mark its @Bean as @LiquibaseDataSource."
95,"If you do so and you want two data sources, remember to create another one and mark it as @Primary."
95,"Alternatively, you can use Liquibase’s native DataSource by setting spring.liquibase.[driver-class-name,url,user,password] in external properties."
95,Setting either spring.liquibase.url or spring.liquibase.user is sufficient to cause Liquibase to use its own DataSource.
95,"If any of the three properties has not been set, the value of its equivalent spring.datasource property will be used."
95,"See LiquibaseProperties for details about available settings such as contexts, the default schema, and others."
95,9.5.3. Use Flyway for test-only migrations
95,"If you want to create Flyway migrations which populate your test database, place them in src/test/resources/db/migration."
95,"A file named, for example, src/test/resources/db/migration/V9999__test-data.sql will be executed after your production migrations and only if you’re running the tests."
95,You can use this file to create the needed test data.
95,This file will not be packaged in your uber jar or your container.
95,9.5.4. Use Liquibase for test-only migrations
95,"If you want to create Liquibase migrations which populate your test database, you have to create a test changelog which also includes the production changelog."
95,"First, you need to configure Liquibase to use a different changelog when running the tests."
95,One way to do this is to create a Spring Boot test profile and put the Liquibase properties in there.
95,"For that, create a file named src/test/resources/application-test.properties and put the following property in there:"
95,Properties
95,spring.liquibase.change-log=classpath:/db/changelog/db.changelog-test.yaml
95,Yaml
95,spring:
95,liquibase:
95,"change-log: ""classpath:/db/changelog/db.changelog-test.yaml"""
95,This configures Liquibase to use a different changelog when running in the test profile.
95,Now create the changelog file at src/test/resources/db/changelog/db.changelog-test.yaml:
95,databaseChangeLog:
95,- include:
95,file: classpath:/db/changelog/db.changelog-master.yaml
95,- changeSet:
95,"runOrder: ""last"""
95,"id: ""test"""
95,changes:
95,# Insert your changes here
95,This changelog will be used when the tests are run and it will not be packaged in your uber jar or your container.
95,"It includes the production changelog and then declares a new changeset, whose runOrder: last setting specifies that it runs after all the production changesets have been run."
95,You can now use for example the insert changeset to insert data or the sql changeset to execute SQL directly.
95,The last thing to do is to configure Spring Boot to activate the test profile when running tests.
95,"To do this, you can add the @ActiveProfiles(""test"") annotation to your @SpringBootTest annotated test classes."
95,9.6. Depend Upon an Initialized Database
95,Database initialization is performed while the application is starting up as part of application context refresh.
95,"To allow an initialized database to be accessed during startup, beans that act as database initializers and beans that require that database to have been initialized are detected automatically."
95,Beans whose initialization depends upon the database having been initialized are configured to depend upon those that initialize it.
95,"If, during startup, your application tries to access the database and it has not been initialized, you can configure additional detection of beans that initialize the database and require the database to have been initialized."
95,9.6.1. Detect a Database Initializer
95,Spring Boot will automatically detect beans of the following types that initialize an SQL database:
95,DataSourceScriptDatabaseInitializer
95,EntityManagerFactory
95,Flyway
95,FlywayMigrationInitializer
95,R2dbcScriptDatabaseInitializer
95,SpringLiquibase
95,"If you are using a third-party starter for a database initialization library, it may provide a detector such that beans of other types are also detected automatically."
95,"To have other beans be detected, register an implementation of DatabaseInitializerDetector in META-INF/spring.factories."
95,9.6.2. Detect a Bean That Depends On Database Initialization
95,Spring Boot will automatically detect beans of the following types that depends upon database initialization:
95,AbstractEntityManagerFactoryBean (unless spring.jpa.defer-datasource-initialization is set to true)
95,DSLContext (jOOQ)
95,EntityManagerFactory (unless spring.jpa.defer-datasource-initialization is set to true)
95,JdbcClient
95,JdbcOperations
95,NamedParameterJdbcOperations
95,"If you are using a third-party starter data access library, it may provide a detector such that beans of other types are also detected automatically."
95,"To have other beans be detected, register an implementation of DependsOnDatabaseInitializationDetector in META-INF/spring.factories."
95,"Alternatively, annotate the bean’s class or its @Bean method with @DependsOnDatabaseInitialization."
95,10. NoSQL
95,Spring Boot offers a number of starters that support NoSQL technologies.
95,This section answers questions that arise from using NoSQL with Spring Boot.
95,10.1. Use Jedis Instead of Lettuce
95,"By default, the Spring Boot starter (spring-boot-starter-data-redis) uses Lettuce."
95,You need to exclude that dependency and include the Jedis one instead.
95,"Spring Boot manages both of these dependencies, allowing you to switch to Jedis without specifying a version."
95,The following example shows how to accomplish this in Maven:
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-data-redis</artifactId>
95,<exclusions>
95,<exclusion>
95,<groupId>io.lettuce</groupId>
95,<artifactId>lettuce-core</artifactId>
95,</exclusion>
95,</exclusions>
95,</dependency>
95,<dependency>
95,<groupId>redis.clients</groupId>
95,<artifactId>jedis</artifactId>
95,</dependency>
95,The following example shows how to accomplish this in Gradle:
95,dependencies {
95,implementation('org.springframework.boot:spring-boot-starter-data-redis') {
95,"exclude group: 'io.lettuce', module: 'lettuce-core'"
95,implementation 'redis.clients:jedis'
95,// ...
95,11. Messaging
95,Spring Boot offers a number of starters to support messaging.
95,This section answers questions that arise from using messaging with Spring Boot.
95,11.1. Disable Transacted JMS Session
95,"If your JMS broker does not support transacted sessions, you have to disable the support of transactions altogether."
95,"If you create your own JmsListenerContainerFactory, there is nothing to do, since, by default it cannot be transacted."
95,"If you want to use the DefaultJmsListenerContainerFactoryConfigurer to reuse Spring Boot’s default, you can disable transacted sessions, as follows:"
95,Java
95,import jakarta.jms.ConnectionFactory;
95,import org.springframework.boot.autoconfigure.jms.DefaultJmsListenerContainerFactoryConfigurer;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.jms.config.DefaultJmsListenerContainerFactory;
95,@Configuration(proxyBeanMethods = false)
95,public class MyJmsConfiguration {
95,@Bean
95,"public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(ConnectionFactory connectionFactory,"
95,DefaultJmsListenerContainerFactoryConfigurer configurer) {
95,DefaultJmsListenerContainerFactory listenerFactory = new DefaultJmsListenerContainerFactory();
95,"configurer.configure(listenerFactory, connectionFactory);"
95,listenerFactory.setTransactionManager(null);
95,listenerFactory.setSessionTransacted(false);
95,return listenerFactory;
95,Kotlin
95,import jakarta.jms.ConnectionFactory
95,import org.springframework.boot.autoconfigure.jms.DefaultJmsListenerContainerFactoryConfigurer
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.jms.config.DefaultJmsListenerContainerFactory
95,@Configuration(proxyBeanMethods = false)
95,class MyJmsConfiguration {
95,@Bean
95,"fun jmsListenerContainerFactory(connectionFactory: ConnectionFactory?,"
95,configurer: DefaultJmsListenerContainerFactoryConfigurer): DefaultJmsListenerContainerFactory {
95,val listenerFactory = DefaultJmsListenerContainerFactory()
95,"configurer.configure(listenerFactory, connectionFactory)"
95,listenerFactory.setTransactionManager(null)
95,listenerFactory.setSessionTransacted(false)
95,return listenerFactory
95,"The preceding example overrides the default factory, and it should be applied to any other factory that your application defines, if any."
95,12. Batch Applications
95,A number of questions often arise when people use Spring Batch from within a Spring Boot application.
95,This section addresses those questions.
95,12.1. Specifying a Batch Data Source
95,"By default, batch applications require a DataSource to store job details."
95,Spring Batch expects a single DataSource by default.
95,"To have it use a DataSource other than the application’s main DataSource, declare a DataSource bean, annotating its @Bean method with @BatchDataSource."
95,"If you do so and want two data sources, remember to mark the other one @Primary."
95,"To take greater control, add @EnableBatchProcessing to one of your @Configuration classes or extend DefaultBatchConfiguration."
95,See the Javadoc of @EnableBatchProcessing
95,and DefaultBatchConfiguration for more details.
95,"For more info about Spring Batch, see the Spring Batch project page."
95,12.2. Running Spring Batch Jobs on Startup
95,Spring Batch auto-configuration is enabled by adding spring-boot-starter-batch to your application’s classpath.
95,"If a single Job bean is found in the application context, it is executed on startup (see JobLauncherApplicationRunner for details)."
95,"If multiple Job beans are found, the job that should be executed must be specified using spring.batch.job.name."
95,"To disable running a Job found in the application context, set the spring.batch.job.enabled to false."
95,See BatchAutoConfiguration for more details.
95,12.3. Running From the Command Line
95,"Spring Boot converts any command line argument starting with -- to a property to add to the Environment, see accessing command line properties."
95,This should not be used to pass arguments to batch jobs.
95,"To specify batch arguments on the command line, use the regular format (that is without --), as shown in the following example:"
95,$ java -jar myapp.jar someParameter=someValue anotherParameter=anotherValue
95,"If you specify a property of the Environment on the command line, it is ignored by the job."
95,Consider the following command:
95,$ java -jar myapp.jar --server.port=7070 someParameter=someValue
95,This provides only one argument to the batch job: someParameter=someValue.
95,12.4. Restarting a stopped or failed Job
95,"To restart a failed Job, all parameters (identifying and non-identifying) must be re-specified on the command line."
95,Non-identifying parameters are not copied from the previous execution.
95,This allows them to be modified or removed.
95,"When you’re using a custom JobParametersIncrementer, you have to gather all parameters managed by the incrementer to restart a failed execution."
95,12.5. Storing the Job Repository
95,Spring Batch requires a data store for the Job repository.
95,"If you use Spring Boot, you must use an actual database."
95,"Note that it can be an in-memory database, see Configuring a Job Repository."
95,13. Actuator
95,Spring Boot includes the Spring Boot Actuator.
95,This section answers questions that often arise from its use.
95,13.1. Change the HTTP Port or Address of the Actuator Endpoints
95,"In a standalone application, the Actuator HTTP port defaults to the same as the main HTTP port."
95,"To make the application listen on a different port, set the external property: management.server.port."
95,"To listen on a completely different network address (such as when you have an internal network for management and an external one for user applications), you can also set management.server.address to a valid IP address to which the server is able to bind."
95,"For more detail, see the ManagementServerProperties source code and “actuator.html” in the “Production-ready features” section."
95,13.2. Customize the ‘whitelabel’ Error Page
95,Spring Boot installs a ‘whitelabel’ error page that you see in a browser client if you encounter a server error (machine clients consuming JSON and other media types should see a sensible response with the right error code).
95,Set server.error.whitelabel.enabled=false to switch the default error page off.
95,Doing so restores the default of the servlet container that you are using.
95,"Note that Spring Boot still tries to resolve the error view, so you should probably add your own error page rather than disabling it completely."
95,Overriding the error page with your own depends on the templating technology that you use.
95,"For example, if you use Thymeleaf, you can add an error.html template."
95,"If you use FreeMarker, you can add an error.ftlh template."
95,"In general, you need a View that resolves with a name of error or a @Controller that handles the /error path."
95,"Unless you replaced some of the default configuration, you should find a BeanNameViewResolver in your ApplicationContext, so a @Bean named error would be one way of doing that."
95,See ErrorMvcAutoConfiguration for more options.
95,See also the section on “Error Handling” for details of how to register handlers in the servlet container.
95,13.3. Customizing Sanitization
95,"To take control over the sanitization, define a SanitizingFunction bean."
95,The SanitizableData with which the function is called provides access to the key and value as well as the PropertySource from which they came.
95,"This allows you to, for example, sanitize every value that comes from a particular property source."
95,Each SanitizingFunction is called in order until a function changes the value of the sanitizable data.
95,13.4. Map Health Indicators to Micrometer Metrics
95,Spring Boot health indicators return a Status type to indicate the overall system health.
95,"If you want to monitor or alert on levels of health for a particular application, you can export these statuses as metrics with Micrometer."
95,"By default, the status codes “UP”, “DOWN”, “OUT_OF_SERVICE” and “UNKNOWN” are used by Spring Boot."
95,"To export these, you will need to convert these states to some set of numbers so that they can be used with a Micrometer Gauge."
95,The following example shows one way to write such an exporter:
95,Java
95,import io.micrometer.core.instrument.Gauge;
95,import io.micrometer.core.instrument.MeterRegistry;
95,import org.springframework.boot.actuate.health.HealthEndpoint;
95,import org.springframework.boot.actuate.health.Status;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyHealthMetricsExportConfiguration {
95,"public MyHealthMetricsExportConfiguration(MeterRegistry registry, HealthEndpoint healthEndpoint) {"
95,// This example presumes common tags (such as the app) are applied elsewhere
95,"Gauge.builder(""health"", healthEndpoint, this::getStatusCode).strongReference(true).register(registry);"
95,private int getStatusCode(HealthEndpoint health) {
95,Status status = health.health().getStatus();
95,if (Status.UP.equals(status)) {
95,return 3;
95,if (Status.OUT_OF_SERVICE.equals(status)) {
95,return 2;
95,if (Status.DOWN.equals(status)) {
95,return 1;
95,return 0;
95,Kotlin
95,import io.micrometer.core.instrument.Gauge
95,import io.micrometer.core.instrument.MeterRegistry
95,import org.springframework.boot.actuate.health.HealthEndpoint
95,import org.springframework.boot.actuate.health.Status
95,import org.springframework.context.annotation.Configuration
95,@Configuration(proxyBeanMethods = false)
95,"class MyHealthMetricsExportConfiguration(registry: MeterRegistry, healthEndpoint: HealthEndpoint) {"
95,init {
95,// This example presumes common tags (such as the app) are applied elsewhere
95,"Gauge.builder(""health"", healthEndpoint) { health ->"
95,getStatusCode(health).toDouble()
95,}.strongReference(true).register(registry)
95,private fun getStatusCode(health: HealthEndpoint) = when (health.health().status) {
95,Status.UP -> 3
95,Status.OUT_OF_SERVICE -> 2
95,Status.DOWN -> 1
95,else -> 0
95,14. Security
95,"This section addresses questions about security when working with Spring Boot, including questions that arise from using Spring Security with Spring Boot."
95,"For more about Spring Security, see the Spring Security project page."
95,14.1. Switch off the Spring Boot Security Configuration
95,"If you define a @Configuration with a SecurityFilterChain bean in your application, this action switches off the default webapp security settings in Spring Boot."
95,14.2. Change the UserDetailsService and Add User Accounts
95,"If you provide a @Bean of type AuthenticationManager, AuthenticationProvider, or UserDetailsService, the default @Bean for InMemoryUserDetailsManager is not created."
95,This means you have the full feature set of Spring Security available (such as various authentication options).
95,The easiest way to add user accounts is by providing your own UserDetailsService bean.
95,14.3. Enable HTTPS When Running behind a Proxy Server
95,Ensuring that all your main endpoints are only available over HTTPS is an important chore for any application.
95,"If you use Tomcat as a servlet container, then Spring Boot adds Tomcat’s own RemoteIpValve automatically if it detects some environment settings, allowing you to rely on the HttpServletRequest to report whether it is secure or not (even downstream of a proxy server that handles the real SSL termination)."
95,"The standard behavior is determined by the presence or absence of certain request headers (x-forwarded-for and x-forwarded-proto), whose names are conventional, so it should work with most front-end proxies."
95,"You can switch on the valve by adding some entries to application.properties, as shown in the following example:"
95,Properties
95,server.tomcat.remoteip.remote-ip-header=x-forwarded-for
95,server.tomcat.remoteip.protocol-header=x-forwarded-proto
95,Yaml
95,server:
95,tomcat:
95,remoteip:
95,"remote-ip-header: ""x-forwarded-for"""
95,"protocol-header: ""x-forwarded-proto"""
95,(The presence of either of those properties switches on the valve.
95,"Alternatively, you can add the RemoteIpValve by customizing the TomcatServletWebServerFactory using a WebServerFactoryCustomizer bean.)"
95,"To configure Spring Security to require a secure channel for all (or some) requests, consider adding your own SecurityFilterChain bean that adds the following HttpSecurity configuration:"
95,Java
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
95,import org.springframework.security.web.SecurityFilterChain;
95,@Configuration
95,public class MySecurityConfig {
95,@Bean
95,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
95,// Customize the application security ...
95,http.requiresChannel((channel) -> channel.anyRequest().requiresSecure());
95,return http.build();
95,Kotlin
95,import org.springframework.context.annotation.Bean
95,import org.springframework.context.annotation.Configuration
95,import org.springframework.security.config.annotation.web.builders.HttpSecurity
95,import org.springframework.security.web.SecurityFilterChain
95,@Configuration
95,class MySecurityConfig {
95,@Bean
95,fun securityFilterChain(http: HttpSecurity): SecurityFilterChain {
95,// Customize the application security ...
95,http.requiresChannel { requests -> requests.anyRequest().requiresSecure() }
95,return http.build()
95,15. Hot Swapping
95,Spring Boot supports hot swapping.
95,This section answers questions about how it works.
95,15.1. Reload Static Content
95,There are several options for hot reloading.
95,"The recommended approach is to use spring-boot-devtools, as it provides additional development-time features, such as support for fast application restarts and LiveReload as well as sensible development-time configuration (such as template caching)."
95,Devtools works by monitoring the classpath for changes.
95,"This means that static resource changes must be ""built"" for the change to take effect."
95,"By default, this happens automatically in Eclipse when you save your changes."
95,"In IntelliJ IDEA, the Make Project command triggers the necessary build."
95,"Due to the default restart exclusions, changes to static resources do not trigger a restart of your application."
95,"They do, however, trigger a live reload."
95,"Alternatively, running in an IDE (especially with debugging on) is a good way to do development (all modern IDEs allow reloading of static resources and usually also allow hot-swapping of Java class changes)."
95,"Finally, the Maven and Gradle plugins can be configured (see the addResources property) to support running from the command line with reloading of static files directly from source."
95,You can use that with an external css/js compiler process if you are writing that code with higher-level tools.
95,15.2. Reload Templates without Restarting the Container
95,Most of the templating technologies supported by Spring Boot include a configuration option to disable caching (described later in this document).
95,"If you use the spring-boot-devtools module, these properties are automatically configured for you at development time."
95,15.2.1. Thymeleaf Templates
95,"If you use Thymeleaf, set spring.thymeleaf.cache to false."
95,See ThymeleafAutoConfiguration for other Thymeleaf customization options.
95,15.2.2. FreeMarker Templates
95,"If you use FreeMarker, set spring.freemarker.cache to false."
95,See FreeMarkerAutoConfiguration for other FreeMarker customization options.
95,15.2.3. Groovy Templates
95,"If you use Groovy templates, set spring.groovy.template.cache to false."
95,See GroovyTemplateAutoConfiguration for other Groovy customization options.
95,15.3. Fast Application Restarts
95,The spring-boot-devtools module includes support for automatic application restarts.
95,While not as fast as technologies such as JRebel it is usually significantly faster than a “cold start”.
95,You should probably give it a try before investigating some of the more complex reload options discussed later in this document.
95,"For more details, see the using.html section."
95,15.4. Reload Java Classes without Restarting the Container
95,"Many modern IDEs (Eclipse, IDEA, and others) support hot swapping of bytecode."
95,"Consequently, if you make a change that does not affect class or method signatures, it should reload cleanly with no side effects."
95,16. Testing
95,Spring Boot includes a number of testing utilities and support classes as well as a dedicated starter that provides common test dependencies.
95,This section answers common questions about testing.
95,16.1. Testing With Spring Security
95,Spring Security provides support for running tests as a specific user.
95,"For example, the test in the snippet below will run with an authenticated user that has the ADMIN role."
95,Java
95,import org.junit.jupiter.api.Test;
95,import org.springframework.beans.factory.annotation.Autowired;
95,import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest;
95,import org.springframework.security.test.context.support.WithMockUser;
95,import org.springframework.test.web.servlet.MockMvc;
95,import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;
95,@WebMvcTest(UserController.class)
95,class MySecurityTests {
95,@Autowired
95,private MockMvc mvc;
95,@Test
95,"@WithMockUser(roles = ""ADMIN"")"
95,void requestProtectedUrlWithUser() throws Exception {
95,"this.mvc.perform(get(""/""));"
95,Kotlin
95,import org.junit.jupiter.api.Test
95,import org.springframework.beans.factory.annotation.Autowired
95,import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest
95,import org.springframework.security.test.context.support.WithMockUser
95,import org.springframework.test.web.servlet.MockMvc
95,import org.springframework.test.web.servlet.request.MockMvcRequestBuilders
95,@WebMvcTest(UserController::class)
95,class MySecurityTests(@Autowired val mvc: MockMvc) {
95,@Test
95,"@WithMockUser(roles = [""ADMIN""])"
95,fun requestProtectedUrlWithUser() {
95,"mvc.perform(MockMvcRequestBuilders.get(""/""))"
95,"Spring Security provides comprehensive integration with Spring MVC Test, and this can also be used when testing controllers using the @WebMvcTest slice and MockMvc."
95,"For additional details on Spring Security’s testing support, see Spring Security’s reference documentation."
95,16.2. Structure @Configuration classes for inclusion in slice tests
95,Slice tests work by restricting Spring Framework’s component scanning to a limited set of components based on their type.
95,"For any beans that are not created through component scanning, for example, beans that are created using the @Bean annotation, slice tests will not be able to include/exclude them from the application context."
95,Consider this example:
95,import org.apache.commons.dbcp2.BasicDataSource;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.jdbc.DataSourceBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
95,import org.springframework.security.web.SecurityFilterChain;
95,@Configuration(proxyBeanMethods = false)
95,public class MyConfiguration {
95,@Bean
95,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
95,http.authorizeHttpRequests((requests) -> requests.anyRequest().authenticated());
95,return http.build();
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,public BasicDataSource secondDataSource() {
95,return DataSourceBuilder.create().type(BasicDataSource.class).build();
95,"For a @WebMvcTest for an application with the above @Configuration class, you might expect to have the SecurityFilterChain bean in the application context so that you can test if your controller endpoints are secured properly."
95,"However, MyConfiguration is not picked up by @WebMvcTest’s component scanning filter because it doesn’t match any of the types specified by the filter."
95,You can include the configuration explicitly by annotating the test class with @Import(MyConfiguration.class).
95,This will load all the beans in MyConfiguration including the BasicDataSource bean which isn’t required when testing the web tier.
95,Splitting the configuration class into two will enable importing just the security configuration.
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
95,import org.springframework.security.web.SecurityFilterChain;
95,@Configuration(proxyBeanMethods = false)
95,public class MySecurityConfiguration {
95,@Bean
95,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
95,http.authorizeHttpRequests((requests) -> requests.anyRequest().authenticated());
95,return http.build();
95,import org.apache.commons.dbcp2.BasicDataSource;
95,import org.springframework.boot.context.properties.ConfigurationProperties;
95,import org.springframework.boot.jdbc.DataSourceBuilder;
95,import org.springframework.context.annotation.Bean;
95,import org.springframework.context.annotation.Configuration;
95,@Configuration(proxyBeanMethods = false)
95,public class MyDatasourceConfiguration {
95,@Bean
95,"@ConfigurationProperties(""app.datasource.second"")"
95,public BasicDataSource secondDataSource() {
95,return DataSourceBuilder.create().type(BasicDataSource.class).build();
95,Having a single configuration class can be inefficient when beans of a certain domain need to be included in slice tests.
95,"Instead, structuring the application’s configuration as multiple granular classes with beans for a specific domain can enable importing them only for specific slice tests."
95,17. Build
95,Spring Boot includes build plugins for Maven and Gradle.
95,This section answers common questions about these plugins.
95,17.1. Generate Build Information
95,"Both the Maven plugin and the Gradle plugin allow generating build information containing the coordinates, name, and version of the project."
95,The plugins can also be configured to add additional properties through configuration.
95,"When such a file is present, Spring Boot auto-configures a BuildProperties bean."
95,"To generate build information with Maven, add an execution for the build-info goal, as shown in the following example:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,<version>3.2.3</version>
95,<executions>
95,<execution>
95,<goals>
95,<goal>build-info</goal>
95,</goals>
95,</execution>
95,</executions>
95,</plugin>
95,</plugins>
95,</build>
95,See the Spring Boot Maven Plugin documentation for more details.
95,The following example does the same with Gradle:
95,springBoot {
95,buildInfo()
95,See the Spring Boot Gradle Plugin documentation for more details.
95,17.2. Generate Git Information
95,Both Maven and Gradle allow generating a git.properties file containing information about the state of your git source code repository when the project was built.
95,"For Maven users, the spring-boot-starter-parent POM includes a pre-configured plugin to generate a git.properties file."
95,"To use it, add the following declaration for the Git Commit Id Plugin to your POM:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>io.github.git-commit-id</groupId>
95,<artifactId>git-commit-id-maven-plugin</artifactId>
95,</plugin>
95,</plugins>
95,</build>
95,"Gradle users can achieve the same result by using the gradle-git-properties plugin, as shown in the following example:"
95,plugins {
95,"id ""com.gorylenko.gradle-git-properties"" version ""2.4.1"""
95,Both the Maven and Gradle plugins allow the properties that are included in git.properties to be configured.
95,The commit time in git.properties is expected to match the following format: yyyy-MM-dd’T’HH:mm:ssZ.
95,This is the default format for both plugins listed above.
95,"Using this format lets the time be parsed into a Date and its format, when serialized to JSON, to be controlled by Jackson’s date serialization configuration settings."
95,17.3. Customize Dependency Versions
95,The spring-boot-dependencies POM manages the versions of common dependencies.
95,The Spring Boot plugins for Maven and Gradle allow these managed dependency versions to be customized using build properties.
95,Each Spring Boot release is designed and tested against this specific set of third-party dependencies.
95,Overriding versions may cause compatibility issues.
95,"To override dependency versions with Maven, see this section of the Maven plugin’s documentation."
95,"To override dependency versions in Gradle, see this section of the Gradle plugin’s documentation."
95,17.4. Create an Executable JAR with Maven
95,The spring-boot-maven-plugin can be used to create an executable “fat” JAR.
95,"If you use the spring-boot-starter-parent POM, you can declare the plugin and your jars are repackaged as follows:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,</plugin>
95,</plugins>
95,</build>
95,"If you do not use the parent POM, you can still use the plugin."
95,"However, you must additionally add an <executions> section, as follows:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,<version>3.2.3</version>
95,<executions>
95,<execution>
95,<goals>
95,<goal>repackage</goal>
95,</goals>
95,</execution>
95,</executions>
95,</plugin>
95,</plugins>
95,</build>
95,See the plugin documentation for full usage details.
95,17.5. Use a Spring Boot Application as a Dependency
95,"Like a war file, a Spring Boot application is not intended to be used as a dependency."
95,"If your application contains classes that you want to share with other projects, the recommended approach is to move that code into a separate module."
95,The separate module can then be depended upon by your application and other projects.
95,"If you cannot rearrange your code as recommended above, Spring Boot’s Maven and Gradle plugins must be configured to produce a separate artifact that is suitable for use as a dependency."
95,The executable archive cannot be used as a dependency as the executable jar format packages application classes in BOOT-INF/classes.
95,This means that they cannot be found when the executable jar is used as a dependency.
95,"To produce the two artifacts, one that can be used as a dependency and one that is executable, a classifier must be specified."
95,"This classifier is applied to the name of the executable archive, leaving the default archive for use as a dependency."
95,"To configure a classifier of exec in Maven, you can use the following configuration:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,<configuration>
95,<classifier>exec</classifier>
95,</configuration>
95,</plugin>
95,</plugins>
95,</build>
95,17.6. Extract Specific Libraries When an Executable Jar Runs
95,Most nested libraries in an executable jar do not need to be unpacked in order to run.
95,"However, certain libraries can have problems."
95,"For example, JRuby includes its own nested jar support, which assumes that the jruby-complete.jar is always directly available as a file in its own right."
95,"To deal with any problematic libraries, you can flag that specific nested jars should be automatically unpacked when the executable jar first runs."
95,Such nested jars are written beneath the temporary directory identified by the java.io.tmpdir system property.
95,Care should be taken to ensure that your operating system is configured so that it will not delete the jars that have been unpacked to the temporary directory while the application is still running.
95,"For example, to indicate that JRuby should be flagged for unpacking by using the Maven Plugin, you would add the following configuration:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,<configuration>
95,<requiresUnpack>
95,<dependency>
95,<groupId>org.jruby</groupId>
95,<artifactId>jruby-complete</artifactId>
95,</dependency>
95,</requiresUnpack>
95,</configuration>
95,</plugin>
95,</plugins>
95,</build>
95,17.7. Create a Non-executable JAR with Exclusions
95,"Often, if you have an executable and a non-executable jar as two separate build products, the executable version has additional configuration files that are not needed in a library jar."
95,"For example, the application.yaml configuration file might be excluded from the non-executable JAR."
95,"In Maven, the executable jar must be the main artifact and you can add a classified jar for the library, as follows:"
95,<build>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,</plugin>
95,<plugin>
95,<artifactId>maven-jar-plugin</artifactId>
95,<executions>
95,<execution>
95,<id>lib</id>
95,<phase>package</phase>
95,<goals>
95,<goal>jar</goal>
95,</goals>
95,<configuration>
95,<classifier>lib</classifier>
95,<excludes>
95,<exclude>application.yaml</exclude>
95,</excludes>
95,</configuration>
95,</execution>
95,</executions>
95,</plugin>
95,</plugins>
95,</build>
95,17.8. Remote Debug a Spring Boot Application Started with Maven
95,"To attach a remote debugger to a Spring Boot application that was started with Maven, you can use the jvmArguments property of the maven plugin."
95,See this example for more details.
95,17.9. Build an Executable Archive From Ant without Using spring-boot-antlib
95,"To build with Ant, you need to grab dependencies, compile, and then create a jar or war archive."
95,"To make it executable, you can either use the spring-boot-antlib module or you can follow these instructions:"
95,"If you are building a jar, package the application’s classes and resources in a nested BOOT-INF/classes directory."
95,"If you are building a war, package the application’s classes in a nested WEB-INF/classes directory as usual."
95,Add the runtime dependencies in a nested BOOT-INF/lib directory for a jar or WEB-INF/lib for a war.
95,Remember not to compress the entries in the archive.
95,Add the provided (embedded container) dependencies in a nested BOOT-INF/lib directory for a jar or WEB-INF/lib-provided for a war.
95,Remember not to compress the entries in the archive.
95,Add the spring-boot-loader classes at the root of the archive (so that the Main-Class is available).
95,"Use the appropriate launcher (such as JarLauncher for a jar file) as a Main-Class attribute in the manifest and specify the other properties it needs as manifest entries — principally, by setting a Start-Class property."
95,The following example shows how to build an executable archive with Ant:
95,"<target name=""build"" depends=""compile"">"
95,"<jar destfile=""target/${ant.project.name}-${spring-boot.version}.jar"" compress=""false"">"
95,<mappedresources>
95,"<fileset dir=""target/classes"" />"
95,"<globmapper from=""*"" to=""BOOT-INF/classes/*""/>"
95,</mappedresources>
95,<mappedresources>
95,"<fileset dir=""src/main/resources"" erroronmissingdir=""false""/>"
95,"<globmapper from=""*"" to=""BOOT-INF/classes/*""/>"
95,</mappedresources>
95,<mappedresources>
95,"<fileset dir=""${lib.dir}/runtime"" />"
95,"<globmapper from=""*"" to=""BOOT-INF/lib/*""/>"
95,</mappedresources>
95,"<zipfileset src=""${lib.dir}/loader/spring-boot-loader-jar-${spring-boot.version}.jar"" />"
95,<manifest>
95,"<attribute name=""Main-Class"" value=""org.springframework.boot.loader.launch.JarLauncher"" />"
95,"<attribute name=""Start-Class"" value=""${start-class}"" />"
95,</manifest>
95,</jar>
95,</target>
95,18. Ahead-of-time processing
95,A number of questions often arise when people use the ahead-of-time processing of Spring Boot applications.
95,This section addresses those questions.
95,18.1. Conditions
95,Ahead-of-time processing optimizes the application and evaluates conditions based on the environment at build time.
95,"Profiles are implemented through conditions and are therefore affected, too."
95,"If you want beans that are created based on a condition in an ahead-of-time optimized application, you have to set up the environment when building the application."
95,The beans which are created while ahead-of-time processing at build time are then always created when running the application and can’t be switched off.
95,"To do this, you can set the profiles which should be used when building the application."
95,"For Maven, this works by setting the profiles configuration of the spring-boot-maven-plugin:process-aot execution:"
95,<profile>
95,<id>native</id>
95,<build>
95,<pluginManagement>
95,<plugins>
95,<plugin>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-maven-plugin</artifactId>
95,<executions>
95,<execution>
95,<id>process-aot</id>
95,<configuration>
95,"<profiles>profile-a,profile-b</profiles>"
95,</configuration>
95,</execution>
95,</executions>
95,</plugin>
95,</plugins>
95,</pluginManagement>
95,</build>
95,</profile>
95,"For Gradle, you need to configure the ProcessAot task:"
95,tasks.withType(org.springframework.boot.gradle.tasks.aot.ProcessAot).configureEach {
95,"args('--spring.profiles.active=profile-a,profile-b')"
95,Profiles which only change configuration properties that don’t influence conditions are supported without limitations when running ahead-of-time optimized applications.
95,19. Traditional Deployment
95,Spring Boot supports traditional deployment as well as more modern forms of deployment.
95,This section answers common questions about traditional deployment.
95,19.1. Create a Deployable War File
95,"Because Spring WebFlux does not strictly depend on the servlet API and applications are deployed by default on an embedded Reactor Netty server, War deployment is not supported for WebFlux applications."
95,The first step in producing a deployable war file is to provide a SpringBootServletInitializer subclass and override its configure method.
95,Doing so makes use of Spring Framework’s servlet 3.0 support and lets you configure your application when it is launched by the servlet container.
95,"Typically, you should update your application’s main class to extend SpringBootServletInitializer, as shown in the following example:"
95,Java
95,import org.springframework.boot.SpringApplication;
95,import org.springframework.boot.autoconfigure.SpringBootApplication;
95,import org.springframework.boot.builder.SpringApplicationBuilder;
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
95,@SpringBootApplication
95,public class MyApplication extends SpringBootServletInitializer {
95,@Override
95,protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
95,return application.sources(MyApplication.class);
95,public static void main(String[] args) {
95,"SpringApplication.run(MyApplication.class, args);"
95,Kotlin
95,import org.springframework.boot.autoconfigure.SpringBootApplication
95,import org.springframework.boot.builder.SpringApplicationBuilder
95,import org.springframework.boot.runApplication
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
95,@SpringBootApplication
95,class MyApplication : SpringBootServletInitializer() {
95,override fun configure(application: SpringApplicationBuilder): SpringApplicationBuilder {
95,return application.sources(MyApplication::class.java)
95,fun main(args: Array<String>) {
95,runApplication<MyApplication>(*args)
95,The next step is to update your build configuration such that your project produces a war file rather than a jar file.
95,"If you use Maven and spring-boot-starter-parent (which configures Maven’s war plugin for you), all you need to do is to modify pom.xml to change the packaging to war, as follows:"
95,<packaging>war</packaging>
95,"If you use Gradle, you need to modify build.gradle to apply the war plugin to the project, as follows:"
95,apply plugin: 'war'
95,The final step in the process is to ensure that the embedded servlet container does not interfere with the servlet container to which the war file is deployed.
95,"To do so, you need to mark the embedded servlet container dependency as being provided."
95,"If you use Maven, the following example marks the servlet container (Tomcat, in this case) as being provided:"
95,<dependencies>
95,<!-- ... -->
95,<dependency>
95,<groupId>org.springframework.boot</groupId>
95,<artifactId>spring-boot-starter-tomcat</artifactId>
95,<scope>provided</scope>
95,</dependency>
95,<!-- ... -->
95,</dependencies>
95,"If you use Gradle, the following example marks the servlet container (Tomcat, in this case) as being provided:"
95,dependencies {
95,// ...
95,providedRuntime 'org.springframework.boot:spring-boot-starter-tomcat'
95,// ...
95,providedRuntime is preferred to Gradle’s compileOnly configuration.
95,"Among other limitations, compileOnly dependencies are not on the test classpath, so any web-based integration tests fail."
95,"If you use the Spring Boot build tools, marking the embedded servlet container dependency as provided produces an executable war file with the provided dependencies packaged in a lib-provided directory."
95,"This means that, in addition to being deployable to a servlet container, you can also run your application by using java -jar on the command line."
95,19.2. Convert an Existing Application to Spring Boot
95,"To convert an existing non-web Spring application to a Spring Boot application, replace the code that creates your ApplicationContext and replace it with calls to SpringApplication or SpringApplicationBuilder."
95,Spring MVC web applications are generally amenable to first creating a deployable war application and then migrating it later to an executable war or jar.
95,See the Getting Started Guide on Converting a jar to a war.
95,"To create a deployable war by extending SpringBootServletInitializer (for example, in a class called Application) and adding the Spring Boot @SpringBootApplication annotation, use code similar to that shown in the following example:"
95,Java
95,import org.springframework.boot.SpringApplication;
95,import org.springframework.boot.autoconfigure.SpringBootApplication;
95,import org.springframework.boot.builder.SpringApplicationBuilder;
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
95,@SpringBootApplication
95,public class MyApplication extends SpringBootServletInitializer {
95,@Override
95,protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
95,// Customize the application or call application.sources(...) to add sources
95,// Since our example is itself a @Configuration class (through
95,// @SpringBootApplication)
95,// we actually do not need to override this method.
95,return application;
95,Kotlin
95,import org.springframework.boot.autoconfigure.SpringBootApplication
95,import org.springframework.boot.builder.SpringApplicationBuilder
95,import org.springframework.boot.runApplication
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
95,@SpringBootApplication
95,class MyApplication : SpringBootServletInitializer() {
95,override fun configure(application: SpringApplicationBuilder): SpringApplicationBuilder {
95,// Customize the application or call application.sources(...) to add sources
95,// Since our example is itself a @Configuration class (through @SpringBootApplication)
95,// we actually do not need to override this method.
95,return application
95,"Remember that, whatever you put in the sources is merely a Spring ApplicationContext."
95,"Normally, anything that already works should work here."
95,"There might be some beans you can remove later and let Spring Boot provide its own defaults for them, but it should be possible to get something working before you need to do that."
95,Static resources can be moved to /public (or /static or /resources or /META-INF/resources) in the classpath root.
95,The same applies to messages.properties (which Spring Boot automatically detects in the root of the classpath).
95,Vanilla usage of Spring DispatcherServlet and Spring Security should require no further changes.
95,"If you have other features in your application (for instance, using other servlets or filters), you may need to add some configuration to your Application context, by replacing those elements from the web.xml, as follows:"
95,A @Bean of type Servlet or ServletRegistrationBean installs that bean in the container as if it were a <servlet/> and <servlet-mapping/> in web.xml.
95,A @Bean of type Filter or FilterRegistrationBean behaves similarly (as a <filter/> and <filter-mapping/>).
95,An ApplicationContext in an XML file can be added through an @ImportResource in your Application.
95,"Alternatively, cases where annotation configuration is heavily used already can be recreated in a few lines as @Bean definitions."
95,"Once the war file is working, you can make it executable by adding a main method to your Application, as shown in the following example:"
95,Java
95,public static void main(String[] args) {
95,"SpringApplication.run(MyApplication.class, args);"
95,Kotlin
95,fun main(args: Array<String>) {
95,runApplication<MyApplication>(*args)
95,"If you intend to start your application as a war or as an executable application, you need to share the customizations of the builder in a method that is both available to the SpringBootServletInitializer callback and in the main method in a class similar to the following:"
95,Java
95,import org.springframework.boot.Banner;
95,import org.springframework.boot.autoconfigure.SpringBootApplication;
95,import org.springframework.boot.builder.SpringApplicationBuilder;
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
95,@SpringBootApplication
95,public class MyApplication extends SpringBootServletInitializer {
95,@Override
95,protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
95,return customizerBuilder(builder);
95,public static void main(String[] args) {
95,customizerBuilder(new SpringApplicationBuilder()).run(args);
95,private static SpringApplicationBuilder customizerBuilder(SpringApplicationBuilder builder) {
95,return builder.sources(MyApplication.class).bannerMode(Banner.Mode.OFF);
95,Kotlin
95,import org.springframework.boot.Banner
95,import org.springframework.boot.autoconfigure.SpringBootApplication
95,import org.springframework.boot.builder.SpringApplicationBuilder
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
95,@SpringBootApplication
95,class MyApplication : SpringBootServletInitializer() {
95,override fun configure(builder: SpringApplicationBuilder): SpringApplicationBuilder {
95,return customizerBuilder(builder)
95,companion object {
95,@JvmStatic
95,fun main(args: Array<String>) {
95,customizerBuilder(SpringApplicationBuilder()).run(*args)
95,private fun customizerBuilder(builder: SpringApplicationBuilder): SpringApplicationBuilder {
95,return builder.sources(MyApplication::class.java).bannerMode(Banner.Mode.OFF)
95,Applications can fall into more than one category:
95,Servlet 3.0+ applications with no web.xml.
95,Applications with a web.xml.
95,Applications with a context hierarchy.
95,Applications without a context hierarchy.
95,"All of these should be amenable to translation, but each might require slightly different techniques."
95,Servlet 3.0+ applications might translate pretty easily if they already use the Spring Servlet 3.0+ initializer support classes.
95,"Normally, all the code from an existing WebApplicationInitializer can be moved into a SpringBootServletInitializer."
95,"If your existing application has more than one ApplicationContext (for example, if it uses AbstractDispatcherServletInitializer) then you might be able to combine all your context sources into a single SpringApplication."
95,The main complication you might encounter is if combining does not work and you need to maintain the context hierarchy.
95,See the entry on building a hierarchy for examples.
95,An existing parent context that contains web-specific features usually needs to be broken up so that all the ServletContextAware components are in the child context.
95,"Applications that are not already Spring applications might be convertible to Spring Boot applications, and the previously mentioned guidance may help."
95,"However, you may yet encounter problems."
95,"In that case, we suggest asking questions on Stack Overflow with a tag of spring-boot."
95,19.3. Deploying a WAR to WebLogic
95,"To deploy a Spring Boot application to WebLogic, you must ensure that your servlet initializer directly implements WebApplicationInitializer (even if you extend from a base class that already implements it)."
95,A typical initializer for WebLogic should resemble the following example:
95,Java
95,import org.springframework.boot.autoconfigure.SpringBootApplication;
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
95,import org.springframework.web.WebApplicationInitializer;
95,@SpringBootApplication
95,public class MyApplication extends SpringBootServletInitializer implements WebApplicationInitializer {
95,Kotlin
95,import org.springframework.boot.autoconfigure.SpringBootApplication
95,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
95,import org.springframework.web.WebApplicationInitializer
95,@SpringBootApplication
95,"class MyApplication : SpringBootServletInitializer(), WebApplicationInitializer"
95,"If you use Logback, you also need to tell WebLogic to prefer the packaged version rather than the version that was pre-installed with the server."
95,You can do so by adding a WEB-INF/weblogic.xml file with the following contents:
95,"<?xml version=""1.0"" encoding=""UTF-8""?>"
95,<wls:weblogic-web-app
95,"xmlns:wls=""http://xmlns.oracle.com/weblogic/weblogic-web-app"""
95,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
95,"xsi:schemaLocation=""http://java.sun.com/xml/ns/javaee"
95,https://java.sun.com/xml/ns/javaee/ejb-jar_3_0.xsd
95,http://xmlns.oracle.com/weblogic/weblogic-web-app
95,"https://xmlns.oracle.com/weblogic/weblogic-web-app/1.4/weblogic-web-app.xsd"">"
95,<wls:container-descriptor>
95,<wls:prefer-application-packages>
95,<wls:package-name>org.slf4j</wls:package-name>
95,</wls:prefer-application-packages>
95,</wls:container-descriptor>
95,</wls:weblogic-web-app>
95,20. Docker Compose
95,This section includes topics relating to the Docker Compose support in Spring Boot.
95,20.1. Customizing the JDBC URL
95,"When using JdbcConnectionDetails with Docker Compose, the parameters of the JDBC URL"
95,can be customized by applying the org.springframework.boot.jdbc.parameters label to the
95,service. For example:
95,services:
95,postgres:
95,image: 'postgres:15.3'
95,environment:
95,- 'POSTGRES_USER=myuser'
95,- 'POSTGRES_PASSWORD=secret'
95,- 'POSTGRES_DB=mydb'
95,ports:
95,- '5432:5432'
95,labels:
95,org.springframework.boot.jdbc.parameters: 'ssl=true&sslmode=require'
95,"With this Docker Compose file in place, the JDBC URL used is jdbc:postgresql://127.0.0.1:5432/mydb?ssl=true&sslmode=require."
95,20.2. Sharing services between multiple applications
95,"If you want to share services between multiple applications, create the compose.yaml file in one of the applications and then use the configuration property spring.docker.compose.file in the other applications to reference the compose.yaml file."
95,"You should also set spring.docker.compose.lifecycle-management to start-only, as it defaults to start-and-stop and stopping one application would shut down the shared services for the other still running applications as well."
95,"Setting it to start-only won’t stop the shared services on application stop, but a caveat is that if you shut down all applications, the services remain running."
95,You can stop the services manually by running docker compose stop on the command line in the directory which contains the compose.yaml file.
95,Last updated 2024-02-22 18:33:28 UTC
96,Persistence Property Extensions Reference | EclipseLink 2.6.x Java Persistence API (JPA) Extensions Reference
96,"Java Persistence API (JPA) Extensions Reference for EclipseLink,"
96,Release 2.6
96,Comments
96,5 Persistence Property Extensions Reference
96,This chapter describes the persistence property extensions.You configure persistence units in the JPA persistence descriptor file: persistence.xml. EclipseLink includes many persistence property enhancements and extensions that can be configured in the persistence.xml file.
96,This chapter includes the following sections:
96,Functional Listing of Persistence Property Extensions
96,Alphabetical Listing of Persistence Property Extensions
96,Functional Listing of Persistence Property Extensions
96,"The following lists the EclipseLink persistence property (persistence.xml file) extensions, categorized by function:"
96,Weaving
96,Customizers
96,Validation and Optimization
96,Caching
96,Mapping
96,Schema generation
96,JDBC configuration
96,Concurrency manager
96,Weaving
96,EclipseLink includes the following persistence property extensions for weaving:
96,weaving
96,weaving.changetracking
96,weaving.eager
96,weaving.fetchgroups
96,weaving.internal
96,weaving.lazy
96,Customizers
96,EclipseLink includes the following persistence property extensions for customizing descriptors and sessions:
96,deploy-on-startup
96,descriptor.customizer
96,session.customizer
96,session.include.descriptor.queries
96,session-event-listener
96,session-name
96,sessions-xml
96,target-database
96,target-server
96,metadata-source
96,metadata-source.properties.file
96,metadata-source.send-refresh-command
96,metadata-source.xml.url
96,Validation and Optimization
96,EclipseLink includes the following persistence property extensions for validation.
96,exception-handler
96,partitioning
96,partitioning.callback
96,profiler
96,Logging
96,EclipseLink includes the following persistence property extensions for logging.
96,logging.connection
96,logging.exceptions
96,logging.file
96,logging.level
96,logging.session
96,logging.thread
96,logging.timestamp
96,Caching
96,EclipseLink includes the following persistence property extensions for caching:
96,cache.coordination.channel
96,cache.coordination.jms.factory
96,cache.coordination.jms.host
96,cache.coordination.jms.reuse-topic-publisher
96,cache.coordination.jms.topic
96,cache.coordination.jndi.initial-context-factory
96,cache.coordination.jndi.password
96,cache.coordination.jndi.user
96,cache.coordination.naming-service
96,cache.coordination.propagate-asynchronously
96,cache.coordination.protocol
96,cache.coordination.remove-connection-on-error
96,cache.coordination.rmi.announcement-delay
96,cache.coordination.rmi.multicast-group
96,cache.coordination.rmi.multicast-group
96,cache.coordination.rmi.packet-time-to-live
96,cache.coordination.rmi.url
96,cache.coordination.thread.pool.size
96,cache.database-event-listener
96,cache.shared
96,cache.size
96,cache.type
96,flush-clear.cache
96,Mapping
96,EclipseLink includes the following persistence property extensions for mappings:
96,composite-unit
96,composite-unit.member
96,composite-unit.properties
96,Schema generation
96,EclipseLink includes the following persistence property extensions for mappings:
96,create-ddl-jdbc-file-name
96,ddl.table-creation-suffix
96,ddl-generation
96,ddl-generation.output-mode
96,drop-ddl-jdbc-file-name
96,JDBC configuration
96,EclipseLink includes the following persistence property extensions for configuring JDBC connections and connection pooling:
96,connection-pool
96,connection-pool.read
96,connection-pool.sequence
96,jdbc.allow-native-sql-queries
96,jdbc.batch-writing
96,jdbc.batch-writing.size
96,jdbc.cache-statements
96,jdbc.cache-statements.size
96,jdbc.connector
96,jdbc.exclusive-connection.is-lazy
96,jdbc.exclusive-connection.mode
96,jdbc.native-sql
96,jdbc.property
96,jdbc.sql-cast
96,jdbc.uppercase-columns
96,Concurrency manager
96,EclipseLink includes the following persistence property extensions for concurrency management:
96,concurrency.manager.waittime
96,concurrency.manager.maxsleeptime
96,concurrency.manager.maxfrequencytodumptinymessage
96,concurrency.manager.maxfrequencytodumpmassivemessage
96,concurrency.manager.allow.interruptedexception
96,concurrency.manager.allow.concurrencyexception
96,concurrency.manager.allow.readlockstacktrace
96,Alphabetical Listing of Persistence Property Extensions
96,"The following lists the EclipseLink persistence property (persitence.xml file) extensions, in alphabetical order:"
96,application-location
96,cache.coordination.channel
96,cache.coordination.jms.factory
96,cache.coordination.jms.host
96,cache.coordination.jms.reuse-topic-publisher
96,cache.coordination.jms.topic
96,cache.coordination.jndi.initial-context-factory
96,cache.coordination.jndi.password
96,cache.coordination.jndi.user
96,cache.coordination.naming-service
96,cache.coordination.propagate-asynchronously
96,cache.coordination.protocol
96,cache.coordination.remove-connection-on-error
96,cache.coordination.rmi.announcement-delay
96,cache.coordination.rmi.multicast-group
96,cache.coordination.rmi.multicast-group
96,cache.coordination.rmi.packet-time-to-live
96,cache.coordination.rmi.url
96,cache.coordination.thread.pool.size
96,cache.database-event-listener
96,cache.shared
96,cache.size
96,cache.type
96,classloader
96,composite-unit
96,composite-unit.member
96,composite-unit.properties
96,concurrency.manager.waittime
96,concurrency.manager.maxsleeptime
96,concurrency.manager.maxfrequencytodumptinymessage
96,concurrency.manager.maxfrequencytodumpmassivemessage
96,concurrency.manager.allow.interruptedexception
96,concurrency.manager.allow.concurrencyexception
96,concurrency.manager.allow.readlockstacktrace
96,connection-pool
96,connection-pool.read
96,connection-pool.sequence
96,create-ddl-jdbc-file-name
96,ddl.table-creation-suffix
96,ddl-generation
96,ddl-generation.output-mode
96,ddl.table-creation-suffix
96,deploy-on-startup
96,descriptor.customizer
96,drop-ddl-jdbc-file-name
96,exception-handler
96,exclude-eclipselink-orm
96,flush-clear.cache
96,id-validation
96,jdbc.allow-native-sql-queries
96,jdbc.batch-writing
96,jdbc.batch-writing.size
96,jdbc.cache-statements
96,jdbc.cache-statements.size
96,jdbc.connector
96,jdbc.exclusive-connection.is-lazy
96,jdbc.exclusive-connection.mode
96,jdbc.native-sql
96,jdbc.property
96,jdbc.sql-cast
96,jdbc.uppercase-columns
96,jpa.uppercase-column-names
96,jpql.parser
96,jpql.validation
96,logging.connection
96,logging.exceptions
96,logging.file
96,logging.level
96,logging.session
96,logging.thread
96,logging.timestamp
96,metadata-source
96,metadata-source.properties.file
96,metadata-source.send-refresh-command
96,metadata-source.xml.url
96,nosql.connection-factory
96,nosql.connection-spec
96,nosql.property
96,oracle.proxy-type
96,orm.throw.exceptions
96,orm.validate.schema
96,partitioning
96,partitioning.callback
96,persistence-context.close-on-commit
96,persistence-context.commit-without-persist-rules
96,persistence-context.flush-mode
96,persistence-context.persist-on-commit
96,persistence-context.reference-mode
96,persistenceunits
96,persistencexml
96,persisencexml.default
96,profiler
96,session.customizer
96,session.include.descriptor.queries
96,session-event-listener
96,session-name
96,sessions-xml
96,target-database
96,target-server
96,temporal.mutable
96,tenant-id
96,transaction.join-existing
96,tuning
96,validate-existence
96,validation-only
96,weaving
96,weaving.changetracking
96,weaving.eager
96,weaving.fetchgroups
96,weaving.internal
96,weaving.lazy
96,application-location
96,Use the eclipselink.application-location property to specify the file system directory in which EclipseLink writes (outputs) DDL files.
96,Values
96,Table 5-1 describes this persistence property's values.
96,Table 5-1 Valid Values for application-location
96,Value
96,Description
96,value
96,"Directory location. The path must be fully qualified. For Windows, use a backslash. For UNIX use a slash."
96,Usage
96,You may set this option only if the value of eclipselink.ddl-generation.output-mode is sql-script or both.
96,Examples
96,Example 5-1 shows how to use this property in the persistence.xml file.
96,Example 5-1 Using application-location in persistence.xml
96,"<property name=""eclipselink.application-location"" value=""c:/YOURDIRECTORY/""/>"
96,Example 5-2 shows how to use this property in a property map.
96,Example 5-2 Using application-location in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.APPLICATION_LOCATION,"
96,"""c:/YOURDIRECTORY/"");"
96,See Also
96,"For more information, see:"
96,"""ddl-generation.output-mode"""
96,cache.coordination.channel
96,Use the eclipselink.cache.coordination.channel property to configure cache coordination for a clustered environment.
96,Values
96,Table 5-2 describes this persistence property's values.
96,Table 5-2 Valid Values for cache.coordination.channel
96,Value
96,Description
96,channel name
96,The channel used for cache coordination. All persistence units using the same channel will be coordinated.
96,Default: EclipseLinkCommandChannel
96,Usage
96,"If multiple EclipseLink deployments reside on the same network, they should be in different channels."
96,Examples
96,Example 5-3 shows how to use this property in the persistence.xml file.
96,Example 5-3 Using application-location in persistence.xml
96,"<property name=""eclipselink.cache.coordination.channel"" value=""EmployeeChannel"" />"
96,Example 5-4 shows how to use this property in a property map.
96,Example 5-4 Using cache.coordination.channel in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.CACHE_COORDINATION_CHANNEL,"
96,"""myChannel"");"
96,See Also
96,"For more information, see:"
96,"""@Cache"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jms.factory
96,"Use the eclipselink.cache.coordination.jms.factory property to configure the JMS topic connection factory name, when using JMS coordination for a clustered environment."
96,Values
96,Table 5-3 describes this persistence property's values.
96,Table 5-3 Valid Values for cache.coordination.jms.factory
96,Value
96,Description
96,name
96,The JMS topic connection factory name.
96,Default: jms/EclipseLinkTopicConnectionFactory
96,Usage
96,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
96,Examples
96,See Example 5-13 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jms.host
96,"Use the eclipselink.cache.coordination.jms.host property to configure the URL of the JMS server that hosts the topic, when using JMS coordination for a clustered environment."
96,Values
96,Table 5-4 describes this persistence property's values.
96,Table 5-4 Valid Values for cache.coordination.jms.host
96,Value
96,Description
96,url
96,The fully-qualified URL for the JMS server.
96,"This is not required if the topic is distributed across the cluster (that is, it can be looked up in local JNDI)."
96,Usage
96,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms). You must use a fully qualified URL.
96,Examples
96,See Example 5-13 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jms.reuse-topic-publisher
96,Use the eclipselink.cache.coordination.jms.reuse-topic-publisher property to specify if the JSM transport manager should cache a TopicPubliser and reuse it for all cache coordination publishing.
96,Values
96,Table 5-5 describes this persistence property's values.
96,Table 5-5 Valid Values for cache.coordination.jms.reuse-topic-publisher
96,Value
96,Description
96,true
96,Caches the topic publisher.
96,false
96,(Default) Does not cache the topic publisher.
96,Usage
96,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
96,Examples
96,See Example 5-13 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jms.topic
96,"Use the eclipselink.cache.coordination.jms.topic property to set the JMS topic name, when using JMS coordination for a clustered environment."
96,Values
96,Table 5-6 describes this persistence property's values.
96,Table 5-6 Valid Values for cache.coordination.jms.topic
96,Value
96,Description
96,name
96,Set the JMS topic name.
96,Default: jms/EclipseLinkTopic
96,Usage
96,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
96,Examples
96,See Example 5-13 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jndi.initial-context-factory
96,"Use the eclipselink.cache.coordination.jndi.initial-context-factory property to set the JNDI InitialContext factory, when using cache coordination for a clustered environment."
96,Values
96,Table 5-7 describes this persistence property's values.
96,Table 5-7 Valid Values for cache.coordination.jndi.initial-context-factory
96,Value
96,Description
96,name
96,Name of the JNDI InitialContext factory.
96,Usage
96,"Normally, you will not need this property when connecting to the local server."
96,Examples
96,Example 5-5 shows how to use this property in the persistence.xml file.
96,Example 5-5 Using cache.coordination.jndi.initial-context-factory in persistence.xml.
96,"<property name=""eclipselink.cache.coordination.jndi.initial-context-factory"""
96,"value=""weblogic.jndi.WLInitialContextFactory/>"
96,Example 5-6 shows how to use this property in a property map.
96,Example 5-6 Using cache.coordination.jndi.initial-context-factory in a property map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertyMap.put
96,"(PersistenceUnitProperties.CACEH_COORDINATION_JNDI_INITIAL_CONTEXT_FACTORY,"
96,"""weblogic.jndi.WLInitialContextFactory"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jndi.password
96,"Use the eclipselink.cache.coordination.jndi.password property to set the password for the cache.coordination.jndi.user, when using cache coordination for a clustered environment."
96,Values
96,Table 5-8 describes this persistence property's values.
96,Table 5-8 Valid Values for cache.coordination.jndi.password
96,Value
96,Description
96,value
96,Password for the cache.coordination.jndi.user.
96,Usage
96,"Normally, you will not need this property when connecting to the local server."
96,Examples
96,Example 5-7 shows how to use this propery in the persistence.xml file.
96,Example 5-7 Using cache.coordination.jndi.password in persistence.xml
96,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""USERNAME""/>"
96,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""PASSWORD""/>"
96,Example 5-8 shows how to use this property in a property map.
96,Example 5-8 Using cache.coordination.jndi.password in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_JNDI_USER,"
96,"""USERNAME"");"
96,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_JNDI_PASSWORD,"
96,"""PASSWORD"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.jndi.user"""
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.jndi.user
96,"Use the eclipselink.cache.coordination.jndi.user property to set JNDI naming service user, when using cache coordination for a clustered environment."
96,Values
96,Table 5-9 describes this persistence property's values.
96,Table 5-9 Valid Values for cache.coordination.jndi.user
96,Value
96,Description
96,value
96,The JNDI user.
96,Usage
96,"Normally, you will not need this property when connecting to the local server."
96,Examples
96,See Example 5-13 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.jndi.password"""
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.naming-service
96,"Use the eclipselink.cache.coordination.naming-service property to specify the naming service to use, when using cache coordination for a clustered environment."
96,Values
96,Table 5-10 describes this persistence property's values.
96,Table 5-10 Valid Values for cache.coordination.naming-service
96,Value
96,Description
96,jndi
96,Uses JNDI.
96,rmi
96,Configures RMI.
96,Usage
96,Cache coordination must be enabled.
96,Examples
96,Example 5-9 shows how to use this property in the persistence.xml file.
96,Example 5-9 Using cache.coordination.naming-service in persistence.xml
96,"<property name=""eclipselink.cache.coordination"" value=""true""/>"
96,"<property name=""eclipselink.cache.coordination.naming-service"" value=""jndi""/>"
96,Example 5-10 shows how to use this property in a property map.
96,Example 5-10 Using cache.coordination.naming-service in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_NAMING_SERVICE,"
96,"""jndi"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.propagate-asynchronously
96,Use the eclipselink.cache.coordination.propagate-asynchronously property to specify if the coordination broadcast should occur asynchronously with the committing thread.
96,The property configures cache coordination for a clustered environment. Set if the coordination broadcast should occur asynchronously with the committing thread. This means the coordination will be complete before the thread returns from the commit of the transaction.
96,Values
96,Table 5-11 describes this persistence property's values.
96,Table 5-11 Valid Values for cache.coordination.propagate-asynchronously
96,Value
96,Description
96,true
96,(Default) EclipseLink will broadcast asynchronously. The coordination will be complete before the thread returns from the committing the transaction.
96,false
96,EclipseLink will broadcast synchronously.
96,Usage
96,"JMS cache coordination is always asynchronous, regardless of this setting."
96,"By default, RMI cache coordination is asynchronous. Use synchronous (eclipselink.cache.coordination.propagate-asynchronously = false) to ensure that all servers are updated before the request returns."
96,Examples
96,Example 5-11 shows how to use this property in the persistence.xml file.
96,Example 5-11 Using cache.coordination.propagate-asynchronously in persistence.xml
96,"<property name=""eclipselink.cache.coordination.propagate-asynchronously"""
96,"value=""false"" />"
96,Example 5-12 shows how to use this property in a property map.
96,Example 5-12 Using cache.coordination.propagate-asynchronously in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertyMap.put
96,"(PersistenceUnitProperties.CACHE_COORDINATION_PROPAGATE_ASYNCHRONOUSLY,"
96,"""false"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.protocol
96,"Use the eclipselink.cache.coordination.protocol property to specify the cache coordination protocol to use. Depending on the cache configuration for each descriptor, this will broadcast cache updates or inserts to the cluster to update or invalidate each session's cache."
96,Values
96,Table 5-12 describes this persistence property's values.
96,Table 5-12 Valid Values for cache.coordination.protocol
96,Value
96,Description
96,jms
96,Use Java Message Service (JMS) to broadcast changes.
96,jms-publishing
96,Use an EJB MessageDrivenBean to be used to broadcast changes. You must configure the MessageDrivenBean separately.
96,rmi
96,Use Java Remote Method Invocation (RMI) to broadcast changes.
96,rmi-iiop
96,Use RMI over the Internet Inter-Orb Protocol (IIOP) to broadcast changes.
96,ClassName
96,The name of a subclass implementation of the TransportManager abstract class
96,Usage
96,You must specify the cache.coordination.protocol for every persistence unit and session in the cluster.
96,Examples
96,Example 5-13 shows how configure JMS cache coordination in the persistence.xml file.
96,Example 5-13 Configuring JMS Cache Coordination in persistence.xml
96,"<property name=""eclipselink.cache.coordination.protocol"" value=""jms"" />"
96,"<property name=""eclipselink.cache.coordination.jms.topic"""
96,"value=""jms/EmployeeTopic"" />"
96,"<property name=""eclipselink.cache.coordination.jms.factory"""
96,"value=""jms/EmployeeTopicConnectionFactory"" />"
96,"If your application is not running in a cluster, you must provide the URL:"
96,"<property name=""eclipselink.cache.coordination.jms.host"""
96,"value=""t3://myserver:7001/"" />"
96,"You can also include a username and password, if required, to access the server (for example, if on a separate domain):"
96,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""weblogic"" />"
96,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""welcome1"" />"
96,Example 5-14 shows how to configure RMI cache coordination in the persistence.xml file.
96,Example 5-14 Configuring RMI Cache Coordination in persistence.xml
96,"<property name=""eclipselink.cache.coordination.protocol"" value=""rmi"" />"
96,"If your application is not running in a cluster, you must provide the URL:"
96,"<property name=""eclipselink.cache.coordination.rmi.url"""
96,"value=""t3://myserver:7001/"" />"
96,"You can also include a username and password, if required, to access the server (for example, if on a separate domain):"
96,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""weblogic"" />"
96,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""welcome1"" />"
96,"By default, RMI cache coordination broadcasts are asynchronous. You can override this, if needed:"
96,"<property name=""eclipselink.cache.coordination.propagate-asynchronously"""
96,"value=""false"" />"
96,"If you have multiple applications on the same server or network, you can specify a separate cache coordination channel for each application:"
96,"<property name=""eclipselink.cache.coordination.channel"" value=""EmployeeChannel"" />"
96,"RMI cache coordination uses a multicast socket to allow servers to find each other. You can configure the multicast settings, if needed:"
96,"<property name=""eclipselink.cache.coordination.rmi.announcement-delay"""
96,"value=""1000"" />"
96,"<property name=""eclipselink.cache.coordination.rmi.multicast-group"""
96,"value=""239.192.0.0"" />"
96,"<property name=""eclipselink.cache.coordination.rmi.multicast-group.port"""
96,"value=""3121"" />"
96,"<property name=""eclipselink.cache.coordination.packet-time-to-live"" value=""2"" />"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.channel"""
96,"""cache.coordination.jms.factory"""
96,"""cache.coordination.jms.host"""
96,"cache.coordination.jms.reuse-topic-publisher""cache.coordination.jms.reuse-topic-publisher"""
96,"""cache.coordination.jms.topic"""
96,"""cache.coordination.jndi.initial-context-factory"""
96,"""cache.coordination.jndi.password"""
96,"""cache.coordination.jndi.user"""
96,"""cache.coordination.naming-service"""
96,"""cache.coordination.propagate-asynchronously"""
96,"""cache.coordination.remove-connection-on-error"""
96,"""cache.coordination.rmi.announcement-delay"""
96,"""cache.coordination.rmi.multicast-group"""
96,"""cache.coordination.rmi.multicast-group"""
96,"""cache.coordination.rmi.packet-time-to-live"""
96,"""cache.coordination.rmi.url"""
96,"""cache.coordination.thread.pool.size"""
96,Cache Coordination Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
96,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
96,cache.coordination.remove-connection-on-error
96,Use the eclipselink.cache.coordination.remove-connection-on-error property to specify if the connection should be removed if EclipseLink encounters a communication error when coordinating the cache.
96,Values
96,Table 5-13 describes this persistence property's values.
96,Table 5-13 Valid Values for cache.coordination.remove-connection-on-error
96,Value
96,Description
96,true
96,Removes the connection if a communication error occurs. EclipseLink will reconnect when the server becomes available.
96,false
96,(Default) Does not remove the connection if a communication error occurs.
96,Usage
96,"Normally, this is used for RMI connections in the event that a server goes down."
96,Examples
96,Example 5-15 shows how to use this property in the persistence.xml file.
96,Example 5-15 Using cache.coordination.remove-connection-on-error in peristence.xml
96,"<property name=""eclipselink.cache.coordination.remove-connection-on-error"""
96,"value=""true""/>"
96,Example 5-16 shows how to use this property in a property map.
96,Example 5-16 Using cache.coordination.remove-connection-on_error in a property map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertyMap.put
96,"(PersistenceUnitProperties.CACHE_COORDINATION_REMOVE_CONNECTION_ON_ERROR,""true"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.rmi.announcement-delay
96,Use the eclipselink.cache.coordination.rmi.announcement-delay property to set the time (in milliseconds) to wait for announcements from other cluster members on startup.
96,Values
96,Table 5-14 describes this persistence property's values.
96,Table 5-14 Valid Values for cache.coordination.rmi.announcement-delay
96,Value
96,Description
96,Numeric
96,"Time (in milliseconds) to wait for announcements, on startup."
96,Default: 1000
96,Usage
96,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
96,Examples
96,See Example 5-14 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.rmi.multicast-group
96,"Use the eclipselink.cache.coordination.rmi.multicast-group property to set the multicast socket group address (used to find other members of the cluster), when using cache coordination for a clustered environment."
96,Values
96,Table 5-15 describes this persistence property's values.
96,Table 5-15 Valid Values for cache.coordination.rmi.multicast-group
96,Value
96,Description
96,Numeric
96,Set the multicast socket group address
96,Default: 239.192.0.0
96,Usage
96,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
96,Examples
96,See Example 5-14 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.rmi.multicast-group.port
96,"Use the eclipselink.cache.coordination.rmi.multicast-group.port property to set the multicast socket group port (used to find other members of the cluster), when using cache coordination for a clustered environment."
96,Values
96,Table 5-16 describes this persistence property's values.
96,Table 5-16 Valid Values for cache.coordination.rmi.multicast-group.port
96,Value
96,Description
96,Numeric
96,Set the multicast socket group port.
96,Default: 3121
96,Usage
96,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
96,Examples
96,See Example 5-14 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.rmi.packet-time-to-live
96,Use the eclipselink.cache.coordination.rmi.packet-time-to-live property to set the number of hops the session announcement data packets will take before expiring. The multicast group is used to find other members of the cluster.
96,Values
96,Table 5-17 describes this persistence property's values.
96,Table 5-17 Valid Values for cache.coordination.rmi.packet-time-to-live
96,Value
96,Description
96,Numeric
96,Number of hops the session announcement data packets will take before expiring.
96,Default: 2
96,Usage
96,"If sessions are hosted on different LANs that are part of WAN, the announcement sent by one session may not reach other sessions. In this case, consult your network administrator for the correct time-to-live value or test your network by increasing the value until each session receives announcement sent by others."
96,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
96,Examples
96,See Example 5-14 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.rmi.url
96,Use the eclipselink.cache.coordination.rmi.url property to set the URL of the host server. This is the URL that other cluster member use to connect to this host.
96,Values
96,Table 5-18 describes this persistence property's values.
96,Table 5-18 Valid Values for cache.coordination.rmi.url
96,Value
96,Description
96,url
96,URL of the host server
96,Default: local
96,Usage
96,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
96,This may not be required in a clustered environment where JNDI is replicated. You can also set the location as a System property or using a SessionCustomizer to avoid requiring a separate persistence.xml file per server.
96,Examples
96,See Example 5-14 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.coordination.thread.pool.size
96,"Use the eclipselink.cache.coordination.thread.pool.size property to configure the size of the thread pool, for cache coordination threads."
96,Values
96,Table 5-19 describes this persistence property's values.
96,Table 5-19 Valid Values for cache.coordination.thread.pool.size
96,Value
96,Description
96,Numeric
96,"Size of the thread pool. If 0, EclipseLink does not use a thread pool; instead threads are spawned when required."
96,Default: 32
96,Usage
96,"For RMI cache coordination, EclipseLink spawns one thread per node to send change notifications and one thread to listen for new node notifications."
96,"For JMS cache coordination, EclipseLink spawns one thread to receive JMS change notification messages (unless MDB is used) and one thread to process the change notification (unless MDB is used)."
96,Examples
96,Example 5-17 shows how to use this property in the persistence.xml file.
96,Example 5-17 Using cache.coordination.thread.pool.size in persistence.xml
96,"<property name=""eclipselink.cache.coordination.thread.pool.size"""
96,"value=""48""/>"
96,Example 5-18 shows how to use this property in a property map.
96,Example 5-18 Using cache.coordination.thread.pool.size in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_THREAD_POOL_SIZE,"
96,"""48"");"
96,See Also
96,"For more information, see:"
96,"""cache.coordination.protocol"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,cache.database-event-listener
96,"Use the eclipselink.cache.database-event-listener property to integrate EclipseLink with a database event notification service, such as Oracle QCN/DCN (Query Change Notification/Database Change Notification)."
96,Values
96,Table 5-20 describes this persistence property's values.
96,Table 5-20 Valid Values for cache.database-event-listener
96,Value
96,Description
96,Class
96,"The name of a class that implements DatabaseEventListener, such as the OracleChangeNotificationListener (org.eclipse.persistence.platform.database.oracle.dcn.OracleChangeNotificationListener)."
96,You can also use DCN and QCN for Oracle.
96,Usage
96,"You can use this property to allow the EclipseLink cache to be invalidated by database change events, triggers, or other services."
96,Examples
96,Example 5-19 shows how to use this property with Oracle DCN.
96,Example 5-19 Using cache.database-event-listener in persistence.xml
96,"<?xml version=""1.0"" encoding=""UTF-8""?>"
96,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
96,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
96,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence"
96,"persistence_2_0.xsd"""
96,"version=""2.0"">"
96,"<persistence-unit name=""acme"" transaction-type=""RESOURCE_LOCAL"">"
96,<provider>org.eclipse.persistence.jpa.PersistenceProvider</provider>
96,<exclude-unlisted-classes>false</exclude-unlisted-classes>
96,<properties>
96,"<property name=""eclipselink.cache.database-event-listener"" value="
96,"""org.eclipse.persistence.platform.database.oracle.dcn.OracleChangeNotificationList"
96,"ener""/>"
96,</properties>
96,</persistence-unit>
96,</persistence>
96,See Also
96,"For more information, see:"
96,"""@Cache"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,"""Database Change Notification"" in Oracle Fusion Middleware Configuring and Managing JDBC Data Sources for Oracle WebLogic Server"
96,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
96,Cache Coordination Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
96,cache.shared
96,Use the eclipselink.cache.shared property prefix to indicate whether an entity's cache is shared (non-isolated).
96,Values
96,Table 5-21 describes this persistence property prefix's values.
96,Table 5-21 Valid Values for cache.shared
96,Value
96,Description
96,true
96,(Default) Shares an entity's cache. The value is case insensitive.
96,false
96,Prevents sharing of an entity's cache. The value is case insensitive.
96,Usage
96,"Form a property name by appending either a valid entity name or class name to class.shared, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.shared property prefix to form a property name that sets the default for all entities."
96,Examples
96,See Example 2-12 for information on how to use this property.
96,cache.size
96,Use the eclipselink.cache.size property prefix to specify the cache size for a specific entity type.
96,Values
96,Table 5-22 describes this persistence property prefix's values.
96,Table 5-22 Valid Values for cache.size
96,Value
96,Description
96,integer
96,The size of the cache. Default: 100 Bytes.
96,Usage
96,"Form a property name by appending either a valid entity name or class name to cache.size, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.size property prefix, indicating that the property value applies to all entities."
96,"For most cache types, the size is only the initial size, not a fixed or maximum size. For CacheType.SoftCache and CacheType.HardCache types, the size is the sub-cache size. The default cache size is 100 Bytes."
96,Examples
96,See Example 2-12 for information on how to use this property.
96,cache.type
96,Use the eclipselink.cache.type property prefix to set the type of cache.
96,Values
96,Table 5-23 describes this persistence property prefix's values
96,Table 5-23 Valid values for cache.type
96,Value
96,Description
96,Weak
96,"Holds all objects in use by the application, and allows any unreferenced objects to be free for garbage collection. This cache type guarantees object identity and allows optimal garbage collection, but provides little caching benefit."
96,Soft
96,"Holds all objects read by the application, and allows any unreferenced objects to be free for garbage collection only when the JVM decides that memory is low. This cache type guarantees object identity, allows for garbage collection when memory is low, and provides optimal caching benefit."
96,SoftWeak
96,"(Default)Holds all objects read by the application, and a fixed-size subcache of MRU objects using Soft references.The SoftWeak cache allows any unreferenced objects not in the sub-cache to be free for garbage collection. The objects in the sub-cache are free to garbage collect only when the JVM decides that memory is low. This cache type guarantees object identity, allows configurable garbage collection, and provides configurable caching benefit."
96,HardWeak
96,"Holds all objects in use by the application, and a fixed-size subcache of MRU objects using normal Hard references. This type allows any unreferenced objects not in the subcache to be free for garbage collection, but not objects in the subcache. This cache type guarantees object identity, allows configurable garbage collection, and provides configurable caching benefit."
96,Full
96,"Holds all objects read by the application. This cache type does not allow garbage collection. This guarantees object identity, allows no garbage collection, and provides complete caching benefit."
96,"WARNING: Use this cache type only for a fixed number of objects; otherwise, memory leakage will occur eventually."
96,NONE
96,"Does not cache any objects, and frees any unreferenced objects for garbage collection. This provides no object identity, allows complete garbage collection, and provides no caching benefit."
96,"WARNING: This cache type should normally not be used. Instead, disable the shared cache through PersistenceUnitProperties.CACHE_SHARED. Lack of object identity can lead to infinite loops for objects that have circular references and no indirection."
96,Usage
96,"Form a property name by appending a valid entity name or class name to cache.type, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.type prefix to form a property name that sets the default for all entities."
96,Valid values for cache.type properties are declared in the CacheType class. The default is SoftWeak.
96,"If you do not want to cache entities, set the cache.shared property."
96,Examples
96,See Example 2-12 for information about how to use this property.
96,See Also
96,"For more information, see:"
96,cache.shared
96,classloader
96,Use the eclipselink.classloader property to create an EntityMangerFactory in the property map to be passed to Persistence.createEntityManagerFactory.
96,Values
96,Table 5-24 describes this persistence property's values.
96,Table 5-24 Valid Values for classloader
96,Value
96,Description
96,value
96,Classloader to use.
96,Usage
96,"This is a dynamic property that must be set at runtime, in the property map. You cannot configure this property in the persistence.xml file."
96,Examples
96,Example 5-20 shows how to use this property in a property map.
96,Example 5-20 Using classloader in a Property Map
96,"properties.put(""eclipselink.classloader"", this.getClass().getClassLoader());"
96,composite-unit
96,Use the eclipselink.composite-unit property to specify if the persistence unit is a composite persistence unit.
96,Values
96,Table 5-25 describes this persistence property's values.
96,Table 5-25 Valid Values for composite-unit
96,Value
96,Description
96,true
96,Persistence unit is a composite persistence unit.
96,false
96,(Default) Persistence unit is not a composite persistence unit.
96,Usage
96,The property must be specified in persistence.xml of a composite persistence unit. The composite persistence unit must contain all persistence units found in JAR files specified by the persistence.xml file.
96,Note:
96,"If this property is passed to the createEntityManagerFactory method or if it is set in system properties, it is ignored.)"
96,Examples
96,Example 5-21 shows how to use this property in the persistence.xml file.
96,Example 5-21 Using composite-unit in persistence.xml
96,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_1_0.xsd"" version=""1.0"">"
96,"<persistence-unit name=""compositePu"" transaction-type=""JTA"">"
96,<provider>
96,org.eclipse.persistence.jpa.PersistenceProvider
96,</provider>
96,<jar-file>member1.jar</jar-file>
96,<jar-file>member2.jar</jar-file>
96,<properties>
96,"<property name=""eclipselink.composite-unit"" value=""true""/>"
96,"<property name=""eclipselink.target-server"" value=""WebLogic_10""/>"
96,</properties>
96,</persistence-unit>
96,</persistence>
96,See Also
96,"For more information, see:"
96,"""composite-unit.member"""
96,"""composite-unit.properties"""
96,"""Using Multiple Databases with a Composite Persistence Unit"" in Solutions Guide for EclispeLink"
96,"""Composite Persistence Units"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/Composite_Persistence_Units"
96,composite-unit.member
96,Use the eclipselink.composite-unit.member property to specify if the persistence unit is a member composite persistence unit.
96,Values
96,Table 5-26 describes this persistence property's values.
96,Table 5-26 Valid Values for composite-unit.member
96,Value
96,Description
96,true
96,The persistence unit must be a member of a composite persistence unit and cannot be used as an independent persistence unit.
96,false
96,(Default) The persistence unit does not have to be a member of a composite persistence unit.
96,Usage
96,Setting this property to true indicates that the persistence unit has dependencies on other persistence units.
96,Note:
96,"If this property is passed to the createEntityManagerFactory method or if it is set in system properties, it is ignored.)"
96,"If this property is true, you may still create EntityManagerFactory, but it cannot be connected. Any attempt to create an entity manger will cause an exception."
96,Query Hint
96,"When executing a native query on a composite persistence unit, use composite-unit.member to specify the name of the composite member persistence unit on which to execute the query."
96,Examples
96,Example 5-22 shows how to use this property in the persistence.xml file.
96,Example 5-22 Using composite-unit.member in persistence.xml
96,Composite member persistence unit memberPu2 is defined in the member2.jar file. It has dependency on a class defined in member1.jar and cannot be used independently.
96,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
96,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
96,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_1_0.xsd"""
96,"version=""1.0"">"
96,"<persistence-unit name=""memberPu2"">"
96,<provider>
96,org.eclipse.persistence.jpa.PersistenceProvider
96,</provider>
96,<mapping-file>META-INF/advanced-entity-mappings2.xml</mapping-file>
96,<jta-data-source>jdbc/MySqlJtaDS</jta-data-source>
96,<exclude-unlisted-classes>false</exclude-unlisted-classes>
96,<properties>
96,"<property name=""eclipselink.composite-unit.member"" value=""true""/>"
96,"<property name=""eclipselink.target-database"""
96,"value=""org.eclipse.persistence.platform.database.MySQLPlatform""/>"
96,</properties>
96,</persistence-unit>
96,</persistence>
96,See Also
96,"For more information, see:"
96,"""@CompositeMember"""
96,"""composite-unit"""
96,"""composite-unit.member"""
96,composite-unit.properties
96,Use the eclipselink.composite-unit.properties property to configure the properties for persistence unit members.
96,Values
96,Table 5-27 describes this persistence property's values.
96,Table 5-27 Valid Values for composite-unit.properties
96,Value
96,Description
96,Map of properties
96,Properties to be passed to the persistence unit. Use the persistence unit's name as the key.
96,Usage
96,Pass this property to createEntityManager method of a composite persistence unit to pass properties to its member persistence units.
96,Examples
96,Example 5-23 shows how to use this property in a property map
96,Example 5-23 Using composite-unit.properties in a Property Map
96,Map props1 = new HashMap();
96,"props1.put(""javax.persistence.jdbc.user"", ""user1"");"
96,"props1.put(""javax.persistence.jdbc.password"", ""password1"");"
96,"props1.put(""javax.persistence.jdbc.driver"", ""oracle.jdbc.OracleDriver"");"
96,"props1.put(""javax.persistence.jdbc.url"", ""jdbc:oracle:thin:@oracle_db_url:1521:db"");"
96,Map props2 = new HashMap();
96,"props2.put(""javax.persistence.jdbc.user"", ""user2"");"
96,"props2.put(""javax.persistence.jdbc.password"", ""password2"");"
96,"props2.put(""javax.persistence.jdbc.driver"", ""com.mysql.jdbc.Driver"");"
96,"props2.put(""javax.persistence.jdbc.url"", "" jdbc:mysql://my_sql_db_url:3306/user2"");"
96,Map memberProps = new HashMap();
96,"memberProps.put(""memberPu1"", props1);"
96,"memberProps.put(""memberPu2"", props2);"
96,Map props = new HashMap();
96,"props.put(""eclipselink.logging.level"", ""FINEST"");"
96,"props.put(""eclipselink.composite-unit.properties"", memberProps);"
96,"EntityManagerFactory emf = Persistence.createEntityManagerFactory(""compositePu"", props);"
96,See Also
96,"For more information, see:"
96,"""composite-unit"""
96,concurrency.manager.waittime
96,This property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager. It control how much time loop wait before it try acquire lock for current thread again. It value is set above above 0 dead lock detection mechanism and related extended logging will be activated.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.waittime
96,Value
96,Description
96,Wait time
96,How much time loop wait before it try acquire lock for current thread again. Default value is 0 (unit is ms). Allowed values are: long
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.waittime in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.waittime"" value=""100"" />"
96,concurrency.manager.maxsleeptime
96,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager. It control how long we are willing to wait before firing up an exception.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.maxsleeptime
96,Value
96,Description
96,Wait time
96,It control how long we are willing to wait before firing up an exception. Default value is 40000 (unit is ms). Allowed values are: long
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.maxsleeptime in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.maxsleeptime"" value=""100"" />"
96,concurrency.manager.maxfrequencytodumptinymessage
96,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager and org.eclipse.persistence.internal.helper.ConcurrencyUtil. It control how frequently the tiny dump log message is created.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.maxfrequencytodumptinymessage
96,Value
96,Description
96,Wait time
96,It control how frequently the tiny dump log message is created. Default value is 40000 (unit is ms). Allowed values are: long
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.maxfrequencytodumptinymessage in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.maxfrequencytodumptinymessage"" value=""100000"" />"
96,concurrency.manager.maxfrequencytodumpmassivemessage
96,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager and org.eclipse.persistence.internal.helper.ConcurrencyUtil. It control how frequently the massive dump log message is created.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.maxfrequencytodumpmassivemessage
96,Value
96,Description
96,Wait time
96,It control how frequently the massive dump log message is created. Default value is 60000 (unit is ms). Allowed values are: long
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.maxfrequencytodumpmassivemessage in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.maxfrequencytodumpmassivemessage"" value=""100000"" />"
96,concurrency.manager.allow.interruptedexception
96,"In the places where use this property normally if a thread is stuck it is because it is doing object building. Blowing the threads ups is not that dangerous. It can be very dangerous for production if the dead lock ends up not being resolved because the productive business transactions will become cancelled if the application has a limited number of retries to for example process an MDB. However, the code spots where we use this constant are not as sensible as when the write lock manager is starving to run commit."
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.allow.interruptedexception
96,Value
96,Description
96,true
96,(Default) If we want the to fire up an exception to try to get the current thread to release all of its acquired locks and allow other threads to progress.
96,false
96,If aborting frozen thread is not effective it is preferable to not fire the interrupted exception let the system.
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.allow.interruptedexception in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.allow.interruptedexception"" value=""true"" />"
96,concurrency.manager.allow.concurrencyexception
96,See valid values table.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.allow.concurrencyexception
96,Value
96,Description
96,true
96,(Default) If we want the to fire up an exception to try to get the current thread to realease all of its acquired locks and allow other threads to progress.
96,false
96,If aborting frozen thread is not effective it is preferable to not fire the concurrency exception let the system freeze and die and force the administration to kill the server. This is preferable to aborting the transactions multiple times without success in resolving the dead lock and having business critical messages that after 3 JMS retries are discarded out. Failing to resolve a dead lock can have terrible impact in system recovery unless we have infinite retries for the business transactions.
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.allow.concurrencyexception in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.allow.concurrencyexception"" value=""true"" />"
96,concurrency.manager.allow.readlockstacktrace
96,Collect debug/trace information during ReadLock acquisition.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for concurrency.manager.allow.readlockstacktrace
96,Value
96,Description
96,true
96,(Default) Collect debug/trace information during ReadLock acquisition.
96,false
96,Don't collect debug/trace information during ReadLock acquisition
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using concurrency.manager.allow.readlockstacktrace in persistence.xml
96,"<property name=""eclipselink.concurrency.manager.allow.readlockstacktrace"" value=""true"" />"
96,connection-pool
96,Use the eclipselink.connection-pool property to configure the various connection pool properties.
96,Values
96,Table 5-28 describes this persistence property's values.
96,Table 5-28 Valid Values for connection-pool
96,Value
96,Description
96,initial
96,Starting (initial) number of connections.
96,min
96,Minimum number of connections.
96,max
96,Maximum number of connections.
96,wait
96,Amount of time (in milliseconds) to wait for a connection from the pool.
96,url
96,URL of the JDBC for the connection.
96,shared
96,"For read connection pools, indicates that read connections are shared across threads."
96,jtaDataSource
96,"JTA DataSource name to use for the connection, if different than the default."
96,nonJtaDataSource
96,"Non-JTA DataSource name to use for the connection, if different than the default."
96,user
96,Username to use for this connection (if different than the default).
96,password
96,Password of the user for this connection (if different than the default).
96,Usage
96,"Append the name of the connection pool and property to be configured. If connection pool is specified, EclipseLink configures the default (write) pool."
96,Examples
96,Example 5-24 shows how to use this property in the persistence.xml file.
96,Example 5-24 Using connection-pool in persistence.xml
96,"<property name=""eclipselink.connection-pool.default.initial"" value=""1"" />"
96,"<property name=""eclipselink.connection-pool.node2.min"" value=""16""/>"
96,"<property name=""eclipselink.connection-pool.node2.max"" value=""16""/>"
96,"<property name=""eclipselink.connection-pool.node2.url"""
96,"value=""jdbc:oracle:thin:@node2:1521:orcl""/>"
96,See Also
96,"For more information, see:"
96,Partitioning Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Partitioning
96,"""Connection Pools"" in Understanding EclipseLink"
96,"""Connection Pooling"" in Solutions Guide for EclispeLink"
96,"""jdbc.cache-statements"""
96,"""connection-pool.read"""
96,"""connection-pool.sequence"""
96,connection-pool.read
96,Use the eclipselink.connection-pool.read property to configure a read connection pool for non-transaction read queries.
96,Values
96,Table 5-29 describes this persistence property's values.
96,Table 5-29 Valid Values for connection-pool.read
96,Value
96,Description
96,initial
96,Starting (initial) number of connection.
96,min
96,Minimum number of connections.
96,max
96,Maximum number of connections.
96,wait
96,Amount of time it takes to get connections from the pool.
96,url
96,URL of the JDBC connection.
96,shared
96,"For read connection pools, indicates that read connections are shared across threads."
96,jtaDataSource
96,"JTA DataSource name to use for the connection, if different than the default."
96,nonJtaDataSource
96,"Non-JTA DataSource name to use for the connection, if different than the default."
96,user
96,Username to use for this connection (if different than the default).
96,password
96,Password of the user for this connection (if different then the default).
96,Usage
96,"By default, EclipseLink does not use a separate read connection pool; the default pool is used for read queries."
96,Examples
96,Example 5-25 shows how to use this property in the persistence.xml file.
96,Example 5-25 Using connection-pool.read in persistence.xml
96,"<property name=""eclipselink.connection-pool.read.min"" value=""16""/>"
96,"<property name=""eclipselink.connection-pool.read.max"" value=""16""/>"
96,See Also
96,"For more information, see:"
96,"""Connection Pools"" in Understanding EclipseLink"
96,"""Connection Pooling"" in Solutions Guide for EclispeLink"
96,"""connection-pool"""
96,connection-pool.sequence
96,Use the eclipselink.connection-pool.sequence property to have the connection pool allocate generated IDs.
96,Values
96,Table 5-30 describes this persistence property's values.
96,Table 5-30 Valid Values for connection-pool.sequence
96,Value
96,Description
96,true
96,Uses the internal connection pool to pool connections from a datasource.
96,false
96,(Default) Does not use the internal connection pool to pool connections from a datasource.
96,Usage
96,"This is only required for TABLE sequencing. By default, EclipseLink does not use a separate sequence connection pool; the default pool is used for sequencing."
96,Examples
96,Example 5-26 shows how to use this property in the persistence.xml file.
96,Example 5-26 Using connection-pool.sequence in persistence.xml
96,"<property name=""eclipselink.connection-pool.sequence"" value=""true""/>"
96,See Also
96,"For more information, see:"
96,"""Connection Pools"" in Understanding EclipseLink"
96,"""Connection Pooling"" in Solutions Guide for EclispeLink"
96,"""connection-pool"""
96,create-ddl-jdbc-file-name
96,Use the eclipselink.create-ddl-jdbc-file-name property to specify the name of the DDL file generated by EclipseLink that contains the SQL statements to create tables for JPA entities.
96,Values
96,Table 5-31 describes this persistence property's values.
96,Table 5-31 Valid Values for create-ddl-jdbc-file-name
96,Value
96,Description
96,File name
96,A file name valid for your operating system.
96,You can prefix the file name with a file path if a concatenation of eclipselink.application-location + eclipselink.create-ddl-jdbc-file-name is valid for your operating system.
96,Usage
96,"If eclipselink.ddl-generation is set to create-tables or drop-and-create-tables, EclipseLink writes this file to the location specified by eclipselink.application-location."
96,Examples
96,See Example 5-27 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""application-location"""
96,"""ddl-generation"""
96,ddl-generation
96,Use the eclipselink.ddl-generation property to specify how EclipseLink generates DDL (Data Definition Language) for the database schema (tables and constraints) on deployment
96,"Note: EclipseLink does not support mixing EclipseLink defined and JPA defined DDL generation properties. If the eclipselink.ddl-generation property is specified, the javax.persistence.schema-generation.database.action, javax.persistence.schema-generation.scripts.action, and javax.persistence.sql-load-script-source JPA defined properties will be ignored."
96,Values
96,Table 5-32 describes this persistence property's values.
96,Table 5-32 Valid Values for ddl-generation
96,Value
96,Description
96,create-tables
96,EclipseLink will attempt to execute a CREATE TABLE SQL for each table.
96,"If the table already exists, EclipseLink will follow the default behavior of your specific database and JDBC driver combination (when a CREATE TABLE SQL is issued for an already existing table). In most cases an exception is thrown and the table is not created; the existing table will be used. EclipseLink will then continue with the next statement."
96,create-or-extend-tables
96,"EclipseLink will attempt to create tables. If the table exists, EclipseLink will add any missing columns."
96,drop-and-create-tables
96,"EclipseLink will attempt to DROP all tables, then CREATE all tables. If any issues are encountered, EclipseLink will follow the default behavior of your specific database and JDBC driver combination, then continue with the next statement."
96,This is useful in development if the schema frequently changes or during testing when the existing data needs to be cleared.
96,"Note: Using drop-and-create will remove all of the data in the tables when they are dropped. You should never use option on a production schema that has valuable data in the database. If the schema changed dramatically, there could be old constraints in the database that prevent the dropping of the old tables. This may require the old schema to be dropped through another mechanism."
96,none
96,(Default) No DDL generated; no schema generated.
96,Usage
96,You can use create-or-extend-tables only when eclipselink.ddl-generation.output-mode = database.
96,"If you are using persistence in a Java SE environment and would like to create the DDL files without creating tables, additionally define a Java system property INTERACT_WITH_DB and set its value to false."
96,DDL_GENERATION must be set in order for this property to take effect.
96,Examples
96,Example 5-27 shows how to use this property in the persistence.xml file.
96,Example 5-27 Using ddl-generation in persistence.xml
96,"<property name=""eclipselink.ddl-generation"" value=""drop-and-create-tables""/>"
96,"<property name=""eclipselink.create-ddl-jdbc-file-name"" value=""createDDL_ddlGeneration.jdbc""/>"
96,"<property name=""eclipselink.drop-ddl-jdbc-file-name"" value=""dropDDL_ddlGeneration.jdbc""/>"
96,"<property name=""eclipselink.ddl-generation.output-mode"" value=""both""/>"
96,Example 5-28 shows how to use this property in a property map.
96,Example 5-28 Using ddl-generation in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.DDL_GENERATION,"
96,PersistenceUnitProperties.DROP_AND_CREATE);
96,"propertiesMap.put(PersistenceUnitProperties.DDL_GENERATION_MODE,"
96,PersistenceUnitProperties.BOTH);
96,"propertiesMap.put(PersistenceUnitProperties.CREATE_JDBC_DDL_FILE, ""create.sql"");"
96,See Also
96,"For more information, see:"
96,"""create-ddl-jdbc-file-name"""
96,"""drop-ddl-jdbc-file-name"""
96,"""ddl-generation.output-mode"""
96,Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/DDL
96,ddl-generation.output-mode
96,Use the eclipselink.ddl-generation.output-mode property to specify where EclipseLink generates and writes the DDL.
96,Values
96,Table 5-33 describes this persistence property's values.
96,Table 5-33 Valid Values for ddl-generation.output-mode
96,Value
96,Description
96,both
96,DDL will be generated and written to both the database and a file.
96,"If eclipselink.ddl-generation is set to create-tables, then eclipselink.create-ddl-jdbc-file-name is written to eclipselink.application-location and executed on the database."
96,"If eclipselink.ddl-generation is set to drop-and-create-tables, then both eclipselink.create-ddl-jdbc-file-name and eclipselink.drop-ddl-jdbc-file-name are written to eclipselink.application-location, and both SQL files are executed on the database."
96,database
96,(Default) DDL will be generated and written to the database only.
96,sql-script
96,DDL will be generated and written to a file only.
96,"If eclipselink.ddl-generation is set to create-tables, then eclipselink.create-ddl-jdbc-file-name is written to eclipselink.application-location. It is not executed on the database."
96,"If eclipselink.ddl-generation is set to drop-and-create-tables, then both eclipselink.create-ddl-jdbc-file-name and eclipselink.drop-ddl-jdbc-file-name are written to eclipselink.application-location. Neither are executed on the database."
96,Usage
96,"You can only use ddl-generation.output-mode if you use ddl-generation. Then, you can optimally set other properties."
96,Examples
96,See Example 5-27 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""application-location"""
96,"""ddl-generation"""
96,"""create-ddl-jdbc-file-name"""
96,ddl.table-creation-suffix
96,Use the eclipselink.ddl.table-creation-suffix property to append a string to generated CREATE Table statements.
96,Values
96,Table 5-34 describes this property's values.
96,Table 5-34 Valid Values for ddl-generation.table-creation-suffix
96,Value
96,Description
96,value
96,The name of the suffix.
96,Usage
96,The ddl.generation property must be set.
96,Examples
96,Example 5-29 shows how to use this property in the persistence.xml file.
96,Example 5-29 Using ddl.table-creation-suffix in persistence.xml
96,"<property name=""eclipselink.ddl.table-creation-suffix"" value=""engine=InnoDB""/>"
96,See Also
96,"For more information, see:"
96,"""ddl-generation"""
96,deploy-on-startup
96,Use the eclipselink.deploy-on-startup property to configure deployment on startup (at the creation of the EntityManagerFactory) instead of occurring the first time an EntityManager is created.
96,Values
96,Table 5-35 describes this persistence property's values.
96,Table 5-35 Valid Values for delay-on-startup
96,Value
96,Description
96,true
96,"Causes a persistence unit to be created when the EntityManager is created, usually during deployment to a Java EE container or servlet container."
96,false
96,"(Default) The persistence unit is not initialized until the first EntityManager is created, or until metadata is required from the EntityManagerFactory."
96,Usage
96,"Using true may increase startup time of a JavaEE server, but will avoid the first request from hanging as the persistence unit is deployed."
96,Examples
96,Example 5-30 shows how to use this property in the peristence.xml file.
96,Example 5-30 Using deploy-on-startup in persistence.xml
96,"<property name=""eclipselink.deploy-on-startup"" value=""true"" />"
96,descriptor.customizer
96,"Use the eclipselink.descriptor.customizer property as a prefix for a property to configure a DescriptorCustomizer. Use this class's customize method, which takes an org.eclipse.persistence.descriptors.ClassDescriptor, to programmatically access advanced EclipseLink descriptor and mapping API for the descriptor associated with the JPA entity."
96,Values
96,Table 5-36 describes this persistence property's values.
96,Table 5-36 Valid Values for descriptor.customizer
96,Value
96,Description
96,name
96,Full name for a class that implements DescriptorCustomizer.
96,Usage
96,You cannot use multiple descriptor customizers.
96,Examples
96,Example 5-31 shows how to use this property in the peristence.xml file.
96,Example 5-31 Using descriptor.customizer in persistence.xml
96,"<property name=""eclipselink.descriptor.customizer.Order"""
96,"value=""acme.sessions.MyDesriptorCustomizer""/>"
96,Example 5-32 shows how to use this property with a property map.
96,Example 5-32 Using descriptor.customizer in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.DESCRIPTOR_CUSTOMIZER+"".Order"","
96,"""acme.sessions.MyDescriptorCustomizer"");"
96,See Also
96,"For more information, see:"
96,Understanding EclipseLink
96,"Section 8.1, ""Entity"" in the JPA Specification http://jcp.org/en/jsr/detail?id=220"
96,drop-ddl-jdbc-file-name
96,Use the eclipselink.drop-ddl-jdbc-file-name property to specify the name of the DDL file generated by EclipseLink that contains the SQL statements to drop tables for JPA entities.
96,Values
96,Table 5-37 describes this persistence property's values.
96,Table 5-37 Valid Values for drop-ddl-jdbc-file-name
96,Value
96,Description
96,File name
96,A file name valid for your operating system.
96,You can prefix the file name with a file path if a concatenation of eclipselink.application-location + eclipselink.create-ddl-jdbc-file-name is valid for your operating system.
96,Usage
96,"If eclipselink.ddl-generation is set to create-tables, EclipseLink writes this file to the location specified by eclipselink.application-location."
96,Examples
96,See Example 5-27 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""ddl-generation"""
96,exception-handler
96,"Use the eclipselink.exception-handler property to specify the EclipseLink exception handler class: an exception handler class that implements the org.eclipse.persistence.exceptions.ExceptionHandler interface. The class must provide a default, no-argument constructor."
96,Values
96,Table 5-38 describes this persistence property's values.
96,Table 5-38 Valid Values for exception-handler
96,Value
96,Description
96,ExceptionHandler class
96,"Use the handleException method of the class, which takes a java.lang.RuntimeException, to:"
96,Re-throw the exception
96,Throw a different exception
96,Retry the query or database operation
96,Usage
96,The ExceptionHandler class name must be fully qualified by its package name.
96,Examples
96,Example 5-33 shows how to use this property in the persistence.xml file.
96,Example 5-33 Using exception-handler in persistence.xml
96,"<property name=""eclipselink.exception-handler"""
96,"value=""my.package.MyExceptionHandler"">"
96,Example 5-34 shows how to use this extension in a property map.
96,Example 5-34 Using exception-handler in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.EXCEPTION_HANDLER_CLASS,"
96,"""my.package.MyExceptionHandler"");"
96,See Also
96,"For more information, see:"
96,"""orm.throw.exceptions"""
96,"""Sessions"" in Understanding EclipseLink"
96,"""Managing and Diagnosing Problems"" in Solutions Guide for EclispeLink"
96,exclude-eclipselink-orm
96,Use the eclipselink.exclude-eclipselink-orm property to exclude an EclipseLink ORM mapping file for a specific persistence unit.
96,Values
96,Table 5-39 describes this persistence property's values.
96,Table 5-39 Valid Values for exclude-eclipselink-orm
96,Value
96,Description
96,true
96,Does not use the eclipselink-orm.xml file.
96,false
96,(Default) EclipseLink uses the eclipselink-orm.xml file.
96,Usage
96,By default the first file found at the resource name: META-INF/eclipselink-orm.xml is processed and overrides configurations specified in annotations and standard mapping files.
96,Examples
96,Example 5-35 shows how to use this property in the persistence.xml file.
96,Example 5-35 Using exclude-eclipselink-orm in persistence.xml
96,"<property name=""eclipselink.exclude-eclipselink-orm"" value=""true""/>"
96,See Also
96,"For more information, see:"
96,"""Building Blocks of a EclipseLink Project"" in Understanding EclipseLink"
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,flush-clear.cache
96,Use the eclipselink.flush-clear.cache property to specify the EclipseLink EntityManager cache behavior when a clear method follows the flush method.
96,Values
96,Table 5-40 describes this persistence property's values.
96,Table 5-40 Valid Values for flush-clear.cache
96,Value
96,Description
96,Drop
96,EclipseLink drops the entire EntityManager cache.
96,"Although this is the fastest mode and uses the least memory, the shared cache may potentially contain stale data after performing the commit."
96,DropInvalidate
96,(Default) EclipseLink drops the entire EntityManager cache. Classes that have at least one updated or deleted object become invalid in the shared cache after performing the commit.
96,"This mode is slower than Drop, but as efficient (in terms of memory usage) and prevents stale data."
96,Merge
96,EclipseLink drops objects the EntityManager cache that have not been flushed.
96,"Although this mode leaves the shared cache in a perfect state after performing the commit, it is the least memory-efficient. In a very large transaction you may run out of memory."
96,Usage
96,"You can specify this property when creating an EntityManagerFactory (in the map passed to the createEntityManagerFactory method or in the persistence.xml file), or an EntityManager (in the map passed to the createEntityManager method)."
96,Note that the latter overrides the former.
96,Examples
96,Example 5-36 shows how to use this property in the persistence.xml file.
96,Example 5-36 Using flush-clear.cache in persistence.xml
96,"<property name=""eclipselink.flush-clear.cache"" value=""Drop""/>"
96,Example 5-37 shows how to use this extension in a property map.
96,Example 5-37 Using flush-clear.cache in a Property Map
96,import org.ecliplse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.FLUSH_CLEAR_CACHE,"
96,FlushClearCache.Drop);
96,See Also
96,"For more information, see:"
96,"""@Cache"""
96,"""Cache Coordination"" in Understanding EclipseLink"
96,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
96,Cache Coordination Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
96,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
96,id-validation
96,Use the eclipselink.id-validation property to define which primary key components values are considered invalid.
96,Values
96,Table 5-41 describes this persistence property's values.
96,Table 5-41 Valid Values for id-validation
96,Value
96,Description
96,Negative
96,"Null, 0 and negative values are invalid for IDs extending Number and primitive int and long IDs."
96,None
96,EclipseLink performs no ID validation.
96,Null
96,Null is invalid All other values are valid.
96,Zero
96,"Null, 0 and negative values are invalid for primitive int and long IDs."
96,Usage
96,Identity and sequencing (with shouldAlwaysOverrideExistingValue configured as true) will override any existing ID value.
96,Examples
96,Example 5-38 shows how to use this property in the persistence.xml file.
96,Example 5-38 Using id-validation in persistence.xml
96,"<property name=""eclipselink.id-validation"" value=""NULL""/>"
96,See Also
96,"For more information, see:"
96,"""Persisting Objects"" in Understanding EclipseLink"
96,"""@PrimaryKey"""
96,jdbc.allow-native-sql-queries
96,"Use the eclipselink.jdbc.allow-native-sql-queries property to specify if user-defined (that is, native) SQL is allowed within a persistence unit."
96,Values
96,Table 5-42 describes this persistence property's values.
96,Table 5-42 Valid Values for jdbc.allow-native-sql-queries
96,Value
96,Description
96,true
96,(Default) EclipseLink allows native SQL.
96,false
96,EclipseLink does not allow native SQL.
96,Usage
96,"Within a multitenant, use this option to minimize the potential impact of revealing multitenant information. By default, any persistence unit with a multitenant entity causes EclipseLink to set eclipselink.jdbc.allow-native-sql-queries as false."
96,Examples
96,Example 5-39 shows how to use this property in the persistence.xml file.
96,Example 5-39 Using jdbc.allow-native-sql-queries in persistence.xml
96,"<property name=""eclipselink.jdbc.allow-native-sql-queries"" value=""false"" />"
96,See Also
96,"For more information, see:"
96,"""Querying"" in Understanding EclipseLink"
96,jdbc.batch-writing
96,Use the eclipselink.jdbc.batch-writing property to configure batch writing to optimize transactions with multiple write functions.
96,Values
96,Table 5-43 describes this persistence property's values.
96,Table 5-43 Valid Values for jdbc.batch-writing
96,Value
96,Description
96,jdbc
96,Use JDBC batch writing.
96,buffered
96,Do not use JDBC batch writing or the platform's native batch writing.
96,oracle-jdbc
96,"Use the Oracle platform's native batch writing. In a property map, use OracleJDBC."
96,Note: This requires an Oracle JDBC driver.
96,custom-class
96,A custom class that extends the BatchWritingMechanism class.
96,none
96,"(Default) Do not use batch writing (that is, turn it off)."
96,Usage
96,"Batch writing allows multiple heterogeneous dynamic SQL statements to be sent to the database as a single execution, or multiple homogeneous parameterized SQL statements to be executed as a single batch execution."
96,Note:
96,Not all JDBC drivers or databases support batch writing.
96,Use eclipselink.jdbc.batch-writing.size to specify the batch size.
96,Examples
96,Example 5-40 shows how to use this property in the persistence.xml file.
96,Example 5-40 Using jdbc.batch-writing in persistence.xml
96,"<property name=""eclipselink.jdbc.batch-writing"" value=""Oracle-JDBC""/>"
96,Example 5-41 shows how to use this property in a property map.
96,Example 5-41 Using jdbc.batch-writing in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.BATCH_WRITING,"
96,BatchWriting.OracleJDBC);
96,See Also
96,"For more information, see:"
96,"""jdbc.batch-writing.size"""
96,"""Batch Writing"" in Solutions Guide for EclispeLink"
96,jdbc.batch-writing.size
96,Use the eclipselink.jdbc.batch-writing.size property to configure the batch size used for batch writing.
96,Values
96,Table 5-44 describes this persistence property's values.
96,Table 5-44 Valid Values for jdbc.batch-writing.size
96,Value
96,Description
96,batch size
96,"For parameterized batch writing, this value is the number of statements to batch (default: 100)."
96,"For dynamic batch writing, this value is the size of the batched SQL buffer (default: 32k)."
96,Examples
96,Example 5-42 shows how to use this property in the persistence.xml file.
96,Example 5-42 Using jdbc.batch-writing.size in persistence.xml
96,"<property name=""eclipselink.jdbc.batch-writing.size"" value=""1000""/>"
96,See Also
96,"For more information, see:"
96,"""jdbc.batch-writing"""
96,"""Batch Writing"" in Solutions Guide for EclispeLink"
96,jdbc.cache-statements
96,Use the eclipselink.jdbc.cache-statements property to specify if JDBC statements should be cached.
96,Values
96,Table 5-45 describes this persistence property's values.
96,Table 5-45 Valid Values for jdbc.cache-statements
96,Value
96,Description
96,true
96,Enable internal statement caching.
96,false
96,(Default) Disable internal statement caching.
96,Usage
96,"You should use this property when using EclipseLink's internal connection pooling. See ""connection-pool"" for more information."
96,Examples
96,Example 5-43 shows how to use this property in the persistence.xml file.
96,Example 5-43 Using jdbc.cache-statements in persistence.xml
96,"<property name=""eclipselink.jdbc.cache-statements"" value=""false""/>"
96,Example 5-44 shows how to use this property in a property map.
96,Example 5-44 Using jdbc.cache-statements in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.CACHE_STATEMENTS, ""false"");"
96,See Also
96,"For more information, see:"
96,"""jdbc.cache-statements.size"""
96,"""connection-pool"""
96,"""Batch Writing"" in Solutions Guide for EclispeLink"
96,jdbc.cache-statements.size
96,Use the eclipselink.jdbc.cache-statements.size property to specify the number of statements held when using internal statement caching.
96,Values
96,Table 5-46 describes this persistence property's values.
96,Table 5-46 Valid Values for jdbc.cache-statements.size
96,Value
96,Description
96,size
96,A string value containing a positive integer or zero (Default: 50).
96,"The maximum value may vary, depending on your JDBC driver."
96,Examples
96,Example 5-45 shows how to use this property in the persistence.xml file.
96,Example 5-45 Using jdbc.cache-statements.size in persistence.xml
96,"<property name=""eclipselink.jdbc.cache-statements.size"" value=""100""/>"
96,Example 5-46 shows how to use this property in a property map.
96,Example 5-46 Using jdbc.cache-statements.size in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.CACHE_STATEMENTS_SIZE, ""100"");"
96,See Also
96,"For more information, see:"
96,"""jdbc.cache-statements"""
96,"""Batch Writing"" in Solutions Guide for EclispeLink"
96,jdbc.connector
96,Use the eclipselink.jdbc.connector property to define a custom connector to connect to the database.
96,Values
96,Table 5-47 describes this persistence property's values.
96,Table 5-47 Valid Values for jdbc.connector
96,Value
96,Description
96,Fully qualified class name
96,A class that implements the Connector interface.
96,Usage
96,"You can use this property to connect to a non-standard connection pool, or provide customized details on how to obtain a connection."
96,This property is not required when using a DataSource or JDBC DriverManager.
96,Examples
96,Example 5-47 shows how to use this property in the persistence.xml file.
96,Example 5-47 Using jdbc.connector in persistence.xml
96,"<property name=""eclipselink.jdbc.connector"" value=""package.MyConnector""/>"
96,jdbc.exclusive-connection.is-lazy
96,Use the eclipselink.jdbc.exclusive-connection.is-lazy property to specify if EclipseLink acquires write connections lazily.
96,Values
96,Table 5-48 describes this persistence property's values.
96,Table 5-48 Valid Values for jdbc.exclusive-connection.is-lazy
96,Value
96,Description
96,true
96,(Default) Acquire write connections lazily.
96,false
96,Do not acquire write connections lazily.
96,Examples
96,Example 5-48 shows how to use this property in the persistence.xml file.
96,Example 5-48 Using jdbc.exclusive-connection.is-lazy in persistence.xml
96,"<property name=""eclipselink.jdbc.exclusive-connection.is-lazy"" value=""false""/>"
96,Example 5-49 shows how to use this property in a property map.
96,Example 5-49 Using jdbc.exclusive-connection.is-lazy in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.EXCLUSIVE_CONNECTION_IS_LAZY,"
96,"""false"");"
96,See Also
96,"For more information, see:"
96,Auditinghttp://wiki.eclipse.org/EclipseLink/Examples/JPA/Auditing
96,jdbc.exclusive-connection.mode
96,Use the eclipselink.jdbc.exclusive-connection.mode property to specify when EclipseLink performs reads through the write connection.
96,Values
96,Table 5-49 describes this persistence property's values.
96,Table 5-49 Valid Values for jdbc.exclusive-connection.mode
96,Value
96,Description
96,Transactional
96,"(Default) Create an isolated client session if some or all entities require isolated cache, otherwise create a client session."
96,Notes:
96,EclipseLink keeps the connection exclusive for the duration of the transaction.
96,"Inside the transaction, EclipseLink performs all writes and reads through the exclusive connection."
96,"Outside the EclipseLink transaction, a new connection is acquired from the connection pool for each read and released back immediately after the query is executed."
96,Isolated
96,"Create an exclusive isolated client session if reading an isolated entity, otherwise raise an error."
96,Notes:
96,EclipseLink keeps the connection exclusive for the lifetime of the owning EntityManager.
96,"Inside the transaction, EclipseLink performs all writes and reads through the exclusive connection."
96,"Outside the EclipseLink transaction, only isolated entities are read through the exclusive connection. For non-isolated entities, EclipseLink acquires a new connection from the connection pool for each read and immediately releases the connection after executing the query."
96,Always
96,"Create an exclusive isolated client session if reading an isolated entity, otherwise create an exclusive client session."
96,Note: EclipseLink keeps the connection exclusive for the lifetime of the owning EntityManager and performs all writes and reads through the exclusive connection.
96,Usage
96,"You can set this property while creating either an EntityManagerFactory (either in the map passed to the createEntityManagerFactory method, or in the persistence.xml file), or an EntityManager (in the map passed to the createEntityManager method). Note that the latter overrides the former."
96,Examples
96,Example 5-50 shows how to use this property in the persistence.xml file.
96,Example 5-50 Using jdbc.exclusive-connection.mode in persitence.xml
96,"property name=""eclipselink.jdbc.exclusive-connection.mode"" value=""Always""/>"
96,Example 5-51 shows how to use this property in a property map.
96,Example 5-51 Using jdbc.exclusive-connection.mode in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.EXCLUSIVE_CONNECTION_MODE, ""Always"");"
96,See Also
96,"For more information, see:"
96,"""jdbc.exclusive-connection.is-lazy"""
96,"""Isolated Client Sessions"" in Understanding EclipseLink"
96,"""Connections"" in Understanding EclipseLink"
96,jdbc.native-sql
96,"Use the eclipselink.jdbc.native-sql property to specify if EclipseLink uses generic SLQ or includes platform-specific (that is, ""native"") SQL statements."
96,Values
96,Table 5-50 describes this persistence property's values.
96,Table 5-50 Valid Values for jdbc.native-sql
96,Value
96,Description
96,true
96,"(Default) Use platform-specific (""native"" ) SQL."
96,false
96,Use generic SQL.
96,Usage
96,"When using platform-specific SQL (eclipselink.jdbc.native-sql = true), EclipseLink uses platform-specific SQL to customize join syntax, date operators, using sequencing, and so on."
96,Examples
96,Example 5-52 shows how to use this property in the persistence.xml file.
96,Example 5-52 Using jdbc.native-sql in persistence.xml
96,"<property name=""eclipselink.jdbc.native-sql"" value=""false""/>"
96,Example 5-53 shows how to use this property in a property map.
96,Example 5-53 Using jdbc.native-sql in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.NATIVE_SQL, ""false"");"
96,See Also
96,"For more information, see:"
96,"""Querying"" in Understanding EclipseLink"
96,"""Query Languages"" in Understanding EclipseLink"
96,jdbc.property
96,Use the eclipselink.jdbc.property prefix to pass JDBC driver-specific connection properties to EclipseLink.
96,Usage
96,Append the JDBC driver-specific property name to this property prefix.
96,Examples
96,Example 5-54 shows how to use this property prefix in the persistence.xml file.
96,Example 5-54 Using jdbc.property in persistence.xml
96,"<property name=""eclipselink.jdbc.property.defaultRowPrefetch"" value=""25""/>"
96,See Also
96,"For more information, see:"
96,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
96,"""Introduction to Data Access"" in Understanding EclipseLink"
96,jdbc.sql-cast
96,"Use the eclipselink.jdbc.sql-cast property to specify if EclipseLink uses platform-specific (that is, ""native"") CAST SQL operations."
96,Note:
96,"Normally, casting is not required. Using it may cause issues."
96,Values
96,Table 5-51 describes this persistence property's values.
96,Table 5-51 Valid Values for jdbc.sql-cast
96,Value
96,Description
96,true
96,Use platform-specific CAST operations.
96,false
96,(Default) Do not use platform-specific CAST operations.
96,Examples
96,Example 5-55 shows how to use this property in the persistence.xml file.
96,Example 5-55 Using jdbc.sql-cast in persistence.xml
96,"<property name=""eclipselink.jdbc.sql-cast"" value=""true""/>"
96,jdbc.uppercase-columns
96,Use the eclipselink.jdbc.uppercase-columns property to force column names from the metadata to be uppercase.
96,Note:
96,"This parameter has been replaced by jpql.parser, which ensures that both sides use uppercase for comparisons."
96,Values
96,Table 5-52 describes this persistence property's values.
96,Table 5-52 Valid Values for jdbc.uppercase-columns
96,Value
96,Description
96,true
96,Forces all column names from the metadata to uppercase.
96,false
96,(Default) Does not force column names from the metadata to uppercase.
96,Usage
96,"When using native SQL queries, the JDBC metadata may return column names in lower case on some platforms. If the column names are uppercase in the mappings (default), they will not match. You should use this parameter to force all column names from the metadata to uppercase."
96,Examples
96,Example 5-56 shows how to use this parameter in the persistence.xml file.
96,Example 5-56 Using jdbc.uppercase-column-names in persistence.xml
96,"<property name=""eclipselink.jpa.uppercase-columns"" value=""true""/>"
96,See Also
96,"For more information, see:"
96,"""jpql.parser"""
96,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
96,"""Introduction to Data Access"" in Understanding EclipseLink"
96,jpql.parser
96,Use the eclipselink.jpql.parser property to configure the JPQL parser parameters.
96,Values
96,Table 5-53 describes this persistence property's values.
96,Table 5-53 Valid Values for jpql.parser
96,Value
96,Description
96,org.eclipse.persistence.internal.jpa.jpql.HermesParser
96,"(Default) Current parser, starting with EclipseLink 2.4, that provides extended JPQL support."
96,org.eclipse.persistence.queries.ANTLRQueryBuilder
96,"Old parser, used for backward compatibility (prior to EclipseLink 2.4)."
96,See Also
96,"For more information, see:"
96,"""jpql.validation"""
96,jpa.uppercase-column-names
96,Use the eclipselink.jpa.uppercase-column-names property to specify JPA processing to uppercase all column name definitions (simulating case insensitivity).
96,Values
96,Table 5-54 describes this persistence property's values.
96,Table 5-54 Valid Values for jpa.uppercase-column-names
96,Value
96,Description
96,true
96,"JDBC metadata returned from the database is returned in uppercase, ensuring fields are the same case. Sets jdbc.uppercase-columns to true."
96,false
96,(Default) Does not return JDBC metadata in uppercase.
96,Usage
96,Use this property to correct situations in which user-defined fields do not match the case returned by the database for native queries.
96,Examples
96,Example 5-57 shows how to use this property in the persistence.xml file.
96,Example 5-57 Using jpa.uppercase-column-names in persistence.xml
96,"<property name=""eclipselink.jpa.uppercase-column-names"" value=""true""/>"
96,See Also
96,"For more information, see:"
96,"""jdbc.uppercase-columns"""
96,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
96,"""Introduction to Data Access"" in Understanding EclipseLink"
96,jpql.validation
96,Use the eclipselink.jpql.parser property to configure the JPQL parser validation level.
96,Values
96,Table 5-55 describes this persistence property's values.
96,Table 5-55 Valid Values for jpql.validation
96,Value
96,Description
96,EclipseLink
96,(Default) Allows EclipseLink JPAL extensions.
96,JPA 1.0
96,Allows valid JPA 1.0 JPQL only.
96,JPA 2.0
96,Allows valid JPA 2.0 JPQL only.
96,JPA 2.1
96,Allows valid JPA 2.1 JPQL only.
96,None
96,No JPQL validation.
96,Usage
96,This parameter applies only when eclipselink.jpql.parser is HermesParser.
96,Examples
96,Example 5-58 shows how to use this property in the persistence.xml file.
96,Example 5-58 Using jpql.validation in persistence.xml
96,"<property name=""eclipselink.jpql.validation"" value=""JPA 1.0""/>"
96,See Also
96,"For more information, see:"
96,"""jpql.parser"""
96,"""Java Persistence Query Language Extensions"""
96,logging.connection
96,Use the eclipselink.logging.connection property to specify if connections are logged.
96,Values
96,Table 5-56 describes this persistence property's values.
96,Table 5-56 Valid Values for logging.connection
96,Value
96,Description
96,true
96,(Default) Logs the connection name.
96,false
96,Does not log the connection name.
96,Usage
96,Using this parameter means that all connections are logged and not masked by the application code.
96,Examples
96,Example 5-59 shows how to use this parameter in the persistence.xml file.
96,Example 5-59 Using logging.connection in persistence.xml
96,"<property name=""eclipselink.logging.connection"" value=""false""/>"
96,See Also
96,"For more information, see:"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,"""logging.level"""
96,logging.exceptions
96,"Use the eclipselink.logging.exceptions property to specify if exceptions are logged when they are thrown, before returning the exception to the calling application."
96,Values
96,Table 5-57 describes this persistence property's values.
96,Table 5-57 Valid Values for logging.exceptions
96,Value
96,Description
96,true
96,(Default) Logs exceptions when they are thrown.
96,false
96,Does not log exceptions when they are thrown.
96,Usage
96,Using this property ensures that all exceptions are logged and not masked by the application code.
96,Examples
96,Example 5-60 shows how to use this property in the peristence.xml file.
96,Example 5-60 Using logging.exceptions in persistence.xml file
96,"<property name=""eclipselink.logging.exceptions"" value=""false"" />"
96,Example 5-61 shows how to use this property in a property map.
96,Example 5-61 Using logging.exceptions in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_EXCEPTIONS, ""false"");"
96,See Also
96,"For more information, see:"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,"""logging.level"""
96,logging.file
96,Use the eclipselink.logging.file property to specify a file location in which to output the log instead of the standard out.
96,Values
96,Table 5-58 describes this persistence property's values.
96,Table 5-58 Valid Values for logging.file
96,Value
96,Description
96,directory name
96,A string location to a directory in which you have write access. The location may be relative to your current working directory or an absolute location.
96,Usage
96,This property applies when used in a Java SE environment.
96,Examples
96,Example 5-62 shows how to use this property in the peristence.xml file.
96,Example 5-62 Using logging.file in persistence.xml file
96,"<property name=""eclipselink.logging.file"" value=""C:\myout\"" />"
96,Example 5-63 shows how to use this property in a property map.
96,Example 5-63 Using logging.file in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_FILE, ""C:\myout\"");"
96,See Also
96,"For more information, see:"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,logging.level
96,Use the eclipselink.logging.level property to specify a specific logging level and control the amount and detail that is emitted.
96,Values
96,Table 5-59 describes this persistence property's values.
96,Table 5-59 Valid Values for logging.level
96,Value
96,Description
96,OFF
96,Disables logging.
96,You may want to use OFF during production in order to avoid the overhead of logging.
96,SEVERE
96,"Logs exceptions indicating that EclipseLink cannot continue, as well as any exceptions generated during login. This includes a stack trace."
96,WARNING
96,"Logs exceptions that do not force EclipseLink to stop, including all exceptions not logged with SEVERE level. This does not include a stack trace."
96,INFO
96,"(Default) Logs the login/logout per sever session, including the user name. After acquiring the session, detailed information is logged."
96,CONFIG
96,"Logs only login, JDBC connection, and database information. You may want to use this log level at deployment time."
96,FINE
96,"Logs all SQL. You may want to use this log level during debugging and testing, but not at production time."
96,FINER
96,"Similar to WARNING, but includes stack trace. You may want to use this log level during debugging and testing, but not at production time."
96,FINEST
96,"Similar to FINER, but includes additional low level information. You may want to use this log level during debugging and testing, but not at production time."
96,ALL
96,Logs at the same level as FINEST.
96,Examples
96,Example 5-64 shows how to use this property in the peristence.xml file.
96,Example 5-64 Using logging.level in persistence.xml file
96,"<property name=""eclipselink.logging.level"" value=""OFF"" />"
96,Example 5-65 shows how to use this property in a property map.
96,Example 5-65 Using logging.level in a Property Map
96,import java.util.logging.Level;
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_LEVEL, Level.OFF);"
96,See Also
96,"For more information, see:"
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,logging.logger
96,Use the eclipselink.logging.logger property to define the type of logger to use.
96,Values
96,Table 5-60 describes this persistence property's values.
96,Table 5-60 Valid Values for logging.logger
96,Value
96,Description
96,Custom logger
96,Fully qualified class name of a custom logger which implements org.eclipse.persistence.logging.SessionLog.
96,JavaLogger
96,Uses java.util.logging.
96,ServerLogger
96,Integrates with the application server's logging.
96,DefaultLogger
96,"(Default) Uses EclipseLink's native logger, DefaultSessionLog."
96,Examples
96,Example 5-66 shows how to use this parameter in the persistence.xml file.
96,Example 5-66 Using logging.logger in persistence.xml
96,"<property name=""eclipselink.logging.logger"" value=""JavaLogger""/>"
96,Example 5-67 shows how to use this parameter in a property map.
96,Example 5-67 Using logging.logger in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_LOGGER,"
96,"""acme.loggers.MyCustomLogger"";"
96,See Also
96,"For more information, see:"
96,Logging examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,Custom logger http://wiki.eclipse.org/EclipseLink/Examples/JPA/CustomLogger
96,logging.parameters
96,Use the eclipselink.logging.parameters property to define if SQL bind parameters are included in exceptions and logs.
96,Note:
96,This parameter applies to bind parameters only. Parameters are always displayed when not using binding.
96,Values
96,Table 5-61 describes this persistence property's values.
96,Table 5-61 Valid Values for logging.parameters
96,Value
96,Description
96,true
96,(Default) Display the parameters.
96,false
96,Do not display the parameters.
96,Usage
96,"By default, when using logging.level of FINE (or greater), SQL bind parameters are displayed. Use this parameter to override the default behavior."
96,Examples
96,Example 5-58 shows how to use this parameter in the persistence.xml file.
96,Example 5-68 Using logging.parameters in persistence.xml
96,"<paramter name=""eclipselink.logging.parameters"" value=""false""/>"
96,See Also
96,"For more information, see:"
96,"""logging.level"""
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,logging.session
96,Use the eclipselink.logging.session property to specify if EclipseLink should include a session identifier in each log message.
96,Values
96,Table 5-62 describes this persistence property's values.
96,Table 5-62 Valid Values for logging.session
96,Value
96,Description
96,true
96,(Default) Log a session identifier.
96,false
96,Do not log a session identifier.
96,Usage
96,This setting is applicable to messages that require a database connection such as SQL and the transaction information to determine on which underlying session (if any) the message was sent.
96,Examples
96,Example 5-69 shows how to use this property in the peristence.xml file.
96,Example 5-69 Using logging.session in persistence.xml file
96,"<property name=""eclipselink.logging.session"" value=""false"" />"
96,Example 5-70 shows how to use this property in a property map.
96,Example 5-70 Using logging.session in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_SESSION, ""false"");"
96,See Also
96,"For more information, see:"
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""logging.level"""
96,logging.thread
96,Use the eclipselink.logging.thread property to specify if EclipseLink should include a thread identifier in each log message.
96,Values
96,Table 5-63 describes this persistence property's values.
96,Table 5-63 Valid Values for logging.thread
96,Value
96,Description
96,true
96,(Default) Log a thread identifier.
96,false
96,Do not log a thread identifier.
96,Usage
96,You should use this property when running multi-threaded applications. EclipseLink will include a hashcode of the thread.
96,Examples
96,Example 5-71 shows how to use this property in the peristence.xml file.
96,Example 5-71 Using logging.thread in persistence.xml file
96,"<property name=""eclipselink.logging.thread"" value=""false"" />"
96,Example 5-72 shows how to use this property in a property map.
96,Example 5-72 Using logging.thread in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_THREAD, ""false"");"
96,See Also
96,"For more information, see:"
96,"""logging.level"""
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,logging.timestamp
96,Use the eclipselink.logging.timestamp property to specify if EclipseLink should include a timestamp in each log message.
96,Values
96,Table 5-64 describes this persistence property's values.
96,Table 5-64 Valid Values for logging.timestamp
96,Value
96,Description
96,true
96,(Default) Log a timestamp.
96,false
96,Do not log a timestamp.
96,Examples
96,Example 5-73 shows how to use this property in the peristence.xml file.
96,Example 5-73 Using logging.timestamp in persistence.xml file
96,"<property name=""eclipselink.logging.timestamp"" value=""false"" />"
96,Example 5-74 shows how to use this property in a property map.
96,Example 5-74 Using logging.timestamp in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.LOGGING_TIMESTAMP, ""false"");"
96,See Also
96,"For more information, see:"
96,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
96,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
96,"""logging.level"""
96,metadata-source
96,Use the eclipselink.metadata-source property to specify the MetadataSource implementation EclipseLink uses to read metadata.
96,Values
96,Table 5-65 describes this persistence property's values.
96,Table 5-65 Valid Values for metadata-source
96,Value
96,Description
96,XML
96,Use XMLMetadataSource.
96,Custom metadata source
96,A custom class name which implements MetadataSource.
96,Usage
96,Use this property with eclipselink.metadata-source.xml.file to access an external mapping file at a fixed URL for a persistence unit.
96,Examples
96,Example 5-75 shows how to use this property in the persistence.xml file.
96,Example 5-75 Using metadata-source in persistence.xml
96,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
96,"<property name=""eclipselink.metadata-source.xml.file"" value=""c:/myfile.xml""/>"
96,See Also
96,"For more information, see:"
96,"""metadata-source.send-refresh-command"""
96,"""metadata-source.xml.file"""
96,"""metadata-source.xml.url"""
96,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
96,"""Extensible Entities"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/Extensible_Entities"
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,metadata-source.properties.file
96,"Use the eclipselink.metadata-source.properties.file property to specify the name of the metadata repository properties file to read from, using the classloader to find the resource."
96,Values
96,Table 5-66 describes this persistence property's values.
96,Table 5-66 Valid Values for metadata-repository.properties.file
96,Value
96,Description
96,Filename
96,Name of the metadata source XML file.
96,Usage
96,Use this property with eclipselink.metadata-source when using an XML repository.
96,Examples
96,Example 5-76 shows how to use this property in the persistence.xml file.
96,Example 5-76 Using metadata-source.properties.file in persistence.xml
96,"<property name=""eclipselink.metadata-source.properties.file"""
96,"value=""c:\myproperties.xml""/>"
96,See Also
96,"For more information, see:"
96,"""metadata-source"""
96,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,metadata-source.send-refresh-command
96,Use the eclipselink.metadata-source.send-refresh-command property with cache coordination for a clustered environment to control how EclipseLink sends RCM refresh metadata commands to the cluster.
96,Values
96,Table 5-67 describes this persistence property's values.
96,Table 5-67 Valid Values for metadata-source.send-refresh-command
96,Value
96,Description
96,true
96,"(Default) To propogate refresh commands to the cluster, you must configure RCM and use the eclipselink.deploy-on-startup property."
96,false
96,Does not propagate refresh commands to the cluster.
96,Usage
96,"If cache coordination is configured and the session is deployed on startup, this property controls the sending of RCM refresh metadata commands to the cluster."
96,These commands will cause the remote instances to refresh their metadata.
96,Examples
96,Example 5-77 shows how to use this property in the persistence.xml file.
96,Example 5-77 Using metadata-source.send-refresh-command in persistence.xml
96,"<property name=""eclipselink.metadata-source-refresh-command"" value=""false""/>"
96,Example 5-78 shows how to use this property in a property map.
96,Example 5-78 Using metadata-source-refresh-command in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.METADATA_SOURCE_RCM_COMMAND, ""false"");"
96,See Also
96,"For more information, see:"
96,"""metadata-source"""
96,"""deploy-on-startup"""
96,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,metadata-source.xml.file
96,"Use the eclipselink.metadata-repository.xml.file property to specify the name of the metadata repository XML file to read from, using the classloader to find the resource."
96,Values
96,Table 5-68 describes this persistence property's values.
96,Table 5-68 Valid Values for metadata-source.xml.file
96,Value
96,Description
96,filename
96,Metadata repository.xml file.
96,Usage
96,Use this property with the eclipselink.metadata-source property when using an XML repository.
96,Examples
96,Example 5-79 shows how to use this property in the persistence.xml file.
96,Example 5-79 Using metadata-source.xml.file in persistence.xml
96,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
96,"<property name=""eclipselink.metadata-source.xml.file"" value=""c:/myfile.xml""/>"
96,See Also
96,"For more information, see:"
96,"""metadata-source"""
96,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,metadata-source.xml.url
96,Use the eclipselink.metadata-source.xml.url property to specify the location of an external mapping file.
96,Values
96,Table 5-69 describes this persistence property's values.
96,Table 5-69 Valid Values for metadata-source.xml.url
96,Value
96,Description
96,url
96,Specifies the metadata repository of the XML URL.
96,Usage
96,The metadata-source property must be set to XML.
96,Examples
96,Example 5-75 shows how to use this property in the persistence.xml file.
96,Example 5-80 Using metadata-source.xml.url in persistence.xml
96,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
96,"<property name=""eclipselink.metadata-source.xml.url"" value=""http://myfile.xml""/>"
96,See Also
96,"For more information, see:"
96,"""metadata-source"""
96,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
96,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
96,multitenant.tenants-share-cache
96,Use the eclipselink.multitenant.tenants-share-cache property to specify if multitenant entities will share the L2 cache.
96,Values
96,Table 5-70 describes this persistence property's values.
96,Table 5-70 Valid Values for multitenant.tenants-share-cache
96,Value
96,Description
96,true
96,Multitenant entities will use an protected cache.
96,false
96,(Default) Multitenant entities will use an isolated cache.
96,Usage
96,WARNING:
96,"When this setting is false, queries that use the cache may return data from other tenants when using the PROTECTED setting."
96,Examples
96,Example 5-81shows how to use this property in the persistence.xml file.
96,Example 5-81 Using multitenant.tenants-share-cache in persistence.xml
96,"<property name=""eclipselink.multitenant.tenants-share-cache"" value=""true"" />"
96,Example 5-82 shows how to use this property in a property map.
96,Example 5-82 Using multitenant.tenants-share-cache in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.MULTITENANT_TENANTS_SHARE_CACHE,"
96,"""true"");"
96,See Also
96,"For more information, see:"
96,"""@Multitenant"""
96,Multitenant examples: http://wiki.eclipse.org/EclipseLink/Examples/JPA/Multitenant
96,"""Using Multitenancy"" in Solutions Guide for EclispeLink"
96,multitenant.tenants-share-emf
96,Use the eclipselink.multitenant.shared-emf property to specify if multitenant entities will be used within a shared entity manager factory.
96,Values
96,Table 5-71 describes this persistence property's values.
96,Table 5-71 Valid Values for multitenant.tenants-share-emf
96,Value
96,Description
96,true
96,(Default) Multitenant entities will be used.
96,false
96,Specify a unique session name.
96,Usage
96,"When setting it to false, you are required to provide a unique session name."
96,Examples
96,Example 5-83 shows how to use this property in the persistence.xml file.
96,Example 5-83 Using multitenant.tenants-share-emf in persistence.xml
96,"<property name=""eclipselink_multitenant_tenants_share_emf"" value=""true"" />"
96,See Also
96,"For more information, see:"
96,"""@Multitenant"""
96,Multitenant examples: http://wiki.eclipse.org/EclipseLink/Examples/JPA/Multitenant
96,"""Using Multitenancy"" in Solutions Guide for EclispeLink"
96,nosql.connection-factory
96,Use the eclipselink.nosql.connection-factory property to specify the JNDI name of a JCA ConnectionFactory or a JCA ConnectionFactory class name that connects to the NoSQL data-source.
96,Values
96,Table 5-72 describes this persistence property's values.
96,Table 5-72 Valid Values for nosql.connection-factory
96,Value
96,Description
96,connection factory
96,JNDI name or class name of the JCA Connection Factory.
96,Usage
96,"This property allows the JCA ConnectionFactory to be used with a NoSql or EIS adapter for a NoSQL datasource (that is, a non-relationship datasource such as a legacy database, NoSQL database, XML database, transactional and messaging systems, or ERP systems)."
96,Examples
96,Example 5-84 shows how to use this property in the persistence.xml file.
96,Example 5-84 Using nosql.connection-factory in persistence.xml
96,"<property name=""eclipselink.nosql.connection-factory"""
96,"value=""MyConnectionFactory"" />"
96,See Also
96,"For more information, see:"
96,"""@NoSql"""
96,"""nosql.property"""
96,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
96,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
96,"""Using NoSQL Databases"" in Understanding EclipseLink"
96,"""Using EclipseLink with Nonrelational Databases"" in Solutions Guide for EclispeLink"
96,nosql.connection-spec
96,Use the eclipselink.nosql.connection-spec property to specify an EISConnectionSpec class name that defines how to connect to the NoSQL datasource.
96,Values
96,Table 5-73 describes this persistence property's values.
96,Table 5-73 Valid Values for nosql.connection-spec
96,Value
96,Description
96,classname
96,IESConnectionSpec classname
96,Usage
96,"This property allows the JCA ConnectionFactory to be used with a NoSql or EIS adapter for a NoSQL datasource (that is, a non-relationship datasource such as a legacy database, NoSQL database, XML database, transactional and messaging systems, or ERP systems)."
96,Examples
96,See Example 5-85 for information on how to use this property.
96,See Also
96,"For more information, see:"
96,"""@NoSql"""
96,"""nosql.property"""
96,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
96,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
96,"""Using NoSQL Databases"" in Understanding EclipseLink"
96,"""Using EclipseLink with Nonrelational Databases"" in Solutions Guide for EclispeLink"
96,nosql.property
96,Use the eclipselink.nosql.property property to set NoSQL-specific connection properties.
96,Values
96,Table 5-74 describes this persistence property's values.
96,Table 5-74 Valid Values for nosql.property
96,Value
96,Description
96,property name
96,A NoSQL property.
96,Usage
96,Append the NoSQL-specific property name to this property.
96,Examples
96,Example 5-85 shows how to use this property in the persistence.xml file.
96,Example 5-85 Using nosql.property in persistence.xml
96,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
96,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
96,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_2_0.xsd"""
96,"version=""2.0"">"
96,"<persistence-unit name=""acme"" transaction-type=""RESOURCE_LOCAL"">"
96,<provider>org.eclipse.persistence.jpa.PersistenceProvider</provider>
96,<exclude-unlisted-classes>false</exclude-unlisted-classes>
96,<properties>
96,"<property name=""eclipselink.target-database"""
96,"value=""org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform""/>"
96,"<property name=""eclipselink.nosql.connection-spec"""
96,"value=""org.eclipse.persistence.nosql.adapters.mongo.MongoConnectionSpec""/>"
96,"<property name=""eclipselink.nosql.property.mongo.port"" value=""27017,"
96,"27017""/>"
96,"<property name=""eclipselink.nosql.property.mongo.host"" value=""host1,"
96,"host2""/>"
96,"<property name=""eclipselink.nosql.property.mongo.db"" value=""acme""/>"
96,</properties>
96,</persistence-unit>
96,</persistence>
96,See Also
96,"For more information, see:"
96,"""@NoSql"""
96,"""Using Non-SQL Databases"" in Understanding EclipseLink"
96,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
96,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
96,"""nosql.connection-factory"""
96,"""nosql.connection-spec"""
96,oracle.proxy-type
96,Use the eclipselink.oracle.proxy-type property to specify the proxy type to be passed to the OracleConnection.openProxySession method.
96,Values
96,Table 5-75 describes this persistence property's values.
96,Table 5-75 Valid Values for oracle.proxy-type
96,Value
96,Description
96,USER_NAME
96,This type uses a user name for authentication when creating a proxy connection.
96,DISTINGUISED_NAME
96,This type uses a distinguished name for authentication when creating a proxy connection.
96,CERTIFICATE
96,This type uses a digital certificate for authentication when creating a proxy connection.
96,Usage
96,This property requires Oracle JDBC version 10.1.0.2 or later and eclipselink.target-database must be configured to use Oracle9 or later.
96,"Typically, you should set this property into EntityManager, through a createEntityManager method or by using proprietary setProperties method on EntityManagerImpl. This causes EntityManager to use proxy connection for writing and reading inside transaction."
96,"If proxy-type and the corresponding proxy property set into EntityManagerFactory, all connections created by the factory will be proxy connections."
96,Examples
96,Example 5-86 shows how to use the property with EntityManager.
96,Example 5-86 Using eclipselink.oracle.proxy-type with EntityManager
96,Map emProperties = new HashMap();
96,"emProperties.put(""eclipselink.oracle.proxy-type"","
96,OracleConnection.PROXYTYPE_USER_NAME);
96,"emProperties.put(OracleConnection.PROXY_USER_NAME, ""john"");"
96,EntityManager em = emf.createEntityManager(emProperties);
96,With injection:
96,"entityManager.setProperty(”eclipselink.oracle.proxy-type”,"
96,OracleConnection.PROXYTYPE_USER_NAME);
96,"entityManager.setProperty(OracleConnection.PROXY_USER_NAME, ”john”);"
96,See Also
96,"For more information, see:"
96,"""target-database"""
96,Oracle Proxy Authentication Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/Oracle/Proxy
96,Auditing example http://wiki.eclipse.org/EclipseLink/Examples/JPA/Auditing
96,orm.throw.exceptions
96,Use the eclipselink.orm.throw.exceptions property to specify if EclipseLink throws an exception or logs a warning when encountering a problem with any of the files in the <mapping-file> element of the persistence.xml file.
96,Values
96,Table 5-76 describes this persistence property's values.
96,Table 5-76 Valid Values for orm.throw.exceptions
96,Value
96,Description
96,true
96,(Default) Throw an exception.
96,false
96,Log a warning only.
96,Examples
96,Example 5-87 shows how to use this property in the persistence.xml file.
96,Example 5-87 Using orm.throw.exceptions in persistence.xml
96,"<property name=""oracle.orm.throw.exceptions"" value=""false""/>"
96,Example 5-88 shows how to use this property in a property map.
96,Example 5-88 Using orm.throw.exceptions in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.ECLIPSELINK_ORM_THROW_EXCEPTIONS,"
96,"""false"");"
96,See Also
96,"For more information, see:"
96,"""exception-handler"""
96,orm.validate.schema
96,Use the orm.validate.schema property to override orm.xml schema validation from its default value of false.
96,Values
96,Table 5-77 describes this persistence property's values.
96,Table 5-77 Valid Values for orm.validate.schema
96,Value
96,Description
96,true
96,Enables schema velidation on on orm.xml file.
96,false
96,(Default) No schema validation is performed on the orm.xml file.
96,Usage
96,Use orm.validate.schema to enable orm.xml schema validation.
96,Examples
96,Example 5-89 shows how to use this property in the persistence.xml file.
96,Example 5-89 Using orm.validate.schema in persistence.xml
96,"<property name=""eclipselink.orm.validate.schema"" value=""true""/>"
96,Example 5-90 shows how to use this property in a property map.
96,Example 5-90 Using orm.validate.schema in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.ORM_VALIDATE_SCHEMA, ""true"");"
96,partitioning
96,"Use the eclipselink.partitioning property to set the default PartitioningPolicy for a persistence unit. The value must be the name of an existing, defined PartitioningPolicy."
96,Values
96,Table 5-78 describes this persistence property's values.
96,Table 5-78 Valid Values for partitioning
96,Value
96,Description
96,name
96,"An existing, defined PartitioningPolicy."
96,Usage
96,Use this property to partition data for a class across multiple difference databases or across a database cluster such as Oracle RAC. Partitioning may provide improved scalability by allowing multiple database machines to service requests.
96,"If multiple partitions are used to process a single transaction, use JTA (Java Transcription API) for proper XA transaction support."
96,Examples
96,Example 5-91 shows how to use this property in the persistence.xml file.
96,Example 5-91 Using partitioning in persistence.xml
96,"<property name=""eclipselink.partitioning"" value=""Replicate"" />"
96,See Also
96,"For more information, see:"
96,Partitioning Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Partitioning
96,"""@Partitioning"""
96,partitioning.callback
96,"Use the eclipselink.partitioning.callback property to integrate an external DataSource's affinity support, such as UCP."
96,Values
96,Table 5-79 describes this persistence property's values.
96,Table 5-79 Valid Values for eclipselink.partitioning.callback
96,Value
96,Description
96,value
96,A class that implements the DataPartitioningCallBack interface.
96,Usage
96,The value must be set to the full class name.
96,Examples
96,Example 5-92 shows how to use this property in the persistence.xml file.
96,Example 5-92 Using partitioning.callback in persistence.xml
96,"<property name=""eclipselink.partitioning.callback"""
96,"value=""mypacakge.MyDataPartitioningCallback""/>"
96,Example 5-93 shows how to use this property in a property map.
96,Example 5-93 Using partitioning.callback in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PARTITIONING_CALLBACK,"
96,"""mypackage.MyDataPartitioningCallback"");"
96,persistence-context.close-on-commit
96,Use the eclipselink.persistence-context.close-on-commit property to specify if the EntityManager will be closed or not used after commit (not extended).
96,Values
96,Table 5-80 describes this persistence property's values.
96,Table 5-80 Valid Values for persistence-context.close-on-commit
96,Value
96,Description
96,true
96,Closes the EntityManager after a commit.
96,false
96,(Default) Does not close the EntityManager after a commit.
96,Usage
96,"For a container-managed EntityManager and most managed applications, you normally set this property to false. This setting avoids additional performance overhead of resuming the persistence context after a commit() transaction."
96,"The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. Alternatively, to apply the property to specific EntityManagers, pass it to createEntityManager method."
96,Examples
96,Example 5-94 shows how to use this property in the persistence.xml file.
96,Example 5-94 Using persistence-context.close-on-commit in persistence.xml
96,"<property name=""eclipselink.persistence-context.close-on-commit"" value=""true""/>"
96,Example 5-95 shows how to use this property in a property map.
96,Example 5-95 Using persistence-context.close-on-commit in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_CLOSE_ON_COMMIT,"
96,"""true"");"
96,persistence-context.commit-without-persist-rules
96,"Use the eclipselink.persistence-context.commit-without-persist-rules property to specify if the EntityManager will search all managed objects and persist any related non-managed new objects that are found, ignoring any absence of CascadeType.PERSIST settings."
96,Values
96,Table 5-81 describes this persistence property's values.
96,Table 5-81 Valid Values for persistence-context.commit-without-persist-rules
96,Value
96,Description
96,true
96,Cascades Entity life-cycle Persist operations to related entities and uses the CascadeType.PERSIST settings.
96,false
96,(Default) Does not cascase Entitiy life-cycle Persist operations to related entities and does not use the CascadeType.PERSIST settings.
96,Usage
96,Setting this property to true replicates the traditional EclipseLink native functionality.
96,Examples
96,Example 5-96 shows how to use this property in the persistence.xml file.
96,Example 5-96 Using persistence-context.commit-without-persist-rules in persistence.xml
96,"<property name=""eclipse.persistence-context.commit-without-persist-rules"""
96,"value=""true""/>"
96,Example 5-97 shows how to use this property in a property map.
96,Example 5-97 Using persistence-context.commit-without-persist-rules in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.PERSISTENCE_CONTEXT_COMMIT_WITHOUT_PERSIST_RULES,"
96,"""true"");"
96,persistence-context.flush-mode
96,Use the eclipselink.persistence-context.flush-mode property to configure the EntityManager FlushMode to be set as a persistence property and specify when flushing occurs.
96,Values
96,Table 5-82 describes this persistence property's values.
96,Table 5-82 Valid Values for persistence-context.flush-mode
96,Value
96,Description
96,auto
96,(Default) Flushing occurs at query execution.
96,commit
96,Flushing occurs at transaction commit.
96,Usage
96,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to the createEntityManager method.
96,Examples
96,Example 5-98 shows how to use this property in the persistence.xml file.
96,Example 5-98 Using persistence-context.flush-mode in persistence.xml
96,"<property name=""eclipselink.persistence-context.flush-mode"" value=""commit"" />"
96,Example 5-99 shows how to use this property in a property map.
96,Example 5-99 Using persistence-context.flush-mode in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_FLUSH_MODE,"
96,"""false"");"
96,See Also
96,"For more information, see:"
96,"""flush"""
96,"""Enhancing Performance"" in Solutions Guide for EclispeLink"
96,persistence-context.persist-on-commit
96,Use the eclipselink.persistence-context.persist-on-commit property to specify if the EntityManager searches all managed objects and persists any related non-managed new objects that are cascade persist. This can be used to avoid the cost of performing this search if persist is always used for new objects.
96,Values
96,Table 5-83 describes this persistence property's values.
96,Table 5-83 Valid Values for persistence-context.persist-on-commit
96,Value
96,Description
96,true
96,(Default) Searches and persists related non-managed new objects that are cascade persist.
96,false
96,Does not search and persist related non-managed new objects that are cascade persist.
96,Usage
96,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to createEntityManager method.
96,Examples
96,Example 5-100 shows how to use this property in the persistence.xml file.
96,Example 5-100 Using persistence-context.persist-on-commit in persistence.xml
96,"<property name=""eclipselink.persistence-context.persist-on-commit"" value=""false""/>"
96,Example 5-101 show how to use this property in a property map.
96,Example 5-101 Using persistence-context.persis-on-commit in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_PERSIST_ON_COMMIT,"
96,"""false"");"
96,persistence-context.reference-mode
96,"Use the eclipselink.persistence-context.reference-mode property to specify if hard or soft (that is, weak) references are used within the Persistence Context."
96,Values
96,Table 5-84 describes this persistence property's values.
96,Table 5-84 Valid Values for persistence-context.reference-mode
96,Value
96,Description
96,hard
96,(Default) EclipseLink references all objects through hard references. These objects will not be available for garbage collection until the referencing artifact (such as the persistence context or unit of work) is released/cleared or closed.
96,weak
96,"References to objects supporting active attribute change tracking (see ""@ChangeTracking"") will be held by weak references. That is, any object no longer referenced directly or indirectly will be available for garbage collection. When a change is made to a change-tracked object, that object is moved to a hard reference and will not be available for garbage collection until flushed."
96,Note: Any changes that have not been flushed in these entities will be lost.
96,"New and removed objects, as well as objects that do not support active attribute change tracking, will also be held by hard references and will not be available for garbage collection."
96,force_weak
96,"All objects, including non-change-tracked objects, are to be held by weak references. When a change is made to a change-tracked object (see ""@ChangeTracking""), that object is moved to a hard reference and will not be available for garbage collection until flushed. However, any objects that do not support active attribute change tracking may be garbage collected before their changes are flushed to a database, which can potentially result in a loss of changes."
96,New and removed objects will be held by hard references and will not be available for garbage collection.
96,Usage
96,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to createEntityManager method.
96,Examples
96,Example 5-102 shows how to use this property in a persistence.xml file.
96,Example 5-102 Using persistence-context.reference-mode in persistence.xml
96,"<property name=""eclipselink.persistence-context.reference-mode"""
96,"value=""FORCE_WEAK""/>"
96,Example 5-103 shows how to use this property in a property map.
96,Example 5-103 Using persistence-context.reference-mode in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_REFERENCE_MODE,"
96,ReferenceMode.FORCE_WEAK);
96,See Also
96,"For more information, see:"
96,"""@ChangeTracking"""
96,persistenceunits
96,"Use the eclipselink.persistenceunits property to specify the set of persistence unit names that will be processed when generating the canonical model. By default, EclipseLink uses all persistence units available in all persistence XML files."
96,Values
96,Table 5-85 describes this persistence property's values.
96,Table 5-85 Valid Values for persistenceunits
96,Value
96,Description
96,names
96,A comma separated list of persistence units
96,"Note: When specifying multiple persistence units, you cannot include a comma ( , ) in the name of a persistence unit."
96,Examples
96,Example 5-104 shows how to use this property in the persistence.xml file.
96,Example 5-104 Using persistenceunits in persistence.xml
96,"<property name=""eclipselink.persistenceunits"" value=""mypu1, mypu2""/>"
96,persistencexml
96,"Use the eclipselink.persistencexml property to specify the full resource name in which to look for the persistence XML files. If omitted, EclipseLink uses the default location: META-INF/persistence.xml."
96,Note:
96,"Currently, this property is used only for the canonical model generator."
96,Values
96,Table 5-86 describes this persistence property's values.
96,Table 5-86 Valid Values for persistencexml
96,Value
96,Description
96,resource name
96,Location of the persistence.xml file.
96,Usage
96,"This property is only used by EclipseLink when it is locating the configuration file. When used within an EJB/Spring container in container-managed mode, the locating and reading of this file is done by the container and will not use this configuration."
96,"If you want to change the default location, use persisencexml.default."
96,Examples
96,Example 5-105 shows how to use this property in the persistence.xml file.
96,Example 5-105 Using persistencexml in persistence.xml
96,"<property name=""eclipselink.persistencexml"" value=""resources/persistence.xml""/>"
96,See Also
96,"For more information, see:"
96,"""persisencexml.default"""
96,persisencexml.default
96,Use the eclipselink.persistencexml.default property to specify the default resource location where the persistence.xml configuration file is located. The default location is META-INF/persistence.xml.
96,Values
96,Table 5-87 describes this persistence property's values.
96,Table 5-87 Valid Values for persistencexml.default
96,Value
96,Description
96,resource location
96,Default resource location of the persistence.xml file.
96,Examples
96,Example 5-106 shows how to use this property in a property map.
96,Example 5-106 Using persistencexml.default in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.ECLIPSELINK_PERSISTENCE_XML_DEFAULT,"
96,"""resources/persistence.xml"");"
96,profiler
96,Use the eclipselink.profiler property to specify which performance profiler to use in order to capture runtime statistics.
96,Values
96,Table 5-88 describes this persistence property's values.
96,Table 5-88 Valid Values for profiler
96,Value
96,Description
96,NoProfiler
96,(Default) Do not use a performance profiler.
96,PerformationMonitor
96,Use EclipseLink performance monitor org.eclipse.persistence.tools.profiler.PerformanceMonitor.
96,PerformanceProfiler
96,Use EclipseLink performance profiler (org.eclipse.persistence.tools.profiler.PerformanceProfiler).
96,QueryMonitor
96,Monitor query executions and cache hits (org.eclipse.persistence.tools.profiler.QueryMonitor class).
96,This option provides a simple low-overhead means for measuring performance of query executions and cache hits. You may want to use this option for performance analysis in a complex system.
96,DMSProfiler
96,Use org.eclipse.persistence.tools.profiler.oracle.DMSPerformanceProfiler. This property is specific to the Oracle Dynamic Monitoring Service (DMS).
96,Custom profiler
96,Specify a custom profiler class name which implements SessionProfiler and provides a no-argument constructor.
96,Examples
96,Example 5-107 shows how to use this property in the persistence.xml file.
96,Example 5-107 Using profiler in persistence.xml
96,"<property name=""eclipselink.profiler"" value=""PerformanceProfiler""/>"
96,Example 5-108 shows how to use this property in a property map.
96,Example 5-108 Using profiler in a Property Map
96,import org.eclipse.persistence.config.ProfilerType;
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.PROFILER,"
96,ProfilerType.PerformanceProfiler);
96,See Also
96,"For more information, see:"
96,session.customizer
96,"Use the eclipselink.session.customizer property to specify a session customizer class that implements the org.eclipse.persistence.config.SessionCustomizer interface. The class must provide a default, no argument constructor."
96,Values
96,Table 5-89 describes this persistence property's values.
96,Table 5-89 Valid Values for session.customizer
96,Value
96,Description
96,class name
96,Fully qualified class name of a SessionCustomizer class.
96,Usage
96,You can use the customize method of the class (which takes an org.eclipse.persistence.sessions.Session) to programmatically access advanced EclipseLink session API. You can use the session customizer class to define multiple session event listeners.
96,Examples
96,Example 5-109 shows how to use this property in the persistence.xml file.
96,Example 5-109 Using session.customizer in persistence.xml
96,"<property name=""eclipselink.session.customizer"""
96,"value=""acme.sessions.MySessionCustomizer""/>"
96,Example 5-110 shows how to use this property in a property map.
96,Example 5-110 Using session.customizer in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.SESSION_CUSTOMIZER,"
96,"""acme.sessions.MySessionCustomizer"");"
96,See Also
96,"For more information, see:"
96,"""session-event-listener"""
96,session.include.descriptor.queries
96,Use the eclipselink.session.include.descriptor.queries property to specify whether all descriptor named queries are copied to the session for use by the entity manager.
96,Values
96,Table 5-90 describes this persistence property's values.
96,Table 5-90 Valid Values for session.include.descriptor.queries
96,Value
96,Description
96,true
96,Copying is enabled.
96,false
96,(Default) Copying is disabled.
96,Examples
96,Example 5-111 shows how to use this property in the persistence.xml file.
96,Example 5-111 Using session.include.descriptor.queries in persistence.xml
96,"<property name=""eclipselink.session.include.descriptor.queries"" value=""true""/>"
96,Example 5-112 shows how to use this property in a property map.
96,Example 5-112 Using session.include.descriptor.queries in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.INCLUDE_DESCRIPTOR_QUERIES, ""true"");"
96,session-event-listener
96,Use the eclipselink.session-event-listener property to specify a descriptor event listener to be added during bootstrapping.
96,Values
96,Table 5-91 describes this persistence property's values.
96,Table 5-91 Valid Values for session-event-listener
96,Value
96,Description
96,Class name
96,A qualified class name for a class that implements the org.eclipse.persistence.sessions.SessionEventListener interface.
96,Usage
96,"To define multiple event listener, you can use a session.customizer class."
96,Examples
96,Example 5-113 shows how to use this property in a persistence.xml file.
96,Example 5-113 Using session-event-listener in persistence.xml
96,"<property name=""eclipselink.session-event-listener"""
96,"value=""mypackage.MyClass.class""/>"
96,Example 5-113 shows how to use this property in a property map.
96,Example 5-114 Using session-event-listener in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.SESSION_EVENT_LISTENER_CLASS,"
96,"""mypackage.MyClass.class"");"
96,See Also
96,"For more information, see:"
96,"""session.customizer"""
96,session-name
96,Use the eclipselink.session-name property to configure a unique name to use when storing the singleton server session within the SessionManager.
96,Values
96,Table 5-92 describes this persistence property's values.
96,Table 5-92 Valid Values for session.name
96,Value
96,Description
96,Name
96,"Unique session name to use instead of the default, EclipseLink-generated session name."
96,Usage
96,"By default, EclipseLink generates a unique session name. You can provide a custom, unique, session name with this property."
96,"When using a sessions-xml file, you must include this session name as the name of the session in the sessions-xml file."
96,Examples
96,Example 5-115 shows how to use this property in the persistence.xml file.
96,Example 5-115 Using session-name in persistence.xml
96,"<property name=""eclipselink.session-name"" value=""MySession""/>"
96,Example 5-116 shows how to use this property in a property map.
96,Example 5-116 Using session-name in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.SESSION_NAME, ""MySession"");"
96,See Also
96,"For more information, see:"
96,"""sessions-xml"""
96,sessions-xml
96,Use the eclipselink.sessions-xml property to use a specified native sessions.xml configuration file (which references a project.xml file) to load configuration and mapping information instead of JPA annotations or EclipseLink XML (as shown in Figure 5-1).
96,Values
96,Table 5-93 describes this persistence property's values.
96,Table 5-93 Valid Values for sessions-xml
96,Value
96,Description
96,configuration file
96,"The resource name of the sessions XML file. If you do not specify the value for this property, it will not be used."
96,Usage
96,"You can use the eclipselink.sessions-xml property as an alternative to using annotations and deployment XML. With this property, EclipseLink builds an in-memory EclipseLink session and project based on this metadata (as shown in Figure 5-1). You can acquire a persistence manager and use it, having defined all entities and so on using only EclipseLink sessions.xml."
96,Figure 5-1 Using the eclipselink.sessions-xml Persistence Property
96,"Description of ""Figure 5-1 Using the eclipselink.sessions-xml Persistence Property"""
96,Examples
96,Example 5-117 shows how to use this property in a persistence.xml file.
96,Example 5-117 Using sessions-xml in the persistence.xml file
96,"<property name=""eclipselink.sessions-xml"" value=""mysession.xml""/>"
96,Example 5-118 shows how to use this property in a property map.
96,Example 5-118 Using sessions-xml in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.SESSIONS_XML, ""mysession.xml"");"
96,See Also
96,"For more information, see:"
96,"""Overriding and Merging"""
96,target-database
96,"Use the eclipselink.target-database property to specify the database to use, controlling custom operations and SQL generation for the specified database."
96,Values
96,Table 5-94 describes this persistence property's values.
96,Table 5-94 Valid Values for target-database
96,Value
96,Description
96,Defined in the TargetDatabase class or a fully qualified class name that extends DatabasePlatform
96,Specify your database:
96,Attunity
96,Auto (Default): EclipseLink attempts to access the database and the JDBC metadata to determine the target database.
96,Cloudscape
96,"Database: Use a generic database, if your target database is not listed and your JDBC driver does not support the metadata required for Auto."
96,DB2
96,DB2Mainframe
96,DBase
96,Derby
96,HSQL
96,Informix
96,JavaDB
96,MaxDB
96,MySQL
96,MySQL4
96,Oracle
96,Oracle10
96,Oracle11
96,Oracle8
96,Oracle9
96,PointBase
96,PostgreSQL
96,SQLAnywhere
96,SQLServer
96,Sybase
96,Symfoware
96,TimesTen
96,Usage
96,"If eclipselink.validation-only = true, you cannot use an Auto class name or short name."
96,Examples
96,Example 5-119 shows how to use this property in the persistence.xml file.
96,Example 5-119 Using target-database in persistence.xml
96,"<property name=""eclipselink.target-database"" value=""Oracle""/>"
96,"<property name=""eclipselink.target-database"""
96,"value=""org.eclipse.persistence.platform.database.HSQLPlatform""/>"
96,Example 5-120 shows how to use this property in a property map.
96,Example 5-120 Using target-database in a Property Map
96,import org.eclipse.persistence.config.TargetDatabase;
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.TARGET_DATABASE, TargetDatabase.Oracle);"
96,See Also
96,"For more information, see:"
96,"""validation-only"""
96,target-server
96,"Use the eclipselink.target-server property to configure the ServerPlatform that will be used to enable integration with a host container. If this property is not specified, the runtime will attempt to detect which ServerPlatform should be used. If detection fails, Default will be used."
96,Values
96,Table 5-95 describes this persistence property's values.
96,Table 5-95 Valid Values for target-server
96,Value
96,Description
96,Defined in the TargetServer class
96,Specify your application server:
96,JBoss: JBoss Application Server
96,OC4J: OC4J persistence provider
96,SAPNetWeaver_7_1: SAP NetWeaver Application Server 7.1 (and higher)
96,SunAS9: Sun Application Server 9
96,WebLogic: Oracle WebLogic Server
96,WebLogic_10: Oracle WebLogic Server 10
96,WebLogic_9: Oracle WebLogic Server 9
96,WebSphere: IBM WebSphere
96,WebSphere_6_1: IBM WebSphere 6.1
96,WebSphere_7: IBM WebSphere 7
96,WebSphere_Liberty: IBM WebSphere Liberty
96,Default (TargetServer.None)
96,Usage
96,"In addition to the supplied values, you can specify a custom server platform by supply the full class name for the platform."
96,Specifying a name of the class implementing ExternalTransactionController sets CustomServerPlatform with this controller.
96,Examples
96,Example 5-121 shows how to use this property in a persistence.xml file.
96,Example 5-121 Using target-server in persistence.xml
96,"<property name=""eclipselink.target-server"" value=""OC4J_10_1_3""/>"
96,Example 5-122 shows how to use this property in a property map.
96,Example 5-122 Using target-server in a Property Map
96,import org.eclipse.persistence.config.TargetServer;
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertiesMap.put(PersistenceUnitProperties.TARGET_SERVER,"
96,TargetServer.OC4J_10_1_3);
96,See Also
96,"For more information, see:"
96,temporal.mutable
96,"Use the eclipselink.temporal.mutable property to configure the default for detecting changes to the temporal field (Date, Calendar)."
96,Values
96,Table 5-96 shows this persistence property's values.
96,Table 5-96 Valid Values for temporal.mutable
96,Value
96,Description
96,true
96,Changes to the object are detected. Disables weaving of attribute change tracking.
96,false
96,(Default) Changes to the object itself are not detected.
96,Usage
96,"By default, it is assumed that temporal fields are replaced, and the temporal object is not changed directly."
96,Examples
96,Example 5-123 shows how to use this property in the persistence.xml file.
96,Example 5-123 Using temporal.mutable in persistence.xml
96,"<property name=""eclipselink.temporal.mutable"" value=""true""/>"
96,Example 5-124 shows how to use this property in a property map.
96,Example 5-124 Using temporal.mutable in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.TEMPORAL_MUTABLE,"
96,"""true"");"
96,tenant-id
96,Use the eclipselink.tenant-id property to specify the default context property used to populate multitenant entities.
96,Values
96,Table 5-97 describes this persistence property's values.
96,Table 5-97 Valid Values for tenant-id
96,Value
96,Description
96,value
96,Name of the default context property.
96,Usage
96,This is a default multitenant property that can be used on its own or with other properties defined by you. You are not obligated to use this property. You are free to specify your own.
96,Examples
96,Example 5-125 shows how to use this property in the persistence.xmlfile.
96,Example 5-125 Using tenant-id in persistence.xml
96,"<property name=""eclipselink.tenant-id"" value=""Oracle""/>"
96,Example 5-126 shows how to use this property in a property map.
96,Example 5-126 Using tenant-id in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.MULTI_TENANT_PROPERTY_DEFAULT,"
96,"""Oracle"");"
96,transaction.join-existing
96,"Use the eclipselink.transaction.join-existing property to force the persistence context to read through the JTA-managed (""write"") connect"
96,ion in case there is an active transaction.
96,Values
96,Table 5-98 describes this persistence property's values.
96,Table 5-98 Valid Values for transaction.join-existing
96,Value
96,Description
96,true
96,Forces the persistence context to read through the JTA-managed connection.
96,false
96,(Default) Does not force the persistence context to read through the JTA-managed connection.
96,Usage
96,"The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. If the property set to true, objects read during transaction will not be placed into the shared cache unless they have been updated. Alternatively, to apply the property only to some EntityManagers, pass it to createEntityManager method."
96,Examples
96,Example 5-127 shows how to use this property in the persistence.xml file.
96,Example 5-127 Using transaction.join-existing in persistence.xml
96,"<property name=""eclipselink.transaction.join-existing"" value=""true""/>"
96,Example 5-128 shows how to use this property in a property map.
96,Example 5-128 Using transaction.join-existing in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.TRANSACTION_JOIN_EXISTING,"
96,"""true"");"
96,See Also
96,"For more information, see:"
96,"""Automated Tuning"" in Solutions Guide for EclispeLink"
96,tuning
96,The eclipselink.tuning property selects the type of tuner to use to configure the persistence unit.
96,Values
96,Table 5-99 describes this persistence property's values.
96,Table 5-99 Valid Values for tuning
96,Value
96,Description
96,standard
96,(Default) Uses the standard tuner and does not change any of the default configuration settings.
96,safe
96,Configures the persistence unit for debugging. This disables caching and several performance optimizations. The purpose is to provide a simplified development and debugging configuration.
96,custom tuner
96,Specifies the full class name of an implementation of the org.eclipse.persistence.tools.tuning.SessionTuner interface.
96,Usage
96,Use automated tuning to set multiple configuration properties as part of a single flag to perform dynamic tuning during different steps of application deployment.
96,Examples
96,Example 5-129 shows how to use this property in the persistence.xml file.
96,Example 5-129 Using tuning in persistence.xml
96,"<property name=""eclipselink.tuning"" value=""safe""/>"
96,validate-existence
96,Use the eclipselink.validate-existence property to specify if EclipseLink should verify an object's existence on persist().
96,Values
96,Table 5-100 describes this persistence property's values.
96,Table 5-100 Valid Values for validate-existence
96,Value
96,Description
96,true
96,EclipseLink verifies the object's existence.
96,false
96,"(Default) EclipseLink assumes the object is new, if it is not in the persistence context."
96,Usage
96,EclipseLink will throw an error if a validated object is not in the persistence context.
96,Examples
96,Example 5-130 shows how to use this property in the persistence.xml file.
96,Example 5-130 Using validate-existence in persistence.xml
96,"<property name=""eclipselink.validate-existence"" value=""true""/>"
96,Example 5-131 shows how to use this property in a proptery map.
96,Example 5-131 Using validate-existence in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.VALIDATE-EXISTENCE,"
96,"""true"");"
96,validation-only
96,Use the eclipselink.validation-only property to validate deployments by initializing descriptors but not connecting to the data source.
96,Values
96,Table 5-101 describes this persistence property's values.
96,Table 5-101 Valid Values for validation-only
96,Value
96,Description
96,true
96,EclipseLink will initialize the descriptors but not log in.
96,false
96,(Default) EclipseLink will initialize the descriptors and log in.
96,Usage
96,"When setting eclipselink.validation-only to true, you must also configure eclipselink.target-database with a non-Auto class name or a short name."
96,Examples
96,Example 5-132 show how to use this property in the persistence.xml file.
96,Example 5-132 Using validation-only in persistence.xml
96,"<property name=""eclipselink.validation-only"" value=""true""/>"
96,Example 5-133 shows how to use this property in a property map.
96,Example 5-133 Using validation-only in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,"propertyMap.put(PersistenceUnitProperties.VALIDATION_ONLY,"
96,"""true"");"
96,See Also
96,"For more information, see:"
96,"""target-database"""
96,weaving
96,"Use the eclipselink.weaving property to specify if EclipseLink weaves the entity classes. EclipseLink JPA uses weaving to enhance JPA entities for such things as lazy loading, change tracking, fetch groups, and internal optimizations."
96,Values
96,Table 5-102 describes this persistence property's values.
96,Table 5-102 Valid values for weaving
96,Value
96,Description
96,true
96,Weave the entity classes dynamically.
96,false
96,Do not weave the entity classes.
96,static
96,Weave the entity classes statically.
96,Examples
96,Example 5-134 shows how to use this property in the persistence.xml file.
96,Example 5-134 Using weaving in persistence.xml
96,"<property name=""eclipse.weaving"" value=""false""/>"
96,Example 5-135 shows how to use this property in a property map.
96,Example 5-135 Using weaving in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.WEAVING, ""false"");"
96,See Also
96,"For more information, see:"
96,"""Using Weaving"" in Understanding EclipseLink"
96,"""Enhancing Performance"" in Solutions Guide for EclipseLink"
96,"""weaving.changetracking"""
96,"""weaving.eager"""
96,"""weaving.fetchgroups"""
96,"""weaving.internal"""
96,"""@ChangeTracking"""
96,weaving.changetracking
96,Use the eclipselink.weaving.changetracking persistence property to:
96,Enable AttributeLevelChangeTracking through weaving.
96,Permit only classes with all mappings to change.
96,Permit tracking to enable change tracking. Mutable basic attributes prevent change tracking.
96,This property is enabled only when weaving is enabled.
96,Values
96,Table 5-103 describes this persistence property's values.
96,Table 5-103 Valid Values for weaving.changetracking
96,Value
96,Description
96,true
96,(Default) Enables this property.
96,false
96,Disables this property.
96,Examples
96,Example 5-136 shows how to use this property in the persistence.xml file.
96,Example 5-136 Using weaving.changetracking in persistence.xml
96,"<property name=""eclipse.weaving.changetracking"" value=""false""/>"
96,Example 5-137 shows how to use this property in a property map.
96,Example 5-137 Using weaving.changetracking in a Property Map
96,import org.eclipselink.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(Persistence.Unit.Properties.WEAVING_CHANGETRACKING,"
96,"value=""false"");"
96,See Also
96,"For more information, see:"
96,"""weaving"""
96,weaving.eager
96,Use the eclipselink.weaving.eager property to specify if EclipseLink uses indirection on eager relationships.
96,Values
96,Table 5-104 describes this persistence property's values.
96,Table 5-104 Valid Values for weaving.eager
96,Value
96,Description
96,true
96,Enables indirection on eager relationships through weaving.
96,false
96,(Default) Disables indirection on eager relationships through weaving.
96,Usage
96,"One-to-one and many-to-one mappings, even when configured with FetchType.EAGER, will effectively become ""lazy."""
96,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
96,Examples
96,Example 5-138 shows how to use this property in the persistence.xml file.
96,Example 5-138 Using weaving in persistence.xml
96,"<property name=""eclipselink.weaving.eager"" value=""true""/>"
96,Example 5-139 shows how to use this extension in a property map
96,Example 5-139 Using weaving in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.WEAVING_EAGER, ""true"");"
96,See Also
96,"For more information, see:"
96,"""weaving"""
96,weaving.fetchgroups
96,"Use the eclipselink.weaving.fetchgroups property to enable FetchGroups through weaving. When this is enabled, lazy direct mapping is supported, as well as descriptor and query-level FetchGroups."
96,FetchGroups allow partial objects to be read and written. Access to un-fetched attributes refreshes (fully-fetches) the object.
96,This property is only considered when weaving is enabled.
96,Values
96,Table 5-105 describes this persistence property's values.
96,Table 5-105 Valid Values for weaving.fetchgroups
96,Value
96,Description
96,true
96,(Default) Enables FetchGroups through weaving.
96,false
96,Disables FetchGroups through weaving.
96,Examples
96,Example 5-140 shows how to use this property in the persistence.xml file.
96,Example 5-140 Using weaving.fetchgroups in persistence.xml
96,"<property name=""eclipselink.weaving.fetchgroups value=""false""/>"
96,Example 5-141 shows how to use this property in a property map.
96,Example 5-141 Using weaving.fetchgroups in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.WEAVING_FETCHGROUPS, ""false"")"
96,See Also
96,"For more information, see:"
96,"""weaving"""
96,weaving.internal
96,Use the eclipselink.weaving.internal property to specify if EclipseLink uses internal optimizations through weaving.
96,Values
96,Table 5-106 describes this persistence property's values.
96,Table 5-106 Valid Values for weaving.internal
96,Value
96,Description
96,true
96,(Default) Enables internal optimizations through weaving.
96,false
96,Disables internal optimizations through weaving.
96,Usage
96,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
96,Examples
96,Example 5-142 shows how to use this property in the persistence.xml file.
96,Example 5-142 Using weaving in persistence.xml
96,"<property name=""eclipselink.weaving.internal"" value=""false""/>"
96,Example 5-143 shows how to use this property in a property map.
96,Example 5-143 Using weaving in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.WEAVING_INTERNAL, ""false"");"
96,See Also
96,"For more information, see:"
96,"""weaving"""
96,weaving.lazy
96,Use the eclipselink.weaving.lazy property to specify if EclipseLink uses lazy one-to-one and many-to-one mappings.
96,Values
96,Table 5-107 describes this persistence property's values.
96,Table 5-107 Valid Values for weaving.lazy
96,Value
96,Description
96,true
96,(Default) Enables lazy one-to-one and many-to-one mappings through weaving.
96,false
96,Disables lazy one-to-one and many-to-one mappings through weaving.
96,Usage
96,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
96,Examples
96,Example 5-144 shows how to use this property in the persistence.xml file.
96,Example 5-144 Using weaving.lazy in persistence.xml
96,"<property name=""eclipselink.weaving.lazy"" value=""false""/>"
96,Example 5-145 shows how to use this property in a property map.
96,Example 5-145 Using weaving.lazy in a Property Map
96,import org.eclipse.persistence.config.PersistenceUnitProperties;
96,propertiesMap.put
96,"(PersistenceUnitProperties.WEAVING_LAZY, ""false"");"
96,See Also
96,"For more information, see:"
96,"""weaving"""
96,Comments
96,Copyright © 2014 by The Eclipse Foundation under the Eclipse Public License (EPL)
97,DBeaver Documentation
97,Skip to content
97,Menu
97,Products
97,Download
97,DBeaver Lite
97,DBeaver Enterprise
97,DBeaver Ultimate
97,CloudBeaver Enterprise
97,Team Edition
97,DBeaver Lite
97,DBeaver Enterprise
97,DBeaver Ultimate
97,CloudBeaver Enterprise
97,Team Edition
97,Сompare products
97,Buy
97,Buy License
97,License types
97,DBeaver
97,CloudBeaver
97,Team Edition
97,Try for Free
97,Academic License
97,Partners
97,Become a sales partner
97,Resellers
97,Technology Partners
97,Universities
97,Company
97,About us
97,Our Clients
97,Contacts
97,Resources
97,Documentation
97,DBeaver
97,CloudBeaver
97,Supported Databases
97,Use Cases
97,For Enterprise
97,For Data Analysis
97,For Software Development
97,For Database Administration
97,For Website Management
97,Cloud migration
97,DBeaver Blog
97,Video
97,Events
97,Sign in
97,Sign in
97,Sign up
97,Your cart is empty.
97,Search
97,DBeaver Documentation DOWNLOAD pdf
97,Version 24.1.EAVersion 24.0Version 23.3Version 23.2Version 23.1Version 23.0Version 22.3Version 22.2Version 22.1Version 22.0Version 21.3Version 21.2Version 21.1Version 21.0Version 7.3Version 7.2Version 7.1Version 7.0Version 6.3Version 6.2
97,Table
97,of Contents
97,DBeaver Desktop Documentation
97,General User Guide
97,Installation
97,Application Window Overview
97,Views
97,Database Navigator
97,Filter Database Objects
97,Configure Filters
97,Simple and Advanced View
97,Projects View
97,Project Explorer
97,Query Manager
97,Background Tasks
97,Database Object Editor
97,Data Editor
97,Navigation
97,Data View and Format
97,Data Filters
97,Data Refresh
97,Data Viewing and Editing
97,Panels
97,Managing Charts
97,Data Search
97,Data transfer
97,SQL Generation
97,Working with spatial/GIS data
97,Working with XML and JSON
97,Managing Data Formats
97,Virtual column expressions
97,Properties Editor
97,ER Diagrams
97,Database Structure Diagrams
97,Custom Diagrams
97,Edit mode
97,SQL Editor
97,Toolbar Customization
97,SQL Templates
97,SQL Assist and Auto-Complete
97,AI Smart Assistance (ChatGPT)
97,SQL Formatting
97,SQL Execution
97,SQL Terminal
97,Variables panel
97,Query Execution Plan
97,Visual Query Builder
97,Script Management
97,Client Side Commands
97,Export Command
97,Debug
97,PostgreSQL Debugger
97,Search
97,File Search
97,DB Full-Text Search
97,DB Metadata Search
97,Schema compare
97,Using Liquibase in DBeaver
97,Data compare
97,MockData generation
97,Spelling
97,"Dashboards, DB monitoring"
97,Projects
97,Project security
97,Team work (Git)
97,Managing Master password
97,Security in PRO products
97,Certificate Management
97,Bookmarks
97,Shortcuts
97,AccessibilityDatabase Management
97,Sample Database
97,Database Connections
97,Edit Connection
97,Invalidate/Reconnect to Database
97,Disconnect from Database
97,Change current user password
97,Advanced settings
97,SSH Configuration
97,Proxy configuration
97,Kubernetes configuration
97,Kerberos authentication
97,Cloud configuration settings
97,AWS Credentials
97,AWS SSO
97,AWS Permissions
97,GCP Credentials
97,GCP SSO
97,Local Client Configuration
97,Connection Types
97,Configure Connection Initialization Settings
97,Tableau integration
97,Transactions
97,Auto and Manual Commit Modes
97,Transaction Log
97,Pending transactions
97,Drivers
97,Database drivers
97,JDBC-ODBC bridge
97,Tasks
97,Data export/import
97,Data migration
97,Data Import and Replace
97,Database backup/restore
97,Task management
97,Task scheduler
97,Composite tasks
97,Sending results by e-mail
97,Uploading result to external storage
97,Cloud Explorer
97,AWS
97,Azure
97,Google
97,Cloud File Explorer DBeaver PRO
97,Enterprise EditionDatabases support
97,Classic
97,Apache Hive/Spark/Impala
97,Cassandra
97,ClickHouse
97,Couchbase
97,Greenplum
97,IBM Db2
97,InfluxDB
97,MariaDB
97,Microsoft SQL Server
97,MongoDB
97,MySQL
97,Oracle
97,PostgreSQL
97,Redis
97,Salesforce
97,SQLite
97,Teradata
97,Cloud
97,AWS
97,Athena
97,DocumentDB
97,DynamoDB
97,Keyspaces
97,Redshift
97,Timestream
97,Azure
97,CosmosDB
97,Databricks
97,Google
97,AlloyDB for PostgreSQL
97,BigQuery
97,Bigtable
97,Cloud SQL for MySQL
97,Cloud SQL for PostgreSQL
97,Cloud SQL for SQL Server
97,Spanner
97,Snowflake Customizing DBeaver
97,Changing interface language
97,"Installing extensions - Themes, version control, etc"
97,User Interface ThemesTroubleshooting
97,Command Line
97,Reset UI settings
97,Reset workspace
97,Troubleshooting system issues
97,Posting issues
97,Log files
97,JDBC trace
97,Thread dumpAdmin Guide
97,Managing connections
97,Managing variables
97,Managing drivers
97,Managing preferences
97,Managing restrictions
97,Windows Silent InstallLicense management
97,License Administration
97,How to Import License
97,How to Reassign License Tutorials
97,Connecting to Oracle Database using JDBC OCI driver
97,Importing CA certificates from your local Java into DBeaver
97,SSL configuration
97,How to set a variable if dbeaver.ini is read-only
97,New Table Creation
97,Creating columns
97,Implementing Constraints
97,Utilizing Foreign-Keys
97,Creating Indexes
97,Incorporating Triggers
97,Oracle
97,Table of contentsOverviewSetting UpOracle connection settingsConnection detailsOracle driver propertiesSecure Connection ConfigurationsPowering Oracle with DBeaverOracle database objectsOracle Features in DBeaver
97,Overview
97,This guide provides instructions for setting up and using Oracle with DBeaver.
97,"Before you start, you must create a connection in DBeaver and select Oracle. If you haven't done this, please refer to"
97,our Database Connection article.
97,DBeaver interacts with the Oracle server using a specific driver. It also supports Oracle cloud database extensions such
97,"as Oracle Cloud JSON and Oracle NetSuite, both of which can be connected through Cloud Explorer."
97,Setting Up
97,This section provides an overview of DBeaver's settings for establishing a direct connection and the
97,"configuration of secure connections using SSH, proxies, and the driver setup for Oracle."
97,Oracle connection settings
97,"In this subsection, we'll outline the settings for establishing a direct connection to a Oracle database using DBeaver."
97,Correctly configuring your connection ensures seamless interaction between DBeaver and your Oracle database.
97,1) The first page of the connection settings requires you to fill in specific fields to establish the initial connection.
97,Field
97,Description
97,"Host If you're connecting via host, enter the host address of your Oracle database here."
97,Database Enter the name of the Oracle database you want to connect to.
97,Port Enter the port number for your Oracle database. The default Oracle port is 1521.
97,Connection identifiers Choose whether you want to connect using a Service name or a SID identifier.
97,"Authentication Choose the type of authentication you want to use for the connection. For detailed guides on authentication types, please refer to the following articles:"
97,- Oracle Database Native
97,- OS Authentication
97,- DBeaver Profile Authentication
97,- Oracle Kerberos Authentication
97,- Oracle Wallet
97,You can also read about security in DBeaver PRO.
97,"Role Choose whether you want to connect using a Normal, SYSDABA or a SYSOPER role. For more details, you can refer to the official Administering User Accounts and Security article."
97,Connection Details Provide additional connection details if necessary.
97,Driver Name This field will be auto-filled based on your selected driver type.
97,"Driver Settings If there are any specific driver settings, configure them here."
97,2) The second page of the connection settings offers additional options that allow you to customize your further
97,connection to the Oracle database.
97,Field
97,Description
97,Language Specify the Session Language.
97,Territory Specify the Session Territory.
97,NLS Date Format Specify NLS (National Language Support) Date Format. The default value of the NLS_DATE_FORMAT parameter is determined by the TERRITORY parameter.
97,NLS Timestamp Format Specify NLS Timestamp format. The default value of the NLS_TIMESTAMP_FORMAT parameter is determined by the TERRITORY parameter.
97,"Length semantics specify the length semantics for VARCHAR2 and CHAR table columns, user-defined object attributes, and PL/SQL variables in database objects created in the session. The length semantics can be specified as either BYTE or CHAR."
97,Currency symbol Specify the Currency symbol.You can find out which currency symbol your current session uses by querying the V$NLS_PARAMETERS view.
97,Show only connected user schema Show only a scheme of a connected user in the Database Navigator.
97,Hide empty schemas Check existence of objects within schema and do not show empty schemas in tree. Enabled by default but it may cause performance problems on databases with very big number of objects.
97,Always show DBA objects Always shows DBA-related metadata objects in tree even if user do not has DBA role.
97,*Always use `DBA_ views**
97,| Use DBA* views instead of ALL*` views wherever it is possible
97,Use SYS schema prefix Use SYS schema prefix in all metadata queries. Otherwise use view names without explicit schema.
97,Simple constraint reading query Use simple metadata queries. May work slower but it is more stable for all Oracle versions.
97,Use UNION for table metadata reading Use legacy table metadata query. With UNION instead JOIN. It helps in some cases speed up reading of table data.
97,Search metadata in synonyms Search for metadata in synonyms among other places. May significantly slow down metadata search as well as autocompletion.
97,Use RULE hint for system catalog queries Adds RULE hint for some system catalog queries (like columns and constraints reading).It significantly increases performance on some Oracle databases (and decreases on others).
97,"Show DATE values specifically as DATE Show DATE data type values specifically as DATE, not as a TIMESTAMP.This setting will not work with disabled date/time formatting."
97,Use metadata queries optimizer Use metadata queries optimizer. May significantly improve metadata reading performance on some systems.
97,Connection details
97,The Connection Details section in DBeaver allows to customize your experience while working with Oracle database. This includes
97,"options for adjusting the Navigator View, setting up Security measures, applying Filters, configuring Connection"
97,"Initialization settings, and setting up Shell Commands. Each of these settings can significantly impact your database"
97,"operations and workflow. For detailed guides on these settings, please refer to the following articles:"
97,Connection Details Configuration
97,Database Navigator
97,Security Settings Guide
97,Filters Settings Guide
97,Connection Initialization Settings Guide
97,Shell Commands Guide
97,Oracle driver properties
97,The settings for Oracle Driver properties enable you to adjust the performance of the Oracle driver.
97,"These adjustments can influence the efficiency, compatibility, and features of your Oracle database."
97,"For a complete walkthrough on setting up Oracle JDBC driver properties, you can refer to the official"
97,Oracle JDBC documentation.
97,These guide detail each driver's properties and how they can be used to optimize Oracle database connections.
97,"You can customize the Oracle driver in DBeaver via the Edit Driver page, accessible by clicking on the Driver"
97,Settings button on the first page of the driver settings. This page offers a range of settings that can influence your
97,"Oracle database connections. For a comprehensive guide on these settings, please refer to our Database drivers article."
97,Secure Connection Configurations
97,"DBeaver supports secure connections to your Oracle database. Guidance on configuring such connections, specifically"
97,"SSH, SSL, Proxy and Kubernetes connections, can be found in various referenced articles. For a comprehensive understanding, please"
97,refer to these articles:
97,SSH Configuration.
97,SSL Configuration
97,Proxy Configuration.
97,Kubernetes Configuration.
97,Powering Oracle with DBeaver
97,DBeaver provides a host of features designed for Oracle databases. This includes the ability to view and manage
97,"databases, along with numerous unique capabilities aimed at optimizing database operations."
97,Oracle database objects
97,DBeaver lets you view and manipulate a wide range of Oracle database objects. DBeaver has extensive support for
97,"various Oracle metadata types, allowing you to interact with a wide variety of database objects, such as:"
97,Schemas
97,Tables
97,Columns
97,Constraints
97,Foreign Keys
97,References
97,Triggers
97,Indexes
97,Partitions
97,Dependencies
97,Views
97,Materialized Views
97,Indexes
97,Sequences
97,Queues
97,Types
97,Packages
97,Procedures
97,Functions
97,Synonyms
97,Schema Triggers
97,Table Triggers
97,Database Links
97,Java
97,Jobs
97,Scheduler
97,Jobs
97,Programs
97,Recycle bin
97,Global metadata
97,Types
97,Public Synonyms
97,Public Database Links
97,User Recycle bin
97,Storage
97,Tablespaces
97,Files
97,Objects
97,Security
97,Users
97,Roles
97,Profiles
97,Administer
97,Session Manager
97,Locks Manager
97,Oracle Features in DBeaver
97,DBeaver isn't limited to typical SQL tasks. It also includes numerous unique features specifically for Oracle.
97,"Beyond regular SQL operations, DBeaver provides a range of Oracle-specific capabilities, such as:"
97,Category
97,Feature
97,Data Types
97,Oracle Nested Tables
97,PL/SQL Support
97,PL/SQL Procedures
97,PL/SQL Functions
97,Security
97,Oracle Permissions
97,Oracle Profiles
97,Oracle Roles
97,Data Organization
97,Oracle Partitions
97,Database Management Oracle Dependencies
97,Performance Tuning
97,Oracle Performance Reports
97,Oracle Execution Plans
97,"Additional features compatible with Oracle, but not exclusive to it:"
97,Category
97,Feature
97,Data Transfer
97,Data Import
97,Data Export
97,Session Management
97,Session Manager
97,Backup and Restore
97,How to Backup/Restore data
97,Schema Management
97,Schema Compare
97,Data Visualization
97,GIS Guide
97,ERD Guide
97,Your email
97,Your issue
97,Did we resolve your issue?
97,"Yes No, get help"
97,Facebook
97,LinkedIn
97,Telegram
97,Reddit
97,Twitter
97,YouTube
97,Products
97,Download
97,DBeaver Lite
97,DBeaver Enterprise
97,DBeaver Ultimate
97,CloudBeaver Enterprise
97,Team Edition
97,Сompare products
97,Buy
97,Buy License
97,License types
97,Try for Free
97,Academic License
97,Partners
97,Become a sales partner
97,Resellers
97,Technology Partners
97,Universities
97,Company
97,About us
97,Our Clients
97,Contacts
97,Resources
97,Documentation
97,Supported Databases
97,Use Cases
97,DBeaver Blog
97,Video
97,Events
97,PrivacyService Level AgreementEnterprise Software AgreementEULA
99,Known issues in Campaign
99,Jump to main content
99,Product Documentation
99,Customer Support
99,Unica OptimizeRelease NotesSystem Requirements Installation Guides Upgrade GuidesUser GuideTroubleshooting and Tuning GuideSystem TablesLicensing GuidePDFsGlossaryNotices
99,Search
99,HomeRelease NotesRelease Notes for version 12.1.0.3Known issues in Campaign Unica Campaign 12.1.0.3 includes the
99,following known issues.
99,Release NotesRelease NotesRelease Notes for version 12.1.0.3System requirements and compatiblityUnica Campaign is part of the Unica suite of products. Unica Campaign version 12.1 requires Unica Platform 12.1. New features and changes in Campaign Unica Campaign 12.1.0.3 includes a number of
99,new features and changes. These are listed underneath. Fixed defects in CampaignThe following defects were fixed in Unica Campaign 12.1.0.3.Known issues in Campaign Unica Campaign 12.1.0.3 includes the
99,following known issues.Known issues in OptimizeKnown limitations in CampaignUnica Campaign 12.1 includes the
99,following known limitations.Known limitations in OptimizeRelease Notes for version 12.1.0.4
99,Known issues in Campaign
99,Unica Campaign 12.1.0.3 includes the
99,following known issues.
99,Table 1.
99,Known issues
99,322046
99,Localization is broken since version 12.0 and not working as
99,expected.
99,VERSION : Campaign.mo file invalid on RHEL : Localization not
99,working.
99,321395
99,"Incorrect unica_aclsnr, unica_acsvr and other utilities version after"
99,upgrade to 12.1.0.3 on RHEL.
99,320648
99,"If user did overlay installation the "".go"" navigation URL changes"
99,"reverted to "".do"" for navigation URLs at"
99,'Affinium|suite|uiNavigation|mainMenu|Campaign'.
99,310343
99,Campaign flowcharts with old eMessage process box will not work in V12.1.
99,312109
99,Campaign upgrade installation from 12.0 to 12.1 completed with three non-fatal
99,errors.
99,300159
99,"HTTP Communication error occurs, while saving flowchart if Unica"
99,Platform and Campaign applications are using the same JNDI.It is
99,suggested to use separate JNDIs for Platform and Campaign
99,applications.
99,295574
99,Deployment of Campaign application in wWebSphere application server
99,failed due to java.lang.NoClassDefFoundError:
99,javax.el.ELManager. Users are required to copy
99,javax.el-3.0.1b11.jar in the Websphere application
99,servers lib directory. They can download
99,javax.el-3.0.1-b11.jar from https://mvnrepository.com/artifact/org.glassfish/javax.el/3.0.1-b11.
99,311916
99,Journey Process box: Searching in Journey PB retains its last search
99,string. User need to manually clear the search criteria.
99,"306110, 306108"
99,"While deploying or starting Campaign application, it throws errors"
99,"related to module-info.class, and warning related to obsolete hibernate"
99,namespace. These can be ignored.
99,304803
99,"Getting ""jcc][t4][10217][10310][4.14.111] Connection read-only mode"
99,"is not enforceable after the connection has been established."" warnings"
99,in WebSphere console log. No impact on application side. These can be
99,ignored.
99,306095
99,Production documentation links on Campaign installer are broken. See
99,the documentation available along with product installers.
99,312150
99,Journey Process box: Search in Associated Journyes with multiple
99,words is not working.
99,310126
99,eMessage instances in Campaign uninstall folder – these can be
99,ignored. Unica no longer support eMessage.
99,312299
99,Campaign or Platform navigation url when contains default http or
99,https port then recent menu’s does not work. If you are using default
99,http (80) or https (443) ports in navigation url please remove them.
99,312231
99,Campaign swagger APIs will not work only on swagger page when
99,marketing platform login method is set to Web Access control.
99,303532
99,"With MariaDB as system database Optimize, Maillist or Calllist"
99,process box execution fail with error 10646. You must enable In DB
99,Optimization in flowchart advance settings to resolve this
99,error.
99,13460
99,When the Export to File option is unchecked on the Fulfillment tab of
99,"the Mail List process, the Summary File option is enabled but should not"
99,be.
99,175825
99,When defaultBehaviorWhenOutputToFile is set to Create New
99,"File, it works only when you select a new output (export) file for the first time. If"
99,"you try to change the existing output file and select a new output file, then the option"
99,Append to Existing Data gets selected by default. It can be changed
99,manually to Create New File.
99,"APAR 198495, PO05293, 198494"
99,"For a custom macro, if a user account has assigned as Not Granted permission for a stored"
99,"object (Custom Macros, Flowchart Templates, Stored Derived Fields, etc.), the custom macro can be"
99,"used to edit, delete, add, move all stored objects regardless of permissions."
99,204347
99,LARGE_DATA: Browser Crash - Mail List process box with 700 segments - Save with 512 offers to
99,each segment.
99,211253
99,File-based input with multiple columns containing date in different format does not read the
99,date correctly.
99,212890
99,Boolean type column on Amazon Redshift is recognized as 'Test' field type in table
99,mapping.
99,220474
99,When the column name of the source table contains Non-ASCII characters and is long (probably
99,"longer than 10 x 3bytes characters in UTF8), the Snapshot process does not run because the temp"
99,table is not created.
99,220705
99,Profiling count for Dimension table fields is incorrect and profiling percentage is displayed
99,as 100% even if profiling is in progress.
99,"APAR 222047, PO06172, 222049"
99,Unica Campaign extract process does not write to DB2 database when flowchart is configured
99,with two extract processes and the second extract process contains a derived field that uses French
99,accented characters in the name of the derived field.
99,"APAR 225568, PO06304, 225572"
99,temptablepostexecutionsql is not run when selecting coremetrics
99,segment.
99,230340
99,Inconsistent behavior for the Mail list process is observed when the data filter is assigned
99,to Effective date on the Parameters tab in a new Mail list process when
99,compared to an edited existing Mail list process. The Mail list process does not output a datetime
99,derived field in the output log file. Changing the effective date parameter values might cause
99,execution failure of the Mail list.
99,230606
99,"In a flowchart where fields are extracted and used in a Mail list process, the fields under"
99,Extract Node on the Personalization tab of the Mail list process are
99,different before and after running the process.
99,231859
99,"While using the Chrome browser, it takes around 20 seconds to respond while loading the"
99,Personalization tab of the Mail list process if it contains large number of treatments and offers
99,"assigned. For example, a Mail list process with 250 cells, each cells with multiple offers, each"
99,offer having multiple attributes.
99,232502
99,A Mail list performance issue occurs when the user changes the input in any way and there is
99,a delay when initially switching tabs. After the user changes the input and switches to the
99,"Treatment or Process tab for the first time, there is a delay of around 10-15 seconds depending on"
99,"the number of inputs selected. After this initial delay, there is no delay until the input is"
99,changed.
99,232835
99,The Campaign application performance is affected and sometimes an exception error message is
99,displayed on the Campaign pages when the application is used for a longer time (some days) with
99,continuous usage. You must restart your Campaign web application and Campaign Listeners
99,238789
99,"While using the Chrome browser, when a user opens a Mail list with 600 cells having multiple"
99,"offers assigned to each cell, the application stops responding. While using the Internet Explorer 11"
99,"browser, nothing is displayed in the Mail list process box."
99,239142
99,"When the Audience process fails because of an incorrect Count filter expression, and the user"
99,"removes the filter expression, adds a Condition, and runs the Audience process again, the process"
99,fails. The Condition is not considered and instead the previous Count filter expression is used
99,"resulting in the failure. To run the Audience process successfully, you must re-open the Audience"
99,"process configuration window, click the Condition, Save and Close. The Audience process now"
99,considers the Condition and runs successfully.
99,"APAR 243895, PO06966, 243897"
99,A space character is converted to '' while profiling.
99,"PMR 223848, 245664"
99,Irrelevant flowcharts turned up in a Campaign when different users access different
99,"flowcharts at the same time, the wrong flowchart seems to be displayed."
99,248007
99,Hive Performance: PRE -Single Insert statements executed for loading data in temp tables
99,(extract) when used with Hive takes 19 minutes for 2000 records.
99,"PMR 269280, 269765"
99,Truncate does not function for the DB2 database. The DeleteAsTruncate
99,"property specifies whether, when an output process is configured to REPLACE TABLE,"
99,Campaign uses TRUNCATE TABLE or deletes from the table. When the value is
99,"TRUE, Campaign runs a TRUNCATE TABLE from the table. When the"
99,"value is FALSE, Campaign runs a DELETE FROM from the table. The"
99,"default value depends on the database type, with a large number of clients moving from Oracle to DB2"
99,the difference in performance is extremely noticeable. The comparison in performance in Snapshots
99,and Mail lists for customers leaving Oracle to DB2 is drastic.
99,269785
99,"For the Chrome and Safari browsers, when the locale is set as Japanese, the labels overlap"
99,the buttons in the Save template window.
99,270528
99,Use and Profile buttons remains enabled for the
99,Select process when the user selects some table fields and then applies the search filter in the
99,Select process such that it does not return any matching table.
99,270814
99,Users cannot select values from the On a trigger drop down by using the mouse on the
99,Scheduler page.
99,271642
99,"A ""Please wait....."" message is displayed multiple times in console mode"
99,installation while upgrading from version 10.1 and also for new Installation
99,271676
99,The Extract table on the DB2 and Oracle databases is not deleted when In-DB optimization is
99,unchecked.
99,272253
99,The Campaign application performance is affected when a user opens the Table mapping window
99,and more than 500 tables are mapped.
99,2968
99,Hot keys are not implemented on most of the windows. In the Dojo implementation it is not
99,possible to use keyboard shortcuts to activate certain functions. Instead the user must click
99,buttons.
99,"PMR 266519, 75262"
99,Clicking Return to previous page distorts the user interface in some
99,"cases. Use the links within the products to navigate, rather than the browser controls."
99,N/A
99,"When Unica Campaign is deployed in an application server cluster environment with 80 port,"
99,the Campaign navigation URL should not contain this port number. For example:
99,http://<host>.<domain>:80/Campaign should be changed to
99,http://<host>.<domain>/Campaign. Change the value of the serverURL
99,property under Campaign|navigation on the Settings > Configuration
99,page.
99,N/A
99,"Mail List process box gets unconfigured with error ""31606: History table are"
99,"changed"" on Flowchart run."
99,There is no issue if the history tables are mapped prior to building a flowchart and adding a
99,Mail List process.
99,"If you do not follow step 1, you can still map the history tables after the error appears. If"
99,"you edit the flowchart and configure the process box, the process will run."
99,N/A
99,"After migrating non-ASCII data to Campaign, in some cases you cannot open Campaign flowcharts"
99,"on the target system. Session flowcharts open successfully. To work around this issue, remigrate the"
99,campaigns in overwrite mode. You can then open the flowcharts.
99,TT 062333
99,Information related to associated products does not appear in offer on Marketing Operations.
99,When an offer created in Campaign with products associated with it is imported in Marketing
99,"Operations, information about the associated products is not available in Marketing"
99,Operations.
99,270655
99,"Table Mapping : Table name containing @ is supported while mapping a table, special"
99,characters like @ are should not be supported in Table names. Since Table name containing @ is
99,"supported since some time, this functioning is not updated now."
99,283637
99,When user has one maillist PB configured in the flowchart and he adds another maillist PB in
99,"the same flowchart, it is observed that All the values from already configured in treatment tab,"
99,"Parameters tab, Personalisation tab of new Maillist PB appear automatically in any new dragged"
99,maillist PB.
99,281389
99,"While running Campaign in the Upgrade mode, the installer should read the installation"
99,properties files from the previous version response files as we had that response available at the
99,"install location from the previous installation. However, it does not read the installation"
99,properties files from the earlier version response files.
99,280623
99,"In Sample PB if we specify '3' in ""# of Samples/Output Cells"" field and Configure [sample1]"
99,"with 3 samples having sample sizes as 50%, 50% and ""All remaining records"" respectively based on"
99,"""Random Sample method"" then after running Flowchart if we reopen the Sample PB ""Process run results"
99,"will be lost. Continue?"" message is displayed in sample PB when remaining All check boxed is"
99,check.
99,283101
99,"CHROME: ""Unable to notify roles/permission changes to Campaign, Deliver OD may be out of"
99,"sync"". On clicking save changes for the Security Policy displays the following pop-up, this happens"
99,"just first time after user logs in and changes anything in the policy and clicks save changes. Also,"
99,this is reproducible just on CHROME. Tested on Chrome V64 and V65.
99,282844
99,Campaign Offer doesn’t sort on 'Channel' & 'Eff./Exp.Dates'. After clicking on every
99,column the offers below it should be sorted in ascending order. If you click on the same column the
99,"second time, the offers beneath it should be sorted in the descending order."
99,283695
99,Segment process box execution failed with extract enabled to User database for the IMPALA
99,user DB. This error does not reproduce when extracting to Unica Campaign server.
99,284436
99,"Platform.war and campaign.war in ear file deployment will not work in Weblogic 12.2.1, work"
99,around is to deploy campaign.war and unica.war separately
99,288228
99,"If the date field uploaded to IMC side is not in mm/dd/yyyy format, the email process box"
99,fails to execute. This data format is required by IMC Importlist API. It works accurately if
99,DELIM_M_D_YYYY(mm/dd/yyyy) format is used for date while uploading to IMC using importlist
99,API.
99,289135
99,TOMCAT : Occasionally Unica Platform or Campaign does not gets started after deploying in
99,tomcat application server. Workaround : delete Campaign and Platform directory available inside
99,Tomcat webapps directory and delete the content of work directory from the path where Tomcat is
99,installed.
99,304945
99,"Refresh command does not refresh the master listener priority or weight, if you need to make"
99,changes in priority or weight you require to take a downtime and restart the application.
99,312445
99,Link Process box - sending data only for the first input cell
99,selected to link.
99,UL-285
99,Connection Salesforce - Subsequent Salesforce actions are failing for
99,Update (Lead/Contact) intermittently.
99,UL-250
99,Connection Mandrill - Intermittenly stop sending emails to targeted
99,audiences.
99,UL-281
99,Connection Mandrill - Subject line with non english characters -
99,mandrill not sending email to user.
99,UL-259/UL-242
99,Connection Mailchimp/Mandril - results would be available for 1000
99,users only.
99,UL-214
99,MailChimp/Mandril - Audience value is getting changed to id field
99,after selection or reopening process box.
99,UL-194
99,MailChimp/Mandril - In Mandrill template if merging field added as
99,localized character - personalization is not working.
99,UL-257
99,Link Connections page takes time to load in the Campaign process box.
99,UL-231
99,Twilio Connection - Only 5 fields are supported for personalization
99,in SMS body/text area.
99,UL-279
99,Connection MailChimp - Branch run or single link process box run is
99,not supported with Link process box with Mailchimp connection. You will
99,need to execute complete flowchart.
99,UL-287
99,Link process box fails when no output data.
99,Unica Campaign and Unica Link applications urls should be having same
99,domain names. Unica Campaign and Link applications deployed on different
99,domains will not work.
99,UL-189
99,Connection Mandrill/MailChimp- Emojis are not working in Mandrill and
99,MailChimp connector Mail Subject line.
99,313650
99,Platform Scheduled flowcharts are not getting executed
99,afterupgrade.Use the quartzjobtool to update scheduler jobs. This is a
99,"requiredstep. If this upgrade tool is not run, any existing scheduled"
99,job willfail to start. The quartzjobtool is in the tools\bin directory
99,underUnica Platform installation. Run this utility from the
99,tools\bindirectory.Example command (Windows): quartzjobtool.batExample
99,command (Unix): ./quartzjobtool.sh
99,306383
99,UBX registration utility is failing with error Caused by:
99,java.lang.ClassNotFoundException: org.jboss.logging.BasicLogger
99,error
99,316802
99,12.1 FP2 MariaDB]:CODE 704: File write error. while extracting
99,extracted fields and table fields to database server through Segment
99,Share: Email
99,Twitter
99,Disclaimer
99,Privacy
99,Terms of use
99,Cookie Preferences
