filenr,sentence
3,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks - MariaDB.org
3,Skip to content
3,Download
3,Documentation
3,Contribute
3,Server Fest
3,Events
3,Sponsor
3,Blog
3,Planet MariaDB Server
3,About
3,Back
3,"Latest releases 11.4.1 (RC), 11.3.2, 11.2.3, 11.1.4, 11.0.5, 10.11.7, 10.6.17,  10.5.24, 10.4.33. Vote in our poll!"
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Search for:
3,Download
3,Documentation
3,Contribute
3,Server Fest
3,Events
3,Sponsor
3,Blog
3,Planet MariaDB Server
3,About
3,Back
3,Search for:
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Post navigation
3,Older Posts Older post:Newer Posts Newer posts:
3,"Tags: chatgpt, openai"
3,Post navigation
3,Older Posts Older post:Newer Posts Newer posts:
3,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks
3,Written by Written by Ian Gilfillan 2023-04-212023-04-21
3,"This is the next in the series of posts exploring ChatGPT and its insights about MariaDB. The series began with ChatGPT, OpenAI and MariaDB Foundation, followed by Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for Beginners."
3,"This week’s topic, suggested by ChatGPT itself, was “MariaDB Performance Tuning”. So how did it do?"
3,"MariaDB is undoubtedly one of the most popular open-source database management systems available today, with a strong focus on performance and ease of use. Whether you are a developer, database administrator, or an IT professional, ensuring that your MariaDB installation performs efficiently is crucial. In this blog, we will discuss some key tips and tricks for tuning your MariaDB server, improving its performance in various environments."
3,Identifying Bottlenecks
3,"Before diving into any optimization techniques, it’s crucial to identify the areas where performance might need improvement. Some common tools that will help you to pinpoint performance bottlenecks are:"
3,"MariaDB’s slow query log: This log records queries that take longer-than-expected to execute, helping you identify problematic SQL code."
3,"Performance Schema: A feature in MariaDB that allows you to monitor various metrics, such as query performance and resource usage."
3,mtop and mytop: These are real-time monitoring tools that show the MySQL server’s status and queries being executed at any given time.
3,"I thought we were talking about MariaDB here? And as far as I know, mtop was last updated in 2004, so I’m not sure this would be very useful. A pet bugbear about ChatGPT is its reluctance to provide links. It sometimes behaves a bit like an ignorant loudmouth, confidently spouting forth on something or other, but when you challenge them to provide evidence, nothing is forthcoming. At least it doesn’t launch into ad hominems. Its advice to use the slow query log and the performance schema can be helpful, but links would be useful. So, here are links to find out more about the slow query log, Performance Schema, as well as mtop and mytop."
3,Using these tools will give you an idea of the specific areas in which your MariaDB server needs tuning.
3,General Performance Tips
3,Optimize SQL queries
3,"Poorly written queries can lead to significant performance hits, and as such, properly optimizing SQL code is a critical step in improving the performance of your MariaDB server."
3,"Use EXPLAIN: The EXPLAIN command helps you understand how the MariaDB optimizer executes a query, allowing you to identify potentially slow operations and improve the query strategy."
3,"Proper indexing: Design and use indexes effectively to optimize search performance. Keep in mind that while more indexes can speed up SELECT queries, they may also slow down INSERT, UPDATE, and DELETE operations."
3,"Avoid using SELECT *: Instead of using SELECT * (which retrieves all columns), specify only the required columns in your queries."
3,"True, but a rather lacking in specifics, and again, links to actually find out anything useful. Try EXPLAIN and Optimizing and Indexes. To save repetition, I’ll add links directly into GPT’s text for the rest of this post."
3,Update MariaDB to the latest version
3,"MariaDB receives regular updates and improvements, so keeping your installation up-to-date is an essential step in getting the best performance possible. New releases might include feature enhancements, bug fixes, and security patches, all of which contribute to better overall performance."
3,Server-specific Performance Tips
3,Configuring InnoDB Variables
3,"InnoDB is the default storage engine for MariaDB, and adjusting its settings can significantly improve the performance of your server. Some critical InnoDB settings to consider are:"
3,"innodb_buffer_pool_size: Allocate more memory to the InnoDB buffer pool to reduce disk I/O. Typically, this value should be set to at least 70-80% of the available memory."
3,"innodb_log_file_size: A larger log file size reduces the need for frequent log rotations, thus improving write performance. However, a log size that is too large might slow down the recovery process after a crash."
3,Tweak Query Cache Variables
3,"MariaDB’s query cache is a feature that stores the results of executed queries in memory, allowing the server to serve identical queries without re-executing them."
3,"query_cache_size: Increase the size of the query cache if you have spare memory, which can improve performance for repeat queries."
3,"query_cache_limit: This value sets the maximum size of an individual query’s result that can be cached. A larger limit might cache more queries, but it might also cause more fragmentation."
3,"This isn’t very good advice. There’s a reason the query cache is disabled by default, as in many environments it actually slows things down. It may help in certain contexts, but you’d want to test this, and not blindly enable it."
3,Hardware Optimizations
3,"When it comes to improving database performance, hardware optimization is often overlooked. Some quick hardware-related adjustments you can make include:"
3,Use fast SSDs for your storage: SSDs can significantly improve read/write performance and offer better reliability than traditional hard disk drives.
3,"Increase available memory: More RAM will allow you to allocate larger InnoDB buffer pools and query caches, reducing I/O operations and improving performance."
3,"In conclusion, performance tuning for MariaDB is a continuous process. Regularly monitoring and optimizing your queries, staying up-to-date with new releases, and fine-tuning server settings as needed will contribute to the overall performance of your MariaDB installation. By following the tips and tricks outlined in this article, you are well on your way to maintaining a high-performance MariaDB server."
3,"Overall, very light on details, but a reasonably helpful high-level overview with just enough wrong to lead someone following it down some dead ends. My guess is that it overly prioritises some well-respected, but now dated, sources. It would help to train GPT to prioritise the official MariaDB Server documentation."
3,Post navigation
3,Older Posts Older posts: Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for BeginnersNewer Posts Newer posts: Code of Conduct Update
3,Contact
3,Facebook
3,Twitter
3,LinkedIn
3,Reddit
3,Instagram
3,Governance
3,Maintenance policy
3,Security policy
3,Privacy policy
3,Logos and badges
3,Usage statistics
3,Service providers
3,Copyright @ 2009 - 2024 MariaDB Foundation.
7,hash_join_cardinality optimizer_switch Flag - MariaDB Knowledge Base
7,Search
7,Products
7,Services
7,Resources
7,About
7,Contact
7,Login
7,Copyright © 2024 MariaDB. All rights reserved.
7,The Ultimate Guide to High Availability with MariaDB
7,Download Now
7,Knowledge Base
7,Contact
7,Login
7,Search
7,Products
7,Services
7,Pricing
7,Resources
7,About Us
7,Download
7,Knowledge Base
7,» MariaDB Server Documentation
7,» High Availability & Performance Tuning
7,» Optimization and Tuning
7,» Query Optimizations
7,» hash_join_cardinality optimizer_switch Flag
7,Home
7,Open Questions
7,MariaDB Server
7,MariaDB MaxScale
7,MariaDB ColumnStore
7,Connectors
7,History
7,Source
7,Flag as Spam / Inappropriate
7,Translate
7,Created
7,"9 months, 2 weeks ago"
7,Modified
7,"7 months, 3 weeks ago"
7,Type
7,article
7,Status
7,active
7,License
7,CC BY-SA / Gnu FDL
7,History
7,Comments
7,EditAttachments
7,No attachments exist
7,Product Versions
7,MariaDB starting with 10.6.13
7,hash_join_cardinality optimizer_switch Flag
7,MariaDB starting with 10.6.13The hash_join_cardinality optimizer_switch flag was added in
7,"MariaDB 11.0.2, MariaDB 10.11.3,"
7,"MariaDB 10.10.4, MariaDB 10.9.6, MariaDB 10.8.8 and MariaDB 10.6.13."
7,"In MySQL and MariaDB, the output cardinality of a part of query has historically been tied to the used access method(s). This is different from the approach used in database textbooks. There, the cardinality ""x JOIN y"" is the same regardless of which access methods are used to compute it."
7,Example
7,Consider a query joining customers with their orders:
7,select *
7,from
7,"customer, orders, ..."
7,where
7,customer.id = orders.customer_id and ...
7,"Suppose, table orders has an index IDX on orders.customer_id."
7,"If the query plan is using this index to fetch orders for each customer, the optimizer will use index statistics from IDX to estimate the number of rows in the customer-joined-with-orders."
7,"On the other hand, if the optimizer considers a query plan that"
7,"joins customer with orders without use of indexes, it will ignore the customer.id = orders.customer_id equality completely and will compute the"
7,output cardinality as if customer was cross-joined with orders.
7,Hash Join
7,"MariaDB supports Block Hash Join. It is not enabled by default, one needs to set it join_cache_level to 3 or a bigger value to enable it."
7,"Before MDEV-30812, Query optimization for Block Hash Join would work as described in the above example: It would assume that the join operation is a cross join."
7,"MDEV-30812 introduces a new optimizer_switch flag, hash_join_cardinality. In MariaDB versions before 11.0, it is off by default."
7,"If one sets it to ON, the optimizer will make use of column histograms when computing the cardinality of hash join operation output."
7,"One can see the computation in the Optimizer Trace,"
7,search for hash_join_cardinality.
7,← GUID/UUID Performance
7,↑ Query Optimizations ↑
7,IGNORE INDEX →
7,Comments
7,Comments loading...
7,"Content reproduced on this site is the property of its respective owners,"
7,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
7,expressed by this content do not necessarily represent those of MariaDB or any other party.
7,↑ Query Optimizations ↑
7,Index Hints: How to Force Query Plans
7,Subquery Optimizations
7,Optimization Strategies
7,Optimizations for Derived Tables
7,Table Elimination
7,Statistics for Optimizing Queries
7,Filesort with Small LIMIT Optimization
7,LIMIT ROWS EXAMINED
7,index_merge sort_intersection
7,MariaDB 5.3 Optimizer Debugging
7,optimizer_switch
7,How to Quickly Insert Data Into MariaDB
7,Index Condition Pushdown
7,Query Limits and Timeouts
7,Aborting Statements that Exceed a Certain Time to Execute
7,Partition Pruning and Selection
7,Big DELETEs
7,Charset Narrowing Optimization
7,Data Sampling: Techniques for Efficiently Finding a Random Row
7,Data Warehousing High Speed Ingestion
7,Data Warehousing Summary Tables
7,Data Warehousing Techniques
7,Equality propagation optimization
7,FORCE INDEX
7,Groupwise Max in MariaDB
7,GUID/UUID Performance
7,hash_join_cardinality optimizer_switch Flag
7,IGNORE INDEX
7,not_null_range_scan Optimization
7,optimizer_adjust_secondary_key_costs
7,"Optimizing for ""Latest News""-style Queries"
7,Pagination Optimization
7,Pivoting in MariaDB
7,Rollup Unique User Counts
7,Rowid Filtering Optimization
7,Sargable DATE and YEAR
7,Sargable UPPER
7,USE INDEX
7,Products
7,Services
7,Pricing
7,Resources
7,About Us
7,Download MariaDB
7,Subscribe to our newsletter!
7,Legal
7,Privacy Policy
7,Cookie Policy
7,Copyright © 2024 MariaDB. All rights reserved.
9,"Oracle Database SQL Tuning Guide, 19c"
9,Previous
9,Next
9,JavaScript must be enabled to correctly display this content
9,SQL Tuning Guide
9,Oracle® Database
9,Oracle® Database
9,SQL Tuning Guide
9,19c
9,E96095-18
9,January 2024
9,Title and Copyright Information
9,"Oracle Database SQL Tuning Guide, 19c"
9,E96095-18
9,"Copyright © 2013, 2024, Oracle and/or its affiliates."
9,Primary Author: Lance Ashdown
9,"Contributing Authors: Nigel Bayliss, Maria Colgan, Tom Kyte"
9,"Contributors: Hermann Baer, Bjorn Bolltoft, Ali Cakmak, Sunil Chakkappen, Immanuel Chan, Deba Chatterjee, Chris Chiappa, Dinesh Das, Kurt Engeleiter, Leonidas Galanis, William Endress, Marcus Fallen, Bruce Golbus, Katsumi Inoue, Praveen Kumar Tupati Jaganath, Mark Jefferys, Shantanu Joshi, Adam Kociubes, Keith Laker, Allison Lee, Sue Lee, Cheng Li, David McDermid, Colin McGregor, Ajit Mylavarapu, Ted Persky, Lei Sheng, Ekrem Soylemez, Hong Su, Murali Thiyagarajah, Randy Urbano, Sahil Vazirani, Bharath Venkatakrishnan, Hailing Yu, John Zimmerman, Frederick Kush"
10,MySQL Performance Tuning Guide
10,ProductAutopilot for MySQLMySQL MonitoringHealth Checks for MySQLCompatibilityFAQSQL Query OptimizationSolutionsvs MySQLTunerFor Hosting ProvidersFor AWS RDSFor DevelopersPricingDocumentationLog inSign up free
10,← ALL POSTSReleem Help CenterMySQL Performance Tuning CenterHow to tune MySQL?MySQL Health ChecksReleem Performance ScoreMySQL Performance MetricsMySQL LatencyMySQL ThroughputMySQL Slow QueriesMySQL Aborted ClientsMySQL Performance ParametersMySQL Server StatusMySQL Configurationbulk_insert_buffer_sizeinnodb_buffer_pool_chunk_sizeinnodb_buffer_pool_instancesinnodb_buffer_pool_sizeinnodb_change_bufferinginnodb_flush_log_at_trx_commitinnodb_flush_methodinnodb_log_buffer_sizeinnodb_log_file_sizeinnodb_max_dirty_pages_pctinnodb_page_cleanersinnodb_purge_threadsinnodb_read_io_threadsinnodb_thread_concurrencyinnodb_write_io_threadsjoin_buffer_sizekey_buffer_sizemax_allowed_packetmax_connectionsmax_heap_table_sizemyisam_sort_buffer_sizeoptimizer_search_depthread_rnd_buffer_sizesort_buffer_sizetable_definition_cachetable_open_cachethread_cache_sizethread_pool_sizethread_stacktmp_table_sizetransaction_prealloc_sizeHow to increase open_files_limitPrivacymain/ blog/ MySQL Performance Tuning GuideMySQL Performance Tuning Guide Comprehensive guide on MySQL Performance tuning
10,"JUL 05, 2023 • WRITTEN BY ROMAN AGABEKOVMySQL is a powerful and widely-used open-source relational database management system (RDBMS). To unlock its full potential, it is crucial for database administrators (DBAs) to optimize and fine-tune their MySQL servers. This comprehensive guide walks through eight critical steps for effective MySQL tuning that can help improve the performance, efficiency, and stability of MySQL databases. MySQL/MariaDB DocumentationStudy MySQL Configuration Best PracticesAnalyze Monitoring Data- What Metrics Should DBAs Monitor?Analyze MySQL Status- Cache Performance Metrics- Database Efficiency Metrics- Temporary Data MetricsUse Scripts for Configuration Recommendations- MySQLTuner- Tuning-Primer Script- Percona Toolkit- phpMyAdmin Advisor- MysqlreportCalculate Values of MySQL Performance SettingsCreate New Configuration FileApply New Configuration FileHow does Releem Help? 1. MySQL/MariaDB Documentation MySQL has excellent documentation resources that are useful for all, even veteran database administrators with years of experience. MySQL provides server reference manuals for each currently supported version:MySQL 8.0 Reference Manual;MySQL 5.7 Reference Manual;MySQL 5.6 Reference Manual (MySQL 5.6 has not been supported since February 2021.);MariaDB Reference ManualIt's helpful for database administrators to be intimately familiar with these resources. And it's highly recommended to take the time to work through the documentation to better understand how MySQL works and how different parameter settings affect database performance.We published the full list of MySQL variables which impact on database performance. 2. Study MySQL Configuration Best Practices There are an array of resources available both online and in print to learn how to configure MySQL. MySQL has hundreds of configuration options, but for many servers, only a handful are critical. The tuning will vary depending on workload and hardware specifications, but DBAs that familiarize themselves with best practices (for their specific version of MySQL) will be better equipped to understand and solve performance issues in a timely manner.Releem has assembled a useful list of articles and resources that are related to MySQL/ MariaDB/Percona configuration. Here are some general articles for database administrators interested in studying or refreshing MySQL best practices:Ten MySQL performance tuning settings after installation;MySQL Server and SQL Performance TuningMySQL Performance Cheat SheetPerformance Tuning and Configurations for your MySQL ServerMAKING IT BETTER: BASIC MYSQL PERFORMANCE TUNING (MYSQLD)InnoDB Performance Optimization BasicsMySQL 101: Tuning MySQL After Upgrading MemoryHow MySQL Opens and Closes Tables 3. Analyze Monitoring Data After studying MySQL documentation and learning best practices, DBAs can use monitoring software to analyze data from the MySQL server. These tools will help monitor server health while providing unique ways to visualize metrics and handle alerts.Zabbix is an open-source monitoring tool capable of monitoring networks, servers, cloud, applications, and services. Zabbix is highly secure and easily scalable.Prometheus is open-source monitoring software marketing its simplicity and visualization tools.Percona Monitoring and Management is an open-source monitoring solution aimed at helping improve database performance and improving data securityNagios XI is a premium monitoring software but offers a free trial for new users. Nagios XI promises to be limitlessly scalable and highly customizable.Releem is an excellent option for easy-to-use monitoring MySQL databases, as it offers a robust and user-friendly solution for database administrators and developers seeking to optimize their systems. By incorporating a wide range of features and benefits, Releem helps database engineers effortlessly monitor and improve their MySQL performance.What Metrics Should DBAs Monitor? Database engineers should examine various indicators of database health to guarantee that databases operate smoothly and efficiently. Resource utilization is one of the most important health check categories. Each operation within a server primarily relies on four main system resources:The CPU is the powerhouse behind the system; The memory encodes, stores, and retrieves information;Disk I/O is the input and output process for data moving from storage to other hardware components;The network consists of the client connections to the server; When these resources are not optimized, this can lead to performance degradation of both the operating system and database. Ultimately, the most critical metric is the speed at which a query is received and the data returned by the server. The following MySQL metrics are associated with the four system resources:Database Connection Utilization;MySQL Latency;MySQL Throughput (Queries per Second);Memory Utilization;Disk Space Usage;CPU Utilization.By monitoring resource utilization, database administrators can identify potential bottlenecks or capacity issues and make informed decisions about resource allocation or scaling. High resource utilization may signify the need for hardware upgrades, resource reallocation, or query optimizations to ensure optimal database performance. 4. Analyze MySQL Status Before utilizing tools like Releem or other performance optimization techniques, it's essential for database administrators to ensure that their MySQL server has been running for at least 24 hours. This allows the server to accumulate a sufficient amount of data and provide more accurate insights into its performance.To analyze MySQL server status and detect any variables that require configuration adjustments, database administrators can query the server using the SHOW GLOBAL STATUS; command. This command will deliver various metrics that reflect the current performance and status of the MySQL server.These metrics are crucial for understanding the health of the database system and can be categorized into different areas, such as cache performance, database efficiency, and temporary data metrics. With Releem, database administrators can easily manage and monitor these health checks. Each of the metrics noted below is tracked by Releem.Cache Performance Metrics;Database Efficiency Metrics;Temporary Data MetricsThread Cache Hit Rate;MyISAM Key Write Ratio;Temporary Disk Data"
10,Thread Cache Ratio;InnoDB Log File Size
10,MyISAM Cache Hit Rate;Sort Merge Passes Ratio
10,InnoDB Cache Hit Rate;Flushing Logs
10,Table Cache Hit Rate
10,"QCache FragmentationCache Performance Metrics These metrics evaluate the efficiency of cache systems in the database, identifying bottlenecks and optimization opportunities. By measuring hit rate and fragmentation across various cache types (e.g., thread, table, MyISAM, and InnoDB), they help ensure data accessibility and optimized cache usage.Thread Cache Hit Rate – Measures the efficiency of thread caching by calculating the percentage of threads reused from the cache instead of creating new ones.Thread Cache Ratio – Represents the proportion of threads created and cached, which helps evaluate the effectiveness of the thread cache configuration.MyISAM Cache Hit Rate – Calculates the percentage of key reads served from the key buffer in MyISAM tables, indicating the efficiency of the key buffer configuration.InnoDB Cache Hit Rate – Determines the percentage of InnoDB data served from the buffer pool, reflecting the efficiency of the InnoDB buffer pool size.Table Cache Hit Rate – Measures the effectiveness of table cache configuration by calculating the percentage of table open requests served from the cache.QCache Fragmentation – Assesses the level of fragmentation within the query cache, which can impact cache efficiency and query performance.Database Efficiency Metrics Monitoring overall database efficiency and performance, these metrics track key write ratios, log file sizes, and sort merge passes. They assess the database's management of data writes, storage, and sorting, helping pinpoint areas where there's room for optimization.MyISAM Key Write Ratio – Indicates the proportion of key buffer writes in relation to key writes requested, which helps assess the effectiveness of the key buffer size in MyISAM tables.InnoDB Log File Size – Evaluates the appropriateness of the InnoDB log file size, which can affect transaction processing and recovery times.Sort Merge Passes Ratio – Computes the proportion of merge passes required during sort operations, with a lower ratio indicating better performance.Flushing Logs – Tracks the frequency of log flushes, which can impact database performance and durability.Temporary Data Metrics Focusing on the creation and management of temporary data during database operations, these metrics help detect issues with temporary storage systems or query execution inefficiencies that cause excessive temporary data creation.Temporary Disk Data – Monitors the amount of temporary data created on disk during query execution, which can impact performance if it's too high. 5. Use Scripts for Configuration Recommendations Tuning MySQL to improve performance requires continuous monitoring and adjustments to the database configuration. One effective approach to achieve this is by using scripts and tools that provide recommendations for improving performance. Let's take a look at some of these tools:MySQLTuner MySQLTuner is a Perl script that analyzes a server's MySQL configuration and provides suggestions for improving performance. By reviewing various server settings and status variables, MySQLTuner identifies potential issues and recommends adjustments to optimize the database. Some of the key features of MySQLTuner include:Analyzing MySQL configuration file (my.cnf or my.ini);Evaluating server status variables and performance metrics;Providing recommendations for adjusting key performance-related settings;Highlighting potential security vulnerabilities and suggesting remediation;To use MySQLTuner, download the script, ensure that Perl is installed on the system, and execute the script. The output will include a summary of the server's status, along with specific recommendations for improving performance. Tuning-Primer Script This script uses data from ""SHOW STATUS LIKE..."" and ""SHOW VARIABLES LIKE..."" commands to generate reasonable recommendations for optimizing server variables. The original script is no longer maintained, but a Tuning-primer version on Github fully supports MariaDB. Percona Toolkit This toolkit consists of advanced open-source command-line tools designed to simplify complex or challenging MySQL tasks, allowing DBAs to focus on tasks that contribute to business goals.phpMyAdmin Advisor The Advisor system offers suggestions on server variables by analyzing MySQL status variables. phpMyAdmin is a free PHP-based tool designed for administering MySQL through a web interface. Mysqlreport Mysqlreport converts values from SHOW STATUS into an easily understandable report, providing a detailed overview of MySQL performance. It is a superior and virtually the only alternative to manually interpreting SHOW STATUS. 6. Calculate Values of MySQL Performance Settings In order to optimize MySQL performance, it's essential to understand how to properly calculate the values of various MySQL settings. This process, commonly referred to as MySQL tuning, involves adjusting parameters to improve database efficiency, increase server speed, and enhance read and write performance. By utilizing Releem, DBAs can essentially skip the manual process of this step because Releem automatically calculates and tunes all of the following variables:Thread_cache_size: Controls the number of threads to be cached for reuse. Query_cache_type: Determines the type of query caching mechanism used.query_cache_size: Sets the size of the query cache in bytes. query_cache_limit: Defines the maximum size of a single query that can be cached.query_cache_min_res_unit: Specifies the minimum result size in bytes for caching.key_buffer_size: Sets the size of the buffer used for index blocks in MyISAM tables. max_allowed_packet: Defines the maximum size of a packet that can be sent between the client and server. max_heap_table_size: Determines the maximum size of a heap table in bytes. tmp_table_size: Sets the maximum size of internal in-memory temporary tables. innodb_file_per_table: Controls whether InnoDB creates a separate file for each table. sort_buffer_size: Specifies the buffer size for sorting operations. read_rnd_buffer_size: Sets the buffer size for random read operations. bulk_insert_buffer_size: Controls the buffer size for bulk insert operations. myisam_sort_buffer_size: Specifies the buffer size for sorting MyISAM indexes during repair. innodb_buffer_pool_chunk_size: Determines the size of each chunk in the InnoDB buffer pool. join_buffer_size: Sets the buffer size for join operations. table_open_cache: Controls the number of open tables that can be cached. table_definition_cache: Determines the number of table definitions to be cached. innodb_flush_log_at_trx_commit: Controls when logs are flushed during a transaction commit. innodb_log_buffer_size: Specifies the size of the InnoDB log buffer. innodb_write_io_threads: Sets the number of I/O threads for writing to the InnoDB buffer pool. innodb_read_io_threads: Sets the number of I/O threads for reading from the InnoDB buffer pool.innodb_flush_method: Determines the method used for flushing data to InnoDB data files. innodb_thread_concurrency: Controls the number of user threads allowed inside InnoDB concurrently. optimizer_search_depth: Specifies the depth of the search tree for the query optimizer. innodb_purge_threads: Sets the number of threads used for purging operations in InnoDB. thread_handling: Determines how threads are managed by the server. max_connections: Controls the maximum number of concurrent connections to the server. innodb_buffer_pool_size: Sets the size of the InnoDB buffer pool. innodb_log_file_size: Specifies the size of the InnoDB log file.thread_pool_size: Determines the number of threads in the thread pool. 7. Create New Configuration File After using tuner tools to analyze a database's performance and gather recommendations for improvements, the next step is to create a new configuration file that incorporates these changes. When MySQL is first installed, a standard configuration file (my.cnf or my.ini) is created in the base directory. This file contains various settings that control the behavior and performance of the MySQL server. By updating this file with the recommended changes, database administrators can optimize server performance and ensure that the database runs efficiently.Here are the essential steps for creating a new configuration file:Backup the original configuration file – Before making any changes, database administrators should create a backup of the original configuration file. This will allow them to revert to the previous settings in case of any issues or unexpected behavior;Create a new configuration file – To create a new configuration file, open the original my.cnf or my.ini file in a text editor. Start making the manual calculated or recommended changes based on the analysis performed using Releem, MySQLTuner, mysqlslap, or other performance analysis tools;Update the parameters – In the new configuration file, update the relevant parameters with the recommended values. These adjustments can include changes to buffer sizes, cache settings, query optimizations, and other performance-related settings. Ensure that these changes are made in the [mysqld] sections of the configuration file;Save and close the new configuration file – Once all the necessary changes have been made, save the new configuration file, ensuring that the original file extension (i.e., .cnf or .ini) is maintained. Close the text editor once the file is saved;"
10,"8. Apply New Configuration File Once the new configuration file has been created and saved, it can be applied to the MySQL server as follows:Restart the MySQL server – To apply the changes made in the new configuration file, restart the MySQL server. This can be done by restarting the MySQL service through the system's control panel or using the appropriate command for the operating system;"
10,systemctl restart mysqld
10,In case of change 'innodb_log_file_size' only in MySQL 5.6.7 or earlier perform the following commands
10,"mysql -e""SET GLOBAL innodb_fast_shutdown = 1"""
10,systemctl stop mysqld
10,mv /var/lib/mysql/ib_logfile[01] /tmp
10,systemctl start mysqld
10,"Test the new settings – After restarting the MySQL server, test the new settings by running some queries, monitoring server performance, and checking system resources. This will help ensure that the changes have been applied correctly and that the database is functioning as expected;How Does Releem Help? Releem has been uniquely developed to help DBAs skip many of the more time-intensive steps outlined above, accelerating the process and allowing database administrators to efficiently and easily monitor and improve MySQL server performance. Releem can provide a new recommended MySQL configuration by analyzing workload, server, and database information and detecting areas of performance degradation."
10,WRITTEN BY ROMAN AGABEKOVReady to dive in? Try Releem today for FREE! No credit card required.Sign Up FreeProductAutopilot for MySQLMySQL MonitoringHealth Checks for MySQLSQL Query Optimization Competitors
10,Pricing
10,DocumentationReport a BugRequest a New FeatureSolutionsFor Developers
10,For AWS RDS
10,For Hosting Providers Wall of LoveCompanyFAQ
10,Blog
10,Privacy
10,Terms of use
10,About us
10,Sitemap SocialTwitter
10,Facebook
10,LinkedIn
10,GitHub
10,"Slack © 2023 Releem, Inc. +1 984 368-5788 500 Westover Drive #11329 Sanford, NC 27330 US"
11,Performance tuning in Athena - Amazon AthenaPerformance tuning in Athena - Amazon AthenaAWSDocumentationAmazon AthenaUser GuideService quotasResource limitsQuery optimization
11,techniquesData optimization
11,techniquesAdditional resourcesPerformance tuning in AthenaThis topic provides general information and specific suggestions for improving the
11,"performance of your Athena queries, and how to work around errors related to limits and"
11,resource usage.
11,Service quotas
11,"Athena enforces quotas for metrics like query running time, the number of concurrent"
11,"queries in an account, and API request rates. For more information about these quotas,"
11,see Service Quotas. Exceeding these
11,"quotas causes a query to fail — either when it is submitted, or during query"
11,execution.
11,Many of the performance optimization tips on this page can help reduce the running
11,time of queries. Optimization frees up capacity so that you can run more queries within
11,the concurrency quota and keeps queries from being cancelled for running too
11,long.
11,Quotas on the number of concurrent queries and API requests are per AWS account and
11,AWS Region. We recommend running one workload per AWS account (or using separate
11,provisioned capacity reservations) to keep workloads from competing for the same
11,quota.
11,"If you run two workloads in the same account, one of the workloads can run a burst of"
11,queries. This can cause the remaining workload to be throttled or blocked from running
11,"queries. To avoid this, you can move the workloads to separate accounts to give each"
11,workload its own concurrency quota. Creating a provisioned capacity reservation for one
11,or both of the workloads accomplishes the same goal.
11,Quotas in other
11,services
11,"When Athena runs a query, it can call other services that enforce quotas. During"
11,"query execution, Athena can make API calls to the AWS Glue Data Catalog, Amazon S3, and other AWS"
11,"services like IAM and AWS KMS. If you use federated queries, Athena also"
11,calls AWS Lambda. All of these services have their own limits and quotas that can be
11,"exceeded. When a query execution encounters errors from these services, it fails and"
11,"includes the error from the source service. Recoverable errors are retried, but"
11,queries can still fail if the issue does not resolve itself in time. Make sure to
11,read error messages thoroughly to determine if they come from Athena or from another
11,service. Some of the relevant errors are covered in this document.
11,"For more information about working around errors caused by Amazon S3 service quotas,"
11,see Avoid having too
11,many files later in this
11,"document. For more information about Amazon S3 performance optimization, see Best practices design patterns: optimizing Amazon S3 performance in the"
11,Amazon S3 User Guide.
11,Resource limits
11,"Athena runs queries in a distributed query engine. When you submit a query, the Athena"
11,engine query planner estimates the compute capacity required to run the query and
11,prepares a cluster of compute nodes accordingly. Some queries like DDL queries run on
11,only one node. Complex queries over large data sets run on much bigger clusters. The
11,"nodes are uniform, with the same memory, CPU, and disk configurations. Athena scales out,"
11,"not up, to process more demanding queries."
11,Sometimes the demands of a query exceed the resources available to the cluster running
11,"the query. When this happens, the query fails with the error Query exhausted"
11,resources at this scale factor.
11,"The resource most commonly exhausted is memory, but in rare cases it can also be disk"
11,space. Memory errors commonly occur when the engine performs a join or a window
11,"function, but they can also occur in distinct counts and aggregations."
11,"Even if a query fails with an 'out of resource' error once, it might succeed when you"
11,run it again. Query execution is not deterministic. Factors such as how long it takes to
11,load data and how intermediate datasets are distributed over the nodes can result in
11,"different resource usage. For example, imagine a query that joins two tables and has a"
11,heavy skew in the distribution of the values for the join condition. Such a query can
11,succeed most of the time but occasionally fail when the most common values end up being
11,processed by the same node.
11,"To prevent your queries from exceeding available resources, use the performance tuning"
11,"tips mentioned in this document. In particular, for tips on how to optimize queries that"
11,"exhaust the resources available, see Optimizing joins, Optimizing window"
11,"functions, and Optimizing queries by using approximations."
11,Query optimization
11,techniques
11,Use the query optimization techniques described in this section to make queries run
11,faster or as workarounds for queries that exceed resource limits in Athena.
11,Optimizing joins
11,There are many different strategies for executing joins in a distributed query
11,engine. Two of the most common are distributed hash joins and queries with complex
11,join conditions.
11,Distributed hash
11,join
11,The most common type of join uses an equality comparison as the join
11,condition. Athena runs this type of join as a distributed hash join.
11,"In a distributed hash join, the engine builds a lookup table (hash table) from"
11,one of the sides of the join. This side is called the build
11,side. The records of the build side are distributed across the
11,nodes. Each node builds a lookup table for its subset. The other side of the
11,"join, called the probe side, is then streamed through the"
11,nodes. The records from the probe side are distributed over the nodes in the
11,same way as the build side. This enables each node to perform the join by
11,looking up the matching records in its own lookup table.
11,When the lookup tables created from the build side of the join don't fit into
11,"memory, queries can fail. Even if the total size of the build side is less than"
11,"the available memory, queries can fail if the distribution of the records has"
11,"significant skew. In an extreme case, all records could have the same value for"
11,the join condition and have to fit into memory on a single node. Even a query
11,with less skew can fail if a set of values gets sent to the same node and the
11,values add up to more than the available memory. Nodes do have the ability to
11,"spill records to disk, but spilling slows query execution and can be"
11,insufficient to prevent the query from failing.
11,"Athena attempts to reorder joins to use the larger relation as the probe side,"
11,"and the smaller relation as the build side. However, because Athena does not"
11,"manage the data in tables, it has limited information and often must assume that"
11,the first table is the larger and the second table is the smaller.
11,"When writing joins with equality-based join conditions, assume that the table"
11,to the left of the JOIN keyword is the probe side and the table to
11,"the right is the build side. Make sure that the right table, the build side, is"
11,the smaller of the tables. If it is not possible to make the build side of the
11,"join small enough to fit into memory, consider running multiple queries that"
11,join subsets of the build table.
11,Other join types
11,"Queries with complex join conditions (for example, queries that use"
11,"LIKE , >, or other operators), are often"
11,"computationally demanding. In the worst case, every record from one side of the"
11,join must be compared to every record on the other side of the join. Because the
11,"execution time grows with the square of the number of records, such queries run"
11,the risk of exceeding the maximum execution time.
11,"To find out how Athena will execute your query in advance, you can use the"
11,"EXPLAIN statement. For more information, see Using EXPLAIN and EXPLAIN ANALYZE in"
11,Athena and Understanding Athena EXPLAIN statement
11,results.
11,Optimizing window
11,functions
11,"Because window functions are resource intensive operations, they can make queries"
11,run slow or even fail with the message Query exhausted resources at this
11,scale factor. Window functions keep all the records that they
11,operate on in memory in order to calculate their result. When the window is very
11,"large, the window function can run out of memory."
11,"To make sure your queries run within the available memory limits, reduce the size"
11,"of the windows that your window functions operate over. To do so, you can add a"
11,PARTITIONED BY clause or narrow the scope of existing partitioning
11,clauses.
11,Use
11,non-window functions instead
11,Sometimes queries with window functions can be rewritten without window
11,"functions. For example, instead of using row_number to find the top"
11,"N records, you can use ORDER BY and"
11,LIMIT. Instead of using row_number or
11,"rank to deduplicate records, you can use aggregate functions"
11,"like max_by, min_by, and arbitrary."
11,"For example, suppose you have a dataset with updates from a sensor. The sensor"
11,periodically reports its battery status and includes some metadata like
11,location. If you want to know the last battery status for each sensor and its
11,"location, you can use this query:"
11,"SELECT sensor_id,"
11,"arbitrary(location) AS location,"
11,"max_by(battery_status, updated_at) AS battery_status"
11,FROM sensor_readings
11,GROUP BY sensor_id
11,"Because metadata like location is the same for every record, you can use the"
11,arbitrary function to pick any value from the group.
11,"To get the last battery status, you can use the max_by function."
11,The max_by function picks the value for a column from the record
11,"where the maximum value of another column was found. In this case, it returns"
11,the battery status for the record with the last update time within the group.
11,This query runs faster and uses less memory than an equivalent query with a
11,window function.
11,Optimizing
11,aggregations
11,"When Athena performs an aggregation, it distributes the records across worker nodes"
11,using the columns in the GROUP BY clause. To make the task of matching
11,"records to groups as efficient as possible, the nodes attempt to keep records in"
11,memory but spill them to disk if necessary.
11,It is also a good idea to avoid including redundant columns in GROUP
11,"BY clauses. Because fewer columns require less memory, a query that"
11,describes a group using fewer columns is more efficient. Numeric columns also use
11,"less memory than strings. For example, when you aggregate a dataset that has both a"
11,"numeric category ID and a category name, use only the category ID column in the"
11,GROUP BY clause.
11,Sometimes queries include columns in the GROUP BY clause to work
11,around the fact that a column must either be part of the GROUP BY
11,"clause or an aggregate expression. If this rule is not followed, you can receive an"
11,error message like the following:
11,EXPRESSION_NOT_AGGREGATE: line 1:8: 'category' must be an aggregate
11,expression or appear in GROUP BY clause
11,"To avoid having to add a redundant columns to the GROUP BY clause,"
11,"you can use the arbitrary function, as in the following example."
11,"SELECT country_id,"
11,"arbitrary(country_name) AS country_name,"
11,COUNT(*) AS city_count
11,FROM world_cities
11,GROUP BY country_id
11,The ARBITRARY function returns an arbitrary value from the group. The
11,function is useful when you know all records in the group have the same value for a
11,"column, but the value does not identify the group."
11,Optimizing top N
11,queries
11,The ORDER BY clause returns the results of a query in sorted order.
11,Athena uses distributed sort to run the sort operation in parallel on multiple
11,nodes.
11,"If you don't strictly need your result to be sorted, avoid adding an ORDER"
11,"BY clause. Also, avoid adding ORDER BY to inner queries if"
11,"they are not strictly necessary. In many cases, the query planner can remove"
11,"redundant sorting, but this is not guaranteed. An exception to this rule is if an"
11,"inner query is doing a top N operation, such as finding the"
11,"N most recent, or N most common values."
11,"When Athena sees ORDER BY together with LIMIT, it"
11,understands that you are running a top N query and uses dedicated
11,operations accordingly.
11,NoteAlthough Athena can also often detect window functions like
11,"row_number that use top N, we recommend the"
11,simpler version that uses ORDER BY and LIMIT. For more
11,"information, see Optimizing window"
11,functions.
11,Include only
11,required columns
11,"If you don't strictly need a column, don't include it in your query. The less data"
11,"a query has to process, the faster it will run. This reduces both the amount of"
11,memory required and the amount of data that has to be sent between nodes. If you are
11,"using a columnar file format, reducing the number columns also reduces the amount of"
11,data that is read from Amazon S3.
11,"Athena has no specific limit on the number of columns in a result, but how queries"
11,are executed limits the possible combined size of columns. The combined size of
11,columns includes their names and types.
11,"For example, the following error is caused by a relation that exceeds the size"
11,limit for a relation descriptor:
11,GENERIC_INTERNAL_ERROR:
11,io.airlift.bytecode.CompilationException
11,"To work around this issue, reduce the number of columns in the query, or create"
11,subqueries and use a JOIN that retrieves a smaller amount of data. If
11,"you have queries that do SELECT * in the outermost query, you should"
11,change the * to a list of only the columns that you need.
11,Optimizing queries by using approximations
11,Athena has support for approximation
11,"aggregate functions for counting distinct values, the most frequent"
11,"values, percentiles (including approximate medians), and creating histograms. Use"
11,these functions whenever exact values are not needed.
11,"Unlike COUNT(DISTINCT col) operations, approx_distinct uses much less memory and runs faster. Similarly, using"
11,numeric_histogram instead of histogram uses approximate methods and therefore less memory.
11,Optimizing LIKE
11,"You can use LIKE to find matching strings, but with long strings,"
11,"this is compute intensive. The regexp_like function is in most cases a faster alternative, and also"
11,provides more flexibility.
11,Often you can optimize a search by anchoring the substring that you are looking
11,"for. For example, if you're looking for a prefix, it is much better to use"
11,'substr%' instead of
11,"'%substr%'. Or, if you're using"
11,"regexp_like, '^substr'."
11,Use UNION ALL
11,instead of UNION
11,UNION ALL and UNION are two ways to combine the results of
11,two queries into one result. UNION ALL concatenates the records from
11,"the first query with the second, and UNION does the same, but also"
11,removes duplicates. UNION needs to process all the records and find the
11,"duplicates, which is memory and compute intensive, but UNION ALL is a"
11,"relatively quick operation. Unless you need to deduplicate records, use UNION"
11,ALL for the best performance.
11,Use UNLOAD for
11,large result sets
11,"When the results of a query are expected to be large (for example, tens of"
11,"thousands of rows or more), use UNLOAD to export the results. In most cases, this is"
11,"faster than running a regular query, and using UNLOAD also gives you"
11,more control over the output.
11,"When a query finishes executing, Athena stores the result as a single uncompressed"
11,"CSV file on Amazon S3. This takes longer than UNLOAD, not only because the"
11,"result is uncompressed, but also because the operation cannot be parallelized. In"
11,"contrast, UNLOAD writes results directly from the worker nodes and"
11,"makes full use of the parallelism of the compute cluster. In addition, you can"
11,configure UNLOAD to write the results in compressed format and in other
11,file formats such as JSON and Parquet.
11,"For more information, see UNLOAD."
11,Use CTAS or Glue ETL to materialize frequently used aggregations
11,'Materializing' a query is a way of accelerating query performance by storing
11,"pre-computed complex query results (for example, aggregations and joins) for reuse"
11,in subsequent queries.
11,"If many of your queries include the same joins and aggregations, you can"
11,materialize the common subquery as a new table and then run queries against that
11,"table. You can create the new table with Creating a table from query results (CTAS), or a dedicated ETL tool like Glue"
11,ETL.
11,"For example, suppose you have a dashboard with widgets that show different aspects"
11,"of an orders dataset. Each widget has its own query, but the queries all share the"
11,"same joins and filters. An order table is joined with a line items table, and there"
11,is a filter to show only the last three months. If you identify the common features
11,"of these queries, you can create a new table that the widgets can use. This reduces"
11,duplication and improves performance. The disadvantage is that you must keep the new
11,table up to date.
11,Reuse query results
11,It's common for the same query to run multiple times within a short duration. For
11,"example, this can occur when multiple people open the same data dashboard. When you"
11,"run a query, you can tell Athena to reuse previously calculated results. You specify"
11,the maximum age of the results to be reused. If the same query was previously run
11,"within that time frame, Athena returns those results instead of running the query"
11,"again. For more information, see Reusing query results here in the"
11,Amazon Athena User Guide and Reduce cost and improve query performance with Amazon Athena Query Result
11,Reuse in the AWS Big Data Blog.
11,Data optimization
11,techniques
11,"Performance depends not only on queries, but also importantly on how your dataset is"
11,organized and on the file format and compression that it uses.
11,Partition your data
11,Partitioning divides your table into parts and keeps the related data together
11,"based on properties such as date, country, or region. Partition keys act as virtual"
11,columns. You define partition keys at table creation and use them for filtering your
11,"queries. When you filter on partition key columns, only data from matching"
11,"partitions is read. For example, if your dataset is partitioned by date and your"
11,"query has a filter that matches only the last week, only the data for the last week"
11,"is read. For more information about partitioning, see Partitioning data in Athena."
11,Pick partition keys that will support your queries
11,"Because partitioning has a significant impact on query performance, be sure to"
11,consider how you partition carefully when you design your dataset and tables. Having
11,too many partition keys can result in fragmented datasets with too many files and
11,"files that are too small. Conversely, having too few partition keys, or no"
11,"partitioning at all, leads to queries that scan more data than necessary."
11,Avoid
11,optimizing for rare queries
11,A good strategy is to optimize for the most common queries and avoid
11,"optimizing for rare queries. For example, if your queries look at time spans of"
11,"days, don't partition by hour, even if some queries filter to that level. If"
11,"your data has a granular timestamp column, the rare queries that filter by hour"
11,can use the timestamp column. Even if rare cases scan a little more data than
11,"necessary, reducing overall performance for the sake of rare cases is usually"
11,not a good tradeoff.
11,"To reduce the amount of data that queries have to scan, and thereby improve"
11,"performance, use a columnar file format and keep the records sorted. Instead of"
11,"partitioning by hour, keep the records sorted by timestamp. For queries on"
11,"shorter time windows, sorting by timestamp is almost as efficient as"
11,"partitioning by hour. Furthermore, sorting by timestamp does not typically hurt"
11,the performance of queries on time windows counted in days. For more
11,"information, see Use columnar file"
11,formats.
11,Note that queries on tables with tens of thousands of partitions perform
11,better if there are predicates on all partition keys. This is another reason to
11,design your partitioning scheme for the most common queries. For more
11,"information, see Query partitions"
11,by equality.
11,Use partition
11,projection
11,Partition projection is an Athena feature that stores partition information not in
11,"the AWS Glue Data Catalog, but as rules in the properties of the table in AWS Glue. When Athena"
11,"plans a query on a table configured with partition projection, it reads the table's"
11,partition projection rules. Athena computes the partitions to read in memory based on
11,the query and the rules instead of looking up partitions in the AWS Glue Data Catalog.
11,"Besides simplifying partition management, partition projection can improve"
11,performance for datasets that have large numbers of partitions. When a query
11,"includes ranges instead of specific values for partition keys, looking up matching"
11,partitions in the catalog takes longer the more partitions there are. With partition
11,"projection, the filter can be computed in memory without going to the catalog, and"
11,can be much faster.
11,"In certain circumstances, partition projection can result in worse performance."
11,"One example occurs when a table is ""sparse."" A sparse table does not have"
11,data for every permutation of the partition key values described by the partition
11,"projection configuration. With a sparse table, the set of partitions calculated from"
11,the query and the partition projection configuration are all listed on Amazon S3 even
11,when they have no data.
11,"When you use partition projection, make sure to include predicates on all"
11,partition keys. Narrow the scope of possible values to avoid unnecessary Amazon S3
11,listings. Imagine a partition key that has a range of one million values and a query
11,"that does not have any filters on that partition key. To run the query, Athena must"
11,perform at least one million Amazon S3 list operations. Queries are fastest when you
11,"query on specific values, regardless of whether you use partition projection or"
11,"store partition information in the catalog. For more information, see Query partitions"
11,by equality.
11,"When you configure a table for partition projection, make sure that the ranges"
11,that you specify are reasonable. If a query doesn't include a predicate on a
11,"partition key, all the values in the range for that key are used. If your dataset"
11,"was created on a specific date, use that date as the starting point for any date"
11,ranges. Use NOW as the end of date ranges. Avoid numeric ranges that
11,"have large number of values, and consider using the injected type"
11,instead.
11,"For more information about partition projection, see Partition projection with Amazon Athena."
11,Use partition
11,indexes
11,Partition indexes are a feature in the AWS Glue Data Catalog that improves partition lookup
11,performance for tables that have large numbers of partitions.
11,The list of partitions in the catalog is like a table in a relational database.
11,The table has columns for the partition keys and an additional column for the
11,"partition location. When you query a partitioned table, the partition locations are"
11,looked up by scanning this table.
11,"Just as with relational databases, you can increase the performance of queries by"
11,adding indexes. You can add multiple indexes to support different query patterns.
11,The AWS Glue Data Catalog partition index supports both equality and comparison operators like
11,">, >=, and < combined with the"
11,"AND operator. For more information, see Working with partition indexes in"
11,AWS Glue in the AWS Glue Developer Guide and Improve Amazon Athena query performance using AWS Glue Data Catalog partition indexes
11,in the AWS Big Data Blog.
11,Always use STRING as the type for partition keys
11,"When you query on partition keys, remember that Athena requires partition keys to"
11,be of type STRING in order to push down partition filtering into AWS Glue.
11,"If the number of partitions is not small, using other types can lead to worse"
11,"performance. If your partition key values are date-like or number-like, cast them to"
11,the appropriate type in your query.
11,Remove old and
11,empty partitions
11,"If you remove data from a partition on Amazon S3 (for example, by using Amazon S3 lifecycle), you should also remove the partition entry from the"
11,"AWS Glue Data Catalog. During query planning, any partition matched by the query is listed on"
11,"Amazon S3. If you have many empty partitions, the overhead of listing these partitions"
11,can be detrimental.
11,"Also, if you have many thousands of partitions, consider removing partition"
11,"metadata for old data that is no longer relevant. For example, if queries never look"
11,"at data older than a year, you can periodically remove partition metadata for the"
11,"older partitions. If the number of partitions grows into the tens of thousands,"
11,removing unused partitions can speed up queries that don't include predicates on all
11,partition keys. For information about including predicates on all partition keys in
11,"your queries, see Query partitions"
11,by equality.
11,Query partitions
11,by equality
11,Queries that include equality predicates on all partition keys run faster because
11,the partition metadata can be loaded directly. Avoid queries in which one or more of
11,"the partition keys does not have a predicate, or the predicate selects a range of"
11,"values. For such queries, the list of all partitions has to be filtered to find"
11,"matching values. For most tables, the overhead is minimal, but for tables with tens"
11,"of thousands or more partitions, the overhead can become significant."
11,"If it is not possible to rewrite your queries to filter partitions by equality,"
11,"you can try partition projection. For more information, see Use partition"
11,projection.
11,Avoid using MSCK REPAIR TABLE for partition maintenance
11,"Because MSCK REPAIR TABLE can take a long time to run, only adds new"
11,"partitions, and does not remove old partitions, it is not an efficient way to manage"
11,partitions (see Considerations and
11,limitations).
11,"Partitions are better managed manually using the AWS Glue Data Catalog APIs, ALTER TABLE ADD PARTITION,"
11,or AWS Glue
11,"crawlers. As an alternative, you can use partition projection, which"
11,"removes the need to manage partitions altogether. For more information, see Partition projection with Amazon Athena."
11,Validate that your queries are compatible with the partitioning scheme
11,You can check in advance which partitions a query will scan by using the EXPLAIN statement. Prefix your query with the
11,"EXPLAIN keyword, then look for the source fragment (for example,"
11,Fragment 2 [SOURCE]) for each table near the bottom of the
11,EXPLAIN output. Look for assignments where the right side is
11,defined as a partition key. The line underneath includes a list of all the values
11,for that partition key that will be scanned when the query is run.
11,"For example, suppose you have a query on a table with a dt partition"
11,key and prefix the query with EXPLAIN. If the values in the query are
11,"dates, and a filter selects a range of three days, the EXPLAIN output"
11,might look something like this:
11,dt := dt:string:PARTITION_KEY
11,":: [[2023-06-11], [2023-06-12], [2023-06-13]]"
11,The EXPLAIN output shows that the planner found three values for this
11,partition key that matched the query. It also shows you what those values are. For
11,"more information about using EXPLAIN, see Using EXPLAIN and EXPLAIN ANALYZE in"
11,Athena
11,and Understanding Athena EXPLAIN statement
11,results.
11,Use columnar file
11,formats
11,Columnar file formats like Parquet and ORC are designed for distributed analytics
11,workloads. They organize data by column instead of by row. Organizing data in
11,columnar format offers the following advantages:
11,Only the columns needed for the query are loaded
11,The overall amount of data that needs to be loaded is reduced
11,"Column values are stored together, so data can be compressed efficiently"
11,Files can contain metadata that allow the engine to skip loading unneeded
11,data
11,"As an example of how file metadata can be used, file metadata can contain"
11,information about the minimum and maximum values in a page of data. If the values
11,"queried are not in the range noted in the metadata, the page can be skipped."
11,One way to use this metadata to improve performance is to ensure that data within
11,"the files are sorted. For example, suppose you have queries that look for records"
11,where the created_at entry is within a short time span. If your data is
11,"sorted by the created_at column, Athena can use the minimum and maximum"
11,values in the file metadata to skip the unneeded parts of the data files.
11,"When using columnar file formats, make sure that your files aren't too small. As"
11,noted in Avoid having too
11,"many files, datasets with"
11,many small files cause performance issues. This is particularly true with columnar
11,"file formats. For small files, the overhead of the columnar file format outweighs"
11,the benefits.
11,Note that Parquet and ORC are internally organized by row groups (Parquet) and
11,"stripes (ORC). The default size for row groups is 128 MB, and for stripes, 64 MB. If"
11,"you have many columns, you can increase the row group and stripe size for better"
11,performance. Decreasing the row group or stripe size to less than their default
11,values is not recommended.
11,"To convert other data formats to Parquet or ORC, you can use AWS Glue ETL or Athena."
11,"For more information about using Athena for ETL, see Using CTAS and INSERT INTO for ETL and data"
11,analysis.
11,Compress data
11,Athena supports a wide range of compression formats. Querying compressed data is
11,faster and also cheaper because you pay for the number of bytes scanned before
11,decompression.
11,The gzip format provides
11,good compression ratios and has wide range support across other tools and services.
11,The zstd (Zstandard) format is
11,a newer compression format with a good balance between performance and compression
11,ratio.
11,"When compressing text files such as JSON and CSV data, try to achieve a balance"
11,between the number of files and the size of the files. Most compression formats
11,require the reader to read files from the beginning. This means that compressed text
11,"files cannot, in general, be processed in parallel. Big uncompressed files are often"
11,"split between workers to achieve higher parallelism during query processing, but"
11,this is not possible with most compression formats.
11,As discussed in Avoid having too
11,"many files, it's better to"
11,have neither too many files nor too few. Because the number of files is the limit
11,"for how many workers can process the query, this rule is especially true for"
11,compressed files.
11,"For more information about using compression in Athena, see Athena compression support."
11,Use bucketing for lookups on keys with high cardinality
11,Bucketing is a technique for distributing records into separate files based on the
11,value of one of the columns. This ensures that all records with the same value will
11,be in the same file. Bucketing is useful when you have a key with high cardinality
11,and many of your queries look up specific values of the key.
11,"For example, suppose you query a set of records for a specific user. If the data"
11,"is bucketed by user ID, Athena knows in advance which files contain records for a"
11,specific ID and which files do not. This enables Athena to read only the files that
11,"can contain the ID, greatly reducing the amount of data read. It also reduces the"
11,compute time that otherwise would be required to search through the data for the
11,specific ID.
11,Disadvantages of
11,bucketing
11,Bucketing is less valuable when queries frequently search for multiple values
11,"in the column that the data is bucketed by. The more values queried, the higher"
11,"the likelihood that all or most files will have to be read. For example, if you"
11,"have three buckets, and a query looks for three different values, all files"
11,might have to be read. Bucketing works best when queries look up single
11,values.
11,"For more information, see Partitioning and bucketing in"
11,Athena.
11,Avoid having too
11,many files
11,Datasets that consist of many small files result in poor overall query
11,"performance. When Athena plans a query, it lists all partition locations, which takes"
11,time. Handling and requesting each file also has a computational overhead.
11,"Therefore, loading a single bigger file from Amazon S3 is faster than loading the same"
11,records from many smaller files.
11,"In extreme cases, you might encounter Amazon S3 service limits. Amazon S3 supports up to"
11,"5,500 requests per second to a single index partition. Initially, a bucket is"
11,"treated as a single index partition, but as request loads increase, it can be split"
11,into multiple index partitions.
11,Amazon S3 looks at request patterns and splits based on key prefixes. If your dataset
11,"consists of many thousands of files, the requests coming from Athena can exceed the"
11,"request quota. Even with fewer files, the quota can be exceeded if multiple"
11,concurrent queries are made against the same dataset. Other applications that access
11,the same files can contribute to the total number of requests.
11,"When the request rate limit is exceeded, Amazon S3 returns the following"
11,error. This error is included in the status information for the query in
11,Athena.
11,SlowDown: Please reduce your request rate
11,"To troubleshoot, start by determining if the error is caused by a single query or"
11,"by multiple queries that read the same files. If the latter, coordinate the running"
11,"of queries so that they don't run at the same time. To achieve this, add a queuing"
11,mechanism or even retries in your application.
11,"If running a single query triggers the error, try combining data files or"
11,modifying the query to read fewer files. The best time to combine small files is
11,"before they are written. To do so, consider the following techniques:"
11,Change the process that writes the files to write larger files. For
11,"example, you could buffer records for a longer time before they are written."
11,Put files in a location on Amazon S3 and use a tool like Glue ETL to combine
11,"them into larger files. Then, move the larger files into the location that"
11,"the table points to. For more information, see Reading input files in"
11,larger groups in the AWS Glue Developer Guide and
11,How can I configure an AWS Glue ETL job to output larger files? in
11,the AWS re:Post Knowledge Center.
11,Reduce the number of partition keys. When you have too many partition
11,"keys, each partition might have only a few records, resulting in an"
11,excessive number of small files. For information about deciding which
11,"partitions to create, see Pick partition keys that will support your queries."
11,Avoid additional storage hierarchies beyond the partition
11,"To avoid query planning overhead, store files in a flat structure in each"
11,partition location. Do not use any additional directory hierarchies.
11,"When Athena plans a query, it lists all files in all partitions matched by the"
11,"query. Although Amazon S3 doesn't have directories per se, the convention is to interpret"
11,the / forward slash as a directory separator. When Athena lists
11,"partition locations, it recursively lists any directory it finds. When files within"
11,"a partition are organized into a hierarchy, multiple rounds of listings"
11,occur.
11,"When all files are directly in the partition location, most of the time only one"
11,"list operation has to be performed. However, multiple sequential list operations are"
11,required if you have more than 1000 files in a partition because Amazon S3 returns only
11,1000 objects per list operation. Having more than 1000 files in a partition can also
11,"create other, more serious performance issues. For more information, see Avoid having too"
11,many files.
11,Use SymlinkTextInputFormat only when necessary
11,Using the SymlinkTextInputFormat technique can be a way to work around situations
11,"when the files for a table are not neatly organized into partitions. For example,"
11,symlinks can be useful when all files are in the same prefix or files with different
11,schemas are in the same location.
11,"However, using symlinks adds levels of indirection to the query execution. These"
11,"levels of indirection impact overall performance. The symlink files have to be read,"
11,and the locations they define have to be listed. This adds multiple round trips to
11,"Amazon S3 that usual Hive tables do not require. In conclusion, you should use"
11,SymlinkTextInputFormat only when better options like reorganizing
11,files are not available.
11,Additional resources
11,"For additional information about performance tuning in Athena, consider the following"
11,resources:
11,Read the AWS Big Data blog post Top 10
11,performance tuning tips for Amazon Athena
11,For an article on using predicate pushdown to improve performance in federated
11,"queries, see Improve federated queries with predicate pushdown in Amazon Athena in"
11,the AWS Big Data Blog.
11,Read other Athena
11,posts in the AWS big data blog
11,Ask a question on AWS
11,re:Post using the Amazon Athena tag
11,Consult the Athena
11,topics in the AWS knowledge center
11,"Contact AWS Support (in the AWS Management Console, click Support,"
11,Support Center)
11,"Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document Conventions Athena capacity reservation APIsPreventing throttlingDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better."
14,RocksDB Tuning Guide · facebook/rocksdb Wiki · GitHub
14,Skip to content
14,Toggle navigation
14,Sign in
14,Product
14,Actions
14,Automate any workflow
14,Packages
14,Host and manage packages
14,Security
14,Find and fix vulnerabilities
14,Codespaces
14,Instant dev environments
14,Copilot
14,Write better code with AI
14,Code review
14,Manage code changes
14,Issues
14,Plan and track work
14,Discussions
14,Collaborate outside of code
14,Explore
14,All features
14,Documentation
14,GitHub Skills
14,Blog
14,Solutions
14,For
14,Enterprise
14,Teams
14,Startups
14,Education
14,By Solution
14,CI/CD & Automation
14,DevOps
14,DevSecOps
14,Resources
14,Learning Pathways
14,"White papers, Ebooks, Webinars"
14,Customer Stories
14,Partners
14,Open Source
14,GitHub Sponsors
14,Fund open source developers
14,The ReadME Project
14,GitHub community articles
14,Repositories
14,Topics
14,Trending
14,Collections
14,Pricing
14,Search or jump to...
14,"Search code, repositories, users, issues, pull requests..."
14,Search
14,Clear
14,Search syntax tips
14,Provide feedback
14,"We read every piece of feedback, and take your input very seriously."
14,Include my email address so I can be contacted
14,Cancel
14,Submit feedback
14,Saved searches
14,Use saved searches to filter your results more quickly
14,Name
14,Query
14,"To see all available qualifiers, see our documentation."
14,Cancel
14,Create saved search
14,Sign in
14,Sign up
14,You signed in with another tab or window. Reload to refresh your session.
14,You signed out in another tab or window. Reload to refresh your session.
14,You switched accounts on another tab or window. Reload to refresh your session.
14,Dismiss alert
14,facebook
14,rocksdb
14,Public
14,Notifications
14,Fork
14,6.1k
14,Star
14,27.1k
14,Code
14,Issues
14,589
14,Pull requests
14,346
14,Actions
14,Projects
14,Wiki
14,Security
14,Insights
14,Additional navigation options
14,Code
14,Issues
14,Pull requests
14,Actions
14,Projects
14,Wiki
14,Security
14,Insights
14,RocksDB Tuning Guide
14,Jump to bottom
14,"Vladimir Tarasenko edited this page Mar 28, 2023"
14,118 revisions
14,The purpose of this guide is to provide you with enough information so you can tune RocksDB for your workload and your system configuration.
14,Basic Tuning Suggestions
14,"RocksDB is flexible and highly configurable. On the other hand, RocksDB improved its self-adaptivity through the years. If you have a normal application on SSD, we don't recommend you to fine tune RocksDB at all. We recommend users to Setup Options and Basic Tuning and no need to tune it unless you see an obvious performance problem. On one hand, the typical configuration suggested on the page, or even out-of-the-box configuration usually works reasonable for normal workloads. On the other hand, fine tuned RocksDB instances are often prone to bigger performance regression when workload or hardware is changed. Users would usually need to continuously tune RocksDB to keep the performance level."
14,"When you need to fine tune RocksDB, we recommend you to to understand basic RocksDB design (there are bunch of Talks and some Publication for that), especially how RocksDB's LSM-tree is implemented and Compaction before starting to do it."
14,"While understanding toolbox of changing RocksDB behavior is important, it might be more important to figure out performance bottleneck by looking at RocksDB's stats. Start with understanding how current system works, and look for options that can address the bottleneck."
14,RocksDB statistics
14,"To understand the bottlenecks of RocksDB, there are some tools that can help you:"
14,statistics -- Set this to rocksdb::CreateDBStatistics(). You can get human-readable RocksDB statistics any time by calling options.statistics.ToString(). See Statistics for details.
14,"Compaction and DB stats -- RocksDB always keeps some stats about compaction and basic DB running status. That's the easiest way of finding the shape of the LSM-tree, and estimate read and write performance from there. See Compaction Stats and DB Status for details"
14,Perf Context and IO Stats Context Perf Context and IO Stats Context can help figure out counters within one specific query.
14,Possibilities of Performance Bottlenecks.
14,System Metrics
14,"Sometimes performance is limited due to a system metric is saturated, and the symptom is sometimes unexpected. Users who fine tune RocksDB should be capable of querying system metrics from operating systems and determine whether specific system metrics are of high usage."
14,"Disk Write Bandwidth. Usually RocksDB compactions try to write more than the SSD drive can take. The symptom could be write stalling by RocksDB (see compaction stats Stalls or statistics STALL_MICROS. Or it can cause reads to be slow, due to compaction backlog and LSM-tree structure skewed. Perf context for read queries can sometimes show that too many SST files are read for one single read. Start with tuning compaction if it happens."
14,"Disk Read IOPs. Note that sustaining read IOPs that can guarantee reasonable read performance is often much lower than hardware spec. We encourage users to measure the read IOPs they want to use through system tools (e.g. fio) and check system metrics agains this measurement. If IOPs is saturated, start with checking compaction. Also try to improve block cache hit rate. Sometimes the issue can be caused by reading index, filter or large data blocks and there are different ways of dealing with them."
14,"CPU. CPU is usually caused by read path but can also caused by compactions. Many options can be impact, e.g. compaction, compression, bloom filters, block size, etc."
14,"Space. This technically is not a bottleneck. But when system metrics are not saturated, performance is good enough and we've already filled the SSD space to almost full, we say it is bottlenecked by space. If a user hopes to serve more data from the host, compaction and compression are major areas to tune. See Space Tuning for a guideline to reduce space usage."
14,Amplification factors
14,"The terminology we also use for tuning RocksDB compactions are amplification factors: write amplification, read amplification and space amplification. These amplification factors connect size of users' logical requests to requests RocksDB made to the underlying hardware. Sometimes it's obvious which factor should be optimize when tuning RocksDB, while sometimes it is not clear. Either way, compaction is key to change the trade-off among the three."
14,Write amplification is the ratio of bytes written to storage versus bytes written to the database.
14,"For example, if you are writing 10 MB/s to the database and you observe 30 MB/s disk write rate, your write amplification is 3. If write amplification is high, the workload may be bottlenecked on disk throughput. For example, if write amplification is 50 and max disk throughput is 500 MB/s, your database can sustain a 10 MB/s write rate. In this case, decreasing write amplification will directly increase the maximum write rate."
14,"High write amplification also decreases flash lifetime. There are two ways in which you can observe your write amplification. The first is to read through the output of DB::GetProperty(""rocksdb.stats"", &stats). The second is to divide your disk write bandwidth (you can use iostat) by your DB write rate."
14,"Read amplification is the number of disk reads per query. If you need to read 5 pages to answer a query, read amplification is 5. Logical reads are those that get data from cache, either the RocksDB block cache or the OS filesystem cache. Physical reads are handled by the storage device, flash or disk. Logical reads are much cheaper than physical reads but still impose a CPU cost. You might be able to estimate the physical read rate from iostat output but that include reads done for queries and for compaction."
14,"Space amplification is the ratio of the size of database files on disk to data size. If you Put 10MB in the database and it uses 100MB on disk, then the space amplification is 10. You will usually want to set a hard limit on space amplification so you don't run out of disk space or memory. See Space Tuning for a guideline to reduce space amplification."
14,"To learn more about the three amplification factors in context of different database algorithms, we strongly recommend Mark Callaghan's talk at Highload."
14,Slowness while system metrics are not saturated
14,"Often, the system metrics are not saturated, but still RocksDB isn't as fast as users want. It could be due to different reasons. Here are some common scenarios:"
14,Compaction Isn't Fast Enough Sometimes SSD is far from saturated but compaction still can't catch up. It's possible that RocksDB compaction doesn't try to maximize resource usage and is bounded by compaction parallelism configured. The default is usually low and there is often room to improve. See Parallelism options.
14,"Cannot Write Fast Enough While writes are usually bottlenecked by write I/Os, in some cases, I/Os are not fast enough and RocksDB just can't write fast enough to WAL and memtable. Users can try unordered write, manual WAL flush and/or shard the same data to multiple DBs and write to them in parallel."
14,Demand Lower Read Latency Sometimes nothing is wrong but users just hope read latency is lower. Start with checking per query status though Perf Context and IO Stats Context to figure out it's CPU or I/O that caused time and try options accordingly.
14,Tuning Flushes and Compactions
14,Flush and compaction are important tuning for multiple bottlenecks and it is complicated. Check Compaction for how RocksDB compaction works.
14,Parallelism options
14,"When compaction is lagging behind while still far from saturating the disk, try to increase compaction parallelism."
14,"In LSM architecture, there are two background processes: flush and compaction. Both can execute concurrently via threads to take advantage of storage technology concurrency. Flush threads are in the HIGH priority pool, while compaction threads are in the LOW priority pool. To increase the number of threads in each pool call:"
14,"options.env->SetBackgroundThreads(num_threads, Env::Priority::HIGH);"
14,"options.env->SetBackgroundThreads(num_threads, Env::Priority::LOW);"
14,To benefit from more threads you might need to set these options to change the max number of concurrent compactions and flushes:
14,"max_background_compactions is the maximum number of concurrent background compactions. The default is 1, but to fully utilize your CPU and storage you might want to increase this to the minimum of (the number of cores in the system, the disk throughput divided by the average throughput of one compaction thread)."
14,max_background_flushes is the maximum number of concurrent flush operations. It is usually good enough to set this to 1.
14,Compaction Priority (only applicable to leveled compaction)
14,"compaction_pri=kMinOverlappingRatio is default in RocksDB, which is optimal for most use cases. We use it in both UDB and msgdb and write amp dropped by more than half, compared to previous default (https://fb.workplace.com/groups/MyRocks.Internal/permalink/1364925913556020/)"
14,Trigger compaction on deletes
14,"When deleting lots of rows, some of the SST files might be filled with tombstones and affected range scan performance. We extended RocksDB’s compaction to track long close-by tombstones. If a key range with high density of tombstones is detected, it immediately triggers another compaction in order to compact them away. This helped to reduce range scan performance skews caused by scanning excessed number of tombstones. In RocksDB, CompactOnDeletionCollectorFactory is the right class to do that."
14,Periodic and TTL Compaction
14,"While compaction styles sometimes usually prioritize files with more deletes, there is no guarantee it. Another set of options can help with that. options.ttl specify a time bound where stale data will be removed from SST files. options.periodic_compaction_seconds makes sure a file goes through compaction filter every once a while, so that comapction filter can remove data accordingly."
14,Flushing options
14,"All writes to RocksDB are first inserted into an in-memory data structure called memtable. Once the active memtable is full, we create a new one and mark the old one read-only. We call the read-only memtable immutable. At any point in time there is exactly one active memtable and zero or more immutable memtables. Immutable memtables are waiting to be flushed to storage. There are three options that control flushing behavior."
14,"write_buffer_size sets the size of a single memtable. Once memtable exceeds this size, it is marked immutable and a new one is created."
14,"max_write_buffer_number sets the maximum number of memtables, both active and immutable. If the active memtable fills up and the total number of memtables is larger than max_write_buffer_number we stall further writes. This may happen if the flush process is slower than the write rate."
14,"min_write_buffer_number_to_merge is the minimum number of memtables to be merged before flushing to storage. For example, if this option is set to 2, immutable memtables are only flushed when there are two of them - a single immutable memtable will never be flushed. If multiple memtables are merged together, less data may be written to storage since two updates are merged to a single key. However, every Get() must traverse all immutable memtables linearly to check if the key is there. Setting this option too high may hurt read performance."
14,Example: options are:
14,write_buffer_size = 512MB;
14,max_write_buffer_number = 5;
14,min_write_buffer_number_to_merge = 2;
14,"with a write rate of 16MB/s. In this case, a new memtable will be created every 32 seconds, and two memtables will be merged together and flushed every 64 seconds. Depending on the working set size, flush size will be between 512MB and 1GB. To prevent flushing from failing to keep up with the write rate, the memory used by memtables is capped at 5*512MB = 2.5GB. When that is reached, any further writes are blocked until the flush finishes and frees memory used by the memtables."
14,Level Style Compaction
14,See Leveled Compaction for details.
14,"In Level style compaction, database files are organized into levels. Memtables are flushed to files in level 0, which contains the newest data. Higher levels contain older data. Files in level 0 may overlap, but files in level 1 and higher are non-overlapping. As a result, Get() usually needs to check each file from level 0, but for each successive level, no more than one file may contain the key. Each level is 10 times larger than the previous one (this multiplier is configurable)."
14,"A compaction may take a few files from level N and compact them with overlapping files from level N+1. Two compactions operating at different levels or at different key ranges are independent and may be executed concurrently. Compaction speed is directly proportional to max write rate. If compaction can't keep up with the write rate, the space used by the database will continue to grow. It is important to configure RocksDB in such a way that compactions may be executed with high concurrency and fully utilize storage."
14,"Compactions at levels 0 and 1 are tricky. Files at level 0 usually span the entire key space. When compacting L0->L1 (from level 0 to level 1), compaction includes all files from level 1. With all files from L1 getting compacted with L0, compaction L1->L2 cannot proceed; it must wait for the L0->L1 compaction to finish. If L0->L1 compaction is slow, it will be the only compaction running in the system most of the time, since other compactions must wait for it to finish."
14,"L0->L1 compaction is also single-threaded. It is hard to achieve good throughput with single-threaded compaction. To see if this is causing issues, check disk utilization. If disk is not fully utilized, there might be an issue with compaction configuration. We usually recommend making L0->L1 as fast as possible by making the size of level 0 similar to size of level 1."
14,"Once you determine the appropriate size of level 1, you must decide the level multiplier. Let's assume your level 1 size is 512 MB, level multiplier is 10 and size of the database is 500GB. Level 2 size will then be 5GB, level 3 51GB and level 4 512GB. Since your database size is 500GB, levels 5 and higher will be empty."
14,"Size amplification is easy to calculate. It is (512 MB + 512 MB + 5GB + 51GB + 512GB) / (500GB) = 1.14. Here is how we would calculate write amplification: every byte is first written out to level 0. It is then compacted into level 1. Since level 1 size is the same as level 0, write amplification of L0->L1 compaction is 2. However, when a byte from level 1 is compacted into level 2, it is compacted with 10 bytes from level 2 (because level 2 is 10x bigger). The same is also true for L2->L3 and L3->L4 compactions."
14,"Total write amplification is therefore approximately 1 + 2 + 10 + 10 + 10 = 33. Point lookups must consult all files in level 0 and at most one file from each other levels. However, bloom filters help by greatly reducing read amplification. Short-lived range scans are a bit more expensive, however. Bloom filters are not useful for range scans, so the read amplification is number_of_level0_files + number_of_non_empty_levels."
14,Let's dive into options that control level compaction. We will start with more important ones and follow with less important ones.
14,"level0_file_num_compaction_trigger -- Once level 0 reaches this number of files, L0->L1 compaction is triggered. We can therefore estimate level 0 size in stable state as write_buffer_size * min_write_buffer_number_to_merge * level0_file_num_compaction_trigger."
14,"max_bytes_for_level_base and max_bytes_for_level_multiplier -- max_bytes_for_level_base is total size of level 1. As mentioned, we recommend that this be around the size of level 0. Each subsequent level is max_bytes_for_level_multiplier larger than previous one. The default is 10 and we do not recommend changing that."
14,"target_file_size_base and target_file_size_multiplier -- Files in level 1 will have target_file_size_base bytes. Each next level's file size will be target_file_size_multiplier bigger than previous one. However, by default target_file_size_multiplier is 1, so files in all L1..Lmax levels are equal. Increasing target_file_size_base will reduce total number of database files, which is generally a good thing. We recommend setting target_file_size_base to be max_bytes_for_level_base / 10, so that there are 10 files in level 1."
14,compression_per_level -- Use this option to set different compressions for different levels. It usually makes sense to avoid compressing levels 0 and 1 and to compress data only in higher levels. You can even set slower compression in highest level and faster compression in lower levels (by highest we mean Lmax).
14,"num_levels -- It is safe for num_levels to be bigger than expected number of levels in the database. Some higher levels may be empty, but this will not impact performance in any way. Only change this option if you expect your number of levels will be greater than 7 (default)."
14,Universal Compaction
14,See Universal Compaction page for more information on universal compaction.
14,"Write amplification of a level style compaction may be high in some cases. For write-heavy workloads, you may be bottlenecked on disk throughput. To optimize for those workloads, RocksDB introduced a new style of compaction that we call universal compaction, intended to decrease write amplification. However, it may increase read amplification and always increases space amplification. Universal compaction has a size limitation. Please be careful when your DB (or column family) size is over 100GB. Check Universal Compaction for details."
14,"With universal compaction, a compaction process may temporarily increase size amplification by a factor of two. In other words, if you store 10GB in database, the compaction process may consume additional 10GB, in addition to space amplification."
14,"However, there are techniques to help reduce the temporary space doubling. If you use universal compaction, we strongly recommend sharding your data and keeping it in multiple RocksDB instances. Let's assume you have S shards. Then, configure Env thread pool with only N compaction threads. Only N shards out of total S shards will have additional space amplification, thus bringing it down to N/S instead of 1. For example, if your DB is 10GB and you configure it with 100 shards, each shard will hold 100MB of data. If you configure your thread pool with 20 concurrent compactions, you will only consume extra 2GB of data instead of 10GB. Also, compactions will execute in parallel, which will fully utilize your storage concurrency."
14,"max_size_amplification_percent -- Size amplification as defined by amount of additional storage needed (in percentage) to store a byte of data in the database. Default is 200, which means that a 100 byte database could require up to 300 bytes of storage. 200 bytes of that 300 bytes are temporary and are used only during compaction. Increasing this limit decreases write amplification, but (obviously) increases space amplification."
14,"compression_size_percent -- Percentage of data in the database that is compressed. Older data is compressed, newer data is not compressed. If set to -1 (default), all data is compressed. Reducing compression_size_percent will reduce CPU usage and increase space amplification."
14,Tuning Other Options
14,General options
14,"filter_policy -- If you're doing point lookups on an uncompacted DB, you definitely want to turn bloom filters on. We use bloom filters to avoid unnecessary disk reads. You should set filter_policy to rocksdb::NewBloomFilterPolicy(bits_per_key). Default bits_per_key is 10, which yields ~1% false positive rate. Larger bits_per_key values will reduce false positive rate, but increase memory usage and space amplification."
14,"block_cache -- We usually recommend setting this to the result of the call rocksdb::NewLRUCache(cache_capacity, shard_bits). Block cache caches uncompressed blocks. OS cache, on the other hand, caches compressed blocks (since that's the way they are stored in files). Thus, it makes sense to use both block_cache and OS cache. We need to lock accesses to block cache and sometimes we see RocksDB bottlenecked on block cache's mutex, especially when DB size is smaller than RAM. In that case, it makes sense to shard block cache by setting shard_bits to a bigger number. If shard_bits is 4, total number of shards will be 16."
14,"allow_os_buffer -- [Deprecated] If false, we will not buffer files in OS cache. See comments above."
14,"max_open_files -- RocksDB keeps all file descriptors in a table cache. If number of file descriptors exceeds max_open_files, some files are evicted from table cache and their file descriptors closed. This means that every read must go through the table cache to lookup the file needed. Set max_open_files to -1 to always keep all files open, which avoids expensive table cache calls."
14,table_cache_numshardbits -- This option controls table cache sharding.
14,Increase it if table cache mutex is contended.
14,"block_size -- RocksDB packs user data in blocks. When reading a key-value pair from a table file, an entire block is loaded into memory. Block size is 4KB by default. Each table file contains an index that lists offsets of all blocks. Increasing block_size means that the index contains fewer entries (since there are fewer blocks per file) and is thus smaller. Increasing block_size decreases memory usage and space amplification, but increases read amplification."
14,Sharing cache and thread pool
14,"Sometimes you may wish to run multiple RocksDB instances from the same process. RocksDB provides a way for those instances to share block cache and thread pool. To share block cache, assign a single cache object to all instances:"
14,first_instance_options.block_cache = second_instance_options.block_cache = rocksdb::NewLRUCache(1GB)
14,This will make both instances share a single block cache of total size 1GB.
14,"Thread pool is associated with Env object. When you construct Options, options.env is set to Env::Default(), which is best in most cases. Since all Options use the same static object Env::Default(), thread pool is shared by default. See Parallelism options to learn how to set number of threads in the thread pool. This way, you can set the maximum number of concurrent running compactions and flushes, even when running multiple RocksDB instances."
14,Write stalls
14,See Write Stalls page for more details.
14,Prefix databases
14,"RocksDB keeps all data sorted and supports ordered iteration. However, some applications don't need the keys to be fully sorted. They are only interested in ordering keys with a common prefix."
14,Those applications can benefit of configuring prefix_extractor for the database.
14,prefix_extractor -- A SliceTransform object that defines key prefixes. Key prefixes are then used to perform some interesting optimizations:
14,"Define prefix bloom filters, which can reduce read amplification of prefix range queries (e.g., give me all keys that start with prefix XXX). Be sure to define Options::filter_policy."
14,Use hash-map-based memtables to avoid binary search costs in memtables.
14,Add hash index to table files to avoid binary search costs in table files.
14,"For more details on (2) and (3), see Custom memtable and table factories. Please be aware that (1) is usually sufficient in reducing I/Os. (2) and (3) can reduce CPU costs in some use cases and usually with some costs of memory. You should only try it if CPU is your bottleneck and you run out of other easier tuning to save CPU, which is not common."
14,Make sure to check comments about prefix_extractor in include/rocksdb/options.h.
14,Bloom filters
14,"Bloom filters are probabilistic data structures that are used to test whether an element is part of a set. Bloom filters in RocksDB are controlled by an option filter_policy. When a user calls Get(key), there is a list of files that may contain the key. This is usually all files on Level 0 and one file from each Level bigger than 0. However, before we read each file, we first consult the bloom filters. Bloom filters will filter out reads for most files that do not contain the key. In most cases, Get() will do only one file read. Bloom filters are always kept in memory for open files, unless BlockBasedTableOptions::cache_index_and_filter_blocks is set to true. Number of open files is controlled by max_open_files option."
14,There are two types of bloom filters: block-based and full filter.
14,Block-based filter (deprecated)
14,"Set up block based filter by calling: options.filter_policy.reset(rocksdb::NewBloomFilterPolicy(10, true)). Block-based bloom filter is built separately for each block. On a read we first consult an index, which returns the block of the key we're looking for. Now that we have a block, we consult the bloom filter for that block."
14,Full filter
14,"Set up full filter by calling: options.filter_policy.reset(rocksdb::NewBloomFilterPolicy(10, false)). Full filters are built per-file. There is only one bloom filter for a file. This means we can first consult the bloom filter without going to the index. In situations when key is not in the bloom filter, we saved one index lookup compared to block-based filter."
14,Full filters could further be partitioned: Partitioned Filters
14,Custom memtable and table format
14,Advanced users may configure custom memtable and table format.
14,memtable_factory -- Defines the memtable. Here's the list of memtables we support:
14,SkipList -- this is the default memtable.
14,HashSkipList -- it only makes sense with prefix_extractor. It keeps keys in buckets based on prefix of the key. Each bucket is a skip list.
14,HashLinkedList -- it only makes sense with prefix_extractor.
14,It keeps keys in buckets based on prefix of the key. Each bucket is a linked list.
14,table_factory -- Defines the table format. Here's the list of tables we support:
14,Block based -- This is the default table. It is suited for storing data on disk and flash storage. It is addressed and loaded in block sized chunks (see block_size option). Thus the name block based.
14,Plain Table -- Only makes sense with prefix_extractor. It is suited for storing data on memory (on tmpfs filesystem). It is byte-addressible.
14,Memory usage
14,"To learn more about how RocksDB uses memory, check out this wiki page: https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB"
14,Difference of spinning disk
14,See Tuning RocksDB on Spinning Disks.
14,Example configurations
14,In this section we will present some RocksDB configurations that we actually run in production.
14,Prefix database on flash storage
14,This service uses RocksDB to perform prefix range scans and point lookups. It is running on Flash storage.
14,options.prefix_extractor.reset(new CustomPrefixExtractor());
14,"Since the service doesn't need total order iterations (see Prefix databases), we define prefix extractor."
14,rocksdb::BlockBasedTableOptions table_options;
14,table_options.index_type = rocksdb::BlockBasedTableOptions::kHashSearch;
14,table_options.block_size = 4 * 1024;
14,options.table_factory.reset(NewBlockBasedTableFactory(table_options));
14,"We use a hash index in table files to speed up prefix lookup, but it increases storage space and memory usage."
14,options.compression = rocksdb::kLZ4Compression;
14,"LZ4 compression reduces CPU usage, but increases storage space."
14,options.max_open_files = -1;
14,"This setting disables looking up files in table cache, thus speeding up all queries. This is always a good thing to set if your server has a big limit on open files."
14,options.options.compaction_style = kCompactionStyleLevel;
14,options.level0_file_num_compaction_trigger = 10;
14,options.level0_slowdown_writes_trigger = 20;
14,options.level0_stop_writes_trigger = 40;
14,options.write_buffer_size = 64 * 1024 * 1024;
14,options.target_file_size_base = 64 * 1024 * 1024;
14,options.max_bytes_for_level_base = 512 * 1024 * 1024;
14,"We use level style compaction. Memtable size is 64MB and is flushed periodically to Level 0. Compaction L0->L1 is triggered when there are 10 level 0 files (total 640MB). When L0 is 640MB, compaction is triggered into L1, the max size of which is 512MB."
14,Total DB size???
14,options.max_background_compactions = 1
14,options.max_background_flushes = 1
14,"There can be only 1 concurrent compaction and 1 flush executing at any given time. However, there are multiple shards in the system, so multiple compactions occur on different shards. Otherwise, storage wouldn't be saturated with only 2 threads writing to storage."
14,options.memtable_prefix_bloom_bits = 1024 * 1024 * 8;
14,"With memtable bloom filter, some accesses to the memtable can be avoided."
14,"options.block_cache = rocksdb::NewLRUCache(512 * 1024 * 1024, 8);"
14,Block cache is configured to be 512MB. (is it shared across the shards?)
14,"Total ordered database, flash storage"
14,This database performs both Get() and total order iteration. Shards????
14,options.env->SetBackgroundThreads(4);
14,We first set a total of 4 threads in the thread pool.
14,options.options.compaction_style = kCompactionStyleLevel;
14,options.write_buffer_size = 67108864; // 64MB
14,options.max_write_buffer_number = 3;
14,options.target_file_size_base = 67108864; // 64MB
14,options.max_background_compactions = 4;
14,options.level0_file_num_compaction_trigger = 8;
14,options.level0_slowdown_writes_trigger = 17;
14,options.level0_stop_writes_trigger = 24;
14,options.num_levels = 4;
14,options.max_bytes_for_level_base = 536870912; // 512MB
14,options.max_bytes_for_level_multiplier = 8;
14,We use level style compaction with high concurrency. Memtable size is 64MB and the total number of level 0 files is 8. This means compaction is triggered when L0 size grows to 512MB. L1 size is 512MB and every level is 8 times larger than the previous one. L2 is 4GB and L3 is 32GB.
14,Database on Spinning Disks
14,Coming soon...
14,In-memory database with full functionalities
14,"In this example, database is mounted in tmpfs file system."
14,Use mmap read:
14,options.allow_mmap_reads = true;
14,"Disable block cache, enable bloom filters and reduce the delta encoding restart interval:"
14,BlockBasedTableOptions table_options;
14,"table_options.filter_policy.reset(NewBloomFilterPolicy(10, true));"
14,table_options.no_block_cache = true;
14,table_options.block_restart_interval = 4;
14,options.table_factory.reset(NewBlockBasedTableFactory(table_options));
14,If you want to prioritize speed. You can disable compression:
14,options.compression = rocksdb::CompressionType::kNoCompression;
14,"Otherwise, enable a lightweight compression, LZ4 or Snappy."
14,Set up compression more aggressively and allocate more threads for flush and compaction:
14,options.level0_file_num_compaction_trigger = 1;
14,options.max_background_flushes = 8;
14,options.max_background_compactions = 8;
14,options.max_subcompactions = 4;
14,Keep all the files open:
14,options.max_open_files = -1;
14,"When reading data, consider to turn ReadOptions.verify_checksums = false."
14,In-memory prefix database
14,"In this example, database is mounted in tmpfs file system. We use customized formats to speed up, while some functionalities are not supported. We support only Get() and prefix range scans. Write-ahead logs are stored on hard drive to avoid consuming memory not used for querying. Prev() is not supported."
14,"Since this database is in-memory, we don't care about write amplification. We do, however, care a lot about read amplification and space amplification. This is an interesting example because we tune the compaction to an extreme so that usually only one SST table exists in the system. We therefore decrease read and space amplification, while write amplification is extremely high."
14,"Since universal compaction is used, we will effectively double our space usage during compaction. This is very dangerous with in-memory database. We therefore shard the data into 400 RocksDB instances. We allow only two concurrent compactions, so only two shards may double space use at any one time."
14,"In this case, prefix hash can be used to allow the system to use hash indexing instead of a binary one, as well as bloom filter for iterations when possible:"
14,options.prefix_extractor.reset(new CustomPrefixExtractor());
14,"Use the memory addressing table format built for low-latency access, which requires mmap read mode to be on:"
14,"options.table_factory = std::shared_ptr<rocksdb::TableFactory>(rocksdb::NewPlainTableFactory(0, 8, 0.85));"
14,options.allow_mmap_reads = true;
14,options.allow_mmap_writes = false;
14,Use hash link list memtable to change binary search to hash lookup in mem table:
14,options.memtable_factory.reset(rocksdb::NewHashLinkListRepFactory(200000));
14,"Enable bloom filter for hash table to reduce memory accesses (usually means CPU cache misses) when reading from mem table to one, for the case where key is not found in mem tables:"
14,options.memtable_prefix_bloom_bits = 10000000;
14,options.memtable_prefix_bloom_probes = 6;
14,"Tune compaction so that, a full compaction is kicked off as soon as we have two files. We hack the parameter of universal compaction:"
14,options.compaction_style = kUniversalCompaction;
14,options.compaction_options_universal.size_ratio = 10;
14,options.compaction_options_universal.min_merge_width = 2;
14,options.compaction_options_universal.max_size_amplification_percent = 1;
14,options.level0_file_num_compaction_trigger = 1;
14,options.level0_slowdown_writes_trigger = 8;
14,options.level0_stop_writes_trigger = 16;
14,Tune bloom filter to minimize memory accesses:
14,options.bloom_locality = 1;
14,"Reader objects for all tables are always cached, avoiding table cache access when reading:"
14,options.max_open_files = -1;
14,"Use one mem table at one time. Its size is determined by the full compaction interval we want to pay. We tune compaction such that after every flush, a full compaction will be triggered, which costs CPU. The larger the mem table size, the longer the compaction interval will be, and at the same time, we see less memory efficiency, worse query performance and longer recovery time when restarting the DB."
14,options.write_buffer_size = 32 << 20;
14,options.max_write_buffer_number = 2;
14,options.min_write_buffer_number_to_merge = 1;
14,Multiple DBs sharing the same compaction pool of 2:
14,options.max_background_compactions = 1;
14,options.max_background_flushes = 1;
14,"options.env->SetBackgroundThreads(1, rocksdb::Env::Priority::HIGH);"
14,"options.env->SetBackgroundThreads(2, rocksdb::Env::Priority::LOW);"
14,Settings for WAL logs:
14,options.bytes_per_sync = 2 << 20;
14,Suggestion for in memory block table
14,"hash_index: In the new version, hash index is enabled for block based table. It would use 5% more storage space but speed up the random read by 50% compared to normal binary search index."
14,table_options.index_type = rocksdb::BlockBasedTableOptions::kHashSearch;
14,"block_size: By default, this value is set to be 4k. If compression is enabled, a smaller block size would lead to higher random read speed because decompression overhead is reduced. But the block size cannot be too small to make compression useless. It is recommended to set it to be 1k."
14,"verify_checksum: As we are storing data in tmpfs and care read performance a lot, checksum could be disabled."
14,Final thoughts
14,"Unfortunately, configuring RocksDB optimally is not trivial. Even we as RocksDB developers don't fully understand the effect of each configuration change. If you want to fully optimize RocksDB for your workload, we recommend experiments and benchmarking, while keeping an eye on the three amplification factors. Also, please don't hesitate to ask us for help on the RocksDB Developer's Discussion Group."
14,Toggle tagle of contents
14,Pages 165
14,Home
14,[To Be Deprecated] Persistent Read Cache
14,A Tutorial of RocksDB SST formats
14,Administration and Data Access Tool
14,Allocating Some Indexes and Bloom Filters using Huge Page TLB
14,Approximate Size
14,Articles about Rocks
14,Asynchronous IO
14,Atomic flush
14,Background Error Handling
14,Basic Operations
14,Benchmarking tools
14,BlobDB
14,Block Cache
14,Block cache analysis and simulation tools
14,Building on Windows
14,Checkpoints
14,Choose Level Compaction Files
14,Column Families
14,Compaction
14,Compaction Filter
14,Compaction Stats and DB Status
14,Compaction Trivial Move
14,Compression
14,Creating and Ingesting SST files
14,CuckooTable Format
14,Daily Off‐peak Time Option
14,Data Block Hash Index
14,Delete A Range Of Keys
14,Delete Stale Files
14,DeleteRange
14,DeleteRange Implementation
14,Developing with an IDE
14,Dictionary Compression
14,Direct IO
14,EventListener
14,Features Not in LevelDB
14,FIFO compaction style
14,Full File Checksum and Checksum Handoff
14,Fuzz Test
14,How to ask a performance related question
14,How to backup RocksDB
14,How to persist in memory RocksDB database
14,How we keep track of live SST files
14,Implement Queue Service Using RocksDB
14,Index Block Format
14,Indexing SST Files for Better Lookup Performance
14,IO Tracer and Parser
14,Iterator
14,Iterator Implementation
14,JNI Debugging
14,Journal
14,Known Issues
14,Leveled Compaction
14,log_format.txt
14,Logger
14,Logging in RocksJava
14,Low Priority Write
14,Lua CompactionFilter
14,Managing Disk Space Utilization
14,MANIFEST
14,Manual Compaction
14,Memory usage in RocksDB
14,Mempurge (Memtable Garbage Collection) [Experimental]
14,MemTable
14,Merge Operator
14,Merge Operator Implementation
14,MultiGet Performance
14,Object Registry
14,Online Verification
14,Open Projects
14,Option String and Option Map
14,Partitioned Index Filters
14,Perf Context and IO Stats Context
14,Performance Benchmark 2014
14,Performance Benchmark 201807
14,Performance Benchmarks
14,Performance Benchmarks October 2022
14,Pipelined Write
14,PlainTable Format
14,Platform Requirements
14,poc
14,poc test
14,poc.html
14,Prefix Seek
14,Projects Being Developed
14,Proposal: Unifying Level and Universal Compactions
14,Proposals on Improving Rocksdb's Options
14,Publication
14,Rate Limiter
14,Read Modify Write Benchmarks
14,Read only and Secondary instances
14,Reducing memcpy overhead when using Iterators
14,Remote Compaction (Experimental)
14,Replication Helpers
14,Rocksdb Architecture Guide
14,Rocksdb BlockBasedTable Format
14,RocksDB Bloom Filter
14,RocksDB Compatibility Between Different Releases
14,RocksDB Configurable Objects
14,RocksDB Contribution Guide
14,RocksDB Extensions
14,RocksDB FAQ
14,RocksDB In Memory Workload Performance Benchmarks
14,RocksDB Options File
14,RocksDB Overview
14,RocksDB Public Communication and Information Channels
14,RocksDB Release Methodology
14,RocksDB Repairer
14,Rocksdb Table Format
14,"RocksDB Trace, Replay, Analyzer, and Workload Generation"
14,RocksDB Troubleshooting Guide
14,RocksDB Tuning Guide
14,Basic Tuning Suggestions
14,RocksDB statistics
14,Possibilities of Performance Bottlenecks.
14,System Metrics
14,Amplification factors
14,Slowness while system metrics are not saturated
14,Tuning Flushes and Compactions
14,Parallelism options
14,Compaction Priority (only applicable to leveled compaction)
14,Trigger compaction on deletes
14,Periodic and TTL Compaction
14,Flushing options
14,Level Style Compaction
14,Universal Compaction
14,Tuning Other Options
14,General options
14,Sharing cache and thread pool
14,Write stalls
14,Prefix databases
14,Bloom filters
14,Block-based filter (deprecated)
14,Full filter
14,Custom memtable and table format
14,Memory usage
14,Difference of spinning disk
14,Example configurations
14,Prefix database on flash storage
14,"Total ordered database, flash storage"
14,Database on Spinning Disks
14,In-memory database with full functionalities
14,In-memory prefix database
14,Suggestion for in memory block table
14,Final thoughts
14,RocksDB Users and Use Cases
14,RocksDB version macros
14,RocksJava API TODO
14,RocksJava Basics
14,RocksJava Performance on Flash Storage
14,SecondaryCache (Experimental)
14,SeekForPrev
14,Setup Options and Basic Tuning
14,Simulation Cache
14,Single Delete
14,Slow Deletion
14,Snapshot
14,Space Tuning
14,Speed Up DB Open
14,SST File Manager
14,Statistics
14,Stress test
14,Subcompaction
14,Tailing Iterator
14,Talks
14,Terminology
14,testpoc.html
14,Tests
14,The Customizable Class
14,Third party language bindings
14,This is a test
14,Thread Pool
14,Tiered Storage (Experimental)
14,Tiered Storage Benchmarking
14,Time to Live
14,Track WAL in MANIFEST
14,Transactions
14,Tuning RocksDB from Java
14,Tuning RocksDB on Spinning Disks
14,Two Phase Commit Implementation
14,Universal Compaction
14,Universal Style Compaction Example
14,unordered_write
14,User defined Timestamp
14,WAL Compression
14,WAL Performance
14,WAL Recovery Modes
14,What's new in RocksDB2.7
14,Wide Columns
14,Write Ahead Log (WAL)
14,Write Ahead Log File Format
14,Write Batch With Index
14,Write Buffer Manager
14,Write Stalls
14,WritePrepared Transactions
14,WriteUnprepared Transactions
14,Show 150 more pages…
14,Contents
14,RocksDB Wiki
14,Overview
14,RocksDB FAQ
14,Terminology
14,Requirements
14,Contributors' Guide
14,Release Methodology
14,RocksDB Users and Use Cases
14,RocksDB Public Communication and Information Channels
14,Basic Operations
14,Iterator
14,Prefix seek
14,SeekForPrev
14,Tailing Iterator
14,Compaction Filter
14,Read-Modify-Write (Merge) Operator
14,Column Families
14,Creating and Ingesting SST files
14,Single Delete
14,Low Priority Write
14,Time to Live (TTL) Support
14,Transactions
14,Snapshot
14,DeleteRange
14,Atomic flush
14,Read-only and Secondary instances
14,Approximate Size
14,User-defined Timestamp
14,Wide Columns
14,BlobDB
14,Online Verification
14,Options
14,Setup Options and Basic Tuning
14,Option String and Option Map
14,RocksDB Options File
14,MemTable
14,Journal
14,Write Ahead Log (WAL)
14,Write Ahead Log File Format
14,WAL Recovery Modes
14,WAL Performance
14,WAL Compression
14,MANIFEST
14,Track WAL in MANIFEST
14,Cache
14,Block Cache
14,SecondaryCache (Experimental)
14,Write Buffer Manager
14,Compaction
14,Leveled Compaction
14,Universal compaction style
14,FIFO compaction style
14,Manual Compaction
14,Subcompaction
14,Choose Level Compaction Files
14,Managing Disk Space Utilization
14,Trivial Move Compaction
14,Remote Compaction (Experimental)
14,SST File Formats
14,Block-based Table Format
14,PlainTable Format
14,CuckooTable Format
14,Index Block Format
14,Bloom Filter
14,Data Block Hash Index
14,Rate Limiter
14,SST File Manager
14,Direct I/O
14,CompressionDictionary Compression
14,Full File Checksum and Checksum Handoff
14,Background Error Handling
14,Huge Page TLB Support
14,Tiered Storage (Experimental)
14,Logging and Monitoring
14,Logger
14,Statistics
14,Compaction Stats and DB Status
14,Perf Context and IO Stats Context
14,EventListener
14,Known Issues
14,Troubleshooting Guide
14,Tests
14,Stress Test
14,Fuzzing
14,Benchmarking
14,Tools / Utilities
14,Administration and Data Access Tool
14,How to Backup RocksDB?
14,Replication Helpers
14,Checkpoints
14,How to persist in-memory RocksDB database
14,Third-party language bindings
14,"RocksDB Trace, Replay, Analyzer, and Workload Generation"
14,Block cache analysis and simulation tools
14,IO Tracer and Parser
14,Implementation Details
14,Delete Stale Files
14,Partitioned Index/Filters
14,WritePrepared-Transactions
14,WriteUnprepared-Transactions
14,How we keep track of live SST files
14,How we index SST
14,Merge Operator Implementation
14,RocksDB Repairer
14,Write Batch With Index
14,Two Phase Commit
14,Iterator's Implementation
14,Simulation Cache
14,[To Be Deprecated] Persistent Read Cache
14,DeleteRange Implementation
14,unordered_write
14,Extending RocksDB
14,RocksDB Configurable Objects
14,The Customizable Class
14,Object Registry
14,RocksJava
14,RocksJava Basics
14,Logging in RocksJava
14,JNI Debugging
14,RocksJava API TODO
14,RocksJava Performance on Flash Storage
14,Tuning RocksDB from Java
14,LuaLua CompactionFilter
14,Performance
14,Performance Benchmarks
14,In Memory Workload Performance
14,Read-Modify-Write (Merge) Performance
14,Delete A Range Of Keys
14,Write Stalls
14,Pipelined Write
14,MultiGet Performance
14,Tuning Guide
14,Memory usage in RocksDB
14,Speed-Up DB Open
14,Implement Queue Service Using RocksDB
14,Asynchronous IO
14,Off-peak in RocksDB
14,Projects Being Developed
14,Misc
14,Building on Windows
14,Developing with an IDE
14,Open Projects
14,Talks
14,Publication
14,Features Not in LevelDB
14,How to ask a performance-related question?
14,Articles about Rocks
14,Clone this wiki locally
14,Footer
14,"© 2024 GitHub, Inc."
14,Footer navigation
14,Terms
14,Privacy
14,Security
14,Status
14,Docs
14,Contact
14,Manage cookies
14,Do not share my personal information
14,You can’t perform that action at this time.
16,Reddit - Dive into anything
16,Skip to main content
16,Reddit and its partners use cookies and similar technologies to provide you with a better experience.
16,"By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising."
16,"By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform."
16,"For more information, please see our"
16,Cookie Notice
16,and our
16,Privacy Policy.
16,Open menu
16,Open navigation
16,Go to Reddit Home
16,r/synology
16,A chip
16,A close button
16,Get app
16,Get the Reddit app
16,Log In
16,Log in to Reddit
16,Expand user menu
16,Open settings menu
16,Log In / Sign Up
16,Advertise on Reddit
16,Shop Collectible Avatars
16,Get the Reddit app
16,Scan this QR code to download the app now
16,Or check it out in the app stores
16,Go to synology
16,r/synology
16,r/synology
16,A community to discuss Synology NAS and networking devices
16,Members
16,Online
16,CallMeGooglyBear
16,ADMIN
16,MOD
16,Speeding up MariaDB - is it possible
16,DSM
16,false
16,I'm trying to move away from my Linux host to Syno for Mariadb.
16,It is slower than molasses in winter. Running on a 920 with 8 GB RAM. Very low utilization otherwise
16,"Looking at logs, it takes seconds for basic transactions. Here is my my.cnf config"
16,[mysqld]
16,skip-networking=0
16,skip-bind-address
16,skip-name-resolve=1
16,character_set_server=utf8
16,aria_pagecache_buffer_size=1G
16,optimizer_search_depth=0
16,innodb_buffer_pool_chunk_size=1G
16,innodb_buffer_pool_instances=4
16,innodb_adaptive_hash_index=OFF
16,innodb_buffer_pool_dump_at_shutdown=ON
16,innodb_buffer_pool_load_at_startup=ON
16,Read more
16,Archived post. New comments cannot be posted and votes cannot be cast.
16,Top 1%
16,Rank by size
16,&nbsp;
16,TOPICS
16,Gaming
16,Valheim
16,Genshin Impact
16,Minecraft
16,Pokimane
16,Halo Infinite
16,Call of Duty: Warzone
16,Path of Exile
16,Hollow Knight: Silksong
16,Escape from Tarkov
16,Watch Dogs: Legion
16,Sports
16,NFL
16,NBA
16,Megan Anderson
16,Atlanta Hawks
16,Los Angeles Lakers
16,Boston Celtics
16,Arsenal F.C.
16,Philadelphia 76ers
16,Premier League
16,UFC
16,Business
16,GameStop
16,Moderna
16,Pfizer
16,Johnson & Johnson
16,AstraZeneca
16,Walgreens
16,Best Buy
16,Novavax
16,SpaceX
16,Tesla
16,Crypto
16,Cardano
16,Dogecoin
16,Algorand
16,Bitcoin
16,Litecoin
16,Basic Attention Token
16,Bitcoin Cash
16,Television
16,The Real Housewives of Atlanta
16,The Bachelor
16,Sister Wives
16,90 Day Fiance
16,Wife Swap
16,The Amazing Race Australia
16,Married at First Sight
16,The Real Housewives of Dallas
16,My 600-lb Life
16,Last Week Tonight with John Oliver
16,Celebrity
16,Kim Kardashian
16,Doja Cat
16,Iggy Azalea
16,Anya Taylor-Joy
16,Jamie Lee Curtis
16,Natalie Portman
16,Henry Cavill
16,Millie Bobby Brown
16,Tom Hiddleston
16,Keanu Reeves
16,RESOURCES
16,About Reddit
16,Advertise
16,Help
16,Blog
16,Careers
16,Press
16,Communities
16,Best of Reddit
16,Topics
16,Impressum
16,Content Policy
16,Privacy Policy
16,User Agreement
16,"Reddit, Inc. © 2024. All rights reserved."
16,Want to browse anonymously?
16,Scan this QR code to download the app now
18,鲲鹏社区-官网丨凝心聚力 共创行业新价值鲲鹏社区鲲鹏开发者鲲鹏开发者社区鲲鹏众智鲲鹏MVP鲲鹏生态创新中心Powered by Kunpeng请启用JavaScript
19,"MySQL vs. MariaDB | NexcessChat with usProductsProductsProductsProductsEnterpriseEnterpriseResourcesResourcesWhy NexcessWhy NexcessPartner ProgramsPartner ProgramsPricingPricingContact UsContact UsSign inSign inMySQL vs. MariaDB: Nexcess application stack with MariaDBKnowledge Base HomeSearch Notice anything different?We've enhanced the appearance of our portal and we're working on updating screenshots. Things might look different, but the functionality remains the same.April 27, 2023By Zachary ArmstrongMySQL is one of the earliest database management systems. At one time, it was available by default with almost every website hosting provider. It was the de facto standard until Oracle Corporation bought Sun Microsystems, along with MySQL. This sale eventually led to the creation of MariaDB, a fork of MySQL.Locate the reliable hosting solution you need In this article, we’ll go over the history of MySQL and MariaDB and see how they compare — MySQL vs. MariaDB — and the performance rationale why the Nexcess application stack is built using MariaDB.How Oracle’s purchase led to MariaDB’s creationGiven that Oracle's main product is Oracle SQL, the MySQL community was apprehensive about its future after the sale. Although Oracle said it would continue to develop MySQL, the rapidly released version 5.5 didn’t eliminate any bugs.Oracle also switched the database to an “open-core” model — selling addons alongside the open-source code — and reduced support options. As a result, some MySQL users looked for alternatives. And some developers decided to leave Oracle and pursue their own projects, taking the then-still free MySQL codebase.One of those projects became MariaDB, which is identical to the original MySQL version at the protocol file format and SQL language level. This level of compatibility means any application built to run on MySQL should run on MariaDB without issue.However, MariaDB is faster and offers some unique features. For example, the Sphinx storage engine (SphinxSE) is integrated into the server itself and doesn’t have to be installed separately. MariaDB also offers advanced backup and data management capabilities.MySQL vs. MariaDB: Tech giants weigh inIn 2013, Red Hat Enterprise Linux switched to using MariaDB by default. The change came in part because of the difficulty in contributing certain patches and features to MySQL after the changes made by Oracle.The same year, Google also announced that it was moving its database infrastructure to MariaDB. A Google engineer noted that although Oracle offers quality development work, it doesn’t offer enough public visibility.MariaDB later partnered with Google to launch its Database-as-a-Service (DBaaS) SkySQL on the Google Cloud Platform. In 2022, Acronis announced a collaboration with MariaDB to offer advanced functionality and convenience for MariaDB users when setting up their backup repositories.In addition, some newer packages of Linux use MariaDB instead of MySQL. And the tech community is very involved in MariaDB’s development. For instance, Alibaba and Wikipedia have submitted several patches to the code. Other big names that use MariaDB include Samsung and Nokia.Comparison: MySQL vs. MariaDB performanceNow that you know a bit about the history of MySQL and MariaDB, let’s take a look at their performance.Rundown of the data storage enginesOracle acquired the InnoDB storage engine when it purchased Innobase in 2005. InnoDB is also available for use on MariaDB. MyISAM was the default data storage engine for MySQL until 2009. It’s still available to use in MySQL, and MariaDB uses some of its code.MariaDB’s Aria storage engine is faster than MyISAM and InnoDB. It also recovers significantly faster than MyISAM after a server crash. MariaDB can also use Percona’s XtraDB, which is based on InnoDB code. However, it performs better than InnoDB, thanks to patches from Google and Percona.Testing MySQL vs. MariaDBThe internet is full of tests conducted by enthusiasts and MariaDB.To better understand why the optimizations performed in MariaDB are so good, we conducted several comparative tests on the Nexcess server. The mysqlslap utility was used in our tests:#mysqlslap --auto-generate-sql --concurrency=$i --number-of-queries=$(($i*400)) --iterations=3The $i variable changes from 10 to 200 in a loop while emulating the simultaneous operation of 10 to 200 clients, each making 400 database requests. Next, it measures the total test execution time.Aria storage engine (MariaDB) vs. MyISAM storage engine (MySQL)The graph below compares the performance of MySQL’s MyISAM storage engine vs. MariaDB’s Aria storage engine. The X axis shows the number of simultaneously working clients. The Y axis shows the time in seconds spent on the test:MariaDB completed the test twice as fast as MySQL.XtraDB storage engine (MariaDB) vs. InnoDB storage engine (MySQL)The graph below compares MariaDB’s XtraDB storage engine vs. MySQL’s InnoDB storage engine:The situation is similar here in that MariaDB won, but by a much more significant margin.MariaDB performance tuning toolsDespite MariaDB’s advantages over MySQL, you’ll still want to optimize its performance to ensure fast and efficient operation. Thankfully, help is at hand. Many MariaDB performance tuning tools are available, and plenty of them work for MySQL, too.MySQLTunerThis Perl-based performance tuner is a good choice for directly configuring MariaDB. However, if you’re just dipping your feet into using this relational database, you’ll like the succinct and clear suggestions for performance improvement.Percona ToolkitThe Percona Toolkit comprises over 30 command-line tools. In particular, the tried-and-true pt-query-digest tool helps analyze queries in MariaDB and pinpoint the slowest ones.MariaDB MaxScaleThis performance tuner from MariaDB has various features, such as load balancing, ensuring an even distribution of resources.MariaDB query cacheDepending on your database setup, enabling MariaDB’s query cache (it’s disabled by default) could give you serious performance optimization for SELECT query results that are repeated often.Performance SchemaPerformance Schema is another MariaDB tuning feature that’s disabled by default. Its primary goal is monitoring the performance of MariaDB servers. Although it appears as a storage engine, it doesn’t function exactly like one.The Performance Schema's various tables give a good overview of specific performance problems, letting you quickly identify the root cause of any issue or issues.WordPress: MySQL vs. MariaDBIf you’re already familiar with MySQL, you won’t have problems using MariaDB. They have the same syntax base, and it’ll only take a few days to ramp up on the minor differences.For example, here’s the code for MySQL vs. MariaDB:If you’re a beginner and need help deciding what to choose, feel free to start with MariaDB. The learning process will be the same, and the Nexcess Knowledge Base can help answer your questions.Almost every WordPress hosting service uses MariaDB instead of MySQL in its application stack. This choice is because MariaDB offers faster performance, greater reliability, and functions that aren’t available in MySQL.Taking advantage of MariaDB's superior performance for your WordPress website with help from NexcessWhen you choose fully managed WordPress hosting from Nexcess, you can focus on your business and let us sort the rest. As just one example documented here, we have a hosting infrastructure and technology stack with proven performance.Hosting support that goes beyond other providersWe believe in a service that goes beyond; one that empowers users and provides stability. Make your web hosting experience simple with proactive site monitoring, security hardening, and a dedicated support team available 24/7/365.See what our customers say about Nexcess Our technical support specialists will help you every step of the way, whether you have questions about improving your databases or complex technical problems requiring deep knowledge of WordPress. You can create a ticket, open a live chat, or give us a call.Click to see how we do fully managed ""Woo"" too!"
19,"Contact us Free migration from Nexcess guarantees a safe transition from any web host. Nexcess also offers pre-installed plugins, the latest updates, powerful server protection, and a free SSL certificate.Contact Nexcess today to choose the hosting plan that’s right for you.Related resourcesWhat is managed hosting? Everything you need to knowWhy choose Nexcess?Compare Nexcess to other managed hostsHow to monitor running MariaDB and MySQL queriesMariaDB performance tuning tips at NexcessMariaDB databases and MariaDB setup in the Nexcess CloudWhat is Nexcess Cloud auto scaling?Managed WordPress & WooCommerce: How to make a WordPress site liveHow to add an SSH key to the server for your Nexcess Cloud accountZachary ArmstrongZachary is a Linux Technician and Cloud Technology Specialist. Since childhood, he has had an interest in computers and the Internet. He decided to share his knowledge and passion through writing. In fact, Zachary believes that a good article can inspire people or help them to disassemble a complex idea into a very simple one for better comprehension. Zachary is a writer who specializes in breaking down complex subjects and making them easy to understand. He has a passion for technology, believes it can change the world for the better, and wants to tell the whole world about it.On a personal note, Zachary loves art and science, especially the field of Neuroscience. If you are also passionate about how the brain works, then you have something to talk about with him always.There is funny story about his work for Nexcess and Liquid Web. Before Zachary explained how he tells people about interesting things on the Internet, his six-year-old sister thought he was a TikToker!Subscribe For Monthly TipsGrow your online business faster with news, tips, strategies, and inspiration.Featured ArticlesTransfer from Pantheon Hosting to Nexcess HostingRefer a friend and get $100Transfer from SiteGround Hosting to Nexcess HostingTransfer from Bluehost hosting to Nexcess hostingMigration guide: transfer my Wix website to Nexcess Transferring Webflow websites to Nexcess hostingTransfer from HostGator hosting to Nexcess hostingMigration Guide: Transfer a Shopify Store to NexcessTransfer from InMotion Hosting to Nexcess hostingTransfer from Hostinger hosting to Nexcess hostingTransfer from Flywheel hosting to Nexcess hostingTransfer from Kinsta hosting to Nexcess hosting Transferring a domain from GoDaddyCategories.htaccessAffiliatesApplicationsBackupsBilling BusinessCDNCDN SSLClient PortalContent Delivery Networks (CDNs)Control Panel ToolsCraft CMSCron JobsDatabasesDev SitesDomain ManagementDrupalEcommerceEmail Enterprise HostingExpressionEngineFTPFile ManagementGetting StartedHostingIP ManagementMagentoMagento 1Magento 2Membership sitesMiscellaneous NexcessNexcess Email ServicesNodeWorxOther ApplicationsOther Best PracticesPCI DSSPWAPerformanceReports and MonitoringSSHSSL ScriptsSecuritySiteWorxStoreBuilderThird Party ClientsWPQuickStartWeb designWeb developmentWebsite ManagementWebsitesWooCommerceWordPressProducts+MagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingProductsMagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingAdd Ons+AutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceAdd OnsAutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceFeatures+Application StackPCI ComplianceDNSFree MigrationFeaturesApplication StackPCI ComplianceDNSFree MigrationPlugins+EventsFundraisingWordPress LMSPluginsEventsFundraisingWordPress LMSResources+Customer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQResourcesCustomer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQCompany+AboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutCompanyAboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutLEGAL  |  © 2024 Nexcess.Net, LLC All Rights Reserved.We use cookies to understand how you interact with our site, to personalize and streamline your experience, and to tailor advertising. By continuing to use our site, you accept our use of cookies and accept our Privacy Policy.Accept"
20,How to Increase Database Performance – 6 Easy Tips - DNSstuff
20,Skip to content
20,Menu
20,Networking
20,Network Monitoring Software
20,Bandwidth Monitoring
20,Scan Network for IP addresses
20,Network Traffic Monitoring Tools
20,IP Address Conflict
20,Network Mapping Tools
20,IPAM Software
20,Observability
20,What is Observability?
20,Server Monitoring Best Practices
20,Systems
20,IT Inventory Software
20,SSL Certificate Monitoring
20,Windows Server Performance Monitoring
20,IIS Performance Monitoring
20,Databases
20,SQL Server Queries Monitoring
20,SQL Server Performance
20,Oracle Database Monitoring
20,How to Increase Database Performance
20,Security
20,Help Desk
20,ITSM
20,IT Service Management (ITSM) Tools
20,ITIL Event Management
20,Incident Management Tools
20,Help Desk vs Service Desk
20,IT Asset Management Software
20,Help Desk Ticketing System
20,Free Help Desk Software
20,Free Tools
20,Compare
20,"How to Increase Database Performance – 6 Easy TipsBy Staff Contributor on July 28, 2023"
20,"Database administrators are all too familiar with the frustration of receiving an endless stream of calls about slow online performance. Instead of trying to resolve each individual issue as it arises, a better solution is to undertake database performance tuning activities that will improve online performance for all your end users. These activities can help you identify any bottlenecks in your system and ensure your infrastructure is able to handle increased loads."
20,"There are several steps you can take to increase database performance. The following six easy tips can help you prevent or rectify possible issues with database performance. Even with these tips, it’s important to remember the best way to increase database performance is always by using the right tools. Based on my experiences and tests I can recommend SolarWinds® Database Performance Analyzer (DPA) and Database Performance Monitor (DPM)."
20,By using even the free trial versions of these tools you’ll be able to:
20,"Monitor and analyze the performance of database servers running individually, in clusters, and as cloud infrastructure."
20,Receive and implement database performance advice on specific databases and related queries.
20,"Monitor database performance in real time and analyze past periods. In addition, the software tested by us enables the detection of anomalies in the load time and the location of bottlenecks in database performance."
20,"After going through the best tips for manually improving performance, I’ll take a closer look at some of the best tools to help you improve performance even further."
20,Why Is Increasing Database Performance Important?Tips To Increase Database PerformanceTools That Can Help YouSummary
20,Why Is Increasing Database Performance Important?
20,"People often wonder whether it’s important to increase database performance. The truth is your business can only ever be as successful as your IT operations allow it to be. In fact, a high-functioning database can have a huge impact on corporate profitability. When data retrieval is slowed down by anything from a poorly written query to an indexing issue, a bottleneck that slows down performance and lowers productivity for the entire organization can emerge. When you learn how to increase database performance, you’re better able to avoid unnecessary financial loss as a result of server inefficiencies."
20,"There are also many financial gains that come with improving the end-user experience. Since your customer-facing websites and applications retrieve data from your centralized database, inefficient indexes and suboptimal queries can have just as big an impact on customers as on your internal end users. As a result, your customer satisfaction is directly linked to database performance. That means knowing how to increase database performance can be one of the most important customer service tools in your toolbox."
20,Tips to Increase Database Performance
20,"While there are many ways you can go about learning how to increase database performance, these six have proven to be some of the most effective and impactful when it comes to avoiding performance degradation."
20,Tip 1: Optimize Queries
20,"In many cases database performance issues are caused by inefficient SQL queries. Optimizing your SQL queries is one of the best ways to increase database performance. When you try to do that manually, you’ll encounter several dilemmas around choosing how best to improve query efficiency. These include understanding whether to write a join or a subquery, whether to use EXISTS or IN, and more. When you know the best path forward, you can write queries that improve efficiency and thus database performance as a whole. That means fewer bottlenecks and fewer unhappy end users."
20,The best way to optimize queries is to use a database performance analysis solution that can guide your optimization efforts by directing you to the most inefficient queries and offering expert advice on how best to improve them.
20,Tip 2: Improve Indexes
20,"In addition to queries, the other essential element of the database is the index. When done right, indexing can increase your database performance and help optimize the duration of your query execution. Indexing creates a data structure that helps keep all your data organized and makes it easier to locate information. Because it’s easier to find data, indexing increases the efficiency of data retrieval and speeds up the entire process, saving both you and the system time and effort."
20,Tip 3: Defragment Data
20,"Data defragmentation is one of the best approaches to increasing database performance. Over time, with so much data constantly being written to and deleted from your database, your data can become fragmented. That fragmentation can slow down the data retrieval process as it interferes with a query’s ability to quickly locate the information it’s looking for. When you defragment data, you allow for relevant data to be grouped together and you erase index page issues. That means your I/O related operations will run faster."
20,Tip 4: Increase Memory
20,"The efficiency of your database can suffer significantly when you don’t have enough memory available for the database to work correctly. Even if it seems like you have a lot of memory in total, you might not be meeting the demands of your database. A good way to figure out if you need more memory is to check how many page faults your system has. When the number of faults is high, it means your hosts are either running low on or completely out of available memory. Increasing your memory allocation will help boost efficiency and overall performance."
20,Tip 5: Strengthen CPU
20,"A better CPU translates directly into a more efficient database. That’s why you should consider upgrading to a higher-class CPU unit if you’re experiencing issues with your database performance. The more powerful your CPU is, the less strain it’ll have when dealing with multiple requests and applications. When assessing your CPU, you should keep track of all the elements of CPU performance, including CPU ready times, which tell you about the times your system tried to use the CPU, but couldn’t because the resources were otherwise occupied."
20,Tip 6: Review Access
20,"Once you know your database hardware is working well, you need to review your database access, including which applications are actually accessing your database. If one of your services or applications is suffering from poor database performance, it’s important not to jump to conclusions about which service or application is responsible for the issue. It’s possible a single client is experiencing the bad performance, but it’s also possible the database as a whole is having issues. Dig into who and what is accessing the database and if it’s only one service that’s having an issue, drill down into its metrics to try and find the root cause."
20,"While these tips can help you increase database performance, manual efforts can only do so much. If you want to save time and energy while strengthening your performance optimization efforts, you should invest in a database performance analysis and monitoring solution. The following tools are the best ones on the market when it comes to increasing database performance."
20,Tools That Can Help You
20,1. SolarWinds Database Performance Analyzer (DPA)
20,"SolarWinds DPA is a powerful multidimensional database performance solution that supports PostgreSQL, MySQL, IBM DB2, Amazon Aurora, SAP ASE, Oracle, Microsoft SQL Server, MariaDB, and Azure SQL databases. It focuses on bottleneck identification and offers automated index and query optimization advisors to help you target your database performance optimization efforts."
20,The solution uses response time analysis — which measures the actual amount of time it takes to complete an operation — to tune queries and improve overall database performance. DPA gives users the insights they need to align their resource provisioning with database performance to help ensure hardware isn’t interfering with performance.
20,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
20,"Working in a database environment often requires database administrators or developers (DevOp) to observe the behavior of the entire environment in which their databases and applications run. DPA easily integrates the process of monitoring databases, hardware, and other entities used by applications and databases."
20,DPA can also be integrated with SolarWinds Storage Resource Monitor (SRM) to get a deeper understanding of your database performance. When DPA and SRM are integrated you can get contextually relevant information on the storage objects related to the databases being monitored by DPA. You can then correlate storage performance with the relevant databases.
20,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
20,"You can try a 14-day free trial, during which DPA is fully functional."
20,Learn more about DPA
20,Download free trial
20,2. SolarWinds® SQL Sentry
20,"SQL Sentry is built to help you quickly identify long-running and high-impact queries, so you can resolve performance problems and quickly increase database performance."
20,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
20,Of note is how SQL Sentry can help you more easily perform the following:
20,Identifying top resource-consuming queries over time with the ability to tune them in the same interface
20,Defining alerting and automated remediation for known performance problems
20,Knowing when a new release or data churn over time has changed the performance profile for one or more queries
20,You can try SQL Sentry free for 14 days.
20,3. SolarWinds Database Performance Monitor (DPM)
20,"Database Performance Monitor is another great solution from SolarWinds. This software puts a bigger focus on monitoring database performance as compared to DPA. DPM offers a convenient and efficient means of monitoring your database performance in real time, around the clock. Check-ups on performance are simple and DPM gives you access to expert guidance that can help you improve database performance. The tool sends you instant recommendations around underperforming queries or server configuration changes. What is important is the way DPM works—it’s most often used to monitor no-SQL databases. This monitoring tool for databases such as Redis, MongoDB, and Azure can work locally and in cloud and hybrid environments. As it works in SaaS model, it allows access to the user dashboard in a web-based user interface."
20,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
20,"DPM helps you understand how your queries are impacting performance and gets to the bottom of any problems the tool detects, thanks to extensive correlated data. Try DPM free for 14 days."
20,4. SolarWinds Server Configuration Monitor (SCM)
20,"Another good solution to consider if your business runs on SQL databases is SolarWinds SCM, which runs SQL queries to connect and monitor any relational databases in your system including MySQL, Microsoft SQL Server, PostgreSQL, or Oracle. SCM collects data through queries then saves it and monitors it for changes. This can help you stay on top of any database configuration changes — including changes to user permissions and schemas — that could negatively impact database performance."
20,"© 2023 SolarWinds Worldwide, LLC. All rights reserved."
20,SCM comes with a free 30-day trial.
20,5. Paessler PRTG Network Monitor
20,"Paessler PRTG Network Monitor is another great tool if you’re looking to increase your database performance. Like DPM, it puts an emphasis on database monitoring. In fact, one of the benefits of the tool is it’s a one-stop shop for infrastructure monitoring covering databases and applications, bandwidth, packets, traffic, cloud services, uptime, ports, IPs, virtual environments, hardware, web services, security, physical environments, disk usage, and IoT devices. The tool is easy to set up and highly customizable, making it a good fit for anyone looking for a simple solution to their database and infrastructure monitoring needs."
20,© 2023 PAESSLER AG. All rights reserved.
20,A free version of PRTG is available for 30 days.
20,Increasing Database Performance Through Monitoring and Analysis
20,"If you’re looking for the best way to increase your database performance, there’s no better option than using professional software."
20,"I compared ease of use, integration with other programs, technical support during installation, and during use of the tools. This allowed me to select and recommend SolarWinds products to database administrators looking for a scalable solution for their daily work in database analysis and monitoring. Although there many monitoring and analysis products on the market, SolarWinds DPA, SQL Sentry, and SCM are a good place to start. The tools cost starts at USD$1,111.* You can check how it works with a 14 days trial version (available on official SolarWinds website: click here)."
20,Related Posts
20,10 MySQL Database Performance Tuning Tips
20,Oracle 12c Performance Tuning — Top Tips for Database Admins
20,Top Db2 Performance Tuning Tips
20,Categories Systems
20,Post navigation
20,MySQL vs. MSSQL – Performance and Main DifferencesCommon Database Problems and Performance Issues
20,Most Popular Posts
20,Best Remote Desktop Connection Manager Tools
20,Ultimate Guide to Windows Event Logs for 2024
20,What Is Syslog? Syslog Server vs. Event Log Explained + Recommended Syslog Management Tool
20,5 Best Tripwire Alternatives 2024
20,"Jitter, Packet Loss, and Latency in Network Performance"
20,Best Syslog Servers in 2024
20,13 Best Service Request Management Software of 2024
20,Top 8 Observability Tools
20,10 Best IT Self-Service Software in 2024
20,8 Best Service-Level Management Tools for 2024
20,Languages
20,English
20,Deutsch
20,Français
20,"© 2024 SolarWinds Worldwide, LLC. All rights reserved."
20,About Us |
20,Trademarks |
20,Privacy Policy |
20,Terms Of Use
20,Scroll back to top
21,Comparison between equivalent Intel & Graviton instances for MariaDB & PostgreSQL on Amazon RDS
21,Search
21,Browse
21,Community
21,About Community
21,Private Forums
21,Private Forums
21,Intel oneAPI Toolkits Private Forums
21,All other private forums and groups
21,Intel AI Software - Private Forums
21,GEH Pilot Community Sandbox
21,Intel® Connectivity Research Program (Private)
21,Intel-Habana Gaudi Technology Forum
21,Developer Software Forums
21,Developer Software Forums
21,Toolkits & SDKs
21,Software Development Tools
21,Software Development Topics
21,Software Development Technologies
21,Intel® DevCloud
21,Intel® Developer Cloud
21,"oneAPI Registration, Download, Licensing and Installation"
21,GPU Compute Software
21,Software Archive
21,Edge Developer Toolbox
21,Product Support Forums
21,Product Support Forums
21,Memory & Storage
21,Embedded Products
21,Visual Computing
21,FPGA
21,Graphics
21,Processors
21,Wireless
21,Ethernet Products
21,Server Products
21,Intel® Enpirion® Power Solutions
21,Intel Unite® App
21,Intel vPro® Platform
21,Intel® Trusted Execution Technology (Intel® TXT)
21,Intel® Unison™ App
21,Intel® QuickAssist Technology (Intel® QAT)
21,Gaming Forums
21,Gaming Forums
21,Intel® ARC™ Graphics
21,Gaming on Intel® Processors with Intel® Graphics
21,Developing Games on Intel Graphics
21,Blogs
21,Blogs
21,@Intel
21,Products and Solutions
21,Tech Innovation
21,Thought Leadership
21,Cloud
21,Examine critical components of Cloud computing with Intel® software experts
21,Success!
21,Subscription added.
21,Success!
21,Subscription removed.
21,"Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your"
21,profile.
21,Intel Community
21,Blogs
21,Tech Innovation
21,Cloud
21,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
21,107 Discussions
21,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
21,Subscribe
21,Article Options
21,Subscribe to RSS Feed
21,Mark as New
21,Mark as Read
21,Bookmark
21,Subscribe
21,Printer Friendly Page
21,Report Inappropriate Content
21,Mohan_Potheri
21,Employee
21,‎03-31-2023
21,08:00 AM
21,"9,985"
21,Introduction:
21,"Databases are typically the crown jewel of enterprise applications. All workloads including web-based e-commerce, social media, cloud services are typically backed by a database. Open-source databases[i] have become completely mainstream over the past decade and are the primary leaders in innovation in the database space. Open-source software has many attributes that make them successful in this cloud era. One of the major benefits is that developers can use open-source software and databases, without any licensing fees. Open-source software as developers code in features that they need quickly and contribute it back to the community. Open-source projects are therefore more agile and have out-evolved closed source alternatives since the early 2000s."
21,"AWS is positioning Graviton (an ARM based processor) aggressively from a price perspective compared to Intel 3rd generation Xeon Scalable Processors based instances.[ii] They are using cost savings as the primary mechanism to lure customers away from Intel based instances. Re-platforming is needed for customers moving to Graviton, which requires enterprise re-certification of the software with associated porting cost.  Intel Xeon leads across most popular database, web, and throughput related workloads. The cloud ecosystem for Intel has developed over the past 15 years, whereas ARM is relatively new and untested. Customers can potentially experience cloud vendor lock-in as Graviton is unique to AWS."
21,"The critical nature of open-source databases in the cloud makes them a good workload to compare Amazon EC2 Intel 3rd generation Xeon Scalable and Graviton instances. MariaDB[iii]  is an open-source variant of MySQL that offers a consistent set of advanced features and functionality across all major cloud platforms. PostgreSQL is one of the most powerful open-source databases known for its proven architecture, reliability, data integrity, robust feature set and extensibility. We will use MariaDB and PostgreSQL as the two open-source relational databases used in comparison testing on AWS EC2 between Intel 3rd generation Xeon Scalable processors and Amazon Graviton."
21,Amazon RDS:
21,"The Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.  Amazon RDS is used in modern applications for data storage in web and mobile applications. Customers move to managed databases from RDS to avoid having to manage their own databases. Many customers want to leverage open-source databases in the public cloud and break free from legacy databases"
21,MariaDB and PostgreSQL are popular open-source relational databases used for cloud-based applications. We will be deploying identically sized RDS instances for these two databases on Intel 3rd generation Xeon Scalable and Graviton based instances and running the commonly used Sysbench workload to compare their relative performance.
21,Instance Configuration:
21,The details about the Amazon EC2 instance choices that were made with Intel and Graviton instances are shown in Table 1.
21,Category
21,Attribute
21,Config1
21,Config2
21,Run Info
21,Testing Date
21,"Nov 3-11, 2022"
21,"Nov 3-11, 2022"
21,Cloud
21,AWS
21,AWS
21,Instance Type and CPU
21,Instance Type
21,db.r6g.4xlarge or
21,db.r6i.4xlarge
21,db.r6g.8xlarge or db.r6i.8xlarge
21,CPU(s)
21,Memory
21,128GB
21,256GB
21,Network BW / Instance
21,12.5 Gbps
21,25 Gbps
21,Storage: Direct attached
21,SSD GP2
21,SSD GP2
21,Drive Summary
21,1 volume 75GB
21,1 volume 75GB
21,Table 1: Instance configuration details for the testing
21,Workload Configuration:
21,Details about the workload and its attributes are shown in Table 2. Sysbench 1.0.18 was run 4 times per configuration and the results were then averaged for both MariaDB and PostgreSQL.
21,Category
21,Attribute
21,Config1
21,Config2
21,Run Info
21,Benchmark
21,sysbench 1.0.18
21,sysbench 1.0.18
21,Dates
21,"Nov 3-11, 2022"
21,"Nov 3-11, 2022"
21,CPUs
21,Thread(s) per Core
21,"1,2,4"
21,"0.5,1,2"
21,Core(s)
21,CPU Models
21,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
21,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
21,BIOS
21,Workload Specific Details
21,Workload
21,MariaDB 10.6.10
21,PostgreSQL 14.4-R1
21,Command Line
21,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --mysql-user=sbtest --mysql-password=password --mysql-host=mariadb-r6g-4xl-v1.couqinukves2.us-east-1.rds.amazonaws.com  --db-driver=mysql --mysql-db=sbtest run
21,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --pgsql-user=sbtest --pgsql-password=password --pgsql-host=pg-r6i-16xl-v1.couqinukves2.us-east-1.rds.amazonaws.com --pgsql-port=5432 --db-driver=pgsql --pgsql-db=sbtest run
21,Table 2: Workload configuration details for the testing
21,MariaDB Results:
21,The results from the sysbench testing for MariaDB are shown in Table 3. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
21,Queries per second
21,Threads
21,r6g.4xlarge (Graviton)
21,r6i.4xlarge (Intel)
21,Abs Diff
21,Percentage
21,Difference (qps)
21,45420.95
21,55450.305
21,10029.355
21,22%
21,82310.385
21,103531.905
21,21221.52
21,26%
21,138068.393
21,161312.673
21,23244.28
21,17%
21,Threads
21,r6g.8xlarge (Graviton)
21,r6i.8xlarge (Intel)
21,Abs Diff
21,Percentage
21,Difference (qps)
21,102649.813
21,127593.153
21,24943.34
21,24%
21,169209.105
21,216709.785
21,47500.68
21,28%
21,250122.328
21,302356.915
21,52234.5875
21,21%
21,Table 3: MariaDB QPS comparison between Intel and Graviton Instances.
21,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 20-30% for MariaDB.
21,Figure 1: Graphical comparison of sysbench performance for MariaDB between Intel and Graviton based instances (Higher is better)
21,PostgreSQL Results:
21,The results from the sysbench testing for PostgreSQL are shown in Table 4. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
21,Queries per second
21,Threads
21,r6g.4xlarge (Graviton)
21,r6i.4xlarge (Intel)
21,Abs Diff
21,Percentage
21,Difference (qps)
21,58117.2825
21,65466.75
21,7349.4675
21,13%
21,99423.5025
21,114673.365
21,15249.8625
21,15%
21,140116.51
21,152913.408
21,12796.8975
21,Queries per second - 32 vCPU (8xlarge)
21,Threads
21,r6g.8xlarge (Graviton)
21,r6i.8xlarge
21,(Intel)
21,Abs Diff
21,Percentage
21,Difference (qps)
21,81133.195
21,125247.73
21,44114.535
21,54%
21,141914.253
21,208751.038
21,66836.785
21,47%
21,219109.908
21,288519.455
21,69409.5475
21,32%
21,Table 4: PostgreSQL QPS comparison between Intel and Graviton Instances.
21,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 30-50% for PostgreSQL.
21,Figure 2: Graphical comparison of sysbench performance for PostgreSQL between Intel and Graviton based instances (Higher is better)
21,Conclusion:
21,"Customers need to be careful with their choice of instances for their workloads in the cloud. Our results show that not all instances are created equal. Intel 3rd generation Xeon Scalable processors-based instances outperform Amazon similar Graviton based instances for open-source relational databases by 20-50% as the results have shown. Intel’s active participation in the open-source community and its innovative HW and SW optimizations work to boost performance of Database workloads as we have shown. By choosing Intel instances and right sizing them based on their performance characteristics in Amazon RDS, lower TCO with optimal performance can be attained."
21,Disclosure text:
21,"Tests were performed in October-November 2022 on AWS in region us-east-1. All configurations used general Purpose SSD gp2 storage. Baseline I/O performance for gp2 storage is 3 IOPS for each GiB, with a minimum of 100 IOPS. This relationship means that larger volumes have better performance. For our experiments we used. 550GiB storage with a baseline performance of 1500 IOPS. We ran the following Database Engines: - MariaDB 10.6.10, - PostgreSQL 14.4-R1. These were run on each of 4 DB server Instances described below. Database server used AWS RDS servers with 4 DB Instance types."
21,db.r6g – memory-optimized instance classes powered by AWS Graviton2 processors
21,"db.r6g.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
21,"db.r6g.4xlarge, 16 vCPU, 128             GB Memory  & Up to 10 Gbps Network interface"
21,db.r6i – memory-optimized instance classes powered by 3rd Generation Intel Xeon Scalable processors
21,"db.r6i.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
21,"db.r6i.4xlarge, 16 vCPU, 128               GB Memory & Up to 10 Gbps Network interface"
21,Pricing URL for MariaDB: https://aws.amazon.com/rds/mariadb/pricing/
21,For PostgreSQL:   https://aws.amazon.com/rds/postgresql/pricing/
21,"DB Client machine details:  For Database client machine, we used the EC2 instance type: c6i.4xlarge with 16vCPU (8 core), with 32 GB Memory, 75 GB GP2 Storage volume  with 12.5GB Network bandwidth powered by 3rd Generation Intel Xeon Scalable processors. The client machines use the following Software Image (AMI) with Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2022-09-14 & ami-0149b2da6ceec4bb0. All DB Instances, as well as the client Instances were run in US-EAST-1 region. Benchmarking Software: We used sysbench tool to load data and to run oltp_read tests on all these configurations. We used sysbench version"
21,1.0.18 (using system LuaJIT 2.1.0-beta3) for all the DB testing.
21,Disclaimer text:
21,"Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation."
21,Bibliography
21,[i] https://www.dbta.com/BigDataQuarterly/Articles/The-Past-Present-and-Future-of-Open-Source-Databases-150954.aspx discusses the present and future of open source databases
21,"[ii] https://www.percona.com/blog/comparing-graviton-arm-performance-to-intel-and-amd-for-mysql-part-3/ compares DB Engines (and clients on the same instances) on M6i.* (Intel) , M6a.* (AMD),  M6g.*(Graviton) EC2 instances."
21,[iii] https://mariadb.com/database-topics/mariadb-vs-mysql/ provides a good comparison of MySQL and MariaDB.
21,Appendix A: DB Configuration Tuning:
21,------------------------------------------------------------------------------------------------------
21,Tuning for MariaDB:
21,We followed this article for performance tuning mariadb:
21,https://mariadb.com/resources/blog/10-database-tuning-tips-for-peak-workloads/
21,The following parameters were tuned.
21,1. InnoDB Buffer Pool Size
21,Making the InnoDB buffer pool size as large as possible ensures you use memory rather than disks for most read operations (because the buffer pool is where data and indexes are cached).
21,LEFT IT UNCHANGED from the RDS default which is {DBInstanceClassMemory*3/4}
21,where
21,DBInstanceClassMemory is a Formula variable with this description:
21,(from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ParamValuesRef.html
21,2. InnoDB Log File Size
21,"The redo logs make sure writes are fast and durable, and  the InnoDB redo space size is important for write-intensive workloads. The logs’ size is determined by innodb_log-file-size. For best results, generally you’ll want to set a combined total size to be at least 1/4 (or even 1/2) of the InnoDB buffer pool size, or equal to one hour’s worth of log entries during peak load. For MariaDB, we set innodb_log_file_size as {DBInstanceClassMemory*(3/4)*(1/4)}:"
21,- We computed and entered the number in a custom parameter group.
21,innodb_log_file_size
21,For 4xl instance this is   25769803776 (24GB of log file size for 128GB of RAM in that instance)
21,For 8xl instance this is   51539607552 (48GB of log file size for 256GB of RAM in that instance)
21,Tuning for PostgreSQL:
21,+-----------------------------------------------------------------------------+
21,CHANGED THIS FOR EVERY DB Instance class before DB Instance creation:
21,+-----------------------------------------------------------------------------+
21,max_wal_size = '96GB'
21,-->Default is 2048 (specified in MB)
21,16xl - 393216
21,8xl - 196608
21,4xl = 98304
21,2xl - 49152
21,+-----------------------------------------------------------------------------+
21,Refer to the blog:
21,https://www.percona.com/blog/2021/01/22/postgresql-on-arm-based-aws-ec2-instances-is-it-any-good/
21,Tags (4)
21,Tags:MariaDBopen sourcePostgreSQLRelational Databases
21,Kudo
21,"You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in."
21,Comment
21,About the Author
21,"Mohan Potheri is a Cloud Solutions Architect with more than 20 years in IT infrastructure, with in depth experience on Cloud architecture. He currently focuses on educating customers and partners on Intel capabilities and optimizations available on Amazon AWS. He is actively engaged with the Intel and AWS Partner communities to develop compelling solutions with Intel and AWS. He is a VMware vExpert (VCDX#98) with extensive knowledge on premises and hybrid cloud. He also has extensive experience with business critical applications such as SAP, Oracle, SQL and Java across UNIX, Linux and Windows environments. Mohan Potheri is an expert on AI/ML, HPC and has been a speaker in multiple conferences such as VMWorld, GTC, ISC and"
21,other Partner events.
21,Community support is provided during standard business hours (Monday to Friday 7AM - 5PM PST). Other contact methods are available here.
21,"Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade."
21,"For more complete information about compiler optimizations, see our Optimization Notice."
21,©Intel Corporation
21,Terms of Use
21,*Trademarks
21,Cookies
21,Privacy
21,Supply Chain Transparency
21,Site Map
23,SLES 15 SP3 | System Analysis and Tuning Guide
23,"Jump to contentdocumentation.suse.com / System Analysis and Tuning Guide On this pageSUSE Linux Enterprise Server 15 SP3System Analysis and Tuning Guide   This guide supports administrators in problem detection,"
23,resolution and optimization. Publication Date:
23,"March 14, 2024"
23,PrefaceAvailable documentationImproving the documentationDocumentation conventionsSupportI Basics1 General notes on system tuning1.1 Be sure what problem to solve1.2 Rule out common problems1.3 Finding the bottleneck1.4 Step-by-step tuningII System monitoring2 System monitoring utilities2.1 Multi-purpose tools2.2 System information2.3 Processes2.4 Memory2.5 Networking2.6 The /proc file system2.7 Hardware information2.8 Files and file systems2.9 User information2.10 Time and date2.11 Graph your data: RRDtool3 System log files3.1 System log files in /var/log/3.2 Viewing and parsing log files3.3 Managing log files with logrotate3.4 Monitoring log files with logwatch3.5 Configuring mail forwarding for root3.6 Forwarding log messages to a central syslog server3.7 Using logger to make system log entriesIII Kernel monitoring4 SystemTap—filtering and analyzing system data4.1 Conceptual overview4.2 Installation and setup4.3 Script syntax4.4 Example script4.5 User space probing4.6 More information5 Kernel probes5.1 Supported architectures5.2 Types of kernel probes5.3 Kprobes API5.4 debugfs Interface5.5 More information6 Hardware-based performance monitoring with Perf6.1 Hardware-based monitoring6.2 Sampling and counting6.3 Installing Perf6.4 Perf subcommands6.5 Counting particular types of event6.6 Recording events specific to particular commands6.7 More information7 OProfile—system-wide profiler7.1 Conceptual overview7.2 Installation and requirements7.3 Available OProfile utilities7.4 Using OProfile7.5 Generating reports7.6 More information8 Dynamic debug—kernel debugging messages8.1 Benefits of dynamic debugging8.2 Checking the status of dynamic debug8.3 Using dynamic debug8.4 Viewing the dynamic debug messagesIV Resource management9 General system resource management9.1 Planning the installation9.2 Disabling unnecessary services9.3 File systems and disk access10 Kernel control groups10.1 Overview10.2 Resource accounting10.3 Setting resource limits10.4 Preventing fork bombs with TasksMax10.5 Controlling I/O with proportional weight policy10.6 More information11 Automatic Non-Uniform Memory Access (NUMA) balancing11.1 Implementation11.2 Configuration11.3 Monitoring11.4 Impact12 Power management12.1 Power management at CPU Level12.2 In-kernel governors12.3 The cpupower tools12.4 Special tuning options12.5 Troubleshooting12.6 More information12.7 Monitoring power consumption with powerTOPV Kernel tuning13 Tuning I/O performance13.1 Switching I/O scheduling13.2 Available I/O elevators with blk-mq I/O path13.3 I/O barrier tuning14 Tuning the task scheduler14.1 Introduction14.2 Process classification14.3 Completely Fair Scheduler14.4 More information15 Tuning the memory management subsystem15.1 Memory usage15.2 Reducing memory usage15.3 Virtual memory manager (VM) tunable parameters15.4 Monitoring VM behavior16 Tuning the network16.1 Configurable kernel socket buffers16.2 Detecting network bottlenecks and analyzing network traffic16.3 Netfilter16.4 Improving the network performance with receive packet steering (RPS)17 Tuning SUSE Linux Enterprise for SAP17.1 Tuning SLE Systems with sapconf 5VI Handling system dumps18 Tracing tools18.1 Tracing system calls with strace18.2 Tracing library calls with ltrace18.3 Debugging and profiling with Valgrind18.4 More information19 Kexec and Kdump19.1 Introduction19.2 Required packages19.3 Kexec internals19.4 Calculating crashkernel allocation size19.5 Basic Kexec usage19.6 How to configure Kexec for routine reboots19.7 Basic Kdump configuration19.8 Analyzing the crash dump19.9 Advanced Kdump configuration19.10 More information20 Using systemd-coredump to debug application crashes20.1 Use and configurationVII Synchronized clocks with Precision Time Protocol21 Precision Time Protocol21.1 Introduction to PTP21.2 Using PTP21.3 Synchronizing the clocks with phc2sys21.4 Examples of configurations21.5 PTP and NTPA GNU licensesA.1 GNU Free Documentation LicenseList of Figures2.1 Example graph created with RRDtool12.1 powerTOP in interactive mode12.2 HTML powerTOP report19.1 YaST Kdump module: start-up pageList of Tables2.1 List of query options of ethtool12.1 C-states13.1 MQ-DEADLINE tunable parameters13.2 BFQ tunable parameters13.3 KYBER tunable parametersList of Examples2.1 vmstat output on a lightly used machine2.2 vmstat output on a heavily used machine (CPU bound)3.1 Example for /etc/logrotate.conf4.1 Simple SystemTap script4.2 Probe with timer event4.3 printf Function with format specifiers4.4 Using global variables4.5 Monitoring incoming TCP connections with tcp_connections.stp12.1 Example output of cpupower frequency-info12.2 Example output of cpupower idle-info12.3 Example cpupower monitor output17.1 Checking Parameters19.1 Kdump: example configuration using a static IP setup21.1 Slave clock using software time stamping21.2 Slave clock using hardware time stamping21.3 Master clock using hardware time stamping21.4 Master clock using software time stamping (not generally recommended)
23,Copyright © 2006–2024
23,SUSE LLC and contributors. All rights reserved.
23,"Permission is granted to copy, distribute and/or modify this document under"
23,"the terms of the GNU Free Documentation License, Version 1.2 or (at your"
23,option) version 1.3; with the Invariant Section being this copyright notice
23,and license. A copy of the license version 1.2 is included in the section
23,entitled “GNU Free Documentation License”.
23,"For SUSE trademarks, see"
23,https://www.suse.com/company/legal/. All
23,third-party trademarks are the property of their respective owners. Trademark
23,"symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates."
23,Asterisks (*) denote third-party trademarks.
23,All information found in this book has been compiled with utmost attention to
23,"detail. However, this does not guarantee complete accuracy. Neither"
23,"SUSE LLC, its affiliates, the authors nor the translators shall be"
23,held liable for possible errors or the consequences thereof.
23,Preface #  1 Available documentation #  Online documentation
23,Our documentation is available online at https://documentation.suse.com.
23,Browse or download the documentation in various formats.
23,Note: Latest updates
23,The latest updates are usually available in the English-language version of this documentation.
23,SUSE Knowledgebase
23,"If you have run into an issue, also check out the Technical Information"
23,Documents (TIDs) that are available online at https://www.suse.com/support/kb/.
23,Search the SUSE Knowledgebase for known solutions driven by customer need.
23,Release notes
23,"For release notes, see"
23,https://www.suse.com/releasenotes/.
23,In your system
23,"For offline use, the release notes are also available under"
23,/usr/share/doc/release-notes on your system.
23,The documentation for individual packages is available at
23,/usr/share/doc/packages.
23,Many commands are also described in their manual
23,"pages. To view them, run man, followed"
23,by a specific command name. If the man command is
23,"not installed on your system, install it with sudo zypper"
23,install man.
23,2 Improving the documentation #
23,Your feedback and contributions to this documentation are welcome.
23,The following channels for giving feedback are available:
23,Service requests and support
23,"For services and support options available for your product, see"
23,https://www.suse.com/support/.
23,"To open a service request, you need a SUSE subscription registered at"
23,SUSE Customer Center.
23,"Go to https://scc.suse.com/support/requests, log"
23,"in, and click Create New."
23,Bug reports
23,Report issues with the documentation at https://bugzilla.suse.com/.
23,"To simplify this process, click the Report"
23,an issue icon next to a headline in the HTML
23,version of this document. This preselects the right product and
23,category in Bugzilla and adds a link to the current section.
23,You can start typing your bug report right away.
23,A Bugzilla account is required.
23,Contributions
23,"To contribute to this documentation, click the Edit source"
23,document icon next to a headline in the HTML version of
23,"this document. This will take you to the source code on GitHub, where you"
23,can open a pull request.
23,A GitHub account is required.
23,Note: Edit source document only available for English
23,The Edit source document icons are only available for the
23,"English version of each document. For all other languages, use the"
23,Report an issue icons instead.
23,For more information about the documentation environment used for this
23,"documentation, see the repository's README."
23,Mail
23,You can also report errors and send feedback concerning the
23,documentation to <doc-team@suse.com>. Include the
23,"document title, the product version, and the publication date of the"
23,"document. Additionally, include the relevant section number and title (or"
23,provide the URL) and provide a concise description of the problem.
23,3 Documentation conventions #
23,The following notices and typographic conventions are used in this
23,document:
23,/etc/passwd: Directory names and file names
23,PLACEHOLDER: Replace
23,PLACEHOLDER with the actual value
23,PATH: An environment variable
23,"ls, --help: Commands, options, and"
23,parameters
23,user: The name of a user or group
23,package_name: The name of a software package
23,"Alt, Alt–F1: A key to press or a key combination. Keys"
23,are shown in uppercase as on a keyboard.
23,"File, File › Save"
23,"As: menu items, buttons"
23,AMD/Intel
23,This paragraph is only relevant for the AMD64/Intel 64 architectures. The
23,arrows mark the beginning and the end of the text block.
23,"IBM Z, POWER"
23,This paragraph is only relevant for the architectures
23,IBM Z and POWER. The arrows
23,mark the beginning and the end of the text block.
23,"Chapter 1, “Example chapter”:"
23,A cross-reference to another chapter in this guide.
23,Commands that must be run with root privileges. You can also
23,prefix these commands with the sudo command to run them
23,as a non-privileged user:
23,# command
23,> sudo command
23,Commands that can be run by non-privileged users:
23,> command
23,Commands can be split into two or multiple lines by a backslash character
23,(\) at the end of a line. The backslash informs the shell that
23,the command invocation will continue after the line's end:
23,> echo a b \
23,c d
23,A code block that shows both the command (preceded by a prompt)
23,and the respective output returned by the shell:
23,> command
23,output
23,Notices
23,Warning: Warning notice
23,Vital information you must be aware of before proceeding. Warns you about
23,"security issues, potential loss of data, damage to hardware, or physical"
23,hazards.
23,Important: Important notice
23,Important information you should be aware of before proceeding.
23,Note: Note notice
23,"Additional information, for example about differences in software"
23,versions.
23,Tip: Tip notice
23,"Helpful information, like a guideline or a piece of practical advice."
23,Compact Notices
23,"Additional information, for example about differences in software"
23,versions.
23,"Helpful information, like a guideline or a piece of practical advice."
23,4 Support #
23,Find the support statement for SUSE Linux Enterprise Server and general information about
23,technology previews below.
23,"For details about the product lifecycle, see"
23,https://www.suse.com/lifecycle.
23,"If you are entitled to support, find details on how to collect information"
23,for a support ticket at
23,https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html.
23,4.1 Support statement for SUSE Linux Enterprise Server #
23,"To receive support, you need an appropriate subscription with SUSE."
23,"To view the specific support offers available to you, go to"
23,https://www.suse.com/support/ and select your product.
23,The support levels are defined as follows:
23,"Problem determination, which means technical support designed to provide"
23,"compatibility information, usage support, ongoing maintenance,"
23,information gathering and basic troubleshooting using available
23,documentation.
23,"Problem isolation, which means technical support designed to analyze"
23,"data, reproduce customer problems, isolate a problem area and provide a"
23,resolution for problems not resolved by Level 1 or prepare for
23,Level 3.
23,"Problem resolution, which means technical support designed to resolve"
23,problems by engaging engineering to resolve product defects which have
23,been identified by Level 2 Support.
23,"For contracted customers and partners, SUSE Linux Enterprise Server is delivered with L3"
23,"support for all packages, except for the following:"
23,Technology previews.
23,"Sound, graphics, fonts, and artwork."
23,Packages that require an additional customer contract.
23,Some packages shipped as part of the module Workstation
23,Extension are L2-supported only.
23,Packages with names ending in -devel (containing header
23,files and similar developer resources) will only be supported together
23,with their main packages.
23,SUSE will only support the usage of original packages.
23,"That is, packages that are unchanged and not recompiled."
23,4.2 Technology previews #
23,"Technology previews are packages, stacks, or features delivered by SUSE"
23,to provide glimpses into upcoming innovations.
23,Technology previews are included for your convenience to give you a chance
23,to test new technologies within your environment.
23,We would appreciate your feedback.
23,"If you test a technology preview, please contact your SUSE representative"
23,and let them know about your experience and use cases.
23,Your input is helpful for future development.
23,Technology previews have the following limitations:
23,Technology previews are still in development.
23,"Therefore, they may be functionally incomplete, unstable, or otherwise"
23,not suitable for production use.
23,Technology previews are not supported.
23,Technology previews may only be available for specific hardware
23,architectures.
23,Details and functionality of technology previews are subject to change.
23,"As a result, upgrading to subsequent releases of a technology preview may"
23,be impossible and require a fresh installation.
23,"SUSE may discover that a preview does not meet customer or market needs,"
23,or does not comply with enterprise standards.
23,Technology previews can be removed from a product at any time.
23,SUSE does not commit to providing a supported version of such
23,technologies in the future.
23,"For an overview of technology previews shipped with your product, see the"
23,release notes at https://www.suse.com/releasenotes.
23,Part I Basics #  1 General notes on system tuning
23,This manual discusses how to find the reasons for performance problems
23,and provides means to solve these problems. Before you start tuning your
23,"system, you should make sure you have ruled out common problems and have"
23,found the cause for the problem. You should also have a detailed plan on
23,"how to tune the system, because applying random tuning tips often will"
23,not help and could make things worse.
23,1 General notes on system tuning #
23,This manual discusses how to find the reasons for performance problems
23,and provides means to solve these problems. Before you start tuning your
23,"system, you should make sure you have ruled out common problems and have"
23,found the cause for the problem. You should also have a detailed plan on
23,"how to tune the system, because applying random tuning tips often will"
23,not help and could make things worse.
23,Procedure 1.1: General approach when tuning a system #
23,Specify the problem that needs to be solved.
23,"In case the degradation is new, identify any recent changes to the"
23,system.
23,Identify why the issue is considered a performance problem.
23,Specify a metric that can be used to analyze performance. This metric
23,"could for example be latency, throughput, the maximum number of"
23,"users that are simultaneously logged in, or the maximum number of active users."
23,Measure current performance using the metric from the previous step.
23,Identify the subsystem(s) where the application is spending the most
23,time.
23,Monitor the system and/or the application.
23,"Analyze the data, categorize where time is being spent."
23,Tune the subsystem identified in the previous step.
23,Remeasure the current performance without monitoring using the same
23,metric as before.
23,"If performance is still not acceptable, start over with"
23,Step 3.
23,1.1 Be sure what problem to solve #
23,"Before starting to tuning a system, try to describe the problem as"
23,exactly as possible. A statement like “The system is slow!”
23,"is not a helpful problem description. For example, it could make a"
23,difference whether the system speed needs to be improved in general or
23,only at peak times.
23,"Furthermore, make sure you can apply a measurement to your problem,"
23,otherwise you cannot verify if the tuning was a success or
23,not. You should always be able to compare “before” and
23,“after”. Which metrics to use depends on the scenario or
23,"application you are looking into. Relevant Web server metrics, for"
23,"example, could be expressed in terms of:"
23,Latency
23,The time to deliver a page
23,Throughput
23,Number of pages served per second or megabytes transferred per second
23,Active users
23,The maximum number of users that can be downloading pages while still
23,receiving pages within an acceptable latency
23,1.2 Rule out common problems #
23,"A performance problem often is caused by network or hardware problems,"
23,"bugs, or configuration issues. Make sure to rule out problems such as the"
23,ones listed below before attempting to tune your system:
23,Check the output of the systemd journal (see
23,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”) for unusual entries."
23,Check (using top or ps) whether a
23,certain process misbehaves by eating up unusual amounts of CPU time or
23,memory.
23,Check for network problems by inspecting
23,/proc/net/dev.
23,"In case of I/O problems with physical disks, make sure it is not caused"
23,by hardware problems (check the disk with the
23,smartmontools) or by a full disk.
23,Ensure that background jobs are scheduled to be carried out in times
23,the server load is low. Those jobs should also run with low priority
23,(set via nice).
23,"If the machine runs several services using the same resources, consider"
23,moving services to another server.
23,"Last, make sure your software is up-to-date."
23,1.3 Finding the bottleneck #
23,Finding the bottleneck very often is the hardest part when tuning a
23,system. SUSE Linux Enterprise Server offers many tools to help you with this task.
23,"See Part II, “System monitoring” for detailed information on"
23,general system monitoring applications and log file analysis. If the
23,"problem requires a long-time in-depth analysis, the Linux kernel offers"
23,means to perform such analysis. See
23,"Part III, “Kernel monitoring” for coverage."
23,"Once you have collected the data, it needs to be analyzed. First, inspect"
23,"if the server's hardware (memory, CPU, bus) and its I/O capacities (disk,"
23,"network) are sufficient. If these basic conditions are met, the system"
23,might benefit from tuning.
23,1.4 Step-by-step tuning #
23,Make sure to carefully plan the tuning itself. It is of vital importance
23,to only do one step at a time. Only by doing so can you
23,measure whether the change made an improvement or even had a negative
23,impact. Each tuning activity should be measured over a sufficient time
23,period to ensure you can do an analysis based on significant
23,"data. If you cannot measure a positive effect, do not make the change"
23,"permanent. Chances are, that it might have a negative effect in the"
23,future.
23,Part II System monitoring #  2 System monitoring utilities
23,"There are number of programs, tools, and utilities which you can use to"
23,examine the status of your system. This chapter introduces some
23,and describes their most important and frequently used parameters.
23,"3 System log filesSystem log file analysis is one of the most important tasks when analyzing the system. In fact, looking at the system log files should be the first thing to do when maintaining or troubleshooting a system. SUSE Linux Enterprise Server automatically logs almost everything that happens on the system i…2 System monitoring utilities #"
23,"There are number of programs, tools, and utilities which you can use to"
23,examine the status of your system. This chapter introduces some
23,and describes their most important and frequently used parameters.
23,Note:
23,Gathering and Analyzing System Information with
23,supportconfig
23,"Apart from the utilities presented in the following, SUSE Linux Enterprise Server"
23,"also contains supportconfig, a tool to create reports"
23,"about the system such as: current kernel version, hardware, installed"
23,"packages, partition setup and much more. These reports are used to"
23,provide the SUSE support with needed information in case a support
23,"ticket is created. However, they can also be analyzed for known issues to"
23,"help resolve problems faster. For this purpose, SUSE Linux Enterprise Server provides"
23,both an appliance and a command line tool for Supportconfig Analysis
23,"(SCA). See Book “Administration Guide”, Chapter 39 “Gathering system information for support” for details."
23,"For each of the described commands, examples of the relevant outputs are"
23,"presented. In the examples, the first line is the command itself (after"
23,the tux > or root #). Omissions are indicated with
23,square brackets ([...]) and long lines are wrapped
23,where necessary. Line breaks for long lines are indicated by a backslash
23,(\).
23,> command -x -y
23,output line 1
23,output line 2
23,"output line 3 is annoyingly long, so long that \"
23,we need to break it
23,output line 4
23,[...]
23,output line 98
23,output line 99
23,The descriptions have been kept short so that we can include as many
23,utilities as possible. Further information for all the commands can be
23,found in the manual pages. Most of the commands also understand the
23,"parameter --help, which produces a brief list of possible"
23,parameters.
23,2.1 Multi-purpose tools #
23,While most Linux system monitoring tools monitor only a single aspect of
23,"the system, there are a few tools with a broader scope. To get"
23,"an overview and find out which part of the system to examine further, use"
23,these tools first.
23,2.1.1 vmstat #
23,"vmstat collects information about processes, memory, I/O, interrupts and"
23,CPU:
23,vmstat [options] [delay [count]]
23,"When called without values for delay and count, it displays average values"
23,"since the last reboot. When called with a value for delay (in seconds), it"
23,displays values for the given period (two seconds in the examples
23,below). The value for count specifies the number of updates vmstat should
23,"perform. If not specified, it will run until manually stopped."
23,Example 2.1: vmstat output on a lightly used machine #  > vmstat 2
23,procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
23,swpd
23,free
23,buff
23,cache
23,cs us sy id wa st
23,44264
23,81520
23,424 935736
23,0 98
23,44264
23,81552
23,424 935736
23,0 100
23,44264
23,81520
23,424 935732
23,0 100
23,44264
23,81520
23,424 935732
23,0 100
23,44264
23,81552
23,424 935732
23,0 100
23,0Example 2.2: vmstat output on a heavily used machine (CPU bound) #  > vmstat 2
23,procs -----------memory----------- ---swap-- -----io---- -system-- -----cpu------
23,swpd
23,free
23,buff
23,cache
23,cs us sy id wa st
23,26236 459640 110240 6312648
23,9944
23,2 4552 6597 95
23,26236 396728 110336 6136224
23,9588
23,0 4468 6273 94
23,26236 554920 110508 6166508
23,7684 27992 4474 4700 95
23,26236 518184 110516 6039996
23,0 10830
23,4 4446 4670 94
23,26236 716468 110684 6074872
23,8734 20534 4512 4061 96
23,0Tip: First line of output
23,The first line of the vmstat output always displays average values
23,since the last reboot.
23,The columns show the following:
23,Shows the number of processes in a runnable state. These processes
23,are either executing or waiting for a free CPU slot. If the number
23,of processes in this column is constantly higher than the number of
23,"CPUs available, this may be an indication of insufficient CPU power."
23,Shows the number of processes waiting for a resource other than a
23,CPU. A high number in this column may indicate an I/O problem
23,(network or disk).
23,swpd
23,The amount of swap space (KB) currently used.
23,free
23,The amount of unused memory (KB).
23,inact
23,Recently unused memory that can be reclaimed. This column is only
23,visible when calling vmstat with the parameter
23,-a (recommended).
23,active
23,Recently used memory that normally does not get reclaimed. This
23,column is only visible when calling vmstat with
23,the parameter -a (recommended).
23,buff
23,File buffer cache (KB) in RAM that contains file system metadata. This
23,column is not visible when calling vmstat with
23,the parameter -a.
23,cache
23,Page cache (KB) in RAM with the actual contents of files. This
23,column is not visible when calling vmstat with
23,the parameter -a.
23,si / so
23,Amount of data (KB) that is moved from swap to RAM
23,(si) or from RAM to swap (so)
23,per second. High so values over a long period of
23,time may indicate that an application is leaking memory and the
23,leaked memory is being swapped out. High si values
23,over a long period of time could mean that an application that was
23,inactive for a very long time is now active again. Combined high
23,si and so values for prolonged
23,periods of time are evidence of swap thrashing and may indicate that
23,more RAM needs to be installed in the system because there is not
23,enough memory to hold the working set size.
23,Number of blocks per second received from a block device (for
23,"example, a disk read). Note that swapping also impacts the values"
23,shown here. The block size may vary between file systems but can
23,be determined using the stat utility. If throughput data is
23,required then iostat may be used.
23,"Number of blocks per second sent to a block device (for example, a"
23,disk write). Note that swapping also impacts the values shown here.
23,Interrupts per second. A high value may indicate a high I/O level
23,"(network and/or disk), but could also be triggered for other reasons"
23,such as inter-processor interrupts triggered by another activity.
23,Make sure to also check /proc/interrupts to
23,identify the source of interrupts.
23,Number of context switches per second. This is the number of times
23,that the kernel replaces executable code of one program in memory
23,with that of another program.
23,Percentage of CPU usage executing application code.
23,Percentage of CPU usage executing kernel code.
23,Percentage of CPU time spent idling. If this value is zero over a
23,"longer time, your CPU(s) are working to full capacity. This"
23,is not necessarily a bad sign—rather refer to the values in
23,columns r and b to determine if
23,your machine is equipped with sufficient CPU power.
23,"If ""wa"" time is non-zero, it indicates throughput lost because of"
23,"waiting for I/O. This may be inevitable, for example, if a file is"
23,"being read for the first time, background writeback cannot keep up,"
23,and so on. It can also be an indicator for a hardware bottleneck
23,"(network or hard disk). Lastly, it can indicate a potential for"
23,tuning the virtual memory manager (refer to
23,"Chapter 15, Tuning the memory management subsystem)."
23,Percentage of CPU time stolen from a virtual machine.
23,See vmstat --help for more options.
23,2.1.2 dstat #
23,dstat is a replacement for tools such as
23,"vmstat, iostat,"
23,"netstat, or ifstat."
23,dstat displays information about the system
23,"resources in real time. For example, you can compare disk usage"
23,"in combination with interrupts from the IDE controller, or compare"
23,network bandwidth with the disk throughput (in the same interval).
23,"By default, its output is presented in readable tables."
23,"Alternatively, CSV output can be produced which is suitable as a"
23,spreadsheet import format.
23,It is written in Python and can be enhanced with plug-ins.
23,This is the general syntax:
23,dstat [-afv] [OPTIONS..] [DELAY [COUNT]]
23,All options and parameters are optional.
23,"Without any parameter, dstat"
23,"displays statistics about CPU (-c,"
23,"--cpu), disk (-d,"
23,"--disk), network (-n,"
23,"--net), paging (-g,"
23,"--page), and the interrupts and context switches of"
23,"the system (-y, --sys); it refreshes"
23,the output every second ad infinitum:
23,# dstat
23,"You did not select any stats, using -cdngy by default."
23,----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--
23,usr sys idl wai hiq siq| read
23,writ| recv
23,send|
23,out | int
23,csw
23,0 100
23,15k
23,44k|
23,0 |
23,82B| 148
23,194
23,0 100
23,0 |5430B
23,170B|
23,0 | 163
23,187
23,0 100
23,0 |6363B
23,842B|
23,0 | 196
23,"185-a, --all"
23,equal to -cdngy (default)
23,"-f, --full"
23,"expand -C, -D,"
23,"-I, -N and -S"
23,discovery lists
23,"-v, --vmstat"
23,"equal to -pmgdsc, -D total"
23,DELAY
23,delay in seconds between each update
23,COUNT
23,the number of updates to display before exiting
23,The default delay is 1 and the count is unspecified (unlimited).
23,"For more information, see the man page of dstat and"
23,its Web page at http://dag.wieers.com/home-made/dstat/.
23,2.1.3 System activity information: sar #
23,sar can generate extensive reports on almost all
23,"important system activities, among them CPU, memory, IRQ usage, I/O, and"
23,networking. It can also generate reports in real time.
23,The sar command gathers data from the
23,/proc file system.
23,Note: sysstat package
23,The sar command is a part of the
23,"sysstat package. Install it with YaST, or with"
23,"the zypper in sysstat command. sysstat.service does not start by default,"
23,and must be enabled and started with the following command:
23,> sudo systemctl enable --now sysstat2.1.3.1 Generating reports with sar #
23,"To generate reports in real time, call sar with an"
23,interval (seconds) and a count. To generate reports from files specify
23,a file name with the option -f instead of interval and
23,"count. If file name, interval and count are not specified,"
23,sar attempts to generate a report from
23,"/var/log/sa/saDD, where"
23,DD stands for the current day. This is the
23,default location to where sadc (the system
23,activity data collector) writes its data.
23,Query multiple files with multiple -f options.
23,sar 2 10
23,"# real time report, 10 times every 2 seconds"
23,sar -f ~/reports/sar_2014_07_17
23,# queries file sar_2014_07_17
23,sar
23,# queries file from today in /var/log/sa/
23,cd /var/log/sa && \
23,sar -f sa01 -f sa02
23,# queries files /var/log/sa/0[12]
23,Find examples for useful sar calls and their
23,interpretation below. For detailed information on the meaning of each
23,"column, refer to the man (1) of"
23,sar.
23,Note: sysstat reporting when the service stops
23,"When the sysstat service is stopped (for example, during"
23,"reboot or shutdown), the tool still collects last-minute statistics by"
23,automatically running the /usr/lib64/sa/sa1 -S ALL 1 1
23,command. The collected binary data is stored in the system activity data
23,file.
23,2.1.3.1.1 CPU usage report: sar #
23,"When called with no options, sar shows a basic"
23,"report about CPU usage. On multi-processor machines, results for all"
23,CPUs are summarized. Use the option -P ALL to also
23,see statistics for individual CPUs.
23,# sar 10 5
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,17:51:29
23,CPU
23,%user
23,%nice
23,%system
23,%iowait
23,%steal
23,%idle
23,17:51:39
23,all
23,"57,93"
23,"0,00"
23,"9,58"
23,"1,01"
23,"0,00"
23,"31,47"
23,17:51:49
23,all
23,"32,71"
23,"0,00"
23,"3,79"
23,"0,05"
23,"0,00"
23,"63,45"
23,17:51:59
23,all
23,"47,23"
23,"0,00"
23,"3,66"
23,"0,00"
23,"0,00"
23,"49,11"
23,17:52:09
23,all
23,"53,33"
23,"0,00"
23,"4,88"
23,"0,05"
23,"0,00"
23,"41,74"
23,17:52:19
23,all
23,"56,98"
23,"0,00"
23,"5,65"
23,"0,10"
23,"0,00"
23,"37,27"
23,Average:
23,all
23,"49,62"
23,"0,00"
23,"5,51"
23,"0,24"
23,"0,00"
23,"44,62"
23,%iowait displays the percentage of time that the
23,CPU was idle while waiting for an I/O request. If this value is
23,"significantly higher than zero over a longer time, there is a"
23,bottleneck in the I/O system (network or hard disk). If the
23,"%idle value is zero over a longer time,"
23,your CPU is working at capacity.
23,2.1.3.1.2 Memory usage report: sar -r #
23,Generate an overall picture of the system memory (RAM) by using the
23,option -r:
23,# sar -r 10 5
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,17:55:27 kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty
23,17:55:37
23,104232
23,1834624
23,94.62
23,627340
23,2677656
23,66.24
23,802052
23,828024
23,1744
23,17:55:47
23,98584
23,1840272
23,94.92
23,624536
23,2693936
23,66.65
23,808872
23,826932
23,2012
23,17:55:57
23,87088
23,1851768
23,95.51
23,605288
23,2706392
23,66.95
23,827260
23,821304
23,1588
23,17:56:07
23,86268
23,1852588
23,95.55
23,599240
23,2739224
23,67.77
23,829764
23,820888
23,3036
23,17:56:17
23,104260
23,1834596
23,94.62
23,599864
23,2730688
23,67.56
23,811284
23,821584
23,3164
23,Average:
23,96086
23,1842770
23,95.04
23,611254
23,2709579
23,67.03
23,815846
23,823746
23,2309
23,The columns kbcommit and %commit
23,show an approximation of the maximum amount of memory (RAM and swap)
23,that the current workload could need. While
23,"kbcommit displays the absolute number in kilobytes,"
23,%commit displays a percentage.
23,2.1.3.1.3 Paging statistics report: sar -B #
23,Use the option -B to display the kernel paging
23,statistics.
23,# sar -B 10 5
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,18:23:01 pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff
23,18:23:11
23,366.80
23,11.60
23,542.50
23,1.10
23,4354.80
23,0.00
23,0.00
23,0.00
23,0.00
23,18:23:21
23,0.00
23,333.30 1522.40
23,0.00 18132.40
23,0.00
23,0.00
23,0.00
23,0.00
23,18:23:31
23,47.20
23,127.40 1048.30
23,0.10 11887.30
23,0.00
23,0.00
23,0.00
23,0.00
23,18:23:41
23,46.40
23,2.50
23,336.10
23,0.10
23,7945.00
23,0.00
23,0.00
23,0.00
23,0.00
23,18:23:51
23,0.00
23,583.70 2037.20
23,0.00 17731.90
23,0.00
23,0.00
23,0.00
23,0.00
23,Average:
23,92.08
23,211.70 1097.30
23,0.26 12010.28
23,0.00
23,0.00
23,0.00
23,0.00
23,The majflt/s (major faults per second) column shows
23,how many pages are loaded from disk into memory. The source of the
23,"faults may be file accesses or faults. At times, many"
23,"major faults are normal. For example, during application start-up"
23,time. If major faults are experienced for the entire lifetime of the
23,application it may be an indication that there is insufficient main
23,"memory, particularly if combined with large amounts of direct scanning"
23,(pgscand/s).
23,The %vmeff column shows the number of pages scanned
23,(pgscand/s) in relation to the ones being reused
23,from the main memory cache or the swap cache
23,(pgsteal/s). It is a measurement of the efficiency
23,of page reclaim. Healthy values are either near 100 (every inactive
23,page swapped out is being reused) or 0 (no pages have been scanned).
23,The value should not drop below 30.
23,2.1.3.1.4 Block device statistics report: sar -d #
23,Use the option -d to display the block device (hard
23,"disk, optical drive, USB storage device, etc.). Make sure to use the"
23,additional option -p (pretty-print) to make the
23,DEV column readable.
23,# sar -d -p 10 5
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,18:46:09 DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,18:46:19 sda
23,1.70
23,33.60
23,0.00
23,19.76
23,0.00
23,0.47
23,0.47
23,0.08
23,18:46:19 sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,18:46:19 DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,18:46:29 sda
23,8.60
23,114.40
23,518.10
23,73.55
23,0.06
23,7.12
23,0.93
23,0.80
23,18:46:29 sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,18:46:29 DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,18:46:39 sda 40.50
23,3800.80
23,454.90
23,105.08
23,0.36
23,8.86
23,0.69
23,2.80
23,18:46:39 sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,18:46:39 DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,18:46:49 sda
23,1.40
23,0.00
23,204.90
23,146.36
23,0.00
23,0.29
23,0.29
23,0.04
23,18:46:49 sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,18:46:49 DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,18:46:59 sda
23,3.30
23,0.00
23,503.80
23,152.67
23,0.03
23,8.12
23,1.70
23,0.56
23,18:46:59 sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,Average: DEV
23,tps rd_sec/s
23,wr_sec/s
23,avgrq-sz
23,avgqu-sz
23,await
23,svctm
23,%util
23,Average: sda 11.10
23,789.76
23,336.34
23,101.45
23,0.09
23,8.07
23,0.77
23,0.86
23,Average: sr0
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,0.00
23,Compare the Average values for
23,"tps, rd_sec/s, and"
23,wr_sec/s of all disks. Constantly high values in
23,the svctm and %util columns
23,could be an indication that I/O subsystem is a bottleneck.
23,"If the machine uses multiple disks, then it is best if I/O is"
23,interleaved evenly between disks of equal speed and capacity. It will
23,be necessary to take into account whether the storage has multiple
23,"tiers. Furthermore, if there are multiple paths to storage then"
23,consider what the link saturation will be when balancing how storage
23,is used.
23,2.1.3.1.5 Network statistics reports: sar -n KEYWORD #
23,The option -n lets you generate multiple network
23,related reports. Specify one of the following keywords along with the
23,-n:
23,DEV: Generates a statistic report for all
23,network devices
23,EDEV: Generates an error statistics report for
23,all network devices
23,NFS: Generates a statistic report for an NFS
23,client
23,NFSD: Generates a statistic report for an NFS
23,server
23,SOCK: Generates a statistic report on sockets
23,ALL: Generates all network statistic reports
23,2.1.3.2 Visualizing sar data #
23,sar reports are not always easy to parse for humans.
23,"kSar, a Java application visualizing your sar data,"
23,creates easy-to-read graphs. It can even generate PDF reports. kSar
23,"takes data generated in real time, and past data from a file. kSar"
23,is licensed under the BSD license and is available from
23,https://sourceforge.net/projects/ksar/.
23,2.2 System information #  2.2.1 Device load information: iostat #
23,"To monitor the system device load, use iostat. It"
23,generates reports that can be useful for better balancing the load
23,between physical disks attached to your system.
23,"To be able to use iostat, install the package"
23,sysstat.
23,The first iostat report shows statistics collected
23,since the system was booted. Subsequent reports cover the time since the
23,previous report.
23,> iostat
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(4 CPU)
23,avg-cpu:
23,%user
23,%nice %system %iowait
23,%steal
23,%idle
23,17.68
23,4.49
23,4.24
23,0.29
23,0.00
23,73.31
23,Device:
23,tps
23,kB_read/s
23,kB_wrtn/s
23,kB_read
23,kB_wrtn
23,sdb
23,2.02
23,36.74
23,45.73
23,3544894
23,4412392
23,sda
23,1.05
23,5.12
23,13.47
23,493753
23,1300276
23,sdc
23,0.02
23,0.14
23,0.00
23,13641
23,Invoking iostat in this way will help you find out
23,"whether throughput is different from your expectation, but not why."
23,Such questions can be better answered by an extended report which can be
23,generated by invoking iostat -x.
23,"Extended reports additionally include, for example, information on average"
23,queue sizes and average wait times.
23,It may also be easier to evaluate the data if idle block devices are
23,excluded using the -z switch.
23,Find definitions for each of the displayed column titles in the
23,man page of iostat (man 1 iostat).
23,You can also specify that a certain device should be monitored at specified
23,intervals.
23,"For example, to generate five reports at three-second intervals for the"
23,"device sda, use:"
23,> iostat -p sda 3 5
23,"To show statistics of network file systems (NFS), there are two similar"
23,utilities:
23,nfsiostat-sysstat is included with the
23,package sysstat.
23,nfsiostat is included with the package
23,nfs-client.
23,Note: Using iostat in multipath setups
23,The iostat command might not show all controllers
23,"that are listed by nvme list-subsys. By default,"
23,iostat filters out all block devices with no I/O.
23,To make iostat show all
23,"devices, use the following command:"
23,iostat -p ALL2.2.2 Processor activity monitoring: mpstat #
23,The utility mpstat examines activities of each
23,"available processor. If your system has one processor only, the global"
23,average statistics will be reported.
23,The timing arguments work the same way as with the
23,iostat command. Entering mpstat 2
23,5 prints five reports for all processors in two-second
23,intervals.
23,# mpstat 2 5
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,13:51:10
23,CPU
23,%usr
23,%nice
23,%sys
23,%iowait
23,%irq
23,%soft
23,%steal
23,%guest
23,%gnice
23,%idle
23,13:51:12
23,all
23,"8,27"
23,"0,00"
23,"0,50"
23,"0,00"
23,"0,00"
23,"0,00"
23,"0,00"
23,"0,00"
23,"0,00"
23,"91,23"
23,13:51:14
23,all
23,"46,62"
23,"0,00"
23,"3,01"
23,"0,00"
23,"0,00"
23,"0,25"
23,"0,00"
23,"0,00"
23,"0,00"
23,"50,13"
23,13:51:16
23,all
23,"54,71"
23,"0,00"
23,"3,82"
23,"0,00"
23,"0,00"
23,"0,51"
23,"0,00"
23,"0,00"
23,"0,00"
23,"40,97"
23,13:51:18
23,all
23,"78,77"
23,"0,00"
23,"5,12"
23,"0,00"
23,"0,00"
23,"0,77"
23,"0,00"
23,"0,00"
23,"0,00"
23,"15,35"
23,13:51:20
23,all
23,"51,65"
23,"0,00"
23,"4,30"
23,"0,00"
23,"0,00"
23,"0,51"
23,"0,00"
23,"0,00"
23,"0,00"
23,"43,54"
23,Average:
23,all
23,"47,85"
23,"0,00"
23,"3,34"
23,"0,00"
23,"0,00"
23,"0,40"
23,"0,00"
23,"0,00"
23,"0,00"
23,"48,41"
23,"From the mpstat data, you can see:"
23,The ratio between the %usr and %sys.
23,"For example, a ratio"
23,of 10:1 indicates the workload is mostly running application code
23,and analysis should focus on the application. A ratio of 1:10
23,indicates the workload is mostly kernel-bound and tuning the kernel
23,"is worth considering. Alternatively, determine why the application is"
23,kernel-bound and see if that can be alleviated.
23,Whether there is a subset of CPUs that are nearly fully
23,utilized even if the system is lightly loaded overall. Few
23,hot CPUs can indicate that the workload is not parallelized and
23,could benefit from executing on a machine with a smaller number of
23,faster processors.
23,2.2.3 Processor frequency monitoring: turbostat #
23,"turbostat shows frequencies, load, temperature, and power"
23,of AMD64/Intel 64 processors. It can operate in two modes: If called
23,"with a command, the command process is forked and statistics are displayed"
23,"upon command completion. When run without a command, it will display"
23,updated statistics every five seconds. Note that
23,turbostat requires the kernel module
23,msr to be loaded.
23,> sudo turbostat find /etc -type d -exec true {} \;
23,0.546880 sec
23,CPU Avg_MHz
23,Busy% Bzy_MHz TSC_MHz
23,416
23,28.43
23,1465
23,3215
23,631
23,37.29
23,1691
23,3215
23,416
23,27.14
23,1534
23,3215
23,270
23,24.30
23,1113
23,3215
23,406
23,26.57
23,1530
23,3214
23,505
23,32.46
23,1556
23,3214
23,270
23,22.79
23,1184
23,3214
23,The output depends on the CPU type and may vary. To display more details
23,"such as temperature and power, use the --debug option. For"
23,"more command line options and an explanation of the field descriptions,"
23,refer to man 8 turbostat.
23,2.2.4 Task monitoring: pidstat #
23,"If you need to see what load a particular task applies to your system,"
23,use pidstat command. It prints activity of every
23,selected task or all tasks managed by Linux kernel if no task is
23,specified. You can also set the number of reports to be displayed and
23,the time interval between them.
23,"For example, pidstat -C firefox 2 3"
23,prints the load statistic for tasks whose command name includes the
23,string “firefox”. There will be three reports printed at
23,two second intervals.
23,# pidstat -C firefox 2 3
23,Linux 4.4.21-64-default (jupiter)
23,10/12/16
23,_x86_64_
23,(2 CPU)
23,14:09:11
23,UID
23,PID
23,%usr %system
23,%guest
23,%CPU
23,CPU
23,Command
23,14:09:13
23,1000
23,387
23,"22,77"
23,"0,99"
23,"0,00"
23,"23,76"
23,firefox
23,14:09:13
23,UID
23,PID
23,%usr %system
23,%guest
23,%CPU
23,CPU
23,Command
23,14:09:15
23,1000
23,387
23,"46,50"
23,"3,00"
23,"0,00"
23,"49,50"
23,firefox
23,14:09:15
23,UID
23,PID
23,%usr %system
23,%guest
23,%CPU
23,CPU
23,Command
23,14:09:17
23,1000
23,387
23,"60,50"
23,"7,00"
23,"0,00"
23,"67,50"
23,firefox
23,Average:
23,UID
23,PID
23,%usr %system
23,%guest
23,%CPU
23,CPU
23,Command
23,Average:
23,1000
23,387
23,"43,19"
23,"3,65"
23,"0,00"
23,"46,84"
23,firefox
23,"Similarly, pidstat -d can be"
23,"used to estimate how much I/O tasks are doing, whether they are"
23,sleeping on that I/O and how many clock ticks the task was stalled.
23,2.2.5 Kernel ring buffer: dmesg #
23,The Linux kernel keeps certain messages in a ring buffer. To view these
23,"messages, enter the command dmesg -T."
23,Older events are logged in the systemd journal. See
23,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal” for more information on the journal."
23,2.2.6 List of open files: lsof #
23,To view a list of all the files open for the process with process ID
23,"PID, use -p. For example, to"
23,"view all the files used by the current shell, enter:"
23,# lsof -p $$
23,COMMAND
23,PID USER
23,TYPE DEVICE SIZE/OFF
23,NODE NAME
23,bash
23,8842 root
23,cwd
23,DIR
23,"0,32"
23,222
23,6772 /root
23,bash
23,8842 root
23,rtd
23,DIR
23,"0,32"
23,166
23,256 /
23,bash
23,8842 root
23,txt
23,REG
23,"0,32"
23,656584 31066 /bin/bash
23,bash
23,8842 root
23,mem
23,REG
23,"0,32"
23,1978832 22993 /lib64/libc-2.19.so
23,[...]
23,bash
23,8842 root
23,CHR
23,"136,2"
23,0t0
23,5 /dev/pts/2
23,bash
23,8842 root
23,255u
23,CHR
23,"136,2"
23,0t0
23,5 /dev/pts/2
23,"The special shell variable $$, whose value is the"
23,"process ID of the shell, has been used."
23,"When used with -i, lsof lists"
23,currently open Internet files as well:
23,# lsof -i
23,COMMAND
23,PID USER
23,TYPE DEVICE SIZE/OFF NODE NAME
23,wickedd-d
23,917 root
23,IPv4
23,16627
23,0t0
23,UDP *:bootpc
23,wickedd-d
23,918 root
23,IPv6
23,20752
23,0t0
23,UDP [fe80::5054:ff:fe72:5ead]:dhcpv6-client
23,sshd
23,3152 root
23,IPv4
23,18618
23,0t0
23,TCP *:ssh (LISTEN)
23,sshd
23,3152 root
23,IPv6
23,18620
23,0t0
23,TCP *:ssh (LISTEN)
23,master
23,4746 root
23,13u
23,IPv4
23,20588
23,0t0
23,TCP localhost:smtp (LISTEN)
23,master
23,4746 root
23,14u
23,IPv6
23,20589
23,0t0
23,TCP localhost:smtp (LISTEN)
23,sshd
23,8837 root
23,IPv4 293709
23,0t0
23,TCP jupiter.suse.de:ssh->venus.suse.de:33619 (ESTABLISHED)
23,sshd
23,8837 root
23,IPv6 294830
23,0t0
23,TCP localhost:x11 (LISTEN)
23,sshd
23,8837 root
23,10u
23,IPv4 294831
23,0t0
23,TCP localhost:x11 (LISTEN)2.2.7 Kernel and udev event sequence viewer: udevadm monitor #
23,udevadm monitor listens to the kernel uevents and
23,events sent out by a udev rule and prints the device path (DEVPATH) of
23,the event to the console. This is a sequence of events while connecting
23,a USB memory stick:
23,Note: Monitoring udev events
23,Only root user is allowed to monitor udev events by running the
23,udevadm command.
23,UEVENT[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2
23,UEVENT[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
23,UEVENT[1138806687] add@/class/scsi_host/host4
23,UEVENT[1138806687] add@/class/usb_device/usbdev4.10
23,UDEV
23,[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2
23,UDEV
23,[1138806687] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
23,UDEV
23,[1138806687] add@/class/scsi_host/host4
23,UDEV
23,[1138806687] add@/class/usb_device/usbdev4.10
23,UEVENT[1138806692] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
23,UEVENT[1138806692] add@/block/sdb
23,UEVENT[1138806692] add@/class/scsi_generic/sg1
23,UEVENT[1138806692] add@/class/scsi_device/4:0:0:0
23,UDEV
23,[1138806693] add@/devices/pci0000:00/0000:00:1d.7/usb4/4-2/4-2.2/4-2.2
23,UDEV
23,[1138806693] add@/class/scsi_generic/sg1
23,UDEV
23,[1138806693] add@/class/scsi_device/4:0:0:0
23,UDEV
23,[1138806693] add@/block/sdb
23,UEVENT[1138806694] add@/block/sdb/sdb1
23,UDEV
23,[1138806694] add@/block/sdb/sdb1
23,UEVENT[1138806694] mount@/block/sdb/sdb1
23,UEVENT[1138806697] umount@/block/sdb/sdb12.3 Processes #  2.3.1 Interprocess communication: ipcs #
23,The command ipcs produces a list of the IPC resources
23,currently in use:
23,# ipcs
23,------ Message Queues --------
23,key
23,msqid
23,owner
23,perms
23,used-bytes
23,messages
23,------ Shared Memory Segments --------
23,key
23,shmid
23,owner
23,perms
23,bytes
23,nattch
23,status
23,0x00000000 65536
23,tux
23,600
23,524288
23,dest
23,0x00000000 98305
23,tux
23,600
23,4194304
23,dest
23,0x00000000 884738
23,root
23,600
23,524288
23,dest
23,0x00000000 786435
23,tux
23,600
23,4194304
23,dest
23,0x00000000 12058628
23,tux
23,600
23,524288
23,dest
23,0x00000000 917509
23,root
23,600
23,524288
23,dest
23,0x00000000 12353542
23,tux
23,600
23,196608
23,dest
23,0x00000000 12451847
23,tux
23,600
23,524288
23,dest
23,0x00000000 11567114
23,root
23,600
23,262144
23,dest
23,0x00000000 10911763
23,tux
23,600
23,2097152
23,dest
23,0x00000000 11665429
23,root
23,600
23,2336768
23,dest
23,0x00000000 11698198
23,root
23,600
23,196608
23,dest
23,0x00000000 11730967
23,root
23,600
23,524288
23,dest
23,------ Semaphore Arrays --------
23,key
23,semid
23,owner
23,perms
23,nsems
23,0xa12e0919 32768
23,tux
23,666
23,22.3.2 Process list: ps #
23,The command ps produces a list of processes. Most
23,parameters must be written without a minus sign. Refer to ps
23,--help for a brief help or to the man page for extensive help.
23,"To list all processes with user and command line information, use"
23,ps axu:
23,> ps axu
23,USER
23,PID %CPU %MEM
23,VSZ
23,RSS TTY
23,STAT START
23,TIME COMMAND
23,root
23,0.0
23,0.3
23,34376
23,4608 ?
23,Jul24
23,0:02 /usr/lib/systemd/systemd
23,root
23,0.0
23,0.0
23,0 ?
23,Jul24
23,0:00 [kthreadd]
23,root
23,0.0
23,0.0
23,0 ?
23,Jul24
23,0:00 [ksoftirqd/0]
23,root
23,0.0
23,0.0
23,0 ?
23,Jul24
23,0:00 [kworker/0:0H]
23,root
23,0.0
23,0.0
23,0 ?
23,Jul24
23,0:00 [kworker/u2:0]
23,root
23,0.0
23,0.0
23,0 ?
23,Jul24
23,0:00 [migration/0]
23,[...]
23,tux
23,12583
23,0.0
23,0.1 185980
23,2720 ?
23,10:12
23,0:00 /usr/lib/gvfs/gvfs-mtp-volume-monitor
23,tux
23,12587
23,0.0
23,0.1 198132
23,3044 ?
23,10:12
23,0:00 /usr/lib/gvfs/gvfs-gphoto2-volume-monitor
23,tux
23,12591
23,0.0
23,0.1 181940
23,2700 ?
23,10:12
23,0:00 /usr/lib/gvfs/gvfs-goa-volume-monitor
23,tux
23,12594
23,8.1 10.6 1418216 163564 ?
23,10:12
23,0:03 /usr/bin/gnome-shell
23,tux
23,12600
23,0.0
23,0.3 393448
23,5972 ?
23,10:12
23,0:00 /usr/lib/gnome-settings-daemon-3.0/gsd-printer
23,tux
23,12625
23,0.0
23,0.6 227776 10112 ?
23,10:12
23,0:00 /usr/lib/gnome-control-center-search-provider
23,tux
23,12626
23,0.5
23,1.5 890972 23540 ?
23,10:12
23,0:00 /usr/bin/nautilus --no-default-window
23,[...]
23,"To check how many sshd processes are running, use the"
23,option -p together with the command
23,"pidof, which lists the process IDs of the given"
23,processes.
23,> ps -p $(pidof sshd)
23,PID TTY
23,STAT
23,TIME COMMAND
23,1545 ?
23,0:00 /usr/sbin/sshd -D
23,4608 ?
23,0:00 sshd: root@pts/0
23,The process list can be formatted according to your needs. The option
23,L returns a list of all keywords. Enter the following
23,command to issue a list of all processes sorted by memory usage:
23,"> ps ax --format pid,rss,cmd --sort rss"
23,PID
23,RSS CMD
23,PID
23,RSS CMD
23,0 [kthreadd]
23,0 [ksoftirqd/0]
23,0 [kworker/0:0]
23,0 [kworker/0:0H]
23,0 [kworker/u2:0]
23,0 [migration/0]
23,0 [rcu_bh]
23,[...]
23,12518 22996 /usr/lib/gnome-settings-daemon-3.0/gnome-settings-daemon
23,12626 23540 /usr/bin/nautilus --no-default-window
23,12305 32188 /usr/bin/Xorg :0 -background none -verbose
23,12594 164900 /usr/bin/gnome-shellUseful ps calls #  ps aux--sort
23,COLUMN
23,Sort the output by COLUMN. Replace
23,COLUMN with
23,pmem for physical memory ratiopcpu for CPU ratiorss for resident set size (non-swapped physical
23,"memory)ps axo pid,%cpu,rss,vsz,args,wchan"
23,"Shows every process, their PID, CPU usage ratio, memory size"
23,"(resident and virtual), name, and their syscall."
23,"ps axfo pid,args"
23,Show a process tree.
23,2.3.3 Process tree: pstree #
23,The command pstree produces a list of processes in
23,the form of a tree:
23,> pstree
23,systemd---accounts-daemon---{gdbus}
23,|-{gmain}
23,|-at-spi-bus-laun---dbus-daemon
23,|-{dconf worker}
23,|-{gdbus}
23,|-{gmain}
23,|-at-spi2-registr---{gdbus}
23,|-cron
23,|-2*[dbus-daemon]
23,|-dbus-launch
23,|-dconf-service---{gdbus}
23,|-{gmain}
23,|-gconfd-2
23,|-gdm---gdm-simple-slav---Xorg
23,|-gdm-session-wor---gnome-session---gnome-setti+
23,|-gnome-shell+++
23,|-{dconf work+
23,|-{gdbus}
23,|-{gmain}
23,|-{gdbus}
23,|-{gmain}
23,|-{gdbus}
23,|-{gmain}
23,|-{gdbus}
23,|-{gmain}
23,[...]
23,The parameter -p adds the process ID to a given name.
23,"To have the command lines displayed as well, use the -a"
23,parameter:
23,2.3.4 Table of processes: top #
23,The command top (an abbreviation of “table of
23,processes”) displays a list of processes that is refreshed every
23,"two seconds. To terminate the program, press q. The"
23,parameter -n 1 terminates the program after a single
23,display of the process list. The following is an example output of the
23,command top -n 1:
23,> top -n 1
23,"Tasks: 128 total,"
23,"1 running, 127 sleeping,"
23,"0 stopped,"
23,0 zombie
23,%Cpu(s):
23,"2.4 us,"
23,"1.2 sy,"
23,"0.0 ni, 96.3 id,"
23,"0.1 wa,"
23,"0.0 hi,"
23,"0.0 si,"
23,0.0 st
23,KiB Mem:
23,"1535508 total,"
23,"699948 used,"
23,"835560 free,"
23,880 buffers
23,KiB Swap:
23,"1541116 total,"
23,"0 used,"
23,1541116 free.
23,377000 cached Mem
23,PID USER
23,VIRT
23,RES
23,SHR S
23,%CPU
23,%MEM
23,TIME+ COMMAND
23,1 root
23,116292
23,4660
23,2028 S 0.000 0.303
23,0:04.45 systemd
23,2 root
23,0 S 0.000 0.000
23,0:00.00 kthreadd
23,3 root
23,0 S 0.000 0.000
23,0:00.07 ksoftirqd+
23,5 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 kworker/0+
23,6 root
23,0 S 0.000 0.000
23,0:00.00 kworker/u+
23,7 root
23,0 S 0.000 0.000
23,0:00.00 migration+
23,8 root
23,0 S 0.000 0.000
23,0:00.00 rcu_bh
23,9 root
23,0 S 0.000 0.000
23,0:00.24 rcu_sched
23,10 root
23,0 S 0.000 0.000
23,0:00.01 watchdog/0
23,11 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 khelper
23,12 root
23,0 S 0.000 0.000
23,0:00.00 kdevtmpfs
23,13 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 netns
23,14 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 writeback
23,15 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 kintegrit+
23,16 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 bioset
23,17 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 crypto
23,18 root
23,0 -20
23,0 S 0.000 0.000
23,0:00.00 kblockd
23,By default the output is sorted by CPU usage (column
23,"%CPU, shortcut Shift–P). Use the following key combinations to"
23,change the sort field:
23,Shift–M: Resident Memory (RES) Shift–N: Process ID (PID) Shift–T: Time (TIME+)
23,"To use any other field for sorting, press F and select"
23,"a field from the list. To toggle the sort order, Use Shift–R."
23,The parameter -U UID
23,monitors only the processes associated with a particular user. Replace
23,UID with the user ID of the user. Use
23,top -U $(id -u) to show processes of the current user
23,2.3.5 IBM Z hypervisor monitor: hyptop #
23,hyptop provides a dynamic real-time view of an
23,"IBM Z hypervisor environment, using the kernel infrastructure via"
23,debugfs. It works with either the z/VM or the LPAR hypervisor. Depending
23,"on the available data it, for example, shows CPU and memory consumption"
23,of active LPARs or z/VM guests. It provides a curses based user
23,interface similar to the top command.
23,hyptop provides two windows:
23,sys_list: Lists systems that the
23,current hypervisor is running
23,sys: Shows one system in more detail
23,You can run hyptop in interactive mode (default) or
23,in batch mode with the -b option. Help in the
23,interactive mode is available by pressing ? after
23,hyptop is started.
23,Output for the sys_list window under LPAR:
23,12:30:48 | CPU-T: IFL(18) CP(3) UN(3)
23,?=help
23,system
23,#cpu
23,cpu
23,mgm
23,Cpu+
23,Mgm+
23,online
23,(str)
23,(#)
23,(%)
23,(%)
23,(hm)
23,(hm)
23,(dhm)
23,H05LP30
23,10 461.14 10.18 1547:41
23,8:15 11:05:59
23,H05LP33
23,4 133.73
23,7.57
23,220:53
23,6:12 11:05:54
23,H05LP50
23,99.26
23,0.01
23,146:24
23,0:12 10:04:24
23,H05LP02
23,99.09
23,0.00
23,269:57
23,0:00 11:05:58
23,TRX2CFA
23,2.14
23,0.03
23,3:24
23,0:04 11:06:01
23,H05LP13
23,1.36
23,0.34
23,4:23
23,0:54 11:05:56
23,TRX1
23,1.22
23,0.14
23,13:57
23,0:22 11:06:01
23,TRX2
23,1.16
23,0.11
23,26:05
23,0:25 11:06:00
23,H05LP55
23,0.00
23,0.00
23,0:22
23,0:00 11:05:52
23,H05LP56
23,0.00
23,0.00
23,0:00
23,0:00 11:05:52
23,413 823.39 23.86 3159:57 38:08 11:06:01
23,"Output for the ""sys_list"" window under z/VM:"
23,12:32:21 | CPU-T: UN(16)
23,?=help
23,system
23,#cpu
23,cpu
23,Cpu+
23,online memuse memmax wcur
23,(str)
23,(#)
23,(%)
23,(hm)
23,(dhm)
23,(GiB)
23,(GiB)
23,(#)
23,T6360004
23,6 100.31
23,959:47 53:05:20
23,1.56
23,2.00
23,100
23,T6360005
23,0.44
23,1:11
23,3:02:26
23,0.42
23,0.50
23,100
23,T6360014
23,0.27
23,0:45 10:18:41
23,0.54
23,0.75
23,100
23,DTCVSW1
23,0.00
23,0:00 53:16:42
23,0.01
23,0.03
23,100
23,T6360002
23,0.00
23,166:26 40:19:18
23,1.87
23,2.00
23,100
23,OPERATOR
23,0.00
23,0:00 53:16:42
23,0.00
23,0.03
23,100
23,T6360008
23,0.00
23,0:37 30:22:55
23,0.32
23,0.75
23,100
23,T6360003
23,0.00 3700:57 53:03:09
23,4.00
23,4.00
23,100
23,NSLCF1
23,0.00
23,0:02 53:16:41
23,0.03
23,0.25
23,500
23,EREP
23,0.00
23,0:00 53:16:42
23,0.00
23,0.03
23,100
23,PERFSVM
23,0.00
23,0:53
23,2:21:12
23,0.04
23,0.06
23,TCPIP
23,0.00
23,0:01 53:16:42
23,0.01
23,0.12 3000
23,DATAMOVE
23,0.00
23,0:05 53:16:42
23,0.00
23,0.03
23,100
23,DIRMAINT
23,0.00
23,0:04 53:16:42
23,0.01
23,0.03
23,100
23,DTCVSW2
23,0.00
23,0:00 53:16:42
23,0.01
23,0.03
23,100
23,RACFVM
23,0.00
23,0:00 53:16:42
23,0.01
23,0.02
23,100
23,75 101.57 5239:47 53:16:42
23,15.46
23,22.50 3000
23,Output for the sys window under LPAR:
23,14:08:41 | H05LP30 | CPU-T: IFL(18) CP(3) UN(3)
23,? = help
23,cpuid
23,type
23,cpu
23,mgm visual.
23,(#)
23,(str)
23,(%)
23,(%) (vis)
23,IFL
23,96.91
23,1.96 |############################################ |
23,IFL
23,81.82
23,1.46 |#####################################
23,IFL
23,88.00
23,2.43 |########################################
23,IFL
23,92.27
23,1.29 |##########################################
23,IFL
23,83.32
23,1.05 |#####################################
23,IFL
23,92.46
23,2.59 |##########################################
23,IFL
23,0.00
23,0.00 |
23,IFL
23,0.00
23,0.00 |
23,IFL
23,0.00
23,0.00 |
23,IFL
23,0.00
23,0.00 |
23,534.79 10.78
23,Output for the sys window under z/VM:
23,15:46:57 | T6360003 | CPU-T: UN(16)
23,? = help
23,cpuid
23,cpu visual
23,(#)
23,(%) (vis)
23,548.72 |#########################################
23,548.722.3.6 A top-like I/O monitor: iotop #
23,The iotop utility displays a table of I/O usage by
23,processes or threads.
23,Note: Installing iotop
23,iotop is not installed by default. You need to
23,install it manually with zypper in iotop as
23,root.
23,iotop displays columns for the I/O bandwidth read and
23,written by each process during the sampling period. It also displays the
23,percentage of time the process spent while swapping in and while waiting
23,"on I/O. For each process, its I/O priority (class/level) is shown. In"
23,"addition, the total I/O bandwidth read and written during the sampling"
23,period is displayed at the top of the interface.
23,The ← and → keys
23,change the sorting.
23,R reverses the sort order.
23,O toggles between showing all processes and threads
23,(default view) and showing only those doing I/O. (This function is
23,similar to adding --only on command line.)
23,P toggles between showing threads (default view) and
23,processes. (This function is similar to --only.)
23,A toggles between showing the current I/O bandwidth
23,(default view) and accumulated I/O operations since
23,iotop was started. (This function is similar to
23,--accumulated.)
23,I lets you change the priority of a thread or a
23,process's threads.
23,Q quits iotop.
23,Pressing any other key will force a refresh.
23,Following is an example output of the command iotop
23,"--only, while find and"
23,emacs are running:
23,# iotop --only
23,Total DISK READ: 50.61 K/s | Total DISK WRITE: 11.68 K/s
23,TID
23,PRIO
23,USER
23,DISK READ
23,DISK WRITE
23,SWAPIN
23,IO>
23,COMMAND
23,3416 be/4 tux
23,50.61 K/s
23,0.00 B/s
23,0.00 %
23,4.05 % find /
23,275 be/3 root
23,0.00 B/s
23,3.89 K/s
23,0.00 %
23,2.34 % [jbd2/sda2-8]
23,5055 be/4 tux
23,0.00 B/s
23,3.89 K/s
23,0.00 %
23,0.04 % emacs
23,iotop can be also used in a batch mode
23,(-b) and its output stored in a file for later
23,"analysis. For a complete set of options, see the manual page"
23,(man 8 iotop).
23,2.3.7 Modify a process's niceness: nice and renice #
23,The kernel determines which processes require more CPU time than others
23,"by the process's nice level, also called niceness. The higher the"
23,"“nice” level of a process is, the less CPU time it will"
23,take from other processes. Nice levels range from -20 (the least
23,“nice” level) to 19. Negative values can only be set by
23,root.
23,Adjusting the niceness level is useful when running a non time-critical
23,"process that lasts long and uses large amounts of CPU time. For example,"
23,compiling a kernel on a system that also performs other tasks. Making
23,"such a process “nicer”, ensures that the other tasks, for"
23,"example a Web server, will have a higher priority."
23,Calling nice without any parameters prints the
23,current niceness:
23,> nice
23,Running nice COMMAND
23,increments the current nice level for the given command by 10. Using
23,nice -n
23,LEVEL
23,COMMAND lets you specify a new niceness
23,relative to the current one.
23,"To change the niceness of a running process, use"
23,renice PRIORITY -p
23,"PROCESS_ID, for example:"
23,> renice +5 3266
23,"To renice all processes owned by a specific user, use the option"
23,-u USER.
23,Process groups are reniced by the option -g PROCESS_GROUP_ID.
23,2.4 Memory #  2.4.1 Memory usage: free #
23,The utility free examines RAM and swap usage. Details
23,of both free and used memory and swap areas are shown:
23,> free
23,total
23,used
23,free
23,shared
23,buffers
23,cached
23,Mem:
23,32900500
23,32703448
23,197052
23,255668
23,5787364
23,-/+ buffers/cache:
23,26660416
23,6240084
23,Swap:
23,2046972
23,304680
23,1742292
23,"The options -b, -k,"
23,"-m, -g show the output in bytes, KB,"
23,"MB, or GB, respectively. The parameter -s delay ensures"
23,that the display is refreshed every DELAY
23,"seconds. For example, free -s 1.5 produces an update"
23,every 1.5 seconds.
23,2.4.2 Detailed memory usage: /proc/meminfo #
23,Use /proc/meminfo to get more detailed information
23,on memory usage than with free. Actually
23,free uses some data from this file. See an
23,example output from a 64-bit system below. Note that it slightly differs
23,on 32-bit systems because of different memory management:
23,MemTotal:
23,1942636 kB
23,MemFree:
23,1294352 kB
23,MemAvailable:
23,1458744 kB
23,Buffers:
23,876 kB
23,Cached:
23,278476 kB
23,SwapCached:
23,0 kB
23,Active:
23,368328 kB
23,Inactive:
23,199368 kB
23,Active(anon):
23,288968 kB
23,Inactive(anon):
23,10568 kB
23,Active(file):
23,79360 kB
23,Inactive(file):
23,188800 kB
23,Unevictable:
23,80 kB
23,Mlocked:
23,80 kB
23,SwapTotal:
23,2103292 kB
23,SwapFree:
23,2103292 kB
23,Dirty:
23,44 kB
23,Writeback:
23,0 kB
23,AnonPages:
23,288592 kB
23,Mapped:
23,70444 kB
23,Shmem:
23,11192 kB
23,Slab:
23,40916 kB
23,SReclaimable:
23,17712 kB
23,SUnreclaim:
23,23204 kB
23,KernelStack:
23,2000 kB
23,PageTables:
23,10996 kB
23,NFS_Unstable:
23,0 kB
23,Bounce:
23,0 kB
23,WritebackTmp:
23,0 kB
23,CommitLimit:
23,3074608 kB
23,Committed_AS:
23,1407208 kB
23,VmallocTotal:
23,34359738367 kB
23,VmallocUsed:
23,145996 kB
23,VmallocChunk:
23,34359588844 kB
23,HardwareCorrupted:
23,0 kB
23,AnonHugePages:
23,86016 kB
23,HugePages_Total:
23,HugePages_Free:
23,HugePages_Rsvd:
23,HugePages_Surp:
23,Hugepagesize:
23,2048 kB
23,DirectMap4k:
23,79744 kB
23,DirectMap2M:
23,2017280 kB
23,These entries stand for the following:
23,MemTotal
23,Total amount of RAM.
23,MemFree
23,Amount of unused RAM.
23,MemAvailable
23,Estimate of how much memory is available for starting new applications
23,without swapping.
23,Buffers
23,File buffer cache in RAM containing file system metadata.
23,Cached
23,Page cache in RAM.
23,"This excludes buffer cache and swap cache, but includes"
23,Shmem memory.
23,SwapCached
23,Page cache for swapped-out memory.
23,"Active, Active(anon),"
23,Active(file)
23,Recently used memory that will not be reclaimed unless necessary or on
23,explicit request.
23,Active is the sum of Active(anon)
23,and Active(file):
23,Active(anon) tracks swap-backed memory.
23,This includes private and shared anonymous mappings and
23,private file pages after copy-on-write.
23,Active(file) tracks other file system backed
23,memory.
23,"Inactive, Inactive(anon),"
23,Inactive(file)
23,Less recently used memory that will usually be reclaimed first.
23,Inactive is the sum of
23,Inactive(anon) and Inactive(file):
23,Inactive(anon) tracks swap backed memory.
23,This includes private and shared anonymous mappings and
23,private file pages after copy-on-write.
23,Inactive(file) tracks other file system backed
23,memory.
23,Unevictable
23,"Amount of memory that cannot be reclaimed (for example, because it is"
23,Mlocked or used as a RAM disk).
23,Mlocked
23,Amount of memory that is backed by the
23,mlock system call.
23,mlock allows processes to define which part of
23,physical RAM their virtual memory should be mapped to.
23,"However, mlock does not guarantee this"
23,placement.
23,SwapTotal
23,Amount of swap space.
23,SwapFree
23,Amount of unused swap space.
23,Dirty
23,"Amount of memory waiting to be written to disk, because it contains"
23,changes compared to the backing storage. Dirty data can be explicitly
23,synchronized either by the application or by the kernel after a short
23,delay. A large amount of dirty data may take considerable time to write
23,to disk resulting in stalls. The total amount of dirty data that can
23,exist at any time can be controlled with the
23,sysctl parameters vm.dirty_ratio
23,"or vm.dirty_bytes (refer to Section 15.1.5, “Writeback” for more details)."
23,Writeback
23,Amount of memory that is currently being written to disk.
23,Mapped
23,Memory claimed with the mmap system call.
23,Shmem
23,"Memory shared between groups of processes, such as IPC data,"
23,"tmpfs data, and shared anonymous memory."
23,Slab
23,Memory allocation for internal data structures of the kernel.
23,SReclaimable
23,"Slab section that can be reclaimed, such as caches (inode, dentry, etc.)."
23,SUnreclaim
23,Slab section that cannot be reclaimed.
23,KernelStack
23,Amount of kernel space memory used by applications (through system calls).
23,PageTables
23,Amount of memory dedicated to page tables of all processes.
23,NFS_Unstable
23,"NFS pages that have already been sent to the server, but are not yet"
23,committed there.
23,Bounce
23,Memory used for bounce buffers of block devices.
23,WritebackTmp
23,Memory used by FUSE for temporary writeback buffers.
23,CommitLimit
23,Amount of memory available to the system based on the overcommit
23,ratio setting. This is only enforced if strict overcommit accounting
23,is enabled.
23,Committed_AS
23,An approximation of the total amount of memory (RAM and swap) that the
23,current workload would need in the worst case.
23,VmallocTotal
23,Amount of allocated kernel virtual address space.
23,VmallocUsed
23,Amount of used kernel virtual address space.
23,VmallocChunk
23,The largest contiguous block of available kernel virtual address space.
23,HardwareCorrupted
23,Amount of failed memory (can only be detected when using ECC RAM).
23,AnonHugePages
23,Anonymous hugepages that are mapped into user space page tables.
23,These are allocated transparently for processes without being
23,"specifically requested, therefore they are also known as"
23,transparent hugepages (THP).
23,HugePages_Total
23,Number of preallocated hugepages for use by
23,SHM_HUGETLB and
23,MAP_HUGETLB or through the
23,"hugetlbfs file system, as defined in"
23,/proc/sys/vm/nr_hugepages.
23,HugePages_Free
23,Number of hugepages available.
23,HugePages_Rsvd
23,Number of hugepages that are committed.
23,HugePages_Surp
23,Number of hugepages available beyond
23,"HugePages_Total (“surplus”), as defined"
23,in /proc/sys/vm/nr_overcommit_hugepages.
23,Hugepagesize
23,Size of a hugepage—on AMD64/Intel 64 the default is 2048 KB.
23,DirectMap4k etc.
23,Amount of kernel memory that is mapped to pages with a given size (in the
23,example: 4 kB).
23,2.4.3 Process memory usage: smaps #
23,Exactly determining how much memory a certain process is consuming is
23,not possible with standard tools like top or
23,"ps. Use the smaps subsystem, introduced in kernel"
23,"2.6.14, if you need exact data. It can be found at"
23,/proc/PID/smaps and
23,shows you the number of clean and dirty memory pages the process with
23,the ID PID is using at that time. It
23,"differentiates between shared and private memory, so you can see"
23,how much memory the process is using without including memory shared
23,with other processes. For more information see
23,/usr/src/linux/Documentation/filesystems/proc.txt
23,(requires the package
23,kernel-source to be
23,installed).
23,smaps is expensive to read. Therefore it is not recommended to monitor
23,"it regularly, but only when closely monitoring a certain process."
23,2.4.4 numaTOP #
23,numaTOP is a tool for NUMA (Non-uniform Memory Access)
23,systems. The tool helps to identify NUMA-related performance
23,bottlenecks by providing real-time analysis of a NUMA system.
23,"Generally speaking, numaTOP allows you to identify and"
23,investigate processes and threads with poor locality (that is
23,poor ratio of local versus remote memory usage) by analyzing
23,"the number of Remote Memory Accesses (RMA), the number of Local Memory"
23,"Accesses (LMA), and the RMA/LMA ratio."
23,numaTOP is supported on PowerPC and the following Intel Xeon
23,"processors: 5500-series, 6500/7500-series, 5600-series,"
23,"E7-x8xx-series, and E5-16xx/24xx/26xx/46xx-series."
23,"numaTOP is available in the official software repositories, and"
23,you can install the tool using the sudo zypper in
23,"numatop command. To launch numaTOP, run the"
23,numatop command. To get an overview of
23,"numaTOP functionality and usage, use the man"
23,numatop command.
23,2.5 Networking #  Tip: Traffic shaping
23,"In case the network bandwidth is lower than expected, you should first"
23,check if any traffic shaping rules are active for your network segment.
23,2.5.1 Basic network diagnostics: ip #
23,ip is a powerful tool to set up and control network
23,interfaces. You can also use it to quickly view basic statistics about
23,"network interfaces of the system. For example, whether the interface is"
23,"up or how many errors, dropped packets, or packet collisions there are."
23,"If you run ip with no additional parameter, it"
23,"displays a help output. To list all network interfaces, enter"
23,ip addr show (or abbreviated as ip
23,a). ip addr show up lists only running
23,network interfaces. ip -s link show
23,DEVICE lists statistics for the specified
23,interface only:
23,# ip -s link show br0
23,"6: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT"
23,link/ether 00:19:d1:72:d4:30 brd ff:ff:ff:ff:ff:ff
23,RX: bytes
23,packets
23,errors
23,dropped overrun mcast
23,6346104756 9265517
23,10860
23,TX: bytes
23,packets
23,errors
23,dropped carrier collsns
23,3996204683 3655523
23,ip can also show interfaces
23,"(link), routing tables (route), and"
23,much more—refer to man 8 ip for details.
23,# ip route
23,default via 192.168.2.1 dev eth1
23,192.168.2.0/24 dev eth0
23,proto kernel
23,scope link
23,src 192.168.2.100
23,192.168.2.0/24 dev eth1
23,proto kernel
23,scope link
23,src 192.168.2.101
23,192.168.2.0/24 dev eth2
23,proto kernel
23,scope link
23,src 192.168.2.102# ip link
23,"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default"
23,link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
23,"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
23,link/ether 52:54:00:44:30:51 brd ff:ff:ff:ff:ff:ff
23,"3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
23,link/ether 52:54:00:a3:c1:fb brd ff:ff:ff:ff:ff:ff
23,"4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000"
23,link/ether 52:54:00:32:a4:09 brd ff:ff:ff:ff:ff:ff2.5.2
23,Show the network usage of processes: nethogs
23,"In some cases, for example if the network traffic suddenly becomes very"
23,"high, it is desirable to quickly find out which application(s) is/are"
23,"causing the traffic. nethogs, a tool with a design"
23,"similar to top, shows incoming and outgoing traffic for"
23,all relevant processes:
23,PID
23,USER
23,PROGRAM
23,DEV
23,SENT
23,RECEIVED
23,27145 root
23,zypper
23,eth0
23,5.719
23,391.749 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30015
23,0.102
23,2.326 KB/sec
23,26635 tux
23,/usr/lib64/firefox/firefox
23,eth0
23,0.026
23,0.026 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30045
23,0.000
23,0.021 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30045
23,0.000
23,0.018 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30015
23,0.000
23,0.018 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30045
23,0.000
23,0.017 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30045
23,0.000
23,0.017 KB/sec
23,root
23,..0:113:80c0:8080:10:160:0:100:30045
23,0.069
23,0.000 KB/sec
23,root
23,unknown TCP
23,0.000
23,0.000 KB/sec
23,TOTAL
23,5.916
23,394.192 KB/sec
23,"Like in top, nethogs features"
23,interactive commands:
23,"M: cycle between display modes (kb/s, kb, b, mb)"
23,R: sort by RECEIVED
23,S: sort by SENTQ: quit2.5.3 Ethernet cards in detail: ethtool #
23,ethtool can display and change detailed aspects of
23,your Ethernet network device. By default it prints the current setting
23,of the specified device.
23,# ethtool eth0
23,Settings for eth0:
23,Supported ports: [ TP ]
23,Supported link modes:
23,10baseT/Half 10baseT/Full
23,100baseT/Half 100baseT/Full
23,1000baseT/Full
23,Supports auto-negotiation: Yes
23,Advertised link modes:
23,10baseT/Half 10baseT/Full
23,100baseT/Half 100baseT/Full
23,1000baseT/Full
23,Advertised pause frame use: No
23,[...]
23,Link detected: yes
23,The following table shows ethtool options that you
23,can use to query the device for specific information:
23,Table 2.1: List of query options of ethtool #
23,ethtool option
23,it queries the device for
23,pause parameter information
23,interrupt coalescing information
23,Rx/Tx (receive/transmit) ring parameter information
23,associated driver information
23,offload information
23,NIC and driver-specific statistics
23,2.5.4 Show the network status: ss #
23,ss is a tool to dump socket statistics and replaces
23,the netstat command. To list all
23,connections use ss without parameters:
23,# ss
23,Netid
23,State
23,Recv-Q Send-Q
23,Local Address:Port
23,Peer Address:Port
23,u_str
23,ESTAB
23,* 14082
23,* 14083
23,u_str
23,ESTAB
23,* 18582
23,* 18583
23,u_str
23,ESTAB
23,* 19449
23,* 19450
23,u_str
23,ESTAB
23,@/tmp/dbus-gmUUwXABPV 18784
23,* 18783
23,u_str
23,ESTAB
23,/var/run/dbus/system_bus_socket 19383 * 19382
23,u_str
23,ESTAB
23,@/tmp/dbus-gmUUwXABPV 18617
23,* 18616
23,u_str
23,ESTAB
23,@/tmp/dbus-58TPPDv8qv 19352
23,* 19351
23,u_str
23,ESTAB
23,* 17658
23,* 17657
23,u_str
23,ESTAB
23,* 17693
23,* 17694
23,[..]
23,"To show all network ports currently open, use the following command:"
23,# ss -l
23,Netid
23,State
23,Recv-Q Send-Q
23,Local Address:Port
23,Peer Address:Port
23,UNCONN
23,rtnl:4195117
23,UNCONN
23,rtnl:wickedd-auto4/811
23,UNCONN
23,rtnl:wickedd-dhcp4/813
23,UNCONN
23,rtnl:4195121
23,UNCONN
23,rtnl:4195115
23,UNCONN
23,rtnl:wickedd-dhcp6/814
23,UNCONN
23,rtnl:kernel
23,UNCONN
23,rtnl:wickedd/817
23,UNCONN
23,rtnl:4195118
23,UNCONN
23,rtnl:nscd/706
23,UNCONN
23,4352
23,tcpdiag:ss/2381
23,[...]
23,"When displaying network connections, you can specify the socket type to"
23,display: TCP (-t) or UDP (-u) for
23,example. The -p option shows the PID and name of the
23,program to which each socket belongs.
23,The following example lists all TCP connections and the programs using
23,these connections. The -a option make sure all
23,established connections (listening and non-listening) are shown. The
23,-p option shows the PID and name of the program to
23,which each socket belongs.
23,# ss -t -a -p
23,State
23,Recv-Q Send-Q
23,Local Address:Port
23,Peer Address:Port
23,LISTEN
23,128
23,*:ssh
23,*:*
23,"users:((""sshd"",1551,3))"
23,LISTEN
23,100
23,127.0.0.1:smtp
23,*:*
23,"users:((""master"",1704,13))"
23,ESTAB
23,132
23,10.120.65.198:ssh
23,10.120.4.150:55715
23,"users:((""sshd"",2103,5))"
23,LISTEN
23,128
23,:::ssh
23,:::*
23,"users:((""sshd"",1551,4))"
23,LISTEN
23,100
23,::1:smtp
23,:::*
23,"users:((""master"",1704,14))2.6 The /proc file system #"
23,The /proc file system is a pseudo file system in
23,which the kernel reserves important information in the form of virtual
23,"files. For example, display the CPU type with this command:"
23,> cat /proc/cpuinfo
23,processor
23,: 0
23,vendor_id
23,: GenuineIntel
23,cpu family
23,: 6
23,model
23,: 30
23,model name
23,: Intel(R) Core(TM) i5 CPU
23,750
23,@ 2.67GHz
23,stepping
23,: 5
23,microcode
23,: 0x6
23,cpu MHz
23,: 1197.000
23,cache size
23,: 8192 KB
23,physical id
23,: 0
23,siblings
23,: 4
23,core id
23,: 0
23,cpu cores
23,: 4
23,apicid
23,: 0
23,initial apicid
23,: 0
23,fpu
23,: yes
23,fpu_exception
23,: yes
23,cpuid level
23,: 11
23,: yes
23,flags
23,: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm ida dtherm tpr_shadow vnmi flexpriority ept vpid
23,bogomips
23,: 5333.85
23,clflush size
23,: 64
23,cache_alignment : 64
23,address sizes
23,": 36 bits physical, 48 bits virtual"
23,power management:
23,[...]Tip: Detailed processor information
23,Detailed information about the processor on the AMD64/Intel 64 architecture is
23,also available by running x86info.
23,Query the allocation and use of interrupts with the following command:
23,> cat /proc/interrupts
23,CPU0
23,CPU1
23,CPU2
23,CPU3
23,121
23,IO-APIC-edge
23,timer
23,IO-APIC-edge
23,rtc0
23,IO-APIC-fasteoi
23,acpi
23,16:
23,11933
23,IO-APIC-fasteoi
23,ehci_hcd:+
23,18:
23,IO-APIC-fasteoi
23,i801_smbus
23,19:
23,117978
23,IO-APIC-fasteoi
23,"ata_piix,+"
23,22:
23,3275185
23,IO-APIC-fasteoi
23,enp5s1
23,23:
23,417927
23,IO-APIC-fasteoi
23,ehci_hcd:+
23,40:
23,2727916
23,HPET_MSI-edge
23,hpet2
23,41:
23,2749134
23,HPET_MSI-edge
23,hpet3
23,42:
23,2759148
23,HPET_MSI-edge
23,hpet4
23,43:
23,2678206
23,HPET_MSI-edge
23,hpet5
23,45:
23,PCI-MSI-edge
23,"aerdrv, P+"
23,46:
23,PCI-MSI-edge
23,"PCIe PME,+"
23,47:
23,PCI-MSI-edge
23,"PCIe PME,+"
23,48:
23,PCI-MSI-edge
23,"PCIe PME,+"
23,49:
23,387
23,PCI-MSI-edge
23,snd_hda_i+
23,50:
23,933117
23,PCI-MSI-edge
23,nvidia
23,NMI:
23,2102
23,2023
23,2031
23,1920
23,Non-maskable interrupts
23,LOC:
23,Local timer interrupts
23,SPU:
23,Spurious interrupts
23,PMI:
23,2102
23,2023
23,2031
23,1920
23,Performance monitoring int+
23,IWI:
23,47331
23,45725
23,52464
23,46775
23,IRQ work interrupts
23,RTR:
23,APIC ICR read retries
23,RES:
23,472911
23,396463
23,339792
23,323820
23,Rescheduling interrupts
23,CAL:
23,48389
23,47345
23,54113
23,50478
23,Function call interrupts
23,TLB:
23,28410
23,26804
23,24389
23,26157
23,TLB shootdowns
23,TRM:
23,Thermal event interrupts
23,THR:
23,Threshold APIC interrupts
23,MCE:
23,Machine check exceptions
23,MCP:
23,Machine check polls
23,ERR:
23,MIS:
23,The address assignment of executables and libraries is contained in the
23,maps file:
23,> cat /proc/self/maps
23,08048000-0804c000 r-xp 00000000 03:03 17753
23,/bin/cat
23,0804c000-0804d000 rw-p 00004000 03:03 17753
23,/bin/cat
23,0804d000-0806e000 rw-p 0804d000 00:00 0
23,[heap]
23,b7d27000-b7d5a000 r--p 00000000 03:03 11867
23,/usr/lib/locale/en_GB.utf8/
23,b7d5a000-b7e32000 r--p 00000000 03:03 11868
23,/usr/lib/locale/en_GB.utf8/
23,b7e32000-b7e33000 rw-p b7e32000 00:00 0
23,b7e33000-b7f45000 r-xp 00000000 03:03 8837
23,/lib/libc-2.3.6.so
23,b7f45000-b7f46000 r--p 00112000 03:03 8837
23,/lib/libc-2.3.6.so
23,b7f46000-b7f48000 rw-p 00113000 03:03 8837
23,/lib/libc-2.3.6.so
23,b7f48000-b7f4c000 rw-p b7f48000 00:00 0
23,b7f52000-b7f53000 r--p 00000000 03:03 11842
23,/usr/lib/locale/en_GB.utf8/
23,[...]
23,b7f5b000-b7f61000 r--s 00000000 03:03 9109
23,/usr/lib/gconv/gconv-module
23,b7f61000-b7f62000 r--p 00000000 03:03 9720
23,/usr/lib/locale/en_GB.utf8/
23,b7f62000-b7f76000 r-xp 00000000 03:03 8828
23,/lib/ld-2.3.6.so
23,b7f76000-b7f78000 rw-p 00013000 03:03 8828
23,/lib/ld-2.3.6.so
23,bfd61000-bfd76000 rw-p bfd61000 00:00 0
23,[stack]
23,ffffe000-fffff000 ---p 00000000 00:00 0
23,[vdso]
23,A lot more information can be obtained from the /proc file system. Some
23,important files and their contents are:
23,/proc/devices
23,Available devices
23,/proc/modules
23,Kernel modules loaded
23,/proc/cmdline
23,Kernel command line
23,/proc/meminfo
23,Detailed information about memory usage
23,/proc/config.gz
23,gzip-compressed configuration file of the kernel
23,currently running
23,/proc/PID/
23,Find information about processes currently running in the
23,"/proc/NNN directories,"
23,where NNN is the process ID (PID) of the
23,relevant process. Every process can find its own characteristics in
23,/proc/self/.
23,Further information is available in the text file
23,/usr/src/linux/Documentation/filesystems/proc.txt
23,(this file is available when the package
23,kernel-source is installed).
23,2.6.1 procinfo #
23,Important information from the /proc file system is
23,summarized by the command procinfo:
23,> procinfo
23,Linux 3.11.10-17-desktop (geeko@buildhost) (gcc 4.8.1 20130909) #1 4CPU [jupiter.example.com]
23,Memory:
23,Total
23,Used
23,Free
23,Shared
23,Buffers
23,Cached
23,Mem:
23,8181908
23,8000632
23,181276
23,85472
23,2850872
23,Swap:
23,10481660
23,1576
23,10480084
23,Bootup: Mon Jul 28 09:54:13 2014
23,Load average: 1.61 0.85 0.74 2/904 25949
23,user
23,1:54:41.84
23,12.7%
23,page in :
23,2107312
23,disk 1:
23,52212r
23,20199w
23,nice
23,0:00:00.46
23,0.0%
23,page out:
23,1714461
23,disk 2:
23,19387r
23,10928w
23,system:
23,0:25:38.00
23,2.8%
23,page act:
23,466673
23,disk 3:
23,548r
23,10w
23,IOwait:
23,0:04:16.45
23,0.4%
23,page dea:
23,272297
23,hw irq:
23,0:00:00.42
23,0.0%
23,page flt:
23,105754526
23,sw irq:
23,0:01:26.48
23,0.1%
23,swap in :
23,idle
23,12:14:43.65
23,81.5%
23,swap out:
23,394
23,guest :
23,0:02:18.59
23,0.2%
23,uptime:
23,3:45:22.24
23,context :
23,99809844
23,irq
23,121 timer
23,irq 41:
23,3238224 hpet3
23,irq
23,1 rtc0
23,irq 42:
23,3251898 hpet4
23,irq
23,0 acpi
23,irq 43:
23,3156368 hpet5
23,irq 16:
23,14589 ehci_hcd:usb1
23,irq 45:
23,"0 aerdrv, PCIe PME"
23,irq 18:
23,0 i801_smbus
23,irq 46:
23,"0 PCIe PME, pciehp"
23,irq 19:
23,"124861 ata_piix, ata_piix, f irq 47:"
23,"0 PCIe PME, pciehp"
23,irq 22:
23,3742817 enp5s1
23,irq 48:
23,"0 PCIe PME, pciehp"
23,irq 23:
23,479248 ehci_hcd:usb2
23,irq 49:
23,387 snd_hda_intel
23,irq 40:
23,3216894 hpet2
23,irq 50:
23,1088673 nvidia
23,"To see all the information, use the parameter -a. The"
23,parameter -nN produces updates of the information every
23,"N seconds. In this case, terminate the"
23,program by pressing Q.
23,"By default, the cumulative values are displayed. The parameter"
23,-d produces the differential values. procinfo
23,-dn5 displays the values that have changed in the last five
23,seconds:
23,2.6.2 System control parameters: /proc/sys/ #
23,System control parameters are used to modify the Linux kernel parameters
23,at runtime. They reside in /proc/sys/ and can be
23,viewed and modified with the sysctl command. To list
23,"all parameters, run sysctl -a. A"
23,single parameter can be listed with sysctl
23,PARAMETER_NAME.
23,Parameters are grouped into categories and can be listed with
23,sysctl CATEGORY or by
23,listing the contents of the respective directories. The most important
23,categories are listed below. The links to further readings require the
23,installation of the package
23,kernel-source.
23,sysctl dev (/proc/sys/dev/)
23,Device-specific information.
23,sysctl fs (/proc/sys/fs/)
23,"Used file handles, quotas, and other file system-oriented parameters."
23,For details see
23,/usr/src/linux/Documentation/sysctl/fs.txt.
23,sysctl kernel (/proc/sys/kernel/)
23,"Information about the task scheduler, system shared memory, and other"
23,kernel-related parameters. For details see
23,/usr/src/linux/Documentation/sysctl/kernel.txt
23,sysctl net (/proc/sys/net/)
23,"Information about network bridges, and general network parameters"
23,(mainly the ipv4/ subdirectory). For details see
23,/usr/src/linux/Documentation/sysctl/net.txt
23,sysctl vm (/proc/sys/vm/)
23,"Entries in this path relate to information about the virtual memory,"
23,"swapping, and caching. For details see"
23,/usr/src/linux/Documentation/sysctl/vm.txt
23,"To set or change a parameter for the current session, use the command"
23,sysctl -w
23,PARAMETER=VALUE.
23,"To permanently change a setting, add a line"
23,PARAMETER=VALUE to
23,/etc/sysctl.conf.
23,2.7 Hardware information #  2.7.1 PCI resources: lspci #  Note: Accessing PCI configuration.
23,Most operating systems require root user privileges to grant access to
23,the computer's PCI configuration.
23,The command lspci lists the PCI resources:
23,# lspci
23,00:00.0 Host bridge: Intel Corporation 82845G/GL[Brookdale-G]/GE/PE \
23,DRAM Controller/Host-Hub Interface (rev 01)
23,00:01.0 PCI bridge: Intel Corporation 82845G/GL[Brookdale-G]/GE/PE \
23,Host-to-AGP Bridge (rev 01)
23,00:1d.0 USB Controller: Intel Corporation 82801DB/DBL/DBM \
23,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #1 (rev 01)
23,00:1d.1 USB Controller: Intel Corporation 82801DB/DBL/DBM \
23,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #2 (rev 01)
23,00:1d.2 USB Controller: Intel Corporation 82801DB/DBL/DBM \
23,(ICH4/ICH4-L/ICH4-M) USB UHCI Controller #3 (rev 01)
23,00:1d.7 USB Controller: Intel Corporation 82801DB/DBM \
23,(ICH4/ICH4-M) USB2 EHCI Controller (rev 01)
23,00:1e.0 PCI bridge: Intel Corporation 82801 PCI Bridge (rev 81)
23,00:1f.0 ISA bridge: Intel Corporation 82801DB/DBL (ICH4/ICH4-L) \
23,LPC Interface Bridge (rev 01)
23,00:1f.1 IDE interface: Intel Corporation 82801DB (ICH4) IDE \
23,Controller (rev 01)
23,00:1f.3 SMBus: Intel Corporation 82801DB/DBL/DBM (ICH4/ICH4-L/ICH4-M) \
23,SMBus Controller (rev 01)
23,00:1f.5 Multimedia audio controller: Intel Corporation 82801DB/DBL/DBM \
23,(ICH4/ICH4-L/ICH4-M) AC'97 Audio Controller (rev 01)
23,"01:00.0 VGA compatible controller: Matrox Graphics, Inc. G400/G450 (rev 85)"
23,02:08.0 Ethernet controller: Intel Corporation 82801DB PRO/100 VE (LOM) \
23,Ethernet Controller (rev 81)
23,Using -v results in a more detailed listing:
23,# lspci -v
23,[...]
23,00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet \
23,Controller (rev 02)
23,Subsystem: Intel Corporation PRO/1000 MT Desktop Adapter
23,"Flags: bus master, 66MHz, medium devsel, latency 64, IRQ 19"
23,"Memory at f0000000 (32-bit, non-prefetchable) [size=128K]"
23,I/O ports at d010 [size=8]
23,Capabilities: [dc] Power Management version 2
23,Capabilities: [e4] PCI-X non-bridge device
23,Kernel driver in use: e1000
23,Kernel modules: e1000
23,Information about device name resolution is obtained from the file
23,/usr/share/pci.ids. PCI IDs not listed in this file
23,are marked “Unknown device.”
23,The parameter -vv produces all the information that
23,"could be queried by the program. To view the pure numeric values, use"
23,the parameter -n.
23,2.7.2 USB devices: lsusb #
23,The command lsusb lists all USB devices. With the
23,"option -v, print a more detailed list. The detailed"
23,information is read from the directory
23,/proc/bus/usb/. The following is the output of
23,"lsusb with these USB devices attached: hub, memory"
23,"stick, hard disk and mouse."
23,# lsusb
23,"Bus 004 Device 007: ID 0ea0:2168 Ours Technology, Inc. Transcend JetFlash \"
23,2.0 / Astone USB Drive
23,Bus 004 Device 006: ID 04b4:6830 Cypress Semiconductor Corp. USB-2.0 IDE \
23,Adapter
23,"Bus 004 Device 005: ID 05e3:0605 Genesys Logic, Inc."
23,Bus 004 Device 001: ID 0000:0000
23,Bus 003 Device 001: ID 0000:0000
23,Bus 002 Device 001: ID 0000:0000
23,"Bus 001 Device 005: ID 046d:c012 Logitech, Inc. Optical Mouse"
23,Bus 001 Device 001: ID 0000:00002.7.3
23,Monitoring and tuning the thermal subsystem: tmon
23,"tmon is a tool to help visualize, tune, and test the"
23,"complex thermal subsystem. When started without parameters,"
23,tmon runs in monitoring mode:
23,┌──────THERMAL ZONES(SENSORS)──────────────────────────────┐
23,│Thermal Zones:
23,acpitz00
23,│Trip Points:
23,└──────────────────────────────────────────────────────────┘
23,┌─────────── COOLING DEVICES ──────────────────────────────┐
23,│ID
23,Cooling Dev
23,Cur
23,Max
23,Thermal Zone Binding
23,│00
23,Processor
23,││││││││││││
23,│01
23,Processor
23,││││││││││││
23,│02
23,Processor
23,││││││││││││
23,│03
23,Processor
23,││││││││││││
23,│04 intel_powerc
23,││││││││││││
23,└──────────────────────────────────────────────────────────┘
23,┌──────────────────────────────────────────────────────────┐
23,40 │
23,│acpitz 0:[
23,8][>>>>>>>>>P9
23,C31
23,└──────────────────────────────────────────────────────────┘
23,┌────────────────── CONTROLS ──────────────────────────────┐
23,│PID gain: kp=0.36 ki=5.00 kd=0.19 Output 0.00
23,"│Target Temp: 65.0C, Zone: 0, Control Device: None"
23,└──────────────────────────────────────────────────────────┘
23,Ctrl-c - Quit
23,TAB - Tuning
23,"For detailed information on how to interpret the data, how to log thermal"
23,data and how to use tmon to test and tune cooling
23,"devices and sensors, refer to the man page: man 8"
23,tmon. The package tmon is not installed by
23,default.
23,2.7.4 MCELog: machine check exceptions (MCE) #  Note: AvailabilityThis tool is only available on AMD64/Intel 64 systems.
23,The mcelog package logs and
23,"parses/translates Machine Check Exceptions (MCE) on hardware errors, including"
23,"I/O, CPU, and memory errors. In addition, mcelog handles predictive bad page"
23,offlining and automatic core offlining when cache errors happen.
23,Formerly this was managed by a cron job executed hourly. Now hardware
23,errors are immediately processed by an mcelog daemon.
23,Note: Support for AMD scalable MCA
23,SUSE Linux Enterprise Server supports AMD's Scalable Machine Check
23,Architecture (Scalable MCA). Scalable MCA improves hardware error
23,reporting in AMD Zen processors. It expands information logged in
23,MCA banks for improved error handling and better diagnosability.
23,mcelog captures MCA messages
23,(rasdaemon and
23,dmesg also capture MCA messages).
23,"See section 3.1, Machine Check Architecture of"
23,Processor Programming Reference (PPR) for AMD Family
23,"17h Model 01h, Revision B1 Processors for detailed"
23,"information,"
23,http://developer.amd.com/wordpress/media/2017/11/54945_PPR_Family_17h_Models_00h-0Fh.pdf.
23,mcelog is configured in /etc/mcelog/mcelog.conf.
23,Configuration options are documented in
23,"man mcelog, and at"
23,http://mcelog.org/. The following example shows
23,only changes to the default file:
23,daemon = yes
23,filter = yes
23,filter-memory-errors = yes
23,no-syslog = yes
23,logfile = /var/log/mcelog
23,run-credentials-user = root
23,run-credentials-group = nobody
23,client-group = root
23,socket-path = /var/run/mcelog-client
23,The mcelog service is not enabled by default. The service can either be
23,"enabled and started via the YaST system services editor, or via command line:"
23,# systemctl enable mcelog
23,# systemctl start mcelog2.7.5 AMD64/Intel 64: dmidecode: DMI table decoder #
23,dmidecode shows the machine's DMI table containing
23,information such as serial numbers and BIOS revisions of the hardware.
23,# dmidecode
23,# dmidecode 2.12
23,SMBIOS 2.5 present.
23,27 structures occupying 1298 bytes.
23,Table at 0x000EB250.
23,"Handle 0x0000, DMI type 4, 35 bytes"
23,Processor Information
23,Socket Designation: J1PR
23,Type: Central Processor
23,Family: Other
23,Manufacturer: Intel(R) Corporation
23,ID: E5 06 01 00 FF FB EB BF
23,Version: Intel(R) Core(TM) i5 CPU
23,750
23,@ 2.67GHz
23,Voltage: 1.1 V
23,External Clock: 133 MHz
23,Max Speed: 4000 MHz
23,Current Speed: 2667 MHz
23,"Status: Populated, Enabled"
23,Upgrade: Other
23,L1 Cache Handle: 0x0004
23,L2 Cache Handle: 0x0003
23,L3 Cache Handle: 0x0001
23,Serial Number: Not Specified
23,Asset Tag: Not Specified
23,Part Number: Not Specified
23,[..]2.7.6 POWER: list hardware #
23,lshw extracts and displays the hardware
23,configuration of the machine.
23,2.8 Files and file systems #
23,"For file system-specific information, refer to"
23,Book “Storage Administration Guide”.
23,2.8.1 Determine the file type: file #
23,The command file determines the type of a file or a
23,list of files by checking /usr/share/misc/magic.
23,> file /usr/bin/file
23,"/usr/bin/file: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), \"
23,"for GNU/Linux 2.6.4, dynamically linked (uses shared libs), stripped"
23,The parameter -f LIST
23,specifies a file with a list of file names to examine. The
23,-z allows file to look inside
23,compressed files:
23,> file /usr/share/man/man1/file.1.gz
23,"/usr/share/man/man1/file.1.gz: gzip compressed data, from Unix, max compression"
23,> file -z /usr/share/man/man1/file.1.gz
23,/usr/share/man/man1/file.1.gz: troff or preprocessor input text \
23,"(gzip compressed data, from Unix, max compression)"
23,The parameter -i outputs a mime type string rather than
23,the traditional description.
23,> file -i /usr/share/misc/magic
23,"/usr/share/misc/magic: text/plain charset=utf-82.8.2 File systems and their usage: mount, df and du #"
23,The command mount shows which file system (device and
23,type) is mounted at which mount point:
23,# mount
23,"/dev/sda2 on / type ext4 (rw,acl,user_xattr)"
23,proc on /proc type proc (rw)
23,sysfs on /sys type sysfs (rw)
23,debugfs on /sys/kernel/debug type debugfs (rw)
23,"devtmpfs on /dev type devtmpfs (rw,mode=0755)"
23,"tmpfs on /dev/shm type tmpfs (rw,mode=1777)"
23,"devpts on /dev/pts type devpts (rw,mode=0620,gid=5)"
23,/dev/sda3 on /home type ext3 (rw)
23,securityfs on /sys/kernel/security type securityfs (rw)
23,fusectl on /sys/fs/fuse/connections type fusectl (rw)
23,gvfs-fuse-daemon on /home/tux/.gvfs type fuse.gvfs-fuse-daemon \
23,"(rw,nosuid,nodev,user=tux)"
23,Obtain information about total usage of the file systems with the
23,command df. The parameter -h (or
23,--human-readable) transforms the output into a form
23,understandable for common users.
23,> df -h
23,Filesystem
23,Size
23,Used Avail Use% Mounted on
23,/dev/sda2
23,20G
23,"5,9G"
23,13G
23,32% /
23,devtmpfs
23,"1,6G"
23,236K
23,"1,6G"
23,1% /dev
23,tmpfs
23,"1,6G"
23,668K
23,"1,6G"
23,1% /dev/shm
23,/dev/sda3
23,208G
23,40G
23,159G
23,20% /home
23,Display the total size of all the files in a given directory and its
23,subdirectories with the command du. The parameter
23,-s suppresses the output of detailed information and
23,gives only a total for each argument. -h again
23,transforms the output into a human-readable form:
23,> du -sh /opt
23,192M
23,/opt2.8.3 Additional information about ELF binaries #
23,Read the content of binaries with the readelf
23,utility. This even works with ELF files that were built for other
23,hardware architectures:
23,> readelf --file-header /bin/ls
23,ELF Header:
23,Magic:
23,7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
23,Class:
23,ELF64
23,Data:
23,"2's complement, little endian"
23,Version:
23,1 (current)
23,OS/ABI:
23,UNIX - System V
23,ABI Version:
23,Type:
23,EXEC (Executable file)
23,Machine:
23,Advanced Micro Devices X86-64
23,Version:
23,0x1
23,Entry point address:
23,0x402540
23,Start of program headers:
23,64 (bytes into file)
23,Start of section headers:
23,95720 (bytes into file)
23,Flags:
23,0x0
23,Size of this header:
23,64 (bytes)
23,Size of program headers:
23,56 (bytes)
23,Number of program headers:
23,Size of section headers:
23,64 (bytes)
23,Number of section headers:
23,Section header string table index: 312.8.4 File properties: stat #
23,The command stat displays file properties:
23,> stat /etc/profile
23,File: `/etc/profile'
23,Size: 9662
23,Blocks: 24
23,IO Block: 4096
23,regular file
23,Device: 802h/2050d
23,Inode: 132349
23,Links: 1
23,Access: (0644/-rw-r--r--)
23,Uid: (
23,root)
23,Gid: (
23,root)
23,Access: 2009-03-20 07:51:17.000000000 +0100
23,Modify: 2009-01-08 19:21:14.000000000 +0100
23,Change: 2009-03-18 12:55:31.000000000 +0100
23,The parameter --file-system produces details of the
23,properties of the file system in which the specified file is located:
23,> stat /etc/profile --file-system
23,"File: ""/etc/profile"""
23,ID: d4fb76e70b4d1746 Namelen: 255
23,Type: ext2/ext3
23,Block size: 4096
23,Fundamental block size: 4096
23,Blocks: Total: 2581445
23,Free: 1717327
23,Available: 1586197
23,Inodes: Total: 655776
23,Free: 4903122.9 User information #  2.9.1 User accessing files: fuser #
23,It can be useful to determine what processes or users are currently
23,"accessing certain files. Suppose, for example, you want to unmount a"
23,file system mounted at /mnt.
23,"umount returns ""device is busy."" The command"
23,fuser can then be used to determine what processes
23,are accessing the device:
23,> fuser -v /mnt/*
23,USER
23,PID ACCESS COMMAND
23,/mnt/notes.txt
23,tux
23,26597 f....
23,less
23,"Following termination of the less process, which was"
23,"running on another terminal, the file system can successfully be"
23,"unmounted. When used with -k option,"
23,fuser will terminate processes accessing the file as
23,well.
23,2.9.2 Who is doing what: w #
23,"With the command w, find out who is logged in to the"
23,system and what each user is doing. For example:
23,> w
23,"16:00:59 up 1 day,"
23,"2:41,"
23,"3 users,"
23,"load average: 0.00, 0.01, 0.05"
23,USER
23,TTY
23,FROM
23,LOGIN@
23,IDLE
23,JCPU
23,PCPU WHAT
23,tux
23,console
23,Wed13
23,?xdm?
23,8:15
23,0.03s /usr/lib/gdm/gd
23,tux
23,console
23,Wed13
23,26:41m
23,0.00s
23,0.03s /usr/lib/gdm/gd
23,tux
23,pts/0
23,Wed13
23,20:11
23,0.10s
23,2.89s /usr/lib/gnome-
23,"If any users of other systems have logged in remotely, the parameter"
23,-f shows the computers from which they have established
23,the connection.
23,2.10 Time and date #  2.10.1 Time measurement with time #
23,Determine the time spent by commands with the time
23,utility. This utility is available in two versions: as a Bash built-in
23,and as a program (/usr/bin/time).
23,> time find . > /dev/null
23,real
23,0m4.051s1
23,user
23,0m0.042s2
23,sys
23,0m0.205s31
23,The real time that elapsed from the command's start-up until it
23,finished.
23,CPU time of the user as reported by the times
23,system call.
23,CPU time of the system as reported by the times
23,system call.
23,The output of /usr/bin/time is much more detailed.
23,It is recommended to run it with the -v switch to
23,produce human-readable output.
23,/usr/bin/time -v find . > /dev/null
23,"Command being timed: ""find ."""
23,User time (seconds): 0.24
23,System time (seconds): 2.08
23,Percent of CPU this job got: 25%
23,Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.03
23,Average shared text size (kbytes): 0
23,Average unshared data size (kbytes): 0
23,Average stack size (kbytes): 0
23,Average total size (kbytes): 0
23,Maximum resident set size (kbytes): 2516
23,Average resident set size (kbytes): 0
23,Major (requiring I/O) page faults: 0
23,Minor (reclaiming a frame) page faults: 1564
23,Voluntary context switches: 36660
23,Involuntary context switches: 496
23,Swaps: 0
23,File system inputs: 0
23,File system outputs: 0
23,Socket messages sent: 0
23,Socket messages received: 0
23,Signals delivered: 0
23,Page size (bytes): 4096
23,Exit status: 02.11 Graph your data: RRDtool #
23,"There are a lot of data in the world around you, which can be easily"
23,"measured in time. For example, changes in the temperature, or the number"
23,of data sent or received by your computer's network interface. RRDtool
23,can help you store and visualize such data in detailed and customizable
23,graphs.
23,RRDtool is available for most Unix platforms and Linux distributions.
23,SUSE® Linux Enterprise Server ships RRDtool as well. Install it either with
23,YaST or by entering
23,zypper install
23,rrdtool in the command line as root.
23,Tip: Bindings
23,"There are Perl, Python, Ruby, and PHP bindings available for RRDtool, so"
23,that you can write your own monitoring scripts in your preferred
23,scripting language.
23,2.11.1 How RRDtool works #
23,RRDtool is an abbreviation of Round Robin Database
23,tool. Round Robin is a method for
23,manipulating with a constant amount of data. It uses the principle of a
23,"circular buffer, where there is no end nor beginning to the data row"
23,which is being read. RRDtool uses Round Robin Databases to store and
23,read its data.
23,"As mentioned above, RRDtool is designed to work with data that change in"
23,time. The ideal case is a sensor which repeatedly reads measured data
23,"(like temperature, speed etc.) in constant periods of time, and then"
23,exports them in a given format. Such data are perfectly ready for
23,"RRDtool, and it is easy to process them and create the desired output."
23,Sometimes it is not possible to obtain the data automatically and
23,regularly. Their format needs to be pre-processed before it is supplied
23,"to RRDtool, and often you need to manipulate RRDtool even manually."
23,The following is a simple example of basic RRDtool usage. It illustrates
23,all three important phases of the usual RRDtool workflow:
23,"creating a database, updating"
23,"measured values, and viewing the output."
23,2.11.2 A practical example #
23,Suppose we want to collect and view information about the memory usage
23,in the Linux system as it changes in time. To make the example more
23,"vivid, we measure the currently free memory over a period of 40 seconds"
23,in 4-second intervals. Three applications that usually consume a lot of
23,"system memory are started and closed: the Firefox Web browser, the"
23,"Evolution e-mail client, and the Eclipse development framework."
23,2.11.2.1 Collecting data #
23,RRDtool is very often used to measure and visualize network traffic. In
23,"such case, the Simple Network Management Protocol (SNMP) is used. This"
23,protocol can query network devices for relevant values of their
23,internal counters. Exactly these values are to be stored with RRDtool.
23,"For more information on SNMP, see"
23,http://www.net-snmp.org/.
23,Our situation is different—we need to obtain the data
23,manually. A helper script free_mem.sh repetitively
23,reads the current state of free memory and writes it to the standard
23,output.
23,> cat free_mem.sh
23,INTERVAL=4
23,for steps in {1..10}
23,DATE=`date +%s`
23,"FREEMEM=`free -b | grep ""Mem"" | awk '{ print $4 }'`"
23,sleep $INTERVAL
23,"echo ""rrdtool update free_mem.rrd $DATE:$FREEMEM"""
23,done
23,"The time interval is set to 4 seconds, and is implemented with the"
23,sleep command.
23,RRDtool accepts time information in a special format - so called
23,Unix time. It is defined as the number of
23,"seconds since the midnight of January 1, 1970 (UTC). For example,"
23,1272907114 represents 2010-05-03 17:18:34.
23,The free memory information is reported in bytes with
23,free -b. Prefer to supply basic
23,units (bytes) instead of multiple units (like kilobytes).
23,The line with the echo ... command contains the
23,"future name of the database file (free_mem.rrd),"
23,and together creates a command line for updating
23,RRDtool values.
23,"After running free_mem.sh, you see an output similar"
23,to this:
23,> sh free_mem.sh
23,rrdtool update free_mem.rrd 1272974835:1182994432
23,rrdtool update free_mem.rrd 1272974839:1162817536
23,rrdtool update free_mem.rrd 1272974843:1096269824
23,rrdtool update free_mem.rrd 1272974847:1034219520
23,rrdtool update free_mem.rrd 1272974851:909438976
23,rrdtool update free_mem.rrd 1272974855:832454656
23,rrdtool update free_mem.rrd 1272974859:829120512
23,rrdtool update free_mem.rrd 1272974863:1180377088
23,rrdtool update free_mem.rrd 1272974867:1179369472
23,rrdtool update free_mem.rrd 1272974871:1181806592
23,It is convenient to redirect the command's output to a file with
23,sh free_mem.sh > free_mem_updates.log
23,to simplify its future execution.
23,2.11.2.2 Creating the database #
23,Create the initial Robin Round database for our example with the
23,following command:
23,> rrdtool create free_mem.rrd --start 1272974834 --step=4 \
23,DS:memory:GAUGE:600:U:U RRA:AVERAGE:0.5:1:24Points to notice #
23,This command creates a file called free_mem.rrd
23,for storing our measured values in a Round Robin type database.
23,The --start option specifies the time (in Unix time)
23,"when the first value will be added to the database. In this example,"
23,it is one less than the first time value of the
23,free_mem.sh output (1272974835).
23,The --step specifies the time interval in seconds
23,with which the measured data will be supplied to the database.
23,The DS:memory:GAUGE:600:U:U part introduces a new
23,data source for the database. It is called
23,"memory, its type is gauge,"
23,"the maximum number between two updates is 600 seconds, and the"
23,minimal and maximal value
23,in the measured range are unknown (U).
23,RRA:AVERAGE:0.5:1:24 creates Round Robin archive
23,(RRA) whose stored data are processed with the
23,consolidation functions (CF) that calculates the
23,average of data points. 3 arguments of the
23,consolidation function are appended to the end of the line.
23,"If no error message is displayed, then"
23,free_mem.rrd database is created in the current
23,directory:
23,> ls -l free_mem.rrd
23,-rw-r--r-- 1 tux users 776 May
23,5 12:50 free_mem.rrd2.11.2.3 Updating database values #
23,"After the database is created, you need to fill it with the measured"
23,"data. In Section 2.11.2.1, “Collecting data”, we already"
23,prepared the file free_mem_updates.log which
23,consists of rrdtool update commands. These commands
23,do the update of database values for us.
23,> sh free_mem_updates.log; ls -l free_mem.rrd
23,-rw-r--r--
23,1 tux users
23,776 May
23,5 13:29 free_mem.rrd
23,"As you can see, the size of free_mem.rrd remained"
23,the same even after updating its data.
23,2.11.2.4 Viewing measured values #
23,"We have already measured the values, created the database, and stored"
23,"the measured value in it. Now we can play with the database, and"
23,retrieve or view its values.
23,"To retrieve all the values from our database, enter the following on"
23,the command line:
23,> rrdtool fetch free_mem.rrd AVERAGE --start 1272974830 \
23,--end 1272974871
23,memory
23,1272974832: nan
23,1272974836: 1.1729059840e+09
23,1272974840: 1.1461806080e+09
23,1272974844: 1.0807572480e+09
23,1272974848: 1.0030243840e+09
23,1272974852: 8.9019289600e+08
23,1272974856: 8.3162112000e+08
23,1272974860: 9.1693465600e+08
23,1272974864: 1.1801251840e+09
23,1272974868: 1.1799787520e+09
23,1272974872: nanPoints to notice #
23,AVERAGE will fetch average value points from the
23,"database, because only one data source is defined"
23,"(Section 2.11.2.2, “Creating the database”) with"
23,AVERAGE processing and no other function is
23,available.
23,The first line of the output prints the name of the data source as
23,"defined in Section 2.11.2.2, “Creating the database”."
23,"The left results column represents individual points in time, while"
23,the right one represents corresponding measured average values in
23,scientific notation.
23,The nan in the last line stands for “not a
23,number”.
23,Now a graph representing the values stored in the database is drawn:
23,> rrdtool graph free_mem.png \
23,--start 1272974830 \
23,--end 1272974871 \
23,--step=4 \
23,DEF:free_memory=free_mem.rrd:memory:AVERAGE \
23,LINE2:free_memory#FF0000 \
23,"--vertical-label ""GB"" \"
23,"--title ""Free System Memory in Time"" \"
23,--zoom 1.5 \
23,--x-grid SECOND:1:SECOND:4:SECOND:10:0:%XPoints to notice #
23,free_mem.png is the file name of the graph to be
23,created.
23,--start and --end limit the time
23,range within which the graph will be drawn.
23,--step specifies the time resolution (in seconds) of
23,the graph.
23,The DEF:... part is a data definition called
23,free_memory. Its data are read from the
23,free_mem.rrd database and its data source called
23,memory. The average value
23,"points are calculated, because no others were defined in"
23,"Section 2.11.2.2, “Creating the database”."
23,The LINE... part specifies properties of the line
23,"to be drawn into the graph. It is 2 pixels wide, its data come from"
23,"the free_memory definition, and its color is"
23,red.
23,--vertical-label sets the label to be printed along
23,"the y axis, and --title sets"
23,the main label for the whole graph.
23,--zoom specifies the zoom factor for the graph. This
23,value must be greater than zero.
23,--x-grid specifies how to draw grid lines and their
23,"labels into the graph. Our example places them every second, while"
23,major grid lines are placed every 4 seconds. Labels are placed every
23,10 seconds under the major grid lines.
23,Figure 2.1: Example graph created with RRDtool #  2.11.3 More information #
23,RRDtool is a very complex tool with a lot of sub-commands and command
23,"line options. Some are easy to understand, but to make it"
23,produce the results you want and fine-tune them according to your liking
23,may require a lot of effort.
23,Apart from RRDtool's man page (man 1 rrdtool) which
23,"gives you only basic information, you should have a look at the"
23,RRDtool home
23,page. There is a detailed
23,documentation
23,of the rrdtool command and all its sub-commands.
23,There are also several
23,tutorials
23,to help you understand the common RRDtool workflow.
23,"If you are interested in monitoring network traffic, have a look at"
23,MRTG (Multi Router
23,Traffic Grapher). MRTG can graph the activity of many network
23,devices. It can use RRDtool.
23,3 System log files #
23,System log file analysis is one of the most important tasks when analyzing
23,"the system. In fact, looking at the system log files should be the first"
23,thing to do when maintaining or troubleshooting a system. SUSE Linux Enterprise Server
23,automatically logs almost everything that happens on the system in detail.
23,"Since the move to systemd, kernel messages and messages of system"
23,services registered with systemd are logged in systemd journal
23,"(see Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”). Other log files (mainly those of"
23,system applications) are written in plain text and can be easily read
23,using an editor or pager. It is also possible to parse them using scripts.
23,This allows you to filter their content.
23,3.1 System log files in /var/log/ #
23,System log files are always located under the
23,/var/log directory. The following list presents an
23,overview of all system log files from SUSE Linux Enterprise Server present after a
23,"default installation. Depending on your installation scope,"
23,/var/log also contains log files from other services
23,and applications not listed here. Some files and directories described
23,"below are “placeholders” and are only used, when the"
23,corresponding application is installed. Most log files are only visible
23,for the user root.
23,apparmor/
23,"AppArmor log files. For more information about AppArmor, see"
23,Book “Security and Hardening Guide”.
23,audit/
23,Logs from the audit framework. See Book “Security and Hardening Guide” for
23,details.
23,ConsoleKit/
23,Logs of the ConsoleKit daemon
23,(daemon for tracking what users are logged in and how they interact
23,with the computer).
23,cups/
23,Access and error logs of the Common Unix Printing System
23,(cups).
23,firewall
23,Firewall logs.
23,gdm/
23,Log files from the GNOME display manager.
23,krb5/
23,Log files from the Kerberos network authentication system.
23,lastlog
23,A database containing information on the last login of each user. Use
23,the command lastlog to view. See man 8
23,lastlog for more information.
23,localmessages
23,"Log messages of some boot scripts, for example the log of the DHCP"
23,client.
23,mail*
23,"Mail server (postfix,"
23,sendmail) logs.
23,messages
23,This is the default place where all kernel and system log messages go
23,and should be the first place (along with
23,/var/log/warn) to look at in case of problems.
23,NetworkManager
23,NetworkManager log files.
23,news/
23,Log messages from a news server.
23,chrony/
23,Logs from the Network Time Protocol daemon
23,(chrony).
23,pk_backend_zypp*
23,PackageKit (with libzypp
23,back-end) log files.
23,samba/
23,"Log files from Samba, the Windows SMB/CIFS file server."
23,warn
23,Log of all system warnings and errors. This should be the first place
23,(along with the output of the systemd journal) to look in case of
23,problems.
23,wtmp
23,"Database of all login/logout activities,"
23,and remote connections. Use the command last to
23,view. See man 1 last for more information.
23,Xorg.NUMBER.log
23,X.Org start-up log file. Refer to these files in case you have
23,problems starting X.Org.
23,The NUMBER in the file name is the
23,"display number. For example, the default Xorg.0.log is the log for display number"
23,"0, and Xorg.1.log is the log for display number"
23,1. Copies from previous X.Org starts are named as Xorg.NUMBER.log.old.
23,Note
23,The X.Org log files are available in the
23,/var/log/ directory
23,only if you start an X.Org session as root.
23,"If you start an X.Org session as any other user, you can locate the log files in the ~/.local/share/xorg/ directory."
23,YaST2/
23,All YaST log files.
23,zypp/
23,libzypp log files. Refer to
23,these files for the package installation history.
23,zypper.log
23,Logs from the command line installer zypper.
23,3.2 Viewing and parsing log files #
23,"To view log files, you can use any text editor. There is also a simple"
23,YaST module for viewing the system log available in the YaST
23,control center under Miscellaneous › System Log.
23,"For viewing log files in a text console, use the commands"
23,less or more. Use
23,head and tail to view the beginning
23,or end of a log file. To view entries appended to a log file in real-time
23,use tail -f. For information about
23,"how to use these tools, see their man pages."
23,To search for strings or regular expressions in log files use
23,grep. awk is useful for parsing and
23,rewriting log files.
23,3.3 Managing log files with logrotate #
23,Log files under /var/log grow on a daily basis and
23,quickly become very large. logrotate is a tool that
23,helps you manage log files and their growth. It allows automatic
23,"rotation, removal, compression, and mailing of log files. Log files can"
23,"be handled periodically (daily, weekly, or monthly) or when exceeding a"
23,particular size.
23,"logrotate is usually run daily by systemd,"
23,"and thus usually modifies log files only once a day. However, exceptions"
23,"occur when a log file is modified because of its size, if"
23,"logrotate is run multiple times a day, or if"
23,--force is enabled.
23,Use
23,/var/lib/misc/logrotate.status to find out when a
23,particular file was last rotated.
23,The main configuration file of logrotate is
23,/etc/logrotate.conf. System packages and
23,"programs that produce log files (for example,"
23,apache2) put their own
23,configuration files in the /etc/logrotate.d/
23,directory. The content of /etc/logrotate.d/ is
23,included via /etc/logrotate.conf.
23,Example 3.1: Example for /etc/logrotate.conf #
23,"# see ""man logrotate"" for details"
23,# rotate log files weekly
23,weekly
23,# keep 4 weeks worth of backlogs
23,rotate 4
23,# create new (empty) log files after rotating old ones
23,create
23,# use date as a suffix of the rotated file
23,dateext
23,# uncomment this if you want your log files compressed
23,#compress
23,# comment these to switch compression to use gzip or another
23,# compression scheme
23,compresscmd /usr/bin/bzip2
23,uncompresscmd /usr/bin/bunzip2
23,# RPM packages drop log rotation information into this directory
23,include /etc/logrotate.dImportant: Avoid permission conflicts
23,The create option pays heed to the modes and
23,ownership of files specified in /etc/permissions*.
23,"If you modify these settings, make sure no conflicts arise."
23,3.4 Monitoring log files with logwatch #
23,"logwatch is a customizable, pluggable log-monitoring"
23,"script. It parses system logs, extracts the important information and"
23,presents them in a human readable manner. To use
23,"logwatch, install the"
23,logwatch package.
23,logwatch can either be used at the command line to
23,"generate on-the-fly reports, or via cron to regularly create custom"
23,"reports. Reports can either be printed on the screen, saved to a file, or"
23,be mailed to a specified address. The latter is especially useful when
23,automatically generating reports via cron.
23,"On the command line, you can tell logwatch for which"
23,service and time span to generate a report and how much detail should be
23,included:
23,# Detailed report on all kernel messages from yesterday
23,logwatch --service kernel --detail High --range Yesterday --print
23,# Low detail report on all sshd events recorded (incl. archived logs)
23,logwatch --service sshd --detail Low --range All --archives --print
23,# Mail a report on all smartd messages from May 5th to May 7th to root@localhost
23,logwatch --service smartd --range 'between 5/5/2005 and 5/7/2005' \
23,--mailto root@localhost --print
23,The --range option has got a complex syntax—see
23,logwatch --range help for details. A
23,list of all services that can be queried is available with the following
23,command:
23,> ls /usr/share/logwatch/default.conf/services/ | sed 's/\.conf//g'
23,"logwatch can be customized to great detail. However,"
23,the default configuration should usually be sufficient. The default
23,configuration files are located under
23,/usr/share/logwatch/default.conf/. Never change them
23,because they would get overwritten again with the next update. Rather
23,place custom configuration in /etc/logwatch/conf/
23,"(you may use the default configuration file as a template, though). A"
23,detailed HOWTO on customizing logwatch is available at
23,/usr/share/doc/packages/logwatch/HOWTO-Customize-LogWatch.
23,The following configuration files exist:
23,logwatch.conf
23,The main configuration file. The default version is extensively
23,commented. Each configuration option can be overwritten on the command
23,line.
23,ignore.conf
23,Filter for all lines that should globally be ignored by
23,logwatch.
23,services/*.conf
23,The service directory holds configuration files for each service you
23,can generate a report for.
23,logfiles/*.conf
23,Specifications on which log files should be parsed for each service.
23,3.5 Configuring mail forwarding for root #
23,"System daemons, cron jobs, systemd"
23,"timers, and other applications can generate messages and send them to the"
23,"root user of the system. By default, each user account owns a local"
23,mailbox and will be notified about new mail messages upon login.
23,These messages can contain security relevant reports and incidents that might
23,require a quick response by the system administrator. To get notified about
23,"these messages in a timely fashion, it is strongly recommended to forward"
23,these mails to a dedicated remote email account that is regularly checked.
23,Procedure 3.1: Configure mail forwarding for the root user #
23,"To forward mail for the root user, perform the following steps:"
23,Install the yast2-mail package:
23,# zypper in yast2-mail
23,Run the interactive YaST mail configuration:
23,# yast mail
23,Choose Permanent as Connection type
23,and proceed with Next.
23,Enter the address of the Outgoing mail server. If
23,"necessary, configure Authentication. It is strongly"
23,recommended to Enforce TLS encryption
23,to prevent
23,potentially sensitive system data from being sent unencrypted over the
23,network. Proceed with Next.
23,Enter the email address to Forward root's mail to and
23,Finish the configuration.
23,Important: Do not accept remote SMTP connections
23,Do not enable Accept remote SMTP
23,"connections, otherwise the local machine will act as a mail"
23,relay.
23,Send a message to test whether mail forwarding works correctly:
23,> mail root
23,subject: test
23,test
23,Use the mailq command to verify that the test message
23,"has been sent. Upon success, the queue should be empty. The message should"
23,be received by the dedicated mail address configured previously.
23,Depending on the number of managed machines and the number of persons who
23,"need to be informed about system events, different email address models can"
23,be established:
23,Collect messages from different systems in an email account that is only
23,accessed by a single person.
23,Collect messages from different systems in a group email account (aliases
23,or mailing list) that can be accessed by all relevant persons.
23,Create separate email accounts for each system.
23,It is crucial that administrators regularly check the related email accounts.
23,"To facilitate this effort and identify impoetant events, avoid sending"
23,unnecessary information. Configure applications to only send relevant
23,information.
23,3.6 Forwarding log messages to a central syslog server #
23,System log data can be forwarded from individual systems to a central
23,syslog server on the network. This allows administrators to get an overview
23,"of events on all hosts, and prevents attackers that succeed in taking over a"
23,system from manipulating system logs to cover their tracks.
23,Setting up a central syslog server consists of two parts. First you configure
23,"the central log server, then the clients for remote logging."
23,3.6.1 Set up the central syslog server #  Procedure 3.2: Configure the central rsyslog server #
23,"To set up a central syslog server, perform the following steps:"
23,Edit the configuration file
23,/etc/rsyslog.d/remote.conf.
23,Uncomment the following lines in the UDP Syslog Server
23,or TCP Syslog Server section of the configuration file.
23,Assign an IP address and port for rsyslogd.
23,TCP example:
23,$ModLoad imtcp.so
23,$UDPServerAddress IP1
23,$InputTCPServerRun PORT2
23,UDP example:
23,$ModLoad imudp.so
23,$UDPServerAddress IP1
23,$UDPServerRun PORT21
23,IP address of the interface for rsyslogd to listen on. If no address is
23,"given, the daemon listens on all interfaces."
23,Port for rsyslogd to listen on.
23,Select a privileged port below 1024. The default is 514.
23,Important: TCP versus UDP protocol
23,Traditionally syslog uses the UDP protocol to transmit log messages over
23,"the network. This involves less overhead, but lacks reliability. Log"
23,messages can get lost under high load.
23,The TCP protocol is more reliable and should be preferred over UDP.
23,Note: UDPServerAddress with TCP
23,The $UDPServerAddress configuration parameter in the
23,TCP example is no error. Despite its name it is used for both TCP and
23,UDP.
23,Save the file.
23,Restart the rsyslog service:
23,> sudo systemctl restart rsyslog.serviceOpen the respective port in the firewall. For firewalld with TCP on port 514 run:
23,> sudo firewall-cmd --add-port 514/tcp --permanent
23,> sudo firewall-cmd --reload
23,"You have now configured the central syslog server. Next, configure clients"
23,for remote logging.
23,3.6.2 Set up the client machines #  Procedure 3.3: Configure a rsyslog instance for remote logging #
23,"To configure a machine for remote logging on a central syslog server,"
23,perform the following steps:
23,Edit the configuration file
23,/etc/rsyslog.d/remote.conf.
23,Uncomment the appropriate line (TCP or UDP) and replace
23,remote-host with the address of the central syslog
23,"server set up in Section 3.6.1, “Set up the central syslog server”."
23,TCP example:
23,# Remote Logging using TCP for reliable delivery
23,"# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional"
23,*.* @@remote-host
23,UDP example:
23,# Remote Logging using UDP
23,"# remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional"
23,*.* @remote-host
23,Save the file.
23,Restart the rsyslog service:
23,> sudo systemctl restart rsyslog.service
23,Verify the proper function of the syslog forwarding:
23,"> logger ""hello world"""
23,The log message hello world should now appear on the
23,central syslog server.
23,You have now configured a system for remote logging to your central syslog
23,server. Repeat this procedure for all systems that should log remotely.
23,3.6.3 More information #
23,This basic setup does not include encryption and is only suitable for
23,"trusted internal networks. TLS encryption is strongly recommended, but"
23,requires a certificate infrastructure.
23,"In this configuration, all messages from remote hosts will be treated the"
23,same on the central syslog server. Consider filtering messages into separate
23,files by remote host or classify them by message category.
23,"For more information about encryption, filtering, and other advanced topics,"
23,consult the RSyslog documentation at
23,https://www.rsyslog.com/doc/master/index.html#manual.
23,3.7 Using logger to make system log entries #
23,logger is a tool for making entries in the system log.
23,It provides a shell command interface to the rsyslogd system log module.
23,"For example, the following line outputs its message in"
23,/var/log/messages or directly in the journal (if no
23,logging facility is running):
23,"> logger -t Test ""This message comes from $USER"""
23,"Depending on the current user and host name, the log contains a line"
23,similar to this:
23,"Sep 28 13:09:31 venus Test: This message comes from tuxPart III Kernel monitoring #  4 SystemTap—filtering and analyzing system dataSystemTap provides a command line interface and a scripting language to examine the activities of a running Linux system, particularly the kernel, in fine detail. SystemTap scripts are written in the SystemTap scripting language, are then compiled to C-code kernel modules and inserted into the kerne…5 Kernel probes"
23,Kernel probes are a set of tools to collect Linux kernel debugging and
23,performance information. Developers and system administrators usually use
23,"them either to debug the kernel, or to find system performance"
23,bottlenecks. The reported data can then be used to tune the system for
23,better performance.
23,6 Hardware-based performance monitoring with Perf
23,Perf is an interface to access the performance monitoring unit (PMU) of a
23,processor and to record and display software events such as page faults.
23,"It supports system-wide, per-thread, and KVM virtualization guest"
23,monitoring.
23,7 OProfile—system-wide profiler
23,OProfile is a profiler for dynamic program analysis. It investigates
23,the behavior of a running program and gathers information. This
23,information can be viewed and gives hints for further optimization.
23,It is not necessary to recompile or use wrapper libraries to
23,"use OProfile. Not even a kernel patch is needed. Usually, when"
23,"profiling an application, a small overhead is expected, depending on the"
23,workload and sampling frequency.
23,8 Dynamic debug—kernel debugging messages
23,Dynamic debug is a powerful debugging feature in the Linux kernel that
23,allows you to enable and disable debugging messages at runtime without
23,the need to recompile the kernel or reboot the system.
23,4 SystemTap—filtering and analyzing system data #
23,SystemTap provides a command line interface and a scripting language to
23,"examine the activities of a running Linux system, particularly the kernel,"
23,in fine detail. SystemTap scripts are written in the SystemTap scripting
23,"language, are then compiled to C-code kernel modules and inserted into the"
23,"kernel. The scripts can be designed to extract, filter and summarize data,"
23,thus allowing the diagnosis of complex performance problems or functional
23,problems. SystemTap provides information similar to the output of tools
23,"like netstat, ps,"
23,"top, and iostat. However, more"
23,filtering and analysis options can be used for the collected information.
23,4.1 Conceptual overview #
23,"Each time you run a SystemTap script, a SystemTap session is started."
23,Several passes are done on the script before it is allowed to run.
23,"Then, the script is compiled into a kernel module and loaded. If the"
23,script has been executed before and no system components have changed
23,"(for example, different compiler or kernel versions, library paths, or"
23,"script contents), SystemTap does not compile the script again. Instead,"
23,it uses the *.c and *.ko data
23,stored in the SystemTap cache (~/.systemtap).
23,"The module is unloaded when the tap has finished running. For an example, see"
23,"the test run in Section 4.2, “Installation and setup” and the"
23,respective explanation.
23,4.1.1 SystemTap scripts #
23,SystemTap usage is based on SystemTap scripts
23,(*.stp). They tell SystemTap which type of
23,"information to collect, and what to do once that information is"
23,collected. The scripts are written in the SystemTap scripting language
23,"that is similar to AWK and C. For the language definition, see"
23,https://sourceware.org/systemtap/langref/. A lot of
23,useful example scripts are available from
23,http://www.sourceware.org/systemtap/examples/.
23,The essential idea behind a SystemTap script is to name
23,"events, and to give them handlers."
23,"When SystemTap runs the script, it monitors for certain events. When an"
23,"event occurs, the Linux kernel runs the handler as a sub-routine, then"
23,"resumes. Thus, events serve as the triggers for handlers to run."
23,Handlers can record specified data and print it in a certain manner.
23,"The SystemTap language only uses a few data types (integers, strings,"
23,"and associative arrays of these), and full control structures (blocks,"
23,"conditionals, loops, functions). It has a lightweight punctuation"
23,(semicolons are optional) and does not need detailed declarations (types
23,are inferred and checked automatically).
23,"For more information about SystemTap scripts and their syntax, refer to"
23,"Section 4.3, “Script syntax” and to the"
23,stapprobes and stapfuncs man
23,"pages, that are available with the"
23,systemtap-docs package.
23,4.1.2 Tapsets #
23,Tapsets are a library of pre-written probes and functions that can be
23,"used in SystemTap scripts. When a user runs a SystemTap script,"
23,SystemTap checks the script's probe events and handlers against the
23,tapset library. SystemTap then loads the corresponding probes and
23,functions before translating the script to C. Like SystemTap scripts
23,"themselves, tapsets use the file name extension"
23,*.stp.
23,"However, unlike SystemTap scripts, tapsets are not meant for direct"
23,execution. They constitute the library from which other scripts can pull
23,"definitions. Thus, the tapset library is an abstraction layer designed"
23,to make it easier for users to define events and functions. Tapsets
23,provide aliases for functions that users could want to specify as an
23,event. Knowing the proper alias is often easier than remembering
23,specific kernel functions that might vary between kernel versions.
23,4.1.3 Commands and privileges #
23,The main commands associated with SystemTap are stap
23,"and staprun. To execute them, you either need"
23,root privileges or must be a member of the
23,stapdev or
23,stapusr group.
23,stap
23,"SystemTap front-end. Runs a SystemTap script (either from file, or"
23,"from standard input). It translates the script into C code, compiles"
23,"it, and loads the resulting kernel module into a running Linux"
23,"kernel. Then, the requested system trace or probe functions are"
23,performed.
23,staprun
23,SystemTap back-end. Loads and unloads kernel modules produced by the
23,SystemTap front-end.
23,"For a list of options for each command, use --help. For"
23,"details, refer to the stap and the"
23,staprun man pages.
23,To avoid giving root access to users solely to enable them to work
23,"with SystemTap, use one of the following SystemTap groups. They are not available"
23,"by default on SUSE Linux Enterprise Server, but you can create the groups and modify the"
23,access rights accordingly.
23,Also adjust the permissions of the
23,staprun command if the security implications are
23,appropriate for your environment.
23,stapdev
23,Members of this group can run SystemTap scripts with
23,"stap, or run SystemTap instrumentation modules"
23,with staprun. As running stap
23,involves compiling scripts into kernel modules and loading them into
23,"the kernel, members of this group still have effective root"
23,access.
23,stapusr
23,Members of this group are only allowed to run SystemTap
23,"instrumentation modules with staprun. In addition,"
23,they can only run those modules from
23,/lib/modules/KERNEL_VERSION/systemtap/.
23,This directory must be owned by root and must only be
23,writable for the root user.
23,4.1.4 Important files and directories #
23,The following list gives an overview of the SystemTap main files and
23,directories.
23,/lib/modules/KERNEL_VERSION/systemtap/
23,Holds the SystemTap instrumentation modules.
23,/usr/share/systemtap/tapset/
23,Holds the standard library of tapsets.
23,/usr/share/doc/packages/systemtap/examples
23,Holds several example SystemTap scripts for various purposes.
23,Only available if the
23,systemtap-docs package is
23,installed.
23,~/.systemtap/cache
23,Data directory for cached SystemTap files.
23,/tmp/stap*
23,"Temporary directory for SystemTap files, including translated C code"
23,and kernel object.
23,4.2 Installation and setup #
23,"As SystemTap needs information about the kernel, some additional"
23,kernel-related packages must be installed. For each kernel you want to
23,"probe with SystemTap, you need to install a set of the following"
23,packages. This set should exactly match the kernel version and flavor
23,(indicated by * in the overview below).
23,Important: Repository for packages with debugging information
23,"If you subscribed your system for online updates, you can find"
23,“debuginfo” packages in the
23,*-Debuginfo-Updates online installation repository
23,relevant for SUSE Linux Enterprise Server 15 SP3. Use YaST to
23,enable the repository.
23,"For the classic SystemTap setup, install the following packages (using"
23,either YaST or zypper).
23,systemtap
23,systemtap-server
23,systemtap-docs (optional)
23,kernel-*-base
23,kernel-*-debuginfo
23,kernel-*-devel
23,kernel-source-*
23,gcc
23,To get access to the man pages and to a helpful collection of example
23,"SystemTap scripts for various purposes, additionally install the"
23,systemtap-docs package.
23,To check if all packages are correctly installed on the machine and if
23,"SystemTap is ready to use, execute the following command as"
23,root.
23,"# stap -v -e 'probe vfs.read {printf(""read performed\n""); exit()}'"
23,It probes the currently used kernel by running a script and returning an
23,"output. If the output is similar to the following, SystemTap is"
23,successfully deployed and ready to use:
23,Pass 1: parsed user script and 59 library script(s) in 80usr/0sys/214real ms.
23,"Pass 2: analyzed script: 1 probe(s), 11 function(s), 2 embed(s), 1 global(s) in"
23,140usr/20sys/412real ms.
23,Pass 3: translated to C into
23,"""/tmp/stapDwEk76/stap_1856e21ea1c246da85ad8c66b4338349_4970.c"" in 160usr/0sys/408real ms."
23,"Pass 4: compiled C into ""stap_1856e21ea1c246da85ad8c66b4338349_4970.ko"" in"
23,2030usr/360sys/10182real ms.
23,Pass 5: starting run.
23,read performed
23,Pass 5: run completed in 10usr/20sys/257real ms.1
23,Checks the script against the existing tapset library in
23,/usr/share/systemtap/tapset/ for any tapsets used.
23,Tapsets are scripts that form a library of pre-written probes and
23,functions that can be used in SystemTap scripts.
23,Examines the script for its components.
23,Translates the script to C. Runs the system C compiler to create a
23,kernel module from it. Both the resulting C code
23,(*.c) and the kernel module
23,"(*.ko) are stored in the SystemTap cache,"
23,~/.systemtap.
23,Loads the module and enables all the probes (events and handlers) in
23,the script by hooking into the kernel. The event being probed is a
23,"Virtual File System (VFS) read. As the event occurs on any processor, a"
23,valid handler is executed (prints the text read
23,performed) and closed with no errors.
23,"After the SystemTap session is terminated, the probes are disabled, and"
23,the kernel module is unloaded.
23,"In case any error messages appear during the test, check the output for"
23,hints about any missing packages and make sure they are installed
23,correctly. Rebooting and loading the appropriate kernel may also be
23,needed.
23,4.3 Script syntax #
23,SystemTap scripts consist of the following two components:
23,SystemTap events (probe points)
23,Name the kernel events at the associated handler should be executed.
23,"Examples for events are entering or exiting a certain function, a"
23,"timer expiring, or starting or terminating a session."
23,SystemTap handlers (probe body)
23,Series of script language statements that specify the work to be done
23,whenever a certain event occurs. This normally includes extracting
23,"data from the event context, storing them into internal variables, or"
23,printing results.
23,An event and its corresponding handler is collectively called a
23,probe. SystemTap events are also called probe
23,points. A probe's handler is also called a probe
23,body.
23,Comments can be inserted anywhere in the SystemTap script in various
23,"styles: using either #, /* */, or"
23,// as marker.
23,4.3.1 Probe format #
23,A SystemTap script can have multiple probes. They must be written in the
23,following format:
23,probe EVENT {STATEMENTS}
23,Each probe has a corresponding statement block. This statement block
23,must be enclosed in { } and contains the statements
23,to be executed per event.
23,Example 4.1: Simple SystemTap script #
23,The following example shows a simple SystemTap script.
23,probe1 begin2
23,"printf4 (""hello world\n"")5"
23,exit ()6
23,}71
23,Start of the probe.
23,Event begin (the start of the SystemTap session).
23,"Start of the handler definition, indicated by {."
23,First function defined in the handler: the printf
23,function.
23,"String to be printed by the printf function,"
23,followed by a line break (/n).
23,Second function defined in the handler: the exit()
23,function. Note that the SystemTap script will continue to run until
23,the exit() function executes. If you want to stop
23,"the execution of the script before, stop it manually by pressing"
23,Ctrl–C.
23,"End of the handler definition, indicated by }."
23,The event begin
23,(the start of the SystemTap session) triggers the handler enclosed in
23,"{ }. Here, that is the printf"
23,function
23,"In this case, it prints hello world followed by a"
23,new line
23,"Then, the script exits."
23,"If your statement block holds several statements, SystemTap executes"
23,these statements in sequence—you do not need to insert special
23,separators or terminators between multiple statements. A statement block
23,"can also be nested within another statement blocks. Generally, statement"
23,blocks in SystemTap scripts use the same syntax and semantics as in the
23,C programming language.
23,4.3.2 SystemTap events (probe points) #
23,SystemTap supports several built-in events.
23,The general event syntax is a dotted-symbol sequence. This allows a
23,breakdown of the event namespace into parts. Each component identifier
23,"may be parameterized by a string or number literal, with a syntax like a"
23,"function call. A component may include a * character,"
23,to expand to other matching probe points. A probe point may be followed
23,"by a ? character, to indicate that it is optional,"
23,and that no error should result if it fails to expand.
23,"Alternately, a probe point may be followed by a !"
23,character to indicate that it is both optional and sufficient.
23,SystemTap supports multiple events per probe—they need to be
23,"separated by a comma (,). If multiple events are"
23,"specified in a single probe, SystemTap will execute the handler when any"
23,of the specified events occur.
23,"In general, events can be classified into the following categories:"
23,Synchronous events: Occur when any process executes an instruction at
23,a particular location in kernel code. This gives other events a
23,reference point (instruction address) from which more contextual data
23,may be available.
23,An example for a synchronous event is
23,vfs.FILE_OPERATION: The
23,entry to the FILE_OPERATION event for
23,"Virtual File System (VFS). For example, in"
23,"Section 4.2, “Installation and setup”, read"
23,is the FILE_OPERATION event used for VFS.
23,Asynchronous events: Not tied to a particular instruction or location
23,"in code. This family of probe points consists mainly of counters,"
23,"timers, and similar constructs."
23,Examples for asynchronous events are: begin (start
23,"of a SystemTap session—when a SystemTap script is run,"
23,"end (end of a SystemTap session), or timer events."
23,"Timer events specify a handler to be executed periodically, like"
23,example
23,"timer.s(SECONDS), or"
23,timer.ms(MILLISECONDS).
23,"When used together with other probes that collect information,"
23,timer events allow you to print periodic updates and see how that
23,information changes over time.
23,Example 4.2: Probe with timer event #
23,"For example, the following probe would print the text “hello"
23,world” every 4 seconds:
23,probe timer.s(4)
23,"printf(""hello world\n"")"
23,"For detailed information about supported events, refer to the"
23,stapprobes man page. The See
23,Also section of the man page also contains links to other
23,man pages that discuss supported events for specific subsystems and
23,components.
23,4.3.3 SystemTap handlers (probe body) #
23,Each SystemTap event is accompanied by a corresponding handler defined
23,"for that event, consisting of a statement block."
23,4.3.3.1 Functions #
23,"If you need the same set of statements in multiple probes, you can"
23,place them in a function for easy reuse. Functions are defined by the
23,keyword function followed by a name. They take any
23,number of string or numeric arguments (by value) and may return a
23,single string or number.
23,function FUNCTION_NAME(ARGUMENTS) {STATEMENTS}
23,probe EVENT {FUNCTION_NAME(ARGUMENTS)}
23,The statements in FUNCTION_NAME are executed
23,when the probe for EVENT executes. The
23,ARGUMENTS are optional values passed into
23,the function.
23,Functions can be defined anywhere in the script. They may take any
23,One of the functions needed very often was already introduced in
23,"Example 4.1, “Simple SystemTap script”: the printf"
23,function for printing data in a formatted way. When using the
23,"printf function, you can specify how arguments"
23,should be printed by using a format string. The format string is
23,"included in quotation marks and can contain further format specifiers,"
23,introduced by a % character.
23,Which format strings to use depends on your list of arguments. Format
23,strings can have multiple format specifiers—each matching a
23,corresponding argument. Multiple arguments can be separated by a comma.
23,"Example 4.3: printf Function with format specifiers #  printf (""1%s2(%d3) open\n4"", execname(), pid())1"
23,"Start of the format string, indicated by ""."
23,String format specifier.
23,Integer format specifier.
23,"End of the format string, indicated by ""."
23,The example above prints the current executable name
23,(execname()) as a string and the process ID
23,"(pid()) as an integer in brackets. Then, a space,"
23,the word open and a line break follow:
23,[...]
23,vmware-guestd(2206) open
23,hald(2360) open
23,[...]
23,Apart from the two functions execname()and
23,pid()) used in
23,"Example 4.3, “printf Function with format specifiers”, a variety of other"
23,functions can be used as printf arguments.
23,Among the most commonly used SystemTap functions are the following:
23,tid()
23,ID of the current thread.
23,pid()
23,Process ID of the current thread.
23,uid()
23,ID of the current user.
23,cpu()
23,Current CPU number.
23,execname()
23,Name of the current process.
23,gettimeofday_s()
23,"Number of seconds since Unix epoch (January 1, 1970)."
23,ctime()
23,Convert time into a string.
23,pp()
23,String describing the probe point currently being handled.
23,thread_indent()
23,Useful function for organizing print results. It (internally) stores
23,an indentation counter for each thread (tid()).
23,"The function takes one argument, an indentation delta, indicating"
23,how many spaces to add or remove from the thread's indentation
23,counter. It returns a string with some generic trace data along with
23,an appropriate number of indentation spaces. The generic data
23,returned includes a time stamp (number of microseconds since the
23,"initial indentation for the thread), a process name, and the thread"
23,"ID itself. This allows you to identify what functions were called,"
23,"who called them, and how long they took."
23,Call entries and exits often do not immediately precede each other
23,(otherwise it would be easy to match them). In between a first call
23,"entry and its exit, usually other call entries and exits"
23,are made. The indentation counter helps you match an entry with its
23,corresponding exit as it indents the next function call in case it
23,is not the exit of the previous one.
23,"For more information about supported SystemTap functions, refer to the"
23,stapfuncs man page.
23,4.3.3.2 Other basic constructs #
23,"Apart from functions, you can use other common constructs in"
23,"SystemTap handlers, including variables, conditional statements (like"
23,"if/else, while"
23,"loops, for loops, arrays or command line arguments."
23,4.3.3.2.1 Variables #
23,"Variables may be defined anywhere in the script. To define one, simply"
23,choose a name and assign a value from a function or expression to it:
23,foo = gettimeofday( )
23,Then you can use the variable in an expression. From the type of
23,"values assigned to the variable, SystemTap automatically infers the"
23,type of each identifier (string or number). Any inconsistencies will
23,"be reported as errors. In the example above, foo"
23,would automatically be classified as a number and could be printed via
23,printf() with the integer format specifier
23,(%d).
23,"However, by default, variables are local to the probe they are used"
23,"in: They are initialized, used and disposed of at each handler"
23,"evocation. To share variables between probes, declare them global"
23,"anywhere in the script. To do so, use the global"
23,keyword outside of the probes:
23,"Example 4.4: Using global variables #  global count_jiffies, count_ms"
23,probe timer.jiffies(100) { count_jiffies ++ }
23,probe timer.ms(100) { count_ms ++ }
23,probe timer.ms(12345)
23,hz=(1000*count_jiffies) / count_ms
23,"printf (""jiffies:ms ratio %d:%d => CONFIG_HZ=%d\n"","
23,"count_jiffies, count_ms, hz)"
23,exit ()
23,This example script computes the CONFIG_HZ setting of the kernel by
23,"using timers that count jiffies and milliseconds, then computing"
23,accordingly. (A jiffy is the duration of one tick of the system timer
23,"interrupt. It is not an absolute time interval unit, since its"
23,duration depends on the clock interrupt frequency of the particular
23,hardware platform). With the global statement it
23,is possible to use the variables count_jiffies and
23,count_ms also in the probe
23,timer.ms(12345). With ++ the
23,value of a variable is incremented by 1.
23,4.3.3.2.2 Conditional statements #
23,There are several conditional statements that you can use in
23,SystemTap scripts. The following are probably the most common:
23,If/else statements
23,They are expressed in the following format:
23,if (CONDITION)1STATEMENT12
23,else3STATEMENT24
23,The if statement compares an integer-valued
23,expression to zero. If the condition expression
23,"is non-zero, the first statement"
23,"is executed. If the condition expression is zero, the second"
23,statement
23,is executed. The else clause
23,and
23,is optional. Both
23,and
23,can also be statement blocks.
23,While loops
23,They are expressed in the following format:
23,while (CONDITION)1STATEMENT2
23,"As long as condition is non-zero, the statement"
23,is executed.
23,can also be a statement block. It must change a value so
23,condition will eventually be zero.
23,For loops
23,They are a shortcut for while loops and are
23,expressed in the following format:
23,for (INITIALIZATION1; CONDITIONAL2; INCREMENT3) statement
23,The expression specified in
23,is used to initialize a counter for the number of loop iterations
23,and is executed before execution of the loop starts. The execution
23,of the loop continues until the loop condition
23,is false. (This expression is checked at the beginning of each loop
23,iteration). The expression specified in
23,is used to increment the loop counter. It is executed at the end of
23,each loop iteration.
23,Conditional operators
23,The following operators can be used in conditional statements:
23,==:
23,Is equal to
23,!=:
23,Is not equal to
23,>=:
23,Is greater than or equal to
23,<=:
23,Is less than or equal to
23,4.4 Example script #
23,If you have installed the
23,"systemtap-docs package, you can"
23,find several useful SystemTap example scripts in
23,/usr/share/doc/packages/systemtap/examples.
23,This section describes a rather simple example script in more detail:
23,/usr/share/doc/packages/systemtap/examples/network/tcp_connections.stp.
23,Example 4.5: Monitoring incoming TCP connections with tcp_connections.stp #  #! /usr/bin/env stap
23,probe begin {
23,"printf(""%6s %16s %6s %6s %16s\n"","
23,"""UID"", ""CMD"", ""PID"", ""PORT"", ""IP_SOURCE"")"
23,"probe kernel.function(""tcp_accept"").return?,"
23,"kernel.function(""inet_csk_accept"").return? {"
23,sock = $return
23,if (sock != 0)
23,"printf(""%6d %16s %6d %6d %16s\n"", uid(), execname(), pid(),"
23,"inet_get_local_port(sock), inet_get_ip_source(sock))"
23,This SystemTap script monitors the incoming TCP connections and helps to
23,identify unauthorized or unwanted network access requests in real time.
23,It shows the following information for each new incoming TCP connection
23,accepted by the computer:
23,User ID (UID)
23,Command accepting the connection (CMD)
23,Process ID of the command (PID)
23,Port used by the connection (PORT)
23,IP address from which the TCP connection originated
23,(IP_SOUCE)
23,"To run the script, execute"
23,stap /usr/share/doc/packages/systemtap/examples/network/tcp_connections.stp
23,"and follow the output on the screen. To manually stop the script, press"
23,Ctrl–C.
23,4.5 User space probing #
23,"For debugging user space applications (like DTrace can do),"
23,SUSE Linux Enterprise Server 15 SP3 supports user space probing with
23,SystemTap: Custom probe points can be inserted in any user space
23,"application. Thus, SystemTap lets you use both kernel space and user space"
23,probes to debug the behavior of the whole system.
23,To get the required utrace infrastructure and the uprobes kernel module
23,"for user space probing, you need to install the"
23,kernel-trace package in
23,addition to the packages listed in
23,"Section 4.2, “Installation and setup”."
23,utrace implements a framework for controlling
23,user space tasks. It provides an interface that can be used by various
23,"tracing “engines”, implemented as loadable kernel modules."
23,"The engines register callback functions for specific events, then attach"
23,to whichever thread they want to trace. As the callbacks are made from
23,"“safe” places in the kernel, this allows for great leeway in"
23,the kinds of processing the functions can do. Various events can be
23,"watched via utrace, for example, system call entry and exit, fork(),"
23,"signals being sent to the task, etc. More details about the utrace"
23,infrastructure are available at
23,https://sourceware.org/systemtap/wiki/utrace.
23,SystemTap includes support for probing the entry into and return from a
23,"function in user space processes, probing predefined markers in"
23,"user space code, and monitoring user-process events."
23,To check if the currently running kernel provides the needed utrace
23,"support, use the following command:"
23,> sudo grep CONFIG_UTRACE /boot/config-`uname -r`
23,"For more details about user space probing, refer to"
23,https://sourceware.org/systemtap/SystemTap_Beginners_Guide/userspace-probing.html.
23,4.6 More information #
23,This chapter only provides a short SystemTap overview. Refer to the
23,following links for more information about SystemTap:
23,https://sourceware.org/systemtap/
23,SystemTap project home page.
23,https://sourceware.org/systemtap/wiki/
23,"Huge collection of useful information about SystemTap, ranging from"
23,detailed user and developer documentation to reviews and comparisons
23,"with other tools, or Frequently Asked Questions and tips. Also"
23,"contains collections of SystemTap scripts, examples and usage stories"
23,and lists recent talks and papers about SystemTap.
23,https://sourceware.org/systemtap/documentation.html
23,"Features a SystemTap Tutorial, a"
23,"SystemTap Beginner's Guide, a Tapset"
23,"Developer's Guide, and a SystemTap Language"
23,Reference in PDF and HTML format. Also lists the relevant
23,man pages.
23,You can also find the SystemTap language reference and SystemTap tutorial
23,in your installed system under
23,/usr/share/doc/packages/systemtap. Example SystemTap
23,scripts are available from the example subdirectory.
23,5 Kernel probes #
23,Kernel probes are a set of tools to collect Linux kernel debugging and
23,performance information. Developers and system administrators usually use
23,"them either to debug the kernel, or to find system performance"
23,bottlenecks. The reported data can then be used to tune the system for
23,better performance.
23,"You can insert these probes into any kernel routine, and specify a handler"
23,to be invoked after a particular break-point is hit. The main advantage of
23,kernel probes is that you no longer need to rebuild the kernel and reboot
23,the system after you make changes in a probe.
23,"To use kernel probes, you typically need to write or obtain a specific"
23,kernel module. Such modules include both the init and
23,the exit function. The init function (such as
23,"register_kprobe()) registers one or more probes,"
23,while the exit function unregisters them. The registration function
23,defines where the probe will be inserted and
23,which handler will be called after the probe is hit.
23,"To register or unregister a group of probes at one time, you can use"
23,relevant
23,register_<PROBE_TYPE>probes()
23,unregister_<PROBE_TYPE>probes()
23,functions.
23,Debugging and status messages are typically reported with the
23,printk kernel routine.
23,printk is a kernel space equivalent of a
23,user space printf routine. For more information
23,"on printk, see"
23,Logging
23,"kernel messages. Normally, you can view these messages by"
23,inspecting the output of the systemd journal (see
23,"Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”). For more information on log files, see"
23,"Chapter 3, System log files."
23,5.1 Supported architectures #
23,Kernel probes are fully implemented on the following
23,architectures:
23,x86
23,AMD64/Intel 64
23,Arm
23,POWER
23,Kernel probes are partially implemented on the
23,following architectures:
23,IA64 (does not support probes on instruction
23,slot1)
23,sparc64 (return probes not yet implemented)
23,5.2 Types of kernel probes #
23,"There are three types of kernel probes: Kprobes,"
23,"Jprobes, and Kretprobes."
23,Kretprobes are sometimes called return
23,probes. You can find source code examples of all three type of
23,probes in the Linux kernel. See the directory
23,/usr/src/linux/samples/kprobes/ (package
23,kernel-source).
23,5.2.1 Kprobes #
23,Kprobes can be attached to any instruction in the Linux kernel.
23,"When Kprobes is registered, it inserts a break-point at the first"
23,byte of the probed instruction. When the processor hits this
23,"break-point, the processor registers are saved, and the processing"
23,"passes to Kprobes. First, a pre-handler is"
23,"executed, then the probed instruction is stepped, and, finally a"
23,post-handler is executed. The control is then passed
23,to the instruction following the probe point.
23,5.2.2 Jprobes #
23,Jprobes is implemented through the Kprobes mechanism. It is
23,inserted on a function's entry point and allows direct access to the
23,arguments of the function which is being probed. Its handler routine
23,must have the same argument list and return value as the probed
23,"function. To end it, call the jprobe_return()"
23,function.
23,"When a jprobe is hit, the processor registers are saved, and the"
23,instruction pointer is directed to the jprobe handler routine. The
23,control then passes to the handler with the same register contents as the
23,"function being probed. Finally, the handler calls the"
23,"jprobe_return() function, and switches the"
23,control back to the control function.
23,"In general, you can insert multiple probes on one function. Jprobe is,"
23,"however, limited to only one instance per function."
23,5.2.3 Return probe #
23,Return probes are also implemented through Kprobes. When the
23,"register_kretprobe() function is called, a"
23,kprobe is attached to the entry of the probed function.
23,"After hitting the probe, the kernel probes mechanism saves the probed"
23,function return address and calls a user-defined return handler. The
23,control is then passed back to the probed function.
23,"Before you call register_kretprobe(), you need"
23,"to set a maxactive argument, which specifies"
23,how many instances of the function can be probed at the same time. If
23,"set too low, you will miss a certain number of probes."
23,5.3 Kprobes API #
23,The programming interface of Kprobes consists of functions which are
23,"used to register and unregister all used kernel probes, and associated"
23,probe handlers. For a more detailed description of these functions and
23,"their arguments, see the information sources in"
23,"Section 5.5, “More information”."
23,register_kprobe()
23,Inserts a break-point on a specified address. When the break-point is
23,"hit, the pre_handler and"
23,post_handler are called.
23,register_jprobe()
23,Inserts a break-point in the specified address. The address needs to
23,be the address of the first instruction of the probed function. When
23,"the break-point is hit, the specified handler is run. The handler"
23,should have the same argument list and return type as the probed.
23,register_kretprobe()
23,Inserts a return probe for the specified function. When the probed
23,"function returns, a specified handler is run. This function returns 0"
23,"on success, or a negative error number on failure."
23,"unregister_kprobe(), unregister_jprobe(), unregister_kretprobe()"
23,Removes the specified probe. You can use it any time after the probe
23,has been registered.
23,"register_kprobes(), register_jprobes(), register_kretprobes()"
23,Inserts each of the probes in the specified array.
23,"unregister_kprobes(), unregister_jprobes(), unregister_kretprobes()"
23,Removes each of the probes in the specified array.
23,"disable_kprobe(), disable_jprobe(), disable_kretprobe()"
23,Disables the specified probe temporarily.
23,"enable_kprobe(), enable_jprobe(), enable_kretprobe()"
23,Temporarily enables disabled probes.
23,5.4 debugfs Interface #
23,"In recent Linux kernels, the Kprobes instrumentation uses the"
23,kernel's debugfs interface. It can list all
23,registered probes and globally switch all probes on or off.
23,5.4.1 Listing registered kernel probes #
23,The list of all currently registered probes is in the
23,/sys/kernel/debug/kprobes/list file.
23,saturn.example.com:~ # cat /sys/kernel/debug/kprobes/list
23,c015d71a
23,vfs_read+0x0
23,[DISABLED]
23,c011a316
23,do_fork+0x0
23,c03dedc5
23,tcp_v4_rcv+0x0
23,The first column lists the address in the kernel where the probe is
23,inserted. The second column prints the type of the probe:
23,"k for kprobe, j for jprobe, and"
23,r for return probe. The third column specifies the
23,"symbol, offset and optional module name of the probe. The following"
23,optional columns include the status information of the probe. If the
23,"probe is inserted on a virtual address which is not valid anymore, it is"
23,marked with [GONE]. If the probe is temporarily
23,"disabled, it is marked with [DISABLED]."
23,5.4.2 Globally enabling/disabling kernel probes #
23,The /sys/kernel/debug/kprobes/enabled file
23,represents a switch with which you can globally and forcibly turn on or
23,"off all the registered kernel probes. To turn them off, simply enter"
23,"# echo ""0"" > /sys/kernel/debug/kprobes/enabled"
23,"on the command line as root. To turn them on again, enter"
23,"# echo ""1"" > /sys/kernel/debug/kprobes/enabled"
23,Note that this way you do not change the status of the probes. If a
23,"probe is temporarily disabled, it will not be enabled automatically but"
23,will remain in the [DISABLED] state after entering
23,the latter command.
23,5.5 More information #
23,"To learn more about kernel probes, look at the following sources of"
23,information:
23,Thorough but more technically oriented information about kernel probes
23,is in /usr/src/linux/Documentation/kprobes.txt
23,(package kernel-source).
23,Examples of all three types of probes (together with related
23,Makefile) are in the
23,/usr/src/linux/samples/kprobes/ directory (package
23,kernel-source).
23,In-depth information about Linux kernel modules and
23,printk kernel routine can be found at
23,The
23,Linux Kernel Module Programming Guide
23,6 Hardware-based performance monitoring with Perf #
23,Perf is an interface to access the performance monitoring unit (PMU) of a
23,processor and to record and display software events such as page faults.
23,"It supports system-wide, per-thread, and KVM virtualization guest"
23,monitoring.
23,You can store resulting information in a report.
23,"This report contains information about, for example, instruction pointers or"
23,what code a thread was executing.
23,Perf consists of two parts:
23,Code integrated into the Linux kernel that is responsible for instructing
23,the hardware.
23,The perf user space utility that allows you to use the
23,kernel code and helps you analyze gathered data.
23,6.1 Hardware-based monitoring #
23,Performance monitoring means collecting information related to how an
23,application or system performs.
23,This information can be obtained either through software-based means or from
23,the CPU or chipset.
23,Perf integrates both of these methods.
23,Many modern processors contain a performance monitoring unit (PMU).
23,The design and functionality of a PMU is CPU-specific.
23,"For example, the number of registers, counters and features supported will"
23,vary by CPU implementation.
23,Each PMU model consists of a set of registers: the performance monitor
23,configuration (PMC) and the performance monitor data (PMD).
23,"Both can be read, but only PMCs are writable."
23,These registers store configuration information and data.
23,6.2 Sampling and counting #
23,Perf supports several profiling modes:
23,Counting.
23,Count the number of occurrences of an event.
23,Event-based sampling.
23,A less exact way of counting: A sample is recorded whenever a certain
23,threshold number of events has occurred.
23,Time-based sampling.
23,A less exact way of counting: A sample is recorded in a defined frequency.
23,Instruction-based sampling (AMD64 only).
23,The processor follows instructions appearing in a given interval and
23,samples which events they produce.
23,This allows following up on individual instructions and seeing which of
23,them is critical to performance.
23,6.3 Installing Perf #
23,The Perf kernel code is already included with the default kernel.
23,"To be able to use the user space utility, install the package"
23,perf.
23,6.4 Perf subcommands #
23,"To gather the required information, the perf tool has"
23,several subcommands. This section gives an overview of the most often used
23,commands.
23,"To see help in the form of a man page for any of the subcommands, use either"
23,perf helpSUBCOMMAND
23,man perf-SUBCOMMAND.
23,perf stat
23,Start a program and create a statistical overview that is displayed after
23,the program quits.
23,perf stat is used to count events.
23,perf record
23,Start a program and create a report with performance counter information.
23,The report is stored as perf.data in the current
23,directory.
23,perf record is used to sample events.
23,perf report
23,Display a report that was previously created with
23,perf record.
23,perf annotate
23,Display a report file and an annotated version of the executed
23,code.
23,"If debug symbols are installed, you will also see the source code"
23,displayed.
23,perf list
23,List event types that Perf can report with the current kernel and with
23,your CPU.
23,"You can filter event types by category—for example, to see hardware"
23,"events only, use perf list hw."
23,The man page for perf_event_open has short descriptions
23,for the most important events.
23,"For example, to find a description of the event"
23,"branch-misses, search for"
23,BRANCH_MISSES (note the spelling differences):
23,> man perf_event_open | grep -A5 BRANCH_MISSES
23,"Sometimes, events may be ambiguous."
23,Note that the lowercase hardware event names are not the name of raw
23,hardware events but instead the name of aliases created by Perf.
23,These aliases map to differently named but similarly defined hardware
23,events on each supported processor.
23,"For example, the cpu-cycles event is mapped to"
23,the hardware event UNHALTED_CORE_CYCLES on
23,Intel processors.
23,"On AMD processors, however, it is mapped to the hardware event"
23,CPU_CLK_UNHALTED.
23,Perf also allows measuring raw events specific to your hardware.
23,"To look up their descriptions, see the"
23,Architecture Software Developer's Manual of your CPU vendor.
23,The relevant documents for AMD64/Intel 64 processors are linked to in
23,"Section 6.7, “More information”."
23,perf top
23,Display system activity as it happens.
23,perf trace
23,This command behaves similarly to strace.
23,"With this subcommand, you can see which system calls are executed by a"
23,particular thread or process and which signals it receives.
23,6.5 Counting particular types of event #
23,"To count the number of occurrences of an event, such as those displayed by"
23,"perf list, use:"
23,# perf stat -e EVENT -a
23,"To count multiple types of events at once, list them separated by commas."
23,"For example, to count cpu-cycles and"
23,"instructions, use:"
23,"# perf stat -e cpu-cycles,instructions -a"
23,"To stop the session, press"
23,Ctrl–C.
23,You can also count the number of occurrences of an event within a particular
23,time:
23,# perf stat -e EVENT -a -- sleep TIME
23,Replace TIME by a value in seconds.
23,6.6 Recording events specific to particular commands #
23,There are various ways to sample events specific to a particular command:
23,"To create a report for a newly invoked command, use:"
23,# perf record COMMAND
23,"Then, use the started process normally."
23,"When you quit the process, the Perf session will also stop."
23,To create a report for the entire system while a newly invoked command is
23,"running, use:"
23,# perf record -a COMMAND
23,"Then, use the started process normally."
23,"When you quit the process, the Perf session will also stop."
23,"To create a report for an already running process, use:"
23,# perf record -p PID
23,Replace PID with a process ID.
23,"To stop the session, press"
23,Ctrl–C.
23,Now you can view the gathered data (perf.data)
23,using:
23,> perf report
23,This will open a pseudo-graphical interface.
23,"To receive help, press H."
23,"To quit, press Q."
23,"If you prefer a graphical interface, try the GTK+ interface of Perf:"
23,> perf report --gtk
23,"However, note that the GTK+ interface is very limited in functionality."
23,6.7 More information #
23,This chapter only provides a short overview. Refer to the following links
23,for more information:
23,https://perf.wiki.kernel.org/index.php/Main_Page
23,The project home page.
23,It also features a tutorial on using perf.
23,http://www.brendangregg.com/perf.html
23,Unofficial page with many one-line examples of how to use
23,perf.
23,http://web.eece.maine.edu/~vweaver/projects/perf_events/
23,"Unofficial page with several resources, mostly relating to the Linux"
23,kernel code of Perf and its API.
23,"This page includes, for example, a CPU compatibility table and a"
23,programming guide.
23,https://www-ssl.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.pdf
23,"The Intel Architectures Software Developer's Manual,"
23,Volume 3B.
23,https://support.amd.com/TechDocs/24593.pdf
23,"The AMD Architecture Programmer's Manual, Volume 2."
23,"Chapter 7, OProfile—system-wide profiler"
23,Consult this chapter for other performance optimizations.
23,7 OProfile—system-wide profiler #
23,OProfile is a profiler for dynamic program analysis. It investigates
23,the behavior of a running program and gathers information. This
23,information can be viewed and gives hints for further optimization.
23,It is not necessary to recompile or use wrapper libraries to
23,"use OProfile. Not even a kernel patch is needed. Usually, when"
23,"profiling an application, a small overhead is expected, depending on the"
23,workload and sampling frequency.
23,7.1 Conceptual overview #
23,OProfile consists of a kernel driver and a daemon for collecting data.
23,It uses the hardware performance counters provided on many
23,processors. OProfile is capable of profiling all code
23,"including the kernel, kernel modules, kernel interrupt handlers, system"
23,"shared libraries, and other applications."
23,Modern processors support profiling through the hardware by performance
23,"counters. Depending on the processor, there can be many counters and each"
23,of these can be programmed with an event to count. Each counter has a
23,"value which determines how often a sample is taken. The lower the value,"
23,the more often it is used.
23,"During the post-processing step, all information is collected and"
23,instruction addresses are mapped to a function name.
23,7.2 Installation and requirements #
23,"To use OProfile, install the oprofile package."
23,"OProfile works on AMD64/Intel 64, IBM Z, and POWER processors."
23,It is useful to install the *-debuginfo package for
23,the respective application you want to profile. If you want to profile
23,"the kernel, you need the debuginfo package as well."
23,7.3 Available OProfile utilities #
23,OProfile contains several utilities to handle the profiling process and
23,its profiled data. The following list is a short summary of programs used
23,in this chapter:
23,opannotate
23,Outputs annotated source or assembly listings mixed with profile
23,information. An annotated report can be used in combination with
23,addr2line to identify the source file and line
23,where hotspots potentially exist. See man addr2line
23,for more information.
23,operf
23,"Profiler tool. After profiling stops, the data that is by default stored in"
23,CUR_DIR/oprofile_data/samples/current
23,"can be processed by opreport, for example."
23,ophelp
23,Lists available events with short descriptions.
23,opimport
23,Converts sample database files from a foreign binary format to the
23,native format.
23,opreport
23,Generates reports from profiled data.
23,7.4 Using OProfile #
23,"With OProfile, you can profile both the kernel and applications. When"
23,"profiling the kernel, tell OProfile where to find the"
23,vmlinuz* file. Use the --vmlinux
23,option and point it to vmlinuz* (usually in
23,"/boot). If you need to profile kernel modules,"
23,"OProfile does this by default. However, make sure you read"
23,http://oprofile.sourceforge.net/doc/kernel-profiling.html.
23,"Applications usually do not need to profile the kernel, therefore you"
23,should use the --no-vmlinux option to reduce the amount
23,of information.
23,7.4.1 Creating a report #
23,"Starting the daemon, collecting data, stopping the daemon, and creating"
23,a report for the application COMMAND.
23,Open a shell and log in as root.
23,Decide if you want to profile with or without the Linux kernel:
23,Profile with the Linux kernel.
23,"Execute the following commands, because"
23,operf can only work with uncompressed
23,images:
23,> cp /boot/vmlinux-`uname -r`.gz /tmp
23,> gunzip /tmp/vmlinux*.gz
23,> operf--vmlinux=/tmp/vmlinux* COMMANDProfile without the Linux kernel.
23,Use the following command:
23,# operf --no-vmlinux COMMAND
23,To see which functions call other functions in the
23,"output, additionally use the --callgraph option and"
23,set a maximum DEPTH:
23,# operf --no-vmlinux --callgraph
23,DEPTH COMMAND
23,operf writes its data to CUR_DIR/oprofile_data/samples/current.
23,After the operf command is finished (or is aborted by
23,"Ctrl–C),"
23,the data can be analyzed with oreport:
23,# opreport
23,Overflow stats not available
23,"CPU: CPU with timer interrupt, speed 0 MHz (estimated)"
23,Profiling through timer interrupt
23,TIMER:0|
23,samples|
23,------------------
23,84877 98.3226 no-vmlinux
23,...7.4.2 Getting event configurations #
23,The general procedure for event configuration is as follows:
23,Use first the events CPU-CLK_UNHALTED and
23,INST_RETIRED to find optimization opportunities.
23,"Use specific events to find bottlenecks. To list them, use the command"
23,perf list.
23,"If you need to profile certain events, first check the available events"
23,supported by your processor with the ophelp command
23,(example output generated from Intel Core i5 CPU):
23,# ophelp
23,"oprofile: available events for CPU type ""Intel Architectural Perfmon"""
23,See Intel 64 and IA-32 Architectures Software Developer's Manual
23,Volume 3B (Document 253669) Chapter 18 for architectural perfmon events
23,This is a limited set of fallback events because oprofile does not know your CPU
23,CPU_CLK_UNHALTED: (counter: all))
23,Clock cycles when not halted (min count: 6000)
23,INST_RETIRED: (counter: all))
23,number of instructions retired (min count: 6000)
23,LLC_MISSES: (counter: all))
23,Last level cache demand requests from this core that missed the LLC (min count: 6000)
23,Unit masks (default 0x41)
23,----------
23,0x41: No unit mask
23,LLC_REFS: (counter: all))
23,Last level cache demand requests from this core (min count: 6000)
23,Unit masks (default 0x4f)
23,----------
23,0x4f: No unit mask
23,BR_MISS_PRED_RETIRED: (counter: all))
23,number of mispredicted branches retired (precise) (min count: 500)
23,Specify the performance counter events with the option
23,--event. Multiple options are possible. This option
23,"needs an event name (from ophelp) and a sample rate,"
23,for example:
23,# operf --events CPU_CLK_UNHALTED:100000Warning: Setting sampling rates with CPU_CLK_UNHALTED
23,Setting low sampling rates can seriously impair the system performance
23,while high sample rates can disrupt the system to such a high degree
23,that the data is useless. It is recommended to tune the performance
23,metric for being monitored with and without OProfile and to
23,experimentally determine the minimum sample rate that disrupts the
23,performance the least.
23,7.5 Generating reports #
23,"Before generating a report, make sure the operf has"
23,stopped. Unless you have provided an output directory with
23,"--session-dir, operf has written its"
23,"data to CUR_DIR/oprofile_data/samples/current,"
23,and the reporting tools opreport and
23,opannotate will look there by default.
23,Calling opreport without any options gives a complete
23,"summary. With an executable as an argument, retrieve profile data only"
23,"from this executable. If you analyze applications written in C++, use the"
23,--demangle smart option.
23,The opannotate generates output with annotations from
23,source code. Run it with the following options:
23,# opannotate --source \
23,--base-dirs=BASEDIR \
23,--search-dirs=SEARCHDIR \
23,--output-dir=annotated/ \
23,/lib/libfoo.so
23,The option --base-dir contains a comma separated list of
23,paths which is stripped from debug source files. These paths were
23,searched prior to looking in --search-dirs. The
23,--search-dirs option is also a comma separated list of
23,directories to search for source files.
23,Note: Inaccuracies in annotated source
23,"Because of compiler optimization, code can disappear and appear in a"
23,different place. Use the information in
23,http://oprofile.sourceforge.net/doc/debug-info.html
23,to fully understand its implications.
23,7.6 More information #
23,This chapter only provides a short overview. Refer to the following links
23,for more information:
23,http://oprofile.sourceforge.net
23,The project home page.
23,Manpages
23,Details descriptions about the options of the different tools.
23,/usr/share/doc/packages/oprofile/oprofile.html
23,Contains the OProfile manual.
23,http://developer.intel.com/
23,Architecture reference for Intel processors.
23,https://www.ibm.com/support/knowledgecenter/ssw_aix_71/assembler/idalangref_arch_overview.html
23,"Architecture reference for PowerPC64 processors in IBM iSeries,"
23,"pSeries, and Blade server systems."
23,8 Dynamic debug—kernel debugging messages #
23,Dynamic debug is a powerful debugging feature in the Linux kernel that
23,allows you to enable and disable debugging messages at runtime without
23,the need to recompile the kernel or reboot the system.
23,"You can use dynamic debugging in several situations, such as:"
23,Troubleshooting kernel issues
23,Developing drivers for new hardware
23,Tracing and auditing security events
23,8.1 Benefits of dynamic debugging #
23,Certain benefits of dynamic debugging are listed below:
23,Real-time debugging
23,Dynamic debugging enables debugging messages without requiring a
23,system reboot. This real-time capability is crucial for diagnosing
23,issues in production environments.
23,Selective debugging
23,You can enable debugging messages for specific parts of the kernel
23,"or even individual modules, allowing you to focus on relevant"
23,information.
23,Performance tuning
23,Use dynamic debugging to monitor and optimize kernel performance by
23,selectively enabling or disabling debugging messages based on the
23,current analysis requirements.
23,8.2 Checking the status of dynamic debug #
23,"For supported kernel versions that are installed by default, dynamic"
23,"debug is already built-in. To check the status of dynamic debug, run the"
23,following command as the root user:
23,# zcat /proc/config.gz | grep CONFIG_DYNAMIC_DEBUG
23,"If dynamic debug is compiled into the kernel, you should see an output"
23,similar to the following:
23,CONFIG_DYNAMIC_DEBUG=y
23,CONFIG_DYNAMIC_DEBUG_CORE=y8.3 Using dynamic debug #
23,"To enable specific debug messages or logs within the running kernel, you"
23,can use the echo command and write to the
23,/sys/kernel/debug/dynamic_debug/control file.
23,The following examples illustrate certain simple uses of dynamic debug:
23,Note
23,"Dynamic debug relies on specific debugging macros, such as"
23,"pr_debug, embedded in the kernel code. These"
23,macros are used by kernel developers to insert debugging messages into
23,the code.
23,The examples in this section assume that the
23,pr_debug macro works correctly because dynamic
23,debug is allowed for the running kernel.
23,Enabling debug messages for a specific kernel source code file
23,To enable the debug messages for a specific kernel source code
23,"file, use the following example:"
23,"# echo ""file FILE_NAME.c +p"" > /sys/kernel/debug/dynamic_debug/controlEnabling debug messages for a specific kernel module"
23,"To enable debug messages for a specific kernel module, use the"
23,following example:
23,"# echo ""module MODULE_NAME +p"" > /sys/kernel/debug/dynamic_debug/controlDisabling debug messages"
23,To disable previously enabled debugging messages for a specific
23,"kernel source code file or a kernel module, run the"
23,echo command with the -p
23,option. For example:
23,"# echo ""file FILE_NAME.c -p"" > /sys/kernel/debug/dynamic_debug/control# echo ""module MODULE_NAME -p"" > /sys/kernel/debug/dynamic_debug/control"
23,"For detailed information about dynamic debug and its use cases, refer to"
23,its
23,official
23,documentation.
23,8.4 Viewing the dynamic debug messages #
23,You can view the dynamic debug messages that were generated based on the
23,"configurations you enabled, by running dmesg and"
23,filtering the output with grep. For example:
23,"# dmesg | grep -i ""FILE_NAME.c"""
23,"Optionally, to continuously monitor the system messages as they are"
23,"generated, you can use the tail command with the"
23,-f option:
23,# tail -f /var/log/messagesPart IV Resource management #  9 General system resource management
23,Tuning the system is not only about optimizing the kernel or getting the
23,"most out of your application, it begins with setting up a lean and fast"
23,system. The way you set up your partitions and file systems can
23,influence the server's speed. The number of active services and the way
23,routine tasks are scheduled also affects performance.
23,10 Kernel control groups
23,Kernel Control Groups (“cgroups”) are a kernel feature for
23,assigning and limiting hardware and system resources for processes.
23,Processes can also be organized in a hierarchical tree structure.
23,11 Automatic Non-Uniform Memory Access (NUMA) balancing
23,There are physical limitations to hardware that are encountered when
23,many CPUs and lots of memory are required. In this
23,"chapter, the important limitation is that there is limited communication"
23,bandwidth between the CPUs and the memory. One architecture modification
23,that was introduced to address this is Non-Uniform Memory Access (NUMA).
23,"In this configuration, there are multiple nodes. Each of the nodes"
23,contains a subset of all CPUs and memory. The access speed to main
23,memory is determined by the location of the memory relative to the CPU.
23,The performance of a workload depends on the application threads
23,accessing data that is local to the CPU the thread is executing on.
23,Automatic NUMA Balancing migrates data on demand to memory nodes that are
23,local to the CPU accessing that data.
23,"Depending on the workload, this can dramatically boost performance when"
23,using NUMA hardware.
23,12 Power management
23,Power management aims at reducing operating costs for energy and cooling
23,systems while at the same time keeping the performance of a system at a
23,"level that matches the current requirements. Thus, power management is"
23,always a matter of balancing the actual performance needs and power
23,saving options for a system. Power management can be implemented and
23,used at different levels of the system. A set of specifications for
23,power management functions of devices and the operating system interface
23,to them has been defined in the Advanced Configuration and Power
23,Interface (ACPI). As power savings in server environments can primarily
23,"be achieved at the processor level, this chapter introduces some"
23,main concepts and highlights some tools for analyzing and influencing
23,relevant parameters.
23,9 General system resource management #
23,Tuning the system is not only about optimizing the kernel or getting the
23,"most out of your application, it begins with setting up a lean and fast"
23,system. The way you set up your partitions and file systems can
23,influence the server's speed. The number of active services and the way
23,routine tasks are scheduled also affects performance.
23,9.1 Planning the installation #
23,A carefully planned installation ensures that the system is set up
23,exactly as you need it for the given purpose. It also saves considerable
23,time when fine tuning the system. All changes suggested in this section
23,can be made in the Installation Settings step during
23,"the installation. See Book “Deployment Guide”, Chapter 8 “Installation steps”, Section 8.15 “Installation settings” for details."
23,9.1.1 Partitioning #
23,"Depending on the server's range of applications and the hardware layout,"
23,the partitioning scheme can influence the machine's performance
23,(although to a lesser extent only). It is beyond the scope of this
23,manual to suggest different partitioning schemes for particular
23,"workloads. However, the following rules will positively affect"
23,performance. They do not apply when using an external storage system.
23,"Make sure there always is some free space available on the disk, since"
23,a full disk delivers inferior performance
23,"Disperse simultaneous read and write access onto different disks by,"
23,for example:
23,"using separate disks for the operating system, data, and log files"
23,placing a mail server's spool directory on a separate disk
23,distributing the user directories of a home server between different
23,disks
23,9.1.2 Installation scope #
23,The installation scope has no direct influence on the machine's
23,"performance, but a carefully chosen scope of packages has advantages. It"
23,is recommended to install the minimum of packages needed to run the
23,server. A system with a minimum set of packages is easier to maintain
23,"and has fewer potential security issues. Furthermore, a tailor made"
23,installation scope also ensures that no unnecessary services are started
23,by default.
23,SUSE Linux Enterprise Server lets you customize the installation scope on the
23,"Installation Summary screen. By default, you can select or remove"
23,"preconfigured patterns for specific tasks, but it is also possible to"
23,start the YaST Software Manager for a fine-grained package-based
23,selection.
23,One or more of the following default patterns may not be needed in all
23,cases:
23,GNOME desktop environment
23,Servers rarely need a full desktop environment. In case a graphical
23,"environment is needed, a more economical solution such as IceWM can"
23,be sufficient.
23,X Window System
23,When solely administrating the server and its applications via
23,"command line, consider not installing this pattern. However, keep in"
23,mind that it is needed to run GUI applications from a remote machine.
23,If your application is managed by a GUI or if you prefer the GUI
23,"version of YaST, keep this pattern."
23,Print server
23,This pattern is only needed if you want to print from the machine.
23,9.1.3 Default target #
23,A running X Window System consumes many resources and is rarely needed on
23,a server. It is strongly recommended to start the system in target
23,multi-user.target. You will still be able to
23,remotely start graphical applications.
23,9.2 Disabling unnecessary services #
23,The default installation starts several services (the number varies
23,"with the installation scope). Since each service consumes resources, it"
23,is recommended to disable the ones not needed. Run YaST › System › Services Manager to start the services
23,management module.
23,"If you are using the graphical version of YaST, you can click the"
23,column headlines to sort the list of services. Use this to get an
23,overview of which services are currently running.
23,Use the Start/Stop button to disable the service for
23,"the running session. To permanently disable it, use the"
23,Enable/Disable button.
23,The following list shows services that are started by default after the
23,"installation of SUSE Linux Enterprise Server. Check which of the components you need,"
23,and disable the others:
23,alsasound
23,Loads the Advanced Linux Sound System.
23,auditd
23,A daemon for the Audit system (see Book “Security and Hardening Guide” for
23,details). Disable this if you do not use Audit.
23,bluez-coldplug
23,Handles cold plugging of Bluetooth dongles.
23,cups
23,A printer daemon.
23,java.binfmt_misc
23,Enables the execution of *.class or
23,*.jar Java programs.
23,nfs
23,Services needed to mount NFS.
23,smbfs
23,Services needed to mount SMB/CIFS file systems from a Windows* server.
23,splash / splash_early
23,Shows the splash screen on start-up.
23,9.3 File systems and disk access #
23,Hard disks are the slowest components in a computer system and therefore
23,often the cause for a bottleneck. Using the file system that best suits
23,your workload helps to improve performance. Using special mount options
23,or prioritizing a process's I/O priority are further means to speed up
23,the system.
23,9.3.1 File systems #
23,"SUSE Linux Enterprise Server ships with several file systems,"
23,"including Btrfs, Ext4, Ext3, Ext2, and XFS. Each file system has"
23,its own advantages and disadvantages. Refer to
23,"Book “Storage Administration Guide”, Chapter 1 “Overview of file systems in Linux” for detailed information."
23,9.3.1.1 NFS #
23,NFS (Version 3) tuning is covered in detail in the NFS Howto at
23,http://nfs.sourceforge.net/nfs-howto/. The first
23,thing to experiment with when mounting NFS shares is increasing the
23,read write blocksize to 32768 by using the mount
23,options wsize and rsize.
23,9.3.2 Time stamp update policy #
23,Each file and directory in a file system has three time stamps associated
23,with it: a time when the file was last read called access
23,"time, a time when the file data was last modified called"
23,"modification time, and a time when the file metadata"
23,was last modified called change time. Keeping access
23,time always up to date has significant performance overhead since every
23,read-only access will incur a write operation. Thus by default every file
23,system updates access time only if current file access time is older than a
23,day or if it is older than file modification or change time.
23,This feature
23,is called relative access time and the corresponding
23,mount option is relatime. Updates of access time can be
23,"completely disabled using the noatime mount option,"
23,however you need to verify your applications do not use it. This can be
23,true for file and Web servers or for network storage. If the default
23,"relative access time update policy is not suitable for your applications,"
23,use the strictatime mount option.
23,Some file systems (for example Ext4) also support lazy time stamp updates.
23,When this feature is enabled using the lazytime mount
23,"option, updates of all time stamps happen in memory but they are not"
23,written to disk. That happens only in response to
23,fsync or sync system
23,"calls, when the file information is written due to another reason such as"
23,"file size update, when time stamps are older than 24 hours, or when cached"
23,file information needs to be evicted from memory.
23,"To update mount options used for a file system, either edit"
23,"/etc/fstab directly, or use the Fstab"
23,Options dialog when editing or adding a partition with the
23,YaST Partitioner.
23,9.3.3 Prioritizing disk access with ionice #
23,The ionice command lets you prioritize disk access
23,for single processes. This enables you to give less I/O priority to
23,"background processes with heavy disk access that are not time-critical,"
23,such as backup jobs. ionice also lets you raise the
23,I/O priority for a specific process to make sure this process always has
23,immediate access to the disk. The caveat of this feature is that standard
23,writes are cached in the page cache and are written back to persistent
23,storage only later by an independent kernel process. Thus the I/O priority
23,setting generally does not apply for these writes. Also be aware that
23,I/O class and priority setting are obeyed only by
23,BFQ I/O scheduler for blk-mq I/O path (refer
23,"to Section 13.2, “Available I/O elevators with blk-mq I/O path”)."
23,You can set
23,the following three scheduling classes:
23,Idle
23,A process from the idle scheduling class is only granted disk access
23,when no other process has asked for disk I/O.
23,Best effort
23,The default scheduling class used for any process that has not asked
23,for a specific I/O priority. Priority within this class can be
23,adjusted to a level from 0 to 7
23,(with 0 being the highest priority). Programs
23,running at the same best-effort priority are served in a round-robin
23,fashion. Some kernel versions treat priority within the best-effort
23,"class differently—for details, refer to the"
23,ionice(1) man page.
23,Real-time
23,Processes in this class are always granted disk access first.
23,Fine-tune the priority level from 0 to
23,7 (with 0 being the highest
23,"priority). Use with care, since it can starve other processes."
23,For more details and the exact command syntax refer to the
23,ionice(1) man page. If you need more reliable
23,"control over bandwidth available to each application, use"
23,Kernel Control Groups as described in
23,"Chapter 10, Kernel control groups."
23,10 Kernel control groups #
23,Kernel Control Groups (“cgroups”) are a kernel feature for
23,assigning and limiting hardware and system resources for processes.
23,Processes can also be organized in a hierarchical tree structure.
23,10.1 Overview #
23,Every process is assigned exactly one administrative cgroup. cgroups are
23,"ordered in a hierarchical tree structure. You can set resource limitations,"
23,"such as CPU, memory, disk I/O, or network bandwidth usage, for single"
23,processes or for whole branches of the hierarchy tree.
23,"On SUSE Linux Enterprise Server, systemd uses cgroups to organize all processes in"
23,"groups, which systemd calls slices. systemd also provides an interface"
23,for setting cgroup properties.
23,The command systemd-cgls displays the hierarchy tree.
23,There are two versions of cgroup APIs provided by the kernel. These differ
23,"in the cgroup attributes they provide, and in the organization of controller"
23,hierarchies. systemd attempts to abstract the differences away. By default
23,"systemd runs in the hybrid mode, which means"
23,controllers are used through the v1 API. cgroup v2 is only used for
23,systemd's own tracking. There is also the unified mode
23,when the controllers are used though v2 API. You may set only one mode.
23,"To enable the unified control group hierarchy, append"
23,systemd.unified_cgroup_hierarchy=1 as a kernel command line
23,"parameter to the GRUB 2 boot loader. (Refer to Book “Administration Guide”, Chapter 14 “The boot loader GRUB 2”"
23,for more details about configuring GRUB 2.)
23,10.2 Resource accounting #
23,Organizing processes into different cgroups can be used to obtain per-cgroup
23,resource consumption data.
23,"The accounting has relatively small but non-zero overhead, whose impact"
23,depends on the workload. Activating accounting for one unit will also
23,"implicitly activate it for all units in the same slice, and for all its"
23,"parent slices, and the units contained in them."
23,The accounting can be set on a per-unit basis with directives such as
23,MemoryAccounting= or globally for all units in
23,/etc/systemd/system.conf with the directive
23,DefaultMemoryAccounting=. Refer to man
23,systemd.resource-control for the exhaustive list of possible
23,directives.
23,10.3 Setting resource limits #  Note: Implicit resource consumption
23,Be aware that resource consumption implicitly depends on the environment
23,"where your workload executes (for example, size of data structures in"
23,"libraries/kernel, forking behavior of utilities, computational efficiency)."
23,Hence it is recommended to (re)calibrate your limits should the environment
23,change.
23,Limitations to cgroups can be set with the systemctl
23,set-property command. The syntax is:
23,# systemctl set-property [--runtime] NAME PROPERTY1=VALUE [PROPERTY2=VALUE]
23,"The configured value is applied immediately. Optionally, use the"
23,"--runtime option, so that the new values do not persist"
23,after reboot.
23,"Replace NAME with a systemd service, scope, or"
23,slice name.
23,"For a complete list of properties and more details, see man"
23,systemd.resource-control.
23,10.4 Preventing fork bombs with TasksMax #
23,systemd supports configuring task count limits both for each individual
23,"leaf unit, or aggregated on slices. Upstream systemd ships with defaults"
23,"that limit the number of tasks in each unit (15% of the kernel global limit,"
23,run /usr/sbin/sysctl kernel.pid_max to see the total
23,"limit). Each user's slice is limited to 33% of the kernel limit. However,"
23,this is different for SLE.
23,10.4.1 Finding the current default TasksMax values #
23,"It became apparent, in practice, that there is not a single default that"
23,applies to all use cases. SUSE Linux Enterprise Server ships with two custom
23,configurations that override the upstream defaults for system units and for
23,"user slices, and sets them both to infinity."
23,/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf
23,contains these lines:
23,[Manager]
23,DefaultTasksMax=infinity
23,/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf
23,contains these lines:
23,[Slice]
23,TasksMax=infinity
23,Use systemctl to verify the DefaultTasksMax value:
23,> systemctl show --property DefaultTasksMax
23,DefaultTasksMax=infinity
23,infinity means having no limit. It is not a requirement
23,"to change the default, but setting some limits may help to prevent system"
23,crashes from runaway processes.
23,10.4.2 Overriding the DefaultTasksMax value #
23,Change the global DefaultTasksMax value by creating a
23,"new override file,"
23,"/etc/systemd/system.conf.d/90-system-tasksmax.conf,"
23,and write the following lines to set a new default limit of 256 tasks per
23,system unit:
23,[Manager]
23,DefaultTasksMax=256
23,"Load the new setting, then verify that it changed:"
23,> sudo systemctl daemon-reload
23,> systemctl show --property DefaultTasksMax
23,DefaultTasksMax=256
23,Adjust this default value to suit your needs. You can set different limits
23,on individual services as needed. This example is for MariaDB. First check
23,the current active value:
23,> systemctl status mariadb.service
23,● mariadb.service - MariaDB database server
23,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
23,Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
23,Docs: man:mysqld(8)
23,https://mariadb.com/kb/en/library/systemd/
23,Main PID: 11845 (mysqld)
23,"Status: ""Taking your SQL requests now..."""
23,Tasks: 30 (limit: 256)
23,CGroup: /system.slice/mariadb.service
23,└─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
23,"The Tasks line shows that MariaDB currently has 30 tasks running, and has"
23,"an upper limit of the default 256, which is inadequate for a database. The"
23,following example demonstrates how to raise MariaDB's limit to 8192.
23,> sudo systemctl set-property mariadb.service TasksMax=8192
23,> systemctl status mariadb.service
23,● mariadb.service - MariaDB database server
23,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
23,Drop-In: /etc/systemd/system/mariadb.service.d
23,└─50-TasksMax.conf
23,Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
23,Docs: man:mysqld(8)
23,https://mariadb.com/kb/en/library/systemd/
23,"Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>"
23,"Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>"
23,Main PID: 3452 (mysqld)
23,"Status: ""Taking your SQL requests now..."""
23,Tasks: 30 (limit: 8192)
23,CGroup: /system.slice/mariadb.service
23,└─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
23,systemctl set-property applies the new limit and creates
23,"a drop-in file for persistence,"
23,"/etc/systemd/system/mariadb.service.d/50-TasksMax.conf,"
23,that contains only the changes you want to apply to the existing unit file.
23,"The value does not have to be 8192, but should be whatever limit is"
23,appropriate for your workloads.
23,10.4.3 Default TasksMax limit on users #
23,"The default limit on users should be fairly high, because user sessions"
23,need more resources. Set your own default for any user by creating a new
23,"file, for example"
23,/etc/systemd/system/user-.slice.d/40-user-taskmask.conf.
23,The following example sets a default of 16284:
23,[Slice]
23,TasksMax=16284Note: Numeric prefixes reference
23,See
23,https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in
23,to learn what numeric prefixes are expected for drop-in files.
23,"Then reload systemd to load the new value, and verify the change:"
23,> sudo systemctl daemon-reload
23,> systemctl show --property TasksMax user-1000.slice
23,TasksMax=16284
23,How do you know what values to use? This varies according to your
23,"workloads, system resources, and other resource configurations. When your"
23,"TasksMax value is too low, you will see error messages"
23,such as Failed to fork (Resources temporarily
23,"unavailable), Can't create thread to handle new"
23,"connection, and Error: Function call 'fork' failed"
23,"with error code 11, 'Resource temporarily unavailable'."
23,"For more information on configuring system resources in systemd, see"
23,systemd.resource-control (5).
23,10.5 Controlling I/O with proportional weight policy #
23,This section introduces using the Linux kernel's block I/O controller to
23,prioritize I/O operations. The cgroup blkio subsystem controls and monitors
23,access to I/O on block devices. State objects that contain the subsystem
23,parameters for a cgroup are represented as pseudo-files within the cgroup
23,"virtual file system, also called a pseudo-file system."
23,The examples in this section show how writing values to some of these
23,"pseudo-files limits access or bandwidth, and reading values from some of"
23,these pseudo-files provides information on I/O operations. Examples are
23,provided for both cgroup-v1 and cgroup-v2.
23,You need a test directory containing two files for testing performance and
23,changed settings. A quick way to create test files fully populated with text
23,is using the yes command. The following example commands
23,"create a test directory, and then populate it with two 537 MB text files:"
23,host1:~ # mkdir /io-cgroup
23,host1:~ # cd /io-cgroup
23,host1:~ # yes this is a test file | head -c 537MB > file1.txt
23,host1:~ # yes this is a test file | head -c 537MB > file2.txt
23,To run the examples open three command shells. Two shells are for reader
23,"processes, and one shell is for running the steps that control I/O. In the"
23,"examples, each command prompt is labeled to indicate if it represents one of"
23,"the reader processes, or I/O."
23,10.5.1 Using cgroup-v1 #
23,The following proportional weight policy files can be used to grant a
23,reader process a higher priority for I/O operations than other reader
23,processes accessing the same disk.
23,blkio.bfq.weight (available in kernels starting with
23,version 5.0 with blk-mq and when using the BFQ I/O scheduler)
23,"To test this, run a single reader process (in the examples, reading from an"
23,"SSD) without controlling its I/O, using file2.txt:"
23,[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5251
23,131072+0 records in
23,131072+0 records out
23,"536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s"
23,Now run a background process reading from the same disk:
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
23,5220
23,...
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5251
23,131072+0 records in
23,131072+0 records out
23,"536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s"
23,"Each process gets half of the throughput for I/O operations. Next, set up"
23,two control groups—one for each process—verify that BFQ is
23,"used, and set a different weight for reader2:"
23,[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/blkio/
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader1
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader2
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5220 > reader1/cgroup.procs
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5251 > reader2/cgroup.procs
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat /sys/block/sda/queue/scheduler
23,mq-deadline kyber [bfq] none
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
23,100
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 200 > reader2/blkio.bfq.weight
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
23,200
23,"With these settings and reader1 in the background, reader2 should have"
23,higher throughput than previously:
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
23,5220
23,...
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5251
23,131072+0 records in
23,131072+0 records out
23,"536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s"
23,The higher proportional weight resulted in higher throughput for reader2.
23,Now double its weight again:
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
23,100
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 400 > reader2/blkio.bfq.weight
23,[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
23,400
23,This results in another increase in throughput for reader2:
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
23,5220
23,...
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5251
23,131072+0 records in
23,131072+0 records out
23,"536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s10.5.2 Using cgroup-v2 #"
23,First set up your test environment as shown at the beginning of this
23,chapter.
23,"Then make sure that the Block IO controller is not active, as that is for"
23,"cgroup-v1. To do this, boot with kernel parameter"
23,"cgroup_no_v1=blkio. Verify that this parameter was used,"
23,and that the IO controller (cgroup-v2) is available:
23,[io-controller] host1:/io-cgroup # cat /proc/cmdline
23,BOOT_IMAGE=... cgroup_no_v1=blkio ...
23,[io-controller] host1:/io-cgroup # cat /sys/fs/cgroup/unified/cgroup.controllers
23,"Next, enable the IO controller:"
23,[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/unified/
23,[io-controller] host1:/sys/fs/cgroup/unified # echo '+io' > cgroup.subtree_control
23,[io-controller] host1:/sys/fs/cgroup/unified # cat cgroup.subtree_control
23,"Now run all the test steps, similarly to the steps for cgroup-v1. Note that"
23,some of the directories are different. Run a single reader process (in the
23,"examples, reading from an SSD) without controlling its I/O, using"
23,file2.txt:
23,[io-controller] host1:/sys/fs/cgroup/unified # cd -
23,[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5633
23,[...]
23,Run a background process reading from the same disk and note your
23,throughput values:
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
23,5633
23,[...]
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,5703
23,[...]
23,Each process gets half of the throughput for I/O operations. Set up two
23,control groups—one for each process—verify that BFQ is the
23,"active scheduler, and set a different weight for reader2:"
23,[io-controller] host1:/io-cgroup # cd -
23,[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader1
23,[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader2
23,[io-controller] host1:/sys/fs/cgroup/unified # echo 5633 > reader1/cgroup.procs
23,[io-controller] host1:/sys/fs/cgroup/unified # echo 5703 > reader2/cgroup.procs
23,[io-controller] host1:/sys/fs/cgroup/unified # cat /sys/block/sda/queue/scheduler
23,mq-deadline kyber [bfq] none
23,[io-controller] host1:/sys/fs/cgroup/unified # cat reader1/io.bfq.weight
23,default 100
23,[io-controller] host1:/sys/fs/cgroup/unified # echo 200 > reader2/io.bfq.weight
23,[io-controller] host1:/sys/fs/cgroup/unified # cat reader2/io.bfq.weight
23,default 200
23,Test your throughput with the new settings. reader2 should show an increase
23,in throughput.
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1 of=/dev/null bs=4k
23,5633
23,[...]
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2 of=/dev/null bs=4k count=131072
23,5703
23,[...]
23,"Try doubling the weight again for reader2, and testing the new setting:"
23,[reader2] host1:/io-cgroup # echo 400 > reader1/blkio.bfq.weight
23,[reader2] host1:/io-cgroup # cat reader2/blkio.bfq.weight
23,400
23,[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
23,[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
23,[...]
23,[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
23,[...]10.6 More information #
23,Kernel documentation (package kernel-source):
23,files in
23,/usr/src/linux/Documentation/admin-guide/cgroup-v1
23,and file
23,/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst.
23,"https://lwn.net/Articles/604609/—Brown, Neil:"
23,"Control Groups Series (2014, 7 parts)."
23,"https://lwn.net/Articles/243795/—Corbet,"
23,Jonathan: Controlling memory use in containers (2007).
23,"https://lwn.net/Articles/236038/—Corbet,"
23,Jonathan: Process containers (2007).
23,11 Automatic Non-Uniform Memory Access (NUMA) balancing #
23,There are physical limitations to hardware that are encountered when
23,many CPUs and lots of memory are required. In this
23,"chapter, the important limitation is that there is limited communication"
23,bandwidth between the CPUs and the memory. One architecture modification
23,that was introduced to address this is Non-Uniform Memory Access (NUMA).
23,"In this configuration, there are multiple nodes. Each of the nodes"
23,contains a subset of all CPUs and memory. The access speed to main
23,memory is determined by the location of the memory relative to the CPU.
23,The performance of a workload depends on the application threads
23,accessing data that is local to the CPU the thread is executing on.
23,Automatic NUMA Balancing migrates data on demand to memory nodes that are
23,local to the CPU accessing that data.
23,"Depending on the workload, this can dramatically boost performance when"
23,using NUMA hardware.
23,11.1 Implementation #
23,Automatic NUMA balancing happens in three basic steps:
23,A task scanner periodically scans a portion of a task's address space
23,and marks the memory to force a page fault when the data is next
23,accessed.
23,The next access to the data will result in a NUMA Hinting Fault. Based
23,"on this fault, the data can be migrated to a memory node associated"
23,with the task accessing the memory.
23,"To keep a task, the CPU it is using and the memory it is accessing"
23,"together, the scheduler groups tasks that share data."
23,"The unmapping of data and page fault handling incurs overhead. However,"
23,commonly the overhead will be offset by threads accessing data associated
23,with the CPU.
23,11.2 Configuration #
23,Static configuration has been the recommended way of tuning workloads on
23,"NUMA hardware for some time. To do this, memory policies can be set with"
23,"numactl, taskset or"
23,cpusets. NUMA-aware applications can use special APIs.
23,"In cases where the static policies have already been created, automatic"
23,NUMA balancing should be disabled as the data access should already be
23,local.
23,numactl --hardware will show the
23,memory configuration of the machine and whether it supports NUMA or not.
23,This is example output from a 4-node machine.
23,> numactl --hardware
23,available: 4 nodes (0-3)
23,node 0 cpus: 0 4 8 12 16 20 24 28 32 36 40 44
23,node 0 size: 16068 MB
23,node 0 free: 15909 MB
23,node 1 cpus: 1 5 9 13 17 21 25 29 33 37 41 45
23,node 1 size: 16157 MB
23,node 1 free: 15948 MB
23,node 2 cpus: 2 6 10 14 18 22 26 30 34 38 42 46
23,node 2 size: 16157 MB
23,node 2 free: 15981 MB
23,node 3 cpus: 3 7 11 15 19 23 27 31 35 39 43 47
23,node 3 size: 16157 MB
23,node 3 free: 16028 MB
23,node distances:
23,node
23,Automatic NUMA balancing can be enabled or disabled for the current
23,session by writing 1 or 0
23,to /proc/sys/kernel/numa_balancing which will
23,enable or disable the feature respectively. To permanently enable or
23,"disable it, use the kernel command line option"
23,numa_balancing=[enable|disable].
23,"If Automatic NUMA Balancing is enabled, the task scanner behavior can be"
23,configured.
23,The task scanner balances the overhead of Automatic NUMA Balancing with
23,the amount of time it takes to identify the best placement of data.
23,numa_balancing_scan_delay_ms
23,The amount of CPU time a thread must consume before its data is
23,scanned. This prevents creating overhead because of short-lived
23,processes.
23,numa_balancing_scan_period_min_ms and
23,numa_balancing_scan_period_max_ms
23,Controls how frequently a task's data is scanned. Depending on the
23,locality of the faults the scan rate will increase or decrease. These
23,settings control the min and max scan rates.
23,numa_balancing_scan_size_mb
23,Controls how much address space is scanned when the task scanner is
23,active.
23,11.3 Monitoring #
23,The most important task is to assign metrics to your workload and measure
23,the performance with Automatic NUMA Balancing enabled and disabled to
23,measure the impact. Profiling tools can be used to monitor local and
23,remote memory accesses if the CPU supports such monitoring. Automatic
23,NUMA Balancing activity can be monitored via the following parameters in
23,/proc/vmstat:
23,numa_pte_updates
23,The amount of base pages that were marked for NUMA hinting faults.
23,numa_huge_pte_updates
23,The amount of transparent huge pages that were marked for NUMA hinting
23,faults. In combination with numa_pte_updates the
23,total address space that was marked can be calculated.
23,numa_hint_faults
23,Records how many NUMA hinting faults were trapped.
23,numa_hint_faults_local
23,Shows how many of the hinting faults were to local nodes. In
23,"combination with numa_hint_faults, the percentage"
23,of local versus remote faults can be calculated. A high percentage of
23,local hinting faults indicates that the workload is closer to being
23,converged.
23,numa_pages_migrated
23,Records how many pages were migrated because they were misplaced. As
23,"migration is a copying operation, it contributes the largest part of"
23,the overhead created by NUMA balancing.
23,11.4 Impact #
23,The following illustrates a simple test case of a 4-node NUMA machine
23,running the SpecJBB 2005
23,using a single instance of the JVM with no static tuning around memory
23,"policies. Note, however, that the impact for each workload will vary and"
23,that this example is based on a pre-release version of SUSE Linux Enterprise Server
23,12.
23,Balancing disabled
23,Balancing enabled
23,TPut 1
23,26629.00 (
23,0.00%)
23,26507.00 ( -0.46%)
23,TPut 2
23,55841.00 (
23,0.00%)
23,53592.00 ( -4.03%)
23,TPut 3
23,86078.00 (
23,0.00%)
23,86443.00 (
23,0.42%)
23,TPut 4
23,116764.00 (
23,0.00%)
23,113272.00 ( -2.99%)
23,TPut 5
23,143916.00 (
23,0.00%)
23,141581.00 ( -1.62%)
23,TPut 6
23,166854.00 (
23,0.00%)
23,166706.00 ( -0.09%)
23,TPut 7
23,195992.00 (
23,0.00%)
23,192481.00 ( -1.79%)
23,TPut 8
23,222045.00 (
23,0.00%)
23,227143.00 (
23,2.30%)
23,TPut 9
23,248872.00 (
23,0.00%)
23,250123.00 (
23,0.50%)
23,TPut 10
23,270934.00 (
23,0.00%)
23,279314.00 (
23,3.09%)
23,TPut 11
23,297217.00 (
23,0.00%)
23,301878.00 (
23,1.57%)
23,TPut 12
23,311021.00 (
23,0.00%)
23,326048.00 (
23,4.83%)
23,TPut 13
23,324145.00 (
23,0.00%)
23,346855.00 (
23,7.01%)
23,TPut 14
23,345973.00 (
23,0.00%)
23,378741.00 (
23,9.47%)
23,TPut 15
23,354199.00 (
23,0.00%)
23,394268.00 ( 11.31%)
23,TPut 16
23,378016.00 (
23,0.00%)
23,426782.00 ( 12.90%)
23,TPut 17
23,392553.00 (
23,0.00%)
23,437772.00 ( 11.52%)
23,TPut 18
23,396630.00 (
23,0.00%)
23,456715.00 ( 15.15%)
23,TPut 19
23,399114.00 (
23,0.00%)
23,484020.00 ( 21.27%)
23,TPut 20
23,413907.00 (
23,0.00%)
23,493618.00 ( 19.26%)
23,TPut 21
23,413173.00 (
23,0.00%)
23,510386.00 ( 23.53%)
23,TPut 22
23,420256.00 (
23,0.00%)
23,521016.00 ( 23.98%)
23,TPut 23
23,425581.00 (
23,0.00%)
23,536214.00 ( 26.00%)
23,TPut 24
23,429052.00 (
23,0.00%)
23,532469.00 ( 24.10%)
23,TPut 25
23,426127.00 (
23,0.00%)
23,526548.00 ( 23.57%)
23,TPut 26
23,422428.00 (
23,0.00%)
23,531994.00 ( 25.94%)
23,TPut 27
23,424378.00 (
23,0.00%)
23,488340.00 ( 15.07%)
23,TPut 28
23,419338.00 (
23,0.00%)
23,543016.00 ( 29.49%)
23,TPut 29
23,403347.00 (
23,0.00%)
23,529178.00 ( 31.20%)
23,TPut 30
23,408681.00 (
23,0.00%)
23,510621.00 ( 24.94%)
23,TPut 31
23,406496.00 (
23,0.00%)
23,499781.00 ( 22.95%)
23,TPut 32
23,404931.00 (
23,0.00%)
23,502313.00 ( 24.05%)
23,TPut 33
23,397353.00 (
23,0.00%)
23,522418.00 ( 31.47%)
23,TPut 34
23,382271.00 (
23,0.00%)
23,491989.00 ( 28.70%)
23,TPut 35
23,388965.00 (
23,0.00%)
23,493012.00 ( 26.75%)
23,TPut 36
23,374702.00 (
23,0.00%)
23,502677.00 ( 34.15%)
23,TPut 37
23,367578.00 (
23,0.00%)
23,500588.00 ( 36.19%)
23,TPut 38
23,367121.00 (
23,0.00%)
23,496977.00 ( 35.37%)
23,TPut 39
23,355956.00 (
23,0.00%)
23,489430.00 ( 37.50%)
23,TPut 40
23,350855.00 (
23,0.00%)
23,487802.00 ( 39.03%)
23,TPut 41
23,345001.00 (
23,0.00%)
23,468021.00 ( 35.66%)
23,TPut 42
23,336177.00 (
23,0.00%)
23,462260.00 ( 37.50%)
23,TPut 43
23,329169.00 (
23,0.00%)
23,467906.00 ( 42.15%)
23,TPut 44
23,329475.00 (
23,0.00%)
23,470784.00 ( 42.89%)
23,TPut 45
23,323845.00 (
23,0.00%)
23,450739.00 ( 39.18%)
23,TPut 46
23,323878.00 (
23,0.00%)
23,435457.00 ( 34.45%)
23,TPut 47
23,310524.00 (
23,0.00%)
23,403914.00 ( 30.07%)
23,TPut 48
23,311843.00 (
23,0.00%)
23,459017.00 ( 47.19%)
23,Balancing Disabled
23,Balancing Enabled
23,Expctd Warehouse
23,48.00 (
23,0.00%)
23,48.00 (
23,0.00%)
23,Expctd Peak Bops
23,310524.00 (
23,0.00%)
23,403914.00 ( 30.07%)
23,Actual Warehouse
23,25.00 (
23,0.00%)
23,29.00 ( 16.00%)
23,Actual Peak Bops
23,429052.00 (
23,0.00%)
23,543016.00 ( 26.56%)
23,SpecJBB Bops
23,6364.00 (
23,0.00%)
23,9368.00 ( 47.20%)
23,SpecJBB Bops/JVM
23,6364.00 (
23,0.00%)
23,9368.00 ( 47.20%)
23,Automatic NUMA Balancing simplifies
23,tuning workloads for high performance on NUMA machines. Where
23,"possible, it is still recommended to statically tune the workload to"
23,"partition it within each node. However, in all other cases, automatic"
23,NUMA balancing should boost performance.
23,12 Power management #
23,Power management aims at reducing operating costs for energy and cooling
23,systems while at the same time keeping the performance of a system at a
23,"level that matches the current requirements. Thus, power management is"
23,always a matter of balancing the actual performance needs and power
23,saving options for a system. Power management can be implemented and
23,used at different levels of the system. A set of specifications for
23,power management functions of devices and the operating system interface
23,to them has been defined in the Advanced Configuration and Power
23,Interface (ACPI). As power savings in server environments can primarily
23,"be achieved at the processor level, this chapter introduces some"
23,main concepts and highlights some tools for analyzing and influencing
23,relevant parameters.
23,12.1 Power management at CPU Level #
23,"At the CPU level, you can control power usage in various ways. For"
23,"example by using idling power states (C-states), changing CPU frequency"
23,"(P-states), and throttling the CPU (T-states). The following sections"
23,give a short introduction to each approach and its significance for power
23,savings. Detailed specifications can be found at
23,http://www.acpi.info/spec.htm.
23,12.1.1 C-states (processor operating states) #
23,Modern processors have several power saving modes called
23,C-states. They reflect the capability of an idle
23,processor to turn off unused components to save power.
23,"When a processor is in the C0 state, it is executing"
23,instructions. A processor running in any other C-state is idle. The
23,"higher the C number, the deeper the CPU sleep mode: more components are"
23,shut down to save power. Deeper sleep states can save large amounts of
23,"energy. Their downside is that they introduce latency. This means, it"
23,takes more time for the CPU to go back to C0.
23,"Depending on workload (threads waking up, triggering CPU usage and then"
23,going back to sleep again for a short period of time) and hardware (for
23,"example, interrupt activity of a network device), disabling the deepest"
23,sleep states can significantly increase overall performance. For details
23,"on how to do so, refer to"
23,"Section 12.3.2, “Viewing kernel idle statistics with cpupower”."
23,Some states also have submodes with different power saving latency
23,levels. Which C-states and submodes are supported depends on the
23,"respective processor. However, C1 is always"
23,available.
23,"Table 12.1, “C-states” gives an overview of the most"
23,common C-states.
23,Table 12.1: C-states #
23,Mode
23,Definition
23,Operational state. CPU fully turned on.
23,First idle state. Stops CPU main internal clocks via software. Bus
23,interface unit and APIC are kept running at full speed.
23,Stops CPU main internal clocks via hardware. State in which the
23,"processor maintains all software-visible states, but may take"
23,longer to wake up through interrupts.
23,Stops all CPU internal clocks. The processor does not need to keep
23,"its cache coherent, but maintains other states. Some processors"
23,have variations of the C3 state that differ in how long it takes to
23,wake the processor through interrupts.
23,"To avoid needless power consumption, it is recommended to test your"
23,workloads with deep sleep states enabled versus deep sleep states
23,"disabled. For more information, refer to"
23,"Section 12.3.2, “Viewing kernel idle statistics with cpupower” or the"
23,cpupower-idle-set(1) man page.
23,12.1.2 P-states (processor performance states) #
23,"While a processor operates (in C0 state), it can be in one of several"
23,CPU performance states (P-states). Whereas C-states
23,"are idle states (all but C0), P-states are"
23,operational states that relate to CPU frequency and voltage.
23,"The higher the P-state, the lower the frequency and voltage at which the"
23,processor runs. The number of P-states is processor-specific and the
23,"implementation differs across the various types. However,"
23,"P0 is always the highest-performance state (except for Section 12.1.3, “Turbo features”). Higher"
23,P-state numbers represent slower processor speeds and lower power
23,"consumption. For example, a processor in P3 state runs"
23,more slowly and uses less power than a processor running in the
23,"P1 state. To operate at any P-state, the processor"
23,"must be in the C0 state, which means that it is"
23,working and not idling. The CPU P-states are also defined in the ACPI
23,"specification, see http://www.acpi.info/spec.htm."
23,C-states and P-states can vary independently of one another.
23,12.1.3 Turbo features #
23,Turbo features allow to dynamically overtick active CPU
23,cores while other cores are in deep sleep states. This increases the performance
23,of active threads while still
23,complying with Thermal Design Power (TDP) limits.
23,"However, the conditions under which a CPU core can use turbo frequencies"
23,are architecture-specific. Learn how to evaluate the efficiency of those
23,"new features in Section 12.3, “The cpupower tools”."
23,12.2 In-kernel governors #   The in-kernel governors belong to the Linux kernel CPUfreq infrastructure and can be
23,used to dynamically scale processor frequencies at runtime. You can think of the governors as a
23,sort of preconfigured power scheme for the CPU. The CPUfreq governors use P-states to
23,change frequencies and lower power consumption. The dynamic governors can switch between CPU
23,"frequencies, based on CPU usage, to allow for power savings while not sacrificing performance."
23,The following governors are available with the CPUfreq subsystem:
23,Performance governor
23,The CPU frequency is statically set to the highest possible for
23,"maximum performance. Consequently, saving power is not the focus of"
23,this governor.
23,"See also Section 12.4.1, “Tuning options for P-states”."
23,Powersave governor
23,The CPU frequency is statically set to the lowest possible. This can
23,"have severe impact on the performance, as the system will never rise"
23,above this frequency no matter how busy the processors are. An important
23,exception is the intel_pstate which defaults to the
23,powersave mode. This is due to a hardware-specific
23,decision but functionally it operates similarly to the
23,on-demand governor.
23,"However, using this governor often does not lead to the expected"
23,power savings as the highest savings can usually be achieved at idle
23,"through entering C-states. With the powersave governor, processes run"
23,at the lowest frequency and thus take longer to finish. This means it
23,takes longer until the system can go into an idle C-state.
23,Tuning options: The range of minimum frequencies available to the
23,"governor can be adjusted (for example, with the"
23,cpupower command line tool).
23,On-demand governor
23,The kernel implementation of a dynamic CPU frequency policy: The
23,governor monitors the processor usage. When it exceeds a
23,"certain threshold, the governor will set the frequency to the highest"
23,"available. If the usage is less than the threshold, the next lowest"
23,"frequency is used. If the system continues to be underemployed, the"
23,frequency is again reduced until the lowest available frequency is
23,set.
23,Important: Drivers and in-kernel governorsNot all drivers use the in-kernel governors to dynamically scale power frequency at
23,"runtime. For example, the intel_pstate driver adjusts power frequency itself. Use"
23,the cpupower frequency-info command to find out which driver your system
23,uses.12.3 The cpupower tools #  The cpupower tools are designed to give an overview
23,of all CPU power-related parameters that are supported
23,"on a given machine, including turbo (or boost) states."
23,Use the tool set to
23,view and modify settings of the kernel-related CPUfreq and cpuidle systems
23,and other settings not related to frequency scaling or idle states. The
23,integrated monitoring framework can access both kernel-related parameters
23,"and hardware statistics. Therefore, it is ideally suited for performance"
23,benchmarks. It also helps you to identify the dependencies between turbo and
23,idle states.
23,"After installing the cpupower package, view the"
23,available cpupower subcommands with
23,cpupower --help. Access the general man page with
23,"man cpupower, and the man pages of the subcommands with"
23,man cpupower-SUBCOMMAND.
23,12.3.1 Viewing current settings with cpupower #
23,The cpupower frequency-info command shows the
23,"statistics of the cpufreq driver used in the kernel. Additionally, it"
23,shows if turbo (boost) states are supported and enabled in the BIOS.
23,"Run without any options, it shows an output similar to the following:"
23,Example 12.1: Example output of cpupower frequency-info #  # cpupower frequency-info
23,analyzing CPU 0:
23,driver: intel_pstate
23,CPUs which run at the same hardware frequency: 0
23,CPUs which need to have their frequency coordinated by software: 0
23,maximum transition latency: 0.97 ms.
23,hardware limits: 1.20 GHz - 3.80 GHz
23,"available cpufreq governors: performance, powersave"
23,current policy: frequency should be within 1.20 GHz and 3.80 GHz.
23,"The governor ""powersave"" may decide which speed to use"
23,within this range.
23,current CPU frequency is 3.40 GHz (asserted by call to hardware).
23,boost state support:
23,Supported: yes
23,Active: yes
23,3500 MHz max turbo 4 active cores
23,3600 MHz max turbo 3 active cores
23,3600 MHz max turbo 2 active cores
23,3800 MHz max turbo 1 active cores
23,"To get the current values for all CPUs, use"
23,cpupower -c all frequency-info.
23,12.3.2 Viewing kernel idle statistics with cpupower #
23,The idle-info subcommand shows the statistics of the
23,cpuidle driver used in the kernel. It works on all architectures that
23,use the cpuidle kernel framework.
23,Example 12.2: Example output of cpupower idle-info #  # cpupower idle-info
23,CPUidle driver: intel_idle
23,CPUidle governor: menu
23,Analyzing CPU 0:
23,Number of idle states: 6
23,Available idle states: POLL C1-SNB C1E-SNB C3-SNB C6-SNB C7-SNB
23,POLL:
23,Flags/Description: CPUIDLE CORE POLL IDLE
23,Latency: 0
23,Usage: 163128
23,Duration: 17585669
23,C1-SNB:
23,Flags/Description: MWAIT 0x00
23,Latency: 2
23,Usage: 16170005
23,Duration: 697658910
23,C1E-SNB:
23,Flags/Description: MWAIT 0x01
23,Latency: 10
23,Usage: 4421617
23,Duration: 757797385
23,C3-SNB:
23,Flags/Description: MWAIT 0x10
23,Latency: 80
23,Usage: 2135929
23,Duration: 735042875
23,C6-SNB:
23,Flags/Description: MWAIT 0x20
23,Latency: 104
23,Usage: 53268
23,Duration: 229366052
23,C7-SNB:
23,Flags/Description: MWAIT 0x30
23,Latency: 109
23,Usage: 62593595
23,Duration: 324631233978
23,After finding out which processor idle states are supported with
23,"cpupower idle-info, individual states can be"
23,disabled using the cpupower idle-set command.
23,"Typically one wants to disable the deepest sleep state, for example:"
23,"# cpupower idle-set -d 5Or, for disabling all CPUs with latencies equal to or higher than 80:# cpupower idle-set -D 8012.3.3 Monitoring kernel and hardware statistics with cpupower #"
23,"Use the monitor subcommand to report processor topology, and monitor frequency"
23,and idle power state statistics over a certain period of time. The
23,"default interval is 1 second, but it can be changed"
23,with the -i. Independent processor sleep states and
23,frequency counters are implemented in the tool—some retrieved
23,"from kernel statistics, others reading out hardware registers. The"
23,available monitors depend on the underlying hardware and the system.
23,List them with cpupower monitor -l.
23,"For a description of the individual monitors, refer to the"
23,cpupower-monitor man page.
23,The monitor subcommand allows you to execute
23,performance benchmarks. To compare kernel statistics with hardware
23,"statistics for specific workloads, concatenate the respective command, for example:cpupower monitor db_test.shExample 12.3: Example cpupower monitor output #  # cpupower monitor"
23,|Mperf
23,|| Idle_Stats
23,CPU | C0
23,| Cx
23,| Freq || POLL | C1
23,| C2
23,| C3
23,3.71| 96.29|
23,2833||
23,0.00|
23,0.00|
23,0.02| 96.32
23,1| 100.0| -0.00|
23,2833||
23,0.00|
23,0.00|
23,0.00|
23,0.00
23,9.06| 90.94|
23,1983||
23,0.00|
23,7.69|
23,6.98| 76.45
23,7.43| 92.57|
23,2039||
23,0.00|
23,2.60| 12.62| 77.521
23,"Mperf shows the average frequency of a CPU, including boost"
23,"frequencies, over time. Additionally, it shows the"
23,percentage of time the CPU has been active (C0)
23,or in any sleep state (Cx). As the turbo states are managed by the
23,"BIOS, it is impossible to get the frequency values at a given"
23,instant. On modern processors with turbo features the Mperf monitor
23,is the only way to find out about the frequency a certain CPU has
23,been running in.
23,Idle_Stats shows the statistics of the cpuidle kernel subsystem. The
23,kernel updates these values every time an idle state is entered or
23,left. Therefore there can be some inaccuracy when cores are in an
23,idle state for some time when the measure starts or ends.
23,"Apart from the (general) monitors in the example above, other"
23,architecture-specific monitors are available. For detailed
23,"information, refer to the cpupower-monitor man"
23,page.
23,"By comparing the values of the individual monitors, you can find"
23,correlations and dependencies and evaluate how well the power saving
23,mechanism works for a certain workload. In
23,Example 12.3 you can
23,see that CPU 0 is idle (the value of
23,"Cx is near 100%), but runs at a very high frequency."
23,This is because the CPUs 0 and 1
23,have the same frequency values which means that there is a dependency
23,between them.
23,12.3.4 Modifying current settings with cpupower #
23,You can use
23,cpupower frequency-set command as root to
23,modify current settings. It allows you to set values for the minimum or
23,maximum CPU frequency the governor may select or to create a new
23,"governor. With the -c option, you can also specify for"
23,which of the processors the settings should be modified. That makes it
23,easy to use a consistent policy across all processors without adjusting
23,the settings for each processor individually. For more details and the
23,"available options, see the man page"
23,cpupower-frequency-set or run
23,cpupower frequency-set
23,--help.
23,12.4 Special tuning options #
23,The following sections highlight important settings.
23,12.4.1 Tuning options for P-states #
23,The CPUfreq subsystem offers several tuning options for P-states:
23,"You can switch between the different governors, influence minimum or"
23,maximum CPU frequency to be used or change individual governor
23,parameters.
23,"To switch to another governor at runtime, use"
23,cpupower frequency-set with the -g option. For
23,"example, running the following command (as root) will activate the"
23,performance governor:
23,# cpupower frequency-set -g performance
23,To set values for the minimum or maximum CPU frequency the governor may
23,"select, use the -d or -u option,"
23,respectively.
23,12.5 Troubleshooting #  BIOS options enabled?
23,"To use C-states or P-states, check your BIOS options:"
23,"To use C-states, make sure to enable CPU C State"
23,or similar options to benefit from power savings at idle.
23,"To use P-states and the CPUfreq governors, make sure to enable"
23,Processor Performance States options or similar.
23,"Even if P-states and C-states are available, it is possible that the"
23,platform firmware is managing CPU frequencies which may be sub-optimal.
23,"For example, if pcc-cpufreq is loaded then the"
23,"OS is only giving hints to the firmware, which is free to ignore the"
23,"hints. This can be addressed by selecting ""OS Management"" or similar"
23,"for CPU frequency managed in the BIOS. After reboot, an alternative"
23,driver will be used but the performance impact should be carefully
23,measured.
23,"In case of a CPU upgrade, make sure to upgrade your BIOS, too. The"
23,BIOS needs to know the new CPU and its frequency stepping to
23,pass this information on to the operating system.
23,Log file information?
23,"Check the systemd journal (see Book “Administration Guide”, Chapter 17 “journalctl: Query the systemd journal”)"
23,for any output regarding the CPUfreq subsystem. Only severe
23,errors are reported there.
23,If you suspect problems with the CPUfreq subsystem on your
23,"machine, you can also enable additional debug output. To do so, either"
23,use cpufreq.debug=7 as boot parameter or execute
23,the following command as root:
23,# echo 7 > /sys/module/cpufreq/parameters/debug
23,This will cause CPUfreq to log more information to
23,"dmesg on state transitions, which is useful for"
23,diagnosis. But as this additional output of kernel messages can be
23,"rather comprehensive, use it only if you are fairly sure that a"
23,problem exists.
23,12.6 More information #
23,Platforms with a Baseboard Management Controller (BMC) may have additional
23,power management configuration options accessible via the service
23,processor. These configurations are vendor specific and therefore not
23,"subject of this guide. For more information, refer to the manuals provided"
23,by your vendor.
23,12.7 Monitoring power consumption with powerTOP #
23,powerTOP helps to identify the causes of unnecessary high power
23,consumption. This is especially
23,"useful for laptops, where minimizing power consumption is"
23,more important. It supports both Intel and AMD processors.
23,Install it in the usual way:
23,> sudo zypper in powertop
23,powerTOP combines various sources of information (analysis of
23,"programs, device drivers, kernel options, number and sources of"
23,interrupts waking up processors from sleep states) and provides
23,"several ways of viewing them. You can launch it in interactive mode,"
23,which runs in an ncurses session (see
23,"Figure 12.1, “powerTOP in interactive mode”):"
23,> sudo powertopFigure 12.1: powerTOP in interactive mode #
23,powerTOP supports exporting reports to HTML and CSV.
23,The following example generates a single report of a 240-second run:
23,> sudo powertop --iteration=1 --time=240 --html=POWERREPORT.HTML
23,It can be useful to run separate reports over time.
23,"The following example runs powerTOP 10 times for 20 seconds each time,"
23,and creates a separate HTML report for each run:
23,> sudo powertop --iteration=10 --time=20 --html=POWERREPORT.HTML
23,This creates 10 time-stamped reports:
23,powerreport-20200108-104512.html
23,powerreport-20200108-104451.html
23,powerreport-20200108-104431.html
23,[...]
23,"An HTML report looks like Figure 12.2, “HTML powerTOP report”:"
23,Figure 12.2: HTML powerTOP report #
23,"The Tuning tab of the HTML reports, and the Tunables tab in the"
23,"interactive mode, both provide commands for testing the various power"
23,"settings. The HTML report prints the commands, which you can copy"
23,"to a root command line for testing, for example"
23,echo '0' > '/proc/sys/kernel/nmi_watchdog'.
23,The ncurses mode provides a simple toggle between Good
23,and Bad. Good runs a command
23,"to enable power saving, and Bad turns off power saving."
23,Enable all powerTOP settings with one command:
23,> sudo powertop --auto-tune
23,None of these changes survive a reboot. To make any changes
23,"permanent, use sysctl, udev,"
23,or systemd to run your selected commands at
23,"boot. powerTOP includes a systemd service file,"
23,/usr/lib/systemd/system/powertop.service. This
23,starts powerTOP with the --auto-tune option:
23,ExecStart=/usr/sbin/powertop --auto-tune
23,"Test this carefully before launching the systemd service,"
23,to see if it gives the results that you want.
23,You probably do not want USB keyboards and mice to go into
23,powersave modes because it is a nuisance to continually wake
23,"them up, and there may be other devices you want left alone. For easier"
23,"testing and configuration editing, extract the commands from"
23,an HTML report with awk:
23,> awk -F '</?td ?>' '/tune/ { print $4 }' POWERREPORT.HTML
23,"In calibrate mode, powerTOP sets up several runs that use different idle"
23,"settings for backlight, CPU, Wi-Fi, USB devices, and disks, and helps to"
23,identify optimal brightness settings on battery power:
23,> sudo powertop --calibrate
23,You may call a file that creates a workload for more accurate calibration:
23,> sudo powertop --calibrate --workload=FILENAME --html=POWERREPORT.HTML
23,"For more information, see:"
23,The powerTOP project page at https://01.org/powertop
23,"Section 2.6.2, “System control parameters: /proc/sys/”"
23,"Book “Administration Guide”, Chapter 15 “The systemd daemon”"
23,"Book “Administration Guide”, Chapter 24 “Dynamic kernel device management with udev”"
23,Part V Kernel tuning #  13 Tuning I/O performance
23,I/O scheduling controls how input/output operations will be submitted to
23,storage. SUSE Linux Enterprise Server offers various I/O algorithms—called
23,elevators—suiting different workloads.
23,Elevators can help to reduce seek operations and can prioritize I/O requests.
23,"14 Tuning the task schedulerModern operating systems, such as SUSE® Linux Enterprise Server, normally run many tasks at the same time. For example, you can be searching in a text file while receiving an e-mail and copying a big file to an external hard disk. These simple tasks require many additional processes to be run by the…15 Tuning the memory management subsystem"
23,To understand and tune the memory management behavior of the
23,"kernel, it is important to first have an overview of how it works and"
23,cooperates with other subsystems.
23,"16 Tuning the networkThe network subsystem is complex and its tuning highly depends on the system use scenario and on external factors such as software clients or hardware components (switches, routers, or gateways) in your network. The Linux kernel aims more at reliability and low latency than low overhead and high thr…17 Tuning SUSE Linux Enterprise for SAPThis chapter presents information about preparing and tuning SUSE Linux Enterprise Server to work optimally with SAP applications with sapconf. sapconf is for SUSE Linux Enterprise systems that install SAP applications. Customers who have SUSE Linux Enterprise Server for SAP Applications should use …13 Tuning I/O performance #"
23,I/O scheduling controls how input/output operations will be submitted to
23,storage. SUSE Linux Enterprise Server offers various I/O algorithms—called
23,elevators—suiting different workloads.
23,Elevators can help to reduce seek operations and can prioritize I/O requests.
23,"Choosing the best suited I/O elevator not only depends on the workload,"
23,"but on the hardware, too. Single ATA disk systems, SSDs, RAID arrays, or"
23,"network storage systems, for example, each require different tuning"
23,strategies.
23,13.1 Switching I/O scheduling #
23,"SUSE Linux Enterprise Server picks a default I/O scheduler at boot-time, which can be"
23,changed on the fly per block device. This makes it possible to set different
23,"algorithms, for example, for the device hosting the system partition and the"
23,device hosting a database.
23,The default I/O scheduler is chosen for each device based on whether the
23,"device reports to be rotational disk or not. For rotational disks, the"
23,BFQ I/O scheduler is picked.
23,Other devices default to MQ-DEADLINE or NONE.
23,"To change the elevator for a specific device in the running system, run"
23,the following command:
23,> sudo echo SCHEDULER > /sys/block/DEVICE/queue/scheduler
23,"Here, SCHEDULER is one of"
23,"bfq, none,"
23,"kyber, or mq-deadline."
23,DEVICE is the block device
23,(sda for example). Note that this change will not
23,persist during reboot. For permanent I/O scheduler change for a particular
23,"device, copy /usr/lib/udev/rules.d/60-io-scheduler.rules to"
23,"/etc/udev/rules.d/60-io-scheduler.rules, and edit"
23,the latter file to suit your needs.
23,Note: Default scheduler on IBM Z
23,"On IBM Z, the default I/O scheduler for a storage device is"
23,set by the device driver.
23,Note: elevator boot parameter removed
23,"The elevator boot parameter has been removed. The blk-mq I/O path replaces cfq, and does not include the"
23,elevator boot parameter.
23,13.2 Available I/O elevators with blk-mq I/O path #
23,Below is a list of elevators available on SUSE Linux Enterprise Server for devices
23,that use the blk-mq I/O path.
23,"If an elevator has tunable parameters, they can be set with the"
23,command:
23,echo VALUE > /sys/block/DEVICE/queue/iosched/TUNABLE
23,"In the command above, VALUE is the desired value for the"
23,TUNABLE and DEVICE is
23,the block device.
23,To find out what elevators are available for a device
23,"(sda for example), run the following"
23,command (the currently selected scheduler is listed in brackets):
23,> cat /sys/block/sda/queue/scheduler
23,[mq-deadline] kyber bfq noneNote: Scheduler options when switching from
23,Legacy Block to blk-mq I/O path
23,"When switching from legacy block to blk-mq I/O path for a device,"
23,the none option is roughly comparable to
23,"noop, mq-deadline is comparable"
23,"to deadline, and bfq is"
23,comparable to cfq.
23,13.2.1 MQ-DEADLINE #
23,MQ-DEADLINE is a
23,latency-oriented I/O scheduler. MQ-DEADLINE has the following
23,tunable parameters:
23,Table 13.1: MQ-DEADLINE tunable parameters #  FileDescriptionwrites_starved Controls how many times reads are preferred
23,over writes. A value of 3 means that
23,three read operations can be done before writes and reads
23,are dispatched on the same selection criteria.
23,Default is 3.
23,read_expire Sets the deadline (current time plus the
23,read_expire value) for read operations in milliseconds.
23,Default is 500.
23,write_expire Sets the deadline (current time plus the
23,write_expire value) for write operations in
23,milliseconds.
23,Default is 5000.
23,front_merges Enables (1) or disables (0) attempts to front
23,merge requests.
23,Default is 1.fifo_batch Sets the maximum number of requests per batch
23,(deadline expiration is only checked for batches). This
23,parameter allows to balance between latency and
23,"throughput. When set to 1 (that is, one"
23,"request per batch), it results in ""first come, first served"""
23,behavior and usually lowest latency. Higher values usually
23,increase throughput.
23,Default is 16.
23,13.2.2 NONE #
23,When NONE is selected
23,"as I/O elevator option for blk-mq, no I/O scheduler"
23,"is used, and I/O requests are passed down to the"
23,device without further I/O scheduling interaction.
23,NONE is the default for
23,NVM Express devices. With no overhead compared to other I/O
23,"elevator options, it is considered the fastest way of passing down"
23,I/O requests on multiple queues to such devices.
23,There are no tunable parameters for NONE.
23,13.2.3 BFQ (Budget Fair Queueing) #
23,BFQ is a
23,"fairness-oriented scheduler. It is described as ""a"
23,proportional-share storage-I/O scheduling algorithm based on the
23,"slice-by-slice service scheme of CFQ. But BFQ assigns budgets,"
23,"measured in number of sectors, to processes instead of time"
23,"slices."" (Source:"
23,linux-4.12/block/bfq-iosched.c)
23,BFQ allows to assign
23,I/O priorities to tasks which are taken into account during
23,"scheduling decisions (see Section 9.3.3, “Prioritizing disk access with ionice”)."
23,BFQ scheduler has the
23,following tunable parameters:
23,Table 13.2: BFQ tunable parameters #  FileDescriptionslice_idleValue in milliseconds specifies how long to
23,"idle, waiting for next request on an empty queue."
23,Default is 8.
23,slice_idle_usSame as slice_idle but in
23,microseconds.
23,Default is 8000.
23,low_latencyEnables (1) or disables (0) BFQ's low latency mode. This
23,"mode prioritizes certain applications (for example, if interactive)"
23,such that they observe lower latency.
23,Default is 1.
23,back_seek_max Maximum value (in Kbytes) for backward seeking.
23,Default is 16384.
23,back_seek_penalty Used to compute the cost of backward seeking.
23,Default is 2.
23,fifo_expire_async Value (in milliseconds) is used to set the
23,timeout of asynchronous requests.
23,Default is 250.
23,fifo_expire_sync Value in milliseconds specifies the
23,timeout of synchronous requests.
23,Default is 125.
23,timeout_sync Maximum time in milliseconds that a task
23,(queue) is serviced after it has been selected.
23,Default is 124.
23,max_budget Limit for number of sectors that are served
23,at maximum within timeout_sync. If set to
23,0 BFQ internally calculates a
23,value based on timeout_sync and an
23,estimated peak rate.
23,Default is 0
23,(set to auto-tuning). strict_guarantees Enables (1) or disables (0) BFQ specific queue handling
23,required to give stricter bandwidth sharing guarantees
23,under certain conditions.
23,Default is 0.
23,13.2.4 KYBER #
23,KYBER is a
23,latency-oriented I/O scheduler. It makes it possible to set target latencies
23,for reads and synchronous writes and throttles I/O requests in
23,order to try to meet these target latencies.
23,Table 13.3: KYBER tunable parameters #  FileDescriptionread_lat_nsecSets the target latency for read operations in
23,nanoseconds.
23,Default is 2000000.
23,write_lat_nsecSets the target latency for write operations in
23,nanoseconds.
23,Default is 10000000.
23,13.3 I/O barrier tuning #
23,"Some file systems (for example, Ext3 or Ext4) send write"
23,barriers to disk after fsync or during transaction commits. Write
23,"barriers enforce proper ordering of writes, making volatile disk write"
23,caches safe to use (at some performance penalty). If your disks are
23,"battery-backed in one way or another, disabling barriers can safely"
23,improve performance.
23,Important: nobarrier is deprecated in XFS
23,Note that the nobarrier option has been completely deprecated
23,"for XFS, and it is not a valid mount option in SUSE Linux Enterprise 15 SP2 and upward. Any"
23,XFS mount command that explicitly specifies the flag will fail to mount the
23,"file system. To prevent this from happening, make sure that no scripts or"
23,fstab entries contain the nobarrier option.
23,Sending write barriers can be disabled using the
23,nobarrier mount option.
23,Warning: Disabling barriers can lead to data loss
23,Disabling barriers when disks cannot guarantee caches are properly
23,written in case of power failure can lead to severe file system
23,corruption and data loss.
23,14 Tuning the task scheduler #
23,"Modern operating systems, such as SUSE® Linux Enterprise Server, normally run many"
23,"tasks at the same time. For example, you can be searching in a"
23,text file while receiving an e-mail and copying a big file to an external
23,hard disk. These simple tasks require many additional processes to be run
23,"by the system. To provide each task with its required system resources,"
23,the Linux kernel needs a tool to distribute available system resources to
23,individual tasks. And this is exactly what the task
23,scheduler does.
23,The following sections explain the most important terms related to a
23,process scheduling. They also introduce information about the task
23,"scheduler policy, scheduling algorithm, description of the task scheduler"
23,"used by SUSE Linux Enterprise Server, and references to other sources of relevant"
23,information.
23,14.1 Introduction #
23,The Linux kernel controls the way that tasks (or processes) are managed
23,"on the system. The task scheduler, sometimes called process"
23,"scheduler, is the part of the kernel that decides which task"
23,to run next. It is responsible for best using system resources to
23,guarantee that multiple tasks are being executed simultaneously. This
23,makes it a core component of any multitasking operating system.
23,14.1.1 Preemption #
23,The theory behind task scheduling is very simple. If there are runnable
23,"processes in a system, at least one process must always be running. If"
23,"there are more runnable processes than processors in a system, not all"
23,the processes can be running all the time.
23,"Therefore, some processes need to be stopped temporarily, or"
23,"suspended, so that others can be running again. The"
23,scheduler decides what process in the queue will run next.
23,"As already mentioned, Linux, like all other Unix variants, is a"
23,multitasking operating system. That means that
23,several tasks can be running at the same time. Linux provides a so
23,"called preemptive multitasking, where the scheduler"
23,decides when a process is suspended. This forced suspension is called
23,preemption. All Unix flavors have been providing
23,preemptive multitasking since the beginning.
23,14.1.2 Timeslice #
23,The time period for which a process will be running before it is
23,preempted is defined in advance. It is called a
23,timeslice of a process and represents the amount of
23,processor time that is provided to each process. By assigning
23,"timeslices, the scheduler makes global decisions for the running system,"
23,and prevents individual processes from dominating over the processor
23,resources.
23,14.1.3 Process priority #
23,The scheduler evaluates processes based on their priority. To calculate
23,"the current priority of a process, the task scheduler uses complex"
23,"algorithms. As a result, each process is given a value according to"
23,which it is “allowed” to run on a processor.
23,14.2 Process classification #
23,Processes are usually classified according to their purpose and behavior.
23,"Although the borderline is not always clearly distinct, generally two"
23,criteria are used to sort them. These criteria are independent and do not
23,exclude each other.
23,One approach is to classify a process either
23,I/O-bound or processor-bound.
23,I/O-bound
23,"I/O stands for Input/Output devices, such as keyboards, mice, or"
23,optical and hard disks. I/O-bound processes spend
23,the majority of time submitting and waiting for requests. They are run
23,"very frequently, but for short time intervals, not to block other"
23,processes waiting for I/O requests.
23,processor-bound
23,"On the other hand, processor-bound tasks use"
23,"their time to execute a code, and usually run until they are preempted"
23,by the scheduler. They do not block processes waiting for I/O
23,"requests, and, therefore, can be run less frequently but for longer"
23,time intervals.
23,Another approach is to divide processes by type into
23,"interactive, batch, and"
23,real-time processes.
23,Interactive processes spend a lot of time waiting
23,"for I/O requests, such as keyboard or mouse operations. The scheduler"
23,"must wake up such processes quickly on user request, or the user will"
23,find the environment unresponsive. The typical delay is approximately
23,"100 ms. Office applications, text editors or image manipulation"
23,programs represent typical interactive processes.
23,Batch processes often run in the background and do
23,not need to be responsive. They usually receive lower priority from the
23,"scheduler. Multimedia converters, database search engines, or log files"
23,analyzers are typical examples of batch processes.
23,Real-time processes must never be blocked by
23,"low-priority processes, and the scheduler guarantees a short response"
23,time to them. Applications for editing multimedia content are a good
23,example here.
23,14.3 Completely Fair Scheduler #
23,"Since the Linux kernel version 2.6.23, a new approach has been taken to"
23,the scheduling of runnable processes. Completely Fair Scheduler (CFS)
23,"became the default Linux kernel scheduler. Since then, important changes"
23,and improvements have been made. The information in this chapter applies
23,to SUSE Linux Enterprise Server with kernel version 2.6.32 and higher (including 3.x
23,"kernels). The scheduler environment was divided into several parts, and"
23,three main new features were introduced:
23,Modular scheduler core
23,The core of the scheduler was enhanced with scheduling
23,classes. These classes are modular and represent scheduling
23,policies.
23,Completely Fair Scheduler
23,"Introduced in kernel 2.6.23 and extended in 2.6.24, CFS tries to"
23,assure that each process obtains its “fair” share of the
23,processor time.
23,Group scheduling
23,"For example, if you split processes into groups according to which"
23,"user is running them, CFS tries to provide each of these groups with"
23,the same amount of processor time.
23,"As a result, CFS brings optimized scheduling for both servers and"
23,desktops.
23,14.3.1 How CFS works #
23,CFS tries to guarantee a fair approach to each runnable task. To find
23,"the most balanced way of task scheduling, it uses the concept of"
23,red-black tree. A red-black tree is a type of
23,self-balancing data search tree which provides inserting and removing
23,entries in a reasonable way so that it remains well balanced.
23,When CFS schedules a task it accumulates “virtual
23,runtime” or vruntime. The next task picked
23,to run is always the task with the minimum accumulated vruntime so
23,far. By balancing the red-black tree when tasks are inserted into the
23,run queue (a planned time line of processes to be
23,"executed next), the task with the minimum vruntime is always the first"
23,entry in the red-black tree.
23,The amount of vruntime a task accrues is related to its priority.
23,High priority tasks gain vruntime at a slower rate than low priority
23,"tasks, which results in high priority tasks being picked to run on the"
23,processor more often.
23,14.3.2 Grouping processes #
23,"Since the Linux kernel version 2.6.24, CFS can be tuned to be fair"
23,to groups rather than to tasks only. Runnable tasks are then grouped
23,"to form entities, and CFS tries to be fair to these entities instead"
23,of individual runnable tasks. The scheduler also tries to be fair to
23,individual tasks within these entities.
23,The kernel scheduler lets you group runnable tasks using control
23,"groups. For more information, see Chapter 10, Kernel control groups."
23,14.3.3 Kernel configuration options #
23,Basic aspects of the task scheduler behavior can be set through the
23,kernel configuration options. Setting these options is part of the
23,kernel compilation process. Because kernel compilation process is a
23,"complex task and out of this document's scope, refer to relevant source"
23,of information.
23,Warning: Kernel compilation
23,"If you run SUSE Linux Enterprise Server on a kernel that was not shipped with it,"
23,"for example on a self-compiled kernel, you lose the entire support"
23,entitlement.
23,14.3.4 Terminology #
23,Documents regarding task scheduling policy often use several technical
23,terms which you need to know to understand the information correctly.
23,Here are some:
23,Latency
23,Delay between the time a process is scheduled to run and the actual
23,process execution.
23,Granularity
23,The relation between granularity and latency can be expressed by the
23,following equation:
23,gran = ( lat / rtasks ) - ( lat / rtasks / rtasks )
23,"where gran stands for granularity,"
23,"lat stand for latency, and"
23,rtasks is the number of running tasks.
23,14.3.4.1 Scheduling policies #
23,The Linux kernel supports the following scheduling policies:
23,SCHED_FIFO
23,Scheduling policy designed for special time-critical applications.
23,It uses the First In-First Out scheduling algorithm.
23,SCHED_BATCH
23,Scheduling policy designed for CPU-intensive tasks.
23,SCHED_IDLE
23,Scheduling policy intended for very low
23,prioritized tasks.
23,SCHED_OTHER
23,Default Linux time-sharing scheduling policy used by the majority of
23,processes.
23,SCHED_RR
23,"Similar to SCHED_FIFO, but uses the Round"
23,Robin scheduling algorithm.
23,14.3.5 Changing real-time attributes of processes with chrt #
23,The chrt command sets or retrieves the real-time
23,"scheduling attributes of a running process, or runs a command with the"
23,specified attributes. You can get or retrieve both the scheduling policy
23,and priority of a process.
23,"In the following examples, a process whose PID is 16244 is used."
23,To retrieve the real-time attributes of an existing
23,task:
23,# chrt -p 16244
23,pid 16244's current scheduling policy: SCHED_OTHER
23,pid 16244's current scheduling priority: 0
23,"Before setting a new scheduling policy on the process, you need to find"
23,out the minimum and maximum valid priorities for each scheduling
23,algorithm:
23,# chrt -m
23,SCHED_SCHED_OTHER min/max priority : 0/0
23,SCHED_SCHED_FIFO min/max priority : 1/99
23,SCHED_SCHED_RR min/max priority : 1/99
23,SCHED_SCHED_BATCH min/max priority : 0/0
23,SCHED_SCHED_IDLE min/max priority : 0/0
23,"In the above example, SCHED_OTHER, SCHED_BATCH, SCHED_IDLE polices only"
23,"allow for priority 0, while that of SCHED_FIFO and SCHED_RR can range"
23,from 1 to 99.
23,To set SCHED_BATCH scheduling policy:
23,# chrt -b -p 0 16244
23,pid 16244's current scheduling policy: SCHED_BATCH
23,pid 16244's current scheduling priority: 0
23,"For more information on chrt, see its man page"
23,(man 1 chrt).
23,14.3.6 Runtime tuning with sysctl #
23,The sysctl interface for examining and changing
23,kernel parameters at runtime introduces important variables by means of
23,which you can change the default behavior of the task scheduler. The
23,"syntax of the sysctl is simple, and all the following"
23,commands must be entered on the command line as root.
23,"To read a value from a kernel variable, enter"
23,# sysctl VARIABLE
23,"To assign a value, enter"
23,# sysctl VARIABLE=VALUE
23,"To get a list of all scheduler related variables, run the"
23,"sysctl command, and use grep"
23,to filter the output:
23,"# sysctl -A | grep ""sched"" | grep -v ""domain"""
23,kernel.sched_cfs_bandwidth_slice_us = 5000
23,kernel.sched_child_runs_first = 0
23,kernel.sched_compat_yield = 0
23,kernel.sched_latency_ns = 24000000
23,kernel.sched_migration_cost_ns = 500000
23,kernel.sched_min_granularity_ns = 8000000
23,kernel.sched_nr_migrate = 32
23,kernel.sched_rr_timeslice_ms = 25
23,kernel.sched_rt_period_us = 1000000
23,kernel.sched_rt_runtime_us = 950000
23,kernel.sched_schedstats = 0
23,kernel.sched_shares_window_ns = 10000000
23,kernel.sched_time_avg_ms = 1000
23,kernel.sched_tunable_scaling = 1
23,kernel.sched_wakeup_granularity_ns = 10000000
23,Note that variables ending with “_ns” and
23,"“_us” accept values in nanoseconds and microseconds,"
23,respectively.
23,A list of the most important task scheduler sysctl
23,tuning variables (located at /proc/sys/kernel/)
23,with a short description follows:
23,sched_cfs_bandwidth_slice_us
23,"When CFS bandwidth control is in use, this parameter controls"
23,the amount of run-time (bandwidth) transferred to a run queue from the
23,task's control group bandwidth pool. Small values allow the global
23,"bandwidth to be shared in a fine-grained manner among tasks, larger"
23,values reduce transfer overhead. See
23,https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt.
23,sched_child_runs_first
23,A freshly forked child runs before the parent continues execution.
23,Setting this parameter to 1 is beneficial for an
23,application in which the child performs an execution after fork. For
23,example make
23,-j<NO_CPUS>
23,performs better when sched_child_runs_first is turned off. The
23,default value is 0.
23,sched_compat_yield
23,Enables the aggressive CPU yielding behavior of the old
23,O(1) scheduler by moving the relinquishing task to
23,the end of the runnable queue (right-most position in the red-black
23,tree). Applications that depend on the sched_yield(2)
23,syscall behavior may see performance improvements by giving other
23,processes a chance to run when there are highly contended resources
23,"(such as locks). On the other hand, given that this call occurs in"
23,"context switching, misusing the call can hurt the workload. Only use it"
23,when you see a drop in performance. The default value is
23,sched_migration_cost_ns
23,Amount of time after the last execution that a task is considered to
23,be “cache hot” in migration decisions. A
23,"“hot” task is less likely to be migrated to another CPU,"
23,so increasing this variable reduces task migrations. The default value is
23,500000 (ns).
23,If the CPU idle time is higher than expected when there are runnable
23,"processes, try reducing this value. If tasks bounce between CPUs or"
23,"nodes too often, try increasing it."
23,sched_latency_ns
23,Targeted preemption latency for CPU bound tasks. Increasing this
23,variable increases a CPU bound task's timeslice. A task's timeslice
23,is its weighted fair share of the scheduling period:
23,timeslice = scheduling period * (task's weight/total weight of tasks
23,in the run queue)
23,The task's weight depends on the task's nice level and the scheduling
23,"policy. Minimum task weight for a SCHED_OTHER task is 15,"
23,"corresponding to nice 19. The maximum task weight is 88761,"
23,corresponding to nice -20.
23,Timeslices become smaller as the load increases. When the number of
23,runnable tasks exceeds
23,"sched_latency_ns/sched_min_granularity_ns,"
23,the slice becomes number_of_running_tasks *
23,"sched_min_granularity_ns. Prior to that, the"
23,slice is equal to sched_latency_ns.
23,This value also specifies the maximum amount of time during which a
23,sleeping task is considered to be running for entitlement
23,calculations. Increasing this variable increases the amount of time a
23,"waking task may consume before being preempted, thus increasing"
23,scheduler latency for CPU bound tasks. The default value is
23,6000000 (ns).
23,sched_min_granularity_ns
23,Minimal preemption granularity for CPU bound tasks. See
23,sched_latency_ns for details. The default
23,value is 4000000 (ns).
23,sched_wakeup_granularity_ns
23,The wake-up preemption granularity. Increasing this variable reduces
23,"wake-up preemption, reducing disturbance of compute bound tasks."
23,Lowering it improves wake-up latency and throughput for latency
23,"critical tasks, particularly when a short duty cycle load component"
23,must compete with CPU bound components. The default value is
23,2500000 (ns).
23,Warning: Setting the right wake-up granularity value
23,Settings larger than half of
23,sched_latency_ns will result in no wake-up
23,preemption. Short duty cycle tasks will be unable to compete with
23,CPU hogs effectively.
23,sched_rr_timeslice_ms
23,Quantum that SCHED_RR tasks are allowed to run before they are
23,preempted and put to the end of the task list.
23,sched_rt_period_us
23,Period over which real-time task bandwidth enforcement is measured.
23,The default value is 1000000 (µs).
23,sched_rt_runtime_us
23,Quantum allocated to real-time tasks during sched_rt_period_us.
23,"Setting to -1 disables RT bandwidth enforcement. By default, RT tasks"
23,"may consume 95%CPU/sec, thus leaving 5%CPU/sec or 0.05s to be used by"
23,SCHED_OTHER tasks. The default value is 950000
23,(µs).
23,sched_nr_migrate
23,Controls how many tasks can be migrated across processors for
23,load-balancing purposes. Because balancing iterates the runqueue
23,"with interrupts disabled (softirq), it can incur in irq-latency"
23,penalties for real-time tasks.
23,Therefore increasing this value
23,may give a performance boost to large SCHED_OTHER threads at the
23,expense of increased irq-latencies for real-time tasks. The default
23,value is 32.
23,sched_time_avg_ms
23,This parameter sets the period over which the time spent running
23,real-time tasks is averaged. That average assists CFS in making
23,load-balancing decisions and gives an indication of how busy a CPU is
23,with high-priority real-time tasks.
23,The optimal setting for this parameter is highly workload
23,"dependent and depends, among other things, on how frequently"
23,real-time tasks are running and for how long.
23,14.3.7 Debugging interface and scheduler statistics #
23,"CFS comes with a new improved debugging interface, and provides runtime"
23,statistics information. Relevant files were added to the
23,"/proc file system, which can be examined simply"
23,with the cat or less command. A
23,list of the related /proc files follows with their
23,short description:
23,/proc/sched_debug
23,"Contains the current values of all tunable variables (see Section 14.3.6, “Runtime tuning with sysctl”) that affect the task"
23,"scheduler behavior, CFS statistics, and information about the run queues"
23,"(CFS, RT and deadline) on all available processors."
23,A summary of the
23,"task running on each processor is also shown, with the task name and"
23,"PID, along with scheduler specific statistics. The first"
23,"being tree-key column, it indicates the task's virtual"
23,"runtime, and its name comes from the kernel sorting all runnable tasks"
23,by this key in a red-black tree. The switches column
23,"indicates the total number of switches (involuntary or not), and"
23,naturally the prio refers to the process priority. The
23,wait-time value indicates the amount of time the task
23,waited to be scheduled. Finally both sum-exec and
23,sum-sleep account for the total amount of time (in
23,"nanoseconds) the task was running on the processor or asleep,"
23,respectively.
23,# cat /proc/sched_debug
23,"Sched Debug Version: v0.11, 4.4.21-64-default #1"
23,ktime
23,: 23533900.395978
23,sched_clk
23,: 23543587.726648
23,cpu_clk
23,: 23533900.396165
23,jiffies
23,: 4300775771
23,sched_clock_stable
23,: 0
23,sysctl_sched
23,.sysctl_sched_latency
23,: 6.000000
23,.sysctl_sched_min_granularity
23,: 2.000000
23,.sysctl_sched_wakeup_granularity
23,: 2.500000
23,.sysctl_sched_child_runs_first
23,: 0
23,.sysctl_sched_features
23,: 154871
23,.sysctl_sched_tunable_scaling
23,: 1 (logaritmic)
23,"cpu#0, 2666.762 MHz"
23,.nr_running
23,: 1
23,.load
23,: 1024
23,.nr_switches
23,: 1918946
23,[...]
23,cfs_rq[0]:/
23,.exec_clock
23,: 170176.383770
23,.MIN_vruntime
23,: 0.000001
23,.min_vruntime
23,: 347375.854324
23,.max_vruntime
23,: 0.000001
23,[...]
23,rt_rq[0]:/
23,.rt_nr_running
23,: 0
23,.rt_throttled
23,: 0
23,.rt_time
23,: 0.000000
23,.rt_runtime
23,: 950.000000
23,dl_rq[0]:
23,.dl_nr_running
23,: 0
23,task
23,PID
23,tree-key
23,switches
23,prio
23,wait-time
23,[...]
23,------------------------------------------------------------------------
23,cc1 63477
23,98876.717832
23,197
23,120
23,0.000000
23,.../proc/schedstat
23,Displays statistics relevant to the current run queue. Also
23,domain-specific statistics for SMP systems are displayed for all
23,"connected processors. Because the output format is not user-friendly,"
23,read the contents of
23,/usr/src/linux/Documentation/scheduler/sched-stats.txt
23,for more information.
23,/proc/PID/sched
23,Displays scheduling information on the process with id
23,PID.
23,# cat /proc/$(pidof gdm)/sched
23,"gdm (744, #threads: 3)"
23,-------------------------------------------------------------------
23,se.exec_start
23,8888.758381
23,se.vruntime
23,6062.853815
23,se.sum_exec_runtime
23,7.836043
23,se.statistics.wait_start
23,0.000000
23,se.statistics.sleep_start
23,8888.758381
23,se.statistics.block_start
23,0.000000
23,se.statistics.sleep_max
23,1965.987638
23,[...]
23,se.avg.decay_count
23,8477
23,policy
23,prio
23,120
23,clock-delta
23,128
23,mm->numa_scan_seq
23,"numa_migrations, 0"
23,"numa_faults_memory, 0, 0, 1, 0, -1"
23,"numa_faults_memory, 1, 0, 0, 0, -114.4 More information #"
23,"To get a compact knowledge about Linux kernel task scheduling, you need"
23,to explore several information sources. Here are some:
23,"For task scheduler System Calls description, see the relevant manual"
23,page (for example man 2 sched_setaffinity).
23,A useful lecture on Linux scheduler policy and algorithm is available
23,http://www.inf.fu-berlin.de/lehre/SS01/OS/Lectures/Lecture08.pdf.
23,A good overview of Linux process scheduling is given in
23,Linux Kernel Development by Robert Love
23,(ISBN-10: 0-672-32512-8). See
23,https://www.informit.com/articles/article.aspx?p=101760.
23,A very comprehensive overview of the Linux kernel internals is given in
23,Understanding the Linux Kernel by Daniel P.
23,Bovet and Marco Cesati (ISBN 978-0-596-00565-8).
23,Technical information about task scheduler is covered in files under
23,/usr/src/linux/Documentation/scheduler.
23,15 Tuning the memory management subsystem #
23,To understand and tune the memory management behavior of the
23,"kernel, it is important to first have an overview of how it works and"
23,cooperates with other subsystems.
23,"The memory management subsystem, also called the virtual memory manager,"
23,will subsequently be called “VM”. The role of the VM
23,is to manage the allocation of physical memory (RAM) for the entire kernel
23,and user programs. It is also responsible for providing a virtual memory
23,environment for user processes (managed via POSIX APIs with Linux
23,"extensions). Finally, the VM is responsible for freeing up RAM when there"
23,"is a shortage, either by trimming caches or swapping out"
23,“anonymous” memory.
23,The most important thing to understand when examining and tuning VM is how
23,its caches are managed. The basic goal of the VM's caches is to minimize
23,the cost of I/O as generated by swapping and file system operations
23,(including network file systems). This is achieved by avoiding I/O
23,"completely, or by submitting I/O in better patterns."
23,Free memory will be used and filled up by these caches as required. The
23,"more memory is available for caches and anonymous memory, the more"
23,"effectively caches and swapping will operate. However, if a memory"
23,"shortage is encountered, caches will be trimmed or memory will be swapped"
23,out.
23,"For a particular workload, the first thing that can be done to improve"
23,performance is to increase memory and reduce the frequency that memory
23,must be trimmed or swapped. The second thing is to change the way caches
23,are managed by changing kernel parameters.
23,"Finally, the workload itself should be examined and tuned as well. If an"
23,"application is allowed to run more processes or threads, effectiveness of"
23,"VM caches can be reduced, if each process is operating in its own area of"
23,the file system. Memory overheads are also increased. If applications
23,"allocate their own buffers or caches, larger caches will mean that less"
23,"memory is available for VM caches. However, more processes and threads can"
23,"mean more opportunity to overlap and pipeline I/O, and may take better"
23,advantage of multiple cores. Experimentation will be required for the best
23,results.
23,15.1 Memory usage #
23,Memory allocations in general can be characterized as
23,"“pinned” (also known as “unreclaimable”),"
23,“reclaimable” or “swappable”.
23,15.1.1 Anonymous memory #
23,"Anonymous memory tends to be program heap and stack memory (for example,"
23,">malloc()). It is reclaimable, except in special"
23,cases such as mlock or if there is no available swap
23,space. Anonymous memory must be written to swap before it can be
23,reclaimed. Swap I/O (both swapping in and swapping out pages) tends to
23,"be less efficient than pagecache I/O, because of allocation and access"
23,patterns.
23,15.1.2 Pagecache #
23,"A cache of file data. When a file is read from disk or network, the"
23,"contents are stored in pagecache. No disk or network access is required,"
23,if the contents are up-to-date in pagecache. tmpfs and shared memory
23,segments count toward pagecache.
23,"When a file is written to, the new data is stored in pagecache before"
23,being written back to a disk or the network (making it a write-back
23,"cache). When a page has new data not written back yet, it is called"
23,“dirty”. Pages not classified as dirty are
23,“clean”. Clean pagecache pages can be reclaimed if there is
23,a memory shortage by simply freeing them. Dirty pages must first be made
23,clean before being reclaimed.
23,15.1.3 Buffercache #
23,"This is a type of pagecache for block devices (for example, /dev/sda). A"
23,file system typically uses the buffercache when accessing its on-disk
23,"metadata structures such as inode tables, allocation bitmaps, and so"
23,forth. Buffercache can be reclaimed similarly to pagecache.
23,15.1.4 Buffer heads #
23,Buffer heads are small auxiliary structures that tend to be allocated
23,upon pagecache access. They can generally be reclaimed easily when the
23,pagecache or buffercache pages are clean.
23,15.1.5 Writeback #
23,"As applications write to files, the pagecache becomes dirty"
23,and the buffercache may become dirty. When the amount of
23,dirty memory reaches a specified number of pages in bytes
23,"(vm.dirty_background_bytes), or when the"
23,amount of dirty memory reaches a specific ratio to total memory
23,"(vm.dirty_background_ratio), or when the pages"
23,have been dirty for longer than a specified amount of time
23,"(vm.dirty_expire_centisecs), the kernel begins"
23,writeback of pages starting with files that had the pages dirtied first.
23,The background bytes and ratios are mutually exclusive and setting one
23,will overwrite the other. Flusher threads perform writeback in the
23,background and allow applications to continue running. If the I/O
23,"cannot keep up with applications dirtying pagecache, and dirty data"
23,reaches a critical setting (vm.dirty_bytes or
23,"vm.dirty_ratio), then applications begin to be"
23,throttled to prevent dirty data exceeding this threshold.
23,15.1.6 Readahead #
23,The VM monitors file access patterns and may attempt to perform
23,readahead. Readahead reads pages into the pagecache from the file system
23,"that have not been requested yet. It is done to allow fewer,"
23,larger I/O requests to be submitted (more efficient). And for I/O to be
23,pipelined (I/O performed at the same time as the application is
23,running).
23,15.1.7 VFS caches #  15.1.7.1 Inode cache #
23,This is an in-memory cache of the inode structures for each file
23,"system. These contain attributes such as the file size, permissions and"
23,"ownership, and pointers to the file data."
23,15.1.7.2 Directory entry cache #
23,This is an in-memory cache of the directory entries in the system.
23,"These contain a name (the name of a file), the inode which it refers"
23,"to, and children entries. This cache is used when traversing the"
23,directory structure and accessing a file by name.
23,15.2 Reducing memory usage #  15.2.1 Reducing malloc (anonymous) usage #
23,Applications running on SUSE Linux Enterprise Server 15 SP3 can allocate
23,more memory compared to older releases. This is because of
23,glibc changing its default
23,behavior while allocating user space memory. See
23,http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html
23,for explanation of these parameters.
23,"To restore behavior similar to older releases, M_MMAP_THRESHOLD should"
23,be set to 128*1024. This can be done with mallopt() call from the
23,"application, or via setting MALLOC_MMAP_THRESHOLD_"
23,environment variable before running the application.
23,15.2.2 Reducing kernel memory overheads #
23,"Kernel memory that is reclaimable (caches, described above) will be"
23,trimmed automatically during memory shortages. Most other kernel memory
23,cannot be easily reduced but is a property of the workload given to the
23,kernel.
23,Reducing the requirements of the user space workload will reduce the
23,"kernel memory usage (fewer processes, fewer open files and sockets,"
23,etc.)
23,15.2.3 Memory controller (memory cgroups) #
23,"If the memory cgroups feature is not needed, it can be switched off by"
23,"passing cgroup_disable=memory on the kernel command line, reducing"
23,memory consumption of the kernel a bit. There is also a slight
23,performance benefit as there is a small amount of accounting overhead
23,when memory cgroups are available even if none are configured.
23,15.3 Virtual memory manager (VM) tunable parameters #
23,When tuning the VM it should be understood that some
23,changes will
23,take time to affect the workload and take full effect. If the workload
23,"changes throughout the day, it may behave very differently at different"
23,times. A change that increases throughput under some conditions may
23,decrease it under other conditions.
23,15.3.1 Reclaim ratios #  /proc/sys/vm/swappiness
23,This control is used to define how aggressively the kernel swaps out
23,anonymous memory relative to pagecache and other caches. Increasing
23,the value increases the amount of swapping. The default value is
23,60.
23,"Swap I/O tends to be much less efficient than other I/O. However,"
23,some pagecache pages will be accessed much more frequently than less
23,used anonymous memory. The right balance should be found here.
23,"If swap activity is observed during slowdowns, it may be worth"
23,reducing this parameter. If there is a lot of I/O activity and the
23,"amount of pagecache in the system is rather small, or if there are"
23,"large dormant applications running, increasing this value might"
23,improve performance.
23,"Note that the more data is swapped out, the longer the system will"
23,take to swap data back in when it is needed.
23,/proc/sys/vm/vfs_cache_pressure
23,This variable controls the tendency of the kernel to reclaim the
23,"memory which is used for caching of VFS caches, versus pagecache and"
23,swap. Increasing this value increases the rate at which VFS caches
23,are reclaimed.
23,"It is difficult to know when this should be changed, other than by"
23,experimentation. The slabtop command (part of the
23,package procps) shows top
23,"memory objects used by the kernel. The vfs caches are the ""dentry"""
23,"and the ""*_inode_cache"" objects. If these are consuming a large"
23,"amount of memory in relation to pagecache, it may be worth trying to"
23,increase pressure. Could also help to reduce swapping. The default
23,value is 100.
23,/proc/sys/vm/min_free_kbytes
23,This controls the amount of memory that is kept free for use by
23,special reserves including “atomic” allocations (those
23,which cannot wait for reclaim). This should not normally be lowered
23,unless the system is being very carefully tuned for memory usage
23,(normally useful for embedded rather than server applications). If
23,“page allocation failure” messages and stack traces are
23,"frequently seen in logs, min_free_kbytes could be increased until the"
23,"errors disappear. There is no need for concern, if these messages are"
23,very infrequent. The default value depends on the amount of RAM.
23,/proc/sys/vm/watermark_scale_factor
23,"Broadly speaking, free memory has high, low and min watermarks. When"
23,the low watermark is reached then kswapd wakes to
23,reclaim memory in the background. It stays awake until free memory
23,reaches the high watermark. Applications will stall and reclaim
23,memory when the min watermark is reached.
23,The watermark_scale_factor defines the amount
23,of memory left in a node/system before kswapd is woken up and how
23,much memory needs to be free before kswapd goes back to sleep.
23,"The unit is in fractions of 10,000. The default value of 10 means"
23,the distances between watermarks are 0.1% of the available memory
23,"in the node/system. The maximum value is 1000, or 10% of memory."
23,"Workloads that frequently stall in direct reclaim, accounted by"
23,"allocstall in /proc/vmstat,"
23,"may benefit from altering this parameter. Similarly, if"
23,"kswapd is sleeping prematurely, as accounted for by"
23,"kswapd_low_wmark_hit_quickly, then it may indicate"
23,that the number of pages kept free to avoid stalls is too low.
23,15.3.2 Writeback parameters #
23,One important change in writeback behavior since SUSE Linux Enterprise Server 10 is
23,that modification to file-backed mmap() memory is accounted immediately
23,as dirty memory (and subject to writeback). Whereas previously it would
23,"only be subject to writeback after it was unmapped, upon an msync()"
23,"system call, or under heavy memory pressure."
23,Some applications do not expect mmap modifications to be subject to such
23,"writeback behavior, and performance can be reduced. Increasing writeback"
23,ratios and times can improve this type of slowdown.
23,/proc/sys/vm/dirty_background_ratio
23,This is the percentage of the total amount of free and reclaimable
23,"memory. When the amount of dirty pagecache exceeds this percentage,"
23,writeback threads start writing back dirty memory. The default value
23,is 10 (%).
23,/proc/sys/vm/dirty_background_bytes
23,This contains the amount of dirty memory at which
23,the background kernel flusher threads will start writeback.
23,dirty_background_bytes is the counterpart of
23,"dirty_background_ratio. If one of them is set,"
23,the other one will automatically be read as 0.
23,/proc/sys/vm/dirty_ratio
23,Similar percentage value as for
23,"dirty_background_ratio. When this is exceeded,"
23,applications that want to write to the pagecache are blocked and
23,wait for kernel background flusher threads to reduce the amount of dirty
23,memory. The default value is 20 (%).
23,/proc/sys/vm/dirty_bytes
23,This file controls the same tunable as dirty_ratio
23,however the amount of dirty memory is in bytes as opposed to a
23,percentage of reclaimable memory. Since both
23,dirty_ratio and dirty_bytes
23,"control the same tunable, if one of them is set, the other one will"
23,automatically be read as 0. The minimum value allowed
23,for dirty_bytes is two pages (in bytes); any value
23,lower than this limit will be ignored and the old configuration will be
23,retained.
23,/proc/sys/vm/dirty_expire_centisecs
23,Data which has been dirty in-memory for longer than this interval
23,will be written out next time a flusher thread wakes up. Expiration
23,is measured based on the modification time of a file's inode.
23,"Therefore, multiple dirtied pages from the same file will all be"
23,written when the interval is exceeded.
23,dirty_background_ratio and
23,dirty_ratio together determine the pagecache
23,"writeback behavior. If these values are increased, more dirty memory is"
23,kept in the system for a longer time. With more dirty memory allowed in
23,"the system, the chance to improve throughput by avoiding writeback I/O"
23,"and to submitting more optimal I/O patterns increases. However, more"
23,dirty memory can either harm latency when memory needs to be reclaimed
23,or at points of data integrity (“synchronization points”) when it
23,needs to be written back to disk.
23,15.3.3 Timing differences of I/O writes between SUSE Linux Enterprise 12 and SUSE Linux Enterprise 11 #
23,The system is required to limit what percentage of the system's memory
23,contains file-backed data that needs writing to disk. This guarantees
23,that the system can always allocate the necessary data structures to
23,complete I/O. The maximum amount of memory that can be dirty and
23,requires writing at any time is controlled by
23,vm.dirty_ratio
23,(/proc/sys/vm/dirty_ratio). The defaults are:
23,SLE-11-SP3:
23,vm.dirty_ratio = 40
23,SLE-12:
23,vm.dirty_ratio = 20
23,The primary advantage of using the lower ratio in SUSE Linux Enterprise 12 is that
23,page reclamation and allocation in low memory situations completes
23,faster as there is a higher probability that old clean pages will be
23,quickly found and discarded. The secondary advantage is that if all
23,"data on the system must be synchronized, then the time to complete the"
23,operation on SUSE Linux Enterprise 12 will be lower than SUSE Linux Enterprise 11 SP3 by default.
23,Most workloads will not notice this change as data is synchronized with
23,fsync() by the application or data is not dirtied
23,quickly enough to hit the limits.
23,"There are exceptions and if your application is affected by this, it"
23,will manifest as an unexpected stall during writes. To prove it is
23,affected by dirty data rate limiting then monitor
23,/proc/PID_OF_APPLICATION/stack
23,and it will be observed that the application spends significant time in
23,balance_dirty_pages_ratelimited. If this is observed
23,"and it is a problem, then increase the value of"
23,vm.dirty_ratio to 40 to restore the SUSE Linux Enterprise 11 SP3
23,behavior.
23,It is important to note that the overall I/O throughput is the same
23,regardless of the setting. The only difference is the timing of when
23,the I/O is queued.
23,This is an example of using dd to asynchronously
23,write 30% of memory to disk which would happen to be affected by the
23,change in vm.dirty_ratio:
23,# MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`
23,# sysctl vm.dirty_ratio=40
23,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
23,"2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s"
23,# sysctl vm.dirty_ratio=20
23,dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
23,"2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s"
23,Note that the parameter affects the time it takes for the command to
23,complete and the apparent write speed of the device. With
23,"dirty_ratio=40, more of the data is cached and"
23,written to disk in the background by the kernel. It is very important
23,to note that the speed of I/O is identical in both cases. To
23,"demonstrate, this is the result when dd synchronizes"
23,the data before exiting:
23,# sysctl vm.dirty_ratio=40
23,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
23,"2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s"
23,# sysctl vm.dirty_ratio=20
23,# dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
23,"2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s"
23,Note that dirty_ratio had almost no impact here and
23,"is within the natural variability of a command. Hence,"
23,dirty_ratio does not directly impact I/O performance
23,but it may affect the apparent performance of a workload that writes
23,data asynchronously without synchronizing.
23,15.3.4 Readahead parameters #  /sys/block/<bdev>/queue/read_ahead_kb
23,"If one or more processes are sequentially reading a file, the kernel"
23,reads some data in advance (ahead) to reduce the amount of
23,time that processes need to wait for data to be available. The actual
23,"amount of data being read in advance is computed dynamically, based"
23,"on how much ""sequential"" the I/O seems to be. This parameter sets the"
23,maximum amount of data that the kernel reads ahead for a single file.
23,If you observe that large sequential reads from a file are not fast
23,"enough, you can try increasing this value. Increasing it too far may"
23,result in readahead thrashing where pagecache used for readahead is
23,"reclaimed before it can be used, or slowdowns because of a large"
23,amount of useless I/O. The default value is 512
23,(KB).
23,15.3.5 Transparent HugePage parameters #
23,Transparent HugePages (THP) provide a way to dynamically allocate huge
23,pages either on‑demand by the process or deferring the allocation
23,until later via the khugepaged kernel thread. This
23,method is distinct from the use of hugetlbfs to
23,manually manage their allocation and use. Workloads with contiguous memory
23,access patterns can benefit greatly from THP. A 1000-fold decrease in page
23,faults can be observed when running synthetic workloads with contiguous
23,memory access patterns.
23,There are cases when THP may be undesirable. Workloads with sparse memory
23,access patterns can perform poorly with THP due to excessive memory
23,"usage. For example, 2 MB of memory may be used at fault time instead of 4"
23,KB for each fault and ultimately lead to premature page reclaim.
23,"On releases older than SUSE Linux Enterprise 12 SP2, it was"
23,possible for an application to stall for long periods of time trying to
23,allocate a THP which frequently led to a recommendation of disabling
23,THP. Such recommendations should be re-evaluated for SUSE Linux Enterprise 12 SP3 and
23,later releases.
23,The behavior of THP may be configured via the
23,transparent_hugepage= kernel parameter or via
23,"sysfs. For example, it may be disabled by adding the kernel parameter"
23,"transparent_hugepage=never, rebuilding your grub2"
23,"configuration, and rebooting. Verify if THP is disabled with:"
23,# cat /sys/kernel/mm/transparent_hugepage/enabled
23,always madvise [never]
23,"If disabled, the value never is shown"
23,in square brackets like in the example above. A value of
23,always will always try and use THP at fault
23,time but defer to khugepaged if the allocation
23,fails. A value of madvise will only allocate THP
23,for address spaces explicitly specified by an application.
23,/sys/kernel/mm/transparent_hugepage/defrag
23,This parameter controls how much effort an application commits when
23,allocating a THP. A value of always is the default
23,for SUSE Linux Enterprise 12 SP1 and earlier releases
23,"that supported THP. If a THP is not available, the application tries to defragment memory."
23,It potentially incurs large stalls in an application if the memory is fragmented and a THP
23,is not available.
23,A value of madvise means that THP allocation
23,requests will only defragment if the application explicitly requests
23,it. This is the default for SUSE Linux Enterprise 12
23,SP2 and later
23,releases.
23,"defer is only available on SUSE Linux Enterprise 12 SP2 and later releases. If a THP is not available, the"
23,application will fall back to using small pages if a THP is not
23,available. It will wake the kswapd and
23,kcompactd kernel threads to defragment memory in
23,the background and a THP will be allocated later by
23,khugepaged.
23,The final option never will use small pages if
23,a THP is unavailable but no other action will take place.
23,15.3.6 khugepaged parameters #
23,khugepaged will be automatically started when
23,transparent_hugepage is set to
23,"always or madvise, and it will be"
23,automatically shut down if it is set to never. Normally
23,this runs at low frequency but the behavior can be tuned.
23,/sys/kernel/mm/transparent_hugepage/khugepaged/defrag
23,A value of 0 will disable khugepaged even though
23,THP may still be used at fault time. This may be important for
23,latency-sensitive applications that benefit from THP but cannot
23,tolerate a stall if khugepaged tries to update an
23,application memory usage.
23,/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan
23,This parameter controls how many pages are scanned by
23,khugepaged in a single pass. A scan identifies
23,small pages that can be reallocated as THP. Increasing this value
23,will allocate THP in the background faster at the cost of CPU
23,usage.
23,/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs
23,khugepaged sleeps for a short interval specified
23,by this parameter after each pass to limit how much CPU usage is
23,used. Reducing this value will allocate THP in the background faster
23,at the cost of CPU usage. A value of 0 will force continual scanning.
23,/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs
23,This parameter controls how long khugepaged will
23,sleep in the event it fails to allocate a THP in the background waiting
23,for kswapd and kcompactd to
23,take action.
23,The remaining parameters for khugepaged are rarely
23,useful for performance tuning but are fully documented in
23,/usr/src/linux/Documentation/vm/transhuge.txt
23,15.3.7 Further VM parameters #
23,"For the complete list of the VM tunable parameters, see"
23,/usr/src/linux/Documentation/sysctl/vm.txt
23,(available after having installed the
23,kernel-source package).
23,15.4 Monitoring VM behavior #
23,Some simple tools that can help monitor VM behavior:
23,vmstat: This tool gives a good overview of what the VM is doing. See
23,"Section 2.1.1, “vmstat” for details."
23,/proc/meminfo: This file gives a detailed
23,breakdown of where memory is being used. See
23,"Section 2.4.2, “Detailed memory usage: /proc/meminfo” for details."
23,slabtop: This tool provides detailed information
23,"about kernel slab memory usage. buffer_head, dentry, inode_cache,"
23,"ext3_inode_cache, etc. are the major caches. This command is available"
23,with the package procps.
23,/proc/vmstat: This file gives a detailed breakdown of
23,internal VM behavior. The information contained within is implementation
23,specific and may not always be available. Some information is duplicated in
23,/proc/meminfo and other information can be presented
23,"in a friendly fashion by utilities. For maximum utility, this file needs to"
23,be monitored over time to observe rates of change. The most important
23,pieces of information that are hard to derive from other sources are as
23,follows:
23,"pgscan_kswapd_*, pgsteal_kswapd_*"
23,These report respectively the number of pages scanned and reclaimed
23,by kswapd since the system started. The ratio
23,between these values can be interpreted as the reclaim efficiency
23,with a low efficiency implying that the system is struggling to
23,reclaim memory and may be thrashing. Light activity here is
23,generally not something to be concerned with.
23,"pgscan_direct_*, pgsteal_direct_*"
23,These report respectively the number of pages scanned and
23,reclaimed by an application directly. This is correlated with
23,increases in the allocstall counter. This is
23,more serious than kswapd activity as these
23,events indicate that processes are stalling. Heavy activity
23,here combined with kswapd and high rates of
23,"pgpgin, pgpout and/or high"
23,rates of pswapin or pswpout
23,are signs that a system is thrashing heavily.
23,More detailed information can be obtained using tracepoints.
23,"thp_fault_alloc, thp_fault_fallback"
23,These counters correspond to how many THPs were allocated directly
23,by an application and how many times a THP was not available and
23,small pages were used. Generally a high fallback rate is harmless
23,unless the application is very sensitive to TLB pressure.
23,"thp_collapse_alloc, thp_collapse_alloc_failed"
23,These counters correspond to how many THPs were allocated by
23,khugepaged and how many times a THP was not
23,available and small pages were used. A high fallback rate implies
23,that the system is fragmented and THPs are not being used even
23,when the memory usage by applications would allow them. It is
23,only a problem for applications that are sensitive to TLB pressure.
23,"compact_*_scanned, compact_stall, compact_fail,"
23,compact_success
23,These counters may increase when THP is enabled and the system is
23,fragmented. compact_stall is incremented when
23,an application stalls allocating THP.
23,The remaining counters
23,"account for pages scanned, the number of defragmentation events"
23,that succeeded or failed.
23,16 Tuning the network #
23,The network subsystem is complex and its tuning highly depends on
23,the system use scenario and on external factors such as software
23,"clients or hardware components (switches, routers, or gateways) in your"
23,network. The Linux kernel aims more at reliability and low latency than
23,"low overhead and high throughput. Other settings can mean less security,"
23,but better performance.
23,16.1 Configurable kernel socket buffers #
23,Networking is largely based on the TCP/IP protocol and a socket interface
23,"for communication; for more information about TCP/IP, see"
23,"Book “Administration Guide”, Chapter 19 “Basic networking”. The Linux kernel handles data it receives"
23,or sends via the socket interface in socket buffers. These kernel socket
23,buffers are tunable.
23,Important: TCP autotuning
23,Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer
23,size exists. This means that manual tuning usually will not
23,improve networking performance considerably. It is often the best not to
23,"touch the following variables, or, at least, to check the outcome of"
23,tuning efforts carefully.
23,"If you update from an older kernel, it is recommended to remove manual"
23,TCP tunings in favor of the autotuning feature.
23,The special files in the /proc file system can
23,modify the size and behavior of kernel socket buffers; for general
23,"information about the /proc file system, see"
23,"Section 2.6, “The /proc file system”. Find networking related files in:"
23,/proc/sys/net/core
23,/proc/sys/net/ipv4
23,/proc/sys/net/ipv6
23,General net variables are explained in the
23,kernel documentation
23,(linux/Documentation/sysctl/net.txt). Special
23,ipv4 variables are explained in
23,linux/Documentation/networking/ip-sysctl.txt and
23,linux/Documentation/networking/ipvs-sysctl.txt.
23,"In the /proc file system, for example, it is"
23,possible to either set the Maximum Socket Receive Buffer and Maximum
23,"Socket Send Buffer for all protocols, or both these options for the TCP"
23,protocol only (in ipv4) and thus overriding the
23,setting for all protocols (in core).
23,/proc/sys/net/ipv4/tcp_moderate_rcvbuf
23,If /proc/sys/net/ipv4/tcp_moderate_rcvbuf is set
23,"to 1, autotuning is active and buffer size is"
23,adjusted dynamically.
23,/proc/sys/net/ipv4/tcp_rmem
23,"The three values setting the minimum, initial, and maximum size of the"
23,Memory Receive Buffer per connection. They define the actual memory
23,"usage, not only TCP window size."
23,/proc/sys/net/ipv4/tcp_wmem
23,"The same as tcp_rmem, but for Memory Send Buffer"
23,per connection.
23,/proc/sys/net/core/rmem_max
23,Set to limit the maximum receive buffer size that applications can
23,request.
23,/proc/sys/net/core/wmem_max
23,Set to limit the maximum send buffer size that applications can
23,request.
23,Via /proc it is possible to disable TCP features
23,that you do not need (all TCP features are switched on by default). For
23,"example, check the following files:"
23,/proc/sys/net/ipv4/tcp_timestamps
23,TCP time stamps are defined in RFC1323.
23,/proc/sys/net/ipv4/tcp_window_scaling
23,TCP window scaling is also defined in RFC1323.
23,/proc/sys/net/ipv4/tcp_sack
23,Select acknowledgments (SACKS).
23,Use sysctl to read or write variables of the
23,/proc file system. sysctl is
23,preferable to cat (for reading) and
23,"echo (for writing), because it also reads settings"
23,"from /etc/sysctl.conf and, thus, those settings"
23,survive reboots reliably. With sysctl you can read all
23,variables and their values easily; as root use the following
23,command to list TCP related settings:
23,> sudo sysctl -a | grep tcpNote: Side effects of tuning network variables
23,Tuning network variables can affect other system resources such as CPU
23,or memory use.
23,16.2 Detecting network bottlenecks and analyzing network traffic #
23,"Before starting with network tuning, it is important to isolate network"
23,bottlenecks and network traffic patterns. There are some tools that can
23,help you with detecting those bottlenecks.
23,The following tools can help analyzing your network traffic:
23,"netstat, tcpdump, and"
23,wireshark. Wireshark is a network traffic analyzer.
23,16.3 Netfilter #
23,The Linux firewall and masquerading features are provided by the
23,Netfilter kernel modules. This is a highly configurable rule based
23,"framework. If a rule matches a packet, Netfilter accepts or denies it or"
23,takes special action (“target”) as defined by rules such as
23,address translation.
23,There are quite a lot of properties Netfilter can take into account.
23,"Thus, the more rules are defined, the longer packet processing may last."
23,"Also advanced connection tracking could be rather expensive and, thus,"
23,slowing down overall networking.
23,"When the kernel queue becomes full, all new packets are dropped, causing"
23,existing connections to fail. The 'fail-open' feature allows a user to
23,temporarily disable the packet inspection and maintain the connectivity
23,"under heavy network traffic. For reference, see https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/."
23,"For more information, see the home page of the Netfilter and iptables"
23,"project, http://www.netfilter.org"
23,16.4 Improving the network performance with receive packet steering (RPS) #
23,Modern network interface devices can move so many packets that the host
23,can become the limiting factor for achieving maximum performance.
23,"To keep up, the system must be able to distribute the work across"
23,multiple CPU cores.
23,Some modern network interfaces can help distribute the work to multiple
23,CPU cores through the implementation of multiple transmission and
23,"multiple receive queues in hardware. However, others are only equipped"
23,with a single queue and the driver must deal with all incoming packets in
23,"a single, serialized stream. To work around this issue, the operating"
23,"system must ""parallelize"" the stream to distribute the work across"
23,multiple CPUs. On SUSE Linux Enterprise Server this is done via Receive Packet
23,Steering (RPS). RPS can also be used in virtual environments.
23,RPS creates a unique hash for each data stream using IP addresses and
23,port numbers. The use of this hash ensures that packets for the same data
23,"stream are sent to the same CPU, which helps to increase performance."
23,RPS is configured per network device receive queue and interface. The
23,configuration file names match the following scheme:
23,/sys/class/net/<device>/queues/<rx-queue>/rps_cpus
23,<device> stands for the network
23,"device, such as eth0, eth1."
23,"<rx-queue> stands for the receive queue,"
23,"such as rx-0, rx-1."
23,"If the network interface hardware only supports a single receive queue,"
23,only rx-0 will exist. If it supports multiple receive
23,"queues, there will be an rx-N directory for"
23,each receive queue.
23,These configuration files contain a comma-delimited list of CPU bitmaps.
23,"By default, all bits are set to 0. With this setting"
23,RPS is disabled and therefore the CPU that handles the interrupt will
23,also process the packet queue.
23,To enable RPS and enable specific CPUs to process packets for the receive
23,"queue of the interface, set the value of their positions in the bitmap to"
23,"1. For example, to enable CPUs 0-3 to process packets"
23,"for the first receive queue for eth0, set the bit positions"
23,0-3 to 1 in binary: 00001111. This representation then
23,needs to be converted to hex—which results in F in
23,this case. Set this hex value with the following command:
23,"> sudo echo ""f"" > /sys/class/net/eth0/queues/rx-0/rps_cpus"
23,If you wanted to enable CPUs 8-15:
23,1111 1111 0000 0000 (binary)
23,0 (decimal)
23,0 (hex)
23,The command to set the hex value of ff00 would be:
23,"> sudo echo ""ff00"" > /sys/class/net/eth0/queues/rx-0/rps_cpus"
23,"On NUMA machines, best performance can be achieved by configuring RPS to"
23,use the CPUs on the same NUMA node as the interrupt for the interface's
23,receive queue.
23,"On non-NUMA machines, all CPUs can be used. If the interrupt rate is very"
23,"high, excluding the CPU handling the network interface can boost"
23,performance. The CPU being used for the network interface can be
23,determined from /proc/interrupts. For example:
23,> sudo cat /proc/interrupts
23,CPU0
23,CPU1
23,CPU2
23,CPU3
23,...
23,51:
23,113915241
23,Phys-fasteoi
23,eth0
23,...
23,"In this case, CPU 0 is the only CPU processing"
23,"interrupts for eth0, since only"
23,CPU0 contains a non-zero value.
23,"On x86 and AMD64/Intel 64 platforms, irqbalance can be used"
23,to distribute hardware interrupts across CPUs. See man 1
23,irqbalance for more details.
23,17 Tuning SUSE Linux Enterprise for SAP #
23,This chapter presents information about preparing and tuning SUSE Linux Enterprise Server
23,to work optimally with SAP applications with sapconf. sapconf is for
23,SUSE Linux Enterprise systems that install SAP applications. Customers who have
23,SUSE Linux Enterprise Server for SAP Applications should use saptune.
23,Note: The sapconf command has been removed
23,"In SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 11 and 12, the sapconf command"
23,was included in the package with the same name.
23,"For SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15, this has been changed:"
23,The command sapconf has been removed from the
23,sapconf package. The package contains a systemd service
23,"only. There is no sapconf command line tool anymore, no sapconf/tuned profiles,"
23,and no tuned.
23,17.1 Tuning SLE Systems with sapconf 5 #
23,The package sapconf is available in SUSE Linux Enterprise Server and SUSE Linux Enterprise Server for SAP Applications.
23,It sets recommended parameters for the following types of SAP applications:
23,"SAP NetWeaver, SAP HANA, and SAP HANA-based applications."
23,Overview of sapconf5 in SUSE® Linux Enterprise Server 12 #  sapconf5 (without tuned)sapconf-netweaver (sapconf profile as a replacement for tuned profile)sapconf-hana (sapconf profile as a replacement for tuned profile)sapconf-bobj (sapconf profile as a replacement for tuned profile)sapconf-ase (sapconf profile as a replacement for tuned profile)Overview of sapconf5 in SUSE® Linux Enterprise Server 15 #  sapconf5 (without tuned)no profiles anymore
23,"Note that if you previously made changes to the system tuning, those"
23,changes may be overwritten by sapconf.
23,sapconf 5 ships a systemd service which applies the tuning and ensures that
23,related services are running.
23,"To use sapconf, make sure"
23,that the package sapconf is installed on your system.
23,Note: No profiles in SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15 SP3
23,"In SUSE Linux Enterprise Server and SUSE Linux Enterprise Server 15, sapconf no longer supports profiles."
23,17.1.1 Verifying sapconf setup #
23,"With sapconf 5.0.2 and up, the check tool sapconf_check is available,"
23,which verifies the correct setup of sapconf. For example:
23,# sapconf_check
23,This is sapconf_check v1.0.
23,It verifies if sapconf is set up correctly and will give advice to do so.
23,Please keep in mind:
23,"{{ - This tool does not check, if the tuning itself works correctly.}}"
23,{{ - Follow the hints from top to down to minimize side effects.}}
23,Checking sapconf
23,================
23,[ OK ] sapconf package has version 5.0.2
23,[ OK ] saptune.service is inactive
23,[ OK ] saptune.service is disabled
23,"[WARN] tuned.service is enabled/active with profile 'virtual-guest -> Sapconf does not require tuned! Run 'systemctl stop tuned.service', if not needed otherwise."
23,[FAIL] sapconf.service is inactive -> Run 'systemctl start sapconf.service' to activate the tuning now.
23,[FAIL] sapconf.service is disabled -> Run 'systemctl enable sapconf.service' to activate sapconf at boot.1 warning(s) have been found.
23,2 error(s) have been found.
23,Sapconf will not work properly!
23,"If sapconf_check finds problems, it will give hints"
23,on how to resolve the issue.
23,The tool will not verify whether the system has been tuned correctly. It
23,only checks that sapconf
23,is set up correctly and has been started.
23,17.1.2 Enabling and disabling sapconf and viewing its status #
23,"After the installation of sapconf, the sapconf service is enabled."
23,You can inspect or change the status of sapconf as described in the
23,following:
23,To see the status of the service
23,sapconf:
23,# systemctl status sapconf
23,The service should be displayed as active (exited).
23,To start the service
23,sapconf:
23,# systemctl start sapconf
23,"Should sapconf be disabled,"
23,enable and start it with:
23,# systemctl enable --now sapconf
23,To stop the service
23,sapconf:
23,# systemctl stop sapconf
23,This command will disable the vast majority of optimizations immediately. The only
23,exceptions from this rule are options that require a system reboot to enable/disable.
23,"To disable sapconf, use:"
23,# systemctl disable sapconf
23,If you have not specifically enabled any of the services that sapconf
23,"depends on yourself, this will also disable most tuning parameters and"
23,all services used by sapconf.
23,Tip: Additional services that sapconf relies on
23,In addition to the sapconf service it also relies on the following two services:
23,sysstat which collects data on
23,system activity.
23,uuidd which generates time-based
23,UUIDs that are guaranteed to be unique even in settings where many
23,processor cores are involved. This is necessary for SAP applications.
23,17.1.3 Configuring sapconf5 #
23,"In general, the default configuration of sapconf already uses the"
23,"parameter values recommended by SAP. However, if you have special"
23,"needs, you can configure the tool to better suit those."
23,All parameters of sapconf can be found in the file
23,/etc/sysconfig/sapconf.
23,The file can be edited directly. All parameters in this file are
23,"explained by means of comments and references to SAP Notes, which can"
23,be viewed at https://launchpad.support.sap.com/.
23,"When sapconf is updated, all customized parameters from this file will"
23,"be preserved as much as possible. However, sometimes parameters cannot"
23,"be transferred cleanly to the new configuration file. Therefore, after"
23,updating it is advisable to check the difference between the previous
23,"custom configuration, which during the update is moved to"
23,"/etc/sysconfig/sapconf.rpmsave,"
23,and the new version at /etc/sysconfig/sapconf.
23,Log messages related to this file are written to
23,/var/log/sapconf.log.
23,"When editing either of these files, you will find that some values are"
23,commented by means of a # character at the beginning of
23,"the line. This means that while the parameter is relevant for tuning, there"
23,is no suitable default for it.
23,"Conversely, you can add # characters to the beginning of"
23,"the line to comment specific parameters. However, you should avoid this"
23,"practice, as it can lead to sapconf not properly applying the profile."
23,"To apply edited configuration, restart sapconf:"
23,# systemctl restart sapconf
23,Confirming that a certain parameter value was applied correctly works
23,"differently for different parameters. Hence, the following serves as an"
23,example only:
23,Example 17.1: Checking Parameters #
23,To confirm that the setting for TCP_SLOW_START was
23,"applied, do the following:"
23,View the log file of sapconf to see whether it applied the value.
23,"Within /var/log/sapconf.log, check for a line"
23,containing this text:
23,Change net.ipv4.tcp_slow_start_after_idle from 1 to 0
23,"Alternatively, the parameter may have already been set correctly"
23,"before sapconf was started. In this case, sapconf will not change"
23,its value:
23,Leaving net.ipv4.tcp_slow_start_after_idle unchanged at 1
23,The underlying option behind TCP_SLOW_START can be
23,manually configured at
23,/proc/sys/net.ipv4.tcp_slow_start_after_idle.
23,"To check its actual current value, use:"
23,# sysctl net.ipv4.tcp_slow_start_after_idle17.1.4 Removing sapconf #
23,"To remove sapconf from a system, uninstall its package with:"
23,# zypper rm sapconf
23,"Note that when doing this, dependencies of sapconf will remain installed."
23,"However, the service sysstat will"
23,"go into a disabled state. If it is still relevant to you, make sure to"
23,enable it again.
23,17.1.5 For more information #
23,The following man pages provide additional information about sapconf:
23,Detailed description of all tuning parameters set by sapconf:
23,man 5 sapconf
23,Information about configuring and customizing the sapconf profile:
23,man 7 sapconf
23,Also see the blog series detailing the updated version of sapconf at
23,https://www.suse.com/c/a-new-sapconf-is-available/.
23,17.1.6 Using tuned together with
23,sapconf #
23,"With version 5, sapconf does not rely on tuned anymore. This means both tools"
23,can be used independently.
23,sapconf will print a warning in its log if the tuned service
23,is started.
23,Important: Using tuned and sapconf together
23,"If you are going to use tuned and sapconf simultaneously,"
23,"be very careful, that both tools do not configure the same system parameters."
23,"Part VI Handling system dumps #  18 Tracing toolsSUSE Linux Enterprise Server comes with several tools that help you obtain useful information about your system. You can use the information for various purposes, for example, to debug and find problems in your program, to discover places causing performance drops, or to trace a running process to f…19 Kexec and Kdump"
23,Kexec is a tool to boot to another kernel from the currently running one.
23,You can perform faster system reboots without any hardware initialization.
23,You can also prepare the system to boot to another kernel if the system
23,crashes.
23,"20 Using systemd-coredump to debug application crashessystemd-coredump collects and displays core dumps, for analyzing application crashes. The core dump contains an image of the process's memory at the time of termination. When a process crashes (or all processes belonging to an application), its default is to log the core dump to the systemd journal,…18 Tracing tools #"
23,SUSE Linux Enterprise Server comes with several tools that help you obtain useful
23,information about your system. You can use the information for various
23,"purposes, for example, to debug and find problems in your program, to"
23,"discover places causing performance drops, or to trace a running process to"
23,find out what system resources it uses. Most of the
23,"tools are part of the installation media. In some cases, they need to be"
23,"installed from the SUSE Software Development Kit, which is a separate download."
23,Note: Tracing and impact on performance
23,"While a running process is being monitored for system or library calls,"
23,the performance of the process is heavily reduced. You are advised to use
23,tracing tools only for the time you need to collect the data.
23,18.1 Tracing system calls with strace #
23,The strace command traces system calls of a process
23,and signals received by the process. strace can either
23,"run a new command and trace its system calls, or you can attach"
23,strace to an already running command. Each line of the
23,"command's output contains the system call name, followed by its arguments"
23,in parentheses and its return value.
23,"To run a new command and start tracing its system calls, enter the"
23,"command to be monitored as you normally do, and add"
23,strace at the beginning of the command line:
23,> strace ls
23,"execve(""/bin/ls"", [""ls""], [/* 52 vars */]) = 0"
23,brk(0)
23,= 0x618000
23,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
23,= 0x7f9848667000
23,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
23,= 0x7f9848666000
23,"access(""/etc/ld.so.preload"", R_OK)"
23,= -1 ENOENT \
23,(No such file or directory)
23,"open(""/etc/ld.so.cache"", O_RDONLY)"
23,= 3
23,"fstat(3, {st_mode=S_IFREG|0644, st_size=200411, ...}) = 0"
23,"mmap(NULL, 200411, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f9848635000"
23,close(3)
23,= 0
23,"open(""/lib64/librt.so.1"", O_RDONLY)"
23,= 3
23,[...]
23,"mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) \"
23,= 0x7fd780f79000
23,"write(1, ""Desktop\nDocuments\nbin\ninst-sys\n"", 31Desktop"
23,Documents
23,bin
23,inst-sys
23,) = 31
23,close(1)
23,= 0
23,"munmap(0x7fd780f79000, 4096)"
23,= 0
23,close(2)
23,= 0
23,exit_group(0)
23,= ?
23,"To attach strace to an already running process, you"
23,need to specify the -p with the process ID
23,(PID) of the process that you want to monitor:
23,> strace -p `pidof cron`
23,Process 1261 attached
23,restart_syscall(<... resuming interrupted call ...>) = 0
23,"stat(""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=2309, ...}) = 0"
23,"select(5, [4], NULL, NULL, {0, 0})"
23,= 0 (Timeout)
23,"socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 5"
23,"connect(5, {sa_family=AF_LOCAL, sun_path=""/var/run/nscd/socket""}, 110) = 0"
23,"sendto(5, ""\2\0\0\0\0\0\0\0\5\0\0\0root\0"", 17, MSG_NOSIGNAL, NULL, 0) = 17"
23,"poll([{fd=5, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 1 ([{fd=5, revents=POLLIN|POLLHUP}])"
23,"read(5, ""\2\0\0\0\1\0\0\0\5\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\6\0\0\0""..., 36) = 36"
23,"read(5, ""root\0x\0root\0/root\0/bin/bash\0"", 28) = 28"
23,close(5)
23,= 0
23,"rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0"
23,"rt_sigaction(SIGCHLD, NULL, {0x7f772b9ea890, [], SA_RESTORER|SA_RESTART, 0x7f772adf7880}, 8) = 0"
23,"rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0"
23,"nanosleep({60, 0}, 0x7fff87d8c580)"
23,= 0
23,"stat(""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=2309, ...}) = 0"
23,"select(5, [4], NULL, NULL, {0, 0})"
23,= 0 (Timeout)
23,"socket(PF_LOCAL, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 5"
23,"connect(5, {sa_family=AF_LOCAL, sun_path=""/var/run/nscd/socket""}, 110) = 0"
23,"sendto(5, ""\2\0\0\0\0\0\0\0\5\0\0\0root\0"", 17, MSG_NOSIGNAL, NULL, 0) = 17"
23,"poll([{fd=5, events=POLLIN|POLLERR|POLLHUP}], 1, 5000) = 1 ([{fd=5, revents=POLLIN|POLLHUP}])"
23,"read(5, ""\2\0\0\0\1\0\0\0\5\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\6\0\0\0""..., 36) = 36"
23,"read(5, ""root\0x\0root\0/root\0/bin/bash\0"", 28) = 28"
23,close(5)
23,[...]
23,The -e option understands several sub-options and
23,"arguments. For example, to trace all attempts to open or write to a"
23,"particular file, use the following:"
23,"> strace -e trace=open,write ls ~"
23,"open(""/etc/ld.so.cache"", O_RDONLY)"
23,= 3
23,"open(""/lib64/librt.so.1"", O_RDONLY)"
23,= 3
23,"open(""/lib64/libselinux.so.1"", O_RDONLY) = 3"
23,"open(""/lib64/libacl.so.1"", O_RDONLY)"
23,= 3
23,"open(""/lib64/libc.so.6"", O_RDONLY)"
23,= 3
23,"open(""/lib64/libpthread.so.0"", O_RDONLY) = 3"
23,[...]
23,"open(""/usr/lib/locale/cs_CZ.utf8/LC_CTYPE"", O_RDONLY) = 3"
23,"open(""."", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3"
23,"write(1, ""addressbook.db.bak\nbin\ncxoffice\n""..., 311) = 311"
23,"To trace only network related system calls, use -e"
23,trace=network:
23,> strace -e trace=network -p 26520
23,Process 26520 attached - interrupt to quit
23,"socket(PF_NETLINK, SOCK_RAW, 0)"
23,= 50
23,"bind(50, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0"
23,"getsockname(50, {sa_family=AF_NETLINK, pid=26520, groups=00000000}, \"
23,[12]) = 0
23,"sendto(50, ""\24\0\0\0\26\0\1\3~p\315K\0\0\0\0\0\0\0\0"", 20, 0,"
23,"{sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 20"
23,[...]
23,The -c calculates the time the kernel spent on each
23,system call:
23,> strace -c find /etc -name xorg.conf
23,/etc/X11/xorg.conf
23,% time
23,seconds
23,usecs/call
23,calls
23,errors syscall
23,------ ----------- ----------- --------- --------- ----------------
23,32.38
23,0.000181
23,181
23,execve
23,22.00
23,0.000123
23,576
23,getdents64
23,19.50
23,0.000109
23,917
23,31 open
23,19.14
23,0.000107
23,888
23,close
23,4.11
23,0.000023
23,mprotect
23,0.00
23,0.000000
23,write
23,[...]
23,0.00
23,0.000000
23,getrlimit
23,0.00
23,0.000000
23,arch_prctl
23,0.00
23,0.000000
23,1 futex
23,0.00
23,0.000000
23,set_tid_address
23,0.00
23,0.000000
23,fadvise64
23,0.00
23,0.000000
23,set_robust_list
23,------ ----------- ----------- --------- --------- ----------------
23,100.00
23,0.000559
23,3633
23,33 total
23,"To trace all child processes of a process, use -f:"
23,> strace -f systemctl status apache2.service
23,"execve(""/usr/bin/systemctl"", [""systemctl"", ""status"", ""apache2.service""], \"
23,0x7ffea44a3318 /* 56 vars */) = 0
23,brk(NULL)
23,= 0x5560f664a000
23,[...]
23,"mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f98c58a5000"
23,"mmap(NULL, 4420544, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f98c524a000"
23,"mprotect(0x7f98c53f4000, 2097152, PROT_NONE) = 0"
23,[...]
23,[pid
23,"9130] read(0, ""\342\227\217 apache2.service - The Apache""..., 8192) = 165"
23,[pid
23,"9130] read(0, """", 8027)"
23,= 0
23,"● apache2.service - The Apache Webserver227\217 apache2.service - Th""..., 193"
23,Loaded: loaded (/usr/lib/systemd/system/apache2.service; disabled; vendor preset: disabled)
23,Active: inactive (dead)
23,) = 193
23,[pid
23,"9130] ioctl(3, SNDCTL_TMR_STOP or TCSETSW, {B38400 opost isig icanon echo ...}) = 0"
23,[pid
23,9130] exit_group(0)
23,= ?
23,[pid
23,9130] +++ exited with 0 +++
23,"<... waitid resumed>{si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=9130, \"
23,"si_uid=0, si_status=0, si_utime=0, si_stime=0}, WEXITED, NULL) = 0"
23,"--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=9130, si_uid=0, \"
23,"si_status=0, si_utime=0, si_stime=0} ---"
23,exit_group(3)
23,= ?
23,+++ exited with 3 +++
23,If you need to analyze the output of strace and the
23,output messages are too long to be inspected directly in the console
23,"window, use -o. In that case, unnecessary messages, such"
23,"as information about attaching and detaching processes, are suppressed."
23,You can also suppress these messages (normally printed on the standard
23,output) with -q. To add time stamps at the beginning of each line
23,"with a system call, use -t:"
23,> strace -t -o strace_sleep.txt sleep 1; more strace_sleep.txt
23,"08:44:06 execve(""/bin/sleep"", [""sleep"", ""1""], [/* 81 vars */]) = 0"
23,08:44:06 brk(0)
23,= 0x606000
23,"08:44:06 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, \"
23,"-1, 0) = 0x7f8e78cc5000"
23,[...]
23,08:44:06 close(3)
23,= 0
23,"08:44:06 nanosleep({1, 0}, NULL)"
23,= 0
23,08:44:07 close(1)
23,= 0
23,08:44:07 close(2)
23,= 0
23,08:44:07 exit_group(0)
23,= ?
23,The behavior and output format of strace can be largely controlled. For
23,"more information, see the relevant manual page (man 1 strace)."
23,18.2 Tracing library calls with ltrace #
23,ltrace traces dynamic library calls of a process. It
23,"is used in a similar way to strace, and most of their"
23,"parameters have a very similar or identical meaning. By default,"
23,ltrace uses /etc/ltrace.conf or
23,"~/.ltrace.conf configuration files. You can,"
23,"however, specify an alternative one with the -F"
23,CONFIG_FILE option.
23,"In addition to library calls, ltrace with the"
23,-S option can trace system calls as well:
23,> ltrace -S -o ltrace_find.txt find /etc -name \
23,xorg.conf; more ltrace_find.txt
23,SYS_brk(NULL)
23,= 0x00628000
23,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
23,= 0x7f1327ea1000
23,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
23,= 0x7f1327ea0000
23,[...]
23,"fnmatch(""xorg.conf"", ""xorg.conf"", 0)"
23,= 0
23,free(0x0062db80)
23,= <void>
23,__errno_location()
23,= 0x7f1327e5d698
23,"__ctype_get_mb_cur_max(0x7fff25227af0, 8192, 0x62e020, -1, 0) = 6"
23,"__ctype_get_mb_cur_max(0x7fff25227af0, 18, 0x7f1327e5d6f0, 0x7fff25227af0,"
23,0x62e031) = 6
23,"__fprintf_chk(0x7f1327821780, 1, 0x420cf7, 0x7fff25227af0, 0x62e031"
23,<unfinished ...>
23,"SYS_fstat(1, 0x7fff25227230)"
23,= 0
23,"SYS_mmap(0, 4096, 3, 34, 0xffffffff)"
23,= 0x7f1327e72000
23,"SYS_write(1, ""/etc/X11/xorg.conf\n"", 19)"
23,= 19
23,[...]
23,You can change the type of traced events with the -e
23,option. The following example prints library calls related to
23,fnmatch and strlen
23,functions:
23,"> ltrace -e fnmatch,strlen find /etc -name xorg.conf"
23,[...]
23,"fnmatch(""xorg.conf"", ""xorg.conf"", 0)"
23,= 0
23,"strlen(""Xresources"")"
23,= 10
23,"strlen(""Xresources"")"
23,= 10
23,"strlen(""Xresources"")"
23,= 10
23,"fnmatch(""xorg.conf"", ""Xresources"", 0)"
23,= 1
23,"strlen(""xorg.conf.install"")"
23,= 17
23,[...]
23,"To display only the symbols included in a specific library, use"
23,-l /path/to/library:
23,> ltrace -l /lib64/librt.so.1 sleep 1
23,"clock_gettime(1, 0x7fff4b5c34d0, 0, 0, 0)"
23,= 0
23,"clock_gettime(1, 0x7fff4b5c34c0, 0xffffffffff600180, -1, 0) = 0"
23,+++ exited (status 0) +++
23,You can make the output more readable by indenting each nested call by
23,the specified number of space with the -n
23,NUM_OF_SPACES.
23,18.3 Debugging and profiling with Valgrind #
23,Valgrind is a set of tools to debug and profile your programs so that
23,they can run both faster and with fewer errors. Valgrind can detect problems
23,"related to memory management and threading, or can also serve as a"
23,framework for building new debugging tools. It is well known that this
23,"tool can incur high overhead, causing, for example, higher runtimes or"
23,changing the normal program behavior under concurrent workloads based on timing.
23,18.3.1 Installation #
23,Valgrind is not shipped with standard SUSE Linux Enterprise Server distribution. To
23,"install it on your system, you need to obtain SUSE Software Development Kit, and either"
23,install it and run
23,zypper install
23,VALGRIND
23,"or browse through the SUSE Software Development Kit directory tree, locate the Valgrind"
23,package and install it with
23,rpm -i
23,valgrind-VERSION_ARCHITECTURE.rpm
23,The SDK is a module for SUSE Linux Enterprise and is available via an online channel from
23,the SUSE Customer Center.
23,Alternatively download it from http://download.suse.com/. (Search for SUSE Linux Enterprise
23,"Software Development Kit). Refer to Book “Deployment Guide”, Chapter 22 “Installing modules, extensions, and third party add-on products” for details."
23,18.3.2 Supported architectures #
23,SUSE Linux Enterprise Server supports Valgrind on the following architectures:
23,AMD64/Intel 64
23,POWER
23,IBM Z
23,18.3.3 General information #
23,The main advantage of Valgrind is that it works with existing compiled
23,executables. You do not need to recompile or modify your programs to
23,use it. Run Valgrind like this:
23,valgrind VALGRIND_OPTIONS
23,your-prog YOUR-PROGRAM-OPTIONS
23,"Valgrind consists of several tools, and each provides specific"
23,functionality. Information in this section is general and valid
23,regardless of the used tool. The most important configuration option is
23,--tool. This option tells Valgrind which tool to run.
23,"If you omit this option, memcheck is selected"
23,"by default. For example, to run find ~"
23,-name .bashrc with Valgrind's
23,"memcheck tools, enter the following in the"
23,command line:
23,valgrind --tool=memcheck find ~ -name
23,.bashrc
23,A list of standard Valgrind tools with a brief description follows:
23,memcheck
23,Detects memory errors. It helps you tune your programs to behave
23,correctly.
23,cachegrind
23,Profiles cache prediction. It helps you tune your programs to run
23,faster.
23,callgrind
23,Works in a similar way to cachegrind but
23,also gathers additional cache-profiling information.
23,exp-drd
23,Detects thread errors. It helps you tune your multi-threaded programs
23,to behave correctly.
23,helgrind
23,Another thread error detector. Similar to
23,exp-drd but uses different techniques for
23,problem analysis.
23,massif
23,A heap profiler. Heap is an area of memory used for dynamic memory
23,allocation. This tool helps you tune your program to use less memory.
23,lackey
23,An example tool showing instrumentation basics.
23,18.3.4 Default options #
23,Valgrind can read options at start-up. There are three places which
23,Valgrind checks:
23,The file .valgrindrc in the home directory of the
23,user who runs Valgrind.
23,The environment variable $VALGRIND_OPTS
23,The file .valgrindrc in the current directory
23,where Valgrind is run from.
23,"These resources are parsed exactly in this order, while later given"
23,options take precedence over earlier processed options. Options specific
23,to a particular Valgrind tool must be prefixed with the tool name and a
23,"colon. For example, if you want cachegrind to"
23,always write profile data to the
23,"/tmp/cachegrind_PID.log,"
23,add the following line to the .valgrindrc file in
23,your home directory:
23,--cachegrind:cachegrind-out-file=/tmp/cachegrind_%p.log
23,18.3.5 How Valgrind works #
23,Valgrind takes control of your executable before it starts. It reads
23,debugging information from the executable and related shared libraries.
23,"The executable's code is redirected to the selected Valgrind tool, and"
23,the tool adds its own code to handle its debugging. Then the code is
23,handed back to the Valgrind core and the execution continues.
23,"For example, memcheck adds its code, which"
23,"checks every memory access. As a consequence, the program runs much"
23,slower than in the native execution environment.
23,"Valgrind simulates every instruction of your program. Therefore, it not"
23,"only checks the code of your program, but also all related libraries"
23,"(including the C library), libraries used for graphical environment, and"
23,"so on. If you try to detect errors with Valgrind, it also detects errors"
23,"in associated libraries (like C, X11, or Gtk libraries). Because you"
23,"probably do not need these errors, Valgrind can selectively, suppress"
23,these error messages to suppression files. The
23,--gen-suppressions=yes tells Valgrind to report these
23,suppressions which you can copy to a file.
23,You should supply a real executable (machine code) as a Valgrind
23,"argument. If your application is run, for example, from a shell or Perl"
23,"script, you will by mistake get error reports related to"
23,/bin/sh (or /usr/bin/perl). In
23,"such cases, you can use"
23,--trace-children=yes to work
23,"around this issue. However, using the executable itself will avoid any"
23,confusion over this issue.
23,18.3.6 Messages #
23,"During its runtime, Valgrind reports messages with detailed errors and"
23,important events. The following example explains the messages:
23,> valgrind --tool=memcheck find ~ -name .bashrc
23,[...]
23,==6558== Conditional jump or move depends on uninitialised value(s)
23,==6558==
23,at 0x400AE79: _dl_relocate_object (in /lib64/ld-2.11.1.so)
23,==6558==
23,by 0x4003868: dl_main (in /lib64/ld-2.11.1.so)
23,[...]
23,==6558== Conditional jump or move depends on uninitialised value(s)
23,==6558==
23,at 0x400AE82: _dl_relocate_object (in /lib64/ld-2.11.1.so)
23,==6558==
23,by 0x4003868: dl_main (in /lib64/ld-2.11.1.so)
23,[...]
23,==6558== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0)
23,"==6558== malloc/free: in use at exit: 2,228 bytes in 8 blocks."
23,"==6558== malloc/free: 235 allocs, 227 frees, 489,675 bytes allocated."
23,"==6558== For counts of detected errors, rerun with: -v"
23,==6558== searching for pointers to 8 not-freed blocks.
23,"==6558== checked 122,584 bytes."
23,==6558==
23,==6558== LEAK SUMMARY:
23,==6558==
23,definitely lost: 0 bytes in 0 blocks.
23,==6558==
23,possibly lost: 0 bytes in 0 blocks.
23,==6558==
23,"still reachable: 2,228 bytes in 8 blocks."
23,==6558==
23,suppressed: 0 bytes in 0 blocks.
23,==6558== Rerun with --leak-check=full to see details of leaked memory.
23,The ==6558== introduces Valgrind's messages and
23,contains the process ID number (PID). You can easily distinguish
23,"Valgrind's messages from the output of the program itself, and decide"
23,which messages belong to a particular process.
23,"To make Valgrind's messages more detailed, use -v or"
23,even -v -v.
23,You can make Valgrind send its messages to three different places:
23,"By default, Valgrind sends its messages to the file descriptor 2,"
23,which is the standard error output. You can tell Valgrind to send its
23,messages to any other file descriptor with the
23,--log-fd=FILE_DESCRIPTOR_NUMBER
23,option.
23,The second and probably more useful way is to send Valgrind's messages
23,to a file with
23,--log-file=FILENAME. This
23,"option accepts several variables, for example, %p"
23,gets replaced with the PID of the currently profiled process. This way
23,you can send messages to different files based on their PID.
23,%q{env_var} is replaced with the value of the
23,related env_var environment variable.
23,The following example checks for possible memory errors during the
23,"Apache Web server restart, while following children processes and"
23,writing detailed Valgrind's messages to separate files distinguished
23,by the current process PID:
23,> valgrind -v --tool=memcheck --trace-children=yes \
23,--log-file=valgrind_pid_%p.log systemctl restart apache2.service
23,"This process created 52 log files in the testing system, and took 75"
23,seconds instead of the usual 7 seconds needed to run sudo
23,"systemctl restart apache2.service without Valgrind, which is"
23,approximately 10 times more.
23,> ls -1 valgrind_pid_*log
23,valgrind_pid_11780.log
23,valgrind_pid_11782.log
23,valgrind_pid_11783.log
23,[...]
23,valgrind_pid_11860.log
23,valgrind_pid_11862.log
23,valgrind_pid_11863.log
23,You may also prefer to send the Valgrind's messages over the network.
23,You need to specify the aa.bb.cc.dd IP address and
23,port_num port number of the network socket with the
23,--log-socket=AA.BB.CC.DD:PORT_NUM
23,"option. If you omit the port number, 1500 will be used."
23,It is useless to send Valgrind's messages to a network socket if no
23,application is capable of receiving them on the remote machine. That
23,"is why valgrind-listener, a simple listener, is"
23,shipped together with Valgrind. It accepts connections on the
23,specified port and copies everything it receives to the standard
23,output.
23,18.3.7 Error messages #
23,"Valgrind remembers all error messages, and if it detects a new error,"
23,the error is compared against old error messages. This way Valgrind
23,"checks for duplicate error messages. In case of a duplicate error, it is"
23,recorded but no message is shown. This mechanism prevents you from being
23,overwhelmed by millions of duplicate errors.
23,The -v option will add a summary of all reports (sorted
23,by their total count) to the end of the Valgrind's execution output.
23,"Moreover, Valgrind stops collecting errors if it detects either 1000"
23,"different errors, or 10 000 000 errors in total. If you want to suppress"
23,"this limit and wish to see all error messages, use"
23,--error-limit=no.
23,"Some errors usually cause other ones. Therefore, fix errors in the same"
23,order as they appear and re-check the program continuously.
23,18.4 More information #
23,"For a complete list of options related to the described tracing tools,"
23,"see the corresponding man page (man 1 strace,"
23,"man 1 ltrace, and man 1"
23,valgrind).
23,To describe advanced usage of Valgrind is beyond the scope of this
23,"document. It is very well documented, see"
23,Valgrind
23,User Manual. These pages are indispensable if you need more
23,advanced information on Valgrind or the usage and purpose of its
23,standard tools.
23,19 Kexec and Kdump #
23,Kexec is a tool to boot to another kernel from the currently running one.
23,You can perform faster system reboots without any hardware initialization.
23,You can also prepare the system to boot to another kernel if the system
23,crashes.
23,19.1 Introduction #
23,"With Kexec, you can replace the running kernel with another one without a"
23,hard reboot. The tool is useful for several reasons:
23,Faster system rebooting
23,"If you need to reboot the system frequently, Kexec can save you"
23,significant time.
23,Avoiding unreliable firmware and hardware
23,Computer hardware is complex and serious problems may occur during the
23,system start-up. You cannot always replace unreliable hardware
23,immediately. Kexec boots the kernel to a controlled environment with the
23,hardware already initialized. The risk of unsuccessful system start is
23,then minimized.
23,Saving the dump of a crashed kernel
23,Kexec preserves the contents of the physical memory. After the
23,"production kernel fails, the"
23,capture kernel (an additional kernel running in a
23,reserved memory range) saves the state of the failed kernel. The saved
23,image can help you with the subsequent analysis.
23,Booting without GRUB 2 configuration
23,"When the system boots a kernel with Kexec, it skips the boot loader"
23,stage. The normal booting procedure can fail because of an error in the
23,"boot loader configuration. With Kexec, you do not depend on a working"
23,boot loader configuration.
23,19.2 Required packages #
23,To use Kexec on SUSE® Linux Enterprise Server to speed up reboots or avoid potential
23,"hardware problems, make sure that the package"
23,kexec-tools is installed. It contains a script
23,"called kexec-bootloader, which reads the boot loader"
23,configuration and runs Kexec using the same kernel options as the normal
23,boot loader.
23,To set up an environment that helps you obtain debug information in case of
23,"a kernel crash, make sure that the package"
23,makedumpfile is installed.
23,The preferred method of using Kdump in SUSE Linux Enterprise Server is through the YaST
23,"Kdump module. To use the YaST module, make sure that the package"
23,yast2-kdump is installed.
23,19.3 Kexec internals #
23,The most important component of Kexec is the
23,/sbin/kexec command. You can load a kernel with Kexec
23,in two different ways:
23,Load the kernel to the address space of a production kernel for a regular
23,reboot:
23,# kexec -l KERNEL_IMAGE
23,You can later boot to this kernel with kexec
23,-e.
23,Load the kernel to a reserved area of memory:
23,# kexec -p KERNEL_IMAGE
23,This kernel will be booted automatically when the system crashes.
23,If you want to boot another kernel and preserve the data of the production
23,"kernel when the system crashes, you need to reserve a dedicated area of the"
23,system memory. The production kernel never loads to this area because it
23,must be always available. It is used for the capture kernel so that the
23,memory pages of the production kernel can be preserved.
23,"To reserve the area, append the option crashkernel to the"
23,boot command line of the production kernel. To determine the necessary
23,"values for crashkernel, follow the instructions in"
23,"Section 19.4, “Calculating crashkernel allocation size”."
23,Note that this is not a parameter of the capture kernel. The capture kernel
23,does not use Kexec.
23,The capture kernel is loaded to the reserved area and waits for the kernel
23,"to crash. Then, Kdump tries to invoke the capture kernel because the"
23,production kernel is no longer reliable at this stage. This means that even
23,Kdump can fail.
23,"To load the capture kernel, you need to include the kernel boot parameters."
23,"Usually, the initial RAM file system is used for booting. You can specify it"
23,with
23,--initrd=FILENAME.
23,With
23,"--append=CMDLINE,"
23,you append options to the command line of the kernel to boot.
23,It is required to include the command line of the production kernel. You can
23,simply copy the command line with
23,"--append=""$(cat"
23,"/proc/cmdline)"" or add more options with"
23,"--append=""$(cat"
23,"/proc/cmdline) more_options""."
23,"For example, to load the /boot/vmlinuz-5.14.21-150500.53-default kernel image"
23,with the command line of the currently running production kernel and the
23,"/boot/initrd file, run the following command:"
23,kexec -l /boot/vmlinuz-5.14.21-150500.53-default \
23,"--append=""$(cat /proc/cmdline)"" --initrd=/boot/initrd"
23,You can always unload the previously loaded kernel. To unload a kernel that
23,"was loaded with the -l option, use the"
23,kexec -u command. To unload a crash
23,"kernel loaded with the -p option, use"
23,kexec -p -u command.
23,19.4 Calculating crashkernel allocation size #
23,"To use Kexec with a capture kernel and to use Kdump in any way, RAM"
23,needs to be allocated for the capture kernel. The allocation size depends on
23,"the expected hardware configuration of the computer, therefore you need to"
23,specify it.
23,The allocation size also depends on the hardware architecture of your
23,computer. Make sure to follow the procedure intended for your system
23,architecture.
23,Procedure 19.1: Allocation size on AMD64/Intel 64 #
23,"To find out the base value for the computer, run the following command:"
23,# kdumptool calibrate
23,Total: 49074
23,Low: 72
23,High: 180
23,MinLow: 72
23,MaxLow: 3085
23,MinHigh: 0
23,MaxHigh: 45824
23,All values are given in megabytes.
23,Take note of the values of Low and
23,High.
23,Note: Significance of Low and High values
23,"On AMD64/Intel 64 computers, the High value stands for the"
23,memory reservation for all available memory. The Low
23,"value stands for the memory reservation in the DMA32 zone, that is, all"
23,the memory up to the 4 GB mark.
23,SIZE_LOW is the amount of memory required by 32-bit-only devices. The
23,kernel will allocate 64M for DMA32 bounce buffers. If your server does
23,"not have any 32-bit-only devices, everything should work with the default"
23,allocation of 72M for SIZE_LOW. A possible exception
23,"to this is on NUMA machines, which may make it appear that more"
23,Low memory is needed. The Kdump kernel may be booted
23,with numa=off to make sure normal kernel allocations
23,do not use Low memory.
23,Adapt the High value from the previous step for the
23,number of LUN kernel paths (paths to storage devices) attached to the
23,computer. A sensible value in megabytes can be calculated using this
23,formula:
23,SIZE_HIGH = RECOMMENDATION + (LUNs / 2)
23,The following parameters are used in this formula:
23,SIZE_HIGH.
23,The resulting value for High.
23,RECOMMENDATION.
23,The value recommended by kdumptool calibrate for
23,High.
23,LUNs.
23,The maximum number of LUN kernel paths that you expect to ever create
23,"on the computer. Exclude multipath devices from this number, as these"
23,are ignored. To get the current number of LUNs available on your
23,"system, run the following command:"
23,cat /proc/scsi/scsi | grep Lun | wc -l
23,"If the drivers for your device make many reservations in the DMA32 zone,"
23,"the Low value also needs to be adjusted. However, there"
23,is no simple formula to calculate these. Finding the right size can
23,therefore be a process of trial and error.
23,"For the beginning, use the Low value recommended by"
23,kdumptool calibrate.
23,The values now need to be set in the correct location.
23,If you are changing the kernel command line directly
23,Append the following kernel option to your boot loader configuration:
23,"crashkernel=SIZE_HIGH,high crashkernel=SIZE_LOW,low"
23,Replace the placeholders SIZE_HIGH and
23,SIZE_LOW with the appropriate value from the
23,previous steps and append the letter M (for
23,megabytes).
23,"As an example, the following is valid:"
23,"crashkernel=36M,high crashkernel=72M,lowIf you are using the YaST GUI:"
23,Set Kdump Low Memory to the determined
23,Low value.
23,Set Kdump High Memory to the determined
23,High value.
23,If you are using the YaST command line interface:
23,Use the following command:
23,"# yast kdump startup enable alloc_mem=LOW,HIGH"
23,Replace LOW with the determined
23,Low value. Replace HIGH
23,with the determined HIGH value.
23,Procedure 19.2: Allocation size on POWER and IBM Z #
23,"To find out the basis value for the computer, run the following in a"
23,terminal:
23,# kdumptool calibrate
23,This command returns a list of values. All values are given in megabytes.
23,Write down the value of Low.
23,Adapt the Low value from the previous step for the
23,number of LUN kernel paths (paths to storage devices) attached to the
23,computer. A sensible value in megabytes can be calculated using this
23,formula:
23,SIZE_LOW = RECOMMENDATION + (LUNs / 2)
23,The following parameters are used in this formula:
23,SIZE_LOW.
23,The resulting value for Low.
23,RECOMMENDATION.
23,The value recommended by kdumptool calibrate for
23,Low.
23,LUNs.
23,The maximum number of LUN kernel paths that you expect to ever create
23,"on the computer. Exclude multipath devices from this number, as these"
23,are ignored.
23,The values now need to be set in the correct location.
23,If you are working on the command line
23,Append the following kernel option to your boot loader configuration:
23,crashkernel=SIZE_LOW
23,Replace the placeholderSIZE_LOW with the
23,appropriate value from the previous step and append the letter
23,M (for megabytes).
23,"As an example, the following is valid:"
23,crashkernel=108MIf you are working in YaST
23,Set Kdump Memory to the determined
23,Low value.
23,Tip: Excluding unused and inactive CCW devices on IBM Z
23,Depending on the number of available devices the calculated amount of
23,memory specified by the crashkernel kernel parameter may
23,"not be sufficient. Instead of increasing the value, you may alternatively"
23,limit the amount of devices visible to the kernel. This will lower the
23,"required amount of memory for the ""crashkernel"" setting."
23,To ignore devices you can run the cio_ignore tool to
23,"generate an appropriate stanza to ignore all devices, except the ones"
23,currently active or in use.
23,> sudo cio_ignore -u -k
23,"cio_ignore=all,!da5d,!f500-f502"
23,"When you run cio_ignore -u -k, the blacklist will"
23,become active and replace any existing blacklist immediately. Unused
23,"devices are not being purged, so they still appear in the channel"
23,subsystem. But adding new channel devices (via CP ATTACH under z/VM or
23,dynamic I/O configuration change in LPAR) will treat them as blacklisted.
23,"To prevent this, preserve the original setting by running sudo"
23,cio_ignore -l first and reverting to that state after running
23,"cio_ignore -u -k. As an alternative, add the generated"
23,stanza to the regular kernel boot parameters.
23,Now add the cio_ignore kernel parameter with the stanza
23,from above to KDUMP_CMDLINE_APPEND in
23,"/etc/sysconfig/kdump, for example:"
23,"KDUMP_COMMANDLINE_APPEND=""cio_ignore=all,!da5d,!f500-f502"""
23,Activate the setting by restarting
23,kdump:
23,systemctl restart kdump.service19.5 Basic Kexec usage #
23,"To use Kexec, ensure the respective service is enabled and running:"
23,Make sure the Kexec service is loaded at system start:
23,> sudo systemctl enable kexec-load.service
23,Make sure the Kexec service is running:
23,> sudo systemctl start kexec-load.service
23,"To verify if your Kexec environment works properly, try rebooting into a"
23,new Kernel with Kexec. Make sure no users are currently logged in and no
23,important services are running on the system. Then run the following
23,command:
23,systemctl kexec
23,The new kernel previously loaded to the address space of the older kernel
23,rewrites it and takes control immediately. It displays the usual start-up
23,"messages. When the new kernel boots, it skips all hardware and firmware"
23,checks. Make sure no warning messages appear.
23,Tip: Using Kexec with the reboot command
23,To make reboot use Kexec rather than performing a
23,"regular reboot, run the following command:"
23,ln -s /usr/lib/systemd/system/kexec.target /etc/systemd/system/reboot.target
23,You can revert this at any time by deleting
23,etc/systemd/system/reboot.target.
23,19.6 How to configure Kexec for routine reboots #
23,"Kexec is often used for frequent reboots. For example, if it takes a long"
23,time to run through the hardware detection routines or if the start-up is
23,not reliable.
23,Note that firmware and the boot loader are not used when the system reboots
23,with Kexec. Any changes you make to the boot loader configuration will be
23,ignored until the computer performs a hard reboot.
23,19.7 Basic Kdump configuration #
23,"You can use Kdump to save kernel dumps. If the kernel crashes, it is"
23,useful to copy the memory image of the crashed environment to the file
23,system. You can then debug the dump file to find the cause of the kernel
23,crash. This is called “core dump”.
23,"Kdump works similarly to Kexec (see Chapter 19, Kexec and Kdump)."
23,The capture kernel is executed after the running production kernel crashes.
23,The difference is that Kexec replaces the production kernel with the
23,"capture kernel. With Kdump, you still have access to the memory space of"
23,the crashed production kernel. You can save the memory snapshot of the
23,crashed kernel in the environment of the Kdump kernel.
23,Tip: Dumps over network
23,"In environments with limited local storage, you need to set up kernel dumps"
23,over the network. Kdump supports configuring the specified network
23,interface and bringing it up via initrd. Both LAN
23,and VLAN interfaces are supported. Specify the network interface and the
23,"mode (DHCP or static) either with YaST, or using the"
23,KDUMP_NETCONFIG option in the
23,/etc/sysconfig/kdump file.
23,Important: Target file system for Kdump must be mounted during configuration
23,"When configuring Kdump, you can specify a location to which the dumped"
23,images will be saved (default: /var/crash). This
23,"location must be mounted when configuring Kdump, otherwise the"
23,configuration will fail.
23,19.7.1 Manual Kdump configuration #
23,Kdump reads its configuration from the
23,/etc/sysconfig/kdump file. To make sure that Kdump
23,"works on your system, its default configuration is sufficient. To use"
23,"Kdump with the default settings, follow these steps:"
23,Determine the amount of memory needed for Kdump by following the
23,"instructions in Section 19.4, “Calculating crashkernel allocation size”. Make sure"
23,to set the kernel parameter crashkernel.
23,Reboot the computer.
23,Enable the Kdump service:
23,# systemctl enable kdump
23,You can edit the options in /etc/sysconfig/kdump.
23,Reading the comments will help you understand the meaning of individual
23,options.
23,Execute the init script once with sudo systemctl start
23,"kdump, or reboot the system."
23,"After configuring Kdump with the default values, check if it works as"
23,expected. Make sure that no users are currently logged in and no important
23,services are running on your system. Then follow these steps:
23,Switch to the rescue target with systemctl isolate
23,rescue.target
23,Restart the Kdump service:
23,# systemctl start kdump
23,Unmount all the disk file systems except the root file system with:
23,# umount -a
23,Remount the root file system in read-only mode:
23,"# mount -o remount,ro /"
23,Invoke a “kernel panic” with the procfs
23,interface to Magic SysRq keys:
23,# echo c > /proc/sysrq-triggerImportant: Size of kernel dumps
23,The KDUMP_KEEP_OLD_DUMPS option controls the number of
23,"preserved kernel dumps (default is 5). Without compression, the size of"
23,the dump can take up to the size of the physical RAM memory. Make sure you
23,have sufficient space on the /var partition.
23,The capture kernel boots and the crashed kernel memory snapshot is saved to
23,the file system. The save path is given by the
23,KDUMP_SAVEDIR option and it defaults to
23,/var/crash. If
23,KDUMP_IMMEDIATE_REBOOT is set to yes
23,", the system automatically reboots the production kernel. Log in and check"
23,that the dump has been created under /var/crash.
23,19.7.1.1 Static IP configuration for Kdump #
23,In case Kdump is configured to use a static IP configuration from a
23,"network device, you need to add the network configuration to the"
23,KDUMP_COMMANDLINE_APPEND variable in
23,/etc/sysconfig/kdump.
23,Important: Changes to the Kdump configuration file
23,After making changes to the /etc/sysconfig/kdump
23,"file, you need to run systemctl restart kdump.service."
23,"Otherwise, the changes will only take effect next time you reboot the"
23,system.
23,Example 19.1: Kdump: example configuration using a static IP setup #
23,The following setup has been configured:
23,eth0 has been configured with the static IP address
23,192.168.1.1/24
23,eth1 has been configured with the static IP address
23,10.50.50.100/20
23,The Kdump configuration in /etc/sysconfig/kdump
23,looks like:
23,KDUMP_CPUS=1
23,KDUMP_IMMEDIATE_REBOOT=yes
23,KDUMP_SAVEDIR=ftp://anonymous@10.50.50.140/crashdump/
23,KDUMP_KEEP_OLD_DUMPS=5
23,KDUMP_FREE_DISK_SIZE=64
23,KDUMP_VERBOSE=3
23,KDUMP_DUMPLEVEL=31
23,KDUMP_DUMPFORMAT=lzo
23,KDUMP_CONTINUE_ON_ERROR=yes
23,KDUMP_NETCONFIG=eth1:static
23,KDUMP_NET_TIMEOUT=30
23,"Using this configuration, Kdump fails to reach the network when trying"
23,"to write the dump to the FTP server. To solve this issue, add the network"
23,configuration to KDUMP_COMMANDLINE_APPEND in
23,/etc/sysconfig/kdump. The general pattern for this
23,looks like the following:
23,KDUMP_COMMANDLINE_APPEND='ip=CLIENT IP:SERVER IP:GATEWAY IP:NETMASK:CLIENT HOSTNAME:DEVICE:PROTOCOL'
23,For the example configuration this would result in:
23,KDUMP_COMMANDLINE_APPEND='ip=10.50.50.100:10.50.50.140:10.60.48.1:255.255.240.0:dump-client:eth1:none'19.7.2 YaST configuration #
23,"To configure Kdump with YaST, you need to install the"
23,yast2-kdump package. Then either start the
23,Kernel Kdump module in the System
23,"category of YaST Control Center, or enter yast2 kdump in the"
23,command line as root.
23,Figure 19.1: YaST Kdump module: start-up page #
23,"In the Start-Up window, select Enable"
23,Kdump.
23,The values for Kdump Memory are automatically
23,"generated the first time you open the window. However, that does not mean"
23,"that they are always sufficient. To set the right values, follow the"
23,"instructions in Section 19.4, “Calculating crashkernel allocation size”."
23,"Important: After hardware changes, set Kdump memory values again"
23,If you have set up Kdump on a computer and later decide to change the
23,"amount of RAM or hard disks available to it, YaST will continue to"
23,display and use outdated memory values.
23,"To work around this, determine the necessary memory again, as described in"
23,"Section 19.4, “Calculating crashkernel allocation size”. Then set it manually in"
23,YaST.
23,"Click Dump Filtering in the left pane, and check what"
23,pages to include in the dump. You do not need to include the following
23,memory content to be able to debug kernel problems:
23,Pages filled with zero
23,Cache pages
23,User data pages
23,Free pages
23,"In the Dump Target window, select the type of the dump"
23,target and the URL where you want to save the dump. If you selected a
23,"network protocol, such as FTP or SSH, you need to enter relevant access"
23,information as well.
23,Tip: Sharing the dump directory with other applications
23,It is possible to specify a path for saving Kdump dumps where other
23,"applications also save their dumps. When cleaning its old dump files,"
23,Kdump will safely ignore other applications' dump files.
23,Fill the Email Notification window information if you
23,want Kdump to inform you about its events via e-mail and confirm your
23,changes with OK after fine tuning Kdump in the
23,Expert Settings window. Kdump is now configured.
23,19.7.3 Kdump over SSH #
23,Dump files usually contain sensitive data which should be protected from
23,unauthorized disclosure. To allow transmission of such data over an
23,"insecure network, Kdump can save dump files to a remote machine using the"
23,SSH protocol.
23,The target host identity must be known to Kdump. This is needed to
23,ensure that sensitive data is never sent to an imposter. When Kdump
23,"generates a new initrd, it runs"
23,ssh-keygen -F TARGET_HOST
23,to query the target host's identity. This works only if
23,TARGET_HOST public key is already known. An
23,easy way to achieve that is to make an SSH connection to
23,TARGET_HOST as root on the Kdump host.
23,Kdump must be able to authenticate to the target machine. Only public
23,"key authentication is currently available. By default, Kdump will use"
23,"root's private key, but it is advisable to make a separate key for"
23,Kdump. This can be done with ssh-keygen:
23,# ssh-keygen -f ~/.ssh/kdump_key
23,"Press Enter when prompted for passphrase (that is,"
23,do not use any passphrase).
23,Open /etc/sysconfig/kdump and set
23,KDUMP_SSH_IDENTITY to
23,kdump_key. You can use full path to the file
23,if it is not placed under ~/.ssh.
23,Set up the Kdump SSH key to authorize logins to the remote host.
23,# ssh-copy-id -i ~/.ssh/kdump_key TARGET_HOST
23,Set up KDUMP_SAVEDIR. There are two options:
23,Secure File Transfer Protocol (SFTP)
23,SFTP is the preferred method for transmitting files over SSH. The
23,target host must enable the SFTP subsystem (SLE default). Example:
23,KDUMP_SAVEDIR=sftp://TARGET_HOST/path/to/dumpsSecure Shell protocol (SSH)
23,Some other distributions use SSH to run some commands on the target
23,host. SUSE Linux Enterprise Server can also use this method. The Kdump user on the
23,target host must have a login shell that can execute these commands:
23,"mkdir, dd and"
23,mv. Example:
23,KDUMP_SAVEDIR=ssh://TARGET_HOST/path/to/dumps
23,Restart the Kdump service to use the new configuration.
23,19.8 Analyzing the crash dump #
23,"After you obtain the dump, it is time to analyze it. There are several"
23,options.
23,The original tool to analyze the dumps is GDB. You can even use it in the
23,"latest environments, although it has several disadvantages and limitations:"
23,GDB was not specifically designed to debug kernel dumps.
23,GDB does not support ELF64 binaries on 32-bit platforms.
23,GDB does not understand other formats than ELF dumps (it cannot debug
23,compressed dumps).
23,That is why the crash utility was implemented. It
23,analyzes crash dumps and debugs the running system as well. It provides
23,functionality specific to debugging the Linux kernel and is much more
23,suitable for advanced debugging.
23,"If you want to debug the Linux kernel, you need to install its debugging"
23,information package in addition. Check if the package is installed on your
23,system with:
23,> zypper se kernel | grep debugImportant: Repository for packages with debugging information
23,"If you subscribed your system for online updates, you can find"
23,“debuginfo” packages in the
23,*-Debuginfo-Updates online installation repository
23,relevant for SUSE Linux Enterprise Server 15 SP3. Use YaST to enable the
23,repository.
23,To open the captured dump in crash on the machine that
23,"produced the dump, use a command like this:"
23,crash /boot/vmlinux-2.6.32.8-0.1-default.gz \
23,/var/crash/2010-04-23-11\:17/vmcore
23,The first parameter represents the kernel image. The second parameter is the
23,dump file captured by Kdump. You can find this file under
23,/var/crash by default.
23,Tip: Getting basic information from a kernel crash dump
23,SUSE Linux Enterprise Server ships with the utility kdumpid (included
23,in a package with the same name) for identifying unknown kernel dumps. It
23,can be used to extract basic information such as architecture and kernel
23,"release. It supports lkcd, diskdump, Kdump files and ELF dumps. When"
23,called with the -v switch it tries to extract additional
23,"information such as machine type, kernel banner string and kernel"
23,configuration flavor.
23,19.8.1 Kernel binary formats #
23,The Linux kernel comes in Executable and Linkable Format (ELF). This file
23,is usually called vmlinux and is directly generated in
23,"the compilation process. Not all boot loaders support ELF binaries,"
23,especially on the AMD64/Intel 64 architecture. The following solutions exist on
23,different architectures supported by SUSE® Linux Enterprise Server.
23,19.8.1.1 AMD64/Intel 64 #
23,Kernel packages for AMD64/Intel 64 from SUSE contain two kernel files:
23,vmlinuz and vmlinux.gz.
23,vmlinuz.
23,This is the file executed by the boot loader.
23,The Linux kernel consists of two parts: the kernel itself
23,(vmlinux) and the setup code run by the boot loader.
23,These two parts are linked together to create
23,vmlinuz (note the distinction: z
23,compared to x).
23,"In the kernel source tree, the file is called"
23,bzImage.
23,vmlinux.gz.
23,This is a compressed ELF image that can be used by
23,crash and GDB. The ELF image is never used by the
23,"boot loader itself on AMD64/Intel 64. Therefore, only a compressed version is"
23,shipped.
23,19.8.1.2 POWER #
23,The yaboot boot loader on POWER also supports
23,"loading ELF images, but not compressed ones. In the POWER kernel"
23,"package, there is an ELF Linux kernel file vmlinux."
23,"Considering crash, this is the easiest architecture."
23,"If you decide to analyze the dump on another machine, you must check both"
23,the architecture of the computer and the files necessary for debugging.
23,You can analyze the dump on another computer only if it runs a Linux
23,"system of the same architecture. To check the compatibility, use the"
23,command uname -i on both computers and
23,compare the outputs.
23,"If you are going to analyze the dump on another computer, you also need"
23,the appropriate files from the kernel and
23,kernel debug packages.
23,"Put the kernel dump, the kernel image from /boot,"
23,and its associated debugging info file from
23,/usr/lib/debug/boot into a single empty directory.
23,"Additionally, copy the kernel modules from"
23,/lib/modules/$(uname -r)/kernel/ and the associated
23,debug info files from /usr/lib/debug/lib/modules/$(uname
23,-r)/kernel/ into a subdirectory named
23,modules.
23,"In the directory with the dump, the kernel image, its debug info file,"
23,"and the modules subdirectory, start the"
23,crash utility:
23,> crash VMLINUX-VERSION vmcore
23,"Regardless of the computer on which you analyze the dump, the crash"
23,utility will produce output similar to this:
23,> crash /boot/vmlinux-5.3.18-8-default.gz \
23,/var/crash/2020-04-23-11\:17/vmcore
23,crash 7.2.1
23,Copyright (C) 2002-2017
23,"Red Hat, Inc."
23,"Copyright (C) 2004, 2005, 2006, 2010"
23,IBM Corporation
23,Copyright (C) 1999-2006
23,Hewlett-Packard Co
23,"Copyright (C) 2005, 2006, 2011, 2012"
23,Fujitsu Limited
23,"Copyright (C) 2006, 2007"
23,VA Linux Systems Japan K.K.
23,"Copyright (C) 2005, 2011"
23,NEC Corporation
23,"Copyright (C) 1999, 2002, 2007"
23,"Silicon Graphics, Inc."
23,"Copyright (C) 1999, 2000, 2001, 2002"
23,"Mission Critical Linux, Inc."
23,"This program is free software, covered by the GNU General Public License,"
23,and you are welcome to change it and/or distribute copies of it under
23,certain conditions.
23,"Enter ""help copying"" to see the conditions."
23,This program has absolutely no warranty.
23,"Enter ""help warranty"" for details."
23,GNU gdb (GDB) 7.6
23,"Copyright (C) 2013 Free Software Foundation, Inc."
23,License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
23,This is free software: you are free to change and redistribute it.
23,"There is NO WARRANTY, to the extent permitted by law."
23,"Type ""show copying"""
23,"and ""show warranty"" for details."
23,"This GDB was configured as ""x86_64-unknown-linux-gnu""."
23,KERNEL: /boot/vmlinux-5.3.18-8-default.gz
23,DEBUGINFO: /usr/lib/debug/boot/vmlinux-5.3.18-8-default.debug
23,DUMPFILE: /var/crash/2020-04-23-11:17/vmcore
23,CPUS: 2
23,DATE: Thu Apr 23 13:17:01 2020
23,UPTIME: 00:10:41
23,"LOAD AVERAGE: 0.01, 0.09, 0.09"
23,TASKS: 42
23,NODENAME: eros
23,RELEASE: 5.3.18-8-default
23,VERSION: #1 SMP 2020-03-31 14:50:44 +0200
23,MACHINE: x86_64
23,(2999 Mhz)
23,MEMORY: 16 GB
23,"PANIC: ""SysRq : Trigger a crashdump"""
23,PID: 9446
23,"COMMAND: ""bash"""
23,TASK: ffff88003a57c3c0
23,[THREAD_INFO: ffff880037168000]
23,CPU: 1
23,STATE: TASK_RUNNING (SYSRQ)
23,crash>
23,The command output prints first useful data: There were 42 tasks running
23,at the moment of the kernel crash. The cause of the crash was a SysRq
23,trigger invoked by the task with PID 9446. It was a Bash process because
23,the echo that has been used is an internal command of
23,the Bash shell.
23,The crash utility builds upon GDB and provides many
23,additional commands. If you enter bt without any
23,"parameters, the backtrace of the task running at the moment of the crash"
23,is printed:
23,crash> bt
23,PID: 9446
23,TASK: ffff88003a57c3c0
23,CPU: 1
23,"COMMAND: ""bash"""
23,#0 [ffff880037169db0] crash_kexec at ffffffff80268fd6
23,#1 [ffff880037169e80] __handle_sysrq at ffffffff803d50ed
23,#2 [ffff880037169ec0] write_sysrq_trigger at ffffffff802f6fc5
23,#3 [ffff880037169ed0] proc_reg_write at ffffffff802f068b
23,#4 [ffff880037169f10] vfs_write at ffffffff802b1aba
23,#5 [ffff880037169f40] sys_write at ffffffff802b1c1f
23,#6 [ffff880037169f80] system_call_fastpath at ffffffff8020bfbb
23,RIP: 00007fa958991f60
23,RSP: 00007fff61330390
23,RFLAGS: 00010246
23,RAX: 0000000000000001
23,RBX: ffffffff8020bfbb
23,RCX: 0000000000000001
23,RDX: 0000000000000002
23,RSI: 00007fa959284000
23,RDI: 0000000000000001
23,RBP: 0000000000000002
23,R8: 00007fa9592516f0
23,R9: 00007fa958c209c0
23,R10: 00007fa958c209c0
23,R11: 0000000000000246
23,R12: 00007fa958c1f780
23,R13: 00007fa959284000
23,R14: 0000000000000002
23,R15: 00000000595569d0
23,ORIG_RAX: 0000000000000001
23,CS: 0033
23,SS: 002b
23,crash>
23,Now it is clear what happened: The internal echo
23,command of Bash shell sent a character to
23,/proc/sysrq-trigger. After the corresponding handler
23,"recognized this character, it invoked the crash_kexec()"
23,function. This function called panic() and Kdump
23,saved a dump.
23,In addition to the basic GDB commands and the extended version of
23,"bt, the crash utility defines other commands related to"
23,the structure of the Linux kernel. These commands understand the internal
23,data structures of the Linux kernel and present their contents in a human
23,"readable format. For example, you can list the tasks running at the moment"
23,"of the crash with ps. With sym, you"
23,"can list all the kernel symbols with the corresponding addresses, or"
23,"inquire an individual symbol for its value. With files,"
23,you can display all the open file descriptors of a process. With
23,"kmem, you can display details about the kernel memory"
23,"usage. With vm, you can inspect the virtual memory of a"
23,"process, even at the level of individual page mappings. The list of useful"
23,commands is very long and many of these accept a wide range of options.
23,The commands that we mentioned reflect the functionality of the common
23,"Linux commands, such as ps and lsof."
23,"To find out the exact sequence of events with the debugger, you need to"
23,know how to use GDB and to have strong debugging skills. Both of these are
23,"out of the scope of this document. In addition, you need to understand the"
23,Linux kernel. Several useful reference information sources are given at
23,the end of this document.
23,19.9 Advanced Kdump configuration #
23,The configuration for Kdump is stored in
23,/etc/sysconfig/kdump. You can also use YaST to
23,configure it. Kdump configuration options are available under
23,System › Kernel
23,Kdump in YaST Control Center. The following Kdump options may
23,be useful for you.
23,You can change the directory for the kernel dumps with the
23,KDUMP_SAVEDIR option. Keep in mind that the size of kernel
23,dumps can be very large. Kdump will refuse to save the dump if the free
23,"disk space, subtracted by the estimated dump size, drops below the value"
23,specified by the KDUMP_FREE_DISK_SIZE option. Note that
23,KDUMP_SAVEDIR understands the URL format
23,"PROTOCOL://SPECIFICATION, where"
23,"PROTOCOL is one of file,"
23,"ftp, sftp, nfs or"
23,"cifs, and specification varies for each"
23,"protocol. For example, to save kernel dump on an FTP server, use the"
23,following URL as a template:
23,ftp://username:password@ftp.example.com:123/var/crash.
23,Kernel dumps are usually huge and contain many pages that are not necessary
23,"for analysis. With KDUMP_DUMPLEVEL option, you can omit"
23,such pages. The option understands numeric value between 0 and 31. If you
23,"specify 0, the dump size will be largest. If you"
23,"specify 31, it will produce the smallest dump."
23,"For a complete table of possible values, see the manual page of"
23,kdump (man 7 kdump).
23,Sometimes it is very useful to make the size of the kernel dump smaller. For
23,"example, if you want to transfer the dump over the network, or if you need"
23,to save some disk space in the dump directory. This can be done with
23,KDUMP_DUMPFORMAT set to compressed. The
23,crash utility supports dynamic decompression of the
23,compressed dumps.
23,Important: Changes to the Kdump configuration file
23,After making changes to the /etc/sysconfig/kdump
23,"file, you need to run systemctl restart kdump.service."
23,"Otherwise, the changes will only take effect next time you reboot the"
23,system.
23,19.10 More information #
23,There is no single comprehensive reference to Kexec and Kdump usage.
23,"However, there are helpful resources that deal with certain aspects:"
23,"For the Kexec utility usage, see the manual page of"
23,kexec (man 8 kexec).
23,IBM provides comprehensive documentation on how to use dump tools on the
23,IBM Z architecture at
23,https://developer.ibm.com/technologies/linux/.
23,You can find general information about Kexec at
23,https://developer.ibm.com/technologies/linux/.
23,"For more details on Kdump specific to SUSE Linux Enterprise Server, see"
23,http://ftp.suse.com/pub/people/tiwai/kdump-training/kdump-training.pdf
23,An in-depth description of Kdump internals can be found at
23,http://lse.sourceforge.net/kdump/documentation/ols2oo5-kdump-paper.pdf
23,For more details on crash dump analysis and debugging
23,"tools, use the following resources:"
23,"In addition to the info page of GDB (info gdb), there"
23,are printable guides at
23,https://sourceware.org/gdb/documentation/ .
23,The crash utility features a comprehensive online help. Use
23,help COMMAND to display the
23,online help for command.
23,"If you have the necessary Perl skills, you can use Alicia to make the"
23,debugging easier. This Perl-based front-end to the crash utility can be
23,found at http://alicia.sourceforge.net/ .
23,"If you prefer to use Python instead, you should install Pykdump. This"
23,package helps you control GDB through Python scripts.
23,A very comprehensive overview of the Linux kernel internals is given in
23,Understanding the Linux Kernel by Daniel P. Bovet
23,and Marco Cesati (ISBN 978-0-596-00565-8).
23,20 Using systemd-coredump to debug application crashes #
23,"systemd-coredump collects and displays core dumps, for analyzing"
23,application crashes. The core dump contains an image of the process's
23,memory at the time of termination. When a process crashes (or all
23,"processes belonging to an application), its default is to log the core"
23,"dump to the systemd journal, including a backtrace if possible, and to"
23,store the core dump in a file in
23,/var/lib/systemd/coredump. You also have the option
23,to examine the dump file with other tools such as gdb
23,"or crash (see Section 19.8, “Analyzing the crash dump”)."
23,Core dumps stored in /var/lib/systemd/coredump
23,are deleted after three days (see the
23,d /var/lib/systemd/coredump line in
23,/usr/lib/tmpfiles.d/systemd.conf).
23,"There is an option to not store core dumps, but to log only to the"
23,"journal, which may be useful to minimize the collection and storage of"
23,sensitive information.
23,20.1 Use and configuration #
23,systemd-coredump is enabled and ready to run by default. The default
23,configuration is in /etc/systemd/coredump.conf:
23,[Coredump]
23,#Storage=external
23,#Compress=yes
23,#ProcessSizeMax=2G
23,#ExternalSizeMax=2G
23,#JournalSizeMax=767M
23,#MaxUse=
23,#KeepFree=
23,"Size units are B, K, M, G, T, P, and E. ExternalSizeMax also supports a value of infinity."
23,"The following example shows how to use Vim for simple testing, by creating a"
23,segfault to generate journal entries and a core dump.
23,Procedure 20.1: Creating a core dump with Vim #
23,Enable the debuginfo-pool and
23,debuginfo-update repositories
23,Install vim-debuginfo
23,Launch vim testfile and type a few characters
23,Get the PID and generate a segfault:
23,> ps ax | grep vim
23,2345 pts/3
23,0:00 vim testfile
23,# kill -s SIGSEGV 2345
23,Vim will emit error messages:
23,Vim: Caught deadly signal SEGV
23,Vim: Finished.
23,Segmentation fault (core dumped)
23,"List your core dumps, then examine them:"
23,# coredumpctl
23,TIME
23,PID
23,UID
23,GID SIG PRESENT EXE
23,Wed 2019-11-12 11:56:47 PST 2345 1000 100 11
23,/bin/vim
23,# coredumpctl info
23,PID: 2345 (vim)
23,UID: 0 (root)
23,GID: 0 (root)
23,Signal: 11 (SEGV)
23,Timestamp: Wed 2019-11-12 11:58:05 PST
23,Command Line: vim testfile
23,Executable: /bin/vim
23,Control Group: /user.slice/user-1000.slice/session-1.scope
23,Unit: session-1.scope
23,Slice: user-1000.slice
23,Session: 1
23,Owner UID: 1000 (tux)
23,Boot ID: b5c251b86ab34674a2222cef102c0c88
23,Machine ID: b43c44a64696799b985cafd95dc1b698
23,Hostname: linux-uoch
23,Coredump: /var/lib/systemd/coredump/core.vim.0.b5c251b86ab34674a2222cef102
23,Message: Process 2345 (vim) of user 0 dumped core.
23,Stack trace of thread 2345:
23,0x00007f21dd87e2a7 kill (libc.so.6)
23,0x000000000050cb35 may_core_dump (vim)
23,0x00007f21ddbfec70 __restore_rt (libpthread.so.0)
23,0x00007f21dd92ea33 __select (libc.so.6)
23,0x000000000050b4e3 RealWaitForChar (vim)
23,0x000000000050b86b mch_inchar (vim)
23,[...]
23,"When you have multiple core dumps, coredumpctl info"
23,"displays all of them. Filter them by PID,"
23,"COMM (command), or EXE (full path to"
23,"the executable), for example, all core dumps for Vim:"
23,# coredumpctl info /bin/vim
23,See a single core dump by PID:
23,# coredumpctl info 2345
23,Output the selected core to gdb:
23,# coredumpctl gdb 2345
23,The asterisk in the PRESENT column indicates that a
23,stored core dump is present. If the field is empty there is no stored core
23,"dump, and coredumpctl retrieves crash information from"
23,the journal. You can control this behavior in
23,/etc/systemd/coredump.conf with the
23,Storage option:
23,"Storage=none—core dumps are logged in the journal, but"
23,not stored. This is useful to minimize collecting and storing sensitive
23,"information, for example for General Data Protection Regulation (GDPR)"
23,compliance.
23,Storage=external—cores are stored in
23,/var/lib/systemd/coredump
23,Storage=journal—cores are stored in the systemd
23,journal
23,"A new instance of systemd-coredump is invoked for every core dump, so"
23,"configuration changes are applied with the next core dump, and there is no"
23,need to restart any services.
23,Core dumps are not preserved after a system restart. You may save them
23,permanently with coredumpctl. The following example
23,filters by the PID and stores the core in
23,vim.dump:
23,# coredumpctl -o vim.dump dump 2345
23,"See man systemd-coredump, man"
23,"coredumpctl, man core, and"
23,man coredump.conf for complete
23,command and option listings.
23,Part VII Synchronized clocks with Precision Time Protocol #  21 Precision Time Protocol
23,"For network environments, it is vital to keep the computer and other devices'"
23,clocks synchronized and accurate. There are several solutions to achieve
23,"this, for example the widely used Network Time Protocol (NTP) described in"
23,"Book “Administration Guide”, Chapter 30 “Time synchronization with NTP”."
23,21 Precision Time Protocol #
23,"For network environments, it is vital to keep the computer and other devices'"
23,clocks synchronized and accurate. There are several solutions to achieve
23,"this, for example the widely used Network Time Protocol (NTP) described in"
23,"Book “Administration Guide”, Chapter 30 “Time synchronization with NTP”."
23,"The Precision Time Protocol (PTP) is a protocol capable of sub-microsecond accuracy, which is"
23,better than what NTP achieves. PTP support is divided between the kernel and
23,"user space. The kernel in SUSE Linux Enterprise Server includes support for PTP clocks,"
23,which are provided by network drivers.
23,21.1 Introduction to PTP #
23,The clocks managed by PTP follow a master-slave hierarchy. The slaves are
23,synchronized to their masters. The hierarchy is updated by the
23,"best master clock (BMC) algorithm, which runs on every"
23,clock. The clock with only one port can be either master or slave. Such a
23,clock is called an ordinary clock (OC). A clock with
23,multiple ports can be master on one port and slave on another. Such a clock
23,is called a boundary clock (BC). The top-level master
23,is called the grandmaster clock. The grandmaster clock
23,can be synchronized with a Global Positioning System (GPS). This way
23,disparate networks can be synchronized with a high degree of accuracy.
23,The hardware support is the main advantage of PTP. It is supported by
23,various network switches and network interface controllers (NIC). While it
23,"is possible to use non-PTP enabled hardware within the network, having"
23,network components between all PTP clocks PTP hardware enabled achieves the
23,best possible accuracy.
23,21.1.1 PTP Linux implementation #
23,"On SUSE Linux Enterprise Server, the implementation of PTP is provided by the"
23,linuxptp package. Install it with zypper
23,install linuxptp. It includes the ptp4l and
23,phc2sys programs for clock synchronization.
23,ptp4l implements the PTP boundary clock and ordinary
23,"clock. When hardware time stamping is enabled, ptp4l"
23,synchronizes the PTP hardware clock to the master clock. With software time
23,"stamping, it synchronizes the system clock to the master clock."
23,phc2sys is needed only with hardware time stamping to
23,synchronize the system clock to the PTP hardware clock on the network
23,interface card (NIC).
23,21.2 Using PTP #  21.2.1 Network driver and hardware support #
23,PTP requires that the used kernel network driver supports either software
23,"or hardware time stamping. Moreover, the NIC must support time stamping in"
23,the physical hardware. You can verify the driver and NIC time stamping
23,capabilities with ethtool:
23,> sudo ethtool -T eth0
23,Time stamping parameters for eth0:
23,Capabilities:
23,hardware-transmit
23,(SOF_TIMESTAMPING_TX_HARDWARE)
23,software-transmit
23,(SOF_TIMESTAMPING_TX_SOFTWARE)
23,hardware-receive
23,(SOF_TIMESTAMPING_RX_HARDWARE)
23,software-receive
23,(SOF_TIMESTAMPING_RX_SOFTWARE)
23,software-system-clock (SOF_TIMESTAMPING_SOFTWARE)
23,hardware-raw-clock
23,(SOF_TIMESTAMPING_RAW_HARDWARE)
23,PTP Hardware Clock: 0
23,Hardware Transmit Timestamp Modes:
23,off
23,(HWTSTAMP_TX_OFF)
23,(HWTSTAMP_TX_ON)
23,Hardware Receive Filter Modes:
23,none
23,(HWTSTAMP_FILTER_NONE)
23,all
23,(HWTSTAMP_FILTER_ALL)
23,Software time stamping requires the following parameters:
23,SOF_TIMESTAMPING_SOFTWARE
23,SOF_TIMESTAMPING_TX_SOFTWARE
23,SOF_TIMESTAMPING_RX_SOFTWARE
23,Hardware time stamping requires the following parameters:
23,SOF_TIMESTAMPING_RAW_HARDWARE
23,SOF_TIMESTAMPING_TX_HARDWARE
23,SOF_TIMESTAMPING_RX_HARDWARE21.2.2 Using ptp4l #
23,ptp4l uses hardware time stamping by default. As
23,"root, you need to specify the network interface capable of hardware"
23,time stamping with the -i option. The -m
23,tells ptp4l to print its output to the standard output
23,instead of the system's logging facility:
23,> sudo ptp4l -m -i eth0
23,selected eth0 as PTP clock
23,port 1: INITIALIZING to LISTENING on INITIALIZE
23,port 0: INITIALIZING to LISTENING on INITIALIZE
23,port 1: new foreign master 00a152.fffe.0b334d-1
23,selected best master clock 00a152.fffe.0b334d
23,port 1: LISTENING to UNCALIBRATED on RS_SLAVE
23,master offset -25937 s0 freq +0 path delay
23,12340
23,master offset -27887 s0 freq +0 path delay
23,14232
23,master offset -38802 s0 freq +0 path delay
23,13847
23,master offset -36205 s1 freq +0 path delay
23,10623
23,master offset
23,-6975 s2 freq -30575 path delay
23,10286
23,port 1: UNCALIBRATED to SLAVE on MASTER_CLOCK_SELECTED
23,master offset
23,-4284 s2 freq -30135 path delay
23,9892
23,The master offset value represents the measured offset
23,from the master (in nanoseconds).
23,"The s0, s1, s2"
23,indicators show the different states of the clock servo:
23,"s0 is unlocked, s1 is clock step, and"
23,s2 is locked. If the servo is in the locked state
23,"(s2), the clock will not be stepped (only slowly"
23,adjusted) if the pi_offset_const option is set to a
23,negative value in the configuration file (see man 8
23,ptp4l for more information).
23,The freq value represents the frequency adjustment of
23,"the clock (in parts per billion, ppb)."
23,The path delay value represents the estimated delay of
23,the synchronization messages sent from the master (in nanoseconds).
23,Port 0 is a Unix domain socket used for local PTP management. Port 1 is the
23,eth0 interface.
23,"INITIALIZING, LISTENING,"
23,UNCALIBRATED and SLAVE are examples
23,"of port states which change on INITIALIZE,"
23,"RS_SLAVE, and MASTER_CLOCK_SELECTED"
23,events. When the port state changes from UNCALIBRATED to
23,"SLAVE, the computer has successfully synchronized with a"
23,PTP master clock.
23,You can enable software time stamping with the -S option.
23,> sudo ptp4l -m -S -i eth3
23,You can also run ptp4l as a service:
23,> sudo systemctl start ptp4l
23,"In this case, ptp4l reads its options from the"
23,"/etc/sysconfig/ptp4l file. By default, this file tells"
23,ptp4l to read the configuration options from
23,/etc/ptp4l.conf. For more information on
23,"ptp4l options and the configuration file settings, see"
23,man 8 ptp4l.
23,"To enable the ptp4l service permanently, run the"
23,following:
23,> sudo systemctl enable ptp4l
23,"To disable it, run"
23,> sudo systemctl disable ptp4l21.2.3 ptp4l configuration file #
23,ptp4l can read its configuration from an optional
23,"configuration file. As no configuration file is used by default, you need"
23,to specify it with -f.
23,> sudo ptp4l -f /etc/ptp4l.conf
23,The configuration file is divided into sections. The global section
23,"(indicated as [global]) sets the program options, clock"
23,"options and default port options. Other sections are port specific, and"
23,they override the default port options. The name of the section is the name
23,"of the configured port—for example, [eth0]. An"
23,empty port section can be used to replace the command line option.
23,[global]
23,verbose
23,time_stamping
23,software
23,[eth0]
23,The example configuration file is an equivalent of the following command's
23,options:
23,> sudo ptp4l -i eth0 -m -S
23,"For a complete list of ptp4l configuration options, see"
23,man 8 ptp4l.
23,21.2.4 Delay measurement #
23,ptp4l measures time delay in two different ways:
23,peer-to-peer (P2P) or end-to-end
23,(E2E).
23,P2P
23,This method is specified with -P.
23,It reacts to changes in the network environment faster and is more
23,accurate in measuring the delay. It is only used in networks where each
23,port exchanges PTP messages with one other port. P2P needs to be
23,supported by all hardware on the communication path.
23,E2E
23,This method is specified with -E. This is the default.
23,Automatic method selection
23,This method is specified with -A. The automatic option
23,"starts ptp4l in E2E mode, and changes to P2P mode if"
23,a peer delay request is received.
23,Important: Common measurement method
23,All clocks on a single PTP communication path must use the same method to
23,measure the time delay. A warning will be printed if either a peer delay
23,"request is received on a port using the E2E mechanism, or an E2E delay"
23,request is received on a port using the P2P mechanism.
23,21.2.5 PTP management client: pmc #
23,You can use the pmc client to obtain more detailed
23,information about ptp41. It reads from the standard
23,input—or from the command line—actions specified by name and
23,"management ID. Then it sends the actions over the selected transport, and"
23,prints any received replies. There are three actions supported:
23,"GET retrieves the specified information,"
23,"SET updates the specified information, and"
23,CMD (or COMMAND) initiates the
23,specified event.
23,"By default, the management commands are addressed to all ports. The"
23,TARGET command can be used to select a particular clock
23,and port for the subsequent messages. For a complete list of management
23,"IDs, run pmc help."
23,> sudo pmc -u -b 0 'GET TIME_STATUS_NP'
23,sending: GET TIME_STATUS_NP
23,90f2ca.fffe.20d7e9-0 seq 0 RESPONSE MANAGMENT TIME_STATUS_NP
23,master_offset
23,283
23,ingress_time
23,1361569379345936841
23,cumulativeScaledRateOffset
23,+1.000000000
23,scaledLastGmPhaseChange
23,gmTimeBaseIndicator
23,lastGmPhaseChange
23,0x0000'0000000000000000.0000
23,gmPresent
23,true
23,gmIdentity
23,00b058.feef.0b448a
23,The -b option specifies the boundary hops value in sent
23,messages. Setting it to zero limits the boundary to the local
23,ptp4l instance. Increasing the value will retrieve the
23,messages also from PTP nodes that are further from the local instance. The
23,returned information may include:
23,stepsRemoved
23,The number of communication nodes to the grandmaster clock.
23,"offsetFromMaster, master_offset"
23,The last measured offset of the clock from the master clock
23,(nanoseconds).
23,meanPathDelay
23,The estimated delay of the synchronization messages sent from the master
23,clock (nanoseconds).
23,gmPresent
23,"If true, the PTP clock is synchronized to the master"
23,clock; the local clock is not the grandmaster clock.
23,gmIdentity
23,This is the grandmaster's identity.
23,"For a complete list of pmc command line options, see"
23,man 8 pmc.
23,21.3 Synchronizing the clocks with phc2sys #
23,Use phc2sys to synchronize the system clock to the PTP
23,hardware clock (PHC) on the network card. The system clock is considered a
23,"slave, while the network card a"
23,master. PHC itself is synchronized with
23,"ptp4l (see Section 21.2, “Using PTP”). Use"
23,-s to specify the master clock by device or network
23,interface. Use -w to wait until ptp4l is
23,in a synchronized state.
23,> sudo phc2sys -s eth0 -w
23,"PTP operates in International Atomic Time (TAI), while"
23,the system clock uses Coordinated Universal Time (UTC).
23,If you do not specify -w to wait for
23,"ptp4l synchronization, you can specify the offset in"
23,seconds between TAI and UTC with -O:
23,> sudo phc2sys -s eth0 -O -35
23,You can run phc2sys as a service as well:
23,> sudo systemctl start phc2sys
23,"In this case, phc2sys reads its options from the"
23,/etc/sysconfig/phc2sys file. For more information on
23,"phc2sys options, see man 8 phc2sys."
23,"To enable the phc2sys service permanently, run the"
23,following:
23,> sudo systemctl enable phc2sys
23,"To disable it, run"
23,> sudo systemctl disable phc2sys21.3.1 Verifying time synchronization #
23,When PTP time synchronization is working properly and hardware time
23,"stamping is used, ptp4l and phc2sys"
23,output messages with time offsets and frequency adjustments periodically to
23,the system log.
23,An example of the ptp4l output:
23,ptp4l[351.358]: selected /dev/ptp0 as PTP clock
23,ptp4l[352.361]: port 1: INITIALIZING to LISTENING on INITIALIZE
23,ptp4l[352.361]: port 0: INITIALIZING to LISTENING on INITIALIZE
23,ptp4l[353.210]: port 1: new foreign master 00a069.eefe.0b442d-1
23,ptp4l[357.214]: selected best master clock 00a069.eefe.0b662d
23,ptp4l[357.214]: port 1: LISTENING to UNCALIBRATED on RS_SLAVE
23,ptp4l[359.224]: master offset
23,3304 s0 freq
23,+0 path delay
23,9202
23,ptp4l[360.224]: master offset
23,3708 s1 freq
23,-28492 path delay
23,9202
23,ptp4l[361.224]: master offset
23,-3145 s2 freq
23,-32637 path delay
23,9202
23,ptp4l[361.224]: port 1: UNCALIBRATED to SLAVE on MASTER_CLOCK_SELECTED
23,ptp4l[362.223]: master offset
23,-145 s2 freq
23,-30580 path delay
23,9202
23,ptp4l[363.223]: master offset
23,1043 s2 freq
23,-28436 path delay
23,8972
23,[...]
23,ptp4l[371.235]: master offset
23,285 s2 freq
23,-28511 path delay
23,9199
23,ptp4l[372.235]: master offset
23,-78 s2 freq
23,-28788 path delay
23,9204
23,An example of the phc2sys output:
23,phc2sys[616.617]: Waiting for ptp4l...
23,phc2sys[628.628]: phc offset
23,66341 s0 freq
23,+0 delay
23,2729
23,phc2sys[629.628]: phc offset
23,64668 s1 freq
23,-37690 delay
23,2726
23,[...]
23,phc2sys[646.630]: phc offset
23,-333 s2 freq
23,-37426 delay
23,2747
23,phc2sys[646.630]: phc offset
23,194 s2 freq
23,-36999 delay
23,2749
23,ptp4l normally writes messages very frequently. You can
23,reduce the frequency with the summary_interval
23,"directive. Its value is an exponent of the 2^N expression. For example, to"
23,"reduce the output to every 1024 (which is equal to 2^10) seconds, add the"
23,following line to the /etc/ptp4l.conf file:
23,summary_interval 10
23,You can also reduce the frequency of the phc2sys
23,command's updates with the -u
23,SUMMARY-UPDATES option.
23,21.4 Examples of configurations #
23,This section includes several examples of ptp4l
23,configuration. The examples are not full configuration files but rather a
23,minimal list of changes to be made to the specific files. The string
23,ethX stands for the actual network interface name
23,in your setup.
23,Example 21.1: Slave clock using software time stamping #
23,/etc/sysconfig/ptp4l:
23,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
23,No changes made to the distribution /etc/ptp4l.conf.
23,Example 21.2: Slave clock using hardware time stamping #
23,/etc/sysconfig/ptp4l:
23,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
23,/etc/sysconfig/phc2sys:
23,OPTIONS=”-s ethX -w”
23,No changes made to the distribution /etc/ptp4l.conf.
23,Example 21.3: Master clock using hardware time stamping #
23,/etc/sysconfig/ptp4l:
23,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
23,/etc/sysconfig/phc2sys:
23,OPTIONS=”-s CLOCK_REALTIME -c ethX -w”
23,/etc/ptp4l.conf:
23,priority1 127Example 21.4: Master clock using software time stamping (not generally recommended) #
23,/etc/sysconfig/ptp4l:
23,OPTIONS=”-f /etc/ptp4l.conf -i ethX”
23,/etc/ptp4l.conf:
23,priority1 12721.5 PTP and NTP #
23,"NTP and PTP time synchronization tools can coexist, synchronizing time from"
23,one to another in both directions.
23,21.5.1 NTP to PTP synchronization #
23,"When chronyd is used to synchronize the local system clock, you can"
23,configure the ptp4l to be the grandmaster clock
23,distributing the time from the local system clock via PTP. Include the
23,priority1 option in /etc/ptp4l.conf:
23,[global]
23,priority1 127
23,[eth0]
23,Then run ptp4l:
23,> sudo ptp4l -f /etc/ptp4l.conf
23,"When hardware time stamping is used, you need to synchronize the PTP"
23,hardware clock to the system clock with phc2sys:
23,> sudo phc2sys -c eth0 -s CLOCK_REALTIME -w21.5.2 Configuring PTP-NTP bridge #
23,If a highly accurate PTP grandmaster is available in a network without
23,"switches or routers with PTP support, a computer may operate as a PTP slave"
23,and a stratum-1 NTP server. Such a computer needs to have two or more
23,"network interfaces, and be close to the grandmaster or have a direct"
23,connection to it. This will ensure highly accurate synchronization in the
23,network.
23,Configure the ptp4l and phc2sys
23,programs to use one network interface to synchronize the system clock using
23,PTP. Then configure chronyd to provide the system time using the other
23,interface:
23,bindaddress 192.0.131.47
23,hwtimestamp eth1
23,local stratum 1Note: NTP and DHCP
23,When the DHCP client command dhclient receives a list
23,"of NTP servers, it adds them to NTP configuration by default. To prevent"
23,"this behavior, set"
23,"NETCONFIG_NTP_POLICY="""""
23,in the /etc/sysconfig/network/config file.
23,A GNU licenses #
23,This appendix contains the GNU Free Documentation License version 1.2.
23,GNU Free Documentation License #
23,"Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,"
23,"Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and"
23,"distribute verbatim copies of this license document, but changing it is not"
23,allowed.
23,0. PREAMBLE
23,"The purpose of this License is to make a manual, textbook, or other"
23,"functional and useful document ""free"" in the sense of freedom: to assure"
23,"everyone the effective freedom to copy and redistribute it, with or without"
23,"modifying it, either commercially or non-commercially. Secondarily, this"
23,License preserves for the author and publisher a way to get credit for their
23,"work, while not being considered responsible for modifications made by"
23,others.
23,"This License is a kind of ""copyleft"", which means that derivative works of"
23,the document must themselves be free in the same sense. It complements the
23,"GNU General Public License, which is a copyleft license designed for free"
23,software.
23,"We have designed this License to use it for manuals for free software,"
23,because free software needs free documentation: a free program should come
23,with manuals providing the same freedoms that the software does. But this
23,License is not limited to software manuals; it can be used for any textual
23,"work, regardless of subject matter or whether it is published as a printed"
23,book. We recommend this License principally for works whose purpose is
23,instruction or reference.
23,1. APPLICABILITY AND DEFINITIONS
23,"This License applies to any manual or other work, in any medium, that"
23,contains a notice placed by the copyright holder saying it can be distributed
23,"under the terms of this License. Such a notice grants a world-wide,"
23,"royalty-free license, unlimited in duration, to use that work under the"
23,"conditions stated herein. The ""Document"", below, refers to any such manual or"
23,"work. Any member of the public is a licensee, and is addressed as ""you"". You"
23,"accept the license if you copy, modify or distribute the work in a way"
23,requiring permission under copyright law.
23,"A ""Modified Version"" of the Document means any work containing the Document"
23,"or a portion of it, either copied verbatim, or with modifications and/or"
23,translated into another language.
23,"A ""Secondary Section"" is a named appendix or a front-matter section of the"
23,Document that deals exclusively with the relationship of the publishers or
23,authors of the Document to the Document's overall subject (or to related
23,matters) and contains nothing that could fall directly within that overall
23,"subject. (Thus, if the Document is in part a textbook of mathematics, a"
23,Secondary Section may not explain any mathematics.) The relationship could be
23,"a matter of historical connection with the subject or with related matters,"
23,"or of legal, commercial, philosophical, ethical or political position"
23,regarding them.
23,"The ""Invariant Sections"" are certain Secondary Sections whose titles are"
23,"designated, as being those of Invariant Sections, in the notice that says"
23,that the Document is released under this License. If a section does not fit
23,the above definition of Secondary then it is not allowed to be designated as
23,Invariant. The Document may contain zero Invariant Sections. If the Document
23,does not identify any Invariant Sections then there are none.
23,"The ""Cover Texts"" are certain short passages of text that are listed, as"
23,"Front-Cover Texts or Back-Cover Texts, in the notice that says that the"
23,Document is released under this License. A Front-Cover Text may be at most 5
23,"words, and a Back-Cover Text may be at most 25 words."
23,"A ""Transparent"" copy of the Document means a machine-readable copy,"
23,represented in a format whose specification is available to the general
23,"public, that is suitable for revising the document straightforwardly with"
23,generic text editors or (for images composed of pixels) generic paint
23,"programs or (for drawings) some widely available drawing editor, and that is"
23,suitable for input to text formatters or for automatic translation to a
23,variety of formats suitable for input to text formatters. A copy made in an
23,"otherwise Transparent file format whose markup, or absence of markup, has"
23,been arranged to thwart or discourage subsequent modification by readers is
23,not Transparent. An image format is not Transparent if used for any
23,"substantial amount of text. A copy that is not ""Transparent"" is called"
23,"""Opaque""."
23,Examples of suitable formats for Transparent copies include plain ASCII
23,"without markup, Texinfo input format, LaTeX input format, SGML or XML using a"
23,"publicly available DTD, and standard-conforming simple HTML, PostScript or"
23,PDF designed for human modification. Examples of transparent image formats
23,"include PNG, XCF and JPG. Opaque formats include proprietary formats that can"
23,"be read and edited only by proprietary word processors, SGML or XML for which"
23,"the DTD and/or processing tools are not generally available, and the"
23,"machine-generated HTML, PostScript or PDF produced by some word processors"
23,for output purposes only.
23,"The ""Title Page"" means, for a printed book, the title page itself, plus such"
23,"following pages as are needed to hold, legibly, the material this License"
23,requires to appear in the title page. For works in formats which do not have
23,"any title page as such, ""Title Page"" means the text near the most prominent"
23,"appearance of the work's title, preceding the beginning of the body of the"
23,text.
23,"A section ""Entitled XYZ"" means a named subunit of the Document whose title"
23,either is precisely XYZ or contains XYZ in parentheses following text that
23,translates XYZ in another language. (Here XYZ stands for a specific section
23,"name mentioned below, such as ""Acknowledgements"", ""Dedications"","
23,"""Endorsements"", or ""History"".) To ""Preserve the Title"" of such a section when"
23,"you modify the Document means that it remains a section ""Entitled XYZ"""
23,according to this definition.
23,The Document may include Warranty Disclaimers next to the notice which states
23,that this License applies to the Document. These Warranty Disclaimers are
23,"considered to be included by reference in this License, but only as regards"
23,disclaiming warranties: any other implication that these Warranty Disclaimers
23,may have is void and has no effect on the meaning of this License.
23,2. VERBATIM COPYING
23,"You may copy and distribute the Document in any medium, either commercially"
23,"or non-commercially, provided that this License, the copyright notices, and"
23,the license notice saying this License applies to the Document are reproduced
23,"in all copies, and that you add no other conditions whatsoever to those of"
23,this License. You may not use technical measures to obstruct or control the
23,"reading or further copying of the copies you make or distribute. However, you"
23,may accept compensation in exchange for copies. If you distribute a large
23,enough number of copies you must also follow the conditions in section 3.
23,"You may also lend copies, under the same conditions stated above, and you may"
23,publicly display copies.
23,3. COPYING IN QUANTITY
23,If you publish printed copies (or copies in media that commonly have printed
23,"covers) of the Document, numbering more than 100, and the Document's license"
23,"notice requires Cover Texts, you must enclose the copies in covers that"
23,"carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the"
23,"front cover, and Back-Cover Texts on the back cover. Both covers must also"
23,clearly and legibly identify you as the publisher of these copies. The front
23,cover must present the full title with all words of the title equally
23,prominent and visible. You may add other material on the covers in addition.
23,"Copying with changes limited to the covers, as long as they preserve the"
23,"title of the Document and satisfy these conditions, can be treated as"
23,verbatim copying in other respects.
23,"If the required texts for either cover are too voluminous to fit legibly, you"
23,should put the first ones listed (as many as fit reasonably) on the actual
23,"cover, and continue the rest onto adjacent pages."
23,If you publish or distribute Opaque copies of the Document numbering more
23,"than 100, you must either include a machine-readable Transparent copy along"
23,"with each Opaque copy, or state in or with each Opaque copy a"
23,computer-network location from which the general network-using public has
23,access to download using public-standard network protocols a complete
23,"Transparent copy of the Document, free of added material. If you use the"
23,"latter option, you must take reasonably prudent steps, when you begin"
23,"distribution of Opaque copies in quantity, to ensure that this Transparent"
23,copy will remain thus accessible at the stated location until at least one
23,year after the last time you distribute an Opaque copy (directly or through
23,your agents or retailers) of that edition to the public.
23,"It is requested, but not required, that you contact the authors of the"
23,"Document well before redistributing any large number of copies, to give them"
23,a chance to provide you with an updated version of the Document.
23,4. MODIFICATIONS
23,You may copy and distribute a Modified Version of the Document under the
23,"conditions of sections 2 and 3 above, provided that you release the Modified"
23,"Version under precisely this License, with the Modified Version filling the"
23,"role of the Document, thus licensing distribution and modification of the"
23,"Modified Version to whoever possesses a copy of it. In addition, you must do"
23,these things in the Modified Version:
23,"Use in the Title Page (and on the covers, if any) a title distinct from"
23,"that of the Document, and from those of previous versions (which should, if"
23,"there were any, be listed in the History section of the Document). You may"
23,use the same title as a previous version if the original publisher of that
23,version gives permission.
23,"List on the Title Page, as authors, one or more persons or entities"
23,"responsible for authorship of the modifications in the Modified Version,"
23,together with at least five of the principal authors of the Document (all
23,"of its principal authors, if it has fewer than five), unless they release"
23,you from this requirement.
23,"State on the Title page the name of the publisher of the Modified Version,"
23,as the publisher.
23,Preserve all the copyright notices of the Document.
23,Add an appropriate copyright notice for your modifications adjacent to the
23,other copyright notices.
23,"Include, immediately after the copyright notices, a license notice giving"
23,the public permission to use the Modified Version under the terms of this
23,"License, in the form shown in the Addendum below."
23,Preserve in that license notice the full lists of Invariant Sections and
23,required Cover Texts given in the Document's license notice.
23,Include an unaltered copy of this License.
23,"Preserve the section Entitled ""History"", Preserve its Title, and add to it"
23,"an item stating at least the title, year, new authors, and publisher of the"
23,Modified Version as given on the Title Page. If there is no section
23,"Entitled ""History"" in the Document, create one stating the title, year,"
23,"authors, and publisher of the Document as given on its Title Page, then add"
23,an item describing the Modified Version as stated in the previous sentence.
23,"Preserve the network location, if any, given in the Document for public"
23,"access to a Transparent copy of the Document, and likewise the network"
23,locations given in the Document for previous versions it was based on.
23,"These may be placed in the ""History"" section. You may omit a network"
23,location for a work that was published at least four years before the
23,"Document itself, or if the original publisher of the version it refers to"
23,gives permission.
23,"For any section Entitled ""Acknowledgements"" or ""Dedications"", Preserve the"
23,"Title of the section, and preserve in the section all the substance and"
23,tone of each of the contributor acknowledgements and/or dedications given
23,therein.
23,"Preserve all the Invariant Sections of the Document, unaltered in their"
23,text and in their titles. Section numbers or the equivalent are not
23,considered part of the section titles.
23,"Delete any section Entitled ""Endorsements"". Such a section may not be"
23,included in the Modified Version.
23,"Do not retitle any existing section to be Entitled ""Endorsements"" or to"
23,conflict in title with any Invariant Section.
23,Preserve any Warranty Disclaimers.
23,If the Modified Version includes new front-matter sections or appendices that
23,qualify as Secondary Sections and contain no material copied from the
23,"Document, you may at your option designate some or all of these sections as"
23,"invariant. To do this, add their titles to the list of Invariant Sections in"
23,the Modified Version's license notice. These titles must be distinct from any
23,other section titles.
23,"You may add a section Entitled ""Endorsements"", provided it contains nothing"
23,"but endorsements of your Modified Version by various parties--for example,"
23,statements of peer review or that the text has been approved by an
23,organization as the authoritative definition of a standard.
23,"You may add a passage of up to five words as a Front-Cover Text, and a"
23,"passage of up to 25 words as a Back-Cover Text, to the end of the list of"
23,Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
23,one of Back-Cover Text may be added by (or through arrangements made by) any
23,"one entity. If the Document already includes a cover text for the same cover,"
23,previously added by you or by arrangement made by the same entity you are
23,"acting on behalf of, you may not add another; but you may replace the old"
23,"one, on explicit permission from the previous publisher that added the old"
23,one.
23,The author(s) and publisher(s) of the Document do not by this License give
23,permission to use their names for publicity for or to assert or imply
23,endorsement of any Modified Version.
23,5. COMBINING DOCUMENTS
23,You may combine the Document with other documents released under this
23,"License, under the terms defined in section 4 above for modified versions,"
23,provided that you include in the combination all of the Invariant Sections of
23,"all of the original documents, unmodified, and list them all as Invariant"
23,"Sections of your combined work in its license notice, and that you preserve"
23,all their Warranty Disclaimers.
23,"The combined work need only contain one copy of this License, and multiple"
23,identical Invariant Sections may be replaced with a single copy. If there are
23,"multiple Invariant Sections with the same name but different contents, make"
23,"the title of each such section unique by adding at the end of it, in"
23,"parentheses, the name of the original author or publisher of that section if"
23,"known, or else a unique number. Make the same adjustment to the section"
23,titles in the list of Invariant Sections in the license notice of the
23,combined work.
23,"In the combination, you must combine any sections Entitled ""History"" in the"
23,"various original documents, forming one section Entitled ""History""; likewise"
23,"combine any sections Entitled ""Acknowledgements"", and any sections Entitled"
23,"""Dedications"". You must delete all sections Entitled ""Endorsements""."
23,6. COLLECTIONS OF DOCUMENTS
23,You may make a collection consisting of the Document and other documents
23,"released under this License, and replace the individual copies of this"
23,License in the various documents with a single copy that is included in the
23,"collection, provided that you follow the rules of this License for verbatim"
23,copying of each of the documents in all other respects.
23,"You may extract a single document from such a collection, and distribute it"
23,"individually under this License, provided you insert a copy of this License"
23,"into the extracted document, and follow this License in all other respects"
23,regarding verbatim copying of that document.
23,7. AGGREGATION WITH INDEPENDENT WORKS
23,A compilation of the Document or its derivatives with other separate and
23,"independent documents or works, in or on a volume of a storage or"
23,"distribution medium, is called an ""aggregate"" if the copyright resulting from"
23,the compilation is not used to limit the legal rights of the compilation's
23,users beyond what the individual works permit. When the Document is included
23,"in an aggregate, this License does not apply to the other works in the"
23,aggregate which are not themselves derivative works of the Document.
23,If the Cover Text requirement of section 3 is applicable to these copies of
23,"the Document, then if the Document is less than one half of the entire"
23,"aggregate, the Document's Cover Texts may be placed on covers that bracket"
23,"the Document within the aggregate, or the electronic equivalent of covers if"
23,the Document is in electronic form. Otherwise they must appear on printed
23,covers that bracket the whole aggregate.
23,8. TRANSLATION
23,"Translation is considered a kind of modification, so you may distribute"
23,translations of the Document under the terms of section 4. Replacing
23,Invariant Sections with translations requires special permission from their
23,"copyright holders, but you may include translations of some or all Invariant"
23,Sections in addition to the original versions of these Invariant Sections.
23,"You may include a translation of this License, and all the license notices in"
23,"the Document, and any Warranty Disclaimers, provided that you also include"
23,the original English version of this License and the original versions of
23,those notices and disclaimers. In case of a disagreement between the
23,translation and the original version of this License or a notice or
23,"disclaimer, the original version will prevail."
23,"If a section in the Document is Entitled ""Acknowledgements"", ""Dedications"","
23,"or ""History"", the requirement (section 4) to Preserve its Title (section 1)"
23,will typically require changing the actual title.
23,9. TERMINATION
23,"You may not copy, modify, sublicense, or distribute the Document except as"
23,"expressly provided for under this License. Any other attempt to copy, modify,"
23,"sublicense or distribute the Document is void, and will automatically"
23,"terminate your rights under this License. However, parties who have received"
23,"copies, or rights, from you under this License will not have their licenses"
23,terminated so long as such parties remain in full compliance.
23,10. FUTURE REVISIONS OF THIS LICENSE
23,"The Free Software Foundation may publish new, revised versions of the GNU"
23,Free Documentation License from time to time. Such new versions will be
23,"similar in spirit to the present version, but may differ in detail to address"
23,new problems or concerns. See
23,https://www.gnu.org/copyleft/.
23,Each version of the License is given a distinguishing version number. If the
23,"Document specifies that a particular numbered version of this License ""or any"
23,"later version"" applies to it, you have the option of following the terms and"
23,conditions either of that specified version or of any later version that has
23,been published (not as a draft) by the Free Software Foundation. If the
23,"Document does not specify a version number of this License, you may choose"
23,any version ever published (not as a draft) by the Free Software Foundation.
23,ADDENDUM: How to use this License for your documents
23,#Copyright (c) YEAR YOUR NAME.
23,"Permission is granted to copy, distribute and/or modify this document"
23,"under the terms of the GNU Free Documentation License, Version 1.2"
23,or any later version published by the Free Software Foundation;
23,"with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts."
23,A copy of the license is included in the section entitled “GNU
23,Free Documentation License”.
23,"If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,"
23,replace the “with...Texts.” line with this:
23,"with the Invariant Sections being LIST THEIR TITLES, with the"
23,"Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST."
23,"If you have Invariant Sections without Cover Texts, or some other combination"
23,"of the three, merge those two alternatives to suit the situation."
23,"If your document contains nontrivial examples of program code, we recommend"
23,releasing these examples in parallel under your choice of free software
23,"license, such as the GNU General Public License, to permit their use in free"
23,software.
23,Share this page
23,© SUSE
23,2024
24,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML) | Managing site performance and scalability | Drupal Wiki guide on Drupal.org"
24,Skip to main content
24,Skip to search
24,"Can we use first and third party cookies and web beacons to understand our audience, and to tailor promotions you see?Yes, pleaseNo, do not track me"
24,Drupal.org home
24,Why Drupal?About Drupal
24,Platform overview
24,Drupal 10
24,Content Authoring
24,Content as a Service
24,Decoupled
24,Accessibility
24,Marketing Automation
24,Multilingual
24,Security
24,Personalization
24,Case studies
24,Video series
24,News
24,Use casesFor Developers
24,For Marketers
24,E-commerce
24,Education
24,FinTech
24,Government
24,Healthcare
24,High Tech
24,Nonprofit
24,Retail
24,Travel
24,ResourcesInstalling Drupal
24,Documentation
24,User guide
24,Local Development Guide
24,Security
24,News
24,Blog
24,Drupal 7 Migrations
24,ServicesFind an Agency Partner
24,Find a Migration Partner
24,Integrations & Hosting
24,Training
24,Become a Certified Partner
24,Partner Press
24,CommunityHow to Contribute
24,About the Community
24,Support
24,Community Governance
24,Jobs/Careers
24,EventsDrupalCon Portland 2024
24,DrupalCon Barcelona 2024
24,Community Events
24,DownloadDownload
24,Modules
24,Themes
24,Distributions
24,Issue queues
24,Browse Repository
24,GiveDrupal Association
24,Become a Supporter
24,Become a Certified Partner
24,Become a Member
24,Make a Donation
24,Discover Drupal
24,Drupal Swag Shop
24,DemoDemo online
24,Download
24,Return to content
24,Search form
24,Search
24,Log in
24,Create account
24,Documentation
24,Search
24,Drupal WikiDrupal 7Managing site performance and scalability
24,Support for Drupal 7 is ending on 5 January 2025—it’s time to migrate to Drupal 10! Learn about the many benefits of Drupal 10 and find migration tools in our resource center.
24,Learn more
24,Advertising sustains the DA. Ads are hidden for members. Join today
24,On this page
24,Basic settings
24,Theme optimization
24,Coding standard and proper use of already existing core API
24,Secure codes
24,DB Query optimization in codes
24,DB table optimization
24,Disable unnecessary modules
24,Remove unnecessary contents and others
24,Cache modules
24,Make changes according to Google Pagespeed and yahoo YSlow suggestions
24,MySQL Settings
24,Apache settings
24,"Also, we can check these options :"
24,1) Turn Page Caching On
24,2) Turn Views caching on
24,Managing site performance and scalability
24,Planning for Performance
24,Caching to improve performance
24,Changing PHP memory limits
24,Content Delivery Network [CDN]
24,Design for Low Bandwidth
24,Increase upload size in your php.ini
24,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
24,Optimizing MySQL
24,Randomizing MySQL Users For Exceeded max_questions Error
24,Server tuning considerations
24,Tuning php.ini for Drupal
24,"Optimizing Drupal to load faster (Server, MySQL, caching, theming, HTML)"
24,Last updated on
24,26 November 2023
24,"Drupal 7 will no longer be supported after January 5, 2025. Learn more and find resources for Drupal 7 sites"
24,"This documentation needs review. See ""Help improve this page"" in the sidebar."
24,Basic settings
24,Configure cron job (for Drupal 6 http://drupal.org/project/poormanscron)
24,Make sure all cache tables are clearing properly especially cache_form
24,Enable cache options on the performance page
24,"(For Drupal 6, http://drupal.org/project/advagg )"
24,Theme optimization
24,Manually Remove blankspaces and comments from .tpl
24,No indentation in .tpl
24,Turn on CSS and JS aggregation in the performance page
24,Manually reduce css file size by removing duplicate and combine similar together
24,Move codes to functions that should be in a custom common module. Use functions for similar problems instead of coding separately. Refer core API
24,Coding standard and proper use of already existing core API
24,http://drupal.org/coding-standards
24,https://drupalize.me/videos/understanding-drupal-coding-standards?p=2012
24,Secure codes
24,http://drupal.org/writing-secure-code
24,DB Query optimization in codes
24,Join db queries whenever possible
24,"For Db update and insert, use core API"
24,Use drupal standard http://drupal.org/coding-standards
24,DB table optimization
24,http://drupal.org/project/db_maintenance
24,Disable unnecessary modules
24,Devel
24,Statistics
24,Update status
24,Use syslog instead of Database logging
24,Remove unnecessary contents and others
24,Cache modules
24,"Make use of object caches to reduce database overhead, e.g. Memcache, Redis or APC"
24,https://drupal.org/project/authcache
24,Some module may help improve
24,http://drupal.org/project/ajaxblocks (not available with Drupal 8 & 9 )
24,Make changes according to Google Pagespeed and yahoo YSlow suggestions
24,MySQL Settings
24,Cache Size say 32MB in MySQL
24,Use https://github.com/initlabopen/mysqlconfigurer for fully automated MySQL performance tuning
24,Apache settings
24,DNS lookup : OFF
24,Set FollowSymLinks everywhere and never set SymLinksIfOwnerMatch
24,Avoid content negotiation. Or use type-map files rather than Options MultiViews directive
24,"KeepAlive on, and KeepAliveTimeout very low (1 or 2 sec)"
24,Disable or comment access.log settings
24,Enable mod_deflate or mod_gzip
24,Install APC server with higher memory limit apc.shm_size = 64
24,"Also, we can check these options :"
24,1) Turn Page Caching On
24,"What page caching does is that instead of using a bunch of database queries to get the data used in making a typical web page, the rendered contents of the web page are stored in a separate database cache table so that it can be recalled quicker. If you have 10 people visiting the site from different computers, Drupal first looks into the database cache table to see if the page is there, if it is, it just gives them the page. Think of saving the output of 50 separate queries so that is accessible with a single query. You obviously are reducing the SQL queries required by a lot. What the page cache table actually stores is HTML content."
24,"Page Caching is that it only works to optimize the page load time for Anonymous users. This is because when you are logged in, you might have blocks that show up on the page that are customized for you, if it served everybody on the same page, they would see your customized information (think of a My Recent Posts block), so Drupal does not use the Page Cache for Authenticated users automatically. This allows you to turn Page Caching on and still get the benefit of Anonymous user page load times but does not break the site for Authenticated users. There are other caching options that will help with Authenticated user page performance, we will talk about those later."
24,"To enable Page Caching, you go to Configuration | Development and select the checkbox next to ""Cache pages for anonymous users""."
24,2) Turn Views caching on
24,"As mentioned when talking about Page Caching only working for anonymous users above, there are other caching options for helping with Authenticated user page performance. One of those options is to turn on caching for blocks and pages that you create using the Views module. This allows you to cache the output of the query used to generate the view, or the end HTML output of your View, and you can tune the cache for them separately. And realize too that this means you can cache portions of a page if you are using one or several Views blocks in the page, it will just cache that block in the page, not the whole page."
24,See more (Drupal7) @ https://www.lullabot.com/articles/a-beginners-guide-to-caching-data-in-d...
24,Help improve this page
24,Page status:
24,Needs review
24,You can:
24,"Log in, click Edit, and edit this page"
24,"Log in, click Discuss, update the Page status value, and suggest an improvement"
24,Log in and create a Documentation issue with your suggestion
24,"Drupal’s online documentation is © 2000-2024 by the individual contributors and can be used in accordance with the Creative Commons License, Attribution-ShareAlike 2.0. PHP code is distributed under the GNU General Public License."
24,Thank you to these Drupal contributors
24,Top Drupal contributor Acquia would like to thank their partners for their contributions to Drupal.
24,Infrastructure management for Drupal.org provided by
24,News itemsNews
24,Planet Drupal
24,Social media
24,Sign up for Drupal news
24,Security advisories
24,Jobs
24,Our communityCommunity
24,"Services, Training & Hosting"
24,Contributor guide
24,Groups & meetups
24,DrupalCon
24,Code of conduct
24,DocumentationDocumentation
24,Drupal Guide
24,Drupal User Guide
24,Developer docs
24,API.Drupal.org
24,Drupal code baseDownload & Extend
24,Drupal core
24,Modules
24,Themes
24,Distributions
24,Governance of communityAbout
24,Web accessibility
24,Drupal Association
24,About Drupal.org
24,Terms of service
24,Privacy policy
24,Drupal is a registered trademark of Dries Buytaert.
25,Performance recommendations - MoodleDocs
25,Forums
25,Documentation
25,Downloads
25,Demo
25,Tracker
25,Development
25,Translation
25,Search
25,Search
25,Moodle Sites
25,What are you looking for?
25,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
25,Moodle.com
25,Our social network to share and curate open educational resources.
25,MoodleNet
25,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
25,Moodle Academy
25,Moodle.com
25,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
25,MoodleNet
25,Our social network to share and curate open educational resources.
25,Moodle Academy
25,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
25,Documentation
25,Menu
25,Main pageTable of contentsDocs overviewRecent changes
25,Log in
25,4.3 docs4.2 docs
25,4.1 docs
25,Article
25,Page Comments
25,View source
25,View history
25,Performance recommendations
25,"From MoodleDocsJump to:navigation, search"
25,Main page ► Managing a Moodle site ► Performance ► Performance recommendations
25,Performance
25,Performance recommendations
25,Performance settings
25,Performance overview
25,Caching
25,Performance FAQ
25,MUC FAQ
25,"Moodle can be made to perform very well, at small usage levels or scaling up to many thousands of users. The factors involved in performance are basically the same as for any PHP-based database-driven system. When trying to optimize your server, try to focus on the factor which will make the most difference to the user. For example, if you have relatively more users browsing than accessing the database, look to improve the webserver performance."
25,Contents
25,1 Obtain a baseline benchmark
25,2 Scalability
25,2.1 Server cluster
25,3 Hardware configuration
25,4 Operating System
25,5 Caching Performance
25,6 Web Server Performance
25,6.1 PHP Performance
25,6.1.1 APC
25,6.2 Apache Performance
25,6.3 IIS Performance
25,6.4 OpenLiteSpeed
25,"6.5 Lighttpd, NginX and Cherokee Performance"
25,6.6 X-Sendfile
25,7 Cron Performance
25,8 Database Performance
25,8.1 MariaDB Performance
25,8.2 MySQL Performance
25,8.3 PostgreSQL Performance
25,8.4 Read replicas
25,8.5 Other database performance links
25,9 Performance of different Moodle modules
25,10 See also
25,Obtain a baseline benchmark
25,"Before attempting any optimization, you should obtain a baseline benchmark of the component of the system you are trying to improve. For Linux try LBS (Note: Last updated May 2002) and for Windows use the Performance Monitor. Once you have quantitative data about how your system is performing currently, you'll be able to determine if the change you have made has had any real impact."
25,"The overall aim of adjustments to improve performance is to use RAM (cacheing) and to reduce disk-based activity. It is especially important to try to eliminate swap file usage as much as you can. If your system starts swapping, this is a sign that you need more RAM."
25,"The optimization order preference is usually: primary storage (more RAM), secondary storage (faster hard disks/improved hard disk configuration), processor (more and faster)."
25,It can be interesting to install and use the Benchmark plugin in order to find the bottlenecks of your system that specifically affect Moodle or do a load test / stress test with tool like JMeter. See moodledev JMeter documentation
25,Scalability
25,Moodle's design (with clear separation of application layers) allows for strongly scalable setups. (Please check the list of large Moodle installations.)
25,"Large sites usually separate the web server and database onto separate servers, although for smaller installations this is typically not necessary."
25,"It is possible to load-balance a Moodle installation, for example by using more than one webserver. The separate webservers should query the same database and refer to the same filestore and cache areas (see Caching), but otherwise the separation of the application layers is complete enough to make this kind of clustering feasible. Similarly, the database could be a cluster of servers (e.g. a MySQL cluster), but this is not an easy task and you should seek expert support, e.g. from a Moodle Partner."
25,"On very large, load-balanced, systems the performance of the shared components become critical. It's important that your shared file areas are properly tuned and that you use an effective cache (Redis is highly recommended). A good understanding of these areas of system administration should be considered a minimum requirement."
25,Server cluster
25,Using Moodle forum discussions:
25,Moodle clustering
25,Software load balancing
25,TCP load balancing
25,Installation for 3000 simultaneous users
25,Hardware configuration
25,Note: The fastest and most effective change that you can make to improve performance is to increase the amount of RAM on your web server - get as much as possible (e.g. 4GB or more). Increasing primary memory will reduce the need for processes to swap to disk and will enable your server to handle more users.
25,"Better performance is gained by obtaining the best processor capability you can, i.e. dual or dual core processors. A modern BIOS should allow you to enable hyperthreading, but check if this makes a difference to the overall performance of the processors by using a CPU benchmarking tool."
25,"If you can afford them, use SCSI hard disks instead of SATA drives. SATA drives will increase your system's CPU utilization, whereas SCSI drives have their own integrated processors and come into their own when you have multiple drives. If you must have SATA drives, check that your motherboard and the drives themselves support NCQ (Native Command Queuing)."
25,"Purchase hard disks with a low seek time. This will improve the overall speed of your system, especially when accessing Moodle's reports. Naturally these days Solid State Drives outperform rotating media immensely, especially Enterprise-Grade SSD's."
25,Size your swap file correctly. The general advice is to set it to 4 x physical RAM.
25,"Use a RAID disk system. Although there are many different RAID configurations you can create, the following generally works best:"
25,install a hardware RAID controller (if you can)
25,the operating system and swap drive on one set of disks configured as RAID-1.
25,"Moodle, Web server and Database server on another set of disks configured as RAID-5."
25,"If your 'moodleData' area is going to be on relatively slow storage (e.g. NFS mount on to a NAS device) you will have performance issues with the default cache configuration (which writes to this storage). See the page on Caching and choose an alternative. Redis is recommended. Using GlusterFS / OCFS2 / GFS2 on a SAN device and Fiber Channel could improve performance (See more info on the Moodle forum thread, NFS performance tuing )"
25,Use gigabit ethernet for improved latency and throughput. This is especially important when you have your webserver and database server separated out on different hosts.
25,Check the settings on your network card. You may get an improvement in performance by increasing the use of buffers and transmit/receive descriptors (balance this with processor and memory overheads) and off-loading TCP checksum calculation onto the card instead of the OS.
25,Read this Case Study on a server stress test with 300 users.
25,See this accompanying report on network traffic and server loads.
25,Also see this SFSU presentation at Educause (using VMWare): [1]
25,Operating System
25,"You can use Linux(recommended), Unix-based, Windows or Mac OS X for the server operating system. *nix operating systems generally require less memory than Mac OS X or Windows servers for doing the same task as the server is configured with just a shell interface. Additionally Linux does not have licensing fees attached, but can have a big learning curve if you're used to another operating system. If you have a large number of processors running SMP, you may also want to consider using a highly tuned OS such as Solaris."
25,Check your own OS and vendor specific instructions for optimization steps.
25,For Linux look at the Linux Performance Team site.
25,"For Linux investigate the hdparm command, e.g. hdparm -m16 -d1 can be used to enable read/write on multiple sectors and DMA. Mount disks with the ""async"" and ""noatime"" options."
25,"For Windows set the sever to be optimized for network applications (Control Panel, Network Connections, LAN connection, Properties, File & Printer Sharing for Microsoft Networks, Properties, Optimization). You can also search the Microsoft TechNet site for optimization documents."
25,Caching Performance
25,"Caching in Moodle can default to disk for a lot of the different caches which is rather slow overall, and so pretty solid gains can be made by moving this to RAM, by use of a Memory Caching Application such as Redis or Memcached. In fact I'd go as far to say the single biggest improvement we made to our (relatively small) Moodle site was installing Redis, and this is amplified when you're using classic Hard Drives rather than SSD's, and especially when they slowly but surely begin to fail (the classic slow to write, but no SMART errors or write errors - just reeeeaaallly slow)."
25,"These will also cache some database queries, meaning that they don't have to be re-run, again improving performance there. Personally, I would recommend Redis over Memcached due to better security features and being more up to date/developed. For more information/how to install Redis in particular, visit the Redis cache store page."
25,Web Server Performance
25,"Most web browsers these days feature Inspector elements which will allow you to watch the time it takes for each page component to load, typically found under the ""Network"" tab. Also, the Yslow extension will evaluate your page against Yahoo's 14 rules, full text Best Practices for Speeding Up Your Web Site, (video) for fast loading websites."
25,PHP Performance
25,"PHP contains a built-in accelerator (for more recent versions of PHP, this is OpCache). Make sure it is enabled."
25,Improvements in read/write performance can be improved by putting the cached PHP pages on a TMPFS filesystem - but remember that you'll lose the cache contents when there is a power failure or the server is rebooted.
25,Performance of PHP is better when installed as an Apache/IIS6 ISAPI module (rather than a CGI). IIS 7.0/7.5 (Windows Server 2008/R2) users should choose a FastCGI installation for best performance.
25,"Also check the memory_limit in php.ini. The default value for the memory_limit directive is 128M. On some sites, it may need to be larger - especially for some backup operations."
25,Also see PHP settings by Moodle version
25,Use PHP-FPM (with apache).
25,APC
25,APC on CentOS 5.x (linux)
25,APC on Debian (linux)
25,Apache Performance
25,"If you are using Apache on a Windows server, use the build from Apache Lounge which is reported to have performance and stability improvements compared to the official Apache download. Note that this is an unofficial build, so may not keep up with official releases."
25,Set the MaxRequestWorkers directive correctly (MaxClients before Apache 2.4). Use this formula to help (which uses 80% of available memory to leave room for spare):
25,MaxRequestWorkers = Total available memory * 80% / Max memory usage of apache process
25,"Memory usage of apache process is usually 10MB but Moodle can easily use up to 100MB per process, so a general rule of thumb is to divide your available memory in megabytes by 100 to get a conservative setting for MaxClients. You are quite likely to find yourself lowering the MaxRequestWorkers from its default of 150 on a Moodle server. To get a more accurate estimate read the value from the shell command:"
25,#ps -ylC httpd --sort:rss
25,"If you need to increase the value of MaxRequestWorkers beyond 256, you will also need to set the ServerLimit directive."
25,Warning: Do not be tempted to set the value of MaxRequestWorkers higher than your available memory as your server will consume more RAM than available and start to swap to disk.
25,Consider reducing the number of modules that Apache loads in the httpd.conf file to the minimum necessary to reduce the memory needed.
25,Use the latest version of Apache - Apache 2 has an improved memory model which reduces memory usage further.
25,"For Unix/Linux systems, consider lowering MaxConnectionsPerChild (MaxRequestsPerChild before Apache 2.4) in httpd.conf to as low as 20-30 (if you set it any lower the overhead of forking begins to outweigh the benefits)."
25,"For a heavily loaded server, consider setting KeepAlive Off (do this only if your Moodle pages do not contain links to resources or uploaded images) or lowering the KeepAliveTimeout to between 2 and 5. The default is 15 (seconds) - the higher the value the more server processes will be kept waiting for possibly idle connections. A more accurate value for KeepAliveTimeout is obtained by observing how long it takes your users to download a page. After altering any of the KeepAlive variables, monitor your CPU utilization as there may be an additional overhead in initiating more worker processes/threads."
25,"As an alternative to using KeepAlive Off, consider setting-up a Reverse Proxy server in front of the Moodle server to cache HTML files with images. You can then return Apache to using keep-alives on the Moodle server."
25,"If you do not use a .htaccess file, set the AllowOverride variable to AllowOverride None to prevent .htaccess lookups."
25,Set DirectoryIndex correctly so as to avoid content-negotiation. Here's an example from a production server:
25,DirectoryIndex index.php index.html index.htm
25,"Unless you are doing development work on the server, set ExtendedStatus Off and disable mod_info as well as mod_status."
25,Leave HostnameLookups Off (as default) to reduce DNS latency.
25,Consider reducing the value of TimeOut to between 30 and 60 (seconds).
25,"For the Options directive, avoid Options Multiviews as this performs a directory scan. To reduce disk I/O further use"
25,Options -Indexes FollowSymLinks
25,Compression reduces response times by reducing the size of the HTTP response
25,Install and enable mod_deflate - refer to documentation or man pages
25,Add this code to the virtual server config file within the <directory> section for the root directory (or within the .htaccess file if AllowOverrides is On):
25,<ifModule mod_deflate.c>
25,AddOutputFilterByType DEFLATE text/html text/plain text/xml text/x-js text/javascript text/css application/javascript
25,</ifmodule>
25,Use Apache event MPM (and not the default Prefork or Worker)
25,IIS Performance
25,All alter this location in the registry:
25,HKLM\SYSTEM\CurrentControlSet\Services\Inetinfo\Parameters\
25,The equivalent to KeepAliveTimeout is ListenBackLog (IIS - registry location is HKLM\ SYSTEM\ CurrentControlSet\ Services\ Inetinfo\ Parameters). Set this to between 2 and 5.
25,Change the MemCacheSize value to adjust the amount of memory (Mb) that IIS will use for its file cache (50% of available memory by default).
25,"Change the MaxCachedFileSize to adjust the maximum size of a file cached in the file cache in bytes. Default is 262,144 (256K)."
25,"Create a new DWORD called ObjectCacheTTL to change the length of time (in milliseconds) that objects in the cache are held in memory. Default is 30,000 milliseconds (30 seconds)."
25,OpenLiteSpeed
25,"OpenLiteSpeed has its own built in cache called LSCache, which is controlled through the Web GUI, and also is compatible with PHP OpCache. More info on optimizing OpenLiteSpeed can be found on the OpenLiteSpeed page."
25,"Lighttpd, NginX and Cherokee Performance"
25,"You can increase server performance by using a light-weight webserver like lighttpd, nginx or cherokee in combination with PHP in FastCGI-mode. Lighttpd was originally created as a proof-of-concept [2] to address the C10k problem and while primarily recommended for memory-limited servers, its design origins and asynchronous-IO model make it a suitable and proven [3] alternative HTTP server for high-load websites and web apps, including Moodle. See the MoodleDocs Lighttpd page for additional information, configuration example and links."
25,"Alternatively, both lighttpd and nginx are capable of performing as a load-balancer and/or reverse-proxy to alleviate load on back-end servers [4], providing benefit without requiring an actual software change on existing servers."
25,Do note that these are likely to be the least tested server environments of all particularly if you are using advanced features such as web services and/or Moodle Networking. They are probably best considered for heavily used Moodle sites with relatively simple configurations.
25,X-Sendfile
25,X-Sendfile modules improve performance when sending large files from Moodle. It is recommended to configure your web server and Moodle to use this feature if available.
25,Configure web server:
25,Apache - https://tn123.org/mod_xsendfile/
25,Lighttpd - http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file
25,Nginx - http://wiki.nginx.org/XSendfile
25,OpenLiteSpeed - https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:config:internal-redirect
25,Enable support in config.php (see config-dist.php):
25,$CFG->xsendfile = 'X-Sendfile';
25,// Apache {@see https://tn123.org/mod_xsendfile/}
25,$CFG->xsendfile = 'X-LIGHTTPD-send-file'; // Lighttpd {@see http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file}
25,$CFG->xsendfile = 'X-Accel-Redirect';
25,// Nginx {@see http://wiki.nginx.org/XSendfile}
25,Configure file location prefixes if your server implementation requires it:
25,$CFG->xsendfilealiases = array(
25,"'/dataroot/' => $CFG->dataroot,"
25,"'/cachedir/' => '/var/www/moodle/cache',"
25,// for custom $CFG->cachedir locations
25,"'/localcachedir/' => '/var/local/cache',"
25,// for custom $CFG->localcachedir locations
25,'/tempdir/'
25,"=> '/var/www/moodle/temp',"
25,// for custom $CFG->tempdir locations
25,'/filedir'
25,"=> '/var/www/moodle/filedir',"
25,// for custom $CFG->filedir locations
25,Cron Performance
25,"Cron is a very important part of the overall performance of Moodle as many asynchronous processes are offloaded to Cron, so it needs to be running and have enough through put to handle the work being given to it by the front ends."
25,See Cron with Unix or Linux#High performance cron tasks
25,Database Performance
25,MariaDB Performance
25,"MariaDB Optimizations are similar to MySQL, but at the same time different due to the way MariaDB operates. Performance as a whole is typically better than MySQL using MariaDB, so if you're looking for Database Optimization, potentially switching from MySQL to MariaDB may help with performance, otherwise if you're already using MariaDB and looking to Optimize it, Performance Recommendations can be found on the MariaDB Page."
25,MySQL Performance
25,"The number one thing you can do to improve MySQL performance is to read, understand and implement the recommendations in the Innodb Buffer Pool article."
25,"The buffer pool size can safely be changed while your server is running, as long as your server has enough memory (RAM) to accommodate the value you set. On a machine that is dedicated to MySQL, you can safely set this value to 80% of available memory."
25,"Consider setting innodb_buffer_pool_instances to the number of cores, vCPUs, or chips you have available. Adjust this value in accordance with the recommendations in the MySQL documentation."
25,The following are MySQL specific settings which can be adjusted for better performance in your my.cnf (my.ini in Windows). The file contains a list of settings and their values. To see the current values use these commands
25,SHOW STATUS;
25,SHOW VARIABLES;
25,"Important: You must make backups of your database before attempting to change any MySQL server configuration. After any change to the my.cnf, restart mysqld."
25,"If you are able, the MySQLTuner tool can be run against your MySQL server and will calculate appropriate configuration values for most of the following settings based on your current load, status and variables automatically."
25,Enable the query cache with
25,query_cache_type = 1.
25,"For most Moodle installs, set the following:"
25,query_cache_size = 36M
25,query_cache_min_res_unit = 2K.
25,The query cache will improve performance if you are doing few updates on the database.
25,Set the table cache correctly. For Moodle 1.6 set
25,table_cache = 256 #(table_open_cache in MySQL > 5.1.2)
25,"(min), and for Moodle 1.7 set"
25,table_cache = 512 #(table_open_cache in MySQL > 5.1.2)
25,"(min). The table cache is used by all threads (connections), so monitor the value of opened_tables to further adjust - if opened_tables > 3 * table_cache(table_open_cache in MySQL > 5.1.2) then increase table_cache up to your OS limit. Note also that the figure for table_cache will also change depending on the number of modules and plugins you have installed. Find the number for your server by executing the mysql statement below. Look at the number returned and set table_cache to this value."
25,mysql>SELECT COUNT(table_name) FROM information_schema.tables WHERE table_schema='yourmoodledbname';
25,Set the thread cache correctly. Adjust the value so that your thread cache utilization is as close to 100% as possible by this formula:
25,thread cache utilization (%) = (threads_created / connections) * 100
25,"The key buffer can improve the access speed to Moodle's SELECT queries. The correct size depends on the size of the index files (.myi) and in Moodle 1.6 or later (without any additional modules and plugins), the recommendation for this value is key_buffer_size = 32M. Ideally you want the database to be reading once from the disk for every 100 requests so monitor that the value is suitable for your install by adjusting the value of key_buffer_size so that the following formulas are true:"
25,key_read / key_read_requests < 0.01
25,key_write / key_write_requests <= 1.0
25,"Set the maximum number of connections so that your users will not see a ""Too many connections"" message. Be careful that this may have an impact on the total memory used. MySQL connections usually last for milliseconds, so it is unusual even for a heavily loaded server for this value to be over 200."
25,Manage high burst activity. If your Moodle install uses a lot of quizzes and you are experiencing performance problems (check by monitoring the value of threads_connected - it should not be rising) consider increasing the value of back_log.
25,"Optimize your tables weekly and after upgrading Moodle. It is good practice to also optimize your tables after performing a large data deletion exercise, e.g. at the end of your semester or academic year. This will ensure that index files are up to date. Backup your database first and then use:"
25,mysql>CHECK TABLE mdl_tablename;
25,mysql>OPTIMIZE TABLE mdl_tablename;
25,"The common tables in Moodle to check are mdl_course_sections, mdl_forum_posts, mdl_log and mdl_sessions (if using dbsessions). Any errors need to be corrected using REPAIR TABLE (see the MySQL manual and this forum script)."
25,Maintain the key distribution. Every month or so it is a good idea to stop the mysql server and run these myisamchk commands.
25,#myisamchk -a -S /pathtomysql/data/moodledir/*.MYI
25,"Warning: You must stop the mysql database process (mysqld) before running any myisamchk command. If you do not, you risk data loss."
25,Reduce the number of temporary tables saved to disk. Check this with the created_tmp_disk_tables value. If this is relatively large (>5%) increase tmp_table_size until you see a reduction. Note that this will have an impact on RAM usage.
25,PostgreSQL Performance
25,"There are some good papers around on tuning PostgreSQL (like this one), and Moodle's case does not seem to be different to the general case."
25,The first thing to recognise is that if you really need to worry about tuning you should be using a separate machine for the database server. If you are not using a separate machine then the answers to many performance questions are substantially muddied by the memory requirements of the rest of the application.
25,"You should probably enable autovacuum, unless you know what you are doing. Many e-learning sites have predictable periods of low use, so disabling autovacuum and running a specific vacuum at those times can be a good option. Or perhaps leave autovacuum running but do a full vacuum weekly in a quiet period."
25,"Set shared_buffers to something reasonable. For versions up to 8.1 my testing has shown that peak performance is almost always obtained with buffers < 10000, so if you are using such a version, and have more than 512M of RAM just set shared_buffers to 10,000 (8MB)."
25,"The buffer management had a big overhaul in 8.2 and ""reasonable"" is now a much larger number. I have not conducted performance tests with 8.2, but the recommendations from others are generally that you should now scale shared_buffers much more with memory and may continue to reap benefits even up to values like 100,000 (80MB). Consider using 1-2% of system RAM."
25,"PostgreSQL will also assume that the operating system is caching its files, so setting effective_cache_size to a reasonable value is also a good idea. A reasonable value will usually be (total RAM - RAM in use by programs). If you are running Linux and leave the system running for a day or two you can look at 'free' and under the 'cached' column you will see what it currently is. Consider taking that number (which is kB) and dividing it by 10 (i.e. allow 20% for other programs cache needs and then divide by 8 to get pages). If you are not using a dedicated database server you will need to decrease that value to account for usage by other programs."
25,"Some other useful parameters that can have positive effects, and the values I would typically set them to on a machine with 4G RAM, are:"
25,work_mem = 10240
25,"That's 10M of RAM to use instead of on-disk sorting and so forth. That can give a big speed increase, but it is per connection and 200 connections * 10M is 2G, so it can theoretically chew up a lot of RAM."
25,maintenance_work_mem = 163840
25,"That's 160M of RAM which will be used by (e.g.) VACUUM, index rebuild, cluster and so forth. This should only be used periodically and should be freed when those processes exit, so I believe it is well worth while."
25,wal_buffers = 64
25,"These buffers are used for the write-ahead log, and there have been a number of reports on the PostgreSQL mailing lists of improvement from this level of increase."
25,This is a little out of date now (version 8.0) but still worth a read: http://www.powerpostgresql.com/Docs
25,And there is lots of good stuff here as well: http://www.varlena.com/GeneralBits/Tidbits/index.php
25,Based on Andrew McMillan's post at Tuning PostgreSQL forum thread.
25,Splitting mdl_log to several tables and using a VIEW with UNION to read them as one. (See Tim Hunt explanation on the Moodle forums)
25,Read replicas
25,Since Moodle 3.9 you can configure read replica's to be used where possible. For very large systems as much as 80-90% of the DB load can be moved away from the primary. For configuration see config-dist:
25,https://github.com/moodle/moodle/blob/master/config-dist.php#L84-L117
25,Other database performance links
25,Consider using a distributed caching system like memcached but note that memcached does not have any security features so it should be used behind a firewall.
25,Consider using PostgreSQL. See how to migrate from MySQL to PostgreSQL (forum discussion).
25,General advice on tuning MySQL parameters (advice from the MySQL manual)
25,InnoDB performance optimization taken from the MySQL performance blog site.
25,Performance of different Moodle modules
25,"Moodle's activity modules, filters, and other plugins can be activated/deactivated. If necessary, you may wish to deactivate some features (such as chat) if not required - but this isn't necessary. Some notes on the performance of certain modules:"
25,"The Chat module is said to be a hog in terms of frequent HTTP requests to the main server. This can be reduced by setting the module to use Streamed updates, or, if you're using a Unix-based webserver, by running the chat in daemon mode. When using the Chat module use the configuration settings to tune for your expected load. Pay particular attention to the chat_old_ping and chat_refresh parameters as these can have greatest impact on server load."
25,The Moodle Cron task is triggered by calling the script cron.php. If this is called over HTTP (e.g. using wget or curl) it can take a large amount of memory on large installations. If it is called by directly invoking the php command (e.g. php -f /path/to/moodle/directory/admin/cli/cron.php) efficiency can be much improved.
25,The Recent activities block is consuming too many resources if you have huge number of records mdl_log. This is being tested to optimize the SQL query.
25,"The Quiz module is known to stretch database performance. However, it has been getting better in recent versions, and we don't know of any good, up-to-date performance measurements. (Here is a case study from 2007 with 300 quiz users.). The following suggestions were described by Al Rachels in this forum thread:"
25,"make sure both Moodle, and the operating system, are installed on a solid state drive"
25,upgrade to and use PHP 7
25,run MySQLTuner and implement its recommendations
25,See Performance settings for more information on performance-related Moodle settings.
25,See also
25,Using Moodle: Hardware and Performance forum
25,Why Your Moodle Site is Slow: Five Simple Settings blog post from Jonathan Moore
25,I teach with Moodle performance testing: http://www.iteachwithmoodle.com/2012/11/17/moodle-2-4-beta-performance-test-comparison-with-moodle-2-3/
25,Moodle 2.4.5 vs 2.5.2 performance and MUC APC cahe store
25,Moodle performance testing 2.4.6 vs 2.5.2 vs 2.6dev
25,Moodle performance analysis revisited (now with MariaDB)
25,"Tim Hunt's blog (May 2, 2013) on performance testing Moodle"
25,"New Relic, Application Performance Monitoring"
25,Performance enhancements for Apache and PHP (Apache Event MPM and PHP-FPM)
25,Performance recommendations
25,Moodle performance investigation – using performance info
25,Moodle Caching at Scale
25,"There have been a lot of discussions on moodle.org about performance, here are some of the more interesting and (potentially) useful ones:"
25,Performance woes!
25,Performance perspectives - a little script
25,Comments on planned server hardware
25,Moodle performance in a pil by Martin Langhoff
25,Advice on optimising php/db code in moodle2+
25,Moodle 2.5 performance testing at the OU
25,100 active users limit with 4vCPU
25,Performance Tip ... shared...
25,"Retrieved from ""https://docs.moodle.org/403/en/index.php?title=Performance_recommendations&oldid=147626"""
25,Category: Performance
25,Tools
25,What links here
25,Related changes
25,Special pages
25,Printable version
25,Permanent link
25,Page information
25,In other languages
25,Español
25,Français
25,日本語
25,Deutsch
25,"This page was last edited on 11 January 2024, at 15:57."
25,Content is available under GNU General Public License unless otherwise noted.
25,Privacy
25,About Moodle Docs
25,Disclaimers
26,Introducing Amazon RDS for MariaDB 10.11 for up to 40% higher transaction throughput | AWS Database Blog
26,Skip to Main Content
26,Click here to return to Amazon Web Services homepage
26,Contact Us
26,Support
26,English
26,My Account
26,Sign In
26,Create an AWS Account
26,Products
26,Solutions
26,Pricing
26,Documentation
26,Learn
26,Partner Network
26,AWS Marketplace
26,Customer Enablement
26,Events
26,Explore More
26,Close
26,عربي
26,Bahasa Indonesia
26,Deutsch
26,English
26,Español
26,Français
26,Italiano
26,Português
26,Tiếng Việt
26,Türkçe
26,Ρусский
26,ไทย
26,日本語
26,한국어
26,中文 (简体)
26,中文 (繁體)
26,Close
26,My Profile
26,Sign out of AWS Builder ID
26,AWS Management Console
26,Account Settings
26,Billing & Cost Management
26,Security Credentials
26,AWS Personal Health Dashboard
26,Close
26,Support Center
26,Expert Help
26,Knowledge Center
26,AWS Support Overview
26,AWS re:Post
26,Click here to return to Amazon Web Services homepage
26,Get Started for Free
26,Contact Us
26,Products
26,Solutions
26,Pricing
26,Introduction to AWS
26,Getting Started
26,Documentation
26,Training and Certification
26,Developer Center
26,Customer Success
26,Partner Network
26,AWS Marketplace
26,Support
26,AWS re:Post
26,Log into Console
26,Download the Mobile App
26,AWS Blog Home
26,Blogs
26,Editions
26,Close
26,Architecture
26,AWS Cloud Operations & Migrations
26,AWS for Games
26,AWS Insights
26,AWS Marketplace
26,AWS News
26,AWS Partner Network
26,AWS Smart Business
26,Big Data
26,Business Intelligence
26,Business Productivity
26,Cloud Enterprise Strategy
26,Cloud Financial Management
26,Compute
26,Contact Center
26,Containers
26,Database
26,Desktop & Application Streaming
26,Developer Tools
26,DevOps
26,Front-End Web & Mobile
26,HPC
26,Industries
26,Integration & Automation
26,Internet of Things
26,Machine Learning
26,Media
26,Messaging & Targeting
26,Microsoft Workloads on AWS
26,.NET on AWS
26,Networking & Content Delivery
26,Open Source
26,Public Sector
26,Quantum Computing
26,Robotics
26,SAP
26,Security
26,Spatial Computing
26,Startups
26,Storage
26,Supply Chain & Logistics
26,Training & Certification
26,Close
26,中国版
26,日本版
26,한국 에디션
26,기술 블로그
26,Edisi Bahasa Indonesia
26,AWS Thai Blog
26,Édition Française
26,Deutsche Edition
26,Edição em Português
26,Edición en Español
26,Версия на русском
26,Türkçe Sürüm
26,AWS Database Blog
26,Introducing Amazon RDS for MariaDB 10.11 for up to 40% higher transaction throughput
26,Sai Kiran Kshatriya | on
26,21 AUG 2023 | in
26,"Advanced (300), Amazon RDS, Announcements, RDS for MariaDB |"
26,Permalink |
26,Comments |
26,Share
26,"MariaDB is a popular open-source high performance database. Amazon Relational Database Service (Amazon RDS) for MariaDB supports multiple major engine versions including 10.4, 10.5, 10.6. Today, Amazon RDS has announced support for MariaDB major version 10.11, which is the latest long-term supported major version from the MariaDB community."
26,"When compared to previous versions, Amazon RDS for MariaDB 10.11 delivers up to 40% higher transaction throughput. This means faster database operations and improved application responsiveness, especially during periods of peak usage."
26,"In this post, we benchmark Amazon RDS for MariaDB version 10.11 and compare it to our last release RDS for MariaDB 10.6. We also go over the latest features introduced by the community in MariaDB 10.11. Whether you are currently using an older version of MariaDB, or, considering migration from another database system, this post outlines the benefits and procedure of upgrading to MariaDB 10.11."
26,Performance of Amazon RDS for MariaDB 10.11
26,"We start by comparing Amazon RDS for MariaDB versions 10.11.4 and 10.6.14 to understand the performance improvement of major version 10.11 over previous versions. Our benchmarking tests were carried out using an r6g.16xlarge instance with 45,000 Provisioned IOPS and 1,500 GB EBS volume. We used sysbench to benchmark write and read operations on both 10.11 and 10.6. The dataset employed for testing consisted of two hundred and fifty tables, each containing approximately 1.3 million rows."
26,"First, we examined the write performance using the default parameters of an RDS for MariaDB instance. For the same workload, we varied the number of threads to understand the impact on write performance. The graphs below illustrate the differences in write performance observed across Amazon RDS for MariaDB 10.11.4 and Amazon RDS for MariaDB 10.6.14. We start observing the performance difference between 10.11.4 and 10.6.14 even when running a single thread. As we increase the number of threads, the performance differential continues to grow, peaking at 512 threads. At 512 threads, Amazon RDS for MariaDB 10.11 delivered a write throughput of 3,224.53 transactions per second (TPS), which is 47.25% higher than the MariaDB 10.6 TPS at 2,189.77."
26,We used the following commands for our sysbench write tests:
26,sysbench parallel_prepare --mysql-port=3306 --db-driver=mariadb --mysql-table --oltp-tables-count=250 --oltp-table-size=1290000 --num-threads=250 -–report-interval=60 –-warmup-time=60 –-db-ps-mode=disable -–reconnect=0 run
26,"sysbench oltp_write --table-size=1290000 --db-driver=mysql --tables=250 -—threads=<1,1024> --simple_ranges=0 –distinct_ranges=0 -–sum_ranges=0 -–order_ranges=0 -–time=1200 -–report-interval=60"
26,-–histogram=on --time=1200 –db-ps-mode=disable --thread-init-timeout=60 run
26,"Next, we tested a read workload using the default parameter configuration. We start observing a performance improvement on 10.11 when our tests scales to 2 threads. Similar to our write tests, as the number of threads increases, the performance gap between Amazon RDS for MariaDB 10.11.4 and 10.6.14 continues to increase, peaking at 512 thread count. At 512 threads, Amazon RDS for MariaDB 10.11 delivered a read throughput of 1255 transactions per second (TPS), which is 15.13% higher than the MariaDB 10.6 TPS at 1090.16."
26,We used the following commands for our sysbench read tests:
26,"sysbench oltp_read --table-size=1290000 --db-driver=mysql --tables=250 -—threads=<1,1024> --simple_ranges=0 -–distinct_ranges=0 -–sum_ranges=0 –-order_ranges=0 -–time=1200 –-report-interval=60 -–histogram=on --time=1200 -–db-ps-mode=disable --thread-init-timeout=60 runNote, your performance gains may vary depending on your workload and instance configuration."
26,"If you want to push the performance of your RDS for MariaDB 10.11 further, you could consider using Amazon RDS Optimized Reads and Optimized Writes. RDS Optimized Reads use NVMe-based SSD block storage, directly attached to the host server, to achieve up to 2X faster complex query processing. And, RDS Optimized Writes deliver up to 2X higher write transaction throughput. When you combine these features with Amazon RDS for MariaDB 10.11, the overall performance gains can be compounded depending on the nature of your workload."
26,"In the next section, we go over the new features introduced with MariaDB 10.11. If you would like to learn the best practices for upgrading your database instance to MariaDB 10.11, you can skip ahead to section “Upgrading to Amazon RDS for MariaDB 10.11”."
26,What’s new in Amazon RDS for MariaDB 10.11?
26,"The MariaDB community has introduced multiple new features with major version 10.11. You can find a complete list of new features in the RDS for MariaDB user guide. In this section, we highlight a select few."
26,1. Lag free ALTER TABLE replication
26,"Performing ALTER TABLE operations on a primary database often introduces replication lag, affecting the availability and consistency of replicas. MariaDB 10.11 eliminates the replication lag caused by ALTER TABLE. When you enable this feature by enabling the binlog_alter_two_phase parameter in the parameter group, the ALTER command is replicated and run simultaneously on replicas, which minimizes the replication lag and ensures a seamless and uninterrupted database experience, allowing you to perform schema changes without worrying about large replication delays."
26,"In addition to enabling the binlog_alter_two_phase parameter in the parameter group, it’s important to ensure that parallel replication is enabled on the replicas and increase the value of the slave_parallel_workers parameter greater than zero."
26,2. Authentication
26,"Amazon RDS for MariaDB 10.11 supports the GRANT TO …PUBLIC command which allows granting privileges to all users with access to the server, including those created after the privileges are granted. We recommend you use this feature cautiously as granting high level privileges to PUBLIC can lead to security issues in the database. If used cautiously, this approach proves particularly useful when you want to define a specific set of privileges for all users without having to repeat the individual grant statements for every user. This allows all users to have a uniform set of privileges, promoting consistency and standardization of privilege management. Administrators can easily review and modify the privileges assigned to all users at once. The GRANT TO … PUBLIC feature can help administrators streamline user privilege management."
26,"In the following example, we create a MariaDB user named `John` who is allowed to connect to our RDS instance."
26,mysql> CREATE USER 'John'@'%' IDENTIFIED BY 'xxxxx';
26,"Query OK, 0 rows affected (0.00 sec)"
26,"Now, we use the GRANT TO …PUBLIC command to grant SELECT privileges to all users with access to the server on the `Titles` table in the `Employees` database. We will later run the SHOW GRANTS command to confirm the privileges assigned TO PUBLIC grants."
26,mysql> GRANT SELECT ON employees.Titles TO PUBLIC;
26,mysql> SHOW GRANTS FOR public;
26,+------------------------------------------------+
26,| Grants for PUBLIC
26,+------------------------------------------------+
26,| GRANT SELECT ON `employees`.`Titles` TO PUBLIC |
26,+------------------------------------------------+
26,"Now, we login as the newly-created MySQL user, John, and query the table in the database."
26,mysql> select user();
26,+--------------------+
26,| user()
26,+--------------------+
26,| John@xxx.xx.xx.xxx |
26,+--------------------+
26,mysql> SHOW GRANTS for 'John'@'%';
26,+-----------------------------------------------------------------------------------------------------+
26,| Grants for John@%
26,+-----------------------------------------------------------------------------------------------------+
26,| GRANT USAGE ON *.* TO `John`@`%` IDENTIFIED BY PASSWORD '*2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19' |
26,+-----------------------------------------------------------------------------------------------------+
26,"From the previous command, we confirm that MariaDB user `John` does not have privileges to run the SELECT query on the `Titles` table in the `Employees` database. However, since we created a GRANT TO … PUBLIC SELECT privilege in our earlier step, SELECT privilege is granted to the MariaDB user and this is confirmed in this example:"
26,mysql> select * from `employees`.`Titles` limit 1;
26,+----+---------+-----------+---------+
26,| id | k
26,| c
26,| pad
26,+----+---------+-----------+---------+
26,1 | 4992833 | 838686419 | 6784796 |
26,+----+---------+-----------+---------+
26,3. New JSON functions
26,"MariaDB 10.11 provides several new functions for additional flexibility and functionality when working with JSON data, including JSON_EQUALS, JSON_NORMALIZE, JSON_OVERLAPS:"
26,JSON_EQUALS: You can use this function to compare the equality between JSON objects regardless of attribute order.
26,"In the following example, we use the JSON_EQUALS function to compare the equality between two JSON objects, '{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}' and '{""Sam"":[10567,34.0,1995],""John"":[10563,22,1988]}'."
26,"A value of 1 indicates that the JSON objects are equal, whereas a value of 0 indicates inequality."
26,"In our example, both JSON objects have the same structure, with the same keys and values (though some values are in different data types, for example 34 to 34.0). Since, our JSON objects have the same structure and members, regardless of member order, the function returns a value of 1, indicating equality."
26,"mysql> SELECT JSON_EQUALS('{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}', '{""Sam"":[10567,34.0,1995],""John"":[10563,22,1988]}') as JSON_EQUALS;"
26,+-------------+
26,| JSON_EQUALS |
26,+-------------+
26,1 |
26,+-------------+
26,"In the next example, we use the JSON_EQUALS function again to compare a different set of JSON objects. These JSON objects are not equal because they have different structures. For example, in the first object, the key “John” has an array with the value 1988, while in the second object, the key “John” has a different structure with only two values. Therefore, the function returns a value of 0, indicating inequality."
26,"mysql> SELECT JSON_EQUALS('{""John"":[10563,22,1988],""Sam"":[10567,34,1995]}', '{""Sam"":[10567,34.0,1995],""John"":[10563,22]}') as JSON_EQUALS;"
26,+-------------+
26,| JSON_EQUALS |
26,+-------------+
26,0 |
26,+-------------+
26,JSON_NORMALIZE: You can use the JSON_NORMALIZE function in combination with a unique key to enforce a unique constraint on the JSON contents in a database. This function converts a JSON object into a canonical form that can be used for comparison purposes for JSON data. This ensures that any future inserts or updates with a similar JSON value that is already present in the table will result in a constraint violation error.
26,Let’s understand this with an example. We create a table named ‘bikemodel’ with two columns: `bike_id`(an auto-incrementing primary key) and `model` (a column to store JSON data)
26,CREATE TABLE bikemodel (
26,"bike_id INT UNSIGNED NOT NULL AUTO_INCREMENT,"
26,"model JSON,"
26,PRIMARY KEY (bike_id)
26,"Here, an ALTER command is run on the `bikemodel` table. A new column named model_normalize is added to store the normalized JSON form of the model column using the JSON_NORMALIZE function. Additionally, a unique key is added on the model_normalize column to enforce the uniqueness constraint on the normalized JSON values."
26,"ALTER TABLE bikemodel ADD COLUMN model_normalize JSON AS (JSON_NORMALIZE(model)) VIRTUAL, ADD UNIQUE KEY (model_normalize);"
26,"We now insert a JSON object into the bikemodel table. The JSON object has attributes like name, mode, and color."
26,"INSERT INTO bikemodel (model) VALUES ('{""name"":""val"",""mode"":""Electric"",""color"":""Magenta""}');"
26,"Query OK, 1 row affected (0.00 sec)"
26,"Now, we attempt to insert a JSON object into the table with slightly different key order and formatting. However, due to the use of the JSON_NORMALIZE function and the unique key constraint on the `model_normalize` column, this insertion fails and results in a constraint violation error. This is because the normalized form of the JSON data is the same as the previously inserted JSON object."
26,"INSERT INTO bikemodel (model) VALUES ('{""mode"":""Electric"",""name"":""val""},""color"":""Magenta""');"
26,"ERROR 1062 (23000): Duplicate entry '{""color"":""Magenta"",""mode"":""Electric"",""name"":""val""}' for key 'model_normalize'"
26,"JSON_OVERLAPS: You can use this function to compare two JSON documents to see if they have any common key/value pairs between objects. It returns true if the two JSON objects have at least one value for the key in common. This also includes array elements between two arrays, array element with scalar and two scalar documents if they have the same type and value."
26,"In the following example, the first JSON object has the keys “John” and “Year” with their corresponding values, and the second JSON object has the keys “SAM” and “Year” with their corresponding values."
26,"mysql> SELECT JSON_OVERLAPS('{""John"": 10567, ""Year"":1988}', {""SAM"":10563, ""Year"":1988}') AS JSON_OVERLAP;"
26,+--------------+
26,| JSON_OVERLAP |
26,+--------------+
26,1 |
26,+--------------+
26,1 row in set (0.00 sec)
26,"The function returns true (1) because both JSON objects share the common key ""Year"" and their corresponding values are not both NULL. This indicates that there is an overlap between the two JSON objects."
26,"In the following example, the first JSON object has the keys ""John"" and ""Year"" with their corresponding values, and the second JSON object has the keys ""JOHN"" and ""Year"" with their corresponding values."
26,"mysql> SELECT JSON_OVERLAPS('{""John"": 10567, ""Year"":1988}', '{""JOHN"":10567, ""Year"":1989}') AS JSON_OVERLAP;"
26,+--------------+
26,| JSON_OVERLAP |
26,+--------------+
26,0 |
26,+--------------+
26,1 row in set (0.00 sec)
26,"The function returns false (0) because although both JSON objects share the common key ""Year"", their corresponding values are different. Therefore, there is no overlap between the two JSON objects."
26,4. SFORMAT function for arbitrary text formatting
26,"SFORMAT function allows for arbitrary text formatting. With this function, you can create custom formatting templates for your data, which can be useful for generating reports, and structured text formatting. Let’s try an example."
26,"We create a table named `employees` with columns `employee_id`, `Name`, and `Year`."
26,"CREATE TABLE employees(employee_id int, Name char(20), Year int);"
26,"Query OK, 0 rows affected (0.01 sec)"
26,"INSERT INTO employees VALUES(10567, 'John', 1988), ('10563', 'Sam', 1965);"
26,We use the SFORMAT function to create custom formatted strings based on the data in the table as follows.
26,"SELECT SFORMAT('Our Employee {} with ID number {} has joined us in the year {}',Name, employee_id, Year) AS 'Employee Joined'"
26,FROM employees;
26,+-----------------------------------------------------------------------+
26,| Employee Joined
26,+-----------------------------------------------------------------------+
26,| Our Employee John with ID number 10567 has joined us in the year 1988 |
26,| Our Employee Sam with ID number 10563 has joined us in the year 1965 |
26,+-----------------------------------------------------------------------+
26,The function replaces the placeholders in the template string with the corresponding values from the employees table and generates formatted strings.
26,SFORMAT() is a flexible way to format strings by which you can control the format of the output and ensure that the values are formatted correctly.
26,5. Query analysis with ANALYZE FORMAT=JSON
26,"ANALYZE FORMAT=JSON in MariaDB 10.11 is a powerful tool for query analysis and optimization. By running the command, you can obtain a comprehensive JSON-based analysis of the query run, which includes both the run plan and the actual query run. This includes information like r_loops (revealing how many times each node in the query plan was run), r_total_time (showing the total run time spent on each node), and r_buffer_size (providing insights into buffer utilization of nodes during the query run). This information will help database administrators and developers to fine-tune query performance, identify areas for potential optimization."
26,"ANALYZE FORMAT=JSON <query> will run the statement and prints the information about the query in EXPLAIN FORMAT=JSON format, we recommend please proceed with caution when running this command on the production workloads and during peak business hours as this command will run the query to provide the required metrics."
26,"In the following example, I run ANALYZE FORMAT=JSON on the query."
26,mysql> ANALYZE FORMAT=JSON select * from employees.Sales where k=4998122\G
26,*************************** 1. row ***************************
26,ANALYZE: {
26,"""query_optimization"": {"
26,"""r_total_time_ms"": 0.100549301"
26,"""query_block"": {"
26,"""select_id"": 1,"
26,"""r_loops"": 1,"
26,"""r_total_time_ms"": 0.471070642,"
26,"""nested_loop"": ["
26,"""table"": {"
26,"""table_name"": ""Sales"","
26,"""access_type"": ""ref"","
26,"""possible_keys"": [""k""],"
26,"""key"": ""k"","
26,"""key_length"": ""4"","
26,"""used_key_parts"": [""k""],"
26,"""ref"": [""const""],"
26,"""r_loops"": 1,"
26,"""rows"": 98,"
26,"""r_rows"": 98,"
26,"""r_table_time_ms"": 0.454686291,"
26,"""r_other_time_ms"": 0.011758278,"
26,"""filtered"": 100,"
26,"""r_filtered"": 100"
26,`Analyze` shows information about the overall analysis of the query.
26,`query_optimization` provides information related to query optimization.
26,`r_total_time_ms` shows total time taken for query optimization in milliseconds (0.100549301 ms in this case).
26,`query_block` section provides information about the query execution process.
26,`select_id` provides information related to Identifier for the SELECT query.
26,`r_loops` shows number of execution loops (1 loop in this case).
26,`r_total_time_ms` shows total time taken for query execution in milliseconds (0.471070642 ms in this case).
26,`nested_loop` subsection indicates the presence of nested loops in the execution process and provides Information about the table used in the query execution.
26,`r_loops` shows number of execution loops (1 loop in this case).
26,`rows` shows total number of rows in the table (98 rows in this case).
26,`r_rows` shows estimated number of rows processed (98 rows in this case).
26,`r_table_time_ms` shows time taken to access the table in milliseconds (0.454686291 ms in this case).
26,`r_other_time_ms` shows other processing time in milliseconds (0.011758278 ms in this case).
26,`filtered` shows percentage of rows that passed through the filter (100% in this case).
26,`r_filtered` shows estimated percentage of rows that passed through the filter (100% in this case).
26,6. Natural sort order
26,"You can use the natural_sort_key() function to perform natural sorting directly within the database, you achieve your requirements of human-friendly sorting to the database environment. This feature allows developers to obtain results that align with user expectations, making sorting more intuitive and comprehensible. Although natural sort functionality exists in various programming languages through built-in or third-party modules, MariaDB’s natural_sort_key() function simplifies the process within the database."
26,"In this example, we create a table named `coordinates` with a single column named `value`, and you insert various string values into it."
26,mysql> create table coordinates(value text);
26,"mysql> insert into coordinates values ('beta1'),('gamma2'),('alpha2'),('alpha11'),('alpha1'),('gamma23'),('gamma36');"
26,We are running a query to select values from the `coordinates` table and sort them using the default sorting order. This default sorting treats numbers in strings as characters and displays the following output.
26,mysql> select value from coordinates order by value;
26,+---------+
26,| value
26,+---------+
26,| alpha1
26,| alpha11 |
26,| alpha2
26,| beta1
26,| gamma2
26,| gamma23 |
26,| gamma3
26,| gamma36 |
26,+---------+
26,"We are running a similar query, but this time we are using the natural_sort_key() function to achieve natural sorting. This function treats numeric portions of strings as numbers, resulting in more intuitive sorting that aligns with user expectations."
26,mysql> select value from coordinates order by natural_sort_key(value);
26,+---------+
26,| value
26,+---------+
26,| alpha1
26,| alpha2
26,| alpha11 |
26,| beta1
26,| gamma2
26,| gamma3
26,| gamma23 |
26,| gamma36 |
26,+---------+
26,MariaDB 10.11 also includes the following enhancements which we do not cover in-depth in this post:
26,"Stored functions now supports IN, OUT and IN OUT parameters qualifiers."
26,Cursors now support IN qualifier.
26,With the UUID data type you can now store 128-bit UUID data.
26,System versioning capabilities have also been enhanced with the addition of the system_versioning_insert_history setting which allows for history modification
26,Upgrading to Amazon RDS for MariaDB 10.11
26,"You have multiple options to upgrade your database to Amazon RDS for MariaDB 10.11: in-place upgrade, using a read replica and Amazon RDS Blue/Green Deployments."
26,"We recommend using Amazon RDS Blue/Green Deployments to minimize risk and downtime during the database upgrade. This managed capability creates two database environments: your current production environment, the blue environment, and a staging environment, the green environment. These two environments remain in sync with each other using native logical replication, so you may safely test your changes on the staging (green) environment and promote when changes have been validated. During testing, we recommend that you test the application on green environment carefully, as enabling writes can result in replication conflicts leading to unexpected data in production environments."
26,"In the example below, we show you how to upgrade an RDS for MariaDB 10.6 instance to MariaDB 10.11 using Amazon RDS Blue/Green Deployments. You can use the same process to upgrade from any prior RDS for MariaDB version (e.g. 10.3, 10.4, 10.5) to version 10.11."
26,"RDS Blue/Green Deployments support the AWS Command line interface (AWS CLI), as well as the AWS Management Console. For this post, we use AWS CLI commands to upgrade our cluster."
26,"You can set the Blue/Green Deployment identifier and the parameters of your database (e.g., source ARN, engine version, and DB cluster parameter group) to be modified for the staging environment. In the following example, we have a MariaDB 10.6.14 version and we create the Amazon RDS Blue/Green Deployment using the following AWS CLI command:"
26,aws rds create-blue-green-deployment \
26,--blue-green-deployment-name mariadb-bgdeployment \
26,--source arn: arn:aws:rds:us-we**-2:9***********:db:m******-10614 \
26,--target-engine-version 10.11 \
26,--target-db-parameter-group-name default.mariadb10.11\
26,--region us-west-2
26,"Amazon RDS Blue/Green Deployments automatically creates a staging environment and run automated tasks to prepare the database. After the Blue/Green Deployment is complete, we have an environment that is ready for testing and promotion, after the validation."
26,"Note that you will be charged for the resources created in the staging environment, including Multi-AZ deployments and any other features such as Amazon RDS Performance Insights that may have been enabled in the production environment."
26,"You can also do the same deployment via the AWS Console by selecting the database and choosing “Create Blue/Green Deployment” on the Actions drop-down menu. For more information, follow the guidance in the New-Fully Managed Blue/Green Deployments in Amazon Aurora and Amazon RDS."
26,"After you validate your applications in the green environment, you can use the following command to switch over the production environment:"
26,aws rds switchover-blue-green-deployment \
26,--blue-green-deployment-identifier mariadb-bgdeployment\
26,--switchover-timeout 600
26,Conclusion
26,"In this post, we benchmarked Amazon RDS for MariaDB 10.11, and observed up to 47.25% improvement in transaction throughput compared to MariaDB 10.6. We also discussed new features introduced in Amazon RDS for MariaDB 10.11. Finally, we outlined how to upgrade to Amazon RDS for MariaDB 10.11 with minimal downtime, using Amazon RDS Blue/Green Deployments. We encourage you to try Amazon RDS for MariaDB 10.11 to test the performance improvements and feature innovations with your workload."
26,About the author
26,"Sai Kiran Kshatriya is an Amazon Web Services Database Specialist Solutions Architect who specializes in Relational Database Engines. He provides Technical Assistance, operational, and database practices to customers."
26,Comments
26,View Comments
26,Resources
26,Getting Started
26,What's New
26,Blog Topics
26,Amazon Aurora
26,Amazon DocumentDB
26,Amazon DynamoDB
26,Amazon ElastiCache
26,Amazon Keyspaces (for Apache Cassandra)
26,Amazon Managed Blockchain
26,Amazon MemoryDB for Redis
26,Amazon Neptune
26,Amazon Quantum Ledger Database (Amazon QLDB)
26,Amazon RDS
26,Amazon Timestream
26,AWS Database Migration Service
26,AWS Schema Conversion Tool
26,Follow
26,Twitter
26,Facebook
26,LinkedIn
26,Twitch
26,Email Updates
26,Sign In to the Console
26,Learn About AWS
26,What Is AWS?
26,What Is Cloud Computing?
26,"AWS Inclusion, Diversity & Equity"
26,What Is DevOps?
26,What Is a Container?
26,What Is a Data Lake?
26,What is Generative AI?
26,AWS Cloud Security
26,What's New
26,Blogs
26,Press Releases
26,Resources for AWS
26,Getting Started
26,Training and Certification
26,AWS Solutions Library
26,Architecture Center
26,Product and Technical FAQs
26,Analyst Reports
26,AWS Partners
26,Developers on AWS
26,Developer Center
26,SDKs & Tools
26,.NET on AWS
26,Python on AWS
26,Java on AWS
26,PHP on AWS
26,JavaScript on AWS
26,Help
26,Contact Us
26,Get Expert Help
26,File a Support Ticket
26,AWS re:Post
26,Knowledge Center
26,AWS Support Overview
26,Legal
26,AWS Careers
26,Create an AWS Account
26,Amazon is an Equal Opportunity Employer:
26,Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.
26,Language
26,عربي
26,Bahasa Indonesia
26,Deutsch
26,English
26,Español
26,Français
26,Italiano
26,Português
26,Tiếng Việt
26,Türkçe
26,Ρусский
26,ไทย
26,日本語
26,한국어
26,中文 (简体)
26,中文 (繁體)
26,Privacy
26,Site Terms
26,Cookie Preferences
26,"© 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved."
27,MySQL Query Optimization: Faster Performance & Data Retrieval | Airbyte
27,Complete the State of Data & AI Survey for a chance to win a Steam Deck!
27,View Press KitProduct
27,"ProductAirbyte CloudFully-managed, get started in minutesAirbyte Self-Managed EnterpriseSecure data movement for your entire orgAirbyte Open SourceUsed by 40k+ companiesPowered by AirbyteEmbed 100s integrations at once in your appcapabilitiesExtract & LoadReliable database and API replication at any scalePyAirbyteThe power of Airbyte to every Python developerConnector builderBuild a new connector  in 10 minAI / LLMs with Proprietary DataEmbeddings from unstructured sourcesTry our demo appExplore our public demoSolutions"
27,"Use CasesDatabase replicationHigh-volume DBs with low latencyArtificial intelligenceMake sense of unstructured data with LLMsAnalyticsMarketing, sales, product, finance, eng & moreEmbed ConnectorsEasily collect credentials from your end-usersResourcesSuccess storiesLearn from other members’ successCompare Airbyte vs. alternativesChoose the right solutions for youBuild vs. BuyEvaluate your costs in both scenariosResource centerOur guides to help you in your journeyPartnersBecome a technology or consulting partnerTHE LARGEST DATA ENGINEERING SURVEYCheck out State of DataNo items found.Developers"
27,"LearnDocsHow to use and contribute to AirbyteBlogData engineering thought leadershipTutorialsImprove your data replication gameQuickstartsDeploy your use case in minutesPublic RoadmapGet a glimpse in the futureData Engineering ResourcesPlace for all data knowledgeCommunityMonthly NewsletterStay up to date. 20k+ subscribersSupport centerAccess our knowledge baseCommunityJoin our 15,000+ data  communityCommunity Reward ProgramLeave your mark in the OSS communityEvents & community callsLive events by the Airbyte teamOur Social PlatformCommunity forumGitHub Discussions for questions, ideasSlack15,000+  share tips and get supportYoutubeLearn more about Airbyte and data engineeringDiscourse (read-only)Previous forum (still lots of knowledge)ConnectorsPricing"
27,"StarTalk to SalesTry it  freeData Engineering ResourcesMySQL Query Optimization: Faster Performance & Data RetrievalAditi Prakash••July 24, 2023•10 min readMySQL is the most used relational database management system (RDBMS) that drives countless modern applications and websites. Data engineers use structured query language (SQL) queries to access and modify the data in MySQL databases. These queries bridge the gap between code and the wealth of data stored in databases. Optimizing MySQL queries is a big part of performance tuning, which is essential for achieving optimal database performance and scalability.In this article, we will explain the core components of a MySQL Query Optimization, list the benefits of optimizing queries, and delve into the commonly used techniques during performance tuning.Understanding MySQL QueriesA MySQL query is an SQL statement that instructs the database to perform specific operations. These queries are used to retrieve, insert, update, or delete data from a MySQL database. The basic structure of a MySQL query has several components:SELECT: Specifies the columns or expressions to retrieve from the database.FROM: Specifies the table or tables from which the data is retrieved.WHERE: Optional condition that filters the data based on specified criteria.JOIN: Combines rows from multiple tables based on a related column between them (optional).GROUP BY: Groups the retrieved data based on one or more columns (optional).HAVING: Filters the grouped data based on specified conditions (optional).ORDER BY: Sorts the retrieved data based on one or more columns (optional).LIMIT: Limits the number of rows returned by the query (optional).Some standard use cases for these statements are: Data Retrieval: You typically use the SELECT statement for data retrieval. It allows you to specify the columns you want to fetch from the MySQL database and filter the data using the WHERE clause based on conditions. Example: SELECT first_name, last_name FROM employees WHERE department = 'HR';Data Insertion: You can use the INSERT statement to add new records to a table. Example:INSERT INTO employees (first_name, last_name, department) VALUES ('John', 'Doe', 'Finance');Data Updating: The UPDATE statement lets you modify existing records in a table. Example:UPDATE employees SET department = 'Marketing' WHERE id = 101;Data Deletion: The DELETE statement removes records from a table based on specified conditions. Example:DELETE FROM employees WHERE department = 'IT';MySQL queries can also involve more complex concepts, like:Joins: To retrieve data from multiple related tables.Aggregate Functions: To perform calculations on groups of data, e.g., SUM, COUNT, AVG.Subqueries: Queries within queries used to retrieve data based on intermediate results.Indexing: Creating indexes for frequently used columns for faster data retrieval.By mastering these concepts, data engineers can interact with a database effectively and perform operations to manipulate data according to their application's needs.Common issues that impact the performance of MySQL queriesThere are seven standard issues that data engineers face when implementing MySQL queries:Missing or inadequate indexes: Proper indexing of the columns used in WHERE, JOIN, and ORDER BY clauses can significantly improve MySQL database performance. Without appropriate indexes, MySQL has to perform full table scans, resulting in slower queries.Inefficient query design: Poorly written queries with complex joins, subqueries, or unnecessary calculations can slow down queries. Simplifying the query structure and optimizing it can improve performance.Large result sets: Retrieving a large number of rows from the database can impact MySQL performance and consume excessive memory. They can use pagination or LIMIT clauses to retrieve only the necessary data.Insufficient hardware resources: If the MySQL server is running on hardware with limited resources (e.g., CPU, memory, disk I/O), it can impact database performance.Locking and contention: Concurrent access to the same data can lead to locking and contention issues.Suboptimal database schema design: Poorly designed database schemas with redundant or excessive normalization can result in complex queries and slower performance.Poor network connectivity: Slow network connections between the client and the MySQL server hinders performance, especially for queries involving large result sets.Importance of MySQL Query OptimizationMySQL Query optimization is crucial for enhancing data retrieval speed and efficiency, directly impacting the application's overall performance and success. Some key benefits of optimizing MySQL performance include:Improved Performance: Optimized queries execute faster, reducing response times for your applications. This enhanced performance leads to a smoother user experience and higher customer satisfaction.Scalability: As your application grows and handles larger data volumes, optimized queries ensure that the database can efficiently handle the increased load without sacrificing performance.Resource Utilization: Efficient queries consume fewer server resources, such as CPU and memory, which lowers infrastructure costs.Reduced Downtime: Enhancing queries minimizes the risk of performance bottlenecks and potential crashes, leading to improved system stability and reduced downtime.Faster Development: Efficient queries lead to shorter development cycles, as developers spend less time troubleshooting slow queries and can focus on building new features and functionalities.Improved User Experience: Faster data retrieval and processing times lead to a more responsive application, keeping users engaged and reducing bounce rates.Database Maintenance: Well-designed queries simplify database maintenance tasks, making it easier to manage and monitor the MySQL database.Cost Savings: Efficient queries can lead to cost savings, as they reduce hardware requirements, optimize server usage, and improve overall system performance.Competitive Advantage: In a highly competitive market, faster application performance can give your business a competitive edge, attracting and retaining customers.Handling High Traffic: For web applications facing heavy user traffic, optimization ensures that the system can handle a high number of concurrent queries without compromising performance.Future-Proofing: Optimized queries can adapt to changing data patterns and growing workloads, ensuring that your application remains responsive and reliable in the long run.Techniques for Optimizing MySQL QueriesHere are some key techniques to improve MySQL performance:#1. Use appropriate indexingIdentify frequently accessed data and the columns used in WHERE, JOIN, and ORDER BY clauses and create indexes on those columns. Indexes allow MySQL to quickly find and retrieve the required data without scanning the entire table.Avoid over-indexing, as too many indexes can slow down insert, update, and delete operations.#2. Optimize SELECT statements and avoid SELECT *Only select the columns you need instead of using ""SELECT *."" This reduces the amount of data transferred and improves database performance.Use aggregate functions (e.g., SUM, COUNT, AVG) selectively to minimize data processing.#3. Utilize the Explain command to understand query executionThe EXPLAIN output shows how MySQL plans to execute the query, including the chosen indexes and the order of table access. Use this command before executing a query to analyze its execution plan, identify potential bottlenecks, and change the query accordingly.#4. Limit the amount of data retrievedUse the LIMIT clause to restrict the number of rows the query returns. This can significantly boost MySQL performance, especially for queries with large result sets.Implement pagination in applications to retrieve data in smaller chunks, reducing the server load and response times.#5. Use joins and avoid unnecessary subqueriesOptimize the use of JOINs by choosing the appropriate type of join (e.g., INNER JOIN, LEFT JOIN) based on the relationship between tables and the desired result.Minimize subqueries, as they can be less efficient than joins. Rewrite subqueries as JOINs where possible.#6. Normalize your database schemaNormalize your database schema to avoid data duplication and maintain data integrity. Use foreign keys to establish relationships between tables and enforce referential integrity.Normalization can lead to better data quality and more efficient queries, reducing the need for complex JOINs and allowing for smaller, more manageable tables.The effectiveness of these MySQL performance optimization techniques can vary depending on the specific database structure, data volume, and the complexity of the queries. Regular monitoring and benchmarking of MySQL performance is essential to find areas to optimize and ensure the efficiency of your MySQL database.Tools for Optimizing MySQL QueriesData engineers can use many tools and platforms for MySQL performance tuning. Some popular tools include:MySQL Performance SchemaMySQL Performance Schema is a built-in instrument for collecting detailed real-time information from the MySQL server. It provides valuable insights for measuring performance, including query execution, resource utilization, and overall server activity. By enabling this feature, you can monitor and diagnose performance issues and generate a slow query log, helping you identify bottlenecks and optimize queries accordingly.You can also analyze database performance and resource usage. Common tables include events_statements_summary_by_digest, events_statements_summary_by_user_by_event_name, etc.MySQL WorkbenchMySQL Workbench is an official graphical tool from MySQL that provides database design, administration, and optimization features. It includes a visual EXPLAIN feature, which helps you interpret query execution plans graphically. MySQL Workbench is user-friendly and suitable for developers and database administrators who prefer a GUI environment.Percona ToolkitPercona Toolkit is a set of command-line tools developed by Percona, a well-known MySQL consulting company. Some tools in this toolkit, like pt-query-digest and pt-query-advisor, are helpful for query analysis and optimization. Pt-query-digest processes MySQL query logs and summarizes how database queries are performing, while pt-query-advisor offers recommendations for optimizing slow queries.Real-World Examples of MySQL Query OptimizationTo help you understand how performance tuning can boost the performance of your MySQL databases, here are two example case studies:Case study 1: Optimizing a complex query for a large-scale data applicationA company operates a large-scale data analytics platform that collects and analyzes vast amounts of data from various sources. One of the queries used in their platform retrieves complex statistical data from multiple tables based on user-defined filters. The query's execution time has been increasing as the data volume grows, hindering the platform's overall performance.Steps for MySQL performance tuning:Indexing: The first step is to analyze the query's execution plan using the EXPLAIN command. For example, suppose the EXPLAIN output reveals that some critical columns used in JOIN and WHERE clauses were not indexed. In that case, appropriate indexes can be created to reduce the query execution time.Caching: Implement caching mechanisms at the application level to store the results of frequently executed queries in a cache. Using a MySQL query cache means user-defined queries don't need to be executed repeatedly.Query Rewriting: Rewrite parts of the query to eliminate redundant calculations and use efficient joins to streamline the query.Sharding: Depending on the scale of data, implementing sharding or partitioning to distribute data across multiple database servers. This reduces the data volume per server, leading to faster query execution.Hardware Optimization: Fine-tune the MySQL server configuration to ensure that the MySQL instance is appropriately utilizing CPU cores and memory.The result: With these optimization efforts, there can be a significant decrease in the execution time of the complex query. Users will experience faster response times and improved platform performance, even with the ever-increasing volume of data.Case study 2: Improving the performance of an e-commerce application with query optimizationAn e-commerce company faces slow loading times and performance issues on its product listing pages, where thousands of products are displayed. The application's database contains millions of product records, and the query fetching product data is becoming a performance bottleneck.Steps to optimize MySQL performance:SELECT Specific Columns: Instead of using ""SELECT *,"" the development team can revise the query to retrieve only the essential columns required for displaying products on the listing page. This reduces data transfer overhead and speeds up queries.Pagination and LIMIT: The team can implement pagination using the LIMIT clause to retrieve a limited number of products per page. This decreases the amount of data to be retrieved and leads to faster loading times for the listing pages.Caching: Since product listings often remain unchanged for a short period, the team can use caching mechanisms to store the query results temporarily. Cached data is served to users to avoid repetitive query execution and reduce the load on the database server.Denormalization: For read-heavy operations like product listings, denormalization can help. The data team can create a separate table with pre-joined and pre-computed data for the product listings.Load Balancing: To handle the increasing user traffic, data engineers can use a load-balanced configuration for the application's database, distributing the query load across multiple servers.The result: With the optimized query and various performance-enhancing techniques, the e-commerce application's product listing pages can load much faster. Users get a smoother and faster shopping experience, leading to higher customer satisfaction.Best Practices for MySQL Query OptimizationData engineers must focus on three factors for optimum MySQL performance:Regular monitoring and optimizationImplement regular monitoring mechanisms for query performance as part of the database maintenance routine. Use tools like MySQL Performance Schema, EXPLAIN, and query profiler to identify and optimize slow queries and bottlenecks.Also, consistently review and update database indexes to align with changing query patterns and data volume. Another area to review is MySQL server performance. Adjust configuration parameters based on workload and hardware capabilities.Training and education for the team on optimization techniquesTrain developers, data engineers, and database administrators on techniques for improving MySQL performance, interpreting EXPLAIN outputs, and indexing strategies.Foster a culture of awareness within the development team and encourage collaboration to optimize queries during code reviews and database design discussions.Incorporating optimization in the initial stages of application designDesign the database schema with a focus on normalization and efficient data retrieval. Carefully plan and optimize critical and frequently used queries during the application design phase. Consider anticipated data volume and scalability requirements when designing the database schema and query logic.The Future of MySQL Query OptimizationAdvancements in MySQL databases and related technologies might change queries and performance tuning in specific ways:Improved Query Optimizer: The query optimizer in MySQL is continually being enhanced to make smarter decisions in choosing the best execution plan for queries. As MySQL evolves, we can expect the optimizer to become more efficient and capable of handling complex queries more effectively.Indexing Innovations: Advancements in database technologies might introduce novel indexing techniques to improve data retrieval speed and reduce the overhead of maintaining indexes. Adaptive, partial, or hybrid indexing approaches could become more prevalent in MySQL performance tuning.Query Rewriting and Auto-Tuning: Future versions of MySQL could feature query rewriting capabilities that automatically optimize poorly written queries. Additionally, auto-tuning mechanisms might dynamically adjust server configuration and indexing strategies based on query patterns and workload.Parallel Query Execution: MySQL might leverage parallel query execution capabilities to process large queries faster. Multi-core processors and distributed computing could be better utilized to improve MySQL performance.Advanced Caching Mechanisms: Future MySQL versions might integrate more sophisticated caching mechanisms, such as intelligent caching based on query access patterns, to reduce the load on the database and improve response times.Hardware-Software Integration: Advancements in hardware technology, such as specialized accelerators (e.g., GPUs), could lead to better integration with MySQL, optimizing certain query operations and improving overall performance.Machine learning and AI developments can also impact queries and MySQL performance in the future. Some potential scenarios include:Query Plan Prediction: Machine learning algorithms can analyze historical query execution data and predict optimal query plans for specific types of queries. This can lead to more efficient query execution without relying solely on the traditional rule-based query optimizer.Auto-Tuning: Machine learning models can be applied to auto-tune various MySQL configuration parameters based on observed workloads, ensuring the database is optimally configured for specific application needs.Anomaly Detection: Machine learning techniques can help detect anomalies in query performance, enabling early identification of performance issues and potential optimizations.Index Recommendation: AI-powered systems can suggest appropriate indexes for frequently executed queries by analyzing historical query patterns and access frequencies.Query Rewrite Suggestions: AI can assist in recommending query rewrites or alternative formulations to improve query performance based on historical data and learned patterns.While machine learning and AI have great potential in optimization, they are not a replacement for traditional optimization methods. Combining the strengths of both approaches can lead to even more effective and efficient MySQL performance tuning.ConclusionQuery optimization builds a solid foundation for a high-performing, scalable, and successful MySQL-driven environment. It results in faster response times, reduces server load, and improves resource utilization. This can significantly enhance the speed and efficiency of query execution. Developers, database administrators, data engineers, and IT professionals must prioritize performance tuning and use it as a powerful tool to unlock the full potential of their MySQL databases and applications. If you're eager to expand your knowledge, delve into our tutorial on MySQL CDC for in-depth insights.You can learn more about databases, query optimization, and data insights on our Content Hub.Limitless data movement with free Alpha and Beta connectorsIntroducing: our Free Connector ProgramThe data movement infrastructure for the modern data teams.Try a 14-day free trialAbout the AuthorAditi Prakash is an experienced B2B SaaS writer who has specialized in data engineering, data integration, ELT and ETL best practices for industry-leading companies since 2021.About the AuthorTable of contentsExample H2Example H3Example H4Example H5Example H6Example H2Example H3Example H4Example H5Example H6Get your data syncing in minutesTry Airbyte freeJoin our newsletter to get all the insights on the data stack."
27,"Related postsWhat is a Data Pipeline?: Components, Types, Uses•March 20, 2024•20 MinsWhat is ETL Data Modeling: Working, Techniques, Benefits and Best Practices•March 19, 2024•15 min readData Pipeline vs. ETL: Optimize Data Flow (Beginner's Guide)•March 14, 2024•15 min readOn-Premise vs. Cloud Data Warehouses: The Comparison Guide•March 12, 2024•15 min readAirbyte is an open-source data integration engine that helps you consolidate your data in your data warehouses, lakes and databases.© 2024 Airbyte, Inc.ProductFeaturesDemo AppConnectorsConnector Development Kit (CDK)Airbyte Open SourceAirbyte CloudAirbyte Self-ManagedCompare Airbyte offersPricingChangelogRoadmapCompare top ELT solutionsRESOURCESDocumentationBlogAirbyte API DocsTerraform Provider DocsCommunityData Engineering ResourcesTutorialsQuickstartsNewsletterResource centerCommunity CallState of Data surveyTop ETL Tools""How to Sync"" TutorialsCOMPANYCompany HandbookAbout UsCareersOpen employee referral programAirbyte YC Startup ProgramPartnersPressData protection - Trust reportTerms of ServicePrivacy PolicyCookie PreferencesDo Not Sell/Share My Personal InformationContact SalesGet answers quick on Airbyte SlackHi there! Did you know our Slack is the most active Slack community on data integration? It’s also the easiest way to get help from our vibrant community.Join Airbyte SlackI’m not interested in joining"
28,MySQL :: MySQL 8.3 Reference Manual :: 10.2.1 Optimizing SELECT Statements
28,Skip to Main Content
28,The world's most popular open source database
28,Contact MySQL
28,Login  |
28,Register
28,MySQL.com
28,Downloads
28,Documentation
28,Developer Zone
28,Developer Zone
28,Downloads
28,MySQL.com
28,Documentation
28,MySQL Server
28,MySQL Enterprise
28,Workbench
28,InnoDB Cluster
28,MySQL NDB Cluster
28,Connectors
28,More
28,MySQL.com
28,Downloads
28,Developer Zone
28,Section Menu:
28,Documentation Home
28,MySQL 8.3 Reference Manual
28,Preface and Legal Notices
28,General Information
28,Installing MySQL
28,Upgrading MySQL
28,Downgrading MySQL
28,Tutorial
28,MySQL Programs
28,MySQL Server Administration
28,Security
28,Backup and Recovery
28,Optimization
28,Optimization Overview
28,Optimizing SQL Statements
28,Optimizing SELECT Statements
28,WHERE Clause Optimization
28,Range Optimization
28,Index Merge Optimization
28,Hash Join Optimization
28,Engine Condition Pushdown Optimization
28,Index Condition Pushdown Optimization
28,Nested-Loop Join Algorithms
28,Nested Join Optimization
28,Outer Join Optimization
28,Outer Join Simplification
28,Multi-Range Read Optimization
28,Block Nested-Loop and Batched Key Access Joins
28,Condition Filtering
28,Constant-Folding Optimization
28,IS NULL Optimization
28,ORDER BY Optimization
28,GROUP BY Optimization
28,DISTINCT Optimization
28,LIMIT Query Optimization
28,Function Call Optimization
28,Window Function Optimization
28,Row Constructor Expression Optimization
28,Avoiding Full Table Scans
28,"Optimizing Subqueries, Derived Tables, View References, and Common Table"
28,Expressions
28,Optimizing IN and EXISTS Subquery Predicates with Semijoin and Antijoin
28,Transformations
28,Optimizing Subqueries with Materialization
28,Optimizing Subqueries with the EXISTS Strategy
28,"Optimizing Derived Tables, View References, and Common Table Expressions"
28,with Merging or Materialization
28,Derived Condition Pushdown Optimization
28,Optimizing INFORMATION_SCHEMA Queries
28,Optimizing Performance Schema Queries
28,Optimizing Data Change Statements
28,Optimizing INSERT Statements
28,Optimizing UPDATE Statements
28,Optimizing DELETE Statements
28,Optimizing Database Privileges
28,Other Optimization Tips
28,Optimization and Indexes
28,How MySQL Uses Indexes
28,Primary Key Optimization
28,SPATIAL Index Optimization
28,Foreign Key Optimization
28,Column Indexes
28,Multiple-Column Indexes
28,Verifying Index Usage
28,InnoDB and MyISAM Index Statistics Collection
28,Comparison of B-Tree and Hash Indexes
28,Use of Index Extensions
28,Optimizer Use of Generated Column Indexes
28,Invisible Indexes
28,Descending Indexes
28,Indexed Lookups from TIMESTAMP Columns
28,Optimizing Database Structure
28,Optimizing Data Size
28,Optimizing MySQL Data Types
28,Optimizing for Numeric Data
28,Optimizing for Character and String Types
28,Optimizing for BLOB Types
28,Optimizing for Many Tables
28,How MySQL Opens and Closes Tables
28,Disadvantages of Creating Many Tables in the Same Database
28,Internal Temporary Table Use in MySQL
28,Limits on Number of Databases and Tables
28,Limits on Table Size
28,Limits on Table Column Count and Row Size
28,Optimizing for InnoDB Tables
28,Optimizing Storage Layout for InnoDB Tables
28,Optimizing InnoDB Transaction Management
28,Optimizing InnoDB Read-Only Transactions
28,Optimizing InnoDB Redo Logging
28,Bulk Data Loading for InnoDB Tables
28,Optimizing InnoDB Queries
28,Optimizing InnoDB DDL Operations
28,Optimizing InnoDB Disk I/O
28,Optimizing InnoDB Configuration Variables
28,Optimizing InnoDB for Systems with Many Tables
28,Optimizing for MyISAM Tables
28,Optimizing MyISAM Queries
28,Bulk Data Loading for MyISAM Tables
28,Optimizing REPAIR TABLE Statements
28,Optimizing for MEMORY Tables
28,Understanding the Query Execution Plan
28,Optimizing Queries with EXPLAIN
28,EXPLAIN Output Format
28,Extended EXPLAIN Output Format
28,Obtaining Execution Plan Information for a Named Connection
28,Estimating Query Performance
28,Controlling the Query Optimizer
28,Controlling Query Plan Evaluation
28,Switchable Optimizations
28,Optimizer Hints
28,Index Hints
28,The Optimizer Cost Model
28,Optimizer Statistics
28,Buffering and Caching
28,InnoDB Buffer Pool Optimization
28,The MyISAM Key Cache
28,Shared Key Cache Access
28,Multiple Key Caches
28,Midpoint Insertion Strategy
28,Index Preloading
28,Key Cache Block Size
28,Restructuring a Key Cache
28,Caching of Prepared Statements and Stored Programs
28,Optimizing Locking Operations
28,Internal Locking Methods
28,Table Locking Issues
28,Concurrent Inserts
28,Metadata Locking
28,External Locking
28,Optimizing the MySQL Server
28,Optimizing Disk I/O
28,Using Symbolic Links
28,Using Symbolic Links for Databases on Unix
28,Using Symbolic Links for MyISAM Tables on Unix
28,Using Symbolic Links for Databases on Windows
28,Optimizing Memory Use
28,How MySQL Uses Memory
28,Monitoring MySQL Memory Usage
28,Enabling Large Page Support
28,Measuring Performance (Benchmarking)
28,Measuring the Speed of Expressions and Functions
28,Using Your Own Benchmarks
28,Measuring Performance with performance_schema
28,Examining Server Thread (Process) Information
28,Accessing the Process List
28,Thread Command Values
28,General Thread States
28,Replication Source Thread States
28,Replication I/O (Receiver) Thread States
28,Replication SQL Thread States
28,Replication Connection Thread States
28,NDB Cluster Thread States
28,Event Scheduler Thread States
28,Language Structure
28,"Character Sets, Collations, Unicode"
28,Data Types
28,Functions and Operators
28,SQL Statements
28,MySQL Data Dictionary
28,The InnoDB Storage Engine
28,Alternative Storage Engines
28,Replication
28,Group Replication
28,MySQL Shell
28,Using MySQL as a Document Store
28,InnoDB Cluster
28,InnoDB ReplicaSet
28,MySQL NDB Cluster 8.3
28,Partitioning
28,Stored Objects
28,INFORMATION_SCHEMA Tables
28,MySQL Performance Schema
28,MySQL sys Schema
28,Connectors and APIs
28,MySQL Enterprise Edition
28,MySQL Workbench
28,MySQL on the OCI Marketplace
28,Telemetry
28,MySQL 8.3 Frequently Asked Questions
28,Error Messages and Common Problems
28,Indexes
28,MySQL Glossary
28,Related Documentation
28,MySQL 8.3 Release Notes
28,Download
28,this Manual
28,PDF (US Ltr)
28,- 40.8Mb
28,PDF (A4)
28,- 40.9Mb
28,Man Pages (TGZ)
28,- 294.0Kb
28,Man Pages (Zip)
28,- 409.0Kb
28,Info (Gzip)
28,- 4.0Mb
28,Info (Zip)
28,- 4.0Mb
28,Excerpts from this Manual
28,MySQL Backup and Recovery
28,MySQL NDB Cluster 8.3
28,MySQL Globalization
28,MySQL Information Schema
28,MySQL Installation Guide
28,MySQL and Linux/Unix
28,MySQL and macOS
28,MySQL Partitioning
28,MySQL Performance Schema
28,MySQL Replication
28,Using the MySQL Yum Repository
28,MySQL Restrictions and Limitations
28,Security in MySQL
28,MySQL and Solaris
28,Building MySQL from Source
28,Starting and Stopping MySQL
28,MySQL Tutorial
28,MySQL and Windows
28,version 8.3
28,8.0
28,current
28,5.7
28,8.0
28,Japanese
28,MySQL 8.3 Reference Manual  /
28,...  /
28,Optimization  /
28,Optimizing SQL Statements  /
28,Optimizing SELECT Statements
28,10.2.1 Optimizing SELECT Statements
28,10.2.1.1 WHERE Clause Optimization10.2.1.2 Range Optimization10.2.1.3 Index Merge Optimization10.2.1.4 Hash Join Optimization10.2.1.5 Engine Condition Pushdown Optimization10.2.1.6 Index Condition Pushdown Optimization10.2.1.7 Nested-Loop Join Algorithms10.2.1.8 Nested Join Optimization10.2.1.9 Outer Join Optimization10.2.1.10 Outer Join Simplification10.2.1.11 Multi-Range Read Optimization10.2.1.12 Block Nested-Loop and Batched Key Access Joins10.2.1.13 Condition Filtering10.2.1.14 Constant-Folding Optimization10.2.1.15 IS NULL Optimization10.2.1.16 ORDER BY Optimization10.2.1.17 GROUP BY Optimization10.2.1.18 DISTINCT Optimization10.2.1.19 LIMIT Query Optimization10.2.1.20 Function Call Optimization10.2.1.21 Window Function Optimization10.2.1.22 Row Constructor Expression Optimization10.2.1.23 Avoiding Full Table Scans
28,"Queries, in the form of SELECT"
28,"statements, perform all the lookup operations in the database."
28,"Tuning these statements is a top priority, whether to achieve"
28,"sub-second response times for dynamic web pages, or to chop"
28,hours off the time to generate huge overnight reports.
28,"Besides SELECT statements, the"
28,tuning techniques for queries also apply to constructs such as
28,CREATE
28,"TABLE...AS SELECT,"
28,INSERT
28,"INTO...SELECT, and WHERE clauses in"
28,DELETE statements. Those
28,statements have additional performance considerations because
28,they combine write operations with the read-oriented query
28,operations.
28,NDB Cluster supports a join pushdown optimization whereby a
28,qualifying join is sent in its entirety to NDB Cluster data
28,"nodes, where it can be distributed among them and executed in"
28,"parallel. For more information about this optimization, see"
28,Conditions for NDB pushdown joins.
28,The main considerations for optimizing queries are:
28,To make a slow SELECT ... WHERE query
28,"faster, the first thing to check is whether you can add an"
28,index. Set up indexes on
28,"columns used in the WHERE clause, to"
28,"speed up evaluation, filtering, and the final retrieval of"
28,"results. To avoid wasted disk space, construct a small set"
28,of indexes that speed up many related queries used in your
28,application.
28,Indexes are especially important for queries that reference
28,"different tables, using features such as"
28,joins and
28,foreign keys. You
28,can use the EXPLAIN statement
28,to determine which indexes are used for a
28,SELECT. See
28,"Section 10.3.1, “How MySQL Uses Indexes” and"
28,"Section 10.8.1, “Optimizing Queries with EXPLAIN”."
28,"Isolate and tune any part of the query, such as a function"
28,"call, that takes excessive time. Depending on how the query"
28,"is structured, a function could be called once for every row"
28,"in the result set, or even once for every row in the table,"
28,greatly magnifying any inefficiency.
28,Minimize the number of
28,full table scans
28,"in your queries, particularly for big tables."
28,Keep table statistics up to date by using the
28,ANALYZE TABLE statement
28,"periodically, so the optimizer has the information needed to"
28,construct an efficient execution plan.
28,"Learn the tuning techniques, indexing techniques, and"
28,configuration parameters that are specific to the storage
28,engine for each table. Both InnoDB and
28,MyISAM have sets of guidelines for
28,enabling and sustaining high performance in queries. For
28,"details, see Section 10.5.6, “Optimizing InnoDB Queries” and"
28,"Section 10.6.1, “Optimizing MyISAM Queries”."
28,You can optimize single-query transactions for
28,"InnoDB tables, using the technique in"
28,"Section 10.5.3, “Optimizing InnoDB Read-Only Transactions”."
28,Avoid transforming the query in ways that make it hard to
28,"understand, especially if the optimizer does some of the"
28,same transformations automatically.
28,If a performance issue is not easily solved by one of the
28,"basic guidelines, investigate the internal details of the"
28,specific query by reading the
28,EXPLAIN plan and adjusting
28,"your indexes, WHERE clauses, join"
28,"clauses, and so on. (When you reach a certain level of"
28,"expertise, reading the"
28,EXPLAIN plan might be your
28,first step for every query.)
28,Adjust the size and properties of the memory areas that
28,MySQL uses for caching. With efficient use of the
28,InnoDB
28,"buffer pool,"
28,"MyISAM key cache, and the MySQL query"
28,"cache, repeated queries run faster because the results are"
28,retrieved from memory the second and subsequent times.
28,Even for a query that runs fast using the cache memory
28,"areas, you might still optimize further so that they require"
28,"less cache memory, making your application more scalable."
28,Scalability means that your application can handle more
28,"simultaneous users, larger requests, and so on without"
28,experiencing a big drop in performance.
28,"Deal with locking issues, where the speed of your query"
28,might be affected by other sessions accessing the tables at
28,the same time.
28,PREV
28,HOME
28,NEXT
28,Related Documentation
28,MySQL 8.3 Release Notes
28,Download
28,this Manual
28,PDF (US Ltr)
28,- 40.8Mb
28,PDF (A4)
28,- 40.9Mb
28,Man Pages (TGZ)
28,- 294.0Kb
28,Man Pages (Zip)
28,- 409.0Kb
28,Info (Gzip)
28,- 4.0Mb
28,Info (Zip)
28,- 4.0Mb
28,Excerpts from this Manual
28,MySQL Backup and Recovery
28,MySQL NDB Cluster 8.3
28,MySQL Globalization
28,MySQL Information Schema
28,MySQL Installation Guide
28,MySQL and Linux/Unix
28,MySQL and macOS
28,MySQL Partitioning
28,MySQL Performance Schema
28,MySQL Replication
28,Using the MySQL Yum Repository
28,MySQL Restrictions and Limitations
28,Security in MySQL
28,MySQL and Solaris
28,Building MySQL from Source
28,Starting and Stopping MySQL
28,MySQL Tutorial
28,MySQL and Windows
28,Contact MySQL Sales
28,USA/Canada: +1-866-221-0634
28,(More Countries »)
28,© 2024 Oracle
28,Products
28,MySQL HeatWave
28,MySQL Enterprise Edition
28,MySQL Standard Edition
28,MySQL Classic Edition
28,MySQL Cluster CGE
28,MySQL Embedded (OEM/ISV)
28,Services
28,Training
28,Certification
28,Support
28,Downloads
28,MySQL Community Server
28,MySQL NDB Cluster
28,MySQL Shell
28,MySQL Router
28,MySQL Workbench
28,Documentation
28,MySQL Reference Manual
28,MySQL Workbench
28,MySQL NDB Cluster
28,MySQL Connectors
28,Topic Guides
28,About MySQL
28,Contact Us
28,Blogs
28,How to Buy
28,Partners
28,Job Opportunities
28,Site Map
28,© 2024 Oracle
28,Privacy /
28,Do Not Sell My Info |
28,Terms of Use |
28,Trademark Policy |
32,Inquiry Regarding Database Size and Performance Optimization -
32,EspoCRM Open Source Community Forum
32,Login or Sign Up
32,Logging in...
32,Remember me
32,Log in
32,Forgot password or user name?
32,or Sign Up
32,Log in with
32,Open source CRM support forum
32,Search in titles only
32,Search in General Discussion only
32,Search
32,Advanced Search
32,Forums
32,Today's Posts
32,Mark Channels Read
32,Member List
32,Calendar
32,"If this is your first visit, be sure to"
32,check out the FAQ by clicking the
32,link above. You may have to register
32,"before you can post: click the register link above to proceed. To start viewing messages,"
32,select the forum that you want to visit from the selection below.
32,Announcement
32,Collapse
32,No announcement yet.
32,Inquiry Regarding Database Size and Performance Optimization
32,Collapse
32,Collapse
32,Posts
32,Latest Activity
32,Photos
32,Search
32,Page
32,of 1
32,Filter
32,Time
32,All Time
32,Today
32,Last Week
32,Last Month
32,Show
32,All
32,Discussions only
32,Photos only
32,Videos only
32,Links only
32,Polls only
32,Events only
32,Filtered by:
32,Clear All
32,new posts
32,Previous
32,template
32,Next
32,yubrajkafle
32,Senior Member
32,Join Date: Oct 2020
32,Posts: 220
32,Share
32,Tweet
32,Inquiry Regarding Database Size and Performance Optimization
32,"08-15-2023, 03:30 AM"
32,"Hello Developer,"
32,"I am writing to inquire about a concern related to the size of my database and instances. Our organization has been utilizing Espo for the past three years, primarily utilizing the stream section to post updates."
32,"Recently, I performed a database backup in preparation for upgrading to the latest instance. To my surprise, the actual size of the database turned out to be approximately 20 GB. Upon further investigation using PHPMyAdmin, I discovered that a significant portion, around 98%, of the database size is attributed to a single entity, namely ""note"" (stream)."
32,"In light of this, I have a few questions regarding the potential impact on performance due to the size of the ""stream"" entity. Could you please advise if this substantial size of the ""stream"" entity might adversely affect the overall performance of our system? Additionally, I am curious about the feasibility of maintaining the ""stream"" section in its current state, especially considering the potential for further size increases in the future. Is there a possibility to optimize or reduce the size of the ""stream"" entity, or is it advisable to continue operating the software with the existing size?"
32,"Given the long-term perspective, I am concerned about the escalating size of the ""stream"" entity and its implications on system performance. Your guidance on how to effectively manage this situation would be greatly appreciated."
32,Thank you for your attention and assistance.
32,"Best regards,"
32,Yubraj Kafle
32,Tags:
32,None
32,yuri
32,Member
32,Join Date: Mar 2014
32,Posts: 7529
32,Share
32,Tweet
32,"08-15-2023, 09:55 AM"
32,"Hi,"
32,"Consider deleting note entries manually (by running SQL). Find out which type of note records you have predominantly, provide some examples. Then it will be easier to give advices."
32,The large note table can impact on performance of the Stream dashlet and the Stream panel on the user detail view. It should not impact on anything else.
32,Comment
32,Post
32,Cancel
32,dimyy
32,Active Community Member
32,Join Date: Jun 2018
32,Posts: 485
32,Share
32,Tweet
32,"08-16-2023, 04:32 AM"
32,You can enable slow query log and see which queries is impacto on performance.
32,https://dev.mysql.com/doc/refman/8.0...query-log.html (if you use mysql)
32,https://mariadb.com/kb/en/slow-query-log-overview/ (mariadb)
32,Comment
32,Post
32,Cancel
32,Previous
32,template
32,Next
32,What is CRM?
32,Documentation
32,CRM Hints and Tips
32,Video Tutorials
32,Download
32,Blog
32,Terms of Service
32,Privacy Policy
32,Go to top
32,"Powered by vBulletin® Version 5.7.5 Copyright © 2024 MH Sub I, LLC dba vBulletin. All rights reserved."
32,All times are GMT. This page was generated at 12:09 PM.
32,Working...
32,Yes
32,Cancel
33,Performance issue when querying views across linked servers - SQL Server | Microsoft Learn
33,Skip to main content
33,This browser is no longer supported.
33,"Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
33,Download Microsoft Edge
33,More info about Internet Explorer and Microsoft Edge
33,Table of contents
33,Exit focus mode
33,Read in English
33,Save
33,Table of contents
33,Read in English
33,Save
33,Edit
33,Print
33,Twitter
33,LinkedIn
33,Facebook
33,Email
33,Table of contents
33,Performance issue when querying views across linked servers
33,Article
33,06/20/2023
33,3 contributors
33,Feedback
33,In this article
33,Applies to:   SQL Server
33,Symptoms
33,Executing a query against views on a linked server remotely takes more time than executing the same query directly against a base table on the linked server.
33,Cause
33,"Executing a query against a view and a base table results in a different behavior because of the data cardinality estimation, which is used for calculating the expected number of rows returned. The number for querying views is set to a constant value of 10,000 while the number for querying base tables is derived from statistical information."
33,Cardinality estimation on a base table queried by linked servers
33,"The Microsoft OLE DB Provider (SQLOLEDB) (no longer maintained) or Microsoft® OLE DB Driver 18 for SQL Server® (MSOLEDBSQL) both support distribution statistics on base tables. SQL Server can utilize the statistics and histogram over linked servers in the same way as any regular query does. For example, if you create a linked server called LS1 that has the AdventureWorks2019 database with the Sales.SalesOrderDetail table, the following queries can utilize the statistical histogram:"
33,SELECT * FROM LS1.AdventureWorks2019.Sales.SalesOrderDetail;
33,SELECT * FROM LS1.AdventureWorks2019.Sales.SalesOrderDetail WHERE SalesOrderID=43659;
33,The following graphical query plans show the estimated number of rows returned on each remote query.
33,Cardinality estimation on views queried by linked servers
33,"Linked server queries against views don't utilize statistics-based cardinality estimation. The cardinality estimation against a view is a constant value of 10,000. The following test illustrates this:"
33,Create a view named dbo.View1 that references the Sales.SalesOrderDetail table from the AdventureWorks2019 database of the linked server LS1.
33,USE AdventureWorks2019;
33,CREATE VIEW [dbo].[view1]
33,SELECT * FROM Sales.SalesOrderDetail;
33,Query the view remotely.
33,SELECT * FROM LS1.AdventureWorks2019.dbo.view1;
33,"The cardinality estimation of the query shows exactly 10,000 rows."
33,Cardinality estimation on a view queried by linked servers with WHERE clause
33,"If you execute a query with the WHERE clause against views on a linked server, the cardinality estimation against views is also a constant value. However, the value varies depending on the compatibility level of the database."
33,Note
33,The cardinality estimation value won't be impacted by the compatibility level of database where the view is defined.
33,"For a database compatibility level of 120 or higher (new version of cardinality estimator), the cardinality estimation is 100."
33,"For a database compatibility level of 110 or lower (legacy cardinality estimator), the cardinality estimation is 1,000."
33,See the following queries for an example:
33,SELECT * FROM LS1.AdventureWorks2019.dbo.view1
33,WHERE SalesOrderID=43659
33,OPTION (USE HINT ('FORCE_DEFAULT_CARDINALITY_ESTIMATION'));
33,SELECT * FROM LS1.AdventureWorks2019.dbo.view1
33,WHERE SalesOrderID=43659
33,OPTION (USE HINT ('FORCE_LEGACY_CARDINALITY_ESTIMATION'));
33,See the following cardinality estimation of the queries for an example.
33,Resolution
33,"In most cases, no action is needed because most application workloads won't be impacted by the cardinality estimation on views when queried from linked servers. If the workload is impacted, use one of the following methods:"
33,Update and change the view as an indexed view. The indexed view has at least one index with corresponding statistics. See the following queries to create and query an indexed view.
33,Note
33,This method will expose actual statistics that represent the underlying data.
33,--Set the options to support indexed views.
33,SET NUMERIC_ROUNDABORT OFF;
33,"SET ANSI_PADDING, ANSI_WARNINGS, CONCAT_NULL_YIELDS_NULL, ARITHABORT,"
33,"QUOTED_IDENTIFIER, ANSI_NULLS ON;"
33,"IF OBJECT_ID ('Sales.vOrderDetails1', 'view') IS NOT NULL"
33,DROP VIEW Sales.vOrderDetails1;
33,--Create view with schemabinding
33,CREATE VIEW Sales.vOrderDetails1
33,WITH SCHEMABINDING
33,"SELECT SalesOrderID,"
33,"SalesOrderDetailID,"
33,"CarrierTrackingNumber,"
33,"OrderQty,"
33,"ProductID,"
33,"UnitPrice,"
33,"UnitPriceDiscount,"
33,"LineTotal,"
33,ModifiedDate
33,FROM Sales.SalesOrderDetail
33,--Create an index on the view
33,CREATE UNIQUE CLUSTERED INDEX IDX_V1
33,"ON Sales.vOrderDetails1 (SalesOrderID, ProductID);"
33,--Select from the materialized view
33,SELECT * FROM LS1.adventureworks2019.Sales.vOrderDetails1;
33,See the following cardinality estimation of the query for an example.
33,"If you execute a query with the WHERE clause, you can increase the cardinality estimation by using the Query Hint OPTION (USE HINT ('FORCE_LEGACY_CARDINALITY_ESTIMATION')). This approach may help in some cases especially when joining with large tables."
33,Note
33,This method is appropriate for cases where the view exposes many rows.
33,Execute the query directly against the base tables instead of views on the linked server if appropriate.
33,See also
33,Cardinality Estimation (SQL Server)
33,Monitor and Tune for Performance
33,Query Hints
33,USE HINT Query Hints
33,Query Processing Architecture Guide
33,Feedback
33,Was this page helpful?
33,Yes
33,Provide product feedback
33,Feedback
33,Coming soon: Throughout 2024 we will be phasing out GitHub Issues as the feedback mechanism for content and replacing it with a new feedback system. For more information see: https://aka.ms/ContentUserFeedback.
33,Submit and view feedback for
33,This product
33,This page
33,View all page feedback
33,Additional resources
33,California Consumer Privacy Act (CCPA) Opt-Out Icon
33,Your Privacy Choices
33,Theme
33,Light
33,Dark
33,High contrast
33,Previous Versions
33,Blog
33,Contribute
33,Privacy
33,Terms of Use
33,Trademarks
33,© Microsoft 2024
33,Additional resources
33,In this article
33,California Consumer Privacy Act (CCPA) Opt-Out Icon
33,Your Privacy Choices
33,Theme
33,Light
33,Dark
33,High contrast
33,Previous Versions
33,Blog
33,Contribute
33,Privacy
33,Terms of Use
33,Trademarks
33,© Microsoft 2024
35,ZFSTuningGuide - FreeBSD Wiki
35,Search:
35,Login
35,ZFSTuningGuide
35,RecentChangesFindPageHelpContentsZFSTuningGuide
35,Immutable PageCommentsInfoAttachments
35,More Actions:
35,Raw Text
35,Print View
35,Render as Docbook
35,Delete Cache
35,------------------------
35,Check Spelling
35,Like Pages
35,Local Site Map
35,------------------------
35,Rename Page
35,Delete Page
35,------------------------
35,Subscribe User
35,------------------------
35,Remove Spam
35,Revert to this revision
35,Package Pages
35,Sync Pages
35,------------------------
35,Load
35,Save
35,SlideShow
35,Contents
35,ZFS Tuning Guide
35,i386
35,amd64
35,Generic ARC discussion
35,L2ARC discussion
35,Application Issues
35,General Tuning
35,Deduplication
35,Suggestions
35,References
35,NFS tuning
35,MySQL
35,"Scrub and Resilver Performance See also: Solaris: ZFS Evil Tuning Guide, loader.conf(5), sysctl(8)."
35,ZFS Tuning Guide
35,"OpenZFS documentation recommends a minimum of 2GB of memory for ZFS; additional memory is strongly recommended when the compression and deduplication features are enabled. Depending on your workload, it may be possible to use ZFS on systems with less memory, but it requires careful tuning to avoid panics from memory exhaustion in the kernel. A 64-bit system is preferred due to its larger address space and better performance on 64-bit variables, which are used extensively by ZFS. 32-bit systems are supported though, with sufficient tuning. History of FreeBSD releases with ZFS is as follows: 7.0+ - original ZFS import, ZFS v6; requires significant tuning for stable operation (no longer supported) 7.2 - still ZFS v6, improved memory handling, amd64 may need no memory tuning (no longer supported) 7.3+ - backport of new ZFS v13 code, similar to the 8.0 code 8.0 - new ZFS v13 code, lots of bug fixes - recommended over all past versions. (no longer supported) 8.1+ - ZFS v14 8.2+ - ZFS v15 8.3+ - ZFS v28 9.0+ - ZFS v28"
35,i386
35,"Typically you need to increase vm.kmem_size_max and vm.kmem_size (with vm.kmem_size_max >= vm.kmem_size) to not get kernel panics (kmem too small). The value depends upon the workload. If you need to extend them beyond 512M, you need to recompile your kernel with increased KVA_PAGES option, e.g. add the following line to your kernel configuration file to increase available space for vm.kmem_size beyond 1 GB: options KVA_PAGES=512 To chose a good value for KVA_PAGES read the explanation in the sys/i386/conf/NOTES file. By default the kernel receives 1 GB of the 4 GB of address space available on the i386 architecture, and this is used for all of the kernel address space needs, not just the kmem map."
35,"By increasing KVA_PAGES you can allocate a larger proportion of the 4 GB address space to the kernel (2 GB in the above example), allowing more room to increase vm.kmem_size."
35,"The trade-off is that user applications have less address space available, and some programs (e.g. those that rely on mapping data at a fixed address that is now in the kernel address space, or which require close to the full 3 GB of address space themselves) may no longer run. If you change KVA_PAGES and the system reboots (no panic) after running a while this may be because the address space for userland applications is too small now. For *really* memory constrained systems it is also recommended to strip out as many unused drivers and options from the kernel (which will free a couple of MB of memory). A stable configuration with vm.kmem_size=""1536M"" has been reported using an unmodified 7.0-RELEASE kernel, relatively sparse drivers as required for the hardware and options KVA_PAGES=512. Some workloads need greatly reduced ARC size and the size of VDEV cache. ZFS manages the ARC through a multi-threaded process. If it requires more memory for ARC ZFS will allocate it. Previously it exceeded arc_max (vfs.zfs.arc_max) from time to time, but with 7.3 and 8-stable as of mid-January 2010 this is not the case anymore. On memory constrained systems it is safer to use an arbitrarily low arc_max. For example it is possible to set vm.kmem_size and vm.kmem_size_max to 512M, vfs.zfs.arc_max to 160M, keeping vfs.zfs.vdev.cache.size to half its default size of 10 Megs (setting it to 5 Megs can even achieve better stability, but this depends upon your workload). There is one example (CySchubert) of ZFS running nicely on a laptop with 768 Megs of physical RAM with the following settings in /boot/loader.conf: vm.kmem_size=""330M"" vm.kmem_size_max=""330M"" vfs.zfs.arc_max=""40M"" vfs.zfs.vdev.cache.size=""5M"" Kernel memory should be monitored while tuning to ensure a comfortable amount of free kernel address space. The following script will summarize kernel memory utilization and assist in tuning arc_max and VDEV cache size. #!/bin/sh -"
35,"TEXT=`kldstat | awk 'BEGIN {print ""16i 0"";} NR>1 {print toupper($4) ""+""} END {print ""p""}' | dc`"
35,DATA=`vmstat -m | sed -Ee '1s/.*/0/;s/.* ([0-9]+)K.*/\1+/;$s/$/1024*p/' | dc`
35,TOTAL=$((DATA + TEXT))
35,"echo TEXT=$TEXT, `echo $TEXT | awk '{print $1/1048576 "" MB""}'`"
35,"echo DATA=$DATA, `echo $DATA | awk '{print $1/1048576 "" MB""}'`"
35,"echo TOTAL=$TOTAL, `echo $TOTAL | awk '{print $1/1048576 "" MB""}'`"
35,"Note: Perhaps there is a more precise way to calculate / measure how large of a vm.kmem_size setting can be used with a particular kernel, but the authors of this wiki do not know it."
35,Experimentation does work.
35,"However, if you set vm.kmem_size too high in loader.conf, the kernel will panic on boot."
35,"You can fix this by dropping to the boot loader prompt and typing set vm.kmem_size=""512M"" (or a similar smaller number known to work.) The vm.kmem_size_max setting is not used directly during the system operation (i.e. it is not a limit which kmem can ""grow"" into) but for initial autoconfiguration of various system settings, the most important of which for this discussion is the ARC size. If kmem_size and arc_max are tuned manually, kmem_size_max will be ignored, but it is still required to be set. The issue of kernel memory exhaustion is a complex one, involving the interaction between disk speeds, application loads and the special caching ZFS does. Faster drives will write the cached data faster but will also fill the caches up faster. Generally, larger and faster drives will need more memory for ZFS. To increase performance, you may increase kern.maxvnodes (in /etc/sysctl.conf) way up if you have the RAM for it (e.g. 400000 for a 2GB system). On i386, keep an eye on vfs.numvnodes during production to see where it stabilizes. (AMD64 uses direct mapping for vnodes, so you don't have to worry about address space for vnodes on this architecture)."
35,amd64
35,"NOTE (gcooper): this blanket statement is far from true 100% of the time, depending on how the system with ZFS is being used. FreeBSD 7.2+ has improved kernel memory allocation strategy and no tuning may be necessary on systems with more than 2 GB of RAM."
35,Generic ARC discussion
35,"The value for vfs.zfs.arc_max needs to be smaller than the value for vm.kmem_size (not only ZFS is using the kmem). To monitor the ARC, you should install the sysutils/zfs-stats port;"
35,"the port is an evolution of the arc_stat.pl script available in Solaris that was ported to FreeBSD by FreeBSD contributor, jhell. To improve the random read performance, a separate L2ARC device can be used (zpool add <pool> cache <device>). A cheap solution is to add an USB memory stick (see http://www.leidinger.net/blog/2010/02/10/making-zfs-faster/). The high performance solution is to add a SSD. Using a L2ARC device will increase the amount of memory ZFS needs to allocate, see http://www.mail-archive.com/zfs-discuss@opensolaris.org/msg34674.html for more info."
35,L2ARC discussion
35,"ZFS has the ability to extend the ARC with one or more L2ARC devices, which provides the best benefit for random read workloads."
35,These L2ARC devices should be faster and/or lower latency than the storage pool.
35,Generally speaking this limits the useful choices to flash based devices.
35,In very large pools the ability to have devices faster than the pool may be problematic.
35,In smaller pools it may be tempting to use a spinning disk as a dedicated L2ARC device.
35,Generally this will result in lower pool performance (and definitely capacity) than if it was just placed in the pool.
35,"There may be scenarios in lower memory systems where a single 15K SAS disk can improve the performance of a small pool of 5.4k or 7.2 drives, but this is not a typical case. By default the L2ARC does not attempt to cache prefetched/streaming workloads, on the assumption that most data of this type is sequential and the combined throughput of your pool disks exceeds the throughput of the L2ARC devices, and therefore, this workload is best left for the pool disks to serve. This is usually the case. If you believe otherwise (number of L2ARC devices X their max throughput > number of pool disks X their max throughput, or you are not doing large amounts of sequential access), then this can be toggled with the following sysctl: vfs.zfs.l2arc_noprefetchThe default value of 1 does not allow caching of streaming and/or sequential workloads, and will not read from L2ARC when prefetching blocks."
35,"Switching it to 0 will allow prefetched/streaming reads to be cached, and may significantly improve performance if you are storing many small files in a large directory hierarchy (since many metadata blocks are read via the prefetcher and would ordinarily always be read from pool disks). The default throttling of loading the L2ARC device is 8 Mbytes/sec, on the assumption that the L2ARC is warming up from a random read workload from spinning disks, for which 8 Mbytes/sec is usually more than the spinning disks can provide. For example, at a 4 Kbyte I/O size, this is 2048 random disk IOPS, which may take at least 20 pool disks to drive. Should the L2ARC throttling be increased from 8 Mbytes, it would make no difference in many configurations, which cannot provide more random IOPS. The downside of increasing the throttling is CPU consumption: the L2ARC periodically scans the ARC to find buffers to cache, based on the throttling size. If you increase the throttling but the pool disks cannot keep up, you burn CPU needlessly. In extreme cases of tuning, this can consume an entire CPU for the ARC scan. If you are using the L2ARC in its typical use case: say, fewer than 30 pool disks, and caching a random read workload for ~4 Kbyte I/O which is mostly being pulled from the pool disks, then 8 Mbytes is usually sufficient. If you are not this typical use case: say, you are caching streaming workloads, or have several dozens of disks, then you may want to consider tuning the rate. Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default. It can be tuned by setting the following sysctls: vfs.zfs.l2arc_write_max"
35,vfs.zfs.l2arc_write_boostThe former value sets the runtime max that data will be loaded into L2ARC.
35,"The latter can be used to accelerate the loading of a freshly booted system. Note that the same caveats apply about these sysctls and pool imports as the previous one. While you can improve the L2ARC warmup rate, keep an eye on increased CPU consumption due to scanning by the l2arc_feed_thread(). Eg, use DTrace to profile on-CPU thread names (see DTrace One-Liners). The known caveats: There's no free lunch."
35,"A properly tuned L2ARC will increase read performance, but it comes at the price of decreased write performance. The pool essentially magnifies writes by writing them to the pool as well as the L2ARC device."
35,Another interesting effect that's been observed is a falloff in L2ARC performance when doing a streaming read from L2ARC while simultaneously doing a heavy write workload.
35,My conjecture is that the write can cause cache thrashing but this hasn't been confirmed at this time. Given a working set close to ARC size an L2ARC can actually hurt performance.
35,"If a system has a 14GB ARC and a 13GB working set, adding an L2ARC device will rob ARC space to map the L2ARC."
35,"If the reduced ARC size is smaller than the working set reads will be evicted from the ARC into the (ostensibly slower) L2ARC. Multiple L2ARC devices are concatenated, there's no provision for mirroring them."
35,If a heavily used L2ARC device fails the pool will continue to operate with reduced performance.
35,There's also no provision for striping reads across multiple devices.
35,If the blocks for a file end up in multiple devices you'll see striping but there's no way to force this behavior. Be very careful when adding devices to a production pool.
35,By default zpool add stripes vdevs to the pool.
35,"If you do this you'll end up striping the device you intended to add as an L2ARC to the pool, and the only way to remove it will be backing up the pool, destroying it, and recreating it. Many SSDs benefit from 4K alignment."
35,Using gpart and gnop on L2ARC devices can help with accomplishing this.
35,Because the pool ID isn't stored on hot spare or L2ARC devices they can get lost if the system changes device names. The caveat about only giving ZFS full devices is a solarism that doesn't apply to FreeBSD.
35,On Solaris write caches are disabled on drives if partitions are handed to ZFS.
35,On FreeBSD this isn't the case.
35,Application Issues
35,"ZFS is a copy-on-write filesystem. As such metadata from the top of the hierarchy is copied in order to maintain consistency in case of sudden failure, i.e. loss of power during a write operation. This obviates the need for an fsck-like requirement of ZFS filesystems at boot. However the downside to this is that applications which perform updates in place to large files, e.g. databases, will likely perform poorly in this application of the filesystem due to excessive I/O from copy-on-write (a fast SLOG device -- e.g. a SSD -- can help regarding the write performance of databases or any application which is doing synchronous writes (e.g. open with O_FSYNC) to the FS to make sure the data is on non-volatile storage when the write-call returns). Additionally, database applications, such as Oracle, maintain a large cache (called the SGA in Oracle) in memory will perform poorly due to double caching of data in the ARC and in the application's own cache. Reducing the ARC to a minimum can improve performance of applications which maintain their own cache. At ZFS Best Practices Guide there are some generic recommendations for ZFS on Solaris which mostly apply to FreeBSD too."
35,General Tuning
35,There are some changes that can be made to improve performance in certain situations and avoid the bursty IO that's often seen with ZFS. Loader tunables (in /boot/loader.conf): # Disable ZFS prefetching
35,# http://southbrain.com/south/2008/04/the-nightmare-comes-slowly-zfs.html
35,"# Increases overall speed of ZFS, but when disk flushing/writes occur,"
35,# system is less responsive (due to extreme disk I/O).
35,# NOTE: Systems with 4 GB of RAM or more have prefetch enabled by default.
35,"vfs.zfs.prefetch_disable=""1"""
35,# Decrease ZFS txg timeout value from 30 (default) to 5 seconds.
35,This
35,"# should increase throughput and decrease the ""bursty"" stalls that"
35,# happen during immense I/O with ZFS.
35,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007343.html
35,# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007355.html
35,# default in FreeBSD since ZFS v28
35,"vfs.zfs.txg.timeout=""5"""
35,Sysctl variables (/etc/sysctl.conf):
35,"# Increase number of vnodes; we've seen vfs.numvnodes reach 115,000"
35,# at times.
35,"Default max is a little over 200,000."
35,Playing it safe...
35,# If numvnodes reaches maxvnode performance substantially decreases.
35,kern.maxvnodes=250000
35,# Set TXG write limit to a lower threshold.
35,"This helps ""level out"""
35,"# the throughput rate (see ""zpool iostat"")."
35,A value of 256MB works well
35,"# for systems with 4 GB of RAM, while 1 GB works well for us w/ 8 GB on"
35,# disks which have 64 MB cache. <<BR>>
35,"# NOTE: in <v28, this tunable is called 'vfs.zfs.txg.write_limit_override'."
35,vfs.zfs.write_limit_override=1073741824
35,Be aware that the vfs.zfs.write_limit_override tuning you see above
35,may need to be adjusted for your system.
35,It's up to you to figure out
35,what works best in your environment.
35,Deduplication
35,"Deduplication is a misunderstood feature in ZFS v21+; some users see it as a silver bullet for increasing capacity by reducing redundancies in data. Here are the author's (gcooper's) observations: There are some resources that suggest that one needs 2GB per TB of storage with deduplication [i] (in fact this is a misinterpretation of the text). In practice with FreeBSD, based on empirical testing and additional reading, it's closer to 5GB per TB. Using deduplication is slower than not running it. Deduplication [on 8.x/9.x at least] lies via stat(2) / statvfs(2); it reports the theoretical used space -- not the actual used space -- which can confuse scripts that look at df output, etc (TODO: find PR that mentions this)."
35,Suggestions
35,"If you are going to use deduplication and your machine is underspec'ed, you must set vfs.zfs.arc_max to a sane value or ZFS will wire down as much available memory as possible, which can create memory starvation scenarios. It's a much better idea in general to use compression -- instead of deduplication -- if you're trying to save space, and you know that you can benefit from compression. When in doubt, check how much you would actually gain from deduplication via zdb -S <zpool> instead of just turning it on. Please note that this will take a while to run, depending on the dataset/zpool selected."
35,References
35,http://blogs.oracle.com/roch/entry/dedup_performance_considerations1
35,NFS tuning
35,"The combination of ZFS and NFS stresses the ZIL to the point that performance falls significantly below expected levels. The best solution is to put the ZIL on a fast SSD (or a pair of SSDs in a mirror, for added redundancy). You can now enable/disable ZIL on a per-dataset basis (as of ZFS version 28 / FreeBSD 8.3+).  zfs set sync=disabled tank/dataset  The next best solution is to disable ZIL with the following setting in loader.conf (up to ZFS version 15): vfs.zfs.zil_disable=""1""the vfs.zfs.zil_disable loader tunable was replaced with the ""sync"" dataset property. Disabling ZIL is not recommended where data consistency is required (such as database servers) but will not result in file system corruption. See ZFS Evil Tuning Guide, section ""Disabling the ZIL (Don't)"". ZFS is designed to be used with ""raw"" drives - i.e. not over already created hardware RAID volumes (this is sometimes called ""JBOD"" or ""passthrough"" mode when used with RAID controllers), but can benefit greatly from good and fast controllers."
35,MySQL
35,This assumes lots of RAM Tweaks for MySQL innodb_flush_log_at_trx_commit=2 skip-innodb_doublewrite Tweaks for ZFS zfs set primarycache=metadata tank/db zfs set atime=off tank/db zfs set recordsize=16k tank/db/innodb zfs set recordsize=128k tank/db/logs zfs set zfs:zfs_nocacheflush = 1 zfs set sync=disabled tank/db Note: MySQL 5.6.6 and newer (and related MariaDB / Percona forks)
35,"has innodb_file_per_table = on as default, so IBD files are not created under tank/db/innodb (defined by innodb_data_home_dir in your my.cnf), they are created under tank/db/<db_name>/ and you should use recordsize=16k on this dataset too or switch back to innodb_file_per_table = off References MySQL Innodb ZFS Best Practices (Oracle)"
35,Scrub and Resilver Performance
35,"If you're getting horrible performance during a scrub or resilver, the following sysctls can be set: vfs.zfs.scrub_delay=0"
35,vfs.zfs.top_maxinflight=128
35,vfs.zfs.resilver_min_time_ms=5000
35,vfs.zfs.resilver_delay=0Setting those sysctls to those values increased my (Shawn Webb's) resilver performance from 7MB/s to 230MB/s.
35,CategoryHowTo CategoryStale CategoryNeedsContent CategoryZfs ZFSTuningGuide
35,(last edited 2023-04-02T17:15:08+0000 by GrahamPerrin)
35,Immutable PageCommentsInfoAttachments
35,More Actions:
35,Raw Text
35,Print View
35,Render as Docbook
35,Delete Cache
35,------------------------
35,Check Spelling
35,Like Pages
35,Local Site Map
35,------------------------
35,Rename Page
35,Delete Page
35,------------------------
35,Subscribe User
35,------------------------
35,Remove Spam
35,Revert to this revision
35,Package Pages
35,Sync Pages
35,------------------------
35,Load
35,Save
35,SlideShow
35,MoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01
37,"Pagination in MySQLSkip to contentProductsSolutionsResourcesDocumentationDocsPricingContactProductsSolutionsResourcesDocumentationPricingContactBlogEngineeringFollow usPagination in MySQLAn overview of the different ways to paginate in MySQL including limit/offset pagination and cursor pagination plus the pros and cons of each.Table of contentsThe importance of deterministic orderingOffset/limit pagination- Strengths of offset/limit pagination- Offset/limit pagination and drifting pages- Performance drawbacks of offset/limit pagination- Deferred joins for faster offset/limit paginationCursor pagination- Drawbacks to cursor-based pagination- Benefits of cursor-based pagination- Cursor based pagination performanceConclusionAny good DBA will tell you to ""select only what you need."" It's one of the most common aphorisms, and for good reason! We don't ever want to select data that we're just going to throw away. One way this advice manifests itself is to not use SELECT * if you don't need all the columns. By limiting the columns returned, you're selecting only what you need.Pagination is another way to ""select only what you need."" Although, this time, we're limiting the rows instead of the columns. Instead of pulling all the records out of the database, we only pull a single page that we're going to show to the user.There are two primary ways to paginate in MySQL: offset/limit and cursors. Which method you choose depends on your use case and your application's requirements. Neither is inherently better than the other. They each have their own strengths and weaknesses.The importance of deterministic orderingBefore we talk about the wonders of pagination, we need to talk about deterministic ordering. When your query is ordered deterministically, it means that MySQL has enough information to order your rows in the exact same way every single time. If you sort your rows by a column that is not unique, MySQL gets to decide which order to return these rows in. Let's look at an example.Given this table full of people named Aaron:| id | first_name | last_name |"
37,|----|------------|-----------|
37,1 | Aaron
37,| Francis
37,2 | Aaron
37,| Smith
37,3 | Aaron
37,| Jones
37,Let's run a query to order those people by their first name:SQLSELECT
37,FROM
37,people
37,ORDER BY
37,first_name
37,"Because all three people have the same first name, MySQL gets to decide which order to return the rows in! Depending on certain factors, the order may change. This is because the ordering is not deterministic enough.This result set is valid because it is ordered by first_name:| id | first_name | last_name |"
37,|----|------------|-----------|
37,2 | Aaron
37,| Smith
37,1 | Aaron
37,| Francis
37,3 | Aaron
37,| Jones
37,"But so is this result set, because it also is ordered by first_name:| id | first_name | last_name |"
37,|----|------------|-----------|
37,3 | Aaron
37,| Jones
37,2 | Aaron
37,| Smith
37,1 | Aaron
37,| Francis
37,"We haven't given MySQL specific enough instructions to produce a deterministically ordered set of results. We've asked it to order the rows by first_name, and it has dutifully complied, but it may not put them in the same order every time.The easiest way to produce deterministic ordering is to order by a unique column because every value will be distinct, and MySQL will have no choice but to return the rows in the same order every time. Of course, that's not very helpful if you need to order by a column that's not unique! In that case, appending a unique column to your ordering does the trick. In most cases, simply adding the id is the best way to go.SQLSELECT"
37,FROM
37,people
37,ORDER BY
37,"first_name, id -- Add ID to ensure deterministic ordering"
37,"Now MySQL knows that when given two first_name values that are the same, it should then look at the id column to determine the order. This is deterministic ordering, and it's a prerequisite to effective pagination.Offset/limit paginationOffset/limit pagination is likely the most common way to paginate in MySQL because it's the easiest to implement. With offset/limit pagination, we're taking advantage of two SQL keywords: OFFSET and LIMIT. The LIMIT keyword tells MySQL how many rows to return, while OFFSET tells MySQL how many rows to skip over.SQLSELECT"
37,FROM
37,people
37,ORDER BY
37,"first_name, id"
37,LIMIT
37,10 -- Only return 10 rows
37,OFFSET
37,10 -- Skip the first 10 rows
37,"In this example, we're selecting all the people from the people table, ordering them by first_name and id, and then limiting the result set to 10 rows. We're also skipping the first 10 rows, returning rows 11-20.To construct an offset/limit query, you need to know the page size and the page number. The page size is how many records you want to show per page, and the page number is what page you want to show. The LIMIT is determined by the page size, and the OFFSET is determined by both the page size and the page number.To calculate the correct offset, multiply the page_number - 1 by the page_size. This ensures that when your user is on the first page, the offset calculates to 0, meaning you're not skipping any rows.SQLSELECT"
37,FROM
37,people
37,ORDER BY
37,"first_name, id"
37,LIMIT
37,10 -- page_size
37,OFFSET
37,10 -- (page_number - 1) * page_size
37,"Tip We have a video overview of offset/limit pagination, if you prefer that medium. Strengths of offset/limit paginationOne of the great strengths of offset/limit pagination is that it's easy to implement and easy to understand. It doesn't require tracking any state over time; each request can stand alone. It doesn't matter what pages the user has visited before. The query construction is always the same. The math is simple. The query is simple.Another strength of this method is that pages are directly addressable. Users who want to navigate from page 1 directly to page 10 can do so quite easily, provided your interface exposes page links. (This is not the case with cursor pagination.) Convincing arguments have been made that directly addressable pages shouldn't ever be exposed to users because they have no semantic meaning. For example, what does page 84 mean? Why not just expose ""next"" and ""back"" buttons? That's a decision that you'll have to make for your application! Many users are used to seeing directly addressable page numbers, and it can be helpful to skip several pages ahead instead of one page at a time. It's up to you to decide what's best for your application, but if you need directly addressable pages, you will need to use offset/limit pagination.Offset/limit pagination and drifting pagesOne weakness of offset/limit pagination is that pages can drift. This is true of cursor-based pagination as well, but it's more likely to happen with offset/limit pagination.Let's look at an example in which your user is viewing page one with ten records. The last person they see on this page is ""Judge Bins."" They don't see her yet, but ""Sonya Dickens"" should be the first person on page 2.| id | first_name | last_name |"
37,|----|------------|-----------|
37,1 | Phillip
37,| Yundt
37,2 | Aaron
37,| Francis
37,3 | Amelia
37,| West
37,4 | Jennifer
37,| Becker
37,5 | Macy
37,| Lind
37,6 | Simon
37,| Lueilwitz |
37,7 | Tyler
37,| Cummerata |
37,8 | Suzanne
37,| Skiles
37,9 | Zoe
37,| Hill
37,| 10 | Judge
37,| Bins
37,|----|------------|-----------| Page break
37,| 11 | Sonya
37,| Dickens
37,| 12 | Hope
37,| Streich
37,| 13 | Kristian
37,| Kerluke
37,| 14 | Stanton
37,| Fisher
37,| 15 | Rasheed
37,| Little
37,| 16 | Deron
37,| Koss
37,| 17 | Trevor
37,| Daniel
37,| 18 | Vernie
37,| Friesen
37,| 19 | Jody
37,| Littel
37,| 20 | Jorge
37,| Nienow
37,"While your user is viewing the page, the person with the id of 2 (Aaron Francis) is deleted.| id | first_name | last_name |"
37,|----|------------|-----------|
37,1 | Phillip
37,| Yundt
37,3 | Amelia
37,| West
37,4 | Jennifer
37,| Becker
37,5 | Macy
37,| Lind
37,6 | Simon
37,| Lueilwitz |
37,7 | Tyler
37,| Cummerata |
37,8 | Suzanne
37,| Skiles
37,9 | Zoe
37,| Hill
37,| 10 | Judge
37,| Bins
37,| 11 | Sonya
37,| Dickens
37,| <-- Sonya is now on page one!
37,|----|------------|-----------| Page break
37,| 12 | Hope
37,| Streich
37,| <-- This is now the first person on page two
37,| 13 | Kristian
37,| Kerluke
37,| 14 | Stanton
37,| Fisher
37,| 15 | Rasheed
37,| Little
37,| 16 | Deron
37,| Koss
37,| 17 | Trevor
37,| Daniel
37,| 18 | Vernie
37,| Friesen
37,| 19 | Jody
37,| Littel
37,| 20 | Jorge
37,| Nienow
37,| 21 | Mara
37,| Grady
37,"The user navigates to page two, and the first person they see is Hope Streich. Because we're naively skipping over the first ten rows, Sonya Dickens has been skipped altogether. Sorry Sonya. Your user never sees her unless they navigate back to page one.Paginating ever-changing data is not an easy problem to solve, and this may be an acceptable tradeoff for you. Even cursor-based pagination is prone to some of these movements, but it's less likely to happen.Performance drawbacks of offset/limit paginationThe way that the OFFSET keyword works is that it discards the first n rows from the result set. It doesn't simply skip over them. Instead, it reads the rows and then discards them. This means that as you work into deeper and deeper pages of your result set, the performance of your query will degrade. This is because the database must read and discard more rows as you move through the result set.Very deep pages can take multiple seconds to load. This is a big issue with offset/limit pagination, and it's one reason cursor-based pagination is so popular. Cursor-based pagination doesn't have this performance drawback because it doesn't use the OFFSET keyword.Deferred joins for faster offset/limit paginationThere is a technique known as a ""deferred join"" that can optimize offset/limit pagination.The deferred join technique is an optimization solution that enables more efficient pagination. It performs the pagination on a subset of the data instead of the entire table. This subset is generated by a subquery, which is joined with the original table later. The technique is called ""deferred"" because the join operation is postponed until after the pagination is done.SQLSELECT * FROM people"
37,INNER JOIN (
37,-- Paginate the narrow subquery instead of the entire table
37,"SELECT id FROM people ORDER BY first_name, id LIMIT 10 OFFSET 450000"
37,) AS tmp USING (id)
37,ORDER BY
37,"first_name, id"
37,"This technique has been widely adopted, and there are libraries available for popular web frameworks such as Rails (FastPage) and Laravel (Fast Paginate).Here is a graph showing the performance of a deferred join vs. the standard offset/limit pagination method, taken from our blog post introducing the FastPage Rails gem.As you can see, the deferred join method is much faster than the standard offset/limit pagination method, especially for deeper pages.If you do decide that offset/limit is the right choice for your application, then you should consider using a deferred join technique to optimize your queries.Cursor paginationNow that we're thoroughly versed on the offset/limit method let's talk about cursor-based pagination. Cursor-based pagination is a method of pagination that uses a ""cursor"" to determine the next page of results. It's important to note that this differs from a database cursor, which is a different concept. When discussing cursors in the context of pagination, we're using the word to mean a pointer, an identifier, a token, or a locator.Tip We also have a video overview of cursor pagination, if you prefer that medium. The idea behind cursor-based pagination is that you have a cursor that points to the last record that the user saw. When the user requests the next page of results, they must send along the cursor, which we use to determine where to start the next page of results.Instead of using the OFFSET keyword, we use the cursor to construct a WHERE clause that filters out all the rows that the user has already seen.Let's start with a simple example. Let's say we have a table of people and want to paginate the results by the id. When the user requests the first page of results, there is no cursor, so we return the first ten rows.SQLSELECT"
37,FROM
37,people
37,ORDER BY
37,LIMIT
37,MySQL returns the following result set:| id | first_name | last_name |
37,|----|------------|-----------|
37,1 | Phillip
37,| Yundt
37,2 | Aaron
37,| Francis
37,3 | Amelia
37,| West
37,4 | Jennifer
37,| Becker
37,5 | Macy
37,| Lind
37,6 | Simon
37,| Lueilwitz |
37,7 | Tyler
37,| Cummerata |
37,8 | Suzanne
37,| Skiles
37,9 | Zoe
37,| Hill
37,| 10 | Judge
37,| Bins
37,"Here is where cursor and offset-based pagination begin to diverge. With cursor-based pagination, we must construct and send the cursor out to the frontend. The cursor is a pointer to the last record that the user has seen. Since we are only sorting by id, the cursor is the id of the last record in the result set. Usually, it would be base64 encoded, but for simplicity, we'll just leave it unencoded.The backend sends out the results and a cursor of id=10, usually called next_page or something similar.JSON{"
37,"""next_page"": ""(id=10)"","
37,"""records"": ["
37,// ...
37,"When the user requests the next page of results, they must return the cursor to the server. The cursor is used to construct a WHERE clause that filters out all the rows the user has already seen.SQLSELECT"
37,FROM
37,people
37,WHERE
37,"id > 10 -- The last id that the user saw was 10, so we start at the next id after 10"
37,ORDER BY
37,LIMIT
37,"You can see that in this query, we're not using the OFFSET keyword at all, but instead, we're jumping straight to the next record after the last record that the user saw. This is the key difference between cursor and offset-based pagination!It gets a bit more complicated if we go back to our original example of sorting by first_name and then id. Since we're sorting by both columns, the cursor must contain both values for the last record that the user has seen.Let's take this example set of records, which is 20 people sorted by first name, and then ID.| id"
37,| first_name | last_name
37,|-------|------------|------------|
37,2 | Aaron
37,| Francis
37,589 | Aaron
37,| Streich
37,3896 | Aaron
37,| Corkery
37,8441 | Aaron
37,| Kreiger
37,9179 | Aaron
37,| Wolf
37,| 10970 | Aaron
37,| Reichert
37,| 13082 | Aaron
37,| Collier
37,| 13704 | Aaron
37,| Braun
37,| 19399 | Aaron
37,| Watsica
37,| 25995 | Aaron
37,| Runte
37,|-------|------------|------------| Page break
37,| 26794 | Aaron
37,| Mayer
37,| 32075 | Aaron
37,| Hahn
37,| 32471 | Aaron
37,| Bahringer
37,| 40612 | Aaron
37,| Abbott
37,| 41202 | Aaron
37,| Willms
37,| 41571 | Aaron
37,| Nienow
37,| 46556 | Aaron
37,| Glover
37,| 48501 | Aaron
37,| Boyle
37,| 50628 | Aaron
37,| Schmeler
37,| 51656 | Aaron
37,| Williamson |
37,"In this case, the last record the user sees on page 1 has an id of 25995. This information alone is not enough for the cursor! We must also add the first_name since it is part of the sort order. The cursor for the last record on page 1 is (first_name=Aaron, id=25995).When the user sends back the cursor, we can construct a WHERE clause that filters out all the rows the user has already seen. This time, it requires a little more thought because we're sorting by two columns. We'll add a first_name filter to show any names after ""Aaron,"" but since first_name has many duplicates, we'll also add an id filter to show any ""Aaron""s that have an id after the last id that the user saw.SQLSELECT"
37,FROM
37,people
37,WHERE
37,(first_name > 'Aaron')
37,-- Names after Aaron
37,"(first_name = 'Aaron' AND id > 25995) -- Aarons, but after the last id that the user saw"
37,ORDER BY
37,"first_name, id"
37,LIMIT
37,"As you add more columns to the sort order, you'll need to add more filters to the WHERE clause.Drawbacks to cursor-based paginationAs you've seen, cursor-based pagination is more complicated to implement than offset-based pagination. Constructing the cursor and the WHERE clause requires more thought. You also have to keep track of that little piece of state: the cursor. This isn't inherently bad, and not all complexity is reducible, but it's something to keep in mind. Most frameworks have cursor-based pagination built in, so you may not have to implement it manually.Another drawback to cursor-based pagination is that it's impossible to address a specific page directly. For instance, if the requirement is to jump directly to page five, it's not possible to do so since the pages themselves are not explicitly numbered, and there is no way to create a cursor without knowing the last record that has been seen. You can only navigate to the next page.Benefits of cursor-based paginationOne of the advantages of cursor-based pagination is its resilience to shifting rows. For example, if a record is deleted, the next record that would have followed is still displayed since the query is working off of the cursor rather than a specific offset.Let's go back to our Sonya Dickens example. The last person they see on this page is ""Judge Bins."" They don't see her yet, but ""Sonya Dickens"" should be the first person on page 2.| id | first_name | last_name |"
37,|----|------------|-----------|
37,1 | Phillip
37,| Yundt
37,2 | Aaron
37,| Francis
37,3 | Amelia
37,| West
37,4 | Jennifer
37,| Becker
37,5 | Macy
37,| Lind
37,6 | Simon
37,| Lueilwitz |
37,7 | Tyler
37,| Cummerata |
37,8 | Suzanne
37,| Skiles
37,9 | Zoe
37,| Hill
37,| 10 | Judge
37,| Bins
37,| <-- The cursor points here
37,|----|------------|-----------| Page break
37,| 11 | Sonya
37,| Dickens
37,| 12 | Hope
37,| Streich
37,| 13 | Kristian
37,| Kerluke
37,| 14 | Stanton
37,| Fisher
37,| 15 | Rasheed
37,| Little
37,| 16 | Deron
37,| Koss
37,| 17 | Trevor
37,| Daniel
37,| 18 | Vernie
37,| Friesen
37,| 19 | Jody
37,| Littel
37,| 20 | Jorge
37,| Nienow
37,"While they are viewing page one, ""Aaron Francis"" is deleted.| id | first_name | last_name |"
37,|----|------------|-----------|
37,1 | Phillip
37,| Yundt
37,3 | Amelia
37,| West
37,| <-- Aaron Francis is deleted
37,4 | Jennifer
37,| Becker
37,5 | Macy
37,| Lind
37,6 | Simon
37,| Lueilwitz |
37,7 | Tyler
37,| Cummerata |
37,8 | Suzanne
37,| Skiles
37,9 | Zoe
37,| Hill
37,| 10 | Judge
37,| Bins
37,| <-- The cursor *still* points here
37,|----|------------|-----------| Page break
37,| 11 | Sonya
37,| Dickens
37,| <-- Sonya is the first person after the cursor
37,| 12 | Hope
37,| Streich
37,| 13 | Kristian
37,| Kerluke
37,| 14 | Stanton
37,| Fisher
37,| 15 | Rasheed
37,| Little
37,| 16 | Deron
37,| Koss
37,| 17 | Trevor
37,| Daniel
37,| 18 | Vernie
37,| Friesen
37,| 19 | Jody
37,| Littel
37,| 20 | Jorge
37,| Nienow
37,"This time, it doesn't matter! The cursor points to the last record that the user saw, and the next record is still Sonya Dickens. We tell the database, ""the last record I saw was ID 10, and I want to see the next ten records."" The database doesn't care that some records were deleted. It just knows that the next record is Sonya Dickens.This is true even if the cursor is pointing to a record that was deleted. If the cursor points to a record that was deleted, we're still telling the database, ""the last record I saw was ID 10, and I want to see the next ten records."" Again, the database doesn't care that the record was deleted. It just knows that the next record is Sonya Dickens.Cursor based pagination performanceCursor-based pagination can be much more performant than offset/limit simply because it accesses much less data. Instead of generating a result set and throwing away everything before the offset, the database can start at the offset and return the next N records. This is especially true if the offset is large. You will need to consider a proper indexing strategy to ensure the database can efficiently find the necessary records.ConclusionPagination is a common requirement for almost every web application or API. Now you understand the different types of pagination and the tradeoffs that come with each.Offset/limit is nice because it's easy to implement and understand, and you can directly address pages. Some downsides are that it can be slower as you navigate deeper into the pages, and it is more prone to drift.Cursor-based pagination is nice because it is more performant and more resilient to shifting rows. Some of the downsides are that it is more complicated to implement, and you cannot directly address pages.Which method you choose is up to you, but hopefully, this article has given you a better understanding of the tradeoffs, and you can now make an informed decision.Want a powerful and performant database that doesn’t slow you down?Try out PlanetScale nowShareNext postSafely making database schema changesWritten byAaron FrancisDeveloper EducationLast updated April 18, 2023Database scaling courseThis 22 lesson video course covers partitioning, caching, replication, sharding, and more.Get startedTable of contentsThe importance of deterministic orderingOffset/limit paginationStrengths of offset/limit paginationOffset/limit pagination and drifting pagesPerformance drawbacks of offset/limit paginationDeferred joins for faster offset/limit paginationCursor paginationDrawbacks to cursor-based paginationBenefits of cursor-based paginationCursor based pagination performanceConclusionRelated postsEngineeringWhat is database sharding and how does it work?EngineeringConnection pooling in VitessEngineeringZero downtime Rails migrations with the PlanetScale Rails gemDatabase scaling courseThis 22 lesson video course covers partitioning, caching, replication, sharding, and more.Get startedCompanyAboutBlogChangelogCareersProductCase studiesEnterprisePricingAI/MLResourcesDocsResourcesMySQL for DevelopersSupportStatusOpen SourceVitessVitess communityGitHubTalk to usCall +1 408 214 1997Contact us© 2024 PlanetScale, Inc. All rights reserved.PrivacyTermsCookiesDo Not Share My Personal Information"
38,Magento Performance Optimization in 7 Steps (updated 2024)
38,Processing... Please wait...Product was successfully added to your shopping cart.
38,Go to cart page
38,Continue shopping
38,Services ↓ Magento 2 Migration Service Magento Development Services Magento Speed Optimization Service Magento 2 Upgrade Service Magento Consulting Services About How to ↓ How to speed up Magento site How to migrate Magento 1 to Magento 2 How to optimize TTFB of a Magento 2 site How to upgrade Magento to the latest version How to speed up Magento 2 website How to optimize Magento time to first byte (TTFB) How to perform a Magento security audit How to optimize Magento SEO How to choose a hosting provider for a Magento storeContact
38,"Magento performance optimization - 7 steps to a faster website (2024) by Konstantin   Last updated December 31, 2023 Server response time optimizationI have been optimizing Magento 2 sites for quite a while. During the process, there are a few things I've learned. I'd like to share them with you.I’ve learned that:Time to First Byte is an important metricMagento Site speed is key to maintaining your conversionsNot all Optimization techniques require programming skillsWhile it is true that some speed optimization tweaks require extensive knowledge of the Magento platform, it is also proven that there are others that require no programming at all; yet they are effective.This post brings together the field-proven performance tuning techniques. You are free to skip steps that require extensive programming. Do your best with the easy ones and you will speed up your Magento store.This guide mainly targets Magento 2. However, if you are still running Magento 1.x, you can apply some of the tweaks found in this post. Or migrate to M2.7 Performance Fixes:Magento optimized hosting.Time to first byte audit.MySQL tuning.Full Page Cache.Varnish.PHP8.CDN.Before we dive into how these seven components can help make Magento faster, we first need to look into why Magento website performance is so important. One of the big things to look for when conducting Magento optimization is site speed. Essentially, site speed measures how fast a site loads and how fast it can travel between web pages.Since customers are always on the go, it’s really important for eCommerce businesses to make sure customers can get to their products quickly.Let’s dive a little more into why Magento site speed is important:Why is Magento Slow?There are a variety of reasons as to why your Magento website is running slowly. However, it mostly happens due to wrong configuration and wrong choice of hosting provider.You see, Magento is not like it’s other competitors. You are not restricted on SKU numbers or anything like that, but with great power comes great responsibility. You do need the right hosting and if you are not tech savvy, you need a trustworthy magento programmer to help you along the way.Another common problem is that there are too many product attributes with too many choices. Product attributes allude to the characteristics of a product. Meaning, if you have a customizable product on your Magento site, your customers would be able to choose the quantity, size, or even the color of the product.Adding color product attributes with just a few choices (black, white, green) would not hurt. However, creating 10 product attributes with hundreds of choices would really slow down a Magento website. If you take a look here, you can see the resolution product attribute with many choices, as well as other multiple choices attributes. Without proper tuning, these factors slow down the Magento website significantly.If that’s not the problem, then what may be the problem is that your website has too much code to load. Generally speaking, one of the many problems my clients face when dealing with a slow website is that the JavaScript, HTML, and CSS all take a lot of time to load because each consists of so much code.I have found what works best with my clients is that I minimize the HTML and JavaScript. As for the CSS, I take the code and I place it at the bottom of the website so that the code will load fast. By the time the CSS starts loading, the web page will already show and the CSS will load without complications.Here is a comprehensive guide on how to make Magento websites fast. Keep these steps in mind when you build your eCommerce store as well as when you run it.1. Magento Optimized HostingYou can speed up Magento with the right service provider. But, you need to do the research and get a good hosting.A common mistake among my clients is that they tend to go for the cheapest hosting offered to them. When they tell me this, I always say the same thing: Cheap hosting is not good and good hosting is not cheap. Hosting is the foundation of your eCommerce store, so be sure to lay a good foundation. Some hosting providers offer Magento optimized solutions. Be sure to do the research, talk to customer support and find the best hosting for you and your store.Shared or dedicated?Shared hosting is a type of hosting that allows many websites to be on one server. This is an ideal route if your website has a small inventory and is just starting out. On the other hand, dedicated hosting allows one website to take over its own server. If you have a lot of inventory and large amounts of traffic, your website would have fewer chances of crashing if it were hosted on its own server.If your website needs to withstand large amounts of traffic, that also probably means that your server can reach maximum capacity on a shared server. This means that your Magento website would be better off on a dedicated server.With these facts in mind, it is really up to you to decide whether to go with shared or dedicated hosting. However, if you are a small store that wants to jump into dedicated hosting, but is intimidated by the price, I can assure you that, in my experience, I have seen many stores running pretty fast on shared servers.Look for the data center that is closest to your customers because you want to eliminate network issues. I would consider choosing the right host as the most important advice. Build your eCommerce business on solid ground.2. Reduce Time to First Byte (TTFB)TTFB is the measure of how quick your website responds to browser requests. The TTFB is generally made up of three separate components:The time needed to send the HTTP requestThe time needed for the server to process the requestThe time needed for the server to send back the first byte of the response to the clientIf you were to notice the white screen on your computer before the homepage loads, that is a clear sign of a website's TTFB. Blank ScreenIn simple words, it is the main speed parameter that tells you how fast your Magento store is. In the Magento platform, it is also one of the most important indicators of Magento site speed.To inspect your site’s platform behind the scenes, there is a Magento built-in tool, called Profiler, to tweak Magento performance. However, there are other ways to reduce your Magento site’s TTFB.An ideal TTFB should be under 1s. You can use online tools like Pingdom to see what is your TTFB is. If your TTFB is over 1s - that is a sign that you should start researching for ways to reduce it.Three Reasons for a Slow TTFB Include:Web server issuesThe main server has reached its full capacitySlow commands due to database optimizationReducing your TTFB requires a few simple tools. In fact, here is a great tutorial on how to reduce your TTFB.3. Optimize MySQL databaseOne important thing that can also affect site's performance is a MySQL database. Sometimes, website databases give slow commands because they need to be optimized. MySQL is a database that is comprised of your website data. Your data can comprise of anything from a simple product listing to a fully comprehensive wishlist.The first thing to do would be to gather MySQL internal statistics with mysqltuner script. This program tells you what is wrong with your database server and what needs fixing. It requires Perl interpreter to be installed on the server.Look for MySQL Table Cache Hit Rate (table_cache). If it says 0% - you are not using a database table cache at all. MySQL Table Cache configuration lets DB keep tables in memory thus speeding up access to its data. Tweak the table_cache parameter in my.cnf file to have table cache hit rate as close to 100% as possible.Another setting to check is Table Open Cache (table_open_cache). That parameter greatly affects performance by making the same table available for different sessions.I would advise you to get an expert in MySQL optimization to check your database server. If you are on shared hosting, your host team most likely takes care of your database tuning for you. If you run dedicated server either spend time learning MySQL configuration or get your DB configured by experts.4. Enable CacheEnabling your site’s cache can improve your overall performance by allowing the site to load faster and keeping track of the pages your visitors access.4.1 Internal Magento CacheEnabling you internal Magento cache is easy. Simply go to the admin panel and do the following:4.2 APC or MemcachedAfter you have enabled your internal Magento cache, the next step is to enable the Magento cache in the app/etc/env.php file. If you need help doing this, you can always consult a certified Magento developer.Check with your hosting to see if it supports APC or Memcached, as these programs cache sessions and other important data.5. Install and configure VarnishVarnish is a little program that caches parts of your webpage. I highly recommend that you install the latest version, as shown on the screen, and run it. Magento 2.0 supports Varnish but if you need to run Varnish on Magento 1.x, you would need a free extension by Nexcess - Turpentine. It’s worth noting that Turpentine is used by many stores and it follows Varnish Cache’s Best Practices. Download VarnishIf you have any questions about installing an extension on your Magento site, feel free to ask here.You should also make sure your host supports Varnish. If you run a dedicated server you might need to install and configure Varnish. If you are not sure, check with your provider to make sure that your hosting supports Varnish.Before going live, be sure to test your varnish implementation! Check if your cart block at the top has changed and check if it contains the products you added.Giving your store a good test drive after Varnish implementation is important because Varnish has been known to break eCommerce stores.6. PHP 8You can improve Magento 2 performance by upgrading to PHP 8. The latest Adobe Commerce 2.4.6 supports it.PHP 8 has several optimizations that could improve loading speed.Perform an upgrade yourself or hire an expert. Never forget to backup data and files.7. Content Delivery Network (CDN) Content Delivery NetworkCDN is a group of regional servers that can transfer static content. In terms of content delivery, you can say that they’re the backbone of the internet. With that, I strongly recommend that you sign up for CDN services. It is cheap and it does serve the static content (ex. images, CSS/JS files, video, etc) faster than your host would. Companies with great CDN plans include MaxCDN, Cloudflare.If anything else, ask your host if they offer CDN services.Why Should I Care?Magento site speed is one of the most important keys to having a successful website and, ultimately, a successful business.Remember that 47% of customers leave a site for loading too slow. That means that for every 100 people that go to your website, a slow Magento website will cause 47 of them to leave in 3 seconds or less.Can you imagine spending so much time creating your website, gaining traction on social media & trying to rank on search engines, only to lose nearly half of the customers that enter your site in three seconds just because your Magento website did not load fast enough?Your business can lose a lot of conversions by simply having a site that is slower than your competitors. On the other hand, your business can convert that much more people if your website is much faster than your competitors.If the following reason was not enough, Magento site speed affects your Google Rankings. In fact, in July of 2018, Google started penalizing websites that had slower loading times than other pages on the SERP (Search Engine Results Page). So if you have noticed a change in your once strong rankings, but did not change your digital marketing strategy, that is a sign that it is time to check your page speed.Aside from all of that, Magento performance optimization makes checkout less irritating for your customers. In our day and age, nobody wants to wait. This is especially true when buying on the internet. Happy customers generally mean repeat customers, which not only makes you more money but also saves you money because it’s cheaper to keep a current customer in your business cycle than to introduce a new customer into your business cycle.These seven tips will make your online store fast. It will attract more potential customers.  If you find this post interesting do not hesitate to sign up for our newsletter and join the 1312 people who receive Magento news, tips and tricks regularly.Sign UpThank You!  4 thoughts on “Magento performance optimization - 7 steps to a faster website (2024)”"
38,Stella George
38,"December 31, 2020 at 8:17 amNyc Article on Magento performance optimization! Thank you for sharing!"
38,Database Optimization Techniques
38,"August 5, 2020 at 5:14 amYour article is very nice & useful. Wish to see much more like this. Thanks for sharing your information"
38,Luis Paul
38,"December 23, 2019 at 8:52 amThe tips you've mentioned in the blog post are highly recommendable."
38,Thanks for the great blog post.
38,Peter Gustafson
38,"March 27, 2019 at 9:17 amGreat article. The need for speed is huge. Thanks for keeping us updated Kon."
38,Name *
38,Email *
38,Website
38,Comment *
38,Leave a comment
38,Please wait...
38,Get Free Quote Now
38,"My ServicesMagento Speed Optimization ServiceMagento 2 Migration ServiceMagento Development ServicesMagento 2 Upgrade ServiceMagento Consulting ServicesMagento Custom Extension Development Recent Posts Adobe Commerce vs Magento 2 - 25 New Features (Updated 2024) Speed up Magento - The Ultimate Guide (Updated 2024) Magento 2 hardware requirements (Updated 2024) Magento 2 Upgrade in 3 Easy Steps (Updated 2024) Magento 2 Slow Loading - 5 Quick Fixes (updated 2024) How to Upgrade Magento 2.3 to 2.4 Tutorial (updated 2024) Magento 2 Migration - The Ultimate Guide (Updated 2024) Magento 2 Hosting Providers Suggestion from an Expert (Updated 2024) Magento performance optimization - 7 steps to a faster website (2024) Speed up Magento 2: The Ultimate Guide (Updated 2024) Magento Security Audit in 2024 Magento SEO Strategies for 2024 Magento 2 Speed Optimization - From 10.0s to 2.7s (Real Case) Magento 2 Performance Optimization (easy guide to get 90%+ PageSpeed) Magento 2 TTFB (Time To First Byte) Optimization Magento 2 and 1 Million Products WooCommerce and Magento 2 - 1 million products benchmark Magento TTFB optimization to reduce time to first byte Magento Enterprise 1.14 to Community 1.9 Downgrade Marius Strajeru, Jisse Reitsma, Rakesh Jesadiya talk about Magento 2 5 Magento Inventory Management Programs Overview + Bonus Magento 1.9.3.2 vs 2.1.4 Performance Benchmark ServicesMagento Development ServicesMagento 2 Migration HelpSpeed Up MagentoMagento 2 Upgrade ServiceMagento Consulting ServicesMagento 2Migrate to Magento 2Adobe Commerce UpgradeMagento 2 Performance OptimizationCompanyAbout MeBlogSitemapPrivacy PolicyTerms of ServiceContact InfoGOIVVY LLCOffice Location: Auburn, Georgia, 30011, USAClick to contact me TwitterLinkedin © 2024 GOIVVY LLC, Certified Magento Developers USA"
39,Using your schema in MariaDBCloudera Docs
39,Using your schema in MariaDB
39,You follow an example of how to create an external table in MariaDB using your own
39,schema.
39,GRANT INSERT ANY TABLE TO bob;
39,GRANT INSERT ANY TABLE TO alice;
39,"Using MariaDB, create an external table based on a user-defined schema."
39,CREATE SCHEMA bob;
39,CREATE TABLE bob.country
39,"int,"
39,name varchar(20)
39,insert into bob.country
39,"values (1, 'India');"
39,insert into bob.country
39,"values (2, 'Russia');"
39,insert into bob.country
39,"values (3, 'USA');"
39,CREATE SCHEMA alice;
39,CREATE TABLE alice.country
39,"int,"
39,name varchar(20)
39,insert into alice.country
39,"values (4, 'Italy');"
39,insert into alice.country
39,"values (5, 'Greece');"
39,insert into alice.country
39,"values (6, 'China');"
39,insert into alice.country
39,"values (7, 'Japan');"
39,Parent topic: External tables based on a non-default schema
40,Jdbi 3 Developer Guide
40,Fork me on GitHub
40,Jdbi 3 Developer Guide
40,"version 3.45.2-SNAPSHOT,"
40,03/14/2024 10:19 -0700
40,Table of Contents
40,Development documentation (3.45.2-SNAPSHOT)
40,Release documentation
40,1. Introduction to Jdbi 3
40,1.1. Quick Links
40,1.2. JDBI v2 (legacy version)
40,1.3. Getting involved
40,1.4. Acknowledgements and Funding
40,2. Using Jdbi in your projects
40,"2.1. License, Dependencies and availability"
40,2.2. JVM version compatibility
40,2.3. Getting started
40,2.3.1. Jdbi modules overview
40,2.3.2. External modules
40,2.3.3. Build tools
40,2.3.4. @Alpha and @Beta Annotations
40,2.3.5. Internal packages
40,2.3.6. Mixing Jdbi component versions
40,3. API Overview
40,3.1. Fluent API
40,3.2. Declarative API
40,4. Core API concepts
40,4.1. The Jdbi class
40,4.2. Handle
40,4.2.1. Obtaining a managed handle
40,4.2.2. Managing the Handle lifecycle manually
40,4.3. Statement types
40,4.3.1. Queries
40,4.3.2. Updates
40,4.3.3. Batches
40,4.3.4. Prepared Batches
40,4.3.5. Stored Procedure Calls
40,4.3.6. Scripts
40,4.3.7. Metadata
40,4.4. Usage in asynchronous applications
40,5. Resource Management
40,5.1. Automatic resource management
40,5.2. Handle resource management
40,5.3. Statement resource management
40,5.3.1. Explicit statement resource management
40,5.3.2. Implicit statement resource cleanup
40,5.3.3. Resources with Streams and Iterators
40,5.3.4. OutParameters return values for stored procedures
40,5.3.5. Attaching statements to the handle lifecycle
40,6. Arguments
40,6.1. Positional Arguments
40,6.2. Named Arguments
40,6.3. Supported Argument Types
40,6.4. Binding Arguments
40,6.5. Argument Annotations
40,6.6. Custom Arguments
40,6.6.1. Using the Argument interface
40,6.6.2. Using the ArgumentFactory interface
40,6.6.3. Using Prepared Arguments for batches
40,6.6.4. The Arguments Registry
40,7. Mappers
40,7.1. Row Mappers
40,7.1.1. RowMappers registry
40,7.1.2. RowMapperFactory
40,7.2. Column Mappers
40,7.2.1. ColumnMappers registry
40,7.2.2. ColumnMapperFactory
40,7.3. Primitive Mapping
40,7.4. Immutables Mapping
40,7.5. Freebuilder Mapping
40,7.6. Reflection Mappers for Beans and POJOs
40,7.6.1. Supported property annotations
40,7.6.2. ConstructorMapper
40,7.6.3. BeanMapper
40,7.6.4. FieldMapper
40,7.7. Map.Entry Mapping
40,8. Codecs
40,8.1. Resolving Types
40,9. Results
40,9.1. ResultBearing
40,9.1.1. Mapping result types with latent operations
40,9.2. ResultIterable
40,9.2.1. Finding a Single Result
40,9.2.2. Stream
40,9.2.3. List
40,9.2.4. Collectors
40,9.2.5. Reduction
40,9.2.6. ResultSetScanner
40,9.3. Joins
40,9.3.1. ResultBearing.reduceRows()
40,9.3.2. ResultBearing.reduceResultSet()
40,9.3.3. JoinRowMapper
40,10. Transactions
40,10.1. Managed Transactions
40,10.2. Unmanaged Transactions
40,10.3. Serializable Transactions
40,11. Configuration
40,11.1. core settings
40,11.2. SQLObject configuration settings
40,11.3. other configuration settings
40,12. SQL Arrays
40,12.1. Registering array types
40,12.2. Binding custom array types
40,12.3. Mapping array types
40,13. SQL Objects
40,13.1. SQL Method annotations
40,13.1.1. @SqlQuery
40,13.1.2. @SqlUpdate
40,13.1.3. @SqlBatch
40,13.1.4. @SqlCall
40,13.1.5. @SqlScript
40,13.2. Using SQL Objects
40,13.2.1. Interface default methods
40,13.3. SQL Object method arguments
40,13.3.1. Mapping arguments to positional parameters
40,13.3.2. Mapping arguments to named parameters
40,13.3.3. Mapping arguments to Java parameter names
40,13.3.4. Consumer and Function arguments
40,13.3.5. Mapping arguments to defined attributes
40,13.4. SQL Object method return values
40,13.4.1. Using Streams and Iterators
40,13.5. SQL Object mapper annotations
40,13.5.1. @RegisterRowMapper
40,13.5.2. @RegisterRowMapperFactory
40,13.5.3. @RegisterColumnMapper
40,13.5.4. @RegisterColumnMapperFactory
40,13.5.5. @RegisterBeanMapper
40,13.5.6. @RegisterConstructorMapper
40,13.5.7. @RegisterFieldMapper
40,13.6. Other SQL Object annotations
40,13.6.1. @Definition
40,13.6.2. @GetGeneratedKeys
40,13.6.3. @SqlLocator
40,13.6.4. @CreateSqlObject
40,13.6.5. @Timestamped
40,13.6.6. @SingleValue
40,"13.6.7. Annotations for Map<K,V> Results"
40,13.6.8. @UseRowReducer
40,13.6.9. @RegisterCollector and @RegisterCollectorFactory
40,13.6.10. Other SQL Object annotations
40,13.6.11. Annotations and Inheritance
40,13.7. Combining SQL Object and the core API
40,13.7.1. Getting access to the handle with the SqlObject mixin
40,13.7.2. Using default Methods
40,13.8. SQL Object Transactions
40,13.8.1. Executing multiple SQL operations in a single transaction
40,14. Miscellaneous
40,14.1. Generated Keys
40,14.2. Qualified Types
40,14.3. Query Templating
40,14.3.1. ClasspathSqlLocator
40,14.4. Statement caching
40,14.4.1. Using custom cache instances
40,15. Testing
40,15.1. JUnit 4
40,15.2. JUnit 5
40,15.2.1. Testing with H2
40,15.2.2. Testing with Postgres
40,15.3. Using Testcontainers
40,15.3.1. Providing custom database information
40,15.3.2. Controlling the extension shutdown timeout
40,16. Third-Party Integration
40,16.1. Google Guava
40,16.2. H2 Database
40,16.3. JSON
40,16.3.1. Jackson 2
40,16.3.2. Gson 2
40,16.3.3. Moshi
40,16.3.4. Operation
40,16.3.5. Usage
40,16.4. Immutables
40,16.5. Freebuilder
40,16.6. JodaTime
40,16.7. Google Guice
40,16.7.1. JSR 330 vs. JEE 9+ annotations
40,16.7.2. Definition modules
40,16.7.3. Using Guice injection in Jdbi classes
40,16.7.4. Jdbi customization using Guice
40,16.7.5. Element configuration modules
40,16.7.6. Advanced Topics
40,16.8. JPA
40,16.9. Kotlin
40,16.9.1. ResultSet Mapping
40,16.9.2. Coroutine support
40,16.9.3. SqlObject
40,16.9.4. Jackson JSON Processing
40,16.10. Lombok
40,16.11. Oracle 12
40,16.12. PostgreSQL
40,16.12.1. hstore
40,16.12.2. @GetGeneratedKeys Annotation
40,16.12.3. Large Objects
40,16.13. Spring
40,16.13.1. XML-based configuration
40,16.13.2. Annotation-based configuration
40,16.13.3. Synchronization between Jdbi and JTA
40,16.13.4. Jdbi Repositories
40,16.14. SQLite
40,16.15. StringTemplate 4
40,16.15.1. Using String template for parsing
40,16.15.2. Locating Stringtemplate files on the class path
40,16.16. Apache Freemarker
40,16.16.1. Using Freemarker for parsing
40,16.16.2. Locating Freemarker templates on the class path
40,16.17. Vavr
40,17. Cookbook
40,17.1. Simple Dependency Injection
40,17.2. LIKE clauses with Parameters
40,18. The Extension Framework
40,18.1. Using Jdbi extensions
40,18.1.1. On-demand extensions
40,18.1.2. Handle lifecycle for extensions
40,18.2. How an extension works
40,18.3. Extension framework SDK
40,18.3.1. Extension configuration
40,18.3.2. The Extension factory
40,18.3.3. Extension handlers and their factories
40,18.3.4. Extension handler customizers
40,18.3.5. Config customizers and their factories
40,18.4. Annotations
40,18.4.1. Extension handler annotation
40,18.4.2. Extension handler customizer annotation
40,18.4.3. Extension handler configurer annotation
40,18.5. Extension metadata
40,19. Advanced Topics
40,19.1. High Availability
40,19.2. Compiling with Parameter Names
40,19.2.1. Maven Setup
40,19.2.2. IntelliJ IDEA setup
40,19.2.3. Eclipse Setup
40,19.3. Working with Generic Types
40,19.3.1. GenericType
40,19.3.2. The GenericTypes helper
40,19.4. NamedArgumentFinder
40,19.5. JdbiConfig
40,19.5.1. Creating a custom JdbiConfig type
40,19.6. JdbiPlugin
40,19.7. StatementContext
40,19.8. TemplateEngine
40,19.9. SqlParser
40,19.10. SqlLogger
40,19.11. ResultProducer
40,19.12. Jdbi SQL object code generator
40,19.13. HandleCallbackDecorator
40,19.13.1. Nesting Callbacks with managed Handles and Transactions
40,20. Appendix
40,20.1. Best Practices
40,20.2. API Reference
40,20.3. Related Projects
40,20.4. Upgrading from v2 to v3
40,Development documentation (3.45.2-SNAPSHOT)
40,This is the documentation for the current development state of Jdbi. All information in here reflects the current state of development on the master branch and is subject to change until it has been released as a numbered version.
40,Permalink to this document
40,Jdbi Javadoc (API Docs)
40,Current release notes
40,Development source tree
40,Release documentation
40,Release 3.45.1 - 2024-03-14 - Mmmmm pie
40,Release 3.45.0 - 2024-02-17
40,Release 3.44.1 - 2024-02-08
40,Release 3.44.0 - 2024-01-31
40,Release 3.43.0 - 2024-01-02
40,Release 3.42.0 - 2023-11-29
40,Release 3.41.3 - 2023-10-02
40,Release 3.41.2 - 2023-09-21
40,Release 3.41.1 - 2023-09-08
40,Release 3.41.0 - 2023-08-15
40,Release 3.40.0 - 2023-08-02
40,Release 3.39.1 - 2023-07-17 - This is the last release of Jdbi to support Java 8!
40,Release 3.39.0 - 2023-07-17
40,Release 3.38.3 - 2023-06-05
40,Release 3.38.2 - 2023-05-04
40,Release 3.38.1 - 2023-05-02
40,Release 3.38.0 - 2023-04-25
40,Release 3.37.1 - 2023-02-08
40,Release 3.37.0 - 2023-02-03
40,Release 3.36.0 - 2022-12-31
40,Release 3.35.0 - 2022-12-03
40,Release 3.34.0 - 2022-10-05
40,Release 3.33.0 - 2022-09-28
40,Release 3.32.0 - 2022-07-25
40,Release 3.31.0 - 2022-07-17
40,Release 3.30.0 - 2022-06-02
40,Release 3.29.0 - 2022-05-21
40,Release 3.28.0 - 2022-03-08
40,Release 3.27.2 - 2022-02-18
40,Release 3.27.1 - 2022-01-25
40,Release 3.27.0 - 2022-01-06
40,1. Introduction to Jdbi 3
40,"Jdbi provides convenient, idiomatic, access to relational data in Java. Jdbi 3 is the third major release, which introduces enhanced support for modern Java, countless refinements to the design and implementation, and enhanced support for modular development through plugins and extensions."
40,"Jdbi is built on top of JDBC. If your data source has a JDBC driver, you can use it with Jdbi. It improves JDBC’s low-level interface, providing a more natural API that is easy to bind to your domain data types."
40,"Jdbi is not an ORM. It is a convenience library to make Java database operations simpler and more pleasant to program than raw JDBC. While there is some ORM-like functionality, Jdbi goes to great length to ensure that there is no hidden magic that makes it hard to understand what is going on."
40,"Jdbi does not hide SQL away. One of the design principles of Jdbi is that SQL is the native language of the database, and it is unnecessary to wrap it into code, deconstruct it, or hide it away. Being able to express a query in raw SQL makes it possible for programmers and data engineers to speak the same language and not fight translation layers."
40,Jdbi does not aim to provide a complete database management framework. It provides the building blocks that allow constructing the mapping between data and objects as appropriate for your application and the necessary primitives to execute SQL code against your database.
40,1.1. Quick Links
40,Jdbi Javadoc (API Docs)
40,Jdbi sample code
40,GitHub Jdbi repository
40,Discussion groups for Jdbi
40,Mailing list
40,Stack Overflow
40,1.2. JDBI v2 (legacy version)
40,JDBI v2 is no longer under active development!
40,Jdbi v2 documentation
40,Already using Jdbi v2? See Upgrading from v2 to v3
40,1.3. Getting involved
40,"Jdbi uses GitHub as the central development hub. Issues, Pull Requests and Discussions all happen here."
40,Please see our Contribution guide for more information on how to contribute to Jdbi.
40,1.4. Acknowledgements and Funding
40,Jdbi is a recipient of the Spotify FOSS 2023 Fund
40,Jdbi is supported by Tidelift
40,2. Using Jdbi in your projects
40,"2.1. License, Dependencies and availability"
40,Jdbi is licensed under the commercial friendly Apache 2.0 license.
40,The core Jdbi module which offers programmatic access uses only slf4j and geantyref as hard dependencies.
40,All Jdbi modules are available through Maven Central. Jdbi also offers a BOM (Bill of materials) module for easy dependency management.
40,2.2. JVM version compatibility
40,Jdbi runs on all Java versions 11 or later. All releases are built with the latest LTS version of Java using Java 11 bytecode compatibility.
40,"Jdbi ended support for Java 8 with version 3.39. There are occasional backports of security relevant things or major bugs but as of version 3.40.0, JDK 11+ is required."
40,2.3. Getting started
40,"Jdbi has a flexible plugin architecture, which makes it easy to fold in support for your favorite libraries (Guava, JodaTime, Spring, Vavr) or database vendors (Oracle, Postgres, H2)."
40,"Jdbi is not an ORM. There is no session cache, change tracking, ""open session in view"", or cajoling the library to understand your schema."
40,"Jdbi provides straightforward mapping between SQL and data accessible through a JDBC driver. You bring your own SQL, and Jdbi executes it."
40,"Jdbi provides several other modules, which enhance the core API with additional"
40,features.
40,2.3.1. Jdbi modules overview
40,Jdbi consists of a large number of modules. Not all are required when writing database code. This is a quick overview of the existing modules and their function:
40,Common modules
40,jdbi3-core
40,The core Jdbi library. Required by all other components.
40,jdbi3-sqlobject
40,SQL Object extension for declarative database access.
40,Testing support
40,jdbi3-testing
40,"Testing framework support. Currently, only supports JUnit4 and JUnit5."
40,jdbi3-testcontainers
40,"Support for arbitrary databases running in Testcontainers. Currently, only supports JUnit 5."
40,External data types and libraries
40,The core module contains support for the Immutables and Freebuilder libraries.
40,jdbi3-guava
40,Support for Google Guava collection and Optional types.
40,jdbi3-jodatime2
40,Support for JodaTime v2 data types.
40,jdbi3-vavr
40,"Support for Vavr Tuples, Collections and Value arguments."
40,JSON Mapping
40,Support for various JSON libraries to map data from the database onto JSON types and vice versa.
40,jdbi3-jackson2
40,Support for Jackson v2.
40,jdbi3-gson
40,Support for Google Gson.
40,jdbi3-moshi
40,Support for Square Moshi.
40,Frameworks
40,jdbi3-guice
40,Support dependency injection and modules with Google Guice.
40,jdbi3-spring5
40,Provides a Spring Framework factory bean to set up Jdbi singleton.
40,jdbi3-jpa
40,Some support for JPA annotations.
40,Database specific types and functions
40,"While Jdbi supports any data source that has a JDBC driver ""out of the box"", its support is limited to the standard JDBC types and mappings. Some databases have additional data types and mappings, and support is added using the following modules:"
40,jdbi3-postgres
40,Support for Postgres data types.
40,jdbi3-sqlite
40,Support for sqlite data types.
40,jdbi3-oracle12
40,Support Oracle returning DML statements.
40,jdbi3-postgis
40,Support for PostGIS types.
40,"While Oracle support is considered part of Jdbi, the actual library is developed and shipped as a separate component for historic reasons."
40,SQL rendering
40,"SQL statements can be rendered before they are sent to the database driver. Unless explicitly configured, Jdbi uses a simple render engine that replaces <…​> placeholders. It is possible to replace this engine with other template engines."
40,jdbi3-stringtemplate4
40,Use the StringTemplate 4 template engine to render SQL statements.
40,jdbi3-commons-text
40,Use Apache Commons Text to render SQL statements.
40,jdbi3-freemarker
40,Use Apache Freemarker to render SQL statements.
40,Cache support
40,jdbi3-caffeine-cache
40,Use the Caffeine caching library for SQL template and parse caching.
40,jdbi3-noop-cache
40,Turn off SQL template and parse caching for testing and debugging.
40,jdbi3-guava-cache
40,Use the Guava Cache caching library for SQL template and parse caching.
40,The guava caching module is considered experimental.
40,Additional Language support
40,"Jdbi can be used from any language and technology running on the JVM that can use Java code (Kotlin, Scala, Clojure, JRuby etc)."
40,"While Java is a first class citizen (and will be for the foreseeable future), we provide modules for some languages that allow more idiomatic access to Jdbi functionality."
40,jdbi3-kotlin
40,Automatically map Kotlin data classes.
40,jdbi3-kotlin-sqlobject
40,Kotlin support for the SQL Object extension.
40,2.3.2. External modules
40,The following modules are maintained outside the main Jdbi tree:
40,Module
40,Source
40,Javadoc
40,Site
40,Description
40,jdbi3-oracle12
40,Github
40,Javadoc
40,Site
40,Support for Oracle 12 and beyond.
40,jdbi3-guava-cache
40,Github
40,Javadoc
40,Site
40,Experimental support for the Guava cache library.
40,"The additional modules are usually released in sync with the main release. If any of the links above for a release version do not resolve, it may be possible that a release was omitted by accident. In that case, please file an issue on the Bug Tracker."
40,2.3.3. Build tools
40,"All Jdbi modules are available through Maven Central, so any project that uses a dependency management tool (Apache Maven, Gradle, sbt, leiningen, Apache Ivy etc.) can access these."
40,For Apache Maven:
40,<dependencies>
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-core</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,</dependencies>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-core:3.45.2-SNAPSHOT"")"
40,"When using multiple Jdbi modules, it is important that all modules use the same version. There is no guarantee that mixing versions will work."
40,Jdbi offers a BOM (Bill of Materials) that can be used to provide a consistent version for all Jdbi components.
40,For Apache Maven:
40,<dependencyManagement>
40,<dependencies>
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-bom</artifactId>
40,<type>pom</type>
40,<version>3.45.2-SNAPSHOT</version>
40,<scope>import</scope>
40,</dependency>
40,</dependencies>
40,</dependencyManagement>
40,adds the Jdbi BOM module into the dependencyManagement section.
40,Jdbi components in use are declared in the <dependencies> section without a version:
40,<dependencies>
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-core</artifactId>
40,</dependency>
40,</dependencies>
40,For Gradle:
40,dependencies {
40,// Load bill of materials (BOM) for Jdbi.
40,"implementation(platform(""org.jdbi:jdbi3-bom:3.45.2-SNAPSHOT""))"
40,adds the BOM as dependency constraints to Gradle.
40,Jdbi components are declared without versions:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-core"")"
40,2.3.4. @Alpha and @Beta Annotations
40,"The @Alpha and @Beta annotations mark APIs as unstable. Each of these annotations signifies that a public API (public class, method or field) is subject to incompatible changes, or even removal, in a future release. Any API bearing these annotations is exempt from any compatibility guarantees."
40,Alpha — Alpha APIs are intended as early preview of features that might eventually get promoted.
40,"Beta — It is generally safe for applications to depend on beta APIs, at the cost of some extra work during upgrades. However, libraries (which get included on an application classpath) should not depend on Beta APIs as the classpath is outside the control of the library."
40,"Note that the presence of this annotation implies nothing about the quality or performance of the API in question, only the fact that it is not ""API-frozen."""
40,"Add Alpha and Beta to your IDE’s ""unstable API usage"" blacklist."
40,2.3.5. Internal packages
40,"Any class in a package that is marked as internal (contains the word ""internal"") is not a part of the public API and may change in a backwards incompatible way. These classes and interfaces may be used across the Jdbi code base and can change (or be removed) without deprecation or announcement."
40,2.3.6. Mixing Jdbi component versions
40,There is no guarantee that Jdbi components of different versions (e.g. jdbi3-core version 1.2.3 and jdbi-sqlobject version 1.3.1) work together. Jdbi versioning is the public API exposed to consumers. All Jdbi components used in a project or service should use the same version. See the section on Build tools on how to use the BOM module to provide consistent versions for all components.
40,3. API Overview
40,Jdbi’s API comes in two flavors:
40,3.1. Fluent API
40,"The Core API provides a fluent, imperative interface. Use Builder style objects to wire up your SQL to rich Java data types."
40,"Jdbi jdbi = Jdbi.create(""jdbc:h2:mem:test""); // (H2 in-memory database)"
40,List<User> users = jdbi.withHandle(handle -> {
40,"handle.execute(""CREATE TABLE \""user\"" (id INTEGER PRIMARY KEY, \""name\"" VARCHAR)"");"
40,// Inline positional parameters
40,"handle.execute(""INSERT INTO \""user\"" (id, \""name\"") VALUES (?, ?)"", 0, ""Alice"");"
40,// Positional parameters
40,"handle.createUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (?, ?)"")"
40,".bind(0, 1) // 0-based parameter indexes"
40,".bind(1, ""Bob"")"
40,.execute();
40,// Named parameters
40,"handle.createUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (:id, :name)"")"
40,".bind(""id"", 2)"
40,".bind(""name"", ""Clarice"")"
40,.execute();
40,// Named parameters from bean properties
40,"handle.createUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (:id, :name)"")"
40,".bindBean(new User(3, ""David""))"
40,.execute();
40,// Easy mapping to any type
40,"return handle.createQuery(""SELECT * FROM \""user\"" ORDER BY \""name\"""")"
40,.mapToBean(User.class)
40,.list();
40,});
40,assertThat(users).containsExactly(
40,"new User(0, ""Alice""),"
40,"new User(1, ""Bob""),"
40,"new User(2, ""Clarice""),"
40,"new User(3, ""David""));"
40,See the chapter introducing the core API for details about Jdbi’s fluent API.
40,3.2. Declarative API
40,"The SQL Object extension is an additional module, which provides a declarative API."
40,Define the SQL to execute and the shape of the results by creating
40,an annotated Java interface.
40,// Declare the API using annotations on a Java interface
40,public interface UserDao {
40,"@SqlUpdate(""CREATE TABLE \""user\"" (id INTEGER PRIMARY KEY, \""name\"" VARCHAR)"")"
40,void createTable();
40,"@SqlUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (?, ?)"")"
40,"void insertPositional(int id, String name);"
40,"@SqlUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (:id, :name)"")"
40,"void insertNamed(@Bind(""id"") int id, @Bind(""name"") String name);"
40,"@SqlUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES (:id, :name)"")"
40,void insertBean(@BindBean User user);
40,"@SqlQuery(""SELECT * FROM \""user\"" ORDER BY \""name\"""")"
40,@RegisterBeanMapper(User.class)
40,List<User> listUsers();
40,Then attach the interface to a Jdbi instance and execute the methods on the resuling class to execute the SQL queries.
40,"Jdbi jdbi = Jdbi.create(""jdbc:h2:mem:test"");"
40,jdbi.installPlugin(new SqlObjectPlugin());
40,// Jdbi implements your interface based on annotations
40,"List<User> userNames = jdbi.withExtension(UserDao.class, dao -> {"
40,dao.createTable();
40,"dao.insertPositional(0, ""Alice"");"
40,"dao.insertPositional(1, ""Bob"");"
40,"dao.insertNamed(2, ""Clarice"");"
40,"dao.insertBean(new User(3, ""David""));"
40,return dao.listUsers();
40,});
40,assertThat(userNames).containsExactly(
40,"new User(0, ""Alice""),"
40,"new User(1, ""Bob""),"
40,"new User(2, ""Clarice""),"
40,"new User(3, ""David""));"
40,"See the chapter on SQL objects for more details about the declarative API. The declarative API uses the fluent API ""under the hood"" and the two styles can be mixed."
40,4. Core API concepts
40,4.1. The Jdbi class
40,The Jdbi class is the main entry point into the library.
40,Each Jdbi instance maintains a set of configuration settings and wraps a JDBC DataSource.
40,Jdbi instances are thread-safe and do not own any database resources.
40,There are a few ways to create a Jdbi instance:
40,use a JDBC URL:
40,// H2 in-memory database
40,"Jdbi jdbi = Jdbi.create(""jdbc:h2:mem:test"");"
40,directly use a DataSource object which was created outside Jdbi.
40,DataSource ds = ...
40,Jdbi jdbi = Jdbi.create(ds);
40,from a custom class that implements ConnectionFactory.
40,"This allows for custom implementations of connection providers such as HA failover, proxy solutions etc."
40,from a single JDBC Connection.
40,"Please note that there is no magic. Any Handle or operation will use this one connection provided. If the connection is thread-safe, then multiple threads accessing the database will work properly, if the connection object is not thread-safe, then Jdbi will not be able to do anything about it."
40,Production code rarely provides a connection object directly to a Jdbi instance. It is useful for testing and debugging.
40,"Applications create a single, shared Jdbi instance per data source, and set up any common configuration there."
40,See Configuration for more details.
40,"Jdbi does not provide connection pooling or other High Availability features, but it can be combined with other software that does."
40,4.2. Handle
40,A Handle wraps an active database connection.
40,Handle instances are created by a Jdbi instance to provide a database connection e.g. for a single HTTP request or event callback). Handles are intended to be short-lived and must be closed to release the database connection and possible other resources.
40,"A Handle is used to prepare and run SQL statements against the database, and manage database transactions. It provides"
40,"access to fluent statement APIs that can bind arguments, execute the statement,"
40,and then map any results into Java objects.
40,A Handle inherits configuration from the Jdbi at the time it is created. See Configuration for more details.
40,"Handles and all attached query objects such as Batch, Call, Query, Script and Update should be used by a single thread and are not thread-safe."
40,They may be used by multiple threads as long as there is coordination that only one thread at a time is accessing them. Managing Handles and query objects across threads is error-prone and should be avoided.
40,4.2.1. Obtaining a managed handle
40,The most convenient way to get access to a handle is by using the withHandle or useHandle methods on the Jdbi class. These methods use callbacks and provide a fully managed handle that is correctly closed and all related resources are released. withHandle allows the callback to return a result while useHandle is just executing operations that do not need to return any value.
40,Providing a return value from a query using the Jdbi#withHandle() method:
40,List<String> names = jdbi.withHandle(handle ->
40,"handle.createQuery(""select name from contacts"")"
40,.mapTo(String.class)
40,.list());
40,"assertThat(names).contains(""Alice"", ""Bob"");"
40,Executing an operation using the Jdbi#useHandle() method:
40,jdbi.useHandle(handle -> {
40,"handle.execute(""create table contacts (id int primary key, name varchar(100))"");"
40,"handle.execute(""insert into contacts (id, name) values (?, ?)"", 1, ""Alice"");"
40,"handle.execute(""insert into contacts (id, name) values (?, ?)"", 2, ""Bob"");"
40,});
40,"You may notice the ""consumer"" vs ""callback"" naming pattern in a few"
40,"places in Jdbi. with- methods return a value and use objects named -Callback. use- methods do not return a value, and use objects named -Consumer. When referring to both type of objects, the term ""callback"" is used throughout the documentation."
40,The Nesting Callbacks with managed Handles and Transactions chapter has more information about nesting callbacks with managed handle objects.
40,4.2.2. Managing the Handle lifecycle manually
40,The Jdbi#open() method returns an unmanaged handle.
40,The Java try-with-resources construct can be used to manage the lifecycle of the handle:
40,try (Handle handle = jdbi.open()) {
40,"result = handle.execute(""insert into contacts (id, name) values (?, ?)"", 3, ""Chuck"");"
40,An unmanaged handle must be used when a stateful object should be passed to the calling code. Stateful objects are iterators and streams that do not collect data ahead of time in memory but provide a data stream from the database through the JDBC Connection to the calling code. Using a callback does not work here because the connection would be closed before the calling code can consume the data.
40,try (Handle handle = jdbi.open()) {
40,"Iterator<String> names = handle.createQuery(""select name from contacts where id = 1"")"
40,.mapTo(String.class)
40,.iterator();
40,assertThat(names).hasNext();
40,String name = names.next();
40,"assertThat(name).isEqualTo(""Alice"");"
40,assertThat(names).isExhausted();
40,"When using Jdbi#open(), you should consider using try-with-resource or a try-finally block to ensure the handle is closed and the database connection is released. Failing to release the handle will leak connections. It is recommended to use a managed handle whenever possible."
40,4.3. Statement types
40,Any database operation within Jdbi uses a statement. Multiple types of statements exist:
40,"Query - SQL statements that return results, e.g. a SELECT or any statement with a RETURNING clause. See createQuery"
40,"Update - SQL statements that return no value such as INSERT, UPDATE, DELETE or a DDL operation. See createUpdate"
40,Batch - a set of operations that are executed as a batch. See createBatch
40,Call - execute a stored procedure. See createCall
40,Script - a SQL script containing multiple statements separated by ; that is executed as a batch. See also createScript
40,Metadata - SQL Metadata access.
40,4.3.1. Queries
40,A Query is a
40,result-bearing SQL statement
40,that returns a result set from the database.
40,"List<Map<String, Object>> users ="
40,"handle.createQuery(""SELECT id, \""name\"" FROM \""user\"" ORDER BY id ASC"")"
40,.mapToMap()
40,.list();
40,assertThat(users).containsExactly(
40,"map(""id"", 1, ""name"", ""Alice""),"
40,"map(""id"", 2, ""name"", ""Bob""));"
40,There are many different methods to collect results from a query.
40,Use the ResultIterable#one() method returns when you expect the result to contain exactly one row. This method returns
40,null only if the returned row maps to null and throws an exception if the result has zero or multiple rows.
40,"String name = handle.select(""SELECT name FROM users WHERE id = ?"", 3)"
40,.mapTo(String.class)
40,.one();
40,Use the ResultIterable#findOne() method when you expect the result to contain zero or one row. Returns
40,"Optional.empty() if there are no rows, or one row that maps to null and throws"
40,an exception if the result has multiple rows.
40,Optional<String> name = handle.select(...)
40,.mapTo(String.class)
40,.findOne();
40,Use the ResultIterable#first() method when you expect the result to contain at least one row. Returns
40,null if the first row maps to null. and throws an exception if the result has
40,zero rows.
40,"String name = handle.select(""SELECT name FROM users WHERE id = ?"", 3)"
40,.mapTo(String.class)
40,.first();
40,Use the ResultIterable#findFirst() method when the result may contain any number of rows. Returns
40,"Optional.empty() if there are no rows, or the first row maps to null."
40,Optional<String> name = handle.select(...)
40,.mapTo(String.class)
40,.findFirst();
40,Multiple result rows can be returned in a list or a set:
40,List<String> names = this.handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.list();
40,Set<String> names = handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.set();
40,Use the ResultIterable#list() and ResultIterable#set() methods to use the default implementations from the collections framework.
40,"For more control over the set and list types, use the ResultIterable#collectIntoList() and ResultIterable#collectIntoSet() methods which use the JdbiCollectors configuration object to retrieve the collection types for List and Set:"
40,List<String> names = handle
40,".registerCollector(List.class, Collectors.toCollection(LinkedList::new))"
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.collectIntoList();
40,Set<String> names = handle
40,".registerCollector(Set.class, Collectors.toCollection(LinkedHashSet::new))"
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.collectIntoSet();
40,The
40,ResultIterable#collectInto() methods allows specifying a container type directly:
40,List<String> names = handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.collectInto(List.class); (1)
40,List<String> names = handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,".collectInto(GenericTypes.parameterizeClass(List.class, String.class)); (2)"
40,collect into a raw collection type
40,collect into a parameterized generic type
40,"For other collections, use ResultIterable#collect() with a"
40,collector or ResultIterable#toCollection() with a collection supplier:
40,Set<String> names = handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.collect(Collectors.toSet());
40,Set<String> names = handle
40,".createQuery(""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.toCollection(() -> new LinkedHashSet<>());
40,The ResultIterable#collectToMap() method allows collecting results directly into a Map:
40,"Map<Integer, String> names = handle"
40,".registerRowMapper(Movie.class, ConstructorMapper.of(Movie.class))"
40,".createQuery(""SELECT * FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(Movie.class)
40,".collectToMap(Movie::id, Movie::title);"
40,You can also stream results:
40,List<String> names = new ArrayList<>();
40,handle.createQuery(
40,"""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.useStream(stream -> stream.forEach(names::add)); (1)
40,List<String> names = new ArrayList<>();
40,try (Stream<String> stream = handle.createQuery(
40,"""SELECT title FROM films WHERE genre = :genre ORDER BY title"")"
40,".bind(""genre"", ""Action"")"
40,.mapTo(String.class)
40,.stream()) { (2)
40,stream.forEach(names::add);
40,ResultIterable#useStream() and ResultIterable#withStream() provide a callback with a managed stream object.
40,ResultIterable#stream() provides a stream that must be managed by user code
40,Equivalent methods for iterators exist as well.
40,"Thus far, all examples have shown a String result type. Of course, you can"
40,map to many other data types such as an integer:
40,int releaseDate = handle
40,".createQuery(""SELECT release_year FROM films WHERE title = :title"")"
40,".bind(""title"", ""The Dark Knight"")"
40,.mapTo(Integer.class)
40,.one();
40,"Or a complex, custom type:"
40,Movie movie = handle
40,".registerRowMapper(Movie.class, ConstructorMapper.of(Movie.class))"
40,".createQuery(""SELECT * FROM films WHERE id = :id"")"
40,".bind(""id"", 1)"
40,.mapTo(Movie.class)
40,.one();
40,4.3.2. Updates
40,"Updates are operations that return an integer number of rows modified, such"
40,"as a database INSERT, UPDATE, or DELETE."
40,"You can execute a simple update with Handle's int execute(String sql, Object…​ args)"
40,method which binds simple positional parameters.
40,"count = handle.execute(""INSERT INTO \""user\"" (id, \""name\"") VALUES(?, ?)"", 4, ""Alice"");"
40,assertThat(count).isOne();
40,"To further customize, use createUpdate:"
40,"int count = handle.createUpdate(""INSERT INTO \""user\"" (id, \""name\"") VALUES(:id, :name)"")"
40,".bind(""id"", 3)"
40,".bind(""name"", ""Charlie"")"
40,.execute();
40,assertThat(count).isOne();
40,Updates may return Generated Keys instead of a result count.
40,4.3.3. Batches
40,A Batch sends many commands to the server in bulk.
40,"After opening the batch, repeated add statements, and invoke add."
40,Batch batch = handle.createBatch();
40,"batch.add(""INSERT INTO fruit VALUES(0, 'apple')"");"
40,"batch.add(""INSERT INTO fruit VALUES(1, 'banana')"");"
40,int[] rowsModified = batch.execute();
40,"The statements are sent to the database in bulk, but each statement is executed separately."
40,There are no parameters.
40,"Each statement returns a modification count, as with an Update, and"
40,those counts are then returned in an int[] array.
40,In common cases all elements will be 1.
40,Some database drivers might return special values in conditions where modification counts are not available.
40,See the
40,executeBatch documentation for details.
40,4.3.4. Prepared Batches
40,A PreparedBatch sends one statement to the server with many argument sets.
40,"The statement is executed repeatedly, once for each batch of arguments that is add-ed to it."
40,The result is again a int[] of modified row count.
40,"PreparedBatch batch = handle.prepareBatch(""INSERT INTO \""user\"" (id, \""name\"") VALUES(:id, :name)"");"
40,for (int i = 100; i < 5000; i++) {
40,"batch.bind(""id"", i).bind(""name"", ""User:"" + i).add();"
40,int[] counts = batch.execute();
40,SqlObject also supports batch inserts:
40,public void testSqlObjectBatch() {
40,BasketOfFruit basket = handle.attach(BasketOfFruit.class);
40,int[] rowsModified = basket.fillBasket(Arrays.asList(
40,"new Fruit(0, ""apple""),"
40,"new Fruit(1, ""banana"")));"
40,"assertThat(rowsModified).containsExactly(1, 1);"
40,assertThat(basket.countFruit()).isEqualTo(2);
40,public interface BasketOfFruit {
40,"@SqlBatch(""INSERT INTO fruit VALUES(:id, :name)"")"
40,int[] fillBasket(@BindBean Collection<Fruit> fruits);
40,"@SqlQuery(""SELECT count(1) FROM fruit"")"
40,int countFruit();
40,"Batching dramatically increases efficiency over repeated single statement execution, but"
40,many databases don’t handle extremely large batches well either.
40,Test with your database
40,"configuration, but often extremely large data sets should be divided"
40,and committed in pieces - or risk bringing your database to its knees.
40,Exception Rewriting
40,The JDBC SQLException class is very old and predates more modern exception
40,"facilities like Throwable’s suppressed exceptions. When a batch fails, there"
40,"may be multiple failures to report, which could not be represented by the base"
40,Exception types of the day.
40,So SQLException has a bespoke
40,getNextException
40,"chain to represent the causes of a batch failure. Unfortunately, by default"
40,"most logging libraries do not print these exceptions out, pushing their"
40,handling into your code. It is very common to forget to handle this situation
40,and end up with logs that say nothing other than
40,"java.sql.BatchUpdateException: Batch entry 1 INSERT INTO something (id, name) VALUES (0, '') was aborted. Call getNextException to see the cause."
40,"Jdbi will rewrite such exceptions into ""suppressed exceptions"" so that your logs are more helpful:"
40,"java.sql.BatchUpdateException: Batch entry 1 INSERT INTO something (id, name) VALUES (0, 'Keith') was aborted. Call getNextException to see the cause."
40,"Suppressed: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""something_pkey"""
40,Detail: Key (id)=(0) already exists.
40,4.3.5. Stored Procedure Calls
40,A Call invokes a database stored procedure. Stored procedures execute on the database and take input and output parameters.
40,Most databases solely support input and output parameters. Input parameters can be mapped using the standard Jdbi
40,"mechanisms for parameters, while output values are returned using OutParameters objects."
40,Some databases do not support all SQL types as out parameters but use a mechanism similar to a query and
40,return a ResultSet object.
40,Using OutParameters to access database return values
40,OutParameters hold the output parameters of a stored procedure Call.
40,"Since they can hold result sets and cursors, OutParameters must be consumed before"
40,closing the Call that it came from.
40,This is an example for a PostgreSQL stored procedure. PostgreSQL (and most other databases) stored procedures use IN parameters to pass values into the procedure and OUT parameters to return values.
40,"CREATE FUNCTION add(a IN INT, b IN INT, sum OUT INT) AS $$"
40,BEGIN
40,sum := a + b;
40,END;
40,$$ LANGUAGE plpgsql
40,Here’s how to call a stored procedure:
40,"try (Call call = handle.createCall(""{:sum = call add(:a, :b)}"")) { (1)"
40,"OutParameters result = call.bind(""a"", 13) (2)"
40,".bind(""b"", 9) (2)"
40,".registerOutParameter(""sum"", Types.INTEGER)"
40,(3) (4)
40,.invoke(); (5)
40,Call Handle.createCall() with the SQL statement. Note that JDBC has a
40,"peculiar SQL format when calling stored procedures, which we must follow."
40,Bind input parameters to the procedure call.
40,"Register out parameters, the values that will be returned from the stored"
40,procedure call. This tells JDBC what data type to expect for each out
40,parameter.
40,Out parameters may be registered by name (as shown in the example) or by
40,"zero-based index, if the SQL is using positional parameters. Multiple output"
40,"parameters may be registered, depending on the output of the stored"
40,procedure itself.
40,"Finally, call invoke() to execute the procedure."
40,Invoking the stored procedure returns an
40,"OutParameters object, which"
40,contains the value(s) returned from the stored procedure call and the results can
40,be extracted from it:
40,"int sum = result.getInt(""sum"");"
40,It is possible to access open cursors as a result set objects by declaring them
40,as Types.REF_CURSOR and then using OutParameters#getRowSet() to access them as ResultBearing objects. This requires
40,support from the JDBC driver for the database and is not supported by all databases. Please consult the documentation for the database driver first before filing a bug.
40,Results must be consumed before closing the statement because closing it will also close all resources and result sets associated with it.
40,The OutParameters object can either be returned from the Call using Call#invoke() or it can be processed as
40,a consumer argument with Call#invoke(Consumer) or a function argument with
40,Call#invoke(Function).
40,"Due to design constraints within JDBC, the parameter data types available"
40,through OutParameters are limited to those types supported directly by JDBC. This cannot be expanded through e.g. mapper registration.
40,Using a ResultSet for procedure output values
40,Some database drivers might return a ResultSet containing the results of a stored procedure. This result set is available as a ResultBearing object by calling OutParameters#getResultSet().
40,"While returning a result set from a procedure call is supported in the JDBC standard, it is pretty uncommon and only very few databases support it. The most prominent one is the JDBC driver for MS SQLServer. Most databases will either return null or an empty result set; in this case the OutParameters#getResultSet() will return an empty ResultBearing object."
40,This is the equivalent procedure from above for MS SQLServer using a result set:
40,CREATE PROCEDURE mssql_add
40,"@a INT,"
40,@b INT
40,BEGIN
40,SELECT @a + @b;
40,END
40,"To access data from this procedure, it retrieves the result set using OutParameters#getResultSet():"
40,"try (Call call = h.createCall(""{call mssql_add(:a, :b)}"")) {"
40,"call.bind(""a"", 13)"
40,".bind(""b"", 9);"
40,OutParameters output = call.invoke();
40,int sum = output.getResultSet() (1)
40,.mapTo(Integer.class) (2)
40,.one();
40,assertThat(sum).isEqualTo(22);
40,Retrieve the output from the procedure using a result set.
40,Map the first column in the result set to an integer value.
40,"It may not be necessary to use a result set if the JDBC driver also supports OUT parameters. However, e.g. MS SQL Server does not allow Types.REF_CURSOR as OUT parameters and requires the use of a result set."
40,"It is possible to mix OUT parameters and a result set. In this case, the result set should be consumed before accessing any out parameters as the database driver might invalidate the result set when accessing out parameters."
40,4.3.6. Scripts
40,A Script parses a String into semicolon terminated statements. The statements
40,can be executed in a single Batch or individually.
40,"try (Script script = handle.createScript(""INSERT INTO \""user\"" VALUES(3, 'Charlie');"""
40,"+ ""UPDATE \""user\"" SET \""name\""='Bobby Tables' WHERE id=2;"")) {"
40,int[] results = script.execute();
40,"assertThat(results).containsExactly(1, 1);"
40,"Script parsing uses a generic SQL parser and creates separate statements. Depending on the database and its driver, there are small differences in how the script needs to be parsed. The most notorious issue is whether the parsed statements need to retain a trailing semicolon or not. Some databases (e.g. Oracle) require the trailing semicolon while others (e.g. MySQL) actually report a syntax error if it is present. As we can not please everyone equally, there is the setScriptStatementsNeedSemicolon() switch, which controls this behavior. By default, Jdbi retains the semicolon if present."
40,Turning off the trailing semicolons for databases that have problems with it:
40,jdbi.withHandle(h -> {
40,// turn off trailing semicolons for script statements
40,h.getConfig(SqlStatements.class).setScriptStatementsNeedSemicolon(false);
40,return h.createScript(ClasspathSqlLocator.removingComments()
40,".getResource(""scripts/mysql-script.sql""))"
40,.execute();
40,});
40,4.3.7. Metadata
40,Jdbi allows access to the Database Metadata through queryMetadata methods on the Handle.
40,Simple values can be queried directly using a method reference:
40,String url = h.queryMetadata(DatabaseMetaData::getURL);
40,boolean supportsTransactions = h.queryMetadata(DatabaseMetaData::supportsTransactions);
40,Many methods on the DatabaseMetaData return a ResultSet. These can be used with the queryMetadata method that returns a ResultBearing.
40,All Jdbi Row Mappers and Column Mappers are available to map these results:
40,List<String> catalogNames = h.queryMetadata(DatabaseMetaData::getCatalogs)
40,.mapTo(String.class)
40,.list();
40,4.4. Usage in asynchronous applications
40,Calling Jdbc and by extension Jdbi is inherently a blocking operation.
40,In asynchronous applications
40,(where results are returned as a CompletionStage
40,or a CompletableFuture)
40,"it is very important never to block on threads that the caller doesn’t control. For this, there is the"
40,JdbiExcecutor class that wraps around a Jdbi instance
40,and makes sure calls are made on a specific thread pool.
40,"To create a JdbiExcecutor instance, we first need to create"
40,a Jdbi instance (see The Jdbi class). Then pass that instance into
40,JdbiExecutor.create():
40,Executor executor = Executors.newFixedThreadPool(8);
40,"JdbiExecutor jdbiExecutor = JdbiExecutor.create(h2Extension.getJdbi(), executor);"
40,It is important to size the executor to the specific implementation.
40,See here for some hints.
40,"To use the jdbiExecutor, we make similar calls to JdbiExcecutor"
40,as we do to Jdbi
40,CompletionStage<Void> futureResult = jdbiExecutor.useHandle(handle -> {
40,"handle.execute(""insert into contacts (id, name) values (?, ?)"", 3, ""Erin"");"
40,});
40,// wait for stage to complete (don't do this in production code!)
40,futureResult.toCompletableFuture().join();
40,"assertThat(h2Extension.getSharedHandle().createQuery(""select name from contacts where id = 3"")"
40,.mapTo(String.class)
40,".list()).contains(""Erin"");"
40,CompletionStage<List<String>> futureResult = jdbiExecutor.withHandle(handle -> {
40,"return handle.createQuery(""select name from contacts where id = 1"")"
40,.mapTo(String.class)
40,.list();
40,});
40,assertThat(futureResult)
40,.succeedsWithin(Duration.ofSeconds(10))
40,.asList()
40,".contains(""Alice"");"
40,CompletionStage<String> futureResult =
40,"jdbiExecutor.withExtension(SomethingDao.class, dao -> dao.getName(1));"
40,assertThat(futureResult)
40,.succeedsWithin(Duration.ofSeconds(10))
40,".isEqualTo(""Alice"");"
40,"Note, since the handle closes as soon as the callback returns, you cannot return"
40,"an iterator, since iteration will call upon the (now closed) handle"
40,CompletionStage<Iterator<String>> futureResult = jdbiExecutor.withHandle(handle -> {
40,"return handle.createQuery(""select name from contacts where id = 1"")"
40,.mapTo(String.class)
40,.iterator();
40,});
40,// wait for stage to complete (don't do this in production code!)
40,Iterator<String> result = futureResult.toCompletableFuture().join();
40,// result.hasNext() fails because the handle is already closed at this point
40,assertThatException().isThrownBy(() -> result.hasNext());
40,5. Resource Management
40,"JDBC operations involve stateful objects: Connection, PreparedStatement and ResultSet are the most common ones. Jdbi understands the lifecycle of these objects and can often fully manage them."
40,"Within Jdbi, two types of stateful objects exist that need to be managed: Handle objects and all Statement objects."
40,Manual resource management for Jdbi objects uses the standard Java try-with-resources construct:
40,try (Handle handle = jdbi.open();
40,"Query query = handle.createQuery(""SELECT * from users"")) {"
40,List<User> users = query.mapTo(User.class).list();
40,5.1. Automatic resource management
40,"Jdbi can manage and release resources automatically, so that try-with-resources blocks are often not needed."
40,"Automatic resource management is only available with the SQL Object API and the Extension API. In these situations, Jdbi can manage all resources:"
40,with methods on a SQL object that was created with Jdbi#onDemand():
40,UserDao userDao = jdbi.onDemand(UserDao.class);
40,User user = userDao.getUser(id);
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users WHERE id = :id"")"
40,@UseRowMapper(UserMapper.class)
40,"User getUser(@Bind(""id"") int id);"
40,with the Jdbi withExtension and useExtension callbacks:
40,"User user = jdbi.withExtension(UserDao.class, dao -> dao.getUser(id));"
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users WHERE id = :id"")"
40,@UseRowMapper(UserMapper.class)
40,"User getUser(@Bind(""id"") int id);"
40,5.2. Handle resource management
40,"Any time a handle is explicitly opened, it must be managed by the calling code. A try-with-resources block is the best way to do this:"
40,try (Handle handle = jdbi.open()) {
40,// handle operations here
40,"Closing the handle releases the connection that is managed by the handle. Handle resource management is only necessary when using the various Jdbi#open() methods on the Jdbi object. Any handle that is passed in through a callback (Jdbi#withHandle(), Jdbi#useHandle(), Jdbi#inTransaction(), Jdbi#useTransaction()) is managed through Jdbi:"
40,Jdbi is still able to manage statement resources even if the handle is managed manually:
40,try (Handle handle = jdbi.open()) { (1)
40,UserDao dao = handle.attach(UserDao.class);
40,User user = userDao.getUser(id);
40,User user = jdbi.withHandle(handle -> { (2)
40,UserDao dao = handle.attach(UserDao.class);
40,return userDao.getUser(id);
40,});
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users WHERE id = :id"")"
40,@UseRowMapper(UserMapper.class)
40,"User getUser(@Bind(""id"") int id);"
40,Jdbi.open() creates a new Handle that must be managed through a try-with-resources block.
40,jdbi.withHandle() passes a managed Handle to the callback and does not require management.
40,5.3. Statement resource management
40,All statement types may use resources that need to be released. There are multiple ways to do; each has its advantages and drawbacks:
40,Explicit statement resource management uses code to manage and release resources
40,Implicit statement resource cleanup relies on the Jdbi framework to release the resources when necessary
40,Attaching statements to the handle lifecycle delegates to the Handle lifecycle
40,5.3.1. Explicit statement resource management
40,"All SQL statement types implement the AutoCloseable interface and can be used in a try-with-resources block. This is the recommended way to manually manage statements, and it ensures that all resources are properly released when they are no longer needed. The try-with-resources pattern ensures that the close() method on the SQL statement is called which releases the resources."
40,"Create, execute and release 100,000 update statements:"
40,try (Handle handle = jdbi.open()) {
40,for (int i = 0; i < 100_000; i++) {
40,"try (Update update = handle.createUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")) {"
40,"update.bind(""id"", i)"
40,".bind(""name"", ""user_"" + i)"
40,.execute();
40,"It is not necessary to use a try-with-resources block, the close() method can be called manually as well:"
40,try (Handle handle = jdbi.open()) {
40,for (int i = 0; i < 100_000; i++) {
40,"Update update = handle.createUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"");"
40,try {
40,"update.bind(""id"", i)"
40,".bind(""name"", ""user_"" + i)"
40,.execute();
40,} finally {
40,update.close();
40,"Explicit statement resource management, while a bit more involved ensures that all resources are always correctly released, especially when there are exceptions thrown during statement execution. While many examples and demo code omit managing the statement types, it is strongly recommended to do so."
40,5.3.2. Implicit statement resource cleanup
40,"If possible, Jdbi will help with resource management through implicit resource cleanup."
40,If a SQL statement operation succeeds
40,"and all data returned by the database was consumed, then Jdbi will release the allocated resources even if the statement is used without a try-with-resources block."
40,"All terminal operations on the ResultIterable interface, except for the stream() and iterator() methods, consume all data from the database, even if they return only a subset (e.g. findOne()). The same applies for operations such as Update or Batch."
40,"For all other operations, the resources will be released if the operation succeeds and does not throw an exception."
40,try (Handle handle = jdbi.open()) {
40,List<User> users = handle
40,".createQuery(""SELECT * from users"") (1)"
40,.mapTo(User.class) (2)
40,.list(); (3)
40,creates a Query object
40,returns an object implementing ResultIterable
40,is a terminal operation that consumes all the data from the database
40,"As long as no exception is thrown, this example above will release all resources allocated by the Query object, even though it is not managed."
40,"This code that works well as long as nothing goes awry! If an unmanaged SQL statement operation fails halfway through (e.g. with a connection timeout or a database problem), then resources will not be cleanup up by Jdbi. This may lead to resource leaks that are difficult to find or reproduce. Some JDBC drivers will also clean up resources that they hand out (such as JDBC statements"
40,"or ResultSet objects) when a connection is closed. In these situations, resources may be released eventually when the handle is closed (which also closes the JDBC connection). This is highly driver dependent and should not be relied upon."
40,5.3.3. Resources with Streams and Iterators
40,"Most methods on objects that implement ResultIterable are terminal. They consume all data returned by the database and will release statement resources at the end. All of these methods may buffer results in memory, something that is not acceptable for very large data sets or when data needs to be streamed."
40,Just using an iterator or stream may not be enough to actually stream data without buffering it in memory. Often the JDBC driver might still buffer data. See the documentation for your database on how to ensure that the driver does not buffer all the results in memory.
40,"For these use cases, Jdbi offers the stream() and iterator() methods, which return lazy-loading objects that return data from the database row by row. These objects are stateful and must be managed."
40,The stream and iterator objects provided by Jdbi will release all their resources when the data stream from the database is exhausted or when an instance is explicitly closed. Standard Java Streams support the try-with-resources pattern directly; the iterator() method returns a ResultIterator object which extends AutoCloseable.
40,"When using a stream or iterator, either the statement or the result object must be managed. Calling close() on either the statement or the stream/iterator will terminate the operation and release all resources."
40,// managing resources through the query SQL statement
40,try (Handle handle = jdbi.open()) {
40,"try (Query query = handle.createQuery(""SELECT * from users"")) { (1)"
40,Stream<User> users = query.mapTo(User.class).stream();
40,// consume the stream
40,// managing resources through the stream
40,try (Handle handle = jdbi.open()) {
40,"try (Stream<User> users = handle.createQuery(""SELECT * from users"")"
40,.mapTo(User.class)
40,.stream()) { (2)
40,// consume the stream
40,use a try-with-resources block with the Query object releases the resources and closes the stream when the block is left
40,use a try-with-resources block with the stream releases the resources and closes the stream when the block is left
40,The same pattern applies to a result iterator.
40,"Streams and iterators are cursor-type ""live"" objects. They require an active statement and result set. Any operation that closes and releases these resources will cause future operations on the stream or iterator to fail."
40,The following code does not work:
40,Stream<User> userStream = jdbi.withHandle(handle ->
40,"handle.createQuery(""SELECT * from users"")"
40,.mapTo(User.class)
40,.stream();
40,(1)
40,});
40,(2)
40,Optional<User> user = stream.findFirst(); (3)
40,Creates a stream object backed by the open result set of the query
40,leaving the callback closes the handle and releases all resources including the result set
40,throws an exception because the result set has already been closed
40,"This can be avoided by using one of the callback methods that ResultIterable offers: withIterator(), useIterator(), withStream(), useStream(). Each of these methods manages the iterator / stream through Jdbi and allow consumption of its contents within a callback:"
40,try (Handle handle = jdbi.open()) {
40,"try (Query query = handle.createQuery(""SELECT * from users"")) {"
40,query.mapTo(User.class).useIterator(it -> {
40,// consume the iterator contents
40,});
40,"While it is not strictly necessary to execute the query in a try-with-resources block because the useIterator() method will close the iterator and release all resources from the query, it is good practice and guards against possible errors that may happen between the creation of the query and the callback execution."
40,Using a jdbi callback allows fully automated resource management:
40,jdbi.useHandle(handle -> {
40,"handle.createQuery(""SELECT * from users"")"
40,.mapTo(User.class)
40,.useIterator(it -> {
40,// consume the iterator contents
40,});
40,});
40,5.3.4. OutParameters return values for stored procedures
40,Interacting with stored procedures generally involves passing values back and forth from database. Jdbi uses OutParameters objects which are returned from the various invoke methods of the Call object.
40,"When using OutParameters with stored procedures, the Call statement must be managed. Calling close() on the statement terminates the operation and release all resources."
40,Closing the stream or iterator associated with an object returned by OutParameters#getResultSet() or OutParameters#getRowSet() is not sufficient as the OutParameters object may hold multiple result sets.
40,"try (Call call = h.createCall(""{call some_procedure()}"")) {"
40,OutParameters output = call.invoke();
40,"output.getRowSet(""result"") (1)"
40,.mapTo(SomeType.class) (2)
40,.useStream(stream -> { ... consume stream ... }); (3)
40,Retrieve the cursor for the result parameter
40,Map the results onto a data type using a row mapper
40,Consume the stream of objects with a consumer argument.
40,"When the stream terminates, the Call statement is not closed as it may have returned multiple cursor parameters. It must be managed using try-with-resources as shown above."
40,"OutParameters objects are cursor-type ""live"" objects. They require an active statment and any operation that closes and releases the statement will cause future operations on a related OutParameters object to fail."
40,The following code does not work:
40,// DOES NOT WORK!
40,OutParameters out
40,= jdbi.withHandle(handle -> {
40,"Call call = handle.createCall(""{call some_procedure()}"");"
40,return call.invoke(); (1)
40,}); (2)
40,"String result = out.getString(""result""); (3)"
40,Creates an OutParameter object and returns it from withHandle.
40,Leaving the callback closes the handle and releases all resources including the statement.
40,throws an exception because closing the handle and the statement invalidates the OutParameters object.
40,This can be avoided by either processing the OutParameters object within the handle callback or by using either the Call#invoke(Consumer) or
40,Call#invoke(Function) callback methods:
40,// process the OutParameters within the handle callback
40,String result
40,= jdbi.withHandle(handle -> {
40,"Call call = handle.createCall(""{call some_procedure()}"");"
40,OutParameters out = call.invoke();
40,"return out.getString(""result"");"
40,});
40,// use a callback method from the Call statement
40,String result
40,= jdbi.withHandle(handle -> {
40,"Call call = handle.createCall(""{call some_procedure()}"");"
40,"return call.invoke(out -> out.getString(""result""));"
40,});
40,5.3.5. Attaching statements to the handle lifecycle
40,"An elegant way to manage resources is attaching the statement to the handle lifecycle. Calling the attachToHandleForCleanup() method, which is available on SQL statements, associates the statement with the handle. When the handle is closed, all attached statements that have not been cleaned up yet, will also be released."
40,"Optional<User> user = handle.createQuery(""SELECT * FROM users WHERE id = :id"")"
40,.attachToHandleForCleanup() (1)
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.stream()
40,.findAny();
40,"By attaching the statement to the handle, all resources are now managed by Jdbi and released even if the operation throws an exception."
40,"This works best for short-lived handles with a few statement operations. As the resources may not be released until the handle is closed, executing a large number of SQL statements that all may need cleaning up could lead to resource exhaustion. When in doubt, prefer the try-with-resources construct over attaching a SQL statements to the handle."
40,It is possible to attach all created statements by default to a handle by calling setAttachAllStatementsForCleanup(true) on the SqlStatements config object:
40,jdbi.getConfig(SqlStatements.class)
40,.setAttachAllStatementsForCleanup(true);
40,try (Handle handle = jdbi.open()) {
40,"Optional<User> user = handle.createQuery(""SELECT * FROM users WHERE id = :id"")"
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.stream()
40,.findAny();
40,This setting can be controlled either per Jdbi object or per Handle.
40,try (Handle handle = jdbi.open()) {
40,handle.getConfig(SqlStatements.class)
40,.setAttachAllStatementsForCleanup(true);
40,"Optional<User> user = handle.createQuery(""SELECT * FROM users WHERE id = :id"")"
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.stream()
40,.findAny();
40,"As a special case, any method on the Jdbi object that takes a callback (withHandle, useHandle, inTransaction, useTransaction) attaches statements created in the callback by default to the handle passed into the callback. These handles are fully managed and will be closed when the callback exits."
40,User user = jdbi.withHandle(handle ->
40,"handle.createQuery(""SELECT * FROM users WHERE id = :id"")"
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.one());
40,This behavior can be controlled through the setAttachCallbackStatementsForCleanup() method.
40,"If a callback executes a huge number of statements, it may be preferable to manage resources manually:"
40,jdbi.useHandle(handle -> {
40,handle.getConfig(SqlStatements.class).setAttachCallbackStatementsForCleanup(false); (1)
40,for (int i = 0; i < 100_000; i++) {
40,"try (Update update = handle.createUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")) {"
40,"update.bind(""id"", i)"
40,".bind(""name"", ""user_"" + i)"
40,.execute();
40,});
40,Turn off automatic attachment of statements to the handle because the callback creates a large number of statements. Modifying this value only changes the setting for the current handle; it does not affect the global setting associated with the Jdbi object that created the handle.
40,"While the attachAllStatementsForCleanup setting affects all statements created outside a Jdbi callback (withHandle, useHandle, inTransaction, useTransaction), inside these callbacks, the behavior is controlled by the attachCallbackStatementsForCleanup setting."
40,The default value for the attachAllStatementsForCleanup is false while the default value for attachCallbackStatementsForCleanup is true.
40,6. Arguments
40,Arguments are Jdbi’s representation of JDBC statement parameters (the ? in SELECT * FROM Foo WHERE bar = ?).
40,"To set a parameter ? on a JDBC PreparedStatement, you would ps.setString(1, ""Baz"")."
40,"With Jdbi, when you bind the string ""Baz"", it will search through all registered ArgumentFactory instances"
40,until it finds one that is willing to convert the String into an Argument.
40,The argument is responsible for setting the String for the placeholder exactly as setString does.
40,Arguments can perform more advanced bindings than simple JDBC supports:
40,"a BigDecimal could be bound as a SQL decimal, a java.time.Year as a SQL int,"
40,or a complex object could be serialized to a byte array and bound as a SQL blob.
40,The use of Jdbi arguments is limited to JDBC prepared statement parameters.
40,"Notably, arguments usually cannot be used to change the structure of a query"
40,"(for example the table or column name, SELECT or INSERT, etc.)"
40,nor may they be interpolated into string literals.
40,See Query Templating and TemplateEngine for more information.
40,6.1. Positional Arguments
40,"When a SQL statement uses ? tokens, Jdbi can bind a values to parameters"
40,at the corresponding index (0-based):
40,"handle.createUpdate(""insert into contacts (id, name) values (?, ?)"")"
40,".bind(0, 3)"
40,".bind(1, ""Chuck"")"
40,.execute();
40,"String name = handle.createQuery(""select name from contacts where id = ?"")"
40,".bind(0, 3)"
40,.mapTo(String.class)
40,.one();
40,6.2. Named Arguments
40,"When a SQL statement uses colon-prefixed tokens like :name, Jdbi can bind"
40,parameters by name:
40,"handle.createUpdate(""insert into contacts (id, name) values (:id, :name)"")"
40,".bind(""id"", 3)"
40,".bind(""name"", ""Chuck"")"
40,.execute();
40,"String name = handle.createQuery(""select name from contacts where id = :id"")"
40,".bind(""id"", 3)"
40,.mapTo(String.class)
40,.one();
40,A parameter or attribute name can contain any valid Java identifier characters and the dot (.).
40,"By default, Jbdi uses the see the ColonPrefixSqlParser that understands"
40,the
40,:foo syntax. This can be changed if the colon is problematic in the SQL dialect used. Jdbi includes support for an alternate #foo syntax out-of-the-box using the HashPrefixSqlParser. It is also possible to create custom parsers for named arguments.
40,"Mixing named and positional arguments is not allowed, as it would become confusing very quickly."
40,6.3. Supported Argument Types
40,"Out of the box, Jdbi supports the following types as SQL statement arguments:"
40,"Primitives: boolean, byte, short, int, long, char, float, and"
40,double
40,"java.lang: Boolean, Byte, Short, Integer, Long, Character,"
40,"Float, Double, String, and Enum (stored as the enum value’s name by default)"
40,java.math: BigDecimal
40,"java.net: Inet4Address, Inet6Address, URL, and URI"
40,"java.sql: Blob, Clob, Date, Time, and Timestamp"
40,"java.time: Instant, LocalDate, LocalDateTime, LocalTime,"
40,"OffsetDateTime, ZonedDateTime, and ZoneId"
40,"java.util: Date, Optional (around any other supported type), and UUID"
40,java.util.Collection and Java arrays (stored as SQL arrays).
40,Some additional setup may be required depending on the type of array element.
40,The binding and mapping method for enum values
40,"can be controlled via the Enums config,"
40,"as well as the annotations @EnumByName,"
40,"@EnumByOrdinal, and"
40,@DatabaseValue.
40,You can also configure Jdbi to support additional argument types.
40,More on that later.
40,6.4. Binding Arguments
40,Arguments to SQL statement can be bound in a few different ways.
40,You can bind individual arguments:
40,"handle.createUpdate(""INSERT INTO contacts (id, name) VALUES (:id, :name)"")"
40,".bind(""id"", 1)"
40,".bind(""name"", ""Alice"")"
40,.execute();
40,You can bind multiple arguments at once from the entries of a Map:
40,"Map<String, Object> contact = new HashMap<>();"
40,"contact.put(""id"", 2)"
40,"contact.put(""name"", ""Bob"");"
40,"handle.createUpdate(""INSERT INTO contacts (id, name) VALUES (:id, :name)"")"
40,.bindMap(contact)
40,.execute();
40,"You can bind multiple values at once, from either a List<T> or a vararg:"
40,List<String> keys = new ArrayList<String>()
40,"keys.add(""user_name"");"
40,"keys.add(""street"");"
40,"handle.createQuery(""SELECT value FROM items WHERE kind in (<listOfKinds>)"")"
40,".bindList(""listOfKinds"", keys)"
40,.mapTo(String.class)
40,.list();
40,"// or, using the 'vararg' definition"
40,"handle.createQuery(""SELECT value FROM items WHERE kind in (<varargListOfKinds>)"")"
40,".bindList(""varargListOfKinds"", ""user_name"", ""docs"", ""street"", ""library"")"
40,.mapTo(String.class)
40,.list();
40,"Using bindList requires writing your SQL with an attribute, not a binding,"
40,despite the fact that your values are bound. The attribute is a placeholder that will be
40,safely rendered to a comma-separated list of binding placeholders.
40,You can bind multiple arguments from properties of a Java Bean:
40,Contact contact = new Contact();
40,contact.setId(3);
40,"contact.setName(""Cindy"");"
40,"handle.createUpdate(""INSERT INTO contacts (id, name) VALUES (:id, :name)"")"
40,.bindBean(contact)
40,.execute();
40,You can also bind an Object’s public fields:
40,Object contact = new Object() {
40,public int id = 0;
40,"public String name = ""Cindy"";"
40,"handle.createUpdate(""INSERT INTO contacts (id, name) VALUES (:id, :name)"")"
40,.bindFields(contact)
40,.execute();
40,"Alternatively, you can bind public, parameter-less methods of an Object:"
40,Object contact = new Object() {
40,public int theId() {
40,return 0;
40,public String theName() {
40,"return ""Cindy"";"
40,"handle.createUpdate(""INSERT INTO contacts (id, name) VALUES (:theId, :theName)"")"
40,.bindMethods(contact)
40,.execute();
40,"Optionally, you can qualify each bound bean/object with a prefix. This can help"
40,remove ambiguity in situations where two or more bound beans have similar
40,property names:
40,"Folder folder = new Folder(1, ""Important Documents"");"
40,Document document =
40,"new Document(100, ""memo.txt"", ""Business business business. Numbers."");"
40,"handle.createUpdate(""INSERT INTO documents (id, folder_id, name, contents) "" +"
40,"""VALUES (:d.id, :f.id, :d.name, :d.contents)"")"
40,".bindBean(""f"", folder)"
40,".bindMethods(""f"", folder)"
40,".bindFields(""d"", document)"
40,.execute();
40,"bindBean(), bindFields(), and bindMethods() may be used to bind nested"
40,"properties, e.g. :user.address.street."
40,bindMap() does not bind nested properties—​map keys are expected to exactly
40,match the bound parameter name.
40,The authors recommend checking out Immutables support for an advanced way
40,to easily bind and map value types.
40,6.5. Argument Annotations
40,"@JdbiProperty(bind=false) allows configuring whether a discovered property is bound as an argument. Turn the bind annotation value off, and the property discovery mechanism will ignore it."
40,6.6. Custom Arguments
40,Occasionally your data model will use data types not natively supported by
40,Jdbi (see Supported Argument Types).
40,"Fortunately, Jdbi can be configured to bind custom data types as arguments,"
40,by implementing a few simple interfaces.
40,Core JDBC features are generally well-supported by all database vendors.
40,"However, more advanced usages like array support or geometry types tend to"
40,quickly become vendor-specific.
40,6.6.1. Using the Argument interface
40,The Argument interface wraps a
40,single value into a binding.
40,static class UUIDArgument implements Argument {
40,private UUID uuid;
40,UUIDArgument(UUID uuid) {
40,this.uuid = uuid;
40,@Override
40,"public void apply(int position, PreparedStatement statement, StatementContext ctx) throws SQLException {"
40,"statement.setString(position, uuid.toString()); (1)"
40,@Test
40,public void uuidArgument() {
40,UUID u = UUID.randomUUID();
40,"assertThat(handle.createQuery(""SELECT CAST(:uuid AS VARCHAR)"")"
40,".bind(""uuid"", new UUIDArgument(u))"
40,.mapTo(String.class)
40,.one()).isEqualTo(u.toString());
40,"Since Argument usually directly calls into JDBC directly, it is given the"
40,one-based index (as expected by JDBC) when it is applied.
40,"Here we use an Argument to directly bind a UUID. In this particular case,"
40,the most obvious approach is to send the UUID to the database as a String. If
40,"your JDBC driver supports custom types directly or efficient binary transfers,"
40,you can leverage them easily here.
40,6.6.2. Using the ArgumentFactory interface
40,The ArgumentFactory
40,"interface provides Argument interface instances for any data type it knows about. By implementing and registering an argument factory, it is possible to bind custom data types without having to explicitly wrap them in objects implementing the Argument interface."
40,Jdbi provides an AbstractArgumentFactory class which simplifies implementing
40,the ArgumentFactory contract:
40,static class UUIDArgumentFactory extends AbstractArgumentFactory<UUID> {
40,UUIDArgumentFactory() {
40,super(Types.VARCHAR); (1)
40,@Override
40,"protected Argument build(UUID value, ConfigRegistry config) {"
40,"return (position, statement, ctx) -> statement.setString(position, value.toString()); (2)"
40,@Test
40,public void uuidArgumentFactory() {
40,UUID u = UUID.randomUUID();
40,handle.registerArgument(new UUIDArgumentFactory());
40,"assertThat(handle.createQuery(""SELECT CAST(:uuid AS VARCHAR)"")"
40,".bind(""uuid"", u)"
40,.mapTo(String.class)
40,.one()).isEqualTo(u.toString());
40,The JDBC SQL type constant to use when
40,binding UUIDs. Jdbi needs this in order to bind UUID values of null. See
40,"PreparedStatement.setNull(int,int)"
40,"Since Argument is a functional interface, it can be implemented as a lambda expression."
40,6.6.3. Using Prepared Arguments for batches
40,Traditional argument factories decide to bind based on both the type and actual value of the binding.
40,This is very flexible but when binding a large PreparedBatch it incurs a serious performance penalty
40,as the entire chain of argument factories must be consulted for each batch of arguments added.
40,"To address this issue, implement ArgumentFactory.Preparable which promises to handle all values"
40,of a given Type.
40,Most built in argument factories now implement the Preparable interface.
40,Preparable argument factories are consulted before traditional argument factories. If you’d prefer
40,"to keep the old behavior, you may disable this feature with getConfig(Arguments.class).setPreparedArgumentsEnabled(false)."
40,6.6.4. The Arguments Registry
40,"When you register an ArgumentFactory, the registration is stored in an"
40,Arguments instance held by Jdbi.
40,"Arguments is a configuration class, which stores all registered argument"
40,factories (including the factories for built-in arguments).
40,"Under the hood, when you bind arguments to a statement, Jdbi consults the"
40,Arguments config object and searches for an ArgumentFactory which knows how
40,to convert a bound object into an Argument.
40,"Later, when the statement is executed, each Argument located during binding"
40,is applied to the JDBC
40,PreparedStatement.
40,"Occasionally, two or more argument factories will support arguments of the same"
40,"data type. When this happens, the last-registered factory wins. Preparable argument factories"
40,always take precedence over base argument factories. This means that
40,"you can override the way any data type is bound, including the data types"
40,supported out of the box.
40,7. Mappers
40,Jdbi makes use of mappers to convert result data into Java objects. There are
40,two types of mappers:
40,"Row Mappers, which map a full row of result set data."
40,"Column Mappers, which map a single column of a result set row."
40,7.1. Row Mappers
40,RowMapper is a functional
40,"interface, which maps the current row of a JDBC"
40,ResultSet to a mapped type. Row
40,mappers are invoked once for each row in the result set.
40,"Since RowMapper is a functional interface, they can be provided inline to a"
40,query using a lambda expression:
40,"List<User> users = handle.createQuery(""SELECT id, \""name\"" FROM \""user\"" ORDER BY id ASC"")"
40,".map((rs, ctx) -> new User(rs.getInt(""id""), rs.getString(""name"")))"
40,.list();
40,"There are three different types being used in the above example. Query,"
40,"returned by Handle.createQuery(), implements the ResultBearing interface."
40,The ResultBearing.map() method takes a RowMapper<T> and returns a
40,"ResultIterable<T>. Finally, ResultBearing.list() collects each row in the"
40,result set into a List<T>.
40,"Row mappers may be defined as classes, which allows for re-use:"
40,class UserMapper implements RowMapper<User> {
40,@Override
40,"public User map(ResultSet rs, StatementContext ctx) throws SQLException {"
40,"return new User(rs.getInt(""id""), rs.getString(""name""));"
40,"List<User> users = handle.createQuery(""SELECT id, \""name\"" FROM \""user\"" ORDER BY id ASC"")"
40,.map(new UserMapper())
40,.list();
40,This RowMapper is equivalent to the lambda mapper above but more explicit.
40,7.1.1. RowMappers registry
40,"Row mappers can be registered for particular types. This simplifies usage,"
40,requiring only that you specify what type you want to map to. Jdbi
40,"automatically looks up the mapper from the registry, and uses it."
40,"jdbi.registerRowMapper(User.class,"
40,"(rs, ctx) -> new User(rs.getInt(""id""), rs.getString(""name""));"
40,try (Handle handle = jdbi.open()) {
40,List<User> users = handle
40,".createQuery(""SELECT id, name FROM user ORDER BY id ASC"")"
40,.mapTo(User.class)
40,.list();
40,A mapper which implements RowMapper with an explicit mapped type (such as the
40,UserMapper class in the previous section) may be registered without specifying
40,the mapped type:
40,handle.registerRowMapper(new UserMapper());
40,"When this method is used, Jdbi inspects the generic class signature of the"
40,mapper to automatically discover the mapped type.
40,It is possible to register more than one mapper for any given type. When this
40,"happens, the last-registered mapper for a given type takes precedence. This"
40,"permits optimizations, like registering a ""default"" mapper for some type, while"
40,allowing that default mapper to be overridden with a different one when
40,appropriate.
40,7.1.2. RowMapperFactory
40,A RowMapperFactory can
40,produce row mappers for arbitrary types.
40,Implementing a factory might be preferable to a regular row mapper if:
40,"The mapper implementation is generic, and could apply to multiple mapped"
40,"types. For example, Jdbi provides a generalized BeanMapper, which maps"
40,columns to bean properties for any bean class.
40,"The mapped type has a generic signature, and/or the mapper could be composed"
40,"of other registered mappers. For example, Jdbi provides a"
40,"Map.Entry<K,V> mapper, provided a mapper is registered"
40,for types K and V.
40,You want to bundle multiple mappers into a single class.
40,"Let’s take an example Pair<L, R> class:"
40,"public final class Pair<L, R> {"
40,public final L left;
40,public final R right;
40,"public Pair(L left, R right) {"
40,this.left = left;
40,this.right = right;
40,"Now, let’s implement a row mapper factory. The factory should produce a"
40,"RowMapper<Pair<L, R>> for any Pair<L, R> type, where the L type is mapped"
40,"from the first column, and R from the second—​assuming there are column"
40,mappers registered for both L and R.
40,Let’s take this one step at a time:
40,public class PairMapperFactory implements RowMapperFactory {
40,"public Optional<RowMapper<?>> build(Type type, ConfigRegistry config) {"
40,// ...
40,"The build method accepts a mapped type, and a config registry. It may return"
40,"Optional.of(someMapper) if it knows how to map that type, or"
40,Optional.empty() otherwise.
40,First we check whether the mapped type is a Pair:
40,if (!Pair.class.equals(GenericTypes.getErasedType(type))) {
40,return Optional.empty();
40,The GenericTypes utility class is discussed in Working with Generic Types.
40,"Next, we extract the L and R generic parameters from the mapped type:"
40,"Type leftType = GenericTypes.resolveType(Pair.class.getTypeParameters()[0], type);"
40,"Type rightType = GenericTypes.resolveType(Pair.class.getTypeParameters()[1], type);"
40,"In the first line, Pair.class.getTypeParameters()[0] gives the type variable"
40,"L. Likewise in the second line, Pair.class.getTypeParameters()[1] gives the type"
40,variable R.
40,We use resolveType() to resolve the types for the L and R type variables
40,in the context of the mapped type.
40,"Now that we have the types for L and R, we can look up the column mappers"
40,"for those types from the ColumnMappers config class, through the config"
40,registry:
40,ColumnMappers columnMappers = config.get(ColumnMappers.class);
40,ColumnMapper<?> leftMapper = columnMappers.findFor(leftType)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for Pair left parameter "" + leftType));"
40,ColumnMapper<?> rightMapper = columnMappers.findFor(rightType)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for Pair right parameter "" + rightType));"
40,The config registry is a locator for config classes. So when we call
40,"config.get(ColumnMappers.class), we get back a ColumnMappers instance with"
40,the current column mapper configuration.
40,Next we call ColumnMappers.findFor() to get the column mappers for the left
40,and right types.
40,"You may have noticed that although this method can return Optional, we are"
40,throwing an exception if we can’t find the left- or right-hand mappers. We have
40,found this to be a best practice: return Optional.empty() if the factory
40,"knows nothing about the mapped type (Pair, in this case). If it knows the"
40,mapped type but is missing some configuration to make it work (e.g. mappers not
40,registered for L or R parameter) it is more helpful to throw an exception
40,"with an informative message, so users can diagnose why the mapper is not"
40,working as expected.
40,"Finally, we construct a pair mapper, and return it:"
40,"RowMapper<?> pairMapper = (rs, ctx) ->"
40,"new Pair(leftMapper.map(rs, 1, ctx), // In JDBC, column numbers start at 1"
40,"rightMapper.map(rs, 2, ctx));"
40,return Optional.of(pairMapper);
40,Here is the factory class all together:
40,public class PairMapperFactory implements RowMapperFactory {
40,"public Optional<RowMapper<?>> build(Type type, ConfigRegistry config) {"
40,if (!Pair.class.equals(GenericTypes.getErasedType(type))) {
40,return Optional.empty();
40,"Type leftType = GenericTypes.resolveType(Pair.class.getTypeParameters()[0], type);"
40,"Type rightType = GenericTypes.resolveType(Pair.class.getTypeParameters()[1], type);"
40,ColumnMappers columnMappers = config.get(ColumnMappers.class);
40,ColumnMapper<?> leftMapper = columnMappers.findFor(leftType)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for Pair left parameter "" + leftType));"
40,ColumnMapper<?> rightMapper = columnMappers.findFor(rightType)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for Pair right parameter "" + rightType));"
40,"RowMapper<?> pairMapper = (rs, ctx) -> new Pair(leftMapper.map(rs, 1, ctx),"
40,"rightMapper.map(rs, 2, ctx));"
40,return Optional.of(pairMapper);
40,Row mapper factories may be registered similar to regular row mappers:
40,jdbi.registerRowMapper(new PairMapperFactory());
40,try (Handle handle = jdbi.open()) {
40,"List<Pair<String, String>> configPairs = handle"
40,".createQuery(""SELECT key, value FROM config"")"
40,".mapTo(new GenericType<Pair<String, String>>() {})"
40,.list();
40,The GenericType utility class is discussed in Working with Generic Types.
40,7.2. Column Mappers
40,ColumnMapper is a functional
40,"interface, which maps a column from the current row of a JDBC"
40,ResultSet to a mapped type.
40,"Since ColumnMapper is a functional interface, they can be provided inline to a query using a lambda expression:"
40,List<Money> amounts = handle
40,".select(""SELECT amount FROM transactions WHERE account_id = ?"", accountId)"
40,".map((rs, col, ctx) -> Money.parse(rs.getString(col))) (1)"
40,.list();
40,"Whenever a column mapper is used to map rows, only the first column of each row"
40,is mapped.
40,"Column mappers may be defined as classes, which allows for re-use:"
40,public class MoneyMapper implements ColumnMapper<Money> {
40,"public Money map(ResultSet r, int columnNumber, StatementContext ctx) throws SQLException {"
40,return Money.parse(r.getString(columnNumber));
40,List<Money> amounts = handle
40,".select(""SELECT amount FROM transactions WHERE account_id = ?"", accountId)"
40,.map(new MoneyMapper())
40,.list();
40,"This ColumnMapper is equivalent to the lambda mapper above, but more explicit."
40,7.2.1. ColumnMappers registry
40,"Column mappers may be registered for specific types. This simplifies usage,"
40,requiring only that you specify what type you want to map to. Jdbi automatically
40,"looks up the mapper from the registry, and uses it."
40,"jdbi.registerColumnMapper(Money.class,"
40,"(rs, col, ctx) -> Money.parse(rs.getString(col)));"
40,List<Money> amounts = jdbi.withHandle(handle ->
40,"handle.select(""SELECT amount FROM transactions WHERE account_id = ?"", accountId)"
40,.mapTo(Money.class)
40,.list());
40,A mapper which implements ColumnMapper with an explicit mapped type (such as
40,the MoneyMapper class in the previous section) may be registered without
40,specifying the mapped type:
40,handle.registerColumnMapper(new MoneyMapper());
40,"When this method is used, Jdbi inspects the generic class signature of the"
40,mapper to automatically discover the mapped type.
40,It is possible to register more than one mapper for any given type. When this
40,"happens, the last-registered mapper for a given type takes precedence. This"
40,"permits optimizations, like registering a ""default"" mapper for some type, while"
40,allowing that default mapper to be overridden with a different one when
40,appropriate.
40,"Out of the box, column mappers are registered for the following types:"
40,"Primitives: boolean, byte, short, int, long, char, float, and"
40,double
40,"java.lang: Boolean, Byte, Short, Integer, Long, Character,"
40,"Float, Double, String, and Enum (stored as the enum value’s name by default)"
40,java.math: BigDecimal
40,byte[] arrays (e.g. for BLOB or VARBINARY columns)
40,"java.net: InetAddress, URL, and URI"
40,java.sql: Timestamp
40,"java.time: Instant, LocalDate, LocalDateTime, LocalTime,"
40,"OffsetDateTime, ZonedDateTime, and ZoneId"
40,java.util: UUID
40,java.util.Collection and Java arrays (for array columns). Some
40,additional setup may be required depending on the type of array element—​see
40,SQL Arrays.
40,The binding and mapping method for enum values
40,"can be controlled via the Enums config,"
40,"as well as the annotations @EnumByName,"
40,"@EnumByOrdinal, and"
40,@DatabaseValue.
40,7.2.2. ColumnMapperFactory
40,A ColumnMapperFactory can
40,produce column mappers for arbitrary types.
40,Implementing a factory might be preferable to a regular column mapper if:
40,"The mapper class is generic, and could apply to multiple mapped types."
40,"The type being mapped is generic, and/or the mapper could be composed"
40,of other registered mappers.
40,You want to bundle multiple mappers into a single class.
40,Let’s create a mapper factory for Optional<T> as an example. The factory
40,"should produce a ColumnMapper<Optional<T>> for any T, provided a column"
40,mapper is registered for T.
40,Let’s take this one step at a time:
40,public class OptionalColumnMapperFactory implements ColumnMapperFactory {
40,"public Optional<ColumnMapper<?>> build(Type type, ConfigRegistry config) {"
40,// ...
40,"The build method accepts a mapped type, and a config registry. It may return"
40,"Optional.of(someMapper) if it knows how to map that type, or"
40,Optional.empty() otherwise.
40,"First, we check whether the mapped type is an Optional:"
40,if (!Optional.class.equals(GenericTypes.getErasedType(type))) {
40,return Optional.empty();
40,The GenericTypes utility class is discussed in Working with Generic Types.
40,"Next, extract the T generic parameter from the mapped type:"
40,"Type t = GenericTypes.resolveType(Optional.class.getTypeParameters()[0], type);"
40,The expression Optional.class.getTypeParameters()[0] gives the type variable
40,We use resolveType() to resolve the type of T in the context of the mapped
40,type.
40,"Now that we have the type of T, we can look up a column mapper for that type"
40,"from the ColumnMappers config class, through the config registry:"
40,ColumnMapper<?> tMapper = config.get(ColumnMappers.class)
40,.findFor(embeddedType)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for parameter "" + embeddedType + "" of type "" + type));"
40,The config registry is a locator for config classes. So when we call
40,"config.get(ColumnMappers.class), we get back a ColumnMappers instance with"
40,the current column mapper configuration.
40,Next we call ColumnMappers.findFor() to get the column mapper for the embedded
40,type.
40,"You may have noticed that although this method can return Optional, we’re"
40,throwing an exception if we can’t find a mapper for the embedded type. We’ve
40,found this to be a best practice: return Optional.empty() if the factory knows
40,"nothing about the mapped type (Optional, in this case). If it knows the mapped"
40,type but is missing some configuration to make it work (e.g. no mapper
40,registered for tye T parameter) it is more helpful to throw an exception with
40,"an informative message, so users can diagnose why the mapper is not working as"
40,expected.
40,"Finally, we construct the column mapper for optionals, and return it:"
40,"ColumnMapper<?> optionalMapper = (rs, col, ctx) ->"
40,"Optional.ofNullable(tMapper.map(rs, col, ctx));"
40,return Optional.of(optionalMapper);
40,Here is the factory class all together:
40,public class OptionalColumnMapperFactory implements ColumnMapperFactory {
40,"public Optional<ColumnMapper<?>> build(Type type, ConfigRegistry config) {"
40,if (!Optional.class.equals(GenericTypes.getErasedType(type))) {
40,return Optional.empty();
40,"Type t = GenericTypes.resolveType(Optional.class.getTypeParameters()[0], type);"
40,ColumnMapper<?> tMapper = config.get(ColumnMappers.class)
40,.findFor(t)
40,.orElseThrow(() -> new NoSuchMapperException(
40,"""No column mapper registered for parameter "" + t + "" of type "" + type));"
40,"ColumnMapper<?> optionalMapper = (rs, col, ctx) -> Optional.ofNullable(tMapper.map(rs, col, ctx));"
40,return Optional.of(optionalMapper);
40,Column mapper factories may be registered similar to regular column mappers:
40,jdbi.registerColumnMapper(new OptionalColumnMapperFactory());
40,try (Handle handle = jdbi.open()) {
40,List<Optional<String>> middleNames = handle
40,".createQuery(""SELECT middle_name FROM contacts"")"
40,.mapTo(new GenericType<Optional<String>>() {})
40,.list();
40,The GenericType utility class is discussed in Working with Generic Types.
40,7.3. Primitive Mapping
40,All Java primitive types have default mappings to their corresponding JDBC types.
40,"Generally, Jdbi will automatically perform boxing and unboxing as appropriate when"
40,it encounters wrapper types.
40,"By default, SQL null mapped to a primitive type will adopt the JDBC default value."
40,This may be disabled by configuring
40,jdbi.getConfig(ColumnMappers.class).setCoalesceNullPrimitivesToDefaults(false).
40,7.4. Immutables Mapping
40,"Immutables value objects may be mapped, see the Immutables section for details."
40,7.5. Freebuilder Mapping
40,"Freebuilder value objects may be mapped, see the Freebuilder section for details."
40,7.6. Reflection Mappers for Beans and POJOs
40,Jdbi provides a number of reflection-based mappers out of the box which treat column
40,names as attributes. The different mappers can be used to map values onto
40,Java Bean properties (BeanMapper)
40,Class fields (FieldMapper)
40,Constructor parameters (ConstructorMapper)
40,There is also an equivalent mapper for Kotlin classes that supports constructor arguments and class properties.
40,Reflective mappers are snake_case aware and will automatically match up these
40,columns to camelCase field/argument/property names.
40,7.6.1. Supported property annotations
40,Reflection mappers support the following annotations.
40,"Unless otherwise noted, all annotations are supported on bean getters and setters for the BeanMapper, constructor parameters for the ConstructorMapper and bean fields for the FieldMapper."
40,@ColumnName allows explicit naming of the column that is mapped to the specific attribute.
40,@Nested for nested beans.
40,"Without this annotation, any attribute is treated as mappable from a single column."
40,This annotation creates a mapper for the nested bean.
40,"There is a limitation that only one type of mapper can be nested at a time; BeanMapper will create another BeanMapper, ConstructorMapper will create another ConstructorMapper etc. @Nested supports an optional prefix for all attributes in the nested bean (@Nested(""prefix""))."
40,@PropagateNull on a given attribute propagates a null value for a specific attribute up.
40,"So if the attribute or column is null, the whole bean will be discarded and a null value will be used instead of the bean itself."
40,This annotation can also be used on the bean class with a column name that must be present in the result set.
40,"When used on an attribute, no value must be set."
40,@Nullable for ConstructorMapper arguments.
40,"If no column is mapped onto a specific constructor argument, don’t fail but use null as value."
40,@JdbiProperty(map=false) allows configuring whether a discovered property is mapped in results.
40,"Turn the map value off, and the property discovery mechanism will ignore it."
40,This used to be called @Unmappable.
40,The @ColumnName annotation only applies while mapping SQL data into Java objects.
40,"When binding object properties (e.g. with bindBean()), bind the property name (:id) rather than the column name (:user_id)."
40,Using @Nullable annotations
40,Jdbi accepts any annotation that is named Nullable to mark a field or method as nullable.
40,These are popular choices:
40,jakarta.annotation.Nullable — jakarta annotation-api
40,javax.annotation.Nullable — findbugs jsr305
40,edu.umd.cs.findbugs.annotations.Nullable — Spotbugs annotations
40,org.jetbrains.annotations.Nullable — Jetbrains annotations
40,org.checkerframework.checker.nullness.qual.Nullable — checker framework
40,org.springframework.lang.Nullable — Spring Framework
40,"To allow Jdbi to discover the @Nullable annotation at runtime, it must use RetentionPolicy.RUNTIME."
40,"When in doubt, prefer the Jakarta variant!."
40,7.6.2. ConstructorMapper
40,ConstructorMapper assigns columns to constructor parameters by
40,name. If the java compiler stores method parameter names in the java
40,classes (using the -parameters flag on the compiler — see also
40,"Compiling with Parameter Names), the constructor mapper will use"
40,these names to select columns.
40,"public User(int id, String name) {"
40,this.id = id;
40,this.name = name;
40,"Otherwise, the names can be explictly set with the @ColumnName"
40,annotation on each constructor parameter:
40,"public User(@ColumnName(""id"") int id, @ColumnName(""name"") String name) {"
40,this.id = id;
40,this.name = name;
40,The standard Java Bean @ConstructorProperties annotation can also be used:
40,"@ConstructorProperties({""id"", ""name""})"
40,"public User(int id, String name) {"
40,this.id = id;
40,this.name = name;
40,Lombok’s @AllArgsConstructor annotation generates the
40,@ConstructorProperties annotation for you.
40,Register a constructor mapper for your mapped class using the factory() or of()
40,methods:
40,handle.registerRowMapper(ConstructorMapper.factory(User.class));
40,"Set<User> userSet = handle.createQuery(""SELECT * FROM \""user\"" ORDER BY id ASC"")"
40,.mapTo(User.class)
40,.collect(Collectors.toSet());
40,assertThat(userSet).hasSize(4);
40,"The constructor parameter names ""id"", ""name"" match the database column names"
40,and as such no custom mapper code is required at all.
40,Constructor mappers can be configured with a column name prefix for each mapped
40,"constructor parameter. This can help to disambiguate mapping joins, e.g. when"
40,two mapped classes have identical property names (like id or name):
40,"handle.registerRowMapper(ConstructorMapper.factory(Contact.class, ""c""));"
40,"handle.registerRowMapper(ConstructorMapper.factory(Phone.class, ""p""));"
40,"handle.registerRowMapper(JoinRowMapper.forTypes(Contact.class, Phone.class);"
40,"List<JoinRow> contactPhones = handle.select(""SELECT "" +"
40,"""c.id cid, c.name cname, "" +"
40,"""p.id pid, p.name pname, p.number pnumber "" +"
40,"""FROM contacts c LEFT JOIN phones p ON c.id = p.contact_id"")"
40,.mapTo(JoinRow.class)
40,.list();
40,"Typically, the mapped class will have a single constructor. If it has multiple"
40,"constructors, Jdbi will pick one based on these rules:"
40,"First, use the constructor annotated with @JdbiConstructor, if any."
40,"Next, use the constructor annotated with @ConstructorProperties, if any."
40,"Otherwise, throw an exception that Jdbi does not know which constructor to"
40,use.
40,"For legacy column names that don’t match up to property names, use the"
40,@ColumnName annotation to provide an exact column name.
40,"public User(@ColumnName(""user_id"") int id, String name) {"
40,this.id = id;
40,this.name = name;
40,Nested constructor-injected types can be mapped using the @Nested annotation:
40,public class User {
40,"public User(int id,"
40,"String name,"
40,@Nested Address address) {
40,// ...
40,public class Address {
40,"public Address(String street,"
40,"String city,"
40,"String state,"
40,String zip) {
40,// ...
40,handle.registerRowMapper(ConstructorMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, street, city, state, zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,"The @Nested annotation has an optional value() attribute, which can be used to apply a column name prefix to each nested constructor parameter:"
40,"public User(int id,"
40,"String name,"
40,"@Nested(""addr"") Address address) {"
40,// ...
40,handle.registerRowMapper(ConstructorMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, addr_street, addr_city, addr_state, addr_zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,"By default, ConstructorMapper expects the result set to contain columns to map every constructor parameter, and will throw an exception if any parameters cannot be mapped."
40,"Parameters annotated @Nullable (see Using @Nullable annotations) may be omitted from the result set, in which"
40,ConstructorMapper will pass null to the constructor for that parameter.
40,public class User {
40,"public User(int id,"
40,"String name,"
40,"@Nullable String passwordHash,"
40,@Nullable @Nested Address address) {
40,// ...
40,"In this example, the id and name columns must be present in the result set, but passwordHash and address are optional."
40,"If they are present, they will be mapped."
40,7.6.3. BeanMapper
40,BeanMapper assigns columns to bean properties. This mapper uses the standard Java Bean conventions.
40,public class UserBean {
40,private int id;
40,private String name;
40,public int getId() {
40,return id;
40,public void setId(int id) {
40,this.id = id;
40,public String getName() {
40,return name;
40,public void setName(String name) {
40,this.name = name;
40,"The standard bean convention requires that the setter returns a void value. The popular ""builder pattern"" setter"
40,convention of returning the bean itself does not work with the standard Java Bean convention and the setter will not be found. This is a common source of problems when using the BeanMapper.
40,"Register a bean mapper for your mapped class, using the factory() method:"
40,handle.registerRowMapper(BeanMapper.factory(UserBean.class));
40,List<UserBean> users = handle
40,".createQuery(""select id, \""name\"" from \""user\"""")"
40,.mapTo(UserBean.class)
40,.list();
40,"Alternatively, call mapToBean() instead of registering a bean mapper:"
40,List<UserBean> users = handle
40,".createQuery(""select id, \""name\"" from \""user\"""")"
40,.mapToBean(UserBean.class)
40,.list();
40,Or use map with an instance:
40,List<UserBean> users = handle
40,".createQuery(""select id, \""name\"" from \""user\"""")"
40,.map(BeanMapper.of(UserBean.class))
40,.list();
40,Bean mappers can be configured with a column name prefix for each mapped
40,"property. This can help to disambiguate mapping joins, e.g. when two mapped"
40,classes have identical property names (like id or name):
40,"handle.registerRowMapper(BeanMapper.factory(ContactBean.class, ""c""));"
40,"handle.registerRowMapper(BeanMapper.factory(PhoneBean.class, ""p""));"
40,"handle.registerRowMapper(JoinRowMapper.forTypes(ContactBean.class, PhoneBean.class));"
40,"List<JoinRow> contactPhones = handle.select(""select """
40,"+ ""c.id cid, c.\""name\"" cname, """
40,"+ ""p.id pid, p.\""name\"" pname, p.\""number\"" pnumber """
40,"+ ""from contacts c left join phones p on c.id = p.contact_id"")"
40,.mapTo(JoinRow.class)
40,.list();
40,"For legacy column names that don’t match up to property names, use the"
40,@ColumnName annotation to provide an exact column name.
40,public class User {
40,private int id;
40,"@ColumnName(""user_id"")"
40,public int getId() { return id; }
40,public void setId(int id) { this.id = id; }
40,The @ColumnName annotation can be placed on either the getter or setter method.
40,"If conflicting annotations exist, then Annotations on the setter are preferred."
40,Nested Java Bean types can be mapped using the @Nested annotation:
40,public class User {
40,private int id;
40,private String name;
40,private Address address;
40,// ... (getters and setters)
40,@Nested (1)
40,public Address getAddress() { ... }
40,public void setAddress(Address address) { ... }
40,public class Address {
40,private String street;
40,private String city;
40,private String state;
40,private String zip;
40,// ... (getters and setters)
40,The @Nested annotation can be placed on either the getter or setter method.
40,The setter is preferred.
40,handle.registerRowMapper(BeanMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, street, city, state, zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,"The @Nested annotation has an optional value() attribute, which can be used to apply a column name prefix to each nested bean property:"
40,"@Nested(""addr"")"
40,public Address getAddress() { ... }
40,handle.registerRowMapper(BeanMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, addr_street, addr_city, addr_state, addr_zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,@Nested properties are left unmodified (i.e. null) if the result set has no columns matching any properties of the nested object.
40,7.6.4. FieldMapper
40,FieldMapper uses reflection
40,to map database columns directly to object fields (including private fields).
40,public class User {
40,public int id;
40,public String name;
40,"Register a field mapper for your mapped class, using the factory() method:"
40,handle.registerRowMapper(FieldMapper.factory(User.class));
40,List<UserBean> users = handle
40,".createQuery(""SELECT id, name FROM user"")"
40,.mapTo(User.class)
40,.list();
40,Field mappers can be configured with a column name prefix for each mapped
40,"field. This can help to disambiguate mapping joins, e.g. when two mapped"
40,classes have identical property names (like id or name):
40,"handle.registerRowMapper(FieldMapper.factory(Contact.class, ""c""));"
40,"handle.registerRowMapper(FieldMapper.factory(Phone.class, ""p""));"
40,"handle.registerRowMapper(JoinRowMapper.forTypes(Contact.class, Phone.class);"
40,List<JoinRow> contactPhones = handle.select(
40,"""SELECT "" +"
40,"""c.id cid, c.name cname, "" +"
40,"""p.id pid, p.name pname, p.number pnumber "" +"
40,"""FROM contacts c LEFT JOIN phones p ON c.id = p.contact_id"")"
40,.mapTo(JoinRow.class)
40,.list();
40,"For legacy column names that don’t match up to field names, use the"
40,@ColumnName annotation to provide an exact column name:
40,public class User {
40,"@ColumnName(""user_id"")"
40,public int id;
40,public String name;
40,Nested field-mapped types can be mapped using the @Nested annotation:
40,public class User {
40,public int id;
40,public String name;
40,@Nested
40,public Address address;
40,public class Address {
40,public String street;
40,public String city;
40,public String state;
40,public String zip;
40,handle.registerRowMapper(FieldMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, street, city, state, zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,"The @Nested annotation has an optional value() attribute, which can be used to apply a column name prefix to each nested field:"
40,public class User {
40,public int id;
40,public String name;
40,"@Nested(""addr"")"
40,public Address address;
40,handle.registerRowMapper(FieldMapper.factory(User.class));
40,List<User> users = handle
40,".select(""SELECT id, name, addr_street, addr_city, addr_state, addr_zip FROM users"")"
40,.mapTo(User.class)
40,.list();
40,@Nested fields are left unmodified (i.e. null) if the result set has no columns matching any fields of the nested object.
40,7.7. Map.Entry Mapping
40,"Out of the box, Jdbi registers a RowMapper<Map.Entry<K,V>>. Since each row in"
40,"the result set is a Map.Entry<K,V>, the entire result set can be easily"
40,"collected into a Map<K,V> (or Guava’s Multimap<K,V>)."
40,A mapper must be registered for both the key and value type.
40,Join rows can be gathered into a map result by specifying the generic map
40,signature:
40,"CharSequence sql = ""select u.id u_id, u.name u_name, p.id p_id, p.phone p_phone """
40,"+ ""from \""user\"" u left join phone p on u.id = p.user_id"";"
40,"Map<User, Phone> map = h.createQuery(sql)"
40,".registerRowMapper(ConstructorMapper.factory(User.class, ""u""))"
40,".registerRowMapper(ConstructorMapper.factory(Phone.class, ""p""))"
40,".collectInto(new GenericType<Map<User, Phone>>() {});"
40,"In the preceding example, the User mapper uses the ""u"" column name prefix, and"
40,"the Phone mapper uses ""p"". Since each mapper only reads columns with the"
40,"expected prefix, the respective id columns are unambiguous."
40,A unique index (e.g. by ID column) can be obtained by setting the key column
40,name:
40,"Map<Integer, User> map = h.createQuery(""select * from \""user\"""")"
40,".setMapKeyColumn(""id"")"
40,.registerRowMapper(ConstructorMapper.factory(User.class))
40,".collectInto(new GenericType<Map<Integer, User>>() {});"
40,Set both the key and value column names to gather a two-column query into a map
40,result:
40,"Map<String, String> map = h.createQuery(""select \""key\"", \""value\"" from config"")"
40,".setMapKeyColumn(""key"")"
40,".setMapValueColumn(""value"")"
40,".collectInto(new GenericType<Map<String, String>>() {});"
40,All the above examples assume a one-to-one key/value relationship. What if
40,there is a one-to-many relationship?
40,"Google Guava provides a Multimap type, which supports mapping multiple"
40,values per key.
40,"First, follow the instructions in the Google Guava section to install the"
40,GuavaPlugin into Jdbi.
40,"Then, simply ask for a Multimap instead of a Map:"
40,"String sql = ""select u.id u_id, u.name u_name, p.id p_id, p.phone p_phone """
40,"+ ""from \""user\"" u left join phone p on u.id = p.user_id"";"
40,"Multimap<User, Phone> map = h.createQuery(sql)"
40,".registerRowMapper(ConstructorMapper.factory(User.class, ""u""))"
40,".registerRowMapper(ConstructorMapper.factory(Phone.class, ""p""))"
40,".collectInto(new GenericType<Multimap<User, Phone>>() {});"
40,"The collectInto() method is worth explaining. When you call it, several things"
40,happen behind the scenes:
40,Consult the JdbiCollectors registry to obtain a
40,CollectorFactory which
40,supports the given container type.
40,"Next, ask that CollectorFactory to extract the element type from the"
40,"container type signature. In the above example, the element type of"
40,"Multimap<User,Phone> is Map.Entry<User,Phone>."
40,Obtain a mapper for that element type from the mapping registry.
40,Obtain a Collector for the
40,container type from the CollectorFactory.
40,"Finally, return map(elementMapper).collect(collector)."
40,"If the lookup for the collector factory, element type, or element mapper"
40,"fails, an exception is thrown."
40,Jdbi can be enhanced to support arbitrary container types.
40,8. Codecs
40,A codec is a replacement for registering an argument and a column mapper for a type. It is responsible for serializing a typed value into a database column and creating a type from a database column.
40,"Codecs are collected in a codec factory, which can be registered with the registerCodecFactory convenience method."
40,// register a single codec
40,"jdbi.registerCodecFactory(CodecFactory.forSingleCodec(type, codec));"
40,// register a few codecs
40,jdbi.registerCodecFactory(CodecFactory.builder()
40,// register a codec by qualified type
40,".addCodec(QualifiedType.of(Foo.class), codec1)"
40,// register a codec by direct java type
40,".addCodec(Foo.class, codec2)"
40,// register a codec by generic type
40,.addCodec(new GenericType<Set<Foo>>() {}. codec3)
40,.build());
40,// register many codecs
40,"Map<QualifiedType<?>, Codec<?>> codecs = ..."
40,jdbi.registerCodecFactory(new CodecFactory(codecs));
40,Codec example:
40,public class Counter {
40,private int count = 0;
40,public Counter() {}
40,public int nextValue() {
40,return count++;
40,private Counter setValue(int value) {
40,this.count = value;
40,return this;
40,private int getValue() {
40,return count;
40,/**
40,* Codec to persist a counter to the database and restore it back.
40,public static class CounterCodec implements Codec<Counter> {
40,@Override
40,public ColumnMapper<Counter> getColumnMapper() {
40,"return (r, idx, ctx) -> new Counter().setValue(r.getInt(idx));"
40,@Override
40,"public Function<Counter, Argument> getArgumentFunction() {"
40,"return counter -> (idx, stmt, ctx) -> stmt.setInt(idx, counter.getValue());"
40,Jdbi core API:
40,// register the codec with JDBI
40,"jdbi.registerCodecFactory(CodecFactory.forSingleCodec(COUNTER_TYPE, new CounterCodec()));"
40,// store object
40,"int result = jdbi.withHandle(h -> h.createUpdate(""INSERT INTO counters (id, \""value\"") VALUES (:id, :value)"")"
40,".bind(""id"", counterId)"
40,".bindByType(""value"", counter, COUNTER_TYPE)"
40,.execute());
40,// load object
40,"Counter restoredCounter = jdbi.withHandle(h -> h.createQuery(""SELECT \""value\"" from counters where id = :id"")"
40,".bind(""id"", counterId)"
40,.mapTo(COUNTER_TYPE).first());
40,SQL Object API uses the codecs transparently:
40,// SQL object dao
40,public interface CounterDao {
40,"@SqlUpdate(""INSERT INTO counters (id, \""value\"") VALUES (:id, :value)"")"
40,"int storeCounter(@Bind(""id"") String id, @Bind(""value"") Counter counter);"
40,"@SqlQuery(""SELECT \""value\"" from counters where id = :id"")"
40,"Counter loadCounter(@Bind(""id"") String id);"
40,// register the codec with JDBI
40,"jdbi.registerCodecFactory(CodecFactory.forSingleCodec(COUNTER_TYPE, new CounterCodec()));"
40,// store object
40,"int result = jdbi.withExtension(CounterDao.class, dao -> dao.storeCounter(counterId, counter));"
40,// load object
40,"Counter restoredCounter = jdbi.withExtension(CounterDao.class, dao -> dao.loadCounter(counterId));"
40,8.1. Resolving Types
40,"By using the TypeResolvingCodecFactory from the guava module, it is possible to use codecs that are registered for subclasses or interface types for concrete classes. This is necessary to e.g. map Auto Value generated classes to database columns."
40,"In the following example, there is only a codec for Value<String> registered, but the code uses beans and classes that are concrete implementations (StringBean contains a StringValue and StringValue implements the Value<String> interface). The TypeResolvingCodecFactory will inspect the types to find a codec for an interface or superclass if no perfect match can be found."
40,// SQL object dao using concrete types
40,public interface DataDao {
40,"@SqlUpdate(""INSERT INTO data (id, \""value\"") VALUES (:bean.id, :bean.value)"")"
40,"int storeData(@BindBean(""bean"") StringBean bean);"
40,"@SqlUpdate(""INSERT INTO data (id, \""value\"") VALUES (:id, :value)"")"
40,"int storeData(@Bind(""id"") String id, @Bind(""value"") StringValue data);"
40,"@SqlQuery(""SELECT \""value\"" from data where id = :id"")"
40,"StringValue loadData(@Bind(""id"") String id);"
40,// generic type representation
40,public static final QualifiedType<Value<String>> DATA_TYPE = QualifiedType.of(new GenericType<Value<String>>() {});
40,public static class DataCodec implements Codec<Value<String>> {
40,@Override
40,public ColumnMapper<Value<String>> getColumnMapper() {
40,"return (r, idx, ctx) -> new StringValue(r.getString(idx));"
40,@Override
40,"public Function<Value<String>, Argument> getArgumentFunction() {"
40,"return data -> (idx, stmt, ctx) -> stmt.setString(idx, data.getValue());"
40,// value interface
40,public interface Value<T> {
40,T getValue();
40,"// bean using concrete types, not interface types."
40,public static class StringBean implements Bean<Value<String>> {
40,private final String id;
40,private final StringValue value;
40,"public StringBean(String id, StringValue value) {"
40,this.id = id;
40,this.value = value;
40,@Override
40,public String getId() {
40,return id;
40,@Override
40,public StringValue getValue() {
40,return value;
40,9. Results
40,A number of operations return data from the database through a JDBC ResultSet:
40,Query operations
40,The Update#executeAndReturnGeneratedKeys and PreparedBatch#executeAndReturnGeneratedKeys methods
40,Metadata operations
40,The JDBC ResultSet class can do simple mapping to Java primitives
40,"and built in classes, but the API is often cumbersome to use."
40,"Jdbi offers many ways to process and map these responses onto other objects using configurable mapping, including the ability to register custom mappers for rows"
40,and columns.
40,A RowMapper converts a row of a ResultSet into a result object.
40,A ColumnMapper converts a single column’s value into a Java object. It can be
40,"used as a RowMapper if there is only one column present, or it can be used to"
40,build more complex RowMapper types.
40,The mapper is selected based on the declared result type of your query.
40,Jdbi iterates over the rows in the ResultSet and presents the mapped results
40,"to you in a container such as a List, Stream, Optional, or Iterator."
40,public static class User {
40,final int id;
40,final String name;
40,"public User(int id, String name) {"
40,this.id = id;
40,this.name = name;
40,@BeforeEach
40,public void setUp() {
40,handle = h2Extension.getSharedHandle();
40,"handle.execute(""CREATE TABLE \""user\"" (id INTEGER PRIMARY KEY AUTO_INCREMENT, \""name\"" VARCHAR)"");"
40,"for (String name : Arrays.asList(""Alice"", ""Bob"", ""Charlie"", ""Data"")) {"
40,"handle.execute(""INSERT INTO \""user\"" (\""name\"") VALUES (?)"", name);"
40,@Test
40,public void findBob() {
40,"User u = findUserById(2).orElseThrow(() -> new AssertionError(""No user found""));"
40,assertThat(u.id).isEqualTo(2);
40,"assertThat(u.name).isEqualTo(""Bob"");"
40,public Optional<User> findUserById(long id) {
40,RowMapper<User> userMapper =
40,"(rs, ctx) -> new User(rs.getInt(""id""), rs.getString(""name""));"
40,"return handle.createQuery(""SELECT * FROM \""user\"" WHERE id=:id"")"
40,".bind(""id"", id)"
40,.map(userMapper)
40,.findFirst();
40,Jdbi operations that involve a ResultSet are defined and executed in multiple steps:
40,"List<User> users = handle.createQuery(""SELECT * FROM users WHERE department = :department"") (1)"
40,".bind(""department"", departmentId) (2)"
40,.mapTo(User.class) (3)
40,.list(); (4)
40,create a Statement that implements ResultBearing
40,execute methods on the statement in builder-style which return the same statement type
40,execute a method defined by ResultBearing that returns a ResultIterable
40,execute a terminal method on ResultIterable that returns the result
40,The ResultBearing and ResultIterable interfaces define all data mapping operations that Jdbi offer.
40,9.1. ResultBearing
40,"The ResultBearing interface represents the result set of a database operation, which has not been mapped to any particular result type."
40,ResultBearing contains two major groups of operations:
40,latent operations that return a ResultIterable. These operations map the unmapped result of an operation to a specific type. They require a method on the ResultIterable to be called to execute the actual operation
40,"terminal operations that collect, scan or reduce the result set. Calling one of these methods, executes the actual operation and returns a result."
40,9.1.1. Mapping result types with latent operations
40,These operations create a map from the data returned by the database onto java objects. This mapping is described in multiple ways:
40,"map() operations using a RowMapper, ColumnMapper or RowViewMapper instance."
40,"mapTo() operations using a type definition. The matching mapper is retrieved from the RowMappers registry. Supports regular Java Types, Generic Types and Qualified Types."
40,"mapToMap() operations where the key is the (lower-cased) column name and the value is the column value. Supports regular Java Types, Generic Types and Qualified Types for the values. The value mapper is retrieved from the RowMappers registry."
40,"mapToBean() maps to a mutable bean instance with that provides setters to set values. For each column name, the corresponding setter is called."
40,This is the most common use case when using the ResultBearing interface:
40,ResultIterable<User> resultIterable = handle
40,".createQuery(""SELECT * from users"")"
40,.mapTo(User.class);
40,List<User> users = resultIterable.list();
40,TODO:
40,Describe terminal operations on the ResultBearing
40,reduceRows
40,RowView
40,reduceResultSet
40,collectInto e.g. with a GenericType token. Implies a mapTo() and a collect()
40,in one operation. e.g. collectInto(new GenericType<List<User>>(){}) is the
40,same as mapTo(User.class).collect(toList())
40,Provide list of container types supported out of the box
40,9.2. ResultIterable
40,"A ResultIterable represents a result set which has been mapped to a specific type, e.g. a ResultIterable<User>."
40,"Almost all operations on the ResultIterable interface are terminal operations. When they finish, they close and release all resources, especially the ResultSet and PreparedStatement objects."
40,See Statement resource management for more details on how resources are released and should be managed.
40,The
40,stream() and iterator() operations are not terminal. They return objects that need to be managed by the caller. See the Resources with Streams and Iterators chapter for more details.
40,TODO:
40,"one(), list(), first(), findOne(), findFirst()"
40,"mention filter, map as non-terminals"
40,"collect, reduce"
40,callback consumers for iterator and stream
40,"ResultIterable.forEach, forEachWithCount"
40,9.2.1. Finding a Single Result
40,ResultIterable.one() returns the only row in the result set. If zero or
40,"multiple rows are encountered, it will throw IllegalStateException."
40,ResultIterable.findOne() returns an Optional<T> of the only row in the
40,"result set, or Optional.empty() if no rows are returned."
40,ResultIterable.first() returns the first row in the result set. If zero
40,"rows are encountered, IllegalStateException is thrown."
40,"ResultIterable.findFirst() returns an Optional<T> of the first row, if"
40,any.
40,9.2.2. Stream
40,Stream integration allows you to use a RowMapper to adapt a ResultSet into
40,the Java Streams framework. The stream will lazily fetch rows from the database as necessary.
40,The stream() method returns a Stream<T>. This is not a terminal operation and the stream must be closed to release any database resources held. See Resources with Streams and Iterators for more details.
40,The withStream() and useStream() methods pass the Stream into a callback.
40,"handle.createQuery(""SELECT id, name FROM user ORDER BY id ASC"")"
40,.map(new UserMapper())
40,.useStream(stream -> {
40,Optional<String> first = stream
40,.filter(u -> u.id > 2)
40,.map(u -> u.name)
40,.findFirst();
40,"assertThat(first).contains(""Charlie"");"
40,});
40,"These methods handle closing the stream for the caller. The withStream() method allows passing a result back to the caller, useStream() only executed the code in the callback."
40,9.2.3. List
40,#list emits a List<T>. This necessarily buffers all results in memory.
40,List<User> users =
40,"handle.createQuery(""SELECT id, name FROM user"")"
40,.map(new UserMapper())
40,.list();
40,9.2.4. Collectors
40,"#collect takes a Collector<T, ?, R> that builds a resulting collection"
40,R<T>. The java.util.stream.Collectors class has a number of interesting
40,Collector implementations to start with.
40,You can also write your own custom collectors.
40,"For example, to accumulate"
40,found rows into a Map:
40,"h.execute(""INSERT INTO something (id, name) VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Chuckles')"");"
40,"Map<Integer, Something> users = h.createQuery(""SELECT id, name FROM something"")"
40,.mapTo(Something.class)
40,".collect(Collector.of(HashMap::new, (accum, item) -> {"
40,"accum.put(item.getId(), item);"
40,// Each entry is added into an accumulator map
40,"}, (l, r) -> {"
40,l.putAll(r);
40,"// While jdbi does not process rows in parallel,"
40,return l;
40,// the Collector contract encourages writing combiners
40,"}, Characteristics.IDENTITY_FINISH));"
40,9.2.5. Reduction
40,#reduce provides a simplified Stream#reduce. Given an identity starting
40,"value and a BiFunction<U, T, U> it will repeatedly combine U until only a"
40,"single remains, and then return that."
40,9.2.6. ResultSetScanner
40,The ResultSetScanner interface accepts a lazily-provided ResultSet
40,and produces the result Jdbi returns from statement execution.
40,Most of the above operations are implemented in terms of ResultSetScanner.
40,The Scanner has ownership of the ResultSet and may advance or seek it.
40,The return value ends up being the final result of statement execution.
40,"Most users should prefer using the higher level result collectors described above,"
40,but someone’s gotta do the dirty work.
40,9.3. Joins
40,Joining multiple tables together is a very common database task. It is also
40,where the mismatch between the relational model and Java’s object model starts
40,to rear its ugly head.
40,Here we present a couple of strategies for retrieving results from more
40,complicated rows.
40,Consider a contact list app as an example. The contact list contains any
40,"number of contacts. Contacts have a name, and any number of phone numbers."
40,"Phone numbers have a type (e.g. home, work) and a phone number:"
40,class Contact {
40,Long id;
40,String name;
40,List<Phone> phones = new ArrayList<>();
40,void addPhone(Phone phone) {
40,phones.add(phone);
40,class Phone {
40,Long id;
40,String type;
40,String phone;
40,"We’ve left out getters, setters, and access modifiers for brevity."
40,"Since we’ll be reusing the same queries, we’ll define them as constants now:"
40,"static final String SELECT_ALL = ""SELECT contacts.id c_id, name c_name, """
40,"+ ""phones.id p_id, type p_type, phones.phone p_phone """
40,"+ ""FROM contacts LEFT JOIN phones ON contacts.id = phones.contact_id """
40,"+ ""order by c_name, p_type "";"
40,"static final String SELECT_ONE = SELECT_ALL + ""where phones.id = :id"";"
40,"Note that we’ve given aliases (e.g. c_id, p_id) to distinguish columns of"
40,the same name (id) from different tables.
40,Jdbi provides a few different APIs for dealing with joined data.
40,9.3.1. ResultBearing.reduceRows()
40,The
40,"""ResultBearing.reduceRows(U, BiFunction)"""
40,method accepts an accumulator seed value and a lambda function. For each row in
40,"the result set, Jdbi calls the lambda with the current accumulator value and a"
40,RowView over the current row of the
40,result set. The value returned for each row becomes the input accumulator
40,"passed in for the next row. After the last row has been processed,"
40,reducedRows() returns the last value returned from the lambda.
40,List<Contact> contacts = handle.createQuery(SELECT_ALL)
40,".registerRowMapper(BeanMapper.factory(Contact.class, ""c""))"
40,".registerRowMapper(BeanMapper.factory(Phone.class, ""p"")) (1)"
40,".reduceRows(new LinkedHashMap<Long, Contact>(), (2)"
40,"(map, rowView) -> {"
40,Contact contact = map.computeIfAbsent( (3)
40,"rowView.getColumn(""c_id"", Long.class),"
40,id -> rowView.getRow(Contact.class));
40,"if (rowView.getColumn(""p_id"", Long.class) != null) { (4)"
40,contact.addPhone(rowView.getRow(Phone.class));
40,return map; (5)
40,.values() (6)
40,.stream()
40,.collect(toList()); (7)
40,"Register row mappers for Contact and Phone. Note the ""c"" and ""p"""
40,arguments used—​these are column name prefixes. By registering mappers with
40,"prefixes, the Contact mapper will only map the c_id and c_name"
40,"columns, whereas the Phone mapper will only map p_id, p_type, and"
40,p_phone.
40,Use an empty LinkedHashMap
40,"as the accumulator seed, mapped by contact ID. LinkedHashMap is a good"
40,"accumulator when selecting multiple master records, since it has fast"
40,storage and lookup while preserving insertion order (which helps honor
40,"ORDER BY clauses). If ordering is unimportant, a HashMap would also"
40,suffice.
40,"Load the Contact from the accumulator if we already have it; otherwise,"
40,initialize it through the RowView.
40,"If p_id column is not null, load the phone number from the current row"
40,and add it to the current contact.
40,Return the input map (now sporting an additional contact and/or phone) as
40,the accumulator for the next row.
40,"At this point, all rows have been read into memory, and we don’t need the"
40,contact ID keys. So we call Map.values() to get a Collection<Contact>.
40,Collect the contacts into a List<Contact>.
40,"Alternatively, the"
40,ResultBearing.reduceRows(RowReducer)
40,variant accepts a RowReducer and
40,returns a stream of reduced elements.
40,"For simple master-detail joins, the"
40,"ResultBearing.reduceRows(BiConsumer<Map<K,V>,RowView>)"
40,method makes it easy to reduce these joins into a stream of master elements.
40,Adapting the example above:
40,List<Contact> contacts = handle.createQuery(SELECT_ALL)
40,".registerRowMapper(BeanMapper.factory(Contact.class, ""c""))"
40,".registerRowMapper(BeanMapper.factory(Phone.class, ""p""))"
40,".reduceRows((Map<Long, Contact> map, RowView rowView) -> { (1)"
40,Contact contact = map.computeIfAbsent(
40,"rowView.getColumn(""c_id"", Long.class),"
40,id -> rowView.getRow(Contact.class));
40,"if (rowView.getColumn(""p_id"", Long.class) != null) {"
40,contact.addPhone(rowView.getRow(Phone.class));
40,(2)
40,.collect(toList()); (3)
40,"The lambda receives a map where result objects will be stored, and a"
40,"RowView. The map is a LinkedHashMap, so the result stream will"
40,yield the result objects in the same order they were inserted.
40,No return statement needed. The same map is reused on every row.
40,This reduceRows() invocation produces a Stream<Contact> (i.e. from
40,"map.values().stream(). In this example, we collect the elements into a"
40,"list, but we could call any Stream method here."
40,You may be wondering about the getRow() and getColumn() calls to rowView.
40,"When you call rowView.getRow(SomeType.class), RowView looks up the"
40,"registered row mapper for SomeType, and uses it to map the current row to a"
40,SomeType object.
40,"Likewise, when you call rowView.getColumn(""my_value"", MyValueType.class),"
40,"RowView looks up the registered column mapper for MyValueType, and uses it"
40,to map the my_value column of the current row to a MyValueType object.
40,"Now let’s do the same thing, but for a single contact:"
40,Optional<Contact> contact = handle.createQuery(SELECT_ONE)
40,".bind(""id"", contactId)"
40,".registerRowMapper(BeanMapper.factory(Contact.class, ""c""))"
40,".registerRowMapper(BeanMapper.factory(Phone.class, ""p""))"
40,".reduceRows(LinkedHashMapRowReducer.<Long, Contact> of((map, rowView) -> {"
40,Contact contact = map.orElseGet(() -> rowView.getRow(Contact.class));
40,"if (rowView.getColumn(""p_id"", Long.class) != null) {"
40,contact.addPhone(rowView.getRow(Phone.class));
40,.findFirst();
40,9.3.2. ResultBearing.reduceResultSet()
40,ResultBearing.reduceResultSet()
40,"is a low-level API similar to reduceRows(), except it provides direct access"
40,to the JDBC ResultSet instead of a RowView for each row.
40,"This method can provide superior performance compared to reduceRows(), at the"
40,expense of verbosity:
40,List<Contact> contacts = handle.createQuery(SELECT_ALL)
40,".reduceResultSet(new LinkedHashMap<Long, Contact>(),"
40,"(acc, resultSet, ctx) -> {"
40,"long contactId = resultSet.getLong(""c_id"");"
40,Contact contact;
40,if (acc.containsKey(contactId)) {
40,contact = acc.get(contactId);
40,} else {
40,contact = new Contact();
40,"acc.put(contactId, contact);"
40,contact.setId(contactId);
40,"contact.setName(resultSet.getString(""c_name""));"
40,"long phoneId = resultSet.getLong(""p_id"");"
40,if (!resultSet.wasNull()) {
40,Phone phone = new Phone();
40,phone.setId(phoneId);
40,"phone.setType(resultSet.getString(""p_type""));"
40,"phone.setPhone(resultSet.getString(""p_phone""));"
40,contact.addPhone(phone);
40,return acc;
40,.values()
40,.stream()
40,.collect(toList());
40,9.3.3. JoinRowMapper
40,The JoinRowMapper takes a set of types to extract from each row. It uses the
40,"mapping registry to determine how to map each given type, and presents you with"
40,a JoinRow that holds all the resulting values.
40,"Let’s consider two simple types, User and Article, with a join table named"
40,"Author. Guava provides a Multimap class, which is very handy for representing"
40,joined tables like this. Assuming we have mappers already registered:
40,h.registerRowMapper(ConstructorMapper.factory(User.class));
40,h.registerRowMapper(ConstructorMapper.factory(Article.class));
40,we can then easily populate a Multimap with the mapping from the database:
40,"Multimap<User, Article> joined = HashMultimap.create();"
40,"h.createQuery(""SELECT * FROM \""user\"" NATURAL JOIN author NATURAL JOIN article"")"
40,".map(JoinRowMapper.forTypes(User.class, Article.class))"
40,".forEach(jr -> joined.put(jr.get(User.class), jr.get(Article.class)));"
40,"While this approach is easy to read and write, it can be inefficient for"
40,certain patterns of data. Consider performance requirements when deciding
40,whether to use high level mapping or more direct low level access with
40,handwritten mappers.
40,You can also use it with SqlObject:
40,public interface UserArticleDao {
40,"@RegisterJoinRowMapper({User.class, Article.class})"
40,"@SqlQuery(""SELECT * FROM \""user\"" NATURAL JOIN author NATURAL JOIN article"")"
40,Stream<JoinRow> getAuthorship();
40,"Multimap<User, Article> joined = HashMultimap.create();"
40,handle.attach(UserArticleDao.class)
40,.getAuthorship()
40,".forEach(jr -> joined.put(jr.get(User.class), jr.get(Article.class)));"
40,assertThat(joined).isEqualTo(JoinRowMapperTest.getExpected());
40,10. Transactions
40,Jdbi provides full support for JDBC transactions.
40,10.1. Managed Transactions
40,A managed transaction is controlled through Jdbi code and provides a handle with an open transaction to user code.
40,"The inTransaction and useTransaction methods on the Jdbi object create a new Handle, open a transaction and pass it to the callback code."
40,"The inTransaction and useTransaction methods on Handle objects open a transaction on the handle itself and then pass it to the callback code. By default, if these methods are called while the handle is already in an open transaction, the existing transaction is reused (no nested transaction is created)."
40,Each of these methods also has a variant that allows setting the transaction isolation level. Changing the transaction isolation level within an open transaction is not supported (and will cause an exception).
40,"At the end of the callback, the transaction is committed, if"
40,the code has not thrown an exception
40,"it was not a nested call from another useTransaction/inTransaction callback. In that case, control is handed back to the original caller and the transaction finishes when that callback returns."
40,the code did not call the rollback() method on the Handle object.
40,Executing a SQL operation in a transaction:
40,// use a Jdbi object to create transaction
40,"public Optional<User> findUserById(Jdbi jdbi, long id) {"
40,return jdbi.inTransaction(transactionHandle ->
40,"transactionHandle.createQuery(""SELECT * FROM users WHERE id=:id"")"
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.findFirst());
40,// use a Handle object to create transaction
40,"public Optional<User> findUserById(Handle handle, long id) {"
40,return handle.inTransaction(transactionHandle ->
40,"transactionHandle.createQuery(""SELECT * FROM users WHERE id=:id"")"
40,".bind(""id"", id)"
40,.mapTo(User.class)
40,.findFirst());
40,The Nesting Callbacks with managed Handles and Transactions chapter has more information about nesting callbacks with managed handle objects.
40,10.2. Unmanaged Transactions
40,Jdbi provides the necessary primitives to control transactions directly from a Handle:
40,the begin()/commit()/rollback() methods provide standard transaction semantics
40,setTransactionIsolationLevel() and getTransactionIsolationLevel() to control the transaction isolation level
40,isInTransaction() returns true if the handle is currently in a transaction
40,savepoint()/releaseSavepoint()/rollbackToSavepoint() for transaction savepoint support. This is not supported by all TransactionHandlers and requires support from the JDBC driver
40,"Transactions are managed by TransactionHandler implementations. By default, transactions are delegated to the JDBC connection and managed through the connection by the database."
40,The transaction handler can be set per Jdbi instance using the setTransactionHandler and getTransactionHandler methods.
40,"In addition to the standard transaction handler, Jdbi includes a handler that works correctly in a container managed environment and a serializable transaction handler that allows multiple concurrent operations on a single handle to retry transparently."
40,10.3. Serializable Transactions
40,"For more advanced queries, sometimes serializable transactions are required."
40,Jdbi includes a transaction runner that is able to retry transactions that abort due to serialization failures.
40,It is important that your transaction does not have side effects as it may be executed multiple times.
40,// Automatically rerun transactions
40,jdbi.setTransactionHandler(new SerializableTransactionRunner());
40,"handle.execute(""CREATE TABLE ints (value INTEGER)"");"
40,// Set up some values
40,"handle.execute(""INSERT INTO ints (value) VALUES(?)"", 10);"
40,"handle.execute(""INSERT INTO ints (value) VALUES(?)"", 20);"
40,"// Run the following twice in parallel, and synchronize"
40,ExecutorService executor = Executors.newCachedThreadPool();
40,CountDownLatch latch = new CountDownLatch(2);
40,Callable<Integer> sumAndInsert = () ->
40,"jdbi.inTransaction(TransactionIsolationLevel.SERIALIZABLE, transactionHandle -> {"
40,// Both threads read initial state of table
40,"int sum = transactionHandle.select(""SELECT sum(value) FROM ints"").mapTo(int.class).one();"
40,"// synchronize threads, make sure that they each has successfully read the data"
40,latch.countDown();
40,latch.await();
40,// Now do the write.
40,synchronized (this) {
40,"// handle can be used by multiple threads, but not at the same time"
40,"transactionHandle.execute(""INSERT INTO ints (value) VALUES(?)"", sum);"
40,return sum;
40,});
40,"// Both of these would calculate 10 + 20 = 30, but that violates serialization!"
40,Future<Integer> result1 = executor.submit(sumAndInsert);
40,Future<Integer> result2 = executor.submit(sumAndInsert);
40,"// One of the transactions gets 30, the other will abort and automatically rerun."
40,"// On the second attempt it will compute 10 + 20 + 30 = 60, seeing the update from its sibling."
40,// This assertion fails under any isolation level below SERIALIZABLE!
40,assertThat(result1.get() + result2.get()).isEqualTo(30 + 60);
40,executor.shutdown();
40,"The above test is designed to run two transactions in lock step. Each attempts to read the sum of all rows in the table, and then insert a new row with that sum. We seed the table with the values 10 and 20."
40,"Without serializable isolation, each transaction reads 10 and 20, and then returns 30. The end result is 30 + 30 = 60, which does not correspond to any serial execution of the transactions!"
40,"With serializable isolation, one of the two transactions is forced to abort and retry. On the second go around, it calculates 10 + 20 + 30 = 60. Adding to 30 from the other, we get 30 + 60 = 90 and the assertion succeeds."
40,11. Configuration
40,"Jdbi aims to be useful out of the box with minimal configuration. If necessary,"
40,there is a wide range of configurations and customizations available to change the
40,default behavior or add in extensions to handle additional database types.
40,Configuration is managed by the
40,ConfigRegistry class.
40,Each Jdbi object that represents a distinct database context (for
40,"example, Jdbi itself, a"
40,"Handle instance, or an attached"
40,SqlObject class) has a
40,separate config registry instance.
40,"When a new context is created, it inherits a copy of its parent"
40,configuration at the time of creation - further modifications to the
40,original will not affect already created configuration contexts.
40,Configuration context copies happen when creating a
40,Handle from
40,"Jdbi, when opening a"
40,SqlStatement from
40,"the Handle, and when attaching or creating an on-demand extension such"
40,as SqlObject.
40,A configurable Jdbi object implements the
40,Configurable interface
40,which allows modification of its configuration as well as retrieving
40,the current context’s configuration for use by Jdbi core or
40,extensions.
40,// fetch the config registry object
40,ConfigRegistry config = jdbi.getConfig();
40,// access the SqlStatements configuration object:
40,SqlStatements sqlStatements = jdbi.getConfig(SqlStatements.class);
40,// modify a setting in SqlStatements with a callback
40,"jdbi.configure(SqlStatements.class, s -> s.setUnusedBindingAllowed(true));"
40,// modify a setting in SqlStatements with direct method invocation
40,jdbi.getConfig(SqlStatements.class).setUnusedBindingAllowed(true);
40,The Configurable
40,interface also adds a number of convenience methods for commonly used
40,configuration objects.
40,See JdbiConfig for more advanced implementation details.
40,11.1. core settings
40,Object
40,Property
40,Type
40,Default
40,Description
40,Arguments
40,bindingNullToPrimitivesPermitted
40,boolean
40,true
40,Allows binding null values for primitive types.
40,preparedArgumentsEnabled
40,boolean
40,true
40,PreparedArguments speed up argument processing but have some backwards compatibility risk with old (pre-3.24.0) releases.
40,untypedNullArgument
40,Argument
40,NullArgument using Types.OTHER.
40,This argument instance is invoked when a null value is assigned to an argument whose type is unknown (this can happen in some corner cases).
40,ColumnMappers
40,coalesceNullPrimitivesToDefaults
40,boolean
40,true
40,Uses the JDBC default value for primitive types if a SQL NULL value was returned by the database.
40,Enums
40,enumStrategy
40,EnumStrategy
40,BY_NAME
40,Sets the strategy to map a Java enum value onto a database column. Available strategies are
40,BY_NAME
40,map the name of the enum from and to the database column
40,BY_ORDINAL
40,map the ordinal number of the enum value from and to the database column
40,Extensions
40,allowProxy
40,boolean
40,true
40,Whether Jdbi is allowed to create proxy
40,instances for classes (as extension and on-demand reference). This is useful for debugging when using the generator to create java classes for sql objects.
40,failFast
40,boolean
40,false
40,"If set to true, extension objects with misconfigured methods will fail at first use of any method. Default is to fail when a misconfigured method is used."
40,Handles
40,forceEndTransactions
40,boolean
40,true
40,"Whether to ensure transaction discipline. If true, transactions must be committed or rolled back before a Handle is closed. If true, any uncommitted"
40,transaction is rolled back and an exception is thrown when the Handle is closed.
40,MapMappers
40,caseChange
40,UnaryOperator<String>
40,LOCALE_LOWER
40,Defines the strategy for mapping the database column names to key names. Available strategies are:
40,NOP
40,"no name mapping, use name as is"
40,LOWER
40,lowercase column names using the ROOT locale
40,UPPER
40,uppercase column names using the ROOT locale
40,LOCALE_LOWER
40,lowercase column names using the current locale
40,LOCALE_UPPER
40,uppercase column names using the current locale
40,Custom strategies can be set by implementing UnaryOperator<String> with custom code.
40,ReflectionMappers
40,caseChange
40,UnaryOperator<String>
40,LOCALE_LOWER
40,Defines the strategy for mapping the database column names to key names. Available strategies are:
40,NOP
40,"no name mapping, use name as is"
40,LOWER
40,lowercase column names using the ROOT locale
40,UPPER
40,uppercase column names using the ROOT locale
40,LOCALE_LOWER
40,lowercase column names using the current locale
40,LOCALE_UPPER
40,uppercase column names using the current locale
40,Custom strategies can be set by implementing UnaryOperator<String> with custom code.
40,strictMatching
40,boolean
40,false
40,"If true, all database columns must be mapped to a property. If any columns are unmatched or any property is unset, an exception is thrown."
40,ResultProducers
40,allowNoResults
40,boolean
40,false
40,"If false, Jdbi throws an exception if a query does not return a result set object (this is different from an empty result, e.g. no rows in a query). When setting this to true, Jdbi uses an empty result set instead."
40,SerializableTransactionRunner.Configuration
40,maxRetries
40,int
40,number of times a transaction is retried if the database reports a serialization error.
40,onFailure
40,Consumer<List<Exception>>
40,<unset>
40,Is called whenever a serialization failure occurs. Can be used e.g. for logging.
40,onSuccess
40,Consumer<List<Exception>>
40,<unset>
40,Is called once a transaction successfully finishes with any exception that has happened during the transaction execution.
40,serializationFailureSqlState
40,String
40,40001
40,SQL state value from a SQLException that is considered a serialization failure. This is defined in the SQL:2011 standard as 40001 but can be different depending on the database.
40,SqlArrayTypes
40,argumentStrategy
40,SqlArrayArgumentStrategy
40,SQL_ARRAY
40,Sets the strategy on how to bind arrays in the database driver.
40,SQL_ARRAY
40,create a SQL array using Connection#createArrayOf and call PreparedStatement#setArray. This is the default and any modern JDBC driver should support it.
40,OBJECT_ARRAY
40,call PreparedStatement#setObject and assume that the driver can handle this.
40,SqlStatements
40,attachAllStatementsForCleanup
40,boolean
40,false
40,Jdbi supports automatic resource management by attaching statements to their handle so that closing the handle will free up all its resources.
40,"If this setting is true, then statements are attached by default."
40,attachCallbackStatementsForCleanup
40,boolean
40,true
40,"Similar to attachAllStatementsForCleanup but for statements created in any of the Jdbi callback methods (withHandle, useHandle, inTransaction, useTransaction)."
40,queryTimeout
40,Integer
40,<unset>
40,Sets the query timeout value in seconds. This value is used to call Statement#setQueryTimeout. Enforcement of the timeout depends on the JDBC driver.
40,scriptStatementsNeedSemicolon
40,boolean
40,true
40,Controls whether the statements parsed in a Script object have trailing semicolons or not. This fixes some issues with specific JDBC drivers (e.g. MySQL when using rewriteBatchedStatements=true).
40,sqlParser
40,SqlParser
40,ColonPrefixSqlParser
40,The parser used to find the placeholders to bind arguments to.
40,Jdbi currently provides the following parsers:
40,ColonPrefixSqlParser
40,the default parser for :name placeholders.
40,HashPrefixSqlParser
40,use #name placeholders
40,Custom implementations must implement SqlParser and may extend CachingSqlParser to benefit from caching.
40,sqlLogger
40,SqlLogger
40,<unset>
40,Controls logging around statement execution. A SqlLogger instance receives the statement context before and after query execution and the context and an exception in case of an execution problem.
40,Jdbi provides Slf4jSqlLogger for the popular slf4j logging framework.
40,templateEngine
40,TemplateEngine
40,DefineAttributeTemplateEngine
40,"The template engine to render SQL statements and substitute placeholders with values that have been defined on a statement (This is for defined attributes, not bound attributes!)."
40,jdbi core
40,DefineAttributeTemplateEngine
40,"the default engine, replaces angle-bracket placeholders (<name>)."
40,jdbi core
40,NoTemplateEngine
40,"Ignore all defined values, do not do any substitutions."
40,jdbi commons-text
40,StringSubstitutorTemplateEngine
40,use the Apache Commons Text StringSubstitutor class
40,jdbi freemarker
40,FreemarkerEngine
40,use Apache Freemarker.
40,jdbi stringtemplate4
40,StringTemplateEngine
40,use StringTemplate 4.
40,Custom implementations must implement TemplateEngine.
40,unusedBindingsAllowed
40,boolean
40,false
40,"All bindings to a SQL operation must be used or an exception is thrown. If this setting is true, then unused bindings in a SQL operation are"
40,ignored.
40,StatementExceptions
40,lengthLimit
40,int
40,1024
40,Controls the maximum length for variable length strings when they are rendered using the SHORT_STATEMENT message rendering strategy.
40,messageRendering
40,MessageRendering
40,SHORT_STATEMENT
40,Sets the message rendering strategy when fetching the message from a StatementException and its subclasses.
40,Controls how error messages from a Statement are displayed (e.g. for logging).
40,NONE
40,"only render the statement exception message itself, do not add any information."
40,PARAMETERS
40,include exception message and the bound parameters
40,SHORT_STATEMENT
40,"include exception message, SQL statement and the bound parameters. Truncate the SQL statement and the list of parameters each to the value of lengthLimit"
40,DETAIL
40,"include exception message, unrendered SQL statement, rendered SQL statement, parsed SQL statement and binding parameters."
40,11.2. SQLObject configuration settings
40,Object
40,Property
40,Type
40,Default
40,Description
40,SqlObjects
40,sqlLocator
40,SqlLocator
40,AnnotationSqlLocator
40,Sets the locator for finding SQL statements. Default is to look for SQL operation annotations.
40,Jdbi provides two implementations of the SqlLocator interface:
40,AnnotationSqlLocator
40,Use annotations (
40,"@SqlBatch,"
40,"@SqlCall,"
40,"@SqlQuery,"
40,@SqlUpdate and
40,@SqlScript) to locate SQL statements
40,SqlObjectClasspathSqlLocator
40,Associates a class or interface with a file located on the classpath (com.foo.Bar#query() becomes com/foo/Bar/query.sql) and loads the SQL from that file. The file may contain comments which are stripped.
40,TimestampedConfig
40,timezone
40,ZoneId
40,system zone id
40,Sets the timezone for the @Timestamped annotation.
40,11.3. other configuration settings
40,Object
40,Property
40,Type
40,Default
40,Description
40,JsonConfig
40,jsonMapper
40,JsonMapper
40,see description
40,"Sets the JSON implementation used to serialize and deserialize json columns. Needs to be set when using the generic Json serialization/deserialization mapper. When installing the jdbi3-gson2, jdbi3-jackson2 or jdbi3-moshi plugin, this is automatically set to the provided implementation."
40,"If none of these modules is loaded, it defaults to an implementation that throws an exception whenever serialization or deserialization is attempted."
40,Gson2Config
40,gson
40,Gson
40,A Gson instance created with the default constructor.
40,Sets the Gson object used to parse and render json text.
40,Jackson2Config
40,mapper
40,ObjectMapper
40,An ObjectMapper instance created with the default constructor.
40,Sets the object mapper instance used to parse and render json text.
40,deserializationView
40,Class<?>
40,<unset>
40,Sets a view (see @JsonView) for deserialization. Can be used to limit the values read when mapping a json column.
40,serializationView
40,Class<?>
40,<unset>
40,Sets a view (see @JsonView) for serialization. Can be used to limit the values written when using a json argument.
40,view
40,Class<?>
40,<unset>
40,Sets a view (see @JsonView) for serialization and deserialization. Can be used to limit the values read and written by json arguments and mappers.
40,MoshiConfig
40,moshi
40,Moshi
40,A Moshi instance created with the default constructor.
40,Sets the Moshi instance used to parse and render json text.
40,FreemarkerConfig
40,freemarkerConfiguration
40,Configuration
40,The default configuration object.
40,Sets the freemarker configuration object for the template engine. See the freemarker documentation for details.
40,The default configuration has the following modifications for Jdbi:
40,uses Configuration.DEFAULT_INCOMPATIBLE_IMPROVEMENTS
40,configures a class template loader that loads templates from the root of the class path.
40,the number format is set to computer.
40,StringTemplates
40,failOnMissingAttribute
40,boolean
40,false
40,"If true, fail rendering a template if a referenced attribute is missing."
40,12. SQL Arrays
40,Jdbi can bind/map Java arrays to/from SQL arrays:
40,"handle.createUpdate(""INSERT INTO groups (id, user_ids) VALUES (:id, :userIds)"")"
40,".bind(""id"", 1)"
40,".bind(""userIds"", new int[] {10, 5, 70})"
40,.execute();
40,"int[] userIds = handle.createQuery(""SELECT user_ids FROM groups WHERE id = :id"")"
40,".bind(""id"", 1)"
40,.mapTo(int[].class)
40,.one();
40,"You can also use Collections in place of arrays, but you’ll need to provide"
40,"the element type if you’re using the fluent API, since it’s erased:"
40,"handle.createUpdate(""INSERT INTO groups (id, user_ids) VALUES (:id, :userIds)"")"
40,".bind(""id"", 1)"
40,".bindArray(""userIds"", int.class, Arrays.asList(10, 5, 70))"
40,.execute();
40,"List<Integer> userIds = handle.createQuery(""SELECT user_ids FROM groups WHERE id = :id"")"
40,".bind(""id"", 1)"
40,.mapTo(new GenericType<List<Integer>>() {})
40,.one();
40,Use @SingleValue for mapping an array result with the SqlObject API:
40,public interface GroupsDao {
40,"@SqlQuery(""SELECT user_ids FROM groups WHERE id = ?"")"
40,@SingleValue
40,List<Integer> getUserIds(int groupId);
40,12.1. Registering array types
40,Any Java array element type you want binding support for needs to be registered
40,with Jdbi’s SqlArrayTypes registry. An array type that is directly supported
40,by your JDBC driver can be registered using:
40,"jdbi.registerArrayType(int.class, ""integer"");"
40,"Here, ""integer"" is the SQL type name that the JDBC driver supports natively."
40,Plugins like PostgresPlugin and H2DatabasePlugin automatically
40,register the most common array element types for their respective databases.
40,"Postgres supports enum array types, so you can register an array type for"
40,"enum Colors { red, blue } using jdbi.registerArrayType(Colors.class, ""colors"")"
40,"where ""colors"" is a user-defined enum type name in your database."
40,12.2. Binding custom array types
40,You can also provide your own implementation of SqlArrayType that converts
40,a custom Java element type to a type supported by the JDBC driver:
40,class UserArrayType implements SqlArrayType<User> {
40,@Override
40,public String getTypeName() {
40,"return ""integer"";"
40,@Override
40,public Object convertArrayElement(User user) {
40,return user.getId();
40,You can now bind instances of User[] to arguments of data type integer[]:
40,"User user1 = new User(1, ""bob"");"
40,"User user2 = new User(42, ""alice"");"
40,handle.registerArrayType(new UserArrayType());
40,"handle.createUpdate(""INSERT INTO groups (id, user_ids) VALUES (:id, :users)"")"
40,".bind(""id"", 1)"
40,".bind(""users"", new User[] {user1, user2})"
40,.execute();
40,"Like the Arguments Registry, if there are multiple SqlArrayType s"
40,"registered for the same data type, the last registered wins."
40,12.3. Mapping array types
40,SqlArrayType only allows you to bind Java array/collection arguments to their
40,"SQL counterparts. To map SQL array columns back to Java types, you can register"
40,a regular ColumnMapper:
40,public class UserIdColumnMapper implements ColumnMapper<UserId> {
40,@Override
40,"public UserId map(ResultSet rs, int col, StatementContext ctx) throws SQLException {"
40,return new UserId(rs.getInt(col));
40,handle.registerColumnMapper(new UserIdColumnMapper());
40,"List<UserId> userIds = handle.createQuery(""SELECT user_ids FROM groups WHERE id = :id"")"
40,".bind(""id"", 1)"
40,.mapTo(new GenericType<List<UserId>>() {})
40,.one();
40,Array columns can be mapped to any container type registered with the
40,JdbiCollectors registry. E.g. a VARCHAR[] may be mapped to an
40,ImmutableList<String> if the Guava plugin is installed.
40,13. SQL Objects
40,"SQL Objects are a declarative-style extension to the fluent-style, programmatic Core APIs. SQL Objects are implemented as an extension to the core API using the"
40,Extension framework.
40,"To start using the SQL Object plugin, add a dependency to your project:"
40,For Apache Maven:
40,<dependencies>
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-sqlobject</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,</dependencies>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-sqlobject:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance:
40,Jdbi jdbi = ...
40,jdbi.installPlugin(new SqlObjectPlugin());
40,"With SQL Object, you declare a public interface class as a SQL Object extension type, add methods for database operations, and specify what SQL statement to"
40,execute.
40,You can specify what each method does in one of two ways:
40,Annotate the method with a SQL method annotation. Jdbi provides a number of these
40,annotations out of the box.
40,"Use Java interface default methods, and provide your own implementation in the method body."
40,"At runtime, you can request an instance of your interface, and Jdbi synthesizes"
40,an implementation based on the annotations and methods you declared.
40,13.1. SQL Method annotations
40,Interface methods can be annotated with one of Jdbi’s SQL method annotations:
40,@SqlQuery - select operations that return data
40,"@SqlUpdate - insert, update, delete etc. operations that modify data"
40,@SqlBatch - bulk operations
40,@SqlCall - call stored procedures
40,@SqlScript - execute multiple statements as a script
40,"As with any extension type, parameters to the method are used as arguments. For the SQL Object extension, the arguments are passed to the statement, and the SQL statement result is mapped to the method return type."
40,"SQL Object provides data mapping onto return types, argument mapping and mapper annotations to allow fully declarative definition of SQL queries."
40,13.1.1. @SqlQuery
40,Use the @SqlQuery annotation for select operations. Each operation may return one or more rows which are mapped onto a return type by the SQL Object extension.
40,This is an example for a simple query:
40,"@SqlQuery(""SELECT id, name FROM users"") (1)"
40,List<User> retrieveUsers(); (2)
40,defines the SQL query that gets executed
40,The method can return any type. The SQL Object framework will map the response from the SQL query onto this data type.
40,The SQL Object plugin uses the same configuration as the Jdbi core framework. Row and column mappers need to be registered to map to the correct types. This can be done through configuration or by using Mapper annotations.
40,13.1.2. @SqlUpdate
40,"Use the @SqlUpdate annotation for operations that modify data (i.e. inserts, updates, deletes)."
40,This is an example that uses PreparedStatement placeholders (?) for arguments:
40,public interface UserDao {
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (?, ?)"") (1)"
40,"void insert(long id, String name); (2)"
40,Method arguments are bound to the ? token in the SQL statement at their respective positions.
40,"id is bound to the first placeholder (?), and name to the second placeholder."
40,"@SqlUpdate can also be used for DDL (Data Definition Language) operations like creating or altering tables. However, we recommend using a schema migration tool such"
40,as Flyway or Liquibase to maintain your database schemas.
40,"By default, a @SqlUpdate method may declare one of these return types:"
40,void - no return value
40,"int or long - returns the update count. Depending on the database vendor and JDBC driver, this may be either the number of rows changed, or the number"
40,matched by the query (regardless of whether any data was changed).
40,boolean - returns true if the update count is greater than zero.
40,Some databases also support returning additional values. See the @GetGeneratedKeys annotation for more details.
40,13.1.3. @SqlBatch
40,Use the @SqlBatch annotation for bulk update operations. It works similar to the PreparedBatch operation in the Jdbi core.
40,public interface ContactDao {
40,"@SqlBatch(""INSERT INTO contacts (id, name, email) VALUES (?, ?, ?)"")"
40,"void bulkInsert(List<Integer> ids,"
40,(1)
40,"Iterator<String> names, (2)"
40,String... emails); (3)
40,a collection argument for the batch operation
40,an iterable argument for the batch operation
40,a varargs argument for the batch operation
40,"When a batch method is called, it iterates through the method’s iterable parameters (collections, iterables, varargs etc.), and executes the SQL statement with the corresponding elements from each parameter."
40,"Thus, a statement like:"
40,contactDao.bulkInsert(
40,"ImmutableList.of(1, 2, 3),"
40,"ImmutableList.of(""foo"", ""bar"", ""baz"").iterator(),"
40,"""a@example.com"", ""b@example.com"", ""c@fake.com"");"
40,would execute:
40,"INSERT INTO contacts (id, name, email) VALUES (1, 'foo', 'a@example.com');"
40,"INSERT INTO contacts (id, name, email) VALUES (2, 'bar', 'b@example.com');"
40,"INSERT INTO contacts (id, name, email) VALUES (3, 'baz', 'c@fake.com');"
40,"Constant values may also be used as parameters to a SQL batch. In this case,"
40,the same value is bound to that parameter for every SQL statement in the batch.
40,public interface UserDao {
40,"@SqlBatch(""INSERT INTO users (tenant_id, id, name) "" +"
40,"""VALUES (:tenantId, :user.id, :user.name)"")"
40,"void bulkInsert(@Bind(""tenantId"") long tenantId, (1)"
40,"@BindBean(""user"") User... users);"
40,Insert each user record using the same tenant_id. See SQL Object arguments mapping for details about the @Bind annotation.
40,Any method annotated with @SqlBatch must have at least one iterable parameter.
40,"By default, methods annotated with @SqlBatch may declare one of these"
40,return types:
40,void - no return value
40,int[] or long[] - returns the update count per execution in the batch.
40,"Depending on the database vendor and JDBC driver, this may be either the number of"
40,"rows changed by a statement, or the number matched by the query (regardless of whether any data was changed)."
40,"boolean[] - returns true if the update count is greater than zero, one value for each execution in the batch."
40,13.1.4. @SqlCall
40,Use the @SqlCall annotation to call stored procedures.
40,public interface AccountDao {
40,"@SqlCall(""{call suspend_account(:id)}"")"
40,void suspendAccount(long id);
40,"@SqlCall methods can return void, or may return"
40,OutParameters if the stored procedure has any output parameters. Each output parameter must be registered
40,with the @OutParameter annotation.
40,"Returning an OutParameters object from an extension method is incompatible with on-demand objects. Any other extension method can be used to create extension objects that can provide output parameters. Alternatively, it is possible to use consumer or function arguments with on-demand extensions."
40,public interface OrderDao {
40,// only works with Handle#attach() or Jdbi#withExtension() / Jdbi#useExtension()
40,"@SqlCall(""{call prepare_order_from_cart(:cartId, :orderId, :orderTotal)}"")"
40,"@OutParameter(name = ""orderId"","
40,sqlType = java.sql.Types.BIGINT)
40,"@OutParameter(name = ""orderTotal"", sqlType = java.sql.Types.DECIMAL)"
40,"OutParameters prepareOrderFromCart(@Bind(""cartId"") long cartId);"
40,// using a consumer argument works with any extension method
40,"@SqlCall(""{call prepare_order_from_cart(:cartId, :orderId, :orderTotal)}"")"
40,"@OutParameter(name = ""orderId"","
40,sqlType = java.sql.Types.BIGINT)
40,"@OutParameter(name = ""orderTotal"", sqlType = java.sql.Types.DECIMAL)"
40,"prepareOrderFromCart(@Bind(""cartId"") long cartId, Consumer<OutParameters> consumer);"
40,// using a function argument also works with any extension method
40,"@SqlCall(""{call prepare_order_from_cart(:cartId, :orderId, :orderTotal)}"")"
40,"@OutParameter(name = ""orderId"","
40,sqlType = java.sql.Types.BIGINT)
40,"@OutParameter(name = ""orderTotal"", sqlType = java.sql.Types.DECIMAL)"
40,"SomeResult prepareOrderFromCart(@Bind(""cartId"") long cartId, Function<OutParameters, SomeResult> function);"
40,Individual output parameters can be extracted from the
40,OutParameters object:
40,"db.useExtension(OrderDao.class, orderDao -> {"
40,OutParameters outParams = orderDao.prepareOrderFromCart(cartId);
40,"long orderId = outParams.getLong(""orderId"");"
40,"double orderTotal = outParams.getDouble(""orderTotal"");"
40,...
40,});
40,@SqlCall supports special arguments for OutParameters processing.
40,13.1.5. @SqlScript
40,Use @SqlScript to execute one or more statements in a batch.
40,"While @SqlBatch executes the same SQL statement with different values, the @SqlScript annotation executes different SQL statements without explicit data bound to each statement."
40,"@SqlScript(""CREATE TABLE <name> (pk int primary key)"")"
40,(1)
40,void createTable(@Define String name); (2)
40,"@SqlScript(""INSERT INTO cool_table VALUES (5), (6), (7)"")"
40,"@SqlScript(""DELETE FROM cool_table WHERE pk > 5"")"
40,"int[] doSomeUpdates(); // returns [ 3, 2 ]"
40,@UseClasspathSqlLocator (3)
40,@SqlScript (4)
40,"@SqlScript(""secondScript"") (5)"
40,int[] externalScript();
40,Sql scripts can use substitute placeholders. The default template engine uses angle brackets (< and >).
40,The @Define annotation provides values for the placeholder attributes
40,The @UseClasspathSqlLocator annotation loads SQL templates from the classpath
40,Use the name of the annotated method to locate the script
40,specify the script name in the annotation
40,"By default, methods annotated with @SqlScript may declare one of these return types:"
40,void - no return value
40,int[] or long[] - return value of each statement executed.
40,"Depending on the database vendor and JDBC driver, this may be either the number of rowsreturn"
40,"nothing changed by a statement, or the number matched by the query (regardless of whether any data was changed)."
40,"boolean[] - returns true if the update count is greater than zero, one value for each statement in the script."
40,13.2. Using SQL Objects
40,"SQL Object types are regular Java interfaces classes that must be public. Once they have been defined, there are multiple ways to use them. They differ in the lifecycle of the underlying Handle object:"
40,attached to an existing handle object
40,The Handle#attach() method uses an existing handle to create a SQL object instance:
40,try (Handle handle = jdbi.open()) {
40,ContactPhoneDao dao = handle.attach(ContactPhoneDao.class);
40,dao.insertFullContact(contact);
40,"Attached SQL Objects have the same lifecycle as the handle—​when the handle is closed, the SQL Object becomes unusable."
40,using Jdbi extension methods
40,SQL Object types can be created using
40,Jdbi#withExtension()
40,"for operations that return a result, or"
40,Jdbi#useExtension()
40,for operations with no result. These methods take a callback (or a lambda object) and provide it with a managed instance of the SQL object type:
40,"jdbi.useExtension(ContactPhoneDao.class, dao -> dao.insertFullContact(alice));"
40,"long bobId = jdbi.withExtension(ContactPhoneDao.class, dao -> dao.insertFullContact(bob));"
40,creating an on-demand object
40,"On-demand instances are constructed with the Jdbi#onDemand() method. They have an open-ended lifecycle, as they obtain and release a connection for each method call. They are thread-safe, and may be reused across an application."
40,ContactPhoneDao dao = jdbi.onDemand(ContactPhoneDao.class);
40,long aliceId = dao.insertFullContact(alice); (1)
40,long bobId = dao.insertFullContact(bob); (1)
40,each of these operations creates and releases a new database connection.
40,"The performance of on-demand objects depends on the database and database connection pooling. For performance critical operations, it may be better to manage the handle manually and use attached objects."
40,On-demand instances open and close the underlying PreparedStatement objects for every call to an extension method. They are incompatible with any object or operation that relies on an open statement to retrieve data from the database (e.g. Stream and Iterator for query operations or OutParameters for call operations). It is possible to use consumer or function arguments to process these values as the callback is executed while the statement is still open.
40,13.2.1. Interface default methods
40,Interface default methods on a SQL object type can provide custom code functionality that uses the same handle as the SQL Object methods.
40,"The SQL object framework manages the underlying handle so that calling a method on the same object on the same thread will use the same Handle object. If the handle is managed by the framework, it will only be closed when the outermost method exits."
40,@Test
40,void testDefaultMethod() {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,assertThat(dao.findUser(1))
40,.isPresent()
40,".contains(new User(1, ""Alice""));"
40,"dao.changeName(1, ""Alex""); (1)"
40,assertThat(dao.findUser(1))
40,.isPresent()
40,".contains(new User(1, ""Alex""));"
40,interface UserDao {
40,"@SqlQuery(""SELECT * FROM users WHERE id = :id"")"
40,Optional<User> findUser(int id);
40,"@SqlUpdate(""UPDATE users SET name = :name WHERE id = :user.id"")"
40,"void updateUser(@BindMethods(""user"") User user, String name);"
40,"default void changeName(int id, String name) {"
40,"findUser(id).ifPresent(user -> updateUser(user, name));"
40,(2)
40,The changeName() method is called by the user code
40,The implementation will call both findUser() and updateUser() with the same handle object.
40,13.3. SQL Object method arguments
40,All SQL Object methods support method arguments. Method arguments are mapped onto SQL statement parameters.
40,13.3.1. Mapping arguments to positional parameters
40,"By default, arguments passed to the method are bound as positional parameters in the SQL statement:"
40,public interface UserDao {
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (?, ?)"") (1)"
40,"void insert(long id, String name); (2)"
40,declares two positional parameters
40,"provides two method parameters. id will be bound as the first, name as the second parameter."
40,This matches the PreparedStatement way of binding arguments to a statement. Jdbi binds the arguments as the correct types and convert them if necessary.
40,13.3.2. Mapping arguments to named parameters
40,"The Jdbi core supports named parameters in SQL statements. When using the SQL Object extension, those can be bound to specific method arguments using annotations."
40,The @Bind annotation binds a single method argument to a named parameter:
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"") (1)"
40,"void insert(@Bind(""name"") String name, @Bind(""id"") long id); (2)"
40,"The SQL statement uses two named parameters, :id and :name"
40,The method declaration annotates each parameter with the @Bind annotation with matching names. The order is not important as each method parameter is explicitly named
40,"Similar to the methods on the Handle, arguments can be bound in many different ways:"
40,Bind any iterable object with @BindList
40,Any Java object that implements the Iterable interface can be bound using the @BindList annotation. This
40,"will iterate the elements in 'a,b,c,d,…​' form."
40,"This annotation requires you to use a template placeholder (using the <field> notation when using the default template engine), similar to the SqlStatement#bindList() method. Templates placeholders are incompatible with batch operations such as PreparedBatch as they are evaluated only once and the evaluated statement is cached. If subsequent batch executions use an iterable object with a different size, the number of placeholders and arguments will not match. If the database supports array types, it is strongly recommended to use bindArray() with a SQL array instead of the bindList() method."
40,"@SqlQuery(""SELECT name FROM users WHERE id in (<userIds>)"")"
40,"List<String> getFromIds(@BindList(""userIds"") List<Long> userIds)"
40,Bind map instances with @BindMap
40,Entries from a Map can be bound using the @BindMap annotation. Each entry from the map is bound as a named parameter using the map key as the name and the map value as the value. Jdbi will bind the values as the correct types and convert them if necessary:
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,(1)
40,"void insert(@BindMap Map<String, ?> map); (2)"
40,"The SQL statement expects two named parameters, :id and :name"
40,"Jdbi will look for two keys, id and a name in the map. The values for these keys are bound to the parameters."
40,Bind bean getters with @BindBean
40,Java Bean getters can be bound with the @BindBean annotation:
40,class UserBean {
40,private int id;
40,private String name;
40,public UserBean() {}
40,public void setId(int id) {
40,this.id = id;
40,public void setName(String name) {
40,this.name = name;
40,public int getId() {
40,return id;
40,public String getName() {
40,return name;
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,void insertUserBean(@BindBean UserBean user);
40,This annotation uses the Java Bean syntax for getters.
40,Bind parameterless public methods with @BindMethods
40,"Similar to @BindBean, the @BindMethods annotation bean methods as parameters. For each binding, it looks for a method with the same name:"
40,class User {
40,private final int id;
40,private final String name;
40,"public User(int id, String name) {"
40,this.id = id;
40,this.name = name;
40,public int id() {
40,return id;
40,public String name() {
40,return name;
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,void insertUser(@BindMethods User user);
40,"When using Java 14+ record types, all getters from the record can be bound using the @BindMethods annotation."
40,Bind public fields from a java object with @BindFields
40,Many objects offer public fields instead of getters. Those can be bound by using the @BindFields annotation:
40,class UserField {
40,public int id;
40,public String name;
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,void insertUserField(@BindFields UserField user);
40,Using object prefixes
40,"The `@BindMap, @BindBean, @BindMethods, and @BindFields annotations support an optional prefix:"
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:user.id, :user.name)"")"
40,(1)
40,"void insertUserBeanPrefix(@BindBean(""user"") UserBean user); (2)"
40,The id and name parameters are prefixed with user.
40,The User object which provides the values uses a prefix. The user.id parameter will be mapped to the getId() method of this bean and the user.name parameter to the getName() method.
40,"By providing prefixes, it is possible to use multiple objects with different prefixes or mix map and regular bound parameters."
40,"@SqlUpdate(""INSERT INTO users (id, name, password) VALUES (:user.id, :user.name, :password)"")"
40,"void insert(@BindMap(""user"") Map<String, ?> map, @Bind(""password"") String password);"
40,"Even if the provided map would contain a key named password, it would not be used, because all values from the map are bound with the user prefix."
40,Using nested properties
40,"Similar to the Core API, the"
40,"@BindBean, @BindFields, and @BindMethods annotation will also map nested properties."
40,A Java object that returns a nested object can be bound and then nested properties are addressed using e.g. :user.address.street.
40,This functionality is not available for Map objects that have been bound with @BindMap. Map keys must match the bound parameter name.
40,class NestedUser {
40,private final User user;
40,private final String tag;
40,"public NestedUser(User user, String tag) {"
40,this.user = user;
40,this.tag = tag;
40,public User user() {
40,return user;
40,public String tag() {
40,return tag;
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:user.id, :user.name)"")"
40,void insertUser(@BindMethods NestedUser user);
40,"""Mixes style"" nested properties are not supported. Any nested object will be inspected in the same way as the root object. All nested objects must use either bean-style, methods or fields."
40,13.3.3. Mapping arguments to Java parameter names
40,"When compiling a project with parameter names enabled, the @Bind annotation is not needed. SQLObjects will bind un-annotated parameters to their names."
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,"void insert(long id, String name);"
40,13.3.4. Consumer and Function arguments
40,Consumer arguments
40,"In addition to the regular method arguments for a SQL Object method, it is possible to use a single Consumer<T> or Function<T> argument in addition to other arguments."
40,A consumer argument is a special return type for SQL operations. They can be used to consume the results from a SQL operation with a callback instead of returning a value from the method.
40,"Any SQL operation method that wants to use a consumer argument must return void. It can declare only a single consumer argument, but can have additional regular method arguments. The consumer argument can be in any position in the argument list. ."
40,"Unless the type T is a Stream, Iterator or Iterable, the consumer is executed once for each row in the result set. The static type of parameter T determines the row type."
40,"@SqlQuery(""SELECT * FROM users WHERE id = :id"")"
40,@RegisterConstructorMapper(User.class)
40,"void consumeUser(int id, Consumer<User> consumer); (1)"
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterConstructorMapper(User.class)
40,void consumeMultiUsers(Consumer<User> consumer); (2)
40,This SQL operation may return no or just one result. The consumer argument will only be invoked if a user exists.
40,This SQL operation may return multiple results. It will invoke the consumer argument once for every result.
40,"If the consumer implements Stream, Iterator or Iterable, then the consumer is executed once with the corresponding object holding the results. The static type of parameter T determines the mapped row type here as well. If the result set is empty, the SQL Object framework may not call the consumer or may call it with an empty result object."
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterConstructorMapper(User.class)
40,void consumeMultiUserIterable(Consumer<Iterable<User>> consumer); (1)
40,This consumer argument is called once if any number of results are present. It may be called with an empty iterable object or not at all if no results are present.
40,"When using Consumer<Iterable> as a consumer argument, the Iterable object passed into the consumer is NOT a general-purpose Iterable as it supports only a single invocation of the Iterable#iterator() method. Invoking it the iterator method again to obtain a second or subsequent iterator may throw IllegalStateException."
40,Function arguments
40,"A function argument can be used to collect or transform the result from a SQL operation. Similar to a consumer argument, it will receive the results of the query. A function argument only supports Stream, Iterator or Iterable as the function input type. The result type of the function must match the return type of the method itself."
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterConstructorMapper(User.class)
40,"Set<User> mapUsers(Function<Stream<User>, Set<User>> function); (1)"
40,The results of the SQL operation are passed as a stream into the Function. It returns a set which is then returned by the method.
40,@SqlCall Consumer and Function arguments
40,The @SqlCall annotation supports either a Consumer argument or a Function argument for the OutParameters return value.
40,A method that has been annotated with @SqlCall may have any number of regular method arguments and
40,declare void or OutParameters as its return type.
40,declare void as its return type and have a single Consumer<OutParameters> consumer argument
40,"declare an arbitrary type T as return type and have a single Function<OutParameters, T> function argument"
40,"When using a consumer or function argument, these are called to process the OutParameters value before the statement is closed."
40,13.3.5. Mapping arguments to defined attributes
40,The Jdbi core framework supports attributes that can be used anywhere in the SQL statement or that can control Query template rendering. The @Define annotation provides this functionality for the SQL Object extension:
40,interface UserDao {
40,"@SqlQuery(""SELECT name FROM <table>"") (1)"
40,"List<String> getNames(@Define(""table"") String tableName); (2)"
40,@Test
40,void testNames() {
40,"List<String> result = jdbi.withExtension(UserDao.class, dao -> dao.getNames(""users"")); (3)"
40,assertThat(result).containsAll(names);
40,declare an attribute placeholder for the table name
40,bind the placeholder to the method argument
40,provide the value for the placeholder when calling the SQL object method
40,13.4. SQL Object method return values
40,The SQL Object framework will try to convert the result of a database operation to the declared return value of a SQL Object method.
40,This is most important for @SqlQuery operations as most other operations have a very limited set of possible return values:
40,"@SqlUpdate - supports void, int, long, boolean"
40,@SqlBatch -
40,"supports void, int[], long[], boolean[]"
40,"@SqlScript - supports void, int[], long[], boolean[]"
40,"@SqlCall - supports void, OutParameters (and function arguments)"
40,"Query methods may return a single row or multiple rows, depending on whether the method return type looks like a collection:"
40,public interface UserDao {
40,"@SqlQuery(""SELECT name FROM users"")"
40,List<String> listNames(); (1)
40,"@SqlQuery(""SELECT name FROM users WHERE id = ?"")"
40,String getName(long id); (2)
40,"@SqlQuery(""SELECT name FROM users WHERE id = ?"")"
40,Optional<String> findName(long id); (3)
40,"@SqlQuery(""SELECT id, name FROM users WHERE id = ?"")"
40,Optional<User> findUser(long id); (4)
40,"Returns a collection of results. This is never null, an empty collection is returned for an empty result set."
40,"Returns a single result. If the query returns a result set with multiple rows, then only the first row is returned. If the row set is empty, null is returned."
40,"Methods may return Optional values. If the query returns no rows (or if the value in the row is null),"
40,Optional.empty() is returned instead of null.
40,SQL Object throws an exception if query returns more than one
40,row.
40,"When returning a complex type, Jdbi must have row and column mappers registered, otherwise it will throw an exception. These can be registered through the Jdbi"
40,core API or with annotations.
40,Jdbi can use different collection types by registering a CollectorFactory with the JdbiCollectors
40,config registry.
40,See
40,BuiltInCollectorFactory
40,for the complete list of collection types supported out of the box. Some
40,Jdbi plugins (e.g. the GuavaPlugin) register additional collection types.
40,13.4.1. Using Streams and Iterators
40,SQL Object method may return
40,"ResultIterable,"
40,ResultIterator or
40,Stream types. Each of these return values represents a cursor-type object that requires an active Handle object with a database connection to support streaming results.
40,"All SQL object type methods are subject to the extension framework Handle lifecycle. When the Handle object is closed, the database connection and the corresponding stream or iterator is closed as well."
40,This is an example of a SQL Object type that returns cursor types:
40,interface UserDao {
40,"@SqlQuery(""SELECT name FROM users"")"
40,Stream<String> getNamesAsStream();
40,"@SqlQuery(""SELECT name FROM users"")"
40,ResultIterable<String> getNamesAsIterable();
40,"@SqlQuery(""SELECT name FROM users"")"
40,ResultIterator<String> getNamesAsIterator();
40,Use the method return value directly
40,Only the Handle#attach() method allows cursor type objects as SQL Object method return values. Calling this method attaches the object to the Handle lifecycle which in turn is managed by user code:
40,@Test
40,void testHandleAttach() {
40,try (Handle handle = jdbi.open()) { (1)
40,UserDao dao = handle.attach(UserDao.class);
40,try (Stream<String> stream = dao.getNamesAsStream()) { (2)
40,List<String> result = stream.collect(Collectors.toList()); (3)
40,assertThat(result).containsAll(names);
40,The handle is managed in user code.
40,The stream is also managed.
40,The returned stream is used within the try-with-resources block.
40,The objects returned from these methods hold database resources that should be closed. Jdbi usually handles resources well but using a try-with-resource block is a good practice:
40,try (ResultIterable<String> names = dao.getNamesAsIterable()) {
40,// ...
40,try (ResultIterator<String> names = dao.getNamesAsIterator()) {
40,// ...
40,try (Stream<String> names = dao.getNamesAsStream()) {
40,// ...
40,The Jdbi#withExtension() or Jdbi#useExtension() methods can not be used:
40,@Test
40,void testWithExtensionFails() {
40,assertThatThrownBy(() -> {
40,"List<String> result = jdbi.withExtension(UserDao.class, UserDao::getNamesAsStream).collect(Collectors.toList()); (1)"
40,assertThat(result).containsAll(names);
40,}).isInstanceOf(ResultSetException.class); (2)
40,The handle is closed when leaving the Jdbi#withExtension() method.
40,Calling the Stream#collect() method on the returned stream causes a ResultSetException.
40,The Jdbi#onDemand() method can not be used either:
40,@Test
40,void testOnDemandFails() {
40,assertThatThrownBy(() -> {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,List<String> result = dao.getNamesAsStream().collect(Collectors.toList()); (1)
40,assertThat(result).containsAll(names);
40,}).isInstanceOf(ResultSetException.class); (2)
40,The handle is closed when leaving the getNamesAsStream method.
40,Calling the Stream#collect() method on the returned stream causes a ResultSetException.
40,Use interface default methods
40,The SQL Object framework closes the Handle object when returning from the outermost method in a SQL object type. It is possible to write processing logic in an interface default method:
40,interface UserDao {
40,"@SqlQuery(""SELECT name FROM users"")"
40,Stream<String> getNamesAsStream();
40,default List<String> getNames() {
40,(1)
40,try (Stream<String> stream = getNamesAsStream()) { (2)
40,return stream.collect(Collectors.toList()); (3)
40,The handle is closed when exiting the getNames method as this is the outermost method in the UserDao object.
40,The stream is managed with a try-with-resources block.
40,The stream can be processed within the getNames method.
40,"The default method can be used with the Jdbi#onDemand(), Jdbi#withExtension() and Jdbi#useExtension() methods:"
40,@Test
40,void testOnDemandDefaultMethod() {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,List<String> result = dao.getNames();
40,assertThat(result).containsAll(names);
40,@Test
40,void testWithExtensionDefaultMethod() {
40,"List<String> result = jdbi.withExtension(UserDao.class, UserDao::getNames);"
40,assertThat(result).containsAll(names);
40,Use consumer or function arguments
40,Using the method return value either directly or through an interface default method has the drawback that the user code to process the cursor-type object must be written within the SQL Object type as it must be executed before closing the Handle object.
40,An elegant way to sidestep this problem is using a consumer argument or a function argument to provide a callback:
40,interface CallbackDao {
40,"@SqlQuery(""SELECT name FROM users"")"
40,void getNamesAsStream(Consumer<Stream<String>> consumer);
40,"@SqlQuery(""SELECT name FROM users"")"
40,"Set<String> getNamesAsSet(Function<Stream<String>, Set<String>> function);"
40,Using a callback argument supports all Jdbi and Handle methods to attach SQL objects:
40,@Test
40,void testHandleAttachConsumer() {
40,try (Handle handle = jdbi.open()) { (1)
40,CallbackDao dao = handle.attach(CallbackDao.class);
40,List<String> result = new ArrayList<>();
40,dao.getNamesAsStream(stream -> stream.forEach(result::add)); (2)
40,assertThat(result).containsAll(names);
40,@Test
40,void testHandleAttachFunction() {
40,try (Handle handle = jdbi.open()) { (1)
40,CallbackDao dao = handle.attach(CallbackDao.class);
40,Set<String> result = dao.getNamesAsSet(stream -> stream.collect(Collectors.toSet())); (2)
40,assertThat(result).containsAll(names);
40,The Handle lifecycle must still be managed by user code.
40,The stream is managed by the SQL object framework and does not need to be closed by user code.
40,This code also works with the Jdbi#onDemand() and Jdbi#useExtension() methods:
40,@Test
40,void testOnDemandConsumer() {
40,CallbackDao dao = jdbi.onDemand(CallbackDao.class);
40,List<String> result = new ArrayList<>();
40,dao.getNamesAsStream(stream -> stream.forEach(result::add));
40,assertThat(result).containsAll(names);
40,@Test
40,void testOnDemandFunction() {
40,CallbackDao dao = jdbi.onDemand(CallbackDao.class);
40,Set<String> result = dao.getNamesAsSet(stream -> stream.collect(Collectors.toSet()));
40,assertThat(result).containsAll(names);
40,@Test
40,void testWithExtensionConsumer() {
40,List<String> result = new ArrayList<>();
40,"jdbi.useExtension(CallbackDao.class,"
40,dao -> dao.getNamesAsStream(stream -> stream.forEach(result::add))); (1)
40,assertThat(result).containsAll(names);
40,@Test
40,void testWithExtensionFunction() {
40,"Set<String> result = jdbi.withExtension(CallbackDao.class,"
40,dao -> dao.getNamesAsSet(stream -> stream.collect(Collectors.toSet())));
40,assertThat(result).containsAll(names);
40,This code uses Jdbi#useExtension() instead of Jdbi#withExtension() as the SQL Object method returns void.
40,"While using a consumer or a function argument is a great way to deal with cursor-type objects, there is the drawback that the user code is called while holding the handle (or the database connection) open. If the callback does very expensive or slow processing, this may hold the connection for a very long time."
40,13.5. SQL Object mapper annotations
40,The most common use for annotations is to register specific row and column mappers to return values from @SqlQuery.
40,13.5.1. @RegisterRowMapper
40,Use @RegisterRowMapper to register a concrete row mapper class:
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterRowMapper(UserMapper.class)
40,List<User> list();
40,Row mappers used with this annotation must meet a few requirements:
40,public class UserMapper implements RowMapper<User> {
40,(1) (2)
40,public UserMapper() { (3)
40,// ...
40,"public T map(ResultSet rs, StatementContext ctx) throws SQLException {"
40,// ...
40,Must be a public class.
40,Must implement RowMapper with an explicit type argument (e.g.
40,RowMapper<User>) instead of a type variable (e.g. RowMapper<T>).
40,"Must have a public, no-argument constructor (or a default constructor)."
40,The @RegisterRowMapper annotation may be repeated multiple times on the same
40,type or method to register multiple mappers.
40,13.5.2. @RegisterRowMapperFactory
40,Use @RegisterRowMapperFactory to register a RowMapperFactory.
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterRowMapperFactory(UserMapperFactory.class)
40,List<User> list();
40,Row mapper factories used with this annotation must meet a few requirements:
40,public class UserMapperFactory implements RowMapperFactory { (1)
40,public UserMapperFactory() { (2)
40,// ...
40,"public Optional<RowMapper<?>> build(Type type, ConfigRegistry config) {"
40,// ...
40,Must be a public class.
40,"Must have a public, no-argument constructor (or a default constructor)."
40,The @RegisterRowMapperFactory annotation may be repeated multiple times on the
40,same type or method to register multiple factories.
40,13.5.3. @RegisterColumnMapper
40,Use @RegisterColumnMapper to register a column mapper:
40,public interface AccountDao {
40,"@SqlQuery(""SELECT balance FROM accounts WHERE id = ?"")"
40,@RegisterColumnMapper(MoneyMapper.class)
40,Money getBalance(long id);
40,Column mappers used with this annotation must meet a few requirements:
40,public class MoneyMapper implements ColumnMapper<Money> {
40,(1) (2)
40,public MoneyMapper() { (3)
40,// ...
40,"public T map(ResultSet r, int columnNumber, StatementContext ctx) throws SQLException {"
40,// ...
40,Must be a public class.
40,Must implement ColumnMapper with an explicit type argument (e.g.
40,ColumnMapper<User>) instead of a type variable (e.g. ColumnMapper<T>).
40,"Must have a public, no-argument constructor (or a default constructor)."
40,The @RegisterColumnMapper annotation may be repeated multiple times on the
40,same type or method to register multiple mappers.
40,13.5.4. @RegisterColumnMapperFactory
40,Use @RegisterColumnMapperFactory to register a column mapper factory:
40,public interface AccountDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterColumnMapperFactory(MoneyMapperFactory.class)
40,List<User> list();
40,Column mapper factories used with this annotation must meet a few requirements:
40,public class UserMapperFactory implements RowMapperFactory { (1)
40,public UserMapperFactory() { (2)
40,// ...
40,"public Optional<RowMapper<?>> build(Type type, ConfigRegistry config) {"
40,// ...
40,Must be a public class.
40,"Must have a public, no-argument constructor (or a default constructor)."
40,The @RegisterColumnMapperFactory annotation may be repeated multiple times on
40,the same type or method to register multiple factories.
40,13.5.5. @RegisterBeanMapper
40,Use @RegisterBeanMapper to register a BeanMapper for a bean class:
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterBeanMapper(User.class)
40,List<User> list();
40,Using the prefix attribute causes the bean mapper to map only those columns
40,that begin with the prefix:
40,public interface UserDao {
40,"@SqlQuery(""SELECT u.id u_id, u.name u_name, r.id r_id, r.name r_name "" +"
40,"""FROM users u LEFT JOIN roles r ON u.role_id = r.id"")"
40,"@RegisterBeanMapper(value = User.class, prefix = ""u"")"
40,"@RegisterBeanMapper(value = Role.class, prefix = ""r"")"
40,"Map<User,Role> getRolesPerUser();"
40,"In this example, the User mapper will map the columns u_id and u_name into"
40,the User.id and User.name properties. Likewise for r_id and r_name into
40,"Role.id and Role.name, respectively."
40,The @RegisterBeanMapper annotation may be repeated (as demonstrated above) on
40,the same type or method to register multiple bean mappers.
40,13.5.6. @RegisterConstructorMapper
40,Use @RegisterConstructorMapper to register a ConstructorMapper for classes
40,that are instantiated with all properties through the constructor.
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterConstructorMapper(User.class)
40,List<User> list();
40,Using the prefix attribute causes the constructor mapper to only map those
40,columns that begin with the prefix:
40,public interface UserDao {
40,"@SqlQuery(""SELECT u.id u_id, u.name u_name, r.id r_id, r.name r_name "" +"
40,"""FROM users u LEFT JOIN roles r ON u.role_id = r.id"")"
40,"@RegisterConstructorMapper(value = User.class, prefix = ""u"")"
40,"@RegisterConstructorMapper(value = Role.class, prefix = ""r"")"
40,"Map<User,Role> getRolesPerUser();"
40,"In this example, the User mapper will map the columns u_id and u_name into"
40,the id and name parameters of the User constructor. Likewise for r_id
40,"and r_name into id and name parameters of the Role constructor,"
40,respectively.
40,The @RegisterConstructorMapper annotation may be repeated multiple times on
40,the same type or method to register multiple constructor mappers.
40,13.5.7. @RegisterFieldMapper
40,Use @RegisterFieldMapper to register a FieldMapper for a given class.
40,public interface UserDao {
40,"@SqlQuery(""SELECT * FROM users"")"
40,@RegisterFieldMapper(User.class)
40,List<User> list();
40,Using the prefix attribute causes the field mapper to only map those columns
40,that begin with the prefix:
40,public interface UserDao {
40,"@SqlQuery(""SELECT u.id u_id, u.name u_name, r.id r_id, r.name r_name "" +"
40,"""FROM users u LEFT JOIN roles r ON u.role_id = r.id"")"
40,"@RegisterFieldMapper(value = User.class, prefix = ""u"")"
40,"@RegisterFieldMapper(value = Role.class, prefix = ""r"")"
40,"Map<User,Role> getRolesPerUser();"
40,"In this example, the User mapper will map the columns u_id and u_name into"
40,the User.id and User.name fields. Likewise for r_id and r_name into the
40,"Role.id and Role.name fields, respectively."
40,The @RegisterConstructorMapper annotation may be repeated multiple times on
40,the same type or method to register multiple constructor mappers.
40,13.6. Other SQL Object annotations
40,13.6.1. @Definition
40,"Annotate a SqlObject type, or a method or field within, with @Definition to define an attribute"
40,for all SqlObject methods of the type. The key will default to the field or method name. The value
40,will default to the result of calling the method or getting the field.
40,"An example use is to take a list of columns and join them using , in order to template in a column list."
40,13.6.2. @GetGeneratedKeys
40,Some SQL statements will cause data to be generated on your behalf at the
40,"database, e.g. a table with an auto-generated primary key, or a primary key"
40,selected from a sequence.
40,The @GetGeneratedKeys annotation may be used to return the keys generated from the SQL statement:
40,"The @GetGeneratedKeys annotation tells Jdbi that the return value should be generated key from the SQL statement, instead of the update count."
40,public interface UserDao {
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (nextval('user_seq'), ?)"")"
40,"@GetGeneratedKeys(""id"")"
40,long insert(String name);
40,Multiple columns may be generated and returned:
40,public interface UserDao {
40,"@SqlUpdate(""INSERT INTO users (id, name, created_on) VALUES (nextval('user_seq'), ?, now())"")"
40,"@GetGeneratedKeys({""id"", ""created_on""})"
40,@RegisterBeanMapper(IdCreateTime.class)
40,IdCreateTime insert(String name);
40,"Databases vary in support for generated keys. Some support only one generated key column per statement, and some (such as Postgres) can return the entire row."
40,You should check your database vendor’s documentation before relying on this behavior.
40,@SqlBatch also supports the annotation:
40,public void sqlObjectBatchKeys() {
40,"db.useExtension(UserDao.class, dao -> {"
40,"List<User> users = dao.createUsers(""Alice"", ""Bob"", ""Charlie"");"
40,assertThat(users).hasSize(3);
40,assertThat(users.get(0).id).isOne();
40,"assertThat(users.get(0).name).isEqualTo(""Alice"");"
40,assertThat(users.get(1).id).isEqualTo(2);
40,"assertThat(users.get(1).name).isEqualTo(""Bob"");"
40,assertThat(users.get(2).id).isEqualTo(3);
40,"assertThat(users.get(2).name).isEqualTo(""Charlie"");"
40,});
40,public interface UserDao {
40,"@SqlBatch(""INSERT INTO users (name) VALUES(?)"")"
40,@GetGeneratedKeys
40,List<User> createUsers(String... names);
40,"When using @SqlBatch, the @GetGeneratedKeys annotations tells SQL Object"
40,"that the return value should be the generated keys from each SQL statement, instead of the update count."
40,public interface UserDao {
40,"@SqlBatch(""INSERT INTO users (id, name) VALUES (nextval('user_seq'), ?)"")"
40,"@GetGeneratedKeys(""id"")"
40,long[] bulkInsert(List<String> names); (1)
40,Returns the generated ID for each inserted name.
40,Multiple columns may be generated and returned in this way:
40,public interface UserDao {
40,"@SqlBatch(""INSERT INTO users (id, name, created_on) VALUES (nextval('user_seq'), ?, now())"")"
40,"@GetGeneratedKeys({""id"", ""created_on""})"
40,@RegisterBeanMapper(IdCreateTime.class)
40,List<IdCreateTime> bulkInsert(String... names);
40,Postgres supports additional functionality when returning generated keys. See PostgreSQL for more details.
40,13.6.3. @SqlLocator
40,"When SQL statements grow in complexity, it may be cumbersome to provide"
40,the statements as Java strings in the SQL method annotations.
40,Jdbi supports the Java Textblock construct available in Java 15 and later. Using a text block is an elegant way to specify multi-line SQL statements.
40,Jdbi provides annotations that let you configure external locations to
40,load SQL statements.
40,@UseAnnotationSqlLocator - use
40,annotation value of the SQL method annotations (this is the default behavior)
40,"@UseClasspathSqlLocator - loads SQL from a file on the classpath, based on the package and name of the SQL Object interface type."
40,package com.foo;
40,@UseClasspathSqlLocator
40,interface BarDao {
40,// loads classpath resource com/foo/BarDao/query.sql
40,@SqlQuery
40,void query();
40,"@UseClasspathSqlLocator is implemented using the ClasspathSqlLocator, as described above."
40,"ClasspathSqlLocator loads a file unchanged by default. Using the @UseClasspathSqlLocator annotation will strip out comments by default! This may lead to unexpected behavior, e.g. SQL Server uses the # character to denote temporary tables. This can be controlled with the stripComments annotation attribute."
40,package com.foo;
40,@UseClasspathSqlLocator(stripComments=false)
40,interface BarDao {
40,// loads classpath resource com/foo/BarDao/query.sql without stripping comment lines
40,@SqlQuery
40,void query();
40,"The jdbi-stringtemplate4 module provides @UseStringTemplateSqlLocator, a SqlLocator, which can load SQL templates from StringTemplate 4 files on the classpath."
40,"The jdbi-freemarker module provides @UseFreemarkerSqlLocator, which can load SQL templates from Freemarker files on the classpath."
40,13.6.4. @CreateSqlObject
40,Use the @CreateSqlObject annotation to reuse a SQL Object type within another object.
40,This is an example where a SQL update defined in a different SQL Object type is executed as part of a transaction:
40,public interface Bar {
40,"@SqlUpdate(""INSERT INTO bar (name) VALUES (:name)"")"
40,@GetGeneratedKeys
40,"int insert(@Bind(""name"") String name);"
40,public interface Foo {
40,@CreateSqlObject
40,Bar createBar();
40,"@SqlUpdate(""INSERT INTO foo (bar_id, name) VALUES (:bar_id, :name)"")"
40,"void insert(@Bind(""bar_id"") int barId, @Bind(""name"") String name);"
40,@Transaction
40,"default void insertBarAndFoo(String barName, String fooName) {"
40,int barId = createBar().insert(barName);
40,"insert(barId, fooName);"
40,The @CreateSqlObject annotation can also be used to
40,process cursor-type objects inside interface default methods:
40,interface NestedDao {
40,@CreateSqlObject
40,UserDao userDao(); (1)
40,default List<String> getNames() {
40,try (Stream<String> stream = userDao().getNamesAsStream()) {
40,return stream.collect(Collectors.toList());
40,@Test
40,void testOnDemandNestedMethod() {
40,NestedDao dao = jdbi.onDemand(NestedDao.class);
40,List<String> result = dao.getNames();
40,assertThat(result).containsAll(names);
40,Returns a nested object that provides a stream of users.
40,13.6.5. @Timestamped
40,You can annotate any statement with @Timestamped to bind an OffsetDateTime object representing the current time as now:
40,public interface Bar {
40,@Timestamped (1)
40,"@SqlUpdate(""INSERT INTO times(val) VALUES(:now)"") (2)"
40,int insert();
40,Bind a timestamp as now
40,Use the timestamp as a named binding.
40,The binding name can be customized:
40,public interface Bar {
40,"@Timestamped(""timestamp"")"
40,"@SqlUpdate(""INSERT INTO times(val) VALUES(:timestamp)"")"
40,int insert();
40,The TimestampedConfig config object allows setting the timezone for the timestamp.
40,13.6.6. @SingleValue
40,"Sometimes, when using advanced SQL features like Arrays, a container type such as"
40,"int[] or List<Integer> can ambiguously mean either ""a single SQL int[]"" or"
40,"""a ResultSet of int""."
40,"Since arrays are not commonly used in normalized schemas, SQL Object assumes by"
40,default that you are collecting a ResultSet into a container object. You can
40,annotate a return type as @SingleValue to override this.
40,"For example, suppose we want to select a varchar[] column from a single row:"
40,public interface UserDao {
40,"@SqlQuery(""SELECT roles FROM users WHERE id = ?"")"
40,@SingleValue
40,List<String> getUserRoles(long userId)
40,"Normally, Jdbi would interpret List<String> to mean that the mapped type is"
40,"String, and to collect all result rows into a list. The @SingleValue"
40,annotation causes Jdbi to treat List<String> as the mapped type instead.
40,"It’s tempting to use the @SingleValue annotation on an Optional<T> type , but usually this is not needed."
40,Optional<T> is implemented as a container of zero-or-one elements.
40,Adding @SingleValue implies that the database itself has a column of a type like optional<varchar>.
40,"The @SingleValue annotation can also be used on an array, collection or iterable method parameter. This causes SQL Object to bind the whole iterable as the parameter value. This is especially useful for @SqlBatch operations (often for a SQL Array parameter):"
40,public interface UserDao {
40,"@SqlBatch(""INSERT INTO users (id, name, roles) VALUES (?, ?, ?)"")"
40,"void bulkInsert(List<Long> ids,"
40,"List<String> names,"
40,@SingleValue List<String> roles);
40,"In this example, each new row would get the same varchar[] value in the roles column."
40,"13.6.7. Annotations for Map<K,V> Results"
40,"SQL Object methods may return Map<K,V> types (see Map.Entry mapping in"
40,"the Core API). In this scenario, each row is mapped to a Map.Entry<K,V>,"
40,and the entries for each row are collected into a single Map instance.
40,A mapper must be registered for both the key and value types.
40,"Gather master/detail join rows into a map, simply by registering mappers"
40,for the key and value types.
40,"@SqlQuery(""select u.id u_id, u.name u_name, p.id p_id, p.phone p_phone """
40,"+ ""from \""user\"" u left join phone p on u.id = p.user_id"")"
40,"@RegisterConstructorMapper(value = User.class, prefix = ""u"")"
40,"@RegisterConstructorMapper(value = Phone.class, prefix = ""p"")"
40,"Map<User, Phone> getMap();"
40,"In the preceding example, the User mapper uses the ""u"" column name prefix, and"
40,"the Phone mapper uses ""p"". Since each mapper only reads the column with the"
40,"expected prefix, the respective id columns are unambiguous."
40,A unique index (e.g. by ID column) can be obtained by setting the key column
40,name:
40,"@SqlQuery(""select * from \""user\"""")"
40,"@KeyColumn(""id"")"
40,@RegisterConstructorMapper(User.class)
40,"Map<Integer, User> getAll();"
40,Set both the key and value column names to gather a two-column query into a map
40,result:
40,"@SqlQuery(""select \""key\"", \""value\"" from config"")"
40,"@KeyColumn(""key"")"
40,"@ValueColumn(""value"")"
40,"Map<String, String> getAll();"
40,All of the above examples assume a one-to-one key/value relationship.
40,What if there is a one-to-many relationship? Google Guava provides a Multimap
40,"type, which supports mapping multiple values per key."
40,"First, follow the instructions in the Google Guava section to install the"
40,GuavaPlugin.
40,"Then, simply specify a Multimap return type instead of Map:"
40,"@SqlQuery(""select u.id u_id, u.name u_name, p.id p_id, p.phone p_phone """
40,"+ ""from \""user\"" u left join phone p on u.id = p.user_id"")"
40,"@RegisterConstructorMapper(value = User.class, prefix = ""u"")"
40,"@RegisterConstructorMapper(value = Phone.class, prefix = ""p"")"
40,"Multimap<User, Phone> getMultimap();"
40,All the examples so far have been Map types where each row in the result set
40,"is a single Map.Entry. However, what if the Map we want to return is"
40,actually a single row or even a single column?
40,Jdbi’s MapMapper maps each row to a
40,"Map<String, Object>, where column names are mapped to column values."
40,Jdbi’s default setting is to convert column names to lowercase for Map keys. This behavior can be
40,changed via the MapMappers config class.
40,"By default, SQL Object treats Map return types as a collection of Map.Entry"
40,"values. Use the @SingleValue annotation to override this, so that the return"
40,type is treated as a single value instead of a collection:
40,"@SqlQuery(""SELECT * FROM users WHERE id = ?"")"
40,@RegisterRowMapper(MapMapper.class)
40,@SingleValue
40,"Map<String, Object> getById(long userId);"
40,"The GenericMapMapperFactory,"
40,provides the same feature but allows value types other than Object as long as a suitable ColumnMapper
40,is registered and all columns are of the same type:
40,"@SqlQuery(""SELECT 1.0 AS LOW, 2.0 AS MEDIUM, 3.0 AS HIGH"")"
40,@RegisterRowMapperFactory(GenericMapMapperFactory.class)
40,@SingleValue
40,"Map<String, BigDecimal> getNumericLevels();"
40,"The PostgreSQL plugin provides an hstore to Map<String, String> column mapper. See hstore for more information."
40,13.6.8. @UseRowReducer
40,@SqlQuery methods that use join queries may reduce master-detail joins
40,into one or more master-level objects. See ResultBearing.reduceRows() for
40,an introduction to row reducers.
40,"Consider a filesystem metaphor with folders and documents. In the join, we’ll"
40,prefix folder columns with f_ and document columns with d_.
40,"@RegisterBeanMapper(value = Folder.class, prefix = ""f"") (1)"
40,"@RegisterBeanMapper(value = Document.class, prefix = ""d"")"
40,public interface DocumentDao {
40,"@SqlQuery(""SELECT "" +"
40,"""f.id f_id, f.name f_name, "" +"
40,"""d.id d_id, d.name d_name, d.contents d_contents "" +"
40,"""FROM folders f LEFT JOIN documents d "" +"
40,"""ON f.id = d.folder_id "" +"
40,"""WHERE f.id = :folderId"" +"
40,"""ORDER BY d.name"")"
40,@UseRowReducer(FolderDocReducer.class) (2)
40,Optional<Folder> getFolder(int folderId); (3)
40,"@SqlQuery(""SELECT "" +"
40,"""f.id f_id, f.name f_name, "" +"
40,"""d.id d_id, d.name d_name, d.contents d_contents "" +"
40,"""FROM folders f LEFT JOIN documents d "" +"
40,"""ON f.id = d.folder_id "" +"
40,"""ORDER BY f.name, d.name"")"
40,@UseRowReducer(FolderDocReducer.class) (2)
40,List<Folder> listFolders(); (3)
40,"class FolderDocReducer implements LinkedHashMapRowReducer<Integer, Folder> { (4)"
40,@Override
40,"public void accumulate(Map<Integer, Folder> map, RowView rowView) {"
40,"Folder f = map.computeIfAbsent(rowView.getColumn(""f_id"", Integer.class), (5)"
40,id -> rowView.getRow(Folder.class));
40,"if (rowView.getColumn(""d_id"", Integer.class) != null) { (6)"
40,f.getDocuments().add(rowView.getRow(Document.class));
40,"In this example, we register the folder and document mappers with a"
40,"prefix, so that each mapper only looks at the columns with that prefix."
40,These mappers are used indirectly by the row reducer in the
40,getRow(Folder.class) and getRow(Document.class) calls.
40,"Annotate the method with @UseRowReducer, and specify the RowReducer"
40,implementation class.
40,The same RowReducer implementation may be used for both single- and
40,multi-master-record queries.
40,LinkedHashMapRowReducer
40,is an abstract RowReducer implementation that uses a LinkedHashMap as the result
40,"container, and returns the values() collection as the result."
40,"Get the Folder for this row from the map by ID, or create it if not in the map."
40,Confirm this row has a document (this is a left join) before mapping a document
40,and adding it to the folder.
40,13.6.9. @RegisterCollector and @RegisterCollectorFactory
40,Convenience annotations to register a Collector for a SqlObject method.
40,@RegisterCollectorFactory allows registration of a factory that returns a collector instance and @RegisterCollector registers a collector implementation:
40,public interface RegisterCollectorDao {
40,@RegisterCollector(StringConcatCollector.class)
40,"@SqlQuery(""select i from i order by i asc"")"
40,String selectWithCollector();
40,@RegisterCollectorFactory(StringConcatCollectorFactory.class)
40,"@SqlQuery(""select i from i order by i asc"")"
40,String selectWithCollectorFactory();
40,class StringConcatCollectorFactory implements CollectorFactory {
40,@Override
40,public boolean accepts(Type containerType) {
40,return containerType == String.class;
40,@Override
40,public Optional<Type> elementType(Type containerType) {
40,return Optional.of(Integer.class);
40,@Override
40,"public Collector<Integer, List<Integer>, String> build(Type containerType) {"
40,return Collector.of(
40,"ArrayList::new,"
40,"List::add,"
40,"(x, y) -> {"
40,x.addAll(y);
40,return x;
40,"i -> i.stream().map(Object::toString).collect(Collectors.joining("" "")));"
40,"class StringConcatCollector implements Collector<Integer, List<Integer>, String> {"
40,@Override
40,public Supplier<List<Integer>> supplier() {
40,return ArrayList::new;
40,@Override
40,"public BiConsumer<List<Integer>, Integer> accumulator() {"
40,return List::add;
40,@Override
40,public BinaryOperator<List<Integer>> combiner() {
40,"return (a, b) -> {"
40,a.addAll(b);
40,return a;
40,@Override
40,"public Function<List<Integer>, String> finisher() {"
40,"return i -> i.stream().map(Object::toString).collect(Collectors.joining("" ""));"
40,@Override
40,public Set<Characteristics> characteristics() {
40,return Collections.emptySet();
40,"The element and result types are inferred from the concrete Collector<Element, ?, Result> implementation’s type parameters. See Collectors for more details."
40,13.6.10. Other SQL Object annotations
40,Jdbi provides many additional annotations out of the box:
40,org.jdbi.v3.sqlobject.config
40,provides annotations for things that can be configured at the Jdbi or
40,"Handle level. This includes registration of mappers and arguments, and for"
40,configuring SQL statement rendering and parsing.
40,org.jdbi.v3.sqlobject.customizer
40,"provides annotations for binding parameters, defining attributes, and"
40,controlling the fetch behavior of the statement’s result set.
40,org.jdbi.v3.jpa
40,"provides the @BindJpa annotation, for binding properties to column according"
40,to JPA @Column annotations.
40,org.jdbi.v3.sqlobject.locator
40,provides annotations that configure Jdbi to load SQL statements from an
40,"alternative source, e.g. a file on the classpath."
40,org.jdbi.v3.sqlobject.statement
40,"provides the @MapTo annotation, which is used for dynamically specifying the"
40,mapped type at the time the method is invoked.
40,org.jdbi.v3.stringtemplate4
40,provides annotations that configure Jdbi to load SQL from StringTemplate 4
40,".stg files on the classpath, and/or to parse SQL templates using the"
40,ST4 template engine.
40,org.jdbi.v3.sqlobject.transaction
40,provides annotations for transaction management in a SQL object. See
40,SQL Object Transactions for details.
40,The SQL Object framework is built on top of the Jdbi Extension framework which can be extended with user-defined annotations. See the
40,extension framework annotation documentation for an overview on how to build your own annotations.
40,13.6.11. Annotations and Inheritance
40,SQL Objects inherit methods and annotations from the interfaces they extend:
40,package com.app.dao;
40,@UseClasspathSqlLocator
40,(1) (2)
40,"public interface CrudDao<T, ID> {"
40,@SqlUpdate (3)
40,void insert(@BindBean T entity);
40,@SqlQuery (3)
40,Optional<T> findById(ID id);
40,@SqlQuery
40,List<T> list();
40,@SqlUpdate
40,void update(@BindBean T entity);
40,@SqlUpdate
40,void deleteById(ID id);
40,See @SqlLocator.
40,Class annotations are inherited by subtypes.
40,"Method and parameter annotations are inherited by subtypes, unless the"
40,subtype overrides the method.
40,package com.app.contact;
40,@RegisterBeanMapper(Contact.class)
40,"public interface ContactDao extends CrudDao<Contact, Long> {}"
40,package com.app.account;
40,@RegisterConstructorMapper(Account.class)
40,"public interface AccountDao extends CrudDao<Account, UUID> {}"
40,"In this example we’re using the @UseClasspathSqlLocator annotation, so each"
40,"method will use SQL loaded from the classpath. Thus, ContactDao methods will"
40,use SQL from:
40,/com/app/contact/ContactDao/insert.sql
40,/com/app/contact/ContactDao/findById.sql
40,/com/app/contact/ContactDao/list.sql
40,/com/app/contact/ContactDao/update.sql
40,/com/app/contact/ContactDao/deleteById.sql
40,Whereas AccountDao will use SQL from:
40,/com/app/account/AccountDao/insert.sql
40,/com/app/account/AccountDao/findById.sql
40,/com/app/account/AccountDao/list.sql
40,/com/app/account/AccountDao/update.sql
40,/com/app/account/AccountDao/deleteById.sql
40,Suppose Account used name()-style accessors instead of getName(). In that
40,"case, we’d want AccountDao to use @BindMethods instead of @BindBean."
40,Let’s override those methods with the right annotations:
40,package com.app.account;
40,@RegisterConstructorMapper(Account.class)
40,"public interface AccountDao extends CrudDao<Account, UUID> {"
40,@Override
40,@SqlUpdate (1)
40,void insert(@BindMethods Account entity);
40,@Override
40,@SqlUpdate (1)
40,void update(@BindMethods Account entity);
40,"Method annotations are not inherited on override, so we must duplicate"
40,those we want to keep.
40,13.7. Combining SQL Object and the core API
40,The SQL Object extension uses the same core API as programmatic SQL operations. There is an underlying Handle object that is used to execute the SQL methods.
40,A SQL Object type can extend the SqlObject mixin interface to get access to the handle. It also exposes a number of operations that can be used to mix declarative SQL Object methods with programmatic core API code:
40,public interface SomeDao extends SqlObject {
40,// ...
40,void mixedCode() {
40,SomeDao dao = jdbi.onDemand(SomeDao.class);
40,dao.withHandle(handle -> {
40,"handle.createQuery(""SELECT * from users"").mapTo(User.class).list();"
40,This interface gives access to the handle and offers SqlObject#withHandle() and
40,SqlObject#useHandle() methods that execute callback code using the same handle object as the methods on the SQL object interface itself.
40,"This is most useful for SQL object instances that get passed from other code, have been created from"
40,Jdbi#onDemand() or use default methods. Any object that was created using the
40,Handle#attach() method will return the handle it was attached to.
40,13.7.1. Getting access to the handle with the SqlObject mixin
40,"Occasionally a use case comes up where SQL Method annotations don’t fit. In these situations, it is possible to ""drop down"" to the Core API using interface"
40,default methods and the SqlObject mixin interface:
40,public interface SplineDao extends SqlObject {
40,default void reticulateSplines(Spline spline) {
40,Handle handle = getHandle();
40,// do tricky stuff using the Core API
40,The Handle can be used for most operations available in the fluent API.
40,"A handle should only be obtained in this way for very specific reasons, and it is a good practice to restrain the use of the handle to code within a Dao class (e.g. a default method). If fluent code and declarative code needs to be mixed, moving from fluent to declarative should be preferred."
40,"A very notable exception is modifying the configuration of the handle. When a SQL object is created, every method on the SQL object gets its own, distinct configuration object. Calling any configuration modification method (any set* or register* method from the Configurable interface) will modify only the configuration object for the getHandle() method but not for any other method."
40,The following code example DOES NOT WORK!
40,@Test
40,public void testSqlObjectHandleConfigDoesNotWork() {
40,List<Sulu> sulus = Arrays.asList(
40,"new Sulu(1, ""George"", ""Takei""),"
40,"new Sulu(2, ""John"", ""Cho""));"
40,"db.useExtension(SuluDao.class, s -> {"
40,Handle h = s.getHandle();
40,// THIS DOES NOT WORK! The argument factory is only
40,"// registered for the getHandle() method, not"
40,// for the insertSulus() method!
40,h.registerArgument(new SuluArgumentFactory());
40,// This call results in an exception that no argument
40,// factory for the Sulu data type could be found!
40,dao.insertSulus(sulus);
40,public interface SuluDao extends SqlObject {
40,"@SqlBatch(""insert into something (id, name) values (:bean.id, :sulu)"")"
40,"void insertSulus(@Bind(""sulu"") @BindBean(""bean"") List<Sulu> sulus);"
40,The code above can be rewritten to move from fluent to declarative API:
40,public void testMixFluentAndSqlObject() {
40,List<Sulu> sulus = Arrays.asList(
40,"new Sulu(1, ""George"", ""Takei""),"
40,"new Sulu(2, ""John"", ""Cho""));"
40,db.withHandle(h -> {
40,// register the argument with the handle before attaching
40,// the dao to the handle
40,h.registerArgument(new SuluArgumentFactory());
40,SuluDao dao = h.attach(SuluDao.class);
40,// all methods on the dao have received the configuration with the
40,// argument factory
40,dao.insertSulus(sulus);
40,return null;
40,});
40,13.7.2. Using default Methods
40,Default methods can be used to group multiple SQL operations into a single method call:
40,public interface ContactPhoneDao {
40,"@SqlUpdate(""INSERT INTO contacts (id, name) VALUES (nextval('contact_id'), :name)"")"
40,long insertContact(@BindBean Contact contact);
40,"@SqlBatch(""INSERT INTO phones (contact_id, type, phone) VALUES (:contactId, :type, :phone)"")"
40,"void insertPhone(long contactId, @BindBean Iterable<Phone> phones);"
40,default long insertFullContact(Contact contact) {
40,long id = insertContact(contact);
40,"insertPhone(id, contact.getPhones());"
40,return id;
40,13.8. SQL Object Transactions
40,Methods on a SQL object can have a @Transaction annotation:
40,@Transaction
40,"@SqlQuery(""SELECT * FROM users WHERE id=:id"")"
40,Optional<User> findUserById(int id);
40,SQL methods with a @Transaction annotation may optionally specify a
40,transaction isolation level:
40,@Transaction(TransactionIsolationLevel.READ_COMMITTED)
40,"@SqlUpdate(""INSERT INTO USERS (name) VALUES (:name)"")"
40,void insertUser(String name);
40,"Similar to the Jdbi#inTransaction() and Jdbi#useTransaction() operations in the core API, transactions are not nested. If a method, that has been annotated with @Transaction, calls another method that is annotated as well (e.g. through an interface default method), then the same transaction will be reused, and it will be committed when the outer transaction ends."
40,"Nested method calls must either use the same transaction isolation level or inner methods must not specify any transaction level. In that case, the transaction level of the outer transaction maintained."
40,@Transaction(TransactionIsolationLevel.READ_UNCOMMITTED)
40,default void outerMethodCallsInnerWithSameLevel() {
40,// this works: isolation levels agree
40,innerMethodSameLevel();
40,@Transaction(TransactionIsolationLevel.READ_UNCOMMITTED)
40,default void innerMethodSameLevel() {}
40,@Transaction(TransactionIsolationLevel.READ_COMMITTED)
40,default void outerMethodWithLevelCallsInnerMethodWithNoLevel() {
40,"// this also works: inner method doesn't specify a level, so the outer method controls."
40,innerMethodWithNoLevel();
40,@Transaction
40,default void innerMethodWithNoLevel() {}
40,@Transaction(TransactionIsolationLevel.REPEATABLE_READ)
40,default void outerMethodWithOneLevelCallsInnerMethodWithAnotherLevel() throws TransactionException {
40,// error! inner method specifies a different isolation level.
40,innerMethodWithADifferentLevel();
40,@Transaction(TransactionIsolationLevel.SERIALIZABLE)
40,default void innerMethodWithADifferentLevel() {}
40,13.8.1. Executing multiple SQL operations in a single transaction
40,A SQL object method uses its own transaction. Multiple method calls can be grouped together to use a shared transaction:
40,"For an existing handle, attach the SQL object inside the transaction:"
40,public interface UserDao {
40,@Transaction
40,"@SqlUpdate(""INSERT INTO users VALUES (:id, :name)"")"
40,"void createUser(int id, String name);"
40,public void createUsers(Handle handle) {
40,handle.useTransaction(transactionHandle -> {
40,UserDao dao = transactionHandle.attach(UserDao.class);
40,// both inserts happen in the same transaction
40,"dao.createUser(1, ""Alice"");"
40,"dao.createUser(2, ""Bob"");"
40,"As an alternative, the Transactional mixin interface can be used to provide transaction support on a SQL object."
40,"Similar to the SqlObject interface, it gives access to all transaction related methods on the Handle and offers the same callbacks as the Handle itself."
40,"With this mixin interface, the Transactional#useTransaction() and Transactional#inTransaction() methods group statements into a single transaction by using a callback:"
40,public interface UserDao extends Transactional<UserDao> {
40,@Transaction
40,"@SqlUpdate(""INSERT INTO users VALUES (:id, :name)"")"
40,"void createUser(int id, String name);"
40,public void createUsers(Jdbi jdbi) {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,dao.useTransaction(transactionDao -> {
40,// both inserts happen in the same transaction
40,"transactionDao.createUser(1, ""Alice"");"
40,"transactionDao.createUser(2, ""Bob"");"
40,});
40,Nested calls can be used as well:
40,public interface UserDao extends Transactional<UserDao> {
40,@Transaction
40,"@SqlUpdate(""INSERT INTO users VALUES (:id, :name)"")"
40,"void createUser(int id, String name);"
40,public void createUsers(Jdbi jdbi) {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,dao.useTransaction(transactionDao1 -> {
40,// both inserts happen in the same transaction
40,"transactionDao1.createUser(1, ""Alice"");"
40,transactionDao1.useTransaction(transactionDao2 -> {
40,"transactionDao2.createUser(2, ""Bob"");"
40,});
40,});
40,SQL object calls and core API calls can be mixed:
40,public interface UserDao extends Transactional<UserDao> {
40,@Transaction
40,"@SqlUpdate(""INSERT INTO users VALUES (:id, :name)"")"
40,"void createUser(int id, String name);"
40,public void createUsers(Jdbi jdbi) {
40,UserDao dao = jdbi.onDemand(UserDao.class);
40,dao.useTransaction(transactionDao -> {
40,// inserts happen in the same transaction
40,"transactionDao.createUser(1, ""Alice"");"
40,"transactionDao.createUser(2, ""Bob"");"
40,// this insert as well
40,transactionDao.getHandle().useTransaction(transactionHandle ->
40,"transactionHandle.createUpdate(""USERT INTO users VALUES (:id, :name)"")"
40,".bind(""id"", 3)"
40,".bind(""name"", ""Charlie"")"
40,.execute();
40,The Transactional interface
40,has the same constraints as the SQLObject mixin. Especially modifying the configuration by obtaining a handle using getHandle() and then calling any set* or register* method from the Configurable interface is not supported and will modify only the configuration for the getHandle() method.
40,14. Miscellaneous
40,14.1. Generated Keys
40,An Update or PreparedBatch may automatically generate keys. These keys
40,are treated separately from normal results. Depending on your database and
40,"configuration, the entire inserted row may be available."
40,There is a lot of variation between databases supporting
40,this feature so please test this feature’s interaction with your database
40,thoroughly.
40,"In PostgreSQL, the entire row is available, so you can immediately map your"
40,inserted names back to full User objects!
40,This avoids the overhead of
40,separately querying after the insert completes.
40,Consider the following table:
40,public static class User {
40,final int id;
40,final String name;
40,"public User(int id, String name) {"
40,this.id = id;
40,this.name = name;
40,@BeforeEach
40,public void setUp() {
40,db = pgExtension.getJdbi();
40,"db.useHandle(h -> h.execute(""CREATE TABLE users (id SERIAL PRIMARY KEY, name VARCHAR)""));"
40,db.registerRowMapper(ConstructorMapper.factory(User.class));
40,You can get generated keys in the fluent style:
40,public void fluentInsertKeys() {
40,db.useHandle(handle -> {
40,"User data = handle.createUpdate(""INSERT INTO users (name) VALUES(?)"")"
40,".bind(0, ""Data"")"
40,.executeAndReturnGeneratedKeys()
40,.mapTo(User.class)
40,.one();
40,assertThat(data.id).isOne(); // this value is generated by the database
40,"assertThat(data.name).isEqualTo(""Data"");"
40,});
40,14.2. Qualified Types
40,"Sometimes the same Java object can correspond to multiple data types in a database. For example,"
40,"a String could be varchar plaintext, nvarchar text, json data, etc., all with different handling requirements."
40,QualifiedType allows you to add such context to a Java type:
40,QualifiedType.of(String.class).with(Json.class);
40,"This QualifiedType still represents the String type, but qualified with the @Json annotation."
40,"It can be used in a way similar to GenericType, to make components"
40,"handling values (mainly ArgumentFactories and ColumnMapperFactories) perform their work differently,"
40,and to have the values handled by different implementations altogether:
40,@Json
40,public class JsonArgumentFactory extends AbstractArgumentFactory<String> {
40,@Override
40,"protected Argument build(String value, ConfigRegistry config) {"
40,// do something specifically for json data
40,"Once registered, this @Json qualified factory will receive only @Json String values."
40,Other factories not qualified as such will not receive this value:
40,QualifiedType<String> json = QualifiedType.of(String.class).with(Json.class);
40,"query.bindByType(""jsonValue"", ""{\""foo\"":1}"", json);"
40,Jdbi chooses factories to handle values by exactly matching their qualifiers. It’s up to the
40,factory implementations to discriminate on the type of the value afterward.
40,"Qualifiers are implemented as Annotations. This allows factories to independently inspect values for qualifiers at the source,"
40,"such as on their Class, to alter their own behavior or to requalify a value"
40,and have it re-evaluated by Jdbi’s lookup chain.
40,Qualifiers being annotations does not mean they inherently activate
40,their function when placed in source classes. Each feature decides its own
40,rules regarding their use.
40,"Arguments can only be qualified for binding via bindByType calls, not regular"
40,"bind or update.execute(Object...). Also, arrays cannot be qualified."
40,These features currently make use of qualified types:
40,@NVarchar and @MacAddr (the latter in jdbi3-postgres) bind and map Strings as nvarchar and macaddr
40,"respectively, instead of the usual varchar."
40,jdbi3-postgres offers HStore.
40,JSON
40,"BeanMapper, @BindBean, @RegisterBeanMapper, mapTobean(), and bindBean() respect qualifiers on getters, setters,"
40,and setter parameters.
40,ConstructorMapper and @RegisterConstructorMapper respect qualifiers on constructor parameters.
40,@BindMethods and bindMethods() respect qualifiers on methods.
40,"@BindFields, @RegisterFieldMapper, FieldMapper and bindFields() respect qualifiers on fields."
40,SqlObject respects qualifiers on methods (applies them to the return type) and parameters.
40,"on parameters of type Consumer<T>, qualifiers are applied to the T."
40,@MapTo
40,@BindJpa and JpaMapper respect qualifiers on getters and setters.
40,"@BindKotlin, bindKotlin(), and KotlinMapper respect qualifiers on constructor parameters, getters, setters, and setter parameters."
40,14.3. Query Templating
40,"Binding query parameters, as described above, is excellent for sending a static set of parameters to the database engine."
40,Binding ensures that the parameterized query string (... where foo = ?) is transmitted to the database without allowing hostile parameter values to inject SQL.
40,"Bound parameters are not always enough. Sometimes a query needs complicated or structural changes before being executed, and parameters just don’t cut it. Templating (using a TemplateEngine) allows you to alter a query’s content with general String manipulations."
40,"Typical uses for templating are optional or repeating segments (conditions and loops), complex variables such as comma-separated lists for IN clauses, and variable substitution for non-bindable SQL elements (like table names). Unlike argument binding, the rendering of attributes performed by TemplateEngines is not SQL-aware. Since they perform generic String manipulations, TemplateEngines can easily produce horribly mangled or subtly defective queries if you don’t use them carefully."
40,Query templating is a common attack vector! Always prefer binding parameters to static SQL over dynamic SQL when possible.
40,"handle.createQuery(""SELECT * FROM <TABLE> WHERE name = :n"")"
40,"// -> ""SELECT * FROM Person WHERE name = :n"""
40,".define(""TABLE"", ""Person"")"
40,"// -> ""SELECT * FROM Person WHERE name = 'MyName'"""
40,".bind(""n"", ""MyName"");"
40,Use a TemplateEngine to perform crude String manipulations on a query. Query parameters should be handled by Arguments.
40,"TemplateEngines and SqlParsers operate sequentially: the initial String will be rendered by the TemplateEngine using attributes, then parsed by the SqlParser with Argument bindings."
40,"If the TemplateEngine outputs text matching the parameter format of the SqlParser, the parser will attempt to bind an Argument to it. This can be useful to e.g. have named parameters of which the name itself is also a variable, but can also cause confusing bugs:"
40,"String paramName = ""arg"";"
40,"handle.createQuery(""SELECT * FROM Foo WHERE bar = :<attr>"")"
40,".define(""attr"", paramName)"
40,// ...
40,".bind(paramName, ""baz""); // <- does not need to know the parameter's name (""arg"")!"
40,"handle.createQuery(""SELECT * FROM emotion WHERE emoticon = <sticker>"")"
40,".define(""sticker"", "":-)"") // -> ""... WHERE emoticon = :-)"""
40,.mapToMap()
40,"// exception: no binding/argument named ""-)"" present"
40,.list();
40,Bindings and definitions are usually separate.
40,You can link them in a limited manner
40,using the stmt.defineNamedBindings() or @DefineNamedBindings customizers.
40,"For each bound parameter (including bean properties), this will define a boolean which is true if the"
40,binding is present and not null.
40,You can use this to
40,craft conditional updates and query clauses.
40,"For example,"
40,class MyBean {
40,long id();
40,String getA();
40,String getB();
40,Instant getModified();
40,"handle.createUpdate(""UPDATE mybeans SET <if(a)>a = :a,<endif> <if(b)>b = :b,<endif> modified=now() WHERE id=:id"")"
40,.bindBean(mybean)
40,.defineNamedBindings()
40,.execute();
40,Also see the section about TemplateEngine.
40,14.3.1. ClasspathSqlLocator
40,You may find it helpful to store your SQL templates in individual files on the
40,"classpath, rather than in string inside Java code."
40,"The ClasspathSqlLocator converts Java type and method names into classpath locations,"
40,"and then reads, parses, and caches the loaded statements."
40,// reads classpath resource com/foo/BarDao/query.sql
40,"ClasspathSqlLocator.create().locate(com.foo.BarDao.class, ""query"");"
40,// same resource as above
40,"ClasspathSqlLocator.create().locate(""com.foo.BarDao.query"");"
40,"By default, any comments in the loaded file are left untouched. Comments can be stripped out by"
40,instantiating the ClasspathSqlLocator with the removingComments() method:
40,"// reads classpath resource com/foo/BarDao/query.sql, stripping all comments"
40,"ClasspathSqlLocator.removingComments().locate(com.foo.BarDao.class, ""query"");"
40,// same resource as above
40,"ClasspathSqlLocator.removingComments().locate(""com.foo.BarDao.query"");"
40,Multiple comment styles are supported:
40,C-style (/* ... */ and // to the end of the line)
40,SQL style (-- to the end of the line)
40,shell style (# to the end of the line; except when followed immediately by the > character; this is required for the Postgres #> and #>> operators).
40,Each piece of core or extension that wishes to participate in
40,"configuration defines a configuration class, for example the SqlStatements"
40,class stores SqlStatement related configuration.
40,"Then, on any Configurable context"
40,(like a Jdbi or Handle) you can change configuration in a type safe way:
40,jdbi.getConfig(SqlStatements.class).setUnusedBindingAllowed(true);
40,jdbi.getConfig(Arguments.class).register(new MyTypeArgumentFactory());
40,jdbi.getConfig(Handles.class).setForceEndTransactions(true);
40,"// Or, if you have a bunch of work to do:"
40,"jdbi.configure(RowMappers.class, rm -> {"
40,rm.register(new TypeARowMapperFactory();
40,rm.register(new TypeBRowMapperFactory();
40,});
40,14.4. Statement caching
40,Jdbi caches statement information in multiple places:
40,preparsed SQL where placeholders have been replaced.
40,rendered statement templates if the template engine supports it.
40,"Caching can dramatically speed up the execution of statements. By default, Jdbi uses a simple LRU in-memory cache with 1,000 entries. This cache is sufficient for most use cases."
40,"For applications than run a lot of different SQL operations, it is possible to use different cache implementations. Jdbi provides a cache SPI for this."
40,The following cache implementations are included:
40,jdbi3-caffeine-cache - using the Caffeine cache library. This used to be the default cache up to version 3.36.0
40,"jdbi3-noop-cache, - disables caching. This is useful for testing and debugging."
40,A cache module can provide a plugin to enable it. Only one plugin can be in use at a time (last one installed wins):
40,// use the caffeine cache library
40,jdbi.installPlugin(new CaffeineCachePlugin());
40,14.4.1. Using custom cache instances
40,"The default cache instances are installed when the Jdbi object is created. When using a cache plugin, the implementation is changed but only very few aspects of the cache can be modified."
40,Full customization is possible by creating the cache outside Jdbi and then use a cache builder adapter:
40,// create a custom Caffeine cache
40,JdbiCacheBuilder customCacheBuilder = new CaffeineCacheBuilder(
40,Caffeine.newBuilder()
40,.maximumSize(100_000)
40,".expireAfterWrite(10, TimeUnit.MINUTES)"
40,.recordStats());
40,SqlStatements config = jdbi.getConfig(SqlStatements.class);
40,config.setTemplateCache(customCacheBuilder));
40,config.setSqlParser(new ColonPrefixSqlParser(customCacheBuilder));
40,"When setting the caches explicitly, no cache plugin needs to be installed."
40,"If the underlying cache library exposes per-cache statistics, these can be accessed through the SqlStatements#cacheStats() and CachingSqlParser#cacheStats() methods."
40,15. Testing
40,"The official test support from Jdbi is in the jdbi-testing package. There are a number of additional JUnit 5 extensions in the jdbi-core test artifact. These are only intended for Jdbi internal use and not part of the official, public API."
40,15.1. JUnit 4
40,The jdbi3-testing artifact provides JdbiRule
40,class which implements TestRule and can be used with the Rule and ClassRule annotations.
40,It provides helpers for writing JUnit 4 tests integrated with a managed database instance. This makes writing unit tests quick and easy!
40,"You must remember to include the database dependency itself, for example to get a pure H2 Java database:"
40,<dependency>
40,<groupId>com.h2database</groupId>
40,<artifactId>h2</artifactId>
40,<version>2.1.224</version>
40,<scope>test</scope>
40,</dependency>
40,"JUnit 4 supports the OTJ embedded postgres component, which needs to be included when using Postgres:"
40,<dependency>
40,<groupId>com.opentable.components</groupId>
40,<artifactId>otj-pg-embedded</artifactId>
40,<version>1.0.2</version>
40,<scope>test</scope>
40,</dependency>
40,15.2. JUnit 5
40,The jdbi3-testing artifact provides JdbiExtension for JUnit 5 based tests.
40,It supports both the @RegisterExtension and @ExtendWith annotations.
40,"When using @RegisterExtension, the extensions can be customized further:"
40,public class Test {
40,@RegisterExtension
40,public JdbiExtension h2Extension = JdbiExtension.h2()
40,withPlugin(new SqlObjectPlugin());
40,@Test
40,public void testWithJunit5() {
40,Jdbi jdbi = h2Extension.getJdbi();
40,Handle handle = h2Extension.openHandle();
40,// ...
40,The JdbiExtension and all its database specific subclasses can be registered as instance fields or as class-level (static) fields.
40,"When registered as an instance field, each test will get a new Jdbi instance. The in-memory database specific subclasses (H2, SQLite or HsqlDB with the generic extension) will reset their content and provide a new instance."
40,"When registered as a static field, the extension will be initialized before all test methods are executed. All tests share the same extension instance and its associated fields. The in-memory database specific subclasses will retain their contents across multiple tests."
40,The javadoc page for JdbiExtension contains additional information on how to customize a programmatically registered instance.
40,"When using the @ExtendWith declarative extension, test methods can access the Jdbi and Handle objects through method parameters:"
40,@ExtendWith(JdbiH2Extension.class)
40,public class Test {
40,@Test
40,"public void testWithJunit5(Jdbi jdbi, Handle handle) {"
40,// ...
40,The instances injected are the same instances as returned by getJdbi() and getSharedHandle().
40,Currently supported databases:
40,"Postgres - JdbiPostgresExtension, JdbiExternalPostgresExtension, JdbiOtjPostgresExtension"
40,H2 - JdbiH2Extension
40,Sqlite - JdbiSqliteExtension
40,"Generic JDBC database support - JdbiGenericExtension. This is a minimal extension that can support any database that support JDBC. The driver for the database must be on the classpath. All database management (schema creation, DDL object management etc.) must be done by the calling code."
40,Testcontainers - JdbiTestcontainersExtension. See Using Testcontainers for detailed documentation.
40,Support for other databases wanted!
40,The JdbiExtension provides a number of convenience methods:
40,public class JdbiExtension {
40,public JdbiExtension postgres(...) // new JdbiPostgresExtension(...)
40,public JdbiExtension externalPostgres(...) // new JdbiExternalPostgresExtension(...)
40,public JdbiExtension otjEmbeddedPostgres() // new JdbiOtjPostgresExtension()
40,public JdbiExtension h2() // new JdbiH2Extension()
40,public JdbiExtension sqlite() // new JdbiSqliteExtension()
40,"Additional functionality may be available by using the database specific implementation classes directly. For most common use cases, the convenience methods should suffice."
40,15.2.1. Testing with H2
40,The H2 database is one of the most popular choices for embedded testing. The JdbiH2Extension class allows for a few additional customizations:
40,// JdbiH2Extension(String) constructor allows H2 options
40,// JDBC URL is jdbc:h2:mem:<UUID>;MODE=MySQL
40,"JdbiH2Extension extension = new JdbiH2Extension(""MODE=MySQL"");"
40,// set H2 user
40,"extension.withUser(""h2-user"");"
40,// set H2 user and password
40,"extension.withCredentials(""h2-user"", ""h2-password"");"
40,15.2.2. Testing with Postgres
40,Postgres is supported through multiple test classes:
40,JdbiPostgresExtension manages a local database on the host. It uses the pg-embedded library that can spin up Postgres instances on various OS and architectures. This is the fastest way to get to a Postgres instance. Most of the Jdbi test classes that use Postgres use this extension.
40,JdbiExternalPostgresExtension supports an external Postgres database and can run tests against any local or remote database.
40,JdbiOtjPostgresExtension supports the OpenTable Java Embedded PostgreSQL component.
40,15.3. Using Testcontainers
40,The Testcontainers project provides a number of services as docker containers with a programmatic interface for JVM code. Jdbi supports JDBC type databases with the JdbiTestcontainersExtension:
40,@Testcontainers
40,class TestWithContainers {
40,@Container
40,"static JdbcDatabaseContainer<?> dbContainer = new YugabyteDBYSQLContainer(""yugabytedb/yugabyte"");"
40,@RegisterExtension
40,JdbiExtension extension = JdbiTestcontainersExtension.instance(dbContainer);
40,// ... test code using the yugabyte engine
40,The dbContainer object is registered as class level field for sharing between all the tests in the test class as described in the JUnit 5 documentation. Some IDEs warn that this field should not be static. This is a false positive/bug. Changing dbContainer to an instance field will result in starting and stopping a new database per test which is a significant performance penalty.
40,The JdbiTestcontainersExtension has built-in support to isolate each test method in a test class. The container can be declared as a static class member which speeds up test execution significantly.
40,"There is built-in support for MySQL, MariaDB, TiDB, PostgreSQL, CockroachDB, YugabyteDB, ClickHouseDB, Oracle XE, Oracle free, Trino, MS SQLServer and DB2. It also supports the various compatible flavors (such as PostGIS for PostgreSQL)."
40,"By default, Jdbi tries to run every test with its own schema instance in a single schema. For database engines that do not support schemas, it will use separate catalogs. For database engines that do not support catalogs or schemas (e.g. MySQL), it will use separate databases."
40,"For each database, Jdbi can provide custom user, catalog and schema names and requires one or more SQL statements to initialize an isolated instance for testing."
40,Table 1. Supported Testcontainer databases
40,Database
40,Isolation type
40,Database User
40,Catalog/Database name
40,Schema name
40,"MySQL, MariaDB, TiDB"
40,Catalog
40,root
40,(random)
40,"PostgreSQL, CockroachDB, YugabyteDB"
40,Schema
40,(default)
40,test
40,(random)
40,ClickhouseDB
40,Catalog
40,(default)
40,(random)
40,"Oracle XE, Oracle free"
40,Schema
40,system
40,(default catalog)
40,(random)
40,Trino
40,Schema
40,(default)
40,memory
40,(random)
40,MS SQLServer
40,Catalog
40,(random)
40,DB2
40,Schema
40,(default)
40,test
40,(random)
40,Database user (default): Jdbi uses the default user as returned by the testcontainer class.
40,Isolation type Catalog: A new catalog (or database) is created for each test. The schema name is not used or the database does not support schemas.
40,Isolation type Schema:
40,A new schema is created in the specified catalog (or database).
40,Catalog or Schema name (random): Jdbi will create a random name.
40,15.3.1. Providing custom database information
40,"While Jdbi makes an effort to support all databases provided by the Testcontainers project that support JDBC, there are some situation when it is necessary to provide additional information:"
40,Using a database that is supported by testcontainer but not yet by Jdbi.
40,Using a custom docker image that is incompatible with the default Jdbi testcontainer support.
40,Require specific behavior in tests that is different from the Jdbi testcontainer support.
40,"The TestcontainersDatabaseInformation#of() method allows creation of custom database information instances which then can be passed into JdbiTestcontainersExtension#instance(TestcontainersDatabaseInformation, JdbcDatabaseContainer) to create extensions with custom behavior."
40,This is an example that uses a custom user with a special database image:
40,@Testcontainers
40,class UseCustomMysqlContainer {
40,private static final TestcontainersDatabaseInformation CUSTOM_MYSQL =
40,TestcontainersDatabaseInformation.of(
40,"""custom_user"","
40,// This user must have privileges to create databases
40,"null,"
40,// do not override the generated catalog name
40,"null,"
40,"// mysql does not know anything about schemas, so this value is ignored"
40,// create a new database from the catalog name
40,"(catalogName, schemaName) -> format(""CREATE DATABASE %s"", catalogName)"
40,@Container
40,static JdbcDatabaseContainer<?> dbContainer = new MySQLContainer<>(
40,"DockerImageName.parse(""custom/mysql-image"").asCompatibleSubstituteFor(""mysql""));"
40,@RegisterExtension
40,"JdbiExtension extension = JdbiTestcontainersExtension.instance(CUSTOM_MYSQL, dbContainer);"
40,// ... test code using the custom mysql instance
40,The Javadoc for the TestcontainersDatabaseInformation class has additional details on how to create custom JdbiTestcontainersExtension instances.
40,15.3.2. Controlling the extension shutdown timeout
40,"Some testcontainer based database may take a long time to create a new schema so when tests run very fast, shutting down the JdbiTestcontainersExtension at the end of a test may interrupt this process and log spurious messages in the logs. Those messages are benign and can be safely ignored. If it is necessary (e.g. because there is a ""no messages"" policy in place at your organization), the shutdown time can be extended by setting the wait time to a higher value than the default of 10 seconds:"
40,@RegisterExtension
40,"JdbiExtension extension = JdbiTestcontainersExtension.instance(CUSTOM_MYSQL, dbContainer)"
40,.setShutdownWaitTimeInSeconds(20); // set shutdown wait time to 20 seconds
40,Using a wait time of 0 seconds will wait until all of the internal state has been shutdown completely. This may result in infinite hangs of the tests when shutting down.
40,16. Third-Party Integration
40,16.1. Google Guava
40,This plugin adds support for the following types:
40,Optional<T> - registers an argument and mapper. Supports Optional for any
40,wrapped type T for which a mapper / argument factory is registered.
40,Most Guava collection and map types - see
40,GuavaCollectors for a complete
40,list of supported types.
40,"To use this plugin, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-guava</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-guava:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance:
40,jdbi.installPlugin(new GuavaPlugin());
40,"With the plugin installed, any supported Guava collection type can"
40,be returned from a SQL object method:
40,public interface UserDao {
40,"@SqlQuery(""select * from users order by name"")"
40,ImmutableList<User> list();
40,"@SqlQuery(""select * from users where id = :id"")"
40,com.google.common.base.Optional<User> getById(long id);
40,16.2. H2 Database
40,This plugin configures Jdbi to correctly handle integer[] and uuid[] data
40,types in an H2 database.
40,This plugin is included with the core jar (but may be extracted to separate
40,artifact in the future). Use it by installing the plugin into your Jdbi
40,instance:
40,jdbi.installPlugin(new H2DatabasePlugin());
40,16.3. JSON
40,The jdbi3-json module adds a @Json type qualifier that allows to store arbitrary Java objects as JSON data in your database.
40,"The actual JSON (de)serialization code is not included. For that, you must install a backing plugin (see below)."
40,Backing plugins will install the JsonPlugin for you.
40,You do not need to install it yourself or include the jdbi3-json dependency directly.
40,The feature has been tested with Postgres json columns
40,and varchar columns in H2 and Sqlite.
40,16.3.1. Jackson 2
40,"This plugin provides JSON backing through Jackson 2. To use it, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-jackson2</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-jackson2:3.45.2-SNAPSHOT"")"
40,jdbi.installPlugin(new Jackson2Plugin());
40,// optionally configure your ObjectMapper (recommended)
40,jdbi.getConfig(Jackson2Config.class).setMapper(myObjectMapper);
40,// now with simple support for Json Views if you want to filter properties:
40,jdbi.getConfig(Jackson2Config.class).setView(ApiProperty.class);
40,16.3.2. Gson 2
40,"This plugin provides JSON backing through Gson 2. To use it, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-gson2</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-gson2:3.45.2-SNAPSHOT"")"
40,jdbi.installPlugin(new Gson2Plugin());
40,// optional
40,jdbi.getConfig(Gson2Config.class).setGson(myGson);
40,16.3.3. Moshi
40,"This plugin provides JSON backing through Moshi. To use it, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-moshi</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-moshi:3.45.2-SNAPSHOT"")"
40,jdbi.installPlugin(new MoshiPlugin());
40,// optional
40,jdbi.getConfig(MoshiConfig.class).setMoshi(myMoshi);
40,16.3.4. Operation
40,Any bound object qualified as @Json
40,will be converted by the registered
40,JsonMapper and requalified as
40,@EncodedJson String.
40,"A corresponding @EncodedJson ArgumentFactory will then be called to store the JSON data,"
40,allowing special JSON handling for your database to be implemented.
40,"If none are found, a factory for plain String will be used instead, to handle the JSON as plaintext."
40,"Mapping works just the same way, but in reverse: an output type qualified as @Json T will be fetched from a"
40,"@EncodedJson String or String ColumnMapper, and then passed through the JsonMapper."
40,Our PostgresPlugin provides qualified factories that will
40,bind/map the @EncodedJson String to/from json or jsonb-typed columns.
40,16.3.5. Usage
40,"handle.execute(""create table myjsons (id serial not null, value json not null)"");"
40,SqlObject:
40,// any json-serializable type
40,class MyJson {}
40,// use @Json qualifier:
40,interface MyJsonDao {
40,"@SqlUpdate(""INSERT INTO myjsons (json) VALUES(:value)"")"
40,// on parameters
40,int insert(@Json MyJson value);
40,"@SqlQuery(""SELECT value FROM myjsons"")"
40,// on result types
40,@Json
40,List<MyJson> select();
40,// also works on bean or property-mapped objects:
40,class MyBean {
40,private final MyJson property;
40,@Json
40,public MyJson getProperty() { return ...; }
40,"With the Fluent API, you provide a QualifiedType<T> any place you’d normally provide a Class<T> or GenericType<T>:"
40,QualifiedType<MyJson> qualifiedType = QualifiedType.of(MyJson.class).with(Json.class);
40,"h.createUpdate(""INSERT INTO myjsons(json) VALUES(:json)"")"
40,".bindByType(""json"", new MyJson(), qualifiedType)"
40,.execute();
40,"MyJson result = h.createQuery(""SELECT json FROM myjsons"")"
40,.mapTo(qualifiedType)
40,.one();
40,16.4. Immutables
40,Immutables is an annotation processor that generates
40,value types based on simple interface descriptions.
40,The value types naturally map very well
40,to property binding and row mapping.
40,Immutables support is still experimental and does not yet support custom naming schemes.
40,"We do support the configurable get, is, and set prefixes."
40,Just tell us about your types by installing the plugin and configuring your Immutables type:
40,jdbi.getConfig(JdbiImmutables.class).registerImmutable(MyValueType.class)
40,The configuration will both register appropriate RowMapper⁠s as well as configure the new bindPojo (or @BindPojo) binders:
40,@Value.Immutable
40,public interface Train {
40,String name();
40,int carriages();
40,boolean observationCar();
40,@Test
40,public void simpleTest() {
40,jdbi.getConfig(JdbiImmutables.class).registerImmutable(Train.class);
40,try (Handle handle = jdbi.open()) {
40,"handle.execute(""create table train (name varchar, carriages int, observation_car boolean)"");"
40,assertThat(
40,"handle.createUpdate(""insert into train(name, carriages, observation_car) values (:name, :carriages, :observationCar)"")"
40,".bindPojo(ImmutableTrain.builder().name(""Zephyr"").carriages(8).observationCar(true).build())"
40,.execute())
40,.isOne();
40,assertThat(
40,"handle.createQuery(""select * from train"")"
40,.mapTo(Train.class)
40,.one())
40,".extracting(""name"", ""carriages"", ""observationCar"")"
40,".containsExactly(""Zephyr"", 8, true);"
40,16.5. Freebuilder
40,Freebuilder is an annotation
40,processor that generates value types based on simple interface or abstract
40,class descriptions. Jdbi supports Freebuilder in much the same way that it
40,supports Immutables.
40,Freebuilder support is still experimental and may not support all Freebuilder
40,implemented features. We do support both JavaBean style getters and setters as
40,well as unprefixed getters and setters.
40,Just tell us about your Freebuilder types by installing the plugin and
40,configuring your Freebuilder type:
40,jdbi.getConfig(JdbiFreebuilder.class).registerFreebuilder(MyFreeBuilderType.class)
40,The configuration will both register appropriate RowMappers as well as
40,configure the new bindPojo (or @BindPojo) binders:
40,@FreeBuilder
40,public interface Train {
40,String name();
40,int carriages();
40,boolean observationCar();
40,class Builder extends FreeBuildersTest_Train_Builder {}
40,@Test
40,public void simpleTest() {
40,jdbi.getConfig(JdbiFreeBuilders.class).registerFreeBuilder(Train.class);
40,try (Handle handle = jdbi.open()) {
40,"handle.execute(""create table train (name varchar, carriages int, observation_car boolean)"");"
40,Train train = new Train.Builder()
40,".name(""Zephyr"")"
40,.carriages(8)
40,.observationCar(true)
40,.build();
40,assertThat(
40,"handle.createUpdate(""insert into train(name, carriages, observation_car) values (:name, :carriages, :observationCar)"")"
40,.bindPojo(train)
40,.execute())
40,.isOne();
40,assertThat(
40,"handle.createQuery(""select * from train"")"
40,.mapTo(Train.class)
40,.one())
40,".extracting(""name"", ""carriages"", ""observationCar"")"
40,".containsExactly(""Zephyr"", 8, true);"
40,16.6. JodaTime
40,This plugin adds support for using joda-time’s DateTime type.
40,"To use this plugin, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-jodatime2</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-jodatime2:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance:
40,jdbi.installPlugin(new JodaTimePlugin());
40,16.7. Google Guice
40,The Guice module adds support for configuring and injecting Jdbi instances in applications and services that use the Google Guice dependency injection framework.
40,Guice support for Jdbi is split into two module types:
40,A Jdbi definition module extends the AbstractJdbiDefinitionModule class. Each of these modules creates a new Jdbi instance which is exposed into the Guice binding process using an annotation.
40,A Jdbi element configuration module extends the AbstractJdbiConfigurationModule. These modules contribute elements that are referenced in Jdbi definition modules.
40,Jdbi definition modules are by far more common. Element configuration modules are completely optional. They are most useful in larger projects.
40,16.7.1. JSR 330 vs. JEE 9+ annotations
40,"For a long time, Java used the JSR-330 annotations in the javax.inject namespace to define dependency injections. The Java Enterprise Edition 9 specification updated the dependency injection specification to version 2.0, which changed the namespace to jakarta.inject."
40,The jdbi3-guice module supports all Google Guice releases since 5.x including 6.x (which uses javax.inject annotations) and 7.x (which uses jakarta.inject).
40,16.7.2. Definition modules
40,Every Jdbi instance is defined in its own Guice module which extends the AbstractJdbiDefinitionModule base class.
40,The
40,annotation instance or class used on the constructor is used to bind the resulting Jdbi object:
40,class GuiceJdbiModule extends AbstractJdbiDefinitionModule {
40,public GuiceJdbiModule() {
40,super(GuiceJdbi.class);
40,@Override
40,protected void configureJdbi() {
40,// bind a custom row mapper
40,bindRowMapper().to(CustomRowMapper.class);
40,// bind a custom column mapper
40,bindColumnMapper().toInstance(new CustomColumnMapper());
40,// bind a Codec
40,"bindCodec(String.class).to(Key.get(CustomStringCodec.class, Custom.class));"
40,// bind a custom array type
40,"bindArrayType(CustomArrayType.class).toInstance(""custom_array"");"
40,// bind a jdbi plugin
40,bindPlugin().toInstance(new SqlObjectPlugin());
40,// bind a customizer
40,bindCustomizer().to(SpecialCustomizer.class);
40,class Application {
40,@Inject
40,@GuiceJdbi
40,private Jdbi jdbi;
40,public static void main(String ... args) {
40,Injector inj = Guice.createInjector(
40,"new GuiceJdbiModule(),"
40,binder -> binder.bind(DataSource.class).annotatedWith(GuiceJdbi.class).toInstance(... data source instance ...);
40,inj.injectMembers(this);
40,"In this example, a new Jdbi object is defined using specific mappers and other customizations. The Jdbi object is exposed to Guice using the annotation passed on the module constructor (GuiceJdbi in the example). The AbstractJdbiDefinitionModule supports both annotation instances and classes, similar to Guice itself."
40,A Jdbi definition module requires that a DataSource object is bound within Guice using the same annotation or annotation class as is passed into the module constructor. Creating this data source is outside the scope of a Jdbi definition module!
40,"When implementing the configureJdbi() method, a number of convenience methods are available as shown above. These methods return Guice LinkedBindingBuilder instances and allow the full range of bindings that guice supports (classes, instances, providers etc)."
40,Table 2. Supported bindings
40,Method
40,Type of binding
40,Jdbi function
40,bindRowMapper()
40,RowMapper<?>
40,Row Mappers
40,bindRowMapper(GenericType<?>)
40,bindRowMapper(Type)
40,bindColumnMapper()
40,ColumnMapper<?>
40,Column Mappers
40,bindColumnMapper(QualifiedType<?>)
40,bindColumnMapper(GenericType<?>)
40,bindColumnMapper(Type)
40,bindCodec(QualifiedType<?>)
40,Codec<?>
40,Codecs
40,bindCodec(GenericType<?>)
40,bindCodec(Type)
40,bindArrayType(Class<?>)
40,String
40,Registering array types
40,bindPlugin()
40,JdbiPlugin
40,Installing Plugins
40,bindCustomizer()
40,GuiceJdbiCustomizer
40,Jdbi customization using Guice
40,Each Jdbi definition module is completely independent and all definitions within the module only apply to the specific Jdbi instance.
40,16.7.3. Using Guice injection in Jdbi classes
40,"The Jdbi related guice modules store the various related elements (mappers, codecs etc.) using Multibindings. As a result, it is not possible to use injection directly when constructing mapper instances."
40,The following example does not work:
40,class JdbiModule extends AbstractJdbiDefinitionModule {
40,public JdbiModule() {
40,"super(Names.named(""data""));"
40,@Override
40,protected void configureJdbi() {
40,bindRowMapper().to(BrokenRowMapper.class);
40,bindColumnMapper().to(CustomStringMapper.class);
40,class CustomStringMapper implements ColumnMapper<String> {
40,@Override
40,"public String map(ResultSet r, int columnNumber, StatementContext ctx) {"
40,long x = r.getLong(columnNumber);
40,"return (x > 1000) ? ""Huge"" : ""Small"";"
40,class BrokenRowMapper implements RowMapper<DataRow> {
40,private final ColumnMapper<String> stringMapper;
40,@Inject
40,public BrokenRowMapper(CustomStringMapper stringMapper) {
40,this.stringMapper = stringMapper;
40,@Override
40,"public DataRow map(ResultSet rs, StatementContext ctx) {"
40,"return new DataRow(rs.getInt(""intValue""),"
40,"stringMapper.map(rs, ""longValue"", ctx));"
40,"Guice will report an error that it cannot locate the CustomStringMapper instance. The Guice Jdbi integration manages mappers etc. as groups and the separate instances are not directly accessible for injection. The right way to compose mappers is using the Jdbi configuration (see JdbiConfig), which is configured through Guice:"
40,class JdbiModule extends AbstractJdbiDefinitionModule {
40,public JdbiModule() {
40,"super(Names.named(""data""));"
40,@Override
40,protected void configureJdbi() {
40,bindRowMapper().to(WorkingRowMapper.class);
40,bindColumnMapper(CustomStringMapper.TYPE).to(CustomStringMapper.class);
40,class CustomStringMapper implements ColumnMapper<String> {
40,"public static QualifiedType<String> TYPE = QualifiedType.of(String.class).with(Names.named(""data""));"
40,@Override
40,"public String map(ResultSet r, int columnNumber, StatementContext ctx) throws SQLException {"
40,long x = r.getLong(columnNumber);
40,"return (x > 1000) ? ""Huge"" : ""Small"";"
40,class WorkingRowMapper implements RowMapper<DataRow> {
40,private ColumnMapper<String> stringMapper;
40,@Override
40,public void init(ConfigRegistry registry) {
40,this.stringMapper = registry.get(ColumnMappers.class).findFor(CustomStringMapper.TYPE)
40,.orElseThrow(IllegalStateException::new);
40,@Override
40,"public DataRow map(ResultSet rs, StatementContext ctx) throws SQLException {"
40,"return new DataRow(rs.getInt(""intValue""),"
40,"stringMapper.map(rs, ""longValue"", ctx));"
40,This limitation only applies to bindings that are made in through the various bindXXX() methods in Jdbi specific modules. Any other binding is available for injection into Jdbi elements:
40,class ThresholdMapper implements ColumnMapper<String> {
40,public static QualifiedType<String> TYPE = QualifiedType.of(String.class).with(Threshold.class);
40,private final long threshold;
40,// Injection of a named constant here.
40,@Inject
40,ThresholdMapper(@Threshold long threshold) {
40,this.threshold = threshold;
40,@Override
40,"public String map(ResultSet r, int columnNumber, StatementContext ctx) throws SQLException {"
40,long x = r.getLong(columnNumber);
40,"return (x > threshold) ? ""Huge"" : ""Small"";"
40,class JdbiModule extends AbstractJdbiDefinitionModule {
40,public JdbiModule() {
40,super(Data.class);
40,@Override
40,protected void configureJdbi() {
40,bindColumnMapper(CustomStringMapper.TYPE).to(CustomStringMapper.class);
40,Injector inj = Guice.createInjector(
40,"new JdbiModule(),"
40,// define injected constant here
40,binder -> binder.bindConstant().annotatedWith(Threshold.class).to(5000L);
40,16.7.4. Jdbi customization using Guice
40,"Many Jdbi specific settings can be configured through the bindXXX() methods available on Jdbi modules (row mappers, column mappers, codecs, plugins etc.)"
40,"However, there are additional features that may not be available through these methods. For these use cases, the GuiceJdbiCustomizer interface can be used."
40,Instances that implement this interface can be added to Jdbi modules using the bindCustomizer() method.
40,Every customizer will get the Jdbi instance passed at construction time and may modify any aspect before it gets exposed to other parts of the application.
40,class GuiceCustomizationModule extends AbstractJdbiDefinitionModule {
40,public GuiceCustomizationModule() {
40,super(Custom.class);
40,@Override
40,protected void configureJdbi() {
40,bindCustomizer().to(MyCustomizer.class);
40,class MyCustomizer implements GuiceJdbiCustomizer {
40,@Override
40,public void customize(Jdbi jdbi) {
40,// set the logger to use Slf4j
40,jdbi.setSqlLogger(new Slf4JSqlLogger());
40,"In combination with Jdbi configuration modules, these customizers allow easy enforcement of standard configurations for all Jdbi instances in larger projects."
40,16.7.5. Element configuration modules
40,Element configuration modules are completely optional and should not be used when only a single Jdbi instance is required. They are intended to help with code organization in larger projects that have more complex needs.
40,"All bindings in a Jdbi module that defines a Jdbi object are local to that module. This is useful if all Jdbi related code can be grouped around the module. In larger projects, some parts of the code (and their Jdbi related elements such as row and column mappers) may be located in different part of the code base."
40,"In larger projects, generic mappers should be available for multiple Jdbi instances. This leads often to a proliferation of small modules that only contain such generic code and is in turn imported into every code module that wants to use them."
40,"To support modular code design, any part of a code base that wants to contribute Jdbi specific classes such as mappers to the overall system can use an element configuration module to expose these to all Jdbi instances in a project."
40,"Jdbi element configuration modules extend AbstractJdbiConfigurationModule and can define mappers, plugins etc. similar to a Jdbi definition module. Anything that is registered in such a module is global and will be applied to all instances even if they are defined in another module."
40,class DomainModule extends AbstractJdbiConfigurationModule {
40,@Override
40,protected void configureJdbi() {
40,bindRowMapper().to(DomainMapper.class);
40,class DomainMapper implements RowMapper<DomainObject> {
40,private ColumnMapper<UUID> uuidMapper;
40,@Override
40,public void init(ConfigRegistry registry) {
40,this.uuidMapper = registry.get(ColumnMappers.class).findFor(UUID.class)
40,.orElseThrow(IllegalStateException::new);
40,@Override
40,"public DomainObject map(ResultSet rs, StatementContext ctx) throws SQLException {"
40,return new DomainObject(
40,"uuidMapper.map(rs, ""id"", ctx),"
40,"rs.getString(""name""),"
40,"rs.getString(""data""),"
40,"rs.getInt(""value""),"
40,"rs.getBoolean(""flag""));"
40,"If the DomainModule is bound within Guice, then all configured Jdbi instances will be able to map DomainObject instances without having to configure them explicitly as a row mapper."
40,Multiple modules extending AbstractJdbiConfigurationModule can be installed in a single injector; the resulting bindings will be aggregated.
40,It is not possible to install a configuration module from within the configureJdbi method of a definition module using the install() or binder().install() methods!
40,Definition modules are Guice private modules and anything defined within them will not be exposed to the general dependency tree. This is a limitation due to the way Guice works.
40,16.7.6. Advanced Topics
40,Exposing additional bindings
40,Each definition module that defines a Jdbi instance keeps all bindings private and exposes only the actual Jdbi object itself. This allows the installation of multiple modules where each definition is completely independent. Sometimes it is useful to attach additional objects and expose them using the same annotations. The most common use cases are data access objects.
40,"Consider a use case where two DataSource instances exist, one annotated as Writer and the other as Reader. Both are accessing databases with the same schema, and it makes sense to have two data access objects that are identical except that they are using the different data sources (this is often referred to as the ""robot legs"" problem of dependency injection)."
40,interface DatabaseAccess {
40,"@SqlUpdate(""INSERT INTO data_table ...."")"
40,int insert(...);
40,"@SqlQuery(""SELECT * FROM data_table"")"
40,Data select();
40,"To bind two instances of this data access object and connect each to the appropriate Jdbi instance, add the binding to the Jdbi definition module and expose it with exposeBinding(Class<?>) or exposeBinding(TypeLiteral<?>):"
40,class DatabaseModule extends AbstractJdbiDefinitionModule {
40,public DatabaseModule(Class<? extends Annotation> a) {
40,super(a);
40,@Provides
40,@Singleton
40,DatabaseAccess createDatabaseAccess(Jdbi jdbi) {
40,return jdbi.onDemand(DatabaseAccess.class);
40,@Override
40,public void configureJdbi() {
40,"// ... bind mappers, plugins etc. ..."
40,exposeBinding(DatabaseAccess.class);
40,Now install the module multiple times with different annotation classes:
40,Injector injector = Guice.createInjector(
40,// bind existing data sources
40,binder -> binder.bind(DataSource.class).annotatedWith(Reader.class).toInstance(...);
40,binder -> binder.bind(DataSource.class).annotatedWith(Writer.class).toInstance(...);
40,"new DatabaseModule(Reader.class),"
40,new DatabaseModule(Writer.class)
40,// fetch object directly from the injector
40,"DatabaseAccess readerAccess = injector.getInstance(Key.get(DatabaseAccess.class, Reader.class));"
40,"DatabaseAccess writerAccess = injector.getInstance(Key.get(DatabaseAccess.class, Writer.class));"
40,Importing external bindings
40,The main use case of guice is code modularization and code reuse. Jdbi definition modules can pull dependencies out of the global dependency definitions and using the importBinding and importBindingLoosely methods.
40,importBinding requires a dependency to exist and pulls it into the definition module. The dependency must be defined using the same annotation or annotation class as the definition module uses.
40,"This example shows how to define an external dependency (SpecialLogger, annotated with Store) in a different module and then pull it into the definition module using importBinding:"
40,// This is logging code that can be located e.g. in a specific part of the code base that
40,// deals with all aspects of logging. The Logging module creates the binding for the special
40,// logger depending on the environment that the code has been deployed in.
40,class LoggingModule extends AbstractModule {
40,private final boolean production;
40,private final Class<? extends Annotation> annotation;
40,"LoggingModule(boolean production, Class<? extends Annotation> annotation) {"
40,this.production = production;
40,this.annotation = annotation;
40,@Override
40,public void configure() {
40,if (production) {
40,bind(SpecialLogger.class).annotatedWith(annotation).toInstance(new MaskingLogger());
40,} else {
40,bind(SpecialLogger.class).annotatedWith(annotation).toInstance(new DebugLogger());
40,// This is Jdbi code that deals with the data store. It can be located in a different part of the
40,"// application. It requires the ""SpecialLogger"" dependency to be bound somewhere."
40,@Singleton
40,class JdbiSpecialLogging implements GuiceJdbiCustomizer {
40,private final SpecialLogger logger;
40,@Inject
40,JdbiSpecialLogging(SpecialLogger logger) {
40,this.logger = logger;
40,@Override
40,public void customize(Jdbi jdbi) {
40,jdbi.setSqlLogger(new SpecialSqlLogger(logger));
40,class DatabaseModule extends AbstractJdbiDefinitionModule {
40,public DatabaseModule(Class<? extends Annotation> a) {
40,super(a);
40,@Override
40,public void configureJdbi() {
40,"... bind mappers, plugins etc. ..."
40,"// pull the ""SpecialLogger"" annotated with Store into the module scope"
40,importBinding(SpecialLogger.class).in(Scopes.SINGLETON);
40,bindCustomizer().to(JdbiSpecialLogging.class);
40,Injector injector = Guice.createInjector(
40,"new LoggingModule(production, Store.class),"
40,new DatabaseModule(Store.class)
40,"importBinding returns a binding builder, that allows different binding styles:"
40,class DatabaseModule extends AbstractJdbiDefinitionModule {
40,@Override
40,public void configureJdbi() {
40,"// simplest use case, pull in Foo.class using the same annotation"
40,importBinding(Foo.class);
40,// supports everything that a ScopedBindingBuilder does
40,importBinding(Foo.class).in(Scopes.SINGLETON);
40,"// supports ""to()"" to bind interfaces to implementation"
40,importBinding(Foo.class).to(FooImpl.class);
40,// supports type literals
40,importBinding(new TypeLiteral<Set<Foo>>() {}).to(FooSet.class);
40,// supports binding into the various binder methods as well
40,// pull SpecialCustomizer using the same annotation as the module and add it to the set of customizers
40,"importBinding(bindCustomizer(), SpecialCustomizer.class).in(Scopes.SINGLETON);"
40,// bind column mapper using a type literal
40,"importBinding(bindColumnMapper(), new TypeLiteral<Map<String, Object>>() {}).to(JsonMapper.class);"
40,"Static bindings require that the dependency is always defined. However, it is often desirable to have optional bindings that do not need to exist. This is supported using the importLooseBinding mechanism."
40,class DropwizardStoreModule extends AbstractModule {
40,@Store
40,@Provides
40,@Singleton
40,"DropwizardJdbiSupport getDropwizardJdbiSupport(@Dropwizard DataSourcConfiguration configuration, @Dropwizard Environment environment) {"
40,"return new DropwizardJdbiSupport(""store"", configuration, environment);"
40,static class DropwizardJdbiSupport implements GuiceJdbiCustomizer {
40,private final String name;
40,private final Environment environment;
40,private final DataSourceConfiguration<?> configuration;
40,"DropwizardJdbiSupport(String name, DataSourceConfiguration configuration, Environment environment) {"
40,this.name = name;
40,this.configuration = configuration;
40,this.environment = environment;
40,@Override
40,public void customize(final Jdbi jdbi) {
40,final String validationQuery = configuration.getValidationQuery();
40,"environment.healthChecks().register(name, new JdbiHealthCheck("
40,"environment.getHealthCheckExecutorService(),"
40,"configuration.getValidationQueryTimeout().orElse(Duration.seconds(5)),"
40,"jdbi,"
40,Optional.of(validationQuery)));
40,class StoreJdbiModule extends AbstractJdbiDefinitionModule {
40,@Override
40,public void configureJdbi() {
40,// ... other Jdbi code bindings for the store ...
40,"importBindingLoosely(bindCustomizer(), GuiceJdbiCustomizer.class)"
40,.withDefault(GuiceJdbiCustomizer.NOP)
40,.to(DropwizardJdbiSupport.class);
40,// production code (running in dropwizard framework)
40,Injector injector = Guice.createInjector(
40,"new DropwizardModule(),"
40,// binds @Dropwizard stuff
40,"new DropwizardStoreModule(), // binds dropwizard support for store jdbi"
40,"new StoreDataSourceModule(), // binds @Store DataSource"
40,new StoreJdbiModule()
40,// Store Jdbi code
40,// test code
40,Injector injector = Guice.createInjector(
40,"new StoreTestingDataSourceModule(), // testing datasource for store"
40,new StoreJdbiModule()
40,// store Jdbi code
40,In this example there is code specific to the dropwizard framework that would not work in unit tests (that are not run within the framework). This code is only bound in the production environment using the DropwizardStoreModule and not present in testing.
40,The StoreJdbiModule uses importBindingLoosely to pull in the DropwizardJdbiSupport binding using the Store annotation if it exists or uses a No-Op otherwise.
40,importBindingLoosely allows for full decoupling of optional dependencies without having to resort to conditionals or separate testing modules.
40,class DatabaseModule extends AbstractJdbiDefinitionModule {
40,@Override
40,public void configureJdbi() {
40,"// simplest use case, pull in Foo.class using the same annotation"
40,importBindingLoosely(Foo.class);
40,// supports everything that a ScopedBindingBuilder does
40,importBindingLoosely(Foo.class).in(Scopes.SINGLETON);
40,"// supports ""to()"" to bind interfaces to implementation"
40,importBindingLoosely(Foo.class).to(FooImpl.class);
40,// supports default value that is used if the binding
40,// is not present
40,importBindingLoosely(Foo.class)
40,".withDefault(new Foo(""default""));"
40,// supports type literals
40,importBindingLoosely(new TypeLiteral<Set<Foo>>() {}).to(FooSet.class);
40,// supports binding into the various binder methods as well
40,// pull SpecialCustomizer using the same annotation as the module and add it to the set of customizers
40,"importBindingLoosely(bindCustomizer(), SpecialCustomizer.class).in(Scopes.SINGLETON);"
40,// bind column mapper using a type literal
40,"importBindingLoosely(bindColumnMapper(), new TypeLiteral<Map<String, Object>>() {}).to(JsonMapper.class);"
40,// full example
40,"importBindingLoosely(bindCustomizer(), GuiceJdbiCustomizer.class)"
40,.withDefault(GuiceJdbiCustomizer.NOP)
40,.to(SpecialCustomizer.class)
40,.asEagerSingleton();
40,Custom element configuration modules
40,"In larger projects, Element configuration modules help to organize the various Jdbi related elements. By default, all modules contribute their configuration to a single, global configuration that is used in all Jdbi definition modules."
40,Sometimes it is useful to create separate configurations that only affect a subset of Jdbi definitions. This can be done by using a custom annotation for both the Jdbi element configuration and the Jdbi definition modules:
40,class CustomConfigurationModule extends AbstractJdbiConfigurationModule {
40,CustomModule() {
40,super(CustomConfiguration.class); // definition modules must use this annotation explictly
40,@Override
40,public void configureJdbi() {
40,bindColumnMapper().to(CustomColumnMapper.class);
40,bindPlugin().to(SpecialDatabaseModule.class);
40,class SpecialDatabaseModule extends AbstractJdbiDefinitionModule {
40,SpecialDatabaseModule() {
40,super(
40,"SpecialDatabase.class,"
40,// The Jdbi instance is bound using this annotation class
40,CustomConfiguration.class
40,// Use an explicit configuration
40,@Override
40,public void configureJdbi() {
40,...
40,The Jdbi element bound with the SpecialDatabase annotation will have the SpecialDatabaseModule loaded and can use the CustomColumnMapper.
40,16.8. JPA
40,Using the JPA plugin is a great way to trick your boss into letting you try
40,"Jdbi. ""No problem boss, it already supports JPA annotations, easy-peasy!"""
40,This plugin adds mapping support for a small subset of JPA entity annotations:
40,Entity
40,MappedSuperclass
40,Column
40,"To use this plugin, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-jpa</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-jpa:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance:
40,jdbi.installPlugin(new JpaPlugin());
40,Honestly though…​ just tear off the bandage and switch to Jdbi proper.
40,16.9. Kotlin
40,Kotlin support is provided by jdbi3-kotlin and
40,jdbi3-kotlin-sqlobject modules.
40,Kotlin API documentation:
40,jdbi3-kotlin
40,jdbi3-kotlin-sqlobject
40,16.9.1. ResultSet Mapping
40,The jdbi3-kotlin plugin adds mapping to Kotlin data classes. It
40,supports data classes where all fields are present in the constructor as well
40,as classes with writable properties. Any fields not present in the constructor
40,will be set after the constructor call. The mapper supports nullable types. It
40,also uses default parameter values in the constructor if the parameter type is
40,not nullable and the value absent in the result set.
40,"To use this plugin, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-kotlin</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-kotlin:3.45.2-SNAPSHOT"")"
40,Ensure the Kotlin compiler’s JVM target version is set to at least 11:
40,<kotlin.compiler.jvmTarget>11</kotlin.compiler.jvmTarget>
40,Then install the Kotlin Plugin into the Jdbi instance:
40,jdbi.installPlugin(KotlinPlugin());
40,"The Kotlin mapper also supports @ColumnName annotation that allows to specify name for a property or parameter explicitly, as well as the @Nested annotation that allows mapping nested Kotlin objects."
40,"Instead of using @BindBean, bindBean(), and @RegisterBeanMapper use @BindKotlin, bindKotlin(), and KotlinMapper"
40,"for qualifiers on constrictor parameters, getter, setters, and setter parameters of Kotlin class."
40,The @ColumnName annotation only applies while mapping SQL data into Java objects.
40,"When binding object properties (e.g. with bindBean()), bind the property name (:id) rather than the column name (:user_id)."
40,If you load all Jdbi plugins via Jdbi.installPlugins() this plugin will be
40,"discovered and registered automatically. Otherwise, you can attach it using"
40,Jdbi.installPlugin(KotlinPlugin()).
40,An example from the test class:
40,"data class IdAndName(val id: Int, val name: String)"
40,data class Thing(
40,"@Nested val idAndName: IdAndName,"
40,"val nullable: String?,"
40,"val nullableDefaultedNull: String? = null,"
40,"val nullableDefaultedNotNull: String? = ""not null"","
40,"val defaulted: String = ""default value"""
40,@Test
40,fun testFindById() {
40,"val qry = h2Extension.sharedHandle.createQuery(""select id, name from something where id = :id"")"
40,"val things: List<Thing> = qry.bind(""id"", brian.idAndName.id).mapTo<Thing>().list()"
40,"assertEquals(1, things.size)"
40,"assertEquals(brian, things[0])"
40,There are two extensions to help:
40,<reified T : Any>ResultBearing.mapTo()
40,<T : Any>ResultIterable<T>.useSequence(block: (Sequence<T>) → Unit)
40,Allowing code like:
40,"val qry = handle.createQuery(""SELECT id, name FROM something WHERE id = :id"")"
40,"val things = qry.bind(""id"", brian.id).mapTo<Thing>.list()"
40,and for using a Sequence that is auto closed:
40,qryAll.mapTo<Thing>.useSequence {
40,it.forEach(::println)
40,16.9.2. Coroutine support
40,"Coroutine support is very much experimental and not yet proven. If you find a bug or see a problem, please file a bug report."
40,Kotlin offers a coroutine extension to support asynchronous programming on the Kotlin platform.
40,"Coroutines work similar to ""green threads"" where operations are mapped onto a thread pool and may be executed on multiple threads. This clashes with the internal model that Jdbi uses to manage Handles (which is by default local to each thread)."
40,The Kotlin module offers support for coroutines with Jdbi. It allows handles to move across thread boundaries and
40,allows seamless use of handles in coroutines.
40,Coroutine support is disabled by default and is activated by passing the enableCoroutineSupport = true property to the constructor of the Kotlin Plugin.
40,val jdbi = Jdbi.create( ... )
40,.installPlugin(KotlinPlugin(enableCoroutineSupport = true))
40,jdbi.inTransactionUnchecked { transactionHandle ->
40,runBlocking(jdbi.inCoroutineContext()) { (1)
40,withContext(Dispatchers.Default + jdbi.inCoroutineContext(transactionHandle)) { (2)
40,launch () {
40,jdbi.useHandleUnchecked { handle -> (3)
40,...
40,launch(Dispatchers.Default) { (4)
40,jdbi.useHandleUnchecked { handle -> (5)
40,...
40,calling the Jdbi#inCoroutineContext() extension method without a parameter disconnects the coroutine scope from any existing thread local elements.
40,calling the Jdbi#inCoroutineContext() extension method with a handle object uses this handle for all coroutines in this scope.
40,calling Jdbi methods within the coroutine scope will use the handle that was provided above.
40,"Launching a coroutine without an explicitly attached handle works as well, in this case, a new handle gets created"
40,The handle here is a newly created handle that only exists within the jdbi callback.
40,"Coroutines that are executed in parallel by different threads at the same time will get the same handle as they share the same coroutine context. While the Handle object can be shared by multiple threads (if the underlying connection object supports it), they are designed"
40,"to be used by ""one thread at a time"" so additional coordination is required in this case."
40,Coroutines are also supported for extension objects such as the SQL Object extension:
40,val jdbi = Jdbi.create( ... )
40,.installPlugin(KotlinPlugin(enableCoroutineSupport = true))
40,.installPlugin(SqlObjectPlugin())
40,data class Something(
40,"val id: Int,"
40,val name: String
40,@RegisterKotlinMapper(Something::class)
40,interface SomethingDao : SqlObject {
40,"@SqlUpdate(""insert into something (id, name) values (:id, :name)"")"
40,fun insert(@BindKotlin something: Something)
40,"@SqlQuery(""select id, name from something where id=:id"")"
40,"fun findById(@Bind(""id"") id: Int): Something"
40,jdbi.withExtensionUnchecked(SomethingDao::class) { dao ->
40,runBlocking(Dispatchers.IO + jdbi.inCoroutineContext()) {
40,coroutineScope {
40,insertSomething(dao)
40,"val first = selectSomething(dao, 1)"
40,delay(1000L)
40,"val second = selectSomething(dao, 2)"
40,private suspend fun insertSomething(dao: SomethingDao): Unit = coroutineScope {
40,"dao.insert(Something(1, ""first name""))"
40,"dao.insert(Something(2, ""second name""))"
40,"private suspend fun selectSomething(dao: SomethingDao, id: Int): Something = coroutineScope {"
40,dao.findById(id)
40,16.9.3. SqlObject
40,The jdbi3-kotlin-sqlobject plugin adds automatic parameter binding by name
40,for Kotlin methods in SqlObjects as well as support for Kotlin default methods.
40,"To use it, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-kotlin-sqlobject</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-kotlin-sqlobject:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance:
40,jdbi.installPlugin(KotlinSqlObjectPlugin());
40,Parameter binding supports individual primitive types as well as Kotlin or
40,JavaBean style objects as a parameter (referenced in binding as
40,:paramName.propertyName). No annotations are needed.
40,If you load all Jdbi plugins via Jdbi.installPlugins() this plugin will be
40,"discovered and registered automatically. Otherwise, you can attach the plugin"
40,via: Jdbi.installPlugin(KotlinSqlObjectPlugin()).
40,An example from the test class:
40,interface ThingDao {
40,"@SqlUpdate(""insert into something (id, name) values (:something.idAndName.id, :something.idAndName.name)"")"
40,fun insert(something: Thing)
40,"@SqlQuery(""select id, name from something"")"
40,fun list(): List<Thing>
40,@BeforeEach
40,fun setUp() {
40,val dao = h2Extension.jdbi.onDemand<ThingDao>()
40,"val brian = Thing(IdAndName(1, ""Brian""), null)"
40,"val keith = Thing(IdAndName(2, ""Keith""), null)"
40,dao.insert(brian)
40,dao.insert(keith)
40,@Test
40,fun testDao() {
40,val dao = h2Extension.jdbi.onDemand<ThingDao>()
40,val rs = dao.list()
40,"assertEquals(2, rs.size.toLong())"
40,"assertEquals(brian, rs[0])"
40,"assertEquals(keith, rs[1])"
40,16.9.4. Jackson JSON Processing
40,Jackson needs a specialized ObjectMapper instance in order to understand deserialization
40,of Kotlin types. Make sure to configure your Jackson plugin:
40,import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
40,jdbi.getConfig(Jackson2Config::class.java).mapper = jacksonObjectMapper()
40,16.10. Lombok
40,Lombok is a tool for auto-generation of mutable and immutable data classes through annotations.
40,@Data
40,public void DataClass {
40,private Long id;
40,private String name;
40,"// autogenerates default constructor, getters, setters, equals, hashCode, and toString"
40,@Value
40,public void ValueClass {
40,private long id;
40,private String name;
40,"// autogenerates all-args constructor, getters, equals, hashCode, and toString"
40,"Classes annotated with @Value are immutable, classes annotated with @Data behave like standard, mutable Java Beans."
40,Use BeanMapper or @RegisterBeanMapper to map @Data classes.
40,Use ConstructorMapper or @RegisterConstructorMapper to map @Value
40,classes.
40,Use bindBean() or @BindBean to bind @Data or @Value classes.
40,"Lombok must be configured to propagate annotations from the source code into the generated classes. Since version 1.18.4, lombok"
40,"supports the lombok.copyableAnnotations setting in its config file. To support all Jdbi features, the following lines must be added:"
40,lombok.copyableAnnotations += org.jdbi.v3.core.mapper.reflect.ColumnName
40,lombok.copyableAnnotations += org.jdbi.v3.core.mapper.Nested
40,lombok.copyableAnnotations += org.jdbi.v3.core.mapper.PropagateNull
40,lombok.copyableAnnotations += org.jdbi.v3.core.annotation.JdbiProperty
40,lombok.copyableAnnotations += org.jdbi.v3.core.mapper.reflect.JdbiConstructor
40,Any additional annotation (such as the @HStore annotation for PostgreSQL support) can also be added:
40,lombok.copyableAnnotations += org.jdbi.v3.postgres.HStore
40,"When using any of the Json mappers (Jackson 2, Gson 2 or Moshi ) with lombok, the @Json annotation must also be added to the lombok config file, otherwise Jdbi can not map json columns to lombok bean properties:"
40,lombok.copyableAnnotations += org.jdbi.v3.json.Json
40,16.11. Oracle 12
40,This module adds support for Oracle RETURNING DML expressions.
40,"To use this feature, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-oracle12</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-oracle12:3.45.2-SNAPSHOT"")"
40,"Then, use the OracleReturning class with an Update or PreparedBatch"
40,to get the returned DML.
40,16.12. PostgreSQL
40,The jdbi3-postgres plugin provides enhanced integration with the
40,PostgreSQL JDBC Driver.
40,"To use this feature, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-postgres</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-postgres:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance.
40,"Jdbi jdbi = Jdbi.create(""jdbc:postgresql://host:port/database"")"
40,.installPlugin(new PostgresPlugin());
40,The plugin configures mappings for java.time types like
40,"Instant or Duration, InetAddress, UUID, typed enums, and hstore."
40,"It also configures SQL array type support for int, long, float, double,"
40,"String, and UUID."
40,See the javadoc for an
40,exhaustive list.
40,"Some Postgres operators, for example the ? query operator, collide"
40,with Jdbi or JDBC specific special characters.
40,"In such cases, you may need to"
40,escape operators to e.g. ?? or \:.
40,16.12.1. hstore
40,"The Postgres plugin provides an hstore to Map<String, String> column mapper"
40,and vice versa argument factory:
40,"Map<String, String> accountAttributes = handle"
40,".select(""SELECT attributes FROM account WHERE id = ?"", userId)"
40,".mapTo(new GenericType<Map<String, String>>() {})"
40,.one();
40,With @HStore qualified type:
40,"QualifiedType<> HSTORE_MAP = QualifiedType.of(new GenericType<Map<String, String>>() {})"
40,.with(HStore.class);
40,"Map<String, String> caps = handle.createUpdate(""UPDATE account SET attributes = :hstore"")"
40,".bindByType(""hstore"", mapOfStrings, HSTORE_MAP)"
40,.execute();
40,"By default, SQL Object treats Map return types as a collection of Map.Entry"
40,"values. Use the @SingleValue annotation to override this, so that the return"
40,type is treated as a single value instead of a collection:
40,public interface AccountDao {
40,"@SqlQuery(""SELECT attributes FROM account WHERE id = ?"")"
40,@SingleValue
40,"Map<String, String> getAccountAttributes(long accountId);"
40,The default variant to install the plugin adds unqualified mappings of raw Map types from and to the hstore Postgres data type. There are situations where this interferes with other mappings of maps. It is recommended to always use the variant with the @HStore qualified type.
40,"To avoid binding the unqualified Argument and ColumnMapper bindings, install the plugin using the static factory method:"
40,"Jdbi jdbi = Jdbi.create(""jdbc:postgresql://host:port/database"")"
40,.installPlugin(PostgresPlugin.noUnqualifiedHstoreBindings());
40,16.12.2. @GetGeneratedKeys Annotation
40,"In Postgres, @GetGeneratedKeys can return the entire modified row if you request generated keys without naming any columns."
40,public interface UserDao {
40,"@SqlUpdate(""INSERT INTO users (id, name, created_on) VALUES (nextval('user_seq'), ?, now())"")"
40,@GetGeneratedKeys
40,@RegisterBeanMapper(User.class)
40,User insert(String name);
40,If a database operation modifies multiple rows (e.g. an update that will modify
40,"several rows), your method can return all the modified rows in a collection:"
40,public interface UserDao {
40,"@SqlUpdate(""UPDATE users SET active = false WHERE id = any(?)"")"
40,@GetGeneratedKeys
40,@RegisterBeanMapper(User.class)
40,List<User> deactivateUsers(long... userIds);
40,16.12.3. Large Objects
40,Postgres supports storing large character or binary data in separate storage from
40,table data.
40,Jdbi allows you to stream this data in and out of the database as part
40,of an enclosing transaction.
40,"Operations for storing, reading, and a hook for deletions are provided."
40,The test case serves as a simple example:
40,public void blobCrud(InputStream myStream) throws IOException {
40,h.useTransaction(th -> {
40,Lobject lob = th.attach(Lobject.class);
40,"lob.insert(1, myStream);"
40,readItBack = lob.readBlob(1);
40,lob.deleteLob(1);
40,assert lob.readBlob(1) == null;
40,});
40,public interface Lobject {
40,"// CREATE TABLE lob (id int, lob oid"
40,"@SqlUpdate(""INSERT INTO lob (id, lob) VALUES (:id, :blob)"")"
40,"void insert(int id, InputStream blob);"
40,"@SqlQuery(""SELECT lob FROM lob WHERE id = :id"")"
40,InputStream readBlob(int id);
40,"@SqlUpdate(""DELETE FROM lob WHERE id = :id RETURNING lo_unlink(lob)"")"
40,void deleteLob(int id);
40,Please refer to
40,Pg-JDBC docs
40,for upstream driver documentation.
40,16.13. Spring
40,"This module integrates Jdbi and Spring / Spring Boot, including JTA support."
40,The spring5 module supports Spring 5 (Spring Boot 2) and newer.
40,We currently test with Spring 5 and Spring 6.
40,"To use jdbi-spring5, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-spring5</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-spring5:3.45.2-SNAPSHOT"")"
40,16.13.1. XML-based configuration
40,For XML-based configurations the class JdbiFactoryBean is made available to set up a
40,Jdbi singleton in a Spring 5 (or Spring Boot) application context.
40,"Then configure the Jdbi factory bean in your Spring container, e.g.:"
40,"<beans xmlns=""http://www.springframework.org/schema/beans"""
40,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
40,"xmlns:aop=""http://www.springframework.org/schema/aop"""
40,"xmlns:tx=""http://www.springframework.org/schema/tx"""
40,"xsi:schemaLocation="""
40,http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd
40,http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd
40,"http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd"">"
40,(1)
40,"<bean id=""db"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource"">"
40,"<property name=""url"" value=""jdbc:h2:mem:testing""/>"
40,</bean>
40,(2)
40,"<bean id=""transactionManager"""
40,"class=""org.springframework.jdbc.datasource.DataSourceTransactionManager"">"
40,"<property name=""dataSource"" ref=""db""/>"
40,</bean>
40,"<tx:annotation-driven transaction-manager=""transactionManager""/>"
40,(3)
40,"<bean id=""jdbi"""
40,"class=""org.jdbi.v3.spring5.JdbiFactoryBean"">"
40,"<property name=""dataSource"" ref=""db""/>"
40,</bean>
40,(4)
40,"<bean id=""service"""
40,"class=""com.example.service.MyService"">"
40,"<constructor-arg ref=""jdbi""/>"
40,</bean>
40,</beans>
40,The SQL data source that Jdbi will connect to. In this example we use an H2
40,"database, but it can be any JDBC-compatible database."
40,Enable configuration of transactions via annotations.
40,Configure JdbiFactoryBean using the data source configured earlier.
40,"Inject a Jdbi instance into a service class. Alternatively, use standard JSR-330 @Inject annotations on the target class instead of configuring it in your beans.xml."
40,Installing Plugins
40,Plugins may be automatically installed by scanning the classpath for
40,ServiceLoader manifests.
40,"<bean id=""jdbi"" class=""org.jdbi.v3.spring5.JdbiFactoryBean"">"
40,...
40,"<property name=""autoInstallPlugins"" value=""true""/>"
40,</bean>
40,Plugins may also be installed explicitly:
40,"<bean id=""jdbi"" class=""org.jdbi.v3.spring5.JdbiFactoryBean"">"
40,...
40,"<property name=""plugins"">"
40,<list>
40,"<bean class=""org.jdbi.v3.sqlobject.SqlObjectPlugin""/>"
40,"<bean class=""org.jdbi.v3.guava.GuavaPlugin""/>"
40,</list>
40,</property>
40,</bean>
40,"Not all plugins are automatically installable. In these situations, you can"
40,auto-install some plugins and manually install the rest:
40,"<bean id=""jdbi"" class=""org.jdbi.v3.spring5.JdbiFactoryBean"">"
40,...
40,"<property name=""autoInstallPlugins"" value=""true""/>"
40,"<property name=""plugins"">"
40,<list>
40,"<bean class=""org.jdbi.v3.core.h2.H2DatabasePlugin""/>"
40,</list>
40,</property>
40,</bean>
40,Global Attributes
40,Global defined attributes may be configured on the factory bean:
40,"<bean id=""jdbi"" class=""org.jdbi.v3.spring5.JdbiFactoryBean"">"
40,"<property name=""dataSource"" ref=""db""/>"
40,"<property name=""globalDefines"">"
40,<map>
40,"<entry key=""foo"" value=""bar""/>"
40,</map>
40,</property>
40,</bean>
40,16.13.2. Annotation-based configuration
40,For annotation based configurations you can add the following method in a
40,@Configuration class:
40,@Configuration
40,public class JdbiConfiguration {
40,@Bean
40,public Jdbi jdbi(DataSource ds) {
40,ConnectionFactory cf = new SpringConnectionFactory(dataSource); (1)
40,final Jdbi jdbi = Jdbi.create(cf);
40,/* additional configuration goes here */
40,return jdbi;
40,This connection factory will enable JTA support.
40,16.13.3. Synchronization between Jdbi and JTA
40,"If you use Spring and databases, you will most likely also use the Java Transaction API (JTA)"
40,to manage the transactions. The JdbiUtil utility can be used to obtain Handles
40,"synchronized with Spring’s JTA, as can be seen in the following fragment:"
40,Handle handle = JdbiUtil.getHandle(jdbi); (1)
40,try {
40,/* Use the handle */
40,} finally {
40,JdbiUtil.closeIfNeeded(handle); (2)
40,"Creates or reuses a handle, optionally synchronizing it with the current transaction"
40,Closes the handle if it is not bound to a Spring transaction.
40,This utility will also make sure that the handle is released when it was bound to a Spring transaction
40,and that transaction is closed.
40,16.13.4. Jdbi Repositories
40,When using SQL objects it might be preferable to have them available for injection.
40,This can be realized using the @EnableJdbiRepositories annotation (generally placed on your main class).
40,This annotation will enable the detection of SQL objects annotated with @JdbiRepository and create a Spring
40,bean for each of them.
40,Beans created like this will also synchronize with JTA.
40,Example usage:
40,@JdbiRepository
40,public interface UserDao {
40,/* SQL Object functionality goes here */
40,@Component
40,public class UserService {
40,@Autowired
40,private UserDao userDao;
40,16.14. SQLite
40,The jdbi3-sqlite plugin provides support for using the
40,SQLite JDBC Driver
40,with Jdbi.
40,The plugin configures mapping for the Java URL type which is not
40,supported by driver.
40,"To use this plugin, first add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-sqlite</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-sqlite:3.45.2-SNAPSHOT"")"
40,Then install the plugin into the Jdbi instance.
40,"Jdbi jdbi = Jdbi.create(""jdbc:sqlite:database"")"
40,.installPlugin(new SQLitePlugin());
40,16.15. StringTemplate 4
40,"This module allows you to plug in the StringTemplate 4 templating engine, in"
40,place of the standard Jdbi templating engine.
40,String Template is a separate dependency:
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-stringtemplate4</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-stringtemplate4:3.45.2-SNAPSHOT"")"
40,16.15.1. Using String template for parsing
40,"To use StringTemplate format in SQL statements, set the template engine to StringTemplateEngine."
40,Defined attributes are provided to the StringTemplate engine to render the SQL:
40,"String sortColumn = ""name"";"
40,"String sql = ""SELECT id, name "" +"
40,"""FROM account "" +"
40,"""ORDER BY <if(sort)> <sortBy>, <endif> id"";"
40,List<Account> accounts = handle
40,.createQuery(sql)
40,.setTemplateEngine(new StringTemplateEngine())
40,".define(""sort"", true)"
40,".define(""sortBy"", sortColumn)"
40,.mapTo(Account.class)
40,.list();
40,"Since StringTemplate by default uses the < character to mark ST expressions,"
40,you might need to escape some SQL:
40,"String datePredicateSql = ""<if(datePredicate)> <dateColumn> \\< :dateFilter <endif>"";"
40,"When using SQL Objects, the @UseStringTemplateEngine annotation activates parsing using StringTemplate:"
40,package com.foo;
40,public interface AccountDao {
40,"@SqlQuery(""SELECT id, name "" +"
40,"""FROM account "" +"
40,"""ORDER BY <if(sort)> <sortBy>, <endif> id"")"
40,@UseStringTemplateEngine
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,16.15.2. Locating Stringtemplate files on the class path
40,SQL templates can be loaded from StringTemplate group files on the classpath by using the StringTemplateSqlLocator:
40,com/foo/AccountDao.sql.stg
40,group AccountDao;
40,"selectAll(sort,sortBy) ::= <<"
40,"SELECT id, name"
40,FROM account
40,"ORDER BY <if(sort)> <sortBy>, <endif> id"
40,ST template = StringTemplateSqlLocator.findStringTemplate(
40,"""com/foo/AccountDao.sql.stg"", ""selectAll"");"
40,"String sql = template.add(""sort"", true)"
40,".add(""sortBy"", sortColumn)"
40,.render();
40,"In this example, since the fully qualified class name is com.foo.AccountDao,"
40,SQL will be loaded from the file com/foo/AccountDao.sql.stg on the
40,"classpath. In the String template group file, the selectAll group will be located and used for rendering."
40,"In SQL objects, the @UseStringTemplateSqlLocator annotation supports loading templates from the classpath:"
40,package com.foo;
40,public interface AccountDao {
40,@SqlQuery
40,@UseStringTemplateSqlLocator
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,"Here, since the fully qualified class name is com.foo.AccountDao,"
40,SQL will be rendered by loading the selectAll template from the com/foo/AccountDao.sql.stg String template group file on the
40,classpath.
40,"By default, the method name is used to locate a template in the template group file. This can be overridden by setting a name in the sql method annotation:"
40,package com.foo;
40,public interface AccountDao {
40,"@SqlQuery(""listSorted"")"
40,@UseStringTemplateSqlLocator
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,"In this example, the SQL template will still be loaded from the template group file"
40,"com/foo/AccountDao.sql.stg on the classpath, however the listSorted"
40,"template will be used, regardless of the method name."
40,There are some options for StringTemplateEngine on the
40,"StringTemplates configuration class,"
40,like whether missing attributes are considered an error or not.
40,"The @UseStringTemplateEngine annotation activates parsing of the value of a SQL method annotation using the String Template engine while the @UseStringTemplateSqlLocator uses string template group files on the class path to load and parse templates. These annotations are mutually exclusive and the behavior, if these annotations are used simultaneously, is undefined."
40,16.16. Apache Freemarker
40,"This module allows you to plug in the Apache Freemarker templating engine, in"
40,place of the standard Jdbi templating engine.
40,Freemarker support is a separate module that needs to be added as a dependency:
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-freemarker</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-freemarker:3.45.2-SNAPSHOT"")"
40,16.16.1. Using Freemarker for parsing
40,"To use the Freemarker format in SQL statements, set the template engine to FreemarkerEngine."
40,Defined attributes are provided to the Freemarker template engine to render the SQL:
40,"String sortColumn = ""name"";"
40,"String sql = ""SELECT id, name "" +"
40,"""FROM account "" +"
40,"""ORDER BY <#if sort> ${sortBy}, </#if> id"";"
40,List<Account> accounts = handle
40,.createQuery(sql)
40,.setTemplateEngine(FreemarkerEngine.instance())
40,".define(""sort"", true)"
40,".define(""sortBy"", sortColumn)"
40,.mapTo(Account.class)
40,.list();
40,"When using SQL Objects, the @UseFreemarkerEngine annotation activates parsing using Apache Freemarker."
40,package com.foo;
40,public interface AccountDao {
40,"@SqlQuery(""SELECT id, name "" +"
40,"""FROM account "" +"
40,"""ORDER BY <#if sort> ${sortBy}, </#if> id"")"
40,@UseFreemarkerEngine
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,16.16.2. Locating Freemarker templates on the class path
40,SQL templates can be loaded from files on the classpath by using the FreemarkerSqlLocator:
40,com/foo/AccountDao/selectAll.sql.ftl
40,"SELECT id, name FROM account ORDER BY <#if sort> ${sortBy}, </#if> id"
40,FreemarkerSqlLocator locator = new FreemarkerSqlLocator(new FreemarkerConfig());
40,"Template template = locator.locate(AccountDao.class, ""selectAll"");"
40,"Map<String, Object> attributes = Map.of("
40,"""sort"", true,"
40,"""sortBy"", sortColumn);"
40,final String sql;
40,try (StringWriter writer = new StringWriter()) {
40,"template.process(attributes, writer);"
40,sql = writer.toString();
40,} catch (TemplateException | IOException e) {
40,"throw new IllegalStateException(""Failed to render template "" + templateName, e);"
40,"In SQL Objects, the @UseFreemarkerSqlLocator annotation supports loading templates:"
40,package com.foo;
40,public interface AccountDao {
40,@SqlQuery
40,@UseFreemarkerSqlLocator
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,"In this example, since the fully qualified class name is com.foo.AccountDao,"
40,SQL will be loaded from the file com/foo/AccountDao/selectAll.sql.ftl on the
40,"classpath. By default, every template must be a separate file on the class path."
40,package com.foo;
40,public interface AccountDao {
40,"@SqlQuery(""listSorted"")"
40,@UseFreemarkerSqlLocator
40,"List<Account> selectAll(@Define boolean sort,"
40,@Define String sortBy);
40,"Setting a value in the SQL operation affects the file loaded. In this example, changing the name to listSorted will load the file com/foo/AccountDao/listSorted.sql.ftl from the classpath."
40,"Freemarker has a wealth of configuration options, please see the Freemarker Website for details."
40,"The @UseFreemarkerEngine annotation activates parsing of the value of a SQL method annotation using the Freemarker engine while the @UseFreemarkerSqlLocator uses files on the class path to load and parse FreeMarker templates. These annotations are mutually exclusive and the behavior, if these annotations are used simultaneously, is undefined."
40,16.17. Vavr
40,The Vavr Plugin offers deep integration of Jdbi with the Vavr functional library:
40,Supports argument resolution of sever Vavr Value types such as
40,"Option<T>, Either<L, T>, Lazy<T>, Try<T> and Validation<T>."
40,Given that for the wrapped type T a Mapper is registered.
40,"Return Vavr collection types from queries. Supported are Seq<T>, Set<T>,"
40,"Map<K, T> and Multimap<K, T> as well as all subtypes thereof."
40,"It is possible to collect into a Traversable<T>, in this case a List<T>"
40,will be returned.
40,For all interface types a sensible default implementation will be used
40,"(e.q. List<T> for Seq<T>). Furthermore Multimap<K, T>s are backed by a Seq<T>"
40,as default value container.
40,Columns can be mapped into Vavr’s Option<T> type.
40,Tuple projections for Jdbi! Yey! Vavr offers Tuples up to a maximum arity of 8.
40,"you can map your query results e.g. to Tuple3<Integer, String, Long>."
40,If you select more columns than the arity of the projection the columns
40,up to that index will be used.
40,"To use the plugin, add a dependency:"
40,For Apache Maven:
40,<dependency>
40,<groupId>org.jdbi</groupId>
40,<artifactId>jdbi3-vavr</artifactId>
40,<version>3.45.2-SNAPSHOT</version>
40,</dependency>
40,For Gradle:
40,dependencies {
40,"implementation(""org.jdbi:jdbi3-vavr:3.45.2-SNAPSHOT"")"
40,Vavr introduced binary incompatibilities between 0.10.x and 1.0.0-alpha. The Jdbi plugin only supports 0.9.x and 0.10.x and requires recompilation for vavr 1.0.0.
40,jdbi.installPlugin(new VavrPlugin());
40,Here are some usage examples of the features listed above:
40,"String query = ""SELECT * FROM users WHERE :name is null or name = :name"";"
40,"Option<String> param = Option.of(""eric"");"
40,// will fetch first user with given name or first user with any name (Option.none)
40,return handle
40,.createQuery(query)
40,".bind(""name"", param)"
40,.mapToBean(User.class)
40,.findFirst();
40,"where param may be one of Option<T>, Either<L, T>, Lazy<T>, Try<T>"
40,"or Validation<T>. Note that in the case of these types, the nested value must"
40,be 'present' otherwise a null value is used (e.g. for Either.Left or
40,Validation.Invalid).
40,"handle.createQuery(""SELECT name FROM users"")"
40,.collectInto(new GenericType<Seq<String>>() {});
40,This works for all the collection types supported. For the nested value
40,row and column mappers already installed in Jdbi will be used.
40,"Therefore, the following would work and can make sense if the column is nullable:"
40,"handle.createQuery(""SELECT middle_name FROM users"") // nulls incoming!"
40,.collectInto(new GenericType<Seq<Option<String>>>() {});
40,"The plugin will obey configured key and value columns for Map<K, T>"
40,"and Multimap<K, T> return types. In the next example we will key users"
40,"by their name, which is not necessarily unique."
40,"Multimap<String, User> usersByName = handle"
40,".createQuery(""SELECT * FROM users"")"
40,".setMapKeyColumn(""name"")"
40,".collectInto(new GenericType<Multimap<String, User>>() {});"
40,Last but not least we can now project simple queries to Vavr tuples like that:
40,"// given a 'tuples' table with t1 int, t2 varchar, t3 varchar, ..."
40,"List<Tuple3<Integer, String, String>> tupleProjection = handle"
40,".createQuery(""SELECT t1, t2, t3 FROM tuples"")"
40,".mapTo(new GenericType<Tuple3<Integer, String, String>>() {})"
40,.list();
40,You can also project complex types into a tuple as long as a row mapper is
40,registered.
40,// given that there are row mappers registered for both complex types
40,"Tuple2<City, Address> tupleProjection = handle"
40,".createQuery(""SELECT cityname, zipcode, street, housenumber FROM "" +"
40,"""addresses WHERE user_id = 1"")"
40,".mapTo(new GenericType<Tuple2<City, Address>>() {})"
40,.one();
40,If you want to mix complex types and simple ones we also got you covered.
40,"Using the TupleMappers class you can configure your projections.(In fact,"
40,you have to - read below!)
40,"handle.configure(TupleMappers.class, c -> c.setColumn(2, ""street"").setColumn(3, ""housenumber""));"
40,"Tuple3<City, String, Integer> result = handle"
40,".createQuery(""SELECT cityname, zipcode, street, housenumber "" +"
40,"""FROM addresses WHERE user_id = 1"")"
40,".mapTo(new GenericType<Tuple3<City, String, Integer>>() {})"
40,.one();
40,Bear in mind:
40,"The configuration of the columns is 1-based, since they reflect"
40,the tuples' values (which you would query by e.g. ._1).
40,Tuples are always mapped fully column-wise or fully via row mappers.
40,If you want to mix row-mapped types and single-column mappings the
40,TupleMappers must be configured properly i.e. all non row-mapped
40,tuple indices must be provided with a column configuration!
40,17. Cookbook
40,This section includes examples of various things you might like to do with Jdbi.
40,17.1. Simple Dependency Injection
40,"Jdbi tries to be independent of using a dependency injection framework,"
40,but it’s straightforward to integrate yours.
40,Just do field injection on a simple custom config type:
40,class InjectedDependencies implements JdbiConfig<InjectedDependencies> {
40,@Inject
40,SomeDependency dep;
40,public InjectedDependencies() {}
40,@Override
40,public InjectedDependencies createCopy() {
40,return this; // effectively immutable
40,Jdbi jdbi = Jdbi.create(myDataSource);
40,myIoC.inject(jdbi.getConfig(InjectedDependencies.class));
40,"// Then, in any component that needs to access it:"
40,getHandle().getConfig(InjectedDependencies.class).dep
40,17.2. LIKE clauses with Parameters
40,"Since JDBC (and therefore Jdbi) does not allow binding parameters into the middle of string literals,"
40,you cannot interpolate bindings into LIKE clauses (LIKE '%:param%').
40,Incorrect usage:
40,"handle.createQuery(""SELECT name FROM things WHERE name like '%:search%'"")"
40,".bind(""search"", ""foo"")"
40,.mapTo(String.class)
40,.list()
40,"This query would try to select where name like '%:search%' literally, without binding any arguments."
40,This is because JDBC drivers will not bind arguments inside string literals.
40,"It never gets that far, though — this query will throw an exception,"
40,because we don’t allow unused argument bindings by default.
40,The solution is to use SQL string concatenation:
40,"handle.createQuery(""SELECT name FROM things WHERE name like '%' || :search || '%'"")"
40,".bind(""search"", ""foo"")"
40,.mapTo(String.class)
40,.list()
40,"Now, search can be properly bound as a parameter to the statement, and it all works as desired."
40,Check the string concatenation syntax of your database before doing this.
40,18. The Extension Framework
40,"The Jdbi core provides a rich, programmatic interface for database operations. It is possible to extend this core by writing extensions that plug into the"
40,Jdbi core framework and provide new functionality.
40,The Extension framework is generic and supports any type of extension that can be registered with the Jdbi core.
40,"Jdbi provides the SQLObject plugin, which offers a declarative API by placing annotations on interface methods."
40,18.1. Using Jdbi extensions
40,Extension types are usually java interface classes. Any interface can be an extension type.
40,Jdbi offers multiple ways to obtain an extension type implementation:
40,Calling either
40,Jdbi#withExtension() or
40,Jdbi#useExtension() methods with an extension type will pass an extension type implementation to a callback that contains user code.
40,The Handle#attach() attaches an extension type implementation directly to an existing
40,Handle object.
40,"Finally, the Jdbi#onDemand() method creates an on-demand implementation of an extension type that is not limited to a callback."
40,This is an extension type for the SQL Object extension which uses annotations to identify functionality:
40,interface SomethingDao {
40,"@SqlUpdate(""INSERT INTO something (id, name) VALUES (:s.id, :s.name)"")"
40,"int insert(@BindBean(""s"") Something s);"
40,"@SqlQuery(""SELECT id, name FROM something WHERE id = ?"")"
40,Optional<Something> findSomething(int id);
40,"If any of the SQL method annotations (@SqlBatch,"
40,"@SqlCall, @SqlQuery,"
40,@SqlScript and @SqlUpdate) is present on either the
40,"extension type itself or any extension type method, the SQL Object extension will be used."
40,The Jdbi#withExtension() and
40,Jdbi#useExtension() methods are used to access
40,SQLObject functionality:
40,"jdbi.useExtension(SomethingDao.class, dao -> {"
40,"dao.insert(new Something(1, ""apple""));"
40,});
40,"Optional<Something> result = jdbi.withExtension(SomethingDao.class, dao -> dao.findSomething(1));"
40,"If a handle has already been created, an extension type can be directly attached with the"
40,Handle#attach() method:
40,SomethingDao dao = handle.attach(SomethingDao.class);
40,Optional<Something> result = dao.findSomething(1);
40,"The Jdbi#withExtension(), Jdbi#useExtension() and Handle#attach() methods are the most common way to use extension type implementations."
40,18.1.1. On-demand extensions
40,An extension type implementation can be acquired by calling the Jdbi#onDemand() method which returns
40,an implementation of the extension type that will transparently create a new handle instance when a method on the extension type is called.
40,This is an example for an on-demand extension:
40,SomethingDao dao = jdbi.onDemand(SomethingDao.class);
40,"dao.insert(new Something(1, ""apple""));"
40,Optional<Something> result = dao.findSomething(1);
40,The Jdbi#onDemand() uses a Java proxy object to provide
40,the on-demand handle object. Java proxies only support Java interface classes as extension types and may not be compatible with
40,GraalVM when creating native code.
40,18.1.2. Handle lifecycle for extensions
40,The difference between the methods to acquire an extension type implementation is the handle (and database connection) lifecycle:
40,Jdbi#withExtension() and
40,Jdbi#useExtension() provides a managed handle object when the callback is entered and reuses it for any extension type method call as long as the code does not return from the callback.
40,The Handle#attach() does not manage the handle at all. Any extension type method call will use the handle that the object was attached to and be part of its lifecycle.
40,"The Jdbi#onDemand() provides a managed handle object for every extension type method call. When the method call returns, the handle is closed."
40,18.2. How an extension works
40,Any interface class can be an extension type. There is nothing special to an extension type unless an extension requires some specific information
40,(e.g. annotations present or specific naming).
40,Extensions should document which information is needed for the extension framework to determine that a specific interface class represents an extension
40,type that they accept.
40,"The extension type creation methods (Jdbi#withExtension(),"
40,"Jdbi#useExtension(),"
40,Handle#attach() and Jdbi#onDemand()) all take the extension type as a parameter and provide an implementation either to the caller or within a callback object.
40,The extension framework selects an ExtensionFactory to handle the extension type and create an implementation. Extension factories must be registered ahead of time with the Jdbi core using the
40,Extensions#register() method.
40,The extension factory then creates an ExtensionMetadata object that contains information about each
40,method on the extension type:
40,Which ExtensionHandler to invoke for each method
40,All ExtensionHandlerCustomizer instances that change the behavior of the extension handler
40,Per extension type and per extension type method specific ConfigCustomizer instances to apply when an extension handler is invoked
40,Extension metadata is created once for each extension type and then reused.
40,"Jdbi usually returns a Java proxy object to user code. Calling a method on that proxy will invoke per-method extension handlers. For each extension handler, objects specific to the invocation"
40,"(Handle, configuration) are managed separately ."
40,"ExtensionHandler,"
40,ExtensionHandlerCustomizer and
40,ConfigCustomizer are the three main extension framework objects to implement:
40,An ExtensionHandler object is created by an
40,"ExtensionHandlerFactory, which can be registered either globally (with the"
40,"Extensions configuration) and then applied to all extensions types, or directly with an extension factory. It is also possible to specify extension handlers through annotations."
40,ExtensionHandlerCustomizer objects can modify the behavior of an extension handler. They are
40,registered globally or provided by an extension factory. They can also be specified through annotations.
40,ConfigCustomizer objects are created by a
40,ConfigCustomizerFactory and modify the configuration that is used when an extension handler is
40,executed. Factory instances are registered globally or with an extension factory. Config customization
40,can also be specified through annotations.
40,18.3. Extension framework SDK
40,This chapter is intended for developers who want to write a new extension for Jdbi and need to understand the Extension framework details. This chapter is only
40,relevant if you plan to write an extension or implement specific extension objects.
40,"This chapter lists all the classes that make up the extension framework, their function and how to use them when writing a new extension. Not all pieces are"
40,needed for every extension.
40,18.3.1. Extension configuration
40,The Extensions class is the configuration object for the extension framework. It contains a registry for
40,"extension factories and global extension objects (ExtensionHandlerFactory,"
40,ExtensionHandlerCustomizer and
40,ConfigCustomizerFactory).
40,"Any extension object registered here will be applied to all extension types, independent of the extension factory which is ultimately chosen to process an"
40,extension type.
40,"By default, the following extension objects are globally registered and can not be removed. These are always processed last (after all user registered extension"
40,objects):
40,An extension handler factory for the @UseExtensionHandler annotation.
40,An extension handler customizer for the @UseExtensionHandlerCustomizer
40,annotation.
40,A config customizer factory for the @UseExtensionConfigurer annotation.
40,"Order matters with extension objects! For each extension object, the extension specific instances are applied first, then the globally registered ones."
40,18.3.2. The Extension factory
40,Extension factories are the core piece for any extension. An ExtensionFactory object can provide all the
40,specific pieces that make up the extension functionality or delegate to annotations.
40,The only method that must be implemented in this interface is
40,"ExtensionFactory#accepts(). If this method returns true, then the given"
40,extension type is handled by this factory. E.g. the SQL Object factory accepts any interface that has at least
40,one SQL annotation on the class itself or a method.
40,"Even though it is possible to implement all the functions of an extension with annotations, there still needs to be an extension factory that accepts the"
40,"extension type. Without such a factory, the extension framework will throw a"
40,NoSuchExtensionException.
40,Any extension factory can provide specific extension objects (by returning instances of
40,"ExtensionHandlerFactory,"
40,ExtensionHandlerCustomizer and
40,ConfigCustomizerFactory) which are only applied when the factory has accepted an extension
40,type. The extension factory provides the
40,"ExtensionFactory#getExtensionHandlerFactories(),"
40,ExtensionFactory#getExtensionHandlerCustomizers()
40,and
40,ExtensionFactory#getConfigCustomizerFactories()
40,getters that a factory can implement.
40,"By default, each returns an empty collection."
40,Java Object methods
40,"When the extension framework returns an implementation of an extension type, it usually returns a Java proxy"
40,object which will invoke an extension handler for every method on the extension type.
40,"In addition to the methods defined by the extension type, the following methods also invoke extension handlers:"
40,"Object#toString() returns ""Jdbi extension proxy for <extension type>"""
40,Object#equals() and Object#hashCode()
40,implementations that ensure that each instance is only equal to itself.
40,Object#finalize() has no effect.
40,All methods except finalize() can be overridden by the extension type (e.g. through an interface default method).
40,Non-virtual factories
40,"Normally, a factory will use extension handlers to provide functionality when an extension type method is called and a"
40,Java proxy object is returned as an implementation of the extension type. This is a virtual
40,factory (because there is no actual object implementing the interface). Extension factories are by default virtual factories.
40,The SQL Object extension is an example of a virtual factory because it never provides an implementation for extension types but
40,dispatches any method calls to a specific handler to execute SQL operations.
40,The extension framework also supports non-virtual factories. These factories produce a backing object that implements the extension type. A non-virtual factory will still return a proxy object to the caller but calling any method on the proxy object may call the corresponding method on the backing object through a special extension handler.
40,This is an example of a non-virtual factory that provides an implementation for a specific extension type:
40,public static class NonVirtualExtensionFactory implements ExtensionFactory {
40,@Override
40,public Set<FactoryFlag> getFactoryFlags() {
40,return EnumSet.of(NON_VIRTUAL_FACTORY); (1)
40,@Override
40,public boolean accepts(Class<?> extensionType) {
40,return extensionType == NonVirtualDao.class;
40,@Override
40,"public <E> E attach(Class<E> extensionType, HandleSupplier handleSupplier) {"
40,return (E) new NonVirtualDaoImpl(handleSupplier); (2)
40,An extension factory declares itself non-virtual by returning the
40,NON_VIRTUAL_FACTORY flag on the
40,ExtensionFactory#getFactoryFlags() method.
40,The extension framework calls the
40,ExtensionFactory#attach() method with
40,the current extension type and a handle supplier. The factory is expected to return an instance of the extension type. Methods on the instance will be wrapped
40,in extension handlers and a proxy object is returned to the caller.
40,Extensions without Java proxy objects
40,A drawback of the standard extension factories is that returning a Java proxy object limits factory functionality
40,and is incompatible with some use cases (e.g. using GraalVM
40,for native compilation).
40,A factory can return the DONT_USE_PROXY flag on the
40,ExtensionFactory#getFactoryFlags() method to signal that the extension framework should
40,return the backing object as-is.
40,"This is an example of a factory that supports abstract classes in addition to interfaces. Abstract classes are incompatible with java proxies, so the factory"
40,must use the DONT_USE_PROXY flag:
40,abstract static class AbstractDao {
40,public abstract Optional<Something> findSomething(int id);
40,Something findFirstSomething() {
40,return findSomething(1).orElseGet(Assertions::fail);
40,public static class DontUseProxyExtensionFactory implements ExtensionFactory {
40,@Override
40,public Set<FactoryFlag> getFactoryFlags() {
40,return EnumSet.of(DONT_USE_PROXY); (1)
40,@Override
40,public boolean accepts(Class<?> extensionType) {
40,return extensionType == AbstractDao.class;
40,@Override
40,"public <E> E attach(Class<E> extensionType, HandleSupplier handleSupplier) {"
40,return extensionType.cast(new DaoImpl(handleSupplier)); (2)
40,An extension factory declares that it does not want to use the proxy mechanism by returning the
40,DONT_USE_PROXY flag from the
40,ExtensionFactory#getFactoryFlags() method. Using this flag bypasses all the extension
40,framework proxy mechanism and calls the
40,ExtensionFactory#attach() method
40,immediately.
40,The extension factory creates an instance that must be compatible with the extension type. It can provide the current extension type and a handle supplier
40,"if needed. Unlike a non-virtual factory, this instance is returned as-is to the caller. Neither proxy object nor extension handlers are created."
40,"Jdbi provides an annotation processor that creates java implementations of extension types. In addition, it"
40,provides an extension factory which does not use proxy objects and returns these as extension type implementations. Calling a method on the extension type will invoke the methods on the generated classes directly.
40,18.3.3. Extension handlers and their factories
40,ExtensionHandler objects are the main place for extension functionality. Each method on an extension type
40,is mapped to an extension handler. Extension handlers are executed by calling the
40,ExtensionHandler#invoke()
40,method.
40,This is an example of an extension handler that provides a value whenever a method on the extension type is called:
40,public static class TestExtensionHandler implements ExtensionHandler {
40,@Override
40,"public Object invoke(HandleSupplier handleSupplier, Object target, Object... args) {"
40,"return new Something((int) args[0], (String) args[1]); (1)"
40,Invoking this handler returns a Something instance.
40,"Extension handlers are not shared, so every extension type method maps onto a separate instance. The"
40,ExtensionHandlerFactory interface provides the
40,ExtensionHandlerFactory#accepts() method which
40,"is called for each method that needs to be mapped. If a factory accepts a method, the"
40,ExtensionHandlerFactory#createExtensionHandler()
40,"method is called, which should return an extension handler instance."
40,This is an example for an extension handler factory that returns an extension handler for methods named getSomething:
40,public static class TestExtensionHandlerFactory implements ExtensionHandlerFactory {
40,@Override
40,"public boolean accepts(Class<?> extensionType, Method method) {"
40,"return method.getName().equals(""getSomething""); (1)"
40,@Override
40,"public Optional<ExtensionHandler> createExtensionHandler(Class<?> extensionType, Method method) {"
40,return Optional.of(new TestExtensionHandler()); (2)
40,interface ExtensionType {
40,"Something getSomething(int id, String name);"
40,The extension handler factory tests that the method has the right name.
40,"If the factory accepts the method, it returns a new instance of the extension handler shown above."
40,The
40,ExtensionHandlerFactory#createExtensionHandler()
40,method returns an Optional<ExtensionHandler> object for backwards compatibility reasons. While it is legal for a
40,"factory to accept a method on an extension type and then return an Optional.empty() object, it is discouraged"
40,in new code.
40,The extension framework provides extension handlers for interface default methods and synthetic methods.
40,An extension handler for a specific extension type can be either registered globally by adding a factory using the
40,Extensions#registerHandlerFactory()
40,"method, or it can be returned on the"
40,ExtensionFactory#getExtensionHandlerFactories()
40,method of the extension factory for the extension type. It can also be configured through an annotation.
40,This is the extension factory for the ExtensionType extension type defined above:
40,public static class TestExtensionFactory implements ExtensionFactory {
40,@Override
40,public boolean accepts(Class<?> extensionType) {
40,return extensionType == ExtensionType.class; (1)
40,@Override
40,public Collection<ExtensionHandlerFactory> getExtensionHandlerFactories(ConfigRegistry config) {
40,return Collections.singleton(new TestExtensionHandlerFactory()); (2)
40,The factory will accept the ExtensionType extension type.
40,"When processing the type, the TestExtensionHandlerFactory shown above will be used when creating the extension handlers."
40,18.3.4. Extension handler customizers
40,ExtensionHandlerCustomizer objects can modify extension handler execution. They are
40,either registered globally with the
40,Extensions#registerHandlerCustomizer()
40,"and will augment any extension handler, or they can be provided by an extension factory by implementing the"
40,ExtensionFactory#getConfigCustomizerFactories()
40,method.
40,Here is an example for an extension handler customizer that logs every method handler invocation. It gets registered as a global extension handler customizer
40,with the Extensions configuration object and will then log every method invocation on an extension type.
40,"jdbi.configure(Extensions.class, e ->"
40,e.registerHandlerCustomizer(new LoggingExtensionHandlerCustomizer())); (1)
40,static class LoggingExtensionHandlerCustomizer implements ExtensionHandlerCustomizer {
40,@Override
40,"public ExtensionHandler customize(ExtensionHandler handler, Class<?> extensionType, Method method) {"
40,"return (handleSupplier, target, args) -> {"
40,"LOG.info(format(""Entering %s on %s"", method, extensionType.getSimpleName()));"
40,try {
40,"return handler.invoke(handleSupplier, target, args);"
40,} finally {
40,"LOG.info(format(""Leaving %s on %s"", method, extensionType.getSimpleName()));"
40,Register as a global ExtensionHandlerCustomizer instance to log every method invocation.
40,This is example code. Jdbi actually offers better logging facilities with SqlLogger and the
40,SqlStatements#setSqlLogger() method.
40,ExtensionHandlerCustomizer instances usually wrap or replace an existing extension
40,handler. They must be stateless and can only modify or operate on the ExtensionHandler object
40,that was passed into the ExtensionHandlerCustomizer#customize()
40,method.
40,18.3.5. Config customizers and their factories
40,ConfigCustomizer instances modify the configuration
40,registry object which is used when an extension type method is invoked. This configuration object is passed to all extension handler customizers and the
40,extension handler itself.
40,Config customizer instances are created by ConfigCustomizerFactory instances which decide whether
40,to provide a config customizer for every method on an extension type by implementing the
40,"ConfigCustomizerFactory#forExtensionType() method, or it can"
40,implement
40,ConfigCustomizerFactory#forExtensionMethod()
40,for per-method selection.
40,This is an example for a global configuration customizer that registers a specific row mapper for all methods on every extension type.
40,"jdbi.configure(Extensions.class, e ->"
40,e.registerConfigCustomizerFactory(new SomethingMapperConfigCustomizerFactory())); (1)
40,static class SomethingMapperConfigCustomizerFactory implements ConfigCustomizerFactory {
40,@Override
40,public Collection<ConfigCustomizer> forExtensionType(Class<?> extensionType) {
40,return Collections.singleton(
40,config -> config.get(RowMappers.class).register(new SomethingMapper()) (2)
40,Register a global config customizer. It will be applied to all extension types.
40,Register the SomethingMapper to the every extension handler for every extension type.
40,18.4. Annotations
40,A Jdbi extension like the SQL Object extension uses annotations on the extension types. It is possible to implement a large part of the
40,functionality directly as annotation related objects without having to provide extension objects through an extension factory.
40,The extension framework offers meta-annotations to configure ExtensionHandler and
40,ExtensionHandlerCustomizer instances directly and another meta-annotation to define
40,ExtensionConfigurer instances which allows configuration modification similar to
40,ConfigCustomizer).
40,Meta-annotations are used to annotate other annotation classes for processing by the extension framework.
40,18.4.1. Extension handler annotation
40,The @UseExtensionHandler meta-annotation marks any other annotation as defining an
40,extension handler.
40,"These annotations must be placed on a method in an extension type, therefore annotations that are marked with the meta-annotation should use the"
40,@Target({ElementType.METHOD}) annotation to limit their scope.
40,The extension framework processes all annotations that use this meta-annotation. It will instantiate the extension handler by locating
40,a constructor that takes both the extension type and a method object
40,a constructor that takes only the extension type
40,a no-argument constructor
40,"If none of those can be found, an exception will be thrown."
40,"When using this meta-annotation, the UseExtensionHandler#id() method must return an id"
40,value that is specific for the extension factory that should process the annotation. This allows multiple extensions that use annotations to co-exist.
40,This is an example of an annotation that provides the handler for the getSomething method on an extension type:
40,"jdbi.configure(Extensions.class, extensions ->"
40,(1)
40,extensions.register(extensionType ->
40,Stream.of(extensionType.getMethods())
40,.anyMatch(method ->
40,Stream.of(method.getAnnotations())
40,.map(annotation -> annotation.annotationType()
40,.getAnnotation(UseExtensionHandler.class)) (2)
40,".anyMatch(a -> a != null && ""test"".equals(a.id())) (3)"
40,)));
40,interface ExtensionType {
40,@SomethingAnnotation (4)
40,"Something getSomething(int id, String name);"
40,@Retention(RetentionPolicy.RUNTIME)
40,@Target({ElementType.METHOD}) (5)
40,"@UseExtensionHandler(id = ""test"", (6)"
40,value = SomethingExtensionHandler.class) (7)
40,@interface SomethingAnnotation {}
40,public static class SomethingExtensionHandler implements ExtensionHandler {
40,@Override
40,"public Object invoke(HandleSupplier handleSupplier, Object target, Object... args) {"
40,"return new Something((int) args[0], (String) args[1]);"
40,An extension factory that implements only the ExtensionFactory#accepts() method can be written as a lambda.
40,The factory accepts this extension type if it has at least one method that is annotated with the
40,@UseExtensionHandler meta-annotation.
40,Ensure that the annotation uses the test id value.
40,The @SomethingAnnotation provides the extension handler. Any method with this annotation will use the same extension handler.
40,The custom annotation targets only methods.
40,"The test id value ensures that the registered factory will accept the extension type. Any extension type must be accepted by a factory, otherwise the"
40,extension framework will reject the extension type.
40,The annotation provides the extension handler class. An instance of this class is created by the extension framework directly without an extension handler
40,factory.
40,"Even though multiple extensions can can be used at the same time, they can not share an extension type."
40,Every extension type will only get extension handlers
40,associated with a single factory. It is not possible to have an extension type where methods are processed by extension handlers that are annotated with
40,annotations that use different id values. Any extension handler must be provided either by annotations that match the id value of the
40,ExtensionFactory or an extension handler factory.
40,"All SQL method annotations in the Jdbi SQL Object extension (@SqlBatch,"
40,"@SqlCall, @SqlQuery,"
40,@SqlScript and @SqlUpdate) are marked with this
40,meta-annotation.
40,18.4.2. Extension handler customizer annotation
40,The @UseExtensionHandlerCustomizer meta-annotation marks any other annotation
40,as defining an extension handler customizer.
40,An example in the SQL object extension is the @Transaction annotation which allows
40,wrapping multiple SQL operations into a database transaction.
40,"Extension handler customizers either apply to all methods in an extension type by adding an annotation to the extension type, or to specific methods by adding"
40,the annotation to a method:
40,@Retention(RetentionPolicy.RUNTIME)
40,"@Target({ElementType.METHOD, ElementType.TYPE}) (1)"
40,@UseExtensionHandlerCustomizer(
40,value = LoggingExtensionHandlerCustomizer.class)
40,@interface LogThis {}
40,interface MethodExtensionType {
40,@LogThis
40,(2)
40,@SomethingAnnotation
40,"Something getSomething(int id, String name);"
40,@LogThis (3)
40,interface InstanceExtensionType {
40,@SomethingAnnotation
40,"Something getSomething(int id, String name);"
40,An annotation that uses the @UseExtensionHandlerCustomizer annotation can
40,be placed on a method or the extension type itself.
40,The extension handler customizer is only applied to an annotated method.
40,The extension handler customizer is applied to all methods on the extension type.
40,The extension framework processes all annotations that use this meta-annotation. It will instantiate the extension handler customizer by locating
40,a constructor that takes both the extension type and a method object
40,a constructor that takes only the extension type
40,a no-argument constructor
40,"If none of those can be found, an exception will be thrown."
40,This is an example for an annotation that provides a handler customizer which logs method entry and exit for any extension handler:
40,public static class LoggingExtensionHandlerCustomizer implements ExtensionHandlerCustomizer {
40,@Override
40,"public ExtensionHandler customize(ExtensionHandler handler, Class<?> extensionType, Method method) {"
40,"return (handleSupplier, target, args) -> {"
40,"LOG.info(format(""Entering %s on %s"", method, extensionType.getSimpleName()));"
40,try {
40,"return handler.invoke(handleSupplier, target, args);"
40,} finally {
40,"LOG.info(format(""Leaving %s on %s"", method, extensionType.getSimpleName()));"
40,Extension handler customizer objects are not tied to a specific extension factory. This is an example where SQL Object method annotations are
40,used with the custom logging annotation:
40,interface SomethingDao {
40,"@SqlUpdate(""INSERT INTO something (id, name) VALUES (:s.id, :s.name)"")"
40,"int insert(@BindBean(""s"") Something s);"
40,"@SqlQuery(""SELECT id, name FROM something WHERE id = ?"")"
40,@LogThis
40,Optional<Something> findSomething(int id);
40,Specifying extension handler customization order
40,"When multiple extension customizers are used on the same extension type method, the order in which they are applied is undefined. If a specific order is"
40,"required (e.g. because one customization depends on another), the"
40,@ExtensionHandlerCustomizationOrder annotation may be used to enforce a
40,specific order.
40,Listing the annotation classes orders the extension customizers from outermost to innermost (outermost is called first). Any unlisted annotation will still be
40,applied but its position is undefined. It is recommended to list all applicable annotation classes when specifying the order.
40,interface OrderedCustomizers {
40,@Bar
40,@Foo
40,"@ExtensionHandlerCustomizationOrder({Foo.class, Bar.class}) (1)"
40,void execute();
40,"The annotation ensures that the Foo extension customizer is called first, then the Bar extension customizer. Otherwise, the order is undefined."
40,18.4.3. Extension handler configurer annotation
40,The @UseExtensionConfigurer meta-annotation marks any other annotation as defining an
40,extension configurer.
40,"Similar to extension handler customizers, configurers either apply to all methods in an extension type by adding an annotation to the extension type, or to"
40,specific methods by adding the annotation to a method.
40,The meta-annotation provides an implementation of the ExtensionConfigurer.
40,the
40,ExtensionConfigurer#configureForType()
40,is called if the anntation is placed on the extension type
40,method provides and
40,ExtensionConfigurer#configureForMethod()
40,is called when the annotation is placed on an extension type method
40,These methods are the equivalent of
40,ConfigCustomizerFactory#forExtensionType() and
40,ConfigCustomizerFactory#forExtensionMethod().
40,The extension framework processes all annotations that use this meta-annotation.
40,It will instantiate the extension configurer by locating
40,"a constructor that takes the annotation, the extension type, and a method object"
40,a constructor that takes the annotation and the extension type
40,a constructor that takes the annotation only
40,a no-argument constructor
40,"If none can be found, an exception will be thrown."
40,This is an example of an extension configurer annotation that adds a specific row mapper:
40,@Retention(RetentionPolicy.RUNTIME)
40,"@Target({ElementType.METHOD, ElementType.TYPE})"
40,@UseExtensionConfigurer(value = SomethingMapperExtensionConfigurer.class) (1)
40,@interface RegisterSomethingMapper {}
40,interface SomethingDao {
40,"@SqlUpdate(""INSERT INTO something (id, name) VALUES (:s.id, :s.name)"")"
40,"int insert(@BindBean(""s"") Something s);"
40,"@SqlQuery(""SELECT id, name FROM something WHERE id = ?"")"
40,@RegisterSomethingMapper (2)
40,Optional<Something> findSomething(int id);
40,public static class SomethingMapperExtensionConfigurer extends SimpleExtensionConfigurer { (3)
40,@Override
40,"public void configure(ConfigRegistry config, Annotation annotation, Class<?> extensionType) {"
40,config.get(RowMappers.class).register(new SomethingMapper());
40,define the extension configurer class in the annotation
40,add the custom annotation to a method in a SQL Object extension type
40,The implementation extends SimpleExtensionConfigurer as it should behave the same when used
40,on an extension type or an extension type method.
40,"Similar to extension handler customizer objects, extension configurers are also not tied to a specific extension factory."
40,"In the example above, a custom"
40,extension configurer is used in combination with the SQL Object extension annotations.
40,"If an extension configurer should behave the same way independent of its position, the implementation should extend"
40,SimpleExtensionConfigurer and implement the
40,SimpleExtensionConfigurer#configure()
40,method.
40,The SQL Object extension implements all per-method and per-class registration of specific objects
40,(e.g. @RegisterColumnMapper or
40,@RegisterRowMapper) through extension configurer objects which in turn create config customizer
40,instances.
40,18.5. Extension metadata
40,Interacting directly with the ExtensionMetadata and
40,ExtensionHandlerInvoker is an advanced use case when writing specific extension
40,"code. For most extensions, all metadata interactions are handled by the framework and don’t require anything special."
40,"For every extension type that is processed by the framework, an ExtensionMetadata object is created. It"
40,collects all the information that is required to create the java proxy object:
40,extension handlers for each extension type method including optional extension handler customizers.
40,instance config customizers which are applied to every method
40,method specific config customizers
40,Extension metadata is created using a builder when an extension type is processed for the first time. It is possible for an
40,ExtensionFactory to participate in the metadata creation. E.g. the SQL Object extension
40,uses this to add extension handlers for the SqlObject interface methods.
40,"If an extension factory wants to participate in the metadata creation, it needs to implement the"
40,ExtensionFactory#buildExtensionMetadata()
40,method which is called with the extension metadata builder right before the actual metadata object is created. See the
40,ExtensionMetadata.Builder documentation for all available methods.
40,Metadata for an extension type can be retrieved from the Extensions configuration object by calling the
40,Extensions#findMetadata().
40,A metadata object can create ExtensionHandlerInvoker instances for all methods
40,on the extension type that is represents. An extension handler invoker binds the actual extension handler instance to a specific configuration object (and the
40,config customizers applied to that configuration object).
40,The main use case is within an extension type implementation class as it requires a HandlerSupplier
40,"instance, which is only available through the"
40,ExtensionFactory#attach() method. By
40,"creating extension handler invokers and metadata manually, an implementation can sidestep all the proxy logic and directly wire up method invocation or redirect"
40,invocations between methods. The jdbi SQL object generator uses this mechanism to create implementation classes that do not need the java proxy
40,logic (which is problematic with GraalVM).
40,This is an example of creating and calling an extension handler invoker directly:
40,@Test
40,public void testIdOne() {
40,"Something idOne = jdbi.withExtension(ExtensionType.class, ExtensionType::getIdOne);"
40,"assertThat(idOne).isEqualTo(new Something(1, ""apple""));"
40,interface ExtensionType {
40,"Something getSomething(int id, String name);"
40,Something getIdOne();
40,(1)
40,public static class TestExtensionFactory implements ExtensionFactory {
40,@Override
40,public boolean accepts(Class<?> extensionType) {
40,return extensionType == ExtensionType.class;
40,@Override
40,"public <E> E attach(Class<E> extensionType, HandleSupplier handleSupplier) {"
40,ExtensionMetadata extensionMetadata = handleSupplier.getConfig() (2)
40,".get(Extensions.class).findMetadata(extensionType, this);"
40,"return extensionType.cast(new ExtensionTypeImpl(extensionMetadata, handleSupplier)); (3)"
40,@Override
40,public Set<FactoryFlag> getFactoryFlags() {
40,return EnumSet.of(DONT_USE_PROXY); (4)
40,@Override
40,public Collection<ExtensionHandlerFactory> getExtensionHandlerFactories(ConfigRegistry config) {
40,return Collections.singleton(new TestExtensionHandlerFactory()); (5)
40,public static class TestExtensionHandlerFactory implements ExtensionHandlerFactory {
40,@Override
40,"public boolean accepts(Class<?> extensionType, Method method) {"
40,"return method.getName().equals(""getSomething"");"
40,(6)
40,@Override
40,"public Optional<ExtensionHandler> createExtensionHandler(Class<?> extensionType, Method method) {"
40,"return Optional.of((handleSupplier, target, args) -> new Something((int) args[0], (String) args[1]));"
40,static class ExtensionTypeImpl implements ExtensionType {
40,private final ExtensionHandlerInvoker getSomethingInvoker;
40,"private ExtensionTypeImpl(ExtensionMetadata extensionMetadata, HandleSupplier handleSupplier) {"
40,ConfigRegistry config = handleSupplier.getConfig();
40,"this.getSomethingInvoker = extensionMetadata.createExtensionHandlerInvoker(this,"
40,"JdbiClassUtils.methodLookup(ExtensionType.class, ""getSomething"", int.class, String.class), (7)"
40,"handleSupplier, config);"
40,@Override
40,"public Something getSomething(int id, String name) {"
40,"return (Something) getSomethingInvoker.invoke(id, name);"
40,@Override
40,public Something getIdOne() {
40,"return (Something) getSomethingInvoker.invoke(1, ""apple""); (8)"
40,The getIdOne method takes no parameters and returns a constant Something object
40,The extension factory retrieves the metadata for the ExtensionType class. This call is usually done by the extension framework but can be done manually
40,when implementing the
40,ExtensionFactory#attach().
40,The extension factory returns an implementation of the extension type.
40,The extension factory requests that the framework returns the implementation as is and not wrap it into extension handlers.
40,The extension factory adds an extension handler factory that provides the actual implementation.
40,"The extension handler factory only accepts the getSomething method, it does not create an extension handler for getIdOne."
40,The extension type uses the metadata object to create an extension handler invoker for the getSomething method. This is handled by the custom extension
40,handler factory.
40,The getIdOne method now invokes the extension handler for the getSomething method and passes constant values to the
40,ExtensionHandlerInvoker#invoke() method.
40,19. Advanced Topics
40,19.1. High Availability
40,Jdbi can be combined with connection pools and high-availability features in your database driver.
40,We’ve used HikariCP in combination
40,with the PgJDBC connection load balancing
40,features with good success.
40,PGSimpleDataSource ds = new PGSimpleDataSource();
40,"ds.setServerName(""host1,host2,host3"");"
40,ds.setLoadBalanceHosts(true);
40,HikariConfig hc = new HikariConfig();
40,hc.setDataSource(ds);
40,hc.setMaximumPoolSize(6);
40,Jdbi jdbi = Jdbi.create(new HikariDataSource(hc)).installPlugin(new PostgresPlugin());
40,"Each Jdbi may be backed by a pool of any number of hosts, but the connections should all be alike."
40,Exactly which parameters must stay the same and which may vary depends on your database and driver.
40,"If you want to have two separate pools, for example a read-only set that connects to read replicas"
40,"and a smaller pool of writers that go only to a single host, you currently should have separate"
40,Jdbi instances each pointed at a separate DataSource.
40,19.2. Compiling with Parameter Names
40,"By default, the Java compiler does not write parameter names of constructors and"
40,"methods to class files. At runtime, using reflection to find"
40,parameter names
40,"will return values like ""arg0"", ""arg1"", etc."
40,"Out of the box, Jdbi uses annotations to name parameters, e.g.:"
40,ConstructorMapper uses the @ConstructorProperties annotation.
40,SQL Object method arguments use the @Bind annotation.
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,"void insert(@Bind(""id"") long id, @Bind(""name"") String name);"
40,"If you compile your code with the -parameters compiler flag, then the need for"
40,these annotations is removed — Jdbi automatically uses the method parameter name:
40,"@SqlUpdate(""INSERT INTO users (id, name) VALUES (:id, :name)"")"
40,"void insert(long id, String name);"
40,19.2.1. Maven Setup
40,Configure the maven-compiler-plugin in your POM:
40,<plugin>
40,<groupId>org.apache.maven.plugins</groupId>
40,<artifactId>maven-compiler-plugin</artifactId>
40,<configuration>
40,<compilerArgs>
40,<arg>-parameters</arg>
40,</compilerArgs>
40,</configuration>
40,</plugin>
40,19.2.2. IntelliJ IDEA setup
40,File → Settings
40,"Build, Execution, Deployment → Compiler → Java Compiler"
40,Additional command-line parameters: -parameters
40,"Click Apply, then OK."
40,Build → Rebuild Project
40,19.2.3. Eclipse Setup
40,Window → Preferences
40,Java → Compiler
40,"Under ""Classfile Generation,"" check the option ""Store information about"
40,"method parameters (usable via reflection)."""
40,19.3. Working with Generic Types
40,Jdbi provides utility classes to make it easier to work with Java generic types.
40,19.3.1. GenericType
40,GenericType represents a generic
40,type signature that can be passed around in a type-safe way.
40,Create a generic type reference by instantiating an anonymous inner class:
40,new GenericType<Optional<String>>() {}
40,This type reference can be passed to any Jdbi method that accepts a
40,"GenericType<T>, e.g.:"
40,List<Optional<String>> middleNames = handle
40,".select(""SELECT middle_name FROM contacts"")"
40,.mapTo(new GenericType<Optional<String>>() {})
40,.list();
40,The GenericType.getType() returns the raw
40,java.lang.reflect.Type object used
40,to represent generics in Java.
40,19.3.2. The GenericTypes helper
40,GenericTypes provides methods
40,for working with Java generic types signatures.
40,All methods in GenericTypes operate in terms of java.lang.reflect.Type.
40,The getErasedType(Type) method accepts a Type and returns the raw Class
40,"for that type, with any generic parameters erased:"
40,Type listOfInteger = new GenericType<List<Integer>>() {}.getType();
40,GenericTypes.getErasedType(listOfInteger); // => List.class
40,GenericTypes.getErasedType(String.class); // => String.class
40,"The resolveType(Type, Type) method takes a generic type, and a context type in"
40,which to resolve it.
40,"For example, given the type variable T from Optional<T>:"
40,Type t = Optional.class.getTypeParameters()[0];
40,And given the context type Optional<String>:
40,Type optionalOfString = new GenericType<Optional<String>>() {}.getType();
40,"The resolveType() method answers the question: ""what is type T, in the context"
40,"of type Optional<String>?"""
40,"GenericTypes.resolveType(t, optionalOfString);"
40,// => String.class
40,This scenario of resolving the first type parameter of some generic supertype is
40,so common that we made a separate method for it:
40,"GenericTypes.findGenericParameter(optionalOfString, Optional.class);"
40,// => Optional.of(String.class)
40,Type listOfInteger = new GenericType<List<Integer>>() {}.getType();
40,"GenericTypes.findGenericParameter(listOfInteger, Collection.class);"
40,// => Optional.of(Integer.class)
40,Note that this method will return Optional.empty() if the type parameter
40,"cannot be resolved, or the types have nothing to do with each other:"
40,"GenericTypes.findGenericParameter(optionalOfString, List.class);"
40,// => Optional.empty();
40,19.4. NamedArgumentFinder
40,The NamedArgumentFinder
40,"interface, as its name suggests, finds arguments by name from some source."
40,"Typically, a single NamedArgumentFinder instance will provide arguments for"
40,several names.
40,"In cases where neither bindBean(), bindFields(), bindMethods(), nor"
40,"bindMap() are a good fit, you can implement your own NamedArgumentFinder and"
40,"bind that, instead of extracting and binding each argument individually."
40,Cache cache = ... // e.g. Guava Cache
40,"NamedArgumentFinder cacheFinder = (name, ctx) ->"
40,Optional.ofNullable(cache.getIfPresent(name))
40,".map(value -> ctx.findArgumentFor(Object.class, value));"
40,stmt.bindNamedArgumentFinder(cacheFinder);
40,"Under the hood, the"
40,"SqlStatement.bindBean(),"
40,"SqlStatement.bindMethods(),"
40,"SqlStatement.bindFields(),"
40,and
40,SqlStatement.bindMap()
40,methods are just creating and binding custom implementations of
40,"NamedArgumentFinder for beans, methods, fields, and maps, respectively."
40,19.5. JdbiConfig
40,Configuration is managed by the
40,ConfigRegistry class.
40,Each Jdbi object that represents a distinct database context (for
40,"example, Jdbi itself, a"
40,"Handle instance, or an attached"
40,SqlObject class) has a
40,separate config registry instance.
40,A context implements the
40,Configurable interface
40,which allows modification of its configuration as well as retrieving
40,the current context’s configuration for use by Jdbi core or
40,extensions.
40,"When a new context is created, it inherits a copy of its parent"
40,configuration at the time of creation - further modifications to the
40,original will not affect already created configuration contexts.
40,Configuration context copies happen when creating a
40,Handle from
40,"Jdbi, when opening a"
40,SqlStatement from
40,"the Handle, and when attaching or creating an on-demand extension such"
40,as SqlObject.
40,The configuration itself is stored in implementations of the
40,JdbiConfig interface.
40,Each implementation must adhere to the contract of the interface; in
40,particular it must have a public no-argument constructor that provides
40,useful defaults and an implementation of the
40,JdbiConfig#createCopy
40,method that is invoked when a configuration registry is cloned.
40,"Configuration should be set on a context before that context is used,"
40,and not changed later. Some configuration classes may be thread-safe but most are
40,"not. Configuration objects should be implemented so that they are cheap to copy,"
40,for example the base ones use copy-on-write collections.
40,"Many of Jdbi’s core features, for example argument or mapper"
40,"registries, are simply implementations of"
40,JdbiConfig that store the
40,registered mappings for later use during query execution.
40,public class ExampleConfig implements JdbiConfig<ExampleConfig> {
40,private String color;
40,private int number;
40,public ExampleConfig() {
40,"color = ""purple"";"
40,number = 42;
40,private ExampleConfig(ExampleConfig other) {
40,this.color = other.color;
40,this.number = other.number;
40,public ExampleConfig setColor(String color) {
40,this.color = color;
40,return this;
40,public String getColor() {
40,return color;
40,public ExampleConfig setNumber(int number) {
40,this.number = number;
40,return this;
40,public int getNumber() {
40,return number;
40,@Override
40,public ExampleConfig createCopy() {
40,return new ExampleConfig(this);
40,19.5.1. Creating a custom JdbiConfig type
40,Create a public class that implements JdbiConfig.
40,"Add a public, no-argument constructor"
40,"Add a private, copy constructor."
40,Implement createCopy() to call the copy constructor.
40,"Add config properties, and provide sane defaults for each property."
40,Ensure that all config properties get copied to the new instance in the copy
40,constructor.
40,Override setConfig(ConfigRegistry) if your config class wants to be able to
40,use other config classes in the registry. E.g. RowMappers registry delegates
40,"to ColumnMappers registry, if it does not have a mapper registered for a given"
40,type.
40,Use that configuration object from other classes that are interested in it.
40,"e.g. BeanMapper, FieldMapper, and ConstructorMapper all use the"
40,ReflectionMappers config class to keep common configuration.
40,19.6. JdbiPlugin
40,JdbiPlugin can be used to bundle bulk configuration.
40,"Plugins may be installed explicitly via Jdbi.installPlugin(JdbiPlugin), or"
40,may be installed automagically from the classpath using the ServiceLoader mechanism
40,via installPlugins().
40,Jars may provide a file in META-INF/services/org.jdbi.v3.core.spi.JdbiPlugin
40,containing the fully qualified class name of your plugin.
40,"In general, Jdbi’s separate artifacts each provide a single relevant plugin (e.g. jdbi3-sqlite),"
40,and such modules will be auto-loadable. Modules that provide no (e.g. jdbi3-commons-text)
40,or multiple (e.g. jdbi3-core) plugins typically will not be.
40,The developers encourage you to install plugins explicitly.
40,Code with declared dependencies
40,on the module it uses is more robust to refactoring and provides useful data
40,for static analysis tools about what code is or is not used.
40,19.7. StatementContext
40,The StatementContext class holds the state for all statements. It is a stateful object that should not be shared between threads. It may be used by different threads as long as this happens sequentially.
40,"The statement context object is passed into most user extension points, e.g. RowMapper, ColumnMapper, or CollectorFactory."
40,The StatementContext is not intended to be extended and extension points should use it to get access to other parts of the system (especially the config registry). They rarely need to make changes to it.
40,19.8. TemplateEngine
40,Jdbi uses a TemplateEngine
40,implementation to render templates into SQL. Template engines take a SQL
40,"template string and the StatementContext as input, and produce a parseable"
40,SQL string as output.
40,"Out of the box, Jdbi is configured to use DefinedAttributeTemplateEngine, which replaces angle-bracked tokens like <name> in SQL statements with"
40,the string value of the named attribute:
40,"String tableName = ""customers"";"
40,Class<?> entityClass = Customer.class;
40,"handle.createQuery(""SELECT <columns> FROM <table>"")"
40,".define(""table"", ""customers"")"
40,".defineList(""columns"", ""id"", ""name"")"
40,.mapToMap()
40,".list() // => ""SELECT id, name FROM customers"""
40,The defineList method defines a list of elements as the comma-separated
40,"splice of String values of the individual elements. In the above example,"
40,"the columns attribute is defined as ""id, name""."
40,Jdbi supports custom template engines through the TemplateEngine
40,"interface. By calling Configurable#setTemplateEngine() method on the Jdbi, Handle, or any SQL statement like Update or Query, the template engine used to render SQL can be changed:"
40,"TemplateEngine templateEngine = (template, ctx) -> {"
40,...
40,jdbi.setTemplateEngine(templateEngine);
40,"Jdbi also provides StringTemplateEngine,"
40,which renders templates using the StringTemplate library. See StringTemplate 4.
40,"Template engines interact with the SQL template caching. A template engine may implement the TemplateEngine#parse() methods to create an intermediary representation of a template that can be used to render a template faster. The output of this method will be cached. Depending on the implementation, defined attributes may be resolved at parse time and not at render time."
40,The most commonly used template engines (DefinedAttributeTemplateEngine and StringTemplateEngine do not support caching).
40,19.9. SqlParser
40,"After the SQL template has been rendered, Jdbi uses a SqlParser to parse out any named parameters from the SQL statement. This Produces a ParsedSql object, which"
40,contains all the information Jdbi needs to bind parameters and execute the SQL statement.
40,"Out of the box, Jdbi is configured to use ColonPrefixSqlParser, which"
40,"recognizes colon-prefixed named parameters, e.g. :name."
40,"handle.createUpdate(""INSERT INTO characters (id, name) VALUES (:id, :name)"")"
40,".bind(""id"", 1)"
40,".bind(""name"", ""Dolores Abernathy"")"
40,.execute();
40,"Jdbi also provides HashPrefixSqlParser, which recognizes hash-prefixed"
40,"parameters, e.g. #hashtag. Other custom parsers implement the SqlParser"
40,interface.
40,"By calling Configurable#setSqlParser() method on the Jdbi, Handle, or any SQL statement like Update or Query, the parser used to define named arguments can be changed:"
40,handle.setSqlParser(new HashPrefixSqlParser())
40,".createUpdate(""INSERT INTO characters (id, name) VALUES (#id, #name)"")"
40,".bind(""id"", 2)"
40,".bind(""name"", ""Teddy Flood"")"
40,.execute();
40,"The default parsers recognize any Java identifier character and the dot (.) as a valid characters in a parameter or attribute name. Even some strange cases like emoji are allowed, although the Jdbi authors encourage appropriate discretion 🧐."
40,"The default parsers try to ignore parameter-like constructions inside of string literals,"
40,since JDBC drivers wouldn’t let you bind parameters there anyway.
40,19.10. SqlLogger
40,The SqlLogger interface
40,"is called before and after executing each statement,"
40,"and given the current StatementContext, to log any relevant information desired:"
40,"mainly the query in various compilation stages,"
40,"attributes and bindings, and important timestamps."
40,There’s a simple Slf4JSqlLogger
40,implementation that logs all executed statements for debugging.
40,19.11. ResultProducer
40,A ResultProducer takes a lazily supplied PreparedStatement and
40,produces a result.
40,"The most common producer path, execute(),"
40,retrieves the ResultSet over the query results and then uses a
40,ResultSetScanner or higher level mapper to produce results.
40,"An example alternate use is to just return the number of rows modified,"
40,as in an UPDATE or INSERT statement:
40,public static ResultProducer<Integer> returningUpdateCount() {
40,"return (statementSupplier, ctx) -> {"
40,try {
40,return statementSupplier.get().getUpdateCount();
40,} finally {
40,ctx.close();
40,"If you acquire the lazy statement, you are responsible for ensuring"
40,that the context is closed eventually to release database resources.
40,Most users will not need to implement the ResultProducer interface.
40,19.12. Jdbi SQL object code generator
40,Jdbi includes an experimental SqlObject code generator.
40,If you
40,include the jdbi3-generator artifact as an annotation processor and
40,"annotate your SqlObject definitions with @GenerateSqlObject, the"
40,generator will produce an implementing class and avoids using
40,Java proxy instances.
40,This may be useful for graal-native compilation.
40,19.13. HandleCallbackDecorator
40,Jdbi allows specifying a decorator that can be applied to all callbacks passed to
40,"useHandle,"
40,"withHandle,"
40,"useTransaction,"
40,or inTransaction.
40,This is for use-cases where you need to perform an action on all callbacks used in
40,"Jdbi. For instance, you might want to have global retries whether a transaction is used"
40,"or not. Jdbi already provides retry handlers for transactions, but you might want to retry auto-commit"
40,(non-transaction) queries as well. E.g.
40,public class ExampleJdbiPlugin
40,"implements JdbiPlugin, HandleCallbackDecorator"
40,@Override
40,public void customizeJdbi(Jdbi jdbi)
40,jdbi.setHandleCallbackDecorator(this);
40,@Override
40,"public <R, X extends Exception> HandleCallback<R, X> decorate(HandleCallback<R, X> callback)"
40,return handle -> {
40,try {
40,if (handle.getConnection().getAutoCommit()) {
40,// do retries for auto-commit
40,"return withRetry(handle, callback);"
40,catch (SQLException e) {
40,throw new ConnectionException(e);
40,// all others get standard behavior
40,return callback.withHandle(handle);
40,"private <R, X extends Exception> R withRetry(Handle handle, HandleCallback<R, X> callback)"
40,throws X
40,while (true) {
40,try {
40,return callback.withHandle(handle);
40,catch (Exception last) {
40,// custom auto-commit retry behavior goes here
40,19.13.1. Nesting Callbacks with managed Handles and Transactions
40,"Jdbi has a number of methods that provide a managed handle or use a managed handle, e.g. Jdbi#withHandle(), Jdbi#useHandle(), Jdbi#inTransaction(), Jdbi#useTransaction(), Handle#inTransaction(), HandleuseTransaction(), SqlObject#withHandle(), SqlObject#useHandle() and methods that use the SQL Object Transactions annotations."
40,"All of these methods use the same underlying mechanism to reuse the handle object if one of these methods is called from another within a callback. In this case, there is no new handle object or transaction created but the same Handle object is passed into the nested callback:"
40,jdbi.useHandle(outerHandle -> {
40,jdbi.useTransaction(nestedHandle -> {
40,jdbi.useHandle(innerHandle -> {
40,"// all three variables (outerHandle, nestedHandle and innerHandle)"
40,// refer to the same object
40,assertThat(innerHandle).isSame(nestedHandle);
40,assertThat(innerHandle).isSame(outerHandleHandle);
40,});
40,});
40,"Nesting callback methods is possible but should be avoided. Reusing the Handle object in nested calls is based on the thread identity, executing methods asynchronously (on another thread) may lead to surprising results, especially when using a thread pool that may execute code on the main thread (where the nested handle would be reused) or from another thread (where a new handle object would be created). The experimental Kotlin Coroutine support manages handle objects transparently between threads for co-routines."
40,20. Appendix
40,20.1. Best Practices
40,Test your SQL Objects (DAOs) against real databases when possible.
40,Jdbi tries to be defensive and fail eagerly when you hold it wrong.
40,Use the -parameters compiler flag to avoid all those
40,"@Bind(""foo"") String foo redundant qualifiers in SQL Object method"
40,parameters.
40,See Compiling with Parameter Names.
40,Use a profiler! The true root cause of performance problems can often be a
40,"surprise. Measure first, then tune for performance. And then measure again"
40,to be sure it made a difference.
40,Don’t forget to bring a towel!
40,20.2. API Reference
40,Javadoc
40,jdbi3-kotlin
40,jdbi3-kotlin-sqlobject
40,20.3. Related Projects
40,Embedded Postgres
40,makes testing against a real database quick and easy.
40,dropwizard-jdbi3
40,provides integration with DropWizard.
40,metrics-jdbi3
40,instruments using DropWizard-Metrics to emit statement timing statistics.
40,"Do you know of a project related to Jdbi? Send us an issue, and we’ll add a link"
40,here!
40,20.4. Upgrading from v2 to v3
40,Already using Jdbi v2?
40,Here’s a quick summary of differences to help you upgrade:
40,General:
40,Maven artifacts renamed and split out:
40,Old: org.jdbi:jdbi
40,"New: org.jdbi:jdbi3-core, org.jdbi:jdbi3-sqlobject, etc."
40,Root package renamed: org.skife.jdbi.v2 → org.jdbi.v3
40,Core API:
40,"DBI, IDBI → Jdbi"
40,Instantiate with Jdbi.create() factory methods instead of constructors.
40,DBIException → JdbiException
40,"Handle.select(String, …​) now returns a Query for further method"
40,"chaining, instead of a List<Map<String, Object>>. Call"
40,"Handle.select(sql, …​).mapToMap().list() for the same effect as v2."
40,Handle.insert() and Handle.update() have been coalesced into
40,Handle.execute().
40,ArgumentFactory is no longer generic.
40,AbstractArgumentFactory is a generic implementation of ArgumentFactory
40,for factories that handle a single argument type.
40,Argument and mapper factories now operate in terms of
40,java.lang.reflect.Type instead of java.lang.Class. This allows Jdbi to
40,handle arguments and mappers for generic types.
40,Argument and mapper factories now have a single build() method that returns
40,"an Optional, instead of separate accepts() and build() methods."
40,ResultSetMapper → RowMapper. The row index parameter was also removed
40,from RowMapper--the current row number can be retrieved directly from the
40,ResultSet.
40,ResultColumnMapper → ColumnMapper
40,ResultSetMapperFactory → RowMapperFactory
40,ResultColumnMapperFactory → ColumnMapperFactory
40,"Query no longer maps to Map<String, Object> by default. Call"
40,"Query.mapToMap(), .mapToBean(type), .map(mapper) or .mapTo(type)."
40,ResultBearing<T> was refactored into ResultBearing (no generic parameter)
40,and ResultIterable<T>. Call .mapTo(type) to get a ResultIterable<T>.
40,TransactionConsumer and TransactionCallback only take a Handle now—​the
40,TransactionStatus argument is removed. Just rollback the handle now.
40,TransactionStatus class removed.
40,CallbackFailedException class removed. The functional interfaces like
40,"HandleConsumer, HandleCallback, TransactionCallback, etc. can now throw"
40,any exception type. Methods like Jdbi.inTransaction that take these
40,callbacks use exception transparency to throw only the exception thrown by
40,"the callback. If your callback throws no checked exceptions, you don’t need"
40,a try/catch block.
40,StatementLocator interface removed from core. All core statements expect to
40,"receive the actual SQL string now. A similar concept, SqlLocator was added"
40,but is specific to SQL Object.
40,"StatementRewriter refactored into TemplateEngine, and SqlParser."
40,StringTemplate no longer required to process <name>-style tokens in SQL.
40,Custom SqlParser implementations must now provide a way to transform raw
40,parameter names to names that will be properly parsed out as named params.
40,SQL Object API:
40,SQL Object support is not installed by default. It must be added as a
40,"separate dependency, and the plugin installed into the Jdbi object:"
40,Jdbi jdbi = Jdbi.create(...);
40,jdbi.installPlugin(new SqlObjectPlugin());
40,SQL Object types in v3 must be public interfaces—​no classes. Method return
40,types must likewise be public. This is due to SQL Object implementation
40,"switching from CGLIB to java.lang.reflect.Proxy, which only supports"
40,interfaces.
40,GetHandle → SqlObject
40,"SqlLocator replaces StatementLocator, and only applies to SQL Objects."
40,@RegisterMapper divided into @RegisterRowMapper and
40,@RegisterColumnMapper.
40,"@Bind annotations on SQL Object method parameters can be made optional,"
40,by compiling your code with the -parameters compiler flag enabled.
40,"@BindIn → @BindList, and no longer requires StringTemplate"
40,On-demand SQL objects don’t play well with methods that return Iterable
40,or FluentIterable. On-demand objects strictly close the handle after each
40,"method call, and no longer ""hold the door open"" for you to finish consuming"
40,the interable as they did in v2. This forecloses a major source of connection
40,leaks.
40,"SQL Objects are no longer closeable — they are either on-demand, or their"
40,lifecycle is tied to the lifecycle of the Handle they are attached to.
40,@BindAnnotation meta-annotation removed. Use
40,@SqlStatementCustomizingAnnotation instead.
40,@SingleValueResult → @SingleValue. The annotation may be used for method
40,"return types, or on @SqlBatch parameters."
40,Version 3.45.2-SNAPSHOT
40,Last updated 03/14/2024 10:19 -0700
41,Oracle Performance Tuning - Step-by-step Guide & Tools for 2024
41,Menu
41,Close
41,Search
41,Search
41,VPN
41,By Use
41,Best VPNs of 2024
41,Business VPN
41,Netflix
41,Kodi
41,Torrenting
41,Hulu
41,Sky Go
41,Gaming
41,BBC iPlayer
41,Tor
41,By OS/Device
41,Mac
41,Windows
41,Linux
41,Windows 10
41,Firestick
41,iPhone and iPad
41,Android
41,Windows Phone
41,DD-WRT Routers
41,By Country
41,China
41,Japan
41,Canada
41,Australia
41,Germany
41,France
41,UAE & Dubai
41,Guides
41,Fastest VPNs
41,Cheapest VPNs
41,Free VPNs
41,How to access the deep web
41,Is torrenting safe and legal?
41,Build your own VPN
41,Facebook privacy and security
41,How to encrypt email
41,How to stay anonymous online
41,How we test VPNs
41,See all
41,Reviews
41,NordVPN
41,Surfshark
41,ExpressVPN
41,IPVanish
41,PrivateVPN
41,StrongVPN
41,CyberGhost
41,PureVPN
41,See all
41,Proxies
41,Reviews
41,Antivirus
41,Reviews
41,Norton Antivirus
41,TotalAV
41,Intego VirusBarrier X9
41,McAfee
41,VIPRE
41,Panda Security
41,Eset
41,See all
41,By OS/Device
41,Mac
41,Windows
41,Guides
41,Best Antivirus in 2024
41,Best Free Firewalls
41,Free Antivirus Software
41,Malware Statistics & Facts
41,See all
41,Compare providers
41,McAfee vs Kaspersky
41,Norton vs Kaspersky
41,McAfee vs Norton
41,Online backup
41,Streaming
41,Kodi
41,Plex
41,Sports Streaming
41,TV Streaming
41,IPTV
41,Blog
41,VPN & Privacy
41,Cloud and Online Backup
41,Information Security
41,More Comparisons
41,Password Managers
41,Identity Theft Protection
41,Usenet
41,Privacy & Security Tools
41,Internet Providers
41,Parental Control Software
41,Net Admin Tools
41,Data Privacy Management
41,Data Recovery Software
41,Crypto
41,Utilities
41,About Us
41,About Our Company
41,Press
41,Software Testing Methodology
41,Editorial Process
41,Join us
41,Contact
41,Net AdminOracle Performance Tuning Guide & Tools
41,We are funded by our readers and may receive a commission when you buy using links on our site.
41,Oracle Performance Tuning Guide & Tools
41,We take a closer look at Oracle performance tuning in this guide. We also look at some of the best database tuning tools.
41,Tim Keary
41,Network Security amd Administration Expert
41,"Updated: April 28, 2023"
41,Have you noticed your database running slower than usual? Then you might need a touch of performance tuning. Performance tuning in Oracle databases eliminates bottlenecks that make applications unresponsive and increases the load your database can handle.
41,What is Oracle Performance Tuning?
41,Performance tuning is the process of administering a database to improve performance. Performance tuning in Oracle databases includes optimizing SQL statements and query execution plans so that the requests can be completed more efficiently.
41,"The organization of a database the SQL statements used to access that data, determine the level of resources needed to respond to queries when an application communicates with the database."
41,Problems like poorly optimized SQL statements force the database to work much harder to retrieve information (resulting in more system resources being used). The more system resources that are used the greater the chance it will affect the experience of users on connected applications.
41,"In an enterprise, users will report a slow application to a database administrator who will then attempt to pinpoint the root cause of the problem. The administrator analyzes statement code and searches for database bottlenecks. The process is extensive, as the administrator has to diagnose the root cause of the problem before it can be addressed."
41,Monitoring Performance in Oracle Databases: Response Time and Throughput
41,When performance tuning an Oracle database there are two metrics that are useful to measure:
41,Response time – How long the database takes to complete a request.
41,System throughput – The number of processes completed in a period of time.
41,"High response time means that an application is providing a slow user experience. On the other hand, low system throughput means that the database only has the resources to manage a small number of tasks in a short time period. An administrator has to be able to know how they are trying to improve performance before tuning."
41,How you optimize an Oracle database comes down to your goals and the type of applications you are using. Many goals like having a fast response time or a high throughput are contradictory.
41,"Tuning for fast response times may speed up individual queries made by users but sacrifice other tasks in the workload. In contrast, achieving a high throughput would aim to optimize the performance of the entire workload to support a larger output of transactions per second (but not necessarily speed up individual queries)."
41,The type of application you’re using makes all the difference. If you’re using an online transaction process (OLTP) application then you would use throughput to measure performance. This is because of the high volume of transactions the application needs to manage.
41,"However, if you were using a decision support system (DSS) with users running queries on everything from a handful of records to thousands of records, then you would measure performance by response time (unless you were supporting lots of users running concurrent queries)!"
41,The Two Types of Tuning: Proactive Monitoring and Bottleneck Elimination
41,"Now that you know what performance tuning is, it’s important to look at the two main models of tuning:"
41,Proactive Monitoring
41,Bottleneck Elimination
41,Database administrators use these two models to manage performance issues and keep applications functioning at a high level.
41,Proactive Monitoring
41,"Proactive monitoring is the process of monitoring a database to discover and address performance issues early rather than simply reacting when there is a problem. With proactive monitoring, administrators will periodically review databases to identify the signs of performance degradation."
41,The idea behind proactive monitoring is to catch issues and inefficiencies before they develop into greater problems further down the line. Some common issues database administrators look out for include:
41,Database wait events – A high number of events can negatively affect database performance. Finding obstructive sessions and killing them can prevent performance degradation.
41,Load average – Monitoring the load average of a server will tell you if server resources are functioning as normal. A high load average can result in slow database performance.
41,Database sessions – Monitoring the number of active sessions can stop you from reaching the maximum (which will prevent you from being able to open new sessions).
41,"However, monitoring proactively does carry some risk. Any changes an administrator makes can result in a decrease in performance for the database. Administrators can mitigate the risks by being cautious before making new changes."
41,Bottleneck Elimination
41,Bottlenecks are one of the most common causes of poor performance. Bottlenecks block requests from reaching the destination and increase the response time of applications. Bottlenecks can be caused by a range of factors from badly coded SQL statements and high resource usage.
41,"Bottleneck elimination is more of a reactive process than proactive monitoring. An administrator identifies a bottleneck and then finds a way to fix it. Fixing a bottleneck is a complex process and depends on what the root cause is (and whether it is internal or external). Recoding SQL statements is one solution for fixing internal bottlenecks, which should be addressed first."
41,"Once internal bottlenecks have been resolved the administrator can start to look at external factors like CPU and storage performance that could be causing the problem. An administrator can choose between making changes to the application (or how it is used), Oracle, or the hardware configuration of the host."
41,How to Performance Tune
41,"Performance tuning an Oracle database is a very complex subject because there are so many different factors that can affect database performance. To keep things simple, we’re going to look at some basic ways you can optimize performance."
41,1. Identify High-Cost Queries
41,The first step to tuning SQL code is to identify high-cost queries that consume excessive resources. Rather than optimizing every line of code it is more efficient to focus on the most widely-used SQL statements and have the largest database / I/O footprint.
41,"One easy way to identify high-cost queries is to use Oracle database monitoring tools (we look at some of these platforms in more detail further below). One useful tool is Oracle SQL Analyze, which can identify resource-intensive SQL statements. Tuning these statements will give you the greatest return on your time investment."
41,2. Minimize the workload (Use Indexes!)
41,You can make the same query in many different ways so it is advantageous to write code that minimizes the workload as much as possible. If you only need a snapshot of data from a table it makes no sense processing thousands of rows you don’t need (all you’re doing is wasting system resources!) A full table scan takes up more database resources and I/O.
41,To eliminate the stress of sustaining a large workload you can use indexes to access small sets of rows rather than processing the entire database at once. Use indexes in those scenarios where a column is regularly queried.
41,3. Use Stateful Connections with Applications
41,Sometimes the cause of poor performance doesn’t come from code but because the connection keeps dropping between the application and the database. If your application isn’t configured correctly then it could form a connect to the database to access a table and then drop the connection once it has the information it needs.
41,"Dropping the connection after accessing the table is terrible for performance. Instead, try to keep a stateful connection so that the application stays connected to the database at all times. Maintaining the connection will stop system resources from being wasted each time the application interacts with the database."
41,4. Collect and Store Optimizer Statistics
41,Optimizer statistics are data that describe a database and its objects. These statistics are used by the database to choose the best execution plan for SQL statements. Regularly collecting and storing optimizer statistics on database objects is essential for maintaining efficiency.
41,"Collecting optimizer statistics makes sure that the database has accurate information on table contents. If the data is inaccurate then the database can choose a poor execution plan, which will affect the end-user experience. Oracle databases can automatically collect optimizer statistics or you can do so manually with the DBMS_STATS package."
41,Tools for Monitoring Oracle DB Query Performance
41,Using a software agent to monitor SQL query performance is the most effective way to manage query performance. Database performance monitoring tools can help to identify poorly performing SQL code and enable you to pinpoint the root cause of performance issues. The user can view query performance on a dashboard so they don’t need to search for information manually.
41,SolarWinds Database Performance Analyzer (FREE TRIAL)
41,SolarWinds Database Performance Analyzer is a tool that can monitor the performance of SQL queries in real-time. You can view a table of the Top SQL Statements in your database to see the highest impact SQL statements. Viewing the top statements allows you to focus your remediation efforts on those statements that have the greatest impact on performance.
41,Why do we recommend it?
41,The SolarWinds Database Performance Analyzer provides cloud-based tracking of activities and issues on Oracle databases on your own servers and on cloud platforms. The system uses AI to predict when performance issues are gathering and issue warnings for action before problems become noticeable to users.
41,Who is it recommended for?
41,"This SaaS package is able to manage and monitor a list of DBMS’s, not just Oracle. It can also watch over database instances from SQL Server, MySQL, MariaDB, PostgreSQL, and MongoDB as well as other relational and NoSQL systems. As it only monitors databases and their hosts, the tool is aimed at large businesses."
41,Pros:
41,Highly intuitive DB management system tailored for medium to large size database implementations
41,"Monitors in real-time, offering a number of alert and notification options that can integrate into popular helpdesk solutions"
41,"Threshold monitoring helps keep teams proactive, and fix issues before they impact performance"
41,Dashboards are highly customizable and can be tailored to individuals or teams
41,Built-in query analysis helps DBAs build more efficient queries
41,Leverages machine learning to identify performance bottlenecks
41,Cons:
41,Would prefer a longer trial period
41,"There is also a blocking analysis feature, which allows you to view the blocking hierarchy of the database and to view the total wait caused. Being able to see those queries that are obscuring the routes of other queries tells you where to make changes to improve performance. You can download a free trial."
41,SolarWinds Database Performance Analyzer
41,Start 14-day FREE Trial
41,dbForge Studio for Oracle (Oracle SQL Profiler)
41,"dbForge Studio for Oracle is an integrated development environment (IDE) that comes with Oracle SQL Profiler. With the profiler, you can identify queries with the highest duration. You can also view query execution plans and sessions statistics for additional information. Any changes you make will be saved so you can revert to earlier versions if you make a mistake."
41,Why do we recommend it?
41,dbForge Studio for Oracle is an on-device package that presents a view of database objects and allows queries to be tested and supports the development of code to run on your Oracle instances. This system can also be used to test database structures rather than for live monitoring.
41,"In the Session Statistics tab, you can compare the results of new queries against older versions. Differences are highlighted in red and green so you can easily tell the performance impact. Session stats you can monitor include, CPU used by this session and sorts (rows). The Session Statistics tab allows you to make sure that your changes are actually improving performance!"
41,Who is it recommended for?
41,"This tool runs on Windows, macOS, and Linux. It isn’t a networked package but it can access databases across a network. This gives the tool limited usage within a business because you have to install the software on each user’s workstation. The system is useful for DBAs and developers. It doesn’t provide database monitoring."
41,Pros:
41,Offers additional features and functionality beyond Management Studio without unnecessary tools and options
41,Excellent overview into the health of multiple SQL server databases
41,Can rewrite queries and profile data from directly inside the tool
41,Cons:
41,Only runs on Windows
41,Requires Microsoft SQL Server Management Studio
41,Tune with a Goal in Mind!
41,Having clear expectations with a specific goal in mind is critical for maintaining your database and tuning the system. Performance tuning is easier when you know what you’re trying to achieve.
41,"For example, if you want to minimize the response time of an application so that it completes queries in 1-3 seconds then you can take action to tune for that scenario. That means diagnosing bottlenecks and performance issues that slow response time down."
41,"Being able to refer back to goals will also help when you’re using a database analyzer. You’ll be able to monitor disk, CPU, and network usage to identify how performance is affected. The better you understand your goals, the better you know how to tune your resources."
41,Oracle performance tuning FAQs
41,"How can I improve my Oracle query performance?Indexing improves data retrieval speed. However, don’t over-index because these indexes could conflict and actually slow down results being returned. Partitioning tables improves data access speed because it reduces the number of records that need to be scanned in order to return the results for a query. Partitioning takes all indexes currently on the table and duplicates them automatically so that they are automatically applied to each partition."
41,"What is SQL performance tuning?SQL performance tuning is the process of ordering joins in an SQL query so that they extract records at maximum speed. There are a number of methods that can be used to improve SQL queries, such as removing outer joins and ordering lines in the WHERE clause so that as many records as possible are filtered out from the first table accessed by the query, thus reducing the amount of sorting that needs to be undertaken in the other tables accessed by the query."
41,"What is Oracle bottleneck?A bottleneck is a point in the process of access to or response from a database that has limited capacity and therefore slows disown access to results because only a limited number of queries can be served. This is different from resource locks, which occur when one process holds a resource and will not release it until another resource is available."
41,What's in this article?What is Oracle Performance Tuning?Monitoring Performance in Oracle Databases: Response Time and ThroughputThe Two Types of Tuning: Proactive Monitoring and Bottleneck EliminationHow to Performance TuneTools for Monitoring Oracle DB Query PerformanceTune with a Goal in Mind!Oracle performance tuning FAQs
41,Comments
41,Leave a Reply Cancel replyCommentName
41,Email
41,This site uses Akismet to reduce spam. Learn how your comment data is processed.
41,Search
41,Search
41,Twitter icon
41,Home
41,Blog
41,Our Authors
41,Privacy policy
41,Cookies Policy
41,Terms of use
41,Disclosure
41,About Comparitech
41,Contact Us
41,Accessibility
41,© 2024 Comparitech Limited. All rights reserved.
41,"Comparitech.com is owned and operated by Comparitech Limited, a registered company in England and Wales (Company No. 09962280), Suite 3 Falcon Court Business Centre, College Road, Maidstone, Kent, ME15 6TF, United Kingdom. Telephone +44(0)333 577 0163"
41,SolarWinds Top 5 Essential IT Tools
41,Manage and Monitor Your Network in One Simple Bundle
41,Help desk ticketing and asset management software
41,Remote support and systems management solution
41,Network configuration and automation software
41,Safe file transfer management solution
41,Network management and troubleshooting software
41,DOWNLOAD FREE TRIAL
41,Fully functional for 14 days
41,Comparitech uses cookies. More info.
41,Close
42,Blogs on MySQL and PostgreSQL Database Optimization | OtterTune
42,"Product TourBlogPlans and PricingContactResourcesLoginGet StartedProduct TourBlogBlogBlogBlogSearch blog hereJan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Jan 4, 2024Databases in 2023: A Year in ReviewFrom the rise of vector databases to SQL:2023 to MariaDB troubles and the FAA outage, 2023 was an exciting year in database history.Dec 18, 2023Automate OtterTune Database Knob Configurations for Terraform with GitHub ActionsThis guide shows you how to use GitHub Actions to stop Terraform from overwriting your OtterTune-powered knob recommendations.Dec 4, 2023OtterTune November 2023 Product UpdateSince releasing the much-anticipated OtterTune v2.0 in April of this year, we’ve been working hard to further enhance our core feature functionality and polish existing workflows while delivering some exciting new features.Aug 30, 2023Query best practices: When should you use the IN instead of the OR operator?Query performance often varies depending on how you write queries. For PostgreSQL, what provides better performance? The IN or OR operator? Jun 7, 2023Yes, PostgreSQL has problems, but we’re sticking with it!While we dislike PostgreSQL's multi-version concurrency control (MVCC), there are ways to address version copying, table bloat, index maintenance and vacuum management. Apr 26, 2023The part of PostgreSQL we hate the mostAs much as we love PostgreSQL at OtterTune, its multi-version concurrency control (MVCC) is a big pain. We go over why.Apr 5, 2023OtterTune v2.0: Fresh look and killah featuresWe provide the same AI-powered automated knob tuning but now expand our recommendations to indexes and cloud settings. Jan 30, 2023How Amazon RDS replication works and why the FAA’s database problem won’t happen in AWSOn 1/11/23, all flights in the US were grounded because of a FAA NOTAM system outage tied to a corrupt database file. Here's why that wouldn't happen if the FAA was using Amazon RDS.Dec 31, 2022Databases in 2022: A Year in ReviewAnother year has gone by, and I’m still alive. As such, it is an excellent time to reflect on what happened in the world of databases last year.Dec 14, 2022OtterTune v1.5What's new in OtterTune v1.5? User approval of configuration changes; health checks for indexes, configuration, database tables, and autovacuum; and new performance insights.Dec 7, 2022Postgres is on the risePostgres is moving on up! But why? In this blog post, we pull together some of the most important reasons.Nov 7, 2022We got our SOC 2 Type II report (and you can, too)We’re excited to announce that we have completed our first SOC 2 Type II audit process with zero exceptions.Oct 12, 2022Cloud spending may be heading for a fallCloud spending is projected to flatten or be reduced. As budgets tighten, what can you do to control costs?Sep 19, 2022OtterTune intern showcase 2022From PostgreSQL autovacuum monitoring to database health check improvements, our interns produced great work in 2022.Aug 30, 2022Fixing slow PostgreSQL queries: How to make a smart optimizer more stupidYou've got a slow-running PostgreSQL query, so you run ANALYZE, but what if ANALYZE can't fix it? Aug 11, 2022OtterTune August ’22 product update: A better tomorrowWe’re pleased to announce OtterTune’s new health checks for database tables and indexes. This product update also includes new ways to interact with and collaborate with your team.Jul 20, 2022Get granular without the grunt work: OtterTune Table Health Checks let you diagnose issues automaticallyOtterTune’s new Table Health Checks automatically examine vital statistics and health checks to diagnose and resolve common database problems within tables.Jun 16, 2022Introducing OtterTune database quick-enableWith OtterTune's Database Quick Enable feature, you get a dashboard that makes OtterTune setup easier by automatically pulling your Amazon RDS and Amazon Aurora database fleet information using the AWS IAM role you provide to OtterTune.Jun 8, 2022Databases! Pandemic! Funding! A look back and a look forwardThe start-up game is rough. We thought it would be helpful to others if we reflected on OtterTune’s journey from an academic research project to a thriving start-up.May 19, 2022Run ANALYZE. Run ANALYZE. Run ANALYZE.We’ve said it before, we’ll say it again: one of the first things you should do, if not the first, when trying to solve a slow-running PostgreSQL query problem is run ANALYZEApr 20, 2022New deployment methods for the OtterTune AgentThe OtterTune agent is an open-source Python program that collects monitoring data from your database. We support installation via CloudFormation, Dock, Kubernetes Manifest and Helm Chart.Mar 30, 2022Keeping your database “Otterly” healthyOur Database Health Checks automatically detect and inform you of possible issues in your database in real-time and provide recommendations to fix what’s going onMar 16, 2022OtterTune March ’22 product update: Protect ya neckWe introduced new features to make sure your database is running correctly and provide insights about how applications are using them. We also added the ability for you to have more control over its configuration tuning process, and make it easier to integrate OtterTune in your production environment.Mar 9, 2022Ten database commandmentsAt OtterTune, we take the database hustle seriously. In memory of Biggie, we present our 10 street rules for databases.Feb 11, 2022Databases, machine learning and Parisian riotsFor Societe Generale (SG), a large French bank, we deployed a custom version of OtterTune for Oracle and saw 45% improvement vs. DBA-selected knobs.Jan 28, 2022Video: OtterTune “Human-in-the-Loop” modeThis short video goes over how to activate human-in-the-loop controls, how to review recommendations, applying and skipping recommendations and controlling acceptable value ranges for configuration knobs.Jan 7, 2022Which PostgreSQL configuration settings does OtterTune optimize?OtterTune optimizes each of 100+ PostgreSQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Jan 7, 2022Which MySQL configuration settings does OtterTune optimize?OtterTune optimizes each of 150+ MySQL configuration settings to support the database’s specific workload. This list goes over all the knobs OtterTune tunes.Dec 28, 2021Databases in 2021: A Year in ReviewIt was a wild year for the database industry, with newcomers overtaking the old guard, vendors fighting over benchmark numbers, and eye-popping funding rounds. It’s worth reflecting and taking stock as we move into 2022Dec 17, 2021Introducing human-in-the-loop control over automatic database tuningAutomated database optimization is great, but we realize many want to review recommendations. Ask and you shall receive! Our human-in-the-loop feature gives you more control.Dec 9, 2021Query latency vs. throughput as a database tuning objectiveThroughput is generally not a good target objective for applications that have daily workload patterns because it varies with the time of day. It’s dependent on the application wheras query latency is less dependent on the application and does not strongly vary with demand.Dec 3, 2021How to simplify Oracle-to-PostgreSQL migration with automatic database tuningAt $47,500 per CPU core, Oracle is pricey proposition against Postgres (which is effectively $0). OtterTune can help those looking to make the Oracle-to-PostgreSQL migration.Nov 11, 2021OtterTune intern showcase 2021From database checkup tools to detecting workload and other fluctuations to maximize tuning success, our interns produced such fantastic work that they're now full-time employees.Nov 4, 2021How OtterTune securely collects database metricsThe OtterTune Agent only pushes metrics and knob settings information out; it does not accept data in. It also does not look at any sensitive user data or queries.Oct 20, 2021Automatic database tuning tips: Handling restartsWhether you use OtterTune, or you tune by hand, these tips are helpful when it comes to the dreaded database restart.Oct 14, 2021OtterTune Oct 2021 product update: Bring da ruckusOur primary focus since the first release of OtterTune has been to make it easier to get started with OtterTune and put it to work auto-tuning your databases. Here’s what’s new in this latest update.Sep 30, 2021You are overpaying Jeff Bezos for your databases (and the things he does with that extra money)We have met AWS users spending hundreds of thousands per month on database services, some of which are spending up to 40% of their monthly AWS bill on databases. And a large portion of this database revenue comes from people overpaying for their unoptimized RDS instancesSep 21, 2021Benchmark: Using machine learning to optimize Amazon RDS MySQL performanceAmazon’s default settings use general-purpose values for the most common workloads. They are not particularly bad for any application’s workload. But of course, this means that they are also not optimal for your database’s specific workload.Sep 14, 2021How will machine learning transform database administration?Autonomous database tuning is a growing trend that database developers and administrators should be tracking.Sep 7, 2021External vs. internal automatic database tuning approachesIn the last 20 years, MySQL and PostgreSQL have increased the number of knobs that they expose to tune by 7x and 5x, respectively. What are the pros and cons of tuning those knobs externally vs. internally?Aug 18, 2021VLDB 2021 video: Applying machine learning-based database tuning in productionAutonomous database operation and tuning, driven by machine learning (ML), is becoming more prevalent. In this VLDB talk, I presented learnings that help DBMS users better understand how to benefit from ML more productively and safely.Aug 11, 2021OtterTune at VLDB 2021The OtterTune team will be participating in full force at VLDB 2021 with a research paper presentation, workshop keynote, award talk, and discussion panel.Jul 28, 2021Seven years of OtterTune R&D: Part 2 – Learnings in the wildOtterTune started out in the academic world. We learned a lot to improve the product from our first real-world deployments.Jul 19, 2021Seven years of OtterTune R&D: Part 1 – The lab yearsOtterTune's roots go back to 2014. We've run over 100,000 trials to build our training corpus and even challenged conference attendees to try and best OtterTune.Jul 7, 2021How to prevent machine learning from wrecking your database OtterTune's algorithms make decisions based on training data, and sometimes these decisions can be unpredictable because the algorithms do not interpret the world the same way humans do. OtterTune has safeguards in place to protect your database.Jun 11, 2021Benchmark: Using machine learning to optimize Amazon RDS PostgreSQL performanceFor Amazon RDS PostgreSQL, which runs better? Amazon's default settings, PGTune or OtterTune? The performance results and cost savings may surprise you.May 26, 2021OtterTune explained in 5 minutesGive us five minutes and we'll show you how OtterTune optimizes databases.May 12, 2021Let's do this!We’re happy to announce the first commercial release of OtterTune software-as-a-service (SaaS) and seed funding From Accel. Our mission is to make databases run better, with less effort, and at a lower cost.Subscribe to blog updates.Subscribe to blog updates.ProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy PolicyProductPlans and PricingFAQDocsProduct UpdatesCompanyAbout UsContactCareersMediaResourcesBlogPodcastsVideosPublicationsSlack CommunityRecord Label© 2024 OtterTuneTerms of usePrivacy Policy"
43,Performance optimization recommendations | Adobe Commerce
43,DocumentationCommerceImplementation Playbook
43,Performance optimization review
43,Last update: Fri Sep 01 2023 00:00:00 GMT+0000 (Coordinated Universal Time)
43,Topics:
43,Cloud
43,CREATED FOR:
43,Experienced
43,Admin
43,Developer
43,"Even as performance optimization can come from many aspects, there are some general recommendations that should be considered for most scenarios. General recommendations include configuration optimization for infrastructure elements, Adobe Commerce backend configuration, and architecture scalability planning."
43,TIP
43,See the Performance Best Practices Guide for more information about performance optimization.
43,Infrastructure
43,The following sections describe recommendations for infrastructure optimizations.
43,DNS lookups
43,DNS lookup is the process of finding which IP address that the domain name belongs to. The browser must wait until the DNS lookup is complete before it can download anything for each request. Reducing DNS lookups is important to improve overall page load times.
43,Content delivery network (CDN)
43,"Use a CDN to optimize asset downloading performance. Adobe Commerce uses Fastly. If you have an on-premises implementation of Adobe Commerce, you should also consider adding a CDN layer."
43,Web latency
43,The location of the datacenter affects the web latency for frontend users.
43,Network bandwidth
43,"Sufficient network bandwidth is one of the key requirements for data exchange between web nodes, databases, caching/session servers, and other services."
43,"Because Adobe Commerce effectively uses caching for high performance, your system can actively exchange data with caching servers like Redis. If Redis is installed on a remote server, you must provide a sufficient network channel between web nodes and the caching server to prevent bottlenecks on read/write operations."
43,Operating system (OS)
43,"Operating system configurations and optimizations are similar for Adobe Commerce when compared to other high-load web applications. As the number of concurrent connections handled by the server increases, the number of available sockets can become fully allocated."
43,CPU of web nodes
43,"One CPU core can serve around 2-4 Adobe Commerce requests without cache effectively. To determine how many web nodes/cores needed to process all incoming requests without putting them into queue, use the equation:"
43,N[Cores] = (N [Expected Requests] / 2) + N [Expected Cron Processes])
43,PHP-FPM settings
43,Optimizing these settings depends on the performance test results for different projects.
43,"ByteCode—To get maximum speed out of Adobe Commerce on PHP 7, you must activate the opcache module and configure it properly."
43,"APCU—Adobe recommends enabling the PHP APCu extension and configuring Composer to optimize for maximum performance. This extension caches file locations for opened files, which increases performance for Adobe Commerce server calls, including pages, Ajax calls, and endpoints."
43,Realpath_cacheconfiguration—Optimizing realpath_cache allows PHP processes to cache paths to files instead of looking them up each time a page loads.
43,Web server
43,Only slight reconfiguration is needed to use nginx as a web server. The nginx web server provides better performance and is easy to configure using the sample configuration file from Adobe Commerce (nginx.conf.sample).
43,Set up PHP-FPM with TCP properly
43,Enable HTTP/2 and Gzip
43,Optimize worker connections
43,Database
43,"This document does not provide in-depth MySQL tuning instructions because each store and environment is different, but Adobe can make general recommendations."
43,"The Adobe Commerce database (and any other database) is sensitive to the amount of memory available for storing data and indexes. To effectively use MySQL data indexation, the amount of memory available should be, at minimum, close to half the size of the data stored in the database."
43,Optimize the innodb_buffer_pool_instances setting to avoid issues with multiple threads attempting to access the same instance. The value of the max_connections parameter should correlate with the total number of PHP threads configured in the application server. Use the following formula to calculate the best value for innodb-thread-concurrency:
43,innodb-thread-concurrency = 2 * (NumCPUs+NumDisks)
43,Session caching
43,Session caching is a good candidate to configure for a separate instance of Redis. Memory configuration for this cache type should consider the site’s cart abandonment strategy and how long a session should expect to remain in the cache.
43,Redis should have enough memory allocated to hold all other caches in memory for optimal performance. Block cache is the key factor in determining the amount of memory to configure. Block cache grows relative to the number of pages on a site (number of SKU x number of store views).
43,Page caching
43,"Adobe highly recommends using Varnish for full page cache on your Adobe Commerce store. The PageCache module is still present in the codebase, but it should be used for development purposes only."
43,"Install Varnish on a separate server in front of the web tier. It should accept all incoming requests and provide cached page copies. To allow Varnish to work effectively with secured pages, an SSL termination proxy can be placed in front of Varnish. Nginx can be used for this purpose."
43,"While Varnish full page cache memory invalidation is effective, Adobe recommends allocating enough memory to Varnish to hold your most popular pages in memory."
43,Message queues
43,"The Message Queue Framework (MQF) is a system that allows a module to publish messages to queues. It also defines the consumers that receive the messages asynchronously. Adobe Commerce supports RabbitMQ as the messaging broker, which provides a scalable platform for sending and receiving messages."
43,Performance testing and monitoring
43,Performance testing before each production release is always recommended to get an estimation of the capability of your ecommerce platform. Keep monitoring after launch and have a scalability and backup plan for handling peak times.
43,NOTE
43,"Adobe Commerce on cloud infrastructure already applies the above infrastructure and architecture optimizations, except for the DNS lookup because it’s out of scope."
43,Search search-heading
43,"Elasticsearch (or OpenSearch) is required as of Adobe Commerce version 2.4, but it’s also a best practice to enable it for versions before 2.4."
43,Operating models
43,"Apart from the previously mentioned common infrastructure optimization recommendations, there are also approaches to enhance the performance for specific business modes and scales. This document does not provide in-depth tuning instructions for all of them because each scenario is different, but Adobe can provide a few high-level options for your reference."
43,Headless architecture
43,"There is a separate section dedicated to headless. In summary, it separates the storefront layer from the platform itself. It is still the same backend, but Adobe Commerce no longer processes requests directly and instead only supports custom storefronts through the GraphQL API."
43,Keep Adobe Commerce updated
43,"Adobe Commerce always has better performance when running the newest version. Even if it is not possible to keep Adobe Commerce up to date after each new version is released, it is still recommended to upgrade when Adobe Commerce introduces significant performance optimizations."
43,"For example, in 2020, Adobe released an optimization to the Redis layer, fixing many inefficiencies, connection issues, and unnecessary data transfer between Redis and Adobe Commerce. Overall performance between 2.3 and 2.4 is night and day and provided significant improvements in cart, checkout, concurrent users, just because of the Redis optimization."
43,Optimize data model
43,"Many problems originate from data, including bad data models, data that is not structured properly, and data that is missing an index."
43,"It looks fine if you’re testing a few connections, but after you deploy to production you might see serious performance degradation. It’s important that systems integrators know how to design a data model (especially for product attributes), avoid adding unnecessary attributes, and keep mandatory attributes that affect business logic (such as pricing, stock availability, and search)."
43,"For those attributes that do not affect business logic but must still be present on the storefront, combine them into a few attributes (for example, JSON format)."
43,"To optimize platform performance, if business logic is not required on the storefront from data or attributes taken from a PIM or an ERP, there is no need to add that attribute into Adobe Commerce."
43,Design for scalability
43,Scalability is important for businesses running campaigns and frequently facing peak traffic times. A scalable architecture and application design can increase resources during peak times and reduce them after it.
43,recommendation-more-help
43,754cbbf3-3a3c-4af3-b6ce-9d34390f3a60
44,10 Tips for MySQL Performance Tuning - YouTubeInfoPresseUrheberrechtKontaktCreatorWerbenEntwicklerImpressumVerträge hier kündigenNutzungsbedingungenDatenschutzRichtlinien & SicherheitWie funktioniert YouTube?Neue Funktionen testen© 2024 Google LLC
45,"Performance | GORM - The fantastic ORM library for Golang, aims to be developer friendly."
45,DocsGenCommunityAPIContribute
45,English
45,English
45,简体中文
45,Deutsch
45,bahasa Indonesia
45,日本語
45,Русский
45,한국어
45,हिन्दी
45,French
45,Italiano
45,Español
45,Performance
45,"GORM optimizes many things to improve the performance, the default performance should be good for most applications, but there are still some tips for how to improve it for your application."
45,"Disable Default TransactionGORM performs write (create/update/delete) operations inside a transaction to ensure data consistency, which is bad for performance, you can disable it during initialization"
45,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
45,"SkipDefaultTransaction: true,})"
45,Caches Prepared StatementCreates a prepared statement when executing any SQL and caches them to speed up future calls
45,"// Globally modedb, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
45,"PrepareStmt: true,})// Session modetx := db.Session(&Session{PrepareStmt: true})tx.First(&user, 1)tx.Find(&users)tx.Model(&user).Update(""Age"", 18)"
45,NOTE Also refer how to enable interpolateparams for MySQL to reduce roundtrip https://github.com/go-sql-driver/mysql#interpolateparams
45,"SQL Builder with PreparedStmtPrepared Statement works with RAW SQL also, for example:"
45,"db, err := gorm.Open(sqlite.Open(""gorm.db""), &gorm.Config{"
45,"PrepareStmt: true,})db.Raw(""select sum(age) from users where role = ?"", ""admin"").Scan(&age)"
45,"You can also use GORM API to prepare SQL with DryRun Mode, and execute it with prepared statement later, checkout Session Mode for details"
45,"Select FieldsBy default GORM select all fields when querying, you can use Select to specify fields you want"
45,"db.Select(""Name"", ""Age"").Find(&Users{})"
45,Or define a smaller API struct to use the smart select fields feature
45,type User struct {
45,uint
45,Name
45,string
45,Age
45,int
45,Gender string
45,// hundreds of fields}type APIUser struct {
45,uint
45,"Name string}// Select `id`, `name` automatically when querydb.Model(&User{}).Limit(10).Find(&APIUser{})// SELECT `id`, `name` FROM `users` LIMIT 10"
45,Iteration / FindInBatchesQuery and process records with iteration or in batches
45,"Index HintsIndex is used to speed up data search and SQL query performance. Index Hints gives the optimizer information about how to choose indexes during query processing, which gives the flexibility to choose a more efficient execution plan than the optimizer"
45,"import ""gorm.io/hints""db.Clauses(hints.UseIndex(""idx_user_name"")).Find(&User{})// SELECT * FROM `users` USE INDEX (`idx_user_name`)db.Clauses(hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForJoin()).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR JOIN (`idx_user_name`,`idx_user_id`)""db.Clauses("
45,"hints.ForceIndex(""idx_user_name"", ""idx_user_id"").ForOrderBy(),"
45,"hints.IgnoreIndex(""idx_user_name"").ForGroupBy(),).Find(&User{})// SELECT * FROM `users` FORCE INDEX FOR ORDER BY (`idx_user_name`,`idx_user_id`) IGNORE INDEX FOR GROUP BY (`idx_user_name`)"""
45,"Read/Write SplittingIncrease data throughput through read/write splitting, check out Database Resolver"
45,Last updated: 2024-03-15
45,PrevNext
45,Platinum Sponsors
45,Become a Sponsor!
45,Gold Sponsors
45,Become a Sponsor!
45,Platinum Sponsors
45,Become a Sponsor!
45,Gold Sponsors
45,Become a Sponsor!
45,Contents
45,Disable Default TransactionCaches Prepared StatementSQL Builder with PreparedStmtSelect FieldsIteration / FindInBatchesIndex HintsRead/Write Splitting
45,Improve this page
45,Back to Top
45,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
45,© 2013~2024 Jinzhu
45,Documentation licensed under CC BY 4.0.
45,感谢 无闻 对域名 gorm.cn 的捐赠
45,浙ICP备2020033190号-1
45,Home
45,DocsGenCommunityAPIContribute
45,Getting StartedOverviewDeclaring ModelsConnecting to DatabaseCRUD InterfaceCreateQueryAdvanced QueryUpdateDeleteRaw SQL & SQL BuilderAssociationsBelongs ToHas OneHas ManyMany To ManyAssociation ModePreloading (Eager Loading)TutorialsContextError HandlingMethod ChainingSessionHooksTransactionsMigrationLoggerGeneric Database InterfacePerformanceCustomize Data TypesScopesConventionsSettingsAdvanced TopicsDatabase ResolverShardingSerializerPrometheusHintsIndexesConstraintsComposite Primary KeySecurityGORM ConfigWrite PluginsWrite DriverChangeLogCommunityContributeTranslate current site
45,English
45,English
45,简体中文
45,Deutsch
45,bahasa Indonesia
45,日本語
45,Русский
45,한국어
45,हिन्दी
45,French
45,Italiano
45,Español
49,10 Strategies to Optimize Database Performance
49,Home
49,Blog
49,About Us
49,Contact Us
49,MySQL Consultation
49,MariaDB Consultation
49,PostgreSQL Consultation
49,SQL Server Consultation
49,MongoDB Consultation
49,Database Consultation
49,Search
49,Join
49,10 Strategies to Optimize Database Performance
49,OptimizDBA Team
49,OptimizDBA Team
49,"October 17, 2023 — 6 minutes read"
49,"10 Strategies to Optimize Database PerformanceOverviewWhat is database performance optimization?Database performance optimization refers to the process of improving the efficiency and speed of a database system. It involves various strategies and techniques to enhance the performance of database operations, such as query optimization, indexing, and data partitioning. By optimizing database performance, organizations can ensure faster response times, improved scalability, and better utilization of system resources. This is particularly important in today's data-driven world, where businesses rely heavily on databases for storing and retrieving large amounts of data for analytics, reporting, and decision-making.Importance of optimizing database performanceOptimizing database performance is crucial for ensuring efficient and smooth operations. A well-performing database can enhance the overall performance of an application or system, while a poorly optimized database can lead to slow response times, increased downtime, and even data loss. By optimizing database performance, organizations can improve query execution, reduce latency, and minimize database differences. This can be achieved through various strategies, such as indexing, query optimization, caching, and regular maintenance tasks.Common challenges in database performance optimizationOne of the common challenges in database performance optimization is the complexity involved in managing large datasets. As databases grow in size, it becomes increasingly difficult to ensure efficient query execution and data retrieval. Another challenge is the need to balance the trade-off between data consistency and performance. Maintaining strict data consistency can impact the overall performance of the database. Additionally, indexing plays a crucial role in optimizing database performance. Properly indexed tables can significantly improve query performance by reducing the number of disk I/O operations. Implementing caching mechanisms is another strategy to enhance database performance. By storing frequently accessed data in memory, caching reduces the need for disk I/O and improves query response time. Lastly, query optimization techniques such as rewriting queries, using appropriate join algorithms, and optimizing query execution plans can greatly improve database performance.Choosing the right database engineUnderstanding different types of database enginesThere are various types of database engines available, each with its own strengths and weaknesses. SQL (Structured Query Language) is a common language used to interact with relational databases. It provides powerful querying capabilities and allows for efficient data retrieval. However, it is important to understand the opportunities with SQL and how to optimize database performance. By optimizing queries, indexing data, and implementing caching strategies, developers can improve the overall performance of their database systems.Evaluating performance factors of database enginesWhen it comes to optimizing database performance, evaluating performance factors of database engines is crucial. Indexing is one such factor that plays a significant role in enhancing database performance. By creating indexes on frequently queried columns, the database engine can quickly locate the required data, resulting in faster query execution. Additionally, indexing can also improve data retrieval efficiency and reduce disk I/O operations.Considerations for selecting the appropriate database engineWhen choosing a database engine, it is important to consider several factors. Database performance, scalability, and data integrity are some of the key considerations. Additionally, the type of data and the workload characteristics should be taken into account. It is crucial to assess the requirements of the application and choose a database engine that can handle the expected workload efficiently. Database engines like MySQL, PostgreSQL, and MongoDB offer different features and capabilities, so it is essential to evaluate them based on the specific needs of the project. Furthermore, considering factors such as data size, read and write operations, and the need for real-time analytics can help in selecting the most suitable database engine.Optimizing database schema designNormalization and denormalization techniquesNormalization and denormalization are two techniques used to optimize database performance. Normalization is the process of organizing data into tables and eliminating redundant data. It helps to minimize data redundancy, improve data integrity, and reduce data anomalies. On the other hand, denormalization is the process of combining tables to improve query performance. It involves duplicating data and adding redundancy to eliminate the need for complex joins. By carefully applying normalization and denormalization techniques, you can achieve a balance between data integrity and query performance, ultimately optimizing database performance.Indexing strategies for efficient data retrievalWhen it comes to data management, efficient data retrieval is crucial for optimal database performance. Implementing effective indexing strategies can significantly improve query performance by reducing the time it takes to locate and retrieve the desired data. There are several indexing techniques that can be employed, such as B-tree and hash indexes. Additionally, utilizing clustered indexes can further enhance data retrieval speed by physically ordering the data on disk. It is important to carefully analyze the data access patterns and query requirements to determine the most appropriate indexing strategy for your database. Regularly monitoring and maintaining the indexes is also essential to ensure their continued effectiveness.Partitioning and sharding for scalabilityPartitioning and sharding are best practices for optimizing database performance and achieving scalability. Partitioning involves dividing a large database into smaller, more manageable parts called partitions. Each partition contains a subset of the data, which allows for faster data access and query execution. Sharding, on the other hand, involves distributing the data across multiple database instances or servers. This helps distribute the workload and allows for parallel processing, leading to improved performance. By implementing partitioning and sharding, organizations can effectively handle large volumes of data, improve response times, and ensure high availability.Query optimization techniquesUnderstanding query execution plansQuery execution plans are essential for optimizing database performance. By analyzing the execution plan, you can identify areas where the query can be improved. One way to improve MariaDB performance is to optimize the indexes used by the query. This can be done by analyzing the query execution plan and identifying the columns that are frequently used in the WHERE clause or JOIN conditions. By creating appropriate indexes on these columns, the database can quickly locate the required data, resulting in faster query execution times. Additionally, optimizing the query itself by rewriting it or using appropriate query hints can also help improve performance. It is important to regularly review and analyze the query execution plans to identify potential bottlenecks and optimize the database performance.Optimizing SQL queries with proper indexingOne of the key strategies to optimize database performance is by optimizing SQL queries with proper indexing. Indexing plays a crucial role in improving query performance by allowing the database engine to quickly locate and retrieve the required data. By creating indexes on the columns frequently used in the WHERE and JOIN clauses, the database can efficiently filter and join the data, resulting in faster query execution. Additionally, monitoring SQL industry trends can help identify new techniques and best practices for query optimization. Regularly updating the database and query optimization techniques based on the latest trends can further enhance performance and keep up with the evolving demands of the industry.Using query hints and optimizer directivesQuery hints and optimizer directives are powerful tools that can be used to improve database performance. By providing additional instructions to the query optimizer, developers can guide the execution plan generation process and achieve better query performance. Query hints allow developers to specify which indexes to use, how to join tables, and other optimizations. Optimizer directives, on the other hand, provide high-level instructions to the optimizer, such as forcing a specific join algorithm or enabling parallel execution. Improving database performance through the use of query hints and optimizer directives requires a deep understanding of the database system and query execution process.Query optimization techniques are crucial for improving the performance of your database. By implementing these techniques, you can experience transaction speeds that are at least twice as fast as before. At OptimizDBA Database Optimization Consulting, we specialize in optimizing databases to achieve optimal performance. With over 20 years of experience, we have helped over 500 clients achieve significant increases in performance. Our average speeds are often 100 times, 1000 times, or even higher! If you're looking to optimize your database and improve its performance, contact us today for a consultation. Let us help you unlock the full potential of your database!"
49,Share this post
49,The link has been copied!
49,Tags
49,Datatabase Optimization Trends
49,Newer post
49,Enhancing Database Performance: Best Practices for MariaDB
49,Older post
49,Enhancing Database Performance: Best Practices for MariaDB
49,Subscribe to new posts
49,Subscribe
49,Processing your application
49,Please check your inbox and click the link to confirm your subscription.
49,There was an error sending the email
49,"OptimizDBA Database Optimization Latest & Highlighted Articles from our blog, updates, and more."
49,Social
49,Links
49,MySQL Consultation
49,MariaDB Consultation
49,PostgreSQL Consultation
49,SQL Server Consultation
49,MongoDB Consultation
49,Links
49,Terms & Conditions
49,© OptimizDBA.com 2024.
49,You’ve successfully subscribed to OptimizDBA.com
49,Welcome back! You’ve successfully signed in.
49,Great! You’ve successfully signed up.
49,Success! Your email is updated.
49,Your link has expired
49,Success! Check your email for magic link to sign-in.
51,Performance tuning | Socket.IO
51,"Skip to main contentThank you for your interest in the user study, aimed at providing better support for Socket.IO users on Azure."
51,"Read our findings from the hundreds of responses and learn about how Azure can help with scaling out Socket.IO apps easily.Socket.IODocsGuideTutorialExamplesEmit cheatsheetServer APIClient APIEcosystemHelpTroubleshootingStack OverflowGitHub DiscussionsSlackNewsBlogTwitterToolsCDNAdmin UIAboutFAQChangelogRoadmapBecome a sponsor4.x4.x3.x2.xChangelogEnglishEnglishFrançaisPortuguês (Brasil)中文（中国）SearchSocket.IODocumentationServerClientEventsAdaptersAdvancedNamespacesCustom parserAdmin UIUsage with PM2Load testingPerformance tuningMigrationsMiscellaneousAdvancedPerformance tuningVersion: 4.xOn this pagePerformance tuningHere are some tips to improve the performance of your Socket.IO server:at the Socket.IO levelat the OS levelYou might also be interested in scaling to multiple nodes.At the Socket.IO level​Since, in most cases, the Socket.IO connection will be established with WebSocket, the performance of your Socket.IO server will be strongly linked to the performance of the underlying WebSocket server (ws, by default).Install ws native add-ons​ws comes with two optional binary add-ons which improve certain operations. Prebuilt binaries are available for the most popular platforms so you don't necessarily need to have a C++ compiler installed on your machine.bufferutil: Allows to efficiently perform operations such as masking and unmasking the data payload of the WebSocket frames.utf-8-validate: Allows to efficiently check if a message contains valid UTF-8 as required by the spec.To install those packages:$ npm install --save-optional bufferutil utf-8-validatePlease note that these packages are optional, the WebSocket server will fallback to the Javascript implementation if they are not available. More information can be found here.Use another WebSocket server implementation​For example, you can use the eiows package, which is a fork of the (now deprecated) uws package:$ npm install eiowsAnd then use the wsEngine option:const { createServer } = require(""http"");const { Server } = require(""socket.io"");const httpServer = createServer();const io = new Server(httpServer, {"
51,"wsEngine: require(""eiows"").Server});Use a custom parser​If you send binary data over the Socket.IO connection, using a custom parser like the one based on msgpack might be interesting, as by default each buffer will be sent in its own WebSocket frame.Usage:Serverconst { createServer } = require(""http"");const { Server } = require(""socket.io"");const parser = require(""socket.io-msgpack-parser"");const httpServer = createServer();const io = new Server(httpServer, {"
51,"parser});Clientconst { io } = require(""socket.io-client"");const parser = require(""socket.io-msgpack-parser"");const socket = io(""https://example.com"", {"
51,"parser});Discard the initial HTTP request​By default, a reference to the first HTTP request of each session is kept in memory. This reference is needed when working with express-session for example (see here), but can be discarded to save memory:io.engine.on(""connection"", (rawSocket) => {"
51,"rawSocket.request = null;});Before:After:At the OS level​There are lots of good articles on how to tune your OS to accept a large number of connections. Please see this one or this one for example.While load testing your Socket.IO server, you will likely reach the two following limits:maximum number of open filesIf you can't go over 1000 concurrent connections (new clients are not able to connect), you have most certainly reached the maximum number of open files:$ ulimit -n1024To increase this number, create a new file /etc/security/limits.d/custom.conf with the following content (requires root privileges):* soft nofile 1048576* hard nofile 1048576And then reload your session. Your new limit should now be updated:$ ulimit -n1048576maximum number of available local portsIf you can't go over 28000 concurrent connections, you have most certainly reached the maximum number of available local ports:$ cat /proc/sys/net/ipv4/ip_local_port_range32768"
51,"60999To increase this number, create a new file /etc/sysctl.d/net.ipv4.ip_local_port_range.conf with the following content (again, requires root privileges):net.ipv4.ip_local_port_range = 10000 65535Note: we used 10000 as a lower bound so it does not include the ports that are used by the services on the machine (like 5432 for a PostgreSQL server), but you can totally use a lower value (down to 1024).Once you reboot your machine, you will now be able to happily go to 55k concurrent connections (per incoming IP).See also:https://unix.stackexchange.com/a/130798https://making.pusher.com/ephemeral-port-exhaustion-and-how-to-avoid-it/Edit this pageLast updated on Mar 14, 2024PreviousLoad testingNextMigrating from 2.x to 3.0At the Socket.IO levelInstall ws native add-onsUse another WebSocket server implementationUse a custom parserDiscard the initial HTTP requestAt the OS levelDocumentationGuideTutorialExamplesServer APIClient APIHelpTroubleshootingStack OverflowGitHub DiscussionsSlackNewsBlogTwitterToolsCDNAdmin UIAboutFAQChangelogRoadmapBecome a sponsorCopyright © 2024 Socket.IO"
52,Lagging replication: Slave_SQL_Running_State: Waiting for dependent transaction to commit - Percona Server for MySQL 8.0 - Percona Community Forum
52,Percona Community Forum
52,Lagging replication: Slave_SQL_Running_State: Waiting for dependent transaction to commit
52,MySQL & MariaDB
52,Percona Server for MySQL 8.0
52,mag1kan1n
52,"July 11, 2023, 10:36am"
52,Good afternoon.
52,"We have a cluster MySQL 8.0.32-24 for Linux on x86_64 (Percona Server, Release 24, Revision e5c6e9d2) Master/Slave. Database size is 916GB. Relation is lagging, in show slave Status\G shows Slave_SQL_Running_State: Waiting for dependent transaction to commit."
52,The lag floating for the last 24 hours rose to 16k and dropped to Seconds_Behind_Master: 8021.
52,telegram-cloud-photo-size-2-5289621519653259718-y1280×238 20.2 KB
52,"We tried to change the parameter replica_parallel_workers=1, but did not help."
52,Could you help with any direction or hints for improvements.
52,Here is the config
52,[mysqld]
52,datadir=/var/lib/mysql
52,socket=/var/lib/mysql/mysql.sock
52,bind-address = 0.0.0.0
52,user=mysql
52,symbolic-links=0
52,log_timestamps = SYSTEM
52,log_error = /var/log/mysql/error.log
52,log_bin_trust_function_creators = 1
52,"sql_mode = ""ERROR_FOR_DIVISION_BY_ZERO,NO_ZERO_IN_DATE,NO_ENGINE_SUBSTITUTION"""
52,log_bin = binlog
52,log_bin_index = binlog_list
52,max_binlog_size = 1G
52,expire_logs_days = 5
52,binlog_row_image = MINIMAL
52,binlog-do-db = dbname
52,server-id = 2
52,relay-log = /var/lib/mysql/slave-relay-bin
52,relay-log-index = /var/lib/mysql/slave-relay-bin.index
52,max_connections = 100
52,innodb_doublewrite = 0
52,sync_binlog = 1
52,wait_timeout=150
52,interactive_timeout=7200
52,back_log = 4096
52,"slave-skip-errors = 1062,1396"
52,key_buffer_size = 512M
52,max_allowed_packet = 1024M
52,table_open_cache = 20000
52,table_definition_cache = 10000
52,sort_buffer_size = 2M
52,read_buffer_size = 1M
52,read_rnd_buffer_size = 1M
52,net_buffer_length = 32K
52,group_concat_max_len = 4K
52,join_buffer_size = 320M
52,max_join_size = 1024M
52,tmp_table_size = 4G
52,max_heap_table_size = 8G
52,thread_cache_size = 80
52,thread_stack = 4M
52,innodb_buffer_pool_size = 72G
52,innodb_buffer_pool_instances = 16
52,innodb_log_file_size = 1G
52,innodb_log_buffer_size = 64M
52,innodb_lock_wait_timeout = 60
52,innodb_file_per_table = 1
52,innodb_page_cleaners = 8
52,innodb_read_io_threads = 32
52,innodb_write_io_threads = 8
52,innodb_flush_method = O_DSYNC
52,innodb_flush_log_at_trx_commit = 2
52,innodb_io_capacity = 100
52,skip-external-locking
52,#innodb_adaptive_hash_index=NO
52,#replica_parallel_workers=4
52,slave_sql_verify_checksum=OFF
52,1 Like
52,Denis_Subbota
52,"July 11, 2023, 11:34am"
52,"Hello mag1kan1n,"
52,"It’s expected to see in process list Waiting for dependent transaction to commit when using multithreaded replication, as not all transactions may be applied in parallel ( it’s consistency cost )"
52,The first thing I would like you to suggest is to relax ACID settings to reduce load on IO system and speedup the replication process:
52,"SET GLOBAL innodb_flush_log_at_trx_commit=2,sync_binlog=0;"
52,"But it would help if you considered the risk of the losing all transactions for the last 1 minute in case of crash of mysql, so it’s not good option to set on writer node in case if you are using this node as replica and the writer as well."
52,"However if it’s pure replica, it’s one of the best way to increase replication speed."
52,"Regards,"
52,Denis Subbota.
52,"Managed Services, Percona."
52,2 Likes
52,Denis_Subbota
52,"July 11, 2023, 11:36am"
52,also you may want to review
52,innodb_log_file_size
52,Percona Database Performance Blog – 18 Oct 17
52,How to Choose the MySQL innodb_log_file_size
52,Peter Zaitsev provides guidance on how to choose the MySQL innodb_log_file_size. Getting the innodb_log_file_file size is important to achieve balance between reasonably fast crash recovery time and good system performance.
52,Est. reading time: 3 minutes
52,1 Like
52,mag1kan1n
52,"July 11, 2023, 11:43am"
52,"Thanks, let’s try it. We also get in Slave_SQL_Running_State: Waiting for replica workers to process their queues"
52,1 Like
52,CTutte
52,"July 11, 2023, 12:13pm"
52,"Hi mag1kan1n,"
52,"There are multiple causes that can make a replica fall behind. You should check if there is CPU starvation, memory swapping , disk saturation else replication lagging behind might be due to contention. Make sure no subsystem is saturated because I see some unusually large buffer configuration and it’s possible there is swapping or high memory pressure (I am specially looking at you join_buffer_size = 320M )."
52,"Since you mentioned parallel replication, if you want to tune replica_parallel_workers please check Estimating potential for MySQL 5.7 parallel replication ."
52,"Note that even if you set multiple parallel workers, at times only 1 (or a few) transactions can be concurrently executing and might make a replica fall behind. For example DDLs and very large transactions will make the replica be single threaded for a while. You can find out parallel replication efficiency with the above blogpost"
52,If replication is mostly single threaded then you should tune your workload rather than tune MySQL configuration.
52,For running DDLs on the primary you should use pt-online-schema-change pt-online-schema-change — Percona Toolkit Documentation
52,"if you have very large transactions then you should split them into smaller chunks. I.e rather than deleting 1M rows in 1 transaction, commit every 10k rows"
52,You can also try setting innodb_flush_method = O_DIRECT rather than O_DSYNC
52,"Last, remember that every parameter that you configure in the database will be a tradeoff between using more resources or relaxing consistency at the cost of performance. Relaxing consistency (disabling innodb_doublewrite like you did) can cause corruption in the event of a crash so you should not change parameters lightly as they can cause more harm than good"
52,Regards
52,2 Likes
52,mag1kan1n
52,"July 11, 2023,"
52,2:26pm
52,"Thanks, made innodb_flush_method = O_DIRECT, will monitor the changes. So far this is the situation for an hour -"
52,изображение2830×522 33.5 KB
52,1 Like
52,matthewb
52,"July 12, 2023, 12:54am"
52,You are throttling yourself:
52,innodb_io_capacity = 100
52,<-- change to default 200
52,tmp_table_size = 4G
52,<-- The actual value used will be the smaller
52,max_heap_table_size = 8G
52,<-- of these two. You should make them the same.
52,sync_binlog = 1
52,<-- causes LOTS of disk IO. You should set to 1000 so it syncs less often
52,2 Likes
52,mag1kan1n
52,"July 12, 2023,"
52,7:10am
52,"Hi all, we were helped by including innodb_flush_method = O_DIRECT instead of O_DSYNC. Now there is no lag on the replica. But as far as I understand from the description, this is a tradeoff between fault tolerance and speed? Here’s a 24-hour graph -"
52,изображение2798×524 50 KB
52,1 Like
52,mag1kan1n
52,"July 12, 2023,"
52,8:08am
52,"Thanks for the advice, we’ll give it a try"
52,1 Like
52,matthewb
52,"July 12, 2023,"
52,2:00pm
52,mag1kan1n:
52,this is a tradeoff between fault tolerance and speed?
52,"The different between O_DSYNC and O_DIRECT is that one uses filesystem buffers, and the other does not. Technically, yes, this can be less fault tolerance because you are skipping the filesystem’s journal, but InnoDB has its own protection against this which is the doublewrite buffer. Ideally, you would turn the dblwb on and use O_DIRECT for maximum performance with data reliability."
52,3 Likes
52,mag1kan1n
52,"July 12, 2023,"
52,7:47pm
52,Thank you so much for the advice. Now with parameters
52,slave_sql_verify_checksum = ON
52,innodb_flush_method = O_DIRECT
52,There is no replication lag.
52,1 Like
52,Related Topics
52,Topic
52,Replies
52,Views
52,Activity
52,A Replication lag on Percona Server 8.0.29-21
52,Percona Distribution for MySQL
52,percona
52,new-release
52,1097
52,"January 10, 2023"
52,Percona Master-Master lag issues
52,Other MySQL® Questions
52,887
52,"March 23, 2015"
52,MySQL replication slave parallel workers only work 1 worker
52,MySQL & MariaDB
52,1744
52,"March 12, 2024"
52,Replication lag after upgrade to Percona Server 8.0
52,2009
52,"January 25, 2023"
52,Laggy master-master gtid replication in 5.7.32
52,Percona Server for MySQL 5.7
52,720
52,"November 3, 2021"
52,Home
52,Categories
52,FAQ/Guidelines
52,Terms of Service
52,Privacy Policy
52,"Powered by Discourse, best viewed with JavaScript enabled"
52,Unanswered | Unsolved | Solved
52,"MySQL, InnoDB, MariaDB and MongoDB are trademarks of their respective owners.Copyright © 2006 - 2021 Percona LLC. All rights reserved."
53,"Connect to a Custom SQL Query - TableauTableau Desktop and Web Authoring HelpConnect to a Custom SQL QueryApplies to: Tableau Cloud, Tableau Desktop, Tableau Server"
53,"Note: Using custom SQL can affect performance of a workbook. Working with your DBA will ensure the best possible custom SQL query. In order to perform the operations necessary for building views in Tableau Desktop, Tableau must be able to control WHERE, GROUP BY and other SQL clauses. Because a custom SQL query may contain those elements, and Tableau cannot inject them into the existing SQL, Tableau must wrap the custom SQL statement within a select statement. When a custom SQL connection is performing slowly, or produces an error, it is often the result of the custom SQL wrapping that Tableau Desktop performs."
53,"For most databases, you can connect"
53,"to a specific query rather than the entire data set. Because databases have slightly different SQL syntax from each other, the custom SQL you use to connect to one database might be different from the custom SQL you might use to connect to another. However, using custom SQL can be useful when you know exactly the information you need and understand how to write SQL queries."
53,"Though there are several common reasons why you might use custom SQL, you can use custom SQL to union your data across tables, recast fields to perform cross-database joins, restructure or reduce the size of your data for analysis, etc."
53,"For Excel and text file data sources, this option is available only in workbooks that were created before Tableau Desktop 8.2 or when using Tableau Desktop on Windows with the legacy connection. To connect to Excel or text files using the legacy connection, connect to the file, and in the Open dialog box, click the Open drop-down menu, and then select Open with Legacy Connection."
53,"NOTE: Beginning with Tableau 2020.2, legacy Excel and Text connections are no longer supported. See the Legacy Connection Alternatives document in Tableau Community for alternatives to using the legacy connection."
53,Connect to a custom SQL query
53,"After connecting to your data, double-click the New Custom SQL option on the Data Source page."
53,Type or paste the query into the text box. The query must be a single SELECT* statement.
53,"When finished, click OK."
53,"When you click OK, the query runs and the custom SQL query table appears in the logical layer of the canvas. Only relevant fields"
53,from the custom SQL query display in the data grid on the Data Source page.
53,"For more information about the logical and physical layers of the canvas, see The Tableau Data Model."
53,Examples of custom SQL queries
53,Combine your tables vertically (union)
53,"If you need to append data to each other, you can use the union option in the physical layer of the canvas in Tableau. In some cases your database does not support this option, so you can use custom SQL instead."
53,"For example, suppose you have the following two tables: November and December."
53,November
53,December
53,"You can use the following custom SQL query to append the second table, December, to the first table, November:"
53,SELECT * FROM November UNION ALL SELECT * FROM December
53,The result of the query looks like this in the data grid:
53,"For more information about the union option, see Union Your Data."
53,Change the data type of a field to do a cross-database join
53,"When you want to perform a join between two tables in the physical layer of the canvas, the data type of the fields you join on must be the same. In cases when the data type of the fields are not the same, you can use custom SQL to change the data type (cast) the field before performing the join."
53,"For example, suppose you want to join two tables, Main and Sub, using the Root and ID fields, respectively. The Root field is a number type and the ID field is a string type. You can use the following custom SQL query to change the data type of Root from a number to a string so that you can join the Main and Sub tables using the Root and ID fields."
53,SELECT
53,[Main].[Root] AS [Root_Number]CAST([Main].[Root] AS INT] AS [Root_String]
53,FROM [Main]
53,The result of this query shows the original Root field and the Root field cast as a string.
53,"For more information about joins and cross-database joins, see Join Your Data."
53,Reduce the size of your data
53,"When working with very large data sets, sometimes you can save time while working with your data if you reduce its size first."
53,"For example, suppose you have a large table called FischerIris. You can use the following custom SQL query to retrieve the specified columns and records thereby reducing the size of the data set that you connect to from Tableau."
53,SELECT
53,"[FischerIris].[Species] AS [Species],"
53,"[FischerIris].[Width] AS [Petal Width],"
53,COUNT([FischerIris].[ID]) AS [Num of Species]
53,FROM [FischerIris]
53,WHERE [FischerIris].[Organ] = 'Petal'
53,AND [FischerIris].[Width] > 15.0000
53,"GROUP BY [FischerIris].[Species], [FischerIris].[Width]"
53,Restructure your data (pivot)
53,"In some cases, you might be working with a table that needs to be restructured before analysis. Though this type of task can be done in the physical layer of the canvas in Tableau by using options like pivot, your database might not support it. In this case, you can use custom SQL instead."
53,"For example, suppose you have the following table:"
53,"To change its structure and optimize your data for analysis in Tableau, you can use the following custom SQL query:"
53,"SELECT Table1.Season ID AS [Season ID],"
53,"Table1.Items - Don't like AS [Quantity],"
53,"""Don't Like"" AS [Reason]"
53,FROM Table1
53,UNION ALL
53,"SELECT Table1.Season ID AS [Season ID],"
53,"Table.Items - Defective AS [Quantity],"
53,"""Defective"" AS [Reason]"
53,FROM Table1
53,UNION ALL
53,"SELECT Table1.Season ID AS [Season ID],"
53,"Table1.Items - Too big AS [Quantity],"
53,"""Too Big"" AS [Reason]"
53,FROM Table1
53,UNION ALL
53,"SELECT Table1.Season ID AS Season ID,"
53,Table1.Items - Too small AS [Quantity]
53,"""Too Small"" AS [Reason]"
53,FROM Table1
53,The result of the query looks like this in the data grid:
53,"For more information about the pivot option, see Pivot Data from Columns to Rows."
53,Combine (join) and aggregate your data
53,"If you need to combine tables and aggregate your data, you can use both a join and default aggregation type options in the physical layer of the canvas in Tableau. In some cases you might need to use custom SQL instead."
53,"For example, suppose you have the following two tables: Orders and Vendors."
53,Orders
53,Vendors
53,You can use the following custom SQL query to find a count on the number of orders and do a left join on the Orders and Vendors tables:
53,"SELECT Vendors.Name,COUNT(Orders.Order) AS Number Of Orders"
53,FROM Orders
53,LEFT JOIN Vendors
53,ON Orders.VendorID=Vendors.VendorID
53,GROUP BY Name;
53,The result of the query looks like this:
53,"For more information about joins, see Join Your Data."
53,Errors when duplicate columns are referenced
53,If your custom SQL query references
53,"duplicate columns, you may get errors when trying to use one of"
53,the columns in your analysis in Tableau. This will happen even if the query is valid.
53,"For example, consider the following query:"
53,SELECT * FROM
53,"authors, titleauthor WHERE authors.au_id = titleauthor.au_id"
53,The
53,"query is valid, but the au_id field is ambiguous"
53,because in this case it exists in both the “authors” table and the “titleauthor”
53,table. Tableau will connect to the query but you will get an error anytime
53,you try to use the au_id field. This is because
53,Tableau doesn’t know which table you are referring to.
53,Note: It is a best practice to define column aliases with an AS clause whenever possible in a Custom SQL Query. This is because each database has its own rules when it comes to automatically generating a column name whenever an alias is not used.
53,Edit a custom SQL query
53,To edit a custom SQL query
53,"On the data source page, in the canvas, double-click the custom SQL query in the logical layer."
53,Hover over the custom SQL table in the physical layer until the arrow displays.
53,Click the arrow and then select Edit Custom SQL Query.
53,"In the dialog box, edit the custom SQL query."
53,To change a custom SQL query name
53,"When you drag a custom SQL query to the logical layer of the canvas, Tableau gives it a default name: Custom SQL Query, Custom SQL Query1, and so on. You can change the default name to something more meaningful."
53,"On the data source page, in the logical layer of the canvas, select the drop-down arrow in the custom SQL query table and select Rename."
53,Enter the name you want to use for your custom SQL query.
53,Use parameters in a custom SQL query
53,"You can use parameters in a custom SQL query statement to replace a constant value with a dynamic value. You can then update the parameter in the workbook to modify the connection. For example, you may connect to a custom SQL query that provides web traffic data for a particular page that is specified by a pageID. Instead of using a constant value for the pageID value in the SQL query, you can insert a parameter. Then after finishing the connection, you can show a parameter control in the workbook. Use the parameter control to switch out the pageID and pull in data for each page of interest without having to edit or duplicate the connection."
53,"In Tableau Desktop, you can create a parameter directly from the Custom SQL dialog box or use any parameters that are part of the workbook. If you create a new parameter, it becomes available for use in the workbook just like any other parameter. See Create Parameters to learn more."
53,"For web authoring (in Tableau Cloud or Tableau Server), you can use an existing parameter published from Tableau Desktop. You cannot create a new parameter in web authoring."
53,To add a parameter to a custom SQL query
53,"On the data source page, in the canvas, hover over the table until the edit icon displays, and then click the edit button."
53,"At the bottom of the dialog box, click Insert Parameter."
53,"Select a constant value in the SQL statement and then, from the"
53,"Insert Parameter drop-down menu select the parameter you want to use instead. If you have not created a parameter yet, select Create a new parameter. Follow the instructions in Create Parameters to create a parameter."
53,Note: Parameters can only replace literal values. They cannot replace expressions or identifiers such as table names.
53,"In the example below, the custom SQL query returns all orders that are marked as Urgent priority. In the custom SQL statement, the order priority is the constant value. If you want to change the connection to see the High priority orders, you would have to edit the data source."
53,"Instead of creating and maintaining many variations of the same query, you can replace the constant order priority value with a parameter. The parameter should contain all of the possible values for Order Priority."
53,"After you create a parameter, you can insert it into the SQL statement to replace the constant value."
53,"After you finish editing the connection, the new parameter is listed in the Parameters area at the bottom of the Data pane and the parameter control displays on the right side of the view. As you select different values, the connection updates."
53,"Note: If you are using an extract, you must refresh the extract in order to reflect changes to the parameter. Publishing a data source that uses Custom SQL parameters includes the parameters. The parameters are transferred to any workbooks that connect to the data source."
53,Tableau Catalog support for custom SQL
53,Support for custom SQL in Tableau Catalog depends on the custom SQL query.
53,"Tableau Catalog is available as part of the Data Management offering for Tableau Server and Tableau Cloud. For more information about Tableau Catalog, see ""About Tableau Catalog"" in the Tableau Server or Tableau Cloud Help."
53,Supported queries
53,"Catalog supports custom SQL queries that meet the ANSI SQL-2003 standard, with three known exceptions:"
53,Time zone expressions
53,Multiset expressions
53,Tableau parameters
53,"Starting in 2021.4, Tableau Catalog also supports use of the Transact-SQL (T-SQL) dialect in Custom SQL, with the following exceptions:"
53,Hints
53,FOR clauses
53,"OPENROWSET, OPENXML, and OPENJSON functions"
53,ODBC scalar functions
53,FOR SYSTEM_TIME
53,TABLESAMPLE
53,MATCH expression
53,CONTAINS expression
53,FREETEXT expression
53,"Starting in Tableau Cloud October 2023 and Tableau Server 2023.3, Tableau Catalog also offers support for custom SQL queries that use PostgreSQL, with the following exceptions:"
53,XML function
53,JSON functions and operators
53,Supported features and functions
53,"Catalog supports the following additional functionality for data sources, workbooks, and flows with connections that use the MySQL or PostgreSQL drivers, for example, Amazon Aurora for MySQL, Amazon RedShift, Pivotal Greenplum Database, MemSQL, Denodo, and others."
53,MySQL GROUP_CONCAT function
53,PostgreSQL arrays
53,PostgreSQL EXTRACT() function
53,"Other custom SQL scenarios and functionality might work, but Tableau doesn't specifically test for or support them."
53,Supported lineage
53,"When an asset uses custom SQL, a message with a Show Custom SQL Query button appears on the Lineage tab of the asset page. Click the button to see the custom SQL used in the connection. Then, if you would like to copy the custom SQL to your clipboard, click Copy."
53,"Some types of custom SQL can cause the upstream lineage to be incomplete. When this happens, a message appears with that information."
53,"Field details cards might not contain links to connected columns, or might not show any connected columns at all."
53,"Column details cards might not contain links to fields that use the column, or might not show any fields at all."
53,"If you are examining a table’s lineage, note that Catalog doesn't support showing column information in the lineage for table metadata gathered using custom SQL. However, if other assets use the same table and don’t use custom SQL, Tableau Catalog might be able to display information about the columns that it has discovered through these other assets."
53,"In the following screenshot, the factAccountOpportunityByQuarter table was indexed because it’s used by a data source. However, because it’s referenced by a custom SQL query, the column information isn't available."
53,"In a case where more than one data source, workbook, or flow uses a table, any of the assets downstream from that table that uses a custom SQL query are excluded when column-level filters are applied. As a result, fewer downstream assets show in the lineage than are actually used."
53,"For more information about using the lineage, see ""Use Lineage for Impact Analysis"" in the Tableau Server(Link opens in a new window) or Tableau Cloud(Link opens in a new window) Help."
53,See Also
53,Use Custom SQL and RAWSQL to perform advanced spatial analysis(Link opens in a new window)
53,Back to topThanks for your feedback!Your feedback has been successfully submitted. Thank you!LegalPrivacyCookie Preferences
54,Community Guide to PostgreSQL GUI Tools - PostgreSQL wiki
54,"Want to edit, but don't see an edit button when logged in?"
54,Click here.
54,Community Guide to PostgreSQL GUI ToolsFrom PostgreSQL wikiJump to navigationJump to search
54,"This page is a list of miscellaneous utilities that work with Postgres (ex: data loaders, comparators etc.)."
54,"Things that don't do queries' ""enter SQL and get it back out again."""
54,"If you'd like to find clients that allows you to ""enter SQL and get it back out again"" see PostgreSQL Clients."
54,If you'd like to find DB visualization or design tools see Design Tools.
54,Contents
54,1 Open Source / Free Software
54,1.1 Libre Office
54,1.2 PASH-Viewer: PostgreSQL Active Session History Viewer
54,"1.3 pgrights: GUI for PostgreSQL roles, privileges and policies"
54,1.4 Sohag Developer
54,1.5 Beekeeper Studio
54,1.6 Execsql.py
54,2 Proprietary
54,2.1 Access
54,2.2 Five
54,2.3 dbForge Studio for PostgreSQL
54,2.4 dbForge Data Compare for PostgreSQL
54,2.5 dbForge Schema Compare for PostgreSQL
54,2.6 TablePlus
54,2.7 Ultorg
54,2.8 WaveMaker Ajax GUI Design Tool
54,2.9 Postgres Compare
54,2.10 Full Convert
54,2.11 Abris Platform
54,2.12 Replicator Pro
54,2.13 DBTools Manager
54,2.14 PostgreSQL PHP Generator
54,2.15 ConvertDB for PosttgreSQL
54,2.16 dotConnect for PostgreSQL
54,2.17 Devart PostgreSQL Data Access Components
54,2.18 Devart ODBC Driver for PostgreSQL
54,2.19 Devart Excel Add-in for PostgreSQL
54,2.20 EMS Database Management Tools for PostgreSQL
54,2.21 SQL Maestro Group products for PostgreSQL
54,2.22 Datanamic DataDiff for PostgreSQL
54,2.23 Datanamic SchemaDiff for PostgreSQL
54,2.24 DB MultiRun PostgreSQL Edition
54,2.25 DB Doc for PostgreSQL
54,2.26 SQL Blob Export
54,2.27 SQL File Import
54,2.28 SQL Image Viewer
54,2.29 SQL Multi Select
54,2.30 Devart SSIS Data Flow Components for PostgreSQL
54,2.31 EDB Postgres Enterprise Manager
54,2.32 ClusterControl by Severalnines
54,2.33 Reportizer
54,2.34 Exportizer Enterprise
54,2.35 TiCodeX SQL Schema Compare
54,2.36 pgMustard
54,2.37 ODBC Driver for PostgreSQL by CData
54,2.38 JDBC Driver for PostgreSQL by CData
54,2.39 ADO.NET Provider for PostgreSQL by CData
54,2.40 Excel Add-In
54,for PostgreSQL by CData
54,2.41 SSIS Components for PostgreSQL by CData
54,2.42 Power BI Connector for PostgreSQL by CData
54,3 Other Resources
54,Open Source / Free Software
54,"This is the list of ""open source and free"" miscellaneous utilities:"
54,Libre Office
54,http://www.libreoffice.org/discover/base/
54,"Supports MySQL/MariaDB, Adabas D, MS Access and PostgreSQL, as well as other JDBC/ODBC databases."
54,PASH-Viewer: PostgreSQL Active Session History Viewer
54,https://github.com/dbacvetkov/PASH-Viewer
54,Java (multi-platform).
54,"Open-source software which provides graphical view of active session history and help you to answer questions like ""What wait events were taking most time?"", ""Which sessions were taking most time?"", ""Which queries were taking most time and what were they doing?"". It also supports Active Session History extension by pgsentinel."
54,"Does not do DB inserts, modifications, etc."
54,"pgrights: GUI for PostgreSQL roles, privileges and policies"
54,https://github.com/apsavin/pgrights
54,"MacOS (based on Electron, so versions for other OS can be build from source code)."
54,"Open-source software which allows you to easily understand what can do (and what can't) a PostgreSQL user with a table's data. In other words, it's a viewer of results of GRANT commands and row-level security rules applied for a particular table and for a particular role."
54,"Only modifies user rights, no other capability."
54,Sohag Developer
54,Sohag Developer
54,Gnu/Linux Windows (Other OS can compile from source code).
54,Build a powerful database applications following few steps using Sohag Developer .
54,Sohag Developer currently supports PostgreSQL database and has a set of CRUD generators that generates (Qt/C++ code and ui forms - PHP web applications uses web forms and bootstrap framework ) to manipulate the data in the database tables or views.
54,Beekeeper Studio
54,https://beekeeperstudio.io/
54,"Beekeeper Studio is a modern cross-platform SQL editor and database manager available for Linux, Mac, and Windows. Some of its features include:"
54,"Clean, smooth, usable UI with dark and light themes"
54,Tabbed Interface
54,Multiple connections at the same time
54,Saved queries and run history
54,Auto-complete
54,Execsql.py
54,https://pypi.org/project/execsql/
54,Execsql.py is a SQL scripting client that reads SQL statements from a text file and sends them to the PostgreSQL server.
54,"The script file can also contain special metacommands that are interpreted by execsql.py and that can be used to import and export data, copy data between databases, conditionally execute script blocks, display data in GUI dialogs, and perform other functions."
54,Data can be exported in 19 different tabular formats or using any of three different template processors.
54,Default and custom logging features can be used to document script actions.
54,Proprietary
54,"This is a list of ""closed source"" projects, some might have some manner of free version."
54,Access
54,https://products.office.com/en-us/access
54,Windows
54,"Yes, you can use MS Access as a PostgreSQL database interface. Supports data access to PostgreSQL tables and views; many ODBC-based limitations and errors."
54,Five
54,https://five.co/
54,Rapidly develop and deliver modern business software to internal or external users.
54,Five is free to use. Develop and test applications locally free of charge. Only subscribe when you build something production worthy.
54,Features:
54,"Store, retrieve and process data from any data source (such as a PostgreSQL DB)"
54,"Rapidly implement business logic using SQL, JavaScript or TypeScript"
54,Cut down the time you spent on front-end development
54,"Scalable, secure and affordable: deploy apps in one click"
54,Keep your data secure on Five’s managed infrastructure
54,Build password-protected multi-user applications
54,Useful developer tools to accelerate your development
54,Free download available for Windows & MacOS
54,dbForge Studio for PostgreSQL
54,http://www.devart.com/dbforge/postgresql/studio/
54,"Microsoft Windows, Linux, macOS"
54,"dbForge Studio for PostgreSQL is a feature-rich IDE designed for database development and management. The tasks that can be handled with its help include completion-aided SQL coding, comparison and synchronization of databases, data management, data analysis and reporting, query optimization, and customizable generation of realistic test data."
54,Key features:
54,SQL Development: Smart code completion and formatting
54,Database Explorer: Object tree navigation and operations from the context menu
54,Database Comparison: Detection of differences in schemas and table data and generation of synchronization scripts
54,Data Import & Export: 10+ most widely used data formats
54,Query Profiler: SQL query performance tuning
54,Data Generator: Creation of meaningful dummy data for testing
54,Pivot Tables: Grouping and summarization of data for analysis
54,Master-Detail Browser: Review and analysis of data in related tables
54,Reporting: Visual data reports in 9 different formats
54,Command-Line Interface: CLI-powered automation of recurring operations
54,dbForge Data Compare for PostgreSQL
54,http://www.devart.com/dbforge/postgresql/datacompare/
54,"Microsoft Windows, Linux, macOS"
54,"dbForge Data Compare for PostgreSQL is an effective tool for table data comparison, which makes it easy to detect differences in data, analyze them, and generate SQL scripts for data synchronization."
54,Key features:
54,Identify the differences between two databases
54,Compare separate tables or table groups by table name mask
54,Compare tables with different structures
54,Compare custom query execution results
54,Generate detailed comparison reports
54,Synchronize data in tables and views fully or partially using scripts
54,Schedule data synchronization with Windows Task Scheduler
54,dbForge Schema Compare for PostgreSQL
54,http://www.devart.com/dbforge/postgresql/schemacompare/
54,"Microsoft Windows, Linux, macOS"
54,"dbForge Schema Compare for PostgreSQL and Amazon Redshift is a free tool for easy and effective comparison of database structure differences. Additionally, it generates and runs SQL scripts for synchronization of source and target schemas."
54,Key features:
54,Find differences in Redshift and PostgreSQL database schemas
54,Generate SQL scripts to update one database with the contents of another
54,Migrate PostgreSQL schemas to Amazon Redshift
54,Apply updates from development databases to staging or production
54,Compare and synchronize pre-object security permissions
54,Compare PL/pgSQL and Python code
54,TablePlus
54,https://tableplus.com/
54,"macOS, Windows, iOS"
54,"TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more."
54,True native built.
54,"Workspace supports multiple tabs, multiple windows"
54,"Powerful SQL editor with full features: auto syntax highlight, auto-suggestion, split pane, favorite and history."
54,"Data Filter & Sorting, import & export"
54,Full-dark theme & modern shortcut
54,"With plugin system, you can be able to write your own new features to work with database per your needs (export charts, pretty json…)."
54,Ultorg
54,https://www.ultorg.com
54,Ultorg is a general-purpose user interface and visual query system that works with any existing PostgreSQL database.
54,"Windows, MacOS, Linux"
54,Key features:
54,Quickly show data across many tables and one-to-many relationships.
54,"See data in continuously auto-formatted table, form, or crosstab layouts."
54,"Build the equivalent of any SQL-92 SELECT query by interacting directly with the data (choose fields, joins, filters, calculations etc.)."
54,Edit data and commit changes back to the database. Insert/delete/dropdown rows from FK relationships etc.
54,"Join PostgreSQL tables with tables from other data sources (CSV files, Google Sheets, and others)."
54,Avoid typing long SQL queries and associated formatting code.
54,WaveMaker Ajax GUI Design Tool
54,http://www.wavemaker.com/
54,"Windows, Macintosh, Linux"
54,"WaveMaker is an Ajax-based GUI design tool for Postgres. WaveMaker is built using itself! WaveMaker generates a standard Java WAR file based on Spring, Hibernate and Dojo. WaveMaker supports Postgres schema creation and import and includes a visual query editor."
54,Postgres Compare
54,frameless
54,https://www.postgrescompare.com/
54,"Windows, Mac & Linux"
54,"A comprehensive tool for identifying the differences between databases and generating an update script to synchronize them. Postgres Compare reads the system catalogs to determine the structure of the database and compares it to another to find the changes. Generate SQL and deploy the alterations, save snapshots for later. Automate the process via the command line."
54,Full Convert
54,https://www.spectralcore.com/fullconvert
54,"Database conversion and synchronization between PostgreSQL and Microsoft Access, dBase, FoxPro, Microsoft Excel, Firebird, Interbase, MySQL, Oracle, Paradox, Microsoft SQL Server, SQL Server, SQL Server Azure, SQL Server Compact(SQLCE), SQLite, Delimited text files (CSV), XML and many more via ODBC."
54,Abris Platform
54,https://abrisplatform.com
54,"Web Application for Linux/Windows, requires Apache+PHP or Docker"
54,Abris Platform is an application development platform for creating Web-based front-ends for PostgreSQL databases. Can be used to quickly create applications with convenient forms via SQL declarative description.
54,Key features:
54,"Quick setup - Abris Platform provides built-in means for flexible data structures configuration (tables, fields and relations). This can be done during system initialization as well as during system expluatation."
54,Single page application - Related entities and tables are instantly accessible for view and edit on the same screen.
54,Search - Use general in-table search and complex column filters.
54,"Charts - Data can be represented via charts of different types: bar charts, pie charts, lines any many others."
54,Maps - Build-in support for geo data. Abris platform allow to vizualize geo-data event in real-time using OpenStreet Map package.
54,"Reporting - Filter settings can be saved as a report description and data can be exported in the following formats: HTML, PDF, Excel."
54,Data import - Insert data in the current open list view from the computer clipboard.
54,"Notifications - Notification pool, that can be filled in PostgreSQL functions."
54,"Administrative tool - Built in administrative tools take care of user management, activity monitoring and auditing and allow to configure user/group access policy on the table and field database level."
54,Replicator Pro
54,https://www.spectralcore.com/replicator
54,"Replicator allows table data comparison and sync - even with heterogeneous databases. It is unique in the fact it can replicate changes only even if source is non-relational (CSV, DBF, Excel documents, Paradox...). Replicator has a built-in scheduler for easy periodic change replication."
54,DBTools Manager
54,http://www.dbtools.com.br
54,Windows
54,Admin
54,"Freeware, available for PostgreSQL and MySQL, allows managing all aspects of the database: db, table, triggers, functions, etc. Includes import/export wizards to migrate data and structure to/from other database engines. Developed by DBTools Software."
54,PostgreSQL PHP Generator
54,http://www.sqlmaestro.com/products/postgresql/phpgenerator/
54,Windows
54,"PostgreSQL PHP Generator is a freeware but powerful PostgreSQL GUI frontend that allows you to generate high-quality PHP scripts for the selected tables, views and queries for the further working with these objects through the web."
54,ConvertDB for PosttgreSQL
54,http://convertdb.com/postgresql has PostgreSQL export/ import tools
54,Windows
54,"The software is able to connect to remote PostgreSQL 9.x/7.4 located on Linux, Solaris, Mac OS X, and Windows."
54,"ConvertDB cross-database migration tools assist in data conversion and synchronization among PostgreSQL, MySQL, MS SQL Server, MS Windows SQL Azure,"
54,and MS Access databases
54,1 Million of records can be transferred in 5-10 minutes.
54,"Bi-directional synchronization between PostgreSQL, MS SQL, MySQL and Oracle"
54,Scheduling migration and synchronization jobs.
54,dotConnect for PostgreSQL
54,https://www.devart.com/dotconnect/postgresql/
54,Windows
54,"dotConnect for PostgreSQL, formerly known as PostgreSQLDirect .NET, is an enhanced ORM enabled data provider for PostgreSQL that builds on ADO.NET technology to present a complete solution for developing PostgreSQL-based database applications. It introduces new approaches for designing application architecture, boosts productivity, and leverages database applications."
54,Key features:
54,Direct Mode
54,Database Application Development Extension
54,PostgreSQL Advanced Features Support
54,Optimized Code
54,ORM Support
54,BIS Support
54,Devart PostgreSQL Data Access Components
54,Windows
54,https://www.devart.com/pgdac/
54,"PostgreSQL Data Access Components (PgDAC) is a library of components that provides native connectivity to PostgreSQL from Delphi, C++Builder, Lazarus (and Free Pascal) on Windows, Mac OS X, iOS, Android, Linux, and FreeBSD for both 32-bit and 64-bit platforms. PgDAC is designed to help programmers develop really lightweight, faster and cleaner PostgreSQL database applications without deploying any additional libraries."
54,Native Connectivity to PostgreSQL
54,PgDAC is a complete replacement for standard PostgreSQL connectivity solutions and presents an efficient alternative to the Borland Database Engine (BDE) and standard dbExpress driver for access to PostgreSQL. It provides direct access to PostgreSQL without PostgreSQL Client.
54,Devart ODBC Driver for PostgreSQL
54,https://www.devart.com/odbc/postgresql/
54,Windows
54,"Devart ODBC Driver for PostgreSQL provides high-performance and feature-rich connectivity solution for ODBC-based applications to access PostgreSQL databases from Windows, both 32-bit and 64-bit. Full support for standard ODBC API functions and data types implemented in our driver makes the interaction of your database applications with PostgreSQL fast, easy and extremely handy."
54,Devart Excel Add-in for PostgreSQL
54,https://www.devart.com/excel-addins/postgresql.html
54,Windows
54,"Devart Excel Add-in for PostgreSQL allows you to quickly and easily connect Microsoft Excel to PostgreSQL, load data from PostgreSQL to Excel, instantly refresh data in an Excel workbook from the database, edit these data, and save them back to PostgreSQL. It enables you to work with PostgreSQL data like with usual Excel worksheets, easily perform data cleansing and de-duplication, and apply all the Excel's powerful data processing and analysis capabilities to these data."
54,EMS Database Management Tools for PostgreSQL
54,http://www.sqlmanager.net/en/products/postgresql
54,Windows
54,PostgreSQL Tools Products Family:
54,EMS SQL Manager for PostgreSQL see PostgreSQL Clients.
54,"SQL Management Studio for PostgreSQL - a single workbench for administering PostgreSQL databases, managing database schema and objects as well as for database design, migration, extraction, query building, data import, export, and database comparison."
54,"SQL Manager for PostgreSQL - high performance graphical tool for PostgreSQL database administration and development. It makes creating and editing PostgreSQL database objects easy and fast, and allows you to run SQL scripts, visually design databases, build SQL queries, extract, print and search metadata, import and export PostgreSQL database data and much more."
54,"Data Export for PostgreSQL - tool to export PostgreSQL database data quickly to any of 19 available formats, including MS Access, MS Excel, MS Word, RTF, HTML, TXT, ODF and more. Data Export for PostgreSQL has a kata kata lucu friendly wizard, which allows you to set various options of PostgreSQL export process visually and a command-line utility to automate your PostgreSQL export jobs using the configuration file."
54,"Data Import for PostgreSQL - tool to import data to PostgreSQL tables from MS Excel 97-2007, MS Access, DBF, TXT, CSV, MS Word 2007, RTF, ODF and HTML files. This utility allows you to quickly import data to one or several PostgreSQL tables or views at once, save all PostgreSQL import parameters set on current wizard session, use special batch insert mode to import PostgreSQL data at the maximum possible speed and much more."
54,"Data Pump for PostgreSQL - migration tool for converting databases and importing table data from an ADO-compatible source (e.g. MS Access, MS SQL database or any other database with ADO support) to PostgreSQL databases."
54,"Data Generator for PostgreSQL - tool for generating test data to PostgreSQL database tables. The utility can help you to simulate the database production environment and allows you to populate several PostgreSQL database tables with test data simultaneously, define tables for generating data, set value ranges, control a wide variety of generation parameters for each field type and much more."
54,DB Comparer for PostgreSQL - a tool for comparing PostgreSQL database schemas and discovering differences in their structures. You can view all the differences in compared database objects and execute an automatically generated script to synchronize structure of PostgreSQL databases and eliminate these differences.
54,DB Extract for PostgreSQL - easy-to-use tool for creating PostgreSQL database backups in a form of SQL scripts. This database script utility allows you to save metadata of all PostgreSQL database objects as well as PostgreSQL table data as database snapshots.
54,"SQL Query for PostgreSQL - a useful tool that lets you quickly and simply build SQL queries to PostgreSQL databases. Visual PostgreSQL query building, as well as direct editing of a query text, is available."
54,Data Comparer for PostgreSQL - tool for PostgreSQL data comparison and synchronization. Using this utility you can view all the differences in compared PostgreSQL tables and execute an automatically generated script to eliminate these differences.
54,SQL Maestro Group products for PostgreSQL
54,http://www.sqlmaestro.com/products/postgresql/
54,Windows
54,SQL Maestro Group offers a number of tools for PostgreSQL.
54,Maestro for PostgreSQL see PostgreSQL Clients.
54,"PostgreSQL Data Wizard provides you with a number of easy-to-use wizards to transfer any database to PostgreSQL, export data from PostgreSQL tables, views and queries to most popular formats, and import data from various sources into PostgreSQL tables."
54,PostgreSQL Code Factory is a
54,GUI tool aimed at the SQL queries and scripts development.
54,PostgreSQL Data Sync is a powerful and easy-to-use tool for database contents comparison and synchronization.
54,PostgreSQL PHP Generator Professional is a frontend that allows you to generate high-quality PHP applications for your database in a few mouse clicks.
54,"SQL Maestro Group also produces similar tools for MySQL, Oracle, MS SQL Server, SQLite, Firebird, DB2, SQL Anywhere, and MaxDB."
54,Datanamic DataDiff for PostgreSQL
54,http://www.datanamic.com/datadiff-for-postgresql/
54,Windows
54,"Datanamic DataDiff for PostgreSQL is a utility for data comparison and synchronization. Compare data for selected tables in two databases, view differences and publish changes quickly and safely. Flexible comparison and synchronization settings will enable you to set up a customized comparison key and to select tables and fields for comparison and for synchronization."
54,"DB Data Difftective can be used for data migrations, verification of (corrupt) data, data auditing etc."
54,Datanamic SchemaDiff for PostgreSQL
54,http://www.datanamic.com/schemadiff-for-postgresql/index.html
54,Windows
54,"Datanamic SchemaDiff for PostgreSQL is a tool for comparison and synchronization of database schemas. It allows you to compare and synchronize tables, views, functions, sequences (generators), stored procedures, triggers and constraints between two databases."
54,DB MultiRun PostgreSQL Edition
54,http://www.datanamic.com/multirun/index.html
54,Windows
54,DB MultiRun is a simple tool to execute multiple SQL scripts on multiple databases quickly.
54,"Define a list of databases, add SQL scripts to execute on these databases and click ""execute"" to run those scripts on the databases in the list. The multi-threaded execution of the SQL scripts makes it complete the task fast. After execution of the scripts, you can examine the results of the executed scripts on each database."
54,DB Doc for PostgreSQL
54,https://www.yohz.com/dbdoc_details.htm
54,Windows
54,"DB Doc helps you document your database schema and generate shareable PDF, HTML, XML, and Microsoft Word document in only 5 steps."
54,"Furthermore, the layout of the generated documents are customizable."
54,SQL Blob Export
54,http://www.yohz.com/sbe_details.htm
54,Windows
54,SQL Blob Export exports unlimited images and files from your tables or queries in 5 simple steps.
54,SQL File Import
54,http://www.yohz.com/sfi_overview.htm
54,Windows
54,"SQL File Import allows you to upload files, images, and other data into your database, without having to write any SQL statements."
54,"SQL File Import supports PostgreSQL, Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL)."
54,A scripting engine allows you to transform data before importing them into your database.
54,A command line version is also included to allow you to perform unattended upload/import tasks.
54,SQL Image Viewer
54,http://www.yohz.com/siv_details.htm
54,Windows
54,"SQL Image Viewer allows you to retrieve, view, convert and export images stored in Firebird, MySQL, Oracle, SQLite, SQL Server, and various ODBC-supported databases (e.g. DB2 and PostgreSQL). It supports the following image formats: BMP, GIF, JPG, PNG, PSD, and TIFF."
54,"It also allows you to export binary data and recognises the following binary file types: PDF, MP3, WAV, 7Z, BZ2, GZ, RAR, ZIP, and has experimental support for DOC, PPT and XLS file types."
54,A command line version is also included to allow you to perform unattended scheduled exports of binary data.
54,SQL Multi Select
54,http://www.yohz.com/sms_details.htm
54,Windows
54,SQL Multi Select is a query tool that allows you to run multiple scripts on multiple servers with a single click.
54,"Result sets from different servers are consolidated into a single view, allowing for easy comparison and analysis."
54,Devart SSIS Data Flow Components for PostgreSQL
54,https://www.devart.com/ssis/
54,Windows
54,Devart SSIS Data Flow Components for PostgreSQL allow you to integrate database and cloud data via SQL Server Integration Services (SSIS).
54,"Devart SSIS Data Flow Components provide easy to set up cost-effective data integration using SSIS ETL engine. They provide high performance data loading, convenient component editors, SQL support for cloud data sources and lots of data source specific features."
54,EDB Postgres Enterprise Manager
54,http://www.enterprisedb.com/products/postgres-enterprise-manager
54,"Windows, Mac OS X, Linux"
54,"Postgres Enterprise Manager is the only solution available today that allows you to intelligently manage, monitor, and tune large scale Postgres installations from a single GUI console."
54,"Monitoring features include: server auto-discovery, over 225 pre-configured ready to run probes, custom probes, alert management, personalized alerts, remote monitoring, versatile charting, custom dashboards and web client."
54,"DBA tools include: database objects management, Postgres Expert (best practice configuration settings), Audit Manager, Log Manager, Log Analysis Expert, Capacity Manager and Team Support."
54,"Developer tools include: Query Tool, Data Grid, SQL Profiler, SQL Debugger and Import tools."
54,"Tuning tools include: At-A-Glance performance dashboards, Tuning Wizard, Performance Diagnostics and Index Advisor."
54,ClusterControl by Severalnines
54,https://severalnines.com/product/clustercontrol/for_postgresql
54,"ClusterControl is an all-inclusive open source database management system that allows you to deplore, monitor, manage and scale your database environments. ClusterControl provides the basic functionality you need to get PostgreSQL up-and-running using our deployment wizard, monitoring and basic management abilities like automatic failover, backups, and restores."
54,Point and Click Replication Deployments - ClusterControl allows you to easily deploy and configure master/slave replication PostgreSQL instances.
54,Advanced Performance Monitoring - ClusterControl monitors queries and detects anomalies with built-in alerts.
54,Automated Failover Handling - ClusterControl detects master failures and automatically promotes a new master
54,"Database Automation - ClusterControl lets you manage configurations, schedule, and restore backups."
54,Reportizer
54,https://www.reportizer.net
54,"Reportizer is a database reporting tool, which allows easy creating, modifying, and printing database reports from different types of databases, including PostgreSQL. Reports can be edited in convenient visual report builder or in text mode. It supports calculating fields, multi-column reports, expressions, grouping, displaying images etc. Reportizer can export reports to HTML, XLSX, image, or internal format. There is an ability to load and print reports from command line. Reportizer allows to manage report collections, which can be held either in files or in database tables."
54,Exportizer Enterprise
54,https://www.vlsoftware.net/exportizer/
54,"Exportizer Enterprise is a database export tool, which can work with PostgreSQL database either as source or destination. It allows to export data to database, file, clipboard, or printer."
54,"Possible sources: ODBC data sources, files of DB (Paradox), DBF (dBase, FoxPro), MDB, ACCDB, XLS, XLSX, GDB, IB, FDB, HTML, UDL, DBC, TXT, CSV types, databases specified by ADO connection strings, and databases like Oracle, SQL Server, PostgreSql, DB2, Informix, SQLite, Interbase etc."
54,"Possible destinations: file formats like text, CSV, XLS, XLSX, RTF, XML, HTML, PDF, DBF, SLK, SQL script, and relational database of any supported type including PostgreSQL."
54,It is possible to export all or selected tables from an open database at once.
54,The data migration can be done in super-fast batch mode.
54,"Exportizer Enterprise can automatically detect the most known image types (JPEG, PNG, GIF, BMP, ICO) in BLOB fields and export them, for example, to HTML or XLSX."
54,Images and other BLOB data can be exported to multiple separate files during a single export operation.
54,There is an ability to specify the source-to-target field mappings.
54,Export operations can be performed either via the program interface or via command line.
54,TiCodeX SQL Schema Compare
54,https://www.ticodex.com/
54,TiCodeX SQL Schema Compare is a tools that allows database administrators to compare multiple database schema in order to manage versioning.
54,"The software runs on Windows, Linux and Mac and supports Microsoft SQL (MS-SQL), MySQL, PostgreSQL, Azure SQL and MS-SQL on Amazon RDS."
54,Key Features:
54,"Runs on Windows, Linux and MacOS"
54,"Localized in English, German and Italian"
54,Compare changes between two SQL Database schemas (as example from development to test to production)
54,View database differences and explore schema changes to see what's going on
54,Automatically create full database migration scripts
54,Securely save database and server login details
54,pgMustard
54,https://www.pgmustard.com/
54,"pgMustard is a performance tool for PostgreSQL that provides a user interface for your explain analyze output, as well as tips on what to do to speed up your query."
54,Features:
54,Performance advice – scored by estimated time-saving potential
54,Per-operation timings¹
54,The number of rows¹ returned by each operation
54,A timing bar – that pins to keep it in context
54,Collapsible subtrees – with fast ones collapsed by default
54,Descriptions of operations
54,Links to blog posts for further reading
54,"¹ Taking loops, threads, subplans, and CTEs into account."
54,Requirements:
54,"TEXT or JSON format query plans, from PostgreSQL 9.6 or newer"
54,The interface and advice are in English
54,"A GitHub or Google account, for signing in"
54,"Web application, no installation required"
54,ODBC Driver for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/odbc/
54,"The CData ODBC Driver for PostgreSQL enables real-time access to PostgreSQL data, directly from any applications that support ODBC connectivity, the most widely supported interface for connecting applications with data. The driver wraps the complexity of accessing PostgreSQL data in a standard ODBC driver compliant with ODBC 3.8. The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
54,JDBC Driver for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/jdbc/
54,"The CData JDBC Driver for PostgreSQL offers the most natural way to connect to PostgreSQL data from Java-based applications and developer technologies. The driver wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, 100%-Java JDBC driver."
54,"The driver hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
54,ADO.NET Provider for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/ado/
54,"The CData ADO.NET Provider for PostgreSQL offers the most natural way to access PostgreSQL data from .NET applications. The provider wraps the complexity of accessing PostgreSQL data in an easy-to-integrate, fully managed ADO.NET Data Provider."
54,"The provider hides the complexity of accessing data and provides additional powerful security features, smart caching, batching, socket management, and more."
54,Excel Add-In
54,for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/excel/
54,"The CData Excel Add-In provides the easiest way to connect to PostgreSQL data from Excel. From the CData ribbon, you can select PostgreSQL data as tables and columns into the spreadsheet. The spreadsheet is then linked with the remote data. To update the data, edit the spreadsheet."
54,SSIS Components for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/ssis/
54,The CData SSIS Components for PostgreSQL enable you to connect SQL Server with PostgreSQL data through SSIS Workflows. The components wrap the complexity of accessing PostgreSQL data in standard SSIS data flow components. You can then connect and synchronize PostgreSQL tables with SQL Server tables.
54,"The components hide the complexity of accessing data and provide additional security features, smart caching, batching, socket management, and more."
54,Power BI Connector for PostgreSQL by CData
54,https://www.cdata.com/drivers/postgresql/powerbi/
54,The CData Power BI Connector for PostgreSQL offers self-service integration with Microsoft Power BI. The connector facilitates live access to PostgreSQL data in Power BI from the Get Data window. The connector also provides direct querying to visualize and analyze PostgreSQL data.
54,Other Resources
54,"PostgreSQL Clients - list of UI's for accessing your db contents (enter sql, get back the results)"
54,Design Tools - tools for designing and visualizing database schemas
54,"Old possibly abandoned projects, see Community_Guide_to_PostgreSQL_Tools_Abandoned"
54,"Retrieved from ""https://wiki.postgresql.org/index.php?title=Community_Guide_to_PostgreSQL_GUI_Tools&oldid=38452"""
54,Navigation menuPage actionsPageDiscussionView sourceHistoryPage actionsPageDiscussionMoreToolsIn other languagesPersonal toolsLog inNavigationMain PageRandom pageRecent changesHelpToolsWhat links hereRelated changesSpecial pagesPrintable versionPermanent linkPage informationSearch
54,"This page was last edited on 1 December 2023, at 19:59.Privacy policyAbout PostgreSQL wikiDisclaimers"
55,"Ask HN: It's 2023, how do you choose between MySQL and Postgres? | Hacker News"
55,Hacker News
55,new | past | comments | ask | show | jobs | submit
55,login
55,"Ask HN: It's 2023, how do you choose between MySQL and Postgres?"
55,219 points by debo_ 10 months ago
55,| hide | past | favorite | 347 comments
55,"Imagine you are starting a new project, and your backing datastore options are restricted to mysql or postgres (and/or their cloud-tailored equivalents.) What sort of functional requirements would cause you to choose one over the other?"
55,gmac 10 months ago
55,| next [–]
55,"Postgres. Fast, full-featured, rock-solid, and a great community.I think many of us can’t be bothered to go over (again) the issues we’ve had with MySQL in the past. The last straw for me was about ten years ago, when I caught MySQL merrily making up nonsense results for a query I’d issued that accidentally didn’t make any sense.Very likely this particular issue, and others like it, have been fixed in the meantime. But I just got the sense that MySQL was developed by people who didn’t quite know what they were doing, and that people who really did know what they were doing weren’t ever likely to be attracted to that to fix it."
55,paulddraper 10 months ago
55,| parent | next [–]
55,"FWIW, it hasn't changed in ten years.Here is an 18-year-old bug, that DELETE triggers don't work for foreign key cascades: https://bugs.mysql.com/bug.php?id=11472That makes the entire feature mostly worthless. Reported in 2005, last updated in 2008.---While I would choose PostgreSQL every time, MySQL has the following advantages:1. Write-performance, due to fundamental design tradeoffs. [1]2. Per-connection resources, due to single-process design.3. Related to #1, no vacuum requirement.[1] https://www.uber.com/blog/postgres-to-mysql-migration/"
55,digitalpacman 10 months ago
55,| root | parent | next [–]
55,Good you shouldn't be using them. You shouldn't be using foreign keys either. It just makes working with data harder and doesn't help with constraining it if your data modifications are inside transactions and properly written statements.
55,kiernanmcgowan 10 months ago
55,| parent | prev | next [–]
55,"Having used postgres for the past decade, I tried MySQL for a side project to see whats changed with it. The sad answer is that it feels like nothing has changed - Oracle seems to have let what used to be a core technology of the industry languish.I'm sure there are use cases where MySQL will be the better choice over postgres, but the future for the stack looks bleak."
55,JohnBooty 10 months ago
55,| root | parent | next [–]
55,Oracle seems to have let what used to be a core
55,technology of the industry languish
55,"I think slowly squeezing the life from MySQL was a very explicit goal for them. After the big wins (Wal-Mart, etc) MySQL had 15-20 years ago I think it was very clear MySQL was going to chip away at more and more of Oracle's business.I wonder how much Oracle spends on MySQL every year? They're spending a lot of money to keep MySQL at kind of a ""not quite good enough"" state. But they can't kill it outright - it'd be like boiling a frog fast instead of slow.In the end, I wonder what extinguishing MySQL really accomplished for them. It might have bought them some breathing room but Postgres quickly filled MySQL's old segment."
55,srcreigh 10 months ago
55,| root | parent | next [–]
55,"Strange take when FB, twitter, Square and new startups such as Faire(#4 valued private YC co) are all using MySQL to some/large degree. Stripe uses MySQL too in combination with other DBs including Postgres."
55,JohnBooty 10 months ago
55,| root | parent | next [–]
55,"You're not considering the timeline of events.I'm unfamiliar with Faire but the rest were already using MySQL at the time of Oracle's acquisition in 2010. Switching backends would have been rough for those companies and this was... 2010, meaning Postgres was not nearly as performant or full featured as it is today. As mentioned in other comments, FB's investment in customizing MySQL has been extensive. They've poured a lot into their own fork of it.More to the point, look at MySQL's progress since 2010. Do you think it has been largely stifled since then, or do you think it has kept pace with Postgres? It's been largely stagnant.I'd love to hear your alternative theory, of course. You think Oracle bought MySQL to... what, exactly? Make it amazing?"
55,connormcd 10 months ago
55,| root | parent | next [–]
55,"(Full disclosure, I work for Oracle)Anyone who says no investment has been into MySQL I suspect never took the time to read the features/release notes for MySQL 8https://dev.mysql.com/doc/refman/8.0/en/mysql-nutshell.html"
55,tapoxi 10 months ago
55,| root | parent | next [–]
55,Didn't 8 ship 5 years ago?
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Their release model changed with MySQL 8 -- they do rolling point releases every quarter with new features sprinkled in as they're ready. Quite a few new major features have been released that way, including INSTANT alters, parallel index builds, CLONE plugin, major changes to how UNDO logs are sized... it's more like Windows 10's release model.Very recently they've mentioned they'll be changing this again to have separate LTS releases, which is a positive change stability-wise."
55,ksec 10 months ago
55,| root | parent | next [–]
55,Really wish they bump up the version number or something. It makes discussions with MySQL a lot easier.
55,JohnBooty 10 months ago
55,| root | parent | prev | next [–]
55,"Well, you got me there. It stagnated for so long relative to others that I didn't realize the pace had picked up in a big way."
55,srcreigh 10 months ago
55,| root | parent | prev | next [–]
55,"Imo mysql is decent enough for many years, and PG has relatively major outstanding architecture problem (see my Other comment in this thread)I don’t think either DB is bad, but the whole MySQL is dead and needs serious work idea doesn’t make sense to me. What problem does MySQL still have that needs fixing in your opinion?"
55,evanelias 10 months ago
55,| root | parent | prev | next [–]
55,"Facebook maintains a patch-set, not a full fork. They still track Oracle's releases and apply their patches on top.Facebook definitely has the resources to migrate to Postgres, if there was any motivating reason to do so. Indeed, Facebook developed their own LSM-based key-value store (RocksDB) and MySQL storage engine based on it (MyRocks) and then migrated the majority of their db fleet to that. In total that's massively more work than migrating to Postgres would have been.Part of the reason was that MyRocks provides better compression than pretty much any other possible choice, and that adds up to an astronomical amount of cost savings at Facebook's scale. In contrast, what would Facebook have gained from moving to Postgres instead?"
55,callalex 10 months ago
55,| root | parent | prev | next [–]
55,You are looking for long term planning where there is none. The only relevant question for those in charge of the project is: “How do I make my quarterly report to investors look slightly better than last quarter’s?”
55,dunno7456 10 months ago
55,| root | parent | prev | next [–]
55,Creating a series of connections very quickly is cheaper in MySQL and MariaDB than in PostgreSQL.
55,"Typically, a connection poller is used before PostgreSQL to support connection scalability.I'm not sure if there has been a recent breakthrough that has changed that."
55,I think that still applies today. Correct me if I'm wrong.
55,reactordev 10 months ago
55,| root | parent | next [–]
55,"You can create a series of connections in postgres just as fast. The connection pooler you are referring to is when you put pgBounce or pgPool in between your pgdb and your client software to expand beyond the physical limits of connections and optimize clustered architectures. MySQL at scale is replication only. A few commercial offerings for MySQL like planetscale have brought MySQL into the 21st century. Postgres has a couple ways of clustering, sharding, scaling, beyond your Wordpress database."
55,OJFord 10 months ago
55,| root | parent | prev | next [–]
55,"It took me until here to realise we were talking about MySQL, not SQLite, because honestly 'in 2023' isn't that the comparison, pg vs sqlite?"
55,gymbeaux 10 months ago
55,| root | parent | prev | next [–]
55,"Actually I would argue that there isn’t a single reason to use MySQL over Postgres (barring the obvious- it’s what the team already knows, or the company already uses, etc.)"
55,davidgerard 10 months ago
55,| root | parent | prev | next [–]
55,"> I'm sure there are use cases where MySQL will be the better choice over postgres, but the future for the stack looks bleak.see, I'm pretty sure there basically weren't. It lucked out at the right moment in the late 1990s. Also, Slashdot used it.The only use case I can think of is when you want an application, and it requires or is highly optimised to MySQL. Otherwise, it should actively be avoided."
55,johnny22 10 months ago
55,| root | parent | prev | next [–]
55,i think one is also referring to mariadb here and not just mysql.
55,"Maybe that's better enough? I wouldn't know, I just go with postres."
55,stouset 10 months ago
55,| parent | prev | next [–]
55,"Yep. The real question here is: it's 2023, why would you choose MySQL over PostgreSQL?Not that there aren't reasons. There are some. But for starting out with a new app without a very, very good reason to do something different? PostgreSQL every day of the week."
55,mardifoufs 10 months ago
55,| root | parent | next [–]
55,Ease of updates is a very good reason. Handling connections too.
55,jesterson 10 months ago
55,| parent | prev | next [–]
55,"Working with MySQL (MariaDB, but doesn't make much difference). Never get any issues that couldn't be explained by architectural or development mistakes.Just as example - how do you create read-only user (SELECT only) in Postgres? In MySQL it's extremely simple and it works, while in Postgres it's a nightmare to create and maintain"
55,Someone 10 months ago
55,| root | parent | next [–]
55,"> Just as example - how do you create read-only user (SELECT only) in Postgres? In MySQL it's extremely simple and it works, while in Postgres it's a nightmare to createIsn’t that"
55,GRANT SELECT ON ALL TABLES IN SCHEMA foo TO bar;
55,?> and maintainIf you mean you want to grant a user select rights to whatever table gets created in the future (a somewhat questionable idea from a security viewpoint):
55,ALTER DEFAULT PRIVILEGES IN SCHEMA foo GRANT SELECT ON TABLES TO bar;
55,"I think both are possible in PostgreSQL 9 and later (https://www.postgresql.org/docs/9.0/sql-grant.html , https://www.postgresql.org/docs/9.0/sql-alterdefaultprivileg...)That version is from 2010.I guess the lesson is that both these systems evolve fairly rapidly. You can’t use quirks you remember from over 5 years ago to judge their current versions."
55,jesterson 10 months ago
55,| root | parent | next [–]
55,It is correct but apparently not sufficient - you need to give CONNECT access to user profile.
55,2muchcoffeeman 10 months ago
55,| parent | prev | next [–]
55,"It’s really unfair because a lot may have changed in 10 years so it might be worth reconsidering.But I’m like you, MySQL did some nonsense once that took me hours to work out. So now I really can’t be bothered with any potential quirks it may still have. This is not an SNL sketch."
55,donatj 10 months ago
55,| prev | next [–]
55,"Unpopular opinion on HN apparently, but MySQL- It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.- Maintenance is simpler and far less necessary in my general experience.- Replication is simpler and more reliable.- You can tell the query optimizer what to do. When this is needed, you'll be thankful. It's a godsend.That said, I wouldn't run Oracle MySQL. I opt for MariaDB on smaller projects and AWS Aurora MySQL for larger projects. Aurora scales insanely well, and replication lag is almost non-existent.In my general experience MySQL was always significantly faster but it's been a number of years since I've worked with Postgres and the comments here seem to indicate that that may no longer be the case. YMMV"
55,stouset 10 months ago
55,| parent | next [–]
55,"> It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.This doesn't really hold water in my opinion.It's not like PostgreSQL is some minefield of misfeatures and quirky behavior. Some of these features exist, but have zero impact on you unless you actually opt to use them. But if you end up needing to: they're there, and you can just start using them.Compare this to MySQL where they simply don't exist no matter how much you may need them. Need to apply an index to the result of a function to quickly fix a performance issue in prod? Sorry, you can't. Need window functions to accurately compute some analytics in a sane period of time? Sorry, you can't. The list of things you can do in PostgreSQL that you simply can't with MySQL is massive and grows every day.The odds that you'll want, need, or greatly benefit at least one of these features is not small. Having the flexibility of knowing these features exist should you ever have a use-case for them is massive."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Your examples regarding MySQL's features are not correct.Need to apply an index to the result of a function? No problem, use a functional index, supported since October 2018: https://dev.mysql.com/doc/refman/8.0/en/create-index.html#cr...Need to use a window function? No problem, supported since April 2018: https://dev.mysql.com/doc/refman/8.0/en/window-functions.htm..."
55,stouset 10 months ago
55,| root | parent | next [–]
55,Fair. I picked two instances that had long been personally painful with MySQL and didn’t exist when I finally fled to greener pastures.There are far more examples than just those two though. :)
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Sure, and there are also plenty of examples of things MySQL can do that Postgres can't.There's no silver bullets in software or databases, no ""one size fits all"". Just various trade-offs between different design decisions."
55,0xf8 10 months ago
55,| root | parent | next [–]
55,"I mean, for most mature ecosystems / technology stacks I think the “no one size fits all, just strategic design trade-offs” theory largely holds true as an"
55,objectively accurate account of the “choosing the best tool“ situation
55,"….but IMO, I think it’s worth noting that occasionally, some rare unique and functionally superlative technology comes along that in practice transcends every alternative from the onset and indefinitely going forwards, sometimes even at a more prominent scale than the MySQL / Postgres projects topic of discussion (which are not small by any means).something maybe like Git, most immediately comes to mind, as an example of the de-facto standard for distributed VCS basically since … 2005* when Linus decided to create it?edit: not 1995"
55,erhaetherth 10 months ago
55,| root | parent | next [–]
55,"> something maybe like Git, most immediately comes to mind, as an example of the de-facto standard for distributed VCS basically since … 1995 when Linus decided to create it?hot take. might be the most popular, maybe even by a large margin, but I think you'll find a good chunk of people who have actually tried different VCSs don't think it's the best."
55,0xf8 10 months ago
55,| root | parent | next [–]
55,"Yah, I think that’s probably true. But that’s also hard to reconcile with the reality of the adoption trending consistently away from any alternative and only towards Git. And “large margins” are indeed pretty objectively the case (from the largest developer surveys the breakdown 10 years ago was like 70% Git to everything, growing to ~95% in 2022). Usually the phenomenon you’re describing, leads to other alternatives becoming more popular not less (even if the most popular standard continues to eclipse the field. Here is would seem these highly likable alternatives for those who took the plunge are nevertheless dwindling into irrelevance…I suspect the die-hard proponents of Mercurial, or SVN, or whatever else, these few pagan heretics that might exist out there wherever they’re hiding, have found themselves in a camp different to the Git standard likely on the basis of electing to be intentionally contrarian / anti-normative as the general catalyst, and rather not, as a function of struggling with Git to the point of being so disillusioned they call it quits and head out looking for greener pastures."
55,"I think in practice the most common result of encountering problems with Git is, fix the problems. And functionally I think that’s resulted only in furthering it’s supremacy over alternatives, despite there existing a handful of cultish weirdos who are _really_ into Mercurial and prefer not to fux with Git as a personal lifestyle choice haha)"
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Giant companies like Google and Meta use alternatives to Git for their internal / closed-source code, because Git couldn't scale to their needs. If I recall correctly, Google transitioned from Perforce to an in-house system, and Meta transitioned from using Mercurial to a custom derivative of it.Again, no one size fits all..."
55,thfuran 10 months ago
55,| root | parent | prev | next [–]
55,Are you serious? Anyone who disagrees with you about what VCS is best must be intentionally contrarian?
55,0xf8 10 months ago
55,| root | parent | next [–]
55,"Haha. That’s a pretty aggressive and surface level read of my comment . For starters it was mostly a joke, and explicitly a conjecture. I was saying, probably most users of a different system do so on for some reason other than, Git being objectively inferior for their purposes (and as others in the thread pointed out, the latter actual is the case at the large scale end of the spectrum for big companies concerned with scaling, not choosing Git).Secondly, no one has disagreed with me on the matter, as I haven’t put forth a personal opinion, I’m simply impartially making referencing to the fact that Git is the de-facto standard. And from an intellectual perspective, was hoping someone might elucidate more into why that is the case, given my conception is an inferential deduction. at best).While I’m personally relatively familiar with Git internals, by no means an expert, its the only distributed VCS I’ve ever used and I don’t know anything substantive enough about the alternatives to credibly make a relative value comparison here. They could be the bees knees for all I know, but it seems unlikely given the position Git holds as far as consensus standard choice."
55,irrational 10 months ago
55,| root | parent | prev | next [–]
55,"What are the “plenty of examples of things MySQL can do that Postgres can’t”? I honestly can’t think of one, much less “plenty”."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"These are just a few random ones off the top of my head!* Handling several thousand connections per second without needing a proxy or pool (MySQL's connection model is thread/conn vs Postgres using process/conn)* Handle extremely high volume of primary key range scan queries (MySQL's InnoDB uses a clustered index, see https://news.ycombinator.com/item?id=35909053)* Handle workloads that lean heavily on UPDATE or DELETE, without major MVCC pain (see https://ottertune.com/blog/the-part-of-postgresql-we-hate-th... for example)* Semi-synchronous logical replication, for environments which cannot tolerate any data loss during failover; this ensures the statements have reached at least one replica but without the huge latency of synchronous replication* Use index hints, to ensure random index stats changes don't cause unexpectedly negative query plan adjustments (see discussion in subthread https://news.ycombinator.com/item?id=35909340)* Handle very high-volume OLTP workloads using direct I/O, since InnoDB's buffer pool design is completely independent of filesystem/OS caching* Achieve best-in-industry compression by using the MyRocks storage engine (MySQL's pluggable storage engine design has a lot of tradeoffs but it is inherently what makes this even possible)* Use UNSIGNED int types, to store twice as high max value in the same number of bytes, if you know negative numbers are not going to be present* Use case-insensitive or accent-insensitive collations out-of-the-box without having to monkey with confusing user-defined CREATE COLLATION* Ease-of-use commands like SHOW CREATE TABLE* Silly cosmetic things like the ability to reorder columns in an ALTER TABLE (see https://wiki.postgresql.org/wiki/Alter_column_position)* A tooling ecosystem which includes multiple battle-tested external online schema change tools, for safely making alterations of any type to tables with billions of rows"
55,bigblackrooster 10 months ago
55,| root | parent | next [–]
55,ratio
55,gymbeaux 10 months ago
55,| root | parent | prev | next [–]
55,"Yeah I can’t think of any. I mean if I had to come up with something, I would talk about tooling. MySQL Workbench (when it works) is by far the easiest to use and most feature-rich of all RDBMS.. MS. It runs on any OS too, which is a ding on SQL Server and not Postgres since Postgres’ is a web app- which has the limitations of a web app (no right click for example- which I know is possible in a web app, but last I checked not implemented for pgadmin)."
55,EamonnMR 10 months ago
55,| parent | prev | next [–]
55,I would disagree on maintenance being simpler. I have never had Postgres randomly munge a table and require me to run a command to fix it.
55,jjeaff 10 months ago
55,| root | parent | next [–]
55,"I have not had that happen in MySQL either, at least, not with innodb. what command would that be?I do remember getting bad tables with myisam tables a decade ago, sometimes after a bad shutdown."
55,EamonnMR 10 months ago
55,| root | parent | next [–]
55,I believe this was the command in question: https://dev.mysql.com/doc/refman/8.0/en/repair-table.html
55,michaelcampbell 10 months ago
55,| root | parent | prev | next [–]
55,"My needs are meager (simple CRUD, low volume), but I haven't had that happen in MySQL either, in over 15 years of running it in production.Not saying it can't happen, but I don't think it's a common occurrence."
55,EamonnMR 10 months ago
55,| root | parent | next [–]
55,WordPress may be uniquely bad.
55,rocho 10 months ago
55,| parent | prev | next [–]
55,"I had the misfortune of inheriting a MySQL 5.6 database and I had to manage it for a year and a half. It wasn't very big (less than 100GB), but:- for some reason it hang periodically and had to be restarted during the night for no apparent reason- compared to Postgres, the tooling is garbage (both for backups and even more for general database administration)- essential features are missing, the most important one of which, for me, was proper CSV import/export. CSV-related functionality is so broken and terribly inconvenient to use. In a specific case I had to write a program to export millions of records manually since MySQL could not generate correct CSV export due to some columns containing text with special characters, quotes, newlines. Any combination of the export parameters (""ENCLOSED BY"", ""ESCAPED BY"" and all the other garbage options) failed in one way or another. I even tried to use non-standard characters like \x15 and \r for column and row separation but even that failed. With Postgres, ""with csv header"" is simple and works every time.I also managed bigger Postgres databases (up to tens of terabytes) and never had the issues I encountered with MySQL."
55,drogus 10 months ago
55,| parent | prev | next [–]
55,"> It's less featureful, and I'd consider that a strong virtue in the YAGNI camp - less to go wrong, less mental overhead.Imagine when you actually need any of the features that PostgreSQL provides like pub/sub, logical replication, JSONB etc. With MySQL you might have to hack a solution that is much more complex or you have to set up an entirely separate tool. What I find nice with PostgreSQL is that for simple cases you can get away without a dedicated key/value store or a queue or a full text search engine. You can do a lot of these kinds of tasks with just a single database."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"> features that PostgreSQL provides like [...] logical replicationMySQL has offered logical replication for considerably longer than Postgres, and it's a substantially more configurable and mature implementation. MySQL's built-in replication has always been logical replication. It's a feature MySQL has had for 23 years -- built-in logical replication is literally one of the top reasons why all the biggest MySQL users chose MySQL originally!"
55,PedroBatista 10 months ago
55,| root | parent | prev | next [–]
55,"Correct, but as far as I know MySQL has those mentioned features but the overall management effort seems to be considerably lower."
55,tonymet 10 months ago
55,| parent | prev | next [–]
55,"less features, simpler admin, more compatibility, more familiarity."
55,I agree
55,droobles 10 months ago
55,| parent | prev | next [–]
55,samesies
55,pawelduda 10 months ago
55,| parent | prev | next [–]
55,"What's the ratio of solving DB perf issues by optimizing it and letting the planner do its work, to telling it what to do? For me it's like 1000:1.And that one case I remember was perfectly solvable the regular way, with a little more time."
55,viraptor 10 months ago
55,| root | parent | next [–]
55,"The problem is that when you you need to tell the planner what to do, you can do it in MySQL, but not postgres. Imagine you've got a production database with lots of traffic which suddenly can't handle anything because it inserted an extra row which tipped the balance and now takes seconds to process a common query.Do you know how to fix the table statistics quickly? Do you know how to change that query to force the execution plan you want? Do you know how long the solution will last until the stats change again?MySQL is a bit more predictable for this case and if things go really bad for some unexpected reason, one comment can fix it.I'm looking at it from ops perspective. The ratio during development doesn't matter that much - all issues are solvable at that stage. For me it's rather ""which situation would I rather be in at 3am""."
55,pawelduda 10 months ago
55,| root | parent | next [–]
55,"I googled a bit and had no idea how many questions about Postgres query planner going nuts are out there. I just imagined this is a problem that creeps over time (giving you time to notice and act in advance, assuming you have monitoring/alerts set up) rather than suddenly tipping the scale - though it probably can happen suddenly after large data import.Personally never ran into this with Postgres nor had anyone I know worry about it - the query planner was reliable for me in 99.99% of cases but yeah, I admit that it's a black box for me that I expect to take care of internals - hopefully it continues to do so, but I got to give it to MySQL for allowing to override it then."
55,ants_a 10 months ago
55,| root | parent | prev | next [–]
55,"There is the pg_hint_plan extension that gives most of what you would want in a hinting system.I think a better fix lies in the direction of making queries more of a first class object with options to nail down plans, add custom logic to pick plans dependent on parameter values, etc."
55,avinassh 10 months ago
55,| root | parent | next [–]
55,"I am on GCP/AWS, its not possible to use extensions. They allow only a few whitelisted extensions. If it is built into the DB, then I can use without any hassles."
55,jitl 10 months ago
55,| root | parent | prev | next [–]
55,"Postgres query planner suddenly deciding to do something silly in the middle of the night has taken Notion down a few times. It's quite annoying, and it's very frustrating to have no recourse."
55,jhas78asd 10 months ago
55,| root | parent | next [–]
55,Any posts on this? Are there bulk data loads that make table stats more stale and affect plans? I’m wondering what would suddenly make a plan selection change a lot that might be a contributing factor.
55,natmaka 10 months ago
55,| root | parent | next [–]
55,"> bulk data loads that make table stats more staleThis is the usual culprit (cure: ""ANALYZE ((tablename))"").Collecting more samples (ALTER TABLE SET STATISTICS...) may be useful.""Extended Statistics"" covers most(?) other cases:"
55,https://www.postgresql.org/docs/current/planner-stats.html#P...
55,https://www.postgresql.org/docs/current/multivariate-statist...
55,pawelduda 10 months ago
55,| root | parent | prev | next [–]
55,Interesting - how do you approach it when it happens and you're under time pressure to bring it back online - assuming you can't just fix query plan? I'd probably start by tweaking stats options and resetting them for problematic tables but don't have further ideas from the top of my head.
55,dijit 10 months ago
55,| prev | next [–]
55,"There is almost no good reason to choose MySQL over PostgreSQL for any operational reason, I did a deep dive many moons ago (before major improvements in performance to postgres) and people were saying that MySQL was faster. I found that not to be true and the differences have only gained even more favour towards postgres.also, I assume you mean MariaDB as MySQL is owned by Oracle and I would greatly implore anyone and everyone to avoid Oracle as if it has herpes.There are a lot of historic problems with MySQL accepting invalid data, committing data even when there are constraint issues, and having very poor transactional isolation, I am not sure if these have improved.Truthfully, the only benefits you gain from using MariaDB or MySQL are:* Memory tables* Having inconsistent replicas (which can be useful when you want your downstream to have less data than your upstream and you know it won’t get updated.)"
55,0xbadcafebee 10 months ago
55,| parent | next [–]
55,"> avoid Oracle as if it has herpesherpes isn't that bad. most people will get it in their lifetime. 1 in 6 people have hsv-2, the less common variant. trying to avoid herpes is like trying to avoid chickenpox (although herpes isn't nearly as harmful as chickenpox).you should avoid Oracle like it's a blood pathogen."
55,hamilyon2 10 months ago
55,| root | parent | next [–]
55,"As a person who has herpes firmly in his nerves, I would say don't underestimate herpes."
55,mxvanzant 10 months ago
55,| root | parent | next [–]
55,I read this guy's book and he has some good ideas (and references to back them up). You don't need to get his book as his website also has all of that material+. http://doctoryourself.com/herpes.htmlAnother related site: http://orthomolecular.org/resources/omns/index.shtml (scroll down for articles).None of the above will help though in deciding between Postgres or MySQL ;)
55,scotty79 10 months ago
55,| root | parent | prev | next [–]
55,Suddenly the analogy got very accurate.
55,soperj 10 months ago
55,| root | parent | prev | next [–]
55,Chickenpox is actually caused by a herpesvirus.
55,herpes varicella zoster.
55,unethical_ban 10 months ago
55,| root | parent | prev | next [–]
55,"hello, fellow person with herpes! (I assume)The worst part about having it is having to talk about having it. It's really not bad as a condition separate from societal concern."
55,stavros 10 months ago
55,| root | parent | next [–]
55,I find similar societal concern when I tell friends I use Oracle.
55,r2_pilot 10 months ago
55,| root | parent | next [–]
55,"Having Oracle experience on the resume is a positive, I suppose, but I'm not sure it's been worth the exposure."
55,enneff 10 months ago
55,| root | parent | prev | next [–]
55,It’s not so bad for most people but if you’re one of the unfortunate few who suffer chronic symptoms it can be truly awful. Not worth playing that lottery if you can avoid it.
55,za3faran 10 months ago
55,| root | parent | prev | next [–]
55,> most people will get it in their lifetimeCitation needed.
55,yakshaving_jgt 10 months ago
55,| root | parent | next [–]
55,Here you go.https://www.who.int/news/item/28-10-2015-globally-an-estimat...
55,za3faran 10 months ago
55,| root | parent | next [–]
55,"Probably in certain countries, given that it is mainly an STD. There are many conservative nations where this won't be an issue hopefully."
55,yakshaving_jgt 10 months ago
55,| root | parent | next [–]
55,"As it says in the linked article, it’s a global epidemic.If you are an adult of typical sexual activity, it is likely you have already had sex with someone infected with herpes.That doesn’t mean you have contracted it — carriers aren’t always shedding the virus.I’m not sure I see any correlation between a country being conservative and an absence of sexually transmitted infection; the 10 countries where HIV is most prevalent are all (as far as I’m aware) relatively conservative.Furthermore, here are some statistics from Wikipedia on HSV which may be referring to some of these conservative countries you’re referring to:> Turkey— High levels of HSV-1 (97%) and HSV-2 (42%) were found amongst pregnant women in the city of Erzurum in Eastern Anatolia Region, Turkey. In Istanbul however, lower HSV-2 seroprevalence was observed; HSV-2 antibodies were found in 4.8% of sexually active adults, while HSV-1 antibodies were found in 85.3%. Only 5% of pregnant women were infected with HSV-2, and 98% were infected with HSV-1. Prevalence of these viruses was higher in sex workers of Istanbul, reaching levels of 99% and 60% for HSV-1 and HSV-2 prevalence respectively.> Jordan— The prevalence of HSV-2 in Jordan is 52.8% for men and 41.5% for women.> Israel— HSV-1 seroprevalence is 59.8% in the population of Israel and increases with age in both genders but the adolescent seroprevalence has been declining as in most industrialized nations. An estimated 9.2% of Israeli adults are infected with HSV-2. Infection of either HSV-1 or HSV-2 is higher in females; HSV-2 seroprevalence reaches 20.5% in females in their 40s. These values are similar to levels in HSV infection in Europe.> Antibodies for HSV-1 or HSV-2 are also more likely to be found individuals born outside of Israel, and individuals residing in Jerusalem and Southern Israel; people of Jewish origin living in Israel are less likely to possess antibodies against herpes. Among pregnant women in Israel a small scale cross sectional study found the prevalence of HSV-2 infection was 13.3% and that of HSV-1 was 94.9%. The HSV-2 infection rate was 3-fold higher among immigrants from the former Soviet Union (27.5%) than among Israeli-born Jewish and Arab women (9%). Approximately 78% of HSV-2 infections in Israel are asymptomatic. HSV-1 causes 66.3% of genital herpes in the Tel Aviv area.> Syria— Genital herpes infection from HSV-2 is predicted to be low in Syria although HSV-1 levels are high. HSV-1 infections is common (95%) among healthy Syrians over the age of 30, while HSV-2 prevalence is low in healthy individuals (0.15%), and persons infected with other sexually transmitted diseases (9.5%). High risk groups for acquiring HSV-2 in Syria, include prostitutes and bar girls; they have 34% and 20% seroprevalence respectively."
55,za3faran 10 months ago
55,| root | parent | next [–]
55,"> If you are an adult of typical sexual activity, it is likely you have already had sex with someone infected with herpes.Again, not in conservative populations where marriage is the typical way to have sexual relations. Syria is one example that you quoted. A smart person would get his/her partner tested if they suspect anything before getting married."
55,yakshaving_jgt 10 months ago
55,| root | parent | next [–]
55,"I'm trying to work out which logical fallacy you're employing here. No True Scotsman? Personal Incredulity?In any case, assuming that infidelity or anything else sinful/haram doesn't occur in ""conservative"" populations strikes me as frightfully naïve.The data is in. Arabs get herpes too."
55,za3faran 10 months ago
55,| root | parent | next [–]
55,"Infidelity does happen, but it's much much less common and is severely looked down upon.Edit: it seems that HSV-1 is not an STD, but it causes genital herpes if engaging in oral sex? Which can explain the big gap between the two in conservative cultures."
55,noodlesUK 10 months ago
55,| parent | prev | next [–]
55,"I think that the only reasons to choose MySQL (or Maria) over Postgres for a new project are operational. Postgres is probably the better database in almost all respects, but major version upgrades are much much more of a pain on Postgres than on almost any other system I have ever used. That being said, I would choose Postgres pretty much every time for a new project. The only reason I would use Maria or MySQL would be if I thought I later would want to have something like Vitess, for which I think there isn't really an equivalent for Postgres."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"> but major version upgrades are much much more of a pain on Postgres than on almost any other system I have ever used.This is a thread comparing MySQL and Postgres and your claim is that postgres is harder to do major version upgrades than anything you have used??Context is important here, have you honestly actually upgraded a MySQL node? It’s a lesson in pain and “major” version changes happen on minor versions, like the entire query planner completely trashing performance in 5.6->5.7Postgres has two forms of updates:1) in place binary upgrade.Fast, clean, simple, requires that you have the binaries for the old and the new database.2) dump/restore.Serialise the database into text files, load a new database and deserialise those files into it.Slow, but works flawlessly & consistently with relatively low danger.MySQL can only do option 2.You can sort of fake an “update” by abusing the fact that MYSQLs replication offers no guarantees, so you can make a new server a replica; then roll over. But it is impossible to know what data was lost in that transition and MySQL will happily continue without ever telling you.I have experienced this behaviour in large e-commerce retailers. MySQL was very popular for a very long time and I am intimately aware of operational best practices and how they are merely patching over an insane system."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"MySQL doesn't use SemVer. MySQL 5.6 vs 5.7 are different ""release series"", and switching between them is considered a ""major"" version change.MySQL absolutely fully supports in-place binary upgrades, saying otherwise is pure FUD. And the upgrade process in MySQL doesn't even iterate over your table data in any way, so claiming it will cause ""data loss"" is also pure FUD.At Facebook we automated rolling in-place updates of our entire fleet, with new point builds of fb-mysql going out several times a month, to the largest MySQL deployment in the world. Worked flawlessly and this was a full decade ago.MySQL is widely considered easier to upgrade (relative to Postgres) because MySQL's built-in replication has always been logical replication. Replicating from an older-version primary to a newer-version replica is fully supported. When upgrading a replica set, the usual dance is ""upgrade the replicas in-place one at a time, promote one of the replicas to be the new primary while temporarily booting out the old primary; upgrade the old primary and then rejoin the replica set""."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"Facebook has, at minimum, 3 teams maintaining MySQL. including a team who genuinely modifies it into submission. so much that they needed 1,700 patches to port their modified version to 8.0.It is not relevant to the discussion to discuss how Facebook has managed to munge it to work reasonably well by pouring thousands of hours of engineer time into the effort; and MySQLs in-place upgrades absolutely do not work the way you describe consistently.I know this because I have been in the code, and only after having experienced it. Maybe some of your lovely colleagues has helped out your particular version to be marginally more sane.It genuinely must be nice having a dozen people who can work around these issues though, I certainly wouldn’t consider it an operational win, most companies have no DB automation engineers, or DB performance engineers or MySQL infrastructure engineers.> Replicating from an older-version primary to a newer-version replica is fully supported.Here also be dragons, as eluded to. I know it works quite often, I have used it.FWIW: I ran global AAA online-only game profile systems on a handful of Postgres machines at about 120k transactions/s in 2016, I would have needed 5x as many instances to do it in MySQL, and this was only tiny part of our hosted infra.. which included a global edge deployment of game servers, auth servers, matchmaking, voice bridges and so on.and we only had two people responsible for the entire operation"
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Please educate me on how my statement about MySQL upgrades is incorrect, I'd love to hear this. I've been using MySQL for 20 years, and while 2 of those years were at Facebook, 18 were not. I've performed MySQL upgrades in quite a wide range of environments, and what you're saying here about lack of in-place upgrades or eating data is simply not aligned with reality.I haven't made any comments regarding performance comparisons, and have also run extremely large DB footprints with tiny teams, but I don't see how any of that is relevant to the specific topic of new-version upgrade procedure!"
55,dijit 10 months ago
55,| root | parent | next [–]
55,"Because it depends so much on your storage engine and schema, I have never seen it recommended because there are circumstances where you have data which is unrepresentative unless you are very careful or you don’t actually use the expressiveness of the DB.I mean, I’ve also seem my share of “ERROR 1071 (42000) at line xxx: Specified key was too long; max key length is xxx bytes” randomly that basically means the machine needs manual recovery.God help you if you don’t have innodb_file_per_table enabled to begin with too.I know you want me to cite exactly. That will take me time to find because I stopped caring about MySQL 7 years ago, but I will dig for you."
55,Volundr 10 months ago
55,| root | parent | next [–]
55,"FWIW while I use Postgres for my own development I've had to administer a number of MySQL servers for other devs. Upgrades have always been updating the MySQL package, restarting MySQL, then running `mysql_upgrade`, and restart the server again. I'm pretty sure the mysql_upgrade has even been missed a number of times and it's worked fine.I won't say it's impossible you ran into issues doing this, but it is the documented and supported upgrade path.I love Postgres, but as someone whose maintained both for years, upgrades (at small scale) are the one area where I'd say MySQL has Postgres beat."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"as long as you upgrade with a minor version, you will have the same experience with postgres.11.0->11.2 will work totally fine, with no command needed."
55,Volundr 10 months ago
55,| root | parent | next [–]
55,"Right, but now go from 11->12, which is the equivalent of the upgrade path I was describing for MySQL. I either need to install both versions and use pg_upgrade to convert the binary files, then remove 11 (and extensions may break this flow) or do pg_dump/restore.Minor versions on both Postgres and MySQL are painless, just install and restart the server. Major upgrades on MySQL are significantly less painful."
55,evanelias 10 months ago
55,| root | parent | prev | next [–]
55,"> I’ve also seem my share of “ERROR 1071 (42000) at line xxx: Specified key was too long; max key length is xxx bytes” randomly that basically means the machine needs manual recovery.What? This error has nothing to do with upgrades, nothing to do with manual recovery, and hasn't been a common problem for many many years.In old versions of MySQL, it just meant you needed to configure a few things to increase the InnoDB index limit to 3072 bytes, instead of the older limit of 767 bytes:innodb_file_per_table=ON"
55,innodb_large_prefix=ON
55,"innodb_file_format=barracudaand then ensure the table's row_format is DYNAMIC or COMPRESSED.But again, all of this happens by default in all modern versions of MySQL and MariaDB.Should it have been the defaults much earlier? Absolutely yes, MySQL used to have bad defaults. It doesn't anymore."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"The error I gave is a similar one to the one I used to get with “major” upgrades that happened when Ubuntu decided it was time to upgrade.It happens and I seriously never claimed that it was an ultra common problem, merely that upgrades in Postgres are more intentional and not painful except for a little extra work between major versions. The standard upgrade path within major versions; 9.x or 10.x or 11.x or 12.x is working just the same as MySQL, except I have much more experience of MySQL completely fumbling their “automatic unattended” upgrade or even the mysql_upgrade command.Mostly because in the real world outside of engineering cultures databasen are massively abused, ISAM tables that are constantly updated, InnoDB ibdata1 in the terabytes, poor configs, replicas that have skipped a few queries, column changes inside a transaction that failed but actually modified data, it happens. Usually I am called in to clean the mess.Major difference here is that Postgres doesn’t leave a mess, so I never have the kind of issues that I am describing in this thread with it, and you don’t because I am guessing that you’re there when they’re installed, someone with knowledge was actively maintaining. or you have a lot of people to help with shortcomings.I get it though. you’ve got your sunk cost knowledge of MySQL and you’ve been on large support teams for it. Maybe you’re afraid I’m suggesting that this knowledge goes out the window. and it has gotten better, but I wouldn’t give my kids watered down led infused soft drinks just because I had suffered through led poisoning. I remember coming to blows with you in other threads over the years because you think MySQL can be saved or is totally fine, but honestly, just, no."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"I'm primarily a software engineer, not a member of ""large support teams"". I've also worked for many years as an independent consultant, brought in when things go wrong, certainly not when they were first ""installed"". I'm not ""afraid"" of anything concerning my knowledge going ""out the window"". If MySQL suddenly disappeared worldwide, I could happily pivot to some other area of software engineering, or I could simply retire. Please stop make assumptions about other people who you know nothing about.I'm responding to you because you're repeatedly posting factually incorrect items, for years. For example you and I have directly discussed the ""MySQL doesn't use SemVer"" thing before on HN, and yet here you are again in this thread, claiming 5.6 to 5.7 should be a ""minor"" upgrade.Anyway, to the topic at hand, as others have also mentioned in this thread: historically the difficulty with Postgres upgrades has been the lack of cross-version replication, due to Postgres WAL replication being a low-level physical replication system. This made it difficult to perform an upgrade while keeping your site fully online. Perhaps the newer logical replication support makes this easier these days. I hope to learn more about it someday. If you can share your process for upgrading a Postgres cluster while keeping it online, that would be helpful and informative."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"1. The log-replication method of upgrading can be performed using the built-in logical replication facilities as well as using external logical replication systems such as pglogical, Slony, Londiste, and Bucardo. Most of which have existed essentially forever.2. Failovers of any database are not instant, but they are indeed quick! So let’s not claim that you can do an upgrade with zero downtime.3. In-place upgrades are extremely fast and you can test the speed using a physical replica before hand, usually it’s a couple of seconds though the docs say minutes.4. MySQLs major version being in the minor position is exactly the kind of “you should be sure you know what you’re doing but we won’t make it obvious” territory that I really despise."
55,JohnBooty 10 months ago
55,| root | parent | next [–]
55,"While you two have agreed on approximately nothing, this has been an informative discussion and I do thank you both."
55,0xf8 10 months ago
55,| root | parent | next [–]
55,"I echo the sentiment and think yours is likely the most pertinent takeaway having made it this far absent reaching any consensus whatsoever haha.It was nevertheless a pretty epic journey of dialectic discourse plunging _deep_ into the esoteric and nuanced realm of expert-level technical minutiae. A mostly intellectual journey, albeit distinctly punctuated by an undertone of emotional angst that steadily progressed in its growing intensity in a manner proportional to the magnitude of your collective disagreement… epic indeed."
55,LammyL 10 months ago
55,| parent | prev | next [–]
55,Is there a good way to do case-insensitive accent-insensitive collations yet in postgresql?
55,"It’s been a holdup for using that for some use cases like searching for data, like a person’s name, in pgsql when the casing or accents don’t match perfectly.Mssql has had this for ever, and I’m pretty sure MySQL has it as well."
55,dijit 10 months ago
55,| root | parent | next [–]
55,"Maybe this helps: https://stackoverflow.com/posts/11007216/revisions ?My gut tells me that I would do it in the query itself though, and not rely on the collation. Maybe I am misunderstanding."
55,sjamaan 10 months ago
55,| root | parent | next [–]
55,"Look under ""Update for Postgres 12 or later"", there they create a collation, an index to make use of it and then a query to make use of it:"
55,SELECT \* FROM users WHERE name = 'João' COLLATE ignore_accent;
55,EwanToo 10 months ago
55,| root | parent | prev | next [–]
55,"Not really, no, it's doable but not easily"
55,throw0101b 10 months ago
55,| parent | prev | next [–]
55,"> There is almost no good reason to choose MySQL over PostgreSQL for any operational reasonGalera is the main one I can think of:* https://galeracluster.com/library/documentation/tech-desc-in...* https://mariadb.com/kb/en/what-is-mariadb-galera-cluster/* https://packages.debian.org/search?keywords=galeraI'm not aware of any multi-master, active-active(-active) replication system that is open source for PostgreSQL."
55,jaachan 10 months ago
55,| root | parent | next [–]
55,We ran a Galera cluster for 4 days and it died on the first ALTER TABLE we did. Only option was to reduce it down to a Primary / Replica setup. I really can't recommend anyone using this.
55,dijit 10 months ago
55,| root | parent | prev | next [–]
55,Last time I looked at Galera it had a limit at 5TiB of data. Which is fine I guess.In the Postgres space there is citusdb which provides multi master.There is also BDR from 2ndquadrant if you want a paid/supported solution. https://www.enterprisedb.com/products/edb-postgres-distribut...
55,davidgerard 10 months ago
55,| root | parent | prev | next [–]
55,"yeah, that's the use case ""I run an application that requires or is highly optimised for MySQL""."
55,avinassh 10 months ago
55,| root | parent | prev | next [–]
55,how do multi master MySQL handles conflicts?
55,asdfman123 10 months ago
55,| parent | prev | next [–]
55,The main problem with herpes is the stigma against it. Don't besmirch it by associating with Oracle.
55,srcreigh 10 months ago
55,| parent | prev | next [–]
55,"Postgres is >50x slower for range queries(example below) and is akin to using array-of-pointers (ie Java) whereas MySQL supports array-of-struct (C). Illustration from Dropbox scaling talk below.Sneak peek photo [1] (from [2]). Just imagine its literally 500-1000x more convoluted per B-tree leaf node. That's every Postgres table unless you CLUSTER periodically.[1]: https://josipmisko.com/img/clustered-vs-nonclustered-index.w...[2]: https://josipmisko.com/posts/clustered-vs-non-clustered-inde...Mind boggling how many people aren't aware of primary indexes in MySQL that is not supported at all in Postgres. For certain data layouts, Postgres pays either 2x storage (covering index containing every single column), >50x worse performance by effectively N+1 bombing the disk for range queries, or blocking your table periodically (CLUSTER).In Postgres the messiness loading primary data after reaching the B-tree leaf nodes pollutes caches and takes longer. This is because you need to load one 8kb page for every row you want, instead of one 8kb with 20-30 rows packed together.Example: Dropbox file history table. They initially used autoinc id for primary key in MySQL. This causes everybodys file changes to be mixed together in chronological order on disk in a B-Tree. The first optimization they made was to change the primary key to (ns_id, latest, id) so that each users (ns_id) latest versions would be grouped together on disk.Dropbox scaling talk: https://youtu.be/PE4gwstWhmc?t=2770If a dropbox user has 1000 files and you can fit 20 file-version rows on each 8kb disk page (400bytes/row), the difference in performance for querying across those 1000 files is 20 + logN disk reads (MySQL) vs 1000 + logN disk reads (Postgres). AKA 400KiB data loaded (MySQL) vs 8.42MiB loaded (Postgres). AKA >50x improvement in query time and disk page cache utilization.In Postgres you get two bad options for doing this: 1) Put every row of the table in the index making it a covering index, and paying to store all data twice (index and PG heap). No way to disable the heap primary storage. 2) Take your DB offline every day and CLUSTER the table.Realistically, PG users pay that 50x cost without thinking about it. Any time you query a list of items in PG even using an index, you're N+1 querying against your disk and polluting your cache.This is why MySQL is faster than Postgres most of the time. Hopefully more people become aware of disk data layout and how it affects query performance.There is a hack for Postgres where you store data in an array within the row. This puts the data contiguously on disk. It works pretty well, sometimes, but it’s hacky. This strategy is part of the Timescale origin story.Open to db perf consulting. email is in my profile."
55,scosman 10 months ago
55,| root | parent | next [–]
55,"I made same comment elsewhere before finding this comment. I can vouch for this speed up in the ratio of records per page. It is very real. Only applies for small records where you can pack many rows into a page, and where you can cleanly partition by user/tenant, but that’s common enough.I will say: we kept every table we didn’t have to migrate for perf reasons in Postgres, and never regretted it.Edit: and the index “fix” for Postgres doesn’t work often. Postgres will validate the row on disk, even if it’s in the index, if the page’s visibility map isn’t set. If you data isn’t write once, there’s a decent chance your page is dirty and it will still make the “heap” fetch."
55,Shorel 10 months ago
55,| root | parent | prev | next [–]
55,"You are confusing two concepts here. In InnoDB, the tables are always ordered by the primary key when written to actual disk storage.This is not the same as ""having a primary key"", Postgres also has primary keys. It just stores the PK index separately from the bulk of the data.Oracle also has primary keys, even if the order of the rows is different to the key order. In Oracle, when the rows are stored in the same order as the keys in the primary index, it is a special case and these tables are called IOT, index ordered tables.The disadvantages of IOT are that inserts are slower, because in a normal table, the data is appended to the table, which is the fastest way to add data, and only the index needs to be reordered. In an IOT, the entire table storage is reordered to take the new data into account.Select queries, OTOH, are much faster when using IOT, for obvious reasons, and this is what you describe in your comment.If you use TEXT, BLOB, or JSON fields, even in MySQL, the actual data is stored separately."
55,srcreigh 10 months ago
55,| root | parent | next [–]
55,"I said primary index, not primary key (primary key and primary index is synonymous in mysql Dropbox example). Primary index is database theory lingo for storing all the primary row data inside a B-tree. It’s synonymous with what you say IOT although that’s a new term for me.You’re incorrect about IOT reordering the entire table at least wrt mysql. MySQL uses a B-tree to store rows, so at most it’s insertion sort on a B-tree node and rare b-tree rebalance. Most b-tree leaf nodes have empty space to allow for adding new data without shifting more than a few hundred other rows. Also, non-IOT tables also need to do a similar process to write to each of its indexes. Last, it’s sort of a tossup since if you’re appending to an IOT table frequently, the right edge of the B-tree is likely cached. (similarly for any small number of paths through the primary index B-tree). At worst Postgres heap will need to surface one new heap disk page for writing, although I’m sure they have some strategy for caching the pages they write new data to.Sorry to spam this info! Glad to see we both love databases and I’m always please to see engagement about this topic!"
55,boloust 10 months ago
55,| root | parent | prev | next [–]
55,"When using clustered indexes, one tradeoff is that if a non-clustered index isn't covering for a query, it will need to perform B-tree traversals to find rows in the clustered index. This can significantly increase the amount of (at least logical) IO compared to heap tables, where the non-clustered indexes can refer directly to the row id.Because you can only have a single clustered index, you're effectively paying for efficient range queries on a single clustering key by making all other queries slower.This tradeoff may or may not be worth it depending on your query patterns. In my experience, you can often get away with adding some subset of columns to a non-clustered index to make it covering, and get efficient range queries without making a copy of the entire dataset.And even with clustered indexes, as soon as you want a range query that's not supported by your clustered index, you're faced with the exact same choices, except that you have to pay the cost of the extra B-tree traversals."
55,srcreigh 10 months ago
55,| root | parent | next [–]
55,"Appreciate the thoughtfulness. I believe the branching factor of even wide keyed tables don’t add significant cost to point lookups. At most one or two extra disk pages needing to be read.Example: 80 bytes keys gives you branching factor of roughly 100. 10M rows and you can pack say 20 rows per page. That’s a 4GB table, give or take. That btree still only has 3 intermediate layers and primary data on a 4th layer. (Calculation is log(10M/20/0.75)/log(100)+1.) The first two layers take up less than a megabyte of ram and are therefore easily cached. So you wind up only needing 2 disk reads for the final two layers. Unless Postgres is caching the entire index for point lookups, it should come out about even.Can’t find any resource saying that btree height exceeds 5, so I’m thinking it’s at worst 2x the (very small) disk read cost vs Postgres."
55,ideal_gas 10 months ago
55,| root | parent | prev | next [–]
55,"(Admitting bias: I've only ever worked with postgres in production with update-heavy tables so I've dealt with more of its problems than MySQL's)Postgres also has other gotchas with indexes - MVCC row visibility isn't stored in the index for obvious performance reasons (writes to non-indexed columns would mean always updating all indexes instead of HOT updates [1]) so you have to hope the version information is cached in the visibility map or else don't really get the benefit of index only scans.But OTOH, I've read that secondary indexes cause other performance penalties with having to refer back to the data in clustered indexes? Never looked into the details because no need to for postgres which we've been very happy with at our scale :)[1] https://www.postgresql.org/docs/current/storage-hot.html"
55,srcreigh 10 months ago
55,| root | parent | next [–]
55,"Interesting. PG docs don’t clarify whether visibility map gets updated for HOT update. Maybe even HOT update spoils index only scans. Although I can’t see why-no new index entries, heap visibility status hasn’t changed for any indexes.. wish to find some answers here but I could not.Wrt secondary indexes, yes and no. There is a cost to traverse a B-tree for point lookups. Also, foreign keys may now be composite keys if primary key is composite as in the Dropbox example.If the secondary index is very different from the primary, it will be more expensive. However it’s pretty common to at least use a “user_id” as the first part of the primary key. This will make partial full scans a lot faster for queries regarding a single user; only need to scan that users data, and it comes at a 1-2 order of magnitude cheaper disk read cost. So you’d need a secondary index only if the data you need is spread across 1000s of pages (megabytes of data for a single user in one table) and you’re looking for only a handful of rows randomly located in that sequence.Twitter is a characteristic case where you need many different clustered sets for the same data (tweets) to power different peoples feeds. I believe twitter just stores many copies of tweets in different clusters in Redis- basically the same as having a (author_id, ts) primary key tweets table and a (follower_id, ts) primary key feed table, both having tweet data inlined. If one clustered table isn’t enough, use two."
55,frodowtf 10 months ago
55,| root | parent | prev | next [–]
55,"That was a great read in contrast to all the ""there's no reason to use mysql"" nonsense in this thread"
55,srcreigh 9 months ago
55,| root | parent | next [–]
55,"That's what I'm going for! I love this type of info, so against the grain and useful it's basically a scandal. :-) Check out your sibling comment about pg_repack though, it's a really awesome PG perf improvement tool."
55,arp242 10 months ago
55,| root | parent | prev | next [–]
55,"I've definitely run in to these kind of issues and learned about them the hard way, but I found that in PostgreSQL it's quite a bit easier to understand what is actually going on due to better documentation and tooling, and I've found this very valuable. Maybe it's just because I've spent more time with PostgreSQL than MariaDB, but MariaDB has often left me quite a bit more confused (on performance, but also other topics)."
55,singron 10 months ago
55,| root | parent | prev | next [–]
55,"If your pg database improves with CLUSTER, you can use pg_repack instead to achieve the same effect without downtime. Besides reordering the heap, it will also clear out bloat from the heap and indexes. I highly recommend partitioning if you have heavy write traffic on large tables since that will keep overhead low and make it complete faster."
55,srcreigh 10 months ago
55,| root | parent | next [–]
55,Thanks. I actually thought of similar pg_repack concept on my own a couple days ago (influenced by gh-ost probably..) I was googling stuff like how to switch a PG replica to master. I was imagining having 2 dbs and running CLUSTER on the inactive one. Probably wouldn't work but anyways I found about pg_repack after researching.I'm now super interested in the perf aspects of running pg_repack. It would definitely require scratch storage space to be able to copy over the largest table in the DB (I'd guess 2.1x the largest table vs 2x the total DB size). I imagine repacking isn't as efficient as putting stuff into a B-tree. But I wouldn't expecting it to be anything like 50x worse like I portrayed above.
55,singron 9 months ago
55,| root | parent | next [–]
55,"It does CREATE TABLE ... AS SELECT * FROM table ORDER BY, which isn't too bad. Then it runs CREATE INDEX for all your indexes, which is usually faster than CREATE INDEX CONCURRENTLY. However, while these are running, a trigger is inserting records for all new writes into a special log table. Once all the indexes are created, it replays that log table until it's empty, and then it briefly locks the source table, flushes the last bit of the log table, and swaps the tables.If you have a lot of write traffic, that log table will get really big, and pg_repack will have to do a lot of work to replay it. It's possible that the log table grows faster than pg_repack can replay it, which will cause it to never finish and eventually exhaust your storage space. You can also run into an issue where the log table never gets completely empty, and pg_repack never initiates that final lock and swap.Partitioning helps a lot because it divides both the size of the tables and the write traffic to each table."
55,JohnBooty 10 months ago
55,| root | parent | prev | next [–]
55,Thanks for that informative link. It's rare in these sorts of discussions.
55,eyelidlessness 10 months ago
55,| parent | prev | next [–]
55,"The only reason I’d consider MariaDB, which I’m surprised I don’t see mentioned currently, is its bitemporal features. There are whole worlds of problems solved that are almost universally badly done in schema/business logic instead.Granted I haven’t had to make a decision like this for several years, I’ve hardly even touched a database except to debug some stuff on localhost that’s outside of my explicit purview. So maybe Postgres solutions have narrowed the gap on this without my knowing it."
55,otabdeveloper4 10 months ago
55,| parent | prev | next [–]
55,Does Postgres have binlog replication yet?
55,dijit 10 months ago
55,| root | parent | next [–]
55,"Yes, for over half a decade at least, but “binlog” is a MySQL term, for postgresql it has the much more apt name: write-ahead log.it is the only official, in-binary replication mechanism."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Postgres WAL replication is a physical replication stream. MySQL binlog replication is a logical replication stream, a higher-level abstraction which is independent of the storage engines.Postgres does separately support logical replication now, but it has some limitations, such as not permitting replication of DDL: https://www.postgresql.org/docs/current/logical-replication-..."
55,paulddraper 10 months ago
55,| parent | prev | next [–]
55,MySQL is certainly faster for writes.https://www.uber.com/blog/postgres-to-mysql-migration/
55,arp242 10 months ago
55,| root | parent | next [–]
55,"I would be wary putting too much stock in a seven year old post on the topic; lots of stuff has changed. Specifically, at least one person claimed that a patch greatly improved PostgreSQL for this use case: https://news.ycombinator.com/item?id=26285452"
55,paulddraper 10 months ago
55,| root | parent | next [–]
55,The MVCC requirement is still there.It's like saying C is faster than Ruby. It always will be.
55,za3faran 10 months ago
55,| parent | prev | next [–]
55,"MySQL is free, regardless of Oracle's ownership."
55,supergram 10 months ago
55,| parent | prev | next [–]
55,> * Memory tablesPostgreSQL has memory tables too.
55,pyuser583 10 months ago
55,| parent | prev | next [–]
55,Does Postgres have an archive mode?
55,dijit 10 months ago
55,| root | parent | next [–]
55,"If you say what you’re trying to actually achieve I can help with a solution, but asking if it supports an arbitrary feature is not going to get the answer you want because depending on what you’re actually using an archive table for, Postgres might have something already built in but it will almost assuredly not be exactly like an archive table storage type."
55,pyuser583 10 months ago
55,| root | parent | next [–]
55,"Sorry, ARCHIVE is a MySQL storage engine. It supports only non destructive transactions: INSERT, REPLACE, and SELECT, but not DELETE and UPDATE.It’s an excellent alternative to use a WORM drive when you’re trying to preserve everything (say, a list of financial transaction).I’ve looked for something like this in Postgres (which I love!), but sadly it doesn’t seem supported.https://dev.mysql.com/doc/refman/8.0/en/archive-storage-engi..."
55,yencabulator 10 months ago
55,| root | parent | next [–]
55,"Saying that REPLACE is non-destructive is a bit weird. Overwriting data is destructive, by definition -- and even if ARCHIVE is append-only underneath, it seems this use of REPLACE is very much a ""logical overwrite"", as there doesn't seem to be any kind of time travel view of the old state.Also, you might be interested in Parquet, perhaps as seen through Delta Lake https://delta.io/ or Postgres Foreign Data Wrappers like https://github.com/adjust/parquet_fdw -- Delta Lake's simple ""LSM of Parquet files in an object store"" design is pretty sweet."
55,ants_a 10 months ago
55,| root | parent | prev | next [–]
55,Why is revoking permissions to run deletes/updates not an option?
55,NuSkooler 10 months ago
55,| prev | next [–]
55,"PostgreSQL every time, unless you have a specific reason, or as already pointed out, you're sure you don't just need SQLite.PSQL in my experience has vastly better tooling, community, and is ahead of the curve tech wise. Same with extensions availability. Or perhaps you need to move away from it to say CockroachDB, or similar which is much easier."
55,grzm 10 months ago
55,| parent | next [–]
55,nit: psql is the command line client. postgres or pg are the more common shortenings of PostgreSQL.https://www.postgresql.org/docs/15/app-psql.html
55,adoxyz 10 months ago
55,| prev | next [–]
55,Choose whichever one you/your team is more familiar with. Both are battle-tested and proven and will likely scale to whatever needs you have.
55,TehShrike 10 months ago
55,| parent | next [–]
55,"This is the correct answer.Whichever one you start out with, you will be annoyed if you switch to the other one 5 years later."
55,"I started out with mysql, and when I started working on a postgres project, I was shocked at some of the ways it was lacking (how am I supposed to store email addresses in a database without collations?).But when postgres folks grouse about stuff in mysql, I'm usually nodding along and saying ""yeah, that would be nice"".They're both great options."
55,"If anybody on your team is already an expert at one of them, use that one."
55,robertlagrant 10 months ago
55,| root | parent | next [–]
55,"> when I started working on a postgres project, I was shocked at some of the ways it was lacking (how am I supposed to store email addresses in a database without collations?)How long ago was this? :)"
55,TehShrike 10 months ago
55,| root | parent | next [–]
55,3 years ago.
55,From this comment thread https://news.ycombinator.com/item?id=35908169 I infer that Postgres still doesn't support collations.
55,robertlagrant 10 months ago
55,| root | parent | next [–]
55,"Oh - is this ci collations, not collations in general?"
55,TehShrike 10 months ago
55,| root | parent | next [–]
55,"A lot of the collations are useful in different circumstances – but the CI ones probably come up the most often, and are necessary to store email addresses in a way where you store the case information that the user typed in, but can do case-insensitive lookups against in the future."
55,funcDropShadow 10 months ago
55,| root | parent | prev | next [–]
55,"According to the documentation of Postgres 12 [1] it is possible to use so called non-deterministic collations, which may express case-insensitivity. If that is what you need.The documentation of Postgres 11 [2] states that this was not possible:"
55,Note that while this system allows creating collations that “ignore case” or
55,"“ignore accents” or similar (using the ks key), PostgreSQL does not at the moment allow such collations to act in a truly case- or accent-insensitive manner. Any strings that compare equal according to the collation but are not byte-wise equal will be sorted according to their byte values."
55,[1]: https://www.postgresql.org/docs/12/collation.html
55,[2]: https://www.postgresql.org/docs/11/collation.html
55,OJFord 10 months ago
55,| root | parent | prev | next [–]
55,"> how am I supposed to store email addresses in a database without collations?Not familiar with MySQL so trying to look that up, but with a constraint? Or just don't do that? - SO answer I found says 'it's much more useful to have johndoe@ and JohnDoe@ treated as the same than it is to support case sensitive email addresses'.. ok, it's also incompliant, but whatever's 'more useful’ I guess!"
55,vosper 10 months ago
55,| parent | prev | next [–]
55,"Having used both in production, I agree with the above. It's not going to make or break your business or project.I will also add that their are giant companies relying on both databases with great success. Facebook still runs on MySQL, and contribute back to it. Youtube I'm not sure about, but it did run on MySQL for a long time, well after it got massive. I'm sure examples exist for Postgres (Amazonm since they moved off Oracle?)"
55,bombcar 10 months ago
55,| parent | prev | next [–]
55,"And if you have experience with one and try to use the other, you may end up foot gunned by something you didn't know about."
55,erulabs 10 months ago
55,| prev | next [–]
55,"Use MySQL if you're expecting to have to do large operational database tasks re: migrations, maintenances, with no ability to go offline. gh-ost, percona-osc, the new INSTANT DDL stuff, is all quite far ahead in MySQL-land. Additionally, Vitess and Planetscale are making huge strides in MySQL performance. There are more people and guides in the world to help recover even the most mutilated of MySQL databases. MySQL gets more love in extremely large enterprise-level organizations, and it shows.Use Postgres if you need some of the quite-good extensions, most notably PostGIS, or if you just want things to work; most documentation will be postgres flavored. Postgres gets more love from web-developers, and it shows."
55,fzeindl 10 months ago
55,| parent | next [–]
55,"This is wrong. MySQL does not support transactional DDL, so you cannot run migration and abort them in the middle.Always use postgresql. It's more logical, more extensible, saner, supports many extensions and is more predictable.MySQL is inconsistent crap, that trades away consistency, correctness and stability for a little bit of performance in standard use cases.Do yourself a favor and always use postgreSQL. I switched 15 years ago and never looked back. Have done 15-50 projects since in psql."
55,WJW 10 months ago
55,| root | parent | next [–]
55,"With a sane migration tool like pt-osc or gh-ost you can absolutely abort in the middle. What's more, you can pause in the middle or even slow down in the middle based on arbitrary logic (ie, pause migration if replication delay rises above a certain value). Postgres is nice and transactional DDL has its place but postgres stopped halfway through IMO. Vanilla Postgres > vanilla MySQL, but the migration story of MySQL + tooling is so far beyond Postgres + tooling that it's not even funny.That said, if you don't expect to have tables with 100m+ rows, even vanilla postgres will be good enough."
55,amflare 10 months ago
55,| root | parent | prev | next [–]
55,Spoken like someone who switched 15 years ago and never looked back.
55,barrkel 10 months ago
55,| root | parent | prev | next [–]
55,"Production MySQL databases of any significant size use pt-osc or gh-ost for schema changes, and these can be throttled, paused, aborted and so on."
55,craigkerstiens 10 months ago
55,| prev | next [–]
55,"Having answered this a ton over the years, don't want to really take shots at MySQL. But Postgres stands in pretty unique ground.1. It's solid as a really reach data platform (more than just a relational database). It's extension framework is quite unique compared to others. It's JSONB support was the first among other relational databases and is feature rich and performant. Multiple index types. Transactional DDL. The list goes on.2. No central owner. A lot of open source is source code is open, but it's maintained by a central company.3. I mentioned extensions, but really that is understated. It can do really advanced geospatial, full text search, time series, the list goes on.Having explained this a ton of times first 10 years ago - https://www.craigkerstiens.com/2012/04/30/why-postgres/ and then again 5 years later with and updated version, most recently tried to capture more of this in an updated form on the Crunchy Data blog - https://www.crunchydata.com/why-postgres"
55,jhas78asd 10 months ago
55,| parent | next [–]
55,Hi Craig! :) https://twitter.com/andatki
55,endgame 10 months ago
55,| prev | next [–]
55,"https://web.archive.org/web/20211206040804/https://blog.sess...From a former MySQL developer:> let me point out something that I've been saying both internally and externally for the last five years (although never on a stage—which explains why I've been staying away from stages talking about MySQL): MySQL is a pretty poor database, and you should strongly consider using Postgres instead."
55,tough 10 months ago
55,| prev | next [–]
55,Pick postgres unless you have a good reason not too?
55,cryptonector 10 months ago
55,| parent | next [–]
55,And even then you pick Postgres.
55,tough 10 months ago
55,| root | parent | next [–]
55,"Can't really loose with postgres, I concur"
55,trympet 10 months ago
55,| prev | next [–]
55,The Postgres query optimizer is more powerful than the MySQL query optimizers [1]. It generally scales better for OLTP. Also tons of extensions that can accelerate your workload.[1] - https://ieeexplore.ieee.org/document/9537400
55,spudlyo 10 months ago
55,| parent | next [–]
55,"It's also more opinionated than the MySQL query optimizer, in that you can't give it hints to prevent it from making a horrible mistake."
55,jhas78asd 10 months ago
55,| root | parent | next [–]
55,There is a way to provide some type of planner hints https://pghintplan.osdn.jp/pg_hint_plan.html
55,manv1 10 months ago
55,| prev | next [–]
55,"The main reason I prefer mysql over PostgreSQL is that mysql is just more consistent - in its commands, quirks, etc.Postgres - is it pg, pgsql, psql, postgres, postgresQL? The answer is ""yes.""Plus the case behavior for tables and column names drives me crazy. It's like some leftover VMS shit. I mean seriously fix it. Can you or can you not use a capital letter for a table/column name? I can never remember. Or you can, but you have to quote it? Fuck.Until recently (which to be fair might be 8-10 years ago) postgres' performance monitoring tools sucked compared to mysql. I know at one point in the last 10 years they still used sunos4 as their base configuration because you know, the OS had been EOL for like a decade at that point.MySQL is fire and forget. psql (or postgres or pg or postgresql?) is not fire and forget. It's twitchy and requires constant vigilance. I don't want a piece of infrastructure that requires constant vigilance.That's not to say I won't use it. It's geo stuff is really great. It's JSON support is better than MongoDB's, from what I've heard. Row level security is awesome. But are those features good enough to overcome psql's quirks? Sometimes."
55,arp242 10 months ago
55,| parent | next [–]
55,"> at one point in the last 10 years they still used sunos4 as their base configuration because you know,What exactly do you mean with this? I tried to find some more information, and all I could find were some fixes from 2001[1] (SunOS 4 was supported until 2003), a minor refactor in 2008 with ""SunOS 4 is probably broken anyway""[2], and"
55,"that's pretty much it. SunOS 4 was moved to ""Unsupported Platforms"" with the release of 7.3, in 2002.[3][1]: https://postgrespro.com/list/thread-id/1598869[2]: https://www.postgresql.org/message-id/20081211091708.0726075...[3]: https://www.postgresql.org/docs/7.3/supported-platforms.html"
55,jhas78asd 10 months ago
55,| parent | prev | next [–]
55,"If you’re talking about the command line client that’s built in, it’s psql. If you can’t remember the command name to launch it or regularly type those other commands when you meant to type psql, you could add aliases to your shell that point to psql. :)Learning any new CLI client is a bit daunting at first. With repetition and intention, I think the commands become very memorable. Eg “describe table” is “dt”."
55,spudlyo 10 months ago
55,| prev | next [–]
55,"This is like asking how you'd choose between Emacs and Vim, Mac and PC, Monoliths and Microservices, Functional and Object Oriented .. you're likely going to elicit a lot of passion and not a ton of objective information.For most applications, either choice is going to be just fine. Use what your team has the most experience with. If you have no experience, try them both out and go with whatever you're most comfortable with."
55,0xf8 10 months ago
55,| parent | next [–]
55,"Nah, trivially the correct answers are:Vim, *NIX (so Mac), monoliths, and lambda calculus all the way—FP!jk, but FWIW I think sometimes, in rare instances, there does exist a pretty unequivocal consensus"
55,“right answer” to this sort of question
55,… maybe such as like:
55,Git vs any other distributed VCS ?
55,Doctor_Fegg 10 months ago
55,| prev | next [–]
55,"For anything involving location, choose Postgres because PostGIS is just so good."
55,solarkraft 10 months ago
55,| parent | next [–]
55,"PostGIS is somewhat of a standard interface for geolocation data and one of the main interfaces of QGis, the other being based on SQLite. It's kind of cool how powerful it is, being a ""real"" database instead of some more contrived format.My adventures with QGis+PostGIS have also led me to discover the fun fact that even setting the TLS mode to ""enforce"" may enforce that you're using TLS ... but not that the certificates are correct. Silly. It also won't use the system's store when set to ""full"". Oh well.By the way: I find Railway to be a great home for low-use Postgres/PostGIS databases. You basically pay for operations and the free tier is totally fine for semi-casual use (about 1-2k entities) with a few people from across the pond."
55,frizlab 10 months ago
55,| parent | prev | next [–]
55,"Good to know! I’m starting a project that will use location, and I chose Postgres, I’m happy to hear this :-)"
55,bratao 10 months ago
55,| prev | next [–]
55,"One big factor that keep us on MySQL is the MyRocks engine. We have huge databases with billions of rows. The MyRocks enable the use of it with heavy compression, that PostgreSQL can´t handle it, as it is much slower and uses 30x more disk usage, even with heavy TOAST tuning and/or ZFS compression."
55,eqvinox 10 months ago
55,| parent | next [–]
55,"To be fair, at the scale of your use case there I really hope you have a proper DBA who understands multiple database systems and their details, and is able to make the best choice for your setup."
55,(At some point the commercial and/or oddball SQL servers become an option too…)For everyone else who's in most cases not even stuffing a million rows into their database… just stick with Postgres :)
55,ahachete 10 months ago
55,| prev | next [–]
55,"At our company, we provide Postgres 24x7 support. We have partners that provide support for other databases, and for some projects we work together with companies that have multiple databases.We have over the years compared the rate of production incidents Postgres vs MySQL. It's roughly 1:10 (MySQL has around 10 times more production incidents than Postgres).You may consider this anecdotal evidence, but numbers managed here are quite significant.The gist is that Postgres is not perfect nor free from required maintenance and occasional production incidents. But for the most part, it does the job. MySQL too, but with (at least from an operational perspective) many more nuances."
55,herpderperator 10 months ago
55,| prev | next [–]
55,"I find PostgreSQL permission management quite convoluted. In MySQL it is simple to query for what grants a user has, but in PostgresSQL you need to write 100 lines of SQL to do the same... and you can't run \du and other commands without psql. Why couldn't they just come up with `SHOW` shortcuts that work in any SQL client?"
55,jhas78asd 10 months ago
55,| parent | next [–]
55,"You can likely get the SQL for a meta command, and you could run the SQL from your preferred client if you don’t use psql. Here is one example: https://dba.stackexchange.com/a/131031I also highly recommend investing in psql skills though if you are a Postgres user."
55,arp242 10 months ago
55,| root | parent | next [–]
55,"Yes, that's the ""100 lines of SQL to do the same"" the previous poster mentioned (obviously they were exaggerating a bit and it's less than literally 100 lines, but it's pretty complex)."
55,iamwil 10 months ago
55,| prev | next [–]
55,"The difference is not significant enough to matter for most projects, esp just starting out. Hence, I mostly choose Postgres, since I don't like Oracle as a company very much."
55,dylan604 10 months ago
55,| parent | next [–]
55,"Whenever I see MySQL, my brain automatically sees MariaDB. What is this Oracle thing you speak of ;-)"
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"MySQL and MariaDB have diverged quite a bit. I recently wrote a roundup of just the differences in table functionality and DDL, and despite keeping the post focused to that relatively-narrow topic, the list of differences is getting VERY long: https://www.skeema.io/blog/2023/05/10/mysql-vs-mariadb-schem..."
55,dylan604 10 months ago
55,| root | parent | next [–]
55,">MySQL and MariaDBAgain, what is this thing you are trying to compare? I just see MariaDB and MariaDB =)However, because of my blindness to actual MySQL, I have totally not paid attention to any differences between the two. I guess ""drop in replacement"" isn't actually true any more. Thanks for the info"
55,geenat 10 months ago
55,| prev | next [–]
55,"I know it seems dumb, but postgres really needs to add the simple developer experience stuff like:SHOW CREATE TABLE;SHOW TABLES;SHOW DATABASES;SHOW PROCESSLIST;CockroachDB added these aliases ages ago."
55,spacedcowboy 10 months ago
55,| parent | next [–]
55,"This.For anything at home, I would use MySQL just for those things. The psql client feels very primitive by comparison to me - even though it isn't."
55,tbarbugli 10 months ago
55,| root | parent | next [–]
55,"I highly suggest investing time learning psql, autocomplete works great and it has a ton of useful slash commands. \d for instance shows you the list of tables. Awesome tool"
55,spacedcowboy 10 months ago
55,| root | parent | next [–]
55,"I don't need it often enough to invest the time. I generally set up a database as backing store to some project, fiddle with it until I'm happy it's working at the scale/performance I want, and then move on to something else.During those few weeks I'm actively using the database on the project, I can either get frustrated beyond belief with the CLI for Postgres, or just use what's at hand with MySQL. In fact, these days SQLite is getting more of my attention anyway, and I wrote a small CLI for it a decade or so back (before the sqlite3 client gave us most of the below) to provide:- Timings for the queries (in fact I called it 'tsql')- Aligned-column displays, with | separators between columns and index-indicators- Ability to parse some blobs (Plists on the Mac, for example) and display- Things like ""SHOW DATABASES"", ""SHOW TABLES"", ""SHOW TABLES LIKE"" etc.Mainly I wrote it to do some benchmarking, but I eventually preferred it over sqlite3 as the CLI.Note that all this is personal stuff - When I do commercial stuff, the answer is always ""what is best understood by the people maintaining it afterwards""..."
55,jhas78asd 10 months ago
55,| root | parent | next [–]
55,Each of the bullets you listed have very straightforward and memorable meta commands that I use on a regular basis with psql. It may be worth learning them just for when you use Postgres. There is also a built in help. These can also be saved into your dot files so you don’t need to memorize them. Happy to show you if you’re interested!
55,Izkata 10 months ago
55,| parent | prev | next [–]
55,"I forget if there's an equivalent for the first one, but from psql there is a translation of mysql's ""DESC table"" as ""\d table"", and the rest are:\dt\lSELECT * FROM pg_stat_activity;"
55,arp242 10 months ago
55,| root | parent | next [–]
55,"> I forget if there's an equivalent for the first oneNot really; PostgreSQL doesn't store the original query, so you'll need to re-create it from pg_class, pg_attribute, and all of that (which is really what \d and such in psql do). The easiest way is probably pg_dump, but it's best to just get used to \-commands because it's really just the same thing."
55,jhas78asd 10 months ago
55,| root | parent | next [–]
55,Find the SQL from meta commands. Example: https://dba.stackexchange.com/a/131031
55,arp242 10 months ago
55,| root | parent | next [–]
55,"That is not equivalent to ""show create table"" at all."
55,funcDropShadow 10 months ago
55,| root | parent | prev | next [–]
55,There is an extension that does that: pgddl. [1][1]: https://github.com/lacanoid/pgddl
55,jpgvm 10 months ago
55,| prev | next [–]
55,"Generally speaking it should always be PostgreSQL.It's the default choice for a number of reasons but chief among them is just that it's higher quality. That is it's developed to higher standards due to community bar being really high (thanks Tom Lane, et al for your stewardship) and testing and analysis of changes to the database being ingrained into the culture.By pursuing correctness first before performance for many years PostgreSQL has built a stronger foundation that is now paying dividends in terms of both performance but also ability to ship powerful features quickly. This has generally resulted in the gap between MySQL and PostgreSQL only continuing to widen over the last 10 years.So when would you consider picking MySQL?To me that comes down to exactly one set of use-cases and that is workloads that are fundamentally incompatible with VACUUM. The PostgreSQL MVCC system requires that table heap be maintained by the VACUUM process to both ensure safety (txid wraparound) and reclaim/reuse heap storage. This process is very expensive for workloads that do a lot of updates, especially on indexed columns (as indices need VACUUMing also), less of an issue for non-indexed columns if you can use HOT (heap only tuple) updates and tune the target fill ratio of heap pages appropriately.In most cases it's highly unlikely your business is going to reach the level of write load where these deficiencies in write behaviour actually matter but it is possible. Uber famously migrated from PostgreSQL primarily because their experiences with write amplification and VACUUMing.If for instance though your data consists of a smaller live hot set and a warm set that is less frequently updated and easily separable by a deterministic factor like time you can very easily use PostgreSQL table partitioning to isolate the two and continue to scale for a very very long time on pure PostgreSQL.In practice this may be fixed in PostgreSQL one day, there was a project called zheap to implement an UNDO log style storage system for PostgreSQL (which would avoid all the VACUUM maintenance etc) but it stalled out, largely I believe because it wasn't able to provide obvious wins quick enough to stimulate further interest. However OrioleDB has picked up the torch now and does in fact seem to be showing very impressive results.If such a storage engine is merged in my mind there will no longer any reason to consider MySQL for production workloads."
55,jhas78asd 10 months ago
55,| parent | next [–]
55,"Thanks for calling out table partitioning. Besides implementing it at one level, multiple levels can be used simultaneously (eg list and range). Tables can be grouped and split out to their own database (aka functional sharding/vertical sharding) and again partitioned. This all takes more effort and investment but keeps you on PostgreSQL. As you said fillfactor can be tuned, more HOT updates. Even analyzing whether the Updates could be turned into inserts that are written at a high rate, not incurring bloat, and then fewer updates are made at a rate that does not outrun Vacuum."
55,test6554 10 months ago
55,| prev | next [–]
55,"I've been using MariaDB (MySQL) as a hobbyist for years. I just set up a couple myqsql servers with phpmyadmin on Raspberry PIs and use them for local development. Basic crud apps, etc.I've always assumed that PostgreSQL is a step up, but never really bothered to look into what I get for the effort. Do I really get anything if I'm not trying to make apps at scale?"
55,eqvinox 10 months ago
55,| parent | next [–]
55,"> I've always assumed that PostgreSQL, but never really bothered to look into what I get for the effort.You're making a (mistaken) assumption that Postgres giving you a ""step up"" means that you also have to put in more effort."
55,"You don't, at least not in my experience."
55,Both are database servers with a bunch of install & setup.
55,"There's phppgadmin if you want an 1:1 replacement for phpmyadmin (no opinion on these, haven't used either).Postgres just gets you farther if you need to at a later point.I would recommend you swap out mysql for postgres on your raspis."
55,You're gaining experience on one of the two.
55,"But experience on Postgres seems to be more useful and valuable (cf. rest of the HN comments), for the same cost of your time."
55,12907835202 10 months ago
55,| parent | prev | next [–]
55,Same position.There's so many things I want to learn I'm not sure postgres is such a step up from MySQL that it's worth being at the top of the list.
55,tmountain 10 months ago
55,| root | parent | next [–]
55,"If you’re not at the level of detail to care about transaction isolation levels, clustered indexes, and the implementation details of replication, you can invest your time elsewhere. For certain use cases and workloads, the differences can matter a lot, but many developers are just looking for a backing store for their CRUD app and in that case, either is likely fine."
55,duiker101 10 months ago
55,| prev | next [–]
55,Most answers seem written by fanboys rather than legit answers.I would say go with what you know and are most comfortable with. You are more likely to get the better outcome.
55,chunk_waffle 10 months ago
55,| parent | next [–]
55,"This.I've heard countless times that Postgres is better and I've watched talks where they show how loosey-goosey MySQL is with some things but I know how to backup, restore, tune, secure and replicate MySQL. I grok it's permissions in depth more than I ever have with Postgres and I've even written a mysql plugin in C so I have that in my toolbox if I need it. So I'd by default, usually go with MySQL (or in some cases SQLite.) but if I didn't have to administer or maintain it, and someone else was handling that I think I'd be fine with Postgres too."
55,NightMKoder 10 months ago
55,| prev | next [–]
55,"I’m going to go for the esoteric opinion: MariaDB. Specifically to get system versioned tables. Imagine having the full change history of every single row for any odd task that you need without the performance penalty of keeping historical data in the same table. It can be a huge amount of leverage.If that’s not your interest, I will admit that Postgres array support is far ahead of any of the MySQLs. Most ORMs don’t use it but you can get optimal query prefetching via array subqueries."
55,heyoni 10 months ago
55,| parent | next [–]
55,"I’m 100% with you regarding system versioned tables. However, I think they’re coming to Postgres soon-ish. I was following the tracker for a while and it looked like it was done."
55,skunkworker 10 months ago
55,| prev | next [–]
55,"Postgres for anything with a single database size < 4TB.But if you need something that can handle 100TB+, go Vitess(mysql compatible)."
55,jhas78asd 10 months ago
55,| parent | next [–]
55,What’s the reason though for Vitess? Postgres supports tables up to 32TB but hopefully you’re splitting them up using declarative partitioning in one or more ways before that. If you have tables that are smaller than a TB and a large memory DB (>1 TB RAM) Postgres should run ok right? I’d also imagine you’re splitting up your database into multiple databases and multiple instances (the writers) well before that as well right?
55,winrid 10 months ago
55,| parent | prev | next [–]
55,Why not Citus?
55,bkuehl 10 months ago
55,| prev | next [–]
55,"A lot of comments for Postgres, but it's the only major DB in 2023 that does not let you choose your character collation when creating a database. That is pretty much a deal breaker day 1. Guess you'll be doing a tolower() on every db search and not use indices which will kill performance or using column collation casts on every search query. I just don't get it.I once tried to migrate a SQL Server DB to Postgres and eventually gave up, with MySQL being a pretty easy switch with some minor stored procedure rewrites.Also it tends to do things way differently than every other DB. VACUUM is just a completely different concept that can footgun you pretty fast.Postgres is pretty powerful but it has certainly made some interesting design choices."
55,arp242 10 months ago
55,| parent | next [–]
55,"> doing a tolower() on every db search and not use indicesIf you create the index with lower() it will uses that; e.g. ""create index on tbl (lower(email))"" and then ""select * from tbl where lower(email)=lower($1)"". That's more or less the standard way to do this but there are some other ways as well. It's more explicit than MySQL, so in that way it's better. It's more effort and easy to forget, and in that way it's worse – either way: it's definitely possible to do case-insensitive searches with indexes.When I first used PostgreSQL I ran in to ""how the hell do I do this?! MySQL just lets me [...]"" kind of issues, but after many years of PostgreSQL usage I now have the opposite when I use MariaDB, which also has its share of awkwardness and issues (just different ones)."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"> It's more explicit than MySQL, so in that way it's better.It sounds like you're under the impression that MySQL just makes everything case-insensitive and is silent about this? That's decidedly not the case.MySQL 8 ships with 41 different character sets, supporting a total of 286 different collations. Collation names explicitly include ""ci"" (case-insensitive) vs ""cs"" (case-sensitive), as well as ""ai"" (accent-insensitive) vs ""as"" (accent-sensitive), and also the language/region conventions used for sorting purposes.You can choose collation at the column-level granularity, as well as setting defaults at the table, schema, and server levels. It's completely explicit and very configurable."
55,arp242 10 months ago
55,| root | parent | next [–]
55,There is no way to see from the query itself if it's case-sensitive or insensitive; that is what I meant.
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"Eh, just from a SQL query alone, there's no way to see that (lower(email)) is indexed, or see column data types etc. That metadata lives in the table definition, which is a normal place for it, so it seems reasonable for the collation / case-insensitivity to not be explicit in the query text.Besides, MySQL also supports functional indexes, so you could do the (lower(email)) approach in MySQL too if you really want!"
55,arp242 10 months ago
55,| root | parent | next [–]
55,"No, you can't see everything, but you can see the exact comparison it's making. Is that useful? You can decide that for yourself but I like being able to see as much of the logic (and thus verify the correctness) in the query itself. Also helps with copy/paste and some other things.I never said you can't use functional indexes in MySQL. Someone said ""you can't do this in PostgreSQL"" and I just commented ""here's (one way) to do this, maybe that's helpful some day"". That's it."
55,evanelias 10 months ago
55,| root | parent | next [–]
55,"My apologies, I misunderstood ""It's more explicit than MySQL"" to imply that you were saying that approach couldn't be used in MySQL."
55,bkuehl 10 months ago
55,| root | parent | prev | next [–]
55,"I was disappointed with MariaDB and tried it before using MySQL. It is so far behind MySQL that it can't be considered equivalent anymore. And I really wanted to use MariaDB instead.The ""MySQL just lets me"" stuff eventually adds up and hinders development. For example, your lower() on the param example is now incompatible with most ORMs unless you do that in code or create a special SQL statement for that. This isn't all fringe cases that you run into when you're months in either. I really wonder on some of these comments saying they've vetter both and Postgres wins hands down.Postgres is solid but it definitely has its warts and downsides too."
55,spprashant 10 months ago
55,| parent | prev | next [–]
55,"Am I missing something here? Postgres does allow you to choose character collation when creating a database, as well as when creating new columns."
55,bkuehl 10 months ago
55,| root | parent | next [–]
55,"I'm sorry, I was wrong in that Postgres does let you specify a few character collations at the DB level, but they are pretty much ASCII vs UNICODE with no case-insensitive configurations. You can create collations in v12 and assign them to particular columns."
55,pierat 10 months ago
55,| prev | next [–]
55,"Friends dont let friends use #Horracle software.That includes VirtualBox, MySQL, Horracle Cloud. Just step back. Walk away. Do not pass go, do not collect $20000 lawyers fees for unintended actions."
55,za3faran 10 months ago
55,| parent | next [–]
55,"That's quite silly. VirtualBox is great, and so is MySQL. They're also both OSS, so no lawyers in the sense you're implying."
55,pierat 10 months ago
55,| root | parent | next [–]
55,"Then you don't know what you're talking about, and are ignorant of the risks of using VirtualBox.That VirtualBox extension pack? That aint free... well, it is for personal use only because they're not shaking individuals down. However, Oracle watches what domains download that extension pack, and sues companies when too many employees download it.You can see that in this reddit thread: https://www.reddit.com/r/sysadmin/comments/d1ttzp/oracle_is_...And this article also addresses the risks of dealing with Horracle.And a company I worked for had dealings with them as well. Again, Horracle played dirty and did bullshit, except over Horracle DB itself."
55,za3faran 10 months ago
55,| root | parent | next [–]
55,"You're basically saying something that isn't free, isn't free."
55,DiabloD3 10 months ago
55,| prev | next [–]
55,Why would I choose MySQL in any year? There is no context you can provide to this question where I wouldn't always choose Postgres.
55,vermaden 10 months ago
55,| prev | next [–]
55,Its simple AF - I just always pick the well proven PostgreSQL database.... if that is too big I use SQLite.
55,scosman 10 months ago
55,| prev | next [–]
55,"Postgres for almost anything. Robustness, ecosystem, accuracy, all top notch.One exception: I did migrate two very large tables (15B+ rows) from Postgres to MySQL for performance reasons. InnodB (MySQL storage engine) can arrange the records on page by primary key, and if you have a multi-value PK (user_id, uuid) it means all records from a user are in the same set of pages. Huuuuge improvement over having your data for a user spread out over N different pages. Memory cache way more efficient. Orders of magnitude speed up on cold queries, better cache hit rate, and cost reduction from smaller servers."
55,jhas78asd 10 months ago
55,| parent | next [–]
55,Did you implement table partitioning with Postgres or consider that before moving?
55,scosman 10 months ago
55,| root | parent | next [–]
55,"Been a while. IIRC it didn’t work with millions of users, and partitioning with many users per partition would have same issues. Also looked at CLUSTER but there was no workable online version."
55,jstx1 10 months ago
55,| prev | next [–]
55,"Also interested in the responses, not because it seems like a close decision but because I would pick postgres by default for anything (anything that isn't simple enough to be done in sqlite)."
55,majestic5762 10 months ago
55,| parent | next [–]
55,Same.
55,azurelake 10 months ago
55,| prev | next [–]
55,"MySQL is still ahead operationally (no vacuum, group replication, gh-ost, optimizer hints, etc.). I would choose it unless one of the Postgres extensions is a killer fit for your project."
55,gardenhedge 10 months ago
55,| prev | next [–]
55,If Postgres was that much better than MySQL then you would expect to see exact reasons on why to pick it. Every comment so far has not listed any reason.
55,mort96 10 months ago
55,| parent | next [–]
55,"Ok, here's one: When you give MySQL invalid data, its standard behavior is to just silently store garbage in your database in many cases where PostgreSQL would've told you that your data is invalid.MySQL's handling of unicode has also been terrible historically, with way too many foot guns, but I don't know if that may be better with recent versions.People aren't providing strong reasons because the question wasn't ""what are some objective reasons for picking one over another"", but ""how do"
55,"you pick between them"". People are simply answering the question OP asked, and a lot of people's process is simply to pick PostgreSQL."
55,AdamJacobMuller 10 months ago
55,| root | parent | next [–]
55,"A lot of the MySQL issues historically have been fixed. UTF-8 is better now, invalid data handling is better (by default even! though your distros default config probably puts you back in permissive mode!) but regardless I'm still using Postgres every single time.The fact is that MySQL historically was terrible for complex schemas with complex types while postgres was a pleasure to work with. MySQL had a huge performance edge for many years but that came at a cost of resiliency and reliability. MySQL has greatly improved on these key areas and Postgres has also made significant performance improvements. Systems these days are also so powerful that the database probably isn't your benchmark.Regardless, I always use Postgres every single time because I am just scarred from years of dealing with MySQL. What even is MySQL is also an open question at this point, there's MySQL and MariaDB and Percona flavors and the founder of Percona was just ousted and I can't be bothered to put in mental energy to untangle all this to even figure out what MySQL I should be developing against.Compare this to Postgres where the community seems to have an extremely steady hand and constant progress. There's no forks, there's no infighting, there's no drama, there's a great regular release schedule with incremental improvements."
55,bcrosby95 10 months ago
55,| root | parent | next [–]
55,"In MySQLandia, we do not speak of utf8, we only speak of utf8mb4."
55,evanelias 10 months ago
55,| root | parent | prev | next [–]
55,"> When you give MySQL invalid data, its standard behavior is to just silently store garbageThis is a common misconception, but this hasn't been the case for over 7 years. MySQL 5.7, released in Oct 2015, changed its defaults to enable strict sql_mode. All prior versions have hit end-of-life for support years ago, so there is no modern version of MySQL with this silent truncation behavior.The only reason this problem persists is because Amazon RDS (all versions and forms, including Aurora) uses nonstandard default settings which disable strict mode!That all said, I do believe Postgres is an excellent database, and a great choice for quite a large range of use-cases. But please, let's compare 2023 Postgres with 2023 MySQL, not 2023 Postgres with much older MySQL. It's only fair."
55,justinclift 10 months ago
55,| root | parent | next [–]
55,"> But please, let's compare 2023 Postgres with 2023 MySQL, not 2023 Postgres with much older MySQL. It's only fair.Heh Heh HehOn a humorous note, the official MySQL page (in the early 2000's) comparing MySQL vs other databases had the same problem.They'd list the latest and greatest MySQL version, but compare it against archaic versions of the others."
55,"Clearly on purpose, because ""Marketing"" probably.Asked them (via official @postgresql.org email address) to please update that page to more a recent PostgreSQL, for fairness."
55,And was completely ignored of course.So it's kind of amusing to see a request for fairness in the opposite direction (which I agree with anyway) ~20 years later. ;)
55,jbverschoor 10 months ago
55,| root | parent | prev | next [–]
55,"Why do they have these settings? Because shit software used shit MySQL +data corruption.It might be better now. But for me it’s in the same shithole as mongodb, php (old style, so no recovery there even though it’s possible to create proper code) and most JavaScript.Other things is that people don’t even want to use Oracle’s MySQL but MariaDB. Why the hell would I want to run a fork of something, and still keep calling it something else.The only reason for MySQL is wordpress"
55,bakugo 10 months ago
55,| root | parent | prev | next [–]
55,"> MySQL's handling of unicode has also been terrible historically, with way too many foot guns, but I don't know if that may be better with recent versions.Unicode generally ""just works"" if the charset in use is utf8mb4. As of MySQL 8.0, this is the default."
55,mort96 10 months ago
55,| root | parent | next [–]
55,"Ah, that's good to hear. I haven't looked seriously at databases other than SQLite for a long time, it would be interesting to see a more up to date evaluation."
55,CaveTech 10 months ago
55,| root | parent | prev | next [–]
55,"Hasnt been the case for a few major versions, unless you want to anthropomorphise your db and and hold it accountable for past behaviour."
55,ok_dad 10 months ago
55,| root | parent | next [–]
55,"> unless you want to anthropomorphise your db and and hold it accountable for past behaviourNo one is holding the literal bits that make up the database executable accountable here, they are indicating they don't trust the devs of MySQL/MariaDB to do a good job. Whether or not that is an accurate view on their part is arguable, but it's pretty clear from context that they don't think that several if/else statements had it out for them."
55,nicoburns 10 months ago
55,| parent | prev | next [–]
55,A few reasons:- Transactional DDL statements (schema modifications)- Better support for UPSERT operations- Better JSON support (including ability to index into JSON columns)- the RETURNING statement to return data that was inserted/updatedIn general Postgres is a lot more featureful than MySQL.
55,et-al 10 months ago
55,| parent | prev | next [–]
55,"If you're doing any sort of spatial logic (e.g. mapping), you'll want to use PostGIS."
55,justin_oaks 10 months ago
55,| parent | prev | next [–]
55,"One thing that bit me on MySQL is that triggers don't execute on cascade deletes/updates: https://bugs.mysql.com/bug.php?id=11472That issue was filed in 2005 and it still isn't fixed.Another gripe I have is that MySQL's JSON Path capabilities are much more limited than the Postgres JSON Path capabilities. MySQL doesn't have filter expressions nor any operators/methods but Postgres does. Don't get me wrong, neither is jq, but I hate having to jump through extra hoops to get my JSON in the right format.Comparehttps://www.postgresql.org/docs/current/functions-json.html#...andhttps://dev.mysql.com/doc/refman/8.0/en/json.html#json-path-..."
55,l5ymep 10 months ago
55,| parent | prev | next [–]
55,"Postgres has a worse implementation of MVCC. It results in more bloat being produced, and slightly slower updates in a highly concurrent environment. Most businesses don't operate at the scale where this matters. But on the flip side, the tooling and developer experience is much better."
55,eqvinox 10 months ago
55,| parent | prev | next [–]
55,Array data type.This has saved my ass a bunch of times.
55,"Not even as a column type, just in complex queries that otherwise became unwieldy monsters. The usual ""started out simple but now it's a frankenbase"" and you're stuck with a shitty schema."
55,(The one thing worse than refactoring code:
55,refactoring databases!)In one case I was able to replace a 15-minute process with thousands of queries with one single query that aggregated all the data into a bunch of arrays.
55,It completed in a few seconds.
55,"(Doing it without arrays would have been possible, but duplicated a lot of data in the result set.)"
55,CuriouslyC 10 months ago
55,| prev | next [–]
55,"The only instance where I'd choose mysql over postgres is if your database needs are very simple, but you need to be able to scale hard, and your devops aren't skilled enough to manage an advanced postgres setup."
55,mgl 10 months ago
55,| prev | next [–]
55,We choose Postgres for extensibility and stability :)
55,david927 10 months ago
55,| parent | next [–]
55,I was going to make a similar joke: you look at them and choose Postgres
55,m0llusk 10 months ago
55,| prev | next [–]
55,PostgreSQL is a community thing and MySQL is Oracle.
55,Maybe make some basic benchmarks for comparison?
55,mixmastamyk 10 months ago
55,| prev | next [–]
55,"1) The choice is Postgres if you care about your data at all.2) Yes, if you are already HUGE and have requirements on Vitesse then by all means use it."
55,"If so, you are not asking this question—see #1.3) It's a blog or something where it doesn't matter, use a static site generator."
55,MrThoughtful 10 months ago
55,| prev | next [–]
55,By choosing SQLite.No server process and a single file per DB which I can put wherever I like.
55,mort96 10 months ago
55,| parent | next [–]
55,"I like SQLite. But I really wish its default behavior wasn't to simply allow garbage data into the database. If I have an int column, don't let me accidentally store a string."
55,eesmith 10 months ago
55,| root | parent | next [–]
55,"https://www.sqlite.org/stricttables.html""In a CREATE TABLE statement, if the ""STRICT"" table-option keyword is added to the end, after the closing "")"", then strict typing rules apply to that table. ... The STRICT keyword at the end of a CREATE TABLE statement is only recognized by SQLite version 3.37.0 (2021-11-27) and later."
55,sqlite> create table x (a int);
55,sqlite> insert into x values ('hello');
55,sqlite> select * from x;
55,hello
55,sqlite> drop table x;
55,sqlite> create table x (a int) strict;
55,sqlite> insert into x values ('hello');
55,Runtime error: cannot store TEXT value in INT column x.a (19)
55,mort96 10 months ago
55,| root | parent | next [–]
55,"Yeah, I know about that, and I'm doing that on all my tables these days. It's just sad that the default behaviour is to allow garbage data, and that if you ever forget to put 'strict' on your tables you'll have a perfectly functional application with no sign that anything is wrong until you suddenly find corrupt data in your database."
55,euroderf 10 months ago
55,| root | parent | prev | next [–]
55,create table strict
55,lisasays 10 months ago
55,| parent | prev | next [–]
55,SQLite is great for its scope - but not in the same class as a full-fledged RDBMS.
55,MrThoughtful 10 months ago
55,| root | parent | next [–]
55,Can you give a specific example of what you are missing when using SQLite?
55,mort96 10 months ago
55,| root | parent | next [–]
55,"At a certain scale, you'll want replication or replication, which SQLite doesn't really do AFAIK. At a scale below that, you'll probably want to be able to have multiple web servers talking to one database server, which SQLite doesn't really do either. I also think SQLite's performance during heavy write workloads is worse than PostgreSQL's?Basically, AFAIK, SQLite becomes problematic once you need more than one computer to handle requests."
55,justinclift 10 months ago
55,| root | parent | next [–]
55,"Just to point out, there are now SQLite replication and various ""distributed database"" projects which seem to work fairly well.They're probably not as battle tested as the PostgreSQL ones, but they are around, have users, and are actively developed.The ones I remember off the top of my head:* https://litestream.io* https://github.com/rqlite/rqlite"
55,"<-- more of a ""distributed database using RAFT"" type of thing* https://github.com/canonical/dqlite"
55,mort96 10 months ago
55,| root | parent | next [–]
55,"Yeah, there are SQLite forks or products built on SQLite which have these sorts of features, but SQLite doesn't. They remains reasons why someone may want to use another database (such as PostgreSQL, or an SQLite fork, or a database which uses SQLite as a storage engine) instead of SQLite.Honestly though, if I need these sorts of distribution features, I would probably prefer the database to have them built in. I don't really see the point in using SQLite at that scale."
55,otoolep 10 months ago
55,| root | parent | prev | next [–]
55,"rqlite[1] creator here, happy to answer any questions.[1] https://www.rqlite.io"
55,lisasays 10 months ago
55,| root | parent | prev | next [–]
55,"Here's a good place to start:https://www.sqlite.org/whentouse.htmlhttps://www.sqlite.org/quirks.htmlFull-scale RDBMSs, especially Postgres, have lots of goodies that either SQLlite doesn't have (or which it does have, but which aren't so richly featured)."
55,"Once you've gotten hooked on a few of these, the distinction will feel a lot more clear.Meanwhile the tipping points in favor of SQLite seem to be embedded systems, and its whole ""service-less"" architecture and plain ease of use."
55,"Which is why it still gets lots of love, for those contexts."
55,endisneigh 10 months ago
55,| root | parent | prev | next [–]
55,"Multi writer, access through the internet without 3rd party software, etc."
55,Avamander 10 months ago
55,| root | parent | prev | next [–]
55,"Performance, especially after a while and certain size."
55,jjav 10 months ago
55,| parent | prev | next [–]
55,"> By choosing SQLite.It's a good reminder to give some thought to whether one actually needs MySQL|Postgres. If not, SQLite is the way to go. Most of my code that uses a DB is using SQLite.But obviously, if you actually need MySQL|Postgres then SQLite is not an option."
55,89 more comments...
55,Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
55,Search:
56,JDBC To Other Databases - Spark 3.3.2 Documentation
56,3.3.2
56,Overview
56,Programming Guides
56,Quick Start
56,"RDDs, Accumulators, Broadcasts Vars"
56,"SQL, DataFrames, and Datasets"
56,Structured Streaming
56,Spark Streaming (DStreams)
56,MLlib (Machine Learning)
56,GraphX (Graph Processing)
56,SparkR (R on Spark)
56,PySpark (Python on Spark)
56,API Docs
56,Scala
56,Java
56,Python
56,"SQL, Built-in Functions"
56,Deploying
56,Overview
56,Submitting Applications
56,Spark Standalone
56,Mesos
56,YARN
56,Kubernetes
56,More
56,Configuration
56,Monitoring
56,Tuning Guide
56,Job Scheduling
56,Security
56,Hardware Provisioning
56,Migration Guide
56,Building Spark
56,Contributing to Spark
56,Third Party Projects
56,Spark SQL Guide
56,Getting Started
56,Data Sources
56,Generic Load/Save Functions
56,Generic File Source Options
56,Parquet Files
56,ORC Files
56,JSON Files
56,CSV Files
56,Text Files
56,Hive Tables
56,JDBC To Other Databases
56,Avro Files
56,Whole Binary Files
56,Troubleshooting
56,Performance Tuning
56,Distributed SQL Engine
56,PySpark Usage Guide for Pandas with Apache Arrow
56,Migration Guide
56,SQL Reference
56,JDBC To Other Databases
56,Data Source Option
56,Spark SQL also includes a data source that can read data from other databases using JDBC. This
56,functionality should be preferred over using JdbcRDD.
56,This is because the results are returned
56,as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
56,The JDBC data source is also easier to use from Java or Python as it does not require the user to
56,provide a ClassTag.
56,"(Note that this is different than the Spark SQL JDBC server, which allows other applications to"
56,run queries using Spark SQL).
56,To get started you will need to include the JDBC driver for your particular database on the
56,"spark classpath. For example, to connect to postgres from the Spark Shell you would run the"
56,following command:
56,./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
56,Data Source Option
56,Spark supports the following case-insensitive options for JDBC. The Data source options of JDBC can be set via:
56,the .option/.options methods of
56,DataFrameReader
56,DataFrameWriter
56,OPTIONS clause at CREATE TABLE USING DATA_SOURCE
56,"For connection properties, users can specify the JDBC connection properties in the data source options."
56,user and password are normally provided as connection properties for
56,logging into the data sources.
56,Property NameDefaultMeaningScope
56,url
56,(none)
56,"The JDBC URL of the form jdbc:subprotocol:subname to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret"
56,read/write
56,dbtable
56,(none)
56,The JDBC table that should be read from or written into. Note that when using it in the read
56,path anything that is valid in a FROM clause of a SQL query can be used.
56,"For example, instead of a full table you could also use a subquery in parentheses. It is not"
56,allowed to specify dbtable and query options at the same time.
56,read/write
56,query
56,(none)
56,A query that will be used to read data into Spark. The specified query will be parenthesized and used
56,as a subquery in the FROM clause. Spark will also assign an alias to the subquery clause.
56,"As an example, spark will issue a query of the following form to the JDBC Source."
56,SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
56,Below are a couple of restrictions while using this option.
56,It is not allowed to specify dbtable and query options at the same time.
56,It is not allowed to specify query and partitionColumn options at the same time. When specifying
56,"partitionColumn option is required, the subquery can be specified using dbtable option instead and"
56,partition columns can be qualified using the subquery alias provided as part of dbtable.
56,Example:
56,"spark.read.format(""jdbc"")"
56,".option(""url"", jdbcUrl)"
56,".option(""query"", ""select c1, c2 from t1"")"
56,.load()
56,read/write
56,driver
56,(none)
56,The class name of the JDBC driver to use to connect to this URL.
56,read/write
56,"partitionColumn, lowerBound, upperBound"
56,(none)
56,"These options must all be specified if any of them is specified. In addition,"
56,numPartitions must be specified. They describe how to partition the table when
56,reading in parallel from multiple workers.
56,"partitionColumn must be a numeric, date, or timestamp column from the table in question."
56,Notice that lowerBound and upperBound are just used to decide the
56,"partition stride, not for filtering the rows in table. So all rows in the table will be"
56,partitioned and returned. This option applies only to reading.
56,read
56,numPartitions
56,(none)
56,The maximum number of partitions that can be used for parallelism in table reading and
56,writing. This also determines the maximum number of concurrent JDBC connections.
56,"If the number of partitions to write exceeds this limit, we decrease it to this limit by"
56,calling coalesce(numPartitions) before writing.
56,read/write
56,queryTimeout
56,The number of seconds the driver will wait for a Statement object to execute to the given
56,"number of seconds. Zero means there is no limit. In the write path, this option depends on"
56,"how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver"
56,checks the timeout of each query instead of an entire JDBC batch.
56,read/write
56,fetchsize
56,"The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows)."
56,read
56,batchsize
56,1000
56,"The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing."
56,write
56,isolationLevel
56,READ_UNCOMMITTED
56,"The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in java.sql.Connection."
56,write
56,sessionInitStatement
56,(none)
56,"After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(""sessionInitStatement"", """"""BEGIN execute immediate 'alter session set ""_serial_direct_read""=true'; END;"""""")"
56,read
56,truncate
56,false
56,"This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off truncate option to use DROP TABLE again. Also, due to the different behavior of TRUNCATE TABLE among DBMS, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect, the user option truncate is ignored."
56,write
56,cascadeTruncate
56,"the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect"
56,"This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care."
56,write
56,createTableOptions
56,"This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.)."
56,write
56,createTableColumnTypes
56,(none)
56,"The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: ""name CHAR(64), comments VARCHAR(1024)""). The specified types should be valid spark sql data types."
56,write
56,customSchema
56,(none)
56,"The custom schema to use for reading data from JDBC connectors. For example, ""id DECIMAL(38, 0), name STRING"". You can also specify partial fields, and the others use the default type mapping. For example, ""id DECIMAL(38, 0)"". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults."
56,read
56,pushDownPredicate
56,true
56,"The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source."
56,read
56,pushDownAggregate
56,false
56,"The option to enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Otherwise, if sets to true, aggregates will be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If numPartitions equals to 1 or the group by key is the same as partitionColumn, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output."
56,read
56,pushDownLimit
56,false
56,"The option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is false, in which case Spark does not push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to true, LIMIT or LIMIT with SORT is pushed down to the JDBC data source. If numPartitions is greater than 1, SPARK still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and numPartitions equals to 1, SPARK will not apply LIMIT or LIMIT with SORT on the result from data source."
56,read
56,pushDownTableSample
56,false
56,"The option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is false, in which case Spark does not push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to true, TABLESAMPLE is pushed down to the JDBC data source."
56,read
56,keytab
56,(none)
56,"Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by --files option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise --files assumed. If both keytab and principal are defined then Spark tries to do kerberos authentication."
56,read/write
56,principal
56,(none)
56,Specifies kerberos principal name for the JDBC client. If both keytab and principal are defined then Spark tries to do kerberos authentication.
56,read/write
56,refreshKrb5Config
56,false
56,This option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before
56,"establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false."
56,"The default value is false. Note that if you set this option to true and try to establish multiple connections,"
56,a race condition can occur. One possble situation would be like as follows.
56,refreshKrb5Config flag is set with security context 1
56,A JDBC connection provider is used for the corresponding DBMS
56,The krb5.conf is modified but the JVM not yet realized that it must be reloaded
56,Spark authenticates successfully for security context 1
56,The JVM loads security context 2 from the modified krb5.conf
56,Spark restores the previously saved security context 1
56,The modified krb5.conf content just gone
56,read/write
56,connectionProvider
56,(none)
56,"The name of the JDBC connection provider to use to connect to this URL, e.g. db2, mssql."
56,Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle
56,the specified driver and options. The selected provider must not be disabled by spark.sql.sources.disabledJdbcConnProviderList.
56,read/write
56,Note that kerberos authentication with keytab is not always supported by the JDBC driver.
56,"Before using keytab and principal configuration options, please make sure the following requirements are met:"
56,The included JDBC driver version supports kerberos authentication with keytab.
56,There is a built-in connection provider which supports the used database.
56,There is a built-in connection providers for the following databases:
56,DB2
56,MariaDB
56,MS Sql
56,Oracle
56,PostgreSQL
56,"If the requirements are not met, please consider using the JdbcConnectionProvider developer API to handle custom authentication."
56,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
56,// Loading data from a JDBC source
56,val jdbcDF = spark.read
56,".format(""jdbc"")"
56,".option(""url"", ""jdbc:postgresql:dbserver"")"
56,".option(""dbtable"", ""schema.tablename"")"
56,".option(""user"", ""username"")"
56,".option(""password"", ""password"")"
56,.load()
56,val connectionProperties = new Properties()
56,"connectionProperties.put(""user"", ""username"")"
56,"connectionProperties.put(""password"", ""password"")"
56,val jdbcDF2 = spark.read
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
56,// Specifying the custom data types of the read schema
56,"connectionProperties.put(""customSchema"", ""id DECIMAL(38, 0), name STRING"")"
56,val jdbcDF3 = spark.read
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
56,// Saving data to a JDBC source
56,jdbcDF.write
56,".format(""jdbc"")"
56,".option(""url"", ""jdbc:postgresql:dbserver"")"
56,".option(""dbtable"", ""schema.tablename"")"
56,".option(""user"", ""username"")"
56,".option(""password"", ""password"")"
56,.save()
56,jdbcDF2.write
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
56,// Specifying create table column data types on write
56,jdbcDF.write
56,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties)"
56,"Find full example code at ""examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"" in the Spark repo."
56,// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
56,// Loading data from a JDBC source
56,Dataset<Row> jdbcDF = spark.read()
56,".format(""jdbc"")"
56,".option(""url"", ""jdbc:postgresql:dbserver"")"
56,".option(""dbtable"", ""schema.tablename"")"
56,".option(""user"", ""username"")"
56,".option(""password"", ""password"")"
56,.load();
56,Properties connectionProperties = new Properties();
56,"connectionProperties.put(""user"", ""username"");"
56,"connectionProperties.put(""password"", ""password"");"
56,Dataset<Row> jdbcDF2 = spark.read()
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
56,// Saving data to a JDBC source
56,jdbcDF.write()
56,".format(""jdbc"")"
56,".option(""url"", ""jdbc:postgresql:dbserver"")"
56,".option(""dbtable"", ""schema.tablename"")"
56,".option(""user"", ""username"")"
56,".option(""password"", ""password"")"
56,.save();
56,jdbcDF2.write()
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
56,// Specifying create table column data types on write
56,jdbcDF.write()
56,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"")"
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", connectionProperties);"
56,"Find full example code at ""examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java"" in the Spark repo."
56,# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
56,# Loading data from a JDBC source
56,jdbcDF = spark.read \
56,".format(""jdbc"") \"
56,".option(""url"", ""jdbc:postgresql:dbserver"") \"
56,".option(""dbtable"", ""schema.tablename"") \"
56,".option(""user"", ""username"") \"
56,".option(""password"", ""password"") \"
56,.load()
56,jdbcDF2 = spark.read \
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
56,"properties={""user"": ""username"", ""password"": ""password""})"
56,# Specifying dataframe column data types on read
56,jdbcDF3 = spark.read \
56,".format(""jdbc"") \"
56,".option(""url"", ""jdbc:postgresql:dbserver"") \"
56,".option(""dbtable"", ""schema.tablename"") \"
56,".option(""user"", ""username"") \"
56,".option(""password"", ""password"") \"
56,".option(""customSchema"", ""id DECIMAL(38, 0), name STRING"") \"
56,.load()
56,# Saving data to a JDBC source
56,jdbcDF.write \
56,".format(""jdbc"") \"
56,".option(""url"", ""jdbc:postgresql:dbserver"") \"
56,".option(""dbtable"", ""schema.tablename"") \"
56,".option(""user"", ""username"") \"
56,".option(""password"", ""password"") \"
56,.save()
56,jdbcDF2.write \
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
56,"properties={""user"": ""username"", ""password"": ""password""})"
56,# Specifying create table column data types on write
56,jdbcDF.write \
56,".option(""createTableColumnTypes"", ""name CHAR(64), comments VARCHAR(1024)"") \"
56,".jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"","
56,"properties={""user"": ""username"", ""password"": ""password""})"
56,"Find full example code at ""examples/src/main/python/sql/datasource.py"" in the Spark repo."
56,# Loading data from a JDBC source
56,"df <- read.jdbc(""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
56,# Saving data to a JDBC source
56,"write.jdbc(df, ""jdbc:postgresql:dbserver"", ""schema.tablename"", user = ""username"", password = ""password"")"
56,"Find full example code at ""examples/src/main/r/RSparkSQLExample.R"" in the Spark repo."
56,CREATE TEMPORARY VIEW jdbcTable
56,USING org.apache.spark.sql.jdbc
56,OPTIONS (
56,"url ""jdbc:postgresql:dbserver"","
56,"dbtable ""schema.tablename"","
56,"user 'username',"
56,password 'password'
56,INSERT INTO TABLE jdbcTable
56,SELECT * FROM resultTable
57,"Generate SQL Queries In Seconds For Free - SQLAI.aiSQLAI.aiAppFeatures Generate SQL With AIGenerate SQL using everyday language.Optimize SQL With AIOptimize SQL and save database resources.Fix SQL With AICheck SQL syntax and let AI fix it.Explain SQL With AIQuickly understand what a SQL query does.Train Your AI (new)Effortlessly train AI to know your database schema.PricingLoading..SQL GeneratorGenerate SQL Queries in Seconds With AIAI generates, fixes, explains and optimizes SQL queries. Add your database schema and effortlessly train AI to understand it using AI-powered vector search. This ensures unparalleled accuracy.Get StartedAppTrusted by more than 100,000 professionalsSQL Made Dirt SimpleAI enables all to benefit from SQL and experts to move faster. Trained AI ensures even more accurate AI generations.SQL GeneratorGenerate SQL Using Everyday LanguageAI turns your instructions into simple or complex SQL queries, including NoSQL.Optimized SQL GeneratorGenerate Optimized SQL Using Everyday LanguageSwitch to the performance optimized SQL generator to get faster queries and optimization tips.Fix SQL QueriesFix SQL Queries With a ClickCopy paste the SQL query that need fixing and AI will generate a fixed query.Optimize SQL QueryOptimize SQL Queries With a ClickCopy paste the SQL query to be optimized and AI generates an optimized query.Explain SQL QueryUnderstand SQL Queries With a ClickCopy paste the SQL query and AI will give a detailed inline explanation.Format SQL QueryFormat SQL Queries With a ClickCopy paste the SQL query and AI will format the query for improved readability.Generate SQL DataGenerate SQL Data Using Everyday LanguageTell AI what data you want and it will generate dummy data in seconds, e.g. 10 users and 20 comments.Data Analytics as Easy as TextingUtilize the power of AI combined with connected databases.Data AnalyticsAsk Your Data AnythingDrag'n'drop your data and AI will examine your data and return a answer.Data InsightsReal-Time Data Insights Right at Your Finger TipsRun AI-generated SQL and NoSQL queries on connected databases and effortless get real-time data insights.Read more Data DashboardsEffortlessly Build Data DashboardsGenerate insightful SQL queries with AI and simply save them to a data dashboard.Read more What our users sayAs a small business owner, this tool has been a game changer. I can now optimize our database without hiring a specialist. It's saved us time and money, and I can confidently say it's improved our operations significantly.Paul G.Verified purchaseI was always intimidated by SQL queries, but this platform has made it incredibly approachable. It feels like I have a tutor right next to me, guiding and improving my queries. Highly recommend for beginners!Maria T.Verified purchaseThe AI-driven recommendations for fixing and optimizing our SQL queries have been spot on. As a database administrator, it's like having a second set of eyes ensuring everything runs smoothly. Efficiency is up, and errors are down.Liang W.Verified purchaseI can't express enough how much time and stress this tool has saved me. The user interface is intuitive, and the AI suggestions are incredibly insightful, especially for complex database operations.Emily R.Verified purchaseComing from a background with minimal SQL experience, I found the learning curve for this tool very gentle. It not only helps fix my mistakes but also explains why they were wrong, which is invaluable for my learning.Raj P.Verified purchaseAs a data scientist, efficiency and accuracy are key. This tool has significantly reduced the time I spend debugging and optimizing queries. It's an essential part of my data toolkit now.Isabella S.Verified purchaseI was skeptical about how much an AI could help with SQL queries, but I stand corrected. This platform has revolutionized the way our team handles databases, making us faster and more reliable.Henrik O.Verified purchaseWhat impressed me most was how this tool scales for different expertise levels. Whether you're a beginner or a seasoned expert, it provides immense value in making SQL tasks simpler and more efficient.Fatima Z.Verified purchaseFor startups like ours, this tool is a lifesaver. It ensures our database management is on point, letting us focus more on product development and less on backend issues. Absolutely a must-have for small tech teams!Olivia M.Verified purchaseThe AI-powered explanations and optimizations have made a noticeable difference in our database performance. It’s like having a skilled SQL developer on the team, but at a fraction of the cost.Gabriel K.Verified purchaseWhat sets this tool apart for me is its ability to teach while it assists. It's not just about fixing errors; it's about understanding them, which has helped me grow my SQL skills substantially.Sophie A.Verified purchaseThis platform has reduced our dependency on database consultants significantly. Now, we can solve complex SQL problems in-house, saving us both time and consultancy fees.Aarav J.Verified purchaseIt’s rare to find a tool that simplifies your workflow while also educating you. The improvements in our database management processes have been remarkable. Truly a game changer for us.Michelle D.Verified purchaseThe precision and intelligence of the AI in diagnosing and fixing SQL queries are unmatched. It's not just a tool; it's a comprehensive solution that has streamlined our database management.Lucas B.Verified purchaseNever thought I'd be able to handle SQL queries with such ease. This tool has empowered me to take on database tasks that I'd usually avoid. The confidence it gives you is amazing.Ana P.Verified purchaseThe ability to generate and optimize SQL queries instantly has transformed how we work. Our projects move faster, with fewer errors, making our clients happier. It's an indispensable tool for our team.Felix U.Verified purchaseI appreciate how it provides clear, actionable advice for optimizing queries. This tool doesn't just offer fixes; it offers learning opportunities, making us better developers in the process.Hana M.Verified purchaseThe impact on our productivity and efficiency has been profound. It’s not just about fixing queries; it’s about understanding them better, which in turn, makes our database management top-notch.Yuki S.Verified purchaseThis tool has been a game changer for remote teams like ours. It allows us to collaboratively troubleshoot and optimize SQL queries in real time, regardless of our varied skill levels.Samantha L.Verified purchaseFrom the seamless integration to the intuitive learning process, this tool has everything any business, large or small, would need to streamline their SQL query management and optimization efforts.Carlos E.Verified purchaseSupported DatabasesMySQLConnectPostgreSQLConnectSQL Server (MS)ConnectOracle PL/SQLConnectBigQuerySQLMariaDBConnectSQLiteSnowflakeDB2SybaseRedshiftTrino SQL"
57,"(AWS Athena)Salesforce SOQL/SOSLPrestoMongoDBConnectDynamoDBPartiQLGraphQLShow allGet StartedBoost your skills and productivity using AI today.Get Started SQLAI.aiProfessional SQL multi-tool for generating, fixing, explaining and optimizing SQL queries and databases.AppSQL examplesAboutBlogAffiliate ProgramContact© 2024 SQLAI.ai. Berlin, Germany.Get the latest AI & SQL developments:LinkedIn or Twitter"
58,Behind the scenes: Introducing OpenShift Virtualization Performance and Scale
58,Skip to contentFeatured linksSupportConsoleDevelopersStart a trial
58,"All Red HatFor customersCustomer supportDocumentationSupport casesSubscription managementRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesTraining and certification For developersHybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesAnsibleFor system administratorsFor architectsRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutExplore morePlatform productsRed Hat Enterprise LinuxA flexible, stable operating system to support hybrid cloud innovation."
58,"Red Hat OpenShiftA container platform to build, modernize, and deploy applications at scale."
58,Red Hat Ansible Automation PlatformA foundation for implementing enterprise-wide automation.
58,Try & buyStart a trialAssess a product with a no-cost trial.
58,Buy onlineBuy select products and services in the Red Hat Store.
58,"Integrate with major cloud providersBuy Red Hat solutions using committed spend from providers, including:"
58,"Featured cloud servicesBuild, deploy, and scale applications quickly. We’ll manage the rest."
58,Red Hat OpenShift Service on AWS
58,Red Hat OpenShift AI
58,Microsoft Azure Red Hat OpenShift
58,See all cloud services
58,See all products
58,By category
58,Application platform
58,Artificial intelligence
58,Edge computing
58,IT automation
58,Linux standardization
58,By organization type
58,Automotive
58,Financial services
58,Healthcare
58,Industrial sector
58,Media and entertainment
58,Public sector
58,Telecommunications
58,By customer
58,British Army
58,Edenor
58,HCA Healthcare
58,Macquarie Bank
58,Tata Consultancy Services
58,UPS
58,Search all success stories
58,Explore solutions
58,Services
58,Consulting
58,Open Innovation Labs
58,Technical Account Management
58,Training & certification
58,All courses and exams
58,All certifications
58,Verify a certification
58,Skills assessment
58,Learning subscription
58,Learning community
58,Red Hat Academy
58,FAQs
58,Connect with learning experts
58,Featured
58,Red Hat System Administration I (RH124)
58,Red Hat OpenShift Administration I (DO280)
58,Red Hat Certified Engineer (RHCE)
58,Explore services
58,Topics
58,Application modernization
58,Automation
58,Cloud computing
58,Cloud-native applications
58,Containers
58,DevOps
58,Edge computing
58,Linux
58,Virtualization
58,See all topics
58,Articles
58,What are cloud services?
58,What is edge computing?
58,What is hybrid cloud?
58,Why build a Red Hat cloud?
58,Cloud vs. edge
58,Red Hat OpenShift vs. Kubernetes
58,Learning Ansible basics
58,What is Linux?
58,More to explore
58,Blog
58,Customer success stories
58,Events and webinars
58,Newsroom
58,Podcasts and video series
58,Resource library
58,Training and certification
58,Explore resources
58,For customers
58,Our partners
58,Red Hat Ecosystem Catalog
58,Find a partner
58,For partners
58,Partner Connect
58,Become a partner
58,Training
58,Support
58,Access the partner portal
58,About us
58,Our company
58,How we work
58,Our social impact
58,Development model
58,Subscription model
58,Product support
58,Open source
58,Open source commitments
58,How we contribute
58,Red Hat on GitHub
58,Company details
58,Analyst relations
58,Blog
58,Locations
58,Newsroom
58,Communities
58,Ansible
58,For system administrators
58,For architects
58,Customer advocacy
58,Explore Red Hat
58,Contact us
58,"For customersCustomer supportDocumentationSupport casesSubscription managementRed Hat Ecosystem CatalogFind a partnerFor partnersPartner portalPartner supportBecome a partner Try, buy, & sellRed Hat MarketplaceRed Hat StoreContact salesStart a trialLearning resourcesTraining and certification For developersHybrid cloud learning hubInteractive labsLearning communityRed Hat TVOpen source communitiesAnsibleFor system administratorsFor architects"
58,For you
58,"NewRecommendationsAs you browse redhat.com, we'll recommend resources you may like. For now, try these.All Red Hat productsTech topicsRed Hat resourcesRed Hat SummitSupportConsoleDevelopersStart a trialContactSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol"
58,Contact us
58,English
58,Select a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed HatProductsSolutionsTraining & servicesResourcesPartnersAboutMenu
58,Search
58,For you
58,Contact us
58,English
58,Log in
58,ProductsSolutionsTraining & servicesResourcesPartnersAboutContact usSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañol
58,Red Hat blog
58,Blog menu
58,Latest posts
58,By product
58,Red Hat Enterprise Linux
58,Red Hat Insights
58,Red Hat OpenShift
58,Red Hat Ansible Automation Platform
58,Red Hat OpenStack Platform
58,Red Hat Cloud Storage and Data Services
58,All products
58,By channel
58,Red Hat News
58,Red Hat Services Speak
58,Cloud native computing
58,Red Hat Security
58,Open Hybrid Cloud
58,Management and automation
58,All channels
58,Behind the scenes: Introducing OpenShift Virtualization Performance and Scale
58,"January 9, 2024Jenifer Abrams"
58,Share
58,Back to all posts
58,"Tags:Products, Topics"
58,"Red Hat OpenShift Virtualization helps to remove workload barriers by unifying virtual machine (VM) deployment and management alongside containerized applications in a cloud-native manner. As part of the larger Performance and Scale team, we have been deeply involved in the measurement and analysis of VMs running on OpenShift since the early days of the KubeVirt open source project and have helped to drive product maturity through new feature evaluation, workload tuning and scale testing. This article dives into several of our focus areas and shares additional insights into running and tuning VM workloads on OpenShift.Tuning and scaling guidesOur team contributes to tuning and scaling documentation to help customers get the most out of their VM deployments. First, we have an overall tuning guide, which you can find in this knowledge base article. This guide covers recommendations for optimizing the Virtualization control plane for high VM ""burst"" creation rates and various tuning options at both the host and VM levels to improve workload performance.Second, we published an in-depth reference architecture that includes an example OpenShift cluster, Red Hat Ceph Storage (RHCS) cluster and network tuning details. It also examines sample VM deployment and boot storm timings, I/O latency scaling performance, VM migration and parallelism and performing cluster upgrades at scale.Team focus areasThe following sections provide an overview of some of our major focus areas and details about the testing we do to characterize and improve the performance of VMs running on OpenShift. Figure 1 below illustrates our focus areas."
58,"Figure 1: OpenShift Virtualization Performance focus areasWorkload performanceWe spend a lot of time focusing on key workloads covering compute, networking and storage components to make sure we have broad coverage. This work includes gathering continuous baselines on different hardware models, updating results as newer releases come out and diving deep into various tuning options to achieve optimal performance.One key workload focus area is database performance. We typically use HammerDB as the workload driver and focus on multiple database types, including MariaDB, PostgreSQL and MSSQL, so we can understand how databases with different characteristics perform. This template provides an example HammerDB VM definition.Another major workload focus area is a high throughput in-memory database, SAP HANA, with a goal of performing within 10% of bare metal. We achieve this by applying some isolation-style tuning at both the host and VM layer, including using CPUManager, adjusting process affinity controlled by systemd, backing the VM with hugepages, and using SRIOV network attachments.To further cover storage performance, we run a set of different I/O application patterns focusing on both IOPs (input/output operations per second) and latency using the Vdbench workload. The application patterns vary the block size, I/O operation type, size and number of files and directories, and adjust the mix of reads and writes. This allows us to cover various I/O behaviors to understand different performance characteristics. We also run another common storage microbenchmark, Fio, to measure various storage profiles. We test multiple persistent storage providers, but our main focus is OpenShift Data Foundation using Block mode RADOS Block Device (RBD) volumes in VMs.We also focus on different types of microbenchmarks to assess other component performance to round out some of these more complex workloads. For networking, we typically use the uperf workload to measure both Stream and RequestResponse test configurations for various message sizes and thread counts, focusing on both the default podnetwork and other container network interface (CNI) types, such as Linux Bridge and OVN-Kubernetes additional networks. For compute tests, we use a variety of benchmarks, such as stress-ng, blackscholes, SPECjbb2005 and others, depending on the focus area.Regression testingUsing an automation framework called benchmark-runner, we continuously run workload configurations and compare the results to known baselines to catch and fix any regressions in pre-release versions of OpenShift Virtualization. Since we care about virtualization performance, we run this continuous testing framework on bare metal systems. We compare workloads with similar configurations across pods, VMs and sandboxed containers to help us understand relative performance. This automation allows us to quickly install new pre-release versions of OpenShift and the operators we focus on, including OpenShift Virtualization, OpenShift Data Foundation, Local Storage Operator and OpenShift sandboxed containers. Characterizing the performance of pre-release versions multiple times each week allows us to catch any regressions early before they are released to customers and enables us to compare performance improvements over time as we update to newer releases with improved features.We are always expanding our continuous automated workload coverage, but the current set of workloads we run regularly includes database benchmarks, compute microbenchmarks, uperf, Vdbench, Fio, and both VM ""boot storm"" and pod startup latency tests that exercise various areas of the cluster to measure how quickly a bulk number of pods or VMs can be started at once.Migration performanceOne advantage of using a shared storage provider that allows RWX access mode is that VM workloads can more seamlessly live migrate during cluster upgrades. We consistently work to improve the speed at which VMs can migrate without significant workload interruption. This involves testing and recommending migration limits and policies to provide safe default values and testing much higher limits to uncover migration component bottlenecks. We also measure the benefits of creating a dedicated migration network and analyze node-level networking and per-VM migration metrics to characterize migration progress over the network.Scaling performanceWe regularly test high-scale environments to uncover any bottlenecks and evaluate tuning options. Our scale testing involves areas ranging from OpenShift control plane scale to Virtualization control plane scale, workload I/O latency scaling, migration parallelism, DataVolume cloning and VM ""burst"" creation tuning.Throughout this testing, we've discovered various scale-related bugs that ultimately led to improvements, allowing us to push the next round of scale tests even higher. Any scale-related best practices we find along the way we document in our general Tuning and Scaling Guide.Hosted cluster performanceAn emerging focus area for us is hosted control plane and hosted cluster performance, specifically examining on-premises bare metal hosted control planes and hosted clusters on OpenShift Virtualization, which uses the KubeVirt cluster provider.Some of our initial work areas are scale testing multiple instances of etcd (see the storage recommendation in the Important section), hosted control plane scale testing with heavy API workload, and hosted workload performance when managing hosted control plane clusters on OpenShift Virtualization. Check out our hosted cluster sizing guidance to see one of the major outcomes of this recent work.What's nextKeep an eye out for upcoming posts covering these performance and scale areas in more detail, including a deeper dive into hosted cluster sizing guidance methodology and in-depth VM migration tuning recommendations.In the meantime, we will keep measuring and analyzing the performance of VMs on OpenShift, pushing new scale boundaries, and focusing on catching and fixing any regressions before releases reach the hands of our customers!Learn more about Red Hat OpenShift Virtualization"
58,About the author
58,Jenifer Abrams
58,Principal Software Engineer
58,"Jenifer joined Red Hat in 2018 and leads the OpenShift Virtualization Performance team. Previously, she spent a decade working at IBM in the Linux Technology Center focused on Linux Performance."
58,Read full bioIcon-Red_Hat-Directional-A-Black-RGB
58,Enter keywords here to search blogs
58,UI_Icon-Red_Hat-Close-A-Black-RGB
58,Search
58,Subscribe to the feed
58,Related posts
58,Introducing OpenShift Service Mesh 2.5
58,(Re)Introducing the Red Hat Universal Base Image
58,Platform engineering for your IT team: How to get started
58,"LinkedInYouTubeFacebookTwitterProductsRed Hat Enterprise LinuxRed Hat OpenShiftRed Hat Ansible Automation PlatformCloud servicesSee all productsToolsTraining and certificationMy accountDeveloper resourcesCustomer supportRed Hat value calculatorRed Hat Ecosystem CatalogFind a partnerTry, buy, & sellProduct trial centerRed Hat MarketplaceRed Hat StoreBuy online (Japan)ConsoleCommunicateContact salesContact customer serviceContact trainingSocialAbout Red HatWe’re the world’s leading provider of enterprise open source solutions—including Linux, cloud, container, and Kubernetes. We deliver hardened solutions that make it easier for enterprises to work across platforms and environments, from the core datacenter to the network edge.Select a languageEnglish简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed Hat legal and privacy linksAbout Red HatJobsEventsLocationsContact Red HatRed Hat BlogDiversity, equity, and inclusionCool Stuff StoreRed Hat SummitRed Hat legal and privacy linksPrivacy statementTerms of useAll policies and guidelinesDigital accessibility"
59,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
59,Home Page
59,Cloud IaaS Solutions
59,Azure VM Solutions
59,AWS VM Solutions
59,GCP VM Solutions
59,WordPress SSO
59,About Us
59,Contact Us
59,Home Page
59,Cloud IaaS Solutions
59,Azure VM Solutions
59,AWS VM Solutions
59,GCP VM Solutions
59,WordPress SSO
59,About Us
59,Contact Us
59,Home Page
59,Cloud IaaS Solutions
59,Azure VM Solutions
59,AWS VM Solutions
59,GCP VM Solutions
59,WordPress SSO
59,About Us
59,Contact Us
59,Aug
59,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
59,by Dennis Muvaa
59,in MariaDBComments
59,MariaDB Performance Tuning: Optimize MariaDB for High Traffic. MariaDB is an open source relational database management system (RDBMS) designed as a suitable alternative to MySQL. Supports multiple storage engines and enables users to easily scale out their database.
59,"To achieve optimal database performance, it’s essential to optimize MariaDB server sufficiently. It reduces query time and improves user experience. Essential when running critical applications that rely on real time data processing. Optimizing MariaDB server leads to efficient resource usage, lower CPU and memory usage, and lower disk I/O."
59,This article discusses MariaDB Performance Tuning: Optimize MariaDB for High Traffic. Read on!
59,Also ReadHow to MariaDB/MySQL Show Users and GRANTS Privileges
59,How to Optimize Your MariaDB Server for High Traffic
59,1. Optimize Your Database Schema
59,"Well, it is essential to normalize your database schema. This step involves reducing data redundancy and bolstering data integrity by breaking down large tables into smaller ones and creating relationships between them. Also, denormalize tables at times i.e merge small tables into larger tables to minimize the need for complex querying."
59,Also ReadHow to setup MariaDB on Windows in Azure/AWS/GCPHow to Setup MariaDB Ubuntu Cloud in Azure/AWS/GCP
59,2. Choose Appropriate Data Types
59,"Choose the right data types for your table columns. It saves space, improves query speed, and strengthens data integrity. In addition, consider using indexes. They greatly improve the speed of data retrieval operations. However, it’s best to note that while indexes improve read performance, they also slow down write operations."
59,3. Use Connection Pooling
59,"Connection pooling helps optimize MariaDB server performance by addressing connection overhead issues. Each new database connection comes with a significant amount of overhead due to handshaking protocols and authentication procedures. These procedures consume considerable processing power and time, especially if the application needs to frequently open and close connections. Therefore, by allowing a pool of reusable connections, the system does to have to repeat this process multiple times, hence improving MariaDB performance."
59,"Use persistent connections, that remain open over multiple requests. This significantly reduces the latency. Overall, it reduces the time it takes to execute a large number of database queries. Especially helpful in high-volume transaction."
59,"Also, leverage connection pooling solutions like ProxySQL. ProxySQL acts as a gateway between MariaDB and the application layer. It manages the connections to the database server and effectively reduces the burden on the database. The pooling solution dynamically adjusts the number of connections in the pool based on the load, allowing for better resource management. Most pooling solutions come with intelligent routing capabilities that help direct queries to different servers based on their type and load."
59,"Also ReadWhat are MariaDB Data Types (Numeric, Date, String) Explained"
59,4. Use Appropriate Storage Engines
59,Following with MariaDB Performance Tuning: Optimize MariaDB for High Traffic. is to choose the right storage engine significantly affects performance. MariaDB has 6 main storage engines:
59,InnoDBMyISAMMariaDB ColumnStoreAriaSpiderMyRocks
59,"InnoDB, the default storage engine for MariaDB and is ideal for transaction heavy workloads. It supports row-level locking, allowing for higher concurrency and faster recovery. Alternatively, use MyISAM for workloads with heady read and light write scenarios. This storage engine has a small footprint that InnoDB anand consumes less memory and disk space than InnoDB. Aria is a modern improvement of MyISAM and allows easy copying between systems."
59,ColumnStore has a parallel data architecture and is suitable for processing petabytes of data. MyRocks provides greater compression than InnoDB and is ideal for high throughput operations. Spider supports partitioning and allows handling of multiple instances as if they were on the same instance.
59,"There are lots of other MariaDB storage engines, each designed for particular operations."
59,5. Allocate Memory Sufficiently
59,Tune memory allocation to improve MariaDB performance. There are various buffers and caches to configure when allocating memory. These include:
59,join_buffer_sizenet_buffer_lengthread_buffer_sizesort_buffer_sizemax_heap_table_sizemrr_buffer_size
59,"The join_buffer_size controls the size of the buffer used for full join operations. The sort_buffer_size dictates the size of the buffer used for sorting. The read_buffer_size is used for sequential table scans, while read_rnd_buffer_size is used when the server reads rows in a sorted order following a sort operation."
59,Also ReadMariadb Create Table – How to Guide (Step by Step)
59,6. Fine-Tune Your Server Configuration
59,"Tweak server settings. Adjust InnoDB settings such as the innodb_buffer_pool_size, where InnoDB engine caches table and index data. A well proportioned buffer pool significantly boosts the database performance by lowering the amount of disk I/O."
59,"If you are using the MyISAM storage engine, consider optimizing parameters like key_buffer_size, used to cache index blocks for MyISAM tables, and thread_cache_size, which sets the number of threads that the server should cache for reuse."
59,"Remember to adjust MariaDB’s query cache feature for optimal performance. For instance, when a SELECT statement is executed, the result set is stored in the query cache. If a similar statement is executed again, the server retrieves the results from the cache instead of executing the query again."
59,7. Use the Right Table Partitioning
59,"Table partitioning improves performance of large databases. It involves dividing a single table into smaller, more manageable pieces. There are different types of table partitioning in MariaDB:"
59,List PartitioningRange PartitioningHash PartitioningKey Partitioning
59,"Range partitioning assigns a range of values to each partitioning. This means each partition contains rows for which the partitioning expression value lies within a given range. On the other hand, list partitioning assigns a list of values to each partition."
59,"In Hash partitioning, the server decides the partition for each data to achieve even partitions.  Well, it applies a consistent hash function to a key and then assigns rows based on the hash function’s result. Key partitioning is a variant of hash partitioning whereby the server assigns the distribution of rows across partitions."
59,Also ReadHow to Install MariaDB on Ubuntu 22.04
59,8. Optimize Your Query
59,"Optimizing your SQL queries helps boost MariaDB server performance. Well-written SQL query reduces the amount of time taken to fetch data. To understand how your queries perform, use the built-in EXPLAIN feature. It helps you understand how MariaDB executes your queries and enables you to see the different types of indexes are in place. With EXPLAIN, you also understand the sequence of table joins."
59,Optimize SQL query by reducing the result set size. Fetching more data than necessary not only slows your query but also adds more burden to your server. Restrict the number of rows your query returns through LIMIT clauses.
59,"Indexing all columns used in ‘where’, ‘group by’, ‘order by’ and ‘join’ clauses from the start. This ensures that the database does not do complete table scans in order to retrieve records. However, you should avoid using functions on indices as MariaDB tends to ignore the index."
59,9. Optimize Hardware Components
59,"To run an efficient MariaDB database, you need robust hardware with the right configurations. First, use solid-state drives (SSDs) for data storage rather than hard disks as they can significantly enhance disk I/O speed. SSDs provide faster read/write speeds compared to traditional hard drives."
59,"Memory optimization is also crucial, as a large memory means larger key and table caches. MariaDB stores data and indexes in memory to quicken read and write operations. To achieve optimal performance, it’s imperative to first set the server variables to utilize the available memory. In addition, use the highest RAM size per slot as it reduces latency compared to having more RAM slots on the motherboard."
59,"Secondly, you need extremely fast processors especially if you run critical workloads. A CPU with fast speeds allows for faster calculations enabling the clients to receive results quickly. CPU resources should also be allocated properly. A server with more CPUs handles more simultaneous connections and perform more operations in parallel."
59,Also ReadHow to Install MariaDB on Windows Server 2019 (Tutorial)
59,10. Use Concurrency and Threading
59,Understanding thread concurrency and how to apply it to your MariaDB server helps improve performance issues. Optimize how InnoDB multitasks between transactions and requests.
59,"InnoDB manages requests via threads, whereby each thread represents a single unit of processing. Therefore, you should configure system variables so as to have many threads executed at the same time."
59,11. Adjust Binary Log Settings
59,"The binary log records changes made to the database and is used for replication and recovery purposes. Choosing the right format for replication is crucial for performance. The row-based format logs changes made to each row, while the mixed format logs changes on a statement basis."
59,Adjust the binary log cache size to control the amount of memory allocated for caching changes before they are written to the binary log. Managing log retention with the expire_logs_days parameter avoids consuming too much disk space.
59,Also ReadHow to Install MariaDB on CentOS 8 (Step by Step Guide)
59,12. Disable DNS Reverse Lookups
59,"Disabling DNS reverse lookups improve the performance of a MariaDB database. Whenever a client connects to the database, MariaDB performs a DNS reverse lookup to convert the client’s IP address to a hostname. This process increases latency hence reducing speed although it provides additional security by verifying the identity of the client. The delay can be significant depending on the network configuration and the DNS server speed. This latency increases if there is a large number of clients connecting to the MariaDB server and is detrimental to the overall performance."
59,"By disabling DNS reverse lookups, MariaDB avoids potential delays and speeds up the process of establishing connections. This saves time especially in a system with high transaction rates or where numerous connections are being established and closed. Besides, it allows MariaDB to accept and handle more client connections in a shorter amount of time."
59,"Disabling DNS reverse lookups comes with potential security risks. DNS lookups provide an additional layer of protection by authenticating the source of incoming connections. Therefore, disabling DNS reverse lookups applies to systems with other robust security measures."
59,13. Check for Idle Connections
59,"Checking for idle connections is a crucial strategy in faster performance. Idle connections represent open database connections that are currently not in use. These connections, although inactive, consume resources. They occupy slots in the connection pool that could be used by other active connections, thereby potentially limiting the database’s ability to handle new incoming connections."
59,"Over time, a build-up of idle connections lead to a degradation in the performance of MariaDB. Identifying and properly handling these idle connections, such as by setting a timeout or closing them after a certain period of inactivity, frees up system resources and improve the overall performance."
59,14. Setting Your Disk I/O Scheduler
59,"Last point of the article MariaDB Performance Tuning: Optimize MariaDB for High Traffic. The Disk I/O Scheduler is a component of the operating system that decides in what order the block I/O operations is submitted to storage devices. Depending on the type of workload, different scheduling algorithms are more efficient. For example, some schedulers prioritize minimizing seek time for read-heavy applications, while others optimize for write-heavy or mixed workloads."
59,Thank you for reading MariaDB Performance Tuning: Optimize MariaDB Server for High Traffic. We shall conclude the rticle now.
59,Also ReadMariaDB vs MySQL Performance Differences (Pros and Cons)
59,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
59,Conclusion
59,"Optimizing MariaDB server is a continuous process, as performance is not pegged on a single factor. Use database monitoring tools to gain visibility into the state of their server and the database. By following the above practices, you are able to avoid lots of performance bottles and have your applications running seamlessly."
59,Related Posts:Optimize Memory & CPU Usage in Node.js: Performance Tuning TechniquesHyper-V Performance: Optimize VM Performance (Memory & CPU)MySQL Performance Tuning: For Optimal Database PerformanceWhat is HPC? High Performance Computing and How it WorksCloud HPC Architecture for Hybrid High-Performance ComputingUsing Redis for Caching: Best Practices and Performance Tuning
59,Tags:
59,MariaDB
59,Dennis Muvaa
59,"Dennis is an expert content writer and SEO strategist in cloud technologies such as AWS, Azure, and GCP."
59,"He's also experienced in cybersecurity, big data, and AI."
59,votes
59,Article Rating
59,Subscribe
59,Login and comment with
59,I allow to create an account
59,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
59,DisagreeAgree
59,Notify of
59,new follow-up comments
59,new replies to my comments
59,Login and comment with
59,I allow to create an account
59,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
59,DisagreeAgree
59,Please login to comment
59,0 Comments
59,Inline Feedbacks
59,View all comments
59,Cloud Infrastructure Services Ltd
59,2A Cedar Drive
59,Hatch End
59,Pinner
59,"HA5 4DE, UK"
59,Terms
59,Privacy Policy
59,Recent Posts
59,Snipe-IT IT Asset Inventory Management on Azure/AWS/GCP
59,Choosing the Right Proxy Server: A Detailed Guide for Businesses
59,Proxy Server 101: Understanding the Basics of Proxy Technology
59,Ansible Roles: Create / Use Roles for Configuration Management
59,The Future of Proxy Servers: Trends and Innovations
59,Magento Server Monitoring: Monitor Magento for Best Performance
59,Redis Techniques: Pub/Sub Messaging / Real-Time Data Analytics
59,SFTP Security: How to Secure File Transfers with SFTP
59,Squid Proxy with SquidGuard: How to Implement Content Filtering
59,How to Install Ansible on Ubuntu 23.04 Server
59,PagesContact Us
59,About Us
59,Azure Marketplace Solutions
59,AWS Marketplace Solutions
59,GCP Marketplace Solutions
59,Azure Management
59,Cloud IaaS Setup & Management Services
59,Active Directory Reporting Tool
59,WordPress SSO
59,Blog
59,Follow Us
59,"wpDiscuz00Would love your thoughts, please comment.x()x| ReplyInsert"
61,HAProxy Logging- How to Tune Timeouts for Performance - Papertrail
61,Tour
61,Product
61,Technology
61,Capabilities
61,Languages
61,Tips from the Team
61,Papertrail Guides
61,Help
61,Pricing
61,Blog
61,Contact
61,Request
61,Demo
61,Log In
61,Sign Up
61,SolarWinds.com Blog Contact Us
61,PRODUCTS
61,FEATURES
61,TIPS FROM THE TEAM
61,RESOURCES
61,LOG IN
61,REQUEST DEMOFREE TRIAL
61,TECHNOLOGY
61,CAPABILITIES
61,LANGUAGES
61,Make Your Logs Work for You
61,"The days of logging in to servers and manually viewing log files are over. SolarWinds® Papertrail™ aggregates logs from applications, devices, and platforms to a central location."
61,View Technology Info
61,FEATURED TECHNOLOGY
61,Kubenetes Cluster LoggingKubernetes LoggingAWS Log ManagementGC Log AnalyzerAWS LoggingDocker LoggingDocker SyslogHeroku LoggingLogging Tools for Azure App
61,Troubleshoot Fast and Enjoy It
61,"SolarWinds® Papertrail™ provides cloud-based log management that seamlessly aggregates logs from applications, servers, network devices, services, platforms, and much more."
61,View Capabilities Info
61,FEATURED CAPABILITIES
61,Log ViewerFirewall Log AnalyzerCentralized Log ManagementLog AnalysisTail LoggingCron Job MonitoringLog AggregatorCloud Logging
61,Aggregate and Search Any Log
61,"SolarWinds® Papertrail™ provides lightning-fast search, live tail, flexible system groups, team-wide access, and integration with popular communications platforms like PagerDuty and Slack to help you quickly track down customer problems, debug app requests, or troubleshoot slow database queries."
61,View Languages Info
61,FEATURED LANGUAGES
61,Linux Logging SoftwareLinux Log ManagementRuby Logging ManagerGolang LoggingHAproxy Log AnalyzerMySQL LoggingNGINX LoggingJavascript LoggingTomcat LoggingApache Log Analyzer
61,EVENT VIEWER
61,APM INTEGRATION
61,BUILT FOR COLLABORATION
61,Meet the Event Viewer
61,It's the heart of Papertrail.
61,Start Tour
61,TAKE A TOUR
61,Live TailSeek by TimeContextElegant SearchAPM IntegrationSkim-abilitySaved SearchesLog Velocity Analytics
61,TBD - APM Integration Title
61,TBD - APM Integration Description
61,TBD Link
61,APM Integration Feature List
61,Feature 1
61,TBD - Built for Collaboration Title
61,TBD - Built for Collaboration Description
61,TBD Link
61,Built for Collaboration Feature List
61,TIPS FROM THE TEAM
61,Tips from the Team
61,"By engineers, for engineers"
61,View More Tips
61,Additional Tips
61,Kubernetes LoggingMonitoring WordPress Error LogsA Guide to Log FilteringManaging and Analyzing Firewall LogsWhat Your Router Logs Say About Your NetworkHow to Diagnose App Issues Using Crash LogsDocker Container Logs5 Reasons LaaS Is Essential for Modern Log Management6 Secrets for Successful Log ManagementHow to Live Tail Docker Log
61,Technical Resources
61,Admin Guide Getting Started Release Notes
61,Educational Resources
61,Take a Tour Tips from the Team Use Cases
61,Connect with Us
61,THWACK Community Blog COVID-19 Resource Center
61,PRODUCTS
61,TECHNOLOGY
61,Kubenetes Cluster Logging Kubernetes Logging AWS Log Management GC Log Analyzer AWS Logging Docker Logging Docker Syslog Heroku Logging Logging Tools for Azure App
61,CAPABILITIES
61,Log Viewer Firewall Log Analyzer Centralized Log Management Log Analysis Tail Logging Cron Job Monitoring Log Aggregator Cloud Logging
61,LANGUAGES
61,Linux Logging Software Linux Log Management Ruby Logging Manager Golang Logging HAproxy Log Analyzer MySQL Logging NGINX Logging Javascript Logging Tomcat Logging Apache Log Analyzer
61,FEATURES
61,EVENT VIEWER
61,Live Tail Seek by Time Context Elegant Search APM Integration Skim-ability Saved Searches Log Velocity Analytics
61,APM INTEGRATION
61,Feature 1
61,BUILT FOR COLLABORATION
61,TIPS FROM THE TEAM
61,TIPS FROM THE TEAM
61,Kubernetes Logging Monitoring WordPress Error Logs A Guide to Log Filtering Managing and Analyzing Firewall Logs What Your Router Logs Say About Your Network How to Diagnose App Issues Using Crash Logs Docker Container Logs 5 Reasons LaaS Is Essential for Modern Log Management 6 Secrets for Successful Log Management How to Live Tail Docker Log
61,RESOURCES
61,Technical Resources
61,Admin Guide Getting Started Release Notes
61,Educational Resources
61,Take a Tour Tips from the Team Use Cases
61,Connect with Us
61,THWACK Community Blog COVID-19 Resource Center
61,LOG IN
61,REQUEST DEMO
61,FREE TRIAL
61,Tips from the Team
61,HAProxy Logging- How to Tune Timeouts for Performance
61,START FREE TRIAL
61,Fully Functional for 14 Days
61,Tips from the Team
61,Logging in Docker – How to Get It Right
61,HAProxy Logging- How to Tune Timeouts for Performance
61,Kubernetes Logging: Tips to Avoid Memory and Resource Issues
61,Centralized Logging for .NET 5 Applications
61,Heroku: Most Common Errors Explained
61,Azure Web Apps Logging With Net 5—How-To
61,7 Problems to Look out for When Analyzing Garbage Collection Logs
61,Logging in Java – Best Practices and Tips
61,How to Diagnose App Issues Using Crash Logs
61,Best Practices for Centralized Logging in Microservices Architecture
61,"Last updated: February 5, 2024"
61,"HAProxy (high availability proxy) is a critical part of modern systems infrastructure. It’s ideally the first point of contact for users who access your application. When configured correctly, HAProxy improves your app’s performance significantly. Through load balancing, HAProxy makes sure each service your application depends on is accessible to users, especially under load conditions otherwise impacting application performance."
61,I’m going to take you through the process of tuning timeouts with the intent to boost application performance. You’ll see how robust HAProxy logging can help you with troubleshooting timeout issues and improve the performance of your application. I’ll quickly go through some of the HAProxy timeout configurations to lay a foundation.
61,"Before we dive into the overview, let’s go over a few reasons why we need HAProxy and the logic behind it. This should help us visualize the “how” part later and understand why it’s worth going through the tuning processes."
61,Why You’ll Need HAProxy
61,"The image above shows the basic design of how users access web. This method works fine if the application doesn’t get much traffic. Once the application gains traction and the number of users increases, you can see application performance begin to decline. When numerous users access the application at the same time, requests can back up and even overwhelm the application."
61,A good analogy is a single-lane road going from point A to point B filling up when there are too many cars. Adding HAProxy as a load balancer is like adding lanes to the road. More lanes mean more vehicles can now travel the road.
61,"Additional lanes won’t increase speed, but cars travel down the road without waiting for the car ahead to advance. However, if you add enough vehicles, even these additional lanes will become congested. This is where the concept of timeouts is essential to avoid jams."
61,"Timeouts terminate connections after a client waits for a predetermined amount of time to access the server. This frees up connections, so active users can access the application. It’s as if the stalled cars are taken off the road, so other cars can move freely. Let’s quickly cover the various types of timeouts before we get to the tuning part."
61,The Three Basic HAProxy Timeouts
61,"Just to show you what timeout configurations look like, here’s a sample including the three basic timeouts."
61,##based on Mesosphere Marathon’s servicerouter.py haproxy config
61,global
61,daemon
61,log 127.0.0.1 local0
61,log 127.0.0.1 local1 notice
61,maxconn 4096
61,tune.ssl.default-dh-param 2048
61,defaults
61,log global
61,retries 3
61,maxconn 2000
61,timeout connect 5s
61,timeout client 50s
61,timeout server 50s
61,Source: GitHub
61,1. Timeout Client
61,"The <timeout client> setting defines the maximum time a client can be inactive when connected to the server. A common value for this timeout is five minutes. You can go with shorter timeouts, even as little as thirty seconds, if you’re attempting to maximize security or the total number of active connections. As the name suggests, this timeout is handled on the client side."
61,2. Timeout Connect
61,"The <timeout connect> works like a grace period. It sets the maximum time the client has to connect to a server. The time it takes a client to connect to a server varies widely, depending on network complexity. The more complex a topology, the longer it can take the client to connect."
61,"The timeout connect allows the client to try to connect again if the initial attempt fails. In addition to the connection time, you’ll need to set the numbers of retries. The default is three, but you can adjust it to fit your environment."
61,3. Timeout Server
61,"When a client sends a request to the server, it expects a response. If the server doesn’t respond in the configured time duration, a <timeout server> is invoked. This is akin to the <timeout client>, only in reverse. You can see in the list of HTTP responses, if a <timeout serve> is invoked, you’ll get a 504 Gateway Timeout response from HAProxy."
61,HAProxy Timeout Tuning for Good Performance
61,"Just by configuring these three timeout values in your haproxy.cfg file, you can achieve a basic level of performance. If you want to take it up a notch, you can set other timeout settings to enhance performance. While the values you set will vary depending on your traffic load and environment, I’ve listed the most common configurations below. To use these, append the following to your configuration file."
61,timeout http-request 10s
61,timeout http-keep-alive 2s
61,timeout queue 5s
61,timeout tunnel 2m
61,timeout client-fin 1s
61,timeout server-fin 1s
61,Timeout HTTP-Request
61,"The <timeout http-request> variable limits the total time each request can persist. Aside from optimizing request performance, it can also defend against denial of service (DDoS) attacks by limiting the time a single request can last. Usually, ten seconds is a good limit."
61,"Some php.ini files may also have this setting, but since the proxy server is the first point of contact with an application, php.ini settings are overridden. This is true unless the application server level (php.ini) setting is shorter than the proxy determined variable."
61,Timeout HTTP-Keep-Alive
61,"As the name suggests, this is a timeout designed to keep a single connection between the client and the server “alive” for a desired amount of time. While the connection is alive, all data packets can pass without needing to ask for a connection again. This essentially makes getting responses from the server to the client faster."
61,"Keep in mind the <timeout http-request> regulates how long a single request can persist, so these two settings work hand in hand. If the <timeout http-keep-alive> isn’t set or has a value less than the <timeout http-request>, the latter determines the connection status."
61,Timeout Queue
61,"The <timeout queue> limits the number of concurrent connections, which can also impact performance. Setting the queue timeout shortens wait times by limiting connections and allowing clients to try connecting again if the queue is full. This is similar to <timeout connect>, except <timeout queue> limits the number of connections."
61,"If you don’t set the <timeout queue>, HAProxy will default to the <timeout connect> settings to manage the queue."
61,Timeout Tunnel
61,"The <timeout tunnel> variable only applies when you’re working with WebSockets. Essentially, it’s <timeout keep-alive> on steroids, often with durations exceeding minutes. It may seem counterproductive and a potential security risk to keep a connection open for that long. However, when used with other timeout configurations, it’s possible to maintain a safe yet high-performing connection."
61,"For instance, the <timeout http-request> variable would prevent an attack even if the <timeout tunnel> is set at several minutes. Remember, though, that the virtual tunnel you create by implementing this timeout requires you to terminate it at some point using the <timeout client-fin> parameter."
61,Timeout Client-Fin
61,"Say a connection drops in the middle of a client request; if you look at the HAProxy logs, you’re likely to see the lost connection is a result of client-side network issues. To handle these types of situations, HAProxy creates a list of dropped client-side connections. The <timeout client-fin> limits the amount of time a client request will be maintained on this list."
61,"This parameter starts ticking when a connection joins the list. Without it, the server will maintain a “maybe they’ll return” sort of connection while others are denied service. To optimize performance, the time values set for this timeout are usually short."
61,Timeout Server-Fin
61,"Much like the <timeout client-fin> concept, abrupt disconnections can also occur on the server side of the application. An optimal setup would include redundant servers for load-balancing. When a server has too many requests, redundancy will allow you to reroute overflow requests to less busy servers and speed up response times."
61,The <timeout server-fin> limits the time the client waits for a server response before an alternate server is queried.
61,HAProxy Logging: How to Determine Perfect Timeouts
61,"By now, you probably can see how environment variables and traffic can help you determine the time allocations for the timeouts above. Finding the perfect balance between peak performance and optimal security is a matter of trial and error. Keeping a close eye on HAProxy logs can help you see the interactions between these different configurations and find the “sweet spot” for your application and environment. You can use HAProxy logs to understand:"
61,"Timestamped metrics about traffic (timing data, connections counters, traffic size)"
61,"Detailed event logs of HAProxy operations (content switching, filtering, persistence)"
61,"Request and responses (headers, status codes, payloads)"
61,Session terminations and tracking where failures are occurring
61,"Knowing when a timeout event occurs and monitoring the events preceding it are the first steps for successfully troubleshooting and tuning timeout settings. If you’re looking for an easy cloud-based log management tool, check out SolarWinds® Papertrail™. Built by engineers for engineers, Papertrail can automatically parse HAProxy logs and help you quickly troubleshoot timeout issues."
61,"It offers a simple search syntax allowing you to search all your logs from a central interface, see events in context, and pinpoint issues. The live tail feature is particularly helpful for real-time troubleshooting. If you want to start HAProxy logging with better results, sign up for a trial or request a demo."
61,"This post was written by Taurai Mutimutema. Taurai is a systems analyst with a knack for writing, which was probably sparked by the need to document technical processes during code and implementation sessions. He enjoys learning new technology and talks about tech even more than he writes."
61,"Aggregate, organize, and manage your logs"
61,"Collect real-time log data from your applications, servers, cloud services, and more"
61,"Search log messages to analyze and troubleshoot incidents, identify trends, and set alerts"
61,"Create comprehensive per-user access control policies, automated backups, and archives of up to a year of historical data"
61,Start Free Trial Fully Functional for 30 Days
61,Let's talk it over
61,"Contact our team, anytime."
61,Toll Free: +1 (855) 679-0752
61,Phone: +1 (512) 498-6011
61,papertrailapp@solarwinds.com
61,Datasheet
61,Help
61,Contact
61,@papertrailapp
61,Legal Documents
61,California Privacy Rights
61,Software Services Agreement
61,Privacy Notice
61,GDPR Resource Center
61,SolarWinds Subscription Center
61,COVID-19 Resource Center
61,"© 2024 SolarWinds Worldwide, LLC. All rights reserved."
61,We’re Geekbuilt ™.
61,"Developed by network and systems engineers who know what it takes to manage today’s dynamic IT environments,"
61,SolarWinds has a deep connection to the IT community.
61,"The result? IT management products that are effective, accessible, and easy to use."
61,COMPANY
61,CARRER CENTER
61,EMAIL PREFNCE CENTER
61,SOLUTION FINDER
61,FOR GOVERNMENT
61,FOR PARTNERS
61,FOR TEST
61,Another Link
61,Privacy Notice
61,Terms of use
61,Security Policy
61,Another Link
61,"© 2021 SolarWinds Worldwide, LLC. All rights reserved."
62,"First steps with pgBackRest, a backup solution for PostgreSQL - Vettabase"
62,Email
62,sales@vettabase.com
62,Schedule Meeting
62,Calendly Booking
62,Phone
62,+44 203 962 5762
62,Home
62,About
62,Careers
62,Resources
62,Database Services
62,Database Automation
62,Automation for MariaDB
62,Automation for MySQL
62,Automation for PostgreSQL
62,Automation for Cassandra
62,Database Training
62,MariaDB Training
62,MySQL Training
62,Database Upgrade
62,Upgrade for MariaDB
62,Upgrade for MySQL
62,Upgrade for PostgreSQL
62,Upgrade for Cassandra
62,Database Health Check
62,MariaDB Health Check
62,MySQL Health Check
62,PostgreSQL Health Check
62,Cassandra Health Check
62,Monthly DBA Time
62,Monthly DBA Time for MariaDB
62,Monthly DBA Time for MySQL
62,Monthly DBA Time for PostgreSQL
62,Monthly DBA Time for Cassandra
62,Services for MindsDB Users
62,ColumnStore Services
62,Blog
62,Recent Posts
62,MariaDB
62,MySQL
62,Cassandra
62,PostgreSQL
62,Testimonials
62,Vettabase Featured Customers
62,Vettabase Partners
62,Case Study : Treedom
62,Contact
62,Free Consultation
62,Home
62,About
62,Careers
62,Resources
62,Database Services
62,Database Automation
62,Database Health Check
62,Database Training
62,Database Upgrade
62,Monthly DBA Time
62,Blog
62,Vettabase Testimonials
62,Contact
62,Free Consultation
62,"First steps with pgBackRest, a backup solution for PostgreSQL by Pramod Gupta | Aug 21, 2023 | PostgreSQL"
62,"pgBackRest is an open source backup tool that creates physical backups with some improvements compared to the classic pg_basebackup tool. pg_basebackup is included in the PostgreSQL binaries, and it offers a great set of features for hot binary backups, remote backups, and standby building, etc."
62,"pgBackRest is a solution that addresses the shortcomings of pg_basebackup. pgBackRest implements all backup features internally using a custom protocol for communicating with remote systems. Powerful features of pgBackRest include parallel backup and restore, local or remote operation, full, incremental, and differential backup types, backup rotation, archive expiration, backup integrity, page checksums, backup resume, streaming compression and checksums, delta restore, and much more.pgBackRest doesn’t rely on traditional backup tools like tar or rsync. This is a custom solution, and the protocol used by pgBackRest is a perfect fit for PostgreSQL-specific backup challenges. It allows for more flexibility and limits the types of connections that are required to perform a backup, which increases security. pgBackRest is a simple, but feature-rich, reliable backup and restore system that can seamlessly scale up to the largest databases and workloads."
62,Important features of pgBackRest
62,"Full, incremental, and differential backups: pgBackRest supports full, incremental, and differential backups. It uses a differential backup model, which allows for efficient backup storage while maintaining the ability to restore to any point in time (PITR)."
62,"Parallel backup and restore. pgBackRest can perform backups and restores in parallel, taking advantage of multiple CPU cores and significantly reducing the time required for these operations."
62,"Point-in-time recovery (PITR). pgBackRest enables Point-in-Time Recovery, allowing you to restore a PostgreSQL database to a specific point in time, rather than just the time of the last backup."
62,"Compression, encryption and integrity checks. It provides options for compressing backup data on the fly to reduce storage requirements and supports encryption of backup files for enhanced security. pgBackRest also performs integrity checks on backup files, ensuring that they are consistent and can be trusted for recovery."
62,"Backup rotation, retention and throttling. The tool includes built-in retention policies to manage backup rotation automatically. You can control how many backups are kept and when they are removed. pgBackRest also provides the ability to throttle the backup and restore operations, preventing them from overloading the system and affecting production performance."
62,"Delta restore and parallel backup verification. Thanks to the delta restore feature, pgBackRest can quickly apply incremental changes to an existing database to reduce the time required for restoration. The tool also supports parallel verification of backups, ensuring that the backups are valid and reliable for recovery."
62,How to use pgBackRest
62,Installation
62,"A prerequisite for installation is the PGDG repository. If we don’t have it already installed, we should be doing that from yum.postgresql.org."
62,sudo yum install pgbackrest
62,"pgBackRest is developed in Perl, so when you install pgBackRest, all dependent Perl libraries will also get installed if they are not already present."
62,Configuring backups
62,"The first step is to create a stanza definition in /etc/pgbackrest.conf. Here is a simple example. We can see that all the pg options are specified with a 1, which serves as the index of the configuration. These indexes are intended for configuring multiple PostgreSQL hosts. For example, a single master is configured with the pg1-path, pg1-host, and similar options. If a standby is configured, then index the pg- options as pg2-host, pg2-path, etc."
62,sudo bash -c 'cat << EOF  > /etc/pgbackrest.conf
62,[global]
62,repo1-path=/var/lib/pgbackrest
62,repo1-retention-full=2
62,[pg0app]
62,pg1-path=/var/lib/pgsql/11/data
62,pg1-port=5432
62,EOF'
62,"The next step is the creation of a stanza. This needs to be done on the server where the repository is located. It is highly recommended to use the same non-root user under which PostgreSQL processes are configured and running. As mentioned before, a stanza holds the backup configuration related to one PostgreSQL instance. In this step the actual internal structure and the definition of stanza will be created out of the stanza definition."
62,pgbackrest stanza-create --stanza=pg0app --log-level-console=info
62,"Do the necessary parameter changes in PostgreSQL. At a minimal level, WAL archiving should be enabled, and archive_command should be using pgbackrest as shown below."
62,ALTER SYSTEM SET wal_level = 'replica';
62,ALTER SYSTEM SET archive_mode = 'on';
62,ALTER SYSTEM SET archive_command = 'pgbackrest --stanza=pg0app archive-push %p';
62,ALTER SYSTEM SET max_wal_senders = '10';
62,ALTER SYSTEM SET hot_standby = 'on';
62,The above-mentioned parameter changes require the restart of the PostgreSQL instance:
62,sudo systemctl restart postgresql-11
62,Now we can check whether we have configured everything correctly using the following command:
62,pgbackrest check --stanza=pg0app --log-level-console=info
62,Running backups
62,Database backup using pgbackrest can be performed using the following command:
62,pgbackrest backup --stanza=pg0app --log-level-console=info
62,Conclusion
62,"In this blog post, we have covered basic understanding and configuration details of the pgBackRest backup tool. In our future articles, we will tackle its advanced features. This backup tool has a number of them, so we expect the post to be helpful for PostgreSQL users. If you are looking for expert PostgreSQL professional services, please contact the Vettabase team."
62,Reference
62,You can find the pgBackRest documentation here.
62,"All content in this blog is distributed under the CreativeCommons Attribution-ShareAlike 4.0 International license. You can use it for your needs and even modify it, but please refer to Vettabase and the author of the original post. Read more about the terms and conditions: https://creativecommons.org/licenses/by-sa/4.0/"
62,About Pramod Gupta
62,"Pramod Gupta is a database professional, who has been working with SQL and NoSQL databases since year 2011. Before joining Vettabase, he served in a number of companies Like Datavail, Lazada Group, Alibaba Group, Paytm, Byju's and Ola as a DBA, Lead DBA, Principle DBA, Database Architect and Database Manager. Pramod has vast experience with MySQL/MariaDB/Percona Server, MongoDB, PostgreSQL, Cassandra, ScyllaDB, Couchbase and Aerospike databases. His professional interests are database Performance tuning, HA, scalabilty and DB architecture design and planning."
62,Recent Posts
62,Hints to optimise queries with a LIKE comparison
62,"Sep 15, 2022In SQL, using the LIKE operator is a powerful way to find strings that match a certain pattern. It's suitable for most use cases, thanks to its two jolly characters: _ means any one character. % means any sequence of zero or more characters. However, many queries out..."
62,AlloyDB versus PostgreSQL: a performance review
62,"Jul 19, 2022What is faster: PostgreSQL or AlloyDB? Some benchmarks and a performance analysis."
62,Tuning PostgreSQL Auto-vacuum
62,"Apr 28, 2022Why autovacuum is important in PostgreSQL and how to configure it properly."
62,Services
62,Database Automation
62,Database Training
62,Database Health Check
62,Monthly DBA Time
62,Database Upgrade
62,← Previous Post
62,Next Post →
62,"backup , PostgreSQL"
62,0 Comments
62,Submit a Comment Cancel replyYour email address will not be published. Required fields are marked *Comment * Name *
62,Email *
62,Website
62,"Save my name, email, and website in this browser for the next time I comment."
62,Submit Comment
62,Subscribe Now
62,"Read our expert blog with articles on MariaDB, MySQL, PostgreSQL, Cassandra and related tools & technologies."
62,Full Name *Email Address *Choose your databaseAll Database PostsMariaDB PostsMySQL PostsPostgreSQL PostsCassandra PostsPolicy consent *I agree with Vettabase Privacy Policy.Marketing consent *I agree to receive marketing communications and discounts info from Vettabase.Sign Up
62,Email
62,sales@vettabase.com
62,Schedule Meeting
62,Calendly Booking
62,Phone
62,+44 203 962 5762
62,"Vettabase specialises in expert consulting and automation for your database infrastructure. We offer professional services and training for MySQL, MariaDB, PostgreSQL and Cassandra. Our specialties include a wide range of services from performance optimisation to complex database upgrades and migrations."
62,Vettabase LtdRegistered in England and WalesCompany number: 12769372
62,Quick Links
62,Home
62,About
62,Careers
62,Database Automation
62,Database Training
62,Database Health Check
62,Monthly DBA Time
62,Database Upgrade
62,Vettabase Testimonials
62,Blog
62,Contact
62,Recent Posts
62,"Coming up next, a ColumnStore webinar"
62,"Mar 20, 2024The 27th is fast approaching but you can still signup to our live webinar where we will be exploring the benefits of time series data using MariaDB ColumnStore. We will be jumping into a live demo with some example data and queries simulating high volume time series..."
62,The benefits of MariaDB ColumnStore
62,"Feb 22, 2024Last week Richard announced our projects on MariaDB ColumnStore. Take a look at his great post, if you didn't already. Since then, I've got some questions from customers, colleagues and friends: why did you guys decide to robustly invest into ColumnStore, and offer..."
62,"Let’s go, MariaDB ColumnStore at Vettabase!"
62,"Feb 14, 2024I have been an avid user of Columnar based storage engines for as long as I have been a DBA. The advantage is instantly obvious once you and your teams start to write or convert queries which previously took tens of minutes, are now returning in fractions of a second...."
62,Policies & Licenses
62,Consultancy Policy
62,Privacy Policy
62,Creative Commons Attribution License (for Vettabase blog only)
62,Follow Us on Social Media
62,FollowFollowFollowFollow
62,"© 2020-2024, Vettabase Ltd., all rights reserved"
62,Website maintained by Butteredhost.com
63,An Introduction to Hibernate 6
63,An Introduction to Hibernate 6
63,version 6.3.2.Final
63,Table of Contents
63,Preface
63,1. Introduction
63,1.1. Hibernate and JPA
63,1.2. Writing Java code with Hibernate
63,"1.3. Hello, Hibernate"
63,"1.4. Hello, JPA"
63,1.5. Organizing persistence logic
63,1.6. Testing persistence logic
63,1.7. Architecture and the persistence layer
63,1.8. Overview
63,2. Configuration and bootstrap
63,2.1. Including Hibernate in your project build
63,2.2. Optional dependencies
63,2.3. Configuration using JPA XML
63,2.4. Configuration using Hibernate API
63,2.5. Configuration using Hibernate properties file
63,2.6. Basic configuration settings
63,2.7. Automatic schema export
63,2.8. Logging the generated SQL
63,2.9. Minimizing repetitive mapping information
63,2.10. Nationalized character data in SQL Server
63,3. Entities
63,3.1. Entity classes
63,3.2. Access types
63,3.3. Entity class inheritance
63,3.4. Identifier attributes
63,3.5. Generated identifiers
63,3.6. Natural keys as identifiers
63,3.7. Composite identifiers
63,3.8. Version attributes
63,3.9. Natural id attributes
63,3.10. Basic attributes
63,3.11. Enumerated types
63,3.12. Converters
63,3.13. Compositional basic types
63,3.14. Embeddable objects
63,3.15. Associations
63,3.16. Many-to-one
63,3.17. One-to-one (first way)
63,3.18. One-to-one (second way)
63,3.19. Many-to-many
63,3.20. Collections of basic values and embeddable objects
63,3.21. Collections mapped to SQL arrays
63,3.22. Collections mapped to a separate table
63,3.23. Summary of annotations
63,3.24. equals() and hashCode()
63,4. Object/relational mapping
63,4.1. Mapping entity inheritance hierarchies
63,4.2. Mapping to tables
63,4.3. Mapping entities to tables
63,4.4. Mapping associations to tables
63,4.5. Mapping to columns
63,4.6. Mapping basic attributes to columns
63,4.7. Mapping associations to foreign key columns
63,4.8. Mapping primary key joins between tables
63,4.9. Column lengths and adaptive column types
63,4.10. LOBs
63,4.11. Mapping embeddable types to UDTs or to JSON
63,4.12. Summary of SQL column type mappings
63,4.13. Mapping to formulas
63,4.14. Derived Identity
63,4.15. Adding constraints
63,5. Interacting with the database
63,5.1. Persistence Contexts
63,5.2. Creating a session
63,5.3. Managing transactions
63,5.4. Operations on the persistence context
63,5.5. Cascading persistence operations
63,5.6. Proxies and lazy fetching
63,5.7. Entity graphs and eager fetching
63,5.8. Flushing the session
63,5.9. Queries
63,5.10. HQL queries
63,5.11. Criteria queries
63,5.12. A more comfortable way to write criteria queries
63,5.13. Native SQL queries
63,"5.14. Limits, pagination, and ordering"
63,5.15. Representing projection lists
63,5.16. Named queries
63,5.17. Controlling lookup by id
63,5.18. Interacting directly with JDBC
63,5.19. What to do when things go wrong
63,6. Compile-time tooling
63,6.1. Named queries and the Metamodel Generator
63,6.2. Generated query methods
63,6.3. Generating query methods as instance methods
63,6.4. Generated finder methods
63,6.5. Paging and ordering
63,6.6. Query and finder method return types
63,6.7. An alternative approach
63,7. Tuning and performance
63,7.1. Tuning the connection pool
63,7.2. Enabling statement batching
63,7.3. Association fetching
63,7.4. Batch fetching and subselect fetching
63,7.5. Join fetching
63,7.6. The second-level cache
63,7.7. Specifying which data is cached
63,7.8. Caching by natural id
63,7.9. Caching and association fetching
63,7.10. Configuring the second-level cache provider
63,7.11. Caching query result sets
63,7.12. Second-level cache management
63,7.13. Session cache management
63,7.14. Stateless sessions
63,7.15. Optimistic and pessimistic locking
63,7.16. Collecting statistics
63,7.17. Tracking down slow queries
63,7.18. Adding indexes
63,7.19. Dealing with denormalized data
63,7.20. Reactive programming with Hibernate
63,8. Advanced Topics
63,8.1. Filters
63,8.2. Multi-tenancy
63,8.3. Using custom-written SQL
63,8.4. Handling database-generated columns
63,8.5. User-defined generators
63,8.6. Naming strategies
63,8.7. Spatial datatypes
63,8.8. Ordered and sorted collections and map keys
63,8.9. Any mappings
63,8.10. Selective column lists in inserts and updates
63,8.11. Using the bytecode enhancer
63,8.12. Named fetch profiles
63,9. Credits
63,Preface
63,Hibernate 6 is a major redesign of the world’s most popular and feature-rich ORM solution.
63,"The redesign has touched almost every subsystem of Hibernate, including the APIs, mapping annotations, and the query language."
63,"This new Hibernate is more powerful, more robust, and more typesafe."
63,"With so many improvements, it’s very difficult to summarize the significance of this work."
63,But the following general themes stand out.
63,Hibernate 6:
63,"finally takes advantage of the advances in relational databases over the past decade, updating the query language to support a raft of new constructs in modern dialects of SQL,"
63,"exhibits much more consistent behavior across different databases, greatly improving portability, and generates much higher-quality DDL from dialect-independent code,"
63,"improves error reporting by more scrupulous validation of queries before access to the database,"
63,"improves the type-safety of O/R mapping annotations, clarifies the separation of API, SPI, and internal implementation, and fixes some long-standing architectural flaws,"
63,"removes or deprecates legacy APIs, laying the foundation for future evolution, and"
63,"makes far better use of Javadoc, putting much more information at the fingertips of developers."
63,"Hibernate 6 and Hibernate Reactive are now core components of Quarkus 3, the most exciting new environment for cloud-native development in Java, and Hibernate remains the persistence solution of choice for almost every major Java framework or server."
63,"Unfortunately, the changes in Hibernate 6 have obsoleted much of the information about Hibernate that’s available in books, in blog posts, and on stackoverflow."
63,"This guide is an up-to-date, high-level discussion of the current feature set and recommended usage."
63,It does not attempt to cover every feature and should be used in conjunction with other documentation:
63,"Hibernate’s extensive Javadoc,"
63,"the Guide to Hibernate Query Language, and"
63,the Hibernate User Guide.
63,The Hibernate User Guide includes detailed discussions of most aspects of Hibernate.
63,"But with so much information to cover, readability is difficult to achieve, and so it’s most useful as a reference."
63,"Where necessary, we’ll provide links to relevant sections of the User Guide."
63,1. Introduction
63,Hibernate is usually described as a library that makes it easy to map Java classes to relational database tables.
63,But this formulation does no justice to the central role played by the relational data itself.
63,So a better description might be:
63,"Hibernate makes relational data visible to a program written in Java, in a natural and typesafe form,"
63,"making it easy to write complex queries and work with their results,"
63,"letting the program easily synchronize changes made in memory with the database, respecting the ACID properties of transactions, and"
63,allowing performance optimizations to be made after the basic persistence logic has already been written.
63,"Here the relational data is the focus, along with the importance of typesafety."
63,"The goal of object/relational mapping (ORM) is to eliminate fragile and untypesafe code, and make large programs easier to maintain in the long run."
63,"ORM takes the pain out of persistence by relieving the developer of the need to hand-write tedious, repetitive, and fragile code for flattening graphs of objects to database tables and rebuilding graphs of objects from flat SQL query result sets."
63,"Even better, ORM makes it much easier to tune performance later, after the basic persistence logic has already been written."
63,"A perennial question is: should I use ORM, or plain SQL?"
63,The answer is usually: use both.
63,JPA and Hibernate were designed to work in conjunction with handwritten SQL.
63,"You see, most programs with nontrivial data access logic will benefit from the use of ORM at least somewhere."
63,"But if Hibernate is making things more difficult, for some particularly tricky piece of data access logic, the only sensible thing to do is to use something better suited to the problem!"
63,Just because you’re using Hibernate for persistence doesn’t mean you have to use it for everything.
63,"Developers often ask about the relationship between Hibernate and JPA, so let’s take a short detour into some history."
63,1.1. Hibernate and JPA
63,"Hibernate was the inspiration behind the Java (now Jakarta) Persistence API, or JPA, and includes a complete implementation of the latest revision of this specification."
63,The early history of Hibernate and JPA
63,"The Hibernate project began in 2001, when Gavin King’s frustration with Entity Beans in EJB 2 boiled over."
63,"It quickly overtook other open source and commercial contenders to become the most popular persistence solution for Java, and the book Hibernate in Action, written with Christian Bauer, was an influential bestseller."
63,"In 2004, Gavin and Christian joined a tiny startup called JBoss, and other early Hibernate contributors soon followed: Max Rydahl Andersen, Emmanuel Bernard, Steve Ebersole, and Sanne Grinovero."
63,"Soon after, Gavin joined the EJB 3 expert group and convinced the group to deprecate Entity Beans in favor of a brand-new persistence API modelled after Hibernate."
63,"Later, members of the TopLink team got involved, and the Java Persistence API evolved as a collaboration between—primarily—Sun, JBoss, Oracle, and Sybase, under the leadership of Linda Demichiel."
63,"Over the intervening two decades, many talented people have contributed to the development of Hibernate."
63,"We’re all especially grateful to Steve, who has led the project for many years, since Gavin stepped back to focus in other work."
63,We can think of the API of Hibernate in terms of three basic elements:
63,"an implementation of the JPA-defined APIs, most importantly, of the interfaces EntityManagerFactory and EntityManager, and of the JPA-defined O/R mapping annotations,"
63,"a native API exposing the full set of available functionality, centered around the interfaces SessionFactory, which extends EntityManagerFactory, and Session, which extends EntityManager, and"
63,"a set of mapping annotations which augment the O/R mapping annotations defined by JPA, and which may be used with the JPA-defined interfaces, or with the native API."
63,"Hibernate also offers a range of SPIs for frameworks and libraries which extend or integrate with Hibernate, but we’re not interested in any of that stuff here."
63,"As an application developer, you must decide whether to:"
63,"write your program in terms of Session and SessionFactory, or"
63,"maximize portability to other implementations of JPA by, wherever reasonable, writing code in terms of"
63,"EntityManager and EntityManagerFactory, falling back to the native APIs only where necessary."
63,"Whichever path you take, you will use the JPA-defined mapping annotations most of the time, and the Hibernate-defined annotations for more advanced mapping problems."
63,"You might wonder if it’s possible to develop an application using only JPA-defined APIs, and, indeed, that’s possible in principle."
63,JPA is a great baseline that really nails the basics of the object/relational mapping problem.
63,"But without the native APIs, and extended mapping annotations, you miss out on much of the power of Hibernate."
63,"Since Hibernate existed before JPA, and since JPA was modelled on Hibernate, we unfortunately have some competition and duplication in naming between the standard and native APIs."
63,For example:
63,Table 1. Examples of competing APIs with similar naming
63,Hibernate
63,JPA
63,org.hibernate.annotations.CascadeType
63,javax.persistence.CascadeType
63,org.hibernate.FlushMode
63,javax.persistence.FlushModeType
63,org.hibernate.annotations.FetchMode
63,javax.persistence.FetchType
63,org.hibernate.query.Query
63,javax.persistence.Query
63,org.hibernate.Cache
63,javax.persistence.Cache
63,@org.hibernate.annotations.NamedQuery
63,@javax.persistence.NamedQuery
63,@org.hibernate.annotations.Cache
63,@javax.persistence.Cacheable
63,"Typically, the Hibernate-native APIs offer something a little extra that’s missing in JPA, so this isn’t exactly a flaw."
63,But it’s something to watch out for.
63,1.2. Writing Java code with Hibernate
63,"If you’re completely new to Hibernate and JPA, you might already be wondering how the persistence-related code is structured."
63,"Well, typically, our persistence-related code comes in two layers:"
63,"a representation of our data model in Java, which takes the form of a set of annotated entity classes, and"
63,a larger number of functions which interact with Hibernate’s APIs to perform the persistence operations associated with your various transactions.
63,"The first part, the data or ""domain"" model, is usually easier to write, but doing a great and very clean job of it will strongly affect your success in the second part."
63,"Most people implement the domain model as a set of what we used to call ""Plain Old Java Objects"", that is, as simple Java classes with no direct dependencies on technical infrastructure, nor on application logic which deals with request processing, transaction management, communications, or interaction with the database."
63,"Take your time with this code, and try to produce a Java model that’s as close as reasonable to the relational data model. Avoid using exotic or advanced mapping features when they’re not really needed."
63,"When in the slightest doubt, map a foreign key relationship using @ManyToOne with @OneToMany(mappedBy=…​) in preference to more complicated association mappings."
63,The second part of the code is much trickier to get right. This code must:
63,"manage transactions and sessions,"
63,"interact with the database via the Hibernate session,"
63,"fetch and prepare data needed by the UI, and"
63,handle failures.
63,"Responsibility for transaction and session management, and for recovery from certain kinds of failure, is best handled in some sort of framework code."
63,"We’re going to come back soon to the thorny question of how this persistence logic should be organized, and how it should fit into the rest of the system."
63,"1.3. Hello, Hibernate"
63,"Before we get deeper into the weeds, we’ll quickly present a basic example program that will help you get started if you don’t already have Hibernate integrated into your project."
63,We begin with a simple gradle build file:
63,build.gradle
63,plugins {
63,id 'java'
63,group = 'org.example'
63,version = '1.0-SNAPSHOT'
63,repositories {
63,mavenCentral()
63,dependencies {
63,// the GOAT ORM
63,implementation 'org.hibernate.orm:hibernate-core:6.3.0.Final'
63,// Hibernate Validator
63,implementation 'org.hibernate.validator:hibernate-validator:8.0.0.Final'
63,implementation 'org.glassfish:jakarta.el:4.0.2'
63,// Agroal connection pool
63,implementation 'org.hibernate.orm:hibernate-agroal:6.3.0.Final'
63,implementation 'io.agroal:agroal-pool:2.1'
63,// logging via Log4j
63,implementation 'org.apache.logging.log4j:log4j-core:2.20.0'
63,// JPA Metamodel Generator
63,annotationProcessor 'org.hibernate.orm:hibernate-jpamodelgen:6.3.0.Final'
63,// Compile-time checking for HQL
63,//implementation 'org.hibernate:query-validator:2.0-SNAPSHOT'
63,//annotationProcessor 'org.hibernate:query-validator:2.0-SNAPSHOT'
63,// H2 database
63,runtimeOnly 'com.h2database:h2:2.1.214'
63,Only the first of these dependencies is absolutely required to run Hibernate.
63,"Next, we’ll add a logging configuration file for log4j:"
63,log4j2.properties
63,rootLogger.level = info
63,rootLogger.appenderRefs = console
63,rootLogger.appenderRef.console.ref = console
63,logger.hibernate.name = org.hibernate.SQL
63,logger.hibernate.level = info
63,appender.console.name = console
63,appender.console.type = Console
63,appender.console.layout.type = PatternLayout
63,appender.console.layout.pattern = %highlight{[%p]} %m%n
63,Now we need some Java code.
63,We begin with our entity class:
63,Book.java
63,package org.hibernate.example;
63,import jakarta.persistence.Entity;
63,import jakarta.persistence.Id;
63,import jakarta.validation.constraints.NotNull;
63,@Entity
63,class Book {
63,@Id
63,String isbn;
63,@NotNull
63,String title;
63,Book() {}
63,"Book(String isbn, String title) {"
63,this.isbn = isbn;
63,this.title = title;
63,"Finally, let’s see code which configures and instantiates Hibernate and asks it to persist and query the entity."
63,Don’t worry if this makes no sense at all right now.
63,It’s the job of this Introduction to make all this crystal clear.
63,Main.java
63,package org.hibernate.example;
63,import org.hibernate.cfg.Configuration;
63,import static java.lang.Boolean.TRUE;
63,import static java.lang.System.out;
63,import static org.hibernate.cfg.AvailableSettings.*;
63,public class Main {
63,public static void main(String[] args) {
63,var sessionFactory = new Configuration()
63,.addAnnotatedClass(Book.class)
63,// use H2 in-memory database
63,".setProperty(URL, ""jdbc:h2:mem:db1"")"
63,".setProperty(USER, ""sa"")"
63,".setProperty(PASS, """")"
63,// use Agroal connection pool
63,".setProperty(""hibernate.agroal.maxSize"", ""20"")"
63,// display SQL in console
63,".setProperty(SHOW_SQL, TRUE.toString())"
63,".setProperty(FORMAT_SQL, TRUE.toString())"
63,".setProperty(HIGHLIGHT_SQL, TRUE.toString())"
63,.buildSessionFactory();
63,// export the inferred database schema
63,sessionFactory.getSchemaManager().exportMappedObjects(true);
63,// persist an entity
63,sessionFactory.inTransaction(session -> {
63,"session.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
63,});
63,// query data using HQL
63,sessionFactory.inSession(session -> {
63,"out.println(session.createSelectionQuery(""select isbn||': '||title from Book"").getSingleResult());"
63,});
63,// query data using criteria API
63,sessionFactory.inSession(session -> {
63,var builder = sessionFactory.getCriteriaBuilder();
63,var query = builder.createQuery(String.class);
63,var book = query.from(Book.class);
63,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
63,book.get(Book_.title)));
63,out.println(session.createSelectionQuery(query).getSingleResult());
63,});
63,Here we’ve used Hibernate’s native APIs.
63,We could have used JPA-standard APIs to achieve the same thing.
63,"1.4. Hello, JPA"
63,"If we limit ourselves to the use of JPA-standard APIs, we need to use XML to configure Hibernate."
63,META-INF/persistence.xml
63,"<persistence xmlns=""https://jakarta.ee/xml/ns/persistence"""
63,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
63,"xsi:schemaLocation=""https://jakarta.ee/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
63,"version=""3.0"">"
63,"<persistence-unit name=""example"">"
63,<class>org.hibernate.example.Book</class>
63,<properties>
63,<!-- H2 in-memory database -->
63,"<property name=""jakarta.persistence.jdbc.url"""
63,"value=""jdbc:h2:mem:db1""/>"
63,<!-- Credentials -->
63,"<property name=""jakarta.persistence.jdbc.user"""
63,"value=""sa""/>"
63,"<property name=""jakarta.persistence.jdbc.password"""
63,"value=""""/>"
63,<!-- Agroal connection pool -->
63,"<property name=""hibernate.agroal.maxSize"""
63,"value=""20""/>"
63,<!-- display SQL in console -->
63,"<property name=""hibernate.show_sql"" value=""true""/>"
63,"<property name=""hibernate.format_sql"" value=""true""/>"
63,"<property name=""hibernate.highlight_sql"" value=""true""/>"
63,</properties>
63,</persistence-unit>
63,</persistence>
63,Note that our build.gradle and log4j2.properties files are unchanged.
63,Our entity class is also unchanged from what we had before.
63,"Unfortunately, JPA doesn’t offer an inSession() method, so we’ll have to implement session and transaction management ourselves."
63,"We can put that logic in our own inSession() function, so that we don’t have to repeat it for every transaction."
63,"Again, you don’t need to understand any of this code right now."
63,Main.java (JPA version)
63,package org.hibernate.example;
63,import jakarta.persistence.EntityManager;
63,import jakarta.persistence.EntityManagerFactory;
63,import java.util.Map;
63,import java.util.function.Consumer;
63,import static jakarta.persistence.Persistence.createEntityManagerFactory;
63,import static java.lang.System.out;
63,import static org.hibernate.cfg.AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION;
63,import static org.hibernate.tool.schema.Action.CREATE;
63,public class Main {
63,public static void main(String[] args) {
63,"var factory = createEntityManagerFactory(""example"","
63,// export the inferred database schema
63,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE));"
63,// persist an entity
63,"inSession(factory, entityManager -> {"
63,"entityManager.persist(new Book(""9781932394153"", ""Hibernate in Action""));"
63,});
63,// query data using HQL
63,"inSession(factory, entityManager -> {"
63,"out.println(entityManager.createQuery(""select isbn||': '||title from Book"").getSingleResult());"
63,});
63,// query data using criteria API
63,"inSession(factory, entityManager -> {"
63,var builder = factory.getCriteriaBuilder();
63,var query = builder.createQuery(String.class);
63,var book = query.from(Book.class);
63,"query.select(builder.concat(builder.concat(book.get(Book_.isbn), builder.literal("": "")),"
63,book.get(Book_.title)));
63,out.println(entityManager.createQuery(query).getSingleResult());
63,});
63,"// do some work in a session, performing correct transaction management"
63,"static void inSession(EntityManagerFactory factory, Consumer<EntityManager> work) {"
63,var entityManager = factory.createEntityManager();
63,var transaction = entityManager.getTransaction();
63,try {
63,transaction.begin();
63,work.accept(entityManager);
63,transaction.commit();
63,catch (Exception e) {
63,if (transaction.isActive()) transaction.rollback();
63,throw e;
63,finally {
63,entityManager.close();
63,"In practice, we never access the database directly from a main() method."
63,So now let’s talk about how to organize persistence logic in a real system.
63,The rest of this chapter is not compulsory.
63,"If you’re itching for more details about Hibernate itself, you’re quite welcome to skip straight to the next chapter, and come back later."
63,1.5. Organizing persistence logic
63,"In a real program, persistence logic like the code shown above is usually interleaved with other sorts of code, including logic:"
63,"implementing the rules of the business domain, or"
63,for interacting with the user.
63,"Therefore, many developers quickly—even too quickly, in our opinion—reach for ways to isolate the persistence logic into some sort of separate architectural layer."
63,We’re going to ask you to suppress this urge for now.
63,The easiest way to use Hibernate is to call the Session or EntityManager directly.
63,"If you’re new to Hibernate, frameworks which wrap JPA are only going to make your life more difficult."
63,We prefer a bottom-up approach to organizing our code.
63,"We like to start thinking about methods and functions, not about architectural layers and container-managed objects."
63,"To illustrate the sort of approach to code organization that we advocate, let’s consider a service which queries the database using HQL or SQL."
63,"We might start with something like this, a mix of UI and persistence logic:"
63,"@Path(""/"") @Produces(""application/json"")"
63,public class BookResource {
63,"@GET @Path(""book/{isbn}"")"
63,public Book getBook(String isbn) {
63,"var book = sessionFactory.fromTransaction(session -> session.find(Book.class, isbn));"
63,return book == null ? Response.status(404).build() : book;
63,"Indeed, we might also finish with something like that—it’s quite hard to identify anything concretely wrong with the code above, and for such a simple case it seems really difficult to justify making this code more complicated by introducing additional objects."
63,"One very nice aspect of this code, which we wish to draw your attention to, is that session and transaction management is handled by generic ""framework"" code, just as we already recommended above."
63,"In this case, we’re using the fromTransaction() method, which happens to come built in to Hibernate."
63,"But you might prefer to use something else, for example:"
63,"in a container environment like Jakarta EE or Quarkus, container-managed transactions and container-managed persistence contexts, or"
63,something you write yourself.
63,"The important thing is that calls like createEntityManager() and getTransaction().begin() don’t belong in regular program logic, because it’s tricky and tedious to get the error handling correct."
63,Let’s now consider a slightly more complicated case.
63,"@Path(""/"") @Produces(""application/json"")"
63,public class BookResource {
63,private static final RESULTS_PER_PAGE = 20;
63,"@GET @Path(""books/{titlePattern}/{page:\\d+}"")"
63,"public List<Book> findBooks(String titlePattern, int page) {"
63,var books = sessionFactory.fromTransaction(session -> {
63,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
63,".setParameter(1, titlePattern)"
63,".setPage(Page.page(RESULTS_PER_PAGE, page))"
63,.getResultList();
63,});
63,return books.isEmpty() ? Response.status(404).build() : books;
63,"This is fine, and we won’t complain if you prefer to leave the code exactly as it appears above."
63,But there’s one thing we could perhaps improve.
63,"We love super-short methods with single responsibilities, and there looks to be an opportunity to introduce one here."
63,"Let’s hit the code with our favorite thing, the Extract Method refactoring. We obtain:"
63,"static List<Book> findBooksByTitleWithPagination(Session session,"
63,"String titlePattern, Page page) {"
63,"return session.createSelectionQuery(""from Book where title like ?1 order by title"", Book.class)"
63,".setParameter(1, titlePattern)"
63,.setPage(page)
63,.getResultList();
63,"This is an example of a query method, a function which accepts arguments to the parameters of a HQL or SQL query, and executes the query, returning its results to the caller."
63,"And that’s all it does; it doesn’t orchestrate additional program logic, and it doesn’t perform transaction or session management."
63,"It’s even better to specify the query string using the @NamedQuery annotation, so that Hibernate can validate the query it at startup time, that is, when the SessionFactory is created, instead of when the query is first executed."
63,"Indeed, since we included the Metamodel Generator in our Gradle build, the query can even be validated at compile time."
63,"We need a place to put the annotation, so lets move our query method to a new class:"
63,@CheckHQL // validate named queries at compile time
63,"@NamedQuery(name=""findBooksByTitle"","
63,"query=""from Book where title like :title order by title"")"
63,class Queries {
63,"static List<Book> findBooksByTitleWithPagination(Session session,"
63,"String titlePattern, Page page) {"
63,"return session.createNamedQuery(""findBooksByTitle"", Book.class)"
63,".setParameter(""title"", titlePattern)"
63,.setPage(page)
63,.getResultList();
63,Notice that our query method doesn’t attempt to hide the EntityManager from its clients.
63,"Indeed, the client code is responsible for providing the EntityManager or Session to the query method."
63,This is a quite distinctive feature of our whole approach.
63,The client code may:
63,"obtain an EntityManager or Session by calling inTransaction() or fromTransaction(), as we saw above, or,"
63,"in an environment with container-managed transactions, it might obtain it via dependency injection."
63,"Whatever the case, the code which orchestrates a unit of work usually just calls the Session or EntityManager directly, passing it along to helper methods like our query method if necessary."
63,@GET
63,"@Path(""books/{titlePattern}"")"
63,public List<Book> findBooks(String titlePattern) {
63,var books = sessionFactory.fromTransaction(session ->
63,"Queries.findBooksByTitleWithPagination(session, titlePattern,"
63,"Page.page(RESULTS_PER_PAGE, page));"
63,return books.isEmpty() ? Response.status(404).build() : books;
63,You might be thinking that our query method looks a bit boilerplatey.
63,"That’s true, perhaps, but we’re much more concerned that it’s not very typesafe."
63,"Indeed, for many years, the lack of compile-time checking for HQL queries and code which binds arguments to query parameters was our number one source of discomfort with Hibernate."
63,"Fortunately, there’s now a solution to both problems: as an incubating feature of Hibernate 6.3, we now offer the possibility to have the Metamodel Generator fill in the implementation of such query methods for you."
63,"This facility is the topic of a whole chapter of this introduction, so for now we’ll just leave you with one simple example."
63,Suppose we simplify Queries to just the following:
63,interface Queries {
63,"@HQL(""where title like :title order by title"")"
63,"List<Book> findBooksByTitleWithPagination(String title, Page page);"
63,Then the Metamodel Generator automatically produces an implementation of the method annotated @HQL in a class named Queries_.
63,We can call it just like we called our handwritten version:
63,@GET
63,"@Path(""books/{titlePattern}"")"
63,public List<Book> findBooks(String titlePattern) {
63,var books = sessionFactory.fromTransaction(session ->
63,"Queries_.findBooksByTitleWithPagination(session, titlePattern,"
63,"Page.page(RESULTS_PER_PAGE, page));"
63,return books.isEmpty() ? Response.status(404).build() : books;
63,"In this case, the quantity of code eliminated is pretty trivial."
63,The real value is in improved type safety.
63,We now find out about errors in assignments of arguments to query parameters at compile time.
63,"At this point, we’re certain you’re full of doubts about this idea."
63,And quite rightly so.
63,"We would love to answer your objections right here, but that will take us much too far off track."
63,So we ask you to file away these thoughts for now.
63,We promise to make it make sense when we properly address this topic later.
63,"And, after that, if you still don’t like this approach, please understand that it’s completely optional."
63,Nobody’s going to come around to your house to force it down your throat.
63,"Now that we have a rough picture of what our persistence logic might look like, it’s natural to ask how we should test our code."
63,1.6. Testing persistence logic
63,"When we write tests for our persistence logic, we’re going to need:"
63,"a database, with"
63,"an instance of the schema mapped by our persistent entities, and"
63,"a set of test data, in a well-defined state at the beginning of each test."
63,"It might seem obvious that we should test against the same database system that we’re going to use in production, and, indeed, we should certainly have at least some tests for this configuration."
63,"But on the other hand, tests which perform I/O are much slower than tests which don’t, and most databases can’t be set up to run in-process."
63,"So, since most persistence logic written using Hibernate 6 is extremely portable between databases, it often makes good sense to test against an in-memory Java database."
63,(H2 is the one we recommend.)
63,"We do need to be careful here if our persistence code uses native SQL, or if it uses concurrency-management features like pessimistic locks."
63,"Whether we’re testing against our real database, or against an in-memory Java database, we’ll need to export the schema at the beginning of a test suite."
63,"We usually do this when we create the Hibernate SessionFactory or JPA EntityManager, and so traditionally we’ve used a configuration property for this."
63,The JPA-standard property is jakarta.persistence.schema-generation.database.action.
63,"For example, if we’re using Configuration to configure Hibernate, we could write:"
63,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
63,Action.SPEC_ACTION_DROP_AND_CREATE);
63,"Alternatively, in Hibernate 6, we may use the new SchemaManager API to export the schema, just as we did above."
63,sessionFactory.getSchemaManager().exportMappedObjects(true);
63,"Since executing DDL statements is very slow on many databases, we don’t want to do this before every test."
63,"Instead, to ensure that each test begins with the test data in a well-defined state, we need to do two things before each test:"
63,"clean up any mess left behind by the previous test, and then"
63,reinitialize the test data.
63,"We may truncate all the tables, leaving an empty database schema, using the SchemaManager."
63,sessionFactory.getSchemaManager().truncateMappedObjects();
63,"After truncating tables, we might need to initialize our test data."
63,"We may specify test data in a SQL script, for example:"
63,/import.sql
63,"insert into Books (isbn, title) values ('9781932394153', 'Hibernate in Action')"
63,"insert into Books (isbn, title) values ('9781932394887', 'Java Persistence with Hibernate')"
63,"insert into Books (isbn, title) values ('9781617290459', 'Java Persistence with Hibernate, Second Edition')"
63,"If we name this file import.sql, and place it in the root classpath, that’s all we need to do."
63,"Otherwise, we need to specify the file in the configuration property jakarta.persistence.sql-load-script-source."
63,"If we’re using Configuration to configure Hibernate, we could write:"
63,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_LOAD_SCRIPT_SOURCE,"
63,"""/org/example/test-data.sql"");"
63,The SQL script will be executed every time exportMappedObjects() or truncateMappedObjects() is called.
63,There’s another sort of mess a test can leave behind: cached data in the second-level cache.
63,We recommend disabling Hibernate’s second-level cache for most sorts of testing.
63,"Alternatively, if the second-level cache is not disabled, then before each test we should call:"
63,sessionFactory.getCache().evictAllRegions();
63,"Now, suppose you’ve followed our advice, and written your entities and query methods to minimize dependencies on ""infrastructure"", that is, on libraries other than JPA and Hibernate, on frameworks,"
63,"on container-managed objects, and even on bits of your own system which are hard to instantiate from scratch."
63,Then testing persistence logic is now straightforward!
63,You’ll need to:
63,"bootstrap Hibernate and create a SessionFactory or EntityManagerFactory and the beginning of your test suite (we’ve already seen how to do that), and"
63,"create a new Session or EntityManager inside each @Test method, using inTransaction(), for example."
63,"Actually, some tests might require multiple sessions."
63,But be careful not to leak a session between different tests.
63,Another important test we’ll need is one which validates our O/R mapping annotations against the actual database schema.
63,"This is again the job of the schema management tooling, either:"
63,"configuration.setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
63,Action.ACTION_VALIDATE);
63,Or:
63,sessionFactory.getSchemaManager().validateMappedObjects();
63,"This ""test"" is one which many people like to run even in production, when the system starts up."
63,1.7. Architecture and the persistence layer
63,"Let’s now consider a different approach to code organization, one we treat with suspicion."
63,"In this section, we’re going to give you our opinion."
63,"If you’re only interested in facts, or if you prefer not to read things that might undermine the opinion you currently hold, please feel free to skip straight to the next chapter."
63,"Hibernate is an architecture-agnostic library, not a framework, and therefore integrates comfortably with a wide range of Java frameworks and containers."
63,"Consistent with our place within the ecosystem, we’ve historically avoided giving out much advice on architecture."
63,"This is a practice we’re now perhaps inclined to regret, since the resulting vacuum has come to be filled with advice from people advocating architectures, design patterns, and extra frameworks which we suspect make Hibernate a bit less pleasant to use than it should be."
63,"In particular, frameworks which wrap JPA seem to add bloat while subtracting some of the fine-grained control over data access that Hibernate works so hard to provide."
63,"These frameworks don’t expose the full feature set of Hibernate, and so the program is forced to work with a less powerful abstraction."
63,"The stodgy, dogmatic, conventional wisdom, which we hesitate to challenge for simple fear of pricking ourselves on the erect hackles that inevitably accompany such dogma-baiting is:"
63,Code which interacts with the database belongs in a separate persistence layer.
63,We lack the courage—perhaps even the conviction—to tell you categorically to not follow this recommendation.
63,"But we do ask you to consider the cost in boilerplate of any architectural layer, and whether the benefits this cost buys are really worth it in the context of your system."
63,"To add a little background texture to this discussion, and at the risk of our Introduction degenerating into a rant at such an early stage, we’re going ask you to humor us while talk a little more about ancient history."
63,An epic tale of DAOs and Repositories
63,"Back in the dark days of Java EE 4, before the standardization of Hibernate, and subsequent ascendance of JPA in Java enterprise development, it was common to hand-code the messy JDBC interactions that Hibernate takes care of today."
63,"In those terrible times, a pattern arose that we used to call Data Access Objects (DAOs)."
63,"A DAO gave you a place to put all that nasty JDBC code, leaving the important program logic cleaner."
63,"When Hibernate arrived suddenly on the scene in 2001, developers loved it."
63,"But Hibernate implemented no specification, and many wished to reduce or at least localize the dependence of their project logic on Hibernate."
63,"An obvious solution was to keep the DAOs around, but to replace the JDBC code inside them with calls to the Hibernate Session."
63,We partly blame ourselves for what happened next.
63,Back in 2002 and 2003 this really seemed like a pretty reasonable thing to do.
63,"In fact, we contributed to the popularity of this approach by recommending—or at least not discouraging—the use of DAOs in Hibernate in Action."
63,"We hereby apologize for this mistake, and for taking much too long to recognize it."
63,"Eventually, some folks came to believe that their DAOs shielded their program from depending in a hard way on ORM, allowing them to ""swap out"" Hibernate, and replace it with JDBC, or with something else."
63,"In fact, this was never really true—there’s quite a deep difference between the programming model of JDBC, where every interaction with the database is explicit and synchronous, and the programming model of stateful sessions in Hibernate, where updates are implicit, and SQL statements are executed asynchronously."
63,"But then the whole landscape for persistence in Java changed in April 2006, when the final draft of JPA 1.0 was approved."
63,"Java now had a standard way to do ORM, with multiple high-quality implementations of the standard API."
63,"This was the end of the line for the DAOs, right?"
63,"Well, no."
63,It wasn’t.
63,"DAOs were rebranded ""repositories"", and continue to enjoy a sort-of zombie afterlife as a front-end to JPA."
63,"But are they really pulling their weight, or are they just unnecessary extra complexity and bloat? An extra layer of indirection that makes stack traces harder to read and code harder to debug?"
63,Our considered view is that they’re mostly just bloat.
63,"The JPA EntityManager is a ""repository"", and it’s a standard repository with a well-defined specification written by people who spend all day thinking about persistence."
63,"If these repository frameworks offered anything actually useful—and not obviously foot-shooty—over and above what EntityManager provides, we would have already added it to EntityManager decades ago."
63,"Ultimately, we’re not sure you need a separate persistence layer at all."
63,At least consider the possibility that it might be OK to call the EntityManager directly from your business logic.
63,We can already hear you hissing at our heresy.
63,"But before slamming shut the lid of your laptop and heading off to fetch garlic and a pitchfork, take a couple of hours to weigh what we’re proposing."
63,"OK, so, look, if it makes you feel better, one way to view EntityManager is to think of it as a single generic ""repository"" that works for every entity in your system."
63,"From this point of view, JPA is your persistence layer."
63,And there’s few good reasons to wrap this abstraction in a second abstraction that’s less generic.
63,"Even where a distinct persistence layer is appropriate, DAO-style repositories aren’t the unambiguously most-correct way to factorize the equation:"
63,"most nontrivial queries touch multiple entities, and so it’s often quite ambiguous which repository such a query belongs to,"
63,"most queries are extremely specific to a particular fragment of program logic, and aren’t reused in different places across the system, and"
63,the various operations of a repository rarely interact or share common internal implementation details.
63,"Indeed, repositories, by nature, exhibit very low cohesion."
63,"A layer of repository objects might make sense if you have multiple implementations of each repository, but in practice almost nobody ever does."
63,"That’s because they’re also extremely highly coupled to their clients, with a very large API surface."
63,"And, on the contrary, a layer is only easily replaceable if it has a very narrow API."
63,"Some people do indeed use mock repositories for testing, but we really struggle to see any value in this."
63,"If we don’t want to run our tests against our real database, it’s usually very easy to ""mock"" the database itself by running tests against an in-memory Java database like H2."
63,"This works even better in Hibernate 6 than in older versions of Hibernate, since HQL is now much more portable between platforms."
63,"Phew, let’s move on."
63,1.8. Overview
63,It’s now time to begin our journey toward actually understanding the code we saw earlier.
63,This introduction will guide you through the basic tasks involved in developing a program that uses Hibernate for persistence:
63,"configuring and bootstrapping Hibernate, and obtaining an instance of SessionFactory or EntityManagerFactory,"
63,"writing a domain model, that is, a set of entity classes which represent the persistent types in your program, and which map to tables of your database,"
63,"customizing these mappings when the model maps to a pre-existing relational schema,"
63,"using the Session or EntityManager to perform operations which query the database and return entity instances, or which update the data held in the database,"
63,"using the Hibernate Metamodel Generator to improve compile-time type-safety,"
63,"writing complex queries using the Hibernate Query Language (HQL) or native SQL, and, finally"
63,tuning performance of the data access logic.
63,"Naturally, we’ll start at the top of this list, with the least-interesting topic: configuration."
63,2. Configuration and bootstrap
63,We would love to make this section short.
63,"Unfortunately, there’s several distinct ways to configure and bootstrap Hibernate, and we’re going to have to describe at least two of them in detail."
63,The four basic ways to obtain an instance of Hibernate are shown in the following table:
63,"Using the standard JPA-defined XML, and the operation Persistence.createEntityManagerFactory()"
63,Usually chosen when portability between JPA implementations is important.
63,Using the Configuration class to construct a SessionFactory
63,"When portability between JPA implementations is not important, this option is quicker, adds some flexibility and saves a typecast."
63,Using the more complex APIs defined in org.hibernate.boot
63,"Used primarily by framework integrators, this option is outside the scope of this document."
63,By letting the container take care of the bootstrap process and of injecting the SessionFactory or EntityManagerFactory
63,Used in a container environment like WildFly or Quarkus.
63,Here we’ll focus on the first two options.
63,Hibernate in containers
63,"Actually, the last option is extremely popular, since every major Java application server and microservice framework comes with built-in support for Hibernate."
63,Such container environments typically also feature facilities to automatically manage the lifecycle of an EntityManager or Session and its association with container-managed transactions.
63,"To learn how to configure Hibernate in such a container environment, you’ll need to refer to the documentation of your chosen container."
63,"For Quarkus, here’s the relevant documentation."
63,"If you’re using Hibernate outside of a container environment,"
63,you’ll need to:
63,"include Hibernate ORM itself, along with the appropriate JDBC driver, as dependencies of your project, and"
63,"configure Hibernate with information about your database,"
63,by specifying configuration properties.
63,2.1. Including Hibernate in your project build
63,"First, add the following dependency to your project:"
63,org.hibernate.orm:hibernate-core:{version}
63,Where {version} is the version of Hibernate you’re using.
63,You’ll also need to add a dependency for the JDBC
63,driver for your database.
63,Table 2. JDBC driver dependencies
63,Database
63,Driver dependency
63,PostgreSQL or CockroachDB
63,org.postgresql:postgresql:{version}
63,MySQL or TiDB
63,com.mysql:mysql-connector-j:{version}
63,MariaDB
63,org.mariadb.jdbc:mariadb-java-client:{version}
63,DB2
63,com.ibm.db2:jcc:{version}
63,SQL Server
63,com.microsoft.sqlserver:mssql-jdbc:${version}
63,Oracle
63,com.oracle.database.jdbc:ojdbc11:${version}
63,com.h2database:h2:{version}
63,HSQLDB
63,org.hsqldb:hsqldb:{version}
63,Where {version} is the latest version of the JDBC driver for your databse.
63,2.2. Optional dependencies
63,"Optionally, you might also add any of the following additional features:"
63,Table 3. Optional dependencies
63,Optional feature
63,Dependencies
63,An SLF4J logging implementation
63,org.apache.logging.log4j:log4j-core
63,or org.slf4j:slf4j-jdk14
63,"A JDBC connection pool, for example, Agroal"
63,org.hibernate.orm:hibernate-agroal
63,and io.agroal:agroal-pool
63,"The Hibernate Metamodel Generator, especially if you’re using the JPA criteria query API"
63,org.hibernate.orm:hibernate-jpamodelgen
63,"The Query Validator, for compile-time checking of HQL"
63,org.hibernate:query-validator
63,"Hibernate Validator, an implementation of Bean Validation"
63,org.hibernate.validator:hibernate-validator
63,and org.glassfish:jakarta.el
63,Local second-level cache support via JCache and EHCache
63,org.hibernate.orm:hibernate-jcache
63,and org.ehcache:ehcache
63,Local second-level cache support via JCache and Caffeine
63,org.hibernate.orm:hibernate-jcache
63,and com.github.ben-manes.caffeine:jcache
63,Distributed second-level cache support via Infinispan
63,org.infinispan:infinispan-hibernate-cache-v60
63,"A JSON serialization library for working with JSON datatypes, for example, Jackson or Yasson"
63,com.fasterxml.jackson.core:jackson-databind
63,or org.eclipse:yasson
63,Hibernate Spatial
63,org.hibernate.orm:hibernate-spatial
63,"Envers, for auditing historical data"
63,org.hibernate.orm:hibernate-envers
63,You might also add the Hibernate bytecode enhancer to your
63,Gradle build if you want to use field-level lazy fetching.
63,2.3. Configuration using JPA XML
63,"Sticking to the JPA-standard approach, we would provide a file named persistence.xml, which we usually place in the META-INF directory of a persistence archive, that is, of the .jar file or directory which contains our entity classes."
63,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
63,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
63,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_0.xsd"""
63,"version=""2.0"">"
63,"<persistence-unit name=""org.hibernate.example"">"
63,<class>org.hibernate.example.Book</class>
63,<class>org.hibernate.example.Author</class>
63,<properties>
63,<!-- PostgreSQL -->
63,"<property name=""jakarta.persistence.jdbc.url"""
63,"value=""jdbc:postgresql://localhost/example""/>"
63,<!-- Credentials -->
63,"<property name=""jakarta.persistence.jdbc.user"""
63,"value=""gavin""/>"
63,"<property name=""jakarta.persistence.jdbc.password"""
63,"value=""hibernate""/>"
63,<!-- Automatic schema export -->
63,"<property name=""jakarta.persistence.schema-generation.database.action"""
63,"value=""drop-and-create""/>"
63,<!-- SQL statement logging -->
63,"<property name=""hibernate.show_sql"" value=""true""/>"
63,"<property name=""hibernate.format_sql"" value=""true""/>"
63,"<property name=""hibernate.highlight_sql"" value=""true""/>"
63,</properties>
63,</persistence-unit>
63,</persistence>
63,"The <persistence-unit> element defines a named persistence unit, that is:"
63,"a collection of associated entity types, along with"
63,"a set of default configuration settings, which may be augmented or overridden at runtime."
63,Each <class> element specifies the fully-qualified name of an entity class.
63,Scanning for entity classes
63,"In some container environments, for example, in any EE container, the <class> elements are unnecessary, since the container will scan the archive for annotated classes, and automatically recognize any class annotated @Entity."
63,Each <property> element specifies a configuration property and its value.
63,Note that:
63,"the configuration properties in the jakarta.persistence namespace are standard properties defined by the JPA spec, and"
63,properties in the hibernate namespace are specific to Hibernate.
63,We may obtain an EntityManagerFactory by calling Persistence.createEntityManagerFactory():
63,EntityManagerFactory entityManagerFactory =
63,"Persistence.createEntityManagerFactory(""org.hibernate.example"");"
63,"If necessary, we may override configuration properties specified in persistence.xml:"
63,EntityManagerFactory entityManagerFactory =
63,"Persistence.createEntityManagerFactory(""org.hibernate.example"","
63,"Map.of(AvailableSettings.JAKARTA_JDBC_PASSWORD, password));"
63,2.4. Configuration using Hibernate API
63,"Alternatively, the venerable class Configuration allows an instance of Hibernate to be configured in Java code."
63,SessionFactory sessionFactory =
63,new Configuration()
63,.addAnnotatedClass(Book.class)
63,.addAnnotatedClass(Author.class)
63,// PostgreSQL
63,".setProperty(AvailableSettings.JAKARTA_JDBC_URL, ""jdbc:postgresql://localhost/example"")"
63,// Credentials
63,".setProperty(AvailableSettings.JAKARTA_JDBC_USER, user)"
63,".setProperty(AvailableSettings.JAKARTA_JDBC_PASSWORD, password)"
63,// Automatic schema export
63,".setProperty(AvailableSettings.JAKARTA_HBM2DDL_DATABASE_ACTION,"
63,Action.SPEC_ACTION_DROP_AND_CREATE)
63,// SQL statement logging
63,".setProperty(AvailableSettings.SHOW_SQL, TRUE.toString())"
63,".setProperty(AvailableSettings.FORMAT_SQL, TRUE.toString())"
63,".setProperty(AvailableSettings.HIGHLIGHT_SQL, TRUE.toString())"
63,// Create a new SessionFactory
63,.buildSessionFactory();
63,"The Configuration class has survived almost unchanged since the very earliest (pre-1.0) versions of Hibernate, and so it doesn’t look particularly modern."
63,"On the other hand, it’s very easy to use, and exposes some options that persistence.xml doesn’t support."
63,Advanced configuration options
63,"Actually, the Configuration class is just a very simple facade for the more modern, much more powerful—but more complex—API defined in the package org.hibernate.boot."
63,"This API is useful if you have very advanced requirements, for example, if you’re writing a framework or implementing a container."
63,"You’ll find more information in the User Guide, and in the package-level documentation of org.hibernate.boot."
63,2.5. Configuration using Hibernate properties file
63,"If we’re using the Hibernate Configuration API, but we don’t want to put certain configuration properties directly in the Java code, we can specify them in a file named hibernate.properties, and place the file in the root classpath."
63,# PostgreSQL
63,jakarta.persistence.jdbc.url=jdbc:postgresql://localhost/example
63,# Credentials
63,jakarta.persistence.jdbc.user=hibernate
63,jakarta.persistence.jdbc.password=zAh7mY$2MNshzAQ5
63,# SQL statement logging
63,hibernate.show_sql=true
63,hibernate.format_sql=true
63,hibernate.highlight_sql=true
63,2.6. Basic configuration settings
63,The class AvailableSettings enumerates all the configuration properties understood by Hibernate.
63,"Of course, we’re not going to cover every useful configuration setting in this chapter."
63,"Instead, we’ll mention the ones you need to get started, and come back to some other important settings later, especially when we talk about performance tuning."
63,Hibernate has many—too many—switches and toggles.
63,"Please don’t go crazy messing about with these settings; most of them are rarely needed, and many only exist to provide backward compatibility with older versions of Hibernate."
63,"With rare exception, the default behavior of every one of these settings was carefully chosen to be the behavior we recommend."
63,The properties you really do need to get started are these three:
63,Table 4. JDBC connection settings
63,Configuration property name
63,Purpose
63,jakarta.persistence.jdbc.url
63,JDBC URL of your database
63,jakarta.persistence.jdbc.user and jakarta.persistence.jdbc.password
63,Your database credentials
63,"In Hibernate 6, you don’t need to specify hibernate.dialect."
63,The correct Hibernate SQL Dialect will be determined for you automatically.
63,The only reason to specify this property is if you’re using a custom user-written Dialect class.
63,"Similarly, neither hibernate.connection.driver_class nor jakarta.persistence.jdbc.driver is needed when working with one of the supported databases."
63,Pooling JDBC connections is an extremely important performance optimization.
63,You can set the size of Hibernate’s built-in connection pool using this property:
63,Table 5. Built-in connection pool size
63,Configuration property name
63,Purpose
63,hibernate.connection.pool_size
63,The size of the built-in connection pool
63,"By default, Hibernate uses a simplistic built-in connection pool."
63,"This pool is not meant for use in production, and later, when we discuss performance, we’ll see how to select a more robust implementation."
63,"Alternatively, in a container environment, you’ll need at least one of these properties:"
63,Table 6. Transaction management settings
63,Configuration property name
63,Purpose
63,jakarta.persistence.transactionType
63,"(Optional, defaults to JTA)"
63,Determines if transaction management is via JTA or resource-local transactions.
63,Specify RESOURCE_LOCAL if JTA should not be used.
63,jakarta.persistence.jtaDataSource
63,JNDI name of a JTA datasource
63,jakarta.persistence.nonJtaDataSource
63,JNDI name of a non-JTA datasource
63,"In this case, Hibernate obtains pooled JDBC database connections from a container-managed DataSource."
63,2.7. Automatic schema export
63,You can have Hibernate infer your database schema from the mapping
63,"annotations you’ve specified in your Java code, and export the schema at"
63,initialization time by specifying one or more of the following configuration
63,properties:
63,Table 7. Schema management settings
63,Configuration property name
63,Purpose
63,jakarta.persistence.schema-generation.database.action
63,"If drop-and-create, first drop the schema and then export tables, sequences, and constraints"
63,"If create, export tables, sequences, and constraints, without attempting to drop them first"
63,"If create-drop, drop the schema and recreate it on SessionFactory startup"
63,"Additionally, drop the schema on SessionFactory shutdown"
63,"If drop, drop the schema on SessionFactory shutdown"
63,"If validate, validate the database schema without changing it"
63,"If update, only export what’s missing in the schema"
63,jakarta.persistence.create-database-schemas
63,"(Optional) If true, automatically create schemas and catalogs"
63,jakarta.persistence.schema-generation.create-source
63,"(Optional) If metadata-then-script or script-then-metadata, execute an additional SQL script when exported tables and sequences"
63,jakarta.persistence.schema-generation.create-script-source
63,(Optional) The name of a SQL DDL script to be executed
63,jakarta.persistence.sql-load-script-source
63,(Optional) The name of a SQL DML script to be executed
63,This feature is extremely useful for testing.
63,"The easiest way to pre-initialize a database with test or ""reference"" data is to place a list of SQL insert statements in a file named, for example, import.sql, and specify the path to this file using the property jakarta.persistence.sql-load-script-source."
63,"We’ve already seen an example of this approach, which is cleaner than writing Java code to instantiate entity instances and calling persist() on each of them."
63,"As we mentioned earlier, it can also be useful to control schema export programmatically."
63,The SchemaManager API allows programmatic control over schema export:
63,sessionFactory.getSchemaManager().exportMappedObjects(true);
63,JPA has a more limited and less ergonomic API:
63,"Persistence.generateSchema(""org.hibernate.example"","
63,"Map.of(JAKARTA_HBM2DDL_DATABASE_ACTION, CREATE))"
63,2.8. Logging the generated SQL
63,"To see the generated SQL as it’s sent to the database, you have two options."
63,"One way is to set the property hibernate.show_sql to true, and Hibernate will log SQL direct to the console."
63,You can make the output much more readable by enabling formatting or highlighting.
63,These settings really help when troubleshooting the generated SQL statements.
63,Table 8. Settings for SQL logging to the console
63,Configuration property name
63,Purpose
63,hibernate.show_sql
63,"If true, log SQL directly to the console"
63,hibernate.format_sql
63,"If true, log SQL in a multiline, indented format"
63,hibernate.highlight_sql
63,"If true, log SQL with syntax highlighting via ANSI escape codes"
63,"Alternatively, you can enable debug-level logging for the category org.hibernate.SQL using your preferred SLF4J logging implementation."
63,"For example, if you’re using Log4J 2 (as above in Optional dependencies), add these lines to your log4j2.properties file:"
63,# SQL execution
63,logger.hibernate.name = org.hibernate.SQL
63,logger.hibernate.level = debug
63,# JDBC parameter binding
63,logger.jdbc-bind.name=org.hibernate.orm.jdbc.bind
63,logger.jdbc-bind.level=trace
63,# JDBC result set extraction
63,logger.jdbc-extract.name=org.hibernate.orm.jdbc.extract
63,logger.jdbc-extract.level=trace
63,But with this approach we miss out on the pretty highlighting.
63,2.9. Minimizing repetitive mapping information
63,"The following properties are very useful for minimizing the amount of information you’ll need to explicitly specify in @Table and @Column annotations, which we’ll discuss below in Object/relational mapping:"
63,Table 9. Settings for minimizing explicit mapping information
63,Configuration property name
63,Purpose
63,hibernate.default_schema
63,A default schema name for entities which do not explicitly declare one
63,hibernate.default_catalog
63,A default catalog name for entities which do not explicitly declare one
63,hibernate.physical_naming_strategy
63,A PhysicalNamingStrategy implementing your database naming standards
63,hibernate.implicit_naming_strategy
63,"An ImplicitNamingStrategy which specifies how ""logical"" names of relational objects should be inferred when no name is specified in annotations"
63,"Writing your own PhysicalNamingStrategy and/or ImplicitNamingStrategy is an especially good way to reduce the clutter of annotations on your entity classes, and to implement your database naming conventions, and so we think you should do it for any nontrivial data model."
63,We’ll have more to say about them in Naming strategies.
63,2.10. Nationalized character data in SQL Server
63,"By default, SQL Server’s char and varchar types don’t accommodate Unicode data."
63,But a Java string may contain any Unicode character.
63,"So, if you’re working with SQL Server, you might need to force Hibernate to use the nchar and nvarchar column types."
63,Table 10. Setting the use of nationalized character data
63,Configuration property name
63,Purpose
63,hibernate.use_nationalized_character_data
63,Use nchar and nvarchar instead of char and varchar
63,"On the other hand, if only some columns store nationalized data, use the @Nationalized annotation to indicate fields of your entities which map these columns."
63,"Alternatively, you can configure SQL Server to use the UTF-8 enabled collation _UTF8."
63,3. Entities
63,An entity is a Java class which represents data in a relational database table.
63,We say that the entity maps or maps to the table.
63,"Much less commonly, an entity might aggregate data from multiple tables, but we’ll get to that later."
63,An entity has attributes—properties or fields—which map to columns of the table.
63,"In particular, every entity must have an identifier or id, which maps to the primary key of the table."
63,"The id allows us to uniquely associate a row of the table with an instance of the Java class, at least within a given persistence context."
63,"We’ll explore the idea of a persistence context later. For now, think of it as a one-to-one mapping between ids and entity instances."
63,An instance of a Java class cannot outlive the virtual machine to which it belongs.
63,But we may think of an entity instance having a lifecycle which transcends a particular instantiation in memory.
63,"By providing its id to Hibernate, we may re-materialize the instance in a new persistence context, as long as the associated row is present in the database."
63,"Therefore, the operations persist() and remove() may be thought of as demarcating the beginning and end of the lifecycle of an entity, at least with respect to persistence."
63,"Thus, an id represents the persistent identity of an entity, an identity that outlives a particular instantiation in memory."
63,"And this is an important difference between entity class itself and the values of its attributes—the entity has a persistent identity, and a well-defined lifecycle with respect to persistence, whereas a String or List representing one of its attribute values doesn’t."
63,An entity usually has associations to other entities.
63,"Typically, an association between two entities maps to a foreign key in one of the database tables."
63,"A group of mutually associated entities is often called a domain model, though data model is also a perfectly good term."
63,3.1. Entity classes
63,An entity must:
63,"be a non-final class,"
63,with a non-private constructor with no parameters.
63,"On the other hand, the entity class may be either concrete or abstract, and it may have any number of additional constructors."
63,An entity class may be a static inner class.
63,Every entity class must be annotated @Entity.
63,@Entity
63,class Book {
63,Book() {}
63,...
63,"Alternatively, the class may be identified as an entity type by providing an XML-based mapping for the class."
63,Mapping entities using XML
63,"When XML-based mappings are used, the <entity> element is used to declare an entity class:"
63,<entity-mappings>
63,<package>org.hibernate.example</package>
63,"<entity class=""Book"">"
63,<attributes> ... </attributes>
63,</entity>
63,...
63,</entity-mappings>
63,"Since the orm.xml mapping file format defined by the JPA specification was modelled closely on the annotation-based mappings, it’s usually easy to go back and forth between the two options."
63,"We won’t have much more to say about XML-based mappings in this Introduction, since it’s not our preferred way to do things."
63,"""Dynamic"" models"
63,We love representing entities as classes because the classes give us a type-safe model of our data.
63,But Hibernate also has the ability to represent entities as detyped instances of java.util.Map.
63,"There’s information in the User Guide, if you’re curious."
63,This must sound like a weird feature for a project that places importance on type-safety.
63,"Actually, it’s a useful capability for a very particular sort of generic code."
63,"For example, Hibernate Envers is a great auditing/versioning system for Hibernate entities."
63,Envers makes use of maps to represent its versioned model of the data.
63,3.2. Access types
63,"Each entity class has a default access type, either:"
63,"direct field access, or"
63,property access.
63,Hibernate automatically determines the access type from the location of attribute-level annotations.
63,Concretely:
63,"if a field is annotated @Id, field access is used, or"
63,"if a getter method is annotated @Id, property access is used."
63,"Back when Hibernate was just a baby, property access was quite popular in the Hibernate community."
63,"Today, however, field access is much more common."
63,"The default access type may be specified explicitly using the @Access annotation, but we strongly discourage this, since it’s ugly and never necessary."
63,Mapping annotations should be placed consistently:
63,"if @Id annotates a field, the other mapping annotations should also be applied to fields, or,"
63,"if @Id annotates a getter, the other mapping annotations should be applied to getters."
63,It is in principle possible to mix field and property access using explicit @Access annotations at the attribute level.
63,We don’t recommend doing this.
63,"An entity class like Book, which does not extend any other entity class, is called a root entity."
63,Every root entity must declare an identifier attribute.
63,3.3. Entity class inheritance
63,An entity class may extend another entity class.
63,@Entity
63,class AudioBook extends Book {
63,AudioBook() {}
63,...
63,A subclass entity inherits every persistent attribute of every entity it extends.
63,A root entity may also extend another class and inherit mapped attributes from the other class.
63,"But in this case, the class which declares the mapped attributes must be annotated @MappedSuperclass."
63,@MappedSuperclass
63,class Versioned {
63,...
63,@Entity
63,class Book extends Versioned {
63,...
63,"A root entity class must declare an attribute annotated @Id, or inherit one from a @MappedSuperclass."
63,A subclass entity always inherits the identifier attribute of the root entity.
63,It may not declare its own @Id attribute.
63,3.4. Identifier attributes
63,An identifier attribute is usually a field:
63,@Entity
63,class Book {
63,Book() {}
63,@Id
63,Long id;
63,...
63,But it may be a property:
63,@Entity
63,class Book {
63,Book() {}
63,private Long id;
63,@Id
63,Long getId() { return id; }
63,void setId(Long id) { this.id = id; }
63,...
63,An identifier attribute must be annotated @Id or @EmbeddedId.
63,Identifier values may be:
63,"assigned by the application, that is, by your Java code, or"
63,generated and assigned by Hibernate.
63,We’ll discuss the second option first.
63,3.5. Generated identifiers
63,"An identifier is often system-generated, in which case it should be annotated @GeneratedValue:"
63,@Id @GeneratedValue
63,Long id;
63,"System-generated identifiers, or surrogate keys make it easier to evolve or refactor the relational data model."
63,"If you have the freedom to define the relational schema, we recommend the use of surrogate keys."
63,"On the other hand, if, as is more common, you’re working with a pre-existing database schema, you might not have the option."
63,"JPA defines the following strategies for generating ids, which are enumerated by GenerationType:"
63,Table 11. Standard id generation strategies
63,Strategy
63,Java type
63,Implementation
63,GenerationType.UUID
63,UUID or String
63,A Java UUID
63,GenerationType.IDENTITY
63,Long or Integer
63,An identity or autoincrement column
63,GenerationType.SEQUENCE
63,Long or Integer
63,A database sequence
63,GenerationType.TABLE
63,Long or Integer
63,A database table
63,GenerationType.AUTO
63,Long or Integer
63,"Selects SEQUENCE, TABLE, or UUID based on the identifier type and capabilities of the database"
63,"For example, this UUID is generated in Java code:"
63,@Id @GeneratedValue UUID id;
63,// AUTO strategy selects UUID based on the field type
63,"This id maps to a SQL identity, auto_increment, or bigserial column:"
63,@Id @GeneratedValue(strategy=IDENTITY) Long id;
63,The @SequenceGenerator and @TableGenerator annotations allow further control over SEQUENCE and TABLE generation respectively.
63,Consider this sequence generator:
63,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
63,Values are generated using a database sequence defined as follows:
63,create sequence seq_book start with 5 increment by 10
63,Notice that Hibernate doesn’t have to go to the database every time a new identifier is needed.
63,"Instead, a given process obtains a block of ids, of size allocationSize, and only needs to hit the database each time the block is exhausted."
63,"Of course, the downside is that generated identifiers are not contiguous."
63,"If you let Hibernate export your database schema, the sequence definition will have the right start with and increment values."
63,"But if you’re working with a database schema managed outside Hibernate, make sure the initialValue and allocationSize members of @SequenceGenerator match the start with and increment specified in the DDL."
63,Any identifier attribute may now make use of the generator named bookSeq:
63,@Id
63,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
63,// reference to generator defined elsewhere
63,Long id;
63,"Actually, it’s extremely common to place the @SequenceGenerator annotation on the @Id attribute that makes use of it:"
63,@Id
63,"@GeneratedValue(strategy=SEQUENCE, generator=""bookSeq"")"
63,// reference to generator defined below
63,"@SequenceGenerator(name=""bookSeq"", sequenceName=""seq_book"", initialValue = 5, allocationSize=10)"
63,Long id;
63,JPA id generators may be shared between entities.
63,"A @SequenceGenerator or @TableGenerator must have a name, and may be shared between multiple id attributes."
63,This fits somewhat uncomfortably with the common practice of annotating the @Id attribute which makes use of the generator!
63,"As you can see, JPA provides quite adequate support for the most common strategies for system-generated ids."
63,"However, the annotations themselves are a bit more intrusive than they should be, and there’s no well-defined way to extend this framework to support custom strategies for id generation."
63,Nor may @GeneratedValue be used on a property not annotated @Id.
63,"Since custom id generation is a rather common requirement, Hibernate provides a very carefully-designed framework for user-defined Generators, which we’ll discuss in User-defined generators."
63,3.6. Natural keys as identifiers
63,Not every identifier attribute maps to a (system-generated) surrogate key.
63,Primary keys which are meaningful to the user of the system are called natural keys.
63,"When the primary key of a table is a natural key, we don’t annotate the identifier attribute @GeneratedValue, and it’s the responsibility of the application code to assign a value to the identifier attribute."
63,@Entity
63,class Book {
63,@Id
63,String isbn;
63,...
63,"Of particular interest are natural keys which comprise more than one database column, and such natural keys are called composite keys."
63,3.7. Composite identifiers
63,"If your database uses composite keys, you’ll need more than one identifier attribute."
63,There are two ways to map composite keys in JPA:
63,"using an @IdClass, or"
63,using an @EmbeddedId.
63,"Perhaps the most immediately-natural way to represent this in an entity class is with multiple fields annotated @Id, for example:"
63,@Entity
63,@IdClass(BookId.class)
63,class Book {
63,Book() {}
63,@Id
63,String isbn;
63,@Id
63,int printing;
63,...
63,But this approach comes with a problem: what object can we use to identify a Book and pass to methods like find() which accept an identifier?
63,The solution is to write a separate class with fields that match the identifier attributes of the entity.
63,The @IdClass annotation of the Book entity identifies the id class to use for that entity:
63,class BookId {
63,String isbn;
63,int printing;
63,BookId() {}
63,"BookId(String isbn, int printing) {"
63,this.isbn = isbn;
63,this.printing = printing;
63,@Override
63,public boolean equals(Object other) {
63,if (other instanceof BookId) {
63,BookId bookId = (BookId) other;
63,return bookId.isbn.equals(isbn)
63,&& bookId.printing == printing;
63,else {
63,return false;
63,@Override
63,public int hashCode() {
63,return isbn.hashCode();
63,Every id class should override equals() and hashCode().
63,This is not our preferred approach.
63,"Instead, we recommend that the BookId class be declared as an @Embeddable type:"
63,@Embeddable
63,class BookId {
63,String isbn;
63,int printing;
63,BookId() {}
63,"BookId(String isbn, int printing) {"
63,this.isbn = isbn;
63,this.printing = printing;
63,...
63,We’ll learn more about Embeddable objects below.
63,"Now the entity class may reuse this definition using @EmbeddedId, and the @IdClass annotation is no longer required:"
63,@Entity
63,class Book {
63,Book() {}
63,@EmbeddedId
63,BookId bookId;
63,...
63,This second approach eliminates some duplicated code.
63,"Either way, we may now use BookId to obtain instances of Book:"
63,"Book book = session.find(Book.class, new BookId(isbn, printing));"
63,3.8. Version attributes
63,An entity may have an attribute which is used by Hibernate for optimistic lock checking.
63,"A version attribute is usually of type Integer, Short, Long, LocalDateTime, OffsetDateTime, ZonedDateTime, or Instant."
63,@Version
63,LocalDateTime lastUpdated;
63,"Version attributes are automatically assigned by Hibernate when an entity is made persistent, and automatically incremented or updated each time the entity is updated."
63,"If an entity doesn’t have a version number, which often happens when mapping legacy data, we can still do optimistic locking."
63,"The @OptimisticLocking annotation lets us specify that optimistic locks should be checked by validating the values of ALL fields, or only the DIRTY fields of the entity."
63,And the @OptimisticLock annotation lets us selectively exclude certain fields from optimistic locking.
63,The @Id and @Version attributes we’ve already seen are just specialized examples of basic attributes.
63,3.9. Natural id attributes
63,"Even when an entity has a surrogate key, it should always be possible to write down a combination of fields which uniquely identifies an instance of the entity, from the point of view of the user of the system."
63,This combination of fields is its natural key.
63,"Above, we considered the case where the natural key coincides with the primary key."
63,"Here, the natural key is a second unique key of the entity, distinct from its surrogate primary key."
63,"If you can’t identify a natural key, it might be a sign that you need to think more carefully about some aspect of your data model."
63,"If an entity doesn’t have a meaningful unique key, then it’s impossible to say what event or object it represents in the ""real world"" outside your program."
63,"Since it’s extremely common to retrieve an entity based on its natural key, Hibernate has a way to mark the attributes of the entity which make up its natural key."
63,Each attribute must be annotated @NaturalId.
63,@Entity
63,class Book {
63,Book() {}
63,@Id @GeneratedValue
63,Long id; // the system-generated surrogate key
63,@NaturalId
63,String isbn; // belongs to the natural key
63,@NaturalId
63,int printing; // also belongs to the natural key
63,...
63,Hibernate automatically generates a UNIQUE constraint on the columns mapped by the annotated fields.
63,Consider using the natural id attributes to implement equals() and hashCode().
63,"The payoff for doing this extra work, as we will see much later, is that we can take advantage of optimized natural id lookups that make use of the second-level cache."
63,"Note that even when you’ve identified a natural key, we still recommend the use of a generated surrogate key in foreign keys, since this makes your data model much easier to change."
63,3.10. Basic attributes
63,A basic attribute of an entity is a field or property which maps to a single column of the associated database table.
63,The JPA specification defines a quite limited set of basic types:
63,Table 12. JPA-standard basic attribute types
63,Classification
63,Package
63,Types
63,Primitive types
63,"boolean, int, double, etc"
63,Primitive wrappers
63,java.lang
63,"Boolean, Integer, Double, etc"
63,Strings
63,java.lang
63,String
63,Arbitrary-precision numeric types
63,java.math
63,"BigInteger, BigDecimal"
63,Date/time types
63,java.time
63,"LocalDate, LocalTime, LocalDateTime, OffsetDateTime, Instant"
63,Deprecated date/time types 💀
63,java.util
63,"Date, Calendar"
63,Deprecated JDBC date/time types 💀
63,java.sql
63,"Date, Time, Timestamp"
63,Binary and character arrays
63,"byte[], char[]"
63,UUIDs
63,java.util
63,UUID
63,Enumerated types
63,Any enum
63,Serializable types
63,Any type which implements java.io.Serializable
63,We’re begging you to use types from the java.time package instead of anything which inherits java.util.Date.
63,Serializing a Java object and storing its binary representation in the database is usually wrong.
63,"As we’ll soon see in Embeddable objects, Hibernate has much better ways to handle complex Java objects."
63,Hibernate slightly extends this list with the following types:
63,Table 13. Additional basic attribute types in Hibernate
63,Classification
63,Package
63,Types
63,Additional date/time types
63,java.time
63,"Duration, ZoneId, ZoneOffset, Year, and even ZonedDateTime"
63,JDBC LOB types
63,java.sql
63,"Blob, Clob, NClob"
63,Java class object
63,java.lang
63,Class
63,Miscellaneous types
63,java.util
63,"Currency, URL, TimeZone"
63,"The @Basic annotation explicitly specifies that an attribute is basic, but it’s often not needed, since attributes are assumed basic by default."
63,"On the other hand, if a non-primitively-typed attribute cannot be null, use of @Basic(optional=false) is highly recommended."
63,@Basic(optional=false) String firstName;
63,@Basic(optional=false) String lastName;
63,String middleName; // may be null
63,Note that primitively-typed attributes are inferred NOT NULL by default.
63,How to make a column not null in JPA
63,There are two standard ways to add a NOT NULL constraint to a mapped column in JPA:
63,"using @Basic(optional=false), or"
63,using @Column(nullable=false).
63,You might wonder what the difference is.
63,"Well, it’s perhaps not obvious to a casual user of the JPA annotations, but they actually come in two ""layers"":"
63,"annotations like @Entity, @Id, and @Basic belong to the logical layer, the subject of the current chapter—they specify the semantics of your Java domain model, whereas"
63,"annotations like @Table and @Column belong to the mapping layer, the topic of the next chapter—they specify how elements of the domain model map to objects in the relational database."
63,"Information may be inferred from the logical layer down to the mapping layer, but is never inferred in the opposite direction."
63,"Now, the @Column annotation, to whom we’ll be properly introduced a bit later, belongs to the mapping layer, and so its nullable member only affects schema generation (resulting in a not null constraint in the generated DDL)."
63,"On the other hand, the @Basic annotation belongs to the logical layer, and so an attribute marked optional=false is checked by Hibernate before it even writes an entity to the database."
63,Note that:
63,"optional=false implies nullable=false, but"
63,nullable=false does not imply optional=false.
63,"Therefore, we prefer @Basic(optional=false) to @Column(nullable=false)."
63,But wait!
63,An even better solution is to use the @NotNull annotation from Bean Validation.
63,"Just add Hibernate Validator to your project build, as described in Optional dependencies."
63,3.11. Enumerated types
63,We included Java enums on the list above.
63,"An enumerated type is considered a sort of basic type, but since most databases don’t have a native ENUM type, JPA provides a special @Enumerated annotation to specify how the enumerated values should be represented in the database:"
63,"by default, an enum is stored as an integer, the value of its ordinal() member, but"
63,"if the attribute is annotated @Enumerated(STRING), it will be stored as a string, the value of its name() member."
63,"//here, an ORDINAL encoding makes sense"
63,@Enumerated
63,@Basic(optional=false)
63,DayOfWeek dayOfWeek;
63,"//but usually, a STRING encoding is better"
63,@Enumerated(EnumType.STRING)
63,@Basic(optional=false)
63,Status status;
63,"In Hibernate 6, an enum annotated @Enumerated(STRING) is mapped to:"
63,"a VARCHAR column type with a CHECK constraint on most databases, or"
63,an ENUM column type on MySQL.
63,Any other enum is mapped to a TINYINT column with a CHECK constraint.
63,JPA picks the wrong default here.
63,"In most cases, storing an integer encoding of the enum value makes the relational data harder to interpret."
63,"Even considering DayOfWeek, the encoding to integers is ambiguous."
63,"If you check java.time.DayOfWeek, you’ll notice that SUNDAY is encoded as 6."
63,"But in the country I was born, SUNDAY is the first day of the week!"
63,So we prefer @Enumerated(STRING) for most enum attributes.
63,An interesting special case is PostgreSQL.
63,"Postgres supports named ENUM types, which must be declared using a DDL CREATE TYPE statement."
63,"Sadly, these ENUM types aren’t well-integrated with the language nor well-supported by the Postgres JDBC driver, so Hibernate doesn’t use them by default."
63,"But if you would like to use a named enumerated type on Postgres, just annotate your enum attribute like this:"
63,@JdbcTypeCode(SqlTypes.NAMED_ENUM)
63,@Basic(optional=false)
63,Status status;
63,The limited set of pre-defined basic attribute types can be stretched a bit further by supplying a converter.
63,3.12. Converters
63,A JPA AttributeConverter is responsible for:
63,"converting a given Java type to one of the types listed above, and/or"
63,perform any other sort of pre- and post-processing you might need to perform on a basic attribute value before writing and reading it to or from the database.
63,Converters substantially widen the set of attribute types that can be handled by JPA.
63,There are two ways to apply a converter:
63,"the @Convert annotation applies an AttributeConverter to a particular entity attribute, or"
63,"the @Converter annotation (or, alternatively, the @ConverterRegistration annotation) registers an AttributeConverter for automatic application to all attributes of a given type."
63,"For example, the following converter will be automatically applied to any attribute of type BitSet, and takes care of persisting the BitSet to a column of type varbinary:"
63,@Converter(autoApply = true)
63,"public static class EnumSetConverter implements AttributeConverter<EnumSet<DayOfWeek>,Integer> {"
63,@Override
63,public Integer convertToDatabaseColumn(EnumSet<DayOfWeek> enumSet) {
63,int encoded = 0;
63,var values = DayOfWeek.values();
63,for (int i = 0; i<values.length; i++) {
63,if (enumSet.contains(values[i])) {
63,encoded |= 1<<i;
63,return encoded;
63,@Override
63,public EnumSet<DayOfWeek> convertToEntityAttribute(Integer encoded) {
63,var set = EnumSet.noneOf(DayOfWeek.class);
63,var values = DayOfWeek.values();
63,for (int i = 0; i<values.length; i++) {
63,if (((1<<i) & encoded) != 0) {
63,set.add(values[i]);
63,return set;
63,"On the other hand, if we don’t set autoapply=true, then we must explicitly apply the converter using the @Convert annotation:"
63,@Convert(converter = BitSetConverter.class)
63,@Basic(optional = false)
63,BitSet bitset;
63,"All this is nice, but it probably won’t surprise you that Hibernate goes beyond what is required by JPA."
63,3.13. Compositional basic types
63,"Hibernate considers a ""basic type"" to be formed by the marriage of two objects:"
63,"a JavaType, which models the semantics of a certain Java class, and"
63,"a JdbcType, representing a SQL type which is understood by JDBC."
63,"When mapping a basic attribute, we may explicitly specify a JavaType, a JdbcType, or both."
63,JavaType
63,An instance of org.hibernate.type.descriptor.java.JavaType represents a particular Java class.
63,It’s able to:
63,"compare instances of the class to determine if an attribute of that class type is dirty (modified),"
63,"produce a useful hash code for an instance of the class,"
63,"coerce values to other types, and, in particular,"
63,convert an instance of the class to one of several other equivalent Java representations at the request of its partner JdbcType.
63,"For example, IntegerJavaType knows how to convert an Integer or int value to the types Long, BigInteger, and String, among others."
63,"We may explicitly specify a Java type using the @JavaType annotation, but for the built-in JavaTypes this is never necessary."
63,@JavaType(LongJavaType.class)
63,"// not needed, this is the default JavaType for long"
63,long currentTimeMillis;
63,"For a user-written JavaType, the annotation is more useful:"
63,@JavaType(BitSetJavaType.class)
63,BitSet bitSet;
63,"Alternatively, the @JavaTypeRegistration annotation may be used to register BitSetJavaType as the default JavaType for BitSet."
63,JdbcType
63,A org.hibernate.type.descriptor.jdbc.JdbcType is able to read and write a single Java type from and to JDBC.
63,"For example, VarcharJdbcType takes care of:"
63,"writing Java strings to JDBC PreparedStatements by calling setString(), and"
63,reading Java strings from JDBC ResultSets using getString().
63,"By pairing LongJavaType with VarcharJdbcType in holy matrimony, we produce a basic type which maps Longs and primitive longss to the SQL type VARCHAR."
63,We may explicitly specify a JDBC type using the @JdbcType annotation.
63,@JdbcType(VarcharJdbcType.class)
63,long currentTimeMillis;
63,"Alternatively, we may specify a JDBC type code:"
63,@JdbcTypeCode(Types.VARCHAR)
63,long currentTimeMillis;
63,The @JdbcTypeRegistration annotation may be used to register a user-written JdbcType as the default for a given SQL type code.
63,JDBC types and JDBC type codes
63,The types defined by the JDBC specification are enumerated by the integer type codes in the class java.sql.Types.
63,Each JDBC type is an abstraction of a commonly-available type in SQL.
63,"For example, Types.VARCHAR represents the SQL type VARCHAR (or VARCHAR2 on Oracle)."
63,"Since Hibernate understand more SQL types than JDBC, there’s an extended list of integer type codes in the class org.hibernate.type.SqlTypes."
63,"For example, SqlTypes.GEOMETRY represents the spatial data type GEOMETRY."
63,AttributeConverter
63,"If a given JavaType doesn’t know how to convert its instances to the type required by its partner JdbcType, we must help it out by providing a JPA AttributeConverter to perform the conversion."
63,"For example, to form a basic type using LongJavaType and TimestampJdbcType, we would provide an AttributeConverter<Long,Timestamp>."
63,@JdbcType(TimestampJdbcType.class)
63,@Convert(converter = LongToTimestampConverter.class)
63,long currentTimeMillis;
63,"Let’s abandon our analogy right here, before we start calling this basic type a ""throuple""."
63,3.14. Embeddable objects
63,"An embeddable object is a Java class whose state maps to multiple columns of a table, but which doesn’t have its own persistent identity."
63,"That is, it’s a class with mapped attributes, but no @Id attribute."
63,An embeddable object can only be made persistent by assigning it to the attribute of an entity.
63,"Since the embeddable object does not have its own persistent identity, its lifecycle with respect to persistence is completely determined by the lifecycle of the entity to which it belongs."
63,An embeddable class must be annotated @Embeddable instead of @Entity.
63,@Embeddable
63,class Name {
63,@Basic(optional=false)
63,String firstName;
63,@Basic(optional=false)
63,String lastName;
63,String middleName;
63,Name() {}
63,"Name(String firstName, String middleName, String lastName) {"
63,this.firstName = firstName;
63,this.middleName = middleName;
63,this.lastName = lastName;
63,...
63,"An embeddable class must satisfy the same requirements that entity classes satisfy, with the exception that an embeddable class has no @Id attribute."
63,"In particular, it must have a constructor with no parameters."
63,"Alternatively, an embeddable type may be defined as a Java record type:"
63,@Embeddable
63,"record Name(String firstName, String middleName, String lastName) {}"
63,"In this case, the requirement for a constructor with no parameters is relaxed."
63,"Unfortunately, as of May 2023, Java record types still cannot be used as @EmbeddedIds."
63,We may now use our Name class (or record) as the type of an entity attribute:
63,@Entity
63,class Author {
63,@Id @GeneratedValue
63,Long id;
63,Name name;
63,...
63,Embeddable types can be nested.
63,"That is, an @Embeddable class may have an attribute whose type is itself a different @Embeddable class."
63,JPA provides an @Embedded annotation to identify an attribute of an entity that refers to an embeddable type.
63,"This annotation is completely optional, and so we don’t usually use it."
63,On the other hand a reference to an embeddable type is never polymorphic.
63,"One @Embeddable class F may inherit a second @Embeddable class E, but an attribute of type E will always refer to an instance of that concrete class E, never to an instance of F."
63,"Usually, embeddable types are stored in a ""flattened"" format."
63,Their attributes map columns of the table of their parent entity.
63,"Later, in Mapping embeddable types to UDTs or to JSON, we’ll see a couple of different options."
63,"An attribute of embeddable type represents a relationship between a Java object with a persistent identity, and a Java object with no persistent identity."
63,We can think of it as a whole/part relationship.
63,"The embeddable object belongs to the entity, and can’t be shared with other entity instances."
63,And it exits for only as long as its parent entity exists.
63,Next we’ll discuss a different kind of relationship: a relationship between Java objects which each have their own distinct persistent identity and persistence lifecycle.
63,3.15. Associations
63,An association is a relationship between entities.
63,We usually classify associations based on their multiplicity.
63,"If E and F are both entity classes, then:"
63,"a one-to-one association relates at most one unique instance E with at most one unique instance of F,"
63,"a many-to-one association relates zero or more instances of E with a unique instance of F, and"
63,a many-to-many association relates zero or more instances of E with zero or more instance of F.
63,An association between entity classes may be either:
63,"unidirectional, navigable from E to F but not from F to E, or"
63,"bidirectional, and navigable in either direction."
63,"In this example data model, we can see the sorts of associations which are possible:"
63,An astute observer of the diagram above might notice that the relationship we’ve presented as a unidirectional one-to-one association could reasonably be represented in Java using subtyping.
63,This is quite normal.
63,A one-to-one association is the usual way we implement subtyping in a fully-normalized relational model.
63,It’s related to the JOINED inheritance mapping strategy.
63,"There are three annotations for mapping associations: @ManyToOne, @OneToMany, and @ManyToMany."
63,They share some common annotation members:
63,Table 14. Association-defining annotation members
63,Member
63,Interpretation
63,Default value
63,cascade
63,Persistence operations which should cascade to the associated entity; a list of CascadeTypes
63,fetch
63,Whether the association is eagerly fetched or may be proxied
63,LAZY for @OneToMany and @ManyToMany
63,EAGER for @ManyToOne 💀💀💀
63,targetEntity
63,The associated entity class
63,Determined from the attribute type declaration
63,optional
63,"For a @ManyToOne or @OneToOne association, whether the association can be null"
63,true
63,mappedBy
63,"For a bidirectional association, an attribute of the associated entity which maps the association"
63,"By default, the association is assumed unidirectional"
63,We’ll explain the effect of these members as we consider the various types of association mapping.
63,Let’s begin with the most common association multiplicity.
63,3.16. Many-to-one
63,A many-to-one association is the most basic sort of association we can imagine.
63,It maps completely naturally to a foreign key in the database.
63,Almost all the associations in your domain model are going to be of this form.
63,"Later, we’ll see how to map a many-to-one association to an association table."
63,"The @ManyToOne annotation marks the ""to one"" side of the association, so a unidirectional many-to-one association looks like this:"
63,class Book {
63,@Id @GeneratedValue
63,Long id;
63,@ManyToOne(fetch=LAZY)
63,Publisher publisher;
63,...
63,"Here, the Book table has a foreign key column holding the identifier of the associated Publisher."
63,A very unfortunate misfeature of JPA is that @ManyToOne associations are fetched eagerly by default.
63,This is almost never what we want.
63,Almost all associations should be lazy.
63,The only scenario in which fetch=EAGER makes sense is if we think there’s always a very high probability that the associated object will be found in the second-level cache.
63,"Whenever this isn’t the case, remember to explicitly specify fetch=LAZY."
63,"Most of the time, we would like to be able to easily navigate our associations in both directions."
63,"We do need a way to get the Publisher of a given Book, but we would also like to be able to obtain all the Books belonging to a given publisher."
63,"To make this association bidirectional, we need to add a collection-valued attribute to the Publisher class, and annotate it @OneToMany."
63,Hibernate needs to proxy unfetched associations at runtime.
63,"Therefore, the many-valued side must be declared using an interface type like Set or List, and never using a concrete type like HashSet or ArrayList."
63,"To indicate clearly that this is a bidirectional association, and to reuse any mapping information already specified in the Book entity, we must use the mappedBy annotation member to refer back to Book.publisher."
63,@Entity
63,class Publisher {
63,@Id @GeneratedValue
63,Long id;
63,"@OneToMany(mappedBy=""publisher"")"
63,Set<Book> books;
63,...
63,The Publisher.books field is called the unowned side of the association.
63,"Now, we passionately hate the stringly-typed mappedBy reference to the owning side of the association."
63,"Thankfully, the Metamodel Generator gives us a way to make it a"
63,bit more typesafe:
63,@OneToMany(mappedBy=Book_.PUBLISHER)
63,// get used to doing it this way!
63,Set<Book> books;
63,We’re going to use this approach for the rest of the Introduction.
63,"To modify a bidirectional association, we must change the owning side."
63,Changes made to the unowned side of an association are never synchronized to the database.
63,"If we desire to change an association in the database, we must change it from the owning side."
63,"Here, we must set Book.publisher."
63,"In fact, it’s often necessary to change both sides of a bidirectional association."
63,"For example, if the collection Publisher.books was stored in the second-level cache, we must also modify the collection, to ensure that the second-level cache remains synchronized with the database."
63,"That said, it’s not a hard requirement to update the unowned side, at least if you’re sure you know what you’re doing."
63,"In principle Hibernate does allow you to have a unidirectional one-to-many, that is, a @OneToMany with no matching @ManyToOne on the other side."
63,"In practice, this mapping is unnatural, and just doesn’t work very well."
63,Avoid it.
63,"Here we’ve used Set as the type of the collection, but Hibernate also allows the use of List or Collection here, with almost no difference in semantics."
63,"In particular, the List may not contain duplicate elements, and its order will not be persistent."
63,@OneToMany(mappedBy=Book_.PUBLISHER)
63,Collection<Book> books;
63,We’ll see how to map a collection with a persistent order much later.
63,"Set, List, or Collection?"
63,"A one-to-many association mapped to a foreign key can never contain duplicate elements, so Set seems like the most semantically correct Java collection type to use here, and so that’s the conventional practice in the Hibernate community."
63,The catch associated with using a set is that we must carefully ensure that Book has a high-quality implementation of equals() and hashCode().
63,"Now, that’s not necessarily a bad thing, since a quality equals() is independently useful."
63,But what if we used Collection or List instead?
63,Then our code would be much less sensitive to how equals() and hashCode() were implemented.
63,"In the past, we were perhaps too dogmatic in recommending the use of Set."
63,Now? I guess we’re happy to let you guys decide.
63,"In hindsight, we could have done more to make clear that this was always a viable option."
63,3.17. One-to-one (first way)
63,"The simplest sort of one-to-one association is almost exactly like a @ManyToOne association, except that it maps to a foreign key column with a UNIQUE constraint."
63,"Later, we’ll see how to map a one-to-one association to an association table."
63,A one-to-one association must be annotated @OneToOne:
63,@Entity
63,class Author {
63,@Id @GeneratedValue
63,Long id;
63,"@OneToOne(optional=false, fetch=LAZY)"
63,Person author;
63,...
63,"Here, the Author table has a foreign key column holding the identifier of the associated Publisher."
63,"A one-to-one association often models a ""type of"" relationship."
63,"In our example, an Author is a type of Person."
63,"An alternative—and often more natural—way to represent ""type of"" relationships in Java is via entity class inheritance."
63,We can make this association bidirectional by adding a reference back to the Author in the Person entity:
63,@Entity
63,class Person {
63,@Id @GeneratedValue
63,Long id;
63,@OneToOne(mappedBy = Author_.PERSON)
63,Author author;
63,...
63,"Person.author is the unowned side, because it’s the side marked mappedBy."
63,Lazy fetching for one-to-one associations
63,Notice that we did not declare the unowned end of the association fetch=LAZY.
63,That’s because:
63,"not every Person has an associated Author, and"
63,"the foreign key is held in the table mapped by Author, not in the table mapped by Person."
63,"Therefore, Hibernate can’t tell if the reference from Person to Author is null without fetching the associated Author."
63,"On the other hand, if every Person was an Author, that is, if the association were non-optional, we would not have to consider the possibility of null references, and we would map it like this:"
63,"@OneToOne(optional=false, mappedBy = Author_.PERSON, fetch=LAZY)"
63,Author author;
63,This is not the only sort of one-to-one association.
63,3.18. One-to-one (second way)
63,An arguably more elegant way to represent such a relationship is to share a primary key between the two tables.
63,"To use this approach, the Author class must be annotated like this:"
63,@Entity
63,class Author {
63,@Id
63,Long id;
63,"@OneToOne(optional=false, fetch=LAZY)"
63,@MapsId
63,Person author;
63,...
63,"Notice that, compared with the previous mapping:"
63,"the @Id attribute is no longer a @GeneratedValue and,"
63,"instead, the author association is annotated @MapsId."
63,This lets Hibernate know that the association to Person is the source of primary key values for Author.
63,"Here, there’s no extra foreign key column in the Author table, since the id column holds the identifier of Person."
63,"That is, the primary key of the Author table does double duty as the foreign key referring to the Person table."
63,The Person class doesn’t change.
63,"If the association is bidirectional, we annotate the unowned side @OneToOne(mappedBy = Author_.PERSON) just as before."
63,3.19. Many-to-many
63,A unidirectional many-to-many association is represented as a collection-valued attribute.
63,It always maps to a separate association table in the database.
63,It tends to happen that a many-to-many association eventually turns out to be an entity in disguise.
63,Suppose we start with a nice clean many-to-many association between Author and Book.
63,"Later on, it’s quite likely that we’ll discover some additional information which comes attached to the association, so that the association table needs some extra columns."
63,"For example, imagine that we needed to report the percentage contribution of each author to a book."
63,That information naturally belongs to the association table.
63,"We can’t easily store it as an attribute of Book, nor as an attribute of Author."
63,"When this happens, we need to change our Java model, usually introducing a new entity class which maps the association table directly."
63,"In our example, we might call this entity something like BookAuthorship, and it would have @OneToMany associations to both Author and Book, along with the contribution attribute."
63,"We can evade the disruption occasioned by such ""discoveries"" by simply avoiding the use of @ManyToMany right from the start."
63,There’s little downside to representing every—or at least almost every—logical many-to-many association using an intermediate entity.
63,A many-to-many association must be annotated @ManyToMany:
63,@Entity
63,class Book {
63,@Id @GeneratedValue
63,Long id;
63,@ManyToMany
63,Set<Author> authors;
63,...
63,"If the association is bidirectional, we add a very similar-looking attribute to Book, but this time we must specify mappedBy to indicate that this is the unowned side of the association:"
63,@Entity
63,class Book {
63,@Id @GeneratedValue
63,Long id;
63,@ManyToMany(mappedBy=Author_.BOOKS)
63,Set<Author> authors;
63,...
63,"Remember, if we wish to the modify the collection we must change the owning side."
63,We’ve again used Sets to represent the association.
63,"As before, we have the option to use Collection or List."
63,But in this case it does make a difference to the semantics of the association.
63,A many-to-many association represented as a Collection or List may contain duplicate elements.
63,"However, as before, the order of the elements is not persistent."
63,"That is, the collection is a bag, not a set."
63,3.20. Collections of basic values and embeddable objects
63,We’ve now seen the following kinds of entity attribute:
63,Kind of entity attribute
63,Kind of reference
63,Multiplicity
63,Examples
63,Single-valued attribute of basic type
63,Non-entity
63,At most one
63,@Basic String name
63,Single-valued attribute of embeddable type
63,Non-entity
63,At most one
63,@Embedded Name name
63,Single-valued association
63,Entity
63,At most one
63,@ManyToOne Publisher publisher
63,@OneToOne Person person
63,Many-valued association
63,Entity
63,Zero or more
63,@OneToMany Set<Book> books
63,@ManyToMany Set<Author> authors
63,"Scanning this taxonomy, you might ask: does Hibernate have multivalued attributes of basic or embeddable type?"
63,"Well, actually, we’ve already seen that it does, at least in two special cases."
63,"So first, lets recall that JPA treats byte[] and char[] arrays as basic types."
63,"Hibernate persists a byte[] or char[] array to a VARBINARY or VARCHAR column, respectively."
63,But in this section we’re really concerned with cases other than these two special cases.
63,"So then, apart from byte[] and char[], does Hibernate have multivalued attributes of basic or embeddable type?"
63,"And the answer again is that it does. Indeed, there are two different ways to handle such a collection, by mapping it:"
63,"to a column of SQL ARRAY type (assuming the database has an ARRAY type), or"
63,to a separate table.
63,So we may expand our taxonomy with:
63,Kind of entity attribute
63,Kind of reference
63,Multiplicity
63,Examples
63,byte[] and char[] arrays
63,Non-entity
63,Zero or more
63,byte[] image
63,char[] text
63,Collection of basic-typed elements
63,Non-entity
63,Zero or more
63,@Array String[] names
63,@ElementCollection Set<String> names
63,Collection of embeddable elements
63,Non-entity
63,Zero or more
63,@ElementCollection Set<Name> names
63,"There’s actually two new kinds of mapping here: @Array mappings, and @ElementCollection mappings."
63,These sorts of mappings are overused.
63,There are situations where we think it’s appropriate to use a collection of basic-typed values in our entity class.
63,But such situations are rare.
63,Almost every many-valued relationship should map to a foreign key association between separate tables.
63,And almost every table should be mapped by an entity class.
63,The features we’re about to meet in the next two subsections are used much more often by beginners than they’re used by experts.
63,"So if you’re a beginner, you’ll save yourself same hassle by staying away from these features for now."
63,We’ll talk about @Array mappings first.
63,3.21. Collections mapped to SQL arrays
63,Let’s consider a calendar event which repeats on certain days of the week.
63,We might represent this in our Event entity as an attribute of type DayOfWeek[] or List<DayOfWeek>.
63,"Since the number of elements of this array or list is upper bounded by 7, this is a reasonable case for the use of an ARRAY-typed column."
63,It’s hard to see much value in storing this collection in a separate table.
63,Learning to not hate SQL arrays
63,"For a long time, we thought arrays were a kind of weird and warty thing to add to the relational model, but recently we’ve come to realize that this view was overly closed-minded."
63,"Indeed, we might choose to view SQL ARRAY types as a generalization of VARCHAR and VARBINARY to generic ""element"" types."
63,"And from this point of view, SQL arrays look quite attractive, at least for certain problems."
63,"If we’re comfortable mapping byte[] to VARBINARY(255), why would we shy away from mapping DayOfWeek[] to TINYINT ARRAY[7]?"
63,"Unfortunately, JPA doesn’t define a standard way to map SQL arrays, but here’s how we can do it in Hibernate:"
63,@Entity
63,class Event {
63,@Id @GeneratedValue
63,Long id;
63,...
63,@Array(length=7)
63,DayOfWeek[] daysOfWeek;
63,// stored as a SQL ARRAY type
63,...
63,"The @Array annotation is optional, but it’s important to limit the amount of storage space the database allocates to the ARRAY column."
63,"Now for the gotcha: not every database has a SQL ARRAY type, and some that do have an ARRAY type don’t allow it to be used as a column type."
63,"In particular, neither DB2 nor SQL Server have array-typed columns."
63,"On these databases, Hibernate falls back to something much worse: it uses Java serialization to encode the array to a binary representation, and stores the binary stream in a VARBINARY column."
63,"Quite clearly, this is terrible."
63,"You can ask Hibernate to do something slightly less terrible by annotating the attribute @JdbcTypeCode(SqlTypes.JSON), so that the array is serialized to JSON instead of binary format."
63,But at this point it’s better to just admit defeat and use an @ElementCollection instead.
63,"Alternatively, we could store this array or list in a separate table."
63,3.22. Collections mapped to a separate table
63,JPA does define a standard way to map a collection to an auxiliary table: the @ElementCollection annotation.
63,@Entity
63,class Event {
63,@Id @GeneratedValue
63,Long id;
63,...
63,@ElementCollection
63,DayOfWeek[] daysOfWeek;
63,// stored in a dedicated table
63,...
63,"Actually, we shouldn’t use an array here, since array types can’t be proxied, and so the JPA specification doesn’t even say they’re supported."
63,"Instead, we should use Set, List, or Map."
63,@Entity
63,class Event {
63,@Id @GeneratedValue
63,Long id;
63,...
63,@ElementCollection
63,List<DayOfWeek> daysOfWeek;
63,// stored in a dedicated table
63,...
63,"Here, each collection element is stored as a separate row of the auxiliary table."
63,"By default, this table has the following definition:"
63,create table Event_daysOfWeek (
63,"Event_id bigint not null,"
63,"daysOfWeek tinyint check (daysOfWeek between 0 and 6),"
63,"daysOfWeek_ORDER integer not null,"
63,"primary key (Event_id, daysOfWeek_ORDER)"
63,"Which is fine, but it’s still a mapping we prefer to avoid."
63,@ElementCollection is one of our least-favorite features of JPA.
63,Even the name of the annotation is bad.
63,The code above results in a table with three columns:
63,"a foreign key of the Event table,"
63,"a TINYINT encoding the enum, and"
63,an INTEGER encoding the ordering of elements in the array.
63,"Instead of a surrogate primary key, it has a composite key comprising the foreign key of Event and the order column."
63,"When—inevitably—we find that we need to add a fourth column to that table, our Java code must change completely."
63,"Most likely, we’ll realize that we need to add a separate entity after all."
63,So this mapping isn’t very robust in the face of minor changes to our data model.
63,"There’s much more we could say about ""element collections"", but we won’t say it, because we don’t want to hand you the gun you’ll shoot your foot with."
63,3.23. Summary of annotations
63,Let’s pause to remember the annotations we’ve met so far.
63,Table 15. Declaring entities and embeddable types
63,Annotation
63,Purpose
63,JPA-standard
63,@Entity
63,Declare an entity class
63,@MappedSuperclass
63,Declare a non-entity class with mapped attributes inherited by an entity
63,@Embeddable
63,Declare an embeddable type
63,@IdClass
63,Declare the identifier class for an entity with multiple @Id attributes
63,Table 16. Declaring basic and embedded attributes
63,Annotation
63,Purpose
63,JPA-standard
63,@Id
63,Declare a basic-typed identifier attribute
63,@Version
63,Declare a version attribute
63,@Basic
63,Declare a basic attribute
63,Default
63,@EmbeddedId
63,Declare an embeddable-typed identifier attribute
63,@Embedded
63,Declare an embeddable-typed attribute
63,Inferred
63,@Enumerated
63,Declare an enum-typed attribute and specify how it is encoded
63,Inferred
63,@Array
63,"Declare that an attribute maps to a SQL ARRAY, and specify the length"
63,Inferred
63,@ElementCollection
63,Declare that a collection is mapped to a dedicated table
63,Table 17. Converters and compositional basic types
63,Annotation
63,Purpose
63,JPA-standard
63,@Converter
63,Register an AttributeConverter
63,@Convert
63,Apply a converter to an attribute
63,@JavaType
63,Explicitly specify an implementation of JavaType for a basic attribute
63,@JdbcType
63,Explicitly specify an implementation of JdbcType for a basic attribute
63,@JdbcTypeCode
63,Explicitly specify a JDBC type code used to determine the JdbcType for a basic attribute
63,@JavaTypeRegistration
63,Register a JavaType for a given Java type
63,@JdbcTypeRegistration
63,Register a JdbcType for a given JDBC type code
63,Table 18. System-generated identifiers
63,Annotation
63,Purpose
63,JPA-standard
63,@GeneratedValue
63,Specify that an identifier is system-generated
63,@SequenceGenerator
63,Define an id generated backed by on a database sequence
63,@TableGenerator
63,Define an id generated backed by a database table
63,@IdGeneratorType
63,Declare an annotation that associates a custom Generator with each @Id attribute it annotates
63,@ValueGenerationType
63,Declare an annotation that associates a custom Generator with each @Basic attribute it annotates
63,Table 19. Declaring entity associations
63,Annotation
63,Purpose
63,JPA-standard
63,@ManyToOne
63,Declare the single-valued side of a many-to-one association (the owning side)
63,@OneToMany
63,Declare the many-valued side of a many-to-one association (the unowned side)
63,@ManyToMany
63,Declare either side of a one-to-one association
63,@OneToOne
63,Declare either side of a one-to-one association
63,@MapsId
63,Declare that the owning side of a @OneToOne association maps the primary key column
63,Phew!
63,"That’s already a lot of annotations, and we have not even started with the annotations for O/R mapping!"
63,3.24. equals() and hashCode()
63,"Entity classes should override equals() and hashCode(), especially when associations are represented as sets."
63,People new to Hibernate or JPA are often confused by exactly which fields should be included in the hashCode().
63,And people with more experience often argue quite religiously that one or another approach is the only right way.
63,"The truth is, there’s no unique right way to do it, but there are some constraints."
63,So please keep the following principles in mind:
63,"You should not include a mutable field in the hashcode, since that would require rehashing every collection containing the entity whenever the field is mutated."
63,"It’s not completely wrong to include a generated identifier (surrogate key) in the hashcode, but since the identifier is not generated until the entity instance is made persistent, you must take great care to not add it to any hashed collection before the identifier is generated. We therefore advise against including any database-generated field in the hashcode."
63,"It’s OK to include any immutable, non-generated field in the hashcode."
63,"We therefore recommend identifying a natural key for each entity, that is, a combination of fields that uniquely identifies an instance of the entity, from the perspective of the data model of the program. The natural key should correspond to a unique constraint on the database, and to the fields which are included in equals() and hashCode()."
63,"In this example, the equals() and hashCode() methods agree with the @NaturalId annotation:"
63,@Entity
63,class Book {
63,@Id @GeneratedValue
63,Long id;
63,@NaturalId
63,@Basic(optional=false)
63,String isbn;
63,...
63,@Override
63,public boolean equals(Object other) {
63,return other instanceof Book
63,&& ((Book) other).isbn.equals(isbn);
63,@Override
63,public int hashCode() {
63,return isbn.hashCode();
63,"That said, an implementation of equals() and hashCode() based on the generated identifier of the entity can work if you’re careful."
63,4. Object/relational mapping
63,"Given a domain model—that is, a collection of entity classes decorated with all the fancy annotations we just met in the previous chapter—Hibernate will happily go away and infer a complete relational schema, and even export it to your database if you ask politely."
63,"The resulting schema will be entirely sane and reasonable, though if you look closely, you’ll find some flaws."
63,"For example, every VARCHAR column will have the same length, VARCHAR(255)."
63,But the process I just described—which we call top down mapping—simply doesn’t fit the most common scenario for the use of O/R mapping.
63,It’s only rarely that the Java classes precede the relational schema.
63,"Usually, we already have a relational schema, and we’re constructing our domain model around the schema."
63,This is called bottom up mapping.
63,"Developers often refer to a pre-existing relational database as ""legacy"" data."
63,"This tends to conjure images of bad old ""legacy apps"" written in COBOL or something."
63,"But legacy data is valuable, and learning to work with it is important."
63,"Especially when mapping bottom up, we often need to customize the inferred object/relational mappings."
63,"This is a somewhat tedious topic, and so we don’t want to spend too many words on it."
63,"Instead, we’ll quickly skim the most important mapping annotations."
63,Hibernate SQL case convention
63,Computers have had lowercase letters for rather a long time now.
63,"Most developers learned long ago that text written in MixedCase, camelCase, or even snake_case is easier to read than text written in SHOUTYCASE."
63,This is just as true of SQL as it is of any other language.
63,"Therefore, for over twenty years, the convention on the Hibernate project has been that:"
63,"query language identifiers are written in lowercase,"
63,"table names are written in MixedCase, and"
63,column names are written in camelCase.
63,"That is to say, we simply adopted Java’s excellent conventions and applied them to SQL."
63,"Now, there’s no way we can force you to follow this convention, even if we wished to."
63,"Hell, you can easily write a PhysicalNamingStrategy which makes table and column names ALL UGLY AND SHOUTY LIKE THIS IF YOU PREFER."
63,"But, by default, it’s the convention Hibernate follows, and it’s frankly a pretty reasonable one."
63,4.1. Mapping entity inheritance hierarchies
63,In Entity class inheritance we saw that entity classes may exist within an inheritance hierarchy.
63,There’s three basic strategies for mapping an entity hierarchy to relational tables.
63,"Let’s put them in a table, so we can more easily compare the points of difference between them."
63,Table 20. Entity inheritance mapping strategies
63,Strategy
63,Mapping
63,Polymorphic queries
63,Constraints
63,Normalization
63,When to use it
63,SINGLE_TABLE
63,"Map every class in the hierarchy to the same table, and uses the value of a discriminator column to determine which concrete class each row represents."
63,"To retrieve instances of a given class, we only need to query the one table."
63,Attributes declared by subclasses map to columns without NOT NULL constraints. 💀
63,Any association may have a FOREIGN KEY constraint. 🤓
63,Subclass data is denormalized. 🧐
63,Works well when subclasses declare few or no additional attributes.
63,JOINED
63,"Map every class in the hierarchy to a separate table, but each table only maps the attributes declared by the class itself."
63,"Optionally, a discriminator column may be used."
63,"To retrieve instances of a given class, we must JOIN the table mapped by the class with:"
63,all tables mapped by its superclasses and
63,all tables mapped by its subclasses.
63,Any attribute may map to a column with a NOT NULL constraint. 🤓
63,Any association may have a FOREIGN KEY constraint. 🤓
63,The tables are normalized. 🤓
63,The best option when we care a lot about constraints and normalization.
63,TABLE_PER_CLASS
63,"Map every concrete class in the hierarchy to a separate table, but denormalize all inherited attributes into the table."
63,"To retrieve instances of a given class, we must take a UNION over the table mapped by the class and the tables mapped by its subclasses."
63,Associations targeting a superclass cannot have a corresponding FOREIGN KEY constraint in the database. 💀💀
63,Any attribute may map to a column with a NOT NULL constraint. 🤓
63,Superclass data is denormalized. 🧐
63,Not very popular.
63,"From a certain point of view, competes with @MappedSuperclass."
63,The three mapping strategies are enumerated by InheritanceType.
63,We specify an inheritance mapping strategy using the @Inheritance annotation.
63,"For mappings with a discriminator column, we should:"
63,"specify the discriminator column name and type by annotating the root entity @DiscriminatorColumn, and"
63,specify the values of this discriminator by annotating each entity in the hierarchy @DiscriminatorValue.
63,For single table inheritance we always need a discriminator:
63,@Entity
63,"@DiscriminatorColumn(discriminatorType=CHAR, name=""kind"")"
63,@DiscriminatorValue('P')
63,class Person { ... }
63,@Entity
63,@DiscriminatorValue('A')
63,class Author { ... }
63,"We don’t need to explicitly specify @Inheritance(strategy=SINGLE_TABLE), since that’s the default."
63,For JOINED inheritance we don’t need a discriminator:
63,@Entity
63,@Inheritance(strategy=JOINED)
63,class Person { ... }
63,@Entity
63,class Author { ... }
63,"However, we can add a discriminator column if we like, and in that case the generated SQL for polymorphic queries will be slightly simpler."
63,"Similarly, for TABLE_PER_CLASS inheritance we have:"
63,@Entity
63,@Inheritance(strategy=TABLE_PER_CLASS)
63,class Person { ... }
63,@Entity
63,class Author { ... }
63,"Hibernate doesn’t allow discriminator columns for TABLE_PER_CLASS inheritance mappings, since they would make no sense, and offer no advantage."
63,"Notice that in this last case, a polymorphic association like:"
63,@ManyToOne Person person;
63,"is a bad idea, since it’s impossible to create a foreign key constraint that targets both mapped tables."
63,4.2. Mapping to tables
63,The following annotations specify exactly how elements of the domain model map to tables of the relational model:
63,Table 21. Annotations for mapping tables
63,Annotation
63,Purpose
63,@Table
63,Map an entity class to its primary table
63,@SecondaryTable
63,Define a secondary table for an entity class
63,@JoinTable
63,Map a many-to-many or many-to-one association to its association table
63,@CollectionTable
63,Map an @ElementCollection to its table
63,"The first two annotations are used to map an entity to its primary table and, optionally, one or more secondary tables."
63,4.3. Mapping entities to tables
63,"By default, an entity maps to a single table, which may be specified using @Table:"
63,@Entity
63,"@Table(name=""People"")"
63,class Person { ... }
63,"However, the @SecondaryTable annotation allows us to spread its attributes across multiple secondary tables."
63,@Entity
63,"@Table(name=""Books"")"
63,"@SecondaryTable(name=""Editions"")"
63,class Book { ... }
63,The @Table annotation can do more than just specify a name:
63,Table 22. @Table annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped table
63,schema 💀
63,The schema to which the table belongs
63,catalog 💀
63,The catalog to which the table belongs
63,uniqueConstraints
63,One or more @UniqueConstraint annotations declaring multi-column unique constraints
63,indexes
63,One or more @Index annotations each declaring an index
63,It only makes sense to explicitly specify the schema in annotations if the domain model is spread across multiple schemas.
63,"Otherwise, it’s a bad idea to hardcode the schema (or catalog) in a @Table annotation."
63,Instead:
63,"set the configuration property hibernate.default_schema (or hibernate.default_catalog), or"
63,simply specify the schema in the JDBC connection URL.
63,The @SecondaryTable annotation is even more interesting:
63,Table 23. @SecondaryTable annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped table
63,schema 💀
63,The schema to which the table belongs
63,catalog 💀
63,The catalog to which the table belongs
63,uniqueConstraints
63,One or more @UniqueConstraint annotations declaring multi-column unique constraints
63,indexes
63,One or more @Index annotations each declaring an index
63,pkJoinColumns
63,"One or more @PrimaryKeyJoinColumn annotations, specifying primary key column mappings"
63,foreignKey
63,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the @PrimaryKeyJoinColumns
63,Using @SecondaryTable on a subclass in a SINGLE_TABLE entity inheritance hierarchy gives us a sort of mix of SINGLE_TABLE with JOINED inheritance.
63,4.4. Mapping associations to tables
63,"The @JoinTable annotation specifies an association table, that is, a table holding foreign keys of both associated entities."
63,This annotation is usually used with @ManyToMany associations:
63,@Entity
63,class Book {
63,...
63,@ManyToMany
63,"@JoinTable(name=""BooksAuthors"")"
63,Set<Author> authors;
63,...
63,But it’s even possible to use it to map a @ManyToOne or @OneToOne association to an association table.
63,@Entity
63,class Book {
63,...
63,@ManyToOne(fetch=LAZY)
63,"@JoinTable(name=""BookPublisher"")"
63,Publisher publisher;
63,...
63,"Here, there should be a UNIQUE constraint on one of the columns of the association table."
63,@Entity
63,class Author {
63,...
63,"@OneToOne(optional=false, fetch=LAZY)"
63,"@JoinTable(name=""AuthorPerson"")"
63,Person author;
63,...
63,"Here, there should be a UNIQUE constraint on both columns of the association table."
63,Table 24. @JoinTable annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped association table
63,schema 💀
63,The schema to which the table belongs
63,catalog 💀
63,The catalog to which the table belongs
63,uniqueConstraints
63,One or more @UniqueConstraint annotations declaring multi-column unique constraints
63,indexes
63,One or more @Index annotations each declaring an index
63,joinColumns
63,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the owning side"
63,inverseJoinColumns
63,"One or more @JoinColumn annotations, specifying foreign key column mappings to the table of the unowned side"
63,foreignKey
63,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the joinColumnss
63,inverseForeignKey
63,An @ForeignKey annotation specifying the name of the FOREIGN KEY constraint on the inverseJoinColumnss
63,"To better understand these annotations, we must first discuss column mappings in general."
63,4.5. Mapping to columns
63,These annotations specify how elements of the domain model map to columns of tables in the relational model:
63,Table 25. Annotations for mapping columns
63,Annotation
63,Purpose
63,@Column
63,Map an attribute to a column
63,@JoinColumn
63,Map an association to a foreign key column
63,@PrimaryKeyJoinColumn
63,"Map the primary key used to join a secondary table with its primary, or a subclass table in JOINED inheritance with its root class table"
63,@OrderColumn
63,Specifies a column that should be used to maintain the order of a List.
63,@MapKeyColumn
63,Specified a column that should be used to persist the keys of a Map.
63,We use the @Column annotation to map basic attributes.
63,4.6. Mapping basic attributes to columns
63,The @Column annotation is not only useful for specifying the column name.
63,Table 26. @Column annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped column
63,table
63,The name of the table to which this column belongs
63,length
63,"The length of a VARCHAR, CHAR, or VARBINARY column type"
63,precision
63,"The decimal digits of precision of a FLOAT, DECIMAL, NUMERIC, or TIME, or TIMESTAMP column type"
63,scale
63,"The scale of a DECIMAL or NUMERIC column type, the digits of precision that occur to the right of the decimal point"
63,unique
63,Whether the column has a UNIQUE constraint
63,nullable
63,Whether the column has a NOT NULL constraint
63,insertable
63,Whether the column should appear in generated SQL INSERT statements
63,updatable
63,Whether the column should appear in generated SQL UPDATE statements
63,columnDefinition 💀
63,A DDL fragment that should be used to declare the column
63,We no longer recommend the use of columnDefinition since it results in unportable DDL.
63,Hibernate has much better ways to customize the generated DDL using techniques that result in portable behavior across different databases.
63,Here we see four different ways to use the @Column annotation:
63,@Entity
63,"@Table(name=""Books"")"
63,"@SecondaryTable(name=""Editions"")"
63,class Book {
63,@Id @GeneratedValue
63,"@Column(name=""bookId"") // customize column name"
63,Long id;
63,"@Column(length=100, nullable=false) // declare column as VARCHAR(100) NOT NULL"
63,String title;
63,"@Column(length=17, unique=true, nullable=false) // declare column as VARCHAR(17) NOT NULL UNIQUE"
63,String isbn;
63,"@Column(table=""Editions"", updatable=false) // column belongs to the secondary table, and is never updated"
63,int edition;
63,We don’t use @Column to map associations.
63,4.7. Mapping associations to foreign key columns
63,The @JoinColumn annotation is used to customize a foreign key column.
63,Table 27. @JoinColumn annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped foreign key column
63,table
63,The name of the table to which this column belongs
63,referencedColumnName
63,The name of the column to which the mapped foreign key column refers
63,unique
63,Whether the column has a UNIQUE constraint
63,nullable
63,Whether the column has a NOT NULL constraint
63,insertable
63,Whether the column should appear in generated SQL INSERT statements
63,updatable
63,Whether the column should appear in generated SQL UPDATE statements
63,columnDefinition 💀
63,A DDL fragment that should be used to declare the column
63,foreignKey
63,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
63,A foreign key column doesn’t necessarily have to refer to the primary key of the referenced table.
63,"It’s quite acceptable for the foreign key to refer to any other unique key of the referenced entity, even to a unique key of a secondary table."
63,Here we see how to use @JoinColumn to define a @ManyToOne association mapping a foreign key column which refers to the @NaturalId of Book:
63,@Entity
63,"@Table(name=""Items"")"
63,class Item {
63,...
63,@ManyToOne(optional=false)
63,// implies nullable=false
63,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"","
63,// a reference to a non-PK column
63,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn"")) // supply a name for the FK constraint"
63,Book book;
63,...
63,In case this is confusing:
63,"bookIsbn is the name of the foreign key column in the Items table,"
63,"it refers to a unique key isbn in the Books table, and"
63,it has a foreign key constraint named ItemsToBooksBySsn.
63,Note that the foreignKey member is completely optional and only affects DDL generation.
63,"If you don’t supply an explicit name using @ForeignKey, Hibernate will generate a quite ugly name."
63,"The reason for this is that the maximum length of foreign key names on some databases is extremely constrained, and we need to avoid collisions."
63,"To be fair, this is perfectly fine if you’re only using the generated DDL for testing."
63,For composite foreign keys we might have multiple @JoinColumn annotations:
63,@Entity
63,"@Table(name=""Items"")"
63,class Item {
63,...
63,@ManyToOne(optional=false)
63,"@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn"")"
63,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")"
63,Book book;
63,...
63,"If we need to specify the @ForeignKey, this starts to get a bit messy:"
63,@Entity
63,"@Table(name=""Items"")"
63,class Item {
63,...
63,@ManyToOne(optional=false)
63,"@JoinColumns(value = {@JoinColumn(name = ""bookIsbn"", referencedColumnName = ""isbn""),"
63,"@JoinColumn(name = ""bookPrinting"", referencedColumnName = ""printing"")},"
63,"foreignKey = @ForeignKey(name=""ItemsToBooksBySsn""))"
63,Book book;
63,...
63,"For associations mapped to a @JoinTable, fetching the association requires two joins, and so we must declare the @JoinColumns inside the @JoinTable annotation:"
63,@Entity
63,class Book {
63,@Id @GeneratedValue
63,Long id;
63,@ManyToMany
63,"@JoinTable(joinColumns=@JoinColumn(name=""bookId""),"
63,"inverseJoinColumns=@joinColumn(name=""authorId""),"
63,"foreignKey=@ForeignKey(name=""BooksToAuthors""))"
63,Set<Author> authors;
63,...
63,"Again, the foreignKey member is optional."
63,4.8. Mapping primary key joins between tables
63,The @PrimaryKeyJoinColumn is a special-purpose annotation for mapping:
63,"the primary key column of a @SecondaryTable—which is also a foreign key referencing the primary table, or"
63,the primary key column of the primary table mapped by a subclass in a JOINED inheritance hierarchy—which is also a foreign key referencing the primary table mapped by the root entity.
63,Table 28. @PrimaryKeyJoinColumn annotation members
63,Annotation member
63,Purpose
63,name
63,The name of the mapped foreign key column
63,referencedColumnName
63,The name of the column to which the mapped foreign key column refers
63,columnDefinition 💀
63,A DDL fragment that should be used to declare the column
63,foreignKey
63,A @ForeignKey annotation specifying the name of the FOREIGN KEY constraint
63,"When mapping a subclass table primary key, we place the @PrimaryKeyJoinColumn annotation on the entity class:"
63,@Entity
63,"@Table(name=""People"")"
63,@Inheritance(strategy=JOINED)
63,class Person { ... }
63,@Entity
63,"@Table(name=""Authors"")"
63,"@PrimaryKeyJoinColumn(name=""personId"") // the primary key of the Authors table"
63,class Author { ... }
63,"But to map a secondary table primary key, the @PrimaryKeyJoinColumn annotation must occur inside the @SecondaryTable annotation:"
63,@Entity
63,"@Table(name=""Books"")"
63,"@SecondaryTable(name=""Editions"","
63,"pkJoinColumns = @PrimaryKeyJoinColumn(name=""bookId"")) // the primary key of the Editions table"
63,class Book {
63,@Id @GeneratedValue
63,"@Column(name=""bookId"") // the name of the primary key of the Books table"
63,Long id;
63,...
63,4.9. Column lengths and adaptive column types
63,Hibernate automatically adjusts the column type used in generated DDL based on the column length specified by the @Column annotation.
63,"So we don’t usually need to explicitly specify that a column should be of type TEXT or CLOB—or worry about the parade of TINYTEXT, MEDIUMTEXT, TEXT, LONGTEXT types on MySQL—because Hibernate will automatically select one of those types if required to accommodate a string of the length we specify."
63,The constant values defined in the class Length are very helpful here:
63,Table 29. Predefined column lengths
63,Constant
63,Value
63,Description
63,DEFAULT
63,255
63,The default length of a VARCHAR or VARBINARY column when none is explicitly specified
63,LONG
63,32600
63,The largest column length for a VARCHAR or VARBINARY that is allowed on every database Hibernate supports
63,LONG16
63,32767
63,The maximum length that can be represented using 16 bits (but this length is too large for a VARCHAR or VARBINARY column on for some database)
63,LONG32
63,2147483647
63,The maximum length for a Java string
63,We can use these constants in the @Column annotation:
63,@Column(length=LONG)
63,String text;
63,@Column(length=LONG32)
63,byte[] binaryData;
63,This is usually all you need to do to make use of large object types in Hibernate.
63,4.10. LOBs
63,JPA provides a @Lob annotation which specifies that a field should be persisted as a BLOB or CLOB.
63,Semantics of the @Lob annotation
63,What the spec actually says is that the field should be persisted
63,…​as a large object to a database-supported large object type.
63,"It’s quite unclear what this means, and the spec goes on to say that"
63,…​the treatment of the Lob annotation is provider-dependent…​
63,which doesn’t help much.
63,Hibernate interprets this annotation in what we think is the most reasonable way.
63,"In Hibernate, an attribute annotated @Lob will be written to JDBC using the setClob() or setBlob() method of PreparedStatement, and will be read from JDBC using the getClob() or getBlob() method of ResultSet."
63,"Now, the use of these JDBC methods is usually unnecessary!"
63,JDBC drivers are perfectly capable of converting between String and CLOB or between byte[] and BLOB.
63,"So unless you specifically need to use these JDBC LOB APIs, you don’t need the @Lob annotation."
63,"Instead, as we just saw in Column lengths and adaptive column types, all you need is to specify a large enough column length to accommodate the data you plan to write to that column."
63,"Unfortunately, the driver for PostgreSQL doesn’t allow BYTEA or TEXT columns to be read via the JDBC LOB APIs."
63,This limitation of the Postgres driver has resulted in a whole cottage industry of bloggers and stackoverflow question-answerers recommending convoluted ways to hack the Hibernate Dialect for Postgres to allow an attribute annotated @Lob to be written using setString() and read using getString().
63,But simply removing the @Lob annotation has exactly the same effect.
63,Conclusion:
63,"on PostgreSQL, @Lob always means the OID type,"
63,"@Lob should never be used to map columns of type BYTEA or TEXT, and"
63,please don’t believe everything you read on stackoverflow.
63,"Finally, as an alternative, Hibernate lets you declare an attribute of type java.sql.Blob or java.sql.Clob."
63,@Entity
63,class Book {
63,...
63,Clob text;
63,Blob coverArt;
63,....
63,"The advantage is that a java.sql.Clob or java.sql.Blob can in principle index up to 263 characters or bytes, much more data than you can fit in a Java String or byte[] array (or in your computer)."
63,"To assign a value to these fields, we’ll need to use a LobHelper."
63,We can get one from the Session:
63,LobHelper helper = session.getLobHelper();
63,book.text = helper.createClob(text);
63,book.coverArt = helper.createBlob(image);
63,"In principle, the Blob and Clob objects provide efficient ways to read or stream LOB data from the server."
63,"Book book = session.find(Book.class, bookId);"
63,"String text = book.text.getSubString(1, textLength);"
63,InputStream bytes = book.images.getBinaryStream();
63,"Of course, the behavior here depends very much on the JDBC driver, and so we really can’t promise that this is a sensible thing to do on your database."
63,4.11. Mapping embeddable types to UDTs or to JSON
63,There’s a couple of alternative ways to represent an embeddable type on the database side.
63,Embeddables as UDTs
63,"First, a really nice option, at least in the case of Java record types, and for databases which support user-defined types (UDTs), is to define a UDT which represents the record type."
63,Hibernate 6 makes this really easy.
63,"Just annotate the record type, or the attribute which holds a reference to it, with the new @Struct annotation:"
63,@Embeddable
63,"@Struct(name=""PersonName"")"
63,"record Name(String firstName, String middleName, String lastName) {}"
63,@Entity
63,class Person {
63,...
63,Name name;
63,...
63,This results in the following UDT:
63,"create type PersonName as (firstName varchar(255), middleName varchar(255), lastName varchar(255))"
63,And the name column of the Author table will have the type PersonName.
63,Embeddables to JSON
63,A second option that’s available is to map the embeddable type to a JSON (or JSONB) column.
63,"Now, this isn’t something we would exactly recommend if you’re defining a data model from scratch, but it’s at least useful for mapping pre-existing tables with JSON-typed columns."
63,"Since embeddable types are nestable, we can map some JSON formats this way, and even query JSON properties using HQL."
63,"At this time, JSON arrays are not supported!"
63,"To map an attribute of embeddable type to JSON, we must annotate the attribute @JdbcTypeCode(SqlTypes.JSON), instead of annotating the embeddable type."
63,But the embeddable type Name should still be annotated @Embeddable if we want to query its attributes using HQL.
63,@Embeddable
63,"record Name(String firstName, String middleName, String lastName) {}"
63,@Entity
63,class Person {
63,...
63,@JdbcTypeCode(SqlTypes.JSON)
63,Name name;
63,...
63,"We also need to add Jackson or an implementation of JSONB—for example, Yasson—to our runtime classpath."
63,To use Jackson we could add this line to our Gradle build:
63,runtimeOnly 'com.fasterxml.jackson.core:jackson-databind:{jacksonVersion}'
63,"Now the name column of the Author table will have the type jsonb, and Hibernate will automatically use Jackson to serialize a Name to and from JSON format."
63,4.12. Summary of SQL column type mappings
63,"So, as we’ve seen, there are quite a few annotations that affect the mapping of Java types to SQL column types in DDL."
63,"Here we summarize the ones we’ve just seen in the second half of this chapter, along with some we already mentioned in earlier chapters."
63,Table 30. Annotations for mapping SQL column types
63,Annotation
63,Interpretation
63,@Enumerated
63,Specify how an enum type should be persisted
63,@Nationalized
63,"Use a nationalized character type: NCHAR, NVARCHAR, or NCLOB"
63,@Lob 💀
63,Use JDBC LOB APIs to read and write the annotated attribute
63,@Array
63,Map a collection to a SQL ARRAY type of the specified length
63,@Struct
63,Map an embeddable to a SQL UDT with the given name
63,@TimeZoneStorage
63,Specify how the time zone information should be persisted
63,@JdbcType or @JdbcTypeCode
63,Use an implementation of JdbcType to map an arbitrary SQL type
63,"In addition, there are some configuration properties which have a global affect on how basic types map to SQL column types:"
63,Table 31. Type mapping settings
63,Configuration property name
63,Purpose
63,hibernate.use_nationalized_character_data
63,Enable use of nationalized character types by default
63,hibernate.type.preferred_boolean_jdbc_type
63,Specify the default SQL column type for mapping boolean
63,hibernate.type.preferred_uuid_jdbc_type
63,Specify the default SQL column type for mapping UUID
63,hibernate.type.preferred_duration_jdbc_type
63,Specify the default SQL column type for mapping Duration
63,hibernate.type.preferred_instant_jdbc_type
63,Specify the default SQL column type for mapping Instant
63,hibernate.timezone.default_storage
63,Specify the default strategy for storing time zone information
63,These are global settings and thus quite clumsy.
63,We recommend against messing with any of these settings unless you have a really good reason for it.
63,There’s one more topic we would like to cover in this chapter.
63,4.13. Mapping to formulas
63,Hibernate lets us map an attribute of an entity to a SQL formula involving columns of the mapped table.
63,"Thus, the attribute is a sort of ""derived"" value."
63,Table 32. Annotations for mapping formulas
63,Annotation
63,Purpose
63,@Formula
63,Map an attribute to a SQL formula
63,@JoinFormula
63,Map an association to a SQL formula
63,@DiscriminatorFormula
63,Use a SQL formula as the discriminator in single table inheritance.
63,For example:
63,@Entity
63,class Order {
63,...
63,"@Column(name = ""sub_total"", scale=2, precision=8)"
63,BigDecimal subTotal;
63,"@Column(name = ""tax"", scale=4, precision=4)"
63,BigDecimal taxRate;
63,"@Formula(""sub_total * (1.0 + tax)"")"
63,BigDecimal totalWithTax;
63,...
63,4.14. Derived Identity
63,"An entity has a derived identity if it inherits part of its primary key from an associated ""parent"" entity."
63,We’ve already met a kind of degenerate case of derived identity when we talked about one-to-one associations with a shared primary key.
63,But a @ManyToOne association may also form part of a derived identity.
63,"That is to say, there could be a foreign key column or columns included as part of the composite primary key."
63,There’s three different ways to represent this situation on the Java side of things:
63,"using @IdClass without @MapsId,"
63,"using @IdClass with @MapsId, or"
63,using @EmbeddedId with @MapsId.
63,Let’s suppose we have a Parent entity class defined as follows:
63,@Entity
63,class Parent {
63,@Id
63,Long parentId;
63,...
63,"The parentId field holds the primary key of the Parent table, which will also form part of the composite primary key of every Child belonging to the Parent."
63,First way
63,"In the first, slightly simpler approach, we define an @IdClass to represent the primary key of Child:"
63,class DerivedId {
63,Long parent;
63,String childId;
63,"// constructors, equals, hashcode, etc"
63,...
63,And a Child entity class with a @ManyToOne association annotated @Id:
63,@Entity
63,@IdClass(DerivedId.class)
63,class Child {
63,@Id
63,String childId;
63,@Id @ManyToOne
63,"@JoinColumn(name=""parentId"")"
63,Parent parent;
63,...
63,"Then the primary key of the Child table comprises the columns (childId,parentId)."
63,Second way
63,"This is fine, but sometimes it’s nice to have a field for each element of the primary key."
63,We may use the @MapsId annotation we met earlier:
63,@Entity
63,@IdClass(DerivedId.class)
63,class Child {
63,@Id
63,Long parentId;
63,@Id
63,String childId;
63,@ManyToOne
63,@MapsId(Child_.PARENT_ID) // typesafe reference to Child.parentId
63,"@JoinColumn(name=""parentId"")"
63,Parent parent;
63,...
63,We’re using the approach we saw previously to refer to the parentId property of Child in a typesafe way.
63,"Note that we must place column mapping information on the association annotated @MapsId, not on the @Id field."
63,We must slightly modify our @IdClass so that field names align:
63,class DerivedId {
63,Long parentId;
63,String childId;
63,"// constructors, equals, hashcode, etc"
63,...
63,Third way
63,The third alternative is to redefine our @IdClass as an @Embeddable.
63,"We don’t actually need to change the DerivedId class, but we do need to add the annotation."
63,@Embeddable
63,class DerivedId {
63,Long parentId;
63,String childId;
63,"// constructors, equals, hashcode, etc"
63,...
63,Then we may use @EmbeddedId in Child:
63,@Entity
63,class Child {
63,@EmbeddedId
63,DerivedId id;
63,@ManyToOne
63,@MapsId(DerivedId_.PARENT_ID) // typesafe reference to DerivedId.parentId
63,"@JoinColumn(name=""parentId"")"
63,Parent parent;
63,...
63,The choice between @IdClass and @EmbeddedId boils down to taste.
63,The @EmbeddedId is perhaps a little DRYer.
63,4.15. Adding constraints
63,Database constraints are important.
63,"Even if you’re sure that your program has no bugs 🧐, it’s probably not the only program with access to the database."
63,Constraints help ensure that different programs (and human administrators) play nicely with each other.
63,"Hibernate adds certain constraints to generated DDL automatically: primary key constraints, foreign key constraints, and some unique constraints."
63,But it’s common to need to:
63,"add additional unique constraints,"
63,"add check constraints, or"
63,customize the name of a foreign key constraint.
63,We’ve already seen how to use @ForeignKey to specify the name of a foreign key constraint.
63,There’s two ways to add a unique constraint to a table:
63,"using @Column(unique=true) to indicate a single-column unique key, or"
63,using the @UniqueConstraint annotation to define a uniqueness constraint on a combination of columns.
63,@Entity
63,"@Table(uniqueConstraints=@UniqueConstraint(columnNames={""title"", ""year"", ""publisher_id""}))"
63,class Book { ... }
63,"This annotation looks a bit ugly perhaps, but it’s actually useful even as documentation."
63,The @Check annotation adds a check constraint to the table.
63,@Entity
63,"@Check(name=""ValidISBN"", constraints=""length(isbn)=13"")"
63,class Book { ... }
63,The @Check annotation is commonly used at the field level:
63,"@Id @Check(constraints=""length(isbn)=13"")"
63,String isbn;
63,5. Interacting with the database
63,"To interact with the database, that is, to execute queries, or to insert, update, or delete data, we need an instance of one of the following objects:"
63,"a JPA EntityManager,"
63,"a Hibernate Session, or"
63,a Hibernate StatelessSession.
63,"The Session interface extends EntityManager, and so the only difference between the two interfaces is that Session offers a few more operations."
63,"Actually, in Hibernate, every EntityManager is a Session, and you can narrow it like this:"
63,Session session = entityManager.unwrap(Session.class);
63,An instance of Session (or of EntityManager) is a stateful session.
63,It mediates the interaction between your program and the database via a operations on a persistence context.
63,"In this chapter, we’re not going to talk much about StatelessSession."
63,We’ll come back to this very useful API when we talk about performance.
63,What you need to know for now is that a stateless session doesn’t have a persistence context.
63,"Still, we should let you know that some people prefer to use StatelessSession everywhere."
63,"It’s a simpler programming model, and lets the developer interact with the database more directly."
63,"Stateful sessions certainly have their advantages, but they’re more difficult to reason about, and when something goes wrong, the error messages can be more difficult to understand."
63,5.1. Persistence Contexts
63,"A persistence context is a sort of cache; we sometimes call it the ""first-level cache"", to distinguish it from the second-level cache."
63,"For every entity instance read from the database within the scope of a persistence context, and for every new entity made persistent within the scope of the persistence context, the context holds a unique mapping from the identifier of the entity instance to the instance itself."
63,"Thus, an entity instance may be in one of three states with respect to a given persistence context:"
63,"transient — never persistent, and not associated with the persistence context,"
63,"persistent — currently associated with the persistence context, or"
63,"detached — previously persistent in another session, but not currently associated with this persistence context."
63,"At any given moment, an instance may be associated with at most one persistence context."
63,"The lifetime of a persistence context usually corresponds to the lifetime of a transaction, though it’s possible to have a persistence context that spans several database-level transactions that form a single logical unit of work."
63,"A persistence context—that is, a Session or EntityManager—absolutely positively must not be shared between multiple threads or between concurrent transactions."
63,"If you accidentally leak a session across threads, you will suffer."
63,Container-managed persistence contexts
63,"In a container environment, the lifecycle of a persistence context scoped to the transaction will usually be managed for you."
63,There are several reasons we like persistence contexts.
63,"They help avoid data aliasing: if we modify an entity in one section of code, then other code executing within the same persistence context will see our modification."
63,"They enable automatic dirty checking: after modifying an entity, we don’t need to perform any explicit operation to ask Hibernate to propagate that change back to the database."
63,"Instead, the change will be automatically synchronized with the database when the session is flushed."
63,They can improve performance by avoiding a trip to the database when a given entity instance is requested repeatedly in a given unit of work.
63,They make it possible to transparently batch together multiple database operations.
63,A persistence context also allows us to detect circularities when performing operations on graphs of entities.
63,"(Even in a stateless session, we need some sort of temporary cache of the entity instances we’ve visited while executing a query.)"
63,"On the other hand, stateful sessions come with some very important restrictions, since:"
63,"persistence contexts aren’t threadsafe, and can’t be shared across threads, and"
63,"a persistence context can’t be reused across unrelated transactions, since that would break the isolation and atomicity of the transactions."
63,"Furthermore, a persistence context holds a hard references to all its entities, preventing them from being garbage collected."
63,"Thus, the session must be discarded once a unit of work is complete."
63,"If you don’t completely understand the previous passage, go back and re-read it until you do."
63,A great deal of human suffering has resulted from users mismanaging the lifecycle of the Hibernate Session or JPA EntityManager.
63,We’ll conclude by noting that whether a persistence context helps or harms the performance of a given unit of work depends greatly on the nature of the unit of work.
63,For this reason Hibernate provides both stateful and stateless sessions.
63,5.2. Creating a session
63,"Sticking with standard JPA-defined APIs, we saw how to obtain an EntityManagerFactory in Configuration using JPA XML."
63,It’s quite unsurprising that we may use this object to create an EntityManager:
63,EntityManager entityManager = entityManagerFactory.createEntityManager();
63,"When we’re finished with the EntityManager, we should explicitly clean it up:"
63,entityManager.close();
63,"On the other hand, if we’re starting from a SessionFactory, as described in Configuration using Hibernate API, we may use:"
63,Session session = sessionFactory.openSession();
63,But we still need to clean up:
63,session.close();
63,Injecting the EntityManager
63,"If you’re writing code for some sort of container environment, you’ll probably obtain the EntityManager by some sort of dependency injection."
63,"For example, in Java (or Jakarta) EE you would write:"
63,@PersistenceContext EntityManager entityManager;
63,"In Quarkus, injection is handled by CDI:"
63,@Inject EntityManager entityManager;
63,"Outside a container environment, we’ll also have to write code to manage database transactions."
63,5.3. Managing transactions
63,"Using JPA-standard APIs, the EntityTransaction interface allows us to control database transactions."
63,The idiom we recommend is the following:
63,EntityManager entityManager = entityManagerFactory.createEntityManager();
63,EntityTransaction tx = entityManager.getTransaction();
63,try {
63,tx.begin();
63,//do some work
63,...
63,tx.commit();
63,catch (Exception e) {
63,if (tx.isActive()) tx.rollback();
63,throw e;
63,finally {
63,entityManager.close();
63,"Using Hibernate’s native APIs we might write something really similar,"
63,"but since this sort of code is extremely tedious, we have a much nicer option:"
63,sessionFactory.inTransaction(session -> {
63,//do the work
63,...
63,});
63,Container-managed transactions
63,"In a container environment, the container itself is usually responsible for managing transactions."
63,"In Java EE or Quarkus, you’ll probably indicate the boundaries of the transaction using the @Transactional annotation."
63,5.4. Operations on the persistence context
63,"Of course, the main reason we need an EntityManager is to do stuff to the database."
63,The following important operations let us interact with the persistence context and schedule modifications to the data:
63,Table 33. Methods for modifying data and managing the persistence context
63,Method name and parameters
63,Effect
63,persist(Object)
63,Make a transient object persistent and schedule a SQL insert statement for later execution
63,remove(Object)
63,Make a persistent object transient and schedule a SQL delete statement for later execution
63,merge(Object)
63,Copy the state of a given detached object to a corresponding managed persistent instance and return
63,the persistent object
63,detach(Object)
63,Disassociate a persistent object from a session without
63,affecting the database
63,clear()
63,Empty the persistence context and detach all its entities
63,flush()
63,"Detect changes made to persistent objects association with the session and synchronize the database state with the state of the session by executing SQL insert, update, and delete statements"
63,"Notice that persist() and remove() have no immediate effect on the database, and instead simply schedule a command for later execution."
63,Also notice that there’s no update() operation for a stateful session.
63,Modifications are automatically detected when the session is flushed.
63,"On the other hand, except for getReference(), the following operations all result in immediate access to the database:"
63,Table 34. Methods for reading and locking data
63,Method name and parameters
63,Effect
63,"find(Class,Object)"
63,Obtain a persistent object given its type and its id
63,"find(Class,Object,LockModeType)"
63,"Obtain a persistent object given its type and its id, requesting the given optimistic or pessimistic lock mode"
63,"getReference(Class,id)"
63,"Obtain a reference to a persistent object given its type and its id, without actually loading its state from the database"
63,getReference(Object)
63,"Obtain a reference to a persistent object with the same identity as the given detached instance, without actually loading its state from the database"
63,refresh(Object)
63,Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database
63,"refresh(Object,LockModeType)"
63,"Refresh the persistent state of an object using a new SQL select to retrieve its current state from the database, requesting the given optimistic or pessimistic lock mode"
63,"lock(Object, LockModeType)"
63,Obtain an optimistic or pessimistic lock on a persistent object
63,Any of these operations might throw an exception.
63,"Now, if an exception occurs while interacting with the database, there’s no good way to resynchronize the state of the current persistence context with the state held in database tables."
63,"Therefore, a session is considered to be unusable after any of its methods throws an exception."
63,The persistence context is fragile.
63,"If you receive an exception from Hibernate, you should immediately close and discard the current session. Open a new session if you need to, but throw the bad one away first."
63,Each of the operations we’ve seen so far affects a single entity instance passed as an argument.
63,But there’s a way to set things up so that an operation will propagate to associated entities.
63,5.5. Cascading persistence operations
63,It’s quite often the case that the lifecycle of a child entity is completely dependent on the lifecycle of some parent.
63,"This is especially common for many-to-one and one-to-one associations, though it’s very rare for many-to-many associations."
63,"For example, it’s quite common to make an Order and all its Items persistent in the same transaction, or to delete a Project and its Filess at once."
63,This sort of relationship is sometimes called a whole/part-type relationship.
63,Cascading is a convenience which allows us to propagate one of the operations listed in Operations on the persistence context from a parent to its children.
63,"To set up cascading, we specify the cascade member of one of the association mapping annotations, usually @OneToMany or @OneToOne."
63,@Entity
63,class Order {
63,...
63,"@OneToMany(mappedby=Item_.ORDER,"
63,"// cascade persist(), remove(), and refresh() from Order to Item"
63,"cascade={PERSIST,REMOVE,REFRESH},"
63,// also remove() orphaned Items
63,orphanRemoval=true)
63,private Set<Item> items;
63,...
63,Orphan removal indicates that an Item should be automatically deleted if it is removed from the set of items belonging to its parent Order.
63,5.6. Proxies and lazy fetching
63,"Our data model is a set of interconnected entities, and in Java our whole dataset would be represented as an enormous interconnected graph of objects."
63,"It’s possible that this graph is disconnected, but more likely it’s connected, or composed of a relatively small number of connected subgraphs."
63,"Therefore, when we retrieve on object belonging to this graph from the database and instantiate it in memory, we simply can’t recursively retrieve and instantiate all its associated entities."
63,"Quite aside from the waste of memory on the VM side, this process would involve a huge number of round trips to the database server, or a massive multidimensional cartesian product of tables, or both."
63,"Instead, we’re forced to cut the graph somewhere."
63,Hibernate solves this problem using proxies and lazy fetching.
63,"A proxy is an object that masquerades as a real entity or collection, but doesn’t actually hold any state, because that state has not yet been fetched from the database."
63,"When you call a method of the proxy, Hibernate will detect the call and fetch the state from the database before allowing the invocation to proceed to the real entity object or collection."
63,Now for the gotchas:
63,Hibernate will only do this for an entity which is currently associated with a persistence context.
63,"Once the session ends, and the persistence context is cleaned up, the proxy is no longer fetchable, and instead its methods throw the hated LazyInitializationException."
63,A round trip to the database to fetch the state of a single entity instance is just about the least efficient way to access data.
63,It almost inevitably leads to the infamous N+1 selects problem we’ll discuss later when we talk about how to optimize association fetching.
63,"We’re getting a bit ahead of ourselves here, but let’s quickly mention the general strategy we recommend to navigate past these gotchas:"
63,All associations should be set fetch=LAZY to avoid fetching extra data when it’s not needed.
63,"As we mentioned earlier, this setting is not the default for @ManyToOne associations, and must be specified explicitly."
63,But strive to avoid writing code which triggers lazy fetching.
63,"Instead, fetch all the data you’ll need upfront at the beginning of a unit of work, using one of the techniques described in Association fetching, usually, using join fetch in HQL or an EntityGraph."
63,It’s important to know that some operations which may be performed with an unfetched proxy don’t require fetching its state from the database.
63,"First, we’re always allowed to obtain its identifier:"
63,"var pubId = entityManager.find(Book.class, bookId).getPublisher().getId(); // does not fetch publisher"
63,"Second, we may create an association to a proxy:"
63,"book.setPublisher(entityManager.getReference(Publisher.class, pubId)); // does not fetch publisher"
63,Sometimes it’s useful to test whether a proxy or collection has been fetched from the database.
63,JPA lets us do this using the PersistenceUnitUtil:
63,boolean authorsFetched = entityManagerFactory.getPersistenceUnitUtil().isLoaded(book.getAuthors());
63,Hibernate has a slightly easier way to do it:
63,boolean authorsFetched = Hibernate.isInitialized(book.getAuthors());
63,"But the static methods of the Hibernate class let us do a lot more, and it’s worth getting a bit familiar them."
63,Of particular interest are the operations which let us work with unfetched collections without fetching their state from the database.
63,"For example, consider this code:"
63,"Book book = session.find(Book.class, bookId);"
63,"// fetch just the Book, leaving authors unfetched"
63,"Author authorRef = session.getReference(Author.class, authorId);"
63,// obtain an unfetched proxy
63,"boolean isByAuthor = Hibernate.contains(book.getAuthors(), authorRef); // no fetching"
63,This code fragment leaves both the set book.authors and the proxy authorRef unfetched.
63,"Finally, Hibernate.initialize() is a convenience method that force-fetches a proxy or collection:"
63,"Book book = session.find(Book.class, bookId);"
63,"// fetch just the Book, leaving authors unfetched"
63,Hibernate.initialize(book.getAuthors());
63,// fetch the Authors
63,"But of course, this code is very inefficient, requiring two trips to the database to obtain data that could in principle be retrieved with just one query."
63,"It’s clear from the discussion above that we need a way to request that an association be eagerly fetched using a database join, thus protecting ourselves from the infamous N+1 selects."
63,One way to do this is by passing an EntityGraph to find().
63,5.7. Entity graphs and eager fetching
63,"When an association is mapped fetch=LAZY, it won’t, by default, be fetched when we call the find() method."
63,We may request that an association be fetched eagerly (immediately) by passing an EntityGraph to find().
63,The JPA-standard API for this is a bit unwieldy:
63,var graph = entityManager.createEntityGraph(Book.class);
63,graph.addSubgraph(Book_.publisher);
63,"Book book = entityManager.find(Book.class, bookId, Map.of(SpecHints.HINT_SPEC_FETCH_GRAPH, graph));"
63,This is untypesafe and unnecessarily verbose.
63,Hibernate has a better way:
63,var graph = session.createEntityGraph(Book.class);
63,graph.addSubgraph(Book_.publisher);
63,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
63,"This code adds a left outer join to our SQL query, fetching the associated Publisher along with the Book."
63,We may even attach additional nodes to our EntityGraph:
63,var graph = session.createEntityGraph(Book.class);
63,graph.addSubgraph(Book_.publisher);
63,graph.addPluralSubgraph(Book_.authors).addSubgraph(Author_.person);
63,Book book = session.byId(Book.class).withFetchGraph(graph).load(bookId);
63,This results in a SQL query with four left outer joins.
63,"In the code examples above, The classes Book_ and Author_ are generated by the JPA Metamodel Generator we saw earlier."
63,They let us refer to attributes of our model in a completely type-safe way.
63,"We’ll use them again, below, when we talk about Criteria queries."
63,JPA specifies that any given EntityGraph may be interpreted in two different ways.
63,A fetch graph specifies exactly the associations that should be eagerly loaded.
63,Any association not belonging to the entity graph is proxied and loaded lazily only if required.
63,A load graph specifies that the associations in the entity graph are to be fetched in addition to the associations mapped fetch=EAGER.
63,"You’re right, the names make no sense."
63,"But don’t worry, if you take our advice, and map your associations fetch=LAZY, there’s no difference between a ""fetch"" graph and a ""load"" graph, so the names don’t matter."
63,JPA even specifies a way to define named entity graphs using annotations.
63,But the annotation-based API is so verbose that it’s just not worth using.
63,5.8. Flushing the session
63,"From time to time, a flush operation is triggered, and the session synchronizes dirty state held in memory—that is, modifications to the state of entities associated with the persistence context—with persistent state held in the database. Of course, it does this by executing SQL INSERT, UPDATE, and DELETE statements."
63,"By default, a flush is triggered:"
63,"when the current transaction commits, for example, when Transaction.commit() is called,"
63,"before execution of a query whose result would be affected by the synchronization of dirty state held in memory, or"
63,when the program directly calls flush().
63,"Notice that SQL statements are not usually executed synchronously by methods of the Session interface like persist() and remove(). If synchronous execution of SQL is desired, the StatelessSession allows this."
63,This behavior can be controlled by explicitly setting the flush mode.
63,"For example, to disable flushes that occur before query execution, call:"
63,entityManager.setFlushMode(FlushModeType.COMMIT);
63,Hibernate allows greater control over the flush mode than JPA:
63,session.setHibernateFlushMode(FlushMode.MANUAL);
63,"Since flushing is a somewhat expensive operation (the session must dirty-check every entity in the persistence context), setting the flush mode to COMMIT can occasionally be a useful optimization."
63,Table 35. Flush modes
63,Hibernate FlushMode
63,JPA FlushModeType
63,Interpretation
63,MANUAL
63,Never flush automatically
63,COMMIT
63,COMMIT
63,Flush before transaction commit
63,AUTO
63,AUTO
63,"Flush before transaction commit, and before execution of a query whose results might be affected by modifications held in memory"
63,ALWAYS
63,"Flush before transaction commit, and before execution of every query"
63,A second way to reduce the cost of flushing is to load entities in read-only mode:
63,"Session.setDefaultReadOnly(false) specifies that all entities loaded by a given session should be loaded in read-only mode by default,"
63,"SelectionQuery.setReadOnly(false) specifies that every entity returned by a given query should be loaded in read-only mode, and"
63,"Session.setReadOnly(Object, false) specifies that a given entity already loaded by the session should be switched to read-only mode."
63,It’s not necessary to dirty-check an entity instance in read-only mode.
63,5.9. Queries
63,Hibernate features three complementary ways to write queries:
63,"the Hibernate Query Language, an extremely powerful superset of JPQL, which abstracts most of the features of modern dialects of SQL,"
63,"the JPA criteria query API, along with extensions, allowing almost any HQL query to be constructed programmatically via a typesafe API, and, of course"
63,"for when all else fails, native SQL queries."
63,5.10. HQL queries
63,A full discussion of the query language would require almost as much text as the rest of this Introduction.
63,"Fortunately, HQL is already described in exhaustive (and exhausting) detail in A Guide to Hibernate Query Language."
63,It doesn’t make sense to repeat that information here.
63,Here we want to see how to execute a query via the Session or EntityManager API.
63,The method we call depends on what kind of query it is:
63,"selection queries return a result list, but do not modify the data, but"
63,"mutation queries modify data, and return the number of modified rows."
63,"Selection queries usually start with the keyword select or from, whereas mutation queries begin with the keyword insert, update, or delete."
63,Table 36. Executing HQL
63,Kind
63,Session method
63,EntityManager method
63,Query execution method
63,Selection
63,"createSelectionQuery(String,Class)"
63,"createQuery(String,Class)"
63,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
63,Mutation
63,createMutationQuery(String)
63,createQuery(String)
63,executeUpdate()
63,So for the Session API we would write:
63,List<Book> matchingBooks =
63,"session.createSelectionQuery(""from Book where title like :titleSearchPattern"", Book.class)"
63,".setParameter(""titleSearchPattern"", titleSearchPattern)"
63,.getResultList();
63,"Or, if we’re sticking to the JPA-standard APIs:"
63,List<Book> matchingBooks =
63,"entityManager.createQuery(""select b from Book b where b.title like :titleSearchPattern"", Book.class)"
63,".setParameter(""titleSearchPattern"", titleSearchPattern)"
63,.getResultList();
63,"The only difference between createSelectionQuery() and createQuery() is that createSelectionQuery() throws an exception if passed an insert, delete, or update."
63,"In the query above, :titleSearchPattern is called a named parameter."
63,We may also identify parameters by a number.
63,These are called ordinal parameters.
63,List<Book> matchingBooks =
63,"session.createSelectionQuery(""from Book where title like ?1"", Book.class)"
63,".setParameter(1, titleSearchPattern)"
63,.getResultList();
63,"When a query has multiple parameters, named parameters tend to be easier to read, even if slightly more verbose."
63,Never concatenate user input with HQL and pass the concatenated string to createSelectionQuery().
63,This would open up the possibility for an attacker to execute arbitrary code on your database server.
63,"If we’re expecting a query to return a single result, we can use getSingleResult()."
63,Book book =
63,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
63,".setParameter(1, isbn)"
63,.getSingleResult();
63,"Or, if we’re expecting it to return at most one result, we can use getSingleResultOrNull()."
63,Book bookOrNull =
63,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
63,".setParameter(1, isbn)"
63,.getSingleResultOrNull();
63,"The difference, of course, is that getSingleResult() throws an exception if there’s no matching row in the database, whereas getSingleResultOrNull() just returns null."
63,"By default, Hibernate dirty checks entities in the persistence context before executing a query, in order to determine if the session should be flushed."
63,"If there are many entities association with the persistence context, then this can be an expensive operation."
63,"To disable this behavior, set the flush mode to COMMIT or MANUAL:"
63,Book bookOrNull =
63,"session.createSelectionQuery(""from Book where isbn = ?1"", Book.class)"
63,".setParameter(1, isbn)"
63,.setHibernateFlushMode(MANUAL)
63,.getSingleResult();
63,Setting the flush mode to COMMIT or MANUAL might cause the query to return stale results.
63,"Occasionally we need to build a query at runtime, from a set of optional conditions."
63,"For this, JPA offers an API which allows programmatic construction of a query."
63,5.11. Criteria queries
63,"Imagine we’re implementing some sort of search screen, where the user of our system is offered several different ways to constrain the query result set."
63,"For example, we might let them search for books by title and/or the author name."
63,"Of course, we could construct a HQL query by string concatenation, but this is a bit fragile, so it’s quite nice to have an alternative."
63,HQL is implemented in terms of criteria objects
63,"Actually, in Hibernate 6, every HQL query is compiled to a criteria query before being translated to SQL."
63,This ensures that the semantics of HQL and criteria queries are identical.
63,First we need an object for building criteria queries.
63,"Using the JPA-standard APIs, this would be a CriteriaBuilder, and we get it from the EntityManagerFactory:"
63,CriteriaBuilder builder = entityManagerFactory.getCriteriaBuilder();
63,"But if we have a SessionFactory, we get something much better, a HibernateCriteriaBuilder:"
63,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
63,The HibernateCriteriaBuilder extends CriteriaBuilder and adds many operations that JPQL doesn’t have.
63,"If you’re using EntityManagerFactory, don’t despair, you have two perfectly good ways to obtain the HibernateCriteriaBuilder associated with that factory."
63,Either:
63,HibernateCriteriaBuilder builder =
63,entityManagerFactory.unwrap(SessionFactory.class).getCriteriaBuilder();
63,Or simply:
63,HibernateCriteriaBuilder builder =
63,(HibernateCriteriaBuilder) entityManagerFactory.getCriteriaBuilder();
63,We’re ready to create a criteria query.
63,CriteriaQuery<Book> query = builder.createQuery(Book.class);
63,Root<Book> book = query.from(Book.class);
63,Predicate where = builder.conjunction();
63,if (titlePattern != null) {
63,"where = builder.and(where, builder.like(book.get(Book_.title), titlePattern));"
63,if (namePattern != null) {
63,"Join<Book,Author> author = book.join(Book_.author);"
63,"where = builder.and(where, builder.like(author.get(Author_.name), namePattern));"
63,query.select(book).where(where)
63,.orderBy(builder.asc(book.get(Book_.title)));
63,"Here, as before, the classes Book_ and Author_ are generated by Hibernate’s JPA Metamodel Generator."
63,Notice that we didn’t bother treating titlePattern and namePattern as parameters.
63,"That’s safe because, by default, Hibernate automatically and transparently treats strings passed to the CriteriaBuilder as JDBC parameters."
63,Execution of a criteria query works almost exactly like execution of HQL.
63,Table 37. Executing criteria queries
63,Kind
63,Session method
63,EntityManager method
63,Query execution method
63,Selection
63,createSelectionQuery(CriteriaQuery)
63,createQuery(CriteriaQuery)
63,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
63,Mutation
63,createMutationQuery(CriteriaUpdate) or createQuery(CriteriaDelete)
63,createQuery(CriteriaUpdate) or createQuery(CriteriaDelte)
63,executeUpdate()
63,For example:
63,List<Book> matchingBooks =
63,session.createSelectionQuery(query)
63,.getResultList();
63,"Update, insert, and delete queries work similarly:"
63,CriteriaDelete<Book> delete = builder.createCriteriaDelete(Book.class);
63,Root<Book> book = delete.from(Book.class);
63,"delete.where(builder.lt(builder.year(book.get(Book_.publicationDate)), 2000));"
63,session.createMutationQuery(delete).executeUpdate();
63,"It’s even possible to transform a HQL query string to a criteria query, and modify the query programmatically before execution:"
63,HibernateCriteriaBuilder builder = sessionFactory.getCriteriaBuilder();
63,"var query = builder.createQuery(""from Book where year(publicationDate) > 2000"", Book.class);"
63,var root = (Root<Book>) query.getRootList().get(0);
63,"query.where(builder.like(root.get(Book_.title), builder.literal(""Hibernate%"")));"
63,"query.orderBy(builder.asc(root.get(Book_.title)), builder.desc(root.get(Book_.isbn)));"
63,List<Book> matchingBooks = session.createSelectionQuery(query).getResultList();
63,Do you find some of the code above a bit too verbose?
63,We do.
63,5.12. A more comfortable way to write criteria queries
63,"Actually, what makes the JPA criteria API less ergonomic than it should be is the need to call all operations of the CriteriaBuilder as instance methods, instead of having them as static functions."
63,The reason it works this way is that each JPA provider has its own implementation of CriteriaBuilder.
63,Hibernate 6.3 introduces the helper class CriteriaDefinition to reduce the verbosity of criteria queries.
63,Our example looks like this:
63,CriteriaQuery<Book> query =
63,"new CriteriaDefinition(entityManagerFactory, Book.class) {{"
63,select(book);
63,if (titlePattern != null) {
63,"restrict(like(book.get(Book_.title), titlePattern));"
63,if (namePattern != null) {
63,var author = book.join(Book_.author);
63,"restrict(like(author.get(Author_.name), namePattern));"
63,orderBy(asc(book.get(Book_.title)));
63,}};
63,"When all else fails, and sometimes even before that, we’re left with the option of writing a query in SQL."
63,5.13. Native SQL queries
63,"HQL is a powerful language which helps reduce the verbosity of SQL, and significantly increases portability of queries between databases."
63,"But ultimately, the true value of ORM is not in avoiding SQL, but in alleviating the pain involved in dealing with SQL result sets once we get them back to our Java program."
63,"As we said right up front, Hibernate’s generated SQL is meant to be used in conjunction with handwritten SQL, and native SQL queries are one of the facilities we provide to make that easy."
63,Table 38. Executing SQL
63,Kind
63,Session method
63,EntityManager method
63,Query execution method
63,Selection
63,"createNativeQuery(String,Class)"
63,"createNativeQuery(String,Class)"
63,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
63,Mutation
63,createNativeMutationQuery(String)
63,createNativeQuery(String)
63,executeUpdate()
63,Stored procedure
63,createStoredProcedureCall(String)
63,createStoredProcedureQuery(String)
63,execute()
63,"For the most simple cases, Hibernate can infer the shape of the result set:"
63,Book book =
63,"session.createNativeQuery(""select * from Books where isbn = ?1"", Book.class)"
63,.getSingleResult();
63,String title =
63,"session.createNativeQuery(""select title from Books where isbn = ?1"", String.class)"
63,.getSingleResult();
63,"However, in general, there isn’t enough information in the JDBC ResultSetMetaData to infer the mapping of columns to entity objects."
63,"So for more complicated cases, you’ll need to use the @SqlResultSetMapping annotation to define a named mapping, and pass the name to createNativeQuery(). This gets fairly messy, so we don’t want to hurt your eyes by showing you an example of it."
63,"By default, Hibernate doesn’t flush the session before execution of a native query."
63,That’s because the session is unaware of which modifications held in memory would affect the results of the query.
63,"So if there are any unflushed changes to Books, this query might return stale data:"
63,List<Book> books =
63,"session.createNativeQuery(""select * from Books"")"
63,.getResultList()
63,There’s two ways to ensure the persistence context is flushed before this query is executed.
63,"Either, we could simply force a flush by calling flush() or by setting the flush mode to ALWAYS:"
63,List<Book> books =
63,"session.createNativeQuery(""select * from Books"")"
63,.setHibernateFlushMode(ALWAYS)
63,.getResultList()
63,"Or, alternatively, we could tell Hibernate which modified state affects the results of the query:"
63,List<Book> books =
63,"session.createNativeQuery(""select * from Books"")"
63,.addSynchronizedEntityClass(Book.class)
63,.getResultList()
63,You can call stored procedures using createStoredProcedureQuery() or createStoredProcedureCall().
63,"5.14. Limits, pagination, and ordering"
63,"If a query might return more results than we can handle at one time, we may specify:"
63,"a limit on the maximum number of rows returned, and,"
63,"optionally, an offset, the first row of an ordered result set to return."
63,The offset is used to paginate query results.
63,There’s two ways to add a limit or offset to a HQL or native SQL query:
63,"using the syntax of the query language itself, for example, offset 10 rows fetch next 20 rows only, or"
63,using the methods setFirstResult() and setMaxResults() of the SelectionQuery interface.
63,"If the limit or offset is parameterized, the second option is simpler."
63,"For example, this:"
63,List<Book> books =
63,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
63,".setParameter(1, titlePattern)"
63,.setMaxResults(MAX_RESULTS)
63,.getResultList();
63,is simpler than:
63,List<Book> books =
63,"session.createSelectionQuery(""from Book where title like ?1 order by title fetch first ?2 rows only"")"
63,".setParameter(1, titlePattern)"
63,".setParameter(2, MAX_RESULTS)"
63,.getResultList();
63,Hibernate’s SelectionQuery has a slightly different way to paginate the query results:
63,List<Book> books =
63,"session.createSelectionQuery(""from Book where title like ?1 order by title"")"
63,".setParameter(1, titlePattern)"
63,.setPage(Page.first(MAX_RESULTS))
63,.getResultList();
63,A closely-related issue is ordering.
63,It’s quite common for pagination to be combined with the need to order query results by a field that’s determined at runtime.
63,"So, as an alternative to the HQL order by clause, SelectionQuery offers the ability to specify that the query results should be ordered by one or more fields of the entity type returned by the query:"
63,List<Book> books =
63,"session.createSelectionQuery(""from Book where title like ?1"")"
63,".setParameter(1, titlePattern)"
63,".setOrder(List.of(Order.asc(Book._title), Order.asc(Book._isbn)))"
63,.setMaxResults(MAX_RESULTS)
63,.getResultList();
63,"Unfortunately, there’s no way to do this using JPA’s TypedQuery interface."
63,"Table 39. Methods for query limits, pagination, and ordering"
63,Method name
63,Purpose
63,JPA-standard
63,setMaxResults()
63,Set a limit on the number of results returned by a query
63,setFirstResult()
63,Set an offset on the results returned by a query
63,setPage()
63,Set the limit and offset by specifying a Page object
63,setOrder()
63,Specify how the query results should be ordered
63,5.15. Representing projection lists
63,"A projection list is the list of things that a query returns, that is, the list of expressions in the select clause."
63,"Since Java has no tuple types, representing query projection lists in Java has always been a problem for JPA and Hibernate."
63,"Traditionally, we’ve just used Object[] most of the time:"
63,var results =
63,"session.createSelectionQuery(""select isbn, title from Book"", Object[].class)"
63,.getResultList();
63,for (var result : results) {
63,var isbn = (String) result[0];
63,var title = (String) result[1];
63,...
63,This is really a bit ugly.
63,Java’s record types now offer an interesting alternative:
63,"record IsbnTitle(String isbn, String title) {}"
63,var results =
63,"session.createSelectionQuery(""select isbn, title from Book"", IsbnTitle.class)"
63,.getResultList();
63,for (var result : results) {
63,var isbn = result.isbn();
63,var title = result.title();
63,...
63,Notice that we’re able to declare the record right before the line which executes the query.
63,"Now, this is only superficially more typesafe, since the query itself is not checked statically, and so we can’t say it’s objectively better."
63,But perhaps you find it more aesthetically pleasing.
63,"And if we’re going to be passing query results around the system, the use of a record type is much better."
63,The criteria query API offers a much more satisfying solution to the problem.
63,Consider the following code:
63,var builder = sessionFactory.getCriteriaBuilder();
63,var query = builder.createTupleQuery();
63,var book = query.from(Book.class);
63,var bookTitle = book.get(Book_.title);
63,var bookIsbn = book.get(Book_.isbn);
63,var bookPrice = book.get(Book_.price);
63,"query.select(builder.tuple(bookTitle, bookIsbn, bookPrice));"
63,var resultList = session.createSelectionQuery(query).getResultList();
63,for (var result: resultList) {
63,String title = result.get(bookTitle);
63,String isbn = result.get(bookIsbn);
63,BigDecimal price = result.get(bookPrice);
63,...
63,"This code is manifestly completely typesafe, and much better than we can hope to do with HQL."
63,5.16. Named queries
63,The @NamedQuery annotation lets us define a HQL query that is compiled and checked as part of the bootstrap process.
63,"This means we find out about errors in our queries earlier, instead of waiting until the query is actually executed."
63,"We can place the @NamedQuery annotation on any class, even on an entity class."
63,"@NamedQuery(name=""10BooksByTitle"","
63,"query=""from Book where title like :titlePattern order by title fetch first 10 rows only"")"
63,class BookQueries {}
63,"We have to make sure that the class with the @NamedQuery annotation will be scanned by Hibernate, either:"
63,"by adding <class>org.hibernate.example.BookQueries</class> to persistence.xml, or"
63,by calling configuration.addClass(BookQueries.class).
63,"Unfortunately, JPA’s @NamedQuery annotation can’t be placed on a package descriptor."
63,"Therefore, Hibernate provides a very similar annotation, @org.hibernate.annotations.NamedQuery which can be specified at the package level."
63,"If we declare a named query at the package level, we must call:"
63,"configuration.addPackage(""org.hibernate.example"")"
63,so that Hibernate knows where to find it.
63,The @NamedNativeQuery annotation lets us do the same for native SQL queries.
63,"There’s much less advantage to using @NamedNativeQuery, because there is very little that Hibernate can do to validate the correctness of a query written in the native SQL dialect of your database."
63,Table 40. Executing named queries
63,Kind
63,Session method
63,EntityManager method
63,Query execution method
63,Selection
63,"createNamedSelectionQuery(String,Class)"
63,"createNamedQuery(String,Class)"
63,"getResultList(), getSingleResult(), or getSingleResultOrNull()"
63,Mutation
63,createNamedMutationQuery(String)
63,createNamedQuery(String)
63,executeUpdate()
63,We execute our named query like this:
63,List<Book> books =
63,entityManager.createNamedQuery(BookQueries_.QUERY_10_BOOKS_BY_TITLE)
63,".setParameter(""titlePattern"", titlePattern)"
63,.getResultList()
63,"Here, BookQueries_.QUERY_10_BOOKS_BY_TITLE is a constant with value ""10BooksByTitle"", generated by the Metamodel Generator."
63,"Note that the code which executes the named query is not aware of whether the query was written in HQL or in native SQL, making it slightly easier to change and optimize the query later."
63,It’s nice to have our queries checked at startup time.
63,It’s even better to have them checked at compile time.
63,"In Organizing persistence logic, we mentioned that the Metamodel Generator can do that for us, with the help of the @CheckHQL annotation, and we presented that as a reason to use @NamedQuery."
63,"But actually, Hibernate has a separate Query Validator capable of performing compile-time validation of HQL query strings that occur as arguments to createQuery() and friends."
63,"If we use the Query Validator, there’s not much advantage to the use of named queries."
63,5.17. Controlling lookup by id
63,"We can do almost anything via HQL, criteria, or native SQL queries."
63,"But when we already know the identifier of the entity we need, a query can feel like overkill."
63,And queries don’t make efficient use of the second level cache.
63,We met the find() method earlier.
63,It’s the most basic way to perform a lookup by id.
63,"But as we also already saw, it can’t quite do everything."
63,"Therefore, Hibernate has some APIs that streamline certain more complicated lookups:"
63,Table 41. Operations for lookup by id
63,Method name
63,Purpose
63,byId()
63,"Lets us specify association fetching via an EntityGraph, as we saw; also lets us specify some additional options, including how the lookup interacts with the second level cache, and whether the entity should be loaded in read-only mode"
63,byMultipleIds()
63,Lets us load a batch of ids at the same time
63,Batch loading is very useful when we need to retrieve multiple instances of the same entity class by id:
63,var graph = session.createEntityGraph(Book.class);
63,graph.addSubgraph(Book_.publisher);
63,List<Book> books =
63,session.byMultipleIds(Book.class)
63,.withFetchGraph(graph)
63,// control association fetching
63,.withBatchSize(20)
63,// specify an explicit batch size
63,.with(CacheMode.GET)
63,// control interaction with the cache
63,.multiLoad(bookIds);
63,"The given list of bookIds will be broken into batches, and each batch will be fetched from the database in a single select."
63,"If we don’t specify the batch size explicitly, a batch size will be chosen automatically."
63,We also have some operations for working with lookups by natural id:
63,Method name
63,Purpose
63,bySimpleNaturalId()
63,For an entity with just one attribute is annotated @NaturalId
63,byNaturalId()
63,For an entity with multiple attributes are annotated @NaturalId
63,byMultipleNaturalId()
63,Lets us load a batch of natural ids at the same time
63,Here’s how we can retrieve an entity by its composite natural id:
63,Book book =
63,session.byNaturalId(Book.class)
63,".using(Book_.isbn, isbn)"
63,".using(Book_.printing, printing)"
63,.load();
63,"Notice that this code fragment is completely typesafe, again thanks to the Metamodel Generator."
63,5.18. Interacting directly with JDBC
63,From time to time we run into the need to write some code that calls JDBC directly.
63,"Unfortunately, JPA offers no good way to do this, but the Hibernate Session does."
63,session.doWork(connection -> {
63,"try (var callable = connection.prepareCall(""{call myproc(?)}"")) {"
63,"callable.setLong(1, argument);"
63,callable.execute();
63,});
63,"The Connection passed to the work is the same connection being used by the session, and so any work performed using that connection occurs in the same transaction context."
63,"If the work returns a value, use doReturningWork() instead of doWork()."
63,"In a container environment where transactions and database connections are managed by the container, this might not be the easiest way to obtain the JDBC connection."
63,5.19. What to do when things go wrong
63,"Object/relational mapping has been called the ""Vietnam of computer science""."
63,"The person who made this analogy is American, and so one supposes that he meant to imply some kind of unwinnable war."
63,"This is quite ironic, since at the very moment he made this comment, Hibernate was already on the brink of winning the war."
63,"Today, Vietnam is a peaceful country with exploding per-capita GDP, and ORM is a solved problem."
63,"That said, Hibernate is complex, and ORM still presents many pitfalls for the inexperienced, even occasionally for the experienced."
63,Sometimes things go wrong.
63,"In this section we’ll quickly sketch some general strategies for avoiding ""quagmires""."
63,Understand SQL and the relational model.
63,Know the capabilities of your RDBMS.
63,Work closely with the DBA if you’re lucky enough to have one.
63,"Hibernate is not about ""transparent persistence"" for Java objects."
63,It’s about making two excellent technologies work smoothly together.
63,Log the SQL executed by Hibernate.
63,You cannot know that your persistence logic is correct until you’ve actually inspected the SQL that’s being executed.
63,"Even when everything seems to be ""working"", there might be a lurking N+1 selects monster."
63,Be careful when modifying bidirectional associations.
63,"In principle, you should update both ends of the association."
63,"But Hibernate doesn’t strictly enforce that, since there are some situations where such a rule would be too heavy-handed."
63,"Whatever the case, it’s up to you to maintain consistency across your model."
63,Never leak a persistence context across threads or concurrent transactions.
63,Have a strategy or framework to guarantee this never happens.
63,"When running queries that return large result sets, take care to consider the size of the session cache."
63,Consider using a stateless session.
63,"Think carefully about the semantics of the second-level cache, and how the caching policies impact transaction isolation."
63,Avoid fancy bells and whistles you don’t need.
63,"Hibernate is incredibly feature-rich, and that’s a good thing, because it serves the needs of a huge number of users, many of whom have one or two very specialized needs."
63,But nobody has all those specialized needs.
63,"In all probability, you have none of them."
63,"Write your domain model in the simplest way that’s reasonable, using the simplest mapping strategies that make sense."
63,"When something isn’t behaving as you expect, simplify."
63,Isolate the problem.
63,"Find the absolute minimum test case which reproduces the behavior, before asking for help online."
63,"Most of the time, the mere act of isolating the problem will suggest an obvious solution."
63,"Avoid frameworks and libraries that ""wrap"" JPA."
63,"If there’s any one criticism of Hibernate and ORM that sometimes does ring true, it’s that it takes you too far from direct control over JDBC."
63,An additional layer just takes you even further.
63,Avoid copy/pasting code from random bloggers or stackoverflow reply guys.
63,"Many of the suggestions you’ll find online just aren’t the simplest solution, and many aren’t correct for Hibernate 6."
63,"Instead, understand what you’re doing; study the Javadoc of the APIs you’re using; read the JPA specification; follow the advice we give in this document; go direct to the Hibernate team on Zulip."
63,"(Sure, we can be a bit cantankerous at times, but we do always want you to be successful.)"
63,Always consider other options.
63,You don’t have to use Hibernate for everything.
63,6. Compile-time tooling
63,The Metamodel Generator is a standard part of JPA.
63,"We’ve actually already seen its handiwork in the code examples earlier: it’s the author of the class Book_, which contains the static metamodel of the entity class Book."
63,The Metamodel Generator
63,Hibernate’s Metamodel Generator is an annotation processor that produces what JPA calls a static metamodel.
63,"That is, it produces a typed model of the persistent classes in our program, giving us a type-safe way to refer to their attributes in Java code."
63,"In particular, it lets us specify entity graphs and criteria queries in a completely type-safe way."
63,The history behind this thing is quite interesting.
63,"Back when Java’s annotation processing API was brand spankin' new, the static metamodel for JPA was proposed by Gavin King for inclusion in JPA 2.0, as a way to achieve type safety in the nascent criteria query API."
63,"It’s fair to say that, back in 2010, this API was not a runaway success."
63,"Tools did not, at the time, feature robust support for annotation processors."
63,And all the explicit generic types made user code quite verbose and difficult to read.
63,(The need for an explicit reference to a CriteriaBuilder instance also contributed verbosity to the criteria API.)
63,"For years, Gavin counted this as one of his more embarrassing missteps."
63,But time has been kind to the static metamodel.
63,"In 2023, all Java compilers, build tools, and IDEs have robust support for annotation processing, and Java’s local type inference (the var keyword) eliminates the verbose generic types."
63,"JPA’s CriteriaBuilder and EntityGraph APIs are still not quite perfect, but the imperfections aren’t related to static type safety or annotation processing."
63,The static metamodel itself is undeniably useful and elegant.
63,"And so now, in Hibernate 6.3, we’re finally ready to go new places with the Metamodel Generator."
63,And it turns out that there’s quite a lot of unlocked potential there.
63,"Now, you still don’t have to use the Metamodel Generator with Hibernate—the APIs we just mentioned still also accept plain strings—but we find that it works well with Gradle and integrates smoothly with our IDE, and the advantage in type-safety is compelling."
63,We’ve already seen how to set up the annotation processor in the Gradle build we saw earlier.
63,"For more details on how to integrate the Metamodel Generator, check out the Static Metamodel Generator section in the User Guide."
63,"Here’s an example of the sort of code that’s generated for an entity class, as mandated by the JPA specification:"
63,Generated Code
63,@StaticMetamodel(Book.class)
63,public abstract class Book_ {
63,/**
63,* @see org.example.Book#isbn
63,**/
63,"public static volatile SingularAttribute<Book, String> isbn;"
63,/**
63,* @see org.example.Book#text
63,**/
63,"public static volatile SingularAttribute<Book, String> text;"
63,/**
63,* @see org.example.Book#title
63,**/
63,"public static volatile SingularAttribute<Book, String> title;"
63,/**
63,* @see org.example.Book#type
63,**/
63,"public static volatile SingularAttribute<Book, Type> type;"
63,/**
63,* @see org.example.Book#publicationDate
63,**/
63,"public static volatile SingularAttribute<Book, LocalDate> publicationDate;"
63,/**
63,* @see org.example.Book#publisher
63,**/
63,"public static volatile SingularAttribute<Book, Publisher> publisher;"
63,/**
63,* @see org.example.Book#authors
63,**/
63,"public static volatile SetAttribute<Book, Author> authors;"
63,"public static final String ISBN = ""isbn"";"
63,"public static final String TEXT = ""text"";"
63,"public static final String TITLE = ""title"";"
63,"public static final String TYPE = ""type"";"
63,"public static final String PUBLICATION_DATE = ""publicationDate"";"
63,"public static final String PUBLISHER = ""publisher"";"
63,"public static final String AUTHORS = ""authors"";"
63,"For each attribute of the entity, the Book_ class has:"
63,"a String-valued constant like TITLE , and"
63,a typesafe reference like title to a metamodel object of type Attribute.
63,We’ve already been using metamodel references like Book_.authors and Book.AUTHORS in the previous chapters.
63,So now let’s see what else the Metamodel Generator can do for us.
63,"The Metamodel Generator provides statically-typed access to elements of the JPA Metamodel. But the Metamodel is also accessible in a ""reflective"" way, via the EntityManagerFactory."
63,EntityType<Book> book = entityManagerFactory.getMetamodel().entity(Book.class);
63,"SingularAttribute<Book,Long> id = book.getDeclaredId(Long.class)"
63,This is very useful for writing generic code in frameworks or libraries.
63,"For example, you could use it to create your own criteria query API."
63,"Automatic generation of finder methods and query methods is a new feature of Hibernate’s implementation of the Metamodel Generator, and an extension to the functionality defined by the JPA specification."
63,"In this chapter, we’re going to explore these features."
63,The functionality described in the rest of this chapter depends on the use of the annotations described in Entities.
63,"The Metamodel Generator is not currently able to generate finder methods and query methods for entities declared completely in XML, and it’s not able to validate HQL which queries such entities."
63,"(On the other hand, the O/R mappings may be specified in XML, since they’re not needed by the Metamodel Generator.)"
63,We’re going to meet three different kinds of generated method:
63,"a named query method has its signature and implementation generated directly from a @NamedQuery annotation,"
63,"a query method has a signature that’s explicitly declared, and a generated implementation which executes a HQL or SQL query specified via a @HQL or @SQL annotation, and"
63,"a finder method annotated @Find has a signature that’s explicitly declared, and a generated implementation inferred from the parameter list."
63,"To whet our appetites, let’s see how this works for a @NamedQuery."
63,6.1. Named queries and the Metamodel Generator
63,"The very simplest way to generate a query method is to put a @NamedQuery annotation anywhere we like, with a name beginning with the magical character #."
63,Let’s just stick it on the Book class:
63,@CheckHQL // validate the query at compile time
63,"@NamedQuery(name = ""#findByTitleAndType"","
63,"query = ""select book from Book book where book.title like :titlen and book.type = :type"")"
63,@Entity
63,public class Book { ... }
63,Now the Metamodel Generator adds the following method declaration to the metamodel class Book_.
63,Generated Code
63,/**
63,* Execute named query {@value #QUERY_FIND_BY_TITLE_AND_TYPE} defined by annotation of {@link Book}.
63,**/
63,"public static List<Book> findByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
63,return entityManager.createNamedQuery(QUERY_FIND_BY_TITLE_AND_TYPE)
63,".setParameter(""titlePattern"", title)"
63,".setParameter(""type"", type)"
63,.getResultList();
63,"We can easily call this method from wherever we like, as long as we have access to an EntityManager:"
63,List<Book> books =
63,"Book_.findByTitleAndType(entityManager, titlePattern, Type.BOOK);"
63,"Now, this is quite nice, but it’s a bit inflexible in various ways, and so this probably isn’t the best way to generate a query method."
63,6.2. Generated query methods
63,The principal problem with generating the query method straight from the @NamedQuery annotation is that it doesn’t let us explicitly specify the return type or parameter list.
63,"In the case we just saw, the Metamodel Generator does a reasonable job of inferring the query return type and parameter types, but we’re often going to need a bit more control."
63,"The solution is to write down the signature of the query method explicitly, as an abstract method in Java."
63,"We’ll need a place to put this method, and since our Book entity isn’t an abstract class, we’ll just introduce a new interface for this purpose:"
63,interface Queries {
63,"@HQL(""where title like :title and type = :type"")"
63,"List<Book> findBooksByTitleAndType(String title, String type);"
63,"Instead of @NamedQuery, which is a type-level annotation, we specify the HQL query using the new @HQL annotation, which we place directly on the query method."
63,This results in the following generated code in the Queries_ class:
63,Generated Code
63,@StaticMetamodel(Queries.class)
63,public abstract class Queries_ {
63,/**
63,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
63,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
63,**/
63,"public static List<Book> findBooksByTitleAndType(@Nonnull EntityManager entityManager, String title, Type type) {"
63,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
63,".setParameter(""title"", title)"
63,".setParameter(""type"", type)"
63,.getResultList();
63,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
63,"""where title like :title and type = :type"";"
63,Notice that the signature differs just slightly from the one we wrote down in the Queries interface: the Metamodel Generator has prepended a parameter accepting EntityManager to the parameter list.
63,"If we want to explicitly specify the name and type of this parameter, we may declare it explicitly:"
63,interface Queries {
63,"@HQL(""where title like :title and type = :type"")"
63,"List<Book> findBooksByTitleAndType(StatelessSession session, String title, String type);"
63,"The Metamodel Generator defaults to using EntityManager as the session type, but other types are allowed:"
63,"Session,"
63,"StatelessSession, or"
63,Mutiny.Session from Hibernate Reactive.
63,The real value of all this is in the checks which can now be done at compile time.
63,"The Metamodel Generator verifies that the parameters of our abstract method declaration match the parameters of the HQL query, for example:"
63,"for a named parameter :alice, there must be a method parameter named alice with exactly the same type, or"
63,"for an ordinal parameter ?2, the second method parameter must have exactly the same type."
63,"The query must also be syntactically legal and semantically well-typed, that is, the entities, attributes, and functions referenced in the query must actually exist and have compatible types."
63,The Metamodel Generator determines this by inspecting the annotations of the entity classes at compile time.
63,The @CheckHQL annotation which instructs Hibernate to validate named queries is not necessary for query methods annotated @HQL.
63,The @HQL annotation has a friend named @SQL which lets us specify a query written in native SQL instead of in HQL.
63,In this case there’s a lot less the Metamodel Generator can do to check that the query is legal and well-typed.
63,We imagine you’re wondering whether a static method is really the right thing to use here.
63,6.3. Generating query methods as instance methods
63,One thing not to like about what we’ve just seen is that we can’t transparently replace a generated static function of the Queries_ class with an improved handwritten implementation without impacting clients.
63,"Now, if our query is only called in one place, which is quite common, this isn’t going to be a big issue, and so we’re inclined to think the static function is fine."
63,"But if this function is called from many places, it’s probably better to promote it to an instance method of some class or interface."
63,"Fortunately, this is straightforward."
63,All we need to do is add an abstract getter method for the session object to our Queries interface.
63,(And remove the session from the method parameter list.)
63,We may call this method anything we like:
63,interface Queries {
63,EntityManager entityManager();
63,"@HQL(""where title like :title and type = :type"")"
63,"List<Book> findBooksByTitleAndType(String title, String type);"
63,"Here we’ve used EntityManager as the session type, but other types are allowed, as we saw above."
63,Now the Metamodel Generator does something a bit different:
63,Generated Code
63,@StaticMetamodel(Queries.class)
63,public class Queries_ implements Queries {
63,private final @Nonnull EntityManager entityManager;
63,public Queries_(@Nonnull EntityManager entityManager) {
63,this.entityManager = entityManager;
63,public @Nonnull EntityManager entityManager() {
63,return entityManager;
63,/**
63,* Execute the query {@value #FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type}.
63,"* @see org.example.Queries#findBooksByTitleAndType(String,Type)"
63,**/
63,@Override
63,"public List<Book> findBooksByTitleAndType(String title, Type type) {"
63,"return entityManager.createQuery(FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type, Book.class)"
63,".setParameter(""title"", title)"
63,".setParameter(""type"", type)"
63,.getResultList();
63,static final String FIND_BOOKS_BY_TITLE_AND_TYPE_String_Type =
63,"""where title like :title and type = :type"";"
63,"The generated class Queries_ now implements the Queries interface, and the generated query method implements our abstract method directly."
63,"Of course, the protocol for calling the query method has to change:"
63,Queries queries = new Queries_(entityManager);
63,"List<Book> books = queries.findByTitleAndType(titlePattern, Type.BOOK);"
63,"If we ever need to swap out the generated query method with one we write by hand, without impacting clients, all we need to do is replace the abstract method with a default method of the Queries interface."
63,For example:
63,interface Queries {
63,EntityManager entityManager();
63,// handwritten method replacing previous generated implementation
63,"default List<Book> findBooksByTitleAndType(String title, String type) {"
63,entityManager()
63,".createQuery(""where title like :title and type = :type"", Book.class)"
63,".setParameter(""title"", title)"
63,".setParameter(""type"", type)"
63,.setFlushMode(COMMIT)
63,.setMaxResults(100)
63,.getResultList();
63,What if we would like to inject a Queries object instead of calling its constructor directly?
63,"As you recall, we don’t think these things really need to be container-managed objects."
63,"But if you want them to be—if you’re allergic to calling constructors, for some reason—then:"
63,"placing jakarta.inject on the build path will cause an @Inject annotation to be added to the constructor of Queries_, and"
63,placing jakarta.enterprise.context on the build path will cause a @Dependent annotation to be added to the Queries_ class.
63,"Thus, the generated implementation of Queries will be a perfectly functional CDI bean with no extra work to be done."
63,Is the Queries interface starting to look a lot like a DAO-style repository object?
63,"Well, perhaps."
63,You can certainly decide to use this facility to create a BookRepository if that’s what you prefer.
63,"But unlike a repository, our Queries interface:"
63,"doesn’t attempt to hide the EntityManager from its clients,"
63,"doesn’t implement or extend any framework-provided interface or abstract class, at least not unless you want to create such a framework yourself, and"
63,isn’t restricted to service a particular entity class.
63,We can have as many or as few interfaces with query methods as we like.
63,There’s no one-one-correspondence between these interfaces and entity types.
63,"This approach is so flexible that we don’t even really know what to call these ""interfaces with query methods""."
63,6.4. Generated finder methods
63,"At this point, one usually begins to question whether it’s even necessary to write a query at all."
63,Would it be possible to just infer the query from the method signature?
63,"In some simple cases it’s indeed possible, and this is the purpose of finder methods."
63,A finder method is a method annotated @Find.
63,For example:
63,@Find
63,Book getBook(String isbn);
63,A finder method may have multiple parameters:
63,@Find
63,"List<Book> getBooksByTitle(String title, Type type);"
63,The name of the finder method is arbitrary and carries no semantics.
63,But:
63,"the return type determines the entity class to be queried, and"
63,"the parameters of the method must match the fields of the entity class exactly, by both name and type."
63,"Considering our first example, Book has a persistent field String isbn, so this finder method is legal."
63,"If there were no field named isbn in Book, or if it had a different type, this method declaration would be rejected with a meaningful error at compile time."
63,"Similarly, the second example is legal, since Book has fields String title and Type type."
63,You might notice that our solution to this problem is very different from the approach taken by others.
63,"In DAO-style repository frameworks, you’re asked to encode the semantics of the finder method into the name of the method."
63,"This idea came to Java from Ruby, and we think it doesn’t belong here."
63,"It’s completely unnatural in Java, and by almost any measure other than counting characters it’s objectively worse than just writing the query in a string literal."
63,At least string literals accommodate whitespace and punctuation characters.
63,"Oh and, you know, it’s pretty useful to be able to rename a finder method without changing its semantics. 🙄"
63,The code generated for this finder method depends on what kind of fields match the method parameters:
63,@Id field
63,Uses EntityManager.find()
63,All @NaturalId fields
63,Uses Session.byNaturalId()
63,"Other persistent fields, or a mix of field types"
63,Uses a criteria query
63,"The generated code also depends on what kind of session we have, since the capabilities of stateless sessions, and of reactive sessions, differ slightly from the capabilities of regular stateful sessions."
63,"With EntityManager as the session type, we obtain:"
63,/**
63,* Find {@link Book} by {@link Book#isbn isbn}.
63,* @see org.example.Dao#getBook(String)
63,**/
63,@Override
63,public Book getBook(@Nonnull String isbn) {
63,"return entityManager.find(Book.class, isbn);"
63,/**
63,* Find {@link Book} by {@link Book#title title} and {@link Book#type type}.
63,"* @see org.example.Dao#getBooksByTitle(String,Type)"
63,**/
63,@Override
63,"public List<Book> getBooksByTitle(String title, Type type) {"
63,var builder = entityManager.getEntityManagerFactory().getCriteriaBuilder();
63,var query = builder.createQuery(Book.class);
63,var entity = query.from(Book.class);
63,query.where(
63,title==null
63,? entity.get(Book_.title).isNull()
63,": builder.equal(entity.get(Book_.title), title),"
63,type==null
63,? entity.get(Book_.type).isNull()
63,": builder.equal(entity.get(Book_.type), type)"
63,return entityManager.createQuery(query).getResultList();
63,It’s even possible to match a parameter of a finder method against a property of an associated entity or embeddable.
63,"The natural syntax would be a parameter declaration like String publisher.name, but because that’s not legal Java, we can write it as String publisher$name, taking advantage of a legal Java identifier character that nobody ever uses for anything else:"
63,@Find
63,List<Book> getBooksByPublisherName(String publisher$name);
63,"A finder method may specify fetch profiles, for example:"
63,@Find(namedFetchProfiles=Book_.FETCH_WITH_AUTHORS)
63,Book getBookWithAuthors(String isbn);
63,This lets us declare which associations of Book should be pre-fetched by annotating the Book class.
63,6.5. Paging and ordering
63,"Optionally, a query method may have additional ""magic"" parameters which do not map to query parameters:"
63,Parameter type
63,Purpose
63,Example argument
63,Page
63,Specifies a page of query results
63,Page.first(20)
63,Order<? super E>
63,"Specifies an entity attribute to order by, if E is the entity type returned by the query"
63,Order.asc(Book_.title)
63,List<Order? super E>
63,(or varargs)
63,"Specifies entity attributes to order by, if E is the entity type returned by the query"
63,"List.of(Order.asc(Book_.title), Order.asc(Book_.isbn))"
63,Order<Object[]>
63,"Specifies a column to order by, if the query returns a projection list"
63,Order.asc(1)
63,List<Object[]>
63,(or varargs)
63,"Specifies columns to order by, if the query returns a projection list"
63,"List.of(Order.asc(1), Order.desc(2))"
63,"Thus, if we redefine our earlier query method as follows:"
63,interface Queries {
63,"@HQL(""from Book where title like :title and type = :type"")"
63,"List<Book> findBooksByTitleAndType(String title, Page page, Order<? super Book>... order);"
63,Then we can call it like this:
63,List<Book> books =
63,"Queries_.findBooksByTitleAndType(entityManager, titlePattern, Type.BOOK,"
63,"Page.page(RESULTS_PER_PAGE, page), Order.asc(Book_.isbn));"
63,6.6. Query and finder method return types
63,A query method doesn’t need to return List.
63,It might return a single Book.
63,"@HQL(""where isbn = :isbn"")"
63,Book findBookByIsbn(String isbn);
63,"For a query with a projection list, Object[] or List<Object[]> is permitted:"
63,"@HQL(""select isbn, title from Book where isbn = :isbn"")"
63,Object[] findBookAttributesByIsbn(String isbn);
63,"But when there’s just one item in the select list, the type of that item should be used:"
63,"@HQL(""select title from Book where isbn = :isbn"")"
63,String getBookTitleByIsbn(String isbn);
63,"@HQL(""select local datetime"")"
63,LocalDateTime getServerDateTime();
63,"A query which returns a selection list may have a query method which repackages the result as a record, as we saw in Representing projection lists."
63,"record IsbnTitle(String isbn, String title) {}"
63,"@HQL(""select isbn, title from Book"")"
63,List<IsbnTitle> listIsbnAndTitleForEachBook(Page page);
63,A query method might even return TypedQuery or SelectionQuery:
63,"@HQL(""where title like :title"")"
63,SelectionQuery<Book> findBooksByTitle(String title);
63,"This is extremely useful at times, since it allows the client to further manipulate the query:"
63,List<Book> books =
63,"Queries_.findBooksByTitle(entityManager, titlePattern)"
63,.setOrder(Order.asc(Book_.title))
63,// order the results
63,".setPage(Page.page(RESULTS_PER_PAGE, page))"
63,// return the given page of results
63,.setFlushMode(FlushModeType.COMMIT)
63,// don't flush session before query execution
63,.setReadOnly(true)
63,// load the entities in read-only mode
63,.setCacheStoreMode(CacheStoreMode.BYPASS)
63,// don't cache the results
63,".setComment(""Hello world!"")"
63,// add a comment to the generated SQL
63,.getResultList();
63,"An insert, update, or delete query must return int or void."
63,"@HQL(""delete from Book"")"
63,int deleteAllBooks();
63,"@HQL(""update Book set discontinued = true where isbn = :isbn"")"
63,void discontinueBook(String isbn);
63,"On the other hand, finder methods are currently much more limited."
63,"A finder method must return an entity type like Book, or a list of the entity type, List<Book>, for example."
63,"As you might expect, for a reactive session, all query methods and finder methods must return Uni."
63,6.7. An alternative approach
63,"What if you just don’t like the ideas we’ve presented in this chapter, preferring to call the Session or EntityManager directly, but you still want compile-time validation for HQL?"
63,"Or what if you do like the ideas, but you’re working on a huge existing codebase full of code you don’t want to change?"
63,"Well, there’s a solution for you, too."
63,"The Query Validator is a separate annotation processor that’s capable of type-checking HQL strings, not only in annotations, but even when they occur as arguments to createQuery(), createSelectionQuery(), or createMutationQuery(). It’s even able to check calls to setParameter(), with some restrictions."
63,"The Query Validator works in javac, Gradle, Maven, and the Eclipse Java Compiler."
63,"Unlike the Metamodel Generator, which is a completely bog-standard Java annotation processor based on only standard Java APIs, the Query Validator makes use of internal compiler APIs in javac and ecj. This means it can’t be guaranteed to work in every Java compiler. The current release is known to work in JDK 11 and above, though JDK 15 or above is preferred."
63,7. Tuning and performance
63,Once you have a program up and running using Hibernate to access
63,"the database, it’s inevitable that you’ll find places where performance is"
63,disappointing or unacceptable.
63,"Fortunately, most performance problems are relatively easy to solve with"
63,"the tools that Hibernate makes available to you, as long as you keep a"
63,couple of simple principles in mind.
63,First and most important: the reason you’re using Hibernate is
63,"that it makes things easier. If, for a certain problem, it’s making"
63,"things harder, stop using it. Solve this problem with a different tool"
63,instead.
63,Just because you’re using Hibernate in your program doesn’t mean
63,you have to use it everywhere.
63,Second: there are two main potential sources of performance bottlenecks in
63,a program that uses Hibernate:
63,"too many round trips to the database, and"
63,memory consumption associated with the first-level (session) cache.
63,So performance tuning primarily involves reducing the number of accesses
63,"to the database, and/or controlling the size of the session cache."
63,"But before we get to those more advanced topics, we should start by tuning"
63,the connection pool.
63,7.1. Tuning the connection pool
63,"The connection pool built in to Hibernate is suitable for testing, but isn’t intended for use in production."
63,"Instead, Hibernate supports a range of different connection pools, including our favorite, Agroal."
63,"To select and configure Agroal, you’ll need to set some extra configuration properties, in addition to the settings we already saw in Basic configuration settings."
63,Properties with the prefix hibernate.agroal are passed through to Agroal:
63,# configure Agroal connection pool
63,hibernate.agroal.maxSize 20
63,hibernate.agroal.minSize 10
63,hibernate.agroal.acquisitionTimeout PT1s
63,hibernate.agroal.reapTimeout PT10s
63,"As long as you set at least one property with the prefix hibernate.agroal, the AgroalConnectionProvider will be selected automatically."
63,There’s many to choose from:
63,Table 42. Settings for configuring Agroal
63,Configuration property name
63,Purpose
63,hibernate.agroal.maxSize
63,The maximum number of connections present on the pool
63,hibernate.agroal.minSize
63,The minimum number of connections present on the pool
63,hibernate.agroal.initialSize
63,The number of connections added to the pool when it is started
63,hibernate.agroal.maxLifetime
63,"The maximum amount of time a connection can live, after which it is removed from the pool"
63,hibernate.agroal.acquisitionTimeout
63,"The maximum amount of time a thread can wait for a connection, after which an exception is thrown instead"
63,hibernate.agroal.reapTimeout
63,The duration for eviction of idle connections
63,hibernate.agroal.leakTimeout
63,The duration of time a connection can be held without causing a leak to be reported
63,hibernate.agroal.idleValidationTimeout
63,A foreground validation is executed if a connection has been idle on the pool for longer than this duration
63,hibernate.agroal.validationTimeout
63,The interval between background validation checks
63,hibernate.agroal.initialSql
63,A SQL command to be executed when a connection is created
63,The following settings are common to all connection pools supported by Hibernate:
63,Table 43. Common settings for connection pools
63,hibernate.connection.autocommit
63,The default autocommit mode
63,hibernate.connection.isolation
63,The default transaction isolation level
63,Container-managed datasources
63,"In a container environment, you usually don’t need to configure a connection pool through Hibernate."
63,"Instead, you’ll use a container-managed datasource, as we saw in Basic configuration settings."
63,7.2. Enabling statement batching
63,"An easy way to improve performance of some transactions, with almost no work at all, is to turn on automatic DML statement batching."
63,"Batching only helps in cases where a program executes many inserts, updates, or deletes against the same table in a single transaction."
63,All we need to do is set a single property:
63,Table 44. Enabling JDBC batching
63,Configuration property name
63,Purpose
63,Alternative
63,hibernate.jdbc.batch_size
63,Maximum batch size for SQL statement batching
63,setJdbcBatchSize()
63,"Even better than DML statement batching is the use of HQL update or delete queries, or even native SQL that calls a stored procedure!"
63,7.3. Association fetching
63,Achieving high performance in ORM means minimizing the number of round trips to the database. This goal should be uppermost in your mind whenever you’re writing data access code with Hibernate. The most fundamental rule of thumb in ORM is:
63,"explicitly specify all the data you’re going to need right at the start of a session/transaction, and fetch it immediately in one or two queries,"
63,and only then start navigating associations between persistent entities.
63,"Without question, the most common cause of poorly-performing data access code in Java programs is the problem of N+1 selects."
63,"Here, a list of N rows is retrieved from the database in an initial query, and then associated instances of a related entity are fetched using N subsequent queries."
63,This isn’t a bug or limitation of Hibernate; this problem even affects typical handwritten JDBC code behind DAOs.
63,"Only you, the developer, can solve this problem, because only you know ahead of time what data you’re going to need in a given unit of work."
63,But that’s OK.
63,Hibernate gives you all the tools you need.
63,"In this section we’re going to discuss different ways to avoid such ""chatty"" interaction with the database."
63,Hibernate provides several strategies for efficiently fetching associations and avoiding N+1 selects:
63,"outer join fetching—where an association is fetched using a left outer join,"
63,"batch fetching—where an association is fetched using a subsequent select with a batch of primary keys, and"
63,subselect fetching—where an association is fetched using a subsequent select with keys re-queried in a subselect.
63,"Of these, you should almost always use outer join fetching."
63,But let’s consider the alternatives first.
63,7.4. Batch fetching and subselect fetching
63,Consider the following code:
63,List<Book> books =
63,"session.createSelectionQuery(""from Book order by isbn"", Book.class)"
63,.getResultList();
63,"books.forEach(book -> book.getAuthors().forEach(author -> out.println(book.title + "" by "" + author.name)));"
63,"This code is very inefficient, resulting, by default, in the execution of N+1 select statements, where n is the number of Books."
63,Let’s see how we can improve on that.
63,SQL for batch fetching
63,"With batch fetching enabled, Hibernate might execute the following SQL on PostgreSQL:"
63,/* initial query for Books */
63,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
63,from Book b1_0
63,order by b1_0.isbn
63,/* first batch of associated Authors */
63,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
63,from Book_Author a1_0
63,join Author a1_1 on a1_1.id=a1_0.authors_id
63,where a1_0.books_isbn = any (?)
63,/* second batch of associated Authors */
63,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
63,from Book_Author a1_0
63,join Author a1_1 on a1_1.id=a1_0.authors_id
63,where a1_0.books_isbn = any (?)
63,The first select statement queries and retrieves Books.
63,The second and third queries fetch the associated Authors in batches.
63,The number of batches required depends on the configured batch size.
63,"Here, two batches were required, so two SQL statements were executed."
63,The SQL for batch fetching looks slightly different depending on the database.
63,"Here, on PostgreSQL, Hibernate passes a batch of primary key values as a SQL ARRAY."
63,SQL for subselect fetching
63,"On the other hand, with subselect fetching, Hibernate would execute this SQL:"
63,/* initial query for Books */
63,"select b1_0.isbn,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
63,from Book b1_0
63,order by b1_0.isbn
63,/* fetch all associated Authors */
63,"select a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name"
63,from Book_Author a1_0
63,join Author a1_1 on a1_1.id=a1_0.authors_id
63,where a1_0.books_isbn in (select b1_0.isbn from Book b1_0)
63,Notice that the first query is re-executed in a subselect in the second query.
63,"The execution of the subselect is likely to be relatively inexpensive, since the data should already be cached by the database."
63,"Clever, huh?"
63,Enabling the use of batch or subselect fetching
63,"Both batch fetching and subselect fetching are disabled by default, but we may enable one or the other globally using properties."
63,Table 45. Configuration settings to enable batch and subselect fetching
63,Configuration property name
63,Property value
63,Alternatives
63,hibernate.default_batch_fetch_size
63,A sensible batch size >1 to enable batch fetching
63,"@BatchSize(), setFetchBatchSize()"
63,hibernate.use_subselect_fetch
63,true to enable subselect fetching
63,"@Fetch(SUBSELECT), setSubselectFetchingEnabled()"
63,"Alternatively, we can enable one or the other in a given session:"
63,session.setFetchBatchSize(5);
63,session.setSubselectFetchingEnabled(true);
63,We may request subselect fetching more selectively by annotating a collection or many-valued association with the @Fetch annotation.
63,@ManyToMany @Fetch(SUBSELECT)
63,Set<Author> authors;
63,"Note that @Fetch(SUBSELECT) has the same effect as @Fetch(SELECT), except after execution of a HQL or criteria query."
63,"But after query execution, @Fetch(SUBSELECT) is able to much more efficiently fetch associations."
63,"Later, we’ll see how we can use fetch profiles to do this even more selectively."
63,That’s all there is to it.
63,"Too easy, right?"
63,"Sadly, that’s not the end of the story."
63,"While batch fetching might mitigate problems involving N+1 selects, it won’t solve them."
63,The truly correct solution is to fetch associations using joins.
63,Batch fetching (or subselect fetching) can only be the best solution in rare cases where outer join fetching would result in a cartesian product and a huge result set.
63,But batch fetching and subselect fetching have one important characteristic in common: they can be performed lazily.
63,"This is, in principle, pretty convenient."
63,"When we query data, and then navigate an object graph, lazy fetching saves us the effort of planning ahead."
63,It turns out that this is a convenience we’re going to have to surrender.
63,7.5. Join fetching
63,"Outer join fetching is usually the best way to fetch associations, and it’s what we use most of the time."
63,"Unfortunately, by its very nature, join fetching simply can’t be lazy."
63,"So to make use of join fetching, we must plan ahead."
63,Our general advice is:
63,"Avoid the use of lazy fetching, which is often the source of N+1 selects."
63,"Now, we’re not saying that associations should be mapped for eager fetching by default!"
63,"That would be a terrible idea, resulting in simple session operations that fetch almost the entire database."
63,Therefore:
63,Most associations should be mapped for lazy fetching by default.
63,"It sounds as if this tip is in contradiction to the previous one, but it’s not."
63,It’s saying that you must explicitly specify eager fetching for associations precisely when and where they are needed.
63,"If we need eager join fetching in some particular transaction, we have four different ways to specify that."
63,Passing a JPA EntityGraph
63,We’ve already seen this in Entity graphs and eager fetching
63,Specifying a named fetch profile
63,We’ll discuss this approach later in Named fetch profiles
63,Using left join fetch in HQL/JPQL
63,See A Guide to Hibernate Query Language for details
63,Using From.fetch() in a criteria query
63,Same semantics as join fetch in HQL
63,"Typically, a query is the most convenient option."
63,Here’s how we can ask for join fetching in HQL:
63,List<Book> booksWithJoinFetchedAuthors =
63,"session.createSelectionQuery(""from Book join fetch authors order by isbn"")"
63,.getResultList();
63,"And this is the same query, written using the criteria API:"
63,var builder = sessionFactory.getCriteriaBuilder();
63,var query = builder.createQuery(Book.class);
63,var book = query.from(Book.class);
63,book.fetch(Book_.authors);
63,query.select(book);
63,query.orderBy(builder.asc(book.get(Book_.isbn)));
63,List<Book> booksWithJoinFetchedAuthors =
63,session.createSelectionQuery(query).getResultList();
63,"Either way, a single SQL select statement is executed:"
63,"select b1_0.isbn,a1_0.books_isbn,a1_1.id,a1_1.bio,a1_1.name,b1_0.price,b1_0.published,b1_0.publisher_id,b1_0.title"
63,from Book b1_0
63,join (Book_Author a1_0 join Author a1_1 on a1_1.id=a1_0.authors_id)
63,on b1_0.isbn=a1_0.books_isbn
63,order by b1_0.isbn
63,Much better!
63,"Join fetching, despite its non-lazy nature, is clearly more efficient than either batch or subselect fetching, and this is the source of our recommendation to avoid the use of lazy fetching."
63,There’s one interesting case where join fetching becomes inefficient: when we fetch two many-valued associations in parallel.
63,Imagine we wanted to fetch both Author.books and Author.royaltyStatements in some unit of work.
63,"Joining both collections in a single query would result in a cartesian product of tables, and a large SQL result set."
63,"Subselect fetching comes to the rescue here, allowing us to fetch books using a join, and royaltyStatements using a single subsequent select."
63,"Of course, an alternative way to avoid many round trips to the database is to cache the data we need in the Java client."
63,"If we’re expecting to find the associated data in a local cache, we probably don’t need join fetching at all."
63,But what if we can’t be certain that all associated data will be in the cache?
63,"In that case, we might be able to reduce the cost of cache misses by enabling batch fetching."
63,7.6. The second-level cache
63,"A classic way to reduce the number of accesses to the database is to use a second-level cache, allowing"
63,data cached in memory to be shared between sessions.
63,"By nature, a second-level cache tends to undermine the ACID properties of transaction processing in a relational database."
63,We don’t use a distributed transaction with two-phase commit to ensure that changes to the cache and database happen atomically.
63,"So a second-level cache is often by far the easiest way to improve the performance of a system, but only at the cost of making it much more difficult to reason about concurrency."
63,And so the cache is a potential source of bugs which are difficult to isolate and reproduce.
63,"Therefore, by default, an entity is not eligible for storage in the second-level cache."
63,We must explicitly mark each entity that will
63,be stored in the second-level cache with the @Cache annotation from org.hibernate.annotations.
63,But that’s still not enough.
63,"Hibernate does not itself contain an implementation of a second-level cache, so it’s necessary to configure an external cache provider."
63,Caching is disabled by default.
63,"To minimize the risk of data loss, we force you to stop and think before any entity goes into the cache."
63,"Hibernate segments the second-level cache into named regions, one for each:"
63,mapped entity hierarchy or
63,collection role.
63,"For example, there might be separate cache regions for Author, Book, Author.books, and Book.authors."
63,"Each region is permitted its own policies for expiry, persistence, and replication. These policies must be configured externally to Hibernate."
63,"The appropriate policies depend on the kind of data an entity represents. For example, a program might have different caching policies for ""reference"" data, for transactional data, and for data used for analytics. Ordinarily, the implementation of those policies is the responsibility of the underlying cache implementation."
63,7.7. Specifying which data is cached
63,"By default, no data is eligible for storage in the second-level cache."
63,An entity hierarchy or collection role may be assigned a region using the @Cache annotation.
63,"If no region name is explicitly specified, the region name is just the name of the entity class or collection role."
63,@Entity
63,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
63,class Publisher {
63,...
63,"@Cache(usage=READ_WRITE, region=""PublishedBooks"")"
63,@OneToMany(mappedBy=Book_.PUBLISHER)
63,Set<Book> books;
63,...
63,The cache defined by a @Cache annotation is automatically utilized by Hibernate to:
63,"retrieve an entity by id when find() is called, or"
63,to resolve an association by id.
63,The @Cache annotation must be specified on the root class of an entity inheritance hierarchy.
63,It’s an error to place it on a subclass entity.
63,"The @Cache annotation always specifies a CacheConcurrencyStrategy, a policy governing access to the second-level cache by concurrent transactions."
63,Table 46. Cache concurrency
63,Concurrency policy
63,Interpretation
63,Explanation
63,READ_ONLY
63,Immutable data
63,Read-only access
63,"Indicates that the cached object is immutable, and is never updated. If an entity with this cache concurrency is updated, an exception is thrown."
63,"This is the simplest, safest, and best-performing cache concurrency strategy. It’s particularly suitable for so-called ""reference"" data."
63,NONSTRICT_READ_WRITE
63,Concurrent updates are extremely improbable
63,Read/write access with no locking
63,"Indicates that the cached object is sometimes updated, but that it’s extremely unlikely that two transactions will attempt to update the same item of data at the same time."
63,"This strategy does not use locks. When an item is updated, the cache is invalidated both before and after completion of the updating transaction. But without locking, it’s impossible to completely rule out the possibility of a second transaction storing or retrieving stale data in or from the cache during the completion process of the first transaction."
63,READ_WRITE
63,Concurrent updates are possible but not common
63,Read/write access using soft locks
63,Indicates a non-vanishing likelihood that two concurrent transactions attempt to update the same item of data simultaneously.
63,"This strategy uses ""soft"" locks to prevent concurrent transactions from retrieving or storing a stale item from or in the cache during the transaction completion process. A soft lock is simply a marker entry placed in the cache while the updating transaction completes."
63,"A second transaction may not read the item from the cache while the soft lock is present, and instead simply proceeds to read the item directly from the database, exactly as if a regular cache miss had occurred."
63,"Similarly, the soft lock also prevents this second transaction from storing a stale item to the cache when it returns from its round trip to the database with something that might not quite be the latest version."
63,TRANSACTIONAL
63,Concurrent updates are frequent
63,Transactional access
63,"Indicates that concurrent writes are common, and the only way to maintain synchronization between the second-level cache and the database is via the use of a fully transactional cache provider. In this case, the cache and the database must cooperate via JTA or the XA protocol, and Hibernate itself takes on little responsibility for maintaining the integrity of the cache."
63,Which policies make sense may also depend on the underlying second-level cache implementation.
63,"JPA has a similar annotation, named @Cacheable."
63,"Unfortunately, it’s almost useless to us, since:"
63,"it provides no way to specify any information about the nature of the cached entity and how its cache should be managed, and"
63,"it may not be used to annotate associations, and so we can’t even use it to mark collection roles as eligible for storage in the second-level cache."
63,7.8. Caching by natural id
63,"If our entity has a natural id, we can enable an additional cache, which holds cross-references from natural id to primary id, by annotating the entity @NaturalIdCache."
63,"By default, the natural id cache is stored in a dedicated region of the second-level cache, separate from the cached entity data."
63,@Entity
63,"@Cache(usage=READ_WRITE, region=""Book"")"
63,"@NaturalIdCache(region=""BookIsbn"")"
63,class Book {
63,...
63,@NaturalId
63,String isbn;
63,@NaturalId
63,int printing;
63,...
63,This cache is utilized when the entity is retrieved using one of the operations of Session which performs lookup by natural id.
63,"Since the natural id cache doesn’t contain the actual state of the entity, it doesn’t make sense to annotate an entity @NaturalIdCache unless it’s already eligible for storage in the second-level cache, that is, unless it’s also annotated @Cache."
63,"It’s worth noticing that, unlike the primary identifier of an entity, a natural id might be mutable."
63,"We must now consider a subtlety that often arises when we have to deal with so-called ""reference data"", that is, data which fits easily in memory, and doesn’t change much."
63,7.9. Caching and association fetching
63,Let’s consider again our Publisher class:
63,"@Cache(usage=NONSTRICT_READ_WRITE, region=""Publishers"")"
63,@Entity
63,class Publisher { ... }
63,"Data about publishers doesn’t change very often, and there aren’t so many of them."
63,Suppose we’ve set everything up so that the publishers are almost always available in the second-level cache.
63,Then in this case we need to think carefully about associations of type Publisher.
63,@ManyToOne
63,Publisher publisher;
63,"There’s no need for this association to be lazily fetched, since we’re expecting it to be available in memory, so we won’t set it fetch=LAZY."
63,"But on the other hand, if we leave it marked for eager fetching then, by default, Hibernate will often fetch it using a join."
63,This places completely unnecessary load on the database.
63,The solution is the @Fetch annotation:
63,@ManyToOne @Fetch(SELECT)
63,Publisher publisher;
63,"By annotating the association @Fetch(SELECT), we suppress join fetching, giving Hibernate a chance to find the associated Publisher in the cache."
63,"Therefore, we arrive at this rule of thumb:"
63,"Many-to-one associations to ""reference data"", or to any other data that will almost always be available in the cache, should be mapped EAGER,SELECT."
63,"Other associations, as we’ve already made clear, should be LAZY."
63,"Once we’ve marked an entity or collection as eligible for storage in the second-level cache, we still need to set up an actual cache."
63,7.10. Configuring the second-level cache provider
63,"Configuring a second-level cache provider is a rather involved topic, and quite outside the scope of this document."
63,"But in case it helps, we often test Hibernate with the following configuration, which uses EHCache as the cache implementation, as above in Optional dependencies:"
63,Table 47. EHCache configuration
63,Configuration property name
63,Property value
63,hibernate.cache.region.factory_class
63,jcache
63,hibernate.javax.cache.uri
63,/ehcache.xml
63,"If you’re using EHCache, you’ll also need to include an ehcache.xml file"
63,that explicitly configures the behavior of each cache region belonging to
63,your entities and collections.
63,You’ll find more information about configuring EHCache here.
63,"We may use any other implementation of JCache, such as Caffeine."
63,JCache automatically selects whichever implementation it finds on the classpath.
63,"If there are multiple implementations on the classpath, we must disambiguate using:"
63,Table 48. Disambiguating the JCache implementation
63,Configuration property name
63,Property value
63,hibernate.javax.cache.provider
63,"The implementation of javax.cache.spiCachingProvider, for example:"
63,org.ehcache.jsr107.EhcacheCachingProvider
63,for EHCache
63,com.github.benmanes.caffeine.jcache.spi.CaffeineCachingProvider
63,for Caffeine
63,"Alternatively, to use Infinispan as the cache implementation, the following settings are required:"
63,Table 49. Infinispan provider configuration
63,Configuration property name
63,Property value
63,hibernate.cache.region.factory_class
63,infinispan
63,hibernate.cache.infinispan.cfg
63,"Path to infinispan configuration file, for example:"
63,org/infinispan/hibernate/cache/commons/builder/infinispan-configs.xml
63,for a distributed cache
63,org/infinispan/hibernate/cache/commons/builder/infinispan-configs-local.xml
63,to test with local cache
63,Infinispan is usually used when distributed caching is required.
63,There’s more about using Infinispan with Hibernate here.
63,"Finally, there’s a way to globally disable the second-level cache:"
63,Table 50. Setting to disable caching
63,Configuration property name
63,Property value
63,hibernate.cache.use_second_level_cache
63,"true to enable caching, or false to disable it"
63,"When hibernate.cache.region.factory_class is set, this property defaults to true."
63,This setting lets us easily disable the second-level cache completely when troubleshooting or profiling performance.
63,You can find much more information about the second-level cache in the User Guide.
63,7.11. Caching query result sets
63,The caches we’ve described above are only used to optimize lookups by id or by natural id.
63,"Hibernate also has a way to cache the result sets of queries, though this is only rarely an efficient thing to do."
63,The query cache must be enabled explicitly:
63,Table 51. Setting to enable the query cache
63,Configuration property name
63,Property value
63,hibernate.cache.use_query_cache
63,true to enable the query cache
63,"To cache the results of a query, call SelectionQuery.setCacheable(true):"
63,"session.createQuery(""from Product where discontinued = false"")"
63,.setCacheable(true)
63,.getResultList();
63,"By default, the query result set is stored in a cache region named default-query-results-region."
63,"Since different queries should have different caching policies, it’s common to explicitly specify a region name:"
63,"session.createQuery(""from Product where discontinued = false"")"
63,.setCacheable(true)
63,".setCacheRegion(""ProductCatalog"")"
63,.getResultList();
63,A result set is cached together with a logical timestamp.
63,"By ""logical"", we mean that it doesn’t actually increase linearly with time, and in particular it’s not the system time."
63,"When a Product is updated, Hibernate does not go through the query cache and invalidate every cached result set that’s affected by the change."
63,"Instead, there’s a special region of the cache which holds a logical timestamp of the most-recent update to each table."
63,"This is called the update timestamps cache, and it’s kept in the region default-update-timestamps-region."
63,It’s your responsibility to ensure that this cache region is configured with appropriate policies.
63,"In particular, update timestamps should never expire or be evicted."
63,"When a query result set is read from the cache, Hibernate compares its timestamp with the timestamp of each of the tables that affect the results of the query, and only returns the result set if the result set isn’t stale."
63,"If the result set is stale, Hibernate goes ahead and re-executes the query against the database and updates the cached result set."
63,"As is generally the case with any second-level cache, the query cache can break the ACID properties of transactions."
63,7.12. Second-level cache management
63,"For the most part, the second-level cache is transparent."
63,"Program logic which interacts with the Hibernate session is unaware of the cache, and is not impacted by changes to caching policies."
63,"At worst, interaction with the cache may be controlled by specifying of an explicit CacheMode:"
63,session.setCacheMode(CacheMode.IGNORE);
63,"Or, using JPA-standard APIs:"
63,entityManager.setCacheRetrieveMode(CacheRetrieveMode.BYPASS);
63,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
63,The JPA-defined cache modes come in two flavors: CacheRetrieveMode and CacheStoreMode.
63,Table 52. JPA-defined cache retrieval modes
63,Mode
63,Interpretation
63,CacheRetrieveMode.USE
63,Read data from the cache if available
63,CacheRetrieveMode.BYPASS
63,Don’t read data from the cache; go direct to the database
63,We might select CacheRetrieveMode.BYPASS if we’re concerned about the possibility of reading stale data from the cache.
63,Table 53. JPA-defined cache storage modes
63,Mode
63,Interpretation
63,CacheStoreMode.USE
63,Write data to the cache when read from the database or when modified; do not update already-cached items when reading
63,CacheStoreMode.REFRESH
63,Write data to the cache when read from the database or when modified; always update cached items when reading
63,CacheStoreMode.BYPASS
63,Don’t write data to the cache
63,We should select CacheStoreMode.BYPASS if we’re querying data that doesn’t need to be cached.
63,It’s a good idea to set the CacheStoreMode to BYPASS just before running a query which returns a large result set full of data that we don’t expect to need again soon.
63,"This saves work, and prevents the newly-read data from pushing out the previously cached data."
63,In JPA we would use this idiom:
63,entityManager.setCacheStoreMode(CacheStoreMode.BYPASS);
63,List<Publisher> allpubs =
63,"entityManager.createQuery(""from Publisher"", Publisher.class)"
63,.getResultList();
63,entityManager.setCacheStoreMode(CacheStoreMode.USE);
63,But Hibernate has a better way:
63,List<Publisher> allpubs =
63,"session.createSelectionQuery(""from Publisher"", Publisher.class)"
63,.setCacheStoreMode(CacheStoreMode.BYPASS)
63,.getResultList();
63,A Hibernate CacheMode packages a CacheRetrieveMode with a CacheStoreMode.
63,Table 54. Hibernate cache modes and JPA equivalents
63,Hibernate CacheMode
63,Equivalent JPA modes
63,NORMAL
63,"CacheRetrieveMode.USE, CacheStoreMode.USE"
63,IGNORE
63,"CacheRetrieveMode.BYPASS, CacheStoreMode.BYPASS"
63,GET
63,"CacheRetrieveMode.USE, CacheStoreMode.BYPASS"
63,PUT
63,"CacheRetrieveMode.BYPASS, CacheStoreMode.USE"
63,REFRESH
63,"CacheRetrieveMode.REFRESH, CacheStoreMode.BYPASS"
63,There’s no particular reason to prefer Hibernate’s CacheMode over the JPA equivalents.
63,This enumeration only exists because Hibernate had cache modes long before they were added to JPA.
63,"For ""reference"" data, that is, for data which is expected to always be found in the second-level cache, it’s a good idea to prime the cache at startup."
63,There’s a really easy way to do this: just execute a query immediately after obtaining the
63,EntityManager or SessionFactory.
63,SessionFactory sessionFactory =
63,setupHibernate(new Configuration())
63,.buildSessionFactory();
63,// prime the second-level cache
63,sessionFactory.inSession(session -> {
63,"session.createSelectionQuery(""from Country""))"
63,.setReadOnly(true)
63,.getResultList();
63,"session.createSelectionQuery(""from Product where discontinued = false""))"
63,.setReadOnly(true)
63,.getResultList();
63,});
63,"Very occasionally, it’s necessary or advantageous to control the cache explicitly, for example, to evict some data that we know to be stale."
63,The Cache interface allows programmatic eviction of cached items.
63,"sessionFactory.getCache().evictEntityData(Book.class, bookId);"
63,Second-level cache management via the Cache interface is not transaction-aware.
63,"None of the operations of Cache respect any isolation or transactional semantics associated with the underlying caches. In particular, eviction via the methods of this interface causes an immediate ""hard"" removal outside any current transaction and/or locking scheme."
63,"Ordinarily, however, Hibernate automatically evicts or updates cached data after modifications, and, in addition, cached data which is unused will eventually be expired according to the configured policies."
63,This is quite different to what happens with the first-level cache.
63,7.13. Session cache management
63,Entity instances aren’t automatically evicted from the session cache when they’re no longer needed.
63,"Instead, they stay pinned in memory until the session they belong to is discarded by your program."
63,"The methods detach() and clear() allow you to remove entities from the session cache, making them available for garbage collection."
63,"Since most sessions are rather short-lived, you won’t need these operations very often."
63,"And if you find yourself thinking you do need them in a certain situation, you should strongly consider an alternative solution: a stateless session."
63,7.14. Stateless sessions
63,"An arguably-underappreciated feature of Hibernate is the StatelessSession interface, which provides a command-oriented, more bare-metal approach to interacting with the database."
63,You may obtain a stateless session from the SessionFactory:
63,StatelessSession ss = getSessionFactory().openStatelessSession();
63,A stateless session:
63,"doesn’t have a first-level cache (persistence context), nor does it interact with any second-level caches, and"
63,"doesn’t implement transactional write-behind or automatic dirty checking, so all operations are executed immediately when they’re explicitly called."
63,"For a stateless session, we’re always working with detached objects."
63,"Thus, the programming model is a bit different:"
63,Table 55. Important methods of the StatelessSession
63,Method name and parameters
63,Effect
63,"get(Class, Object)"
63,"Obtain a detached object, given its type and its id, by executing a select"
63,fetch(Object)
63,Fetch an association of a detached object
63,refresh(Object)
63,Refresh the state of a detached object by executing
63,a select
63,insert(Object)
63,Immediately insert the state of the given transient object into the database
63,update(Object)
63,Immediately update the state of the given detached object in the database
63,delete(Object)
63,Immediately delete the state of the given detached object from the database
63,upsert(Object)
63,Immediately insert or update the state of the given detached object using a SQL merge into statement
63,"There’s no flush() operation, and so update() is always explicit."
63,"In certain circumstances, this makes stateless sessions easier to work with, but with the caveat that a stateless session is much more vulnerable to data aliasing effects, since it’s easy to get two non-identical Java objects which both represent the same row of a database table."
63,"If we use fetch() in a stateless session, we can very easily obtain two objects representing the same database row!"
63,"In particular, the absence of a persistence context means that we can safely perform bulk-processing tasks without allocating huge quantities of memory."
63,Use of a StatelessSession alleviates the need to call:
63,"clear() or detach() to perform first-level cache management, and"
63,setCacheMode() to bypass interaction with the second-level cache.
63,"Stateless sessions can be useful, but for bulk operations on huge datasets, Hibernate can’t possibly compete with stored procedures!"
63,"When using a stateless session, you should be aware of the following additional limitations:"
63,"persistence operations never cascade to associated instances,"
63,"changes to @ManyToMany associations and @ElementCollections cannot be made persistent, and"
63,operations performed via a stateless session bypass callbacks.
63,7.15. Optimistic and pessimistic locking
63,"Finally, an aspect of behavior under load that we didn’t mention above is row-level data contention."
63,"When many transactions try to read and update the same data, the program might become unresponsive with lock escalation, deadlocks, and lock acquisition timeout errors."
63,There’s two basic approaches to data concurrency in Hibernate:
63,"optimistic locking using @Version columns, and"
63,database-level pessimistic locking using the SQL for update syntax (or equivalent).
63,"In the Hibernate community it’s much more common to use optimistic locking, and Hibernate makes that incredibly easy."
63,"Where possible, in a multiuser system, avoid holding a pessimistic lock across a user interaction."
63,"Indeed, the usual practice is to avoid having transactions that span user interactions. For multiuser systems, optimistic locking is king."
63,"That said, there is also a place for pessimistic locks, which can sometimes reduce the probability of transaction rollbacks."
63,"Therefore, the find(), lock(), and refresh() methods of the reactive session accept an optional LockMode."
63,We can also specify a LockMode for a query.
63,"The lock mode can be used to request a pessimistic lock, or to customize the behavior of optimistic locking:"
63,Table 56. Optimistic and pessimistic lock modes
63,LockMode type
63,Meaning
63,READ
63,An optimistic lock obtained implicitly whenever
63,an entity is read from the database using select
63,OPTIMISTIC
63,An optimistic lock obtained when an entity is
63,"read from the database, and verified using a"
63,select to check the version when the
63,transaction completes
63,OPTIMISTIC_FORCE_INCREMENT
63,An optimistic lock obtained when an entity is
63,"read from the database, and enforced using an"
63,update to increment the version when the
63,transaction completes
63,WRITE
63,A pessimistic lock obtained implicitly whenever
63,an entity is written to the database using
63,update or insert
63,PESSIMISTIC_READ
63,A pessimistic for share lock
63,PESSIMISTIC_WRITE
63,A pessimistic for update lock
63,PESSIMISTIC_FORCE_INCREMENT
63,A pessimistic lock enforced using an immediate
63,update to increment the version
63,7.16. Collecting statistics
63,We may ask Hibernate to collect statistics about its activity by setting this configuration property:
63,Configuration property name
63,Property value
63,hibernate.generate_statistics
63,true to enable collection of statistics
63,The statistics are exposed by the Statistics object:
63,long failedVersionChecks =
63,sessionFactory.getStatistics()
63,.getOptimisticFailureCount();
63,long publisherCacheMissCount =
63,sessionFactory.getStatistics()
63,.getEntityStatistics(Publisher.class.getName())
63,.getCacheMissCount()
63,Hibernate’s statistics enable observability.
63,Both Micrometer and SmallRye Metrics are capable of exposing these metrics.
63,7.17. Tracking down slow queries
63,"When a poorly-performing SQL query is discovered in production, it can sometimes be hard to track down exactly where in the Java code the query originates."
63,Hibernate offers two configuration properties that can make it easier to identify a slow query and find its source.
63,Table 57. Settings for tracking slow queries
63,Configuration property name
63,Purpose
63,Property value
63,hibernate.log_slow_query
63,Log slow queries at the INFO level
63,"The minimum execution time, in milliseconds, which characterizes a ""slow"" query"
63,hibernate.use_sql_comments
63,Prepend comments to the executed SQL
63,true or false
63,"When hibernate.use_sql_comments is enabled, the text of the HQL query is prepended as a comment to the generated SQL, which usually makes it easy to find the HQL in the Java code."
63,The comment text may be customized:
63,"by calling Query.setComment(comment) or Query.setHint(AvailableHints.HINT_COMMENT,comment), or"
63,via the @NamedQuery annotation.
63,"Once you’ve identified a slow query, one of the best ways to make it faster is to actually go and talk to someone who is an expert at making queries go fast."
63,"These people are called ""database administrators"", and if you’re reading this document you probably aren’t one."
63,Database administrators know lots of stuff that Java developers don’t.
63,"So if you’re lucky enough to have a DBA about, you don’t need to Dunning-Kruger your way out of a slow query."
63,An expertly-defined index might be all you need to fix a slow query.
63,7.18. Adding indexes
63,The @Index annotation may be used to add an index to a table:
63,@Entity
63,"@Table(indexes=@Index(columnList=""title, year, publisher_id""))"
63,class Book { ... }
63,"It’s even possible to specify an ordering for an indexed column, or that the index should be case-insensitive:"
63,@Entity
63,"@Table(indexes=@Index(columnList=""(lower(title)), year desc, publisher_id""))"
63,class Book { ... }
63,This lets us create a customized index for a particular query.
63,Note that SQL expressions like lower(title) must be enclosed in parentheses in the columnList of the index definition.
63,It’s not clear that information about indexes belongs in annotations of Java code.
63,"Indexes are usually maintained and modified by a database administrator, ideally by an expert in tuning the performance of one particular RDBMS."
63,So it might be better to keep the definition of indexes in a SQL DDL script that your DBA can easily read and modify.
63,"Remember, we can ask Hibernate to execute a DDL script using the property javax.persistence.schema-generation.create-script-source."
63,7.19. Dealing with denormalized data
63,"A typical relational database table in a well-normalized schema has a relatively small number of columns, and so there’s little to be gained by selectively querying columns and populating only certain fields of an entity class."
63,"But occasionally, we hear from someone asking how to map a table with a hundred columns or more!"
63,This situation can arise when:
63,"data is intentionally denormalized for performance,"
63,"the results of a complicated analytic query are exposed via a view, or"
63,someone has done something crazy and wrong.
63,Let’s suppose that we’re not dealing with the last possibility.
63,Then we would like to be able to query the monster table without returning all of its columns.
63,"At first glance, Hibernate doesn’t offer a perfect bottled solution to this problem."
63,This first impression is misleading.
63,"Actually, Hibernate features more than one way to deal with this situation, and the real problem is deciding between the ways."
63,We could:
63,"map multiple entity classes to the same table or view, being careful about ""overlaps"" where a mutable column is mapped to more than one of the entities,"
63,"use HQL or native SQL queries returning results into record types instead of retrieving entity instances, or"
63,use the bytecode enhancer and @LazyGroup for attribute-level lazy fetching.
63,"Some other ORM solutions push the third option as the recommended way to handle huge tables, but this has never been the preference of the Hibernate team or Hibernate community."
63,It’s much more typesafe to use one of the first two options.
63,7.20. Reactive programming with Hibernate
63,"Finally, many systems which require high scalability now make use of reactive programming and reactive streams."
63,Hibernate Reactive brings O/R mapping to the world of reactive programming.
63,You can learn much more about Hibernate Reactive from its Reference Documentation.
63,"Hibernate Reactive may be used alongside vanilla Hibernate in the same program, and can reuse the same entity classes."
63,This means you can use the reactive programming model exactly where you need it—perhaps only in one or two places in your system.
63,You don’t need to rewrite your whole program using reactive streams.
63,8. Advanced Topics
63,"In the last chapter of this Introduction, we turn to some topics that don’t really belong in an introduction."
63,"Here we consider some problems, and solutions, that you’re probably not going to run into immediately if you’re new to Hibernate."
63,"But we do want you to know about them, so that when the time comes, you’ll know what tool to reach for."
63,8.1. Filters
63,"Filters are one of the nicest and under-usedest features of Hibernate, and we’re quite proud of them."
63,"A filter is a named, globally-defined, parameterized restriction on the data that is visible in a given session."
63,Examples of well-defined filters might include:
63,"a filter that restricts the data visible to a given user according to row-level permissions,"
63,"a filter which hides data which has been soft-deleted,"
63,"in a versioned database, a filter that displays versions which were current at a given instant in the past, or"
63,a filter that restricts to data associated with a certain geographical region.
63,A filter must be declared somewhere.
63,A package descriptor is as good a place as any for a @FilterDef:
63,"@FilterDef(name = ""ByRegion"","
63,"parameters = @ParamDef(name = ""region"", type = String.class))"
63,package org.hibernate.example;
63,This filter has one parameter.
63,"Fancier filters might in principle have multiple parameters, though we admit this must be quite rare."
63,"If you add annotations to a package descriptor, and you’re using Configuration to configure Hibernate, make sure you call Configuration.addPackage() to let Hibernate know that the package descriptor is annotated."
63,"Typically, but not necessarily, a @FilterDef specifies a default restriction:"
63,"@FilterDef(name = ""ByRegion"","
63,"parameters = @ParamDef(name = ""region"", type = String.class),"
63,"defaultCondition = ""region = :region"")"
63,package org.hibernate.example;
63,"The restriction must contain a reference to the parameter of the filter, specified using the usual syntax for named parameters."
63,Any entity or collection which is affected by a filter must be annotated @Filter:
63,@Entity
63,@Filter(name = example_.BY_REGION)
63,class User {
63,@Id String username;
63,String region;
63,...
63,"Here, as usual, example_.BY_REGION is generated by the Metamodel Generator, and is just a constant with the value ""ByRegion""."
63,"If the @Filter annotation does not explicitly specify a restriction, the default restriction given by the @FilterDef will be applied to the entity."
63,But an entity is free to override the default condition.
63,@Entity
63,"@Filter(name = example_.FILTER_BY_REGION, condition = ""name = :region"")"
63,class Region {
63,@Id String name;
63,...
63,Note that the restriction specified by the condition or defaultCondition is a native SQL expression.
63,Table 58. Annotations for defining filters
63,Annotation
63,Purpose
63,@FilterDef
63,Defines a filter and declares its name (exactly one per filter)
63,@Filter
63,Specifies how a filter applies to a given entity or collection (many per filter)
63,"By default, a new session comes with every filter disabled."
63,A filter may be explicitly enabled in a given session by calling enableFilter() and assigning arguments to the parameters of the filter.
63,You should do this right at the start of the session.
63,sessionFactory.inTransaction(session -> {
63,session.enableFilter(example_.FILTER_BY_REGION)
63,".setParameter(""region"", ""es"")"
63,.validate();
63,...
63,});
63,"Now, any queries executed within the session will have the filter restriction applied."
63,Collections annotated @Filter will also have their members correctly filtered.
63,"On the other hand, filters are not applied to @ManyToOne associations, nor to find()."
63,This is completely by design and is not in any way a bug.
63,More than one filter may be enabled in a given session.
63,"When we only need to filter rows by a static condition with no parameters, we don’t need a filter, since @SQLRestriction provides a much simpler way to do that."
63,"We’ve mentioned that a filter can be used to implement versioning, and to provide historical views of the data."
63,"Being such a general-purpose construct, filters provide a lot of flexibility here."
63,"But if you’re after a more focused/opinionated solution to this problem, you should definitely check out Envers."
63,Using Envers for auditing historical data
63,"Envers is an add-on to Hibernate ORM which keeps a historical record of each versioned entity in a separate audit table, and allows past revisions of the data to be viewed and queried."
63,"A full introduction to Envers would require a whole chapter, so we’ll just give you a quick taste here."
63,"First, we must mark an entity as versioned, using the @Audited annotation:"
63,@Audited @Entity
63,"@Table(name=""CurrentDocument"")"
63,"@AuditTable(""DocumentRevision"")"
63,class Document { ... }
63,"The @AuditTable annotation is optional, and it’s better to set either org.hibernate.envers.audit_table_prefix or org.hibernate.envers.audit_table_suffix and let the audit table name be inferred."
63,The AuditReader interface exposes operations for retrieving and querying historical revisions.
63,It’s really easy to get hold of one of these:
63,AuditReader reader = AuditReaderFactory.get(entityManager);
63,Envers tracks revisions of the data via a global revision number.
63,We may easily find the revision number which was current at a given instant:
63,Number revision = reader.getRevisionNumberForDate(datetime);
63,We can use the revision number to ask for the version of our entity associated with the given revision number:
63,"Document doc = reader.find(Document.class, id, revision);"
63,"Alternatively, we can directly ask for the version which was current at a given instant:"
63,"Document doc = reader.find(Document.class, id, datetime);"
63,We can even execute queries to obtain lists of entities current at the given revision number:
63,List documents =
63,reader.createQuery()
63,".forEntitiesAtRevision(Document.class, revision)"
63,.getResultList();
63,"For much more information, see the User Guide."
63,Another closely-related problem is multi-tenancy.
63,8.2. Multi-tenancy
63,A multi-tenant database is one where the data is segregated by tenant.
63,"We don’t need to actually define what a ""tenant"" really represents here; all we care about at this level of abstraction is that each tenant may be distinguished by a unique identifier."
63,And that there’s a well-defined current tenant in each session.
63,We may specify the current tenant when we open a session:
63,var session =
63,sessionFactory.withOptions()
63,.tenantIdentifier(tenantId)
63,.openSession();
63,"Or, when using JPA-standard APIs:"
63,var entityManager =
63,"entityManagerFactory.createEntityManager(Map.of(HibernateHints.HINT_TENANT_ID, tenantId));"
63,"However, since we often don’t have this level of control over creation of the session, it’s more common to supply an implementation of CurrentTenantIdentifierResolver to Hibernate."
63,There are three common ways to implement multi-tenancy:
63,"each tenant has its own database,"
63,"each tenant has its own schema, or"
63,"tenants share tables in a single schema, and rows are tagged with the tenant id."
63,"From the point of view of Hibernate, there’s little difference between the first two options."
63,Hibernate will need to obtain a JDBC connection with permissions on the database and schema owned by the current tenant.
63,"Therefore, we must implement a MultiTenantConnectionProvider which takes on this responsibility:"
63,"from time to time, Hibernate will ask for a connection, passing the id of the current tenant, and then we must create an appropriate connection or obtain one from a pool, and return it to Hibernate, and"
63,"later, Hibernate will release the connection and ask us to destroy it or return it to the appropriate pool."
63,Check out DataSourceBasedMultiTenantConnectionProviderImpl for inspiration.
63,The third option is quite different.
63,"In this case we don’t need a MultiTenantConnectionProvider, but we will need a dedicated column holding the tenant id mapped by each of our entities."
63,@Entity
63,class Account {
63,@Id String id;
63,@TenantId String tenantId;
63,...
63,The @TenantId annotation is used to indicate an attribute of an entity which holds the tenant id.
63,"Within a given session, our data is automatically filtered so that only rows tagged with the tenant id of the current tenant are visible in that session."
63,Native SQL queries are not automatically filtered by tenant id; you’ll have to do that part yourself.
63,"To make use of multi-tenancy, we’ll usually need to set at least one of these configuration properties:"
63,Table 59. Multi-tenancy configuration
63,Configuration property name
63,Purpose
63,hibernate.tenant_identifier_resolver
63,Specifies the CurrentTenantIdentifierResolver
63,hibernate.multi_tenant_connection_provider
63,Specifies the MultiTenantConnectionProvider
63,8.3. Using custom-written SQL
63,"We’ve already discussed how to run queries written in SQL, but occasionally that’s not enough."
63,Sometimes—but much less often than you might expect—we would like to customize the SQL used by Hibernate to perform basic CRUD operations for an entity or collection.
63,For this we can use @SQLInsert and friends:
63,@Entity
63,"@SQLInsert(sql = ""insert into person (name, id, valid) values (?, ?, true)"", check = COUNT)"
63,"@SQLUpdate(sql = ""update person set name = ? where id = ?"")"
63,"@SQLDelete(sql = ""update person set valid = false where id = ?"")"
63,"@SQLSelect(sql = ""select id, name from person where id = ? and valid = true"")"
63,public static class Person { ... }
63,"If the custom SQL should be executed via a CallableStatement, just specify callable=true."
63,"Any SQL statement specified by one of these annotations must have exactly the number of JDBC parameters that Hibernate expects, that is, one for each column mapped by the entity, in the exact order Hibernate expects. In particular, the primary key columns must come last."
63,"However, the @Column annotation does lend some flexibility here:"
63,"if a column should not be written as part of the custom insert statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(insertable=false), or"
63,"if a column should not be written as part of the custom update statement, and has no corresponding JDBC parameter in the custom SQL, map it @Column(updatable=false)."
63,"If you need custom SQL, but are targeting multiple dialects of SQL, you can use the annotations defined in DialectOverrides."
63,"For example, this annotation lets us override the custom insert statement just for PostgreSQL:"
63,"@DialectOverride.SQLInsert(dialect = PostgreSQLDialect.class,"
63,"override = @SQLInsert(sql=""insert into person (name,id) values (?,gen_random_uuid())""))"
63,It’s even possible to override the custom SQL for specific versions of a database.
63,Sometimes a custom insert or update statement assigns a value to a mapped column which is calculated when the statement is executed on the database.
63,"For example, the value might be obtained by calling a SQL function:"
63,"@SQLInsert(sql = ""insert into person (name, id) values (?, gen_random_uuid())"")"
63,But the entity instance which represents the row being inserted or updated won’t be automatically populated with that value.
63,And so our persistence context loses synchronization with the database.
63,"In situations like this, we may use the @Generated annotation to tell Hibernate to reread the state of the entity after each insert or update."
63,8.4. Handling database-generated columns
63,"Sometimes, a column value is assigned or mutated by events that happen in the database, and aren’t visible to Hibernate."
63,For example:
63,"a table might have a column value populated by a trigger,"
63,"a mapped column might have a default value defined in DDL, or"
63,"a custom SQL insert or update statement might assign a value to a mapped column, as we saw in the previous subsection."
63,"One way to deal with this situation is to explicitly call refresh() at appropriate moments, forcing the session to reread the state of the entity."
63,But this is annoying.
63,The @Generated annotation relieves us of the burden of explicitly calling refresh().
63,"It specifies that the value of the annotated entity attribute is generated by the database, and that the generated value should be automatically retrieved using a SQL returning clause, or separate select after it is generated."
63,A useful example is the following mapping:
63,@Entity
63,class Entity {
63,@Generated @Id
63,"@ColumnDefault(""gen_random_uuid()"")"
63,UUID id;
63,The generated DDL is:
63,create table Entity (
63,"id uuid default gen_random_uuid() not null,"
63,primary key (uuid)
63,"So here the value of id is defined by the column default clause, by calling the PostgreSQL function gen_random_uuid()."
63,"When a column value is generated during updates, use @Generated(event=UPDATE)."
63,"When a value is generated by both inserts and updates, use @Generated(event={INSERT,UPDATE})."
63,"For columns which should be generated using a SQL generated always as clause, prefer the @GeneratedColumn annotation, so that Hibernate automatically generates the correct DDL."
63,"Actually, the @Generated and @GeneratedColumn annotations are defined in terms of a more generic and user-extensible framework for handling attribute values generated in Java, or by the database."
63,"So let’s drop down a layer, and see how that works."
63,8.5. User-defined generators
63,"JPA doesn’t define a standard way to extend the set of id generation strategies, but Hibernate does:"
63,"the Generator hierarchy of interfaces in the package org.hibernate.generator lets you define new generators, and"
63,the @IdGeneratorType meta-annotation from the package org.hibernate.annotations lets you write an annotation which associates a Generator type with identifier attributes.
63,"Furthermore, the @ValueGenerationType meta-annotation lets you write an annotation which associates a Generator type with a non-@Id attribute."
63,"These APIs are new in Hibernate 6, and supersede the classic IdentifierGenerator interface and @GenericGenerator annotation from older versions of Hibernate."
63,"However, the older APIs are still available and custom IdentifierGenerators written for older versions of Hibernate continue to work in Hibernate 6."
63,Hibernate has a range of built-in generators which are defined in terms of this new framework.
63,Table 60. Built-in generators
63,Annotation
63,Implementation
63,Purpose
63,@Generated
63,GeneratedGeneration
63,Generically handles database-generated values
63,@GeneratedColumn
63,GeneratedAlwaysGeneration
63,Handles values generated using generated always
63,@CurrentTimestamp
63,CurrentTimestampGeneration
63,Generic support for database or in-memory generation of creation or update timestamps
63,@CreationTimestamp
63,CurrentTimestampGeneration
63,A timestamp generated when an entity is first made persistent
63,@UpdateTimestamp
63,CurrentTimestampGeneration
63,"A timestamp generated when an entity is made persistent, and regenerated every time the entity is modified"
63,@UuidGenerator
63,UuidGenerator
63,A more flexible generator for RFC 4122 UUIDs
63,"Furthermore, support for JPA’s standard id generation strategies is also defined in terms of this framework."
63,"As an example, let’s look at how @UuidGenerator is defined:"
63,@IdGeneratorType(org.hibernate.id.uuid.UuidGenerator.class)
63,@ValueGenerationType(generatedBy = org.hibernate.id.uuid.UuidGenerator.class)
63,@Retention(RUNTIME)
63,"@Target({ FIELD, METHOD })"
63,public @interface UuidGenerator { ... }
63,@UuidGenerator is meta-annotated both @IdGeneratorType and @ValueGenerationType because it may be used to generate both ids and values of regular attributes.
63,"Either way, this Generator class does the hard work:"
63,public class UuidGenerator
63,// this generator produced values before SQL is executed
63,implements BeforeExecutionGenerator {
63,// constructors accept an instance of the @UuidGenerator
63,"// annotation, allowing the generator to be ""configured"""
63,// called to create an id generator
63,public UuidGenerator(
63,"org.hibernate.annotations.UuidGenerator config,"
63,"Member idMember,"
63,CustomIdGeneratorCreationContext creationContext) {
63,"this(config, idMember);"
63,// called to create a generator for a regular attribute
63,public UuidGenerator(
63,"org.hibernate.annotations.UuidGenerator config,"
63,"Member member,"
63,GeneratorCreationContext creationContext) {
63,"this(config, idMember);"
63,...
63,@Override
63,public EnumSet<EventType> getEventTypes() {
63,"// UUIDs are only assigned on insert, and never regenerated"
63,return INSERT_ONLY;
63,@Override
63,"public Object generate(SharedSessionContractImplementor session, Object owner, Object currentValue, EventType eventType) {"
63,// actually generate a UUID and transform it to the required type
63,return valueTransformer.transform( generator.generateUuid( session ) );
63,You can find out more about custom generators from the Javadoc for @IdGeneratorType and for org.hibernate.generator.
63,8.6. Naming strategies
63,"When working with a pre-existing relational schema, it’s usual to find that the column and table naming conventions used in the schema don’t match Java’s naming conventions."
63,"Of course, the @Table and @Column annotations let us explicitly specify a mapped table or column name."
63,But we would prefer to avoid scattering these annotations across our whole domain model.
63,"Therefore, Hibernate lets us define a mapping between Java naming conventions, and the naming conventions of the relational schema."
63,Such a mapping is called a naming strategy.
63,"First, we need to understand how Hibernate assigns and processes names."
63,Logical naming is the process of applying naming rules to determine the logical names of objects which were not explicitly assigned names in the O/R mapping.
63,"That is, when there’s no @Table or @Column annotation."
63,"Physical naming is the process of applying additional rules to transform a logical name into an actual ""physical"" name that will be used in the database."
63,"For example, the rules might include things like using standardized abbreviations, or trimming the length of identifiers."
63,"Thus, there’s two flavors of naming strategy, with slightly different responsibilities."
63,Hibernate comes with default implementations of these interfaces:
63,Flavor
63,Default implementation
63,An ImplicitNamingStrategy is responsible for assigning a logical name when none is specified by an annotation
63,A default strategy which implements the rules defined by JPA
63,A PhysicalNamingStrategy is responsible for transforming a logical name and producing the name used in the database
63,A trivial implementation which does no processing
63,"We happen to not much like the naming rules defined by JPA, which specify that mixed case and camel case identifiers should be concatenated using underscores."
63,We bet you could easily come up with a much better ImplicitNamingStrategy than that!
63,(Hint: it should always produce legit mixed case identifiers.)
63,A popular PhysicalNamingStrategy produces snake case identifiers.
63,Custom naming strategies may be enabled using the configuration properties we already mentioned without much explanation back in Minimizing repetitive mapping information.
63,Table 61. Naming strategy configuration
63,Configuration property name
63,Purpose
63,hibernate.implicit_naming_strategy
63,Specifies the ImplicitNamingStrategy
63,hibernate.physical_naming_strategy
63,Specifies the PhysicalNamingStrategy
63,8.7. Spatial datatypes
63,Hibernate Spatial augments the built-in basic types with a set of Java mappings for OGC spatial types.
63,"Geolatte-geom defines a set of Java types implementing the OGC spatial types, and codecs for translating to and from database-native spatial datatypes."
63,Hibernate Spatial itself supplies integration with Hibernate.
63,"To use Hibernate Spatial, we must add it as a dependency, as described in Optional dependencies."
63,Then we may immediately use Geolatte-geom and JTS types in our entities.
63,No special annotations are needed:
63,import org.locationtech.jts.geom.Point;
63,import jakarta.persistence.*;
63,@Entity
63,class Event {
63,Event() {}
63,"Event(String name, Point location) {"
63,this.name = name;
63,this.location = location;
63,@Id @GeneratedValue
63,Long id;
63,String name;
63,Point location;
63,The generated DDL uses geometry as the type of the column mapped by location:
63,create table Event (
63,"id bigint not null,"
63,"location geometry,"
63,"name varchar(255),"
63,primary key (id)
63,Hibernate Spatial lets us work with spatial types just as we would with any of the built-in basic attribute types.
63,var geometryFactory = new GeometryFactory();
63,...
63,"Point point = geometryFactory.createPoint(new Coordinate(10, 5));"
63,"session.persist(new Event(""Hibernate ORM presentation"", point));"
63,But what makes this powerful is that we may write some very fancy queries involving functions of spatial types:
63,Polygon triangle =
63,geometryFactory.createPolygon(
63,new Coordinate[] {
63,"new Coordinate(9, 4),"
63,"new Coordinate(11, 4),"
63,"new Coordinate(11, 20),"
63,"new Coordinate(9, 4)"
63,Point event =
63,"session.createQuery(""select location from Event where within(location, :zone) = true"", Point.class)"
63,".setParameter(""zone"", triangle)"
63,.getSingleResult();
63,"Here, within() is one of the functions for testing spatial relations defined by the OpenGIS specification."
63,"Other such functions include touches(), intersects(), distance(), boundary(), etc."
63,Not every spatial relation function is supported on every database.
63,A matrix of support for spatial relation functions may be found in the User Guide.
63,"If you want to play with spatial functions on H2, run the following code first:"
63,sessionFactory.inTransaction(session -> {
63,session.doWork(connection -> {
63,try (var statement = connection.createStatement()) {
63,"statement.execute(""create alias if not exists h2gis_spatial for \""org.h2gis.functions.factory.H2GISFunctions.load\"""");"
63,"statement.execute(""call h2gis_spatial()"");"
63,});
63,} );
63,8.8. Ordered and sorted collections and map keys
63,"Java lists and maps don’t map very naturally to foreign key relationships between tables, and so we tend to avoid using them to represent associations between our entity classes."
63,"But if you feel like you really need a collection with a fancier structure than Set, Hibernate does have options."
63,"The first three options let us map the index of a List or key of a Map to a column, and are usually used with a @ElementCollection, or on the owning side of an association:"
63,Table 62. Annotations for mapping lists and maps
63,Annotation
63,Purpose
63,JPA-standard
63,@OrderColumn
63,Specifies the column used to maintain the order of a list
63,@ListIndexBase
63,The column value for the first element of the list (zero by default)
63,@MapKeyColumn
63,Specifies the column used to persist the keys of a map
63,(used when the key is of basic type)
63,@MapKeyJoinColumn
63,Specifies the column used to persist the keys of a map
63,(used when the key is an entity)
63,@ManyToMany
63,@OrderColumn // order of list is persistent
63,List<Author> authors = new ArrayList<>();
63,@ElementCollection
63,"@OrderColumn(name=""tag_order"") @ListIndexBase(1) // order column and base value"
63,List<String> tags;
63,@ElementCollection
63,"@CollectionTable(name = ""author_bios"","
63,// table name
63,"joinColumns = @JoinColumn(name = ""book_isbn"")) // column holding foreign key of owner"
63,"@Column(name=""bio"")"
63,// column holding map values
63,"@MapKeyJoinColumn(name=""author_ssn"")"
63,// column holding map keys
63,"Map<Author,String> biographies;"
63,"For a Map representing an unowned @OneToMany association, the column must also be mapped on the owning side, usually by an attribute of the target entity."
63,In this case we usually use a different annotation:
63,Table 63. Annotation for mapping an entity attribute to a map key
63,Annotation
63,Purpose
63,JPA-standard
63,@MapKey
63,Specifies an attribute of the target entity which acts as the key of the map
63,@OneToMany(mappedBy = Book_.PUBLISHER)
63,@MapKey(name = Book_.TITLE) // the key of the map is the title of the book
63,"Map<String,Book> booksByTitle = new HashMap<>();"
63,"Now, let’s introduce a little distinction:"
63,"an ordered collection is one with an ordering maintained in the database, and"
63,a sorted collection is one which is sorted in Java code.
63,These annotations allow us to specify how the elements of a collection should be ordered as they are read from the database:
63,Table 64. Annotations for ordered collections
63,Annotation
63,Purpose
63,JPA-standard
63,@OrderBy
63,Specifies a fragment of JPQL used to order the collection
63,@SQLOrder
63,Specifies a fragment of SQL used to order the collection
63,"On the other hand, the following annotations specify how a collection should be sorted in memory, and are used for collections of type SortedSet or SortedMap:"
63,Table 65. Annotations for sorted collections
63,Annotation
63,Purpose
63,JPA-standard
63,@SortNatural
63,Specifies that the elements of a collection are Comparable
63,@SortComparator
63,Specifies a Comparator used to sort the collection
63,"Under the covers, Hibernate uses a TreeSet or TreeMap to maintain the collection in sorted order."
63,8.9. Any mappings
63,An @Any mapping is a sort of polymorphic many-to-one association where the target entity types are not related by the usual entity inheritance.
63,The target type is distinguished using a discriminator value stored on the referring side of the relationship.
63,This is quite different to discriminated inheritance where the discriminator is held in the tables mapped by the referenced entity hierarchy.
63,"For example, consider an Order entity containing Payment information, where a Payment might be a CashPayment or a CreditCardPayment:"
63,interface Payment { ... }
63,@Entity
63,class CashPayment { ... }
63,@Entity
63,class CreditCardPayment { ... }
63,"In this example, Payment is not be declared as an entity type, and is not annotated @Entity. It might even be an interface, or at most just a mapped superclass, of CashPayment and CreditCardPayment. So in terms of the object/relational mappings, CashPayment and CreditCardPayment would not be considered to participate in the same entity inheritance hierarchy."
63,"On the other hand, CashPayment and CreditCardPayment do have the same identifier type."
63,This is important.
63,"An @Any mapping would store the discriminator value identifying the concrete type of Payment along with the state of the associated Order, instead of storing it in the table mapped by Payment."
63,@Entity
63,class Order {
63,...
63,@Any
63,@AnyKeyJavaClass(UUID.class)
63,//the foreign key type
63,"@JoinColumn(name=""payment_id"") // the foreign key column"
63,"@Column(name=""payment_type"")"
63,// the discriminator column
63,// map from discriminator values to target entity types
63,"@AnyDiscriminatorValue(discriminator=""CASH"", entity=CashPayment.class)"
63,"@AnyDiscriminatorValue(discriminator=""CREDIT"", entity=CreditCardPayment.class)"
63,Payment payment;
63,...
63,"It’s reasonable to think of the ""foreign key"" in an @Any mapping as a composite value made up of the foreign key and discriminator taken together. Note, however, that this composite foreign key is only conceptual and cannot be declared as a physical constraint on the relational database table."
63,There are a number of annotations which are useful to express this sort of complicated and unnatural mapping:
63,Table 66. Annotations for @Any mappings
63,Annotations
63,Purpose
63,@Any
63,Declares that an attribute is a discriminated polymorphic association mapping
63,@AnyDiscriminator
63,Specify the Java type of the discriminator
63,@JdbcType or @JdbcTypeCode
63,Specify the JDBC type of the discriminator
63,@AnyDiscriminatorValue
63,Specifies how discriminator values map to entity types
63,@Column or @Formula
63,Specify the column or formula in which the discriminator value is stored
63,@AnyKeyJavaType or @AnyKeyJavaClass
63,"Specify the Java type of the foreign key (that is, of the ids of the target entities)"
63,@AnyKeyJdbcType or @AnyKeyJdbcTypeCode
63,Specify the JDBC type of the foreign key
63,@JoinColumn
63,Specifies the foreign key column
63,"Of course, @Any mappings are disfavored, except in extremely special cases, since it’s much more difficult to enforce referential integrity at the database level."
63,There’s also currently some limitations around querying @Any associations in HQL.
63,This is allowed:
63,from Order ord
63,join CashPayment cash
63,on id(ord.payment) = cash.id
63,Polymorphic association joins for @Any mappings are not currently implemented.
63,8.10. Selective column lists in inserts and updates
63,"By default, Hibernate generates insert and update statements for each entity during boostrap, and reuses the same insert statement every time an instance of the entity is made persistent, and the same update statement every time an instance of the entity is modified."
63,This means that:
63,"if an attribute is null when the entity is made persistent, its mapped column is redundantly included in the SQL insert, and"
63,"worse, if a certain attribute is unmodified when other attributes are changed, the column mapped by that attribute is redundantly included in the SQL update."
63,"Most of the time, this just isn’t an issue worth worrying about."
63,"The cost of interacting with the database is usually dominated by the cost of a round trip, not by the number of columns in the insert or update."
63,"But in cases where it does become important, there are two ways to be more selective about which columns are included in the SQL."
63,The JPA-standard way is to indicate statically which columns are eligible for inclusion via the @Column annotation.
63,"For example, if an entity is always created with an immutable creationDate, and with no completionDate, then we would write:"
63,@Column(updatable=false) LocalDate creationDate;
63,@Column(insertable=false) LocalDate completionDate;
63,"This approach works quite well in many cases, but often breaks down for entities with more than a handful of updatable columns."
63,An alternative solution is to ask Hibernate to generate SQL dynamically each time an insert or update is executed.
63,We do this by annotating the entity class.
63,Table 67. Annotations for dynamic SQL generation
63,Annotation
63,Purpose
63,@DynamicInsert
63,Specifies that an insert statement should be generated each time an entity is made persistent
63,@DynamicUpdate
63,Specifies that an update statement should be generated each time an entity is modified
63,"It’s important to realize that, while @DynamicInsert has no impact on semantics, the more useful @DynamicUpdate annotation does have a subtle side effect."
63,"The wrinkle is that if an entity has no version property, @DynamicUpdate opens the possibility of two optimistic transactions concurrently reading and selectively updating a given instance of the entity."
63,"In principle, this might lead to a row with inconsistent column values after both optimistic transactions commit successfully."
63,"Of course, this consideration doesn’t arise for entities with a @Version attribute."
63,But there’s a solution!
63,Well-designed relational schemas should have constraints to ensure data integrity.
63,That’s true no matter what measures we take to preserve integrity in our program logic.
63,We may ask Hibernate to add a check constraint to our table using the @Check annotation.
63,Check constraints and foreign key constraints can help ensure that a row never contains inconsistent column values.
63,8.11. Using the bytecode enhancer
63,Hibernate’s bytecode enhancer enables the following features:
63,"attribute-level lazy fetching for basic attributes annotated @Basic(fetch=LAZY) and for lazy non-polymorphic associations,"
63,interception-based—instead of the usual snapshot-based—detection of modifications.
63,"To use the bytecode enhancer, we must add the Hibernate plugin to our gradle build:"
63,plugins {
63,"id ""org.hibernate.orm"" version ""6.3.0.Final"""
63,hibernate { enhancement }
63,Consider this field:
63,@Entity
63,class Book {
63,...
63,"@Basic(optional = false, fetch = LAZY)"
63,@Column(length = LONG32)
63,String fullText;
63,...
63,"The fullText field maps to a clob or text column, depending on the SQL dialect."
63,"Since it’s expensive to retrieve the full book-length text, we’ve mapped the field fetch=LAZY, telling Hibernate not to read the field until it’s actually used."
63,"Without the bytecode enhancer, this instruction is ignored, and the field is always fetched immediately, as part of the initial select that retrieves the Book entity."
63,"With bytecode enhancement, Hibernate is able to detect access to the field, and lazy fetching is possible."
63,"By default, Hibernate fetches all lazy fields of a given entity at once, in a single select, when any one of them is accessed."
63,"Using the @LazyGroup annotation, it’s possible to assign fields to distinct ""fetch groups"", so that different lazy fields may be fetched independently."
63,"Similarly, interception lets us implement lazy fetching for non-polymorphic associations without the need for a separate proxy object."
63,"However, if an association is polymorphic, that is, if the target entity type has subclasses, then a proxy is still required."
63,Interception-based change detection is a nice performance optimization with a slight cost in terms of correctness.
63,"Without the bytecode enhancer, Hibernate keeps a snapshot of the state of each entity after reading from or writing to the database."
63,"When the session flushes, the snapshot state is compared to the current state of the entity to determine if the entity has been modified."
63,Maintaining these snapshots does have an impact on performance.
63,"With bytecode enhancement, we may avoid this cost by intercepting writes to the field and recording these modifications as they happen."
63,"This optimization isn’t completely transparent, however."
63,Interception-based change detection is less accurate than snapshot-based dirty checking.
63,"For example, consider this attribute:"
63,byte[] image;
63,"Interception is able to detect writes to the image field, that is, replacement of the whole array."
63,"It’s not able to detect modifications made directly to the elements of the array, and so such modifications may be lost."
63,8.12. Named fetch profiles
63,We’ve already seen two different ways to override the default fetching strategy for an association:
63,"JPA entity graphs, and"
63,"the join fetch clause in HQL, or, equivalently, the method From.fetch() in the criteria query API."
63,A third way is to define a named fetch profile.
63,"First, we must declare the profile, by annotating a class or package:"
63,"@FetchProfile(name = ""EagerBook"")"
63,@Entity
63,class Book { ... }
63,"Note that even though we’ve placed this annotation on the Book entity, a fetch profile—unlike an entity graph—isn’t ""rooted"" at any particular entity."
63,"We may specify association fetching strategies using the fetchOverrides member of the @FetchProfile annotation, but frankly it looks so messy that we’re embarrassed to show it to you here."
63,"Similarly, a JPA entity graph may be defined using @NamedEntityGraph."
63,"But the format of this annotation is even worse than @FetchProfile(fetchOverrides=…​), so we can’t recommend it. 💀"
63,A better way is to annotate an association with the fetch profiles it should be fetched in:
63,"@FetchProfile(name = ""EagerBook"")"
63,@Entity
63,class Book {
63,...
63,@ManyToOne(fetch = LAZY)
63,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
63,Publisher publisher;
63,@ManyToMany
63,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
63,Set<Author> authors;
63,...
63,@Entity
63,class Author {
63,...
63,@OneToOne
63,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
63,Person person;
63,...
63,"Here, once again, Book_.PROFILE_EAGER_BOOK is generated by the Metamodel Generator, and is just a constant with the value ""EagerBook""."
63,"For collections, we may even request subselect fetching:"
63,"@FetchProfile(name = ""EagerBook"")"
63,"@FetchProfile(name = ""BookWithAuthorsBySubselect"")"
63,@Entity
63,class Book {
63,...
63,@OneToOne
63,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
63,Person person;
63,@ManyToMany
63,"@FetchProfileOverride(profile = Book_.PROFILE_EAGER_BOOK, mode = JOIN)"
63,"@FetchProfileOverride(profile = Book_.BOOK_WITH_AUTHORS_BY_SUBSELECT,"
63,mode = SUBSELECT)
63,Set<Author> authors;
63,...
63,We may define as many different fetch profiles as we like.
63,Table 68. Annotations for defining fetch profiles
63,Annotation
63,Purpose
63,@FetchProfile
63,"Declares a named fetch profile, optionally including a list of @FetchOverrides"
63,@FetchProfile.FetchOverride
63,Declares a fetch strategy override as part of the @FetchProfile declaration
63,@FetchProfileOverride
63,"Specifies the fetch strategy for the annotated association, in a given fetch profile"
63,A fetch profile must be explicitly enabled for a given session:
63,session.enableFetchProfile(Book_.PROFILE_EAGER_BOOK);
63,"Book eagerBook = session.find(Book.class, bookId);"
63,So why or when might we prefer named fetch profiles to entity graphs?
63,"Well, it’s really hard to say."
63,"It’s nice that this feature exists, and if you love it, that’s great."
63,But Hibernate offers alternatives that we think are more compelling most of the time.
63,The one and only advantage unique to fetch profiles is that they let us very selectively request subselect fetching.
63,"We can’t do that with entity graphs, and we can’t do it with HQL."
63,There’s a special built-in fetch profile named org.hibernate.defaultProfile which is defined as the profile with @FetchProfileOverride(mode=JOIN) applied to every eager @ManyToOne or @OneToOne association.
63,If you enable this profile:
63,"session.enableFetchProfile(""org.hibernate.defaultProfile"");"
63,Then outer joins for such associations will automatically be added to every HQL or criteria query.
63,This is nice if you can’t be bothered typing out those join fetches explicitly.
63,And in principle it even helps partially mitigate the problem of JPA having specified the wrong default for the fetch member of @ManyToOne.
63,9. Credits
63,The full list of contributors to Hibernate ORM can be found on the
63,GitHub repository.
63,The following contributors were involved in this documentation:
63,Gavin King
63,Version 6.3.2.Final
63,Last updated 2023-11-23 14:49:43 UTC
64,Spark Tips. Optimizing JDBC data source reads - Blog | luminousmen
64,Home
64,Tags
64,Projects
64,About
64,License
64,Spark Tips. Optimizing JDBC data source reads
64,#spark
64,#data
64,In the context of the post we will be talking about reading from JDBC only but the same approaches applies to the writing as well.
64,"On one of the projects I had to connect to SQL databases from Spark using JDBC. For those who do not know, JDBC is an application programming interface (API) to use SQL statements in, ahem, Java SE applications."
64,The example of usage from PySpark:
64,df = spark.read \
64,".format(""jdbc"") \"
64,".option(""url"", ""jdbc:postgresql:postgres"") \"
64,".option(""dbtable"", ""db.table"") \"
64,".option(""user"", ""user"")\"
64,".option(""password"", ""pass"") \"
64,.load()
64,df.write\
64,".format(""parquet"")\"
64,.saveAsTable(...)
64,"It looks good, but it doesn't really work — it either works but very slowly, or it completely crashes depending on the size of the table."
64,"When transferring large amounts of data between Spark and an external RDBMS by default JDBC data sources loads data sequentially using a single executor thread, which can significantly slow down your application performance, and potentially exhaust the resources of your source system."
64,"In order to read data concurrently, the Spark JDBC data source must be configured with appropriate partitioning information so that it can issue multiple concurrent queries to the external database."
64,Spark’s JDBC data source partitioning options
64,Spark JDBC reader is capable of reading data in parallel by splitting it into several partitions. There are four options provided by DataFrameReader:
64,"partitionColumn is the name of the column used for partitioning. An important condition is that the column must be numeric (integer or decimal), date or timestamp type. If the partitionColumn parameter is not specified, Spark will use a single executor and create one non-empty partition. Reading data will not be distributed or parallelized."
64,numPartitions is the maximum number of partitions that can be used for simultaneous table reading and writing.
64,The lowerBound and upperBound boundaries used to define the partition width. These boundaries determines how many rows from a given range of partition column values can be within a single partition.
64,"To better understand what these are and what they control, let's go to the source code."
64,"Apache Spark's implementation of partitioning can be found in this snippet of source code. From the source code we see that the data is partitioned using partitionColumn, which splits the values to the numPartitions groups using stride like this:"
64,Example
64,"For example, suppose you choose numPartitions=10, lowerBound=0, upperBound=10000. So the code would look something like this:"
64,df = spark.read \
64,".format(""jdbc"") \"
64,".option(""url"", ""jdbc:postgresql:postgres"") \"
64,".option(""dbtable"", ""db.table"") \"
64,".option(""user"", ""user"")\"
64,".option(""password"", ""pass"") \"
64,".option(""numPartitions"", ""10"") \"
64,".option(""lowerBound"", ""0"") \"
64,".option(""upperBound"", ""10000"") \"
64,.load()
64,"Under the hood, Spark generates an SQL query for each partition with an individual filter on the partitioning column. So for our example it is equivalent to running these 10 queries (one for each partition):"
64,SELECT * FROM db.table WHERE partitionColumn <= 1000
64,SELECT * FROM db.table WHERE partitionColumn BETWEEN 1000 and 2000
64,...
64,SELECT * FROM db.table WHERE partitionColumn > 9000
64,Note on lowerBound and upperBound
64,"The lowerBound and upperBound define partitioning boundaries, but they DO NOT participate in filtering rows of the table. Therefore, Spark partitions and returns ALL the rows of the table. It is important to note that all data will be read whether partitioning is used or not."
64,"For example suppose we have partitionColumn data range in [0, 10000] and we set numPartitions=10, lowerBound=4000 and upperBound=5000. As shown in the illustration above, the first and last partitions will contain all the data outside of the corresponding upper or lower boundary."
64,"Another example, suppose we have partitionColumn data range in [2000, 4000] and we set numPartitions=10, lowerBound=0 and upperBound=10000. In this case, then only 2 of the 10 queries (one for each partition) will do all the work, not ideal. In this scenario, the best configuration would be numPartitions=10, lowerBound=2000, upperBound=4000."
64,"From these examples, we can conclude that lower and upper bounds should be close to the actual values present in the partitioning column. This can greatly affect performance. Probably the easiest way to determine them is something similar to this:"
64,"query = f""""""SELECT MIN({partitionColumn}), MAX({partitionColumn}) FROM ({db.table})"""""""
64,min_max_df = spark.read \
64,".format(""jdbc"") \"
64,".option(""url"", ""jdbc:postgresql:postgres"") \"
64,".option(""dbtable"", ""db.table"") \"
64,".option(""user"", ""user"")\"
64,".option(""password"", ""pass"") \"
64,".option(""query"", query) \"
64,.load()
64,"lowerBound, upperBound = min_max_df.collect()[0]"
64,Optimizing performance
64,Performance issues can be checked either with the Spark UI or with your cluster metrics.
64,Set numPartitions wisely
64,"Spark tries to reuse existing partitions as much as possible and therefore indirectly the numParitions parameter also affects the degree of parallelism of all subsequent operations on the DataFrame until the repartition method is called. Thus, ideally, each of the executors should work with roughly the same amount of data so rows evenly distributed across partitions. In this way, we get properly balanced partitions, which helps improve application performance. We discussed that subject in this blog post."
64,"Also keep in mind that by increasing number of partitions also increases the number of concurrent requests and connections made to the database. Most RDBMS systems have limits on the number of simultaneous connections. So try to keep the number of partitions at a reasonable limit. If you need a lot of concurrency after receiving JDBC rows (because you're running something CPU-bound in Spark), but you don't want to issue too many simultaneous database queries, consider using fewer partitions to read JDBC and then doing an explicit repartition() in Spark."
64,Choose proper partitionColumn
64,"The key to balanced partitions is to set the partitioning options correctly. With this in mind, we can optimize the whole process by choosing the right partition column and the upperBound and lowerBound values so that the partitionColumn strides are about the same size. To achieve this the values in partitionColumn should be evenly distributed to avoid skewing the data."
64,"Analyze the columns available to determine if there are columns with high cardinality and even distribution that can be distributed among the desired number of partitions. These are good candidates for partitionColumn. Additionally, you should determine the exact range of values. Aggregations with different measures of centrality and skewness as well as histograms and basic key counts are good research tools."
64,"Depending on the RDBMS, you can use width_bucket (PostgreSQL, Oracle) or an equivalent function to get a decent idea of how the data will be distributed in Spark after loading with partitionColumn, lowerBound, upperBound, numPartitons."
64,SELECT
64,width_bucket(
64,"partitionColum, lowerBound, upperBound, numPartitons"
64,") AS bucket,"
64,COUNT(*)
64,FROM db.table
64,ORDER BY bucket
64,"If possible, create a new one (perhaps a multi-column hash) to distribute the partitions more evenly."
64,Consider index columns for partitionColumn
64,"If you read using one of the partitioning options, Spark issues concurrent queries to the JDBC database. If these queries require a full table scan, this can cause bottlenecks in the database and become extremely slow. Partitioning is most efficient when performed on an indexed column. Therefore, when selecting a column for partitioning, you should consider the effect of indexes and choose a column so that queries of individual partitions can be run in parallel efficiently."
64,Push down optimization
64,"Spark can push down some conditions to the database, but only those in the WHERE clause. Everything else, such as constraints, counts, ordering, groups and conditions, is handled on the Spark side and requires both significant data transfer and handling with Spark, except df.count() — this operation simply goes into statistics which are stored in the database, in different situations join can also be pushed down."
64,This optimization reduces the amount of data loaded and helps you use query optimizations (such as RDBMS indexes) defined at the data source level.
64,"You can prune columns and pushdown query predicates to the database with DataFrame methods, like this:"
64,"df.select(""column1"", ""column2"", ""column3"")"
64,Use batchsize to boost writing speed
64,"Another JDBC parameter is related to the record part and defines the number of batches performed for the insertion operation. This number is specified with the batchsize option. Spark will group the inserted strings into batches, each with the number of elements defined by the option."
64,df = spark.read \
64,".format(""jdbc"") \"
64,".option(""url"", ""jdbc:postgresql:postgres"") \"
64,".option(""dbtable"", ""db.table"") \"
64,".option(""user"", ""user"")\"
64,".option(""password"", ""pass"") \"
64,".option(""batchsize"",""10000"") \"
64,.load()
64,Use fetchsize to boost reading speed
64,Yet another JDBC parameter which controls the number of rows fetched per iteration from a remote JDBC database. It defaults to low fetch size (e.g. Oracle with 10 rows).
64,df = spark.read \
64,".format(""jdbc"") \"
64,".option(""url"", ""jdbc:postgresql:postgres"") \"
64,".option(""dbtable"", ""db.table"") \"
64,".option(""user"", ""user"")\"
64,".option(""password"", ""pass"") \"
64,".option(""fetchsize"",""10000"") \"
64,.load()
64,"If you set fetchsize parameter too low, the workload can become queued up due to the large number of queries between Spark and the external database to get the full set of results. If fetchsize parameter is too high, it can lead to increased GC activity (so GC is suspended) and, in the worst case, OOM problems. The optimal parameter depends on the workload (as it depends on the result schema, the size of the rows in the results, and so on), but even a small increase in this parameter over the default value can result in huge performance gains."
64,Materials
64,Apache Spark docs: JDBC To Other Databases
64,"High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark by Holden Karau, Rachel Warren"
64,Spark in Action by Jean-Georges Perrin
64,#spark
64,#data
64,Last updated
64,Oct 01 2023
64,Previous post
64,Buy
64,me a coffee
64,"More? Well, there you go:"
64,Spark Tips. Partition Tuning
64,Spark Partitions
64,Spark core concepts explained
64,Tags
64,About
64,License
64,Subscribe
64,RSS
64,Feedly
64,Telegram
64,Support
64,Uptime
64,© Copyright luminousmen.com. All Rights Reserved. No AI Used. All opinions are my own. Some links are affiliate links where I earn a commission.
65,Database Tools and SQL | IntelliJ IDEA Documentation
65,"IntelliJ IDEA 2023.3 HelpDatabase Tools and SQLEnable the Database Tools and SQL plugin This functionality relies on the Database Tools and SQL plugin,"
65,"which is bundled and enabled in IntelliJ IDEA by default. If the relevant features aren't available, make sure that you didn't disable the plugin. The Database Tools and SQL plugin is available only in IntelliJ IDEA Ultimate.Press Ctrl+Alt+S to open the IDE settings and then select Plugins.Open the Installed tab, find the Database Tools and SQL plugin, and select the checkbox next to the plugin name.This section provides an overview of basic functionality that is available with the Database Tools and SQL plugin in IntelliJ IDEA. For more information about database features, refer to the official DataGrip documentation.The database management functionality in IntelliJ IDEA is supported by the Database Tools and SQL plugin. The plugin provides support for all the features that are available in DataGrip, the standalone database management environment for developers. With the plugin, you can query, create and manage databases. Databases can work locally, on a server, or in the cloud. The plugin supports MySQL, PostgreSQL, Microsoft SQL Server, SQLite, MariaDB, Oracle, Apache Cassandra, and others. See the full list of supported vendors in Create a data source.Get started with database toolsThese topics are focused on describing features, functionality, the application interface, and might help you to get started quickly.GlossaryLearn the main concepts.Create a data sourceStart working with your database by creating a connection configuration for it.Run a queryConnect an SQL file to a data source and run your code.View resultsConnect an SQL file to a data source and run your code.Work with dataView and edit data in data editor.Export/ImportImport and export your data to and from various formats with or without special tools like mysqldump, pg_dump, pg_restore, or psql.DiagramsGenerate diagrams for database objects, build query and execution plans, generate flame graphs for EXPLAIN statements.TroubleshootingConnectivity issue is a very common definition and might include many problems. This troubleshooting article will guide you through the most common steps to identify and fix the issue. If you could not find the answer, try to"
65,contact JetBrains and we can try to help you.Last modified: 19 March 2024Tailwind CSSGlossary for the database management functionality
66,"NetBackup guides for Windows and UNIX, documents, download"
66,Veritas™ Services and Operations Readiness Tools (SORT)
66,Notifications
66,Help
66,Sign In
66,SORT
66,PRODUCTS
66,A - G
66,Access
66,Access Appliance
66,Backup Exec
66,Data Insight
66,eDiscovery Platform
66,Enterprise Vault
66,Flex Appliance
66,H - N
66,InfoScale and Storage Foundation HA
66,InfoScale for Kubernetes
66,InfoScale Operations Manager
66,NetBackup
66,NetBackup Appliances
66,NetBackup Flex Scale
66,NetBackup IT Analytics
66,O - Z
66,Resiliency Platform
66,SaaS Backup
66,Veritas Alta Archiving
66,Veritas Alta SaaS Protection
66,MY SORT
66,Dashboard
66,DashboardCustomize to quick access to SORT main features
66,For Login Users
66,"ReportsView,export and manage uploaded reports generated by SORT Data Collectors"
66,"SystemsView, compare host configurations"
66,"NotificationsSubscribe notifications for documents, patches etc"
66,"Shared GroupsCreate, manage groups to share uploaded reports"
66,DOWNLOADS
66,Tools
66,"SORT Data CollectorsAssess installation/upgrade, risks, product deployment"
66,Product UtilitiesUtilities for products
66,For NetBackup
66,NetBackup Deployment TemplatesDeployment templates for Chef & SCCM
66,NetBackup 2038 Conversion Time CheckerEstimate time for infinite images conversion
66,NetBackup UTF-8 Checker ToolIdentify if there are any invalid UTF-8 characters in database
66,For InfoScale
66,"High Availability AgentsFilter, view and download HA agents"
66,"ASL/APM/DDI/DDLFilter, view and download ASL/APM/DDI/DDL packages"
66,"Operations ManagerFilter, view and download VOM add-ons"
66,"Ansible ModulesAnsible modules, playbook"
66,"SCOM PacksFilter, view and download SCOM Packs"
66,InfoScale Secure Boot KeyPublic key
66,VxexplorerGather debugging information about systems for InfoScale
66,KNOWLEDGE BASE
66,Knowledge Base
66,"DocumentsFilter, view, download documents for all products"
66,Products and Platforms LookupsFind out what product versions can be installed on what platforms
66,Product EOSLFind out End of Support Life for all products
66,"Product FeaturesView, compare product features among versions"
66,"Installation and Upgrade ChecklistsOnline checklist, NetBackup, InfoScale, Storage Foundation"
66,"Risk Assessment ChecklistsConfiguration recommendations, NetBackup, InfoScale, Storage Foundation"
66,For NetBackup
66,"NetBackup Compatibility Lists and CheckersCompatibility list, database, operating system"
66,NetBackup Hotfix and EEB Release AuditorFind out the releases which the EEBs are included in
66,NetBackup Status CodeSearch the status code
66,NetBackup Future Platform and Feature PlansView the future plans for NetBackup
66,"For Backup Exec Software Compatibility List (SCL)Compatibility list for operating system, applications and databases, VMware and Hyper-V, simplified disaster recovery"
66,"Hardware Compatibility List (HCL)Compatibility list for cloud storage, Disk Targets - OST-Appliance, NDMP Platforms"
66,For InfoScale
66,"Hardware Compatibility List (HCL)Compatibility list for disk arrays, servers, switches"
66,Software Compatibility List (SCL)Compatibility list for database
66,"Kernel / SRU / TL LookupsFind out the certified kernel versions, SRUs and TLs"
66,License CalculatorsCore Plus
66,"Error CodesSearch UMIs, solutions"
66,Disk Group / Disk Layout VersionsFind out the disk groups versions and disk layout version for VxVM and VxFS
66,For Appliances
66,"Appliance Compatibility List (ACL)Compatibility list for NetBackup Appliances, Flex Appliance etc"
66,Appliance UMI Solution RequestRequest a solution for specified Appliance UMI
66,CONTACT US
66,HOME > KNOWLEDGE BASE > Documents > Document Details
66,"To use SORT, JavaScript must be enabled. How to enable JavaScript."
66,Windows and UNIX
66,ALL
66,English
66,Chinese (Simplified)
66,French
66,Japanese
66,Release: 10.0
66,© 2024 Veritas
66,Site Map
66,Third Party Notice
66,Privacy Policy
66,Terms of Service
66,Cookie Preferences
66,VERITAS
66,Help Guide
66,Report an issue about Veritas products or find entitlement? Contact Support
66,Interested in Veritas Products? Veritas Sales
66,Want to download patches? Download Center
66,Join Veritas Community? Veritas Open eXchange
66,"For SORT site issues, submit the form below"
66,Name:
66,* From (email address):
66,Comment:
66,Copy me
66,Type the numbers in the picture:
66,SEND
66,Username
66,Password
66,Forgot password?
66,Don't have an account? Create One.
66,Why Register?
66,"Get notifications about ASLs/APMs, HCLs, patches, and high availability agents"
66,"As a registered user, you can create notifications to receive updates about NetBackup Future Platform and Feature Plans, NetBackup hot fixes/EEBs in released versions, Array Support Libraries (ASLs)/Array Policy Modules (APMs), hardware compatibility lists (HCLs), patches and high availability agents. In addition, you can create system-specific notifications customized to your environment."
66,Compare configurations
66,"The Compare Configurations feature lets you compare different system scans by the data collector. When you sign in, you can choose a target system, compare reports run at different times, and easily see how the system's configuration has changed."
66,Save configurations
66,"After logging in, you can retrieve past reports, share reports with colleagues, review notifications you received, and retain custom settings. Anonymous users cannot access these features."
66,Bulk uploader
66,"As a registered user,you can upload multiple reports, using the Bulk Uploader."
67,DBeaver Documentation
67,Skip to content
67,Menu
67,Products
67,Download
67,DBeaver Lite
67,DBeaver Enterprise
67,DBeaver Ultimate
67,CloudBeaver Enterprise
67,Team Edition
67,DBeaver Lite
67,DBeaver Enterprise
67,DBeaver Ultimate
67,CloudBeaver Enterprise
67,Team Edition
67,Сompare products
67,Buy
67,Buy License
67,License types
67,DBeaver
67,CloudBeaver
67,Team Edition
67,Try for Free
67,Academic License
67,Partners
67,Become a sales partner
67,Resellers
67,Technology Partners
67,Universities
67,Company
67,About us
67,Our Clients
67,Contacts
67,Join our team
67,Resources
67,Documentation
67,DBeaver
67,CloudBeaver
67,Supported Databases
67,Use Cases
67,For Enterprise
67,For Data Analysis
67,For Software Development
67,For Database Administration
67,For Website Management
67,Cloud migration
67,DBeaver Blog
67,Video
67,Events
67,Sign in
67,Sign in
67,Sign up
67,Your cart is empty.
67,Search
67,DBeaver Documentation DOWNLOAD pdf
67,Version 24.1.EAVersion 24.0Version 23.3Version 23.2Version 23.1Version 23.0Version 22.3Version 22.2Version 22.1Version 22.0Version 21.3Version 21.2Version 21.1Version 21.0Version 7.3Version 7.2Version 7.1Version 7.0Version 6.3Version 6.2
67,Table
67,of Contents
67,DBeaver Desktop Documentation
67,General User Guide
67,Installation
67,Application Window Overview
67,Views
67,Database Navigator
67,Filter Database Objects
67,Configure Filters
67,Simple and Advanced View
67,Projects View
67,Project Explorer
67,Query Manager
67,Background Tasks
67,Database Object Editor
67,Data Editor
67,Navigation
67,Data View and Format
67,Data Filters
67,Data Refresh
67,Data Viewing and Editing
67,Panels
67,Managing Charts
67,Data Search
67,Data transfer
67,SQL Generation
67,Working with spatial/GIS data
67,Working with XML and JSON
67,Managing Data Formats
67,Virtual column expressions
67,Properties Editor
67,ER Diagrams
67,Database Structure Diagrams
67,Custom Diagrams
67,Edit mode
67,SQL Editor
67,Toolbar Customization
67,SQL Templates
67,SQL Assist and Auto-Complete
67,AI Smart Assistance (ChatGPT)
67,SQL Formatting
67,SQL Execution
67,SQL Terminal
67,Variables panel
67,Query Execution Plan
67,Visual Query Builder
67,Script Management
67,Client Side Commands
67,Export Command
67,Debug
67,PostgreSQL Debugger
67,Search
67,File Search
67,DB Full-Text Search
67,DB Metadata Search
67,Schema compare
67,Using Liquibase in DBeaver
67,Data compare
67,MockData generation
67,Spelling
67,"Dashboards, DB monitoring"
67,Projects
67,Project security
67,Team work (Git)
67,Managing Master password
67,Security in PRO products
67,Certificate Management
67,Bookmarks
67,Shortcuts
67,AccessibilityDatabase Management
67,Sample Database
67,Database Connections
67,Edit Connection
67,Invalidate/Reconnect to Database
67,Disconnect from Database
67,Change current user password
67,Advanced settings
67,SSH Configuration
67,Proxy configuration
67,Kubernetes configuration
67,Kerberos authentication
67,Cloud configuration settings
67,AWS Credentials
67,AWS SSO
67,AWS Permissions
67,GCP Credentials
67,GCP SSO
67,Local Client Configuration
67,Connection Types
67,Configure Connection Initialization Settings
67,Tableau integration
67,Transactions
67,Auto and Manual Commit Modes
67,Transaction Log
67,Pending transactions
67,Drivers
67,Database drivers
67,JDBC-ODBC bridge
67,Tasks
67,Data export/import
67,Data migration
67,Data Import and Replace
67,Database backup/restore
67,Task management
67,Task scheduler
67,Composite tasks
67,Sending results by e-mail
67,Uploading result to external storage
67,Cloud Explorer
67,AWS
67,Azure
67,Google
67,Cloud File Explorer DBeaver PRO
67,Enterprise EditionDatabases support
67,Classic
67,Apache Hive/Spark/Impala
67,Cassandra
67,ClickHouse
67,Couchbase
67,Greenplum
67,IBM Db2
67,InfluxDB
67,MariaDB
67,Microsoft SQL Server
67,MongoDB
67,MySQL
67,Oracle
67,PostgreSQL
67,Redis
67,Salesforce
67,SQLite
67,Teradata
67,Cloud
67,AWS
67,Athena
67,DocumentDB
67,DynamoDB
67,Keyspaces
67,Redshift
67,Timestream
67,Azure
67,CosmosDB
67,Databricks
67,Google
67,AlloyDB for PostgreSQL
67,BigQuery
67,Bigtable
67,Cloud SQL for MySQL
67,Cloud SQL for PostgreSQL
67,Cloud SQL for SQL Server
67,Spanner
67,SnowflakeCustomizing DBeaver
67,Changing interface language
67,"Installing extensions - Themes, version control, etc"
67,User Interface ThemesTroubleshooting
67,Command Line
67,Reset UI settings
67,Reset workspace
67,Troubleshooting system issues
67,Posting issues
67,Log files
67,JDBC trace
67,Thread dumpAdmin Guide
67,Managing connections
67,Managing variables
67,Managing drivers
67,Managing preferences
67,Managing restrictions
67,Windows Silent InstallLicense management
67,License Administration
67,How to Import License
67,How to Reassign License Tutorials
67,Connecting to Oracle Database using JDBC OCI driver
67,Importing CA certificates from your local Java into DBeaver
67,SSL configuration
67,How to set a variable if dbeaver.ini is read-only
67,New Table Creation
67,Creating columns
67,Implementing Constraints
67,Utilizing Foreign-Keys
67,Creating Indexes
67,Incorporating Triggers
67,Oracle
67,Table of contentsOverviewSetting UpOracle connection settingsConnection detailsOracle driver propertiesSecure Connection ConfigurationsPowering Oracle with DBeaverOracle database objectsOracle Features in DBeaver
67,Overview
67,This guide provides instructions for setting up and using Oracle with DBeaver.
67,"Before you start, you must create a connection in DBeaver and select Oracle. If you haven't done this, please refer to"
67,our Database Connection article.
67,DBeaver interacts with the Oracle server using a specific driver. It also supports Oracle cloud database extensions such
67,"as Oracle Cloud JSON and Oracle NetSuite, both of which can be connected through Cloud Explorer."
67,Setting Up
67,This section provides an overview of DBeaver's settings for establishing a direct connection and the
67,"configuration of secure connections using SSH, proxies, and the driver setup for Oracle."
67,Oracle connection settings
67,"In this subsection, we'll outline the settings for establishing a direct connection to a Oracle database using DBeaver."
67,Correctly configuring your connection ensures seamless interaction between DBeaver and your Oracle database.
67,1) The first page of the connection settings requires you to fill in specific fields to establish the initial connection.
67,Field
67,Description
67,"Host If you're connecting via host, enter the host address of your Oracle database here."
67,Database Enter the name of the Oracle database you want to connect to.
67,Port Enter the port number for your Oracle database. The default Oracle port is 1521.
67,Connection identifiers Choose whether you want to connect using a Service name or a SID identifier.
67,"Authentication Choose the type of authentication you want to use for the connection. For detailed guides on authentication types, please refer to the following articles:"
67,- Oracle Database Native
67,- OS Authentication
67,- DBeaver Profile Authentication
67,- Oracle Kerberos Authentication
67,- Oracle Wallet
67,You can also read about security in DBeaver PRO.
67,"Role Choose whether you want to connect using a Normal, SYSDABA or a SYSOPER role. For more details, you can refer to the official Administering User Accounts and Security article."
67,Connection Details Provide additional connection details if necessary.
67,Driver Name This field will be auto-filled based on your selected driver type.
67,"Driver Settings If there are any specific driver settings, configure them here."
67,2) The second page of the connection settings offers additional options that allow you to customize your further
67,connection to the Oracle database.
67,Field
67,Description
67,Language Specify the Session Language.
67,Territory Specify the Session Territory.
67,NLS Date Format Specify NLS (National Language Support) Date Format. The default value of the NLS_DATE_FORMAT parameter is determined by the TERRITORY parameter.
67,NLS Timestamp Format Specify NLS Timestamp format. The default value of the NLS_TIMESTAMP_FORMAT parameter is determined by the TERRITORY parameter.
67,"Length semantics specify the length semantics for VARCHAR2 and CHAR table columns, user-defined object attributes, and PL/SQL variables in database objects created in the session. The length semantics can be specified as either BYTE or CHAR."
67,Currency symbol Specify the Currency symbol.You can find out which currency symbol your current session uses by querying the V$NLS_PARAMETERS view.
67,Show only connected user schema Show only a scheme of a connected user in the Database Navigator.
67,Hide empty schemas Check existence of objects within schema and do not show empty schemas in tree. Enabled by default but it may cause performance problems on databases with very big number of objects.
67,Always show DBA objects Always shows DBA-related metadata objects in tree even if user do not has DBA role.
67,*Always use `DBA_ views**
67,| Use DBA* views instead of ALL*` views wherever it is possible
67,Use SYS schema prefix Use SYS schema prefix in all metadata queries. Otherwise use view names without explicit schema.
67,Simple constraint reading query Use simple metadata queries. May work slower but it is more stable for all Oracle versions.
67,Use UNION for table metadata reading Use legacy table metadata query. With UNION instead JOIN. It helps in some cases speed up reading of table data.
67,Search metadata in synonyms Search for metadata in synonyms among other places. May significantly slow down metadata search as well as autocompletion.
67,Use RULE hint for system catalog queries Adds RULE hint for some system catalog queries (like columns and constraints reading).It significantly increases performance on some Oracle databases (and decreases on others).
67,"Show DATE values specifically as DATE Show DATE data type values specifically as DATE, not as a TIMESTAMP.This setting will not work with disabled date/time formatting."
67,Use metadata queries optimizer Use metadata queries optimizer. May significantly improve metadata reading performance on some systems.
67,Connection details
67,The Connection Details section in DBeaver allows to customize your experience while working with Oracle database. This includes
67,"options for adjusting the Navigator View, setting up Security measures, applying Filters, configuring Connection"
67,"Initialization settings, and setting up Shell Commands. Each of these settings can significantly impact your database"
67,"operations and workflow. For detailed guides on these settings, please refer to the following articles:"
67,Connection Details Configuration
67,Database Navigator
67,Security Settings Guide
67,Filters Settings Guide
67,Connection Initialization Settings Guide
67,Shell Commands Guide
67,Oracle driver properties
67,The settings for Oracle Driver properties enable you to adjust the performance of the Oracle driver.
67,"These adjustments can influence the efficiency, compatibility, and features of your Oracle database."
67,"For a complete walkthrough on setting up Oracle JDBC driver properties, you can refer to the official"
67,Oracle JDBC documentation.
67,These guide detail each driver's properties and how they can be used to optimize Oracle database connections.
67,"You can customize the Oracle driver in DBeaver via the Edit Driver page, accessible by clicking on the Driver"
67,Settings button on the first page of the driver settings. This page offers a range of settings that can influence your
67,"Oracle database connections. For a comprehensive guide on these settings, please refer to our Database drivers article."
67,Secure Connection Configurations
67,"DBeaver supports secure connections to your Oracle database. Guidance on configuring such connections, specifically"
67,"SSH, SSL, Proxy and Kubernetes connections, can be found in various referenced articles. For a comprehensive understanding, please"
67,refer to these articles:
67,SSH Configuration.
67,SSL Configuration
67,Proxy Configuration.
67,Kubernetes Configuration.
67,Powering Oracle with DBeaver
67,DBeaver provides a host of features designed for Oracle databases. This includes the ability to view and manage
67,"databases, along with numerous unique capabilities aimed at optimizing database operations."
67,Oracle database objects
67,DBeaver lets you view and manipulate a wide range of Oracle database objects. DBeaver has extensive support for
67,"various Oracle metadata types, allowing you to interact with a wide variety of database objects, such as:"
67,Schemas
67,Tables
67,Columns
67,Constraints
67,Foreign Keys
67,References
67,Triggers
67,Indexes
67,Partitions
67,Dependencies
67,Views
67,Materialized Views
67,Indexes
67,Sequences
67,Queues
67,Types
67,Packages
67,Procedures
67,Functions
67,Synonyms
67,Schema Triggers
67,Table Triggers
67,Database Links
67,Java
67,Jobs
67,Scheduler
67,Jobs
67,Programs
67,Recycle bin
67,Global metadata
67,Types
67,Public Synonyms
67,Public Database Links
67,User Recycle bin
67,Storage
67,Tablespaces
67,Files
67,Objects
67,Security
67,Users
67,Roles
67,Profiles
67,Administer
67,Session Manager
67,Locks Manager
67,Oracle Features in DBeaver
67,DBeaver isn't limited to typical SQL tasks. It also includes numerous unique features specifically for Oracle.
67,"Beyond regular SQL operations, DBeaver provides a range of Oracle-specific capabilities, such as:"
67,Category
67,Feature
67,Data Types
67,Oracle Nested Tables
67,PL/SQL Support
67,PL/SQL Procedures
67,PL/SQL Functions
67,Security
67,Oracle Permissions
67,Oracle Profiles
67,Oracle Roles
67,Data Organization
67,Oracle Partitions
67,Database Management Oracle Dependencies
67,Performance Tuning
67,Oracle Performance Reports
67,Oracle Execution Plans
67,"Additional features compatible with Oracle, but not exclusive to it:"
67,Category
67,Feature
67,Data Transfer
67,Data Import
67,Data Export
67,Session Management
67,Session Manager
67,Backup and Restore
67,How to Backup/Restore data
67,Schema Management
67,Schema Compare
67,Data Visualization
67,GIS Guide
67,ERD Guide
67,Your email
67,Your issue
67,Did we resolve your issue?
67,"Yes No, get help"
67,Facebook
67,LinkedIn
67,Telegram
67,Reddit
67,Twitter
67,YouTube
67,Products
67,Download
67,DBeaver Lite
67,DBeaver Enterprise
67,DBeaver Ultimate
67,CloudBeaver Enterprise
67,Team Edition
67,Сompare products
67,Buy
67,Buy License
67,License types
67,Try for Free
67,Academic License
67,Partners
67,Become a sales partner
67,Resellers
67,Technology Partners
67,Universities
67,Company
67,About us
67,Our Clients
67,Contacts
67,Join our team
67,Resources
67,Documentation
67,Supported Databases
67,Use Cases
67,DBeaver Blog
67,Video
67,Events
67,PrivacyService Level AgreementEnterprise Software AgreementEULA
69,Joining Multiple Tables in SQL: An Comprehensive How-to Guide
69,FeaturesDatabasesPricingBuyWhat's newSupportDownload for free
69,JOIN
69,How to Join Three or More Tables with SQL
69,Author:
69,Leslie S. Gyamfi
69,Length:
69,7 MINS
69,Type:
69,GUIDE
69,Published:
69,2023-07-24introThis article explains how to join three or more tables with SQL and how DbVisualizer can help you to achieve the goal - have a read!
69,Tools used in the tutorial
69,Tool
69,Description
69,Link
69,DBVISUALIZER
69,TOP RATED DATABASE MANAGEMENT TOOL AND SQL CLIENT
69,DOWNLOAD
69,"Joining Multiple Tables in SQL: An Comprehensive How-to Guide.PrefaceAs a data analyst or developer, understanding how to join multiple tables with SQL is a critical skill for effectively querying and manipulating data. In many cases, data is stored across multiple tables in a relational database, and combining this data requires joining the tables together based on a common key or set of keys. This process of joining tables allows for more complex queries and powerful insights, as well as the ability to create views and reports that span multiple sources of data. This article will guide you through joining multiple tables with DbVisualizer.Getting StartedLet’s get started with using DbVisualizer to connect to our server by following the step-by-step guide below:Step 1: Connecting To The Database ServerFirstly, we'll need to employ DbVisualizer to establish a connection to our database server. In today’s lesson, we’ll be using the Postgres database server. We can establish a connection to a database server in DbVisualizer by creating a new database connection from the ""Create Database Connection"" menu, selecting a driver for your database, entering the connection details such as the name, database type, server IP, port, user account, and password and clicking ""Connect"" to access the database as shown in the image below:"
69,"Connecting to a Database Server in DbVisualizer.Once, you have successfully created the database connection, a list of all Postgres database connections will appear on the left pane of the DbVisualizer window as shown here:"
69,"A List of All Databases.Step 2: Creating The DatabaseNow that we’ve initiated a connection to the database server, it’s time to create a new database for this particular tutorial. This can be done by right-clicking on the database node and selecting ‘Create database’ from the menu as shown below:"
69,"Creating a Database.Pretty easy right? Let’s continue by creating the database table that will be very useful in this tutorial.Step 3: Creating The Database TableOnce you connect to the database server and create a database, you will need to create tables within that database.All you’ll need to do is to navigate to PostgreSQL Connection ->Databases ->Tutorial(name of the database created) -> Schema-> public-> Tables, then right-click and select the ‘Create Table’ option as shown in the image below:"
69,"Creating a Table within DbVisualizer.The “Create Table” dialog will be displayed after clicking the “Create Table” option. Next, give your database a name and add columns by using the ‘+’ button located on the right side of the dialog. Once you’re done customizing your table, it's time to execute it by clicking the ‘Execute’ button."
69,The ‘Create Table’ Dialog.You’ll now be able to find the created table in the list of tables in your database by opening the ‘Tables’ tree in the database.
69,"The Created Table.Step 4: Importing Table DataSince we’re going to deal with data in this tutorial, let us import a dataset we can work with in this tutorial. To import the data into your newly created table, select the ""Import Table Data"" option from the right-click menu of the ‘employees’ table node. An import wizard will come up, allowing you to import data from any source, including Excel and CSV files."
69,"Importing Data Into a Table.Next, import the data by selecting the data file from your computer as shown below:"
69,"The Import Wizard.Navigate through the import wizard by continuously clicking on the ""Next"" button until you reach the final window. Now, initiate the import process by clicking on the ""Import"" button. If all goes well, you will find a success indicator in the import log indicating that your data has been imported successfully."
69,The Import Process Completed Successfully.Great! Your database will now be populated with data from the CSV file you imported and would be displayed in DbVisualizer as shown below:
69,"The Imported Data.Since we’re talking about three or more tables, go on to create three tables and import data into each of them for the purpose of this tutorial by following the exact same steps we have employed. This should be a hitch-free process, so don’t fret! In my case, I have added two extra tables named ‘department’ and ‘detail’ as shown."
69,"‘Department’ and ‘Detail’ Tables.Joining The Tables with SQLNow that our database has more than two populated tables, let us learn how to join our tables. By harnessing the power of SQL, we have the ability to create queries that would join these tables together. Let’s look at how we can achieve this:1. Using JOINSFrom the tables created above, we want to join the three tables together. Let us, therefore, build a query that will join the three tables by first selecting whatever column we would want to print, joining the tables ‘employees’ and ‘details’ based on the common ‘employee_id’ attribute, and then finally joining the ‘department’ table to the ‘details’ table based on the common ‘department_id’ attribute."
69,Copy
69,"SELECT employee_name, department_name, manager_contact, salary"
69,FROM employees e
69,INNER JOIN details d
69,ON e.employees_id = d.employee_id
69,INNER JOIN department dp
69,ON dp.department_id = d.department_id;
69,Running the query above will provide you with the results seen in the table below:
69,"Joined tables with SQL.Great! We have successfully joined three tables with a JOIN. But that’s not all, let us try out another method known as the Parent-Child relationship method.2. Using the Parent-Child RelationshipThe parent-child relationship method of joining three tables involves using a common column that acts as a parent in one table and a child in another table to join the tables together."
69,Copy
69,"SELECT employee_name, department_name, manager_contact, salary"
69,"FROM employees e, details d, department dp"
69,WHERE e.employees_id = d.employee_id AND
69,d.department_id = dp.department_id;
69,"The SQL query uses a parent-child relationship to join three tables: employees, details, and department. The employees table is the parent table, the details table is the child table while the department table is another table being joined.The employee_id column in the employees table and the details table acts as the parent and child column, respectively. Similarly, the department_id column in the details table and the department table act as the parent and child columns, respectively.The WHERE clause specifies the conditions for joining the tables based on the matching values of the employee_id and department_id columns in the employees, details, and department tables.Running the query above will provide you with the results seen in DbVisualizer below:"
69,"An Output of the Query for the Parent-Child Technique.Pretty cool yeah? You can also join three or more tables by using subqueries. Let’s look at that in the section below.3. Using SubqueriesTo join three or more tables using a subquery, you can use a nested SELECT statement to retrieve data from one table based on the values in another table, and then join the results of the subquery with the remaining tables in the outer SELECT statement.For an example showing how to join three or more tables with subqueries, let us build a subquery that will retrieve the department_id, department_name, and manager_contact columns from the department table. The subquery is then joined with the details table on the department_id column, and the resulting table is joined with the employees table on the employee_id column."
69,Copy
69,"SELECT e.employee_name, d.department_name, d.manager_contact, dt.salary"
69,FROM employees e
69,INNER JOIN details dt
69,ON e.employees_id = dt.employee_id
69,INNER JOIN (
69,"SELECT department_id, department_name, manager_contact"
69,FROM department
69,) d
69,ON dt.department_id = d.department_id;
69,Running the query above will provide you with the results seen in DbVisualizer below:
69,"The Subquery Technique Query Output.ConclusionIn this article, we delved into the process and essence of setting up a database connection. We also explored the various options for joining multiple tables from using JOINs, to using the Parent-Child relationship method and to using Subqueries.By following the instructions outlined in this article, you'll be able to make meaningful deductions from several datasets within a span of tables that will help you understand your data better and communicate your findings to others.Now that you have a grasp of this concept, it's time to put your newfound knowledge to test. However, do note that joining multiple tables with SQL is not everything you need to know. While SQL is a powerful tool for querying and manipulating data, there are many other aspects of database management that are equally important, including data modeling, database design, and performance tuning which can all be tackled effortlessly using SQL clients like DbVisualizer.FAQsWhat are some best practices for joining multiple tables with SQL?Some of the best practices for joining multiple tables with SQL include: using aliases for table names to improve readability, specifying join conditions explicitly to avoid ambiguous unexpected results, avoiding unnecessary joins or subqueries to improve performance, using appropriate indexing to speed up query execution, and testing the query with sample data before running it on production data to ensure accuracy.What is the advantage of using subqueries to join multiple tables with SQL?Subqueries can be useful when joining tables that have complex or nested relationships, or when you need to filter or group data based on values in another table. They can also help simplify the query and make it easier to read and understand.Why do we need to join multiple tables with SQL?Joining multiple tables with SQL allows us to combine data from different tables and create more meaningful and useful insights. Joining tables can help identify and resolve data inconsistencies and ensure that the data is accurate."
69,About the author
69,Leslie S. Gyamfi
69,Leslie Gyamfi is a mobile/web app developer with a passion for creating innovative solutions. He is dedicated to delivering high-quality products and technical articles. You can connect with him on LinkedIn
69,Sign up to receive The Table's roundup
69,Submit ->
69,"By submitting this form, I agree to the DbVis Privacy Policy"
69,Submit ->
69,More from the table
69,Title
69,Author
69,Tags
69,Length
69,Published
69,title
69,Outer Join in SQL: A Comprehensive Guide
69,author
69,Ochuko Onojakpor
69,tags
69,JOIN
69,11 min
69,2024-02-19
69,title
69,Inner Join in SQL: A Comprehensive Guide
69,author
69,Ochuko Onojakpor
69,tags
69,JOIN
69,10 MINS
69,2023-11-30
69,title
69,Understanding Self Joins in SQL
69,author
69,Bonnie
69,tags
69,JOIN
69,5 MINS
69,2023-10-12
69,title
69,SQL Cheat Sheet: The Ultimate Guide to All Types of SQL JOINS
69,author
69,Leslie S. Gyamfi
69,tags
69,JOIN
69,12 MINS
69,2023-07-31
69,title
69,How to join your tables using ERD
69,author
69,Scott A. Adams
69,tags
69,ERD
69,JOIN
69,8 MINS
69,2021-12-21
69,title
69,INSERT INTO SQL Clause
69,author
69,Leslie S. Gyamfi
69,tags
69,INSERT
69,SQL
69,6 min
69,2024-03-18
69,title
69,Enhancing Business Operations with Visualization Tools
69,author
69,TheTable
69,tags
69,Data Visualization Tools
69,3 min
69,2024-03-15
69,title
69,Postgres TEXT vs VARCHAR: Comparing String Data Types
69,author
69,Antonello Zanini
69,tags
69,POSTGRESQL
69,TEXT
69,VARCHAR
69,6 min
69,2024-03-14
69,title
69,ALTER TABLE ADD COLUMN in SQL: A Comprehensive Guide
69,author
69,TheTable
69,tags
69,SQL
69,5 min
69,2024-03-12
69,title
69,Schemas in PostgreSQL
69,author
69,Leslie S. Gyamfi
69,tags
69,POSTGRESQL
69,SCHEMA
69,6 min
69,2024-03-11
69,"Read more from theTableThe content provided on dbvis.com/thetable, including but not limited to code and examples, is intended for educational and informational purposes only. We do not make any warranties or representations of any kind. Read more here.ENGINEERED IN NACKA, SWEDEN.Copyright 2024.ProductDownloadFeature listDatabase featuresSupportReleasesPartnersResellersPartnersLegalLegal termsPrivacy PolicyCookie PolicySecure SoftwareEULACompanyTheTable BlogOur storyWork with usContact usBrand assetsSocialTwitterFacebookLinkedInENGINEERED IN NACKA, SWEDEN.Copyright 2024."
69,Close ✗
69,Cookie policyWe use cookies to ensure that we give you the best experience on our website. However you can change your cookie settings at any time in your browser settings. Please find our cookie policy here ↗
69,Cookies are fine
70,Example: Deploying WordPress and MySQL with Persistent Volumes | Kubernetes
70,Example: Deploying WordPress and MySQL with Persistent Volumes | KubernetesDocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsRelease Information
70,v1.29
70,v1.28
70,v1.27
70,v1.26
70,v1.25English中文 (Chinese)
70,日本語 (Japanese)
70,한국어 (Korean)
70,"KubeCon + CloudNativeCon Europe 2024Join us for four days of incredible opportunities to collaborate, learn and share with the cloud native community.Buy your ticket now! 19 - 22 March | Paris, France"
70,Documentation
70,Available Documentation Versions
70,Getting started
70,Learning environment
70,Production environment
70,Container Runtimes
70,Installing Kubernetes with deployment tools
70,Bootstrapping clusters with kubeadm
70,Installing kubeadm
70,Troubleshooting kubeadm
70,Creating a cluster with kubeadm
70,Customizing components with the kubeadm API
70,Options for Highly Available Topology
70,Creating Highly Available Clusters with kubeadm
70,Set up a High Availability etcd Cluster with kubeadm
70,Configuring each kubelet in your cluster using kubeadm
70,Dual-stack support with kubeadm
70,Turnkey Cloud Solutions
70,Best practices
70,Considerations for large clusters
70,Running in multiple zones
70,Validate node setup
70,Enforcing Pod Security Standards
70,PKI certificates and requirements
70,Concepts
70,Overview
70,Objects In Kubernetes
70,Kubernetes Object Management
70,Object Names and IDs
70,Labels and Selectors
70,Namespaces
70,Annotations
70,Field Selectors
70,Finalizers
70,Owners and Dependents
70,Recommended Labels
70,Kubernetes Components
70,The Kubernetes API
70,Cluster Architecture
70,Nodes
70,Communication between Nodes and the Control Plane
70,Controllers
70,Leases
70,Cloud Controller Manager
70,About cgroup v2
70,Container Runtime Interface (CRI)
70,Garbage Collection
70,Mixed Version Proxy
70,Containers
70,Images
70,Container Environment
70,Runtime Class
70,Container Lifecycle Hooks
70,Workloads
70,Pods
70,Pod Lifecycle
70,Init Containers
70,Sidecar Containers
70,Ephemeral Containers
70,Disruptions
70,Pod Quality of Service Classes
70,User Namespaces
70,Downward API
70,Workload Management
70,Deployments
70,ReplicaSet
70,StatefulSets
70,DaemonSet
70,Jobs
70,Automatic Cleanup for Finished Jobs
70,CronJob
70,ReplicationController
70,Autoscaling Workloads
70,Managing Workloads
70,"Services, Load Balancing, and Networking"
70,Service
70,Ingress
70,Ingress Controllers
70,Gateway API
70,EndpointSlices
70,Network Policies
70,DNS for Services and Pods
70,IPv4/IPv6 dual-stack
70,Topology Aware Routing
70,Networking on Windows
70,Service ClusterIP allocation
70,Service Internal Traffic Policy
70,Storage
70,Volumes
70,Persistent Volumes
70,Projected Volumes
70,Ephemeral Volumes
70,Storage Classes
70,Volume Attributes Classes
70,Dynamic Volume Provisioning
70,Volume Snapshots
70,Volume Snapshot Classes
70,CSI Volume Cloning
70,Storage Capacity
70,Node-specific Volume Limits
70,Volume Health Monitoring
70,Windows Storage
70,Configuration
70,Configuration Best Practices
70,ConfigMaps
70,Secrets
70,Resource Management for Pods and Containers
70,Organizing Cluster Access Using kubeconfig Files
70,Resource Management for Windows nodes
70,Security
70,Cloud Native Security
70,Pod Security Standards
70,Pod Security Admission
70,Service Accounts
70,Pod Security Policies
70,Security For Windows Nodes
70,Controlling Access to the Kubernetes API
70,Role Based Access Control Good Practices
70,Good practices for Kubernetes Secrets
70,Multi-tenancy
70,Hardening Guide - Authentication Mechanisms
70,Kubernetes API Server Bypass Risks
70,Security Checklist
70,Policies
70,Limit Ranges
70,Resource Quotas
70,Process ID Limits And Reservations
70,Node Resource Managers
70,"Scheduling, Preemption and Eviction"
70,Kubernetes Scheduler
70,Assigning Pods to Nodes
70,Pod Overhead
70,Pod Scheduling Readiness
70,Pod Topology Spread Constraints
70,Taints and Tolerations
70,Scheduling Framework
70,Dynamic Resource Allocation
70,Scheduler Performance Tuning
70,Resource Bin Packing
70,Pod Priority and Preemption
70,Node-pressure Eviction
70,API-initiated Eviction
70,Cluster Administration
70,Certificates
70,Cluster Networking
70,Logging Architecture
70,Metrics For Kubernetes System Components
70,System Logs
70,Traces For Kubernetes System Components
70,Proxies in Kubernetes
70,API Priority and Fairness
70,Installing Addons
70,Windows in Kubernetes
70,Windows containers in Kubernetes
70,Guide for Running Windows Containers in Kubernetes
70,Extending Kubernetes
70,"Compute, Storage, and Networking Extensions"
70,Network Plugins
70,Device Plugins
70,Extending the Kubernetes API
70,Custom Resources
70,Kubernetes API Aggregation Layer
70,Operator pattern
70,Tasks
70,Install Tools
70,Install and Set Up kubectl on Linux
70,Install and Set Up kubectl on macOS
70,Install and Set Up kubectl on Windows
70,Administer a Cluster
70,Administration with kubeadm
70,Certificate Management with kubeadm
70,Configuring a cgroup driver
70,Reconfiguring a kubeadm cluster
70,Upgrading kubeadm clusters
70,Upgrading Linux nodes
70,Upgrading Windows nodes
70,Changing The Kubernetes Package Repository
70,Migrating from dockershim
70,Changing the Container Runtime on a Node from Docker Engine to containerd
70,Migrate Docker Engine nodes from dockershim to cri-dockerd
70,Find Out What Container Runtime is Used on a Node
70,Troubleshooting CNI plugin-related errors
70,Check whether dockershim removal affects you
70,Migrating telemetry and security agents from dockershim
70,Generate Certificates Manually
70,"Manage Memory, CPU, and API Resources"
70,Configure Default Memory Requests and Limits for a Namespace
70,Configure Default CPU Requests and Limits for a Namespace
70,Configure Minimum and Maximum Memory Constraints for a Namespace
70,Configure Minimum and Maximum CPU Constraints for a Namespace
70,Configure Memory and CPU Quotas for a Namespace
70,Configure a Pod Quota for a Namespace
70,Install a Network Policy Provider
70,Use Antrea for NetworkPolicy
70,Use Calico for NetworkPolicy
70,Use Cilium for NetworkPolicy
70,Use Kube-router for NetworkPolicy
70,Romana for NetworkPolicy
70,Weave Net for NetworkPolicy
70,Access Clusters Using the Kubernetes API
70,Advertise Extended Resources for a Node
70,Autoscale the DNS Service in a Cluster
70,Change the Access Mode of a PersistentVolume to ReadWriteOncePod
70,Change the default StorageClass
70,Switching from Polling to CRI Event-based Updates to Container Status
70,Change the Reclaim Policy of a PersistentVolume
70,Cloud Controller Manager Administration
70,Configure a kubelet image credential provider
70,Configure Quotas for API Objects
70,Control CPU Management Policies on the Node
70,Control Topology Management Policies on a node
70,Customizing DNS Service
70,Debugging DNS Resolution
70,Declare Network Policy
70,Developing Cloud Controller Manager
70,Enable Or Disable A Kubernetes API
70,Encrypting Confidential Data at Rest
70,Decrypt Confidential Data that is Already Encrypted at Rest
70,Guaranteed Scheduling For Critical Add-On Pods
70,IP Masquerade Agent User Guide
70,Limit Storage Consumption
70,Migrate Replicated Control Plane To Use Cloud Controller Manager
70,Namespaces Walkthrough
70,Operating etcd clusters for Kubernetes
70,Reserve Compute Resources for System Daemons
70,Running Kubernetes Node Components as a Non-root User
70,Safely Drain a Node
70,Securing a Cluster
70,Set Kubelet Parameters Via A Configuration File
70,Share a Cluster with Namespaces
70,Upgrade A Cluster
70,Use Cascading Deletion in a Cluster
70,Using a KMS provider for data encryption
70,Using CoreDNS for Service Discovery
70,Using NodeLocal DNSCache in Kubernetes Clusters
70,Using sysctls in a Kubernetes Cluster
70,Utilizing the NUMA-aware Memory Manager
70,Verify Signed Kubernetes Artifacts
70,Configure Pods and Containers
70,Assign Memory Resources to Containers and Pods
70,Assign CPU Resources to Containers and Pods
70,Configure GMSA for Windows Pods and containers
70,Resize CPU and Memory Resources assigned to Containers
70,Configure RunAsUserName for Windows pods and containers
70,Create a Windows HostProcess Pod
70,Configure Quality of Service for Pods
70,Assign Extended Resources to a Container
70,Configure a Pod to Use a Volume for Storage
70,Configure a Pod to Use a PersistentVolume for Storage
70,Configure a Pod to Use a Projected Volume for Storage
70,Configure a Security Context for a Pod or Container
70,Configure Service Accounts for Pods
70,Pull an Image from a Private Registry
70,"Configure Liveness, Readiness and Startup Probes"
70,Assign Pods to Nodes
70,Assign Pods to Nodes using Node Affinity
70,Configure Pod Initialization
70,Attach Handlers to Container Lifecycle Events
70,Configure a Pod to Use a ConfigMap
70,Share Process Namespace between Containers in a Pod
70,Use a User Namespace With a Pod
70,Create static Pods
70,Translate a Docker Compose File to Kubernetes Resources
70,Enforce Pod Security Standards by Configuring the Built-in Admission Controller
70,Enforce Pod Security Standards with Namespace Labels
70,Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller
70,"Monitoring, Logging, and Debugging"
70,Troubleshooting Applications
70,Debug Pods
70,Debug Services
70,Debug a StatefulSet
70,Determine the Reason for Pod Failure
70,Debug Init Containers
70,Debug Running Pods
70,Get a Shell to a Running Container
70,Troubleshooting Clusters
70,Troubleshooting kubectl
70,Resource metrics pipeline
70,Tools for Monitoring Resources
70,Monitor Node Health
70,Debugging Kubernetes nodes with crictl
70,Auditing
70,Debugging Kubernetes Nodes With Kubectl
70,Developing and debugging services locally using telepresence
70,Windows debugging tips
70,Manage Kubernetes Objects
70,Declarative Management of Kubernetes Objects Using Configuration Files
70,Declarative Management of Kubernetes Objects Using Kustomize
70,Managing Kubernetes Objects Using Imperative Commands
70,Imperative Management of Kubernetes Objects Using Configuration Files
70,Update API Objects in Place Using kubectl patch
70,Managing Secrets
70,Managing Secrets using kubectl
70,Managing Secrets using Configuration File
70,Managing Secrets using Kustomize
70,Inject Data Into Applications
70,Define a Command and Arguments for a Container
70,Define Dependent Environment Variables
70,Define Environment Variables for a Container
70,Expose Pod Information to Containers Through Environment Variables
70,Expose Pod Information to Containers Through Files
70,Distribute Credentials Securely Using Secrets
70,Run Applications
70,Run a Stateless Application Using a Deployment
70,Run a Single-Instance Stateful Application
70,Run a Replicated Stateful Application
70,Scale a StatefulSet
70,Delete a StatefulSet
70,Force Delete StatefulSet Pods
70,Horizontal Pod Autoscaling
70,HorizontalPodAutoscaler Walkthrough
70,Specifying a Disruption Budget for your Application
70,Accessing the Kubernetes API from a Pod
70,Run Jobs
70,Running Automated Tasks with a CronJob
70,Coarse Parallel Processing Using a Work Queue
70,Fine Parallel Processing Using a Work Queue
70,Indexed Job for Parallel Processing with Static Work Assignment
70,Job with Pod-to-Pod Communication
70,Parallel Processing using Expansions
70,Handling retriable and non-retriable pod failures with Pod failure policy
70,Access Applications in a Cluster
70,Deploy and Access the Kubernetes Dashboard
70,Accessing Clusters
70,Configure Access to Multiple Clusters
70,Use Port Forwarding to Access Applications in a Cluster
70,Use a Service to Access an Application in a Cluster
70,Connect a Frontend to a Backend Using Services
70,Create an External Load Balancer
70,List All Container Images Running in a Cluster
70,Set up Ingress on Minikube with the NGINX Ingress Controller
70,Communicate Between Containers in the Same Pod Using a Shared Volume
70,Configure DNS for a Cluster
70,Access Services Running on Clusters
70,Extend Kubernetes
70,Configure the Aggregation Layer
70,Use Custom Resources
70,Extend the Kubernetes API with CustomResourceDefinitions
70,Versions in CustomResourceDefinitions
70,Set up an Extension API Server
70,Configure Multiple Schedulers
70,Use an HTTP Proxy to Access the Kubernetes API
70,Use a SOCKS5 Proxy to Access the Kubernetes API
70,Set up Konnectivity service
70,TLS
70,Configure Certificate Rotation for the Kubelet
70,Manage TLS Certificates in a Cluster
70,Manual Rotation of CA Certificates
70,Manage Cluster Daemons
70,Perform a Rolling Update on a DaemonSet
70,Perform a Rollback on a DaemonSet
70,Running Pods on Only Some Nodes
70,Networking
70,Adding entries to Pod /etc/hosts with HostAliases
70,Extend Service IP Ranges
70,Validate IPv4/IPv6 dual-stack
70,Extend kubectl with plugins
70,Manage HugePages
70,Schedule GPUs
70,Tutorials
70,Hello Minikube
70,Learn Kubernetes Basics
70,Create a Cluster
70,Using Minikube to Create a Cluster
70,Deploy an App
70,Using kubectl to Create a Deployment
70,Explore Your App
70,Viewing Pods and Nodes
70,Expose Your App Publicly
70,Using a Service to Expose Your App
70,Scale Your App
70,Running Multiple Instances of Your App
70,Update Your App
70,Performing a Rolling Update
70,Configuration
70,Example: Configuring a Java Microservice
70,"Externalizing config using MicroProfile, ConfigMaps and Secrets"
70,Configuring Redis using a ConfigMap
70,Security
70,Apply Pod Security Standards at the Cluster Level
70,Apply Pod Security Standards at the Namespace Level
70,Restrict a Container's Access to Resources with AppArmor
70,Restrict a Container's Syscalls with seccomp
70,Stateless Applications
70,Exposing an External IP Address to Access an Application in a Cluster
70,Example: Deploying PHP Guestbook application with Redis
70,Stateful Applications
70,StatefulSet Basics
70,Example: Deploying WordPress and MySQL with Persistent Volumes
70,Example: Deploying Cassandra with a StatefulSet
70,"Running ZooKeeper, A Distributed System Coordinator"
70,Services
70,Connecting Applications with Services
70,Using Source IP
70,Explore Termination Behavior for Pods And Their Endpoints
70,Reference
70,Glossary
70,API Overview
70,Kubernetes API Concepts
70,Server-Side Apply
70,Client Libraries
70,Common Expression Language in Kubernetes
70,Kubernetes Deprecation Policy
70,Deprecated API Migration Guide
70,Kubernetes API health endpoints
70,API Access Control
70,Authenticating
70,Authenticating with Bootstrap Tokens
70,Certificates and Certificate Signing Requests
70,Admission Controllers
70,Dynamic Admission Control
70,Managing Service Accounts
70,Authorization Overview
70,Using RBAC Authorization
70,Using ABAC Authorization
70,Using Node Authorization
70,Mapping PodSecurityPolicies to Pod Security Standards
70,Webhook Mode
70,Kubelet authentication/authorization
70,TLS bootstrapping
70,Validating Admission Policy
70,"Well-Known Labels, Annotations and Taints"
70,Audit Annotations
70,Kubernetes API
70,Workload Resources
70,Pod
70,PodTemplate
70,ReplicationController
70,ReplicaSet
70,Deployment
70,StatefulSet
70,ControllerRevision
70,DaemonSet
70,Job
70,CronJob
70,HorizontalPodAutoscaler
70,HorizontalPodAutoscaler
70,PriorityClass
70,PodSchedulingContext v1alpha2
70,ResourceClaim v1alpha2
70,ResourceClaimTemplate v1alpha2
70,ResourceClass v1alpha2
70,Service Resources
70,Service
70,Endpoints
70,EndpointSlice
70,Ingress
70,IngressClass
70,Config and Storage Resources
70,ConfigMap
70,Secret
70,Volume
70,PersistentVolumeClaim
70,PersistentVolume
70,StorageClass
70,VolumeAttachment
70,CSIDriver
70,CSINode
70,CSIStorageCapacity
70,Authentication Resources
70,ServiceAccount
70,TokenRequest
70,TokenReview
70,CertificateSigningRequest
70,ClusterTrustBundle v1alpha1
70,SelfSubjectReview
70,Authorization Resources
70,LocalSubjectAccessReview
70,SelfSubjectAccessReview
70,SelfSubjectRulesReview
70,SubjectAccessReview
70,ClusterRole
70,ClusterRoleBinding
70,Role
70,RoleBinding
70,Policy Resources
70,LimitRange
70,ResourceQuota
70,NetworkPolicy
70,PodDisruptionBudget
70,IPAddress v1alpha1
70,Extend Resources
70,CustomResourceDefinition
70,MutatingWebhookConfiguration
70,ValidatingWebhookConfiguration
70,ValidatingAdmissionPolicy v1beta1
70,Cluster Resources
70,Node
70,Namespace
70,Event
70,APIService
70,Lease
70,RuntimeClass
70,FlowSchema v1beta3
70,PriorityLevelConfiguration v1beta3
70,Binding
70,ComponentStatus
70,ClusterCIDR v1alpha1
70,Common Definitions
70,DeleteOptions
70,LabelSelector
70,ListMeta
70,LocalObjectReference
70,NodeSelectorRequirement
70,ObjectFieldSelector
70,ObjectMeta
70,ObjectReference
70,Patch
70,Quantity
70,ResourceFieldSelector
70,Status
70,TypedLocalObjectReference
70,Other Resources
70,ValidatingAdmissionPolicyBindingList v1beta1
70,Common Parameters
70,Instrumentation
70,Service Level Indicator Metrics
70,CRI Pod & Container Metrics
70,Node metrics data
70,Kubernetes Metrics Reference
70,Kubernetes Issues and Security
70,Kubernetes Issue Tracker
70,Kubernetes Security and Disclosure Information
70,CVE feed
70,Node Reference Information
70,Kubelet Checkpoint API
70,Articles on dockershim Removal and on Using CRI-compatible Runtimes
70,Node Labels Populated By The Kubelet
70,Kubelet Device Manager API Versions
70,Node Status
70,Networking Reference
70,Protocols for Services
70,Ports and Protocols
70,Virtual IPs and Service Proxies
70,Setup tools
70,Kubeadm
70,kubeadm init
70,kubeadm join
70,kubeadm upgrade
70,kubeadm config
70,kubeadm reset
70,kubeadm token
70,kubeadm version
70,kubeadm alpha
70,kubeadm certs
70,kubeadm init phase
70,kubeadm join phase
70,kubeadm kubeconfig
70,kubeadm reset phase
70,kubeadm upgrade phase
70,Implementation details
70,Command line tool (kubectl)
70,kubectl Quick Reference
70,kubectl reference
70,kubectl
70,kubectl annotate
70,kubectl api-resources
70,kubectl api-versions
70,kubectl apply
70,kubectl apply edit-last-applied
70,kubectl apply set-last-applied
70,kubectl apply view-last-applied
70,kubectl attach
70,kubectl auth
70,kubectl auth can-i
70,kubectl auth reconcile
70,kubectl auth whoami
70,kubectl autoscale
70,kubectl certificate
70,kubectl certificate approve
70,kubectl certificate deny
70,kubectl cluster-info
70,kubectl cluster-info dump
70,kubectl completion
70,kubectl config
70,kubectl config current-context
70,kubectl config delete-cluster
70,kubectl config delete-context
70,kubectl config delete-user
70,kubectl config get-clusters
70,kubectl config get-contexts
70,kubectl config get-users
70,kubectl config rename-context
70,kubectl config set
70,kubectl config set-cluster
70,kubectl config set-context
70,kubectl config set-credentials
70,kubectl config unset
70,kubectl config use-context
70,kubectl config view
70,kubectl cordon
70,kubectl cp
70,kubectl create
70,kubectl create clusterrole
70,kubectl create clusterrolebinding
70,kubectl create configmap
70,kubectl create cronjob
70,kubectl create deployment
70,kubectl create ingress
70,kubectl create job
70,kubectl create namespace
70,kubectl create poddisruptionbudget
70,kubectl create priorityclass
70,kubectl create quota
70,kubectl create role
70,kubectl create rolebinding
70,kubectl create secret
70,kubectl create secret docker-registry
70,kubectl create secret generic
70,kubectl create secret tls
70,kubectl create service
70,kubectl create service clusterip
70,kubectl create service externalname
70,kubectl create service loadbalancer
70,kubectl create service nodeport
70,kubectl create serviceaccount
70,kubectl create token
70,kubectl debug
70,kubectl delete
70,kubectl describe
70,kubectl diff
70,kubectl drain
70,kubectl edit
70,kubectl events
70,kubectl exec
70,kubectl explain
70,kubectl expose
70,kubectl get
70,kubectl kustomize
70,kubectl label
70,kubectl logs
70,kubectl options
70,kubectl patch
70,kubectl plugin
70,kubectl plugin list
70,kubectl port-forward
70,kubectl proxy
70,kubectl replace
70,kubectl rollout
70,kubectl rollout history
70,kubectl rollout pause
70,kubectl rollout restart
70,kubectl rollout resume
70,kubectl rollout status
70,kubectl rollout undo
70,kubectl run
70,kubectl scale
70,kubectl set
70,kubectl set env
70,kubectl set image
70,kubectl set resources
70,kubectl set selector
70,kubectl set serviceaccount
70,kubectl set subject
70,kubectl taint
70,kubectl top
70,kubectl top node
70,kubectl top pod
70,kubectl uncordon
70,kubectl version
70,kubectl wait
70,kubectl Commands
70,kubectl
70,JSONPath Support
70,kubectl for Docker Users
70,kubectl Usage Conventions
70,Component tools
70,Feature Gates
70,Feature Gates (removed)
70,kubelet
70,kube-apiserver
70,kube-controller-manager
70,kube-proxy
70,kube-scheduler
70,Debug cluster
70,Flow control
70,Configuration APIs
70,Client Authentication (v1)
70,Client Authentication (v1beta1)
70,Event Rate Limit Configuration (v1alpha1)
70,Image Policy API (v1alpha1)
70,kube-apiserver Admission (v1)
70,kube-apiserver Audit Configuration (v1)
70,kube-apiserver Configuration (v1)
70,kube-apiserver Configuration (v1alpha1)
70,kube-apiserver Configuration (v1beta1)
70,kube-apiserver Encryption Configuration (v1)
70,kube-controller-manager Configuration (v1alpha1)
70,kube-proxy Configuration (v1alpha1)
70,kube-scheduler Configuration (v1)
70,kubeadm Configuration (v1beta3)
70,kubeadm Configuration (v1beta4)
70,kubeconfig (v1)
70,Kubelet Configuration (v1)
70,Kubelet Configuration (v1alpha1)
70,Kubelet Configuration (v1beta1)
70,Kubelet CredentialProvider (v1)
70,WebhookAdmission Configuration (v1)
70,External APIs
70,Kubernetes Custom Metrics (v1beta2)
70,Kubernetes External Metrics (v1beta1)
70,Kubernetes Metrics (v1beta1)
70,Scheduling
70,Scheduler Configuration
70,Scheduling Policies
70,Other Tools
70,Mapping from dockercli to crictl
70,Contribute
70,Contribute to Kubernetes Documentation
70,Suggesting content improvements
70,Contributing new content
70,Opening a pull request
70,Documenting for a release
70,Blogs and case studies
70,Reviewing changes
70,Reviewing pull requests
70,For approvers and reviewers
70,Localizing Kubernetes documentation
70,Participating in SIG Docs
70,Roles and responsibilities
70,Issue Wranglers
70,PR wranglers
70,Documentation style overview
70,Content guide
70,Style guide
70,Diagram guide
70,Writing a new topic
70,Page content types
70,Content organization
70,Custom Hugo Shortcodes
70,Updating Reference Documentation
70,Quickstart
70,Contributing to the Upstream Kubernetes Code
70,Generating Reference Documentation for the Kubernetes API
70,Generating Reference Documentation for kubectl Commands
70,Generating Reference Documentation for Metrics
70,Generating Reference Pages for Kubernetes Components and Tools
70,Advanced contributing
70,Viewing Site Analytics
70,Docs smoke test pageKubernetes DocumentationTutorialsStateful ApplicationsExample: Deploying WordPress and MySQL with Persistent VolumesExample: Deploying WordPress and MySQL with Persistent VolumesThis tutorial shows you how to deploy a WordPress site and a MySQL database using
70,Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.A PersistentVolume (PV) is a piece
70,"of storage in the cluster that has been manually provisioned by an administrator,"
70,or dynamically provisioned by Kubernetes using a StorageClass.
70,A PersistentVolumeClaim (PVC)
70,is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and
70,PersistentVolumeClaims are independent from Pod lifecycles and preserve data through
70,"restarting, rescheduling, and even deleting Pods.Warning: This deployment is not suitable for production use cases, as it uses single instance"
70,WordPress and MySQL Pods. Consider using
70,WordPress Helm Chart
70,to deploy WordPress in production.Note: The files provided in this tutorial are using GA Deployment APIs and are specific
70,to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier
70,"version of Kubernetes, please update the API version appropriately, or reference"
70,"earlier versions of this tutorial.ObjectivesCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yaml witha Secret generatorMySQL resource configsWordPress resource configsApply the kustomization directory by kubectl apply -k ./Clean upBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must"
70,be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a
70,"cluster, you can create one by using"
70,minikube
70,"or you can use one of these Kubernetes playgrounds:KillercodaPlay with KubernetesTo check the version, enter kubectl version.The example shown on this page works with kubectl 1.27 and above.Download the following configuration files:mysql-deployment.yamlwordpress-deployment.yamlCreate PersistentVolumeClaims and PersistentVolumesMySQL and Wordpress each require a PersistentVolume to store data."
70,Their PersistentVolumeClaims will be created at the deployment step.Many cluster environments have a default StorageClass installed.
70,"When a StorageClass is not specified in the PersistentVolumeClaim,"
70,"the cluster's default StorageClass is used instead.When a PersistentVolumeClaim is created, a PersistentVolume is dynamically"
70,"provisioned based on the StorageClass configuration.Warning: In local clusters, the default StorageClass uses the hostPath provisioner."
70,hostPath volumes are only suitable for development and testing. With hostPath
70,"volumes, your data lives in /tmp on the node the Pod is scheduled onto and does"
70,not move between nodes. If a Pod dies and gets scheduled to another node in the
70,"cluster, or the node is rebooted, the data is lost.Note: If you are bringing up a cluster that needs to use the hostPath provisioner,"
70,"the --enable-hostpath-provisioner flag must be set in the controller-manager component.Note: If you have a Kubernetes cluster running on Google Kubernetes Engine, please"
70,follow this guide.Create a kustomization.yamlAdd a Secret generatorA Secret is an object that stores a piece
70,"of sensitive data like a password or key. Since 1.14, kubectl supports the"
70,management of Kubernetes objects using a kustomization file. You can create a Secret
70,by generators in kustomization.yaml.Add a Secret generator in kustomization.yaml from the following command.
70,You will need to replace YOUR_PASSWORD with the password you want to use.cat <<EOF >./kustomization.yaml
70,secretGenerator:
70,- name: mysql-pass
70,literals:
70,- password=YOUR_PASSWORD
70,EOF
70,Add resource configs for MySQL and WordPressThe following manifest describes a single-instance MySQL Deployment. The MySQL
70,container mounts the PersistentVolume at /var/lib/mysql. The MYSQL_ROOT_PASSWORD
70,environment variable sets the database password from the Secret.application/wordpress/mysql-deployment.yaml
70,apiVersion: v1
70,kind: Service
70,metadata:
70,name: wordpress-mysql
70,labels:
70,app: wordpress
70,spec:
70,ports:
70,- port: 3306
70,selector:
70,app: wordpress
70,tier: mysql
70,clusterIP: None
70,---
70,apiVersion: v1
70,kind: PersistentVolumeClaim
70,metadata:
70,name: mysql-pv-claim
70,labels:
70,app: wordpress
70,spec:
70,accessModes:
70,- ReadWriteOnce
70,resources:
70,requests:
70,storage: 20Gi
70,---
70,apiVersion: apps/v1
70,kind: Deployment
70,metadata:
70,name: wordpress-mysql
70,labels:
70,app: wordpress
70,spec:
70,selector:
70,matchLabels:
70,app: wordpress
70,tier: mysql
70,strategy:
70,type: Recreate
70,template:
70,metadata:
70,labels:
70,app: wordpress
70,tier: mysql
70,spec:
70,containers:
70,- image: mysql:8.0
70,name: mysql
70,env:
70,- name: MYSQL_ROOT_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,- name: MYSQL_DATABASE
70,value: wordpress
70,- name: MYSQL_USER
70,value: wordpress
70,- name: MYSQL_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,ports:
70,- containerPort: 3306
70,name: mysql
70,volumeMounts:
70,- name: mysql-persistent-storage
70,mountPath: /var/lib/mysql
70,volumes:
70,- name: mysql-persistent-storage
70,persistentVolumeClaim:
70,claimName: mysql-pv-claim
70,The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the
70,PersistentVolume at /var/www/html for website data files. The WORDPRESS_DB_HOST environment variable sets
70,"the name of the MySQL Service defined above, and WordPress will access the database by Service. The"
70,WORDPRESS_DB_PASSWORD environment variable sets the database password from the Secret kustomize generated.application/wordpress/wordpress-deployment.yaml
70,apiVersion: v1
70,kind: Service
70,metadata:
70,name: wordpress
70,labels:
70,app: wordpress
70,spec:
70,ports:
70,- port: 80
70,selector:
70,app: wordpress
70,tier: frontend
70,type: LoadBalancer
70,---
70,apiVersion: v1
70,kind: PersistentVolumeClaim
70,metadata:
70,name: wp-pv-claim
70,labels:
70,app: wordpress
70,spec:
70,accessModes:
70,- ReadWriteOnce
70,resources:
70,requests:
70,storage: 20Gi
70,---
70,apiVersion: apps/v1
70,kind: Deployment
70,metadata:
70,name: wordpress
70,labels:
70,app: wordpress
70,spec:
70,selector:
70,matchLabels:
70,app: wordpress
70,tier: frontend
70,strategy:
70,type: Recreate
70,template:
70,metadata:
70,labels:
70,app: wordpress
70,tier: frontend
70,spec:
70,containers:
70,- image: wordpress:6.2.1-apache
70,name: wordpress
70,env:
70,- name: WORDPRESS_DB_HOST
70,value: wordpress-mysql
70,- name: WORDPRESS_DB_PASSWORD
70,valueFrom:
70,secretKeyRef:
70,name: mysql-pass
70,key: password
70,- name: WORDPRESS_DB_USER
70,value: wordpress
70,ports:
70,- containerPort: 80
70,name: wordpress
70,volumeMounts:
70,- name: wordpress-persistent-storage
70,mountPath: /var/www/html
70,volumes:
70,- name: wordpress-persistent-storage
70,persistentVolumeClaim:
70,claimName: wp-pv-claim
70,Download the MySQL deployment configuration file.curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
70,Download the WordPress configuration file.curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml
70,Add them to kustomization.yaml file.cat <<EOF >>./kustomization.yaml
70,resources:
70,- mysql-deployment.yaml
70,- wordpress-deployment.yaml
70,EOF
70,Apply and VerifyThe kustomization.yaml contains all the resources for deploying a WordPress site and a
70,MySQL database. You can apply the directory bykubectl apply -k ./
70,Now you can verify that all objects exist.Verify that the Secret exists by running the following command:kubectl get secrets
70,The response should be like this:NAME
70,TYPE
70,DATA
70,AGE
70,mysql-pass-c57bb4t7mf
70,Opaque
70,Verify that a PersistentVolume got dynamically provisioned.kubectl get pvc
70,Note: It can take up to a few minutes for the PVs to be provisioned and bound.The response should be like this:NAME
70,STATUS
70,VOLUME
70,CAPACITY
70,ACCESS MODES
70,STORAGECLASS
70,AGE
70,mysql-pv-claim
70,Bound
70,pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002
70,20Gi
70,RWO
70,standard
70,77s
70,wp-pv-claim
70,Bound
70,pvc-8cd0df54-4044-11e9-b2bb-42010a800002
70,20Gi
70,RWO
70,standard
70,77s
70,Verify that the Pod is running by running the following command:kubectl get pods
70,Note: It can take up to a few minutes for the Pod's Status to be RUNNING.The response should be like this:NAME
70,READY
70,STATUS
70,RESTARTS
70,AGE
70,wordpress-mysql-1894417608-x5dzt
70,1/1
70,Running
70,40s
70,Verify that the Service is running by running the following command:kubectl get services wordpress
70,The response should be like this:NAME
70,TYPE
70,CLUSTER-IP
70,EXTERNAL-IP
70,PORT(S)
70,AGE
70,wordpress
70,LoadBalancer
70,10.0.0.89
70,<pending>
70,80:32406/TCP
70,Note: Minikube can only expose Services through NodePort. The EXTERNAL-IP is always pending.Run the following command to get the IP Address for the WordPress Service:minikube service wordpress --url
70,The response should be like this:http://1.2.3.4:32406
70,"Copy the IP address, and load the page in your browser to view your site.You should see the WordPress set up page similar to the following screenshot.Warning: Do not leave your WordPress installation on this page. If another user finds it,"
70,"they can set up a website on your instance and use it to serve malicious content.Either install WordPress by creating a username and password or delete your instance.Cleaning upRun the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:kubectl delete -k ./"
70,What's nextLearn more about Introspection and DebuggingLearn more about JobsLearn more about Port ForwardingLearn how to Get a Shell to a ContainerFeedbackWas this page helpful?Yes
70,"NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on"
70,Stack Overflow.
70,Open an issue in the GitHub Repository if you want to
70,report a problem
70,"suggest an improvement.Last modified August 24, 2023 at 6:38 PM PST: Use code_sample shortcode instead of code shortcode (e8b136c3b3) Edit this page"
70,Create child page
70,Create an issue
70,Print entire sectionObjectivesBefore you beginCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yamlAdd a Secret generatorAdd resource configs for MySQL and WordPressApply and VerifyCleaning upWhat's nextDocumentation
70,Blog
70,Training
70,Partners
70,Community
70,"Case Studies© 2024 The Kubernetes Authors | Documentation Distributed under CC BY 4.0Copyright © 2024 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage pageICP license: 京ICP备17074266号-3"
72,Distributed Mode | JuiceFS Document Center
72,"Skip to main contentHomeProductsCommunity EditionCloud ServiceSolutionsBy Use CaseBig DataAIKubernetesBy IndustryQuantitative TradingSelf-DrivingCommunityDocsCommunity Editon DocsCloud Service DocsJuiceFS CSI Driver DocsBlogEnglishEnglishChineseGitHubSearchCommunity EditionIntroductionQuick StartInstallationStandalone ModeDistributed ModeKey FeaturesDeploymentBenchmark & ProfilingAdministrationSecurityTutorialsReferenceFAQDevelopmentCommunityRelease NotesQuick StartDistributed ModeOn this pageDistributed ModeThe previous document introduces how to create a file system that can be mounted on any host by using an ""object storage"" and a ""SQLite"" database. Thanks to the feature that the object storage is accessible by any computer with privileges on the network, we can also access the same JuiceFS file system on different computers by simply copying the SQLite database file to any computer that needs to access the storage.However, the real-time availability of the files is not guaranteed if the file system is shared by the above approach. Since SQLite is a single file database that cannot be accessed by multiple computers at the same time, a database that supports network access is needed, such as Redis, PostgreSQL, MySQL, etc., which allows a file system to be mounted and read by multiple computers in a distributed environment.In this document, a multi-user ""cloud database"" is used to replace the single-user ""SQLite"" database used in the previous document, aiming to implement a distributed file system that can be mounted on any computer on the network for reading and writing.Network Database​The meaning of ""Network Database"" here refers to the database that allows multiple users to access it simultaneously through the network. From this perspective, the database can be simply divided into:Standalone Database: which is a single-file database and is usually only accessed locally, such as SQLite, Microsoft Access, etc.Network Database: which usually has complex multi-file structures, provides network-based access interfaces and supports simultaneous access by multiple users, such as Redis, PostgreSQL, etc.JuiceFS currently supports the following network-based databases.Key-Value Database: Redis, TiKV, etcd, FoundationDBRelational Database: PostgreSQL, MySQL, MariaDBDifferent databases have different performance and stability. For example, Redis is an in-memory key-value database with an excellent performance but a relatively weak reliability, while PostgreSQL is a relational database which is more reliable but has a less excellent performance than the in-memory database.The document that specifically introduces how to select database will come soon.Cloud Database​Cloud computing platforms usually offer a wide variety of cloud database, such as Amazon RDS for various relational database versions and Amazon ElastiCache for Redis-compatible in-memory database products, which allows to create a multi-copy and highly available database cluster by a simple initial setup.Of course, you can also build your own database on the server.For simplicity, we take Amazon ElastiCache for Redis as an example. The most basic information of a network database consists of the following 2 items.Database Address: the access address of the database; the cloud platform may provide different links for internal and external networks.Username and Password: authentication information used to access the database.Hands-on Practice​1. Install Client​Install the JuiceFS client on all computers that need to mount the file system, refer to ""Installation"" for details.2. Preparing Object Storage​Here is a pseudo sample with Amazon S3 as an example. You can also switch to other object storage (refer to JuiceFS Supported Storage for details).Bucket Endpoint: https://myjfs.s3.us-west-1.amazonaws.comAccess Key ID: ABCDEFGHIJKLMNopqXYZAccess Key Secret: ZYXwvutsrqpoNMLkJiHgfeDCBA3. Preparing Database​Here is a pseudo sample with Amazon ElastiCache for Redis as an example. You can also switch to other types of databases (refer to JuiceFS Supported Databases for details).Database Address: myjfs-sh-abc.apse1.cache.amazonaws.com:6379Database Username: tomDatabase Password: mypasswordThe format for using a Redis database in JuiceFS is as follows.redis://<username>:<password>@<Database-IP-or-URL>:6379/1tipRedis versions lower than 6.0 do not take username, so omit the <username> part in the URL, e.g. redis://:[email protected]:6379/1 (please note that the colon in front of the password is a separator and needs to be preserved).4. Creating a file system​The following command creates a file system that supports cross-network, multi-machine simultaneous mounts, and shared reads and writes using an object storage and a Redis database.juicefs format \"
72,--storage s3 \
72,--bucket https://myjfs.s3.us-west-1.amazonaws.com \
72,--access-key ABCDEFGHIJKLMNopqXYZ \
72,--secret-key ZYXwvutsrqpoNMLkJiHgfeDCBA \
72,redis://tom:[email protected]:6379/1 \
72,"myjfsOnce the file system is created, the terminal will output something like the following.2021/12/16 16:37:14.264445 juicefs[22290] <INFO>: Meta address: redis://@myjfs-sh-abc.apse1.cache.amazonaws.com:6379/12021/12/16 16:37:14.277632 juicefs[22290] <WARNING>: maxmemory_policy is ""volatile-lru"", please set it to 'noeviction'.2021/12/16 16:37:14.281432 juicefs[22290] <INFO>: Ping redis: 3.609453ms2021/12/16 16:37:14.527879 juicefs[22290] <INFO>: Data uses s3://myjfs/myjfs/2021/12/16 16:37:14.593450 juicefs[22290] <INFO>: Volume is formatted as {Name:myjfs UUID:4ad0bb86-6ef5-4861-9ce2-a16ac5dea81b Storage:s3 Bucket:https://myjfs AccessKey:ABCDEFGHIJKLMNopqXYZ SecretKey:removed BlockSize:4096 Compression:none Shards:0 Partitions:0 Capacity:0 Inodes:0 EncryptKey:}infoOnce a file system is created, the relevant information including name, object storage, access keys, etc. are recorded in the database. In the current example, the file system information is recorded in the Redis database, so any computer with the database address, username, and password information can mount and read the file system.5. Mounting the file system​Since the ""data"" and ""metadata"" of this file system are stored in cloud services, the file system can be mounted on any computer with a JuiceFS client installed for shared reads and writes at the same time. For example:juicefs mount redis://tom:[email protected]:6379/1 ~/jfsStrong data consistency guarantee​JuiceFS guarantees a ""close-to-open"" consistency, which means that when two or more clients read and write the same file at the same time, the changes made by client A may not be immediately visible to client B. Other client is guaranteed to see the latest data when they re-opens the file only if client A closes the file, no matter whether the file is on the same node with A or not.Increase cache size to improve performance​Since object storage is a network-based storage service, it will inevitably encounter access latency. To solve this problem, JuiceFS provides and enables caching mechanism by default, i.e. allocating a part of local storage as a buffer layer between data and object storage, and caching data asynchronously to local storage when reading files. Please refer to ""Cache"" for more details.JuiceFS will set 100GiB cache in $HOME/.juicefs/cache or /var/jfsCache directory by default. Setting a larger cache space on a faster SSD can effectively improve read and write performance of JuiceFS even more .You can use --cache-dir to adjust the location of the cache directory and --cache-size to adjust the size of the cache space, e.g.:juicefs mount"
72,--background \
72,--cache-dir /mycache \
72,--cache-size 512000 \
72,redis://tom:[email protected]:6379/1 \
72,"~/jfsnoteThe JuiceFS process needs permission to read and write to the --cache-dir directory.The above command sets the cache directory in the /mycache directory and specifies the cache space as 500GiB.Auto-mount on boot​In a Linux environment, you can set up automatic mounting when mounting a file system via the --update-fstab option, which adds the options required to mount JuiceFS to /etc/fstab. For example:noteThis feature requires JuiceFS version 1.1.0 and above$ sudo juicefs mount --update-fstab --max-uploads=50 --writeback --cache-size 204800 redis://tom:[email protected]:6379/1 <MOUNTPOINT>$ grep <MOUNTPOINT> /etc/fstabredis://tom:[email protected]:6379/1 <MOUNTPOINT> juicefs _netdev,max-uploads=50,writeback,cache-size=204800 0 0$ ls -l /sbin/mount.juicefslrwxrwxrwx 1 root root 29 Aug 11 16:43 /sbin/mount.juicefs -> /usr/local/bin/juicefsRefer to ""Mount JuiceFS at Boot Time"" for more details.6. Verify the file system​After the file system is mounted, you can use the juicefs bench command to perform basic performance tests and functional verification of the file system to ensure that the JuiceFS file system can be accessed normally and its performance meets expectations.infoThe juicefs bench command can only complete basic performance tests. If you need a more complete evaluation of JuiceFS, please refer to ""JuiceFS Performance Evaluation Guide"".juicefs bench ~/jfsAfter running the juicefs bench command, N large files (1 by default) and N small files (100 by default) will be written to and read from the JuiceFS file system according to the specified concurrency (1 by default), and statistics the throughput of read and write and the latency of a single operation, as well as the latency of accessing the metadata engine.If you encounter any problems during the verification of the file system, please refer to the ""Fault Diagnosis and Analysis"" document for troubleshooting first.7. Unmounting the file system​You can unmount the JuiceFS file system (assuming the mount point path is ~/jfs) by the command juicefs umount.juicefs umount ~/jfsUnmounting failure​If the command fails to unmount the file system after execution, it will prompt Device or resource busy.2021-05-09 22:42:55.757097 I | fusermount: failed to unmount ~/jfs: Device or resource busyexit status 1This failure happens probably because some programs are reading or writing files in the file system when executing unmount command. To avoid data loss, you should first determine which processes are accessing files in the file system (e.g. via the command lsof) and try to release the files before re-executing the unmount command.cautionThe following command may result in file corruption and loss, so be careful to use it!You can add the option --force or -f to force the file system unmounted if you are clear about the consequence of the operation.juicefs umount --force ~/jfsEdit this pageLast updated on Jan 5, 2024PreviousStandalone ModeNextCacheNetwork DatabaseCloud DatabaseHands-on Practice1. Install Client2. Preparing Object Storage3. Preparing Database4. Creating a file system5. Mounting the file systemStrong data consistency guaranteeIncrease cache size to improve performanceAuto-mount on boot6. Verify the file system7. Unmounting the file systemUnmounting failureFollow us on TwitterLet's chat on SlackProductCommunity EditionCloud ServiceSolutionMachine LearningBig DataKubernetes PVfor Self-Drivingfor Quantitative TradingResourcesCommunity Edition DocsCloud Service DocsJuiceFS CSI Driver DocsCommunityBlogJoin SlackCompanyAbout JuicedataCareerTerms of ServicePrivacy PolicyContact UsHot TopicsKubernetes CSI DriverClickHouse Data-TieringCephFS vs. JuiceFSAlluxio vs. JuiceFSS3FS vs. JuiceFSDistributed File System ComparisonCopyright © 2017-2024 Juicedata, Inc."
73,Working with Databases: Active Record | The Definitive Guide to Yii 2.0 | Yii PHP Framework
73,Guide
73,API
73,Wiki
73,Forum
73,Community Live Chat
73,Extensions
73,Resources
73,Members
73,Hall of Fame
73,Badges
73,More Learn
73,Books
73,Resources
73,Develop
73,Download Yii
73,Report an Issue
73,Report a Security Issue
73,Contribute to Yii
73,Donate
73,About
73,What is Yii?
73,Release Cycle
73,News
73,License
73,Team
73,Official Logos and Design
73,Login
73,The Definitive Guide to Yii 2.0
73,Download PDF
73,Offline HTML (tar.gz)
73,Offline HTML (tar.bz2) English العربية
73,Español
73,Français
73,Bahasa Indonesia
73,日本語
73,Polski
73,Português brasileiro
73,Русский
73,Українська
73,Oʻzbekcha
73,简体中文
73,Tiếng Việt
73,Version 2.0 1.1
73,1.0
73,"Active RecordDeclaring Active Record ClassesSetting a table nameActive records are called ""models""Connecting to DatabasesQuerying DataAccessing DataData TransformationRetrieving Data in ArraysRetrieving Data in BatchesSaving DataData ValidationMassive AssignmentUpdating CountersDirty AttributesDefault Attribute ValuesAttributes TypecastingJSON in MySQL and PostgreSQLArrays in PostgreSQLUpdating Multiple RowsDeleting DataActive Record Life CyclesNew Instance Life CycleQuerying Data Life CycleSaving Data Life CycleDeleting Data Life CycleRefreshing Data Life CycleWorking with TransactionsOptimistic LocksWorking with Relational DataDeclaring RelationsAccessing Relational DataDynamic Relational QueryRelations via a Junction TableChaining relation definitions via multiple tablesLazy Loading and Eager LoadingJoining with RelationsRelation table aliasesInverse RelationsSaving RelationsCross-Database RelationsCustomizing Query ClassesSelecting extra fieldsUser Contributed Notes"
73,SideNav
73,Search IntroductionAbout Yii
73,Upgrading from Version 1.1
73,Getting StartedWhat do you need to know
73,Installing Yii
73,Running Applications
73,Saying Hello
73,Working with Forms
73,Working with Databases
73,Generating Code with Gii
73,Looking Ahead
73,Application StructureApplication Structure Overview
73,Entry Scripts
73,Applications
73,Application Components
73,Controllers
73,Models
73,Views
73,Modules
73,Filters
73,Widgets
73,Assets
73,Extensions
73,Handling RequestsRequest Handling Overview
73,Bootstrapping
73,Routing and URL Creation
73,Requests
73,Responses
73,Sessions and Cookies
73,Handling Errors
73,Logging
73,Key ConceptsComponents
73,Properties
73,Events
73,Behaviors
73,Configurations
73,Aliases
73,Class Autoloading
73,Service Locator
73,Dependency Injection Container
73,Working with DatabasesDatabase Access Objects
73,Query Builder
73,Active Record
73,Migrations
73,Sphinx
73,Redis
73,MongoDB
73,ElasticSearch
73,Getting Data from UsersCreating Forms
73,Validating Input
73,Uploading Files
73,Collecting Tabular Input
73,Getting Data for Multiple Models
73,Extending ActiveForm on the Client Side
73,Displaying DataData Formatting
73,Pagination
73,Sorting
73,Data Providers
73,Data Widgets
73,Working with Client Scripts
73,Theming
73,SecuritySecurity Overview
73,Authentication
73,Authorization
73,Working with Passwords
73,Cryptography
73,Auth Clients
73,Best Practices
73,CachingCaching Overview
73,Data Caching
73,Fragment Caching
73,Page Caching
73,HTTP Caching
73,RESTful Web ServicesQuick Start
73,Resources
73,Controllers
73,Filtering Collections
73,Routing
73,Response Formatting
73,Authentication
73,Rate Limiting
73,Versioning
73,Error Handling
73,Development ToolsDebug Toolbar and Debugger
73,Generating Code using Gii
73,Generating API Documentation
73,TestingTesting Overview
73,Testing environment setup
73,Unit Tests
73,Functional Tests
73,Acceptance Tests
73,Fixtures
73,Special TopicsAdvanced Project Template
73,Building Application from Scratch
73,Console Commands
73,Core Validators
73,Docker
73,Internationalization
73,Mailing
73,Performance Tuning
73,Shared Hosting Environment
73,Template Engines
73,Working with Third-Party Code
73,Using Yii as a micro framework
73,WidgetsGridView
73,ListView
73,DetailView
73,ActiveForm
73,Pjax
73,Menu
73,LinkPager
73,LinkSorter
73,Bootstrap Widgets
73,jQuery UI Widgets
73,HelpersHelpers Overview
73,ArrayHelper
73,Html
73,Json
73,Url
73,16 followers
73,Active Record ¶
73,Declaring Active Record Classes
73,Connecting to Databases
73,Querying Data
73,Accessing Data
73,Saving Data
73,Deleting Data
73,Active Record Life Cycles
73,Working with Transactions
73,Optimistic Locks
73,Working with Relational Data
73,Saving Relations
73,Cross-Database Relations
73,Customizing Query Classes
73,Selecting extra fields
73,Active Record provides an object-oriented interface
73,"for accessing and manipulating data stored in databases. An Active Record class is associated with a database table,"
73,"an Active Record instance corresponds to a row of that table, and an attribute of an Active Record"
73,"instance represents the value of a particular column in that row. Instead of writing raw SQL statements,"
73,you would access Active Record attributes and call Active Record methods to access and manipulate the data stored
73,in database tables.
73,"For example, assume Customer is an Active Record class which is associated with the customer table"
73,and name is a column of the customer table. You can write the following code to insert a new
73,row into the customer table:
73,$customer = new Customer();
73,$customer->name = 'Qiang';
73,$customer->save();
73,"The above code is equivalent to using the following raw SQL statement for MySQL, which is less"
73,"intuitive, more error prone, and may even have compatibility problems if you are using a different kind of database:"
73,"$db->createCommand('INSERT INTO `customer` (`name`) VALUES (:name)', ["
73,"':name' => 'Qiang',"
73,])->execute();
73,Yii provides the Active Record support for the following relational databases:
73,MySQL 4.1 or later: via yii\db\ActiveRecord
73,PostgreSQL 7.3 or later: via yii\db\ActiveRecord
73,SQLite 2 and 3: via yii\db\ActiveRecord
73,Microsoft SQL Server 2008 or later: via yii\db\ActiveRecord
73,Oracle: via yii\db\ActiveRecord
73,CUBRID 9.3 or later: via yii\db\ActiveRecord (Note that due to a bug in
73,"the cubrid PDO extension, quoting of values will not work, so you need CUBRID 9.3 as the client as well as the server)"
73,"Sphinx: via yii\sphinx\ActiveRecord, requires the yii2-sphinx extension"
73,"ElasticSearch: via yii\elasticsearch\ActiveRecord, requires the yii2-elasticsearch extension"
73,"Additionally, Yii also supports using Active Record with the following NoSQL databases:"
73,"Redis 2.6.12 or later: via yii\redis\ActiveRecord, requires the yii2-redis extension"
73,"MongoDB 1.3.0 or later: via yii\mongodb\ActiveRecord, requires the yii2-mongodb extension"
73,"In this tutorial, we will mainly describe the usage of Active Record for relational databases."
73,"However, most content described here are also applicable to Active Record for NoSQL databases."
73,Declaring Active Record Classes
73,"¶To get started, declare an Active Record class by extending yii\db\ActiveRecord."
73,Setting a table name ¶By default each Active Record class is associated with its database table.
73,The tableName() method returns the table name by converting the class name via yii\helpers\Inflector::camel2id().
73,You may override this method if the table is not named after this convention.
73,Also a default tablePrefix can be applied. For example if
73,"tablePrefix is tbl_, Customer becomes tbl_customer and OrderItem becomes tbl_order_item."
73,"If a table name is given as {{%TableName}}, then the percentage character % will be replaced with the table prefix."
73,"For example, {{%post}} becomes {{tbl_post}}. The brackets around the table name are used for"
73,quoting in an SQL query.
73,"In the following example, we declare an Active Record class named Customer for the customer database table."
73,namespace app\models;
73,use yii\db\ActiveRecord;
73,class Customer extends ActiveRecord
73,const STATUS_INACTIVE = 0;
73,const STATUS_ACTIVE = 1;
73,/**
73,* @return string the name of the table associated with this ActiveRecord class.
73,public static function tableName()
73,return '{{customer}}';
73,"Active records are called ""models"" ¶Active Record instances are considered as models. For this reason, we usually put Active Record"
73,classes under the app\models namespace (or other namespaces for keeping model classes).
73,"Because yii\db\ActiveRecord extends from yii\base\Model, it inherits all model features,"
73,"such as attributes, validation rules, data serialization, etc."
73,Connecting to Databases
73,"¶By default, Active Record uses the db application component"
73,as the DB connection to access and manipulate the database data. As explained in
73,"Database Access Objects, you can configure the db component in the application configuration like shown"
73,"below,"
73,return [
73,'components' => [
73,'db' => [
73,"'class' => 'yii\db\Connection',"
73,"'dsn' => 'mysql:host=localhost;dbname=testdb',"
73,"'username' => 'demo',"
73,"'password' => 'demo',"
73,"If you want to use a different database connection other than the db component, you should override"
73,the getDb() method:
73,class Customer extends ActiveRecord
73,// ...
73,public static function getDb()
73,"// use the ""db2"" application component"
73,return \Yii::$app->db2;
73,Querying Data
73,"¶After declaring an Active Record class, you can use it to query data from the corresponding database table."
73,The process usually takes the following three steps:
73,Create a new query object by calling the yii\db\ActiveRecord::find() method;
73,Build the query object by calling query building methods;
73,Call a query method to retrieve data in terms of Active Record instances.
73,"As you can see, this is very similar to the procedure with query builder. The only difference"
73,"is that instead of using the new operator to create a query object, you call yii\db\ActiveRecord::find()"
73,to return a new query object which is of class yii\db\ActiveQuery.
73,Below are some examples showing how to use Active Query to query data:
73,// return a single customer whose ID is 123
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::find()
73,->where(['id' => 123])
73,->one();
73,// return all active customers and order them by their IDs
73,// SELECT * FROM `customer` WHERE `status` = 1 ORDER BY `id`
73,$customers = Customer::find()
73,->where(['status' => Customer::STATUS_ACTIVE])
73,->orderBy('id')
73,->all();
73,// return the number of active customers
73,// SELECT COUNT(*) FROM `customer` WHERE `status` = 1
73,$count = Customer::find()
73,->where(['status' => Customer::STATUS_ACTIVE])
73,->count();
73,// return all customers in an array indexed by customer IDs
73,// SELECT * FROM `customer`
73,$customers = Customer::find()
73,->indexBy('id')
73,->all();
73,"In the above, $customer is a Customer object while $customers is an array of Customer objects. They are"
73,all populated with the data retrieved from the customer table.
73,"Info: Because yii\db\ActiveQuery extends from yii\db\Query, you can use all query building methods and"
73,query methods as described in the Section Query Builder.
73,"Because it is a common task to query by primary key values or a set of column values, Yii provides two shortcut"
73,methods for this purpose:
73,yii\db\ActiveRecord::findOne(): returns a single Active Record instance populated with the first row of the query result.
73,yii\db\ActiveRecord::findAll(): returns an array of Active Record instances populated with all query result.
73,Both methods can take one of the following parameter formats:
73,a scalar value: the value is treated as the desired primary key value to be looked for. Yii will determine
73,automatically which column is the primary key column by reading database schema information.
73,an array of scalar values: the array is treated as the desired primary key values to be looked for.
73,an associative array: the keys are column names and the values are the corresponding desired column values to
73,be looked for. Please refer to Hash Format for more details.
73,The following code shows how these methods can be used:
73,// returns a single customer whose ID is 123
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::findOne(123);
73,"// returns customers whose ID is 100, 101, 123 or 124"
73,"// SELECT * FROM `customer` WHERE `id` IN (100, 101, 123, 124)"
73,"$customers = Customer::findAll([100, 101, 123, 124]);"
73,// returns an active customer whose ID is 123
73,// SELECT * FROM `customer` WHERE `id` = 123 AND `status` = 1
73,$customer = Customer::findOne([
73,"'id' => 123,"
73,"'status' => Customer::STATUS_ACTIVE,"
73,]);
73,// returns all inactive customers
73,// SELECT * FROM `customer` WHERE `status` = 0
73,$customers = Customer::findAll([
73,"'status' => Customer::STATUS_INACTIVE,"
73,]);
73,"Warning: If you need to pass user input to these methods, make sure the input value is scalar or in case of"
73,"array condition, make sure the array structure can not be changed from the outside:"
73,// yii\web\Controller ensures that $id is scalar
73,public function actionView($id)
73,$model = Post::findOne($id);
73,// ...
73,"// explicitly specifying the column to search, passing a scalar or array here will always result in finding a single record"
73,$model = Post::findOne(['id' => Yii::$app->request->get('id')]);
73,// do NOT use the following code! it is possible to inject an array condition to filter by arbitrary column values!
73,$model = Post::findOne(Yii::$app->request->get('id'));
73,Note: Neither yii\db\ActiveRecord::findOne() nor yii\db\ActiveQuery::one() will add LIMIT 1 to
73,"the generated SQL statement. If your query may return many rows of data, you should call limit(1) explicitly"
73,"to improve the performance, e.g., Customer::find()->limit(1)->one()."
73,"Besides using query building methods, you can also write raw SQLs to query data and populate the results into"
73,Active Record objects. You can do so by calling the yii\db\ActiveRecord::findBySql() method:
73,// returns all inactive customers
73,$sql = 'SELECT * FROM customer WHERE status=:status';
73,"$customers = Customer::findBySql($sql, [':status' => Customer::STATUS_INACTIVE])->all();"
73,Do not call extra query building methods after calling findBySql() as they
73,will be ignored.
73,Accessing Data
73,"¶As aforementioned, the data brought back from the database are populated into Active Record instances, and"
73,each row of the query result corresponds to a single Active Record instance. You can access the column values
73,"by accessing the attributes of the Active Record instances, for example,"
73,"// ""id"" and ""email"" are the names of columns in the ""customer"" table"
73,$customer = Customer::findOne(123);
73,$id = $customer->id;
73,$email = $customer->email;
73,Note: The Active Record attributes are named after the associated table columns in a case-sensitive manner.
73,Yii automatically defines an attribute in Active Record for every column of the associated table.
73,You should NOT redeclare any of the attributes.
73,"Because Active Record attributes are named after table columns, you may find you are writing PHP code like"
73,"$customer->first_name, which uses underscores to separate words in attribute names if your table columns are"
73,"named in this way. If you are concerned about code style consistency, you should rename your table columns accordingly"
73,"(to use camelCase, for example)."
73,Data Transformation
73,¶It often happens that the data being entered and/or displayed are in a format which is different from the one used in
73,"storing the data in a database. For example, in the database you are storing customers' birthdays as UNIX timestamps"
73,"(which is not a good design, though), while in most cases you would like to manipulate birthdays as strings in"
73,"the format of 'YYYY/MM/DD'. To achieve this goal, you can define data transformation methods in the Customer"
73,Active Record class like the following:
73,class Customer extends ActiveRecord
73,// ...
73,public function getBirthdayText()
73,"return date('Y/m/d', $this->birthday);"
73,public function setBirthdayText($value)
73,$this->birthday = strtotime($value);
73,"Now in your PHP code, instead of accessing $customer->birthday, you would access $customer->birthdayText, which"
73,will allow you to input and display customer birthdays in the format of 'YYYY/MM/DD'.
73,Tip: The above example shows a generic way of transforming data in different formats. If you are working with
73,"date values, you may use DateValidator and yii\jui\DatePicker,"
73,which is easier to use and more powerful.
73,Retrieving Data in Arrays
73,"¶While retrieving data in terms of Active Record objects is convenient and flexible, it is not always desirable"
73,"when you have to bring back a large amount of data due to the big memory footprint. In this case, you can retrieve"
73,data using PHP arrays by calling asArray() before executing a query method:
73,// return all customers
73,// each customer is returned as an associative array
73,$customers = Customer::find()
73,->asArray()
73,->all();
73,"Note: While this method saves memory and improves performance, it is closer to the lower DB abstraction layer"
73,and you will lose most of the Active Record features. A very important distinction lies in the data type of
73,"the column values. When you return data in Active Record instances, column values will be automatically typecast"
73,"according to the actual column types; on the other hand when you return data in arrays, column values will be"
73,"strings (since they are the result of PDO without any processing), regardless their actual column types."
73,Retrieving Data in Batches
73,"¶In Query Builder, we have explained that you may use batch query to minimize your memory"
73,"usage when querying a large amount of data from the database. You may use the same technique in Active Record. For example,"
73,// fetch 10 customers at a time
73,foreach (Customer::find()->batch(10) as $customers) {
73,// $customers is an array of 10 or fewer Customer objects
73,// fetch 10 customers at a time and iterate them one by one
73,foreach (Customer::find()->each(10) as $customer) {
73,// $customer is a Customer object
73,// batch query with eager loading
73,foreach (Customer::find()->with('orders')->each() as $customer) {
73,// $customer is a Customer object with the 'orders' relation populated
73,Saving Data
73,"¶Using Active Record, you can easily save data to the database by taking the following steps:"
73,Prepare an Active Record instance
73,Assign new values to Active Record attributes
73,Call yii\db\ActiveRecord::save() to save the data into database.
73,"For example,"
73,// insert a new row of data
73,$customer = new Customer();
73,$customer->name = 'James';
73,$customer->email = 'james@example.com';
73,$customer->save();
73,// update an existing row of data
73,$customer = Customer::findOne(123);
73,$customer->email = 'james@newexample.com';
73,$customer->save();
73,"The save() method can either insert or update a row of data, depending on the state"
73,"of the Active Record instance. If the instance is newly created via the new operator, calling"
73,"save() will cause insertion of a new row; If the instance is the result of a query method,"
73,calling save() will update the row associated with the instance.
73,You can differentiate the two states of an Active Record instance by checking its
73,isNewRecord property value. This property is also used by
73,save() internally as follows:
73,"public function save($runValidation = true, $attributeNames = null)"
73,if ($this->getIsNewRecord()) {
73,"return $this->insert($runValidation, $attributeNames);"
73,} else {
73,"return $this->update($runValidation, $attributeNames) !== false;"
73,Tip: You can call insert() or update()
73,directly to insert or update a row.
73,Data Validation
73,"¶Because yii\db\ActiveRecord extends from yii\base\Model, it shares the same data validation feature."
73,You can declare validation rules by overriding the rules() method and perform
73,data validation by calling the validate() method.
73,"When you call save(), by default it will call validate()"
73,"automatically. Only when the validation passes, will it actually save the data; otherwise it will simply return false,"
73,and you can check the errors property to retrieve the validation error messages.
73,"Tip: If you are certain that your data do not need validation (e.g., the data comes from trustable sources),"
73,you can call save(false) to skip the validation.
73,Massive Assignment
73,"¶Like normal models, Active Record instances also enjoy the massive assignment feature."
73,"Using this feature, you can assign values to multiple attributes of an Active Record instance in a single PHP statement,"
73,"like shown below. Do remember that only safe attributes can be massively assigned, though."
73,$values = [
73,"'name' => 'James',"
73,"'email' => 'james@example.com',"
73,$customer = new Customer();
73,$customer->attributes = $values;
73,$customer->save();
73,Updating Counters
73,"¶It is a common task to increment or decrement a column in a database table. We call these columns ""counter columns""."
73,You can use updateCounters() to update one or multiple counter columns.
73,"For example,"
73,$post = Post::findOne(100);
73,// UPDATE `post` SET `view_count` = `view_count` + 1 WHERE `id` = 100
73,$post->updateCounters(['view_count' => 1]);
73,"Note: If you use yii\db\ActiveRecord::save() to update a counter column, you may end up with inaccurate result,"
73,because it is likely the same counter is being saved by multiple requests which read and write the same counter value.
73,Dirty Attributes
73,"¶When you call save() to save an Active Record instance, only dirty attributes"
73,are being saved. An attribute is considered dirty if its value has been modified since it was loaded from DB or
73,saved to DB most recently. Note that data validation will be performed regardless if the Active Record
73,instance has dirty attributes or not.
73,Active Record automatically maintains the list of dirty attributes. It does so by maintaining an older version of
73,the attribute values and comparing them with the latest one. You can call yii\db\ActiveRecord::getDirtyAttributes()
73,to get the attributes that are currently dirty. You can also call yii\db\ActiveRecord::markAttributeDirty()
73,to explicitly mark an attribute as dirty.
73,"If you are interested in the attribute values prior to their most recent modification, you may call"
73,getOldAttributes() or getOldAttribute().
73,Note: The comparison of old and new values will be done using the === operator so a value will be considered dirty
73,even if it has the same value but a different type. This is often the case when the model receives user input from
73,HTML forms where every value is represented as a string.
73,To ensure the correct type for e.g. integer values you may apply a validation filter:
73,"['attributeName', 'filter', 'filter' => 'intval']. This works with all the typecasting functions of PHP like"
73,"intval(), floatval(),"
73,"boolval, etc..."
73,Default Attribute Values
73,"¶Some of your table columns may have default values defined in the database. Sometimes, you may want to pre-populate your"
73,"Web form for an Active Record instance with these default values. To avoid writing the same default values again,"
73,you can call loadDefaultValues() to populate the DB-defined default values
73,into the corresponding Active Record attributes:
73,$customer = new Customer();
73,$customer->loadDefaultValues();
73,"// $customer->xyz will be assigned the default value declared when defining the ""xyz"" column"
73,Attributes Typecasting
73,"¶Being populated by query results, yii\db\ActiveRecord performs automatic typecast for its attribute values, using"
73,information from database table schema. This allows data retrieved from table column
73,"declared as integer to be populated in ActiveRecord instance with PHP integer, boolean with boolean and so on."
73,"However, typecasting mechanism has several limitations:"
73,"Float values are not be converted and will be represented as strings, otherwise they may loose precision."
73,Conversion of the integer values depends on the integer capacity of the operation system you use. In particular:
73,values of column declared as 'unsigned integer' or 'big integer' will be converted to PHP integer only at 64-bit
73,"operation system, while on 32-bit ones - they will be represented as strings."
73,Note that attribute typecast is performed only during populating ActiveRecord instance from query result. There is no
73,automatic conversion for the values loaded from HTTP request or set directly via property access.
73,"The table schema will also be used while preparing SQL statements for the ActiveRecord data saving, ensuring"
73,"values are bound to the query with correct type. However, ActiveRecord instance attribute values will not be"
73,converted during saving process.
73,Tip: you may use yii\behaviors\AttributeTypecastBehavior to facilitate attribute values typecasting
73,on ActiveRecord validation or saving.
73,"Since 2.0.14, Yii ActiveRecord supports complex data types, such as JSON or multidimensional arrays."
73,"JSON in MySQL and PostgreSQL ¶After data population, the value from JSON column will be automatically decoded from JSON according to standard JSON"
73,decoding rules.
73,"To save attribute value to a JSON column, ActiveRecord will automatically create a JsonExpression object"
73,that will be encoded to a JSON string on QueryBuilder level.
73,"Arrays in PostgreSQL ¶After data population, the value from Array column will be automatically decoded from PgSQL notation to an ArrayExpression"
73,"object. It implements PHP ArrayAccess interface, so you can use it as an array, or call ->getValue() to get the array itself."
73,"To save attribute value to an array column, ActiveRecord will automatically create an ArrayExpression object"
73,that will be encoded by QueryBuilder to an PgSQL string representation of array.
73,You can also use conditions for JSON columns:
73,"$query->andWhere(['=', 'json', new ArrayExpression(['foo' => 'bar'])])"
73,To learn more about expressions building system read the Query Builder – Adding custom Conditions and Expressions
73,article.
73,Updating Multiple Rows
73,"¶The methods described above all work on individual Active Record instances, causing inserting or updating of individual"
73,"table rows. To update multiple rows simultaneously, you should call updateAll(), instead,"
73,which is a static method.
73,// UPDATE `customer` SET `status` = 1 WHERE `email` LIKE `%@example.com%`
73,"Customer::updateAll(['status' => Customer::STATUS_ACTIVE], ['like', 'email', '@example.com']);"
73,"Similarly, you can call updateAllCounters() to update counter columns of"
73,multiple rows at the same time.
73,// UPDATE `customer` SET `age` = `age` + 1
73,Customer::updateAllCounters(['age' => 1]);
73,Deleting Data
73,"¶To delete a single row of data, first retrieve the Active Record instance corresponding to that row and then call"
73,the yii\db\ActiveRecord::delete() method.
73,$customer = Customer::findOne(123);
73,$customer->delete();
73,"You can call yii\db\ActiveRecord::deleteAll() to delete multiple or all rows of data. For example,"
73,Customer::deleteAll(['status' => Customer::STATUS_INACTIVE]);
73,Note: Be very careful when calling deleteAll() because it may totally
73,erase all data from your table if you make a mistake in specifying the condition.
73,Active Record Life Cycles
73,¶It is important to understand the life cycles of Active Record when it is used for different purposes.
73,"During each life cycle, a certain sequence of methods will be invoked, and you can override these methods"
73,to get a chance to customize the life cycle. You can also respond to certain Active Record events triggered
73,during a life cycle to inject your custom code. These events are especially useful when you are developing
73,Active Record behaviors which need to customize Active Record life cycles.
73,"In the following, we will summarize the various Active Record life cycles and the methods/events that are involved"
73,in the life cycles.
73,New Instance Life Cycle
73,"¶When creating a new Active Record instance via the new operator, the following life cycle will happen:"
73,Class constructor.
73,init(): triggers an EVENT_INIT event.
73,Querying Data Life Cycle
73,"¶When querying data through one of the querying methods, each newly populated Active Record will"
73,undergo the following life cycle:
73,Class constructor.
73,init(): triggers an EVENT_INIT event.
73,afterFind(): triggers an EVENT_AFTER_FIND event.
73,Saving Data Life Cycle
73,"¶When calling save() to insert or update an Active Record instance, the following"
73,life cycle will happen:
73,beforeValidate(): triggers
73,an EVENT_BEFORE_VALIDATE event. If the method returns false
73,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
73,"Performs data validation. If data validation fails, the steps after Step 3 will be skipped."
73,afterValidate(): triggers
73,an EVENT_AFTER_VALIDATE event.
73,beforeSave(): triggers
73,an EVENT_BEFORE_INSERT
73,or EVENT_BEFORE_UPDATE event. If the method returns false
73,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
73,Performs the actual data insertion or updating.
73,afterSave(): triggers
73,an EVENT_AFTER_INSERT
73,or EVENT_AFTER_UPDATE event.
73,Deleting Data Life Cycle
73,"¶When calling delete() to delete an Active Record instance, the following"
73,life cycle will happen:
73,beforeDelete(): triggers
73,an EVENT_BEFORE_DELETE event. If the method returns false
73,"or yii\base\ModelEvent::$isValid is false, the rest of the steps will be skipped."
73,Performs the actual data deletion.
73,afterDelete(): triggers
73,an EVENT_AFTER_DELETE event.
73,Note: Calling any of the following methods will NOT initiate any of the above life cycles because they work on the
73,database directly and not on a record basis:
73,yii\db\ActiveRecord::updateAll()
73,yii\db\ActiveRecord::deleteAll()
73,yii\db\ActiveRecord::updateCounters()
73,yii\db\ActiveRecord::updateAllCounters()
73,Note: DI is not supported by default due to performance concerns. You can add support if needed by overriding
73,the instantiate() method to instantiate the class via Yii::createObject():
73,public static function instantiate($row)
73,return Yii::createObject(static::class);
73,Refreshing Data Life Cycle
73,"¶When calling refresh() to refresh an Active Record instance, the"
73,EVENT_AFTER_REFRESH event is triggered if refresh is successful and the method returns true.
73,Working with Transactions
73,¶There are two ways of using transactions while working with Active Record.
73,"The first way is to explicitly enclose Active Record method calls in a transactional block, like shown below,"
73,$customer = Customer::findOne(123);
73,Customer::getDb()->transaction(function($db) use ($customer) {
73,$customer->id = 200;
73,$customer->save();
73,// ...other DB operations...
73,});
73,// or alternatively
73,$transaction = Customer::getDb()->beginTransaction();
73,try {
73,$customer->id = 200;
73,$customer->save();
73,// ...other DB operations...
73,$transaction->commit();
73,} catch(\Exception $e) {
73,$transaction->rollBack();
73,throw $e;
73,} catch(\Throwable $e) {
73,$transaction->rollBack();
73,throw $e;
73,Note: in the above code we have two catch-blocks for compatibility
73,with PHP 5.x and PHP 7.x. \Exception implements the \Throwable interface
73,"since PHP 7.0, so you can skip the part with \Exception if your app uses only PHP 7.0 and higher."
73,The second way is to list the DB operations that require transactional support in the yii\db\ActiveRecord::transactions()
73,"method. For example,"
73,class Customer extends ActiveRecord
73,public function transactions()
73,return [
73,"'admin' => self::OP_INSERT,"
73,"'api' => self::OP_INSERT | self::OP_UPDATE | self::OP_DELETE,"
73,// the above is equivalent to the following:
73,"// 'api' => self::OP_ALL,"
73,The yii\db\ActiveRecord::transactions() method should return an array whose keys are scenario
73,names and values are the corresponding operations that should be enclosed within transactions. You should use the following
73,constants to refer to different DB operations:
73,OP_INSERT: insertion operation performed by insert();
73,OP_UPDATE: update operation performed by update();
73,OP_DELETE: deletion operation performed by delete().
73,Use the | operators to concatenate the above constants to indicate multiple operations. You may also use the shortcut
73,constant OP_ALL to refer to all three operations above.
73,Transactions that are created using this method will be started before calling beforeSave()
73,and will be committed after afterSave() has run.
73,Optimistic Locks
73,¶Optimistic locking is a way to prevent conflicts that may occur when a single row of data is being
73,"updated by multiple users. For example, both user A and user B are editing the same wiki article"
73,"at the same time. After user A saves his edits, user B clicks on the ""Save"" button in an attempt to"
73,"save his edits as well. Because user B was actually working on an outdated version of the article,"
73,it would be desirable to have a way to prevent him from saving the article and show him some hint message.
73,Optimistic locking solves the above problem by using a column to record the version number of each row.
73,"When a row is being saved with an outdated version number, a yii\db\StaleObjectException exception"
73,"will be thrown, which prevents the row from being saved. Optimistic locking is only supported when you"
73,"update or delete an existing row of data using yii\db\ActiveRecord::update() or yii\db\ActiveRecord::delete(),"
73,respectively.
73,"To use optimistic locking,"
73,Create a column in the DB table associated with the Active Record class to store the version number of each row.
73,The column should be of big integer type (in MySQL it would be BIGINT DEFAULT 0).
73,Override the yii\db\ActiveRecord::optimisticLock() method to return the name of this column.
73,Implement OptimisticLockBehavior inside your model class to automatically parse its value from received requests.
73,Remove the version attribute from validation rules as OptimisticLockBehavior should handle it.
73,"In the Web form that takes user inputs, add a hidden field to store the current version number of the row being updated."
73,"In the controller action that updates the row using Active Record, try and catch the yii\db\StaleObjectException"
73,"exception. Implement necessary business logic (e.g. merging the changes, prompting staled data) to resolve the conflict."
73,"For example, assume the version column is named as version. You can implement optimistic locking with the code like"
73,the following.
73,// ------ view code -------
73,use yii\helpers\Html;
73,// ...other input fields
73,"echo Html::activeHiddenInput($model, 'version');"
73,// ------ controller code -------
73,use yii\db\StaleObjectException;
73,public function actionUpdate($id)
73,$model = $this->findModel($id);
73,try {
73,if ($model->load(Yii::$app->request->post()) && $model->save()) {
73,"return $this->redirect(['view', 'id' => $model->id]);"
73,} else {
73,"return $this->render('update', ["
73,"'model' => $model,"
73,]);
73,} catch (StaleObjectException $e) {
73,// logic to resolve the conflict
73,// ------ model code -------
73,use yii\behaviors\OptimisticLockBehavior;
73,public function behaviors()
73,return [
73,"OptimisticLockBehavior::class,"
73,public function optimisticLock()
73,return 'version';
73,Note: Because OptimisticLockBehavior will ensure the record is only saved
73,"if user submits a valid version number by directly parsing getBodyParam(), it"
73,may be useful to extend your model class and do step 2 in parent model while attaching the behavior (step 3) to the child
73,class so you can have an instance dedicated to internal use while tying the other to controllers responsible of receiving
73,"end user inputs. Alternatively, you can implement your own logic by configuring its value property."
73,Working with Relational Data
73,"¶Besides working with individual database tables, Active Record is also capable of bringing together related data,"
73,"making them readily accessible through the primary data. For example, the customer data is related with the order"
73,"data because one customer may have placed one or multiple orders. With appropriate declaration of this relation,"
73,you'll be able to access a customer's order information using the expression $customer->orders which gives
73,back the customer's order information in terms of an array of Order Active Record instances.
73,Declaring Relations
73,"¶To work with relational data using Active Record, you first need to declare relations in Active Record classes."
73,"The task is as simple as declaring a relation method for every interested relation, like the following,"
73,class Customer extends ActiveRecord
73,// ...
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,class Order extends ActiveRecord
73,// ...
73,public function getCustomer()
73,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
73,"In the above code, we have declared an orders relation for the Customer class, and a customer relation"
73,for the Order class.
73,Each relation method must be named as getXyz. We call xyz (the first letter is in lower case) the relation name.
73,Note that relation names are case sensitive.
73,"While declaring a relation, you should specify the following information:"
73,the multiplicity of the relation: specified by calling either hasMany()
73,or hasOne(). In the above example you may easily read in the relation
73,declarations that a customer has many orders while an order only has one customer.
73,the name of the related Active Record class: specified as the first parameter to
73,either hasMany() or hasOne().
73,A recommended practice is to call Xyz::class to get the class name string so that you can receive
73,IDE auto-completion support as well as error detection at compiling stage.
73,the link between the two types of data: specifies the column(s) through which the two types of data are related.
73,The array values are the columns of the primary data (represented by the Active Record class that you are declaring
73,"relations), while the array keys are the columns of the related data."
73,"An easy rule to remember this is, as you see in the example above, you write the column that belongs to the related"
73,Active Record directly next to it. You see there that customer_id is a property of Order and id is a property
73,of Customer.
73,Warning: Relation name relation is reserved. When used it will produce ArgumentCountError.
73,Accessing Relational Data
73,"¶After declaring relations, you can access relational data through relation names. This is just like accessing"
73,"an object property defined by the relation method. For this reason, we call it relation property."
73,"For example,"
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::findOne(123);
73,// SELECT * FROM `order` WHERE `customer_id` = 123
73,// $orders is an array of Order objects
73,$orders = $customer->orders;
73,"Info: When you declare a relation named xyz via a getter method getXyz(), you will be able to access"
73,xyz like an object property. Note that the name is case sensitive.
73,"If a relation is declared with hasMany(), accessing this relation property"
73,will return an array of the related Active Record instances; if a relation is declared with
73,"hasOne(), accessing the relation property will return the related"
73,Active Record instance or null if no related data is found.
73,"When you access a relation property for the first time, a SQL statement will be executed, like shown in the"
73,"above example. If the same property is accessed again, the previous result will be returned without re-executing"
73,"the SQL statement. To force re-executing the SQL statement, you should unset the relation property"
73,first: unset($customer->orders).
73,"Note: While this concept looks similar to the object property feature, there is an"
73,important difference. For normal object properties the property value is of the same type as the defining getter method.
73,"A relation method however returns an yii\db\ActiveQuery instance, while accessing a relation property will either"
73,return a yii\db\ActiveRecord instance or an array of these.
73,$customer->orders; // is an array of `Order` objects
73,$customer->getOrders(); // returns an ActiveQuery instance
73,"This is useful for creating customized queries, which is described in the next section."
73,Dynamic Relational Query
73,"¶Because a relation method returns an instance of yii\db\ActiveQuery, you can further build this query"
73,"using query building methods before performing DB query. For example,"
73,$customer = Customer::findOne(123);
73,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 200 ORDER BY `id`
73,$orders = $customer->getOrders()
73,"->where(['>', 'subtotal', 200])"
73,->orderBy('id')
73,->all();
73,"Unlike accessing a relation property, each time you perform a dynamic relational query via a relation method,"
73,"a SQL statement will be executed, even if the same dynamic relational query was performed before."
73,Sometimes you may even want to parametrize a relation declaration so that you can more easily perform
73,"dynamic relational query. For example, you may declare a bigOrders relation as follows,"
73,class Customer extends ActiveRecord
73,public function getBigOrders($threshold = 100)
73,"return $this->hasMany(Order::class, ['customer_id' => 'id'])"
73,"->where('subtotal > :threshold', [':threshold' => $threshold])"
73,->orderBy('id');
73,Then you will be able to perform the following relational queries:
73,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 200 ORDER BY `id`
73,$orders = $customer->getBigOrders(200)->all();
73,// SELECT * FROM `order` WHERE `customer_id` = 123 AND `subtotal` > 100 ORDER BY `id`
73,$orders = $customer->bigOrders;
73,Relations via a Junction Table
73,"¶In database modelling, when the multiplicity between two related tables is many-to-many,"
73,"a junction table is usually introduced. For example, the order"
73,table and the item table may be related via a junction table named order_item. One order will then correspond
73,"to multiple order items, while one product item will also correspond to multiple order items."
73,"When declaring such relations, you would call either via() or viaTable()"
73,to specify the junction table. The difference between via() and viaTable()
73,is that the former specifies the junction table in terms of an existing relation name while the latter directly uses
73,"the junction table. For example,"
73,class Order extends ActiveRecord
73,public function getItems()
73,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
73,"->viaTable('order_item', ['order_id' => 'id']);"
73,"or alternatively,"
73,class Order extends ActiveRecord
73,public function getOrderItems()
73,"return $this->hasMany(OrderItem::class, ['order_id' => 'id']);"
73,public function getItems()
73,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
73,->via('orderItems');
73,"The usage of relations declared with a junction table is the same as that of normal relations. For example,"
73,// SELECT * FROM `order` WHERE `id` = 100
73,$order = Order::findOne(100);
73,// SELECT * FROM `order_item` WHERE `order_id` = 100
73,// SELECT * FROM `item` WHERE `item_id` IN (...)
73,// returns an array of Item objects
73,$items = $order->items;
73,Chaining relation definitions via multiple tables
73,¶Its further possible to define relations via multiple tables by chaining relation definitions using via().
73,"Considering the examples above, we have classes Customer, Order, and Item."
73,"We can add a relation to the Customer class that lists all items from all the orders they placed,"
73,"and name it getPurchasedItems(), the chaining of relations is show in the following code example:"
73,class Customer extends ActiveRecord
73,// ...
73,public function getPurchasedItems()
73,"// customer's items, matching 'id' column of `Item` to 'item_id' in OrderItem"
73,"return $this->hasMany(Item::class, ['id' => 'item_id'])"
73,->via('orderItems');
73,public function getOrderItems()
73,"// customer's order items, matching 'id' column of `Order` to 'order_id' in OrderItem"
73,"return $this->hasMany(OrderItem::class, ['order_id' => 'id'])"
73,->via('orders');
73,public function getOrders()
73,// same as above
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,Lazy Loading and Eager Loading
73,"¶In Accessing Relational Data, we explained that you can access a relation property"
73,of an Active Record instance like accessing a normal object property. A SQL statement will be executed only when
73,you access the relation property the first time. We call such relational data accessing method lazy loading.
73,"For example,"
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::findOne(123);
73,// SELECT * FROM `order` WHERE `customer_id` = 123
73,$orders = $customer->orders;
73,// no SQL executed
73,$orders2 = $customer->orders;
73,"Lazy loading is very convenient to use. However, it may suffer from a performance issue when you need to access"
73,the same relation property of multiple Active Record instances. Consider the following code example. How many
73,SQL statements will be executed?
73,// SELECT * FROM `customer` LIMIT 100
73,$customers = Customer::find()->limit(100)->all();
73,foreach ($customers as $customer) {
73,// SELECT * FROM `order` WHERE `customer_id` = ...
73,$orders = $customer->orders;
73,"As you can see from the code comment above, there are 101 SQL statements being executed! This is because each"
73,"time you access the orders relation property of a different Customer object in the for-loop, a SQL statement"
73,will be executed.
73,"To solve this performance problem, you can use the so-called eager loading approach as shown below,"
73,// SELECT * FROM `customer` LIMIT 100;
73,// SELECT * FROM `orders` WHERE `customer_id` IN (...)
73,$customers = Customer::find()
73,->with('orders')
73,->limit(100)
73,->all();
73,foreach ($customers as $customer) {
73,// no SQL executed
73,$orders = $customer->orders;
73,"By calling yii\db\ActiveQuery::with(), you instruct Active Record to bring back the orders for the first 100"
73,"customers in one single SQL statement. As a result, you reduce the number of the executed SQL statements from 101 to 2!"
73,You can eagerly load one or multiple relations. You can even eagerly load nested relations. A nested relation is a relation
73,"that is declared within a related Active Record class. For example, Customer is related with Order through the orders"
73,"relation, and Order is related with Item through the items relation. When querying for Customer, you can eagerly"
73,load items using the nested relation notation orders.items.
73,The following code shows different usage of with(). We assume the Customer class
73,"has two relations orders and country, while the Order class has one relation items."
73,"// eager loading both ""orders"" and ""country"""
73,"$customers = Customer::find()->with('orders', 'country')->all();"
73,// equivalent to the array syntax below
73,"$customers = Customer::find()->with(['orders', 'country'])->all();"
73,// no SQL executed
73,$orders= $customers[0]->orders;
73,// no SQL executed
73,$country = $customers[0]->country;
73,"// eager loading ""orders"" and the nested relation ""orders.items"""
73,$customers = Customer::find()->with('orders.items')->all();
73,// access the items of the first order of the first customer
73,// no SQL executed
73,$items = $customers[0]->orders[0]->items;
73,"You can eagerly load deeply nested relations, such as a.b.c.d. All parent relations will be eagerly loaded."
73,"That is, when you call with() using a.b.c.d, you will eagerly load"
73,"a, a.b, a.b.c and a.b.c.d."
73,"Info: In general, when eagerly loading N relations among which M relations are defined with a"
73,"junction table, a total number of N+M+1 SQL statements will be executed."
73,Note that a nested relation a.b.c.d counts as 4 relations.
73,"When eagerly loading a relation, you can customize the corresponding relational query using an anonymous function."
73,"For example,"
73,// find customers and bring back together their country and active orders
73,// SELECT * FROM `customer`
73,// SELECT * FROM `country` WHERE `id` IN (...)
73,// SELECT * FROM `order` WHERE `customer_id` IN (...) AND `status` = 1
73,$customers = Customer::find()->with([
73,"'country',"
73,'orders' => function ($query) {
73,$query->andWhere(['status' => Order::STATUS_ACTIVE]);
73,])->all();
73,"When customizing the relational query for a relation, you should specify the relation name as an array key"
73,and use an anonymous function as the corresponding array value. The anonymous function will receive a $query parameter
73,which represents the yii\db\ActiveQuery object used to perform the relational query for the relation.
73,"In the code example above, we are modifying the relational query by appending an additional condition about order status."
73,"Note: If you call select() while eagerly loading relations, you have to make sure"
73,"the columns referenced in the relation declarations are being selected. Otherwise, the related models may not"
73,"be loaded properly. For example,"
73,"$orders = Order::find()->select(['id', 'amount'])->with('customer')->all();"
73,"// $orders[0]->customer is always `null`. To fix the problem, you should do the following:"
73,"$orders = Order::find()->select(['id', 'amount', 'customer_id'])->with('customer')->all();"
73,Joining with Relations
73,"¶Note: The content described in this subsection is only applicable to relational databases, such as"
73,"MySQL, PostgreSQL, etc."
73,The relational queries that we have described so far only reference the primary table columns when
73,"querying for the primary data. In reality we often need to reference columns in the related tables. For example,"
73,"we may want to bring back the customers who have at least one active order. To solve this problem, we can"
73,build a join query like the following:
73,// SELECT `customer`.* FROM `customer`
73,// LEFT JOIN `order` ON `order`.`customer_id` = `customer`.`id`
73,// WHERE `order`.`status` = 1
73,// SELECT * FROM `order` WHERE `customer_id` IN (...)
73,$customers = Customer::find()
73,->select('customer.*')
73,"->leftJoin('order', '`order`.`customer_id` = `customer`.`id`')"
73,->where(['order.status' => Order::STATUS_ACTIVE])
73,->with('orders')
73,->all();
73,Note: It is important to disambiguate column names when building relational queries involving JOIN SQL statements.
73,A common practice is to prefix column names with their corresponding table names.
73,"However, a better approach is to exploit the existing relation declarations by calling yii\db\ActiveQuery::joinWith():"
73,$customers = Customer::find()
73,->joinWith('orders')
73,->where(['order.status' => Order::STATUS_ACTIVE])
73,->all();
73,"Both approaches execute the same set of SQL statements. The latter approach is much cleaner and drier, though."
73,"By default, joinWith() will use LEFT JOIN to join the primary table with the"
73,related table. You can specify a different join type (e.g. RIGHT JOIN) via its third parameter $joinType. If
73,"the join type you want is INNER JOIN, you can simply call innerJoinWith(), instead."
73,Calling joinWith() will eagerly load the related data by default.
73,"If you do not want to bring in the related data, you can specify its second parameter $eagerLoading as false."
73,Note: Even when using joinWith() or innerJoinWith()
73,with eager loading enabled the related data will not be populated from the result of the JOIN query. So there's
73,still an extra query for each joined relation as explained in the section on eager loading.
73,"Like with(), you can join with one or multiple relations; you may customize the relation"
73,queries on-the-fly; you may join with nested relations; and you may mix the use of with()
73,"and joinWith(). For example,"
73,$customers = Customer::find()->joinWith([
73,'orders' => function ($query) {
73,"$query->andWhere(['>', 'subtotal', 100]);"
73,])->with('country')
73,->all();
73,"Sometimes when joining two tables, you may need to specify some extra conditions in the ON part of the JOIN query."
73,This can be done by calling the yii\db\ActiveQuery::onCondition() method like the following:
73,// SELECT `customer`.* FROM `customer`
73,// LEFT JOIN `order` ON `order`.`customer_id` = `customer`.`id` AND `order`.`status` = 1
73,// SELECT * FROM `order` WHERE `customer_id` IN (...)
73,$customers = Customer::find()->joinWith([
73,'orders' => function ($query) {
73,$query->onCondition(['order.status' => Order::STATUS_ACTIVE]);
73,])->all();
73,"This above query brings back all customers, and for each customer it brings back all active orders."
73,Note that this differs from our earlier example which only brings back customers who have at least one active order.
73,"Info: When yii\db\ActiveQuery is specified with a condition via onCondition(),"
73,the condition will be put in the ON part if the query involves a JOIN query. If the query does not involve
73,"JOIN, the on-condition will be automatically appended to the WHERE part of the query."
73,Thus it may only contain conditions including columns of the related table.
73,Relation table aliases
73,"¶As noted before, when using JOIN in a query, we need to disambiguate column names. Therefore often an alias is"
73,defined for a table. Setting an alias for the relational query would be possible by customizing the relation query in the following way:
73,$query->joinWith([
73,'orders' => function ($q) {
73,$q->from(['o' => Order::tableName()]);
73,This however looks very complicated and involves either hardcoding the related objects table name or calling Order::tableName().
73,"Since version 2.0.7, Yii provides a shortcut for this. You may now define and use the alias for the relation table like the following:"
73,// join the orders relation and sort the result by orders.id
73,$query->joinWith(['orders o'])->orderBy('o.id');
73,The above syntax works for simple relations. If you need an alias for an intermediate table when joining over
73,"nested relations, e.g. $query->joinWith(['orders.product']),"
73,you need to nest the joinWith calls like in the following example:
73,$query->joinWith(['orders o' => function($q) {
73,$q->joinWith('product p');
73,}])
73,->where('o.amount > 100');
73,Inverse Relations
73,"¶Relation declarations are often reciprocal between two Active Record classes. For example, Customer is related"
73,"to Order via the orders relation, and Order is related back to Customer via the customer relation."
73,class Customer extends ActiveRecord
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,class Order extends ActiveRecord
73,public function getCustomer()
73,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
73,Now consider the following piece of code:
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::findOne(123);
73,// SELECT * FROM `order` WHERE `customer_id` = 123
73,$order = $customer->orders[0];
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer2 = $order->customer;
73,"// displays ""not the same"""
73,echo $customer2 === $customer ? 'same' : 'not the same';
73,"We would think $customer and $customer2 are the same, but they are not! Actually they do contain the same"
73,"customer data, but they are different objects. When accessing $order->customer, an extra SQL statement"
73,is executed to populate a new object $customer2.
73,"To avoid the redundant execution of the last SQL statement in the above example, we should tell Yii that"
73,customer is an inverse relation of orders by calling the inverseOf() method
73,like shown below:
73,class Customer extends ActiveRecord
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id'])->inverseOf('customer');"
73,"With this modified relation declaration, we will have:"
73,// SELECT * FROM `customer` WHERE `id` = 123
73,$customer = Customer::findOne(123);
73,// SELECT * FROM `order` WHERE `customer_id` = 123
73,$order = $customer->orders[0];
73,// No SQL will be executed
73,$customer2 = $order->customer;
73,"// displays ""same"""
73,echo $customer2 === $customer ? 'same' : 'not the same';
73,Note: Inverse relations cannot be defined for relations involving a junction table.
73,"That is, if a relation is defined with via() or viaTable(),"
73,you should not call inverseOf() further.
73,Saving Relations
73,"¶When working with relational data, you often need to establish relationships between different data or destroy"
73,"existing relationships. This requires setting proper values for the columns that define the relations. Using Active Record,"
73,you may end up writing the code like the following:
73,$customer = Customer::findOne(123);
73,$order = new Order();
73,$order->subtotal = 100;
73,// ...
73,"// setting the attribute that defines the ""customer"" relation in Order"
73,$order->customer_id = $customer->id;
73,$order->save();
73,Active Record provides the link() method that allows you to accomplish this task more nicely:
73,$customer = Customer::findOne(123);
73,$order = new Order();
73,$order->subtotal = 100;
73,// ...
73,"$order->link('customer', $customer);"
73,The link() method requires you to specify the relation name and the target Active Record
73,instance that the relationship should be established with. The method will modify the values of the attributes that
73,"link two Active Record instances and save them to the database. In the above example, it will set the customer_id"
73,attribute of the Order instance to be the value of the id attribute of the Customer instance and then save it
73,to the database.
73,Note: You cannot link two newly created Active Record instances.
73,The benefit of using link() is even more obvious when a relation is defined via
73,"a junction table. For example, you may use the following code to link an Order instance"
73,with an Item instance:
73,"$order->link('items', $item);"
73,The above code will automatically insert a row in the order_item junction table to relate the order with the item.
73,Info: The link() method will NOT perform any data validation while
73,saving the affected Active Record instance. It is your responsibility to validate any input data before
73,calling this method.
73,The opposite operation to link() is unlink()
73,"which breaks an existing relationship between two Active Record instances. For example,"
73,$customer = Customer::find()->with('orders')->where(['id' => 123])->one();
73,"$customer->unlink('orders', $customer->orders[0]);"
73,"By default, the unlink() method will set the foreign key value(s) that specify"
73,"the existing relationship to be null. You may, however, choose to delete the table row that contains the foreign key value"
73,by passing the $delete parameter as true to the method.
73,"When a junction table is involved in a relation, calling unlink() will cause"
73,"the foreign keys in the junction table to be cleared, or the deletion of the corresponding row in the junction table"
73,if $delete is true.
73,Cross-Database Relations
73,¶Active Record allows you to declare relations between Active Record classes that are powered by different databases.
73,"The databases can be of different types (e.g. MySQL and PostgreSQL, or MS SQL and MongoDB), and they can run on"
73,"different servers. You can use the same syntax to perform relational queries. For example,"
73,"// Customer is associated with the ""customer"" table in a relational database (e.g. MySQL)"
73,class Customer extends \yii\db\ActiveRecord
73,public static function tableName()
73,return 'customer';
73,public function getComments()
73,// a customer has many comments
73,"return $this->hasMany(Comment::class, ['customer_id' => 'id']);"
73,"// Comment is associated with the ""comment"" collection in a MongoDB database"
73,class Comment extends \yii\mongodb\ActiveRecord
73,public static function collectionName()
73,return 'comment';
73,public function getCustomer()
73,// a comment has one customer
73,"return $this->hasOne(Customer::class, ['id' => 'customer_id']);"
73,$customers = Customer::find()->with('comments')->all();
73,You can use most of the relational query features that have been described in this section.
73,Note: Usage of joinWith() is limited to databases that allow cross-database JOIN queries.
73,"For this reason, you cannot use this method in the above example because MongoDB does not support JOIN."
73,Customizing Query Classes
73,"¶By default, all Active Record queries are supported by yii\db\ActiveQuery. To use a customized query class"
73,"in an Active Record class, you should override the yii\db\ActiveRecord::find() method and return an instance"
73,"of your customized query class. For example,"
73,// file Comment.php
73,namespace app\models;
73,use yii\db\ActiveRecord;
73,class Comment extends ActiveRecord
73,public static function find()
73,return new CommentQuery(get_called_class());
73,"Now whenever you are performing a query (e.g. find(), findOne()) or defining a relation (e.g. hasOne())"
73,"with Comment, you will be calling an instance of CommentQuery instead of ActiveQuery."
73,"You now have to define the CommentQuery class, which can be customized in many creative ways to improve your query building experience. For example,"
73,// file CommentQuery.php
73,namespace app\models;
73,use yii\db\ActiveQuery;
73,class CommentQuery extends ActiveQuery
73,// conditions appended by default (can be skipped)
73,public function init()
73,$this->andOnCondition(['deleted' => false]);
73,parent::init();
73,// ... add customized query methods here ...
73,public function active($state = true)
73,return $this->andOnCondition(['active' => $state]);
73,"Note: Instead of calling onCondition(), you usually should call"
73,andOnCondition() or orOnCondition()
73,to append additional conditions when defining new query building methods so that any existing conditions are not overwritten.
73,This allows you to write query building code like the following:
73,$comments = Comment::find()->active()->all();
73,$inactiveComments = Comment::find()->active(false)->all();
73,"Tip: In big projects, it is recommended that you use customized query classes to hold most query-related code"
73,so that the Active Record classes can be kept clean.
73,You can also use the new query building methods when defining relations about Comment or performing relational query:
73,class Customer extends \yii\db\ActiveRecord
73,public function getActiveComments()
73,"return $this->hasMany(Comment::class, ['customer_id' => 'id'])->active();"
73,$customers = Customer::find()->joinWith('activeComments')->all();
73,// or alternatively
73,class Customer extends \yii\db\ActiveRecord
73,public function getComments()
73,"return $this->hasMany(Comment::class, ['customer_id' => 'id']);"
73,$customers = Customer::find()->joinWith([
73,'comments' => function($q) {
73,$q->active();
73,])->all();
73,"Info: In Yii 1.1, there is a concept called scope. Scope is no longer directly supported in Yii 2.0,"
73,and you should use customized query classes and query methods to achieve the same goal.
73,"Selecting extra fields ¶When Active Record instance is populated from query results, its attributes are filled up by corresponding column"
73,values from received data set.
73,You are able to fetch additional columns or values from query and store it inside the Active Record.
73,"For example, assume we have a table named room, which contains information about rooms available in the hotel."
73,"Each room stores information about its geometrical size using fields length, width, height."
73,Imagine we need to retrieve list of all available rooms with their volume in descending order.
73,"So you can not calculate volume using PHP, because we need to sort the records by its value, but you also want volume"
73,to be displayed in the list.
73,"To achieve the goal, you need to declare an extra field in your Room Active Record class, which will store volume value:"
73,class Room extends \yii\db\ActiveRecord
73,public $volume;
73,// ...
73,"Then you need to compose a query, which calculates volume of the room and performs the sort:"
73,$rooms = Room::find()
73,->select([
73,"'{{room}}.*', // select all columns"
73,"'([[length]] * [[width]] * [[height]]) AS volume', // calculate a volume"
73,->orderBy('volume DESC') // apply sort
73,->all();
73,foreach ($rooms as $room) {
73,echo $room->volume; // contains value calculated by SQL
73,Ability to select extra fields can be exceptionally useful for aggregation queries.
73,Assume you need to display a list of customers with the count of orders they have made.
73,"First of all, you need to declare a Customer class with orders relation and extra field for count storage:"
73,class Customer extends \yii\db\ActiveRecord
73,public $ordersCount;
73,// ...
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,"Then you can compose a query, which joins the orders and calculates their count:"
73,$customers = Customer::find()
73,->select([
73,"'{{customer}}.*', // select all customer fields"
73,'COUNT({{order}}.id) AS ordersCount' // calculate orders count
73,->joinWith('orders') // ensure table junction
73,->groupBy('{{customer}}.id') // group the result to ensure aggregation function works
73,->all();
73,"A disadvantage of using this method would be that, if the information isn't loaded on the SQL query - it has to be calculated"
73,"separately. Thus, if you have found particular record via regular query without extra select statements, it"
73,will be unable to return actual value for the extra field. Same will happen for the newly saved record.
73,$room = new Room();
73,$room->length = 100;
73,$room->width = 50;
73,$room->height = 2;
73,"$room->volume; // this value will be `null`, since it was not declared yet"
73,Using the __get() and __set() magic methods
73,we can emulate the behavior of a property:
73,class Room extends \yii\db\ActiveRecord
73,private $_volume;
73,public function setVolume($volume)
73,$this->_volume = (float) $volume;
73,public function getVolume()
73,if (empty($this->length) || empty($this->width) || empty($this->height)) {
73,return null;
73,if ($this->_volume === null) {
73,$this->setVolume(
73,$this->length * $this->width * $this->height
73,return $this->_volume;
73,// ...
73,"When the select query doesn't provide the volume, the model will be able to calculate it automatically using"
73,the attributes of the model.
73,You can calculate the aggregation fields as well using defined relations:
73,class Customer extends \yii\db\ActiveRecord
73,private $_ordersCount;
73,public function setOrdersCount($count)
73,$this->_ordersCount = (int) $count;
73,public function getOrdersCount()
73,if ($this->isNewRecord) {
73,return null; // this avoid calling a query searching for null primary keys
73,if ($this->_ordersCount === null) {
73,$this->setOrdersCount($this->getOrders()->count()); // calculate aggregation on demand from relation
73,return $this->_ordersCount;
73,// ...
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,"With this code, in case 'ordersCount' is present in 'select' statement - Customer::ordersCount will be populated"
73,"by query results, otherwise it will be calculated on demand using Customer::orders relation."
73,"This approach can be as well used for creation of the shortcuts for some relational data, especially for the aggregation."
73,For example:
73,class Customer extends \yii\db\ActiveRecord
73,/**
73,* Defines read-only virtual property for aggregation data.
73,public function getOrdersCount()
73,if ($this->isNewRecord) {
73,return null; // this avoid calling a query searching for null primary keys
73,return empty($this->ordersAggregation) ? 0 : $this->ordersAggregation[0]['counted'];
73,/**
73,* Declares normal 'orders' relation.
73,public function getOrders()
73,"return $this->hasMany(Order::class, ['customer_id' => 'id']);"
73,/**
73,"* Declares new relation based on 'orders', which provides aggregation."
73,public function getOrdersAggregation()
73,return $this->getOrders()
73,"->select(['customer_id', 'counted' => 'count(*)'])"
73,->groupBy('customer_id')
73,->asArray(true);
73,// ...
73,foreach (Customer::find()->with('ordersAggregation')->all() as $customer) {
73,echo $customer->ordersCount; // outputs aggregation data from relation without extra query due to eager loading
73,$customer = Customer::findOne($pk);
73,$customer->ordersCount; // output aggregation data from lazy loaded relation
73,Query BuilderGo to Top Migrations
73,Found a typo or you think this page needs improvement?
73,Edit it on github !
73,User Contributed Notes 4
73,#20193
73,Yii ActiveRecord looks brilliantly simple and effective for drastically reducing the amount of custom SQL one has to write.
73,"It would be great to see more examples of using joins. For example, if using leftJoin(), what array/object structure is returned? Is ActiveRecord smart enough to nest the joined records underneath each parent record?"
73,Simon East at
73,"Apr 13, 2018, 5:17:49 AM"
73,#20194
73,"It would be great to see more examples of using joins. For example, if using leftJoin(), what array/object structure is returned? Is ActiveRecord smart enough to nest the joined records underneath each parent record?"
73,This is what you get when using with() or joinWith().
73,Lazy Loading and Eager Loading is about calling with() to fetch related records together with the primary query.
73,Joining with Relations is about calling joinWith() to join the table of a related record together with the primary query.
73,"The objects returned are the same but the difference is in which queries are executed in the background if you access a relation, e.g. $post->author."
73,CeBe at
73,"Apr 13, 2018, 6:07:50 AM"
73,#21079
73,"I think this page require a section may be titled as ""Updating Relations"" that shows standard ways to update relational data specially for Many to Many relationships via the junction table."
73,Said Bakr at
73,"Jul 30, 2022, 12:17:16 AM"
73,#21148
73,for anyone wondering how to use link()
73,"public function afterSave($insert, $changedAttributes)"
73,"parent::afterSave($insert, $changedAttributes);"
73,"$categories = Yii::$app->request->getBodyParam('categories', []);"
73,"$authors = Yii::$app->request->getBodyParam('authors', []);"
73,$categoryModels = Categories::findAll($categories);
73,$authorsModels = [];
73,foreach ($authors as $authorData) {
73,$author = Author::findOne(['name' => $authorData['name']]);
73,if (!$author) {
73,$author = new Author();
73,$author->name = $authorData['name'];
73,$author->save();
73,$authorsModels[] = $author;
73,if ($this->isNewRecord) {
73,"$this->link('categories', $categoryModels);"
73,"$this->link('authors', $authorsModels);"
73,} else {
73,"$this->unlinkAll('categories', true);"
73,"$this->unlinkAll('authors', true);"
73,foreach ($categoryModels as $categoryModel) {
73,"$this->link('categories', $categoryModel);"
73,foreach ($authorsModels as $authorModel) {
73,"$this->link('authors', $authorModel);"
73,This is Example Assuming you have Authors and Books and Categories Tables
73,"and Junction Tables (BookAuthors, BookCategories)"
73,I have predefined categories and I want to Link book with author in the author Table and if it isn't found in the Author table create it and then link it with junction table.
73,in this part:
73,if ($this->isNewRecord) {
73,"$this->link('categories', $categoryModels);"
73,"$this->link('authors', $authorsModels);"
73,I tried doing it without condition but it throws error didn't find any logical reason for that. if anyone knows why it would be helpful to let us know.
73,Hope someone find this helpful.
73,Saleh
73,Abuhussein at
73,"May 12, 2023, 1:20:50 PM"
73,Leave a comment
73,Signup or Login in order to comment.
73,About
73,About Yii
73,News
73,License
73,Contact Us
73,Downloads
73,Framework
73,Documentation
73,Extensions
73,Logo
73,Documentation
73,Guide
73,API
73,Wiki
73,Resources
73,Development
73,Contribute
73,Latest Updates
73,Report a Bug
73,Report Security Issue
73,Community
73,Forum
73,Live Chat
73,Facebook Group
73,Hall of Fame
73,Badges
73,Terms of service
73,License
73,Website Source Code
73,© 2008 - 2024 Yii
73,Design: Eshill
73,Terms of service
73,License
73,Website Source Code
73,© 2008 - 2024 Yii
73,Design: Eshill
73,Supported by
77,Grouping and aggregation query optimization - AnalyticDB for MySQL - Alibaba Cloud Documentation Center
77,Document Center
77,All Products
77,Search
77,Document Center
77,AnalyticDB for MySQL
77,User Guide
77,Performance Optimization
77,Tuning queries
77,Grouping and aggregation query optimization
77,all-products-head
77,This Product
77,This Product
77,All Products
77,AnalyticDB for MySQL:Grouping and aggregation query optimization
77,Document Center
77,AnalyticDB for MySQL:Grouping and aggregation query optimization
77,"Last Updated:May 11, 2023"
77,"This topic describes how to optimize grouping and aggregation queries in AnalyticDB for MySQL. Grouping and aggregation processAnalyticDB for MySQL is a distributed data warehouse. By default, it performs the following steps to execute a distributed aggregate query:Perform partial aggregation on data. Partial aggregate nodes use only a small amount of memory. The aggregation process is complete in a streaming manner, which prevents workloads of partial aggregate nodes from piling up. After partial aggregation is complete, redistribute data among nodes based on partial aggregation results obtained by grouping and then perform final aggregation. Partial aggregation results are transferred over networks to the nodes of a downstream stage. (For more information, see Factors that affect query performance.) The amount of data to be transferred over networks is small because the partial aggregation is performed on the data. This reduces the network pressure. After the data is redistributed, final aggregation is performed. On the final aggregate node, the values and aggregation state of a group must be maintained in the memory until all data is processed. This ensures that no new data needs to be processed for a specific group value. Therefore, the final aggregate node may occupy a large amount of memory. For example, the following SQL statement for grouping and aggregation is executed: SELECT sum(A), max(B) FROM tb1 GROUP BY C,D;When the preceding statement is executed to perform grouping and aggregation, partial aggregation is first performed on data on the Node1 and Node2 nodes of the upstream stage. Partial aggregation results are partial sum(A), partial max(B), C, and D. These partial aggregation results are transferred over networks to the Node 3 and Node4 nodes of the downstream stage for final aggregation, as shown in the following figure. Use hints to optimize grouping and aggregationScenariosIn most scenarios, two-step aggregation can strike a good balance between memory and network resources. However, in special scenarios, two-step aggregation may not be the best choice. For example, large numbers of groups must be processed by using grouping and aggregation because the GROUP BY column has a large number of unique values. Assume a scenario that requires mobile numbers or user IDs for grouping. If you use the two-step aggregation method, partial aggregation is performed although only a small amount of data can be aggregated. Moreover, the partial aggregation step involves multiple operations, such as calculating hash values of groups, deduplication, and executing aggregation functions. The amount of data to be transferred over networks is not reduced in the partial aggregation step due to large numbers of groups. However, large amounts of computing resources are consumed. SolutionTo solve the preceding problem of a low aggregation rate, you can add the /*aggregation_path_type=single_agg*/ hint to skip partial aggregation and directly perform final aggregation when you execute a query. This reduces unnecessary computing overheads. Note If the /*aggregation_path_type=single_agg*/ hint is used in an SQL statement, all grouping and aggregation queries in the SQL statement use the specified optimization process. Therefore, the best method is to first analyze the characteristics of aggregation operators in the original execution plan, evaluate the benefits brought by the hint, and then decide whether to use this optimization scheme. Optimization descriptionIf the aggregation rate is low, the amount of data to be transferred over networks is not reduced on the Node1 and Node2 nodes during partial aggregation and consumes large amounts of computing resources. After optimization, partial aggregation is not performed on the Node1 and Node2 nodes. All data (A, B, C, and D) is directly aggregated by the Node3 and Node4 nodes of the downstream stage, which reduces the amount of required computing resources, as shown in the following figure. Note This optimization may not reduce memory usage. If the aggregation rate is low, large amounts of data are accumulated in memory for deduplication and aggregation to ensure that all data for a specific group value is processed."
77,Thank you! We've received your
77,feedback.
78,“How-to” Guides
78,Dark Theme
78,“How-to” Guides
78,Table of Contents
78,Back to index
78,1. Spring Boot Application
78,1.1. Create Your Own FailureAnalyzer
78,1.2. Troubleshoot Auto-configuration
78,1.3. Customize the Environment or ApplicationContext Before It Starts
78,1.4. Build an ApplicationContext Hierarchy (Adding a Parent or Root Context)
78,1.5. Create a Non-web Application
78,2. Properties and Configuration
78,2.1. Automatically Expand Properties at Build Time
78,2.1.1. Automatic Property Expansion Using Maven
78,2.1.2. Automatic Property Expansion Using Gradle
78,2.2. Externalize the Configuration of SpringApplication
78,2.3. Change the Location of External Properties of an Application
78,2.4. Use ‘Short’ Command Line Arguments
78,2.5. Use YAML for External Properties
78,2.6. Set the Active Spring Profiles
78,2.7. Set the Default Profile Name
78,2.8. Change Configuration Depending on the Environment
78,2.9. Discover Built-in Options for External Properties
78,3. Embedded Web Servers
78,3.1. Use Another Web Server
78,3.2. Disabling the Web Server
78,3.3. Change the HTTP Port
78,3.4. Use a Random Unassigned HTTP Port
78,3.5. Discover the HTTP Port at Runtime
78,3.6. Enable HTTP Response Compression
78,3.7. Configure SSL
78,3.7.1. Using PEM-encoded files
78,3.8. Configure HTTP/2
78,3.8.1. HTTP/2 With Tomcat
78,3.8.2. HTTP/2 With Jetty
78,3.8.3. HTTP/2 With Reactor Netty
78,3.8.4. HTTP/2 With Undertow
78,3.9. Configure the Web Server
78,"3.10. Add a Servlet, Filter, or Listener to an Application"
78,"3.10.1. Add a Servlet, Filter, or Listener by Using a Spring Bean"
78,Disable Registration of a Servlet or Filter
78,"3.10.2. Add Servlets, Filters, and Listeners by Using Classpath Scanning"
78,3.11. Configure Access Logging
78,3.12. Running Behind a Front-end Proxy Server
78,3.12.1. Customize Tomcat’s Proxy Configuration
78,3.13. Enable Multiple Connectors with Tomcat
78,3.14. Enable Tomcat’s MBean Registry
78,3.15. Enable Multiple Listeners with Undertow
78,3.16. Create WebSocket Endpoints Using @ServerEndpoint
78,4. Spring MVC
78,4.1. Write a JSON REST Service
78,4.2. Write an XML REST Service
78,4.3. Customize the Jackson ObjectMapper
78,4.4. Customize the @ResponseBody Rendering
78,4.5. Handling Multipart File Uploads
78,4.6. Switch Off the Spring MVC DispatcherServlet
78,4.7. Switch off the Default MVC Configuration
78,4.8. Customize ViewResolvers
78,5. Jersey
78,5.1. Secure Jersey endpoints with Spring Security
78,5.2. Use Jersey Alongside Another Web Framework
78,6. HTTP Clients
78,6.1. Configure RestTemplate to Use a Proxy
78,6.2. Configure the TcpClient used by a Reactor Netty-based WebClient
78,7. Logging
78,7.1. Configure Logback for Logging
78,7.1.1. Configure Logback for File-only Output
78,7.2. Configure Log4j for Logging
78,7.2.1. Use YAML or JSON to Configure Log4j 2
78,7.2.2. Use Composite Configuration to Configure Log4j 2
78,8. Data Access
78,8.1. Configure a Custom DataSource
78,8.2. Configure Two DataSources
78,8.3. Use Spring Data Repositories
78,8.4. Separate @Entity Definitions from Spring Configuration
78,8.5. Configure JPA Properties
78,8.6. Configure Hibernate Naming Strategy
78,8.7. Configure Hibernate Second-Level Caching
78,8.8. Use Dependency Injection in Hibernate Components
78,8.9. Use a Custom EntityManagerFactory
78,8.10. Using Multiple EntityManagerFactories
78,8.11. Use a Traditional persistence.xml File
78,8.12. Use Spring Data JPA and Mongo Repositories
78,8.13. Customize Spring Data’s Web Support
78,8.14. Expose Spring Data Repositories as REST Endpoint
78,8.15. Configure a Component that is Used by JPA
78,8.16. Configure jOOQ with Two DataSources
78,9. Database Initialization
78,9.1. Initialize a Database Using JPA
78,9.2. Initialize a Database Using Hibernate
78,9.3. Initialize a Database Using Basic SQL Scripts
78,9.4. Initialize a Spring Batch Database
78,9.5. Use a Higher-level Database Migration Tool
78,9.5.1. Execute Flyway Database Migrations on Startup
78,9.5.2. Execute Liquibase Database Migrations on Startup
78,9.5.3. Use Flyway for test-only migrations
78,9.5.4. Use Liquibase for test-only migrations
78,9.6. Depend Upon an Initialized Database
78,9.6.1. Detect a Database Initializer
78,9.6.2. Detect a Bean That Depends On Database Initialization
78,10. NoSQL
78,10.1. Use Jedis Instead of Lettuce
78,11. Messaging
78,11.1. Disable Transacted JMS Session
78,12. Batch Applications
78,12.1. Specifying a Batch Data Source
78,12.2. Running Spring Batch Jobs on Startup
78,12.3. Running From the Command Line
78,12.4. Restarting a stopped or failed Job
78,12.5. Storing the Job Repository
78,13. Actuator
78,13.1. Change the HTTP Port or Address of the Actuator Endpoints
78,13.2. Customize the ‘whitelabel’ Error Page
78,13.3. Customizing Sanitization
78,13.4. Map Health Indicators to Micrometer Metrics
78,14. Security
78,14.1. Switch off the Spring Boot Security Configuration
78,14.2. Change the UserDetailsService and Add User Accounts
78,14.3. Enable HTTPS When Running behind a Proxy Server
78,15. Hot Swapping
78,15.1. Reload Static Content
78,15.2. Reload Templates without Restarting the Container
78,15.2.1. Thymeleaf Templates
78,15.2.2. FreeMarker Templates
78,15.2.3. Groovy Templates
78,15.3. Fast Application Restarts
78,15.4. Reload Java Classes without Restarting the Container
78,16. Testing
78,16.1. Testing With Spring Security
78,16.2. Structure @Configuration classes for inclusion in slice tests
78,17. Build
78,17.1. Generate Build Information
78,17.2. Generate Git Information
78,17.3. Customize Dependency Versions
78,17.4. Create an Executable JAR with Maven
78,17.5. Use a Spring Boot Application as a Dependency
78,17.6. Extract Specific Libraries When an Executable Jar Runs
78,17.7. Create a Non-executable JAR with Exclusions
78,17.8. Remote Debug a Spring Boot Application Started with Maven
78,17.9. Build an Executable Archive From Ant without Using spring-boot-antlib
78,18. Ahead-of-time processing
78,18.1. Conditions
78,19. Traditional Deployment
78,19.1. Create a Deployable War File
78,19.2. Convert an Existing Application to Spring Boot
78,19.3. Deploying a WAR to WebLogic
78,20. Docker Compose
78,20.1. Customizing the JDBC URL
78,20.2. Sharing services between multiple applications
78,This section provides answers to some common ‘how do I do that…​’ questions that often arise when using Spring Boot.
78,"Its coverage is not exhaustive, but it does cover quite a lot."
78,"If you have a specific problem that we do not cover here, you might want to check stackoverflow.com to see if someone has already provided an answer."
78,This is also a great place to ask new questions (please use the spring-boot tag).
78,We are also more than happy to extend this section.
78,"If you want to add a ‘how-to’, send us a pull request."
78,1. Spring Boot Application
78,This section includes topics relating directly to Spring Boot applications.
78,1.1. Create Your Own FailureAnalyzer
78,"FailureAnalyzer is a great way to intercept an exception on startup and turn it into a human-readable message, wrapped in a FailureAnalysis."
78,"Spring Boot provides such an analyzer for application-context-related exceptions, JSR-303 validations, and more."
78,You can also create your own.
78,AbstractFailureAnalyzer is a convenient extension of FailureAnalyzer that checks the presence of a specified exception type in the exception to handle.
78,You can extend from that so that your implementation gets a chance to handle the exception only when it is actually present.
78,"If, for whatever reason, you cannot handle the exception, return null to give another implementation a chance to handle the exception."
78,FailureAnalyzer implementations must be registered in META-INF/spring.factories.
78,The following example registers ProjectConstraintViolationFailureAnalyzer:
78,org.springframework.boot.diagnostics.FailureAnalyzer=\
78,com.example.ProjectConstraintViolationFailureAnalyzer
78,"If you need access to the BeanFactory or the Environment, declare them as constructor arguments in your FailureAnalyzer implementation."
78,1.2. Troubleshoot Auto-configuration
78,"The Spring Boot auto-configuration tries its best to “do the right thing”, but sometimes things fail, and it can be hard to tell why."
78,There is a really useful ConditionEvaluationReport available in any Spring Boot ApplicationContext.
78,You can see it if you enable DEBUG logging output.
78,"If you use the spring-boot-actuator (see the Actuator chapter), there is also a conditions endpoint that renders the report in JSON."
78,Use that endpoint to debug the application and see what features have been added (and which have not been added) by Spring Boot at runtime.
78,Many more questions can be answered by looking at the source code and the Javadoc.
78,"When reading the code, remember the following rules of thumb:"
78,Look for classes called *AutoConfiguration and read their sources.
78,Pay special attention to the @Conditional* annotations to find out what features they enable and when.
78,Add --debug to the command line or a System property -Ddebug to get a log on the console of all the auto-configuration decisions that were made in your app.
78,"In a running application with actuator enabled, look at the conditions endpoint (/actuator/conditions or the JMX equivalent) for the same information."
78,Look for classes that are @ConfigurationProperties (such as ServerProperties) and read from there the available external configuration options.
78,The @ConfigurationProperties annotation has a name attribute that acts as a prefix to external properties.
78,"Thus, ServerProperties has prefix=""server"" and its configuration properties are server.port, server.address, and others."
78,"In a running application with actuator enabled, look at the configprops endpoint."
78,Look for uses of the bind method on the Binder to pull configuration values explicitly out of the Environment in a relaxed manner.
78,It is often used with a prefix.
78,Look for @Value annotations that bind directly to the Environment.
78,"Look for @ConditionalOnExpression annotations that switch features on and off in response to SpEL expressions, normally evaluated with placeholders resolved from the Environment."
78,1.3. Customize the Environment or ApplicationContext Before It Starts
78,A SpringApplication has ApplicationListeners and ApplicationContextInitializers that are used to apply customizations to the context or environment.
78,Spring Boot loads a number of such customizations for use internally from META-INF/spring.factories.
78,There is more than one way to register additional customizations:
78,"Programmatically, per application, by calling the addListeners and addInitializers methods on SpringApplication before you run it."
78,"Declaratively, for all applications, by adding a META-INF/spring.factories and packaging a jar file that the applications all use as a library."
78,The SpringApplication sends some special ApplicationEvents to the listeners (some even before the context is created) and then registers the listeners for events published by the ApplicationContext as well.
78,See “Application Events and Listeners” in the ‘Spring Boot features’ section for a complete list.
78,It is also possible to customize the Environment before the application context is refreshed by using EnvironmentPostProcessor.
78,"Each implementation should be registered in META-INF/spring.factories, as shown in the following example:"
78,org.springframework.boot.env.EnvironmentPostProcessor=com.example.YourEnvironmentPostProcessor
78,The implementation can load arbitrary files and add them to the Environment.
78,"For instance, the following example loads a YAML configuration file from the classpath:"
78,Java
78,import java.io.IOException;
78,import org.springframework.boot.SpringApplication;
78,import org.springframework.boot.env.EnvironmentPostProcessor;
78,import org.springframework.boot.env.YamlPropertySourceLoader;
78,import org.springframework.core.env.ConfigurableEnvironment;
78,import org.springframework.core.env.PropertySource;
78,import org.springframework.core.io.ClassPathResource;
78,import org.springframework.core.io.Resource;
78,import org.springframework.util.Assert;
78,public class MyEnvironmentPostProcessor implements EnvironmentPostProcessor {
78,private final YamlPropertySourceLoader loader = new YamlPropertySourceLoader();
78,@Override
78,"public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) {"
78,"Resource path = new ClassPathResource(""com/example/myapp/config.yml"");"
78,PropertySource<?> propertySource = loadYaml(path);
78,environment.getPropertySources().addLast(propertySource);
78,private PropertySource<?> loadYaml(Resource path) {
78,"Assert.isTrue(path.exists(), () -> ""Resource "" + path + "" does not exist"");"
78,try {
78,"return this.loader.load(""custom-resource"", path).get(0);"
78,catch (IOException ex) {
78,"throw new IllegalStateException(""Failed to load yaml configuration from "" + path, ex);"
78,Kotlin
78,import org.springframework.boot.SpringApplication
78,import org.springframework.boot.env.EnvironmentPostProcessor
78,import org.springframework.boot.env.YamlPropertySourceLoader
78,import org.springframework.core.env.ConfigurableEnvironment
78,import org.springframework.core.env.PropertySource
78,import org.springframework.core.io.ClassPathResource
78,import org.springframework.core.io.Resource
78,import org.springframework.util.Assert
78,import java.io.IOException
78,class MyEnvironmentPostProcessor : EnvironmentPostProcessor {
78,private val loader = YamlPropertySourceLoader()
78,"override fun postProcessEnvironment(environment: ConfigurableEnvironment, application: SpringApplication) {"
78,"val path: Resource = ClassPathResource(""com/example/myapp/config.yml"")"
78,val propertySource = loadYaml(path)
78,environment.propertySources.addLast(propertySource)
78,private fun loadYaml(path: Resource): PropertySource<*> {
78,"Assert.isTrue(path.exists()) { ""Resource $path does not exist"" }"
78,return try {
78,"loader.load(""custom-resource"", path)[0]"
78,} catch (ex: IOException) {
78,"throw IllegalStateException(""Failed to load yaml configuration from $path"", ex)"
78,The Environment has already been prepared with all the usual property sources that Spring Boot loads by default.
78,It is therefore possible to get the location of the file from the environment.
78,The preceding example adds the custom-resource property source at the end of the list so that a key defined in any of the usual other locations takes precedence.
78,A custom implementation may define another order.
78,"While using @PropertySource on your @SpringBootApplication may seem to be a convenient way to load a custom resource in the Environment, we do not recommend it."
78,Such property sources are not added to the Environment until the application context is being refreshed.
78,This is too late to configure certain properties such as logging.* and spring.main.* which are read before refresh begins.
78,1.4. Build an ApplicationContext Hierarchy (Adding a Parent or Root Context)
78,You can use the ApplicationBuilder class to create parent/child ApplicationContext hierarchies.
78,See “features.html” in the ‘Spring Boot features’ section for more information.
78,1.5. Create a Non-web Application
78,Not all Spring applications have to be web applications (or web services).
78,"If you want to execute some code in a main method but also bootstrap a Spring application to set up the infrastructure to use, you can use the SpringApplication features of Spring Boot."
78,"A SpringApplication changes its ApplicationContext class, depending on whether it thinks it needs a web application or not."
78,The first thing you can do to help it is to leave server-related dependencies (such as the servlet API) off the classpath.
78,"If you cannot do that (for example, you run two applications from the same code base) then you can explicitly call setWebApplicationType(WebApplicationType.NONE) on your SpringApplication instance or set the applicationContextClass property (through the Java API or with external properties)."
78,Application code that you want to run as your business logic can be implemented as a CommandLineRunner and dropped into the context as a @Bean definition.
78,2. Properties and Configuration
78,This section includes topics about setting and reading properties and configuration settings and their interaction with Spring Boot applications.
78,2.1. Automatically Expand Properties at Build Time
78,"Rather than hardcoding some properties that are also specified in your project’s build configuration, you can automatically expand them by instead using the existing build configuration."
78,This is possible in both Maven and Gradle.
78,2.1.1. Automatic Property Expansion Using Maven
78,You can automatically expand properties from the Maven project by using resource filtering.
78,"If you use the spring-boot-starter-parent, you can then refer to your Maven ‘project properties’ with @..@ placeholders, as shown in the following example:"
78,Properties
78,[email protected]@
78,[email protected]@
78,Yaml
78,app:
78,"encoding: ""@project.build.sourceEncoding@"""
78,java:
78,"version: ""@java.version@"""
78,"Only production configuration is filtered that way (in other words, no filtering is applied on src/test/resources)."
78,"If you enable the addResources flag, the spring-boot:run goal can add src/main/resources directly to the classpath (for hot reloading purposes)."
78,Doing so circumvents the resource filtering and this feature.
78,"Instead, you can use the exec:java goal or customize the plugin’s configuration."
78,See the plugin usage page for more details.
78,"If you do not use the starter parent, you need to include the following"
78,element inside the <build/> element of your pom.xml:
78,<resources>
78,<resource>
78,<directory>src/main/resources</directory>
78,<filtering>true</filtering>
78,</resource>
78,</resources>
78,You also need to include the following element inside <plugins/>:
78,<plugin>
78,<groupId>org.apache.maven.plugins</groupId>
78,<artifactId>maven-resources-plugin</artifactId>
78,<version>2.7</version>
78,<configuration>
78,<delimiters>
78,<delimiter>@</delimiter>
78,</delimiters>
78,<useDefaultDelimiters>false</useDefaultDelimiters>
78,</configuration>
78,</plugin>
78,The useDefaultDelimiters property is important if you use standard Spring placeholders (such as ${placeholder}) in your configuration.
78,"If that property is not set to false, these may be expanded by the build."
78,2.1.2. Automatic Property Expansion Using Gradle
78,"You can automatically expand properties from the Gradle project by configuring the Java plugin’s processResources task to do so, as shown in the following example:"
78,tasks.named('processResources') {
78,expand(project.properties)
78,"You can then refer to your Gradle project’s properties by using placeholders, as shown in the following example:"
78,Properties
78,app.name=${name}
78,app.description=${description}
78,Yaml
78,app:
78,"name: ""${name}"""
78,"description: ""${description}"""
78,"Gradle’s expand method uses Groovy’s SimpleTemplateEngine, which transforms ${..} tokens."
78,The ${..} style conflicts with Spring’s own property placeholder mechanism.
78,"To use Spring property placeholders together with automatic expansion, escape the Spring property placeholders as follows: \${..}."
78,2.2. Externalize the Configuration of SpringApplication
78,"A SpringApplication has bean property setters, so you can use its Java API as you create the application to modify its behavior."
78,"Alternatively, you can externalize the configuration by setting properties in spring.main.*."
78,"For example, in application.properties, you might have the following settings:"
78,Properties
78,spring.main.web-application-type=none
78,spring.main.banner-mode=off
78,Yaml
78,spring:
78,main:
78,"web-application-type: ""none"""
78,"banner-mode: ""off"""
78,"Then the Spring Boot banner is not printed on startup, and the application is not starting an embedded web server."
78,"Properties defined in external configuration override and replace the values specified with the Java API, with the notable exception of the primary sources."
78,Primary sources are those provided to the SpringApplication constructor:
78,Java
78,import org.springframework.boot.Banner;
78,import org.springframework.boot.SpringApplication;
78,import org.springframework.boot.autoconfigure.SpringBootApplication;
78,@SpringBootApplication
78,public class MyApplication {
78,public static void main(String[] args) {
78,SpringApplication application = new SpringApplication(MyApplication.class);
78,application.setBannerMode(Banner.Mode.OFF);
78,application.run(args);
78,Kotlin
78,import org.springframework.boot.Banner
78,import org.springframework.boot.SpringApplication
78,import org.springframework.boot.autoconfigure.SpringBootApplication
78,@SpringBootApplication
78,object MyApplication {
78,@JvmStatic
78,fun main(args: Array<String>) {
78,val application = SpringApplication(MyApplication::class.java)
78,application.setBannerMode(Banner.Mode.OFF)
78,application.run(*args)
78,Or to sources(…​) method of a SpringApplicationBuilder:
78,Java
78,import org.springframework.boot.Banner;
78,import org.springframework.boot.builder.SpringApplicationBuilder;
78,public class MyApplication {
78,public static void main(String[] args) {
78,new SpringApplicationBuilder()
78,.bannerMode(Banner.Mode.OFF)
78,.sources(MyApplication.class)
78,.run(args);
78,Kotlin
78,import org.springframework.boot.Banner
78,import org.springframework.boot.builder.SpringApplicationBuilder
78,object MyApplication {
78,@JvmStatic
78,fun main(args: Array<String>) {
78,SpringApplicationBuilder()
78,.bannerMode(Banner.Mode.OFF)
78,.sources(MyApplication::class.java)
78,.run(*args)
78,"Given the examples above, if we have the following configuration:"
78,Properties
78,"spring.main.sources=com.example.MyDatabaseConfig,com.example.MyJmsConfig"
78,spring.main.banner-mode=console
78,Yaml
78,spring:
78,main:
78,"sources: ""com.example.MyDatabaseConfig,com.example.MyJmsConfig"""
78,"banner-mode: ""console"""
78,The actual application will show the banner (as overridden by configuration) and uses three sources for the ApplicationContext.
78,The application sources are:
78,MyApplication (from the code)
78,MyDatabaseConfig (from the external config)
78,MyJmsConfig(from the external config)
78,2.3. Change the Location of External Properties of an Application
78,"By default, properties from different sources are added to the Spring Environment in a defined order (see “features.html” in the ‘Spring Boot features’ section for the exact order)."
78,You can also provide the following System properties (or environment variables) to change the behavior:
78,spring.config.name (SPRING_CONFIG_NAME): Defaults to application as the root of the file name.
78,spring.config.location (SPRING_CONFIG_LOCATION): The file to load (such as a classpath resource or a URL).
78,"A separate Environment property source is set up for this document and it can be overridden by system properties, environment variables, or the command line."
78,"No matter what you set in the environment, Spring Boot always loads application.properties as described above."
78,"By default, if YAML is used, then files with the ‘.yaml’ and ‘.yml’ extension are also added to the list."
78,If you want detailed information about the files that are being loaded you can set the logging level of org.springframework.boot.context.config to trace.
78,2.4. Use ‘Short’ Command Line Arguments
78,Some people like to use (for example) --port=9000 instead of --server.port=9000 to set configuration properties on the command line.
78,"You can enable this behavior by using placeholders in application.properties, as shown in the following example:"
78,Properties
78,server.port=${port:8080}
78,Yaml
78,server:
78,"port: ""${port:8080}"""
78,"If you inherit from the spring-boot-starter-parent POM, the default filter token of the maven-resources-plugins has been changed from ${*} to @ (that is, @maven.token@ instead of ${maven.token}) to prevent conflicts with Spring-style placeholders."
78,"If you have enabled Maven filtering for the application.properties directly, you may want to also change the default filter token to use other delimiters."
78,"In this specific case, the port binding works in a PaaS environment such as Heroku or Cloud Foundry."
78,"In those two platforms, the PORT environment variable is set automatically and Spring can bind to capitalized synonyms for Environment properties."
78,2.5. Use YAML for External Properties
78,"YAML is a superset of JSON and, as such, is a convenient syntax for storing external properties in a hierarchical format, as shown in the following example:"
78,spring:
78,application:
78,"name: ""cruncher"""
78,datasource:
78,"driver-class-name: ""com.mysql.jdbc.Driver"""
78,"url: ""jdbc:mysql://localhost/test"""
78,server:
78,port: 9000
78,Create a file called application.yaml and put it in the root of your classpath.
78,"Then add snakeyaml to your dependencies (Maven coordinates org.yaml:snakeyaml, already included if you use the spring-boot-starter)."
78,"A YAML file is parsed to a Java Map<String,Object> (like a JSON object), and Spring Boot flattens the map so that it is one level deep and has period-separated keys, as many people are used to with Properties files in Java."
78,The preceding example YAML corresponds to the following application.properties file:
78,spring.application.name=cruncher
78,spring.datasource.driver-class-name=com.mysql.jdbc.Driver
78,spring.datasource.url=jdbc:mysql://localhost/test
78,server.port=9000
78,See “features.html” in the ‘Spring Boot features’ section for more information about YAML.
78,2.6. Set the Active Spring Profiles
78,"The Spring Environment has an API for this, but you would normally set a System property (spring.profiles.active) or an OS environment variable (SPRING_PROFILES_ACTIVE)."
78,"Also, you can launch your application with a -D argument (remember to put it before the main class or jar archive), as follows:"
78,$ java -jar -Dspring.profiles.active=production demo-0.0.1-SNAPSHOT.jar
78,"In Spring Boot, you can also set the active profile in application.properties, as shown in the following example:"
78,Properties
78,spring.profiles.active=production
78,Yaml
78,spring:
78,profiles:
78,"active: ""production"""
78,A value set this way is replaced by the System property or environment variable setting but not by the SpringApplicationBuilder.profiles() method.
78,"Thus, the latter Java API can be used to augment the profiles without changing the defaults."
78,See “features.html” in the “Spring Boot features” section for more information.
78,2.7. Set the Default Profile Name
78,The default profile is a profile that is enabled if no profile is active.
78,"By default, the name of the default profile is default, but it could be changed using a System property (spring.profiles.default) or an OS environment variable (SPRING_PROFILES_DEFAULT)."
78,"In Spring Boot, you can also set the default profile name in application.properties, as shown in the following example:"
78,Properties
78,spring.profiles.default=dev
78,Yaml
78,spring:
78,profiles:
78,"default: ""dev"""
78,See “features.html” in the “Spring Boot features” section for more information.
78,2.8. Change Configuration Depending on the Environment
78,Spring Boot supports multi-document YAML and Properties files (see features.html for details) which can be activated conditionally based on the active profiles.
78,"If a document contains a spring.config.activate.on-profile key, then the profiles value (a comma-separated list of profiles or a profile expression) is fed into the Spring Environment.acceptsProfiles() method."
78,"If the profile expression matches then that document is included in the final merge (otherwise, it is not), as shown in the following example:"
78,Properties
78,server.port=9000
78,#---
78,spring.config.activate.on-profile=development
78,server.port=9001
78,#---
78,spring.config.activate.on-profile=production
78,server.port=0
78,Yaml
78,server:
78,port: 9000
78,---
78,spring:
78,config:
78,activate:
78,"on-profile: ""development"""
78,server:
78,port: 9001
78,---
78,spring:
78,config:
78,activate:
78,"on-profile: ""production"""
78,server:
78,port: 0
78,"In the preceding example, the default port is 9000."
78,"However, if the Spring profile called ‘development’ is active, then the port is 9001."
78,"If ‘production’ is active, then the port is 0."
78,The documents are merged in the order in which they are encountered.
78,Later values override earlier values.
78,2.9. Discover Built-in Options for External Properties
78,Spring Boot binds external properties from application.properties (or YAML files and other places) into an application at runtime.
78,"There is not (and technically cannot be) an exhaustive list of all supported properties in a single location, because contributions can come from additional jar files on your classpath."
78,A running application with the Actuator features has a configprops endpoint that shows all the bound and bindable properties available through @ConfigurationProperties.
78,The appendix includes an application.properties example with a list of the most common properties supported by Spring Boot.
78,The definitive list comes from searching the source code for @ConfigurationProperties and @Value annotations as well as the occasional use of Binder.
78,"For more about the exact ordering of loading properties, see ""features.html""."
78,3. Embedded Web Servers
78,Each Spring Boot web application includes an embedded web server.
78,"This feature leads to a number of how-to questions, including how to change the embedded server and how to configure the embedded server."
78,This section answers those questions.
78,3.1. Use Another Web Server
78,Many Spring Boot starters include default embedded containers.
78,"For servlet stack applications, the spring-boot-starter-web includes Tomcat by including spring-boot-starter-tomcat, but you can use spring-boot-starter-jetty or spring-boot-starter-undertow instead."
78,"For reactive stack applications, the spring-boot-starter-webflux includes"
78,"Reactor Netty by including spring-boot-starter-reactor-netty, but you can use spring-boot-starter-tomcat, spring-boot-starter-jetty, or spring-boot-starter-undertow instead."
78,"When switching to a different HTTP server, you need to swap the default dependencies for those that you need instead."
78,"To help with this process, Spring Boot provides a separate starter for each of the supported HTTP servers."
78,The following Maven example shows how to exclude Tomcat and include Jetty for Spring MVC:
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-web</artifactId>
78,<exclusions>
78,<!-- Exclude the Tomcat dependency -->
78,<exclusion>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-tomcat</artifactId>
78,</exclusion>
78,</exclusions>
78,</dependency>
78,<!-- Use Jetty instead -->
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-jetty</artifactId>
78,</dependency>
78,The following Gradle example configures the necessary dependencies and a module replacement to use Undertow in place of Reactor Netty for Spring WebFlux:
78,dependencies {
78,"implementation ""org.springframework.boot:spring-boot-starter-undertow"""
78,"implementation ""org.springframework.boot:spring-boot-starter-webflux"""
78,modules {
78,"module(""org.springframework.boot:spring-boot-starter-reactor-netty"") {"
78,"replacedBy(""org.springframework.boot:spring-boot-starter-undertow"", ""Use Undertow instead of Reactor Netty"")"
78,"spring-boot-starter-reactor-netty is required to use the WebClient class, so you may need to keep a dependency on Netty even when you need to include a different HTTP server."
78,3.2. Disabling the Web Server
78,"If your classpath contains the necessary bits to start a web server, Spring Boot will automatically start it."
78,"To disable this behavior configure the WebApplicationType in your application.properties, as shown in the following example:"
78,Properties
78,spring.main.web-application-type=none
78,Yaml
78,spring:
78,main:
78,"web-application-type: ""none"""
78,3.3. Change the HTTP Port
78,"In a standalone application, the main HTTP port defaults to 8080 but can be set with server.port (for example, in application.properties or as a System property)."
78,"Thanks to relaxed binding of Environment values, you can also use SERVER_PORT (for example, as an OS environment variable)."
78,"To switch off the HTTP endpoints completely but still create a WebApplicationContext, use server.port=-1 (doing so is sometimes useful for testing)."
78,"For more details, see “web.html” in the ‘Spring Boot Features’ section, or the ServerProperties source code."
78,3.4. Use a Random Unassigned HTTP Port
78,To scan for a free port (using OS natives to prevent clashes) use server.port=0.
78,3.5. Discover the HTTP Port at Runtime
78,You can access the port the server is running on from log output or from the WebServerApplicationContext through its WebServer.
78,The best way to get that and be sure it has been initialized is to add a @Bean of type ApplicationListener<WebServerInitializedEvent> and pull the container out of the event when it is published.
78,"Tests that use @SpringBootTest(webEnvironment=WebEnvironment.RANDOM_PORT) can also inject the actual port into a field by using the @LocalServerPort annotation, as shown in the following example:"
78,Java
78,import org.springframework.boot.test.context.SpringBootTest;
78,import org.springframework.boot.test.context.SpringBootTest.WebEnvironment;
78,import org.springframework.boot.test.web.server.LocalServerPort;
78,@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT)
78,class MyWebIntegrationTests {
78,@LocalServerPort
78,int port;
78,// ...
78,Kotlin
78,import org.springframework.boot.test.context.SpringBootTest
78,import org.springframework.boot.test.context.SpringBootTest.WebEnvironment
78,import org.springframework.boot.test.web.server.LocalServerPort
78,@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT)
78,class MyWebIntegrationTests {
78,@LocalServerPort
78,var port = 0
78,// ...
78,"@LocalServerPort is a meta-annotation for @Value(""${local.server.port}"")."
78,Do not try to inject the port in a regular application.
78,"As we just saw, the value is set only after the container has been initialized."
78,"Contrary to a test, application code callbacks are processed early (before the value is actually available)."
78,3.6. Enable HTTP Response Compression
78,"HTTP response compression is supported by Jetty, Tomcat, Reactor Netty, and Undertow."
78,"It can be enabled in application.properties, as follows:"
78,Properties
78,server.compression.enabled=true
78,Yaml
78,server:
78,compression:
78,enabled: true
78,"By default, responses must be at least 2048 bytes in length for compression to be performed."
78,You can configure this behavior by setting the server.compression.min-response-size property.
78,"By default, responses are compressed only if their content type is one of the following:"
78,text/html
78,text/xml
78,text/plain
78,text/css
78,text/javascript
78,application/javascript
78,application/json
78,application/xml
78,You can configure this behavior by setting the server.compression.mime-types property.
78,3.7. Configure SSL
78,"SSL can be configured declaratively by setting the various server.ssl.* properties, typically in application.properties or application.yaml."
78,The following example shows setting SSL properties using a Java KeyStore file:
78,Properties
78,server.port=8443
78,server.ssl.key-store=classpath:keystore.jks
78,server.ssl.key-store-password=secret
78,server.ssl.key-password=another-secret
78,Yaml
78,server:
78,port: 8443
78,ssl:
78,"key-store: ""classpath:keystore.jks"""
78,"key-store-password: ""secret"""
78,"key-password: ""another-secret"""
78,Using configuration such as the preceding example means the application no longer supports a plain HTTP connector at port 8080.
78,Spring Boot does not support the configuration of both an HTTP connector and an HTTPS connector through application.properties.
78,"If you want to have both, you need to configure one of them programmatically."
78,"We recommend using application.properties to configure HTTPS, as the HTTP connector is the easier of the two to configure programmatically."
78,3.7.1. Using PEM-encoded files
78,You can use PEM-encoded files instead of Java KeyStore files.
78,You should use PKCS#8 key files wherever possible.
78,PEM-encoded PKCS#8 key files start with a -----BEGIN PRIVATE KEY----- or -----BEGIN ENCRYPTED PRIVATE KEY----- header.
78,"If you have files in other formats, e.g., PKCS#1 (-----BEGIN RSA PRIVATE KEY-----) or SEC 1 (-----BEGIN EC PRIVATE KEY-----), you can convert them to PKCS#8 using OpenSSL:"
78,openssl pkcs8 -topk8 -nocrypt -in <input file> -out <output file>
78,The following example shows setting SSL properties using PEM-encoded certificate and private key files:
78,Properties
78,server.port=8443
78,server.ssl.certificate=classpath:my-cert.crt
78,server.ssl.certificate-private-key=classpath:my-cert.key
78,server.ssl.trust-certificate=classpath:ca-cert.crt
78,Yaml
78,server:
78,port: 8443
78,ssl:
78,"certificate: ""classpath:my-cert.crt"""
78,"certificate-private-key: ""classpath:my-cert.key"""
78,"trust-certificate: ""classpath:ca-cert.crt"""
78,"Alternatively, the SSL trust material can be configured in an SSL bundle and applied to the web server as shown in this example:"
78,Properties
78,server.port=8443
78,server.ssl.bundle=example
78,Yaml
78,server:
78,port: 8443
78,ssl:
78,"bundle: ""example"""
78,The server.ssl.bundle property can not be combined with the discrete Java KeyStore or PEM property options under server.ssl.
78,See Ssl for details of all of the supported properties.
78,3.8. Configure HTTP/2
78,You can enable HTTP/2 support in your Spring Boot application with the server.http2.enabled configuration property.
78,Both h2 (HTTP/2 over TLS) and h2c (HTTP/2 over TCP) are supported.
78,"To use h2, SSL must also be enabled."
78,"When SSL is not enabled, h2c will be used."
78,"You may, for example, want to use h2c when your application is running behind a proxy server that is performing TLS termination."
78,3.8.1. HTTP/2 With Tomcat
78,Spring Boot ships by default with Tomcat 10.1.x which supports h2c and h2 out of the box.
78,"Alternatively, you can use libtcnative for h2 support if the library and its dependencies are installed on the host operating system."
78,"The library directory must be made available, if not already, to the JVM library path."
78,You can do so with a JVM argument such as -Djava.library.path=/usr/local/opt/tomcat-native/lib.
78,More on this in the official Tomcat documentation.
78,3.8.2. HTTP/2 With Jetty
78,"For HTTP/2 support, Jetty requires the additional org.eclipse.jetty.http2:jetty-http2-server dependency."
78,To use h2c no other dependencies are required.
78,"To use h2, you also need to choose one of the following dependencies, depending on your deployment:"
78,org.eclipse.jetty:jetty-alpn-java-server to use the JDK built-in support
78,org.eclipse.jetty:jetty-alpn-conscrypt-server and the Conscrypt library
78,3.8.3. HTTP/2 With Reactor Netty
78,The spring-boot-webflux-starter is using by default Reactor Netty as a server.
78,Reactor Netty supports h2c and h2 out of the box.
78,"For optimal runtime performance, this server also supports h2 with native libraries."
78,"To enable that, your application needs to have an additional dependency."
78,"Spring Boot manages the version for the io.netty:netty-tcnative-boringssl-static ""uber jar"", containing native libraries for all platforms."
78,Developers can choose to import only the required dependencies using a classifier (see the Netty official documentation).
78,3.8.4. HTTP/2 With Undertow
78,Undertow supports h2c and h2 out of the box.
78,3.9. Configure the Web Server
78,"Generally, you should first consider using one of the many available configuration keys and customize your web server by adding new entries in your application.properties or application.yaml file."
78,See “Discover Built-in Options for External Properties”).
78,"The server.* namespace is quite useful here, and it includes namespaces like server.tomcat.*, server.jetty.* and others, for server-specific features."
78,See the list of application-properties.html.
78,"The previous sections covered already many common use cases, such as compression, SSL or HTTP/2."
78,"However, if a configuration key does not exist for your use case, you should then look at WebServerFactoryCustomizer."
78,"You can declare such a component and get access to the server factory relevant to your choice: you should select the variant for the chosen Server (Tomcat, Jetty, Reactor Netty, Undertow) and the chosen web stack (servlet or reactive)."
78,The example below is for Tomcat with the spring-boot-starter-web (servlet stack):
78,Java
78,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
78,import org.springframework.stereotype.Component;
78,@Component
78,public class MyTomcatWebServerCustomizer implements WebServerFactoryCustomizer<TomcatServletWebServerFactory> {
78,@Override
78,public void customize(TomcatServletWebServerFactory factory) {
78,// customize the factory here
78,Kotlin
78,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer
78,import org.springframework.stereotype.Component
78,@Component
78,class MyTomcatWebServerCustomizer : WebServerFactoryCustomizer<TomcatServletWebServerFactory?> {
78,override fun customize(factory: TomcatServletWebServerFactory?) {
78,// customize the factory here
78,Spring Boot uses that infrastructure internally to auto-configure the server.
78,"Auto-configured WebServerFactoryCustomizer beans have an order of 0 and will be processed before any user-defined customizers, unless it has an explicit order that states otherwise."
78,"Once you have got access to a WebServerFactory using the customizer, you can use it to configure specific parts, like connectors, server resources, or the server itself - all using server-specific APIs."
78,In addition Spring Boot provides:
78,Server
78,Servlet stack
78,Reactive stack
78,Tomcat
78,TomcatServletWebServerFactory
78,TomcatReactiveWebServerFactory
78,Jetty
78,JettyServletWebServerFactory
78,JettyReactiveWebServerFactory
78,Undertow
78,UndertowServletWebServerFactory
78,UndertowReactiveWebServerFactory
78,Reactor
78,N/A
78,NettyReactiveWebServerFactory
78,"As a last resort, you can also declare your own WebServerFactory bean, which will override the one provided by Spring Boot."
78,"When you do so, auto-configured customizers are still applied on your custom factory, so use that option carefully."
78,"3.10. Add a Servlet, Filter, or Listener to an Application"
78,"In a servlet stack application, that is with the spring-boot-starter-web, there are two ways to add Servlet, Filter, ServletContextListener, and the other listeners supported by the Servlet API to your application:"
78,"Add a Servlet, Filter, or Listener by Using a Spring Bean"
78,"Add Servlets, Filters, and Listeners by Using Classpath Scanning"
78,"3.10.1. Add a Servlet, Filter, or Listener by Using a Spring Bean"
78,"To add a Servlet, Filter, or servlet *Listener by using a Spring bean, you must provide a @Bean definition for it."
78,Doing so can be very useful when you want to inject configuration or dependencies.
78,"However, you must be very careful that they do not cause eager initialization of too many other beans, because they have to be installed in the container very early in the application lifecycle."
78,"(For example, it is not a good idea to have them depend on your DataSource or JPA configuration.)"
78,You can work around such restrictions by initializing the beans lazily when first used instead of on initialization.
78,"In the case of filters and servlets, you can also add mappings and init parameters by adding a FilterRegistrationBean or a ServletRegistrationBean instead of or in addition to the underlying component."
78,"If no dispatcherType is specified on a filter registration, REQUEST is used."
78,This aligns with the servlet specification’s default dispatcher type.
78,"Like any other Spring bean, you can define the order of servlet filter beans; please make sure to check the “web.html” section."
78,Disable Registration of a Servlet or Filter
78,"As described earlier, any Servlet or Filter beans are registered with the servlet container automatically."
78,"To disable registration of a particular Filter or Servlet bean, create a registration bean for it and mark it as disabled, as shown in the following example:"
78,Java
78,import org.springframework.boot.web.servlet.FilterRegistrationBean;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyFilterConfiguration {
78,@Bean
78,public FilterRegistrationBean<MyFilter> registration(MyFilter filter) {
78,FilterRegistrationBean<MyFilter> registration = new FilterRegistrationBean<>(filter);
78,registration.setEnabled(false);
78,return registration;
78,Kotlin
78,import org.springframework.boot.web.servlet.FilterRegistrationBean
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyFilterConfiguration {
78,@Bean
78,fun registration(filter: MyFilter): FilterRegistrationBean<MyFilter> {
78,val registration = FilterRegistrationBean(filter)
78,registration.isEnabled = false
78,return registration
78,"3.10.2. Add Servlets, Filters, and Listeners by Using Classpath Scanning"
78,"@WebServlet, @WebFilter, and @WebListener annotated classes can be automatically registered with an embedded servlet container by annotating a @Configuration class with @ServletComponentScan and specifying the package(s) containing the components that you want to register."
78,"By default, @ServletComponentScan scans from the package of the annotated class."
78,3.11. Configure Access Logging
78,"Access logs can be configured for Tomcat, Undertow, and Jetty through their respective namespaces."
78,"For instance, the following settings log access on Tomcat with a custom pattern."
78,Properties
78,server.tomcat.basedir=my-tomcat
78,server.tomcat.accesslog.enabled=true
78,server.tomcat.accesslog.pattern=%t %a %r %s (%D microseconds)
78,Yaml
78,server:
78,tomcat:
78,"basedir: ""my-tomcat"""
78,accesslog:
78,enabled: true
78,"pattern: ""%t %a %r %s (%D microseconds)"""
78,The default location for logs is a logs directory relative to the Tomcat base directory.
78,"By default, the logs directory is a temporary directory, so you may want to fix Tomcat’s base directory or use an absolute path for the logs."
78,"In the preceding example, the logs are available in my-tomcat/logs relative to the working directory of the application."
78,"Access logging for Undertow can be configured in a similar fashion, as shown in the following example:"
78,Properties
78,server.undertow.accesslog.enabled=true
78,server.undertow.accesslog.pattern=%t %a %r %s (%D milliseconds)
78,server.undertow.options.server.record-request-start-time=true
78,Yaml
78,server:
78,undertow:
78,accesslog:
78,enabled: true
78,"pattern: ""%t %a %r %s (%D milliseconds)"""
78,options:
78,server:
78,record-request-start-time: true
78,"Note that, in addition to enabling access logging and configuring its pattern, recording request start times has also been enabled."
78,This is required when including the response time (%D) in the access log pattern.
78,Logs are stored in a logs directory relative to the working directory of the application.
78,You can customize this location by setting the server.undertow.accesslog.dir property.
78,"Finally, access logging for Jetty can also be configured as follows:"
78,Properties
78,server.jetty.accesslog.enabled=true
78,server.jetty.accesslog.filename=/var/log/jetty-access.log
78,Yaml
78,server:
78,jetty:
78,accesslog:
78,enabled: true
78,"filename: ""/var/log/jetty-access.log"""
78,"By default, logs are redirected to System.err."
78,"For more details, see the Jetty documentation."
78,3.12. Running Behind a Front-end Proxy Server
78,"If your application is running behind a proxy, a load-balancer or in the cloud, the request information (like the host, port, scheme…​) might change along the way."
78,"Your application may be running on 10.10.10.10:8080, but HTTP clients should only see example.org."
78,"RFC7239 ""Forwarded Headers"" defines the Forwarded HTTP header; proxies can use this header to provide information about the original request."
78,"You can configure your application to read those headers and automatically use that information when creating links and sending them to clients in HTTP 302 responses, JSON documents or HTML pages."
78,"There are also non-standard headers, like X-Forwarded-Host, X-Forwarded-Port, X-Forwarded-Proto, X-Forwarded-Ssl, and X-Forwarded-Prefix."
78,"If the proxy adds the commonly used X-Forwarded-For and X-Forwarded-Proto headers, setting server.forward-headers-strategy to NATIVE is enough to support those."
78,"With this option, the Web servers themselves natively support this feature; you can check their specific documentation to learn about specific behavior."
78,"If this is not enough, Spring Framework provides a ForwardedHeaderFilter for the servlet stack and a ForwardedHeaderTransformer for the reactive stack."
78,You can use them in your application by setting server.forward-headers-strategy to FRAMEWORK.
78,"If you are using Tomcat and terminating SSL at the proxy, server.tomcat.redirect-context-root should be set to false."
78,This allows the X-Forwarded-Proto header to be honored before any redirects are performed.
78,"If your application runs in Cloud Foundry, Heroku or Kubernetes, the server.forward-headers-strategy property defaults to NATIVE."
78,"In all other instances, it defaults to NONE."
78,3.12.1. Customize Tomcat’s Proxy Configuration
78,"If you use Tomcat, you can additionally configure the names of the headers used to carry “forwarded” information, as shown in the following example:"
78,Properties
78,server.tomcat.remoteip.remote-ip-header=x-your-remote-ip-header
78,server.tomcat.remoteip.protocol-header=x-your-protocol-header
78,Yaml
78,server:
78,tomcat:
78,remoteip:
78,"remote-ip-header: ""x-your-remote-ip-header"""
78,"protocol-header: ""x-your-protocol-header"""
78,Tomcat is also configured with a regular expression that matches internal proxies that are to be trusted.
78,See the server.tomcat.remoteip.internal-proxies entry in the appendix for its default value.
78,"You can customize the valve’s configuration by adding an entry to application.properties, as shown in the following example:"
78,Properties
78,"server.tomcat.remoteip.internal-proxies=192\\.168\\.\\d{1,3}\\.\\d{1,3}"
78,Yaml
78,server:
78,tomcat:
78,remoteip:
78,"internal-proxies: ""192\\.168\\.\\d{1,3}\\.\\d{1,3}"""
78,You can trust all proxies by setting the internal-proxies to empty (but do not do so in production).
78,"You can take complete control of the configuration of Tomcat’s RemoteIpValve by switching the automatic one off (to do so, set server.forward-headers-strategy=NONE) and adding a new valve instance using a WebServerFactoryCustomizer bean."
78,3.13. Enable Multiple Connectors with Tomcat
78,"You can add an org.apache.catalina.connector.Connector to the TomcatServletWebServerFactory, which can allow multiple connectors, including HTTP and HTTPS connectors, as shown in the following example:"
78,Java
78,import org.apache.catalina.connector.Connector;
78,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyTomcatConfiguration {
78,@Bean
78,public WebServerFactoryCustomizer<TomcatServletWebServerFactory> connectorCustomizer() {
78,return (tomcat) -> tomcat.addAdditionalTomcatConnectors(createConnector());
78,private Connector createConnector() {
78,"Connector connector = new Connector(""org.apache.coyote.http11.Http11NioProtocol"");"
78,connector.setPort(8081);
78,return connector;
78,Kotlin
78,import org.apache.catalina.connector.Connector
78,import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyTomcatConfiguration {
78,@Bean
78,fun connectorCustomizer(): WebServerFactoryCustomizer<TomcatServletWebServerFactory> {
78,return WebServerFactoryCustomizer { tomcat: TomcatServletWebServerFactory ->
78,tomcat.addAdditionalTomcatConnectors(
78,createConnector()
78,private fun createConnector(): Connector {
78,"val connector = Connector(""org.apache.coyote.http11.Http11NioProtocol"")"
78,connector.port = 8081
78,return connector
78,3.14. Enable Tomcat’s MBean Registry
78,Embedded Tomcat’s MBean registry is disabled by default.
78,This minimizes Tomcat’s memory footprint.
78,"If you want to use Tomcat’s MBeans, for example so that they can be used by Micrometer to expose metrics, you must use the server.tomcat.mbeanregistry.enabled property to do so, as shown in the following example:"
78,Properties
78,server.tomcat.mbeanregistry.enabled=true
78,Yaml
78,server:
78,tomcat:
78,mbeanregistry:
78,enabled: true
78,3.15. Enable Multiple Listeners with Undertow
78,"Add an UndertowBuilderCustomizer to the UndertowServletWebServerFactory and add a listener to the Builder, as shown in the following example:"
78,Java
78,import io.undertow.Undertow.Builder;
78,import org.springframework.boot.web.embedded.undertow.UndertowServletWebServerFactory;
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyUndertowConfiguration {
78,@Bean
78,public WebServerFactoryCustomizer<UndertowServletWebServerFactory> undertowListenerCustomizer() {
78,return (factory) -> factory.addBuilderCustomizers(this::addHttpListener);
78,private Builder addHttpListener(Builder builder) {
78,"return builder.addHttpListener(8080, ""0.0.0.0"");"
78,Kotlin
78,import io.undertow.Undertow
78,import org.springframework.boot.web.embedded.undertow.UndertowBuilderCustomizer
78,import org.springframework.boot.web.embedded.undertow.UndertowServletWebServerFactory
78,import org.springframework.boot.web.server.WebServerFactoryCustomizer
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyUndertowConfiguration {
78,@Bean
78,fun undertowListenerCustomizer(): WebServerFactoryCustomizer<UndertowServletWebServerFactory> {
78,return WebServerFactoryCustomizer { factory: UndertowServletWebServerFactory ->
78,factory.addBuilderCustomizers(
78,UndertowBuilderCustomizer { builder: Undertow.Builder -> addHttpListener(builder) })
78,private fun addHttpListener(builder: Undertow.Builder): Undertow.Builder {
78,"return builder.addHttpListener(8080, ""0.0.0.0"")"
78,3.16. Create WebSocket Endpoints Using @ServerEndpoint
78,"If you want to use @ServerEndpoint in a Spring Boot application that used an embedded container, you must declare a single ServerEndpointExporter @Bean, as shown in the following example:"
78,Java
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.web.socket.server.standard.ServerEndpointExporter;
78,@Configuration(proxyBeanMethods = false)
78,public class MyWebSocketConfiguration {
78,@Bean
78,public ServerEndpointExporter serverEndpointExporter() {
78,return new ServerEndpointExporter();
78,Kotlin
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.web.socket.server.standard.ServerEndpointExporter
78,@Configuration(proxyBeanMethods = false)
78,class MyWebSocketConfiguration {
78,@Bean
78,fun serverEndpointExporter(): ServerEndpointExporter {
78,return ServerEndpointExporter()
78,The bean shown in the preceding example registers any @ServerEndpoint annotated beans with the underlying WebSocket container.
78,"When deployed to a standalone servlet container, this role is performed by a servlet container initializer, and the ServerEndpointExporter bean is not required."
78,4. Spring MVC
78,Spring Boot has a number of starters that include Spring MVC.
78,Note that some starters include a dependency on Spring MVC rather than include it directly.
78,This section answers common questions about Spring MVC and Spring Boot.
78,4.1. Write a JSON REST Service
78,"Any Spring @RestController in a Spring Boot application should render JSON response by default as long as Jackson2 is on the classpath, as shown in the following example:"
78,Java
78,import org.springframework.web.bind.annotation.RequestMapping;
78,import org.springframework.web.bind.annotation.RestController;
78,@RestController
78,public class MyController {
78,"@RequestMapping(""/thing"")"
78,public MyThing thing() {
78,return new MyThing();
78,Kotlin
78,import org.springframework.web.bind.annotation.RequestMapping
78,import org.springframework.web.bind.annotation.RestController
78,@RestController
78,class MyController {
78,"@RequestMapping(""/thing"")"
78,fun thing(): MyThing {
78,return MyThing()
78,"As long as MyThing can be serialized by Jackson2 (true for a normal POJO or Groovy object), then localhost:8080/thing serves a JSON representation of it by default."
78,"Note that, in a browser, you might sometimes see XML responses, because browsers tend to send accept headers that prefer XML."
78,4.2. Write an XML REST Service
78,"If you have the Jackson XML extension (jackson-dataformat-xml) on the classpath, you can use it to render XML responses."
78,The previous example that we used for JSON would work.
78,"To use the Jackson XML renderer, add the following dependency to your project:"
78,<dependency>
78,<groupId>com.fasterxml.jackson.dataformat</groupId>
78,<artifactId>jackson-dataformat-xml</artifactId>
78,</dependency>
78,"If Jackson’s XML extension is not available and JAXB is available, XML can be rendered with the additional requirement of having MyThing annotated as @XmlRootElement, as shown in the following example:"
78,Java
78,import jakarta.xml.bind.annotation.XmlRootElement;
78,@XmlRootElement
78,public class MyThing {
78,private String name;
78,// getters/setters ...
78,public String getName() {
78,return this.name;
78,public void setName(String name) {
78,this.name = name;
78,Kotlin
78,import jakarta.xml.bind.annotation.XmlRootElement
78,@XmlRootElement
78,class MyThing {
78,var name: String? = null
78,"You will need to ensure that the JAXB library is part of your project, for example by adding:"
78,<dependency>
78,<groupId>org.glassfish.jaxb</groupId>
78,<artifactId>jaxb-runtime</artifactId>
78,</dependency>
78,"To get the server to render XML instead of JSON, you might have to send an Accept: text/xml header (or use a browser)."
78,4.3. Customize the Jackson ObjectMapper
78,Spring MVC (client and server side) uses HttpMessageConverters to negotiate content conversion in an HTTP exchange.
78,"If Jackson is on the classpath, you already get the default converter(s) provided by Jackson2ObjectMapperBuilder, an instance of which is auto-configured for you."
78,The ObjectMapper (or XmlMapper for Jackson XML converter) instance (created by default) has the following customized properties:
78,MapperFeature.DEFAULT_VIEW_INCLUSION is disabled
78,DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES is disabled
78,SerializationFeature.WRITE_DATES_AS_TIMESTAMPS is disabled
78,SerializationFeature.WRITE_DURATIONS_AS_TIMESTAMPS is disabled
78,Spring Boot also has some features to make it easier to customize this behavior.
78,You can configure the ObjectMapper and XmlMapper instances by using the environment.
78,Jackson provides an extensive suite of on/off features that can be used to configure various aspects of its processing.
78,These features are described in several enums (in Jackson) that map onto properties in the environment:
78,Enum
78,Property
78,Values
78,com.fasterxml.jackson.databind.cfg.EnumFeature
78,spring.jackson.datatype.enum.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.databind.cfg.JsonNodeFeature
78,spring.jackson.datatype.json-node.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.databind.DeserializationFeature
78,spring.jackson.deserialization.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.core.JsonGenerator.Feature
78,spring.jackson.generator.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.databind.MapperFeature
78,spring.jackson.mapper.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.core.JsonParser.Feature
78,spring.jackson.parser.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.databind.SerializationFeature
78,spring.jackson.serialization.<feature_name>
78,"true, false"
78,com.fasterxml.jackson.annotation.JsonInclude.Include
78,spring.jackson.default-property-inclusion
78,"always, non_null, non_absent, non_default, non_empty"
78,"For example, to enable pretty print, set spring.jackson.serialization.indent_output=true."
78,"Note that, thanks to the use of relaxed binding, the case of indent_output does not have to match the case of the corresponding enum constant, which is INDENT_OUTPUT."
78,"This environment-based configuration is applied to the auto-configured Jackson2ObjectMapperBuilder bean and applies to any mappers created by using the builder, including the auto-configured ObjectMapper bean."
78,The context’s Jackson2ObjectMapperBuilder can be customized by one or more Jackson2ObjectMapperBuilderCustomizer beans.
78,"Such customizer beans can be ordered (Boot’s own customizer has an order of 0), letting additional customization be applied both before and after Boot’s customization."
78,Any beans of type com.fasterxml.jackson.databind.Module are automatically registered with the auto-configured Jackson2ObjectMapperBuilder and are applied to any ObjectMapper instances that it creates.
78,This provides a global mechanism for contributing custom modules when you add new features to your application.
78,"If you want to replace the default ObjectMapper completely, either define a @Bean of that type and mark it as @Primary or, if you prefer the builder-based approach, define a Jackson2ObjectMapperBuilder @Bean."
78,"Note that, in either case, doing so disables all auto-configuration of the ObjectMapper."
78,"If you provide any @Beans of type MappingJackson2HttpMessageConverter, they replace the default value in the MVC configuration."
78,"Also, a convenience bean of type HttpMessageConverters is provided (and is always available if you use the default MVC configuration)."
78,It has some useful methods to access the default and user-enhanced message converters.
78,See the “Customize the @ResponseBody Rendering” section and the WebMvcAutoConfiguration source code for more details.
78,4.4. Customize the @ResponseBody Rendering
78,Spring uses HttpMessageConverters to render @ResponseBody (or responses from @RestController).
78,You can contribute additional converters by adding beans of the appropriate type in a Spring Boot context.
78,"If a bean you add is of a type that would have been included by default anyway (such as MappingJackson2HttpMessageConverter for JSON conversions), it replaces the default value."
78,A convenience bean of type HttpMessageConverters is provided and is always available if you use the default MVC configuration.
78,"It has some useful methods to access the default and user-enhanced message converters (For example, it can be useful if you want to manually inject them into a custom RestTemplate)."
78,"As in normal MVC usage, any WebMvcConfigurer beans that you provide can also contribute converters by overriding the configureMessageConverters method."
78,"However, unlike with normal MVC, you can supply only additional converters that you need (because Spring Boot uses the same mechanism to contribute its defaults)."
78,"Finally, if you opt out of the Spring Boot default MVC configuration by providing your own @EnableWebMvc configuration, you can take control completely and do everything manually by using getMessageConverters from WebMvcConfigurationSupport."
78,See the WebMvcAutoConfiguration source code for more details.
78,4.5. Handling Multipart File Uploads
78,Spring Boot embraces the servlet 5 jakarta.servlet.http.Part API to support uploading files.
78,"By default, Spring Boot configures Spring MVC with a maximum size of 1MB per file and a maximum of 10MB of file data in a single request."
78,"You may override these values, the location to which intermediate data is stored (for example, to the /tmp directory), and the threshold past which data is flushed to disk by using the properties exposed in the MultipartProperties class."
78,"For example, if you want to specify that files be unlimited, set the spring.servlet.multipart.max-file-size property to -1."
78,The multipart support is helpful when you want to receive multipart encoded file data as a @RequestParam-annotated parameter of type MultipartFile in a Spring MVC controller handler method.
78,See the MultipartAutoConfiguration source for more details.
78,It is recommended to use the container’s built-in support for multipart uploads rather than introducing an additional dependency such as Apache Commons File Upload.
78,4.6. Switch Off the Spring MVC DispatcherServlet
78,"By default, all content is served from the root of your application (/)."
78,"If you would rather map to a different path, you can configure one as follows:"
78,Properties
78,spring.mvc.servlet.path=/mypath
78,Yaml
78,spring:
78,mvc:
78,servlet:
78,"path: ""/mypath"""
78,If you have additional servlets you can declare a @Bean of type Servlet or ServletRegistrationBean for each and Spring Boot will register them transparently to the container.
78,"Because servlets are registered that way, they can be mapped to a sub-context of the DispatcherServlet without invoking it."
78,"Configuring the DispatcherServlet yourself is unusual but if you really need to do it, a @Bean of type DispatcherServletPath must be provided as well to provide the path of your custom DispatcherServlet."
78,4.7. Switch off the Default MVC Configuration
78,The easiest way to take complete control over MVC configuration is to provide your own @Configuration with the @EnableWebMvc annotation.
78,Doing so leaves all MVC configuration in your hands.
78,4.8. Customize ViewResolvers
78,"A ViewResolver is a core component of Spring MVC, translating view names in @Controller to actual View implementations."
78,"Note that ViewResolvers are mainly used in UI applications, rather than REST-style services (a View is not used to render a @ResponseBody)."
78,"There are many implementations of ViewResolver to choose from, and Spring on its own is not opinionated about which ones you should use."
78,"Spring Boot, on the other hand, installs one or two for you, depending on what it finds on the classpath and in the application context."
78,"The DispatcherServlet uses all the resolvers it finds in the application context, trying each one in turn until it gets a result."
78,"If you add your own, you have to be aware of the order and in which position your resolver is added."
78,WebMvcAutoConfiguration adds the following ViewResolvers to your context:
78,An InternalResourceViewResolver named ‘defaultViewResolver’.
78,"This one locates physical resources that can be rendered by using the DefaultServlet (including static resources and JSP pages, if you use those)."
78,It applies a prefix and a suffix to the view name and then looks for a physical resource with that path in the servlet context (the defaults are both empty but are accessible for external configuration through spring.mvc.view.prefix and spring.mvc.view.suffix).
78,You can override it by providing a bean of the same type.
78,A BeanNameViewResolver named ‘beanNameViewResolver’.
78,This is a useful member of the view resolver chain and picks up any beans with the same name as the View being resolved.
78,It should not be necessary to override or replace it.
78,A ContentNegotiatingViewResolver named ‘viewResolver’ is added only if there are actually beans of type View present.
78,"This is a composite resolver, delegating to all the others and attempting to find a match to the ‘Accept’ HTTP header sent by the client."
78,"There is a useful blog about ContentNegotiatingViewResolver that you might like to study to learn more, and you might also look at the source code for detail."
78,You can switch off the auto-configured ContentNegotiatingViewResolver by defining a bean named ‘viewResolver’.
78,"If you use Thymeleaf, you also have a ThymeleafViewResolver named ‘thymeleafViewResolver’."
78,It looks for resources by surrounding the view name with a prefix and suffix.
78,"The prefix is spring.thymeleaf.prefix, and the suffix is spring.thymeleaf.suffix."
78,"The values of the prefix and suffix default to ‘classpath:/templates/’ and ‘.html’, respectively."
78,You can override ThymeleafViewResolver by providing a bean of the same name.
78,"If you use FreeMarker, you also have a FreeMarkerViewResolver named ‘freeMarkerViewResolver’."
78,It looks for resources in a loader path (which is externalized to spring.freemarker.templateLoaderPath and has a default value of ‘classpath:/templates/’) by surrounding the view name with a prefix and a suffix.
78,"The prefix is externalized to spring.freemarker.prefix, and the suffix is externalized to spring.freemarker.suffix."
78,"The default values of the prefix and suffix are empty and ‘.ftlh’, respectively."
78,You can override FreeMarkerViewResolver by providing a bean of the same name.
78,"If you use Groovy templates (actually, if groovy-templates is on your classpath), you also have a GroovyMarkupViewResolver named ‘groovyMarkupViewResolver’."
78,It looks for resources in a loader path by surrounding the view name with a prefix and suffix (externalized to spring.groovy.template.prefix and spring.groovy.template.suffix).
78,"The prefix and suffix have default values of ‘classpath:/templates/’ and ‘.tpl’, respectively."
78,You can override GroovyMarkupViewResolver by providing a bean of the same name.
78,"If you use Mustache, you also have a MustacheViewResolver named ‘mustacheViewResolver’."
78,It looks for resources by surrounding the view name with a prefix and suffix.
78,"The prefix is spring.mustache.prefix, and the suffix is spring.mustache.suffix."
78,"The values of the prefix and suffix default to ‘classpath:/templates/’ and ‘.mustache’, respectively."
78,You can override MustacheViewResolver by providing a bean of the same name.
78,"For more detail, see the following sections:"
78,WebMvcAutoConfiguration
78,ThymeleafAutoConfiguration
78,FreeMarkerAutoConfiguration
78,GroovyTemplateAutoConfiguration
78,5. Jersey
78,5.1. Secure Jersey endpoints with Spring Security
78,Spring Security can be used to secure a Jersey-based web application in much the same way as it can be used to secure a Spring MVC-based web application.
78,"However, if you want to use Spring Security’s method-level security with Jersey, you must configure Jersey to use setStatus(int) rather sendError(int)."
78,This prevents Jersey from committing the response before Spring Security has had an opportunity to report an authentication or authorization failure to the client.
78,"The jersey.config.server.response.setStatusOverSendError property must be set to true on the application’s ResourceConfig bean, as shown in the following example:"
78,import java.util.Collections;
78,import org.glassfish.jersey.server.ResourceConfig;
78,import org.springframework.stereotype.Component;
78,@Component
78,public class JerseySetStatusOverSendErrorConfig extends ResourceConfig {
78,public JerseySetStatusOverSendErrorConfig() {
78,register(Endpoint.class);
78,"setProperties(Collections.singletonMap(""jersey.config.server.response.setStatusOverSendError"", true));"
78,5.2. Use Jersey Alongside Another Web Framework
78,"To use Jersey alongside another web framework, such as Spring MVC, it should be configured so that it will allow the other framework to handle requests that it cannot handle."
78,"First, configure Jersey to use a filter rather than a servlet by configuring the spring.jersey.type application property with a value of filter."
78,"Second, configure your ResourceConfig to forward requests that would have resulted in a 404, as shown in the following example."
78,import org.glassfish.jersey.server.ResourceConfig;
78,import org.glassfish.jersey.servlet.ServletProperties;
78,import org.springframework.stereotype.Component;
78,@Component
78,public class JerseyConfig extends ResourceConfig {
78,public JerseyConfig() {
78,register(Endpoint.class);
78,"property(ServletProperties.FILTER_FORWARD_ON_404, true);"
78,6. HTTP Clients
78,Spring Boot offers a number of starters that work with HTTP clients.
78,This section answers questions related to using them.
78,6.1. Configure RestTemplate to Use a Proxy
78,"As described in io.html, you can use a RestTemplateCustomizer with RestTemplateBuilder to build a customized RestTemplate."
78,This is the recommended approach for creating a RestTemplate configured to use a proxy.
78,The exact details of the proxy configuration depend on the underlying client request factory that is being used.
78,6.2. Configure the TcpClient used by a Reactor Netty-based WebClient
78,When Reactor Netty is on the classpath a Reactor Netty-based WebClient is auto-configured.
78,"To customize the client’s handling of network connections, provide a ClientHttpConnector bean."
78,The following example configures a 60 second connect timeout and adds a ReadTimeoutHandler:
78,Java
78,import io.netty.channel.ChannelOption;
78,import io.netty.handler.timeout.ReadTimeoutHandler;
78,import reactor.netty.http.client.HttpClient;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.http.client.ReactorResourceFactory;
78,import org.springframework.http.client.reactive.ClientHttpConnector;
78,import org.springframework.http.client.reactive.ReactorClientHttpConnector;
78,@Configuration(proxyBeanMethods = false)
78,public class MyReactorNettyClientConfiguration {
78,@Bean
78,ClientHttpConnector clientHttpConnector(ReactorResourceFactory resourceFactory) {
78,HttpClient httpClient = HttpClient.create(resourceFactory.getConnectionProvider())
78,.runOn(resourceFactory.getLoopResources())
78,".option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 60000)"
78,.doOnConnected((connection) -> connection.addHandlerLast(new ReadTimeoutHandler(60)));
78,return new ReactorClientHttpConnector(httpClient);
78,Kotlin
78,import io.netty.channel.ChannelOption
78,import io.netty.handler.timeout.ReadTimeoutHandler
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.http.client.reactive.ClientHttpConnector
78,import org.springframework.http.client.reactive.ReactorClientHttpConnector
78,import org.springframework.http.client.ReactorResourceFactory
78,import reactor.netty.http.client.HttpClient
78,@Configuration(proxyBeanMethods = false)
78,class MyReactorNettyClientConfiguration {
78,@Bean
78,fun clientHttpConnector(resourceFactory: ReactorResourceFactory): ClientHttpConnector {
78,val httpClient = HttpClient.create(resourceFactory.connectionProvider)
78,.runOn(resourceFactory.loopResources)
78,".option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 60000)"
78,.doOnConnected { connection ->
78,connection.addHandlerLast(ReadTimeoutHandler(60))
78,return ReactorClientHttpConnector(httpClient)
78,Note the use of ReactorResourceFactory for the connection provider and event loop resources.
78,This ensures efficient sharing of resources for the server receiving requests and the client making requests.
78,7. Logging
78,"Spring Boot has no mandatory logging dependency, except for the Commons Logging API, which is typically provided by Spring Framework’s spring-jcl module."
78,"To use Logback, you need to include it and spring-jcl on the classpath."
78,"The recommended way to do that is through the starters, which all depend on spring-boot-starter-logging."
78,"For a web application, you only need spring-boot-starter-web, since it depends transitively on the logging starter."
78,"If you use Maven, the following dependency adds logging for you:"
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-web</artifactId>
78,</dependency>
78,Spring Boot has a LoggingSystem abstraction that attempts to configure logging based on the content of the classpath.
78,"If Logback is available, it is the first choice."
78,"If the only change you need to make to logging is to set the levels of various loggers, you can do so in application.properties by using the ""logging.level"" prefix, as shown in the following example:"
78,Properties
78,logging.level.org.springframework.web=debug
78,logging.level.org.hibernate=error
78,Yaml
78,logging:
78,level:
78,"org.springframework.web: ""debug"""
78,"org.hibernate: ""error"""
78,You can also set the location of a file to which the log will be written (in addition to the console) by using logging.file.name.
78,"To configure the more fine-grained settings of a logging system, you need to use the native configuration format supported by the LoggingSystem in question."
78,"By default, Spring Boot picks up the native configuration from its default location for the system (such as classpath:logback.xml for Logback), but you can set the location of the config file by using the logging.config property."
78,7.1. Configure Logback for Logging
78,"If you need to apply customizations to logback beyond those that can be achieved with application.properties, you will need to add a standard logback configuration file."
78,You can add a logback.xml file to the root of your classpath for logback to find.
78,You can also use logback-spring.xml if you want to use the Spring Boot Logback extensions.
78,The Logback documentation has a dedicated section that covers configuration in some detail.
78,Spring Boot provides a number of logback configurations that can be included in your own configuration.
78,These includes are designed to allow certain common Spring Boot conventions to be re-applied.
78,The following files are provided under org/springframework/boot/logging/logback/:
78,"defaults.xml - Provides conversion rules, pattern properties and common logger configurations."
78,console-appender.xml - Adds a ConsoleAppender using the CONSOLE_LOG_PATTERN.
78,file-appender.xml - Adds a RollingFileAppender using the FILE_LOG_PATTERN and ROLLING_FILE_NAME_PATTERN with appropriate settings.
78,"In addition, a legacy base.xml file is provided for compatibility with earlier versions of Spring Boot."
78,A typical custom logback.xml file would look something like this:
78,"<?xml version=""1.0"" encoding=""UTF-8""?>"
78,<configuration>
78,"<include resource=""org/springframework/boot/logging/logback/defaults.xml""/>"
78,"<include resource=""org/springframework/boot/logging/logback/console-appender.xml"" />"
78,"<root level=""INFO"">"
78,"<appender-ref ref=""CONSOLE"" />"
78,</root>
78,"<logger name=""org.springframework.web"" level=""DEBUG""/>"
78,</configuration>
78,Your logback configuration file can also make use of System properties that the LoggingSystem takes care of creating for you:
78,${PID}: The current process ID.
78,${LOG_FILE}: Whether logging.file.name was set in Boot’s external configuration.
78,${LOG_PATH}: Whether logging.file.path (representing a directory for log files to live in) was set in Boot’s external configuration.
78,${LOG_EXCEPTION_CONVERSION_WORD}: Whether logging.exception-conversion-word was set in Boot’s external configuration.
78,${ROLLING_FILE_NAME_PATTERN}: Whether logging.pattern.rolling-file-name was set in Boot’s external configuration.
78,Spring Boot also provides some nice ANSI color terminal output on a console (but not in a log file) by using a custom Logback converter.
78,See the CONSOLE_LOG_PATTERN in the defaults.xml configuration for an example.
78,"If Groovy is on the classpath, you should be able to configure Logback with logback.groovy as well."
78,"If present, this setting is given preference."
78,Spring extensions are not supported with Groovy configuration.
78,Any logback-spring.groovy files will not be detected.
78,7.1.1. Configure Logback for File-only Output
78,"If you want to disable console logging and write output only to a file, you need a custom logback-spring.xml that imports file-appender.xml but not console-appender.xml, as shown in the following example:"
78,"<?xml version=""1.0"" encoding=""UTF-8""?>"
78,<configuration>
78,"<include resource=""org/springframework/boot/logging/logback/defaults.xml"" />"
78,"<property name=""LOG_FILE"" value=""${LOG_FILE:-${LOG_PATH:-${LOG_TEMP:-${java.io.tmpdir:-/tmp}}/}spring.log}""/>"
78,"<include resource=""org/springframework/boot/logging/logback/file-appender.xml"" />"
78,"<root level=""INFO"">"
78,"<appender-ref ref=""FILE"" />"
78,</root>
78,</configuration>
78,"You also need to add logging.file.name to your application.properties or application.yaml, as shown in the following example:"
78,Properties
78,logging.file.name=myapplication.log
78,Yaml
78,logging:
78,file:
78,"name: ""myapplication.log"""
78,7.2. Configure Log4j for Logging
78,Spring Boot supports Log4j 2 for logging configuration if it is on the classpath.
78,"If you use the starters for assembling dependencies, you have to exclude Logback and then include Log4j 2 instead."
78,"If you do not use the starters, you need to provide (at least) spring-jcl in addition to Log4j 2."
78,"The recommended path is through the starters, even though it requires some jiggling."
78,The following example shows how to set up the starters in Maven:
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-web</artifactId>
78,</dependency>
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter</artifactId>
78,<exclusions>
78,<exclusion>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-logging</artifactId>
78,</exclusion>
78,</exclusions>
78,</dependency>
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-log4j2</artifactId>
78,</dependency>
78,Gradle provides a few different ways to set up the starters.
78,One way is to use a module replacement.
78,"To do so, declare a dependency on the Log4j 2 starter and tell Gradle that any occurrences of the default logging starter should be replaced by the Log4j 2 starter, as shown in the following example:"
78,dependencies {
78,"implementation ""org.springframework.boot:spring-boot-starter-log4j2"""
78,modules {
78,"module(""org.springframework.boot:spring-boot-starter-logging"") {"
78,"replacedBy(""org.springframework.boot:spring-boot-starter-log4j2"", ""Use Log4j2 instead of Logback"")"
78,The Log4j starters gather together the dependencies for common logging requirements (such as having Tomcat use java.util.logging but configuring the output using Log4j 2).
78,"To ensure that debug logging performed using java.util.logging is routed into Log4j 2, configure its JDK logging adapter by setting the java.util.logging.manager system property to org.apache.logging.log4j.jul.LogManager."
78,7.2.1. Use YAML or JSON to Configure Log4j 2
78,"In addition to its default XML configuration format, Log4j 2 also supports YAML and JSON configuration files."
78,"To configure Log4j 2 to use an alternative configuration file format, add the appropriate dependencies to the classpath and name your configuration files to match your chosen file format, as shown in the following example:"
78,Format
78,Dependencies
78,File names
78,YAML
78,com.fasterxml.jackson.core:jackson-databind + com.fasterxml.jackson.dataformat:jackson-dataformat-yaml
78,log4j2.yaml + log4j2.yml
78,JSON
78,com.fasterxml.jackson.core:jackson-databind
78,log4j2.json + log4j2.jsn
78,7.2.2. Use Composite Configuration to Configure Log4j 2
78,Log4j 2 has support for combining multiple configuration files into a single composite configuration.
78,"To use this support in Spring Boot, configure logging.log4j2.config.override with the locations of one or more secondary configuration files."
78,"The secondary configuration files will be merged with the primary configuration, whether the primary’s source is Spring Boot’s defaults, a standard location such as log4j.xml, or the location configured by the logging.config property."
78,8. Data Access
78,Spring Boot includes a number of starters for working with data sources.
78,This section answers questions related to doing so.
78,8.1. Configure a Custom DataSource
78,"To configure your own DataSource, define a @Bean of that type in your configuration."
78,"Spring Boot reuses your DataSource anywhere one is required, including database initialization."
78,"If you need to externalize some settings, you can bind your DataSource to the environment (see “features.html”)."
78,The following example shows how to define a data source in a bean:
78,Java
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(prefix = ""app.datasource"")"
78,public SomeDataSource dataSource() {
78,return new SomeDataSource();
78,Kotlin
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(prefix = ""app.datasource"")"
78,fun dataSource(): SomeDataSource {
78,return SomeDataSource()
78,The following example shows how to define a data source by setting properties:
78,Properties
78,app.datasource.url=jdbc:h2:mem:mydb
78,app.datasource.username=sa
78,app.datasource.pool-size=30
78,Yaml
78,app:
78,datasource:
78,"url: ""jdbc:h2:mem:mydb"""
78,"username: ""sa"""
78,pool-size: 30
78,"Assuming that SomeDataSource has regular JavaBean properties for the URL, the username, and the pool size, these settings are bound automatically before the DataSource is made available to other components."
78,"Spring Boot also provides a utility builder class, called DataSourceBuilder, that can be used to create one of the standard data sources (if it is on the classpath)."
78,The builder can detect the one to use based on what is available on the classpath.
78,It also auto-detects the driver based on the JDBC URL.
78,The following example shows how to create a data source by using a DataSourceBuilder:
78,Java
78,import javax.sql.DataSource;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.jdbc.DataSourceBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.datasource"")"
78,public DataSource dataSource() {
78,return DataSourceBuilder.create().build();
78,Kotlin
78,import javax.sql.DataSource
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.boot.jdbc.DataSourceBuilder
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.datasource"")"
78,fun dataSource(): DataSource {
78,return DataSourceBuilder.create().build()
78,"To run an app with that DataSource, all you need is the connection information."
78,Pool-specific settings can also be provided.
78,Check the implementation that is going to be used at runtime for more details.
78,The following example shows how to define a JDBC data source by setting properties:
78,Properties
78,app.datasource.url=jdbc:mysql://localhost/test
78,app.datasource.username=dbuser
78,app.datasource.password=dbpass
78,app.datasource.pool-size=30
78,Yaml
78,app:
78,datasource:
78,"url: ""jdbc:mysql://localhost/test"""
78,"username: ""dbuser"""
78,"password: ""dbpass"""
78,pool-size: 30
78,"However, there is a catch."
78,"Because the actual type of the connection pool is not exposed, no keys are generated in the metadata for your custom DataSource and no completion is available in your IDE (because the DataSource interface exposes no properties)."
78,"Also, if you happen to have Hikari on the classpath, this basic setup does not work, because Hikari has no url property (but does have a jdbcUrl property)."
78,"In that case, you must rewrite your configuration as follows:"
78,Properties
78,app.datasource.jdbc-url=jdbc:mysql://localhost/test
78,app.datasource.username=dbuser
78,app.datasource.password=dbpass
78,app.datasource.pool-size=30
78,Yaml
78,app:
78,datasource:
78,"jdbc-url: ""jdbc:mysql://localhost/test"""
78,"username: ""dbuser"""
78,"password: ""dbpass"""
78,pool-size: 30
78,You can fix that by forcing the connection pool to use and return a dedicated implementation rather than DataSource.
78,"You cannot change the implementation at runtime, but the list of options will be explicit."
78,The following example shows how create a HikariDataSource with DataSourceBuilder:
78,Java
78,import com.zaxxer.hikari.HikariDataSource;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.jdbc.DataSourceBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.datasource"")"
78,public HikariDataSource dataSource() {
78,return DataSourceBuilder.create().type(HikariDataSource.class).build();
78,Kotlin
78,import com.zaxxer.hikari.HikariDataSource
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.boot.jdbc.DataSourceBuilder
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyDataSourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.datasource"")"
78,fun dataSource(): HikariDataSource {
78,return DataSourceBuilder.create().type(HikariDataSource::class.java).build()
78,"You can even go further by leveraging what DataSourceProperties does for you — that is, by providing a default embedded database with a sensible username and password if no URL is provided."
78,"You can easily initialize a DataSourceBuilder from the state of any DataSourceProperties object, so you could also inject the DataSource that Spring Boot creates automatically."
78,"However, that would split your configuration into two namespaces: url, username, password, type, and driver on spring.datasource and the rest on your custom namespace (app.datasource)."
78,"To avoid that, you can redefine a custom DataSourceProperties on your custom namespace, as shown in the following example:"
78,Java
78,import com.zaxxer.hikari.HikariDataSource;
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.context.annotation.Primary;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDataSourceConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource"")"
78,public DataSourceProperties dataSourceProperties() {
78,return new DataSourceProperties();
78,@Bean
78,"@ConfigurationProperties(""app.datasource.configuration"")"
78,public HikariDataSource dataSource(DataSourceProperties properties) {
78,return properties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
78,Kotlin
78,import com.zaxxer.hikari.HikariDataSource
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.context.annotation.Primary
78,@Configuration(proxyBeanMethods = false)
78,class MyDataSourceConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource"")"
78,fun dataSourceProperties(): DataSourceProperties {
78,return DataSourceProperties()
78,@Bean
78,"@ConfigurationProperties(""app.datasource.configuration"")"
78,fun dataSource(properties: DataSourceProperties): HikariDataSource {
78,return properties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
78,"This setup puts you in sync with what Spring Boot does for you by default, except that a dedicated connection pool is chosen (in code) and its settings are exposed in the app.datasource.configuration sub namespace."
78,"Because DataSourceProperties is taking care of the url/jdbcUrl translation for you, you can configure it as follows:"
78,Properties
78,app.datasource.url=jdbc:mysql://localhost/test
78,app.datasource.username=dbuser
78,app.datasource.password=dbpass
78,app.datasource.configuration.maximum-pool-size=30
78,Yaml
78,app:
78,datasource:
78,"url: ""jdbc:mysql://localhost/test"""
78,"username: ""dbuser"""
78,"password: ""dbpass"""
78,configuration:
78,maximum-pool-size: 30
78,Spring Boot will expose Hikari-specific settings to spring.datasource.hikari.
78,This example uses a more generic configuration sub namespace as the example does not support multiple datasource implementations.
78,"Because your custom configuration chooses to go with Hikari, app.datasource.type has no effect."
78,"In practice, the builder is initialized with whatever value you might set there and then overridden by the call to .type()."
78,See “data.html” in the “Spring Boot features” section and the DataSourceAutoConfiguration class for more details.
78,8.2. Configure Two DataSources
78,"If you need to configure multiple data sources, you can apply the same tricks that are described in the previous section."
78,"You must, however, mark one of the DataSource instances as @Primary, because various auto-configurations down the road expect to be able to get one by type."
78,"If you create your own DataSource, the auto-configuration backs off."
78,"In the following example, we provide the exact same feature set as the auto-configuration provides on the primary data source:"
78,Java
78,import com.zaxxer.hikari.HikariDataSource;
78,import org.apache.commons.dbcp2.BasicDataSource;
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.jdbc.DataSourceBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.context.annotation.Primary;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDataSourcesConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first"")"
78,public DataSourceProperties firstDataSourceProperties() {
78,return new DataSourceProperties();
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first.configuration"")"
78,public HikariDataSource firstDataSource(DataSourceProperties firstDataSourceProperties) {
78,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,public BasicDataSource secondDataSource() {
78,return DataSourceBuilder.create().type(BasicDataSource.class).build();
78,Kotlin
78,import com.zaxxer.hikari.HikariDataSource
78,import org.apache.commons.dbcp2.BasicDataSource
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.boot.jdbc.DataSourceBuilder
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.context.annotation.Primary
78,@Configuration(proxyBeanMethods = false)
78,class MyDataSourcesConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first"")"
78,fun firstDataSourceProperties(): DataSourceProperties {
78,return DataSourceProperties()
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first.configuration"")"
78,fun firstDataSource(firstDataSourceProperties: DataSourceProperties): HikariDataSource {
78,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,fun secondDataSource(): BasicDataSource {
78,return DataSourceBuilder.create().type(BasicDataSource::class.java).build()
78,firstDataSourceProperties has to be flagged as @Primary so that the database initializer feature uses your copy (if you use the initializer).
78,Both data sources are also bound for advanced customizations.
78,"For instance, you could configure them as follows:"
78,Properties
78,app.datasource.first.url=jdbc:mysql://localhost/first
78,app.datasource.first.username=dbuser
78,app.datasource.first.password=dbpass
78,app.datasource.first.configuration.maximum-pool-size=30
78,app.datasource.second.url=jdbc:mysql://localhost/second
78,app.datasource.second.username=dbuser
78,app.datasource.second.password=dbpass
78,app.datasource.second.max-total=30
78,Yaml
78,app:
78,datasource:
78,first:
78,"url: ""jdbc:mysql://localhost/first"""
78,"username: ""dbuser"""
78,"password: ""dbpass"""
78,configuration:
78,maximum-pool-size: 30
78,second:
78,"url: ""jdbc:mysql://localhost/second"""
78,"username: ""dbuser"""
78,"password: ""dbpass"""
78,max-total: 30
78,"You can apply the same concept to the secondary DataSource as well, as shown in the following example:"
78,Java
78,import com.zaxxer.hikari.HikariDataSource;
78,import org.apache.commons.dbcp2.BasicDataSource;
78,import org.springframework.beans.factory.annotation.Qualifier;
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.context.annotation.Primary;
78,@Configuration(proxyBeanMethods = false)
78,public class MyCompleteDataSourcesConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first"")"
78,public DataSourceProperties firstDataSourceProperties() {
78,return new DataSourceProperties();
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first.configuration"")"
78,public HikariDataSource firstDataSource(DataSourceProperties firstDataSourceProperties) {
78,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,public DataSourceProperties secondDataSourceProperties() {
78,return new DataSourceProperties();
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second.configuration"")"
78,public BasicDataSource secondDataSource(
78,"@Qualifier(""secondDataSourceProperties"") DataSourceProperties secondDataSourceProperties) {"
78,return secondDataSourceProperties.initializeDataSourceBuilder().type(BasicDataSource.class).build();
78,Kotlin
78,import com.zaxxer.hikari.HikariDataSource
78,import org.apache.commons.dbcp2.BasicDataSource
78,import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.context.annotation.Primary
78,@Configuration(proxyBeanMethods = false)
78,class MyCompleteDataSourcesConfiguration {
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first"")"
78,fun firstDataSourceProperties(): DataSourceProperties {
78,return DataSourceProperties()
78,@Bean
78,@Primary
78,"@ConfigurationProperties(""app.datasource.first.configuration"")"
78,fun firstDataSource(firstDataSourceProperties: DataSourceProperties): HikariDataSource {
78,return firstDataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource::class.java).build()
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,fun secondDataSourceProperties(): DataSourceProperties {
78,return DataSourceProperties()
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second.configuration"")"
78,fun secondDataSource(secondDataSourceProperties: DataSourceProperties): BasicDataSource {
78,return secondDataSourceProperties.initializeDataSourceBuilder().type(BasicDataSource::class.java).build()
78,The preceding example configures two data sources on custom namespaces with the same logic as Spring Boot would use in auto-configuration.
78,Note that each configuration sub namespace provides advanced settings based on the chosen implementation.
78,8.3. Use Spring Data Repositories
78,Spring Data can create implementations of @Repository interfaces of various flavors.
78,"Spring Boot handles all of that for you, as long as those @Repository annotations are included in one of the auto-configuration packages, typically the package (or a sub-package) of your main application class that is annotated with @SpringBootApplication or @EnableAutoConfiguration."
78,"For many applications, all you need is to put the right Spring Data dependencies on your classpath."
78,"There is a spring-boot-starter-data-jpa for JPA, spring-boot-starter-data-mongodb for Mongodb, and various other starters for supported technologies."
78,"To get started, create some repository interfaces to handle your @Entity objects."
78,Spring Boot determines the location of your @Repository definitions by scanning the auto-configuration packages.
78,"For more control, use the @Enable…Repositories annotations from Spring Data."
78,"For more about Spring Data, see the Spring Data project page."
78,8.4. Separate @Entity Definitions from Spring Configuration
78,Spring Boot determines the location of your @Entity definitions by scanning the auto-configuration packages.
78,"For more control, use the @EntityScan annotation, as shown in the following example:"
78,Java
78,import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
78,import org.springframework.boot.autoconfigure.domain.EntityScan;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,@EnableAutoConfiguration
78,@EntityScan(basePackageClasses = City.class)
78,public class MyApplication {
78,// ...
78,Kotlin
78,import org.springframework.boot.autoconfigure.EnableAutoConfiguration
78,import org.springframework.boot.autoconfigure.domain.EntityScan
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,@EnableAutoConfiguration
78,@EntityScan(basePackageClasses = [City::class])
78,class MyApplication {
78,// ...
78,8.5. Configure JPA Properties
78,"Spring Data JPA already provides some vendor-independent configuration options (such as those for SQL logging), and Spring Boot exposes those options and a few more for Hibernate as external configuration properties."
78,Some of them are automatically detected according to the context so you should not have to set them.
78,"The spring.jpa.hibernate.ddl-auto is a special case, because, depending on runtime conditions, it has different defaults."
78,"If an embedded database is used and no schema manager (such as Liquibase or Flyway) is handling the DataSource, it defaults to create-drop."
78,"In all other cases, it defaults to none."
78,The dialect to use is detected by the JPA provider.
78,"If you prefer to set the dialect yourself, set the spring.jpa.database-platform property."
78,The most common options to set are shown in the following example:
78,Properties
78,spring.jpa.hibernate.naming.physical-strategy=com.example.MyPhysicalNamingStrategy
78,spring.jpa.show-sql=true
78,Yaml
78,spring:
78,jpa:
78,hibernate:
78,naming:
78,"physical-strategy: ""com.example.MyPhysicalNamingStrategy"""
78,show-sql: true
78,"In addition, all properties in spring.jpa.properties.* are passed through as normal JPA properties (with the prefix stripped) when the local EntityManagerFactory is created."
78,You need to ensure that names defined under spring.jpa.properties.* exactly match those expected by your JPA provider.
78,Spring Boot will not attempt any kind of relaxed binding for these entries.
78,"For example, if you want to configure Hibernate’s batch size you must use spring.jpa.properties.hibernate.jdbc.batch_size."
78,"If you use other forms, such as batchSize or batch-size, Hibernate will not apply the setting."
78,"If you need to apply advanced customization to Hibernate properties, consider registering a HibernatePropertiesCustomizer bean that will be invoked prior to creating the EntityManagerFactory."
78,This takes precedence to anything that is applied by the auto-configuration.
78,8.6. Configure Hibernate Naming Strategy
78,Hibernate uses two different naming strategies to map names from the object model to the corresponding database names.
78,"The fully qualified class name of the physical and the implicit strategy implementations can be configured by setting the spring.jpa.hibernate.naming.physical-strategy and spring.jpa.hibernate.naming.implicit-strategy properties, respectively."
78,"Alternatively, if ImplicitNamingStrategy or PhysicalNamingStrategy beans are available in the application context, Hibernate will be automatically configured to use them."
78,"By default, Spring Boot configures the physical naming strategy with CamelCaseToUnderscoresNamingStrategy."
78,"Using this strategy, all dots are replaced by underscores and camel casing is replaced by underscores as well."
78,"Additionally, by default, all table names are generated in lower case."
78,"For example, a TelephoneNumber entity is mapped to the telephone_number table."
78,"If your schema requires mixed-case identifiers, define a custom CamelCaseToUnderscoresNamingStrategy bean, as shown in the following example:"
78,Java
78,import org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy;
78,import org.hibernate.engine.jdbc.env.spi.JdbcEnvironment;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyHibernateConfiguration {
78,@Bean
78,public CamelCaseToUnderscoresNamingStrategy caseSensitivePhysicalNamingStrategy() {
78,return new CamelCaseToUnderscoresNamingStrategy() {
78,@Override
78,protected boolean isCaseInsensitive(JdbcEnvironment jdbcEnvironment) {
78,return false;
78,Kotlin
78,import org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy
78,import org.hibernate.engine.jdbc.env.spi.JdbcEnvironment
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyHibernateConfiguration {
78,@Bean
78,fun caseSensitivePhysicalNamingStrategy(): CamelCaseToUnderscoresNamingStrategy {
78,return object : CamelCaseToUnderscoresNamingStrategy() {
78,override fun isCaseInsensitive(jdbcEnvironment: JdbcEnvironment): Boolean {
78,return false
78,"If you prefer to use Hibernate’s default instead, set the following property:"
78,spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
78,"Alternatively, you can configure the following bean:"
78,Java
78,import org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,class MyHibernateConfiguration {
78,@Bean
78,PhysicalNamingStrategyStandardImpl caseSensitivePhysicalNamingStrategy() {
78,return new PhysicalNamingStrategyStandardImpl();
78,Kotlin
78,import org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,internal class MyHibernateConfiguration {
78,@Bean
78,fun caseSensitivePhysicalNamingStrategy(): PhysicalNamingStrategyStandardImpl {
78,return PhysicalNamingStrategyStandardImpl()
78,See HibernateJpaAutoConfiguration and JpaBaseConfiguration for more details.
78,8.7. Configure Hibernate Second-Level Caching
78,Hibernate second-level cache can be configured for a range of cache providers.
78,"Rather than configuring Hibernate to lookup the cache provider again, it is better to provide the one that is available in the context whenever possible."
78,"To do this with JCache, first make sure that org.hibernate.orm:hibernate-jcache is available on the classpath."
78,"Then, add a HibernatePropertiesCustomizer bean as shown in the following example:"
78,Java
78,import org.hibernate.cache.jcache.ConfigSettings;
78,import org.springframework.boot.autoconfigure.orm.jpa.HibernatePropertiesCustomizer;
78,import org.springframework.cache.jcache.JCacheCacheManager;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyHibernateSecondLevelCacheConfiguration {
78,@Bean
78,public HibernatePropertiesCustomizer hibernateSecondLevelCacheCustomizer(JCacheCacheManager cacheManager) {
78,"return (properties) -> properties.put(ConfigSettings.CACHE_MANAGER, cacheManager.getCacheManager());"
78,Kotlin
78,import org.hibernate.cache.jcache.ConfigSettings
78,import org.springframework.boot.autoconfigure.orm.jpa.HibernatePropertiesCustomizer
78,import org.springframework.cache.jcache.JCacheCacheManager
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,class MyHibernateSecondLevelCacheConfiguration {
78,@Bean
78,fun hibernateSecondLevelCacheCustomizer(cacheManager: JCacheCacheManager): HibernatePropertiesCustomizer {
78,return HibernatePropertiesCustomizer { properties ->
78,properties[ConfigSettings.CACHE_MANAGER] = cacheManager.cacheManager
78,This customizer will configure Hibernate to use the same CacheManager as the one that the application uses.
78,It is also possible to use separate CacheManager instances.
78,"For details, see the Hibernate user guide."
78,8.8. Use Dependency Injection in Hibernate Components
78,"By default, Spring Boot registers a BeanContainer implementation that uses the BeanFactory so that converters and entity listeners can use regular dependency injection."
78,You can disable or tune this behavior by registering a HibernatePropertiesCustomizer that removes or changes the hibernate.resource.beans.container property.
78,8.9. Use a Custom EntityManagerFactory
78,"To take full control of the configuration of the EntityManagerFactory, you need to add a @Bean named ‘entityManagerFactory’."
78,Spring Boot auto-configuration switches off its entity manager in the presence of a bean of that type.
78,8.10. Using Multiple EntityManagerFactories
78,"If you need to use JPA against multiple data sources, you likely need one EntityManagerFactory per data source."
78,The LocalContainerEntityManagerFactoryBean from Spring ORM allows you to configure an EntityManagerFactory for your needs.
78,"You can also reuse JpaProperties to bind settings for each EntityManagerFactory, as shown in the following example:"
78,Java
78,import javax.sql.DataSource;
78,import org.springframework.boot.autoconfigure.orm.jpa.JpaProperties;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.orm.jpa.JpaVendorAdapter;
78,import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;
78,import org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter;
78,@Configuration(proxyBeanMethods = false)
78,public class MyEntityManagerFactoryConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.jpa.first"")"
78,public JpaProperties firstJpaProperties() {
78,return new JpaProperties();
78,@Bean
78,"public LocalContainerEntityManagerFactoryBean firstEntityManagerFactory(DataSource firstDataSource,"
78,JpaProperties firstJpaProperties) {
78,EntityManagerFactoryBuilder builder = createEntityManagerFactoryBuilder(firstJpaProperties);
78,"return builder.dataSource(firstDataSource).packages(Order.class).persistenceUnit(""firstDs"").build();"
78,private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder(JpaProperties jpaProperties) {
78,JpaVendorAdapter jpaVendorAdapter = createJpaVendorAdapter(jpaProperties);
78,"return new EntityManagerFactoryBuilder(jpaVendorAdapter, jpaProperties.getProperties(), null);"
78,private JpaVendorAdapter createJpaVendorAdapter(JpaProperties jpaProperties) {
78,// ... map JPA properties as needed
78,return new HibernateJpaVendorAdapter();
78,Kotlin
78,import javax.sql.DataSource
78,import org.springframework.boot.autoconfigure.orm.jpa.JpaProperties
78,import org.springframework.boot.context.properties.ConfigurationProperties
78,import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.orm.jpa.JpaVendorAdapter
78,import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean
78,import org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter
78,@Configuration(proxyBeanMethods = false)
78,class MyEntityManagerFactoryConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.jpa.first"")"
78,fun firstJpaProperties(): JpaProperties {
78,return JpaProperties()
78,@Bean
78,fun firstEntityManagerFactory(
78,"firstDataSource: DataSource?,"
78,firstJpaProperties: JpaProperties
78,): LocalContainerEntityManagerFactoryBean {
78,val builder = createEntityManagerFactoryBuilder(firstJpaProperties)
78,"return builder.dataSource(firstDataSource).packages(Order::class.java).persistenceUnit(""firstDs"").build()"
78,private fun createEntityManagerFactoryBuilder(jpaProperties: JpaProperties): EntityManagerFactoryBuilder {
78,val jpaVendorAdapter = createJpaVendorAdapter(jpaProperties)
78,"return EntityManagerFactoryBuilder(jpaVendorAdapter, jpaProperties.properties, null)"
78,private fun createJpaVendorAdapter(jpaProperties: JpaProperties): JpaVendorAdapter {
78,// ... map JPA properties as needed
78,return HibernateJpaVendorAdapter()
78,The example above creates an EntityManagerFactory using a DataSource bean named firstDataSource.
78,It scans entities located in the same package as Order.
78,It is possible to map additional JPA properties using the app.first.jpa namespace.
78,"When you create a bean for LocalContainerEntityManagerFactoryBean yourself, any customization that was applied during the creation of the auto-configured LocalContainerEntityManagerFactoryBean is lost."
78,"For example, in case of Hibernate, any properties under the spring.jpa.hibernate prefix will not be automatically applied to your LocalContainerEntityManagerFactoryBean."
78,"If you were relying on these properties for configuring things like the naming strategy or the DDL mode, you will need to explicitly configure that when creating the LocalContainerEntityManagerFactoryBean bean."
78,You should provide a similar configuration for any additional data sources for which you need JPA access.
78,"To complete the picture, you need to configure a JpaTransactionManager for each EntityManagerFactory as well."
78,"Alternatively, you might be able to use a JTA transaction manager that spans both."
78,"If you use Spring Data, you need to configure @EnableJpaRepositories accordingly, as shown in the following examples:"
78,Java
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
78,@Configuration(proxyBeanMethods = false)
78,"@EnableJpaRepositories(basePackageClasses = Order.class, entityManagerFactoryRef = ""firstEntityManagerFactory"")"
78,public class OrderConfiguration {
78,Kotlin
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.data.jpa.repository.config.EnableJpaRepositories
78,@Configuration(proxyBeanMethods = false)
78,"@EnableJpaRepositories(basePackageClasses = [Order::class], entityManagerFactoryRef = ""firstEntityManagerFactory"")"
78,class OrderConfiguration
78,Java
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
78,@Configuration(proxyBeanMethods = false)
78,"@EnableJpaRepositories(basePackageClasses = Customer.class, entityManagerFactoryRef = ""secondEntityManagerFactory"")"
78,public class CustomerConfiguration {
78,Kotlin
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.data.jpa.repository.config.EnableJpaRepositories
78,@Configuration(proxyBeanMethods = false)
78,"@EnableJpaRepositories(basePackageClasses = [Customer::class], entityManagerFactoryRef = ""secondEntityManagerFactory"")"
78,class CustomerConfiguration
78,8.11. Use a Traditional persistence.xml File
78,Spring Boot will not search for or use a META-INF/persistence.xml by default.
78,"If you prefer to use a traditional persistence.xml, you need to define your own @Bean of type LocalEntityManagerFactoryBean (with an ID of ‘entityManagerFactory’) and set the persistence unit name there."
78,See JpaBaseConfiguration for the default settings.
78,8.12. Use Spring Data JPA and Mongo Repositories
78,Spring Data JPA and Spring Data Mongo can both automatically create Repository implementations for you.
78,"If they are both present on the classpath, you might have to do some extra configuration to tell Spring Boot which repositories to create."
78,The most explicit way to do that is to use the standard Spring Data @EnableJpaRepositories and @EnableMongoRepositories annotations and provide the location of your Repository interfaces.
78,There are also flags (spring.data.*.repositories.enabled and spring.data.*.repositories.type) that you can use to switch the auto-configured repositories on and off in external configuration.
78,"Doing so is useful, for instance, in case you want to switch off the Mongo repositories and still use the auto-configured MongoTemplate."
78,"The same obstacle and the same features exist for other auto-configured Spring Data repository types (Elasticsearch, Redis, and others)."
78,"To work with them, change the names of the annotations and flags accordingly."
78,8.13. Customize Spring Data’s Web Support
78,Spring Data provides web support that simplifies the use of Spring Data repositories in a web application.
78,Spring Boot provides properties in the spring.data.web namespace for customizing its configuration.
78,"Note that if you are using Spring Data REST, you must use the properties in the spring.data.rest namespace instead."
78,8.14. Expose Spring Data Repositories as REST Endpoint
78,"Spring Data REST can expose the Repository implementations as REST endpoints for you,"
78,provided Spring MVC has been enabled for the application.
78,Spring Boot exposes a set of useful properties (from the spring.data.rest namespace) that customize the RepositoryRestConfiguration.
78,"If you need to provide additional customization, you should use a RepositoryRestConfigurer bean."
78,"If you do not specify any order on your custom RepositoryRestConfigurer, it runs after the one Spring Boot uses internally."
78,"If you need to specify an order, make sure it is higher than 0."
78,8.15. Configure a Component that is Used by JPA
78,"If you want to configure a component that JPA uses, then you need to ensure that the component is initialized before JPA."
78,"When the component is auto-configured, Spring Boot takes care of this for you."
78,"For example, when Flyway is auto-configured, Hibernate is configured to depend upon Flyway so that Flyway has a chance to initialize the database before Hibernate tries to use it."
78,"If you are configuring a component yourself, you can use an EntityManagerFactoryDependsOnPostProcessor subclass as a convenient way of setting up the necessary dependencies."
78,"For example, if you use Hibernate Search with Elasticsearch as its index manager, any EntityManagerFactory beans must be configured to depend on the elasticsearchClient bean, as shown in the following example:"
78,Java
78,import jakarta.persistence.EntityManagerFactory;
78,import org.springframework.boot.autoconfigure.orm.jpa.EntityManagerFactoryDependsOnPostProcessor;
78,import org.springframework.stereotype.Component;
78,/**
78,* {@link EntityManagerFactoryDependsOnPostProcessor} that ensures that
78,* {@link EntityManagerFactory} beans depend on the {@code elasticsearchClient} bean.
78,@Component
78,public class ElasticsearchEntityManagerFactoryDependsOnPostProcessor
78,extends EntityManagerFactoryDependsOnPostProcessor {
78,public ElasticsearchEntityManagerFactoryDependsOnPostProcessor() {
78,"super(""elasticsearchClient"");"
78,Kotlin
78,import org.springframework.boot.autoconfigure.orm.jpa.EntityManagerFactoryDependsOnPostProcessor
78,import org.springframework.stereotype.Component
78,@Component
78,class ElasticsearchEntityManagerFactoryDependsOnPostProcessor :
78,"EntityManagerFactoryDependsOnPostProcessor(""elasticsearchClient"")"
78,8.16. Configure jOOQ with Two DataSources
78,"If you need to use jOOQ with multiple data sources, you should create your own DSLContext for each one."
78,See JooqAutoConfiguration for more details.
78,"In particular, JooqExceptionTranslator and SpringTransactionProvider can be reused to provide similar features to what the auto-configuration does with a single DataSource."
78,9. Database Initialization
78,An SQL database can be initialized in different ways depending on what your stack is.
78,"Of course, you can also do it manually, provided the database is a separate process."
78,It is recommended to use a single mechanism for schema generation.
78,9.1. Initialize a Database Using JPA
78,"JPA has features for DDL generation, and these can be set up to run on startup against the database."
78,This is controlled through two external properties:
78,spring.jpa.generate-ddl (boolean) switches the feature on and off and is vendor independent.
78,spring.jpa.hibernate.ddl-auto (enum) is a Hibernate feature that controls the behavior in a more fine-grained way.
78,This feature is described in more detail later in this guide.
78,9.2. Initialize a Database Using Hibernate
78,"You can set spring.jpa.hibernate.ddl-auto explicitly to one of the standard Hibernate property values which are none, validate, update, create, and create-drop."
78,Spring Boot chooses a default value for you based on whether it thinks your database is embedded.
78,It defaults to create-drop if no schema manager has been detected or none in all other cases.
78,An embedded database is detected by looking at the Connection type and JDBC url.
78,"hsqldb, h2, and derby are candidates, while others are not."
78,Be careful when switching from in-memory to a ‘real’ database that you do not make assumptions about the existence of the tables and data in the new platform.
78,You either have to set ddl-auto explicitly or use one of the other mechanisms to initialize the database.
78,You can output the schema creation by enabling the org.hibernate.SQL logger.
78,This is done for you automatically if you enable the debug mode.
78,"In addition, a file named import.sql in the root of the classpath is executed on startup if Hibernate creates the schema from scratch (that is, if the ddl-auto property is set to create or create-drop)."
78,This can be useful for demos and for testing if you are careful but is probably not something you want to be on the classpath in production.
78,It is a Hibernate feature (and has nothing to do with Spring).
78,9.3. Initialize a Database Using Basic SQL Scripts
78,Spring Boot can automatically create the schema (DDL scripts) of your JDBC DataSource or R2DBC ConnectionFactory and initialize its data (DML scripts).
78,"By default, it loads schema scripts from optional:classpath*:schema.sql and data scripts from optional:classpath*:data.sql."
78,The locations of these schema and data scripts can be customized using spring.sql.init.schema-locations and spring.sql.init.data-locations respectively.
78,The optional: prefix means that the application will start even when the files do not exist.
78,"To have the application fail to start when the files are absent, remove the optional: prefix."
78,"In addition, Spring Boot processes the optional:classpath*:schema-${platform}.sql and optional:classpath*:data-${platform}.sql files (if present), where ${platform} is the value of spring.sql.init.platform."
78,This allows you to switch to database-specific scripts if necessary.
78,"For example, you might choose to set it to the vendor name of the database (hsqldb, h2, oracle, mysql, postgresql, and so on)."
78,"By default, SQL database initialization is only performed when using an embedded in-memory database."
78,"To always initialize an SQL database, irrespective of its type, set spring.sql.init.mode to always."
78,"Similarly, to disable initialization, set spring.sql.init.mode to never."
78,"By default, Spring Boot enables the fail-fast feature of its script-based database initializer."
78,"This means that, if the scripts cause exceptions, the application fails to start."
78,You can tune that behavior by setting spring.sql.init.continue-on-error.
78,"Script-based DataSource initialization is performed, by default, before any JPA EntityManagerFactory beans are created."
78,schema.sql can be used to create the schema for JPA-managed entities and data.sql can be used to populate it.
78,"While we do not recommend using multiple data source initialization technologies, if you want script-based DataSource initialization to be able to build upon the schema creation performed by Hibernate, set spring.jpa.defer-datasource-initialization to true."
78,This will defer data source initialization until after any EntityManagerFactory beans have been created and initialized.
78,schema.sql can then be used to make additions to any schema creation performed by Hibernate and data.sql can be used to populate it.
78,The initialization scripts support -- for single line comments and /* */ for block comments.
78,Other comment formats are not supported.
78,"If you are using a Higher-level Database Migration Tool, like Flyway or Liquibase, you should use them alone to create and initialize the schema."
78,Using the basic schema.sql and data.sql scripts alongside Flyway or Liquibase is not recommended and support will be removed in a future release.
78,"If you need to initialize test data using a higher-level database migration tool, please see the sections about Flyway and Liquibase."
78,9.4. Initialize a Spring Batch Database
78,"If you use Spring Batch, it comes pre-packaged with SQL initialization scripts for most popular database platforms."
78,Spring Boot can detect your database type and execute those scripts on startup.
78,"If you use an embedded database, this happens by default."
78,"You can also enable it for any database type, as shown in the following example:"
78,Properties
78,spring.batch.jdbc.initialize-schema=always
78,Yaml
78,spring:
78,batch:
78,jdbc:
78,"initialize-schema: ""always"""
78,You can also switch off the initialization explicitly by setting spring.batch.jdbc.initialize-schema to never.
78,9.5. Use a Higher-level Database Migration Tool
78,Spring Boot supports two higher-level migration tools: Flyway and Liquibase.
78,9.5.1. Execute Flyway Database Migrations on Startup
78,"To automatically run Flyway database migrations on startup, add the org.flywaydb:flyway-core to your classpath."
78,"Typically, migrations are scripts in the form V<VERSION>__<NAME>.sql (with <VERSION> an underscore-separated version, such as ‘1’ or ‘2_1’)."
78,"By default, they are in a directory called classpath:db/migration, but you can modify that location by setting spring.flyway.locations."
78,This is a comma-separated list of one or more classpath: or filesystem: locations.
78,"For example, the following configuration would search for scripts in both the default classpath location and the /opt/migration directory:"
78,Properties
78,"spring.flyway.locations=classpath:db/migration,filesystem:/opt/migration"
78,Yaml
78,spring:
78,flyway:
78,"locations: ""classpath:db/migration,filesystem:/opt/migration"""
78,You can also add a special {vendor} placeholder to use vendor-specific scripts.
78,Assume the following:
78,Properties
78,spring.flyway.locations=classpath:db/migration/{vendor}
78,Yaml
78,spring:
78,flyway:
78,"locations: ""classpath:db/migration/{vendor}"""
78,"Rather than using db/migration, the preceding configuration sets the directory to use according to the type of the database (such as db/migration/mysql for MySQL)."
78,The list of supported databases is available in DatabaseDriver.
78,Migrations can also be written in Java.
78,Flyway will be auto-configured with any beans that implement JavaMigration.
78,FlywayProperties provides most of Flyway’s settings and a small set of additional properties that can be used to disable the migrations or switch off the location checking.
78,"If you need more control over the configuration, consider registering a FlywayConfigurationCustomizer bean."
78,Spring Boot calls Flyway.migrate() to perform the database migration.
78,"If you would like more control, provide a @Bean that implements FlywayMigrationStrategy."
78,Flyway supports SQL and Java callbacks.
78,"To use SQL-based callbacks, place the callback scripts in the classpath:db/migration directory."
78,"To use Java-based callbacks, create one or more beans that implement Callback."
78,Any such beans are automatically registered with Flyway.
78,They can be ordered by using @Order or by implementing Ordered.
78,"Beans that implement the deprecated FlywayCallback interface can also be detected, however they cannot be used alongside Callback beans."
78,"By default, Flyway autowires the (@Primary) DataSource in your context and uses that for migrations."
78,"If you like to use a different DataSource, you can create one and mark its @Bean as @FlywayDataSource."
78,"If you do so and want two data sources, remember to create another one and mark it as @Primary."
78,"Alternatively, you can use Flyway’s native DataSource by setting spring.flyway.[url,user,password] in external properties."
78,Setting either spring.flyway.url or spring.flyway.user is sufficient to cause Flyway to use its own DataSource.
78,"If any of the three properties has not been set, the value of its equivalent spring.datasource property will be used."
78,You can also use Flyway to provide data for specific scenarios.
78,"For example, you can place test-specific migrations in src/test/resources and they are run only when your application starts for testing."
78,"Also, you can use profile-specific configuration to customize spring.flyway.locations so that certain migrations run only when a particular profile is active."
78,"For example, in application-dev.properties, you might specify the following setting:"
78,Properties
78,"spring.flyway.locations=classpath:/db/migration,classpath:/dev/db/migration"
78,Yaml
78,spring:
78,flyway:
78,"locations: ""classpath:/db/migration,classpath:/dev/db/migration"""
78,"With that setup, migrations in dev/db/migration run only when the dev profile is active."
78,9.5.2. Execute Liquibase Database Migrations on Startup
78,"To automatically run Liquibase database migrations on startup, add the org.liquibase:liquibase-core to your classpath."
78,"When you add the org.liquibase:liquibase-core to your classpath, database migrations run by default for both during application startup and before your tests run."
78,"This behavior can be customized by using the spring.liquibase.enabled property, setting different values in the main and test configurations."
78,"It is not possible to use two different ways to initialize the database (for example Liquibase for application startup, JPA for test runs)."
78,"By default, the master change log is read from db/changelog/db.changelog-master.yaml, but you can change the location by setting spring.liquibase.change-log."
78,"In addition to YAML, Liquibase also supports JSON, XML, and SQL change log formats."
78,"By default, Liquibase autowires the (@Primary) DataSource in your context and uses that for migrations."
78,"If you need to use a different DataSource, you can create one and mark its @Bean as @LiquibaseDataSource."
78,"If you do so and you want two data sources, remember to create another one and mark it as @Primary."
78,"Alternatively, you can use Liquibase’s native DataSource by setting spring.liquibase.[driver-class-name,url,user,password] in external properties."
78,Setting either spring.liquibase.url or spring.liquibase.user is sufficient to cause Liquibase to use its own DataSource.
78,"If any of the three properties has not been set, the value of its equivalent spring.datasource property will be used."
78,"See LiquibaseProperties for details about available settings such as contexts, the default schema, and others."
78,9.5.3. Use Flyway for test-only migrations
78,"If you want to create Flyway migrations which populate your test database, place them in src/test/resources/db/migration."
78,"A file named, for example, src/test/resources/db/migration/V9999__test-data.sql will be executed after your production migrations and only if you’re running the tests."
78,You can use this file to create the needed test data.
78,This file will not be packaged in your uber jar or your container.
78,9.5.4. Use Liquibase for test-only migrations
78,"If you want to create Liquibase migrations which populate your test database, you have to create a test changelog which also includes the production changelog."
78,"First, you need to configure Liquibase to use a different changelog when running the tests."
78,One way to do this is to create a Spring Boot test profile and put the Liquibase properties in there.
78,"For that, create a file named src/test/resources/application-test.properties and put the following property in there:"
78,Properties
78,spring.liquibase.change-log=classpath:/db/changelog/db.changelog-test.yaml
78,Yaml
78,spring:
78,liquibase:
78,"change-log: ""classpath:/db/changelog/db.changelog-test.yaml"""
78,This configures Liquibase to use a different changelog when running in the test profile.
78,Now create the changelog file at src/test/resources/db/changelog/db.changelog-test.yaml:
78,databaseChangeLog:
78,- include:
78,file: classpath:/db/changelog/db.changelog-master.yaml
78,- changeSet:
78,"runOrder: ""last"""
78,"id: ""test"""
78,changes:
78,# Insert your changes here
78,This changelog will be used when the tests are run and it will not be packaged in your uber jar or your container.
78,"It includes the production changelog and then declares a new changeset, whose runOrder: last setting specifies that it runs after all the production changesets have been run."
78,You can now use for example the insert changeset to insert data or the sql changeset to execute SQL directly.
78,The last thing to do is to configure Spring Boot to activate the test profile when running tests.
78,"To do this, you can add the @ActiveProfiles(""test"") annotation to your @SpringBootTest annotated test classes."
78,9.6. Depend Upon an Initialized Database
78,Database initialization is performed while the application is starting up as part of application context refresh.
78,"To allow an initialized database to be accessed during startup, beans that act as database initializers and beans that require that database to have been initialized are detected automatically."
78,Beans whose initialization depends upon the database having been initialized are configured to depend upon those that initialize it.
78,"If, during startup, your application tries to access the database and it has not been initialized, you can configure additional detection of beans that initialize the database and require the database to have been initialized."
78,9.6.1. Detect a Database Initializer
78,Spring Boot will automatically detect beans of the following types that initialize an SQL database:
78,DataSourceScriptDatabaseInitializer
78,EntityManagerFactory
78,Flyway
78,FlywayMigrationInitializer
78,R2dbcScriptDatabaseInitializer
78,SpringLiquibase
78,"If you are using a third-party starter for a database initialization library, it may provide a detector such that beans of other types are also detected automatically."
78,"To have other beans be detected, register an implementation of DatabaseInitializerDetector in META-INF/spring.factories."
78,9.6.2. Detect a Bean That Depends On Database Initialization
78,Spring Boot will automatically detect beans of the following types that depends upon database initialization:
78,AbstractEntityManagerFactoryBean (unless spring.jpa.defer-datasource-initialization is set to true)
78,DSLContext (jOOQ)
78,EntityManagerFactory (unless spring.jpa.defer-datasource-initialization is set to true)
78,JdbcClient
78,JdbcOperations
78,NamedParameterJdbcOperations
78,"If you are using a third-party starter data access library, it may provide a detector such that beans of other types are also detected automatically."
78,"To have other beans be detected, register an implementation of DependsOnDatabaseInitializationDetector in META-INF/spring.factories."
78,"Alternatively, annotate the bean’s class or its @Bean method with @DependsOnDatabaseInitialization."
78,10. NoSQL
78,Spring Boot offers a number of starters that support NoSQL technologies.
78,This section answers questions that arise from using NoSQL with Spring Boot.
78,10.1. Use Jedis Instead of Lettuce
78,"By default, the Spring Boot starter (spring-boot-starter-data-redis) uses Lettuce."
78,You need to exclude that dependency and include the Jedis one instead.
78,"Spring Boot manages both of these dependencies, allowing you to switch to Jedis without specifying a version."
78,The following example shows how to accomplish this in Maven:
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-data-redis</artifactId>
78,<exclusions>
78,<exclusion>
78,<groupId>io.lettuce</groupId>
78,<artifactId>lettuce-core</artifactId>
78,</exclusion>
78,</exclusions>
78,</dependency>
78,<dependency>
78,<groupId>redis.clients</groupId>
78,<artifactId>jedis</artifactId>
78,</dependency>
78,The following example shows how to accomplish this in Gradle:
78,dependencies {
78,implementation('org.springframework.boot:spring-boot-starter-data-redis') {
78,"exclude group: 'io.lettuce', module: 'lettuce-core'"
78,implementation 'redis.clients:jedis'
78,// ...
78,11. Messaging
78,Spring Boot offers a number of starters to support messaging.
78,This section answers questions that arise from using messaging with Spring Boot.
78,11.1. Disable Transacted JMS Session
78,"If your JMS broker does not support transacted sessions, you have to disable the support of transactions altogether."
78,"If you create your own JmsListenerContainerFactory, there is nothing to do, since, by default it cannot be transacted."
78,"If you want to use the DefaultJmsListenerContainerFactoryConfigurer to reuse Spring Boot’s default, you can disable transacted sessions, as follows:"
78,Java
78,import jakarta.jms.ConnectionFactory;
78,import org.springframework.boot.autoconfigure.jms.DefaultJmsListenerContainerFactoryConfigurer;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.jms.config.DefaultJmsListenerContainerFactory;
78,@Configuration(proxyBeanMethods = false)
78,public class MyJmsConfiguration {
78,@Bean
78,"public DefaultJmsListenerContainerFactory jmsListenerContainerFactory(ConnectionFactory connectionFactory,"
78,DefaultJmsListenerContainerFactoryConfigurer configurer) {
78,DefaultJmsListenerContainerFactory listenerFactory = new DefaultJmsListenerContainerFactory();
78,"configurer.configure(listenerFactory, connectionFactory);"
78,listenerFactory.setTransactionManager(null);
78,listenerFactory.setSessionTransacted(false);
78,return listenerFactory;
78,Kotlin
78,import jakarta.jms.ConnectionFactory
78,import org.springframework.boot.autoconfigure.jms.DefaultJmsListenerContainerFactoryConfigurer
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.jms.config.DefaultJmsListenerContainerFactory
78,@Configuration(proxyBeanMethods = false)
78,class MyJmsConfiguration {
78,@Bean
78,"fun jmsListenerContainerFactory(connectionFactory: ConnectionFactory?,"
78,configurer: DefaultJmsListenerContainerFactoryConfigurer): DefaultJmsListenerContainerFactory {
78,val listenerFactory = DefaultJmsListenerContainerFactory()
78,"configurer.configure(listenerFactory, connectionFactory)"
78,listenerFactory.setTransactionManager(null)
78,listenerFactory.setSessionTransacted(false)
78,return listenerFactory
78,"The preceding example overrides the default factory, and it should be applied to any other factory that your application defines, if any."
78,12. Batch Applications
78,A number of questions often arise when people use Spring Batch from within a Spring Boot application.
78,This section addresses those questions.
78,12.1. Specifying a Batch Data Source
78,"By default, batch applications require a DataSource to store job details."
78,Spring Batch expects a single DataSource by default.
78,"To have it use a DataSource other than the application’s main DataSource, declare a DataSource bean, annotating its @Bean method with @BatchDataSource."
78,"If you do so and want two data sources, remember to mark the other one @Primary."
78,"To take greater control, add @EnableBatchProcessing to one of your @Configuration classes or extend DefaultBatchConfiguration."
78,See the Javadoc of @EnableBatchProcessing
78,and DefaultBatchConfiguration for more details.
78,"For more info about Spring Batch, see the Spring Batch project page."
78,12.2. Running Spring Batch Jobs on Startup
78,Spring Batch auto-configuration is enabled by adding spring-boot-starter-batch to your application’s classpath.
78,"If a single Job bean is found in the application context, it is executed on startup (see JobLauncherApplicationRunner for details)."
78,"If multiple Job beans are found, the job that should be executed must be specified using spring.batch.job.name."
78,"To disable running a Job found in the application context, set the spring.batch.job.enabled to false."
78,See BatchAutoConfiguration for more details.
78,12.3. Running From the Command Line
78,"Spring Boot converts any command line argument starting with -- to a property to add to the Environment, see accessing command line properties."
78,This should not be used to pass arguments to batch jobs.
78,"To specify batch arguments on the command line, use the regular format (that is without --), as shown in the following example:"
78,$ java -jar myapp.jar someParameter=someValue anotherParameter=anotherValue
78,"If you specify a property of the Environment on the command line, it is ignored by the job."
78,Consider the following command:
78,$ java -jar myapp.jar --server.port=7070 someParameter=someValue
78,This provides only one argument to the batch job: someParameter=someValue.
78,12.4. Restarting a stopped or failed Job
78,"To restart a failed Job, all parameters (identifying and non-identifying) must be re-specified on the command line."
78,Non-identifying parameters are not copied from the previous execution.
78,This allows them to be modified or removed.
78,"When you’re using a custom JobParametersIncrementer, you have to gather all parameters managed by the incrementer to restart a failed execution."
78,12.5. Storing the Job Repository
78,Spring Batch requires a data store for the Job repository.
78,"If you use Spring Boot, you must use an actual database."
78,"Note that it can be an in-memory database, see Configuring a Job Repository."
78,13. Actuator
78,Spring Boot includes the Spring Boot Actuator.
78,This section answers questions that often arise from its use.
78,13.1. Change the HTTP Port or Address of the Actuator Endpoints
78,"In a standalone application, the Actuator HTTP port defaults to the same as the main HTTP port."
78,"To make the application listen on a different port, set the external property: management.server.port."
78,"To listen on a completely different network address (such as when you have an internal network for management and an external one for user applications), you can also set management.server.address to a valid IP address to which the server is able to bind."
78,"For more detail, see the ManagementServerProperties source code and “actuator.html” in the “Production-ready features” section."
78,13.2. Customize the ‘whitelabel’ Error Page
78,Spring Boot installs a ‘whitelabel’ error page that you see in a browser client if you encounter a server error (machine clients consuming JSON and other media types should see a sensible response with the right error code).
78,Set server.error.whitelabel.enabled=false to switch the default error page off.
78,Doing so restores the default of the servlet container that you are using.
78,"Note that Spring Boot still tries to resolve the error view, so you should probably add your own error page rather than disabling it completely."
78,Overriding the error page with your own depends on the templating technology that you use.
78,"For example, if you use Thymeleaf, you can add an error.html template."
78,"If you use FreeMarker, you can add an error.ftlh template."
78,"In general, you need a View that resolves with a name of error or a @Controller that handles the /error path."
78,"Unless you replaced some of the default configuration, you should find a BeanNameViewResolver in your ApplicationContext, so a @Bean named error would be one way of doing that."
78,See ErrorMvcAutoConfiguration for more options.
78,See also the section on “Error Handling” for details of how to register handlers in the servlet container.
78,13.3. Customizing Sanitization
78,"To take control over the sanitization, define a SanitizingFunction bean."
78,The SanitizableData with which the function is called provides access to the key and value as well as the PropertySource from which they came.
78,"This allows you to, for example, sanitize every value that comes from a particular property source."
78,Each SanitizingFunction is called in order until a function changes the value of the sanitizable data.
78,13.4. Map Health Indicators to Micrometer Metrics
78,Spring Boot health indicators return a Status type to indicate the overall system health.
78,"If you want to monitor or alert on levels of health for a particular application, you can export these statuses as metrics with Micrometer."
78,"By default, the status codes “UP”, “DOWN”, “OUT_OF_SERVICE” and “UNKNOWN” are used by Spring Boot."
78,"To export these, you will need to convert these states to some set of numbers so that they can be used with a Micrometer Gauge."
78,The following example shows one way to write such an exporter:
78,Java
78,import io.micrometer.core.instrument.Gauge;
78,import io.micrometer.core.instrument.MeterRegistry;
78,import org.springframework.boot.actuate.health.HealthEndpoint;
78,import org.springframework.boot.actuate.health.Status;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyHealthMetricsExportConfiguration {
78,"public MyHealthMetricsExportConfiguration(MeterRegistry registry, HealthEndpoint healthEndpoint) {"
78,// This example presumes common tags (such as the app) are applied elsewhere
78,"Gauge.builder(""health"", healthEndpoint, this::getStatusCode).strongReference(true).register(registry);"
78,private int getStatusCode(HealthEndpoint health) {
78,Status status = health.health().getStatus();
78,if (Status.UP.equals(status)) {
78,return 3;
78,if (Status.OUT_OF_SERVICE.equals(status)) {
78,return 2;
78,if (Status.DOWN.equals(status)) {
78,return 1;
78,return 0;
78,Kotlin
78,import io.micrometer.core.instrument.Gauge
78,import io.micrometer.core.instrument.MeterRegistry
78,import org.springframework.boot.actuate.health.HealthEndpoint
78,import org.springframework.boot.actuate.health.Status
78,import org.springframework.context.annotation.Configuration
78,@Configuration(proxyBeanMethods = false)
78,"class MyHealthMetricsExportConfiguration(registry: MeterRegistry, healthEndpoint: HealthEndpoint) {"
78,init {
78,// This example presumes common tags (such as the app) are applied elsewhere
78,"Gauge.builder(""health"", healthEndpoint) { health ->"
78,getStatusCode(health).toDouble()
78,}.strongReference(true).register(registry)
78,private fun getStatusCode(health: HealthEndpoint) = when (health.health().status) {
78,Status.UP -> 3
78,Status.OUT_OF_SERVICE -> 2
78,Status.DOWN -> 1
78,else -> 0
78,14. Security
78,"This section addresses questions about security when working with Spring Boot, including questions that arise from using Spring Security with Spring Boot."
78,"For more about Spring Security, see the Spring Security project page."
78,14.1. Switch off the Spring Boot Security Configuration
78,"If you define a @Configuration with a SecurityFilterChain bean in your application, this action switches off the default webapp security settings in Spring Boot."
78,14.2. Change the UserDetailsService and Add User Accounts
78,"If you provide a @Bean of type AuthenticationManager, AuthenticationProvider, or UserDetailsService, the default @Bean for InMemoryUserDetailsManager is not created."
78,This means you have the full feature set of Spring Security available (such as various authentication options).
78,The easiest way to add user accounts is by providing your own UserDetailsService bean.
78,14.3. Enable HTTPS When Running behind a Proxy Server
78,Ensuring that all your main endpoints are only available over HTTPS is an important chore for any application.
78,"If you use Tomcat as a servlet container, then Spring Boot adds Tomcat’s own RemoteIpValve automatically if it detects some environment settings, allowing you to rely on the HttpServletRequest to report whether it is secure or not (even downstream of a proxy server that handles the real SSL termination)."
78,"The standard behavior is determined by the presence or absence of certain request headers (x-forwarded-for and x-forwarded-proto), whose names are conventional, so it should work with most front-end proxies."
78,"You can switch on the valve by adding some entries to application.properties, as shown in the following example:"
78,Properties
78,server.tomcat.remoteip.remote-ip-header=x-forwarded-for
78,server.tomcat.remoteip.protocol-header=x-forwarded-proto
78,Yaml
78,server:
78,tomcat:
78,remoteip:
78,"remote-ip-header: ""x-forwarded-for"""
78,"protocol-header: ""x-forwarded-proto"""
78,(The presence of either of those properties switches on the valve.
78,"Alternatively, you can add the RemoteIpValve by customizing the TomcatServletWebServerFactory using a WebServerFactoryCustomizer bean.)"
78,"To configure Spring Security to require a secure channel for all (or some) requests, consider adding your own SecurityFilterChain bean that adds the following HttpSecurity configuration:"
78,Java
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
78,import org.springframework.security.web.SecurityFilterChain;
78,@Configuration
78,public class MySecurityConfig {
78,@Bean
78,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
78,// Customize the application security ...
78,http.requiresChannel((channel) -> channel.anyRequest().requiresSecure());
78,return http.build();
78,Kotlin
78,import org.springframework.context.annotation.Bean
78,import org.springframework.context.annotation.Configuration
78,import org.springframework.security.config.annotation.web.builders.HttpSecurity
78,import org.springframework.security.web.SecurityFilterChain
78,@Configuration
78,class MySecurityConfig {
78,@Bean
78,fun securityFilterChain(http: HttpSecurity): SecurityFilterChain {
78,// Customize the application security ...
78,http.requiresChannel { requests -> requests.anyRequest().requiresSecure() }
78,return http.build()
78,15. Hot Swapping
78,Spring Boot supports hot swapping.
78,This section answers questions about how it works.
78,15.1. Reload Static Content
78,There are several options for hot reloading.
78,"The recommended approach is to use spring-boot-devtools, as it provides additional development-time features, such as support for fast application restarts and LiveReload as well as sensible development-time configuration (such as template caching)."
78,Devtools works by monitoring the classpath for changes.
78,"This means that static resource changes must be ""built"" for the change to take effect."
78,"By default, this happens automatically in Eclipse when you save your changes."
78,"In IntelliJ IDEA, the Make Project command triggers the necessary build."
78,"Due to the default restart exclusions, changes to static resources do not trigger a restart of your application."
78,"They do, however, trigger a live reload."
78,"Alternatively, running in an IDE (especially with debugging on) is a good way to do development (all modern IDEs allow reloading of static resources and usually also allow hot-swapping of Java class changes)."
78,"Finally, the Maven and Gradle plugins can be configured (see the addResources property) to support running from the command line with reloading of static files directly from source."
78,You can use that with an external css/js compiler process if you are writing that code with higher-level tools.
78,15.2. Reload Templates without Restarting the Container
78,Most of the templating technologies supported by Spring Boot include a configuration option to disable caching (described later in this document).
78,"If you use the spring-boot-devtools module, these properties are automatically configured for you at development time."
78,15.2.1. Thymeleaf Templates
78,"If you use Thymeleaf, set spring.thymeleaf.cache to false."
78,See ThymeleafAutoConfiguration for other Thymeleaf customization options.
78,15.2.2. FreeMarker Templates
78,"If you use FreeMarker, set spring.freemarker.cache to false."
78,See FreeMarkerAutoConfiguration for other FreeMarker customization options.
78,15.2.3. Groovy Templates
78,"If you use Groovy templates, set spring.groovy.template.cache to false."
78,See GroovyTemplateAutoConfiguration for other Groovy customization options.
78,15.3. Fast Application Restarts
78,The spring-boot-devtools module includes support for automatic application restarts.
78,While not as fast as technologies such as JRebel it is usually significantly faster than a “cold start”.
78,You should probably give it a try before investigating some of the more complex reload options discussed later in this document.
78,"For more details, see the using.html section."
78,15.4. Reload Java Classes without Restarting the Container
78,"Many modern IDEs (Eclipse, IDEA, and others) support hot swapping of bytecode."
78,"Consequently, if you make a change that does not affect class or method signatures, it should reload cleanly with no side effects."
78,16. Testing
78,Spring Boot includes a number of testing utilities and support classes as well as a dedicated starter that provides common test dependencies.
78,This section answers common questions about testing.
78,16.1. Testing With Spring Security
78,Spring Security provides support for running tests as a specific user.
78,"For example, the test in the snippet below will run with an authenticated user that has the ADMIN role."
78,Java
78,import org.junit.jupiter.api.Test;
78,import org.springframework.beans.factory.annotation.Autowired;
78,import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest;
78,import org.springframework.security.test.context.support.WithMockUser;
78,import org.springframework.test.web.servlet.MockMvc;
78,import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;
78,@WebMvcTest(UserController.class)
78,class MySecurityTests {
78,@Autowired
78,private MockMvc mvc;
78,@Test
78,"@WithMockUser(roles = ""ADMIN"")"
78,void requestProtectedUrlWithUser() throws Exception {
78,"this.mvc.perform(get(""/""));"
78,Kotlin
78,import org.junit.jupiter.api.Test
78,import org.springframework.beans.factory.annotation.Autowired
78,import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest
78,import org.springframework.security.test.context.support.WithMockUser
78,import org.springframework.test.web.servlet.MockMvc
78,import org.springframework.test.web.servlet.request.MockMvcRequestBuilders
78,@WebMvcTest(UserController::class)
78,class MySecurityTests(@Autowired val mvc: MockMvc) {
78,@Test
78,"@WithMockUser(roles = [""ADMIN""])"
78,fun requestProtectedUrlWithUser() {
78,"mvc.perform(MockMvcRequestBuilders.get(""/""))"
78,"Spring Security provides comprehensive integration with Spring MVC Test, and this can also be used when testing controllers using the @WebMvcTest slice and MockMvc."
78,"For additional details on Spring Security’s testing support, see Spring Security’s reference documentation."
78,16.2. Structure @Configuration classes for inclusion in slice tests
78,Slice tests work by restricting Spring Framework’s component scanning to a limited set of components based on their type.
78,"For any beans that are not created through component scanning, for example, beans that are created using the @Bean annotation, slice tests will not be able to include/exclude them from the application context."
78,Consider this example:
78,import org.apache.commons.dbcp2.BasicDataSource;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.jdbc.DataSourceBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
78,import org.springframework.security.web.SecurityFilterChain;
78,@Configuration(proxyBeanMethods = false)
78,public class MyConfiguration {
78,@Bean
78,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
78,http.authorizeHttpRequests((requests) -> requests.anyRequest().authenticated());
78,return http.build();
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,public BasicDataSource secondDataSource() {
78,return DataSourceBuilder.create().type(BasicDataSource.class).build();
78,"For a @WebMvcTest for an application with the above @Configuration class, you might expect to have the SecurityFilterChain bean in the application context so that you can test if your controller endpoints are secured properly."
78,"However, MyConfiguration is not picked up by @WebMvcTest’s component scanning filter because it doesn’t match any of the types specified by the filter."
78,You can include the configuration explicitly by annotating the test class with @Import(MyConfiguration.class).
78,This will load all the beans in MyConfiguration including the BasicDataSource bean which isn’t required when testing the web tier.
78,Splitting the configuration class into two will enable importing just the security configuration.
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,import org.springframework.security.config.annotation.web.builders.HttpSecurity;
78,import org.springframework.security.web.SecurityFilterChain;
78,@Configuration(proxyBeanMethods = false)
78,public class MySecurityConfiguration {
78,@Bean
78,public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
78,http.authorizeHttpRequests((requests) -> requests.anyRequest().authenticated());
78,return http.build();
78,import org.apache.commons.dbcp2.BasicDataSource;
78,import org.springframework.boot.context.properties.ConfigurationProperties;
78,import org.springframework.boot.jdbc.DataSourceBuilder;
78,import org.springframework.context.annotation.Bean;
78,import org.springframework.context.annotation.Configuration;
78,@Configuration(proxyBeanMethods = false)
78,public class MyDatasourceConfiguration {
78,@Bean
78,"@ConfigurationProperties(""app.datasource.second"")"
78,public BasicDataSource secondDataSource() {
78,return DataSourceBuilder.create().type(BasicDataSource.class).build();
78,Having a single configuration class can be inefficient when beans of a certain domain need to be included in slice tests.
78,"Instead, structuring the application’s configuration as multiple granular classes with beans for a specific domain can enable importing them only for specific slice tests."
78,17. Build
78,Spring Boot includes build plugins for Maven and Gradle.
78,This section answers common questions about these plugins.
78,17.1. Generate Build Information
78,"Both the Maven plugin and the Gradle plugin allow generating build information containing the coordinates, name, and version of the project."
78,The plugins can also be configured to add additional properties through configuration.
78,"When such a file is present, Spring Boot auto-configures a BuildProperties bean."
78,"To generate build information with Maven, add an execution for the build-info goal, as shown in the following example:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,<version>3.2.3</version>
78,<executions>
78,<execution>
78,<goals>
78,<goal>build-info</goal>
78,</goals>
78,</execution>
78,</executions>
78,</plugin>
78,</plugins>
78,</build>
78,See the Spring Boot Maven Plugin documentation for more details.
78,The following example does the same with Gradle:
78,springBoot {
78,buildInfo()
78,See the Spring Boot Gradle Plugin documentation for more details.
78,17.2. Generate Git Information
78,Both Maven and Gradle allow generating a git.properties file containing information about the state of your git source code repository when the project was built.
78,"For Maven users, the spring-boot-starter-parent POM includes a pre-configured plugin to generate a git.properties file."
78,"To use it, add the following declaration for the Git Commit Id Plugin to your POM:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>io.github.git-commit-id</groupId>
78,<artifactId>git-commit-id-maven-plugin</artifactId>
78,</plugin>
78,</plugins>
78,</build>
78,"Gradle users can achieve the same result by using the gradle-git-properties plugin, as shown in the following example:"
78,plugins {
78,"id ""com.gorylenko.gradle-git-properties"" version ""2.4.1"""
78,Both the Maven and Gradle plugins allow the properties that are included in git.properties to be configured.
78,The commit time in git.properties is expected to match the following format: yyyy-MM-dd’T’HH:mm:ssZ.
78,This is the default format for both plugins listed above.
78,"Using this format lets the time be parsed into a Date and its format, when serialized to JSON, to be controlled by Jackson’s date serialization configuration settings."
78,17.3. Customize Dependency Versions
78,The spring-boot-dependencies POM manages the versions of common dependencies.
78,The Spring Boot plugins for Maven and Gradle allow these managed dependency versions to be customized using build properties.
78,Each Spring Boot release is designed and tested against this specific set of third-party dependencies.
78,Overriding versions may cause compatibility issues.
78,"To override dependency versions with Maven, see this section of the Maven plugin’s documentation."
78,"To override dependency versions in Gradle, see this section of the Gradle plugin’s documentation."
78,17.4. Create an Executable JAR with Maven
78,The spring-boot-maven-plugin can be used to create an executable “fat” JAR.
78,"If you use the spring-boot-starter-parent POM, you can declare the plugin and your jars are repackaged as follows:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,</plugin>
78,</plugins>
78,</build>
78,"If you do not use the parent POM, you can still use the plugin."
78,"However, you must additionally add an <executions> section, as follows:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,<version>3.2.3</version>
78,<executions>
78,<execution>
78,<goals>
78,<goal>repackage</goal>
78,</goals>
78,</execution>
78,</executions>
78,</plugin>
78,</plugins>
78,</build>
78,See the plugin documentation for full usage details.
78,17.5. Use a Spring Boot Application as a Dependency
78,"Like a war file, a Spring Boot application is not intended to be used as a dependency."
78,"If your application contains classes that you want to share with other projects, the recommended approach is to move that code into a separate module."
78,The separate module can then be depended upon by your application and other projects.
78,"If you cannot rearrange your code as recommended above, Spring Boot’s Maven and Gradle plugins must be configured to produce a separate artifact that is suitable for use as a dependency."
78,The executable archive cannot be used as a dependency as the executable jar format packages application classes in BOOT-INF/classes.
78,This means that they cannot be found when the executable jar is used as a dependency.
78,"To produce the two artifacts, one that can be used as a dependency and one that is executable, a classifier must be specified."
78,"This classifier is applied to the name of the executable archive, leaving the default archive for use as a dependency."
78,"To configure a classifier of exec in Maven, you can use the following configuration:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,<configuration>
78,<classifier>exec</classifier>
78,</configuration>
78,</plugin>
78,</plugins>
78,</build>
78,17.6. Extract Specific Libraries When an Executable Jar Runs
78,Most nested libraries in an executable jar do not need to be unpacked in order to run.
78,"However, certain libraries can have problems."
78,"For example, JRuby includes its own nested jar support, which assumes that the jruby-complete.jar is always directly available as a file in its own right."
78,"To deal with any problematic libraries, you can flag that specific nested jars should be automatically unpacked when the executable jar first runs."
78,Such nested jars are written beneath the temporary directory identified by the java.io.tmpdir system property.
78,Care should be taken to ensure that your operating system is configured so that it will not delete the jars that have been unpacked to the temporary directory while the application is still running.
78,"For example, to indicate that JRuby should be flagged for unpacking by using the Maven Plugin, you would add the following configuration:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,<configuration>
78,<requiresUnpack>
78,<dependency>
78,<groupId>org.jruby</groupId>
78,<artifactId>jruby-complete</artifactId>
78,</dependency>
78,</requiresUnpack>
78,</configuration>
78,</plugin>
78,</plugins>
78,</build>
78,17.7. Create a Non-executable JAR with Exclusions
78,"Often, if you have an executable and a non-executable jar as two separate build products, the executable version has additional configuration files that are not needed in a library jar."
78,"For example, the application.yaml configuration file might be excluded from the non-executable JAR."
78,"In Maven, the executable jar must be the main artifact and you can add a classified jar for the library, as follows:"
78,<build>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,</plugin>
78,<plugin>
78,<artifactId>maven-jar-plugin</artifactId>
78,<executions>
78,<execution>
78,<id>lib</id>
78,<phase>package</phase>
78,<goals>
78,<goal>jar</goal>
78,</goals>
78,<configuration>
78,<classifier>lib</classifier>
78,<excludes>
78,<exclude>application.yaml</exclude>
78,</excludes>
78,</configuration>
78,</execution>
78,</executions>
78,</plugin>
78,</plugins>
78,</build>
78,17.8. Remote Debug a Spring Boot Application Started with Maven
78,"To attach a remote debugger to a Spring Boot application that was started with Maven, you can use the jvmArguments property of the maven plugin."
78,See this example for more details.
78,17.9. Build an Executable Archive From Ant without Using spring-boot-antlib
78,"To build with Ant, you need to grab dependencies, compile, and then create a jar or war archive."
78,"To make it executable, you can either use the spring-boot-antlib module or you can follow these instructions:"
78,"If you are building a jar, package the application’s classes and resources in a nested BOOT-INF/classes directory."
78,"If you are building a war, package the application’s classes in a nested WEB-INF/classes directory as usual."
78,Add the runtime dependencies in a nested BOOT-INF/lib directory for a jar or WEB-INF/lib for a war.
78,Remember not to compress the entries in the archive.
78,Add the provided (embedded container) dependencies in a nested BOOT-INF/lib directory for a jar or WEB-INF/lib-provided for a war.
78,Remember not to compress the entries in the archive.
78,Add the spring-boot-loader classes at the root of the archive (so that the Main-Class is available).
78,"Use the appropriate launcher (such as JarLauncher for a jar file) as a Main-Class attribute in the manifest and specify the other properties it needs as manifest entries — principally, by setting a Start-Class property."
78,The following example shows how to build an executable archive with Ant:
78,"<target name=""build"" depends=""compile"">"
78,"<jar destfile=""target/${ant.project.name}-${spring-boot.version}.jar"" compress=""false"">"
78,<mappedresources>
78,"<fileset dir=""target/classes"" />"
78,"<globmapper from=""*"" to=""BOOT-INF/classes/*""/>"
78,</mappedresources>
78,<mappedresources>
78,"<fileset dir=""src/main/resources"" erroronmissingdir=""false""/>"
78,"<globmapper from=""*"" to=""BOOT-INF/classes/*""/>"
78,</mappedresources>
78,<mappedresources>
78,"<fileset dir=""${lib.dir}/runtime"" />"
78,"<globmapper from=""*"" to=""BOOT-INF/lib/*""/>"
78,</mappedresources>
78,"<zipfileset src=""${lib.dir}/loader/spring-boot-loader-jar-${spring-boot.version}.jar"" />"
78,<manifest>
78,"<attribute name=""Main-Class"" value=""org.springframework.boot.loader.launch.JarLauncher"" />"
78,"<attribute name=""Start-Class"" value=""${start-class}"" />"
78,</manifest>
78,</jar>
78,</target>
78,18. Ahead-of-time processing
78,A number of questions often arise when people use the ahead-of-time processing of Spring Boot applications.
78,This section addresses those questions.
78,18.1. Conditions
78,Ahead-of-time processing optimizes the application and evaluates conditions based on the environment at build time.
78,"Profiles are implemented through conditions and are therefore affected, too."
78,"If you want beans that are created based on a condition in an ahead-of-time optimized application, you have to set up the environment when building the application."
78,The beans which are created while ahead-of-time processing at build time are then always created when running the application and can’t be switched off.
78,"To do this, you can set the profiles which should be used when building the application."
78,"For Maven, this works by setting the profiles configuration of the spring-boot-maven-plugin:process-aot execution:"
78,<profile>
78,<id>native</id>
78,<build>
78,<pluginManagement>
78,<plugins>
78,<plugin>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-maven-plugin</artifactId>
78,<executions>
78,<execution>
78,<id>process-aot</id>
78,<configuration>
78,"<profiles>profile-a,profile-b</profiles>"
78,</configuration>
78,</execution>
78,</executions>
78,</plugin>
78,</plugins>
78,</pluginManagement>
78,</build>
78,</profile>
78,"For Gradle, you need to configure the ProcessAot task:"
78,tasks.withType(org.springframework.boot.gradle.tasks.aot.ProcessAot).configureEach {
78,"args('--spring.profiles.active=profile-a,profile-b')"
78,Profiles which only change configuration properties that don’t influence conditions are supported without limitations when running ahead-of-time optimized applications.
78,19. Traditional Deployment
78,Spring Boot supports traditional deployment as well as more modern forms of deployment.
78,This section answers common questions about traditional deployment.
78,19.1. Create a Deployable War File
78,"Because Spring WebFlux does not strictly depend on the servlet API and applications are deployed by default on an embedded Reactor Netty server, War deployment is not supported for WebFlux applications."
78,The first step in producing a deployable war file is to provide a SpringBootServletInitializer subclass and override its configure method.
78,Doing so makes use of Spring Framework’s servlet 3.0 support and lets you configure your application when it is launched by the servlet container.
78,"Typically, you should update your application’s main class to extend SpringBootServletInitializer, as shown in the following example:"
78,Java
78,import org.springframework.boot.SpringApplication;
78,import org.springframework.boot.autoconfigure.SpringBootApplication;
78,import org.springframework.boot.builder.SpringApplicationBuilder;
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
78,@SpringBootApplication
78,public class MyApplication extends SpringBootServletInitializer {
78,@Override
78,protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
78,return application.sources(MyApplication.class);
78,public static void main(String[] args) {
78,"SpringApplication.run(MyApplication.class, args);"
78,Kotlin
78,import org.springframework.boot.autoconfigure.SpringBootApplication
78,import org.springframework.boot.builder.SpringApplicationBuilder
78,import org.springframework.boot.runApplication
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
78,@SpringBootApplication
78,class MyApplication : SpringBootServletInitializer() {
78,override fun configure(application: SpringApplicationBuilder): SpringApplicationBuilder {
78,return application.sources(MyApplication::class.java)
78,fun main(args: Array<String>) {
78,runApplication<MyApplication>(*args)
78,The next step is to update your build configuration such that your project produces a war file rather than a jar file.
78,"If you use Maven and spring-boot-starter-parent (which configures Maven’s war plugin for you), all you need to do is to modify pom.xml to change the packaging to war, as follows:"
78,<packaging>war</packaging>
78,"If you use Gradle, you need to modify build.gradle to apply the war plugin to the project, as follows:"
78,apply plugin: 'war'
78,The final step in the process is to ensure that the embedded servlet container does not interfere with the servlet container to which the war file is deployed.
78,"To do so, you need to mark the embedded servlet container dependency as being provided."
78,"If you use Maven, the following example marks the servlet container (Tomcat, in this case) as being provided:"
78,<dependencies>
78,<!-- ... -->
78,<dependency>
78,<groupId>org.springframework.boot</groupId>
78,<artifactId>spring-boot-starter-tomcat</artifactId>
78,<scope>provided</scope>
78,</dependency>
78,<!-- ... -->
78,</dependencies>
78,"If you use Gradle, the following example marks the servlet container (Tomcat, in this case) as being provided:"
78,dependencies {
78,// ...
78,providedRuntime 'org.springframework.boot:spring-boot-starter-tomcat'
78,// ...
78,providedRuntime is preferred to Gradle’s compileOnly configuration.
78,"Among other limitations, compileOnly dependencies are not on the test classpath, so any web-based integration tests fail."
78,"If you use the Spring Boot build tools, marking the embedded servlet container dependency as provided produces an executable war file with the provided dependencies packaged in a lib-provided directory."
78,"This means that, in addition to being deployable to a servlet container, you can also run your application by using java -jar on the command line."
78,19.2. Convert an Existing Application to Spring Boot
78,"To convert an existing non-web Spring application to a Spring Boot application, replace the code that creates your ApplicationContext and replace it with calls to SpringApplication or SpringApplicationBuilder."
78,Spring MVC web applications are generally amenable to first creating a deployable war application and then migrating it later to an executable war or jar.
78,See the Getting Started Guide on Converting a jar to a war.
78,"To create a deployable war by extending SpringBootServletInitializer (for example, in a class called Application) and adding the Spring Boot @SpringBootApplication annotation, use code similar to that shown in the following example:"
78,Java
78,import org.springframework.boot.SpringApplication;
78,import org.springframework.boot.autoconfigure.SpringBootApplication;
78,import org.springframework.boot.builder.SpringApplicationBuilder;
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
78,@SpringBootApplication
78,public class MyApplication extends SpringBootServletInitializer {
78,@Override
78,protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
78,// Customize the application or call application.sources(...) to add sources
78,// Since our example is itself a @Configuration class (through
78,// @SpringBootApplication)
78,// we actually do not need to override this method.
78,return application;
78,Kotlin
78,import org.springframework.boot.autoconfigure.SpringBootApplication
78,import org.springframework.boot.builder.SpringApplicationBuilder
78,import org.springframework.boot.runApplication
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
78,@SpringBootApplication
78,class MyApplication : SpringBootServletInitializer() {
78,override fun configure(application: SpringApplicationBuilder): SpringApplicationBuilder {
78,// Customize the application or call application.sources(...) to add sources
78,// Since our example is itself a @Configuration class (through @SpringBootApplication)
78,// we actually do not need to override this method.
78,return application
78,"Remember that, whatever you put in the sources is merely a Spring ApplicationContext."
78,"Normally, anything that already works should work here."
78,"There might be some beans you can remove later and let Spring Boot provide its own defaults for them, but it should be possible to get something working before you need to do that."
78,Static resources can be moved to /public (or /static or /resources or /META-INF/resources) in the classpath root.
78,The same applies to messages.properties (which Spring Boot automatically detects in the root of the classpath).
78,Vanilla usage of Spring DispatcherServlet and Spring Security should require no further changes.
78,"If you have other features in your application (for instance, using other servlets or filters), you may need to add some configuration to your Application context, by replacing those elements from the web.xml, as follows:"
78,A @Bean of type Servlet or ServletRegistrationBean installs that bean in the container as if it were a <servlet/> and <servlet-mapping/> in web.xml.
78,A @Bean of type Filter or FilterRegistrationBean behaves similarly (as a <filter/> and <filter-mapping/>).
78,An ApplicationContext in an XML file can be added through an @ImportResource in your Application.
78,"Alternatively, cases where annotation configuration is heavily used already can be recreated in a few lines as @Bean definitions."
78,"Once the war file is working, you can make it executable by adding a main method to your Application, as shown in the following example:"
78,Java
78,public static void main(String[] args) {
78,"SpringApplication.run(MyApplication.class, args);"
78,Kotlin
78,fun main(args: Array<String>) {
78,runApplication<MyApplication>(*args)
78,"If you intend to start your application as a war or as an executable application, you need to share the customizations of the builder in a method that is both available to the SpringBootServletInitializer callback and in the main method in a class similar to the following:"
78,Java
78,import org.springframework.boot.Banner;
78,import org.springframework.boot.autoconfigure.SpringBootApplication;
78,import org.springframework.boot.builder.SpringApplicationBuilder;
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
78,@SpringBootApplication
78,public class MyApplication extends SpringBootServletInitializer {
78,@Override
78,protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
78,return customizerBuilder(builder);
78,public static void main(String[] args) {
78,customizerBuilder(new SpringApplicationBuilder()).run(args);
78,private static SpringApplicationBuilder customizerBuilder(SpringApplicationBuilder builder) {
78,return builder.sources(MyApplication.class).bannerMode(Banner.Mode.OFF);
78,Kotlin
78,import org.springframework.boot.Banner
78,import org.springframework.boot.autoconfigure.SpringBootApplication
78,import org.springframework.boot.builder.SpringApplicationBuilder
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
78,@SpringBootApplication
78,class MyApplication : SpringBootServletInitializer() {
78,override fun configure(builder: SpringApplicationBuilder): SpringApplicationBuilder {
78,return customizerBuilder(builder)
78,companion object {
78,@JvmStatic
78,fun main(args: Array<String>) {
78,customizerBuilder(SpringApplicationBuilder()).run(*args)
78,private fun customizerBuilder(builder: SpringApplicationBuilder): SpringApplicationBuilder {
78,return builder.sources(MyApplication::class.java).bannerMode(Banner.Mode.OFF)
78,Applications can fall into more than one category:
78,Servlet 3.0+ applications with no web.xml.
78,Applications with a web.xml.
78,Applications with a context hierarchy.
78,Applications without a context hierarchy.
78,"All of these should be amenable to translation, but each might require slightly different techniques."
78,Servlet 3.0+ applications might translate pretty easily if they already use the Spring Servlet 3.0+ initializer support classes.
78,"Normally, all the code from an existing WebApplicationInitializer can be moved into a SpringBootServletInitializer."
78,"If your existing application has more than one ApplicationContext (for example, if it uses AbstractDispatcherServletInitializer) then you might be able to combine all your context sources into a single SpringApplication."
78,The main complication you might encounter is if combining does not work and you need to maintain the context hierarchy.
78,See the entry on building a hierarchy for examples.
78,An existing parent context that contains web-specific features usually needs to be broken up so that all the ServletContextAware components are in the child context.
78,"Applications that are not already Spring applications might be convertible to Spring Boot applications, and the previously mentioned guidance may help."
78,"However, you may yet encounter problems."
78,"In that case, we suggest asking questions on Stack Overflow with a tag of spring-boot."
78,19.3. Deploying a WAR to WebLogic
78,"To deploy a Spring Boot application to WebLogic, you must ensure that your servlet initializer directly implements WebApplicationInitializer (even if you extend from a base class that already implements it)."
78,A typical initializer for WebLogic should resemble the following example:
78,Java
78,import org.springframework.boot.autoconfigure.SpringBootApplication;
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
78,import org.springframework.web.WebApplicationInitializer;
78,@SpringBootApplication
78,public class MyApplication extends SpringBootServletInitializer implements WebApplicationInitializer {
78,Kotlin
78,import org.springframework.boot.autoconfigure.SpringBootApplication
78,import org.springframework.boot.web.servlet.support.SpringBootServletInitializer
78,import org.springframework.web.WebApplicationInitializer
78,@SpringBootApplication
78,"class MyApplication : SpringBootServletInitializer(), WebApplicationInitializer"
78,"If you use Logback, you also need to tell WebLogic to prefer the packaged version rather than the version that was pre-installed with the server."
78,You can do so by adding a WEB-INF/weblogic.xml file with the following contents:
78,"<?xml version=""1.0"" encoding=""UTF-8""?>"
78,<wls:weblogic-web-app
78,"xmlns:wls=""http://xmlns.oracle.com/weblogic/weblogic-web-app"""
78,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
78,"xsi:schemaLocation=""http://java.sun.com/xml/ns/javaee"
78,https://java.sun.com/xml/ns/javaee/ejb-jar_3_0.xsd
78,http://xmlns.oracle.com/weblogic/weblogic-web-app
78,"https://xmlns.oracle.com/weblogic/weblogic-web-app/1.4/weblogic-web-app.xsd"">"
78,<wls:container-descriptor>
78,<wls:prefer-application-packages>
78,<wls:package-name>org.slf4j</wls:package-name>
78,</wls:prefer-application-packages>
78,</wls:container-descriptor>
78,</wls:weblogic-web-app>
78,20. Docker Compose
78,This section includes topics relating to the Docker Compose support in Spring Boot.
78,20.1. Customizing the JDBC URL
78,"When using JdbcConnectionDetails with Docker Compose, the parameters of the JDBC URL"
78,can be customized by applying the org.springframework.boot.jdbc.parameters label to the
78,service. For example:
78,services:
78,postgres:
78,image: 'postgres:15.3'
78,environment:
78,- 'POSTGRES_USER=myuser'
78,- 'POSTGRES_PASSWORD=secret'
78,- 'POSTGRES_DB=mydb'
78,ports:
78,- '5432:5432'
78,labels:
78,org.springframework.boot.jdbc.parameters: 'ssl=true&sslmode=require'
78,"With this Docker Compose file in place, the JDBC URL used is jdbc:postgresql://127.0.0.1:5432/mydb?ssl=true&sslmode=require."
78,20.2. Sharing services between multiple applications
78,"If you want to share services between multiple applications, create the compose.yaml file in one of the applications and then use the configuration property spring.docker.compose.file in the other applications to reference the compose.yaml file."
78,"You should also set spring.docker.compose.lifecycle-management to start-only, as it defaults to start-and-stop and stopping one application would shut down the shared services for the other still running applications as well."
78,"Setting it to start-only won’t stop the shared services on application stop, but a caveat is that if you shut down all applications, the services remain running."
78,You can stop the services manually by running docker compose stop on the command line in the directory which contains the compose.yaml file.
78,Last updated 2024-02-22 18:33:28 UTC
79,�      ��k�#G� �~�_D-�	�
79,ޙ��.V�Ӝ!�
79,"{�g�R� "" "
79,@���J�y�>��=R�4��3�+�ޫ}h��ՕvW���ꞣ���
79,"���_�f���g ���n����""�����������~�{=��>~�Y���m��,������>o8�e�e�u�~�5"""
79,"�'og�4��O���:���A���m�O��:o��|�89�����~��:��u�;��,�4JO���_W��4�C������v���Wg��iԉ.�d�]!�2�y��ۋ4��CK^�8�Ͼ���:Y�y��BAx��u��hv��N�yq�v�Ǐ�cs�[��0���m��""+h;�6�`!֖P"
79,"긥%�����~,�}{��)�������Fˋ�&���z"
79,M�C�4y���NO�����<^��+RG֞&��sZ��4\��8�b�&m��E�U��t�Y�_4~��{�|�V�E�*��7I
79,X}=�h ���4X�etqe-�:��E]���%_ZN
79,����@F���rV��x�]�GJ]o��$ɳ7��� C�rf�r�\��n�
79,�ņ�-�t
79,��:�l������W��I�v����8_FO~��$˜��t���`=��O��x=w\��e��qLs�K�|�D��_���S�C�Q�ф���N�l�<�dЁԡ�n�JsZi�8�:ۦ�� �U�t��fSv
79,��� ];��
79,��� q�
79,�-
79,"F�H����.��I���\4����2��/Q����@,lF�������o���Rhc�T>�,�����"
79,�pi�ݾ;j+3j�Zj��<�D�Gw/Y٠�m�_q���K�������������N��7���-�;�*l�$�A�X�
79,�7>�����
79,�L���x�q��ֺ �N��S��H<
79,"�t�'S�M� 2S�}����f^Պ3""ׄ�t��f�-�x�%��UP�yx��&��R9��$@��۵�4"
79,���6\k�@~
79,��M�4�e��*Ho.�
79,�d�6�����ҁ4B�Eh�wyK���Li���0��-;�?L��ڙX��A
79,t�@uR�/|7�۰n!�Kd��k
79,v:�*^o�(SK���
79,i�0IÏ����kw���ӗ�~QG�8����j?�D?Zd��I���p��Ϟ��/��}��'p
79,�k�g�Y\2=%8�*
79,Oa�v{��d�-��B�5X�m|t�SR�
79,tp�I�B����Q��{��(�QNf�qgD�p4
79,����7(��(oO3
79,"W��-N �5�h8��,�;����C�`�7Jdo��з�y����7h�7Z��\zG��"
79,����|��|�>��7���?�
79,"�l�J��W0h'��ڡ�ޠ��d�։�e�Mo8(n��@�SB��ʁ:���`ш���Ͽ�""��|�����h	R�"
79,_h�/f��[z��-���(O��=&��M5����2�yS
79,��y�:i�Kt'Y�yα9y��w���U���!��E�
79,"�3�9�k��L�@[���.v`�|V*?��E{	�?:7W�Y*l��f���y<�9���[�!C�Y���A�h����MD6w�,� �m�n�"
79,���[��6o[�
79,0U|
79,"d��n��H��u�""U��]�4/� �ג�.��-�p�-"
79,#��I�V���
79,"""�3�3"
79,")J,�v�|R���]F��tß����v��W�ޞ4�K��=���\�0.�S[����&��ɚϓ��(}@��n}�Y4�4�C�L��""YG���""�X���z�/�ׄ�fBt���_x����y��[�]r��,�"
79,�t�|�tx��#ח ^g�
79,��H^%W8
79,�� <~
79,"�""��M,#�~�5���0����f5Q#J�"
79,�rй���y�f m����}�P!j���g��E�h�J/�V|�b�\��Q �$�Z��3�xN�Fk[�
79,v&(x�z/����usw]䟥/�@�
79,u~���wNb��i�<�|=o���	��0��;�Ct���Y�+&Z�� i}U
79,l�Dc�>tغ�)����[o�H9�
79,c'�
79,"?�x�*T�B""..�g�J.�-��n~�F_n�ty����KA����U��'�7���e�[^�2�"
79,���p
79,"""$j� alm�������yz���@0�}D�(|'k�;�dr�Y^�vN������6�s�~�;M��O������U��W;�����.�_��H��q"
79,2sJ�d�-��zg
79,��f��j�5ɓ�䐺��L�o��(��ʉ~
79,�|:�
79,�/Rh
79,���nD �D���@��F��V��>�1��ވ����[��R��Z��z����ϩ08`B<\����Gg���&}�?4AV�Ut=s�W��6��P� ����#�L
79,��C\m��gT�E)`����
79,N�	B9��:X 7`74�m�/�[�$m��R�����~�hpA�9[T
79,�&�y
79,-��A���:�Q�_Y
79,"���h��V.k�&�o��x�O�跷i�$o]��n�,�6X�'���8��ڑE�p\ m�,��V�T���$���""�#��r""��̊�E""�)p@��RҸ)�����A�0iM�D$�@.ełkm�%������>��0"
79,"���v�ݬ�Le(&���Qe�3j�!6����6>Y#����1�Bm�n[���,� �'g�P�"
79,�e�}��}��L���@���
79,zfg	��-���7��� 9��pY��̅�E}i��m
79,�{N��Z���H���E��&
79,�^�\e%�/�.�*S�/@uჭ���kBQ��M��P����BUR
79,i��U[)'e���J�B�V��\��[��a�ݞ[�bm�L�^���Qq����E�Y1�l�䂎L2i:5������f
79,��)� ]�\����YX�QGjkl�E���%��mȝi�c��Tdq鯳t'���sD� rU3�
79,Y{N�xR
79,���6
79,�:E����O�~�>�~o����%��9�Q�?�(�~��`��~�_�/W��u2���т���?��w`XeY��пb���8kl?k毦���C5�8(
79,"\fK����p�ݎ}Z�˛��I��""~�I4�.�����&v��zI��	�~89�+���Gu��"
79,���&��`��Mt�N��1:7
79,"��0{����j�|��vU�6d����G�����/�f�[E+��v��B�=M��^Z�GF���t?Fj�?M>L�u�`:����C����w �s��9,4��i�0�D$\���y� ��>"
79,@A��r���z
79,���-�7�\�����[�g#υ_#��{�������\�{��맃��>
79,���{������t��D�z��������c�<�������g������w�A�=�|��~�G����FxC�
79,[��S�L��;����S����.�(��w��
79,"4!����yf#��H}o@�=�Q?#��}�gL������?}��ƀ����޸���:ʒUt���М""/��;����u�� ��x8"
79,���i'
79,�m
79,h���z���`J��4��T$O0xbܜe���[DW��w'��hO�e(�l s���`��w�5�dz�k��
79,"�;S��(��[K,�كi�LhmQ������)�'����h�t����`���'�e�"
79,"v\��,M�|纓̝,���#/"
79,D��<��a�z��Þ��g��A4~�$�Bx0��{}� �*ǣY8�:
79,�&	�
79,V6oH���(Z�=�G�>ˣ`y�����1��kl�4�y���w�ӌ��9�h0
79,��a��
79,���u�
79,+�Ic
79,����a��<Qd�)hc�ܺx=K��)�
79,���L�%a�G��l<
79,864��?�O(O�
79,�J��Yv������-��#�>h5�G�$r~�~��
79,IZ��G˫��A����i
79,",[��K��6Z��	ưb,"
79,}R��j�=�C�̗�և�z���Q0MZ��>j�I�Cp��
79,�Х�F�@W�q��cg�x
79,i���3��h�J�����[�_��o�O�&L�7[ggd
79,�NȾ�n��r��+
79,�I��Q��R+���s�I�Y���N����I��7�z5wt�Ѹ>w-��8I�2��������w�� <Θ@cB�*HOL
79,�db/�*:�6�4���pi0�Y��?���L6	�/�i�]�.{A&
79,%rQ��n~�� ��~~�Hy;i�r<�3^/�E���mt�8����a
79,o�ߜ��>�J�'�ae/�;�^�����Z��E���j�<� ��=OVg�>�ES2����mW
79,�����������n�c������
79,Wؼ����}Z����7 ���xN���0v
79,��>|b�N{�9�g�*�R�d��.w� �-~w�r�D�bo���)����v�G
79,�(=�
79,"v�	�1X?��B�T ��#������/��3�4���\|Ϸ0��,s<��4^�[��|w�t�"
79,@B��Vq.�X��vӀ
79,�rq�>��a
79,���
79,� ep>�򰕧�J�*�!UZ@��!D�)��Jּ8��mV>o���67�
79,^I9���8+�3^!Z�u���{��F)�7�gg ��4�⪸�R�4�*��#�3da�Gx��~�o7��-Ö
79,���u6�V�V��{0�� ��@��
79,H-��\�Hy�B�	��l�3���I�͓��ψZ�����.��d�@9����0
79,"� R�X�!L�H��B8��""����r?f1* ,�w��d�"
79,"�'�������~&�p�2)%$+�������ZT�5�%�ec""*-�=!���2�?"
79,��Z��gٻ��뗡�p0����R�o-5
79,�J�U�X��4���)�N��DPT2P��o�_��:[F���/�Sg��\XE�Z���M�y�q;� A_5�S�����UC�N���O�8��C��⇰��5����%���8��e����`��2ޜ�D��)���:d�v�����sy�n�T+I��jǻ:Y&ӗ�:��rT?!g}P�Xh*�����1�]j��25B=�����m{
79,"\������^�i�']�KSH""7!�Y�C� 5�aU3	��Pdg��|����n��挐"
79,��:��E��s��+�B0Bxs\3��8�H&j����N�b蝒t�X�T�r)�ܤ>-�B��$z�	�K��]IS�D*	�Xŀ�
79,EqG$�B
79,"��{JX�QȘ}�-ԓ�P��z�	�AO��jRg��""}H���ů��A����dm4�6��Wq�y�@ܸ�(���k�,h��� m��"
79,"����Ýq��EuI6����T�Qեu��]����JO~��B*,�ć!CL���,� �;}\5��"
79,D%�
79,���N�V
79,Z$JO{ߕ����
79,"b�g""�c�:�tT�N�p�g�`ƖeB˨H�����CX"
79,���NQ�$��
79,E҂/
79,�nhO�5��n�Λ���bu�Kj���ӝ3.��o�@E4��> &z���3�����ɦ��'ib7BʷR��Z�ye
79,"��s�{L_�Sf�*�M�e`���r{��d��$�1�X$G%��Ήk ��5���IӢ ߦhz�x*,;k ��s恱B\t�� DV	�t�ݤE5~�+��}��0e�,"
79,i#��
79,��8$���d��%��Ғ��ɢ�+C�TL�xM�Ng��8M�B8�^K���4��
79,��AO���	�֩L֊(�;�NA|K\���	�x�¢����0�d
79,"��U,(ݠ�/*�U�Z�St-]�1��He�e�ݩ��M�z{[�W�G�7�����Fv����="
79,`0)����౽\�<Ҿ
79,��Uy5����6��qE
79,�K������$
79,"2�x��� �N��n_�������i�\M���Xqy�/y�Hj�S��G�K�M�[L$ڞ!|/���8�O�7�G��ө�V�`�TW-�Vrl��6���^˭i���',��x4����^�`Dh�S�)�:�^��_i��~9V��^���l"
79,���z`	�����Le@!�w'5
79,�D���**�
79,�e�1�
79,@�i�\��m/��(e��q.���+qM��
79,XFP�~��-�
79,��r�
79,�q�[-LrO�Md-�=��4�
79,��l�
79,eH��8��o���ރkJ͵[�k:%�`a
79,"Ȋl�G^�8_H��K���<�ϗ�""��6"
79,"�q��ɑ""�~V�YXz��$��1"
79,#��o����
79,:���!��ɥ�S��a�u��X+�%P���Ʀ���}�zT�X�
79,d$W��
79,";���R˞��$7gWZ��'<&�6㫝@}Ô23�NG0�!��qʸW�|Qr��?A�""�s{�P�ޤI��b��IOl:o�m���G�g�����&�"
79,"xL(06���mi�$m��A�kI���k@�l�z��V8]0��w���2��az������n�&�B�,-O�	O���йy~�'�p�*��I���"
79,��A�C'���*8���K����8\��Ʊ���4`��c|6(�Ca
79,�Q�\�v=u!'I
79,�l3�ΐ_��*�hԓ��
79,�Q�6%L�M�'ټ�iU)\~���Sd���~@��U���B<��*aޱ�#�zPȷ���F�oLN���_7��S�߫��J�����o��EV�������\�3уc�
79,��Y��}MIQVdSM�s{�ʠ�l;��`2zM�د�	r��]�h�FAy���@Hǋ�
79,_ME:	4���D�>�t���bP�}�~W*�b
79,b_v� -O��*�KK��K$�yGo�
79,a`x��͋PX��M�d��٪4�F�f(1��$���e����rs�+�Yψ���|F3�0k��J�
79,��lkW�j�/:-
79,n�#<M�m���?������=	5C�q�q��
79,��=#��I�q�\V�s�鏢:�v�fI�;Q�؉d��f&S&
79,�o$́N�|
79,��h�ڞO�5�v0��]����G������iSo��Ps���`�%BmU�#�����)�4����sYQ�	=���M_�Xf�wx��)
79,d�֖�v%̂-*�覎�3
79,nv~���\o\<� o]�9ץ{SԒ�-��0��^DitB���R85��Ȟ�������	���΍��x��8[��<�d2�v������M LCO(͆�0����8�疦3?�#3�r
79,̕l3_
79,"ƶ�(E��ͦ{	���(;�4���I���""��"
79,9�e#�0�|��ٴ
79,� �
79,���2�O��H�
79,?��a� ���t���A���|��4
79,���
79,�Np�ZjTiZV)P�R��a ���m�}��
79,+4sOUg�����('Hg��J
79,�=<�%�AW�
79,���z�&�>�{*��@
79,q�7X�6�m����T�q��h�$K�~
79,�+7�D�<ݮ��{�mQ��(�<�]
79,"&����Q�Oat�y6��3""��٨7���"
79,������h`���ƃ�>�����h�i�
79,�b?�C��������S�m���ᓡGYa��L=ZC����:ADO2�*&H��tB�OQ7�Q��n0����'��x����Z�4��F���4��*�D
79,"�!�|�;���h؛��DXC��Nu!+̊�!�gd""�Μu+�W"
79,���2
79,.q�L?k�z4f�������ѰF��W
79,�3��H[ ��Y��
79,U@���~� �
79,�����r��XV�ھ:f�`E�bu�/tK{���7���Ŭ���>�T�����Y�N�ŁD�҆y��^ik��}}u�U���
79,�� �aO���
79,�TG��ח�e+W��Y<5�V8
79,"�1���(���St��˳�腯]���M9��R=��q�Ay���qqFk!x�ШM�=""�����b%�~Y��� ��A"
79,!0J.ђ�+{�����eXN^rT�L��
79,"P""�iiW�z�t��J{���i��F3��|�L^B�U��i�O�Z8��9d�$�"
79,"�W���0��;��!���̥�xv�YR�`˓Q�]��<��d,D'�OԷ��e�,�5� T��1z{���mpx	ulII梠A��M���f"
79,o�%ѽ-�
79,"�|(�n�Ѱ�֦��;�T�'�ei(���������2�H��k�#�b�[�8�ɐ��O���Z=ܡ<߾�z�謁��y��=J����')���-sC�+��F|���r�H�������H�z1�c���7F���#ؕ��b\���6ctm�%��R�m<�N�,���d�lUC��>Pɝ�v�Ւ"
79,��`�EGy
79,"�(�Ǔ9��""A-�����,|�(9�(���]���΀t/8�'"
79,��y*�+sRj�ݢs��D�5�R���F
79,��Tf!�Zt'���G�l�
79,��b_�|Bs�H5����CŅ�Pt=�u�B4΂4
79,D]�o����@|mxYX	i:�uU�
79,�R1%��Dp
79,o���+ێ�)���f�EJ�B� ):
79,"�Z�^�q���""�-)e�oh��^��b�"
79,� �~ܲS�(|�K�n:���y=�.���(�-��P�/�����i72�#O1m���/.>t��}n�(��rSm[x^&
79,�5�qN���-���Ж�s���	y�N�Yf�t�]��SKJl�X�������Q>�&C	WK�Zʄ��|�w&9�@
79,I�Wb���rI�H�e�OY�ju
79,"�֌�L�R&�jk+��H�N����Ό%,�xݫ��ިo.��rs��G���,6�$9&V�v|����~=�t�R!�i�Rꍙ�/��(�"
79,E��X�
79,M�[ؘ�uL�
79,d�tY=��P-��4��|�Aj�#�V6N/�����n�Z�7M�����b�86�Y�؝�Ԣ��u�U�Q���Q�k�
79,X���ĕM^���y�́6��'+s�]yr����B�E镍������*��x���0��c���Ƶ� ���
79,��gB
79,�j4���E��_%k�X���A�7 b�����r��4�UD
79,")��H�^����u�碂�K,��Զ������6�$�,$<��"
79,ݮhPئ�^d�1���4#�MӔ��ޮ����V�z;
79,��6ţ\�R
79,:�b�4��CQ�u	|�YV���E�b81
79,�<2WH��w��H�R�	6�q
79,",b�g�ұp �rƖ��@_�5��K���I2b���h�Z��7,�6��k��t���T"
79,V4�v�+�vo[H�{}��\і�
79,�)U��nIpV2� -|�p��*˪��F�s �^Gx�R<�Piܥ��Jf�D�f�
79,�:���
79,~fs�tE�_��F�������e<�Q8r�q��!Q
79,")Ͷ��`�=�����וW�/�U[���YcR��@Wi���2h9�įT-���w�m��Т�f7� �,:L��"
79,"�0d8�+6C�Z%�yb)�GZ�>%���4���i�����-��,�F�A�rl���h��m@�O,�N�d�F��"
79,"��؛��,��UĶV���r�Gf�"
79,"��ț�*�6�Q	!�\�~�""t��%���:�@�YP7"
79,�N3�����aL�ҍT��
79,��G�xhN�I���x���'�^'�K�
79,��f	.:��C�r��^g�=���戂��
79,��z����=9:`=b�e��z]��WBm�;tDb�7;�|R���8 ���E��j�$�
79,"}��x��M+;���$�.r��L�����KԹ.��=��4s��WW.�@�tO�K��,X"
79,"9��H�t�����\���J)!ºc�u����@R�l���""�_�,4hX�7S�ŋy%8O\�PՐA���*�"
79,��tV���Q
79,��UtI���
79,*��
79,P5φ �q9��ة�(M�Q[����[�<9!��dB]��B�-Z� ���
79,"�Y�A(����*8�P��%|#�7vT��3�fֶ�y��""ޙ�""���ѴE>D�� ���"
79,��[����
79,"q���E�a�2FkT��������X*��$��SF��,&���ƛ����0֛]egu4�0)"
79,"o��*rJ)�WJ;,��ԪQ0�aoȁ�f�t�"
79,"o�IUSȠ��2�dq���H}ҕ56F��A-��2������"")Uʛa�����#���"
79,8mW+���� ��j�A��&E�rI���*^����]Z�ٹ3C$��)xS�
79,�Jd�#э�� �5���G��U�cox]4]�1
79,H<��4�����Xē_ez�2��@�M�.IПfQ�w�N�kN�l
79,f���B�oͅ���[=W�'d
79,"W�j	m���yw�s��A��xn���ե&��c��EZ��,Y*E���Y�D^u�^���o���Z��D7"
79,��jԶ
79,���m���;1�W݋�^eIN��C}�Ѿ���
79,�d�
79,tΠ��Y�YZ���
79,"�W��,�{QX�t2�W�C�n� �g��!��l�dA�щ�$+��V��&��15�"
79,��� *_
79,ЫʃM�ؔۡN�BM+��ꛉ�vM�i���<�'�1�-���ܔצ�Z~LR��#�~�A����AScj��w���v���¾�T�v�����Lv��� �Y�#����^�W�~���ێ컩
79,�gw7�a��� ���g=�
79,"��hz���>%���q��Q�s`�.�/���""�f����j3o'����l}��4��'<�;�"
79,"��]sP-����2�Lۛņ""{�u?�g�2w���x�W�A4����i-���8��*�N��u�|y�Ev��R��/���3A"
79,u���g�:�g/\I@��|��3yЁ�E��s
79,;� l�����Yn�)Ε�;���6�  i.T�@+;��Ф(9uhm�
79,?Z�P��M�X'� �����5.F}we��Iq;i
79,d 6��)Lo+
79,6�eL�N��[_dɺ�L�A�]4H��l
79,0�f�x�k|���U�8c
79,����S
79,���h5�Ka�>`� �0�|
79,�#|�RIcg?'���~w??��t�P��
79,��
79,"(�|\r>%����;P���s�v��q�/+,q�7�h�8�"
79,F�G��!s6n[0OAt�7�c�G3B�C�
79,��G�:t������{?���1�
79,�)��CH��/n[e�t2�[��v�j�ہB�Γ�����h�#5κ�ՠ�V�����x�`���zHr
79,"#�G+���%9�,1#>����t�S,E���&C��6�>+���u���N�\[�2OÒ�"
79,"��iY,u�b�Hkˮ,"
79,��T�ԇTva��_g�
79,"�����=��9�U�%���=�1Ɵю|�6xV�}'����W/�]]f96�ۗ[̔M���4�r���b�/�""0.F�""��dm9x��8��4ݮ��ip���""��K��䢡��N���-,��96�⡟??"
79,�f=���co4����7yA�7v������Ë��qzQ!�\I��\�.�M���+�$`��i�1%��`G�	}�B9�E�`����ψ�7��e�+�2�0��v��fmbk����:Ô�3L��5��8=EC>kϓd���MLZ��b���a���`/�Y����
79,���q
79,������y/尾�7�S��>F;�%+\d�C4e�
79,�e֥ƺ�ܹ3�
79,h��9����KO���OT���t��g�aE��
79,fA�;	;��`8�M;��A�Gb���V2I�E��j�M�ޓ0IAms�s�D��t�⁢ײ��U~x�`�L��]��g�F0�����0q/�uxI� �q�Wy
79,�ah�(� �a�7��]�3`��p(2�.��ؘ��g��TL�d�<
79,λی�0
79,d\fW�K��<��)hJ��
79,",�k�"
79,"=@Ԁ���4""C�"
79,"L�;�����������uJ��q��O�7��Pџ<�ޝ<��D��(,�,���%_uk����tᠥ��V�&-�|��i�흒O�m傔���u�=�߈�#"
79,"�S{Yؽ+(zW��ߖ=~�uG�0� �$w�p�t��zr7��3{���i�;�W����Q�,)���gá�?z5]nÈo6"
79,yw�.� ���#y��(�{��l�y��4
79,��?�u��ُ��(��h��}Mm ]�
79,��:�s��`
79,]N�z�E�
79,�^���`�Me6
79,�0����UF�zw
79,Z��i4�!���t���
79,��Ea�oa�3K����_h�������G?}�{�aM�xv��ӧ��{��{�����o���6�d�g���n� �����֐�����9t:{��b2��z��׻��*�6~F7��<9�s30L��y���.<� ��2��ǶBѫM�/:���{
79,"��X �����IY�x�i���m~ѷ��|���E�n���vM�X'��U�:��z�\����b,��|��17�T�y�^|�F{�\�4�9N�2�4��`߽Q�~�h"
79,���Z7���Q��y���$obvThgކ�=σ��M1���a�/��$;�]���ӈ��)�*xu�	�؛穈�\�j�V�oT���&����3
79,"�ۢ�z�`U�q�e�؁6��,��W"
79,"\*�""a2ݮH,�^zF�Y��(^"
79,"r�'2s ͈�=��&�l��D�9�y�ɸ�,,[�4�c�`�R��$�#-���.e�q���5e�Ψ�ǳs��������������?�����ӛ���y�.|y���!����?��~t}q�h���ON������"
79,6��q�I
79,_\�ios_4H���@�V�ٸEr�{�36���6`
79,",����6^�I �G����'$�Q�"
79,ht�lB��6?}�x�
79,L�j���[�mue�9cF([����蔾h����{�� �.���-hȀU�l��J%fa ���W����_
79,"��5K��+=�̦f�Ek ֢�1��kl&""$���1f>�[d^�"
79,6M�ο͂�2��F�6];dV�J4$�87ylWv��\
79,���q��S��e��
79,���/��}ܒ.����Px����z������C��
79,T��Ǐ�G�ͫ�!4�h��ɵ�I�t	�����싖�
79,F��	K��ŉ(��
79,A�y{
79,���>A͡�
79,"��%,zJ�Q+�<"
79,"za2sVI�]"";'tO��c�e?(DYs'�(��~��]<��y��R���'4�yB���N��h��]c"
79,�[:P�9�����b�>��_��lO1{�jI8Z�*j������wK�sk]�t�4��<�.���?����cLh���d���A^�r-NO�pz}��hO��']��Ƙ�0����:0L�?��^Jz�% a	�~����~������8�O6���4&�V �4��4!B�e�&
79,"+>�,X!l� e�O�4��Y��	�gB�vs!�"
79,"�ʑi���[R�+�L�00�[T�Q�� ��@�a�z�~'�=BM�""<P7��J�X�2�,~q��{V�D�%m�[A+�5%�2�46&՜4�m�#P�+@Τ4`������F�B�oP[�������P<�F~��Չ�"
79,��V@����֔L�
79,"��'��GYw���f;����Ԯ�B�u�$L�pÐ�);�f�q�&��Fh�bhbb""U&<,�M�^��z>��>~|E'������� ~܀n0�S�Q�G����$�����g�����[�aC����'7��u�d�<_��{���чF;+�.����,�ͬ������C�$S�SjV�m>�f�KR �fR*k��6#4�C���9(}/�"
79,ՁW3bi'���3x����:��J�:�:�q}<�)�y�ψ�۠�6�ߒR@��OBV%���w�ڢ���P�����H�I��9]�R��}���i
79,��|�P�j6�ݕ�hBN$����\�|��`@�`'(i�D��
79,"�/2��j��*,��Ԡ��\8+0�-V"
79,���
79,��h�d#Z���6o���iUz񻅇
79,��܁��
79,_Ї[���m:
79,h=^
79,���v�P��f���Y>��
79,"0��;UoJKA""y-�Vl�9��N��ޢ� j<�Ui~�����I� ��mt��c�9��U�ߓ,)���T�P+(�_�"
79,��Y�`�D���zF��dxO�'А6�u~��h �P�>�	uB����D�g��-�S|�l�4�@�xǠ>�s�60|yK�����4��� �
79,"+!�,e%@�l�0`DmA`yh����UE��=��/N�R"
79,"��Y�q��U��Đᢨ`�ٚq���ZN�""��/���`�1�x-�,zAe��l �����5��N*G���'"
79,��i$�$^
79,"�������~��]��,Bu�?$�"
79,��B+3�۶����0�`�R����[ͯ�}2�Z���]�
79,�Ȣ���k�-�����UJ./N��Y���X��)'�9���@����������[�ֽ5M'�b�!�Ӌ���`��d�!pD�\$��o��}r�|�x�����Z��v�㲻-�w
79,"s�>fg㉤��Ĉ1�+O~������$���,,��[��+��>m��J{��D�a���	2��wz����?9z�JNb�K�������yT|׸:V�!~p��&S�jū{���Q�ڑ�vQ"
79,PgW���i�<_ש2���`M��B~�E�;��&��B����@�2�Yu&�[H�s���ɴ�\�^�E��Q	�����`�2x;;PR�?
79,^�un�Jbz
79,�*E�C��
79,u�H��fʚ��S
79,"=�x ��Z5&X�����>{�����OMV�Ś���$;�0�_��0y	I�k�S&�����4N9j6D�*�AD�)����S�Q\DUBB����)5�����h�KfgY�T ��/?T��>lǨ��t)�A��,�"
79,-�
79,"Xow��:kp�����0�s��5<���""Z��"
79,��)��I��C^���^M_��	�<�U�
79,HD��3j)�%�*���)��
79,Ӏ?�(�k����lw�B�<�(��ڣB9�[T�aj�V�3<�Dtfb&�_�Ϙ�w
79,�e	�_dJ4��
79,0TS:���%��E����퉦u F`��&��/v���lMH���g?����]�>��~|��<;���>;[���6-���&�����J��w���Ʒ��E9J���2!�*�
79,")��}��B���_��?Z�,�,�����<)�"
79,Bd��A-�k�9u�>A%5�Z��R�
79,S.��n��wޓ9��湸!���4X�ٙ¬�$! �L��
79,"���,�}b6�#�&�O��p��og�t�5"
79,"c���-��x�8o_Y�i�N��8甪�,"
79,Q!DD
79,�v��
79,"6,�q�BL,�\ĥK�(�j�}��!؂�E�k>k|��j<�9�6�L��'"
79,�Z�
79,$���wM>�����oַ
79,"=���w�R�)��%�:d""r7k$�t��T\��d��B"
79,"��&M{��*�2r�Y� <��k����H���)��k0�/A��""Rʬu�""��T��ӷ��3e����@M�g�"
79,B�����28�{}���4��
79,A����c&�S�4I�e���d���	��T��w�� T��i�����Q㜥:!��
79,��N��]%�ƳuB��X@_����7j��BΈ�{���>˃	a拆�i�8#w���A�G�����#<�q�)ٸ;��]�׃}�7h��E'�%�n�>�?��=O�5��	~!A�4XNO:��
79,;o9�v��YdY��*aB��N.��Zo�o@�3���^�bq���p2������h(����9	�ݽ�
79,��}
79,$�O_���{ɳ`2� ϴ��2��cV$����¸cMΙ� ��4n]Dˍ�����7.�KN�>U�����I��U��
79,"�d�""#y8�eGS2%K'Y���v���g5!��"
79,"/Ýi~�Y��d��r��&�㑂M��8���χМYy�[�/'!Z�,Xm�<�����]%�dTn�y����VD�����:F��$�3�E��4��sP���pW�y%��V'"
79,.��!	$c�[�
79,C��
79,��Q��
79,�R���	)�d�:
79,l�B��+�^�q�9�;�+���T�B
79,�X����6��)~)�$�_J�I��5a0
79,�����>_���%΢���Z�F��W�|�$9s��K\�x�!�D$�
79,De:�eL���Ҋ[z�N�5
79,"��&G�oZ�iZ�~!�'*���:%Ź3e���,M��m�"
79,���b)U�d�p�M?��/�
79,�b��u��u�A���$UR�F�V]�~�e ��=�8Q�L��?H1�RpP�B%hc�M	��
79,"�A}�FK~����9�""s�G�jݕk��q�G�9y'��v�U%B�J��AQ�P@�l`WJxKt���y��"
79,�gS�v��-\O��]��l��L8�� �����ݤɜ�օ�3t�B�*Y^�t֕(Q
79,u=r���ztY
79,+K<9[b�@���5�6��o&�>
79,�� G��f�YÍo�c����U���
79,T�e��C�����n�r;>�G
79,"�,{bZU{�I�U����X��"
79,Z� )���]ci��k ����mw���&�H��5?�9����3����7�u������Ɠ�٢
79,Q��Gw2��P�
79,"��o""U0  �t>	N����~S�bWZ���"
79,"����(�v��ڌa�)��Mt�""y@59�T�Ga�р]qr �"
79,"��4�����R�""BCS�"
79,��S�)ߓ�ى���
79,Aь������ۼrD��9|�m9�=(=*o1`@D�Q.i��IA�5E�]�;@�	�
79,y�~A�����#��Κ���
79,s)0�[�}՛3d/E_Q�٤���xsVj����Q�
79,�s�NB�r
79,��P���Τgd�Q�Md3���gN�
79,"(R.,����w�𶸤�WM� US�L"
79,$�+a����6VHE{� �Ja���6%�����ISr4�
79,g�y�M
79,�NJm�H�N���x-��m��ڈB�ŝ87�z1��ųѮd2��
79,��u.qv/�ſ�-��.
79,��]
79,��<��`�t�G�h�*��<�f�)˯5��i����˞���Ԡ|�I%\�L	/�q
79,k7�:��-gҤ<��c_˦HR[��8�ms[i�i��Je�e$�H
79,"���#,��AA�o�2��5�?2e4颈_�}+ٽ��"
79,T����L����S�� ��\������!�=h-)�
79,g��Ŋ��Qg<�{�;.�am����#8#������
79,�o�^�N�����$Q�Mt�%���j���7��8�~'y`�x����o�ލ �&��t�W��]�L(���v��������w�n��v�^���ߞ�iw��h	
79,�څ���h�
79,��A
79,�_���-�1@�vJT����o�Ҷ`[�SC�A�҇׷�O��;O��%�D�D��Q��;Y�����͡�
79,���6���$�N7ݑb���4#�T�]~����[�
79,Ӻ�L\��a�W����[IvC�ij��n���
79,���dQ�g�
79,��-8��
79,w�^Ft�О�5���2r��z ���!��0�
79,�'s���[��/`��Q>��L�����` s�_�c5���*j�&
79,"M,�M��m�����9	�4d�Y="
79,"}���`�or��5LD����:=���/�7n7��.U,U�N�R��"
79,���T{�-�w)�/�D�9~�U1
79,n�6��=f6��
79,��;����]Z������q�'i�I��p{�ǩ�����R���
79,�BǄ�D�lg
79,"���M��wl2�r�p�&�њ�P""����;R�n�(�ԏ���O�+�m��x� ��Z-�����缦 ��'��"
79,���B}x+!��n�}�iV�kLm��ߵU*1O��JL
79,��|��I�>)��+Ȃ�
79,����s�U2:���>x�y�o��(���o�����|�D�/Z6.�&�8�3g���@
79,��H	6���
79,�8:����j}.�
79,c<���
79,"w���-m��,4�"
79,�O�|��@�7|%SXf�50@��zR0b�0[)EGY5G�$����{7ߒu�$�a6/���Wo����h�%�eW��KWX��[dR3�4T&N��&G��ȁn�\�Y�dˏ���
79,�ĸK&J����:AY�~1ZgC�CS$�@%�I
79,5(�P�얷�
79,"�F��""*k0Aܶ�a��"
79,"�Xq��R[6K��r��etC�S2�������=�l罾3,�+Kz��{�o.���k�Zȶڤ Nㆲ2�U2���d�"
79,�x�K�~.`��
79,�xrïѦ��ȑ�rم�L��Y���޹��
79,"�����Ym��	""s��"
79,�B1��1W?�%4LD��hю�X��E�
79,��Lv�m9G>a
79,"qXI]�ǷY_��i ��O��� )L�����fi��=���]�A�C$O2��㒸�""�B�։���x/""�`�>^��h��Jʠtj�qS�x�祿�a"
79,2둆!
79,A��|Ms���������
79,�����´lzzE�t�R�=��
79,"ӹ_ ` �Ѡ�)*$��Ƅ��H��A-�,��OE�"
79,��ܶ�鐼k䜆'���nE�7���p�
79,蒺�KbL��&�_�S���a��k<򑕖&�JO�>b�(��7y|���Qi�k�
79,�7�b�Փ�
79,"4�����1�\�,��*LR��(�8��>x~��1I�j��m�Mҷ��VBӺ"
79,���
79,"-�-���R(>�/�:Ju:=Z���v����d݉r�,@8q"
79,)�u�$+��ms;&���\�tp6[5�EK.���@��梧��㨎DF]mr�k�U-f#������J�Ux4�2TǢ���>}��u�;���q�J���h�e��E_]}�*��W)v };���J�Wˣ��:�t�	��K`��!�V��>6��N��.��ǜl�hɷ��5:w������{�ȁDF*_�����	��ߡ^��f�*�=� 6�E�/:�v���N�� h��}�}Ń>��7�����hه�C��@���������WK�)>s�'��s@A���h:C9�#���ǅg��Fp��8g�W�n�U
79,a��4S��N��!F(q��ۏ��\
79,*P!I3@����q�mW�LZ\�0�~�PӾ���e'�[d�U��b�
79,J	d��
79,�`�o ��ݝrV��A�c�����
79,\��
79,�fp���a�h���#�߁�-��'�|�
79,"҆,9��a�3z�u(:"
79,"�,*"
79,&����Щ C���Ɖr�)>��qڽ��g�_��޶QL�M.���O�b6����
79,�_�9�6:�?��fL�I��4 �it�l��$2����.�Y2�a�!0�9����v�b2TN�+\���dP
79,"��ER�Y�ҝ���Q],�("
79,�	����_hhH��7��Y�p
79,Ҭ�o�Ǻ\���X�L����Ϡ��!ǻ��}dH�
79,h3[{��2��EL���V:v���7��
79,&���1
79,"���[�(��t�0�*BN��,�A�Z����uR��╥�f�򗌞|{D܊"
79,dQ��H�JH2�9w������
79,�KM�q�*
79,sr4tE
79,��Mtk��Ղ	���������F4���Ш�r����z�Z��e��aE�d��um��������0��
79,��G�GP�@�
79,* �*�
79,tv����
79,�&M��4�f��C�e��-�WD
79,V��I�y]�Ae
79,u@� s�vj�V������
79,"��M=d�&��/w�4,�����]u�*��"
79,��np>�m��j������L���ښqy�ӆ`%TF
79,�Ί�ݗ״�\�
79,��eljp��8�eF'2��ׄ�8��h$r��އ�ƨ�b84 SCl�5�挒������M�s�/��+^����z7���F-
79,�$6�	�0�cl���V�{��nR��!�`k�G.�����+��ΰ�LȠ-�R��LhV���N��[��x�z�R�a
79,�������B+�t�EJ]��o��k�z'hY&����t%�eb}e{�
79,G��s�����Lڢ^
79,";�`� m�{iC��W���Y0C��M=���ãS��,��""��v"
79,"��Ju�r-GX��v!�""�JhՈ-B2�U��l�4 D���9��ɷR2>A��4��t�n6Q:E)�"
79,"��X��:�ly��3�C�""&1S��a%�Au��/�.*A������c=Pb,����e�T�Tմ!��k�2X��"
79,�a��
79,�r�����N�gs�T˘��ʰ�Y�
79,H��~�N
79,E�z�Z!8
79,��B^_s=�U�A�
79,�C�ܦ
79,�>�LӔq���}��$i
79,��TP�
79,l�mf�@ZYE�٢��K�YS1U*L^����S2wjf�n_o�
79,�Qc'���+�b�)�[
79,��˽#g��eY�*ȃt''��޷�N���h
79,"���sʮ;��E�m'5�""�3��<��""1��y�e&�qJiZU9H4L�E	������Z���5J:��-�ߊ�l�5�'�^�L��6c�2��|�B�����|(��)���I{d.���P����వ�j\��R��Z�A[�"
79,i���YSI��� F���E�
79,Y�鼉'c����������ޯ֡��a��������.�$��2E%��
79,"LK9|$iˣ�,���v�04�QҁȾ�)"
79,96K�}�
79,7J��*11�Ok������Da��G�=
79,q�S��k�aM�[�(<�\$
79,G���5b��K޻���{��
79,��4N�q5SƑ�#�|
79,�k������5�!�K8 ����uǘu�0�|yNp�/�B� �bE-��Ր���uЇG�EL�ZV#�=J=�h�R
79,yv�:ȷ6�
79,�n���n��n���-��;�a���l�{5!��E�55�Ea��<^f�~q���xD*KϤ�V�h%�ƺ#Nf
79,�-��H{1Y�w����/;U5�E)�
79,�Z�&)��
79,h��
79,Aj������*^��55� uGL� ��f�2-U����F����/z�=~lW���~/J5Op('
79,e�|��=�����w�Rj�4�!i����%nrP�T�h�)���ri���|��?�y�A/�#5k\�D��D�Y�(Ƥ���T�H*J���|K�Ե�`��Div/
79,x�2N��-�m����r���+�a���c��T0��R}G߀����
79,�T�k�&��:��x�
79,u�|�FWi�n��t����{�%�D�w�ƿݷ�ee��dd�w�qz��-�9�\!�<�՝Ua<���WA&KK�پ0s-5N��^5�\�P�	!��U�t8�c���:+��)=�%��Q&ϥ���~c��XB�^WJ��R<�\oe�;IV�n�
79,s/��+��7/\�� -�1������p3����lS�q;
79,"�>�l��yն��G��֡;,j�4I�U���//K,j�:,�k/��W�[�ڕ��6`�H]ϮL�G�"
79,�c�\-��K�������x�S�
79,��34�:�5`�'�Ò�)siޓ����c����sq
79,"�,�%H�?�����p�cB}l��>(�"
79,F��%�q	R�O�~��{8�1�>6��
79,�#�Ò�)��w?��?
79,"��P���?��a�r\�ȹ��1 ����c���A��0�<,Y�K�������	���g���sq"
79,"�,�%H�?�������Ǆ���3zP�9�8K�������g�p�cB}l�?(�"
79,"F��%�q	R�����Çt �J<��B|�;�""݉X�隭�pW������"
79,���J::�;Jy
79,��H_�}�_�;?B'�����hԯ��>ͯ�<������hԯ��>ͯ��
79,������hԯ��>ͯ�z
79,������hԯ��>ͯ�
79,������hԯ��>ͯ�
79,������hԯ��>ͯ�W
79,������hԯ��>ͯ��
79,������hԯ��>ͯ�
79,�hy`s�!��_���U����G����H`�%b-*e��=ߞ�N��
79,�RQ�#[�9-$�v�����8�w��?f�IH���6k���N9�Ѻ��ތ�޸H!��''���H�[�y�b�!kӝ!z[����X��w��
79,"�?~p�� p6R��:����a�#3/u�y��H�JU�""��0��8Q܅�M[�-}M�l9UM�H3X�f�Nٛ`�yF�������M���?�������!�1d���"
79,"�z�x���iQs8���+� ���o@x޷�""A�kJ�!a���x =*E"
79,K��
79,�ڪ�
79,�M����d;]`��%P�۞׷�6�4M�K!��&�fQ�e��4
79,�U����߆�Z
79,"�V���~�f�"";�M-<j���AqנMi'^��#��""�����jv�m��X1��Ҫ��T�DjCF�?�\x��C��B�F������f"
79,lu�#s�eJ'�}�t��P�T�8��b��**wYR����'
79,�i�eU�C�ġ
79,�.\�'�+�*PqϐQ9�U��j(6K�;MK ��+�����2�p���)[�;$aJ�Җk�'f��
79,:�v
79,��)7J	��j��=%፺��
79,��������s�n$Y��d-#\��ӻy��@J��B���pI��`�%8U�ܥ*��
79,��cr
79,"�NI6���]�d5""P�Xg�d��W���� ۠�wܱ��<`�'��������߳}gt �⾡���"
79,���M�݃����9u9��q��.�!���m�B�+�;��5RN#�Z����PFӿ7�����b��_���/�CX�ͷ�����
79,)�谯7��I
79,Mܶ	T��IF�w��a����2P��B����
79,���ȉ-d�s1ɧt��t��!msT`�u?y���g[
79,�7R�cE�tm�X�/)���_��3d��n���1F�l�/�dFF��_7�?*:����1���')�7Mq�ߦG`���(x4ec�{��T]�v���v�ǫ��M�ܓ������\��Hʒ{J���>
79,�2��8�[ �?��(�m�̂�8�y�Ѽ��'�����)}�6���zK�0�T������k���b��-l52�|��B/@3#��ĥ�?��nn2�G�#�A����3SƦ$h>��w�h�y�;6��\E�&������~|�rF%F
79,�#6�`�_�|�Cv��1�
79,� dB�?If��x���������ʪ1�����㠒u/�Tt�
79,"\L�$��,&W`�25"
79,�x��
79,G����F�{��rvY�K˕E�6
79,��rÅY����
79,�f�A���_��6XҧM5�����:#��lZ���U�9u��=gq
79,�@���W��?��O
79,�vY	�ͮ��Z���k:�
79,�3��
79,Z�o����t�[��F��Uy�����;To?
79,�j�1��
79,bõ�p�V�u
79,߻��LG6����W\
79,O�=
79,��8
79,���s�ƫʶ�s�-����e�#=��k��(�5g���\2��͇d���r���]��&s�$�C��4A���\P[w��޴�3v�Nc������#�M��
79,"�4��%����vD�)�x�p}{�F�*L�!�����}���m�Yt~!Dm�kO�ړ����=(O���:�vF�ow��!kC)Rȶ""���fn���0z�� �wW�Nw����x�B�.��ix�ڠ���k�zGj���"
79,��A��6hp�ix�֠]��Az�-
79,Q>���YFT��5PM�.�n�
79,�-�m�Bq V@>�w�����^
79,6-\;�B �m�KSG�[�%�HB��g�x�D�Ke�_��-�[�w&���	��z���c����H�`5�U���
79,�F��%J�
79,�F��C��c�	� <[i�6B���p4��Q�E
79,�]��X��uc�
79,�4N��gd/�l�4��(
79,")N� �BC��IR�D}H�ڊ��]i�wn��M�'�0ȃ3��4����j�<��Q�m��\���;ɫ����:���}�p j���E�o�NO������v��O}����'o�&9S,��b��)eN����Ox�or�pI;"
79,��E S�0l
79,?��TB;bd~*�aG�ȱ�s�ɭ��l�J/�����b�\e�Oe�fEH�	~3e���xX���>�6~�_FǞ�
79,ߗ��gw�E�������^TWpݗ�
79,�{R]����lW����etC��3'ߦkͫ8�'�2�ov��b�<st�Eg3K��y���|D]d<�� �
79,% ~F@l��ů h����k��'�.����}���yK�
79,*�3
79,"�%L�f�(��.��Vk:��C��$�45�z���8�����d�ۺl�d�0|+(i�dYNl5�a���-��|��,	!�,	Bz?1��¡���	�D��Q���i'�֑��*X"
79,{ץ{��
79,���Ӟ�Ʃ6��
79,�@I+*�@)
79,p!n��Q� ��z ԫ($��˻Q�Y�F:0@�l�z����i�j�D��ٲj���P�jTD!%�C�
79,"��Ҧ*F��""3mh���(�RQ�P�^t��V�٤���"
79,"H""�Ȱ@"
79,���=��!�w4�|TU����'�6�����(a�ف1�hW/�����s
79,"�ɍJ,Xgy""�:��[b�����y��Y���0��e6�(�<>�������5-�{��8-S���ê"
79,"ծ���E�X�S�{�.ʩ��=(�|��""(�[��u��lI�@<�ָ"
79,"���B*""{��/(��`�\/���B�3K��ir�i�UVNN�b�EĒ��� �N_�8*�}�G���D*�:&p\"
79,�.V�(��gq�V;_Bߡ=U�
79,"�w,�T��h����V@���\�Ok�����xP�<"
79,�$�f�#��;yB�u�b
79,"B�K��.9Gm9M�w&��Ԉ#���m%���2Ž��'�P,��v�E��ݴ"
79,mYVo��\�s��J�S�A�}�W�ߥ�'�9��!��toΐKY����$ߕWԵ�&뙋��D
79,U��eB���x
79,W���M��]� I7X�
79,"w0�X`S�r��!J��^�;j�u�2��,�l�;=dRtHQJvZBٲK��� �?:A-���%=��c����,фѵX_�m�5ԢG���נ�f���n��3� ���� BuQ�ҴJ?1Մ��"
79,�����l��!
79,"�(�"""
79,"�m'�s�:H���0��""1g��.��椓F���o%M��u���͖�dө_�L�;����q�c17Ers�#����}�q��0���2a4�-�vV�AN��D�q���oz�����uB65;J��L�}N�-��s�yv�"
79,�L޷*K���GlJ��׼���D%�+SG�y��
79,n��F�ND�(�De��;ǡ-ӻx*����Ճ��
79,�5�O��H`>4�&����_��	��U7���}�1��>�A
79,���G
79,���A�3G
79,"�c�-�ռ%��N]��Z�v�6�x""_w�p]>�`�2��Nn@��e�ԍR�"
79,���no��o���΃d
79,"�qT7�Mb ���!��f��a|oO����{n����rL��p"""
79,�si#ch�d�C\�
79,�٩Q�>
79,w�X�V���w7��*��B��9�H�^G����΍��Q�5&��!�&�����U�a�wl�)�l�XH�t
79,���M�f��z�Td��l��EHp/��ȣ*�
79,�!������$�3IS���LP�H�S���
79,"��2�@��:""�jacjY`X;�Ĕ.��=TӧR��N����ӽ�l��� �a"
79,B�����
79,�D!�7 	���Xu�Ǫo�j�T���PӸ���t!�s�d=|a�MӘ��:��a���?u�͍o��U�����yL��
79,�KzTUK5pK7���e��
79,ޱ�fśdyT��w��$Y픸��*��g�8�r�9���F$���B~Ea�R�qiFE����uv���m�J�i�CN���K�ƊY1h�8!�FB�
79,",֦�a;z�v�7��^j�@��L{oV%B�W"
79,�7���� �EV:�g��~a>�'ae%�N�˭��	C�	䑭4w*
79,Y�=[��TpM�Տ�(�u�Eۿ��e�t��_q<�Fm�7n
79,�ˈ�6jW|�TA��:U(Xu�
79,U5�j�w?���
79,v�b֝�
79,"Lu�)N>���W�D<������!׫ȉכm�^[y��8A�|�0m��,����,V-�2���=���بj�r���y�Z!��Ѹ�?{B0Mr�^��G�E4}	ī��""���ۃ��n|+��"
79,���7x����
79,��L;��WZ���(�G��U'
79,P�������Q�AT#Qf���4�(wK�C��U��Ҧ����B���[X��Ks>eA��^%)��*��w�tV�1���7;
79,)'gi��X���:5Oݛ�
79,1Qv�c�HcH6m�-RO׮é|{yɢ��
79,��@|�1�l0&
79,�3�����k9�b�:�����zb^̇��S���5УU�
79,��WMW���p��jzWM_���p�
79,"�jW�P�f�pՌ�jFW�X�f������V""⚷���L{֬��w[���k��S0XQY���mf^�e�z�8r���/}"
79,"��U����D���G$��Qj^��S�Uz��h=����K0dV5�I��j�����,��.ȓtw���o���k����"
79,�AL�
79,"B5�M""�{sa��5Rb�4z��"
79,-���GP���&V�je�W�{{lN^�C���-�k
79,��
79,2����5�Fel��-�$A�e��V@+��J�b�V�=:�v���:8h�33�;҈�������RTm�Rb���
79,"_�))��ϻz��Gv�k]q�[�D�W��y��=���ި7ݚ�>%��Z����T��[�TmaF3�S�^���mb�䉐�ʸ,�x�}�_��#��'�"
79,"�l�Q""*�v�ed H�>�;�E���"
79,Y[�����g%���L���:F����z��ӓ�
79,�f��1;_��v�S���T�Ʈ�c�(��>
79,"x�~}�ś�7 ���ѕZw,�]M�4�c�BV���t��D=�wg� ���Y�;t���"
79,"�Ĵ�;����N��h���N��:�j�#���׮`̻p��XЈ��R"""
79,8I*Z�Ԩ��
79,�gw����!����c&S�n�
79,�O��J��F��v��V�TShm$�BL�W�r@�oT1�� ��R��	�;�\��$gs�FI�.�EB��a��R�GGv�
79,�P�6:�(����Q[��0
79,N$����&B����e#�j���+�X���X|b�%�Y�%�l
79,"\B�KY�`U""ʭpP�eU�xP��e��e�n 1�Z��Pa�\�q@׌�l��.m�����h3�b���f;�f,�W#��t��d�$ztȥۥ�����{@N���<�,�b�p�s���We��p�~�.�)^�դd�6&��j��w�6 �Q��0$��x���Ө8',<""�c�""�*wtKbu�c�g# ��g8�l��hl�b���D��""��/"
79,9�9aI4R�q�LM�%jQꥣi�Y
79,"��B�B����19]��m1��# �L{��""EU���O�E "
79,k�F�6���ʆ��)Od�2�*��	 �7g�u�8	�Jf�2G��կI��gq�
79,]�:&��d�
79,"��� �j)�~%����ɧ�$���r�,�&��W<����*7υ^ʓ�ԡ�5��T��/!�"
79,�:��$��\��a��'f5�zc�R���w�Ӑ��(�g�:��B*
79,"�}gn�,�ybѻ�+;/hہ�_����"
79,��W�_ʪ�\�ennˆI^
79,"����^&^�""=�[�-7z+E�>�u� K��PX�5?�sU���N���wx�m���P1�O�y"
79,*iB\�b_�H
79,"�ݗ���""�S�]�,�-��Er��!@�S"
79,4��Q�&�;�!�K��w����t��e��OA�� �ہ�YBQ�@��F0�TϬje�aS�nEkH���R�7�()�Z�0����P{Ҙ�I��K(k��ƠK*8����H@w�
79,"ibSD�'@3�]�� M�5�.ʑW�=�1V�f��z<�eߣ6�����`��6�=1)'��M���!!��a�*�,��z�Bc0���aFlΦ*/M�<�a�=��z���"
79,V�W�1�ø�II�pǚ&��O���PW'
79,"�i��Ձe""��<�g�qk�&adΑщ:3p����*U=H"
79,�ˏ��I�6���\�=J�B�
79,H���~(��H�
79,�#q�ޟ��+�=���^(%��9/��H��F���
79,a阑�1�Aʜ/���^
79,K-?J]�s�ɩ��>K/���N�	���BȽ
79,���>(
79,���ƸJ교]�CH<�;�>��Av9�
79,c�a��
79,��I����>��@v�!��.�{�3Y����D�^������~��i.���BYi~�CA
79,"�#��̸�-�{!��l5�a^^e�,��`���l��"
79,"_1�½������{�,ld�0�Σ)b���-�{�cfiU�Ŷ��"
79,�E��4S�|aסhJ+��v
79,$�jW<�4���
79,��Vy���vO�%샞���-��������I�ᠱ��K��0_Z�aep��	s�ɤ�hTV��
79,�@W)��1w����?��7
79,�P]e�
79,�bg
79,*-��	S�h�
79,=c�i�
79,$QΤ���
79,Ǉ*M=��x����1N��
79,�P0�<���`�Eg���\+���U	&�����
79,"'��+`���6��E�i_�4,A�*� �e%ؑ��"
79,����S�
79,@��y�HYKNᝒFBxC�&9���;��
79,��_/썱��/-%i6
79,[Q��.�(ų�$Ǳ$�-@s���i
79,"�;����7��� ���l��8Yn�Z���,��pC]���<�y�Lg�2�N�ƛ����/*�`���X̠*@x�<{�Qx�������ӳu���N�04�""��ދ�&�"
79,R���*k�`�j�0����
79,xH�f��_��Q�uM6�
79,"f!�u�5�0�Dp����4)h�$���M���)�""���)��^W�"
79,h��U�z��egX��C:CV�ڝ���:á�u�B�:#{�+W�e���Ι͓�mWkѧ!>�˰�
79,"�D:Zo�cVY^�W#}���""-X�%8h�Y��sn{�/^�ad;���9�|흨6~7�nPϜ�b�x��F<0Bq~�	����.�|Ms�a���Zj��hV�f�,�Ozw��0ELa_(��2""I���s��۳v*��0��a�V��;E�/kc�v�"
79,�h�%X�����M��b��7�~����7�
79,8|AZ@�t��0�����Q��ݨ۔��l�
79,",P�t�-ۋ����~g6q���G���""��E�R�z}׊fA0"
79,F�Gaz�TFk~�+�«;Sj̂��z4�aG�  f��@�(X���H/�e�k�+<y��w��x ��v��t&c���{'
79,��(㏆���`��`.�.��|��oXX�p�Xy�-�E�
79,�Kq
79,B&�
79,h��jH�j��jJ�*�[��'����
79,�k��)��5i���b8�^fsD<�-���㤏W(�
79,"?R|��g5B�d�""�5����>���g��J������	z����{�s[r"
79,"dT) w�-�3�,�N�����.�N������~~k(�����w��wSV�J,�(�9^��%_4GW��4u���>���.�*��.��BAA��E@��;�O)z8�w""���.�SPT�/^���c"
79,I6~�V%d��\>�]�y9K��t��<�.�����B$렴��/��)i����g
79,"�i��,�a��=����%J=�%�ɵ��D��-IZ���U��qW���""6�|�v)0E%A��+�dXi)p����Aq!,���2k�����RW%�0���^�ѥ�8Z1qIc��l)eTo����ig��,��{A�@�Ƭ)�6y�F@�E��їֲ5hAޭ1{W"
79,d���/�^�0�������n�NBO
79,"ϐ�,�v������ǈ�g@H/V$�ad������D�������^�e��7���"
79,�߯@�J���E�h
79,"25T�s�""Y�:�7M�t�T�[&`�Z� �c#�v��s��^鲎rP�^����7�`�1�.� L���"
79,L{��`A����&�Fn!�i��	�?���
79,�U���O[
79,�g��'���տm�f�fZ�S8�9X�����d���~�K�47
79,��k4�I/a�Ot[
79,�ߘ`Q�`��	o9�!�O��gM/�J؀��	�3��W
79,",	�7"
79,�?R�������/~�ʜt� �B��*$IF�0�^�'֟��\�7L�?Qٹ(`蠱1DK47���UX
79,"�1�R���Ut�L�QlY��%��	x��4F]�hSK4~a��v��D�&�b)c��ch���%�:HCk����ph��*��?T�ֆ���@E&9���B-����E��,�a@���$/̓���C����"
79,CFJL򃷝ie~�
79,[�ɄD�P7C�g-�n*���V�����NuЋB �]���l-�Ơ�����V�<S�?����֟��
79,/��#S?���(����)������3�L�
79,�����%s*�V�A���ߛ�7�ʘַ�f��Ĉ����
79,.��)�a���e��������o�����ڀ�r��Ew��L���D�3U��5�41u���iM��*T
79,͓�0a�w� t��k
79,����	vb��OT:P4�
79,"h��	v@�,�譝$��������7L)u1�͂�Ȁ����"
79,+o���W�a�
79,"�,�Y"
79,���ߏ�~6�L�����-��:}i�������M\�)Hd}7��_
79,஍�˿g)�	�&�/��
79,�x�5�d�
79,��g��K`>s�������a)u�c�0����e
79,b���P-r��U}���20~�L����
79,�tQ�����:z�RC���R�zT
79,�WK�4Z%&���R�*��
79,�i(��W ��%z|ua���8[řa~��Yh�����V
79,C'���G��8g7��h�[��;���蠆��
79,"*I]a[��q�^�l+���^�b�����""�di��"
79,"L���>�k#4Y *�_��Z�Pǿ� �Q�4�;3���fC��kc��� �s#�""Z��O���������<�Ҝ�l��csr	��o�١�y�^��h"
79,���1~�
79,c�enZ�f�M�n�l���pC+~l����C#<&������d������0�9n���EE�^0�scr��#4]4f�e���ӿk.h���g
79,X\�%��I:��O�E�4~�
79,0|�
79,"��,V���a�"
79,���������N�
79,"�o���XJ,ͳ�1���B��f��"
79,^o�O���@��xi6	��P�+�����0�W�I��6�
79,"����n�o��+>�,�܀�U"
79,"P`��7�""B`-#�*�_M�t���.��_�ԉo�S"
79,4�u`��/�n�$�7fh��7�B����2�f�Ҩ��R���e`C
79,[͒� �U(ln��7>���Z
79,"4�%5 �7F�y�\F&�7��o3_���""�����`��[�\�Fp"
79,���/uh��\Qp�����'?���̰Rf�c�S��bs
79,"w�#��,�)�����o pf���V��g�}��N̥j8�_��(m"
79,�����*J�=�]
79,�T��|i>S���6�bϨ;W�a�`�I{�aѷ`
79,���͐���b����Y�����<<���~���2����+��w��;���^gt��
79,�y����3?��v�csi��������z4
79,���Ɇ�
79,��	�V�G~�OH�	=�
79,M��͕�����Z��k�{������%��־��� )&3�q�ty�����z�hjMlӤ�����W��Tf)Żcy[��
79,"PtRx��WKL]"""
79,"( ,�}��&,GI,�sW��'ZK���I�������Dd��^��H.<�"
79,"�r�*F�,K���xeV#�]�Hɝ�%#�r���H"
79,��ZM����L&e�)9A�
79,!�Py�owۛ��請�
79,q4��g�[�!<^�r[���+�+��J��X��D0���G���i��x�'�N��2�lh]�{�n���W`�Y�� U
79,"�d��T�Ե""i�]�u��e~EL��iZ0ӌ��7�N����i,�����$��""���尋�m_DqO�}f��l�.�E��s��H7�j��R�t���"
79,&��
79,�<���O�E�oj�E��L�t��	� T�:��MB�����(�ˮJʚ�*עW���J:(���;�2��8cko����s��h�zW*;��)�b��/�2�v��'��-��uI.�!���5a�7!�1�J{��ٵ\R����ӛp��*��]�_�!�� ����*µO̦��ڛ��pA?
79,"r�����[`��""v\N"
79,J��d�넦�6�&�E!N
79,"�!�������:aǏ����X�Aq&�""��g�cy�����B�����"
79,�HǪp��0M�p�����N*9���GGU�M��No�d
79,�g������S�lh��eY#�t�Y��	�=���E�5��\R��٢��?��E^4H:�F+Z�
79,"|��m����V��� �]W�����b���@M��dK�-�JGQb��#���\_�yF8�""�0ͻ�z�p��4�H��"
79,�yw_͕�=P�Ժ�j��q��@�;R��Լ�A�����+��S�@�t�`�W]�ԡpwWt�K��Y��%�M�����
79,�֋�v��^���*qǥC/$��������G��ybs舔�=�ɶ�`V�|y�!;��g՜�
79,m�zS��-�tO�n�q��&g/2@�
79,�3N@�3-��in
79,"an8�=�""a��{��"
79,Ґ6ۻ7�!⏘�_��<�p>	v���%˓|m���F%�-mm��%��i�&�e�1}zf�N��i�/�9���%&o�ɸ�����؟�_o�	��ۖ�&!���|���X+Ғ���P
79,.���K뛅K��lA����ʠ_{�Fp}��l�
79,r����3v��-0cw<��#7T�Q�UvG�̱#���H� �
79,�����T�8l�{��GЫθ�
79,zx��6;3�}[&T[��BR���>f�[��ћ�
79,"|c�ܛ@�|U""M_�<��#�=Ma���|,o{���Yg'�B�p�^bO���?Z[�<���B����\+�n�.9���C3-��|څ�V��°����&��m��dI�-���$ �|Q�)�"
79,tW�	��K�Pl�k
79,[0}�
79,@i��u+Z���z[WA�ӥ�(�٣[�
79,�����%[�Z_��S �R�8���I���;�i!TM��a��4b�d������/E T�q�sY^�$�5K�Ĵ��}��ʻ[xK:�<
79,����mW@�U�
79,����b��
79,+7����yz÷{AwX'�	�l���t��1�̝폻�@Yk0�X����^4א���w	���c�x��4Mʳm�>�#��z��D��c��G� )�!<΢e4�[�X�q
79,[��y�(�&�$�Jw�
79,2�t�4�LL��	�A
79,l#���
79,Ƌ�Ulo
79,���ܐ�tj�U
79,y�-�\�t��ur
79,��t%�'޿��A��=J�)�H�:��ȗ姸��l�
79,G���c9�U5������Q������U��)�5�
79,"M#eS�M�Kg���xeƒ�`'X3 ��D8��C"""
79,����l�S��)EL�B%�2n+����%
79,�0�</Dw/Do/D/Ġ���H$��V#�)����8�/���t.��ɵ��!���I��N��{oxi{^�N���Ɠe�$h�)�O�������`����Il��Ԏ�
79,����R ��t&��B��\�:��-n&�F�`8���]ܔXc����Oo^�o�ڭ`
79,q3�	H
79,��y=��X�07+@t�ba$.��h
79,�#�*Ȑ��(�b�Y
79,�v�^�!����i@��p�x��d&��(�rH%Yަ�
79,$�1�P��������_�^GY��|���)��B��&���
79,"�j�2""�Ň�`�&5i��*�i=D�냆0j�n��)�xqq�����ēC���G��GTZE$�(t��"
79,"HeBn�ʺo�Ӧ��(�L3��0NiAY�E;�""��#��"
79,E?�e
79,"UP���$8�Z���p�����;s�*~0]c��qJ��C|<�&/�$P���ߍAS��S�����Ϩ�T��{�B��ڤ��ԩK�Np�@K3fXV��l�e�,4"
79,1��Q�o��
79,")U9l=-�Rӊ��V���e@��i��W��""%��$�2��D"
79,��4��PuZ]��x���*W��0(cw�'J
79,s'j9Kc�Y0�
79,"�t=��+5�z�}Ca�f���ʊ̣ ���I9ݣ�b;�v�2�}�MN�h�`�w-�W�+�U_�/��/""d""�慕��"
79,g�oC.�J
79,��S�`�
79,����*���;>{�s��Պ�b!;�
79,"#���,N3��/CG������q�n���xP�S��W|D�H�Q��F|�r=��k�cSsV/�K�h{�IUPA�X$�D5�����m'Q��Ε]M	5���ޟ-N>!Lo9ML��^3J	�"
79,�V��i�4�V]�C�D\�$����@��꒽
79,�0�^l���>j��>�c���Nq�Iz�p�YQ��f�����oU�x	t��7xT��xd���k�lu�k`���ʠ�u
79,"�7]�F��9���nQ�O�,2�@�������*B�&2���g�c�#��O.y�&QİW��)B�ADYYo����=+�Pڿ��EY���$���~��B�Z""M�$�(���!�@y)t_ySt]y.v[yŻ�61��K��^qIW�`"
79,Q�%+���&m����*�e�U 
79,���*hY#ҵ�Ӹ����.5�I��Ub&cFJ`
79,U�1�C{C�#��I���PbU�X
79,�r���/Yw���ҧ�DN�����lU�CUD��f����%i8M|f S�
79,��/|�L���E��P�-3���pw�V]nR��1e��_��B�'�v?��Lsg�@B��X���{�)墴�]2�%��؈3��ت��
79,�.u���.M�e�W2�A�_�d[Y�޼�%�
79,y�:~g�+�r��-
79,�[�P��aNh�d�Q&�7�9�t�xcm(G�zM|dfi0T�q�
79,ƸEJß7ε�Fb&0i��p)ֻ��\c��7�7���X� 5��͖<^���>
79,e����+؏r�=*IB��A�
79,"�P�3�=�s, "
79,L�s
79,"i�f��D{���b+���""��"
79,"�m��2��h��a\�7���mM�\S����W#匘� EE6��V,r�M� �bE��"
79,[�û�_�ތ`i�
79,�@WI�C����ӕ�%
79,H>�5�f�)�*NAV�-���F��2�{+5������e��oEAYK��8Nus�	�*�[N��{�/��(��{����^
79,�'䡭H����ݾܞ�g����wTډJO�h&�oihE��%��dF����/'���a5ueO�
79,��*�����/2�a/Bѝew[�*�`rŪ�B0)|e�t�DZw=�G��W�
79,",βf\"
79,"l9rɅ���SK�]�Zw���Q���$�o�S�CU��,a�Jٯ5k�F����"
79,��1��e�(����ǃֶ�1�g�0����b{�e��]�o��(PbX�կؽ�j
79,��K7Uׯ�2
79,���ͱޮ^o߮�%Ow�
79,gsψ�~[z�
79,p��^(���]3mN�י�'mPm�m�I#�d�R��zpk��nC����v�=�����7[Gg[
79,�TY�N���@E�
79,�yH�F�=�Uj������������
79,�)��r��ߔ�9N3(>2F=9�sO�Gi-a
79,"�����4�wb�,x�X5�Giq5}�����"
79,}��M��	Cr+yX��܄�ɋ�M6�A�v�j2n
79,޶�3�X�z��Ė�x�RIb�i�)�H��ɯH]�
79,9�<:gj�
79,N��=0F�n`֒�2���
79,�=�\c�4e�T�	�'���$�#
79,���
79,;R��a!�r9�N;ʧ���T}�L0�Ql���9#B��>�2��(&�זp�¶%	��������sc�j(�{�֩��<�Gg�dc�vWA
79,p=�wL�G��~e�!Q�Z�=J(-�#*�H��>
79,"a-�ϟ9�*,;-z��XG""���*��t�FJ�"
79,"�!J�!�tzMIO3r��?6A~��@�^N�,97��-4�GjH)�&'"
79,"�,�"
79,6a-e�֑F_n�4
79,��>�s]+��uX�7`�����i⣧
79,�b:
79,",l��%I졞ӕ"
79,d=0���'l���o�x�
79,y��PR)�I��+��	������)m{
79,LH`�{��'�7�M��6��8���2$�2J��:勴|f�\u#H�7V�ډJ��]�ř��}����Yu	Pr���(�]�1
79,"�sc�	񨃸�ķ��W,"
79,�~C�
79,�tmP�Z.lm$G/���K
79,Pd�a�
79,")L�$�dF��AHJs=��x��/��321EF""��H�A6;zգ�j�N��"
79,I~m�@�y�ȌH�wk}ы)y����
79,������
79,t]uh�]SFGp?�f�3�yM��-E	P�_�s���<��kZS����6zp;U
79,�f� �
79,�Nbc��k30�6��;m�*|#���Lq��Xj9	�S3
79,�u!�M^tCZ!C
79,!C� C� Cj )Ўi�x
79,���fg��k|�x���@�Q4�M�d�hK�3���� x%\�����y��� �p�|n
79,)�Ǒs��U�e�����sY��8T�Ė�C(3к�9懛���{jʫe=�Sop���)��/�M)�ݢ�dhоY0�Ļ׋���+�f�]ח�^n�}i��7ٔ3Չ'ͼj�JɅR���8��`�8m��i3L�$����_(PU?B�G��ja
79,"+��ې�]���Rʜ�^j1�\O(�st��E,�C6�ե!n��1�ȝja�k*�WI2�3��lMA�3㳸"
79,���]n���*>�j(+
79,}���
79,\5�A��wg� �o����׫���h]x���u�H#����$:z?�([���*>V
79,h8��=�\��ޝ�u�$/�ޏ޾y�k>Z
79,"h@z�=]��Л��PS�,Y<��7�d�HJ)z�)�!��G� "
79,�&�HR>��
79,�9�[��aZ$�F4�V4Q9��I6�
79,(v�5�{�(N��=iֹ^�2�!I�3g�(���^���i�yt��\
79,"ߢh/��ͥ������F��[A�""7@�I��oRg>�����h�!J��,��:�؁d��ܤ�U"
79,��9mz鏓]��6f���9��jL����HZ�J���h���y&��
79,�U�檝÷\7�i�:�1�-�zC�{���W�����
79,"�n�a	�߹t�ʛb�����f��,O�d}I&��dޒ^�xm�O�&FR(�޹,4��I�"
79,����)�s5�P�k����\}#ҿ?��f#�-�5
79,"�*Xw꽵����o!y*�֖NM5堜-Q>'o��%-�!e1�A�7��˧),`����Vmq3�߬"
79,6;L�����oè������C��D���3B�`�NX�:�zF
79,"�x��4�ERd��+z�d������t� �jQK�j����g���S���k�o���^N��\��\w����^���,?|m#�#;	��W���˷��$f����SXa�ǯ�'�J|�������΂�܆b�K|(-"
79,sXz*��Ґ����M�zȖ��&i��2LI*�vN�
79,"��nӔlbN�%F��5��x9�6b�$��B��` �ʥ`ʺ U�h�5�,8"
79,�Z�wJ+
79,�Gs'V��J�a��҄��ᑣ�=v�]	��
79,�X���!���H�l�#�:�
79,|�P�~	���<�>}�N�}�Q��)-R�t!��
79,�銒x�#����$�%F��ִ}^6�9��W�
79,H���5�i�\���]V�
79,�7����f%�����}}��ψ��4����j����(���Q���3x�����u�z�{��>z�_�>}��G��u��� �
79,�P�?���HSi!�ߧ�io��Wqt�N򊾀�����_���w_��B�6`���~��o ���>���؇+���%{0f�]�����.)��܎O��Wp��\�S�
79,��u�K����
79,"Y�O�i���Ɉ�.v�	I��g��F�!�j�b�wH����G����?��� .���""��#F)8�[� d��a�CI��Z�r)z��*51*D�P"
79,fK	�����4[K>ţ�U
79,X�Fo�m��}�׷8��j55@��#��3td��L�.�����r3�^҄���m%��� u�4 ��GѴ3�Ga���
79,"���""r�Ħ�""f�y�Q�w��G�"
79,lǏT��.���[�����'���숈�z��=X�dߏfC�>�
79,��ȭu��a�'~Ә-Bb	��8�x�z�F�䋘�#sY�i����yo ן��'xh�3���M
79,���J��s�r�%Z�A䇅3��
79,�7J
79,���8N��l�7]x���/�UT�zH:2$�]{I����j�<���aw��/�� <%�
79,�2�;@��I{0@B��GĚy�iw���8��
79,�Qܖ͆U'�4LZ��}
79,"""��d�A��Gf@;��aM�K�'N���/˃4/Sn��4��"
79,"Vb�e,�q#"
79,��7�=��0
79,bN�漆
79,E2�'P�0�1Upd
79,��x`B�]/�4:�W�Dk
79,�	Z��
79,��fsW
79,�ׅ�®�vg��j2��Z
79,"���5�T�M�d���U!��Q݉8I�$�rV(G�r��7�}:d���G�bQ��1��d	""�"
79,N&T��o�L��u)hSі0k�M����8[�rS�Zʀ�zkka#�� Fp��+�BS�P]�Wq�b��Z�
79,p��v��-�	�~=��Qc��U���N5���
79,p���ay2���+��t>�Z��
79,��j�[.cm�X�BV��m�Y
79,���0 ��\Օ������{�Q[&ܣ����
79,��ɏ{Tp�T�Ge��5ǫ���O�w�K�
79,�C��}��v�Ֆl��i���& �-|9��;
79,w}�v8�r�p�5���� ���~����՗[w��.���t���#�
79,"$��ڑ�r�5X�ڇ<�k��[A$��""�ڢ�>�u8�(""<�G���;��Y ܧ}�TT[B�D�x�����"
79,�>r����ڊu
79,ۏ޴f&�A��t�f�h����}�aDh\���*���O0
79,"��J""+����W�1U�_��^"
79,"���M�'��~Ҡ�b�""Z� �S�ϲ�T#c2T���5��V�qГlkn�"
79,���\Z���(�a�
79,"���3qM�Σ��bt���e�_����,#��b�=�eD2��ݔf��Dx���Ř���"
79,�.*0�D_���Vn����#V��bR��RL�$T?Vr�1l�m���]�y��&Q��x���d�)���+wӊ+4[�/��k�Z��
79,"��-��\p""p?O��4r����8M�vq���xv�2�)9-i|&�^t�3�C��V�����-�r����"
79,�N�����23����0ZТ
79,r����K$i��4�I�ԃ��kս
79,���y���-��T�<]�2i
79,����?juF]�fEH1��lY�T�{
79,��\��S� ����f�G-��W4b��
79,�;X�Y��n�r��Ș��((�wv�)y#�[�ӌۊ��m:4�M�.Y��<�������u��h��C�
79,"K�,La��*I)U��3�Y�vт�Tv/ѥ[��\�"
79,2WK���/��z]�\;��LxU��.��]P
79,"�N����0""H�֍�S�"
79,"8�~����GM+�,cJ""6�zDʵU�V���@1LI7�D=PACkײ�ZL�f�v�Y�&Ŭ;l��w��"
79,��h
79,Y=e��k2��-!�
79,�9�ޮx1'�h*�����(�T�mS�I
79,�O�Y�Cl��hbC��1k�n��Y����&
79,x~���X!���Y_U�?Q$0l�*��aAlͺ�HX�x�qasZy�/��֞e�w';9	��C�|�e����J�G�
79,��ޙӠj<��M�b����hR������Cq�c$�}�n�$�}�-Q�]ˢV�$�S��B�(D�0���|[|'�_R�X݁�!9�-?�Hǔ�B_J)�
79,In����A�ɴ
79,"e� ^�.*U�۵��[���,Z��"
79,�r'��Ҝ��C-�U�6���L6�1��+��Ζ9��L��)&­`
79,�<Cc��4
79,`$^ 53
79,CtB��v���4ڎrp�d^k���$f��L����M?g����ռ�S��B�x���/Eei�
79,�(��
79,�kVk�̦_^A>-�����{�2��Y *
79,�/G�z�
79,� ���x3��]�&���D�^W��ɞ-�kGN`L[XI0��X��P��Bl��
79,��Te�\gă��#2�
79,�X�8B=9�+�gi�=1�5
79,"Q3v�,�{��G#"
79,��o@G�1���ARLq��%A�I<
79,�@�
79,"ώdh���40>,S`��`�:"
79,�]GO�
79,�u��}�v��c��V�vBd�+��{Baj%�XBS��H��0U�[�n����
79,�`8�e1�r
79,��%	�_�;!y-�퍙m��A���a���2a�l�/�t�>�δ߁ٻ�_�ϣU������{������k�˺��6
79,'��DmV�Y�㮇�'4���&=�1P
79,�����N�_M
79,$�2=p{!�	��oH
79,g�z
79,(��4@��(�
79,vI���F|��'ʽ j_x�)�U��3I�Kil)�վNO���	��`TU�宕�q� �w_?�
79,���r���t6��v��5
79,�n�2Rz~~�r
79,"�`��/)�@�/��2�',��6�GW����t""`�p�"
79,�<�Տ��aB�wR
79,��v<
79,w;]��e��$=��S)��Pzء�}�Ʉ^��4q���\��;#;}�T<��.�o��zZm*�Y��
79,��cr����K&J�Ҩ=�79��ę�������(�q/h迪��OdpI��XX��#��҆G�
79,ګ�6B��I@����$�J�cO���/�$p
79,"Q{ �p�B8r�`ofd���+��R�.�ŊO�pqAh""�VE$(��R�aޗ��"
79,��X�;m
79,R�I#R�2{8Io����S�~e���&�a�
79,��dz���52)s�0�k8�:
79,����sR��;O�`��:�ܵ�i.�܀EO�~�Xº�����8�rI��i��GA���
79,H�d����:x�	���Et�β��8�@��c��u��|$>��\�S�w�Y�x��{�5�)^Htcq��WW�DR�
79,")����>�f�S���sۤ�f}4�L�8k��b�]b�-.)�v���.I�fv�Ϙ3���70hAmR[4���E��`�M�!q�lh�nE��""�U��-9����Q�"
79,*���U�ZlѾ1��ai�y�+���7b
79,�W��O��v�
79,51�t��
79,�����6��ٍ�i#v���(^�C���Dz+֤] g�1�a��*�;{���X)}&���<���9I��^�M�10qψ�1�����xjܐ���)�QL�$yv�ި�`��_��
79,xh_�܌]�
79,"�^�/����ʫ�c&�k�@����Z.d{P��H&�""�L�&�ɥ���l[��D��"
79,"�!(�[���d��ܩ�*SK��|~H��(����*tI��,���C%/xP��R�Gy��Kf�Dn&6T�20MN:?`47�H���,N1h�/�/o��?��/�0�l� ���ҁѰ;��$tg��_(Q1��m|I���LpL�ߒfj-n�#�=�f8t�x�Q'7)f�O��мU+M��WO�rԗ��\\t,z�׫(��x�Ri�T�a�f�2�B���%��/2yCMy5ְlp��"
79,"�	�Hx�c<���Z�?�^�""����	���*}^)���T�,�ӷ�:إj9/���+ѵ�"
79,c�HS>�i��.C�$�|I�1}�հ�݌�$�E��Y�kd�ss�k/k�}
79,�:��m�쐢[|4��)fhm�JOS�k�U�ʕ�Ik�+8��[}s�k򆚈��t|
79,"���X�fg�v��N�b�Vس0�N�0T�4Q,L�B�pg�B�*"
79,u�TrP(wG��!ۜR!G�7e:�WC�
79,"+�b���\K�^""Ǖ�i�vgq�Œ^B_�|���,~�*�~��~���w������=�k_n�?���O���s�g�6"
79,�{1<����l:�<��͊
79,��i��L&~g4
79,��^�3�ƽ�t��;]r�0�+L�$�^4ogW�c
79,w�	;
79,{#�\���ւb^L~{]dxG��tk/
79,G�M�y�v�
79,"iX:B��c#���7�n��e��g[H8tn*ig�UOG��L@,���״|�M#�ڤ�K�D���c�\ϖ���f�v��މ�����:��!d�t�RgK""�˺U�"
79,�n�{���pb
79,",}tW�z���;�^`�&s�,�;��"
79,"z�,ظ`��3@*OV�ķ�R�"
79,"xb��,�F�x�MV���Q����at�7�����/�&D�}��R�QÒ"
79,ʞS>�_�j��'����+�J;P
79,�u�`:��{�I@�%�Jv��@���a�/ng�
79,�J{
79,]S�4�A����
79,/��:�}�4C���N����v4�VNݮ^ǡ��}���A#+<�@|E!S_
79,`.�:�h�Bw�t+@1�
79,"������aQ��$�7��*""|F�T�Ř�RvEfl�[���#ُ(��"
79,",,���4�|�`�n��m�e����JJ�݊�#��{�p�72��d�v����ɥ�|�����à�"
79,",��-���яZ*Zc�"
79,���9~Gu��Gc!���w��YI9�S�e��l<�]�t�X�z��<����C�1Ӑ����<2{H�s�l� ̤X)
79,����
79,"���fXz��*Fc�!8��!��dS�&����}0N3~�(��/f$�,��d/Eo�?2cps�K"
79,)�xz��
79,&�:�^���7�%�t���s��dI��S�H_�
79,kA��*W�S}��`���V:��d�W�*�eT)m�8gṴK2��G�H�����G�9�[�]Q�a�.��Â�N�
79,�y��� �x���@8�����JvV���NH��Q��+�Az�9�aq�7Y�� ��F�ۈ�C�:�
79,T�ޏ�m�eʙh�Ê�-'c�~G��&˪�^֚�@K��*[m����H��%�/�Ⱦ��� ٓ�
79,����7�ٿ��p����Y<�t$�-hsE�^J
79,�=��6:U6��ĳ�:n���m�-qB%�t0컊5D����#rl��
79,�z�\�
79,"�2��i��XϹK�By"")X�+J2b������F� Y��.���x�i"
79,g���w
79,"�a��D&�I���n�ȷj�$�����Uj�&=G�""����X�r �\&"
79,C.nɐU�7�XB
79,M4��V���v��U�/����N0<|SVa�:f��U
79,F	�����
79,vQ��5�9�ؘ���{����n���Fq���[3�2� z��l�����WSpO؜Ǖ^2���Y�6�
79,�_���\-�<3
79,�'8+�����!��e
79,"V֢_��F�0��oM�T��A=""�T�aJ�|F�����X��T���T� =���"
79,"*�,Yf�|BN�rF� �;�����<M��"
79,*o=�'����zl����78P~
79,c�Q
79,"��y��'I�� �����N""�? �풇��$�Xx��^�����2��p2��»�!�n�u�}V�t'�"
79,��b�DQ�L�Z�Z�h
79,H|��=KW�4L�
79,���O�#r��^���'��ھ�5�A��ȅ\����7%��L
79,"U��_�M�j��a�������$�� [�К�'�zՓ^�biTW&��lL�֖F�s����W 8(@9���=���${���>ｆXD\��N5�,\��Y�pƼ���_��$�P�����+bK����=�y�7	�^:�İP��Ӑ�+"
79,"a<��i;!G��q�7ʨYb��z�v""���v�"
79,�]t��rE�����(}e��x���g:���y=��-<�_1��%iH[(�ɢ�g8\
79,GyF~�������V�W:
79,LF.b�%��ar�m���1l��'��_NY1�6���5`JχW��Ģ���#�l���@�2��s���Z�H	*�b�lkD���XEMe�ڏH�
79,O��<7=�e��YY�.��q6�4)����
79,Oac�'�|_���}��r���
79,��1(���0q��SД`\.�㙿#�/|���/���Ңo
79,"��{~���-�n���x�1p����ɦiVv��ܒ� z� �Is��מ�s�s�{Cj-����i��""���g�1���:"
79,��b�D�����$N�]4h����H��ű���
79,"?U��{��d�k�X�,� ���n��`N��u������Ï�i<��kBb"
79,��`{伙?G�����{�p������t;�.����E�pN�54����6���/��/�QZI�^w��w&�
79,�c0����h
79,D�q�;���o;���x؝z�p<�@��]�?��zC/��'��a�E�Br�3'��(�6����f;Yn��=捊եX��b��jڌz�ig ��g��x����g�n�۟��6��/�����E�Ν�M�W��iw:
79,��}�o�����l8
79,MP
79,"����G�-�q� �""�z���:ް���݉�΂�h؍F��M6�3X��N���Y"
79,͋h��}&��RT���y�[J�ji�o���-���5���Y� [qwϷ�c*�
79,�#?����q�O`���`��q8��ix��%�`0A&�
79,��I��߆�
79,�Q8���6
79,"L��p��c?�����w���""��$�ld�<����x4��Qo<��)��?�u{�0�t�Ի"
79,�>�v����=o8
79,��l�����|ţ�a/'��lr|�Fٺ� ��E���k�Q�� �z�0����3
79,Ӊ7v��t8��¶���Ðڤ	F��4(3`Y�Q��f��+�^���'
79,`���dV�s��.s�*�݁�	�}�o��t����
79,�~'u��Y0�bG��-�=w�
79,8yGU{
79,"�Y��d~�u����ǣ�"";"
79,�*� ��zr���^�y���z`�NQ��v`A�a���I֛u:Q�[��~{��*�^��N8w�Ѭ?�z�0�Ͻ�l��WP�E�����I�Z�{�aϛE#��&�g0
79,v�Q�;
79,f�h\p��}8Ih�����x:�����{��
79,fA�;	;��`8�M;��i8�X��
79,2�Y��r�r�`������%�Lx�mF���_9}�y��Ϟ}��O?s�<�+4z��z��/��Q�7Kf΅�k`�������O??����$���|~���OI��O;����>?
79,"����种V�W�5�i�V�ػჂ�}�""��G�\��8�5`�1�����s���ΰρ%�����M1��߹�ҋA���6noρ��9�.��3(r�/F%��h"
79,�>D��6ۮ�Q����n�IZY+:�ϝ�I����
79,y�_���i���Y�V�O�jsv������&�����I��<��
79,�<ϓ�ֆy�>��$i���~ r
79,�Ƴ���ټ�LX��mQ�����Emr���h��@Y�
79,����MvA˯t/�	�x�0�'9��6�i��?��y�����IԞC�pP��A
79,�m��Vzqr�6�I�Z�jM�<��m�v�6/ݜ�
79,"�k������""�,q�,	��ήc$?@�m͍�2�7�XAD��|��S�=�u?��""o�����~�!}Ұ�M$��w^뜽&�"
79,g���P_Q$@����Ԗ������oF�g_��>Gf��ғ^Q� �<�'}�I�T�<'C��
79,����h��7�#Q����E_l'ւ�[V{����P�E
79,�3�a�
79,������$}���L&���4�D�g�r搋��S#�wp.|4��(�g��O�
79,�gq
79,�pywI.�=i�Y�h���'�3\Aab5����]�<�$
79,"�����u��9{ͻm������y�+�E#O6�V@2I_4���%���LG@t[̬��w�""��j�Ϣ�IЊ�""XAI�"
79,�S�����s�󼝥�
79,h���5����v�����
79,�sä�5���4Y��$�h\o�W��TNg�Vv���-�Q/Z�6��Ŏ�
79,z�*/���jm��� ����j�H��0|����8�qu9i|��Q��й��@�
79,",}�[*�4d9����xR.=�Y16A"
79,%�Ue�W���중L� �ܸ�t
79,���v�t�
79,k���[�7�<@��n�
79,"b��񩰰qy�].oOHc_���m��""���N�(f�~������q�G��������m$I��z�(vw��dP��TTA)�A��R�����3sg��"
79,)�`G%1��w� ��� ��
79,f���.�׳o�c�c��}�53�82H��̬����8x����Nnn�W���R
79,�F.9@�����J���.�����
79,�K�I�Ly���T9w�Q0��I* �D�c�EXA�/؇-����������(�9c)z��
79,�+ �-VA�����tm
79,��1u�8lG
79,.;*J-
79,�EZ�@�^�
79,���?�5X�]'��'���b9�H����E�x�k��g�.m߻�%��0R H���u��t�{�q�W+�֓����rL�+B�����{1�(�k��HH.��w �d�s��]|_���x<yd�J^���CL�J� 3�_�(Ԗ��$���
79,=��Һ���
79,�?m
79,"�O�j&�P�e} �s_��*,WO���5+i8�M����>h:�j�X�28""����$�`ݦ Q*鏛""�4VP���F�lB]Z��V(��/��"
79, ~���I�V��*�N
79,�IR�׃�a�9�wH<؇7b�5}	��=��C
79,�]�9*�i�^
79,���->���q�蜔|�Gh�y�l�~��]�F��=�6/R/$�_`��Ԅ�D�o��.�X
79,"�	:7�e 1�A��c�G�z%��Σ���Q����O����a ,xJa"
79,O�+�[�?����̝h��7���OSBA���
79,@}��1�> ��;��3'�0��� ��#y�@[���M
79,n(d�{���tK
79,"��""�g�+ץT"
79,@���4�SY�:�[����l�ںi!��
79,������.(;
79,Ј�Z�!fp��5��/ ��j����>�/�gr��ʣ�ڹKuA�5�S����x�`��=
79,-=�/\ �}~���]Z�
79,"�m""�'"
79,"ڱ?П�҂�@nF��i��i1�i�z:n��-Ϊ��m�ṉ�-����3�[�OG��3�V��,˨v�z�i=��j3A����?�_�c��s�/;e�����~��çm���'���e�"
79,a��ll��
79,��
79,"��""�4���nQ�s�F�z"
79,�H��6=e�7 ����A�T��
79,"ާ�B��-�����@��fS�m�����lb۲���x�$J J_�Z,��;=�4�hQC�����""�AH����Y�(��l9\t"
79,��h��C���
79,Q�B��r_
79,Pj�Ѷ�O~��~0�fO����ا�	�E��(�l��Q���o��tҷL���XP���<4J�(�
79,"���C�bE���NT�M.����Y���Q�DJ�6,�Z3��n�g��`�2��ۤ�"
79,K����?���nn�S��e�y
79,pS�U�gv�W�����Z�m�lQ\v���vD�tD50Ţ
79,RD] I��L4+|��Xٲ7��k1����!i��
79,"�B�c%��S,��kF��E�,T,�.�U�n�v8*M�������t�B84��{l� ���L-[#7(&�*-YWXS0��R%p� d�ŝ��M�ν"
79,"�gq6�2��E�%���z��,"
79,"`�����x{���G��)�HsX̙�,�K�;�"
79,"��%��4�p8*6��]�ܬ�{�j���9��n�k݆���0�9g9/����sf�Г"""
79,x�#Y
79,e����C����6E�@�|y� �z��
79,"��b.�(���7x7��""u��訳q�u��b�S!�o�;��""��W"
79,7�q��
79,"""i>߮5�F�0�֭�L����XU�(y�ޚ'y-���@��%��3ӝ W���+�i��4�A���ʓ%6��Djq��>�r�{� �>m���CDH`�=�޼Y!���󜇯�����>�����$I0ah-��̘3�N��EV�$8��%K���cֹ3w�Gk6D�F�|\�I�E^R�fa�9q�蕢ܠI���dY_>M��p���"
79,�/H��777�
79,���D@b��]��
79,������
79,Ld)�؋6ʉ�ɲx �1
79,"�׻��p�F��_�˓��1��8=�z���2}J ���F 莖,��"
79,"�x{IYB5��fvN�W���""�K�V��.�玧���ʭ?(Q�[�t�6�*�Lq@��"
79,���çl���>/qq�1S �~c;g�
79,��;������K��
79,���gh{&�h��ie�):
79,� ��J�I�`F ���·�.^t��{�;b�9^��r���ڲ=�����[t�� ���Վ�$O�����
79,`��ׄ5J;
79,�K��~TU���opo
79,9��G�����X���
79,X��� 9���;G?Y�%�c�������XPHفɸN3�E>v~��	��5�f��t��9!��0��°���m�6a�i���r��i��cb���`�>��b�%m��X
79,��E�������N1��ƞ���Th�G=���T��h?Gw[�
79,"?śM�X��x:�#�>5,�r�3�u�W��@�}�m:��zsd1�<��"
79,���
79,��O�l���W`�ݹu�!t��s��d�	���� �
79,��3 ��S	��y��@
79,���.8���BY/n�
79,{�W�Y���Ą����h(���3z'�Q�{�sS�y�!�^qg
79,}a�~܌�
79,W��7�+�e��g��Rd�W֢�)��1��?W�2+v(�a���.1�]&
79,p�����u
79,"�PJU�-ܠ)D�Qr�=��뮐zw""���6~�%�WG�e�og��le����6�¾�f>�6l,���\#�_����}"
79,��p�os_��)r�<��
79,��U��L�X
79,"��P\ lJ[PF6F�GTl���""�!�"
79,�$^D��̋޽����8�S	�F�@�y���������{?p�
79,�ݔ
79,�{`���y��}$g<��>�j0�y�:��x(�C;����Kސ��0�߁+� �o��Nz���4
79,�b�U������a���v.A����$+���K
79,�ldc7y�]
79,��͈%�c污<:�V��Q~���t���c��1���~��
79,"ƺ�*�r����Jo��f�2jLk6��i,�!�"
79,3�|�!��(���ўpU���g~�)Gܜ	�
79,"�-t���(��h����dS�ͨ�7d�W�,�A�^���ӿe��Sh��Sz�A����:ڋ��K7���l���d��w�էM�������}ڦ��7h����+�ɽr�0���i3z0���"
79,�o1��ԫ�p�rv`ʙshg������(�j�.I����e�7e�WK7q˙��m�
79,�hhbS67�QC{)z���}�0����?��]+��򚇧����+����.�
79,�����]��o�)�� \��7n��A�B
79,R3j�/���c��I�Z&>W��
79,�Q�i{b��O������͍0��W0RG
79,�|���Et2c�p��/���k
79,�xP�������i
79,��͌pgc��9��A�
79,�H��v�Rn�b
79,�BAB����xƊ z�	n 2|ƙ
79,�:X��Ƅ�]3T���o�a_����ɝNA��p��a�
79,����߲r��~�:�Nz�0l�K�$g&��JNPDhG@�F?��Ď��
79,"�dF�h�zމO?y�bݻ@l��N<��%ht���lbW�����/.������Wδ]2�#z��/ �Bq�=d/�S/�u�A[y��gp�F�Ǩ�[��!���""�l�i"
79,��I
79,6���h��_A7���RX�؃JW��x�E�~v�P���N�����Nh�~����(�l����PѮ�p戴��A)A�׭�
79,)5�۸�vx �w
79,���S_�K��U�q�G
79,.Q`�-Dn~�'�J�$<6�Ȏ��?��D9�>
79,cB�^���及e�c[*���
79,���Ш�У�3�J
79,0�9�)�����h4
79,�U��O_�BL���W��~��Ul_x�s
79,�R����F@���8}R�B�=��(I}�-�g��ZG��'gL!�Kr
79,U�*��5T)�F��3���Ʀ��J
79,"��z���;��^�����""6b�J�}R��ٔF�Q,z Q��0���e�""m��7D��#�~���""���!�&u�!�3g˺�B_�J�>�6*��?q�W�i��l�ctP;�{M�	p�"
79,E�B� rZ�y䜢D`L��^�[x&�Sa'Uw
79,"�^�7(ɇ��;I�, ~��X��WP~���S�UJ���."
79,4��AD��Q�M%t�Q(v7H�t˲dQ�ڙP
79,�[�R�ޔ5�����
79,;����&K�8�J�M݉L��
79,_�A${}���g�q�
79,[]ݤ��*
79,=qG�m���=�$l��f��8���ɱr��͡���c�h�4z��c
79,'�%m�d�zP
79,4a��yKw��E�]ٜ8�ne�=�G�H��&�>G�k��>W��vR�ҰZc����~
79,to�����
79,"��T3P'ئ�V�-�,�"
79,"c��;�$ӥ��郂""��3"
79,"��+��"""
79,$mx�<�@b-�����e�J[.����`�}�-:>
79,z��<�!{��K����M�F
79,�y��D�{�Y�.6�0K�z�4��8�5�F�@��<��9��~���D ��HtN
79,}0%�}x�����7W�i�(Dl������롳-��%��g���Q+�zc�k��`���2l׎O��w�������+k��Wo]x~��>賗���e
79,������}0
79,h֫���i�jU͉��pҭ�c��������Y�����z]��>�:
79,��p�gHb���y�� N�����1w
79,o#p���*6p
79,"{B|�T*�N��ܚDSR�o6qa,X;I�����XxJ���F�"
79,"W��ZF,��e�"
79,"�R�b�����v��rz.��:(��K�����UCUm��m�'hr�b0""���q.6�}�V��"
79,�r	������G<��5�h��c���0M$������3�kY��`��F����/��+u�d�+���R��-E�*F��Ա����b���6�����
79,"��j5z��s�)��Vqv�&��P-ơ�,���>5�C�ֵ�B�� ��$��V"
79,�5����u&Ǉ�$�e��-	����a �0	����¯0d����:>�)���r�ȝp�{*�Q�3�E�k��w��������0�_y�=w��0�VY� �w�1j1�	�W_��sFX�P�ѝ���������oW
79,�9�3<Q�z�vq�Y����������������ʣ���.�r�<�i�����?�����������+�|<g�Q��<^^� �?�~�$lp��O��B(n�����v�7�{�
79,�VG.���ؽix�A�����b�[����*3M8jM����UK��(K�>hHh��*x�A��}U��
79,E��$!�G)A&dU-V�M9���r���f�A5��U:��T13�����@�Q?\Q�L}v���f~/���*{�Vc��M��g��;#����9��u�_O�Ϛ�^���}��5�ִ^=��/͡yyx�{u���l
79,"M�������~'�6�@'���e��F��ĢV����B""�k����HGLx��*%>���鿤��8�K�*��^��<������n��������]������s�����"
79,"�<���""��UK�T�z3쎏�"
79,V��l����z��|�e�d�;C�[`����
79,x���
79,�W��=�=ך�0�cq!��	@��
79,"�� G� ""W�&����[K Sd�Z2cd^mT"
79,�CSTF��o��
79,�s�×;�@�:(�/L�kk�g�Oi�j�t��N��b��� w�n)���kn�j d���h����~0U�
79,"�{N?��M��_a[,��7M��F��`�!w�	��w�F��?�H�-����"
79,"״�6�x����B�S?��B�""�{|�"
79,2K�&Yt�
79,0)�)�� ߁x���ѪBQ�[�oΑA�@��e8��>0Dq��� ���4�/럏O��Z��^
79,٭K`���#���a{��i
79,"�]�@4=xz,��S�O����m��}Gܱ�"
79,0G�G<4k�EAA:�`���7�N]��=
79,�od��
79,��8�eEdJ~��!Ӣ�\?���;��a�
79,��|5�����Q
79,��ĺ?Z���4*��7�V
79,�r�xZ�:�9��y�
79,�yT�;�2:8�޽qΜ�TWU
79,"��}c8�pf׏s�Lأ�����[�a���r�*��#\z`�ǵ򿕍�Ͼ1\[4�A�""?���?�b"
79,LٍK�H�����sV5�'?����I�ۡJmG�?X@Y�d5�W�0n�������B�V��Uh�
79,��0��VU_Ϻ��w±�$�y��+9��
79,L�𝺴a���]�RƯ
79,"���z˨W��z����8�u�y�|ƫU���""N�"
79,����\s��S�����������
79,i����xK��Mc����M��L����K5�C�Qd��蛋�a�YKǍ;t|#K�
79,]cǫw��������
79,�C���4dn�]�
79,��C	`�Z�CC�t�����~��ݿ��`�
79,"""�_S�"
79,f�����wU��z��w�z�0=o��s�
79,AU}�yrY.�ae>h���;
79,*({�	�|�0���B��Jq�c3!���d�2u'����&�.:8.���Ř��b�Wa��1q�� ��B	���
79,/�W0m�j2����` ��:|�+������2��0�*�.Ka�ɡK�&<�Q����S
79,G�<���
79,�KڞZ(���	�NôZ���#)�
79,�����<�W&#��.��`�`��C�2��>�!����}pe���Cw�Č��C�Z�X
79,�1�	XD����CF��@�6]���C��R�4K�$
79,w���]�45�r��$���9R5
79,"""a^��Ǉ�4�9=�"
79,�?���*���V�g4
79,~>��\Q
79,Ԑ�=!QO���
79,�&�
79,y@t�8VDf́��
79,�-b��	(����(��ҝ�vX(2� �F�^3
79,"F���V���X[""%�X����r�f"
79,i(W̬
79,G�+��5�k�ގ
79,s�<�оΘ{Ӌ�ҩ�I���h�`�������^L��4
79,"qj��'�$K""#k�t/;��	3"
79,"?���""=�(�ȜF�-�Ic�����1�	�xF@;��]_�~"
79,������j���n?��8�N'�c��|�d�89}w�9��f�
79,�#΅'��a��
79,⯌ے|�V�R�PK����#�H���~>)X �����C-�Vx� I%�A��K
79,�*�
79,"��^�@�V""��LE|���&�މB��|��C,��0ˏ�_�g4��G!�]ZK�eh�0H,�	1\��-����>�+?ށ�3ƒ��Xb�`���j:��b�>�7�m�'l� }�YȀ��e��.koV���n����yf����R�ﻍ�@sUBM�H��5 �|��Tm��3���.^��GOD�G� �*�E;��]�ː:����B"
79,�pvCy���0�i�iϸ
79,����q�|����{i�
79,���x
79,4����<��1����hl�!���yZ�&㑣[�R�����&�ٷ�n����x�g���r
79,"��X͌�V�S}Xޡk�������#9�)׮��Ju/C���R&/��}�m��-NV/+�Ó������Ǉ6��E��2 ��2D�0�(|��ܭ""���3����?�%�"
79,ְ�L��R�`����Gf�#
79,�>��2�Z8�ʛ秏6�� ���&�Yr�P�C�d���~����F>r-~~�M|7g�*F&�ue����
79,���T�L
79,$�@g����k>�[�/i
79,�v�
79,"G9�,�Ҽ���]�(��]Z�e �"
79,��ܟM��Fےb�2���ە
79,"��+� �.��$�D�� ��V���K�S�]}���ǋ�-3b,�x̟��I�1j��.[~�	YZ:�h�N�s�ߦX����-��"
79,"֧��Q_u�$��r�%�RA:�Y*Hǻ5�P��""\��-&�󜝈fFN���ߏ�r}�'�V;���l�cIϵ���ǒ���_�I�@)�@�<�8A�xl�t1�r��"
79,"�%�q��L9.��3����#C�r�mO�����5,"
79,Qp!
79,�E�������%�Ն��_)�/nX� _�����
79," �����w�F�N\����eI�N����B_%�,�]�+�"
79,",�9�πB�s$)CΕ"
79,@���u
79,���;�1����R�v
79,;�h��
79,En>�������*
79,".�����_of0�)U/Ur�]~�>��}F�[�Wԃ_�.?	��.nm��e�U��b��,����!"
79,��-�D�ېA�Z����?:��״�X��
79,\~~��e���?�e^�CG
79,��L��Z�k	h�j�X��Bx
79,"�""�"
79,~}�OA�l�����p��}=��WQ�
79,"X�i,�t^�6OR��sN�"""
79,�^]���@a�� {�¾��(��c=6�E�#�z��`޽}
79,��׳Ͳ��	��z�;�
79,� Z|p�п�QK|����٩(��K
79,����
79,~9we����5z�q����
79,����q(u��O��=����Aۢϖq��r�}c�a���5�]X�8��ϗ;�Z�g�
79,+ox�:��>�q�'���Y�\$A0�
79,�`z��<��lII�6�7��r����w
79,�Mk�d�Q=5��V�E�
79,>M�
79,Y(��
79,I5������!(��T�cN(Ґ
79,����3���
79,"�W0*�ʿ��,���o��%�.fs�`ބ>��&w�3�g�@��,"
79,�k*�u
79,�V�����q0�������-0�2��W��w��J�컏~=���Ce��k��<�nی��w�F}�z�(�x
79,Z��ޙ߶�ۃ�����@���%-ǉ�
79,�!���nNS�����Xw)Ϋ�+�o�Fճ����jȇ�7U�
79,�䗕��#Ņo��9�{�!B��@1?���������o'ě{m�@=K���b
79,���WZ�r�+'��J�����v�1���#s�6���E�-*�����䱥��}�B|���N/��WAw�w����AԷ
79,B���Q��?{t��T�R@Yv���eF/J.
79,=�y���ƈ]�}�/G�O>�b_�4�
79,"��YΞGEo5��}���@ $w�k,'�C�[���g��9��f���31��7}""%Ǟ�#�&���yo����{}��|5�f�۳?�C�t����Y�����"
79,�9!�5��{�s��
79,��6+���ưR~r���:����gwz�2&>�ݼ�������Z���_�A�F_��t�nKF���A�<��z�3p����݊�����������Oߤ��G����w@�~� K��<�~e��0�o��`b�k�Db��+n4&?�
79,"���u�Ų��ȵ#<�r���ku*g��+|��eŚ���]Eq�neF����|1p���5��ۮ���ϖ��""""����	t�˚��>�g�&��}p]��0/l��P�"
79,c�+�ٔ{eysjs��vb&
79,὜��1��.�r#
79,~�)LiY���bo��gO�=8��ؽ����۲�C�w���f���3�a_� /��U���	�����6S�2޵�O��
79,ǂ���Wѯ�i)K�7
79,�ų���S���{}�ywEʘ����j��Ǧ��h��2�wk\߾o�F���M��>����{ע3_��V�P���Z�����'�mڻ7q:�&��+�V��}{�I
79,��<��� �J����
79,p�C��V�e�V��ޮw�n����Ŭ��n�+(g�5`�s�~Yc�Ͼ�x��D�/���6nI���J���v��mZ�����ή@��9
79,E�V���3
79,�YRw�u�?p�@a
79,",��ߐ7b7��頦a7��_]�p��+W���"
79,�tӷ��2�-a���-TJ�b�-�
79,��)-�
79,�l�o�ٴ�
79,/m��1{dH�ks�Fw�~��C\z�`����_�p�W<��ȣ
79,"����3Wԏ�,v�b'��G�p�"
79,u� �Ф2��M�
79,"�z{E,c�	�.��8Ή����}����=d���-s�C��EGCXq#/�ٷ���<����e��·/>Qpq�Q�Dd��z��,�[�(sڽ��d�DJ���o��|u�""�ty""���&�RL�;�Z ���0Z2$�C�n�#���$Qj"
79,�.����A
79,"{P����a6�,㋲ic�C�x$M��&����Wpg��b�g(�HWq�����(�P"
79,�ax���s!��(�
79,%�>�+��E'���w=E#��D�͸�@���3*�p���R4�>ft�`�wf���q�E�eT�FڳG�Y�e��۹c���] -x���h�&���d���W��
79,d��b��
79,*�F��>�(a
79,'=�����z�9M�0wЩR�
79,�H�|�Q{<`�s��O�S����c���l�@��͡�4l�x����-
79,�'ǳ��F�~�!'�
79,��P�X�'❟��:3/#�L����mIj��c�#y9c����pE^�LX9oC(�3
79,�e�
79,j�9.a�d3[��&��%�LF�g�=�-.��U�20Ҭ5G�xP�˦���{`�jz&���5��\//����=X7f{�I�� o2��
79,�h���� �ɯ�F��S9���$z��I������JN�y�p
79,�K$���J�`����f~r����)a�3�n'r��=n�B�=��+P/?���-���i�Q�
79,R-�H�a�
79,nOZ?��03/jNnȔ�42������]	<���8���BÜ�~s��=�*�?i7��ۃu#���-g�
79,T�Β�Gix��^sN_=�1g�3Ws?=��4s�)����9�ȣ.9'O
79,L��E:�ӣ��-���0H�K*K�&�
79,_��Ak�B�<
79,f���oyDǵlԖ91;
79,��̍�1?��0{�K�Jy5?{���I�
79,l�fSw�ͧ���By�A�ُ�ɬ�f�z8ꛟWjA����J^��Eٖ��w�jh�lq�tU��6�
79,?����
79,Mkt�qgv�������ۏF'��b�^�@_���s�N
79,wی�������y�56��*<�Ɔ
79,"gK�,["
79,��;v����
79,��R��t8�z�u!���
79,pJc������虅g�զժ�͖�4�
79,�ji�^k��f�FW7U
79,"+��V�tk!mZn�c�6¦�^,n�����{��qߝx&y7:�A��*~�w�H"
79,�f]w��E�s;��	���!��]��=?>f�<�ļs;&�z�`bۃō��q���0�꘍�-��;���ׂ�dϪG�䠁��Pu������֮>Ë���w�b\�պj��Ѕj��'K��\�mq��W�][�����=6
79,ئO���~-�
79,"Tm�X�R^��m��	�7f��&�u�""���"
79,Q�H���Zyw�i	�5)$Q����~5Ki^
79,"#0""�!V���7���P��l���=P"
79,q��6�%H#Hp�w=u2PK-�F���ZȞL��� �(@wr�
79,"`��q,"
79,�ƴ��E2��g
79,"^q�)r kZ36qG����O�4���ʪq ����H۞&��φ���pr\""��SX�}�y��-�qO9���e��m]X�9A��"
79,m�j����'�U��N^ďn����G���L���j-��ţ��?<�b
79,n�'�#~x�^��J�#
79,��v�����
79,UMbp������
79,"�""΢n�(��ʢ�f ���@7z��)^gCs�_@~_w�鮈M1�-��Ա;��s�X۳���R��+�Ł�ΐ\c(��gg���;"
79,!�_���&߱�\��/�#asU��^��4m��=0;����
79,�k'��^w
79,��)u���v��SjT9�f@��G�O}���v��� :�z��W
79,��:<1:�j���ė�Ǩe�'_��������������35��Ń��.�n`��)���� �$�'nGYD���q�vsI�k�nK���V��0L��4Z\�����VՌ���'�
79,"�8Ň��C�V{t�#:0W�����d�j��}��-��]1�b��}*����nV��lhF�h� ��t�1��u���܁�bi�yPؤ��yf�]���(��7U��q.cߺR��zf�b.Ψ@���3���(���n}RB=S����""���ǉ耧thZy򗖡��J]y��@9�p+�@ ���f�$���@b�q�+zF/	�,`D+8N��J��KE̛͛� �@�"
79,ĻMl��ڈ^2���G�n�ѷr7��w0T����w5�����^�x�w?�9u�oNWˉ����H�����A&���OD��?�&�9�/@�(cf!
79,"�Ő@��3Z���8��H""y��7��^oIQ�<�baF~Otd� %ԅ��f��ʆ�z¢"
79,k����`*cNK��1��åp*��u���]���z̲'��O�H�g��]!
79,ne�u��Z
79,����2�=�\�*9:ua���:���a����B��ؙ����5_���8ȿ9�K�-孆[1�5�q0�UӨ��޸�
79,�ҭ�*��7�����a0�W�']����iK9dSEo�`�՟�}�5�g2�oD�L�D�Pl&V�IR�&	93����C
79,�H�JhWFÐ�?WL�+�}˘��i�(ӈ#�=�o��
79,S���j�#���*2'>�
79,"�߀R����c�旫L~���{I5sT6ԋ��a|����%���""��UD��u	��t��=�"
79,�#f;��fA��9�D
79,T�['q
79,Տ]?��k$9z<0c�vQB�K@C�]=�( �UYY{�jUӮ�v]��iӆ�]�.���
79,��]�EpyU���Y/ٵ4�!
79,��%E״K���B�
79,��â��Kr�
79,�\���@
79,]6��4�o)}
79,�i��;�gB�OS��
79,"�}�-i���[Ï����-a��m,��q��׼_�fFP���3�"
79,�1jQk�Q��P�'��#_\g�Y����+��I�RLw��u��T�Z虒�T:�ӶA��� Y!}�I$=H)�
79,�QN_
79,�(�^�T��NNE�d�F�u��Ο6�H�
79,�f�8~���s�+�k�%T�؉M�a�1z���i�lT�6r��bw�
79,��N1�r�H�Z�Z��I��=�����������|�LA=�q���t�����x��oT8嫛��HԊKY��
79,��Z�̀hW{̐��G3|T�p�C��r��l��FŨT���T+z[�W����Zi��㫕�Q�u��T_Ci�������چ�S�+Z
79,�Z����_ͨ�J�Y1�M�w͠�Pn .��.�7�z>�[�^i�M��UzV���q��z���b@W�5��?�Y�N�Ct���x�:�����F����԰*��T�o�ǦY1�T!vA@�#�1h�/Ty�
79,qZ�%��e�6㳅4��y�_��&�o������б�DЀ�}W���o��g���N�]k�6O���
79,��
79,"�.2�����U#z7N��M,|��WZ*Í�z�گ��d�"" �m���Ѿ����&���Zj�_���Ŝ�K��ԝ"
79,�_�f�O~v����O�����k(w/s��b�*��d
79,������G�+��
79,s�f�
79,"�N@�,vt=K��8;vX�kcvk6%�)#�"
79,"Y�~�k�^j�l'�""�������9seW\PM�"
79,��mER� ��:$��B0_ �庁\���!% �`��Ѻn t�
79,5[h���Q-PH(.������c'�H(���l��ͅ&Ì�m~%(.%
79,"��""R1;� ta�(+�X��(��l��ϟ��)��J�Y <�-Z�"
79,"��ZQN),+"
79,��O�K
79,0l�\\��HY�k��V��U�Q���1�$f�7a!ސ~�߭ a
79,�G��
79,y��r���	�o;�
79,gJ}�F C� W�^���!��
79,��+)J>�Sr0<c_t����$A1+ʮ@�NZë�4L�R<2�p�V2A��?B9'^��p#�}�
79,����
79,�+@���$���+(	�[ˆ�!n.���
79,9�a��<��
79,���ȴ�+���-$�w ���SO�N-B�dc���2K
79,�.Wp
79,�J�������e>m<���@G���B�3�e
79,"m�׋�dz�	���Y�uR������s���L<VH���O�Y Z�""�)S��"
79,���S9�/˳8�#
79,�-i6�v5pq�
79,V�s���6�
79,"!��;~G� Z+���o��TK�(w�'Y�2&M^��A�?���[��rQ`�)�=C��(M�L��=�u���Lv�+-e\V���}5�)ɘ+�F��1ѫ,��<	h@*M��N�8�m�1@0У@ޓ�y� 0����}�"
79,���P��%]Ӝ�m1f�{-�)�:f�
79,�	YqyT�W��++��܋I#�}�h
79,�:��ʊ�
79,�C�%j[��b9�7�l
79,S�Y��-2g��[��w�I�1a
79,"����""�<"
79,"Qk%tD����	�	���)��oO��I:�Z+�|��vV���,u�;�t"
79,:]�>8ߏ�!�&�[
79,"j�ݬ��SW?�|���!��V������Q��p�q""<�����@�s"
79,��̐;�
79,g-��q����.�<� �
79,"����P.�b \��Y���G��""u.-\����ɧ_Ҿ��G��%}�_�3�wuT�CV��q��M��=���  r�E�r4�0��4S��p:�P�O���"
79,��]`��Ǩ��� �{(��� ۥ\-�F
79,J�#q��m�
79,-����oi+�xW&1�­��4m��E�e�e���(�ra��<&����%�K(�V	�����J@�\�����
79,�QrHtvJ���d�!�ڲ��
79,I2Z��*�;
79,[PM
79,Q�7�%���c2����~l0�R�V�%���h��)}|���m)���j��N
79,�5{-�
79,�?qi�í*~�F���I[���\@đ����#B�X��q��{Q��Q1�
79,!�]F���C]����>p��=�����O��# ���
79,"���a""��˒֗�f�夹�"
79, e}�&
79,�P�
79,�M���(�ڙM�
79,�����u����B�{R
79,��Q-4�j���Z��I-|w}x�ݾ��Z���[Q
79,"�]?�'�X��i�g��9&tJ倢������7\Ps�b�rZR��[9�!���hK�byb�r��RvQ�O�L�	DP4>�����CZ	�և]� I��i�V*�i@""lp+5�e���n""P�'�b"
79,"}""���"
79,"1�=���E""�=���ȇ`>1AЏ��B����L���p�;|�"
79,!�O�
79,��C5��/�`X)ǉ�xt�NH
79,�(��K�F�m�=/�
79,��0`K(�W��D!�!c�.�ў�c���	���k%�v
79,CB^oB�e@ƾB&�
79,�f�c�s�P
79,g��LDw�I�.��
79, �3��Ooud7��
79,y4�����$�4qD��)�x;�ͳS�rq[)
79,�RI���ȥ�
79,���X
79,�?X���Ec�4�dP4R�
79,"�����`;[�������q|k�""cqTpq��Co!Ǣ���"
79,k�ΐ���4g����fA�d�zį�Wwe���:��p���h�)��V�}�T����
79,_+���1�v+
79,���h�+���g)��
79,"SH��Ȃ��$DD,I=�zJ"
79,E��Ýp�����
79,�1�a#�ܾ.ӧ=�)+�]���aӻ�҂��F��
79,"f�G�<ր��,�ȕV&�TY�}�]am�+]j(��ԧ�^"
79,8�S��z��չS
79,"��Nz��d�j�,m�)�_2XW"
79,����p��OB���~0%pҦ�
79,*er��
79,��X��Ftla����
79,�Za��J��0Kqɯ�H�����t*ݥ]e����؅�@�hk:���`ǈ�Lx�ɨ돷�{q�� 
79,����Ɔl*���FpK�_zkA
79,�Ͽ�~�Pb(�%�	C��N�.�H��:}��w�P�a�q
79,"O�`�Ք����m�3����ZQ�:[� ��J,y��7"
79,;	7��
79,��_��#�I�+��dX��Ϋ��b!K
79,"�[�`�FƾKc�,�m)U�ԓ�fHg�{rC�zХJ�@?�"
79,�{Z���0�}#3����I��B Fl�eB�(�_ k	��uD���o
79,����M>�
79,3-Hg�]x/1
79,"_:�������d7ŬE����n�daKx�#7�7�L��<�D-�I��#���Hh�sSN���?��(E n��e1""�H"
79,��UL��匛ߝ)����Պ�(�n�χ�x�aE/��g˙��ko���7����K!��
79,"� ��U8��&��2��B&ҟ�a�����0yȮ��z����s��.m&=�9��>�J�h��:==ƛH�x��'>m�e�T9�N,١�g�pٿ�|,=��Q��q/P�2A�B�]ɂ@"
79,"I��R3W6�Z&""[.�$��"
79,�p2���~ؘ�7�	 &�?Ћ�DnnF�-~�#�:2A�!�=�nS��l�
79,{J����f��_Cvm'�D�2]��ѳA�)
79,"��z��H���T7&gZt=L0:������,r�`��xkX���h��b�Z�q?�H�C 誓3_儲$X��Eqy�&�u�w.���	V}@b�;�%��Z�bbz��=�f)�7i_�X��Y@�$m	�,�,!!�'�^�x�8'��v���xS1c�ɱ0�ƇYY"
79,"O[�;`����X8GN��� ���z"".D�"
79,3�|�p�C�=��}oE$%i��HdU�m3uY���.	��Z�*�(#��
79,9oSOz�QH9#O��
79,"P���Y�^�H��͆,B�"
79,"�PԤ*n�ӽ�?�vy�{�.�}���?�vymE��z7�D{sz����g��vy�5����vŻ�I�(!��,x=���r���o�nz��A0t6%s��60u�s����^Ħ��C��"
79,����d=�'@����Ϩ(Ϧ�(�������.
79,�=8ۼl׏�Q_�
79,%��ⵊZ�Z��d
79,=F9�r�L䤽$��?�����=
79,O��x���r362**z��
79,&ވ����CB�!3�p#%q=���3��~c{�r�ܑ���
79,���{�F�
79,��tw�V����°
79,"x�=�ډY$�b`�,�D��mq��7�C�""""���""E���n!u&�FÎ"
79,�$.�5iL��
79,��X ���@�ⲻw�9h~bwl��
79,X!l0��d���i
79,}	5s�0��cN�-�D�̲�
79,6��
79,|�e�������+��B|C�36�n 4*a
79,���J��>���TF˫'��
79,�?��=y��*�@D�@?q
79,"""%�E���f����/�, ŀv�3�h�,F�	�������O�FƇ�iG.�""qB@,���hڛ a�Mc�������f�D�x(���1 �)I���"
79,�� �jH65(#�hH�P�V>a��u
79,�w��Cو!9;��x��̖x���ꎐ��wn:D�r��F��
79,"��m(�N�� �n�'&��2�y�{���""\��ƒ���Ge��h"
79,��s+
79,EG��
79,\�`�PaJ�d
79,��I#��qp��
79,"a롛%,>+n�'xf�U�*�=�\>��8��c�� �g/�-�lUM�T�)�s\5I)@"
79,"�t^)<,"
79,�F)
79,�GfH�O[�ǩ�hn���%-�\/��-��sh�@9��Z 	��G�ixi�-��y݉�)΀;u�rsIj@t�!�
79,�z�b`�>���S���
79,"S�#����Į�g�#U�>ط="":g_/=(�;����"
79,���f��Y
79,_/=�苯=2�^�
79,v1�D?�B��b��-�
79,��V�PKO�E�r��e� �����m??���h�妰Hg��:Q�<���N��s��MYm&�4Ov-�8�p�((���%�8F�G>���ޒw=�ћ3�	����A|HC:�����D��<�
79,"b��j�|),�LQ�1���b1�u1����Aj����c�"
79,�Y��i
79,"�%,�Y5WF�!r��):D�3��	H<�$6���͇�w/7�+#LC3�r{��xxx��i/U!x��"
79,"��7�%��-t�����|į�/�8A,�g�)����>]zOG�\�@�*d*�������F �;�����"
79,v'~X�4
79,�^VC%J9��rB@��S�++�B���g���3#�fP�@���z��Ȃ-I�PO|I
79,��NB��2�U����l4L��\��ۿ��]#K�BF}iI�Ԕ��(����5
79,"�|��K�n맔�p�K""�"
79,�9�<�V*
79,~�����9t�
79,�kJ�3��>N]��+�c�_�
79,��6p`kʻ)j�ġ���|}�Ce��F�Zo�D�p���[��eH�������I��-۩�
79,������;(�e#m�s�ljwr�1�O���黓���fs��S$��I7���M�ptzx/7�C�w��C�$�'6RVJoꟉ3�q��3o
79,il6I*�GUcZ��;��� {�Y7e�u�k�FJ�:�x�Ɍ��;�֦]�c&�<�
79,Q&<ʔ�
79,"oy�ΈZ�w��><��ֲ,+�o��b�o�j]���"
79,O�*�G����Y���ҍ�\��݂p
79,J�Y�`���T�֙��lΖ^kO�S��(7�6q�C
79,�V�H��6���R�[ �'����d�e儱���!\�q'V�~
79,�yE0%���D��>���W�Y�59Z�f=:bv�('�qF�l0��
79,���ب�}����=6����KR7.9��P����@��NM�
79,"�9źy�V�""�=�XD��"
79,�e.L�%%L^
79,���+�O�J1�e�XN���M�TZ-*�u����
79,��v{��B�N�$���Me�2�:��*�Sqӗ�
79,Bc#��I:��R��
79,%���IΚH�5�:��˖�m�����(�4��y[�D`2X�����0X�v�����_}�_2�TJ)��j�Z6ꕖ�6*m�FeT�j����Q���J�Le��ǧe(S�
79,*ch��)G5~VD�z������~
79,��58i��Z�a
79,"�שF�.�ω����R��]�""��n�غk��5�	����x#���_ە�I�ŻV����4�~�	�i��j��o��m#�O������4h�#R(y�h"
79,"�1 08�3�50�I�@�As��bj������=2}�H;��̌�Q���,�ތ�p��+���RH�[��+""�$��^�I�#�'UN8h�(�Ӹm)/��sΤ��4�Ո���K;@Ha"
79,�����8����uif@xx�#u�u��(4�
79,Pp��=�_���3:J؁����>ˍq5s��<	-.��dh��b�
79,�;	�'ޣ /
79,�8 U��*�y���Z�h�qT���?��O	�O	�O	�O	�O	�O	�O	����A4��.�'Ň�n2
79,���q5!
79,gc��3'E&xx��0���a/l܊�@ԸLQ����v��f�-��K�ʡ0�M0PȈ���d�?~=��JP�2
79,�q=vp�֜��J�X$��/�����
79,"@�(�G9����F�a���L���S���Ϗ��d�C�<`}��""S9�S�ć�5��c�5"
79,"�E�c�T�#[D����S2""�0P�̣�j���0�9��ɷ���o����Rd!P���h�I�y�4���J���N�F:����mC�8䝄~�*�x�0Ӯ�^��E/g�/|'0������'"
79,*����ߌ�/w��
79,�?K�k����#�E��[u�3�1�l�
79,�g�z���Z���
79,Ӭ��ي��ǃ�
79,�og�&i�W�v	)�i��_�Vi�'
79,��m�l����i@SzSB�����|V��ka�@
79,��֚��B�M��M���^�޶���_5Z��� R
79,+���!_0����P����]�V�d�t��R}���v��*U�E
79,0����\��^U �.W�4�V�t��jW�_ѕTO}՚�&�m��	0�
79,��N��b:�
79,�+~D�M(�E���VG��xu�)��ok�rP�֨�8�Fx
79,"�4��ۚmdFP@��\�^�\Hr�ey�k{t�a*1'*3+*��""Q�J��IE����7�"
79,"��Q�C<Q��k�����������;4*���O} ��}	d��1^�J�?A�jѨ4��B���k5L�pk5kk@u�����pߪ��������	�j�%J��l-��P�m�8��v�.�k@ӑ���C��	̇.P�e%d��z	���ƀ��F��S�F����u����04.A�h`��w""�o9�a��Dz�DJ����;��o�>���rDJC �Qi8�s�h�Wojڟd��#�H6��#jW�@��:��5 �(����5���Z��Z��+����J�"
79,Z-ďx� �>�j5�	@��PG��l��l�V7�W�Q#���@a�YVr
79,�M�]�9P�Qo�4���Hŏ(�TGQ3��m�%��=n��^i�t|\3���eCG�I
79,�ӵ��+��@�u�^�bZ��Rn�L44G7>� ����x��s]�nG����aWۍ��+Dg
79,�� Ꟈ���9K
79,��J�l�+�\�$�$>P�7�
79,��R�FW�'���*�Fo�R
79,�o�
79,�FP���_Q�NX��4Ȭ
79,*�P�߸LS�2�t�
79,"0�*.�n��?1�*rJ��n�e����I�5aE �J����Qo ���""��r4f�A#oʿј�s�n"
79,�p�E��AAqG�c���Ū��X���K�(�7={
79,l�'\Q)��%�%��%�ޱ�2�{T���p�*n��6��6��<�
79,�KSqw�7��U��W�-	�@�w�/�V^�3���
79,�Z� F 9�7���y*���^g��!LO�c#�
79,"Ko������t]kZac`����v�""/ݔ���v���7����;�ZsY�iU��'"
79,�Y����*p����
79,4�*���]b4�h
79,�lRuy��X%cR�)&md>M��
79,�֨T�jE7����
79,"ꄁt����:�F���M��� ��""�+XZ�ZoC�f��B���BQ`��Tjն������V�""���[��*�v0�.�Ah�	ũ�n���%��ȑ�'��z7`0M`�uR}Z�%Chy:�hz"
79,�vI&�Z
79,C�x$�Q���5��MB�8��ncl�u��Wu������6�r��~�����+�d�(4�����q|��4��u��UQ
79,��6
79,GG��	��v%
79,(_̉��t*��C�^A(2 8}�V�f�ר�+��V�d
79,��*�j��]�u�jCS�n���j��-z����u|W՛(�4Zj�Rm����������N.����Wa�W60��
79,����#p�~����AgCy*N�woЉH�ɻ��
79,�0����l������4��Y�7�Fc�b�Q�����o��.���Z�U{r��#�TT�i�h5�:-j�y)Ũ*� �;�
79,��6ࡪ+C��L�
79,"�Ї�aP!<�+|H�.-���7��;'���^Nq����I:�eC�00����Yն�rg�m��@�s�pޛn�s�����%���7���]�FK�����+�K�Y0�g��`�z�""� 7I�bŃ=sr��T��)G��|'	""4<˾�MF���pb"
79,"�h�DzC��	���}�Hw��	��g�'�8���oNR�""�>�(��ݽN�#���+�����c��1ݨ� |������'h�e�@W�#tIe��=U����y"
79,�Pk&ы(۞�Pd�0h���ܱ8
79,�T�Q�r�sc�2$��-����a�\ ��Jt9)�SyB�Ȝkæ(䎊�x��S&�v��H�$�Qڅ3�ؑ���2��n��yͧ��'5u.y�v�:ǁ�e��[W�S��V3U��$Y��Mz
79,SR0t
79,N;o�֩b�
79,�NH?[�F�T��Z�^H~��� ^�I�LE�K�%
79,���L@��t
79,�(��
79,@Z��g!�P95L��H@lG�7
79,SKOUJ������U6
79,f���s�U:v�������a���L2��7�����D����{�
79,�Se�
79,w��}���(
79,>��U�<�~/��gU�����5������wF��/ [��Z���������[��w/_�
79,iM���i��
79,����W�B~��ϯ.��F��NG����O�bxꕇ�
79,^!�
79,�/�@�J�	�#3�_仢4c�3�Č_S
79,���:~�m6��
79,�*�̜
79,!wH�
79,5���wi�
79,���7*�t�+1MQ�&C͉Ś�fA(	eJ
79,D�U�'�����ǘf-�eh�	ZXr����VSQ�7����6O@3mմn���RXJQ
79,dCbЋ�16!{qL0�{݆�I���؅
79,��<�A�?�He �ҫL���:�jRP�`bM��6�����
79,"�${�N䲜�7bY^߿�l��ߟP��Y&���9�j�b5�,�ٟ2��2��"
79,�<�ʔg����)�yK�Vv�����J�=�\|��~�Tb�$�
79,�+�-2�
79,@D3ˊhPrUm)��ؚ�����p�Y�4q���cRj��}S\0��&}�u
79,�} �1�=Ԧx-?6sű�Jq��v��LW0 Z��L^�c6�y�I^3�4��;՚
79,�·����
79,�#�H<�1δ5�VeD�P
79,��ҙ1��DV|8 Ŭ�^���vį<@2k��A �r���G�`1X�
79,�c� |��˧zmS�n��L��!������L�:�]���
79,��d҅���@z'�8:u
79,"��VA��)�އ���QK_���^,Z`�(�%��Lh#g{9~�}ȓ�`�K��P4�Hf� ��qQi�����}�� v�ɸEn:՜�~��[�>���+N�u���%��8j��թc�j�M��"".$T���KQ!۪�^;jM�"
79,�//���t��B�iz6%.�!��W@ k<
79,���f
79,"A?l��TX(��}�\����A�|�������oV""�yF_��9tאnHRv���X�K�)"
79,�3�p������
79,�r���k
79,S�
79,c �16��ab�s���u6����7���}s%6���V6�������紧ҡqhg<�>��S)b ��$�#�	Q1�L�MYN$�
79, �r+I�(�$���q}*�
79,"�R��Z���Nz%�2hBa���PD@ k��q��F��r_��݁�o �%W~�Q�)��/�)o����}f�S�?��{���;��!�c�r&�)�6�	M""v^"
79,o�w�`E:)F�.�
79,�S�Je�������{tVa���d1�F0�y�H
79,a?��-;��=~��~��ܝ
79,"��梚��?,�'�Jd�+}_��""<�5}}j�U!2���X��{��i9�^~}t��rL|��x���g5�e��"
79,��&�
79,�h?�}�qq��+P�qT�_
79,��}4�(������<1��w������� %j��=�ur��8��W]�����rȔVq�x��C�1D��%Y����Xw��`6k�
79,�G����ԅQ��
79,Žy⻫7����[m&?��'����5��L�īL
79,"�������,���ɇ]�)�`��e������kq$K�b��c��i��א�"
79,�X�|����z���l+l�2��8����iN5gP@k��KY��m��>��b��K�
79,"��,>�ί"
79,P�i���
79,%<������
79,�� ��9fP���R��Cmx��.�{ؕ��<��$����3y��Lؓ({�
79,R^��|3`�s.ҡt��{�
79,$��m��녝�<@@`�4��m�f�� ���6W:���c� ]�9�
79,(�(#
79,�98}{t�����NNwO�+?+چ��l����6fBt6��Xo' ��kv���	g�
79,{c�����ʜ7?B�_�}~�
79,��E�t��al���^O�' �#L�Q���=���2nO�K��`N+>Y�8�
79,�B�@%x�
79,"q+UuX=�`��u���x[,m��d"
79,��������`@��{u�x=mu3'�zo�����5|����9��w
79,����忦��6l6��տ���a�����������ߙ�����?�h���l���7P�ǜ�:��g�&��>�׵�˿b� 
79,ku��jV{�j��j�
79,�ڭW�Z��k�6�y�j2�ڨ���
79,7ڦ�Դ�avu�w[�zM�i-VktY�itk5��k���b��`]��6�Ն�m�j�ΪZ���z:�5]7����g
79,"��_N8Q�{$�T&��+,��帞R�K7"
79,w|��!�O��?_���3�����@
79,�G'�ЍLC|@)B~���`�{2��)E�T�-��*�.I�:�
79,">�3�*Q�,.v"
79,"���\�I�:_���q��D�JG""J<�?��	��~�]���NHq���,�"
79,�p�{��d����Բ�t�>�'
79,�=��w*N.���m�^���('_a��� @W�
79,"�.��z��q��?��B�ݥ&��t�&���A0�""�K��4EPa��H9M<�K���1'H�~ �ۻ�t5 I=�7U�y�G��_�[�\��s�����^�ǥ�@��S�"
79,� �)���z�}
79,;o�Տ�T��͔p�&��
79,",�D��ta� ��'2뤏�?gD\b�G"
79,��~�L ��u��=zt����f�fA�u�!�r1�j��^�qȋ����:� Mܳyv��e&������+3��Q���V߇o�5�h#;?�@a'��x���m��y��
79,�Z}� ��A�J
79,b ��j�0|z�6��Au
79,&�=�;:T��&��|
79,Ji���Q��?N3�17AKD�R����
79,"�sNd��Kjo!����,��g3�C�i2Q^a��C�W^�����>#�_�^0�"
79,"�ܿ@���e��,��*J��B��ܑ+?&6i����$�4�4�kh6�V"
79,}�%!�MW9?N�ܐ�O2��tLY�#�ڭ�y~�޶|��&��5�(��z���Bħ���fy�=���
79,�x6�Ь��6�
79,"�h�[�z�G���hP��U��pŻ��y(�˫;փ�Ou�`H�;�Wxw���Á�KE��VԠR�U��=�#�,Pފ�U*dH:#�*Գ��[�4�2�zH���{�?�+i�}��}�M"
79,;/��]�����t4�'���d�
79,"m�n%8���?p��9{��]�3/@S�8}܎�9V�L�5*Մ�J�""n����2�L~O��8�e�1+e�=U����Z��%�('��e�d���iP��="
79,�����ZUx��dW�[�
79,��R��J��h����PMM����Q��|ĝ�uԱl�[�;����k����{eFG�+��e�(���!���
79,"���Ũ����a�3��t����W��Nƣ���jh�����3q�c�.�5�m���-�ǊT^��ț"""
79,i�G&�:+3��=/'�QQ�P)j�%n���K�k���i�f#��x�H~��}v`���)�_�P���#���s 3I��˃M�%1f��%3���moJf|�W��
79,��d�������ɛ[}t{���x��i���
79,������C�)��1:�+跣��u�:�r�V��0�^5jUMcgm�ҫ5���zV]�a�Jvn����z�]�3ި5����Z�a44��m�f3[!�~
79,7��
79,>J��cg�N$&`{�M�\�cf@XI#e�����
79,Z��d��
79,���tQ(�Z~B3GQ|ߩD�T@5L�����?_t�M��6Mc1�(��psK�
79,J�I��!�
79,�:tӂ��k1�l�GP���+�
79,"],�_��`C��l�vv8�ux��W̹�4}H��+p�1s�&8%"
79,"�Q��A�""g[P��b�a�VS9�`Cǎ�_&ܛVF.h=X;AQ�më�M�\	�����`�]�"
79,"_��b��""�3�+^�!�"
79,�`j�l�W7j=ޮ7zz�j5��z]��fK��`�-�u뚮��3jMXQ]�5Z�at
79,"�l4�zk���v�u�:t��eU��fh]]猙F�V�Y���W�7����,��� =j�ZS�k�fz��۫����:H{F��k��՚M�L7�j�޴V����h3n4"
79,"����e�F��Lo���Z�����׺��i������] H�^o��֗J�!r%���*�����5նZ5�&y�a/`}y�_����op�o\��Z]X�*,=X�w��W���};Cn�d��{O�����y뷌��&ӻZ�Uk���m�V�޵j���,�&�+�[��9�"
79,f���^G����a����l����m7�Z�ihZ�m�cV�kV�pI�έ.�魌J
79,>wdx|�8��/�Ώ�����EZ�/7t�uFǡK���GxH��BR�d�	�1��>l| ����}
79,�U|�(o|��ԃB��Bq6d��~_E�*|*�0��T p�k
79,"^&޾�}~�����.�7+�9�]���l�{I:Fo��+��0ް���|,~����i��`�N�������=���VE�N���'��h }"
79,�~�
79,"% �����b��0! 9%3 &\�	T�m,�f��K�v�Ȟ<�J_�����ܔ�a���r8�4��˽�H�t�w�U�(P9w�Q&�Tچ��Ѷ�OA��~0�~����x����T��O��ǹ�+s.���@�&�""}��Fs���%ʘ0���]�(�P���c�ʜ	��Se��nJ7b"
79,G�Y^4$�v�.
79,Oe��4�����Q	RC�3D/��oؐ�|�蕺
79,g4
79,N'1\B�
79,��fѣ�Γ'��S�K�
79,��ζX���.Y
79,"-l,Q�G�>�%�*�~�ҍFşv��"
79,"77b,�h�"
79,[ak �p�
79,"�������{?嗃I��a�L�ާm;���v�X�>,[��on��T�ݼ�X��X���"
79,�J�+�����������|Z*��	��xiG��7�P�S���D˝��$Y�C��&J��^�
79,"6,�EȐ�0��0�/3�|����P-��T�\THsE��|�F������pL�~F�qAe<�E"
79,B��P��!�v1�o����H�$P��
79,�XE�\ RJ����H�X�����6�����hטVc �W��^5�V��Q}HJ=W�
79,������A@����{��A����|��՘�s��x���KY'LmaK�>n~��+W����<��q����&}�q�UV���M�i|������
79,�����_��V|H���sQ!���{�H-l}Aa<􇟥���>�q�#ZD�&l�ܧTE��TڣP.lTj�*��6��G�*�{���V��G�%�g����]
79,1�-�e
79,#����T�p_��)���L�P
79,�[E����x�r���D_
79,�?A��O�� F_t��89zS��L��ݛv�n 0&�������1� ��K�3�-���V��p�F��r��o���<��x�0�(4�ش-��|g�����ylZ$
79,"H���w,Quq�T�:�{�iD}*��7 i��"
79,T�@��)����t<\���a� e
79,)[�
79,�/l����ǉժ��׫~�����8�
79,�O
79,"s�uS�J?��o������u��}%*I(=y�c�g5�ۭi��I��H\��<�|33�Z�|�p�1�>��^4]""��b#�>��^7��譑x�eއݹ�nd�z�1�c�"
79,W�R~��SW�ƽ���zb�c��ϝ^R��)�3���^8
79,�=B�bU��z]+mٙ
79,��
79,�2�x�h��
79,",��a�_�`����z����"
79,���vS*�
79,�zX���Zdm�Pf�5�)44M�T��xH*��V�e�@�\;A��9@���{Atfz/�hiDh���
79,"��:?je���S9 ���s�x�I����ϓ�Q�I����� VsYbԧ�""ɸj7ҋ`�Vbh<�{~m�q�>DЕ9QN�b"
79,6�ôˏ�6r�ba��P
79,cz�2��
79, �/��8MÍ |3�^��j�
79,�!�1f
79,|��*���Q��RR�B'�@�0��)�ĳJĺ ~9��D��
79,"���4�f��J�!�""\I>M0���N"
79,�(4����W4��.�P�
79,�{����#���N�J?��X��M��� �b�i��'Į���x�!���Ry�(S
79,'n>ź�ST�
79,"ea�&���c�f��\�XP���3���_� �� �X!����ҵ-�*��bL�$���B��#��0�$H��2�j��)Ģ��_��� ��ו`�G�E�Z�}�c�K���A<�+���>}�$�}Y,�	u�o~���Lݥ�{��c�vF"
79,� ���w
79,^�1�A�+�֓����rL�+B�����{1�(�k��HH.��w �d�s��]|_���x<yd�J^���CL�J� 3�_�(Ԗrh�*���1%�֕O�vP(_V[g
79,"�(`��8���m�%7.�F��J�*ga���z�0���T;�mHJ�M1�aچ��6o�*k�Q�]3��:k�-�Ыz�0+Q%R�Jh׵�j��1�S��""����w�3��*!n"
79,���EЭU����mb��F�V7Z�6 ��ڵ�i6*��h�~
79,j����)�a��j7��ֳ�-
79,�֮��٪55ޮizB(�G�:���-�L��� ��
79,���+ޕ��7)e���%��*���g�ρw^��!��c�
79,�s&�ś�y���ͬ�
79,Y��
79,"���k��ta���V�5,�s���9��d�V�ٕ���I?*VPƨ)�(�6ԥn�@��dH�[���7,�v�e����*k��z�n�Jv�""̛;����w��="
79,Fw�L�u��VzldN�~0�f�Qkב��fU뽦��x����Y�U�HmN�jXp�D����6
79,I�6�V�UӌZ��kM��ծVo�X�լ�V�mVPI�|d�)Ǐ��3]�{��
79,�l�@��.6	w$�����>p��m񣑬bd���
79,_`��Ԅ�D�o���Wa�p���d��<��-z�uP8���	��߹cf�A8Wt�(z���{
79,m��F�O��	tE~K�ǮoSpӭ�;	(���H<�iJ�!g���W��GsnცFC
79,`��|�y�k�F��Y�����7�м�
79,�A�tK
79,��?`x>�r]X�C��Y��}*�[��
79,"aun�Z��m,6X"
79,"�v��^�6�L.?���d�P""V"
79,"�\,����IBZHW | ������i��}�)��v��!�e�"
79,ޞɗ�L*�r+���
79,T֣sJ�5'K�2��O�
79,{��
79,*n�o�p�6
79,P'���%ёKv���A��g����O�S�պp��KnO�z]�bZ�h�td��z��
79,���Ս9nK|�4T)$��
79,"�V��,"
79,"�F�^cZ�l���LC��w�����We�엝�]�r�����?�����6^��/�`��`c�p��X½tn,��t`t�ʝ�7bþױ���8�MOY�g�~�����9�O�����t�;8��lʽ��7�8��������y���A�,��%�C����}��?�h/⸞��� ����S����V��^`�8afZ�#h~��Q����vҷ hф�9�rF9Q�R>�l�"
79,",S���"
79,"\Y�M.�,q<1_�9�(��`�����I"
79,N_D7�ٚ]v��`QM�
79,"��r�EG|Uq{=t�w�O��3.�""���x�oG��NGT�$�!��JG�+� �g�Y��3Bg�ވsw�hcQ�o&7D�G���XI8�K?�Q/}�/"
79,"�s��~T�]���_�� *�7��Y�m��}�c' Ȗ�gj��A1QUiɺ�,e��}��ry&��^�Q�5��U �t��/�JH�#��z蠒�,�6"
79,"x��G$#�vF��)�FsX̙�,�J�;��Ԫ�%��4�p8*6��]�K"
79,��[�˜
79,�fA�v�2����:¸D���)+ �O�B��!J�_�����W('�cG>VAa�)ß��C
79,��&��<��!N�o��1c
79,wH�'�ˉ���<�7G�O*C��B}�[͖��Z�gz��5@�k�
79,"�b��l��/q�6W�Z���cw<��A�S�~�8W�]�`��=:��G@\�Xu���i�?*Mn�f��k7���l��f�����MSk��9���z""����ġ�\����Th7�F��n�9k�ڍv����U"
79,"�W��,P{B"
79,�me�E�8����N��Q!K8�eT{
79,ު��=��n�Э��-�����4f/� �KE9ȑ��g���2Q�D0JT
79,���bܙ~e	�Y��`�Z�|`e��ٿw
79,"a\�O�h��4<""��A���5��هϞ�<|��"
79,"��d+Up-�;(��h����\�:Ԭܻ��vkL��)�=�Č):R�'�߁|L��""/)�@�Ӣ�Hd�z�c��W�OS�:\@!�0�_�"
79,"��lS&��G'b,�����G�l��]C[c�"
79,��r�b�J=0��1
79,��;���c��HZ�ȡ�R	S���
79,�Ku��jx�
79,"љ���s$��pY���^�,�+��6.SR��x	�PvNI�"
79,"xC��������9+��6���1�ǽ���{z���""7 e�c2b�?z�b��2���� ("
79,���<����>��1L�;b�Ul���Pc�k���� ���-2�	���?�.�|D_Ȇ�j�2���{�^��=�0�ZԾ��4��U��7�9?��J�x�=ș(<��b3�zr�VzM 䵂�
79,"[�����˃7x�	��<��3����x`�������[�����;Ga:���>��g�c�c�J'ͷ ����Oc�uɊ�? >��,Q"
79,���!
79,&�R�Jh��a�m��6������-s����ۄ5�	�y'>HN�G����I�(D�ce��6����.m�����m����8���s�t����G���NG�9��W��)�l�s��=N�O�8��O
79,��LP�{���
79,4����GXY=���0K����c�l���=�� ��4o~�
79,oj�s�KxT��xȑ�����j H&���
79,�#��ݞgb��rc�h�����P�K��
79,��U
79,"�.��㰏bz���N��xꋏ�8i��v^ۗ\yŝ1�E��q3�,H\���쯼��;�%���J~�}���L��@D���̊���_���B�|�e� �ɯS"
79,��xB
79,�kAi*ti]Q�L�K�/�]!��DFݐ.�� �
79,p�q�����	ʺ��P
79,�&�Iͼ
79,K�~-��
79,"""�.d_�z�3������Jz\�x�8�"
79,B�)�:s�
79,�Mi
79,���H����^2�A�8d�
79,"�ċ�����{(����qf+�""�(53o)�_~�G�tλ�=z��M� d={��� 7~��O���_#�4���"
79,j6|
79,�F�gE�+bU��l*�����w�8�c+b˴S���ӿe��S�}�!��n�>j��/rE��K7���l�����h狼O{�5�w�V�
79,�i;�\�#@�@�1�	H�T�C�j8~0��s��oDKg��u�L���*���9g�8v���r|N�dR
79,�%y��8;l��MY���M�rƱ�����FC>�$
79,�p�����
79,7j�WM������e��[���?�ݮV��(|=�軂�#��}�y�84�Kg� =O�
79,��߸���
79,1H���/r�=�*���~Ѝs��  �� k0[�� 
80,Dedicated with Large Site | Tuning Guide | LiteSpeed Web Server | LiteSpeed Documentation
80,Skip to content
80,LiteSpeed Documentation
80,Dedicated with Large Site
80,Initializing search
80,GitHub
80,Get Started
80,LiteSpeed Web Server
80,LiteSpeed Web ADC
80,LSCache
80,Cloud
80,Licenses
80,Other Products
80,LiteSpeed Documentation
80,GitHub
80,Get Started
80,Get Started
80,Welcome
80,LiteSpeed Web Server
80,LiteSpeed Web Server
80,Overview
80,Trial License Installation
80,Installation
80,Updates
80,Configuration
80,Configuration
80,Configuration
80,reCAPTCHA
80,Security Headers
80,Bubblewrap
80,cgroups
80,OCSP Stapling
80,Command Reference
80,Real-Time Stats
80,Tuning
80,Tuning
80,Shared Hosting Server
80,Dedicated with Large Site
80,Dedicated with Large Site
80,Table of contents
80,Enable Caching
80,Increase Maximum Connections
80,Avoid Forking PHP Worker Processes
80,Increase PHP Running Time
80,Move PHP Session Data
80,Check PHP Info
80,timezonedb
80,xdebug
80,snmp
80,Troubleshooting
80,Troubleshooting
80,WebAdmin Console
80,External Applications
80,External Applications
80,PHP
80,PHP
80,Overview
80,Getting Started
80,Configuration
80,Configuration
80,Controlling LSPHP
80,LSPHP Modes
80,LSPHP Options
80,Advanced
80,Troubleshooting
80,Troubleshooting
80,Overview
80,503
80,Improve PHP Performance
80,How-To
80,Advanced
80,Extensions
80,Perl
80,Perl
80,Overview
80,Perl Configuration
80,Control Panels
80,Control Panels
80,cPanel
80,cPanel
80,Overview
80,Quick Start Guide
80,WHM LiteSpeed Plugin
80,WHM LiteSpeed Plugin
80,Installation
80,Switching Licenses
80,WP Cache Management
80,cPanel Plugin
80,Troubleshooting
80,PHP
80,PHP
80,PHP
80,Per-User php.ini
80,Override Auto-Detected PHP
80,PHP Selector
80,TimeZoneDB
80,CloudLinux
80,LSCache Setup
80,Security
80,Security
80,ModSecurity/WAF
80,DDoS Attack Protection
80,reCAPTCHA Protection
80,WordPress Protection
80,Tuning
80,ZeroConf Clusters
80,How-To
80,How-To
80,Enable or Disable QUIC and HTTP/3
80,GeoLocation Support
80,Cloudflare Issues
80,mod_pagespeed
80,Websocket Proxy
80,Rewrite Rule Proxy
80,PHP Without Timeout
80,LSMCD for MultiUser
80,Compression
80,ECC Certificates Quick Start
80,OCSP Stapling on cPanel
80,Troubleshooting
80,Troubleshooting
80,Apache Migration FAQ
80,403 Error
80,500 Error
80,503 Error
80,Fix High Load
80,Server Issues
80,Enabling Rewrite Logs
80,Toggle Debug Logging
80,Submit a Bug Report
80,Uninstall
80,Plesk
80,Plesk
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,Troubleshooting
80,General
80,403 Error
80,DirectAdmin
80,DirectAdmin
80,Overview
80,Installation
80,Configuration
80,FAQ
80,CyberPanel
80,CyberPanel
80,Overview
80,Enhance
80,Enhance
80,Overview
80,Interworx
80,Interworx
80,Overview
80,Installation
80,LSCache Setup
80,Benchmarking Tips
80,LiteSpeed Web ADC
80,LiteSpeed Web ADC
80,Overview
80,Installation
80,Configuration
80,Configuration
80,Basic Configuration
80,Additional Configuration
80,Cache Configuration
80,Security Configuration
80,ZeroConf Configuration
80,Commands Reference
80,Troubleshooting
80,Frequently Asked Questions
80,LSCache
80,LSCache
80,What is LiteSpeed Cache?
80,Basic Concepts
80,Getting Started
80,Troubleshooting
80,Tips
80,LSCache Plugins
80,LSCache Plugins
80,CS-Cart
80,CS-Cart
80,Overview
80,Installation
80,Troubleshooting
80,FAQ
80,Drupal
80,Drupal
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,FAQ
80,Joomla!
80,Joomla!
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,FAQ
80,Laravel
80,Laravel
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,FAQ
80,Magento
80,Magento
80,Overview
80,Installation
80,Configuration
80,Crawler Script
80,Troubleshooting
80,FAQ
80,MediaWiki
80,MediaWiki
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,FAQ
80,OpenCart
80,OpenCart
80,Overview
80,Installation
80,Configuration
80,Troubleshooting
80,FAQ
80,PrestaShop
80,PrestaShop
80,Overview
80,Installation
80,Configuration
80,Crawler Script
80,Troubleshooting
80,FAQ
80,WordPress
80,WordPress
80,Overview
80,Installation
80,Beginner's Guide
80,Configuration
80,Configuration
80,Dashboard
80,Presets
80,General
80,Cache
80,CDN
80,Image Optimization
80,Page Optimization
80,Database
80,Crawler
80,Toolbox
80,LiteSpeed Options Metabox
80,Troubleshooting
80,Troubleshooting
80,Troubleshooting Guide
80,CSS/JS Issues
80,Media Issues
80,Crawler Issues
80,CDN Support Issues
80,Admin
80,API
80,WordPress CLI
80,Third Party Compatibility
80,FAQ
80,XenForo
80,XenForo
80,Overview
80,Installation and Configuration
80,Troubleshooting
80,FAQ
80,Third Party Plugins
80,Third Party Plugins
80,Craft CMS
80,Craft CMS
80,Overview
80,Installation
80,Troubleshooting
80,FAQ
80,Flarem
80,Flarem
80,Overview
80,JTL-Shop
80,JTL-Shop
80,Overview
80,LSCache Without a Plugin
80,LSCache Without a Plugin
80,Overview
80,Installation
80,Configuration
80,LSCache Developers Guide
80,LSCache Developers Guide
80,Overview
80,Basic Controls
80,Advanced Concepts
80,Usage Examples
80,Cloud
80,Cloud
80,Images
80,Images
80,What are Cloud Images?
80,WordPress
80,CyberPanel
80,Node.js
80,Django
80,Rails
80,Joomla!
80,Drupal
80,Virtuozzo
80,Virtuozzo
80,What is Virtuozzo?
80,WordPress
80,Magento
80,Docker
80,Docker
80,What is Docker?
80,OpenLiteSpeed
80,LiteSpeed Enterprise
80,WordPress + OLS
80,WordPress + LSWS
80,Drupal
80,Magento
80,PrestaShop
80,Node.js OLS Reverse Proxy
80,Kubernetes
80,Kubernetes
80,What is Kubernetes?
80,Installation
80,Usage Considerations
80,Samples
80,Helm Configuration
80,Controller Configuration
80,Load Balancer Configuration
80,Metrics and Prometheus
80,Gateway
80,Advanced Deployments
80,Troubleshooting
80,RunCloud
80,RunCloud
80,Overview
80,WordPress
80,Licenses
80,Licenses
80,Overview
80,Products
80,Products
80,LiteSpeed Web Server
80,LiteSpeed Web Server with CyberPanel
80,LiteSpeed Web ADC
80,LiteSpeed Cache
80,Trial
80,How-To
80,Troubleshooting
80,Billing
80,FAQs
80,Other Products
80,Other Products
80,Overview
80,QUIC.cloud CDN
80,QUIC.cloud CDN
80,What is QUIC.cloud?
80,LiteSpeed Memcached
80,LiteSpeed Memcached
80,What is LiteSpeed Memcached?
80,Getting Started
80,Configuration
80,Configuration
80,Configuration Overview
80,General
80,Replication
80,SASL
80,cPanel Plugin
80,Commands
80,Troubleshooting
80,OpenLiteSpeed
80,OpenLiteSpeed
80,What is OpenLiteSpeed?
80,Table of contents
80,Enable Caching
80,Increase Maximum Connections
80,Avoid Forking PHP Worker Processes
80,Increase PHP Running Time
80,Move PHP Session Data
80,Check PHP Info
80,timezonedb
80,xdebug
80,snmp
80,"Dedicated Server with a Large Site¶ A dedicated server has different challenges than a shared hosting server. With shared hosting, usually you wish to give high performance without allowing a single user to monopolize server resources. However, on a dedicated server with a relatively large site, you want to maximize the performance without limiting the usage. Most of the default LiteSpeed WebAdmin Console settings are suitable for both scenarios, but there are a few exceptions. Here are some settings that you can adjust to push the maximum capacity. Enable Caching¶ Caching is the key to improving site performance. It is fundamental. If possible, you should always enable caching. If there is an LSCache plugin available for your application, use it. If a plugin is not available, you may also consider configuring LSCache through rewrite rules. Be sure to test for a cache hit before undertaking any benchmark testing. Increase Maximum Connections¶ With caching enabled to avoid PHP and MySQL processes, the site will undoubtedly be faster. However, there will still sometimes be uncached pages, such as shopping cart checkout pages, or other pages that have expired in cache. For this content, the performance of the application is at the mercy of PHP performance. The PHP SuEXEC Max CONN setting controls how many concurrent PHP processes are allowed. The default of 10 may not be enough for a dedicated server. How high you increase it depends on the size of the server, especially in terms of RAM and CPU. While you could technically set it as high as 1000, higher is not necessarily better. An 8-core physical server, for example, will likely not benefit from any more than fifty to eighty processes. Avoid Forking PHP Worker Processes¶ With shared hosting, LiteSpeed Web Server dynamically forks PHP worker processes in order to save server resources. The server spawns new worker groups or forks a child PHP worker process as necessary to serve incoming requests. While this preserves server resources, it penalizes performance by slowing down PHP’s initial start-up response time. This trade-off makes sense for shared hosting, but you may not wish it on a dedicated server. Set the LSAPI_AVOID_FORK environment variable to 1 in order to keep the PHP worker process alive for a longer period of time. Doing so ensures that there will always be a PHP worker available to serve incoming requests, and no delay in PHP response time."
80,"Tip You can set the environment variable within WebAdmin Console in the PHP tab, Environment Variables field."
80,"Increase PHP Running Time¶ The purpose of Max Idle Time is to specify the number of seconds before an external application is stopped by the server. To make PHP live longer, simply increase Max Idle Time to a large number, such as 86400, in the PHP tab, or at the External App level. Move PHP Session Data¶ If the server has enough free memory, you can move PHP sessions and opcode cache disk storage to /dev/shm. Check PHP Info¶ Take a look at your phpinfo.php page, and check these few modules: timezonedb¶ It’s recommended to have the timezonedb module enabled for PHP. You may not notice the absence of timezonedb on your dedicated server, but enabling it can keep PHP from needing to scan hundreds of directories. xdebug¶ Make sure the PHP xdebugmodule is disabled if you don't need it. xdebug is for developer IDE integration only and should not be installed by default. snmp¶ Same for snmp. The snmp module will scan and parse available MIB files, but not everyone needs snmp support."
80,"Last update: August 4, 2023"
80,Copyright © 2013-2023 LiteSpeed Technologies Inc.
80,Made with
80,Material for MkDocs
81,"MySQL Compatibility | PingCAP DocsHomeTiDB CloudTiDBPlaygroundForumContact UsLanguage​​Sign InTry Freev7.5Docs HomeAbout TiDBTiDB IntroductionTiDB 7.5 Release NotesFeaturesMySQL CompatibilityTiDB LimitationsCreditsRoadmapQuick StartDevelopDeployMigrateIntegrateMaintainMonitor and AlertTroubleshootPerformance TuningTutorialsTiDB ToolsReferenceFAQsRelease NotesGlossaryMySQL CompatibilityTiDB is highly compatible with the MySQL protocol and the common features and syntax of MySQL 5.7 and MySQL 8.0. The ecosystem tools for MySQL (PHPMyAdmin, Navicat, MySQL Workbench, DBeaver and more) and the MySQL client can be used for TiDB.TiDB is highly compatible with the MySQL protocol and the common features and syntax of MySQL 5.7 and MySQL 8.0. The ecosystem tools for MySQL (PHPMyAdmin, Navicat, MySQL Workbench, DBeaver and more) and the MySQL client can be used for TiDB.However, some features of MySQL are not supported in TiDB. This could be because there is now a better way to solve the problem (such as the use of JSON instead of XML functions) or a lack of current demand versus effort required (such as stored procedures and functions). Additionally, some features might be difficult to implement in a distributed system.It's important to note that TiDB does not support the MySQL replication protocol. Instead, specific tools are provided to replicate data with MySQL:Replicate data from MySQL: TiDB Data Migration (DM) is a tool that supports full data migration and incremental data replication from MySQL or MariaDB into TiDB.Replicate data to MySQL: TiCDC is a tool for replicating the incremental data of TiDB by pulling TiKV change logs. TiCDC uses the MySQL sink to replicate the incremental data of TiDB to MySQL.NoteThis page describes general differences between MySQL and TiDB. For more information on compatibility with MySQL in the areas of security and pessimistic transaction mode, refer to the dedicated pages on Security and Pessimistic Transaction Mode.NoteFor information about transaction differences between MySQL and TiDB, see Pessimistic Transaction Mode.You can try out TiDB features on TiDB Playground.Unsupported featuresStored procedures and functionsTriggersEventsUser-defined functionsFULLTEXT syntax and indexes #1793SPATIAL (also known as GIS/GEOMETRY) functions, data types and indexes #6347Character sets other than ascii, latin1, binary, utf8, utf8mb4, and gbk.SYS schemaOptimizer traceXML FunctionsX-Protocol #1109Column-level privileges #9766XA syntax (TiDB uses a two-phase commit internally, but this is not exposed via an SQL interface)CREATE TABLE tblName AS SELECT stmt syntax #4754CHECK TABLE syntax #4673CHECKSUM TABLE syntax #1895REPAIR TABLE syntaxOPTIMIZE TABLE syntaxHANDLER statementCREATE TABLESPACE statement""Session Tracker: Add GTIDs context to the OK packet""Descending Index #2519SKIP LOCKED syntax #18207Lateral derived tables #40328Differences from MySQLAuto-increment IDIn TiDB, the auto-incremental column values (IDs) are globally unique and incremental within a single TiDB server. To make the IDs incremental among multiple TiDB servers, you can use the AUTO_INCREMENT MySQL compatibility mode. However, the IDs are not necessarily allocated sequentially, so it is recommended that you avoid mixing default and custom values to prevent encountering the Duplicated Error message.You can use the tidb_allow_remove_auto_inc system variable to allow or forbid removing the AUTO_INCREMENT column attribute. To remove the column attribute, use the ALTER TABLE MODIFY or ALTER TABLE CHANGE syntax.TiDB does not support adding the AUTO_INCREMENT column attribute, and once removed, it cannot be recovered.For TiDB v6.6.0 and earlier versions, auto-increment columns in TiDB behave the same as in MySQL InnoDB, requiring them to be primary keys or index prefixes. Starting from v7.0.0, TiDB removes this restriction, allowing for more flexible table primary key definitions. #40580For more details, see AUTO_INCREMENT.NoteIf you do not specify a primary key when creating a table, TiDB uses _tidb_rowid to identify the row. The allocation of this value shares an allocator with the auto-increment column (if such a column exists). If you specify an auto-increment column as the primary key, TiDB uses this column to identify the row. In this situation, the following situation might occur:mysql> CREATE TABLE t(id INT UNIQUE KEY AUTO_INCREMENT);"
81,"Query OK, 0 rows affected (0.05 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,mysql> INSERT INTO t VALUES();
81,"Query OK, 1 rows affected (0.00 sec)"
81,"mysql> SELECT _tidb_rowid, id FROM t;"
81,+-------------+------+
81,| _tidb_rowid | id
81,+-------------+------+
81,2 |
81,1 |
81,4 |
81,3 |
81,6 |
81,5 |
81,+-------------+------+
81,3 rows in set (0.01 sec)
81,"As shown, because of the shared allocator, the id increments by 2 each time. This behavior changes in MySQL compatibility mode, where there is no shared allocator and therefore no skipping of numbers.NoteThe AUTO_INCREMENT attribute might cause hotspot in production environments. See Troubleshoot HotSpot Issues for details. It is recommended to use AUTO_RANDOM instead.NoteThe AUTO_INCREMENT attribute might cause hotspot in production environments. See Troubleshoot HotSpot Issues for details. It is recommended to use AUTO_RANDOM instead.Performance schemaTiDB utilizes a combination of Prometheus and Grafana for storing and querying performance monitoring metrics. In TiDB, performance schema tables do not return any results.To check performance metrics in TiDB Cloud, you can either check the cluster overview page on the TiDB Cloud console or use third-party monitoring integrations. Performance schema tables return empty results in TiDB.Query Execution PlanThe output format, content, and privilege settings of Query Execution Plan (EXPLAIN/EXPLAIN FOR) in TiDB differ significantly from those in MySQL.In TiDB, the MySQL system variable optimizer_switch is read-only and has no effect on query plans. Although optimizer hints can be used in similar syntax to MySQL, the available hints and their implementation might differ.For more information, refer to Understand the Query Execution Plan.Built-in functionsTiDB supports most of the built-in functions in MySQL, but not all. You can use the statement SHOW BUILTINS to get a list of the available functions.For more information, refer to the TiDB SQL Grammar.DDL operationsIn TiDB, all supported DDL changes can be performed online. However, there are some major restrictions on DDL operations in TiDB compared to MySQL:When using a single ALTER TABLE statement to alter multiple schema objects (such as columns or indexes) of a table, specifying the same object in multiple changes is not supported. For example, if you execute the ALTER TABLE t1 MODIFY COLUMN c1 INT, DROP COLUMN c1 command, the Unsupported operate same column/index error is output.It is not supported to modify multiple TiDB-specific schema objects using a single ALTER TABLE statement, such as TIFLASH REPLICA, SHARD_ROW_ID_BITS, and AUTO_ID_CACHE.TiDB does not support the changes of some data types using ALTER TABLE. For example, TiDB does not support the change from the DECIMAL type to the DATE type. If a data type change is unsupported, TiDB reports the Unsupported modify column: type %d not match origin %d error. Refer to ALTER TABLE for more details.The ALGORITHM={INSTANT,INPLACE,COPY} syntax functions only as an assertion in TiDB, and does not modify the ALTER algorithm. See ALTER TABLE for further details.Adding/Dropping the primary key of the CLUSTERED type is unsupported. For more details about the primary key of the CLUSTERED type, refer to clustered index.Different types of indexes (HASH|BTREE|RTREE|FULLTEXT) are not supported, and will be parsed and ignored when specified.TiDB supports HASH, RANGE, LIST, and KEY partitioning types. Currently, the KEY partition type does not support partition statements with an empty partition column list. For an unsupported partition type, TiDB returns Warning: Unsupported partition type %s, treat as normal table, where %s is the specific unsupported partition type.Range, Range COLUMNS, List, and List COLUMNS partitioned tables support ADD, DROP, TRUNCATE, and REORGANIZE operations. Other partition operations are ignored.Hash and Key partitioned tables support ADD, COALESCE, and TRUNCATE operations. Other partition operations are ignored.The following syntaxes are not supported for partitioned tables:SUBPARTITION{CHECK|OPTIMIZE|REPAIR|IMPORT|DISCARD|REBUILD} PARTITIONFor more details on partitioning, see Partitioning.Analyzing tablesIn TiDB, Statistics Collection differs from MySQL in that it completely rebuilds the statistics for a table, making it a more resource-intensive operation that takes longer to complete. In contrast, MySQL/InnoDB performs a relatively lightweight and short-lived operation.For more information, refer to ANALYZE TABLE.Limitations of SELECT syntaxTiDB does not support the following SELECT syntax:SELECT ... INTO @variableSELECT .. GROUP BY expr does not imply GROUP BY expr ORDER BY expr as it does in MySQL 5.7.For more details, see the SELECT statement reference.UPDATE statementSee the UPDATE statement reference.ViewsViews in TiDB are not updatable and do not support write operations such as UPDATE, INSERT, and DELETE.Temporary tablesFor more information, see Compatibility between TiDB local temporary tables and MySQL temporary tables.Character sets and collationsTo learn about the character sets and collations supported by TiDB, see Character Set and Collation Overview.For information on the MySQL compatibility of the GBK character set, refer to GBK compatibility .TiDB inherits the character set used in the table as the national character set.Storage enginesTiDB allows for tables to be created with alternative storage engines. Despite this, the metadata as described by TiDB is for the InnoDB storage engine as a way to ensure compatibility.To specify a storage engine using the --store option, it is necessary to start the TiDB server. This storage engine abstraction feature is similar to MySQL.SQL modesTiDB supports most SQL modes:The compatibility modes, such as Oracle and PostgreSQL are parsed but ignored. Compatibility modes are deprecated in MySQL 5.7 and removed in MySQL 8.0.The ONLY_FULL_GROUP_BY mode has minor semantic differences from MySQL 5.7.The NO_DIR_IN_CREATE and NO_ENGINE_SUBSTITUTION SQL modes in MySQL are accepted for compatibility, but are not applicable to TiDB.Default differencesTiDB has default differences when compared with MySQL 5.7 and MySQL 8.0:Default character set:TiDB’s default value is utf8mb4.MySQL 5.7’s default value is latin1.MySQL 8.0’s default value is utf8mb4.Default collation:TiDB’s default collation is utf8mb4_bin.MySQL 5.7’s default collation is utf8mb4_general_ci.MySQL 8.0’s default collation is utf8mb4_0900_ai_ci.Default SQL mode:TiDB’s default SQL mode includes these modes: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION.MySQL’s default SQL mode:The default SQL mode in MySQL 5.7 is the same as TiDB.The default SQL mode in MySQL 8.0 includes these modes: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION.Default value of lower_case_table_names:The default value in TiDB is 2, and only 2 is currently supported.MySQL defaults to the following values:On Linux: 0. It means that table and database names are stored on disk according to the letter case specified in the CREATE TABLE or CREATE DATABASE statement. Name comparisons are case-sensitive.On Windows: 1. It means table names are stored in lowercase on disk, and name comparisons are not case-sensitive. MySQL converts all table names to lowercase on storage and lookup. This behavior also applies to database names and table aliases.On macOS: 2. It means table and database names are stored on disk according to the letter case specified in the CREATE TABLE or CREATE DATABASE statement, but MySQL converts them to lowercase on lookup. Name comparisons are not case-sensitive.Default value of explicit_defaults_for_timestamp:The default value in TiDB is ON, and only ON is currently supported.MySQL defaults to the following values:For MySQL 5.7: OFF.For MySQL 8.0: ON.Date and TimeTiDB supports named timezones with the following considerations:TiDB uses all the timezone rules presently installed in the system for calculation, typically the tzdata package. This makes it possible to use all timezone names without needing to import timezone table data. Importing timezone table data will not change the calculation rules.Currently, MySQL uses the local timezone by default, then relies on the current timezone rules built into the system (for example, when daylight savings time begins) for calculation. Without importing timezone table data, MySQL cannot specify the timezone by name.Type system differencesThe following column types are supported by MySQL but not by TiDB:SQL_TSI_* (includes SQL_TSI_MONTH, SQL_TSI_WEEK, SQL_TSI_DAY, SQL_TSI_HOUR, SQL_TSI_MINUTE, and SQL_TSI_SECOND, but excludes SQL_TSI_YEAR)Incompatibility due to deprecated featuresTiDB does not implement specific features deprecated in MySQL, including:Specifying precision for floating-point types. MySQL 8.0 deprecates this feature, and it is recommended to use the DECIMAL type instead.The ZEROFILL attribute. MySQL 8.0 deprecates this feature, and it is recommended to pad numeric values in your application instead.CREATE RESOURCE GROUP, DROP RESOURCE GROUP, and ALTER RESOURCE GROUP statementsThe following statements for creating, modifying, and dropping resource groups have different supported parameters than MySQL. For details, see the following documents:CREATE RESOURCE GROUPDROP RESOURCE GROUPALTER RESOURCE GROUPWas this page helpful?YesNoDownload PDFRequest docs changesAsk questions on DiscordPlaygroundNewOne-stop & interactive experience of TiDB's capabilities WITHOUT registration.What's on this pageUnsupported featuresDifferences from MySQLAuto-increment IDPerformance schemaQuery Execution PlanBuilt-in functionsDDL operationsAnalyzing tablesLimitations of SELECT syntaxUPDATE statementViewsTemporary tablesCharacter sets and collationsStorage enginesSQL modesDefault differencesDate and TimeType system differencesIncompatibility due to deprecated featuresCREATE RESOURCE GROUP, DROP RESOURCE GROUP, and ALTER RESOURCE GROUP statementsWhat's on this pageProductsTiDBTiDB DedicatedTiDB ServerlessPricingGet DemoGet StartedEcosystemIntegrationsTiKVTiFlashOSS InsightResourcesTiDB Cloud RoadmapTiDB RoadmapFAQsBlogEducationSupportDiscordForumSlackSupport PortalCompanyAbout UsCareersLegalContact Us© 2024 PingCAP. All Rights Reserved.Privacy Policy."
82,"Express Tutorial Part 3: Using a Database (with Mongoose) - Learn web development | MDNSkip to main contentSkip to searchSkip to select languageMDN Web DocsOpen main menuReferencesReferencesOverview / Web TechnologyWeb technology reference for developersHTMLStructure of content on the webCSSCode used to describe document styleJavaScriptGeneral-purpose scripting languageHTTPProtocol for transmitting web resourcesWeb APIsInterfaces for building web applicationsWeb ExtensionsDeveloping extensions for web browsersWeb TechnologyWeb technology reference for developersGuidesGuidesOverview / MDN Learning AreaLearn web developmentMDN Learning AreaLearn web developmentHTMLLearn to structure web content with HTMLCSSLearn to style content using CSSJavaScriptLearn to run scripts in the browserAccessibilityLearn to make the web accessible to allPlusPlusOverviewA customized MDN experienceAI Help (beta)Get real-time assistance and supportUpdatesAll browser compatibility updates at a glanceDocumentationLearn how to use MDN PlusFAQFrequently asked questions about MDN PlusCurriculumNewBlogPlayAI Help BetaSearch MDNClear search inputSearchThemeLog inSign up for freeGuidesServer-side website programmingExpress web framework (Node.js/JavaScript)Express Tutorial Part 3: Using a Database (with Mongoose)Article ActionsEnglish (US)Filter sidebarClear filter inputIn this articleOverviewDesigning the LocalLibrary modelsMongoose primerSetting up the MongoDB databaseInstall MongooseConnect to MongoDBDefining the LocalLibrary SchemaTesting — create some itemsSummarySee alsoComplete beginners start here!Getting started with the webGetting started with the webInstalling basic softwareWhat will your website look like?Dealing with filesHTML basicsCSS basicsJavaScript basicsPublishing your websiteHow the web worksHTML — Structuring the webIntroduction to HTMLIntroduction to HTMLGetting started with HTMLWhat's in the head? Metadata in HTMLHTML text fundamentalsCreating hyperlinksAdvanced text formattingDocument and website structureDebugging HTMLMarking up a letterStructuring a page of contentMultimedia and embeddingMultimedia and embeddingImages in HTMLVideo and audio contentFrom object to iframe — other embedding technologiesAdding vector graphics to the webResponsive imagesMozilla splash pageHTML tablesHTML tablesHTML table basicsHTML table advanced features and accessibilityStructuring planet dataCSS — Styling the webCSS first stepsCSS first stepsWhat is CSS?Getting started with CSSHow CSS is structuredHow CSS worksStyling a biography pageCSS building blocksCSS building blocksCSS selectorsType, class, and ID selectorsAttribute selectorsPseudo-classes and pseudo-elementsCombinatorsCascade, specificity, and inheritanceCascade layersThe box modelBackgrounds and bordersHandling different text directionsOverflowing contentCSS values and unitsSizing items in CSSImages, media, and form elementsStyling tablesDebugging CSSOrganizing your CSSFundamental CSS comprehensionCreating fancy letterheaded paperA cool-looking boxStyling textCSS styling textFundamental text and font stylingStyling listsStyling linksWeb fontsTypesetting a community school homepageCSS layoutCSS layoutIntroduction to CSS layoutNormal FlowFlexboxGridsFloatsPositioningMultiple-column layoutResponsive designBeginner's guide to media queriesLegacy layout methodsSupporting older browsersFundamental layout comprehensionJavaScript — Dynamic client-side scriptingJavaScript first stepsJavaScript first stepsWhat is JavaScript?A first splash into JavaScriptWhat went wrong? Troubleshooting JavaScriptStoring the information you need — VariablesBasic math in JavaScript — numbers and operatorsHandling text — strings in JavaScriptUseful string methodsArraysSilly story generatorJavaScript building blocksJavaScript building blocksMaking decisions in your code — conditionalsLooping codeFunctions — reusable blocks of codeBuild your own functionFunction return valuesIntroduction to eventsImage galleryIntroducing JavaScript objectsIntroducing JavaScript objectsJavaScript object basicsObject prototypesObject-oriented programmingClasses in JavaScriptWorking with JSONObject building practiceAdding features to our bouncing balls demoAsynchronous JavaScriptAsynchronous JavaScriptIntroducing asynchronous JavaScriptHow to use promisesHow to implement a promise-based APIIntroducing workersSequencing animationsClient-side web APIsClient-side web APIsIntroduction to web APIsManipulating documentsFetching data from the serverThird-party APIsDrawing graphicsVideo and Audio APIsClient-side storageWeb forms — Working with user dataCore forms learning pathwayWeb form building blocksYour first formHow to structure a web formBasic native form controlsThe HTML5 input typesOther form controlsStyling web formsAdvanced form stylingUI pseudo-classesClient-side form validationSending form dataAdvanced forms articlesHow to build custom form controlsSending forms through JavaScriptCSS property compatibility table for form controlsHTML forms in legacy browsersAccessibility — Make the web usable by everyoneAccessibility guidesAccessibilityWhat is accessibility?HTML: A good basis for accessibilityCSS and JavaScript accessibility best practicesWAI-ARIA basicsAccessible multimediaMobile accessibilityAssessment: Accessibility troubleshootingPerformance — Making websites fast and responsivePerformance guidesWeb performanceThe ""why"" of web performanceWhat is web performance?Perceived performanceMeasuring performanceMultimedia: ImagesMultimedia: videoJavaScript performance optimizationHTML performance optimizationCSS performance optimizationThe business case for web performanceMathML — Writing mathematics with MathMLMathML first stepsMathML first stepsGetting started with MathMLMathML Text ContainersMathML fractions and rootsMathML scripted elementsMathML tablesThree famous mathematical formulasGames — Developing games for the webGuides and tutorialsIntroduction to game development for the WebTechniques for game developmentTutorialsPublishing gamesTools and testingClient-side web development toolsUnderstanding client-side web development toolsClient-side tooling overviewCommand line crash coursePackage management basicsIntroducing a complete toolchainDeploying our appIntroduction to client-side frameworksIntroduction to client-side frameworksFramework main featuresReactGetting started with ReactBeginning our React todo listComponentizing our React appReact interactivity: Events and stateReact interactivity: Editing, filtering, conditional renderingAccessibility in ReactReact resourcesEmberGetting started with EmberEmber app structure and componentizationEmber interactivity: Events, classes and stateEmber Interactivity: Footer functionality, conditional renderingRouting in EmberEmber resources and troubleshootingVueGetting started with VueCreating our first Vue componentRendering a list of Vue componentsAdding a new todo form: Vue events, methods, and modelsStyling Vue components with CSSUsing Vue computed propertiesVue conditional rendering: editing existing todosVue refs and lifecycle methods for focus managementVue resourcesSvelteGetting started with SvelteStarting our Svelte to-do list appDynamic behavior in Svelte: working with variables and propsComponentizing our Svelte appAdvanced Svelte: Reactivity, lifecycle, accessibilityWorking with Svelte storesTypeScript support in SvelteDeployment and next stepsAngularGetting started with AngularBeginning our Angular todo list appStyling our Angular appCreating an item componentFiltering our to-do itemsBuilding Angular applications and further resourcesGit and GitHubGit and GitHubCross browser testingCross browser testingIntroduction to cross-browser testingStrategies for carrying out testingHandling common HTML and CSS problemsHandling common JavaScript problemsHandling common accessibility problemsImplementing feature detectionIntroduction to automated testingSetting up your own test automation environmentServer-side website programmingFirst stepsServer-side website programming first stepsIntroduction to the server sideClient-Server OverviewServer-side web frameworksWebsite securityDjango web framework (Python)Django Web Framework (Python)Django introductionSetting up a Django development environmentDjango Tutorial: The Local Library websiteDjango Tutorial Part 2: Creating a skeleton websiteDjango Tutorial Part 3: Using modelsDjango Tutorial Part 4: Django admin siteDjango Tutorial Part 5: Creating our home pageDjango Tutorial Part 6: Generic list and detail viewsDjango Tutorial Part 7: Sessions frameworkDjango Tutorial Part 8: User authentication and permissionsDjango Tutorial Part 9: Working with formsDjango Tutorial Part 10: Testing a Django web applicationDjango Tutorial Part 11: Deploying Django to productionDjango web application securityAssessment: DIY Django mini blogExpress Web Framework (Node.js/JavaScript)Express web framework (Node.js/JavaScript)Express/Node introductionSetting up a Node development environmentExpress Tutorial: The Local Library websiteExpress Tutorial Part 2: Creating a skeleton websiteExpress Tutorial Part 3: Using a Database (with Mongoose)Express Tutorial Part 4: Routes and controllersExpress Tutorial Part 5: Displaying library dataExpress Tutorial Part 6: Working with formsExpress Tutorial Part 7: Deploying to productionFurther resourcesCommon questionsCommon questionsUse HTML to solve common problemsUse CSS to solve common problemsSolve common problems in your JavaScript codeWeb mechanicsTools and setupDesign and accessibilityIn this articleOverviewDesigning the LocalLibrary modelsMongoose primerSetting up the MongoDB databaseInstall MongooseConnect to MongoDBDefining the LocalLibrary SchemaTesting — create some itemsSummarySee alsoExpress Tutorial Part 3: Using a Database (with Mongoose)"
82,Previous
82,Overview: Express Nodejs
82,Next
82,"This article briefly introduces databases, and how to use them with Node/Express apps. It then goes on to show how we can use Mongoose to provide database access for the LocalLibrary website. It explains how object schema and models are declared, the main field types, and basic validation. It also briefly shows a few of the main ways in which you can access model data."
82,Prerequisites:
82,Express Tutorial Part 2: Creating a skeleton website
82,Objective:
82,To be able to design and create your own models using Mongoose.
82,"OverviewLibrary staff will use the Local Library website to store information about books and borrowers, while library members will use it to browse and search for books, find out whether there are any copies available, and then reserve or borrow them. In order to store and retrieve information efficiently, we will store it in a database."
82,"Express apps can use many different databases, and there are several approaches you can use for performing Create, Read, Update and Delete (CRUD) operations. This tutorial provides a brief overview of some of the available options and then goes on to show in detail the particular mechanisms selected.What databases can I use?Express apps can use any database supported by Node (Express itself doesn't define any specific additional behavior/requirements for database management). There are many popular options, including PostgreSQL, MySQL, Redis, SQLite, and MongoDB."
82,"When choosing a database, you should consider things like time-to-productivity/learning curve, performance, ease of replication/backup, cost, community support, etc. While there is no single ""best"" database, almost any of the popular solutions should be more than acceptable for a small-to-medium-sized site like our Local Library."
82,For more information on the options see Database integration (Express docs).What is the best way to interact with a database?There are two common approaches for interacting with a database:
82,"Using the databases' native query language, such as SQL."
82,"Using an Object Relational Mapper (""ORM""). An ORM represents the website's data as JavaScript objects, which are then mapped to the underlying database. Some ORMs are tied to a specific database, while others provide a database-agnostic backend."
82,"The very best performance can be gained by using SQL, or whatever query language is supported by the database. ODM's are often slower because they use translation code to map between objects and the database format, which may not use the most efficient database queries (this is particularly true if the ODM supports different database backends, and must make greater compromises in terms of what database features are supported)."
82,The benefit of using an ORM is that programmers can continue to think in terms of JavaScript objects rather than database semantics — this is particularly true if you need to work with different databases (on either the same or different websites). They also provide an obvious place to perform data validation.
82,"Note: Using ODM/ORMs often results in lower costs for development and maintenance! Unless you're very familiar with the native query language or performance is paramount, you should strongly consider using an ODM."
82,What ORM/ODM should I use?There are many ODM/ORM solutions available on the npm package manager site (check out the odm and orm tags for a subset!).
82,A few solutions that were popular at the time of writing are:
82,Mongoose: Mongoose is a MongoDB object modeling tool designed to work in an asynchronous environment.
82,"Waterline: An ORM extracted from the Express-based Sails web framework. It provides a uniform API for accessing numerous different databases, including Redis, MySQL, LDAP, MongoDB, and Postgres."
82,"Bookshelf: Features both promise-based and traditional callback interfaces, providing transaction support, eager/nested-eager relation loading, polymorphic associations, and support for one-to-one, one-to-many, and many-to-many relations. Works with PostgreSQL, MySQL, and SQLite3."
82,"Objection: Makes it as easy as possible to use the full power of SQL and the underlying database engine (supports SQLite3, Postgres, and MySQL)."
82,"Sequelize is a promise-based ORM for Node.js and io.js. It supports the dialects PostgreSQL, MySQL, MariaDB, SQLite, and MSSQL and features solid transaction support, relations, read replication and more."
82,"Node ORM2 is an Object Relationship Manager for NodeJS. It supports MySQL, SQLite, and Progress, helping to work with the database using an object-oriented approach."
82,"GraphQL: Primarily a query language for restful APIs, GraphQL is very popular, and has features available for reading data from databases."
82,"As a general rule, you should consider both the features provided and the ""community activity"" (downloads, contributions, bug reports, quality of documentation, etc.) when selecting a solution. At the time of writing Mongoose is by far the most popular ODM, and is a reasonable choice if you're using MongoDB for your database.Using Mongoose and MongoDB for the LocalLibraryFor the Local Library example (and the rest of this topic) we're going to use the Mongoose ODM to access our library data. Mongoose acts as a front end to MongoDB, an open source NoSQL database that uses a document-oriented data model. A ""collection"" of ""documents"" in a MongoDB database is analogous to a ""table"" of ""rows"" in a relational database."
82,"This ODM and database combination is extremely popular in the Node community, partially because the document storage and query system looks very much like JSON, and is hence familiar to JavaScript developers."
82,"Note: You don't need to know MongoDB in order to use Mongoose, although parts of the Mongoose documentation are easier to use and understand if you are already familiar with MongoDB."
82,"The rest of this tutorial shows how to define and access the Mongoose schema and models for the LocalLibrary website example.Designing the LocalLibrary modelsBefore you jump in and start coding the models, it's worth taking a few minutes to think about what data we need to store and the relationships between the different objects."
82,"We know that we need to store information about books (title, summary, author, genre, ISBN) and that we might have multiple copies available (with globally unique ids, availability statuses, etc.). We might need to store more information about the author than just their name, and there might be multiple authors with the same or similar names. We want to be able to sort information based on the book title, author, genre, and category."
82,"When designing your models it makes sense to have separate models for every ""object"" (a group of related information). In this case some obvious candidates for these models are books, book instances, and authors."
82,"You might also want to use models to represent selection-list options (e.g. like a drop-down list of choices), rather than hard-coding the choices into the website itself — this is recommended when all the options aren't known up front or may change. A good example is a genre (e.g. fantasy, science fiction, etc.)."
82,"Once we've decided on our models and fields, we need to think about the relationships between them."
82,"With that in mind, the UML association diagram below shows the models we'll define in this case (as boxes). As discussed above, we've created models for the book (the generic details of the book), book instance (status of specific physical copies of the book available in the system), and author. We have also decided to have a model for the genre so that values can be created dynamically. We've decided not to have a model for the BookInstance:status — we will hard code the acceptable values because we don't expect these to change. Within each of the boxes, you can see the model name, the field names and types, and also the methods and their return types."
82,"The diagram also shows the relationships between the models, including their multiplicities. The multiplicities are the numbers on the diagram showing the numbers (maximum and minimum) of each model that may be present in the relationship. For example, the connecting line between the boxes shows that Book and a Genre are related. The numbers close to the Book model show that a Genre must have zero or more Books (as many as you like), while the numbers on the other end of the line next to the Genre show that a book can have zero or more associated Genres."
82,"Note: As discussed in our Mongoose primer below it is often better to have the field that defines the relationship between the documents/models in just one model (you can still find the reverse relationship by searching for the associated _id in the other model). Below we have chosen to define the relationship between Book/Genre and Book/Author in the Book schema, and the relationship between the Book/BookInstance in the BookInstance Schema. This choice was somewhat arbitrary — we could equally well have had the field in the other schema."
82,"Note: The next section provides a basic primer explaining how models are defined and used. As you read it, consider how we will construct each of the models in the diagram above."
82,Database APIs are asynchronous
82,"Database methods to create, find, update, or delete records are asynchronous."
82,"What this means is that the methods return immediately, and the code to handle the success or failure of the method runs at a later time when the operation completes."
82,"Other code can execute while the server is waiting for the database operation to complete, so the server can remain responsive to other requests."
82,JavaScript has a number of mechanisms for supporting asynchronous behavior.
82,Historically JavaScript relied heavily on passing callback functions to asynchronous methods to handle the success and error cases.
82,In modern JavaScript callbacks have largely been replaced by Promises.
82,Promises are objects that are (immediately) returned by an asynchronous method that represent its future state.
82,"When the operation completes, the promise object is ""settled"", and resolves an object that represents the result of the operation or an error."
82,"There are two main ways you can use promises to run code when a promise is settled, and we highly recommend that you read How to use promises for a high level overview of both approaches."
82,"In this tutorial, we'll primarily be using await to wait on promise completion within an async function, because this leads to more readable and understandable asynchronous code."
82,"The way this approach works is that you use the async function keyword to mark a function as asynchronous, and then inside that function apply await to any method that returns a promise."
82,When the asynchronous function is executed its operation is paused at the first await method until the promise settles.
82,From the perspective of the surrounding code the asynchronous function then returns and the code after it is able to run.
82,"Later when the promise settles, the await method inside the asynchronous function returns with the result, or an error is thrown if the promise was rejected."
82,"The code in the asynchronous function then executes until either another await is encountered, at which point it will pause again, or until all the code in the function has been run."
82,You can see how this works in the example below.
82,myFunction() is an asynchronous function that is called within a try...catch block.
82,"When myFunction() is run, code execution is paused at methodThatReturnsPromise() until the promise resolves, at which point the code continues to aFunctionThatReturnsPromise() and waits again."
82,"The code in the catch block runs if an error is thrown in the asynchronous function, and this will happen if the promise returned by either of the methods is rejected."
82,jsasync function myFunction {
82,// ...
82,await someObject.methodThatReturnsPromise();
82,// ...
82,await aFunctionThatReturnsPromise();
82,// ...
82,try {
82,// ...
82,myFunction();
82,// ...
82,} catch (e) {
82,// error handling code
82,The asynchronous methods above are run in sequence.
82,If the methods don't depend on each other then you can run them in parallel and finish the whole operation more quickly.
82,"This is done using the Promise.all() method, which takes an iterable of promises as input and returns a single Promise."
82,"This returned promise fulfills when all of the input's promises fulfill, with an array of the fulfillment values."
82,"It rejects when any of the input's promises rejects, with this first rejection reason."
82,The code below shows how this works.
82,"First, we have two functions that return promises."
82,We await on both of them to complete using the promise returned by Promise.all().
82,"Once they both complete await returns and the results array is populated,"
82,"the function then continues to the next await, and waits until the promise returned by anotherFunctionThatReturnsPromise() is settled."
82,You would call the myFunction() in a try...catch block to catch any errors.
82,jsasync function myFunction {
82,// ...
82,"const [resultFunction1, resultFunction2] = await Promise.all(["
82,"functionThatReturnsPromise1(),"
82,functionThatReturnsPromise2()
82,]);
82,// ...
82,await anotherFunctionThatReturnsPromise(resultFunction1);
82,"Promises with await/async allow both flexible and ""comprehensible"" control over asynchronous execution!Mongoose primerThis section provides an overview of how to connect Mongoose to a MongoDB database, how to define a schema and a model, and how to make basic queries."
82,Note: This primer is heavily influenced by the Mongoose quick start on npm and the official documentation.
82,Installing Mongoose and MongoDB
82,Mongoose is installed in your project (package.json) like any other dependency — using npm.
82,"To install it, use the following command inside your project folder:"
82,bashnpm install mongoose
82,"Installing Mongoose adds all its dependencies, including the MongoDB database driver, but it does not install MongoDB itself. If you want to install a MongoDB server then you can download installers from here for various operating systems and install it locally. You can also use cloud-based MongoDB instances."
82,"Note: For this tutorial, we'll be using the MongoDB Atlas cloud-based database as a service free tier to provide the database. This is suitable for development and makes sense for the tutorial because it makes ""installation"" operating system independent (database-as-a-service is also one approach you might use for your production database)."
82,Connecting to MongoDB
82,Mongoose requires a connection to a MongoDB database.
82,You can require() and connect to a locally hosted database with mongoose.connect() as shown below (for the tutorial we'll instead connect to an internet-hosted database).
82,js// Import the mongoose module
82,"const mongoose = require(""mongoose"");"
82,// Set `strictQuery: false` to globally opt into filtering by properties that aren't in the schema
82,// Included because it removes preparatory warnings for Mongoose 7.
82,// See: https://mongoosejs.com/docs/migrating_to_6.html#strictquery-is-removed-and-replaced-by-strict
82,"mongoose.set(""strictQuery"", false);"
82,// Define the database URL to connect to.
82,"const mongoDB = ""mongodb://127.0.0.1/my_database"";"
82,"// Wait for database to connect, logging an error if there is a problem"
82,main().catch((err) => console.log(err));
82,async function main() {
82,await mongoose.connect(mongoDB);
82,"Note: As discussed in the Database APIs are asynchronous section, here we await on the promise returned by the connect() method within an async function."
82,"We use the promise catch() handler to handle any errors when trying to connect, but we might also have called main() within a try...catch block."
82,You can get the default Connection object with mongoose.connection.
82,If you need to create additional connections you can use mongoose.createConnection().
82,"This takes the same form of database URI (with host, database, port, options, etc.) as connect() and returns a Connection object)."
82,Note that createConnection() returns immediately; if you need to wait on the connection to be established you can call it with asPromise() to return a promise (mongoose.createConnection(mongoDB).asPromise()).
82,"Defining and creating modelsModels are defined using the Schema interface. The Schema allows you to define the fields stored in each document along with their validation requirements and default values. In addition, you can define static and instance helper methods to make it easier to work with your data types, and also virtual properties that you can use like any other field, but which aren't actually stored in the database (we'll discuss a bit further below)."
82,"Schemas are then ""compiled"" into models using the mongoose.model() method. Once you have a model you can use it to find, create, update, and delete objects of the given type."
82,Note: Each model maps to a collection of documents in the MongoDB database. The documents will contain the fields/schema types defined in the model Schema.
82,Defining schemas
82,"The code fragment below shows how you might define a simple schema. First you require() mongoose, then use the Schema constructor to create a new schema instance, defining the various fields inside it in the constructor's object parameter."
82,js// Require Mongoose
82,"const mongoose = require(""mongoose"");"
82,// Define a schema
82,const Schema = mongoose.Schema;
82,const SomeModelSchema = new Schema({
82,"a_string: String,"
82,"a_date: Date,"
82,});
82,"In the case above we just have two fields, a string and a date. In the next sections, we will show some of the other field types, validation, and other methods."
82,Creating a model
82,Models are created from schemas using the mongoose.model() method:
82,js// Define schema
82,const Schema = mongoose.Schema;
82,const SomeModelSchema = new Schema({
82,"a_string: String,"
82,"a_date: Date,"
82,});
82,// Compile model from schema
82,"const SomeModel = mongoose.model(""SomeModel"", SomeModelSchema);"
82,"The first argument is the singular name of the collection that will be created for your model (Mongoose will create the database collection for the above model SomeModel above), and the second argument is the schema you want to use in creating the model."
82,"Note: Once you've defined your model classes you can use them to create, update, or delete records, and run queries to get all records or particular subsets of records. We'll show you how to do this in the Using models section, and when we create our views."
82,Schema types (fields)
82,A schema can have an arbitrary number of fields — each one represents a field in the documents stored in MongoDB.
82,An example schema showing many of the common field types and how they are declared is shown below.
82,jsconst schema = new Schema({
82,"name: String,"
82,"binary: Buffer,"
82,"living: Boolean,"
82,"updated: { type: Date, default: Date.now() },"
82,"age: { type: Number, min: 18, max: 65, required: true },"
82,"mixed: Schema.Types.Mixed,"
82,"_someId: Schema.Types.ObjectId,"
82,"array: [],"
82,"ofString: [String], // You can also have an array of each of the other types too."
82,"nested: { stuff: { type: String, lowercase: true, trim: true } },"
82,});
82,"Most of the SchemaTypes (the descriptors after ""type:"" or after field names) are self-explanatory. The exceptions are:"
82,"ObjectId: Represents specific instances of a model in the database. For example, a book might use this to represent its author object. This will actually contain the unique ID (_id) for the specified object. We can use the populate() method to pull in the associated information when needed."
82,Mixed: An arbitrary schema type.
82,"[]: An array of items. You can perform JavaScript array operations on these models (push, pop, unshift, etc.). The examples above show an array of objects without a specified type and an array of String objects, but you can have an array of any type of object."
82,The code also shows both ways of declaring a field:
82,"Field name and type as a key-value pair (i.e. as done with fields name, binary and living)."
82,"Field name followed by an object defining the type, and any other options for the field. Options include things like:"
82,default values.
82,built-in validators (e.g. max/min values) and custom validation functions.
82,Whether the field is required
82,"Whether String fields should automatically be set to lowercase, uppercase, or trimmed (e.g. { type: String, lowercase: true, trim: true })"
82,For more information about options see SchemaTypes (Mongoose docs).
82,Validation
82,"Mongoose provides built-in and custom validators, and synchronous and asynchronous validators. It allows you to specify both the acceptable range of values and the error message for validation failure in all cases."
82,The built-in validators include:
82,All SchemaTypes have the built-in required validator. This is used to specify whether the field must be supplied in order to save a document.
82,Numbers have min and max validators.
82,Strings have:
82,enum: specifies the set of allowed values for the field.
82,match: specifies a regular expression that the string must match.
82,maxLength and minLength for the string.
82,The example below (slightly modified from the Mongoose documents) shows how you can specify some of the validator types and error messages:
82,jsconst breakfastSchema = new Schema({
82,eggs: {
82,"type: Number,"
82,"min: [6, ""Too few eggs""],"
82,"max: 12,"
82,"required: [true, ""Why no eggs?""],"
82,drink: {
82,"type: String,"
82,"enum: [""Coffee"", ""Tea"", ""Water""],"
82,});
82,For complete information on field validation see Validation (Mongoose docs).
82,Virtual properties
82,"Virtual properties are document properties that you can get and set but that do not get persisted to MongoDB. The getters are useful for formatting or combining fields, while setters are useful for de-composing a single value into multiple values for storage. The example in the documentation constructs (and deconstructs) a full name virtual property from a first and last name field, which is easier and cleaner than constructing a full name every time one is used in a template."
82,Note: We will use a virtual property in the library to define a unique URL for each model record using a path and the record's _id value.
82,For more information see Virtuals (Mongoose documentation).
82,Methods and query helpers
82,"A schema can also have instance methods, static methods, and query helpers. The instance and static methods are similar, but with the obvious difference that an instance method is associated with a particular record and has access to the current object. Query helpers allow you to extend mongoose's chainable query builder API (for example, allowing you to add a query ""byName"" in addition to the find(), findOne() and findById() methods).Using modelsOnce you've created a schema you can use it to create models. The model represents a collection of documents in the database that you can search, while the model's instances represent individual documents that you can save and retrieve."
82,We provide a brief overview below. For more information see: Models (Mongoose docs).
82,"Note: Creation, update, deletion and querying of records are asynchronous operations that return a promise."
82,The examples below show just the use of the relevant methods and await (i.e. the essential code for using the methods).
82,The surrounding async function and try...catch block to catch errors are omitted for clarity.
82,For more information on using await/async see Database APIs are asynchronous above.
82,Creating and modifying documents
82,To create a record you can define an instance of the model and then call save() on it.
82,The examples below assume SomeModel is a model (with a single field name) that we have created from our schema.
82,js// Create an instance of model SomeModel
82,"const awesome_instance = new SomeModel({ name: ""awesome"" });"
82,// Save the new model instance asynchronously
82,await awesome_instance.save();
82,You can also use create() to define the model instance at the same time as you save it.
82,"Below we create just one, but you can create multiple instances by passing in an array of objects."
82,"jsawait SomeModel.create({ name: ""also_awesome"" });"
82,Every model has an associated connection (this will be the default connection when you use mongoose.model()). You create a new connection and call .model() on it to create the documents on a different database.
82,"You can access the fields in this new record using the dot syntax, and change the values. You have to call save() or update() to store modified values back to the database."
82,js// Access model field values using dot notation
82,console.log(awesome_instance.name); //should log 'also_awesome'
82,"// Change record by modifying the fields, then calling save()."
82,"awesome_instance.name = ""New cool name"";"
82,await awesome_instance.save();
82,Searching for records
82,"You can search for records using query methods, specifying the query conditions as a JSON document. The code fragment below shows how you might find all athletes in a database that play tennis, returning just the fields for athlete name and age. Here we just specify one matching field (sport) but you can add more criteria, specify regular expression criteria, or remove the conditions altogether to return all athletes."
82,"jsconst Athlete = mongoose.model(""Athlete"", yourSchema);"
82,"// find all athletes who play tennis, returning the 'name' and 'age' fields"
82,const tennisPlayers = await Athlete.find(
82,"{ sport: ""Tennis"" },"
82,"""name age"","
82,).exec();
82,Note: It is important to remember that not finding any results is not an error for a search — but it may be a fail-case in the context of your application.
82,If your application expects a search to find a value you can check the number of entries returned in the result.
82,"Query APIs, such as find(), return a variable of type Query."
82,You can use a query object to build up a query in parts before executing it with the exec() method.
82,exec() executes the query and returns a promise that you can await on for the result.
82,js// find all athletes that play tennis
82,"const query = Athlete.find({ sport: ""Tennis"" });"
82,// selecting the 'name' and 'age' fields
82,"query.select(""name age"");"
82,// limit our results to 5 items
82,query.limit(5);
82,// sort by age
82,query.sort({ age: -1 });
82,// execute the query at a later time
82,query.exec();
82,"Above we've defined the query conditions in the find() method. We can also do this using a where() function, and we can chain all the parts of our query together using the dot operator (.) rather than adding them separately."
82,"The code fragment below is the same as our query above, with an additional condition for the age."
82,jsAthlete.find()
82,".where(""sport"")"
82,".equals(""Tennis"")"
82,".where(""age"")"
82,.gt(17)
82,.lt(50) // Additional where query
82,.limit(5)
82,.sort({ age: -1 })
82,".select(""name age"")"
82,.exec();
82,"The find() method gets all matching records, but often you just want to get one match. The following methods query for a single record:"
82,findById(): Finds the document with the specified id (every document has a unique id).
82,findOne(): Finds a single document that matches the specified criteria.
82,"findByIdAndDelete(), findByIdAndUpdate(), findOneAndRemove(), findOneAndUpdate(): Finds a single document by id or criteria and either updates or removes it. These are useful convenience functions for updating and removing records."
82,Note: There is also a countDocuments() method that you can use to get the number of items that match conditions. This is useful if you want to perform a count without actually fetching the records.
82,There is a lot more you can do with queries. For more information see: Queries (Mongoose docs).
82,Working with related documents — population
82,"You can create references from one document/model instance to another using the ObjectId schema field, or from one document to many using an array of ObjectIds. The field stores the id of the related model. If you need the actual content of the associated document, you can use the populate() method in a query to replace the id with the actual data."
82,"For example, the following schema defines authors and stories."
82,"Each author can have multiple stories, which we represent as an array of ObjectId."
82,Each story can have a single author.
82,The ref property tells the schema which model can be assigned to this field.
82,"jsconst mongoose = require(""mongoose"");"
82,const Schema = mongoose.Schema;
82,const authorSchema = new Schema({
82,"name: String,"
82,"stories: [{ type: Schema.Types.ObjectId, ref: ""Story"" }],"
82,});
82,const storySchema = new Schema({
82,"author: { type: Schema.Types.ObjectId, ref: ""Author"" },"
82,"title: String,"
82,});
82,"const Story = mongoose.model(""Story"", storySchema);"
82,"const Author = mongoose.model(""Author"", authorSchema);"
82,We can save our references to the related document by assigning the _id value.
82,"Below we create an author, then a story, and assign the author id to our story's author field."
82,"jsconst bob = new Author({ name: ""Bob Smith"" });"
82,await bob.save();
82,"// Bob now exists, so lets create a story"
82,const story = new Story({
82,"title: ""Bob goes sledding"","
82,"author: bob._id, // assign the _id from our author Bob. This ID is created by default!"
82,});
82,await story.save();
82,Note: One great benefit of this style of programming is that we don't have to complicate the main path of our code with error checking.
82,"If any of the save() operations fail, the promise will reject and an error will be thrown."
82,"Our error handling code deals with that separately (usually in a catch() block), so the intent of our code is very clear."
82,"Our story document now has an author referenced by the author document's ID. In order to get the author information in the story results we use populate(), as shown below."
82,"jsStory.findOne({ title: ""Bob goes sledding"" })"
82,".populate(""author"") // Replace the author id with actual author information in results"
82,.exec();
82,"Note: Astute readers will have noted that we added an author to our story, but we didn't do anything to add our story to our author's stories array. How then can we get all stories by a particular author? One way would be to add our story to the stories array, but this would result in us having two places where the information relating authors and stories needs to be maintained."
82,"A better way is to get the _id of our author, then use find() to search for this in the author field across all stories."
82,jsStory.find({ author: bob._id }).exec();
82,This is almost everything you need to know about working with related items for this tutorial. For more detailed information see Population (Mongoose docs).One schema/model per file
82,"While you can create schemas and models using any file structure you like, we highly recommend defining each model schema in its own module (file), then exporting the method to create the model."
82,This is shown below:
82,js// File: ./models/somemodel.js
82,// Require Mongoose
82,"const mongoose = require(""mongoose"");"
82,// Define a schema
82,const Schema = mongoose.Schema;
82,const SomeModelSchema = new Schema({
82,"a_string: String,"
82,"a_date: Date,"
82,});
82,"// Export function to create ""SomeModel"" model class"
82,"module.exports = mongoose.model(""SomeModel"", SomeModelSchema);"
82,You can then require and use the model immediately in other files. Below we show how you might use it to get all instances of the model.
82,js// Create a SomeModel model just by requiring the module
82,"const SomeModel = require(""../models/somemodel"");"
82,// Use the SomeModel object (model) to find all SomeModel records
82,const modelInstances = await SomeModel.find().exec();
82,"Setting up the MongoDB databaseNow that we understand something of what Mongoose can do and how we want to design our models, it's time to start work on the LocalLibrary website. The very first thing we want to do is set up a MongoDB database that we can use to store our library data."
82,"For this tutorial, we're going to use the MongoDB Atlas cloud-hosted sandbox database. This database tier is not considered suitable for production websites because it has no redundancy, but it is great for development and prototyping. We're using it here because it is free and easy to set up, and because MongoDB Atlas is a popular database as a service vendor that you might reasonably choose for your production database (other popular choices at the time of writing include Compose, ScaleGrid and ObjectRocket)."
82,"Note: If you prefer, you can set up a MongoDB database locally by downloading and installing the appropriate binaries for your system. The rest of the instructions in this article would be similar, except for the database URL you would specify when connecting."
82,"In the Express Tutorial Part 7: Deploying to Production tutorial we host both the application and database on Railway, but we could equally well have used a database on MongoDB Atlas."
82,"You will first need to create an account with MongoDB Atlas (this is free, and just requires that you enter basic contact details and acknowledge their terms of service)."
82,"After logging in, you'll be taken to the home screen:"
82,Click the + Create button in the Overview section.
82,This will open the Deploy your database screen. Click on the M0 FREE option template.
82,Scroll down the page to see the different options you can choose.
82,Select any provider and region from the Provider and Region sections. Different regions offer different providers.
82,You can change the name of your Cluster under Cluster Name. We are naming it Cluster0 for this tutorial.
82,Tags are optional. We will not use them here.
82,Click the Create button (creation of the cluster will take some minutes).
82,This will open the Security Quickstart section.
82,Enter a username and password. Remember to copy and store the credentials safely as we will need them later on. Click the Create User button.
82,Note: Avoid using special characters in your MongoDB user password as mongoose may not parse the connection string properly.
82,Enter 0.0.0.0/0 in the IP Address field. This tells MongoDB that we want to allow access from anywhere. Click the Add Entry button.
82,Note: It is a best practice to limit the IP addresses that can connect to your database and other resources. Here we allow a connection from anywhere because we don't know where the request will come from after deployment.
82,Click the Finish and Close button.
82,This will open the following screen. Click on the Go to Overview button.
82,You will return to the Overview screen. Click on the Database section under the Deployment menu on the left. Click the Browse Collections button.
82,This will open the Collections section. Click the Add My Own Data button.
82,This will open the Create Database screen.
82,Enter the name for the new database as local_library.
82,Enter the name of the collection as Collection0.
82,Click the Create button to create the database.
82,You will return to the Collections screen with your database created.
82,Click the Overview tab to return to the cluster overview.
82,From the Cluster0 Overview screen click the Connect button.
82,This will open the Connect to Cluster screen.
82,Click the Drivers option under the Connect to your application section.
82,You will now be shown the Connect screen.
82,Select the Node driver and version as shown.
82,DO NOT follow the step 2.
82,Click the Copy icon to copy the connection string.
82,Paste this in your local text editor.
82,Update the username and password with your user's password.
82,"Insert the database name ""local_library"" in the path before the options (...mongodb.net/local_library?retryWrites...)"
82,Save the file containing this string somewhere safe.
82,"You have now created the database, and have a URL (with username and password) that can be used to access it."
82,This will look something like: mongodb+srv://your_user_name:your_password@cluster0.lz91hw2.mongodb.net/local_library?retryWrites=true&w=majority
82,Install Mongoose
82,Open a command prompt and navigate to the directory where you created your skeleton Local Library website.
82,"Enter the following command to install Mongoose (and its dependencies) and add it to your package.json file, unless you have already done so when reading the Mongoose Primer above."
82,bashnpm install mongoose
82,Connect to MongoDB
82,Open /app.js (in the root of your project) and copy the following text below where you declare the Express application object (after the line const app = express();).
82,Replace the database URL string ('insert_your_database_url_here') with the location URL representing your own database (i.e. using the information from MongoDB Atlas).
82,js// Set up mongoose connection
82,"const mongoose = require(""mongoose"");"
82,"mongoose.set(""strictQuery"", false);"
82,"const mongoDB = ""insert_your_database_url_here"";"
82,main().catch((err) => console.log(err));
82,async function main() {
82,await mongoose.connect(mongoDB);
82,"As discussed in the Mongoose primer above, this code creates the default connection to the database and reports any errors to the console."
82,Note that hard-coding database credentials in source code as shown above is not recommended.
82,"We do it here because it shows the core connection code, and because during development there is no significant risk that leaking these details will expose or corrupt sensitive information."
82,We'll show you how to do this more safely when deploying to production!
82,Defining the LocalLibrary Schema
82,"We will define a separate module for each model, as discussed above."
82,Start by creating a folder for our models in the project root (/models) and then create separate files for each of the models:
82,/express-locallibrary-tutorial
82,// the project root
82,/models
82,author.js
82,book.js
82,bookinstance.js
82,genre.js
82,Author model
82,Copy the Author schema code shown below and paste it into your ./models/author.js file.
82,"The schema defines an author as having String SchemaTypes for the first and family names (required, with a maximum of 100 characters), and Date fields for the dates of birth and death."
82,"jsconst mongoose = require(""mongoose"");"
82,const Schema = mongoose.Schema;
82,const AuthorSchema = new Schema({
82,"first_name: { type: String, required: true, maxLength: 100 },"
82,"family_name: { type: String, required: true, maxLength: 100 },"
82,"date_of_birth: { type: Date },"
82,"date_of_death: { type: Date },"
82,});
82,// Virtual for author's full name
82,"AuthorSchema.virtual(""name"").get(function () {"
82,// To avoid errors in cases where an author does not have either a family name or first name
82,// We want to make sure we handle the exception by returning an empty string for that case
82,"let fullname = """";"
82,if (this.first_name && this.family_name) {
82,"fullname = `${this.family_name}, ${this.first_name}`;"
82,return fullname;
82,});
82,// Virtual for author's URL
82,"AuthorSchema.virtual(""url"").get(function () {"
82,// We don't use an arrow function as we'll need the this object
82,return `/catalog/author/${this._id}`;
82,});
82,// Export model
82,"module.exports = mongoose.model(""Author"", AuthorSchema);"
82,"We've also declared a virtual for the AuthorSchema named ""url"" that returns the absolute URL required to get a particular instance of the model — we'll use the property in our templates whenever we need to get a link to a particular author."
82,Note: Declaring our URLs as a virtual in the schema is a good idea because then the URL for an item only ever needs to be changed in one place.
82,"At this point, a link using this URL wouldn't work, because we haven't got any routes handling code for individual model instances."
82,We'll set those up in a later article!
82,"At the end of the module, we export the model.Book model"
82,Copy the Book schema code shown below and paste it into your ./models/book.js file.
82,"Most of this is similar to the author model — we've declared a schema with a number of string fields and a virtual for getting the URL of specific book records, and we've exported the model."
82,"jsconst mongoose = require(""mongoose"");"
82,const Schema = mongoose.Schema;
82,const BookSchema = new Schema({
82,"title: { type: String, required: true },"
82,"author: { type: Schema.Types.ObjectId, ref: ""Author"", required: true },"
82,"summary: { type: String, required: true },"
82,"isbn: { type: String, required: true },"
82,"genre: [{ type: Schema.Types.ObjectId, ref: ""Genre"" }],"
82,});
82,// Virtual for book's URL
82,"BookSchema.virtual(""url"").get(function () {"
82,// We don't use an arrow function as we'll need the this object
82,return `/catalog/book/${this._id}`;
82,});
82,// Export model
82,"module.exports = mongoose.model(""Book"", BookSchema);"
82,The main difference here is that we've created two references to other models:
82,"author is a reference to a single Author model object, and is required."
82,genre is a reference to an array of Genre model objects. We haven't declared this object yet!
82,BookInstance model
82,"Finally, copy the BookInstance schema code shown below and paste it into your ./models/bookinstance.js file."
82,"The BookInstance represents a specific copy of a book that someone might borrow and includes information about whether the copy is available, on what date it is expected back, and ""imprint"" (or version) details."
82,"jsconst mongoose = require(""mongoose"");"
82,const Schema = mongoose.Schema;
82,const BookInstanceSchema = new Schema({
82,"book: { type: Schema.Types.ObjectId, ref: ""Book"", required: true }, // reference to the associated book"
82,"imprint: { type: String, required: true },"
82,status: {
82,"type: String,"
82,"required: true,"
82,"enum: [""Available"", ""Maintenance"", ""Loaned"", ""Reserved""],"
82,"default: ""Maintenance"","
82,"due_back: { type: Date, default: Date.now },"
82,});
82,// Virtual for bookinstance's URL
82,"BookInstanceSchema.virtual(""url"").get(function () {"
82,// We don't use an arrow function as we'll need the this object
82,return `/catalog/bookinstance/${this._id}`;
82,});
82,// Export model
82,"module.exports = mongoose.model(""BookInstance"", BookInstanceSchema);"
82,The new things we show here are the field options:
82,"enum: This allows us to set the allowed values of a string. In this case, we use it to specify the availability status of our books (using an enum means that we can prevent mis-spellings and arbitrary values for our status)."
82,"default: We use default to set the default status for newly created book instances to ""Maintenance"" and the default due_back date to now (note how you can call the Date function when setting the date!)."
82,"Everything else should be familiar from our previous schema.Genre model - challenge!Open your ./models/genre.js file and create a schema for storing genres (the category of book, e.g. whether it is fiction or non-fiction, romance or military history, etc.)."
82,The definition will be very similar to the other models:
82,The model should have a String SchemaType called name to describe the genre.
82,This name should be required and have between 3 and 100 characters.
82,"Declare a virtual for the genre's URL, named url."
82,Export the model.
82,Testing — create some itemsThat's it. We now have all models for the site set up!
82,In order to test the models (and to create some example books and other items that we can use in our next articles) we'll now run an independent script to create items of each type:
82,Download (or otherwise create) the file populatedb.js inside your express-locallibrary-tutorial directory (in the same level as package.json).
82,"Note: The code in populatedb.js may be useful in learning JavaScript, but understanding it is not necessary for this tutorial."
82,"Run the script using node in your command prompt, passing in the URL of your MongoDB database (the same one you replaced the insert_your_database_url_here placeholder with, inside app.js earlier):"
82,bashnode populatedb <your MongoDB url>
82,"Note: On Windows you need to wrap the database URL inside double ("")."
82,On other operating systems you may need single (') quotation marks.
82,"The script should run through to completion, displaying items as it creates them in the terminal."
82,Note: Go to your database on MongoDB Atlas (in the Collections tab).
82,"You should now be able to drill down into individual collections of Books, Authors, Genres and BookInstances, and check out individual documents."
82,"SummaryIn this article, we've learned a bit about databases and ORMs on Node/Express, and a lot about how Mongoose schema and models are defined. We then used this information to design and implement Book, BookInstance, Author and Genre models for the LocalLibrary website."
82,"Last of all, we tested our models by creating a number of instances (using a standalone script). In the next article we'll look at creating some pages to display these objects.See also"
82,Database integration (Express docs)
82,Mongoose website (Mongoose docs)
82,Mongoose Guide (Mongoose docs)
82,Validation (Mongoose docs)
82,Schema Types (Mongoose docs)
82,Models (Mongoose docs)
82,Queries (Mongoose docs)
82,Population (Mongoose docs)
82,Previous
82,Overview: Express Nodejs
82,Next
82,"Help improve MDNWas this page helpful to you?YesNoLearn how to contribute.This page was last modified on Feb 26, 2024 by MDN contributors.View this page on GitHub • Report a problem with this contentMDN logoYour blueprint for a better internet.MDN on MastodonMDN on X (formerly Twitter)MDN on GitHubMDN Blog RSS FeedMDNAboutBlogCareersAdvertise with usSupportProduct helpReport an issueOur communitiesMDN CommunityMDN ForumMDN ChatDevelopersWeb TechnologiesLearn Web DevelopmentMDN PlusHacks BlogMozilla logoWebsite Privacy NoticeCookiesLegalCommunity Participation GuidelinesVisit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.Portions of this content are ©1998–2024 by individual mozilla.org contributors. Content available under a Creative Commons license."
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning | Journal of Cheminformatics | Full Text
83,Skip to main content
83,Advertisement
83,Search
83,Explore journals
83,Get published
83,About BMC
83,My account
83,Search all BMC articles
83,Search
83,Journal of Cheminformatics
83,Home
83,About
83,Articles
83,Submission Guidelines
83,About the Editors
83,Calls for Papers
83,Submit manuscript
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
83,Download PDF
83,Download ePub
83,Download PDF
83,Download ePub
83,Research
83,Open access
83,Published: 01 February 2024
83,DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning
83,"Jonghyun Lee1, Dae Won Jun1,2, Ildae Song3 & …Yun Kim4 Show authors"
83,Journal of Cheminformatics
83,"volume 16, Article number: 14 (2024)"
83,Cite this article
83,890 Accesses
83,2 Altmetric
83,Metrics details
83,"AbstractThe drug discovery process is demanding and time-consuming, and machine learning-based research is increasingly proposed to enhance efficiency. A significant challenge in this field is predicting whether a drug molecule’s structure will interact with a target protein. A recent study attempted to address this challenge by utilizing an encoder that leverages prior knowledge of molecular and protein structures, resulting in notable improvements in the prediction performance of the drug-target interactions task. Nonetheless, the target encoders employed in previous studies exhibit computational complexity that increases quadratically with the input length, thereby limiting their practical utility. To overcome this challenge, we adopt a hint-based learning strategy to develop a compact and efficient target encoder. With the adaptation parameter, our model can blend general knowledge and target-oriented knowledge to build features of the protein sequences. This approach yielded considerable performance enhancements and improved learning efficiency on three benchmark datasets: BIOSNAP, DAVIS, and Binding DB. Furthermore, our methodology boasts the merit of necessitating only a minimal Video RAM (VRAM) allocation, specifically 7.7GB, during the training phase (16.24% of the previous state-of-the-art model). This ensures the feasibility of training and inference even with constrained computational resources."
83,"IntroductionThe process of drug discovery is often compared to finding a needle in a haystack, requiring substantial funds and labor forces. Unfortunately, most newly discovered drugs fail to obtain approval for clinical use due to unexpected adverse drug reactions, insufficient drug effects, and low binding affinity [1,2,3,4,5]. Artificial intelligence has emerged as a promising tool for reducing expenses in various fields of drug discovery, including the predictions of drug toxicity, drug-drug interaction, and molecule properties, among others. In the first step of drug discovery, which involves drug repurposing and/or repositioning, it is critical to identify candidates of druggable molecules that target a specific protein. In this context, drug-target interaction (DTI) prediction tasks have emerged as a crucial area of research.Previous studies on DTI prediction can be broadly categorized into three categories: simulation-based molecular docking, structural similarity, and deep neural network (DNN) approach. Molecular docking simulation utilized 3D structures of proteins and molecules and simulated the binding sites [6,7,8]. While it offers a clear visual understanding, obtaining a 3D structure of a feature is challenging and it was hard to collect large datasets effectively. Conversely, the similarity-based technique proposed binding candidates using priorly established drug-target pairs. While this approach showed considerable predictions for recognized pairs based on similarity, it confronts difficulties in determining similarity for previously unobserved pairs [9, 10]. DNNs have exhibited proficient results in DTI prediction, similar to their successful implementations in various other domains. A pioneering study, DeepDTA [11], employed a drug and target encoder built on Convolutional Neural Networks (CNN) for the prediction of binding affinities. Instead of relying on highly complex datasets, the DeepDTA leveraged 1D expressions of the molecular structure system, Simplified Molecular Input Line Entry System (SMILES), and amino acid sequences, for drug and target, respectively. With hierarchical CNN layers, similar to conventional CNNs used for image recognition, DeepDTA can interpret the interactions of a given drug-target pair. After the DeepDTA, a multitude of research initiatives have been undertaken to either enhance the encoder’s capability or predict interactions more effectively. Such advancements encompass the deployment of CNNs [12,13,14], the development of interactions within gated cross attentions [15], the adoption of encoders that perceive molecular structures in graph format [16,17,18], computing similarity using enhanced DNN-based kernels [19,20,21], encode sequence using generative models [22, 23], and the integration of multi-modal techniques [24,25,26,27].The Transformer architecture [28], renowned for its proficiency in sequence processing, has been extensively employed as an encoder [29,30,31,32,33,34,35,36,37]. Nonetheless, it possesses a fundamental limitation: the computational expense escalates quadratically with the increase in the input length (see more details in Appendix C). Consequently, a majority of research initiatives have leaned towards its application as a drug encoder rather than for proteins [30,31,32,33, 37]. Recent advancements have brought forth efficient transformer methodologies, suggesting the potential for significantly reducing the computational demands in protein-encoding [38,39,40,41]. Concurrently, the ProtTrans project [35], leveraging the established Bidirectional Encoder Representations from Transformers (BERT) [42] model and its training methodology has undertaken pre-training of a protein encoder using an expansive set of amino acids and subsequently made it publicly available. As of now, the academic community lacks a publicly accessible, pre-trained model based on the efficient transformer, thereby preserving the relevance and utility of ProtTrans. A recent study, that utilized both transformer-based encoders for representing drugs and targets was proposed [43]. The prediction performances were considerably improved, however, due to the large size of the protein encoder, they truncated the protein language model into half its size.To reach an efficient computing model, knowledge distillation techniques were proposed [44, 45]. The key concept of knowledge distillation is distilling the knowledge from the large and complex model to the small and simple model with minimum loss of knowledge (See more details on Appendix A). However, DistillProtBERT (260 million parameters) [46], a model employing knowledge distillation from ProtBERT (420 million parameters) [35], is less efficient due to the inherent complexity of the amino acid sequence.To address this, we proposed a more efficient learning method than knowledge distillation, namely hint-based knowledge adaptation. This method involves using the intermediate features of the teacher model as hints, representing an expansion of knowledge distillation inspired by FitNet [47]. We term this approach “general knowledge” as it provides a general understanding of the target sequence, though lacking direct knowledge of the DTI task. It is assumed that this general knowledge, serving as a hint to the sequence, will facilitate successful learning despite the small size and simplicity of the student model. Conversely, the student model, designed to directly learn DTI performance, was structured in a simplified form compared to the original ProtBERT. In essence, knowledge adaptation presents an efficient means of leveraging both general knowledge of the target sequence and task-specific knowledge related to DTI simultaneously. This underscores the concept of adapting the teacher’s knowledge to the student’s knowledge, in contrast to knowledge distillation, which directly conveys task-specific knowledge.In this study, we proposed a Dual Language Model-based DTI model named DLM-DTI. The DLM-DTI was a lightweight and efficient, but accurate DTI prediction model. With the knowledge adaptation, the rich information from ProtBERT successfully adapted to predict DTI tasks. This study has several key contributions:"
83,"The hint-based knowledge adaptation technique, despite its compact parameterization, demonstrates considerably improved performance compared to baseline methods."
83,"By utilizing cached outputs from the teacher network, we achieved a notable reduction in computational costs."
83,"The knowledge adaptation approach is model-agnostic, offering flexibility in the selection of pre-trained models and architectures."
83,"Materials and methodsProblem definitionIn binary DTI classification, the goal is to predict the target value, \(Y_i\), for a given pair of \(X_i\), where \(\text {X}_i = \{ \text {x}_{\textrm{drug}}^i, \text {x}_{\textrm{target}}^i \}\), and \(\text {Y}_i \in \{ 0, 1 \}\) for \(i=1,\cdots , N\). The prediction of DTI can be viewed as a mapping function \(f(X_i) \rightarrow [0,1]\), which maps the drug-target pairs to a probability score of the interaction.Sequence representationSequence representations and embeddings involve converting a sequence, like a sentence, into a format that a computer model can understand. The first step is turning each part of the sequence into tokens, which are basically integer numbers that the model can work with. In this study, each part of the sequence is treated as a separate token. Special tokens, like a class token, are also added to grasp the overall meaning of the entire sequence. The concept of tokenization and special tokens is illustrated in Fig. 1.Fig. 1The concept of sequence representation and pre-training is illustrated. In A, the tokenization of a drug sequence (SMILES string) is depicted. In B, the tokenized elements are converted into integer values according to the predefined dictionary, and the encoder model (in this example, ChemBERTa) restores masked tokens into the original tokens (tokens colored in gray). After pre-training, the class token (CLS) is used to represent a given sequenceFull size imageDataset configurationsWe employ three datasets, namely DAVIS, Binding DB, and BIOSNAP, to train and evaluate the DLM-DTI. The DAVIS dataset consists of 68 drugs and 379 proteins, with 11,103 interactions measured in dissociation constant (\({K}_{d}\)) [48]. The interactions are categorized as positive or negative, with 1506 and 9597 instances, respectively. Similarly, the Binding DB dataset includes 10,665 drugs and 1413 proteins, with 32,601 interactions measured in \({K}_{d}\) [49]. The interactions are categorized as positive or negative, with 9166 and 23,435 instances, respectively. In this study, the threshold value for \({K}_{d}\) is set to 30 units, and interactions with \({K}_{d}\) values less than 30 units are considered positive binding interactions between the given drug and protein pair [29, 43]. The BIOSNAP dataset is initially composed of positive interactions only; however, negative pairs are added in the MolTrans study. The BIOSNAP dataset comprises 4510 drugs and 2181 proteins, with 27,482 interactions, including 13,741 positive and 13,741 negative instances [29].The integrated data training was first proposed by Kang et al., and they demonstrated improvements [43]. In this setting, training and validation datasets were merged, and a model was trained using integrated datasets. After the training steps, the trained model with integrated training datasets was evaluated on individual test datasets. For example, to test the BIOSNAP test dataset, the model was first trained using DAVIS, Binding DB, and BIOSNAP’s training datasets, and then tested on BIOSNAP’s test dataset. Generally, the diversity and quantity of datasets are linked to the improvement of prediction performance. Therefore, we also assessed the impact of dataset integrations using DLM-DTI. A summary of the dataset description is presented in Table 1.Table 1 The description of datasetsFull size tableTo ensure a fair comparison of model performance, we employ the same training, validation, and testing datasets used in previous studies [29, 43]. The datasets are split into training, validation, and testing datasets in the ratio of 7:1:2, respectively. The number of interactions for each data splitting is summarized in Table 2.Table 2 The number of interactions for each splitFull size tableModel configurationsThe process flow of DLM-DTI is depicted in Fig. 2. DLM-DTI was comprised of three primary components: the drug encoder, target encoder, and interaction prediction head. Notably, the target encoder encompasses both the teacher and student models of language models for protein sequences.Fig. 2The process flow of DLM-DTI. The drug and target sequences feed into their respective encoders. The encoded sequences are then merged, and the probability of bindings is computed using the interaction prediction head. DLM-DTI only utilizes the class token (CLS) of each encoded sequence because the class token preserves the abstract meaning of the entire sequence. The features of target sequences are computed using a teacher-student-based architecture, specifically employing a hint-based learning strategyFull size imageDrug encoderThe drug encoder converts SMILES sequences into meaningful features, serving as a mapping function from molecule sequences to a meaningful chemical space. We employed the ChemBERTa encoder, which was trained on various canonical SMILES and learned chemical space. Further details are described in Appendix B.The class token of the last hidden layer was extracted as input for the interaction prediction head. The encoding process of the drug sequence can be represented as follows:$$\begin{aligned} z_{\textrm{drug}} = f\left( \text {LN}(x_{\textrm{class}})\right) , \end{aligned}$$"
83,(1)
83,"where \(\text {LN}(\cdot )\) denotes the layer normalization layer, \(f(\cdot )\) denotes the projection function used to align the dimensions, and the hidden dimensions were set to 512 in this study. The upper limit of the drug sequence length was 512 tokens, corresponding to the maximum sequence length of the original ChemBERTa encoder [31].Target encoderSimilar to the drug encoder, the target encoder also extracts meaningful features from raw target sequences (amino acid sequences). The target encoder in this study was composed of both a teacher and a student model. The teacher model used for target sequence encoding was the ProtBERT model, pre-trained on UniRef and big fantastic database databases [35]. Details of ProtBERT are described in Appendix C. The original ProtBERT model was trained on sequences up to 40 K characters, with 420 million parameters. The student model was designed to match the original teacher model, ProtBERT, however, the number of layers was reduced. Except for the number of layers, the student model followed the hyperparameters of the teacher model. The number of parameters of the student model was 6.2% of the teacher model; teacher model: 420.0 million, student model: 26 million. The detailed parameters of the target encoder are presented in Table 3.Table 3 The specific parameters of target encoderFull size tableIn most cases, fully fine-tuning the large model was impractical due to restrictions on datasets and the associated computational expenses. To address this challenge, we adopted a hint-based training scheme that kind of knowledge distillation comprises both a teacher model and a student model. The teacher model was prevented from parameter updates, enabling solely the parameters of the student model to be updated. Given that the teacher model’s output was not subject to training, it retained a fixed form, thus enabling us to cache outputs of the teacher model prior to the training and inference step. This strategy markedly minimizes computational redundancy, thereby optimizing computational efficiency. Considering the teacher model’s output was not trained, it served as a form of hint to which the task-specific model (student model) could refer. The teacher and student models were combined using class token mixing to encode the target sequence. The output class token was treated as a “hint” that contained general knowledge of the given protein sequence. On the other hand, the output class token of the student model was considered as task-oriented specific knowledge. To mix the general knowledge and task-specific knowledge, we added two class tokens with learnable gating parameters (\(\lambda\)). The encoding process of the target sequence can be represented as follows:$$\begin{aligned} z_{\textrm{target}} = \lambda g\left( \text {LN} (x^{\textrm{student}}_{\textrm{class}})) + (1 - \lambda ) h (\text {LN}(x^{\textrm{teacher}}_{\textrm{class}})\right) , \end{aligned}$$"
83,(2)
83,"where \(g(\cdot )\) and \(h(\cdot )\) are the projection functions used to align the dimensions, and the adaptation parameter \(\lambda\) is a learnable parameter initialized randomly from a uniform distribution, \(\lambda \sim Uniform(0, 1)\). The term “adaptation” was employed to describe the process of adjusting general knowledge to suit the specific requirements of a particular task. An elevated value of the adaptation parameter indicated an increased emphasis of the model on the class token derived from the teacher model. In contrast, a decreased value of the adaptation parameter signified a predominant utilization of task-specific information obtained from the student model. The hidden dimensions of the class token mixing were set to 1024 in this study. The maximum length of the target sequence was set to 545 tokens, which covered 95% of proteins in the datasets, and the same max protein sequence lengths of previous studies [29, 43].Interaction prediction headThe class tokens of drug and target sequences have abstract meanings for each sequence. The interaction prediction head aggregated the features of drug-target pairs and predicted binding probability. In this step, there were multiple choices for mixing the features; for example, cross attention, capsule network, etc. However, we simply employed concatenation that showed stable performances in the previous work [43].The interaction module consists of three sequential blocks. Each block is structured with a Fully Connected (FC) layer, followed by an activation function and subsequently a dropout layer. The respective dimensions of the FC layers are 2048, 1024, and 512. The chosen activation function for these blocks is the Gaussian Error Linear Unit (GeLU). Additionally, a dropout rate of 0.1 has been employed for regularization. A detailed schematic of this configuration can be found in Fig. 3, and the specific parameter values are summarized in Table 4.Fig. 3Structure of the interaction prediction head. The interaction prediction head mixes the features of the drug-target pair to predict the binding probability of a given pair. The number under the block indicates the feature dimensionFull size imageTable 4 The detailed parameters of interaction prediction headFull size tableExperimental setupEvaluation metricsWe used the Area Under the Receiver Operation Characteristics curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) as primary evaluation metrics. AUROC is one of the most favorable metrics to measure classification performance, particularly in the medical field; however, it could be easily overestimated when the data has class imbalance [50]. Therefore, AUPRC is a relatively robust metric for measuring classification performance in imbalanced settings [50]. Sensitivity and specificity scores were utilized as sub-metrics, and the threshold for these sub-metrics was simply set to 0.5.Model training hyperparametersThe DLM-DTI was optimized using the AdamW optimizer with a learning rate of 0.0001. A cosine annealing learning rate scheduler was employed to adjust the learning rate. The binary cross-entropy loss was used to calculate the difference between predictions and ground truth. The model was trained for 50 epochs, and the best-performing parameters were selected based on the AUPRC score during validation. Due to severe class imbalance, the model could easily be overfitted to the dominant class. To prevent the selection of an overfitted model, we set the selection criteria as AUPRC rather than AUROC or the minimum loss coefficient. Automated mixed precision was utilized, and the batch size was set to 32. The best combination of hyperparameters was determined through iterative experiments.The use of a class imbalance sampler did not show any benefit for model training; therefore, we did not apply an imbalance sampler. Instead, AUPRC-based optimization demonstrated better performance in predicting binding probability.Hardware and softwardWe used a single NVIDIA A100 GPU to train DLM-DTI. The Python (v3.8) and PyTorch deep learning framework (v1.13) for trained DLM-DTI.ResultsBinding probability predictionThe baseline models, namely MolTrans [29] and the approach by Kang et al. [43], along with our proposed DLM-DTI, were trained on the same training datasets and evaluated using identical test datasets. Table 5 presents a summary of the evaluation results obtained from these experiments. MolTrans was exclusively trained on individual datasets and evaluated individually. In contrast, both Kang et al. and our DLM-DTI were trained using both individual and combined dataset settings. This approach was claimed in Kang et al., and therefore the previous study, MolTrans, did not experiment with an integrated dataset.Within the BIOSNAP dataset, DLM-DTI showed an improved AUPRC score (absolute value; percentage) than MolTrans (0.013; 1.44%), and Kang et al. (0.014 \(\sim\) 0.017; 1.56 \(\sim\) 1.90%). The AUROC score was improved compared to MolTrans (0.019; 2.12%), however, the AUROC showed similarity to Kang et al.’s model. Similarly, in the Binding DB, DLM-DTI exhibited a considerably improved AUPRC score than other methods, MolTrans (0.021; 3.38%), and Kang et al.’s model (0.004 \(\sim\) 0.02; 0.63 \(\sim\) 3.21%), respectively.In the DAVIS dataset, the performance of the DLM-DTI was degraded, and its performance was similar to that of MolTrans. The training with an integrated dataset showed benefits for the DLM-DTI only in the DAVIS dataset.Table 5 The prediction performance of binding affinityFull size tableAdaptation parameter, \(\lambda\)"
83,"During the training, the randomly initialized adaptation parameter \(\lambda\) gradually decreased and converged, as illustrated in Fig. 4. The adaptation parameter controlled the feature weights from the teacher and student encoder. As mentioned earlier, the teacher encoder contained general knowledge of the target sequence, and the student encoder had narrow but specific task-related knowledge. With the adaptation parameter, the DLM-DTI modulated the importance of each feature to accurately predict binding probability.Fig. 4Variation of the adaptation parameter (\(\lambda\)) during model training processFull size imageTo evaluate the effect of teacher-student architecture-based target sequence encoding, two ablation studies were conducted."
83,\(\lambda\) set to 0: Only the teacher encoder (general knowledge) was utilized.
83,\(\lambda\) set to 1: Only the student encoder (task-specific knowledge) was utilized.
83,"The adaptation setting (which utilized both teacher-student encoders) showed the best performance (AUROC: 0.912; AUPRC: 0.643) compared to the teacher encoder-only setting (AUROC: 0.911; AUPRC: 0.635) or the student encoder-only setting (AUROC: 0.900; AUPRC: 0.635). The effect of the \(\lambda\) parameter is summarized in Table 6.Table 6 The prediction performance of binding affinityFull size tableThe student encoder-only setting exhibited the poorest prediction performance (Rank: \(\text {3}^{\textrm{rd}}\)). This implies that two layers of simple and shallow networks were not sufficient to capture the complex patterns and features of target sequences to accurately predict DTIs. However, the teacher encoder-only setting demonstrated comparable performance (Rank: \(\text {2}^{\textrm{nd}}\)). This suggests that the general knowledge of the teacher model has the potential to predict binding probability. The teacher encoder-only setting corresponds to linear probing, where the training strategy only updates the prediction head without adjusting the weights of the encoder [51, 52]. The prediction performance of linear probing is considered as an encoder’s existing knowledge.Time and memory analysisTypically, a model’s performance exhibits a direct correlation with its parameter count, suggesting that larger models often yield superior outcomes. Nonetheless, this advantage comes with a caveat; substantial models necessitate considerable computational resources during both the training and inference stages. In light of this, we embarked on a systematic analysis comparing training time and parameter counts (Table 7). The metric for training time was derived by computing the mean learning time across three epochs, utilizing the Binding DB dataset.Table 7 Time and memory analysis of baseline models and DLM-DTIFull size tableDLM-DTI showed the best AUPRC score (0.643), only with 24.56% (86.7 million) of parameters compared to the Kang et al. (353.0 million) [43]. Additionally, DLM-DTI required 7.7 GB video random access memory (VRAM), and 63.00 s for a single training epoch. It was 16.24% (47.4 GB), and 9.98% (631.00 s) of the Kang et al. [43]. The MolTrans required the smallest VRAM (5.9 GB), however, the AUPRC score (0.622) was slightly lower than DLM-DTI (0.643). In our experimental setting, DLM-DTI required 7.7 GB of VRAM, therefore, it could be trained on conventional graphic processing units (GPUs), not for high-performing research machines (See details on 2.5.2).Cold drug, target, and bindingsIn addressing DTI challenges, the cold splitting testing approach is widely adopted [36, 53], primarily due to the inherent difficulties in dataset procurement and the paramount importance of achieving generalization for novel pairs. The term “cold splitting” pertains to scenarios where previously unseen drug-target interactions are involved, ones that were excluded from both the training and validation datasets. To simulate this condition, we conducted experiments where we isolated cold drugs, cold targets, and cold binding interactions from the test set of models trained to utilize the Binding DB dataset. We identified a total of 2,127 cold drugs and 136 cold targets. Specifically, a cold drug configuration encompasses all interactions associated with a cold drug, while a cold target configuration comprises all interactions associated with a cold target. The cold bindings were the interactions between cold drugs and cold targets, and only 114 pairs were identified. The performances of cold-splitting datasets are summarized in Table 8. DLM-DTI’s performance was comparable to the baseline models in the context of the cold drug, yet exhibited a minor deterioration to the cold target and was found to be most deficient in addressing cold binding. Conversely, Kang et al. [43] manifested commendable prediction capabilities across all testing scenarios. MolTrans [29] exhibited a performance metric closely mirroring Kang et al. in terms of AUROC, but fell short when evaluated using AUPRC.Table 8 The classification performances within the cold splitting settingsFull size tableDiscussionIn this study, we suggested a lightweight but accurate DTI prediction model, namely DLM-DTI. The main hurdle for utilizing protein sequence-based language models, such as ProtBERT [35], was heavy computing resource requirements. To comprehend the complex and long sequence of a protein, it needed heavy and large architectures and an intensive pre-training process. The DLM-DTI mitigated the computational burden caused by the protein encoder, by using a knowledge adaptation. DLM-DTI achieved improved AUPRC performance, especially in Binding DB (0.63 \(\sim\) 3.38%), and BIOSNAP (1.44 \(\sim\) 1.9%) datasets. The most interesting point was that DLM-DTI utilized only 25% of parameters (86.7 million) compared to the previous state-of-the-art model, Kang et al. (353 million) [43]. Additionally, DLM-DTI required only 7.7 GB of VRAM, and 63 s for each training epoch, that of 16.24%, and 9.98% of Kang et al. [43].The Transformer-based language model has exhibited impressive capabilities across various applications, including molecular and protein sequences. However, pre-training has emerged as a key approach to further optimize the model’s functional and semantic relationship learning from large sequence datasets [35,36,37, 42, 43]. Despite the promising results, the computational cost of the language model increases significantly with the input length. To address this challenge, Kang et al. proposed a Kang et al. approach, which employed only half of the pre-trained target encoder [43]. The methodology employed by the ELECTRA-DTA model aligns closely with our approach [36]. In the ELECTRA-DTA framework, the features originating from the pre-trained drug encoder and protein encoder are individually averaged. Subsequently, these averaged features are compactly represented as a compressed feature vector. This vector is subsequently incorporated into a squeeze-and-excitation network, aiming to enhance the predictive capabilities of the model. Their approach can also be perceived as a tactical maneuver to circumvent the necessity of fine-tuning the complete encoder. However, it is important to note that we could not directly compare the prediction performance of our DLM-DTI approach to that of ELECTRA-DTA due to differences in the target tasks, with DLM-DTI using binary classification and ELECTRA-DTA using \(pK_{d}\) regression.In our study, we introduced an adaptation parameter to efficiently generate meaningful protein features. The adaptation parameter, denoted as \(\lambda\), was randomly initialized and tuned. This parameter controlled the weights of knowledge from both the teacher model (providing general knowledge) and the student model (capturing task-specific knowledge). In the ablation studies (Table 6), the absence of knowledge adaptation resulted in significant degradation of performance for both the teacher-only and student-only settings. However, the DLM-DTI with knowledge adaptation exhibited weaknesses in generalization performance. Kang et al.’s [43] work also demonstrated strong performance under cold-splitting conditions (Table 8). In contrast, our DLM-DTI, which either matched or outperformed Kang et al. on the complete dataset, showed reduced effectiveness in cold-splitting evaluations, particularly concerning cold-binding interactions. This may be attributed to the over-reduction of the student model, limiting generalization performance. Inspired by recent examples that incorporate natural language-based prior knowledge to enhance prediction performance, we aim to improve our approach by adding natural language information related to the function of proteins in future work [54]. Interestingly, integrated dataset training did not prove beneficial for DLM-DTI. In Kang et al. [43], training with integrated datasets demonstrated outstanding performances. Large-scale Transformer-based architectures typically require a substantial amount of data to realize their full potential. However, DLM-DTI introduces a small-scale student model, and it is speculated that the small size was sufficient for effective learning.Recently, foundation models based on large language models have been widely studied [55, 56]. A shared challenge between these models and protein sequence encoders pertains to the intricacies involved in fine-tuning. Due to the scarcity of annotated data and the extensive parameters within these models, innovative strategies for effective fine-tuning have been proposed. For instance, a method called low-rank adaptation (LoRA) [57], similar to our own approach, adopt a technique where only the adaptation layer is adjusted. This is achieved by integrating a low-rank adaptation layer, which eliminates the need for comprehensive fine-tuning across all layers. This approach proves to be more cost-effective and quicker to converge compared to the resource-intensive process of complete fine-tuning. Therefore, in our future study, we plan to compare the performances of a fine-tuning model using LoRA’s adaptation approaches. Furthermore, there is a need for enhancement in the design of the interaction head. Currently, this component is composed of a sequence of straightforward FC layers, which exhibits reduced effectiveness in cold bindings. To address this, potential strategies include the integration of a squeeze-and-excitation network [58], capsule network [59], cross-attention [60], and other alternatives.ConclusionIn this study, we employed knowledge adaptation to efficiently and accurately predict binding probability. The knowledge adaptation was efficiently tuned with both general knowledge and task-specific knowledge through the teacher-student architectures. With only 25% of the model parameters, DLM-DTI exhibited considerable performance compared to the previous state-of-the-art model. Notably, DLM-DTI required 7.7 GB of VRAM, allowing training on conventional GPUs without the need for high-performing GPUs."
83,Availability of data and materials
83,The datasets are available at: https://github.com/kexinhuang12345/MolTrans/tree/master/dataset.
83,Code availability
83,The source codes are available at: https://github.com/jonghyunlee1993/DLM-DTI_hint-based-learning/tree/master.
83,"ReferencesAnusuya S, Kesherwani M, Priya KV, Vimala A, Shanmugam G, Velmurugan D, Gromiha MM (2018) Drug-target interactions: prediction methods and applications. Curr Protein Pept Sci 19(6):537–561Article"
83,CAS
83,PubMed
83,Google Scholar
83,Ledford H (2011) 4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. Nature looks at ways they might be saved. Nature 477(7366):526–529Article
83,CAS
83,PubMed
83,Google Scholar
83,"Zheng Y, Wu Z (2021) A machine learning-based biological drug-target interaction prediction method for a tripartite heterogeneous network. ACS Omega 6(4):3037–3045Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Ashburn TT, Thor KB (2004) Drug repositioning: identifying and developing new uses for existing drugs. Nat Rev Drug Discovery 3(8):673–683Article"
83,CAS
83,PubMed
83,Google Scholar
83,Strittmatter SM (2014) Overcoming drug development bottlenecks with repurposing: old drugs learn new tricks. Nat Med 20(6):590–591Article
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Li H, Leung K-S, Wong M-H, Ballester PJ (2015) Low-quality structural and interaction data improves binding affinity prediction via random forest. Molecules 20(6):10947–10962Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Trott O, Olson AJ (2010) Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J Comput Chem 31(2):455–461Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Luo H, Mattes W, Mendrick DL, Hong H (2016) Molecular docking for identification of potential targets for drug repurposing. Curr Top Med Chem 16(30):3636–3645Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Pahikkala T, Airola A, Pietilä S, Shakyawar S, Szwajda A, Tang J, Aittokallio T (2015) Toward more realistic drug-target interaction predictions. Brief Bioinform 16(2):325–337Article"
83,CAS
83,PubMed
83,Google Scholar
83,"He T, Heidemeyer M, Ban F, Cherkasov A, Ester M (2017) Simboost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines. J Cheminformatics 9(1):1–14Article"
83,Google Scholar
83,"Öztürk H, Özgür A, Ozkirimli E (2018) Deepdta: deep drug-target binding affinity prediction. Bioinformatics 34(17):821–829Article"
83,Google Scholar
83,"Lee I, Keum J, Nam H (2019) Deepconv-dti: prediction of drug-target interactions via deep learning with convolution on protein sequences. PLoS Comput Biol 15(6):1007129Article"
83,Google Scholar
83,"Lee I, Nam H (2022) Sequence-based prediction of protein binding regions and drug-target interactions. J Cheminformatics 14(1):1–15Article"
83,Google Scholar
83,"Zeng Y, Chen X, Luo Y, Li X, Peng D (2021) Deep drug-target binding affinity prediction with multiple attention blocks. Brief Bioinform 22(5):117Article"
83,Google Scholar
83,"Kim Y, Shin B (2021) An interpretable framework for drug-target interaction with gated cross attention. In: Machine Learning for Healthcare Conference, pp. 337–353. PMLRNguyen T, Le H, Quinn TP, Nguyen T, Le TD, Venkatesh S (2021) Graphdta: predicting drug-target binding affinity with graph neural networks. Bioinformatics 37(8):1140–1147Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Thafar MA, Alshahrani M, Albaradei S, Gojobori T, Essack M, Gao X (2022) Affinity2vec: drug-target binding affinity prediction through representation learning, graph mining, and machine learning. Sci Rep 12(1):1–18Article"
83,Google Scholar
83,"Liao J, Chen H, Wei L, Wei L (2022) Gsaml-dta: an interpretable drug-target binding affinity prediction model based on graph neural networks with self-attention mechanism and mutual information. Comput Biol Med 150:106145Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Su X, Hu L, You Z, Hu P, Wang L, Zhao B (2022) A deep learning method for repurposing antiviral drugs against new viruses via multi-view nonnegative matrix factorization and its application to sars-cov-2. Brief Bioinform 23(1):526Article"
83,Google Scholar
83,"Li Y-C, You Z-H, Yu C-Q, Wang L, Wong L, Hu L, Hu P-W, Huang Y-A (2022) Ppaedti: personalized propagation auto-encoder model for predicting drug-target interactions. IEEE J Biomed Health Inform 27(1):573–582Article"
83,Google Scholar
83,"Thafar MA, Olayan RS, Albaradei S, Bajic VB, Gojobori T, Essack M, Gao X (2021) Dti2vec: drug-target interaction prediction using network embedding and ensemble learning. J Cheminformatics 13(1):1–18Article"
83,Google Scholar
83,"Zhao L, Wang J, Pang L, Liu Y, Zhang J (2020) Gansdta: predicting drug-target binding affinity using gans. Front Genetics 1243Chen Y, Wang Z, Wang L, Wang J, Li P, Cao D, Zeng X, Ye X, Sakurai T (2023) Deep generative model for drug design from protein target sequence. J Cheminformatics 15(1):38Article"
83,CAS
83,Google Scholar
83,"Liu G, Singha M, Pu L, Neupane P, Feinstein J, Wu H-C, Ramanujam J, Brylinski M (2021) Graphdti: a robust deep learning predictor of drug-target interactions from multiple heterogeneous data. J Cheminformatics 13(1):1–17Article"
83,Google Scholar
83,"Yan X, Liu Y (2022) Graph-sequence attention and transformer for predicting drug-target affinity. RSC Adv 12(45):29525–29534Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Hua Y, Song X, Feng Z, Wu X (2023) Mfr-dta: a multi-functional and robust model for predicting drug-target binding affinity and region. Bioinformatics 39(2):056Article"
83,Google Scholar
83,"Bian J, Zhang X, Zhang X, Xu D, Wang G (2023) Mcanet: shared-weight-based multiheadcrossattention network for drug-target interaction prediction. Brief Bioinform 24(2):082Article"
83,Google Scholar
83,"Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Proc Syst 30Huang K, Xiao C, Glass LM, Sun J (2021) Moltrans: molecular interaction transformer for drug-target interaction prediction. Bioinformatics 37(6):830–836Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Honda S, Shi S, Ueda HR (2019) Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738Chithrananda S, Grand G, Ramsundar B (2020) Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Molecule attention transformer. arXiv preprint arXiv:2002.08264Fabian B, Edlich T, Gaspar H, Segler M, Meyers J, Fiscato M, Ahmed M (2020) Molecular representation learning with language models and domain-relevant auxiliary tasks. arXiv preprint arXiv:2011.13230Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) Unified rational protein engineering with sequence-based deep representation learning. Nat Methods 16(12):1315–1322Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M, et al (2020) Prottrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225Wang J, Wen N, Wang C, Zhao L, Cheng L (2022) Electra-dta: a new compound-protein binding affinity prediction model based on the contextualized sequence encoding. J Cheminformatics 14(1):1–14Article"
83,Google Scholar
83,"Shin B, Park S, Kang K, Ho JC (2019) Self-attention based molecule representation for predicting drug-target interaction. In: Machine Learning for Healthcare Conference, pp. 230–248. PMLRXiong Y, Zeng Z, Chakraborty R, Tan M, Fung G, Li Y, Singh V (2021) Nyströmformer: A nyström-based algorithm for approximating self-attention. Proc AAAI Conf Artif Intell 35:14138–14148PubMed"
83,PubMed Central
83,Google Scholar
83,"Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509Press O, Smith NA, Lewis M (2021) Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409Dao T, Fu D, Ermon S, Rudra A, Ré C (2022) Flashattention: fast and memory-efficient exact attention with io-awareness. Adv Neural Inf Process Syst 35:16344–16359"
83,Google Scholar
83,"Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805Kang H, Goo S, Lee H, Chae J-W, Yun H-Y, Jung S (2022) Fine-tuning of bert model to accurately predict drug-target interactions. Pharmaceutics 14(8):1710Article"
83,CAS
83,PubMed
83,PubMed Central
83,Google Scholar
83,"Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531Gou J, Yu B, Maybank SJ, Tao D (2021) Knowledge distillation: a survey. Int J Comput Vision 129:1789–1819Article"
83,Google Scholar
83,"Geffen Y, Ofran Y, Unger R (2022) Distilprotbert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts. Bioinformatics 38(Supplement–2):95–98Article"
83,Google Scholar
83,"Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550Davis MI, Hunt JP, Herrgard S, Ciceri P, Wodicka LM, Pallares G, Hocker M, Treiber DK, Zarrinkar PP (2011) Comprehensive analysis of kinase inhibitor selectivity. Nat Biotechnol 29(11):1046–1051Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Liu T, Lin Y, Wen X, Jorissen RN, Gilson MK (2007) Bindingdb: a web-accessible database of experimentally determined protein-ligand binding affinities. Nucleic Acids Res 35(suppl-1):198–201Article"
83,Google Scholar
83,"Saito T, Rehmsmeier M (2015) The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. PLoS ONE 10(3):0118432Article"
83,Google Scholar
83,"Kumar A, Raghunathan A, Jones RM, Ma T, Liang P (2022) Fine-tuning can distort pretrained features and underperform out-of-distribution. In: International Conference on Learning Representations. https://openreview.net/forum?id=UYneFzXSJWhAlain G, Bengio Y (2016) Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644Chatterjee A, Walters R, Shafi Z, Ahmed OS, Sebek M, Gysi D, Yu R, Eliassi-Rad T, Barabási A-L, Menichetti G (2021) Ai-bind: improving binding predictions for novel protein targets and ligands. arXiv preprint arXiv:2112.13168Chen YT, Zou J (2023) Genept: a simple but hard-to-beat foundation model for genes and cells built from chatgpt. bioRxiv, 2023–10Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al (2023) Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham P, Chung HW, Sutton C, Gehrmann S, et al (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W (2021) Lora: low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141Sabour S, Frosst N, Hinton GE (2017) Dynamic routing between capsules. Adv Neural Inf Proc Syst 30Gheini M, Ren X, May J (2021) Cross-attention is all you need: adapting pretrained transformers for machine translation. arXiv preprint arXiv:2104.08771Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692Shibata Y, Kida T, Fukamachi S, Takeda M, Shinohara A, Shinohara T, Arikawa S (1999) Byte pair encoding: a text compression scheme that accelerates pattern matchingRogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Inf Model 50(5):742–754Article"
83,CAS
83,PubMed
83,Google Scholar
83,"Katharopoulos A, Vyas A, Pappas N, Fleuret F (2020) Transformers are rnns: Fast autoregressive transformers with linear attention. In: International Conference on Machine Learning, pp. 5156–5165. PMLRDownload referencesFunding(1) This work was supported by research grants from Daegu Catholic University in 2022. (2) This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (RS-2022-00166945).Author informationAuthors and AffiliationsDepartment of Medical and Digital Engineering, Hanyang University College of Engineering, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaJonghyun Lee & Dae Won JunDepartment of Internal Medicine, Hanyang University College of Medicine, 222, Wangsimni-ro, Seongdong-gu, Seoul, 04763, KoreaDae Won JunDepartment of Pharmaceutical Science and Technology, Kyungsung University, 309, Suyeong-ro, Nam-gu, Busan, 48434, KoreaIldae SongCollege of Pharmacy, Deagu Catholic University, 13-13, Hayang-ro, Hayang-eup, Gyeongsan-si, 38430, Gyeongsangbuk-do, KoreaYun KimAuthorsJonghyun LeeView author publicationsYou can also search for this author in"
83,PubMed Google ScholarDae Won JunView author publicationsYou can also search for this author in
83,PubMed Google ScholarIldae SongView author publicationsYou can also search for this author in
83,PubMed Google ScholarYun KimView author publicationsYou can also search for this author in
83,"PubMed Google ScholarContributionsConceptualization, JL, DJ, IS, and YK; methodology, JL, and YK; writing—original draft preparation, JL; writing—review and editing, YK; supervision, DJ, and YK; formal analysis, JL; resources, YK; All authors have read and agreed to the published version of the manuscript.Corresponding authorCorrespondence to"
83,Yun Kim.Ethics declarations
83,Ethics approval and consent to participate
83,Not applicable.
83,Consent for publication
83,Not applicable.
83,Competing interests
83,The author(s) declare that they have no conflict of interest.
83,"Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.AppendicesAppendix A Knowledge DistillationRecent high-performing DNN models boast millions or billions of parameters, necessitating extensive and high-performance hardware resources, such as GPU clusters and TPU pods. Knowledge distillation was proposed to develop a lightweight model while retaining robust information processing capabilities [44, 45]. The knowledge distillation process involves two models, specifically the teacher model and the student model. Conventionally, knowledge distillation begins by training the teacher model, a complex and high-capacity model, on the target task. Subsequently, the acquired knowledge from the teacher model is transferred to the student model, a more lightweight counterpart. This transfer is typically achieved by encouraging the student model to mimic the outputs [44] or internal representations [47] of the teacher model. The overarching goal is to distill the comprehensive knowledge captured by the teacher model into a more compact and computationally efficient student model.FitNet [47] introduces the concept of “hints” to enhance the knowledge distillation approach. In addition to replicating the output of the current teacher model, hints guide the student to mimic intermediate features together. This inclusion of hints enhances the performance of knowledge distillation by enabling the learning of not only the final result but also the intermediate features. In this context, a hint can be interpreted as providing information about both the intermediate features and the final feature.Appendix B Drug Encoder: ChemBERTaChemBERTa is a Transformer-based model pre-trained using 10 million SMILES sequences [31]. Based on RoBERTa [61], a model known for its outstanding performance in natural language processing, ChemBERTa comprises 12 attention heads and 6 layers. Drug sequences, expressed in Canonical SMILES, are tokenized using a subword-level tokenizer, while a byte-pair encoder (BPE) tokenizer is employed to group frequently occurring elements together into larger chunks for more efficient processing. BPE stands as a blend of character and word-level representations, facilitating the management of extensive vocabularies in natural language corpora. Guided by the insight that less common or unfamiliar words can frequently be broken down into several recognized subwords, BPE identifies the optimal word segmentation through an iterative and greedy merging of frequently occurring character pairs [62]. ChemBERTa has a total of 767 tokens, including a class token to encapsulate the abstract meaning of the entire sequence, a start of sequence token (SOS), an end of sequence token (EOS), and a pad token to mark the start and end of the sequence.ChemBERTa was trained using masked language modeling (MLM), where the task involves masking a portion of the entire sequence and then restoring the corresponding tokens; 15% of the total sequence was masked. The maximum processable sequence length is 512 tokens. ChemBERTa, pre-trained using MLM tasks, can then be used as an encoder for drug sequences because it has been trained on restoration tasks and has an understanding of molecule sequences. ChemBERTa can perform comparably to the commonly used extended-connectivity fingerprint (ECFP) [63] in molecule properties prediction tasks using the ChemBERTa encoder, and it was employed in this study due to its availability through the HuggingFace API, facilitating easy utilization.Appendix C Target Encoder: ProtBERTProtBERT, a component of the ProtTrans project, is a BERT model trained on an extensive dataset of amino acid sequences [35]. It underwent training using the same MLM approach as ChemBERTa, with 15% masking (Appendix B). However, owing to the intricacy of amino acid sequences, ProtBERT consists of 30 layers and 16 attention heads, resulting in a total parameter count of 4.2 million. Each element is considered one token in ProtBERT, and it comprises 30 tokens, including special tokens. Notably, it was trained to handle sequences of up to 4000 tokens, accommodating the typically extended length of amino acid sequences.However, ProtBERT uses the Transformer’s core operation, self-attention, where the amount of computation increases as the square of the length of a given sequence. The self-attention operation is as follows:$$\begin{aligned} \text {Attention}(Q, K) = \text {softmax} \left( \frac{{QK^T}}{{\sqrt{d_k}}}\right) , \end{aligned}$$"
83,(C1)
83,"where the query (Q) is the product of input sequence x and learnable parameter \(W_{Q}\), and key (K) is the product of input sequence x and learnable parameter \(W_{K}\).Therefore, a substantial amount of memory and computational resources must be allocated to manage long sequences of amino acids. This constitutes a significant bottleneck in the practical utilization of ProtBERT. While recent proposals, such as efficient self-attention computations using linear transformers [64] and Nystrom approximation [38], aim to address this challenge, pre-training with such approaches remains expensive. As an illustration, ProtBERT underwent training utilizing 1,024 tensor processing units (TPUs), a resource allocation typically inaccessible in standard research environments. Consequently, this study emphasizes the efficient utilization of the previously published ProtBERT, prioritizing practical application over creating a new pre-training model that might reduce computational requirements.Rights and permissions"
83,Open Access
83,"This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data."
83,"Reprints and permissionsAbout this articleCite this articleLee, J., Jun, D.W., Song, I. et al. DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning."
83,"J Cheminform 16, 14 (2024). https://doi.org/10.1186/s13321-024-00808-1Download citationReceived: 09 September 2023Accepted: 22 January 2024Published: 01 February 2024DOI: https://doi.org/10.1186/s13321-024-00808-1Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard"
83,Provided by the Springer Nature SharedIt content-sharing initiative
83,KeywordsDrug-target interactionsPre-trained language modelKnowledge adaptationLightweight framework
83,Download PDF
83,Download ePub
83,Advertisement
83,Journal of Cheminformatics
83,ISSN: 1758-2946
83,Contact us
83,Submission enquiries: journalsubmissions@springernature.com
83,Read more on our blogs
83,Receive BMC newsletters
83,Manage article alerts
83,Language editing for authors
83,Scientific editing for authors
83,Policies
83,Accessibility
83,Press center
83,Support and Contact
83,Leave feedback
83,Careers
83,Follow BMC
83,BMC Twitter page
83,BMC Facebook page
83,BMC Weibo page
83,"By using this website, you agree to our"
83,"Terms and Conditions,"
83,"Your US state privacy rights,"
83,Privacy
83,statement and
83,Cookies policy.
83,Your privacy choices/Manage cookies we use in the preference centre.
83,© 2024 BioMed Central Ltd unless otherwise stated. Part of
83,Springer Nature.
84,Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers) · GitHub
84,Skip to content
84,All gists
84,Back to GitHub
84,Sign in
84,Sign up
84,Sign in
84,Sign up
84,You signed in with another tab or window. Reload to refresh your session.
84,You signed out in another tab or window. Reload to refresh your session.
84,You switched accounts on another tab or window. Reload to refresh your session.
84,Dismiss alert
84,"Instantly share code, notes, and snippets."
84,fevangelou/my.cnf
84,Last active
84,"February 14, 2024 09:14"
84,Star
84,104
84,You must be signed in to star a gist
84,Fork
84,You must be signed in to fork a gist
84,Star
84,You must be signed in to star a gist
84,Code
84,Revisions
84,Stars
84,104
84,Forks
84,Embed
84,Embed
84,Embed this gist in your website.
84,Share
84,Copy sharable link for this gist.
84,Clone via HTTPS
84,Clone using the web URL.
84,Learn more about clone URLs
84,Clone this repository at &lt;script src=&quot;https://gist.github.com/fevangelou/0da9941e67a9c9bb2596.js&quot;&gt;&lt;/script&gt;
84,Save fevangelou/0da9941e67a9c9bb2596 to your computer and use it in GitHub Desktop.
84,Download ZIP
84,Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers)
84,Raw
84,my.cnf
84,# === Optimized my.cnf configuration for MySQL/MariaDB (on cPanel/WHM servers) ===
84,"# by Fotis Evangelou, developer of Engintron (engintron.com)"
84,# ~ Updated December 2021 ~
84,# The settings provided below are a starting point for a 8-16 GB RAM server with 4-8 CPU cores.
84,"# If you have different resources available you should adjust accordingly to save CPU, RAM & disk I/O usage."
84,"# The settings marked with a specific comment or the word ""UPD"" (after the value)"
84,# should be adjusted for your system by using database diagnostics tools like:
84,# https://github.com/major/MySQLTuner-perl
84,# or
84,# https://github.com/BMDan/tuning-primer.sh
84,"# Run either of these scripts before optimizing your database, at least 1 hr after the optimization & finally"
84,# at least once a day for 3 days (without restarting the database) to see how your server performs and if you need
84,"# to re-adjust anything. The more MySQL/MariaDB runs without restarting, the more usage data it gathers, so these"
84,# diagnostics scripts will report in mode detail how MySQL/MariaDB performs.
84,"# IMPORTANT NOTE: If there is NO comment after a setting value, then 99,9% of the times you won't need to adjust it."
84,# --- THINGS TO DO AFTER YOU UPDATE MY.CNF - TROUBLESHOOTING ---
84,"# If any terminal commands are mentioned, make sure you execute them as ""root"" user."
84,"# If MySQL or MariaDB cannot start (or restart), then perform the following actions."
84,# 1. If the server had the stock database configuration and you added or updated any
84,"""innodb_log_*"" settings (as suggested below), then execute these commands ONLY"
84,the first time you apply this configuration:
84,$ rm -rvf /var/lib/mysql/ib_logfile*
84,$ touch /var/lib/mysql/mysql.sock
84,$ touch /var/lib/mysql/mysql.pid
84,$ chown -R mysql:mysql /var/lib/mysql
84,$ /scripts/restartsrv_mysql
84,or use the shorthand command:
84,$ rm -rvf /var/lib/mysql/ib_logfile*; touch /var/lib/mysql/mysql.sock; touch /var/lib/mysql/mysql.pid; chown -R mysql:mysql /var/lib/mysql; /scripts/restartsrv_mysql
84,"IMPORTANT: If you edit this file from the Engintron WHM app in cPanel/WHM,"
84,then you DO NOT need to execute the above terminal commands. When you save
84,"the file through the Engintron WHM app, these terminal commands will be"
84,executed automatically after the file is saved on disk.
84,"# 2. If the setting ""bind-address"" is not commented out, then make sure the file /etc/hosts is"
84,"properly configured. A good example of a ""clean"" /etc/hosts file is something like this:"
84,127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
84,::1
84,localhost localhost.localdomain localhost6 localhost6.localdomain6
84,1.2.3.4
84,hostname.domain.tld hostname # <-- Replace accordingly!
84,Finally restart the database using the related cPanel script:
84,$ /scripts/restartsrv_mysql
84,"# 3. If the database service cannot restart even after the first 2 steps, make sure the database data folder"
84,"(common for either MySQL or MariaDB) ""/var/lib/mysql"" is owned by the ""mysql"" user AND group."
84,"Additionally, the folder itself can have 0751 or 0755 file permissions. To fix it, simply do this:"
84,$ chown -R mysql:mysql /var/lib/mysql
84,$ chmod 0755 /var/lib/mysql
84,Finally restart the database using the related cPanel script:
84,$ /scripts/restartsrv_mysql
84,"# 4. Adjust SQL settings under ""Tweak Settings"" in WHM:"
84,"After applying the optimized my.cnf file, you'll also want to DISABLE the following 3 settings"
84,"in the ""SQL"" tab of Tweak Settings in WHM:"
84,- Allow cPanel & WHM to determine the best value for your MySQL open_files_limit configuration?
84,- Allow cPanel & WHM to determine the best value for your MySQL max_allowed_packet configuration?
84,- Allow cPanel & WHM to determine the best value for your MySQL innodb_buffer_pool_size configuration?
84,# ~ FIN ~
84,[mysql]
84,port
84,= 3306
84,socket
84,= /var/lib/mysql/mysql.sock
84,[mysqld]
84,# === Required Settings ===
84,basedir
84,= /usr
84,bind_address
84,= 127.0.0.1 # Change to 0.0.0.0 to allow remote connections
84,datadir
84,= /var/lib/mysql
84,#default_authentication_plugin
84,= mysql_native_password # Enable in MySQL 8+ or MariaDB 10.6+ for backwards compatibility with common CMSs
84,max_allowed_packet
84,= 256M
84,max_connect_errors
84,= 1000000
84,pid_file
84,= /var/lib/mysql/mysql.pid
84,port
84,= 3306
84,skip_external_locking
84,socket
84,= /var/lib/mysql/mysql.sock
84,tmpdir
84,= /tmp
84,user
84,= mysql
84,# === SQL Compatibility Mode ===
84,# Enable for b/c with databases created in older MySQL/MariaDB versions
84,# (e.g. when using null dates)
84,#sql_mode
84,"= ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES"
84,# Crappy SQL queries/schema? Go bold!
84,#sql_mode
84,"= """""
84,# === InnoDB Settings ===
84,default_storage_engine
84,= InnoDB
84,innodb_buffer_pool_instances
84,= 4
84,# Use 1 instance per 1GB of InnoDB pool size - max is 64
84,innodb_buffer_pool_size
84,= 4G
84,# Use up to 70-80% of RAM
84,innodb_file_per_table
84,= 1
84,innodb_flush_log_at_trx_commit
84,= 0
84,innodb_flush_method
84,= O_DIRECT
84,innodb_log_buffer_size
84,= 16M
84,innodb_log_file_size
84,= 1G
84,innodb_sort_buffer_size
84,= 4M
84,# UPD - Defines how much data is read into memory for sorting operations before writing to disk (default is 1M / max is 64M)
84,innodb_stats_on_metadata
84,= 0
84,#innodb_use_fdatasync
84,= 1
84,# Only (!) for MySQL v8.0.26+
84,#innodb_temp_data_file_path
84,= ibtmp1:64M:autoextend:max:20G # Control the maximum size for the ibtmp1 file
84,#innodb_thread_concurrency
84,= 4
84,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
84,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
84,# the overall load produced by MySQL/MariaDB.
84,innodb_read_io_threads
84,= 64
84,innodb_write_io_threads
84,= 64
84,#innodb_io_capacity
84,= 2000
84,"# Depends on the storage tech - use 2000 for SSD, more for NVMe"
84,#innodb_io_capacity_max
84,= 4000
84,# Usually double the value of innodb_io_capacity
84,# === MyISAM Settings ===
84,# The following 3 options are ONLY supported by MariaDB & up to MySQL 5.7
84,# Do NOT un-comment on MySQL 8.x+
84,#query_cache_limit
84,= 4M
84,# UPD
84,#query_cache_size
84,= 64M
84,# UPD
84,#query_cache_type
84,= 1
84,# Enabled by default
84,key_buffer_size
84,= 24M
84,# UPD
84,low_priority_updates
84,= 1
84,concurrent_insert
84,= 2
84,# === Connection Settings ===
84,max_connections
84,= 100
84,# UPD - Important: high no. of connections = high RAM consumption
84,back_log
84,= 512
84,thread_cache_size
84,= 100
84,thread_stack
84,= 192K
84,interactive_timeout
84,= 180
84,wait_timeout
84,= 180
84,# For MySQL 5.7+ only (disabled by default)
84,#max_execution_time
84,= 90000 # Set a timeout limit for SELECT statements (value in milliseconds).
84,"# This option may be useful to address aggressive crawling on large sites,"
84,# but it can also cause issues (e.g. with backups). So use with extreme caution and test!
84,# More info at: https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time
84,# For MariaDB 10.1.1+ only (disabled by default)
84,#max_statement_time
84,= 90
84,"# The equivalent of ""max_execution_time"" in MySQL 5.7+ (set above)"
84,"# The variable is of type double, thus you can use subsecond timeout."
84,# For example you can use value 0.01 for 10 milliseconds timeout.
84,# More info at: https://mariadb.com/kb/en/aborting-statements/
84,# === Buffer Settings ===
84,# Handy tip for managing your database's RAM usage:
84,"# The following values should be treated carefully as they are added together and then multiplied by your ""max_connections"" value."
84,"# Other options will also add up to RAM consumption (e.g. tmp_table_size). So don't go switching your ""join_buffer_size"" to 1G, it's harmful & inefficient."
84,"# Use one of the database diagnostics tools mentioned at the top of this file to count your database's potential total RAM usage, so you know if you are within"
84,"# reasonable limits. Remember that other services will require enough RAM to operate properly (like Apache or PHP-FPM), so set your limits wisely."
84,join_buffer_size
84,= 4M
84,# UPD
84,read_buffer_size
84,= 3M
84,# UPD
84,read_rnd_buffer_size
84,= 4M
84,# UPD
84,sort_buffer_size
84,= 4M
84,# UPD
84,# === Table Settings ===
84,"# In systemd managed systems like Ubuntu 16.04+ or CentOS 7+, you need to perform an extra action for table_open_cache & open_files_limit"
84,# to be overriden (also see comment next to open_files_limit).
84,"# E.g. for MySQL 5.7, please check: https://dev.mysql.com/doc/refman/5.7/en/using-systemd.html"
84,# and for MariaDB check: https://mariadb.com/kb/en/library/systemd/
84,table_definition_cache
84,= 40000 # UPD
84,table_open_cache
84,= 40000 # UPD
84,open_files_limit
84,= 60000 # UPD - This can be 2x to 3x the table_open_cache value or match the system's
84,# open files limit usually set in /etc/sysctl.conf and /etc/security/limits.conf
84,# In systemd managed systems this limit must also be set in:
84,# - /etc/systemd/system/mysql.service.d/override.conf (for MySQL 5.7+ in Ubuntu) or
84,# - /etc/systemd/system/mysqld.service.d/override.conf (for MySQL 5.7+ in CentOS) or
84,# - /etc/systemd/system/mariadb.service.d/override.conf (for MariaDB)
84,# otherwise changing open_files_limit will have no effect.
84,# To edit the right file execute:
84,# $ systemctl edit mysql (or mysqld or mariadb)
84,"# and set ""LimitNOFILE="" to something like 100000 or more (depending on your system limits for MySQL)"
84,"# or use ""LimitNOFILE=infinity"" for MariaDB only."
84,# Finally merge the changes with:
84,# $ systemctl daemon-reload; systemctl restart mysql (or mysqld or mariadb)
84,max_heap_table_size
84,= 128M
84,# Increase to 256M or 512M if you have lots of temporary tables because of missing indices in JOINs
84,tmp_table_size
84,= 128M
84,# Use same value as max_heap_table_size
84,# === Search Settings ===
84,ft_min_word_len
84,= 3
84,# Minimum length of words to be indexed for search results
84,# === Binary Logging ===
84,disable_log_bin
84,= 1
84,# Binary logging disabled by default
84,#log_bin
84,"# To enable binary logging, uncomment this line & only one of the following 2 lines"
84,# that corresponds to your actual MySQL/MariaDB version.
84,"# Remember to comment out the line with ""disable_log_bin""."
84,#expire_logs_days
84,= 1
84,# Keep logs for 1 day - For MySQL 5.x & MariaDB before 10.6 only
84,#binlog_expire_logs_seconds
84,= 86400 # Keep logs for 1 day (in seconds) - For MySQL 8+ & MariaDB 10.6+ only
84,# === Error & Slow Query Logging ===
84,log_error
84,= /var/lib/mysql/mysql_error.log
84,log_queries_not_using_indexes
84,= 0
84,# Disabled on production
84,long_query_time
84,= 5
84,slow_query_log
84,= 0
84,# Disabled on production
84,slow_query_log_file
84,= /var/lib/mysql/mysql_slow.log
84,[mysqldump]
84,# Variable reference
84,# For MySQL 5.7+:
84,https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html
84,# For MariaDB:
84,https://mariadb.com/kb/en/library/mysqldump/
84,quick
84,quote_names
84,max_allowed_packet
84,= 1024M
84,Load earlier comments...
84,Copy link
84,Author
84,fevangelou
84,commented
84,"May 19, 2020"
84,"Run systemctl status mysqld.service to see why MySQL won't start. If there is no practical hint there, see MySQL's error log."
84,"In any case, make sure the contents of this my.cnf are properly copied into your server's /etc/my.cnf file."
84,"Sorry, something went wrong."
84,Copy link
84,EvangelosBalafoutis
84,commented
84,"May 19, 2020"
84,edited by fevangelou
84,Hello Fotis.
84,The file is properly copied. I tried on another server with no problem to restart.
84,Because I had mysql down I return to the old my.cnf.
84,But mmy error log says
84,2020-05-18T22:39:27.121991Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:27.122710Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:28.435750Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:28.435948Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:28.598329Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:28.598350Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:28.598384Z 0 [Note] Governor connected
84,2020-05-18T22:39:28.598388Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:28.598695Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:28.598700Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:28.598705Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:28.599674Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1108942 ...
84,2020-05-18T22:39:28.603292Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:28.603317Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:28.603321Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:28.603324Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:28.603327Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:28.603329Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:28.603503Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:28.603583Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:28.629156Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:28.629174Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:29.129267Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:29.629363Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:30.129474Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:30.629602Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:31.129723Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:31.129751Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:31.130257Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:31.130274Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:31.130282Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:31.130287Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:31.130293Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:31.130297Z 0 [ERROR] Aborting
84,2020-05-18T22:39:31.130318Z 0 [Note] Binlog end
84,2020-05-18T22:39:31.130399Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:31.130410Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:31.131464Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:32.431097Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:32.431252Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:32.588494Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:32.588515Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:32.588550Z 0 [Note] Governor connected
84,2020-05-18T22:39:32.588554Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:32.588858Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:32.588862Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:32.588868Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:32.589848Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1108988 ...
84,2020-05-18T22:39:32.593669Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:32.593689Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:32.593693Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:32.593698Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:32.593701Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:32.593704Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:32.593894Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:32.593984Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:32.620481Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:32.620500Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:33.120626Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:33.620740Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:34.120842Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:34.620919Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:35.121029Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:35.121039Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:35.121400Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:35.121408Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:35.121413Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:35.121417Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:35.121421Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:35.121423Z 0 [ERROR] Aborting
84,2020-05-18T22:39:35.121437Z 0 [Note] Binlog end
84,2020-05-18T22:39:35.121492Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:35.121498Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:35.122266Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:36.439434Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:36.439590Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:36.598534Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:36.598555Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:36.598592Z 0 [Note] Governor connected
84,2020-05-18T22:39:36.598596Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:36.598953Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:36.598957Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:36.598963Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:36.599938Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109023 ...
84,2020-05-18T22:39:36.603630Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:36.603653Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:36.603657Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:36.603662Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:36.603665Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:36.603668Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:36.603841Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:36.603944Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:36.628651Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:36.628672Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:37.128780Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:37.628892Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:38.129025Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:38.629111Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:39.129214Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:39.129227Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:39.129601Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:39.129609Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:39.129615Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:39.129619Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:39.129623Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:39.129625Z 0 [ERROR] Aborting
84,2020-05-18T22:39:39.129644Z 0 [Note] Binlog end
84,2020-05-18T22:39:39.129689Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:39.129695Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:39.130426Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:40.428814Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:40.429004Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:40.588371Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:40.588393Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:40.588427Z 0 [Note] Governor connected
84,2020-05-18T22:39:40.588431Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:40.588735Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:40.588739Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:40.588745Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:40.589742Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109081 ...
84,2020-05-18T22:39:40.593523Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:40.593546Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:40.593550Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:40.593553Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:40.593557Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:40.593560Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:40.593727Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:40.593805Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:40.620202Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:40.620220Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:41.120328Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:41.620456Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:42.120598Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:42.620718Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:43.120849Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:43.120866Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:43.121237Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:43.121248Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:43.121254Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:43.121257Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:43.121261Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:43.121264Z 0 [ERROR] Aborting
84,2020-05-18T22:39:43.121278Z 0 [Note] Binlog end
84,2020-05-18T22:39:43.121321Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:43.121338Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:43.122104Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:44.430237Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:44.430391Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:44.588306Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:44.588328Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:44.588363Z 0 [Note] Governor connected
84,2020-05-18T22:39:44.588367Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:44.588673Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:44.588677Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:44.588683Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:44.589669Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109129 ...
84,2020-05-18T22:39:44.593358Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:44.593382Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:44.593385Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:44.593388Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:44.593391Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:44.593394Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:44.593562Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:44.593641Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:44.620025Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:44.620046Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:45.120147Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:45.620277Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:46.120412Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:46.620542Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:47.120657Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:47.120668Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:47.121029Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:47.121038Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:47.121044Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:47.121047Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:47.121051Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:47.121054Z 0 [ERROR] Aborting
84,2020-05-18T22:39:47.121067Z 0 [Note] Binlog end
84,2020-05-18T22:39:47.121111Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:47.121118Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:47.121808Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,2020-05-18T22:39:48.431640Z 0 [Warning] Could not increase number of max_open_files to more than 50000 (request: 80110)
84,2020-05-18T22:39:48.431795Z 0 [Warning] Changed limits: table_open_cache: 24945 (requested 40000)
84,2020-05-18T22:39:48.588549Z 0 [Note] libgovernor.so found
84,2020-05-18T22:39:48.588570Z 0 [Note] All governors functions found too
84,2020-05-18T22:39:48.588605Z 0 [Note] Governor connected
84,2020-05-18T22:39:48.588609Z 0 [Note] All governors lve functions found too
84,"2020-05-18T22:39:48.588923Z 0 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --slow-query-log is not set"
84,2020-05-18T22:39:48.588928Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
84,2020-05-18T22:39:48.588933Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
84,2020-05-18T22:39:48.589908Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.29-cll-lve) starting as process 1109169 ...
84,2020-05-18T22:39:48.593632Z 0 [Note] InnoDB: PUNCH HOLE support available
84,2020-05-18T22:39:48.593658Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
84,2020-05-18T22:39:48.593661Z 0 [Note] InnoDB: Uses event mutexes
84,2020-05-18T22:39:48.593664Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
84,2020-05-18T22:39:48.593667Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
84,2020-05-18T22:39:48.593670Z 0 [Note] InnoDB: Using Linux native AIO
84,2020-05-18T22:39:48.593843Z 0 [Note] InnoDB: Number of pools: 1
84,2020-05-18T22:39:48.593948Z 0 [Note] InnoDB: Using CPU crc32 instructions
84,2020-05-18T22:39:48.619840Z 0 [Warning] InnoDB: io_setup() failed with EAGAIN. Will make 5 attempts before giving up.
84,2020-05-18T22:39:48.619863Z 0 [Warning] InnoDB: io_setup() attempt 1.
84,2020-05-18T22:39:49.119916Z 0 [Warning] InnoDB: io_setup() attempt 2.
84,2020-05-18T22:39:49.620033Z 0 [Warning] InnoDB: io_setup() attempt 3.
84,2020-05-18T22:39:50.120145Z 0 [Warning] InnoDB: io_setup() attempt 4.
84,2020-05-18T22:39:50.620255Z 0 [Warning] InnoDB: io_setup() attempt 5.
84,2020-05-18T22:39:51.120368Z 0 [ERROR] InnoDB: io_setup() failed with EAGAIN after 5 attempts.
84,2020-05-18T22:39:51.120379Z 0 [Note] InnoDB: You can disable Linux Native AIO by setting innodb_use_native_aio = 0 in my.cnf
84,2020-05-18T22:39:51.120748Z 0 [ERROR] InnoDB: Cannot initialize AIO sub-system
84,2020-05-18T22:39:51.120757Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error
84,2020-05-18T22:39:51.120763Z 0 [ERROR] Plugin 'InnoDB' init function returned error.
84,2020-05-18T22:39:51.120767Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
84,2020-05-18T22:39:51.120771Z 0 [ERROR] Failed to initialize builtin plugins.
84,2020-05-18T22:39:51.120773Z 0 [ERROR] Aborting
84,2020-05-18T22:39:51.120788Z 0 [Note] Binlog end
84,2020-05-18T22:39:51.120831Z 0 [Note] Shutting down plugin 'CSV'
84,2020-05-18T22:39:51.120847Z 0 [Note] Shutting down plugin 'MyISAM'
84,2020-05-18T22:39:51.121623Z 0 [Note] /usr/sbin/mysqld: Shutdown complete
84,"Sorry, something went wrong."
84,Copy link
84,EvangelosBalafoutis
84,commented
84,"May 21, 2020"
84,edited by fevangelou
84,Hello Fotis. I removed every line had to do with innodb and it did restarted if this make sence.
84,I removed
84,# InnoDB Settings
84,default_storage_engine
84,= InnoDB
84,innodb_buffer_pool_instances
84,= 2
84,# Use 1 instance per 1GB of InnoDB pool size
84,innodb_buffer_pool_size
84,= 2G
84,# Use up to 70-80% of RAM
84,innodb_file_per_table
84,= 1
84,innodb_flush_log_at_trx_commit
84,= 0
84,innodb_flush_method
84,= O_DIRECT
84,innodb_log_buffer_size
84,= 16M
84,innodb_log_file_size
84,= 512M
84,innodb_stats_on_metadata
84,= 0
84,#innodb_temp_data_file_path
84,= ibtmp1:64M:autoextend:max:20G # Control the maximum size for the ibtmp1 file
84,#innodb_thread_concurrency
84,= 7
84,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
84,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
84,# the overall load produced by MySQL/MariaDB.
84,innodb_read_io_threads
84,= 64
84,innodb_write_io_threads
84,= 64
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"May 22, 2020"
84,"@EvangelosBalafoutis This is not a solution though... Seeing the logs that you sent, it's most likely you have limits enforced by MySQL Governor (by CloudLinux). For the record, unless your server hosts 300+ accounts, you probably don't need CloudLinux. And hey, if MySQL Governor worked, you wouldn't be looking to optimize MySQL, right? Food for thought..."
84,"Sorry, something went wrong."
84,Copy link
84,EvangelosBalafoutis
84,commented
84,"May 22, 2020"
84,"Thank you for the info Fotis, Ill think about asking the client to remove it."
84,"Sorry, something went wrong."
84,Copy link
84,EvangelosBalafoutis
84,commented
84,"May 22, 2020"
84,Or I'll try with completely remove mysql govenor and let you know. Thank you my friend.
84,"Sorry, something went wrong."
84,Copy link
84,ghost
84,commented
84,"Nov 11, 2020"
84,edited by fevangelou
84,Cant do anything on wp site without mysql and php-fpm using 60% or higher cpu. What is causing this? Here is my.cnf
84,[mysql]
84,port
84,= 3306
84,socket
84,= /var/lib/mysql/mysql.sock
84,[mysqld]
84,# Required Settings
84,basedir
84,= /usr
84,bind_address
84,= 0.0.0.0 # Change to 0.0.0.0 to allow remote connections
84,datadir
84,= /var/lib/mysql
84,max_allowed_packet
84,= 16M
84,max_connect_errors
84,= 1000000
84,pid_file
84,= /var/lib/mysql/mysql.pid
84,port
84,= 3306
84,skip_external_locking
84,socket
84,= /var/lib/mysql/mysql.sock
84,tmpdir
84,= /tmp
84,user
84,= mysql
84,performance_schema
84,= ON
84,skip-name-resolve
84,# to pinpoint aborted connection we need this:
84,log-warnings=2
84,# InnoDB Settings
84,default_storage_engine
84,= InnoDB
84,innodb_buffer_pool_instances
84,= 4
84,# Use 1 instance per 1GB of InnoDB pool size
84,innodb_buffer_pool_size
84,= 4G
84,# Use up to 70-80% of RAM
84,innodb_file_per_table
84,= On
84,innodb_flush_log_at_trx_commit
84,= 2
84,innodb_flush_method
84,= O_DIRECT
84,innodb_log_buffer_size
84,= 8M
84,innodb_log_file_size
84,= 512M
84,innodb_stats_on_metadata
84,= 0
84,#innodb_temp_data_file_path
84,= ibtmp1:3G:autoextend # Control the maximum size for the ibtmp1 file
84,innodb_thread_concurrency
84,= 0
84,# Optional: Set to the number of CPUs on your system (minus 1 or 2) to better
84,"# contain CPU usage. E.g. if your system has 8 CPUs, try 6 or 7 and check"
84,# the overall load produced by MySQL/MariaDB.
84,innodb_read_io_threads
84,= 128
84,innodb_write_io_threads
84,= 128
84,innodb_use_native_aio
84,= 0
84,# MyISAM Settings
84,query_cache_limit
84,= 4M
84,"# UPD - Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
84,query_cache_size
84,= 0
84,"# UPD - Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
84,query_cache_type
84,= 0
84,"# Option supported by MariaDB & up to MySQL 5.7, remove this line on MySQL 8.x"
84,#key_buffer_size
84,= 32M
84,# UPD
84,low_priority_updates
84,= 1
84,concurrent_insert
84,= 2
84,# Connection Settings
84,max_connections
84,= 200
84,# UPD - Important: high no. of connections = more RAM consumption
84,back_log
84,= 512
84,thread_cache_size
84,= 100
84,thread_stack
84,= 192K
84,interactive_timeout
84,= 300
84,wait_timeout
84,= 300
84,# For MySQL 5.7+ only (disabled by default)
84,#max_execution_time
84,= 30000 # Set a timeout limit for SELECT statements (value in milliseconds).
84,"# This option may be useful to address aggressive crawling on large sites,"
84,# but it can also cause issues (e.g. with backups). So use with extreme caution and test!
84,# More info at: https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time
84,# For MariaDB 10.1.1+ only (disabled by default)
84,#max_statement_time
84,= 30
84,"# The equivalent of ""max_execution_time"" in MySQL 5.7+ (set above)"
84,"# The variable is of type double, thus you can use subsecond timeout."
84,# For example you can use value 0.01 for 10 milliseconds timeout.
84,# More info at: https://mariadb.com/kb/en/aborting-statements/
84,# Buffer Settings
84,#join_buffer_size
84,= 4M
84,# UPD
84,#read_buffer_size
84,= 3M
84,# UPD
84,#read_rnd_buffer_size
84,= 4M
84,# UPD
84,#sort_buffer_size
84,= 4M
84,# UPD
84,# Table Settings
84,"# In systemd managed systems like CentOS 7, you need to perform an extra action for table_open_cache & open_files_limit"
84,# to be overriden (also see comment next to open_files_limit).
84,"# E.g. for MySQL 5.7 (when it's supported in cPanel), please check: https://dev.mysql.com/doc/refman/5.7/en/using-systemd.html"
84,# and for MariaDB check: https://mariadb.com/kb/en/library/systemd/
84,table_definition_cache
84,= 40000 # UPD
84,table_open_cache
84,= 40000 # UPD
84,open_files_limit
84,= 60000 # UPD - This can be 2x to 3x the table_open_cache value or match the system's
84,# open files limit usually set in /etc/sysctl.conf or /etc/security/limits.conf
84,# In systemd managed systems this limit must also be set in:
84,# /etc/systemd/system/mysqld.service.d/override.conf (for MySQL 5.7+) and
84,# /etc/systemd/system/mariadb.service.d/override.conf (for MariaDB)
84,max_heap_table_size
84,= 126M
84,tmp_table_size
84,= 128M
84,# Search Settings
84,ft_min_word_len
84,= 3
84,# Minimum length of words to be indexed for search results
84,# Logging
84,log_error
84,= /var/lib/mysql/mysql_error.log
84,log_queries_not_using_indexes
84,= 1
84,long_query_time
84,= 5
84,slow_query_log
84,= 0
84,# Disabled for production
84,slow_query_log_file
84,= /var/lib/mysql/mysql_slow.log
84,[mysqldump]
84,# Variable reference
84,# For MySQL 5.7: https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html
84,# For MariaDB:
84,https://mariadb.com/kb/en/library/mysqldump/
84,quick
84,quote_names
84,max_allowed_packet
84,= 64M
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Nov 13, 2020"
84,"@darnellkeithj It's really way off. You are increasing some variables to irrational values (e.g. max_connections, innodb_*_io_threads -which should not exceed 64- and others) and commenting out variables (e,g, *_buffer_size) which are important. You should seek professional performance auditing (which extends beyond MySQL/MariaDB). If you need my services you can always email me at engintron [at] gmail [dot] com."
84,"Sorry, something went wrong."
84,Copy link
84,talk2rajasimman
84,commented
84,"Dec 2, 2020"
84,I have Intel Xeon E3-1230 v2 - 3.3 GHz - 4 core(s) 8 threads.
84,RAM:	16GB - DDR3
84,Hello @fevangelou can you suggest me the my.cnf file. Really i am confused lot about this configuration.
84,"Sorry, something went wrong."
84,Copy link
84,karimrattani
84,commented
84,"Jan 23, 2021"
84,"Thanks for the config, I had to change below config otherwise MySQL failed to restart"
84,innodb_read_io_threads
84,= 40
84,innodb_write_io_threads
84,= 40
84,"Sorry, something went wrong."
84,Copy link
84,locksmithunit
84,commented
84,"Mar 5, 2021"
84,edited
84,is because you have 40 cores 4M 8C like mine.
84,"i did that 16. just in case, is not good to read everything from the DISK anyway."
84,the only thing I little bit confused about.
84,is this:
84,innodb_io_capacity = 1000
84,https://dev.mysql.com/doc/refman/8.0/en/innodb-configuring-io-capacity.html
84,this very good but it depends on your drive.
84,"in this case, I dont know what they talking about if you have Linux."
84,they recommended 1000 as well.
84,innodb_read_io_threads = 16
84,innodb_write_io_threads = 16
84,innodb_io_capacity = 1000
84,This can be very good to cPanel on a VPS cloud 4M 8C
84,BUT YOU MUST SPEAK WITH YOUR HOSTING ASK THEM IF IS SSD OR SATA 2 WITH 7200RPM
84,If they hosting with SSD (Must of the VPS hosting with SSD)
84,so need to uncomment the io_capacitiy = 1000
84,This 16M 8C is not what must of the hosting selling today...
84,I think is be better to improve the old version of the 4M 8C
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Nov 11, 2021"
84,Updated with better defaults and new performance related additions for MySQL 8.
84,"Sorry, something went wrong."
84,Copy link
84,locksmithunit
84,commented
84,"Nov 13, 2021"
84,edited
84,MariaDB 10.5?
84,I try and adjust the file already but i was needed to change it and comment out a lot from the configuration.
84,btw MariaDB 10.5 by default force you to put socket_unix=off
84,user name changed to MariaDB and not MySQL...
84,can you do one smaller and matching MariaDB 10.5? (BTW CPANEL FORCE CLIENTS INSTALL IT AND UPGRADE TO 10.5)
84,"CPANEL ALREADY ELECTED MARIADB 10.6 EXPERIMENTAL,"
84,It is a matter of time everybody will move these versions... :/
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Nov 25, 2021"
84,"Config updated with new tool references for DB diagnostics, minor changes in defaults and additional details in open_files_limit."
84,"Sorry, something went wrong."
84,Copy link
84,raramuridesign
84,commented
84,"Dec 2, 2021"
84,@fevangelou Thank you. This has been a great improvement on our servers. Appreciated
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Dec 2, 2021"
84,@raramuridesign You're most welcome Matthew!
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Dec 16, 2021"
84,New version of the config released.
84,"Binary logging is now disabled by default, ""innodb_sort_buffer_size"" has been bumped to 4M as a better default value, ""default_authentication_plugin"" is referenced (but commented by default - read the comments there), new performance related comments added in the buffers section."
84,"Sorry, something went wrong."
84,Copy link
84,dandidan2
84,commented
84,"Jan 8, 2022"
84,Can i hire you for my.cnf optimize for a server with 128gb ram?
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Jan 11, 2022"
84,@dandidan2 Sure - contact details are here: https://github.com/engintron/engintron#commercial-support--server-optimization-services
84,"Sorry, something went wrong."
84,Copy link
84,soulkeeperxx
84,commented
84,"Jul 9, 2022"
84,@dandidan2 Sure - contact details are here: https://github.com/engintron/engintron#commercial-support--server-optimization-services
84,Can I use your help to optimize my WHM ? I have a VDS with high specs and I need some optimization :D
84,couldnt find your contact detail as your site is down.
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Jul 18, 2022"
84,"Nothing was ever down... The URL above ALSO links to: ""...simply email us at: engintron [at] gmail [dot] com"""
84,"Sorry, something went wrong."
84,Copy link
84,raramuridesign
84,commented
84,"Jul 26, 2022"
84,@fevangelou any chance you could look at a version of this for a native mariadb install thats not on whm/cpanel?
84,"Sorry, something went wrong."
84,Copy link
84,asciixster
84,commented
84,"Mar 19, 2023"
84,@fevangelou any change you can update this configs to 8.0.32?
84,"Since we have ""forced"" to move to almalinux and 8.0.32 is enforced? thk you"
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Mar 22, 2023"
84,@raramuridesign
84,Here you go Matthew: https://gist.github.com/fevangelou/fb72f36bbe333e059b66
84,@asciixster
84,The config works just fine with MySQL 8 on Almalinux.
84,"To both, there are distinct comments for any differences between MySQL and MariaDB. Read them please. :)"
84,"Sorry, something went wrong."
84,Copy link
84,siamnews
84,commented
84,"Mar 22, 2023"
84,via email
84,bind_address is wrong
84,bind-address is right
84,Boris Sullivan
84,"Deputy Marketing Manager, Siam News Network"
84,***@***.***
84,Address: 160 Robinson Road
84,#14-04 Singapore Business Federation Centre
84,Website: https://www.siamnewsnetwork.net
84,<https://www.facebook.com/siamnewsnetwork>
84,<https://www.linkedin.com/company/siam-news-network/>
84,<https://www.twitter.com/ThailandBizNews>
84,"On Wed, Mar 22, 2023 at 2:31 PM Boris Sullivan ***@***.***>"
84,wrote:
84,You have a typo in your config :
84,bind_adress should be bind-adress
84,Boris Sullivan
84,"Deputy Marketing Manager, Siam News Network"
84,***@***.***
84,Address: 160 Robinson Road
84,#14-04 Singapore Business Federation Centre
84,Website: https://www.siamnewsnetwork.net
84,<https://www.facebook.com/siamnewsnetwork>
84,<https://www.linkedin.com/company/siam-news-network/>
84,<https://www.twitter.com/ThailandBizNews>
84,"On Wed, Mar 22, 2023 at 2:09 PM Fotis Evangelou ***@***.***>"
84,wrote:
84,> ***@***.**** commented on this gist.
84,> ------------------------------
84,> @raramuridesign <https://github.com/raramuridesign>
84,> Here you go Matthew:
84,> https://gist.github.com/fevangelou/fb72f36bbe333e059b66
84,> @asciixster <https://github.com/asciixster>
84,> The config works just fine with MySQL 8 on Almalinux.
84,"> To both, there are distinct comments for any differences between MySQL"
84,> and MariaDB. Read them please. :)
84,> —
84,"> Reply to this email directly, view it on GitHub"
84,> <https://gist.github.com/fevangelou/0da9941e67a9c9bb2596#gistcomment-4511874>
84,> or unsubscribe
84,> <https://github.com/notifications/unsubscribe-auth/ABYNHEUQPF4YACZF4NDMKS3W5L2YLBFKMF2HI4TJMJ2XIZLTSKBKK5TBNR2WLJDHNFZXJJDOMFWWLK3UNBZGKYLEL52HS4DFQKSXMYLMOVS2I5DSOVS2I3TBNVS3W5DIOJSWCZC7OBQXE5DJMNUXAYLOORPWCY3UNF3GS5DZVRZXKYTKMVRXIX3UPFYGLK2HNFZXIQ3PNVWWK3TUUZ2G64DJMNZZDAVEOR4XAZNEM5UXG5FFOZQWY5LFVAZTANZVGE4TINVHORZGSZ3HMVZKMY3SMVQXIZI>
84,> .
84,> You are receiving this email because you commented on the thread.
84,> Triage notifications on the go with GitHub Mobile for iOS
84,> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
84,> or Android
84,> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>
84,> .
84,"Sorry, something went wrong."
84,Copy link
84,raramuridesign
84,commented
84,"Mar 22, 2023"
84,@fevangelou Thanks ;-)
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Mar 22, 2023"
84,@siamnews Not exactly. You can define variables with both dashes and underscores.
84,"Sorry, something went wrong."
84,Copy link
84,theozsnowman
84,commented
84,"Apr 12, 2023"
84,ive tried this config on a Centos 7 server running Maria 10.6.12
84,are the following warnings normal on restart?
84,Apr 12 15:41:25 xxx.server.com systemd[1]: Starting MariaDB 10.6.12 database server...
84,Apr 12 15:41:25 xxx.server.com mariadbd[8812]: 2023-04-12 15:41:25 0 [Warning] Could not increase number of max_open_files to more than 40000 (request: 640139)
84,Apr 12 15:41:25 xxx.server.com mariadbd[8812]: 2023-04-12 15:41:25 0 [Warning] Changed limits: max_open_files: 40000
84,max_connections: 100 (was 100)
84,table_cache: 19935 (was 40000)
84,"Sorry, something went wrong."
84,Copy link
84,Author
84,fevangelou
84,commented
84,"Apr 18, 2023"
84,"@theozsnowman Read the comment next to ""open_files_limit"" in the config."
84,"Sorry, something went wrong."
84,Sign up for free
84,to join this conversation on GitHub.
84,Already have an account?
84,Sign in to comment
84,Footer
84,"© 2024 GitHub, Inc."
84,Footer navigation
84,Terms
84,Privacy
84,Security
84,Status
84,Docs
84,Contact
84,Manage cookies
84,Do not share my personal information
84,You can’t perform that action at this time.
85,Optimizing MySQL: Adding Data to Tables - Simple Talk
85,Redgate Hub
85,Product articles
85,University
85,Events
85,Forums
85,Community
85,Simple Talk
85,Home
85,Cloud
85,DevOps
85,Sysadmin
85,Development
85,Databases
85,Opinion
85,Books
85,Blogs
85,Log in
85,Sign up
85,Lukas Vileikis
85,22 May 2023
85,3126 views
85,Home
85,Databases
85,MySQL
85,Optimizing MySQL: Adding Data to Tables
85,Lukas Vileikis
85,22 May 2023
85,3126 views
85,Optimizing MySQL: Adding Data to Tables
85,"Welcome back to the MySQL optimization series! In case you haven’t been following this series, in the past couple of articles we have discussed the basics of query optimization, and told you how to optimize SELECT queries for performance as well."
85,"In this blog, we’re further learning ways to optimize INSERT operations and look at alternatives when you need to load more than a few rows in the LOAD DATA INFILE statement."
85,How Does INSERT Work?
85,"As the name suggests, INSERT is the query that’s used to create data in tables in a database. The internal functioning of a basic INSERT query is pretty much identical to that of aSELECTstatement as seen in the previous article, first, the database checks for permissions, opens tables, initializes operations, checks whether rows need to be locked, inserts data (updates the indexes), and then finishes coming to a stop."
85,We can back this up by performing some profiling:
85,Image 1 – INSERT Query Profiling – the Query
85,Image 2 – INSERT Query Profiling – Status and Duration
85,"To know what these status codes mean in detail, please refer to preview blog about SELECT queries where we explain them one by one, but in a nutshell, the profiling codes outlined above enable MySQL to figure out the following things, amongst others:"
85,Whether appropriate privileges have been set.
85,"Whether tables are ready to perform any operations on (i.e., whether they are locked or not.)"
85,How best to perform the INSERT operation. This step includes*:
85,Sending the query to the server.
85,Parsing the query.
85,Scanning the table for any indexes or partitions.
85,Inserting rows into tables
85,Adding data to applicable indexes.
85,Closing the process.
85,"* Do note that each step takes up a proportion of the total time INSERT queries take. According to the MySQL documentation, the fastest process is the closing of the query, the second fastest one pertains to sending the query to the server and parsing it, and the slowest is adding data to the table. (Note that for very small amounts of data, connecting to the server can be more costly)."
85,"As far as profiling is concerned, it’s indeed helpful when performing read operations, but not very much in other cases: knowing how queries work is a good thing to do, but when optimizing INSERT queries, profiling doesn’t take us far and that’s why we need to employ techniques that we’ll share with you in a second."
85,"Another thing to remember is that no matter what database management system is in use (INNODB, etc.), many queries go through the same steps – INSERT queries share many steps with SELECT queries too, however, while there are a lot of shared steps, it’s important to remember that all queries are different in their own regard too – an INSERT query is different from a SELECT query in that SELECT queries benefit from indexes and partitions, while they generally make INSERT queries slower."
85,"Advanced DBAs will also know that INSERT queries are sometimes used in concert with SELECT queries – we’ll start from the basic ways that will help you improve your INSERT query performance, then gradually evolve towards more complex scenarios."
85,Basic Ways to Improve INSERT Query Performance
85,"To improve INSERT query performance, start with the advice above: profile your queries, then remove indexes or partitions on a table if necessary. All indexes and partitions will make all INSERT queries slower because of one simple reason – every time data is inserted into a table, all indexes and partitions on that table must be updated for the database to know where the row resides. That’s true no matter what kind of partitions or indexes are in use – the more partitions or indexes exist on a table, the slower your query will become."
85,"Note: Remember that performance tuning must be treated as a holistic activity and we are focusing on tuningINSERT statements only here. Indexes are needed by most of your SQL statements and unless you are adding a LOT of data to your table, removing them just to make a particular INSERT operations go a little bit faster (and then killing read performance) is typically not desirable."
85,"Another very popular and simple way to improve query performance is to combine small operations into a single, larger operation. We can also lock tables and unlock tables only when all operations are completed, and these approaches look like so:"
85,Image 3 – a basic way to improve INSERT query performance.
85,"And that’s the crux of it – to optimize INSERT query speed, we need to “think big” in terms of queries. The whole magic goes like this: instead of having many queries that insert a couple of rows each, run one huge query that inserts many rows at once after making sure that all the table locks, index updates, and consistency checking are as delayed as possible (ideally delay these processes until the very end of the query.)"
85,"When it is feasible, delaying everything isn’t hard and can be done by following some or all these steps. Consider performing these steps if you’re working with more than a million rows and (or) whenever you see that your INSERT queries can be combined into a single operation just like in the example above (it’s not necessary to follow the steps from the top to bottom – following one or two steps will usually be a good start):"
85,"Lock the tables before inserting data into them and unlock them once all the data has been inserted, but not before (see example above.) By locking your tables, you will ensure that the data within them will not be modified while the data will be inserted."
85,"Consider starting a transaction (START TRANSACTION) before running INSERT queries – once done, issue a COMMIT query."
85,Avoid adding any indexes before insert operations have been completed.
85,"If possible, make sure that the table you’re inserting data into is not partitioned because partitioning splits your table into sub-tables and once you insert data into your table, MySQL has to go through a couple of additional steps to figure out which partition to insert what data into, etc. Also, while the proper use of partitions help improve the performance of SELECT statements, partitioned tables will inevitably take up a little more space on the disk, so be wary of that."
85,"Follow these steps and that’s it – you’re on the way to INSERT heaven! However, do note that these are only the basics and such an approach may not cover your scenario – in that case, we may need to perform some additional optimization by allocating the number of I/O threads within InnoDB by modifying my.cnf (refer to the options in the screenshot below), or improve query performance by doing other things you will learn in this article. Keep reading – your INSERT query performance will soon skyrocket!"
85,Improving INSERT Query Performance Beyond the Basics
85,"Once you’re sure that all of your INSERT queries are profiled and doing what you expect, employ the basic steps outlined above to improve their performance. If that didn’t help, consider the following advice:"
85,"We can start an explicit transaction using START TRANSACTION or issue autocommit=0, run all of our INSERT queries, then run a COMMIT query. By doing so we delay commit operations until after the very last INSERT query has been finished – in that case, MySQL saves time because it doesn’t commit SQL statements as soon as they’re executed, but commits them all at once later instead. The bigger the data set is, the more time will be saved."
85,"Such an approach isn’t very feasible with billions of rows (INSERT queries are not designed for more than 50 or 100 million rows at a time – somewhat depends on hardware in use – and we need to use LOAD DATA for that since INSERT statements come with a lot of overhead – more on that later), but if we’re dealing not dealing with that many rows, it certainly could be viable."
85,We can modify the my.cnf file and add or modify the following options to increase the I/O capabilities of InnoDB (the main storage engine within MySQL):
85,Image 3 – Increasing I/O Capabilities of InnoDB
85,"The innodb_read_io_threads option sets the number of threads handling read operations – in most cases, the value of 32 is sufficient and should be left at default."
85,"The innodb_write_io_threads option sets the number of threads handling write operations – in most cases, the value of 32 is also sufficient and should be left at default."
85,The innodb_io_capacity option defines the total I/O capacity of InnoDB and its value should be set at the maximum value of IOPS available to the server
85,"It is mostly related to the threads that perform various database-related tasks in the background (working with the buffer pool, writing changes, etc.) – these threads are trying to not have a negative effect on InnoDB at the same time. for more information, head over to the MySQL documentation over here."
85,"For more details on optimizing my.cnf for general performance, I previously posted the following article on Simple-Talk: Optimizing my.cnf for MySQL performance."
85,"We can insert more data at once by using the bulk insert operation available within the INSERT statement itself. When using such an approach, we can avoid defining the columns we load the data into if we load the data into all of them at once as well. Do note that if we elect to ignore columns and we have an AUTO_INCREMENT column, we must specify the value as NULL (an example is given below.)"
85,Image 4 – INSERT INTO Example
85,Additional considerations for INSERT performance
85,"These steps previously covered will help you optimize your INSERT queries; however, these are only the basics. Advanced DBAs know of a couple of additional ways to improve their INSERT query performance and some of advanced tips include combining INSERT statements with SELECTstatements too!"
85,Locks and Inserting rows concurrently
85,"Many of you know that issuing INSERT statements means locks for InnoDB: MySQL deals with each statement differently, and as far as INSERT statements are concerned, InnoDB row-level locks are advantageous for the end-user: the storage engine sets a lock on the inserted row thus not disrupting any operations with any of the other rows in the same table. Users can still work with their InnoDB-based tables as they’re inserting data as long as the rows they need are not locked and they didn’t take a table lock)."
85,"Before any row is inserted, MySQL also lets InnoDB know that a row is going to be inserted by setting an insert intention lock. The purpose of that lock is to let other transactions know that data is inserted into a certain position, so that other transactions do not insert data into the same position."
85,"To make the best out of locks within InnoDB, follow the advice above – make use of bulk INSERT statements and consider delaying all commit operations until after the very last INSERT query: bulk INSERT statements will insert data in a quicker fashion and committing an operation only after the last query will be faster as well."
85,Inserting rows from a SELECT Query
85,"As noted above, INSERT queries are the primary way of inserting data into MySQL, but many DBAs will know that INSERT queries are not always simple either: certain situations may require us to use them in concert with other – typically SELECT – queries too. A query like so would also work quite successfully:"
85,123
85,"INSERT INTO table_name (FirstColumnName, SecondColumnName)SELECT  FirstColumnName, SecondColumnName FROM another_table [options]"
85,For example:
85,Image 5 – INSERT and SELECT Queries Together
85,"Note that in this case indexes will not be beneficial to the table where new rows are being created, but they will be beneficial fetching rows from the table in the SELECT statement. Note that in the sample query, the another_table reference in the query can be the same table. So:"
85,123
85,"INSERT INTO table_name (FirstColumnName, SecondColumnName) SELECT FirstColumnName, SecondColumnName FROM table_name [options]"
85,"Is possible as well, in which indexes can (depending on if you have a where clause on the table,) can both be helpful and detrimental at the same time."
85,The CREATE TABLE … SELECT Query
85,"To select data directly into a new table, we can also employ a CREATE TABLE statement together with a SELECT query. In other words, we can create a table and insert data into it without running a separate INSERT query afterwards. There are two ways to do this:"
85,Run a CREATE TABLE statement with a SELECT statement without defining any columns or data types:
85,CREATE TABLE new_table [AS] SELECT [columns] FROM old_table;
85,"MySQL will recreate all columns, data types and data from the old_table."
85,You can also define the columns and constraints and insert data to the new table like so:
85,CREATE TABLE new_table (
85,"column1 VARCHAR(5) NOT NULL,"
85,column2 VARCHAR(200) NOT NULL DEFAULT ‘None’
85,) ENGINE = InnoDB
85,"SELECT demo_column1, demo_column2"
85,FROM old_table;
85,Selecting data from one table to insert directly into another table is generally faster than INSERT INTO ... SELECT.
85,The Best Way to Load Massive Data Sets – INSERT vs. LOAD DATA INFILE
85,LOAD DATA INFILE is built for blazing fast data insertion from text files. Speed is achieved by ignoring or eliminating overhead posed by INSERT queries done by:
85,"Working with “cleaner” data (data only separated by certain denominators (think “,”, “:”, “|”, or not separated at all.)"
85,Providing us with the ability to only load data into specific columns or skip loading data into certain rows or columns altogether.
85,"To use LOAD DATA INFILE, make sure you have a file that’s separating its columns by a certain denominator (common denominators include, but are not limited to “,”, the TAB sign, spaces, the “|”, “:”, and “-“ characters, etc.) and preferably one that’s saved in a CSV or TXT format. Save the file in a directory, remember the path towards that directory, then use the following query (replace the path to the file with your file, the “|” sign with your denominator and demo_table with the name of your table) – use IGNORE to ignore all errors posed by the query (duplicate key issues, etc.):"
85,LOAD DATA INFILE ‘/var/lib/mysql/tmp/data.csv’ [IGNORE] INTO TABLE demo_table FIELDS TERMINATED BY ‘|’;
85,"To export data from your database and make the data able to be re-imported by using the LOAD DATA INFILE query, use the SELECT * INTO OUTFILE query – use IGNORE if you want to ignore all errors."
85,1234
85,SELECT *FROM demo_table [IGNORE] INTO OUTFILE '/var/lib/mysql/tmp/data.csv' FIELDS TERMINATED BY '|';
85,"LOAD DATA INFILE has many parameters that can be used as well. These parameters include, but are not limited to:"
85,The PARTITION parameter allows us to define the partition where we want to insert data into.
85,Combining the IGNORE option with the LINES or ROWS options to tell MySQL how many lines or rows to ignore when inserting data.
85,Providing us with the ability to set the values of certain columns by using the SET option (the query below takes data from the file called “demo.csv” and loads the data with its columns terminated by “:” into a table called demo after ignoring the 100 lines from the beginning and also sets the date column to the current date when inserting all of the rows):
85,LOAD DATA INFILE ‘demo.csv’ INTO TABLE demo FIELDS TERMINATED BY ‘:’ IGNORE 100 LINES SET date=CURRENT_DATE();
85,"LOAD DATA INFILE can be made even faster if we employ DEFAULT constraints. If there are columns that need to be populated with the same data for each row, the DEFAULT constraint will be faster than fetching it from the data stream. That way, all columns having the default keyword will be pre-filled without the need to load data into them."
85,"Such an approach is very convenient to save time when one row consists of the same data and we don’t want to employ ALTER queries (those queries make a copy of the table on the disk, insert data into it, perform operations on the newly created table, then swap the two tables.) An example is given below:"
85,1234
85,"CREATE A TABLE demo_table (`demo_prefilled_column` VARCHAR(120) NOT NULL           <strong>DEFAULT 'Value Here’</strong>,) ENGINE = InnoDB;"
85,"To dive even deeper into LOAD DATA INFILE, refer to the MySQL documentation itself."
85,"Indexes, Partitions, and INSERT FAQ"
85,"If even LOAD DATA INFILE doesn’t help, you may want to look into indexes and partitions. Remember what we said above? The more indexes or partitions your table has, the slower your INSERT query will be. You already know the drill – but you may be pressed with some extra questions, some of them being the following:"
85,Question
85,Answer
85,Should I drop all of the indexes before running an INSERT query?
85,"Typically No – while indexes are used to improve the performance of finding rows, and generally slow down INSERT statements usually it is fine to let the INSERT statement process maintain indexes as part of the insert process."
85,"However, in some cases when loading hundreds of millions or billions of rows, indexes should certainly be removed (they can be added again by using an ALTER TABLE query.)"
85,Does using a PRIMARY KEY constraint on a table slow INSERT statements down?
85,Yes – primary keys are always indexed and indexes make INSERT queries slower.
85,Does the amount of indexes or partitions on a table make a difference in INSERT query performance?
85,"Yes – the more indexes or partitions you have on a table, the slower your INSERT queries will be."
85,Are LOAD DATA INFILE queries always faster than INSERT statements? Why?
85,Yes – LOAD DATA INFILE queries always come with less overhead than INSERT queries do.
85,"When optimizing INSERT statements, is there a difference of what storage engine is in use?"
85,Yes – aim to use InnoDB unless you need specific features:
85,MyISAM will make COUNT(*) aggregate queries faster since the storage engine stores the row count inside of its metadata.
85,ARCHIVE will let you archive data with little footprint on the disk.
85,MEMORY is useful for temporary tables since all of its data is stored in the memory.
85,CSV will be advantageous if we need a way to migrate data into some sort of spreadsheet software.
85,"For a complete list of storage engines, check the MySQL documentation."
85,"When inserting data, is there a difference if the table we insert data into is locked or not?"
85,Yes – inserting data into locked tables is generally faster than the other way around since database management systems check for table locks as part of the query execution process.
85,How do I achieve a balance between INSERT and SELECT queries?
85,"Testing – Load testing is important to see what effects any change will have on a system. A lot will depend on how you are using a system, and how concurrent users use a system is important. If users are simultaneously creating and querying data, it is different than if data is loaded in a time window, then queried at other times."
85,"Generally, aim to have only as many indexes and partitions as necessary, SELECT as few rows as possible, and where possible, use LOAD DATA INFILE instead of INSERT."
85,Foreign keys – are they a problem for INSERT queries?
85,"No – INSERT queries will be slower on a table with foreign keys since the database must check that there are no foreign key violations, but in most cases, they won’t make much of a performance difference (but will make a large data integrity difference). As is the case with indexes, foreign key constraints should be dropped once we have a lot (hundreds of millions of rows) of data to insert."
85,The CHECK constraint – does it slow down inserting data?
85,"Yes – the CHECK constraint was introduced in MySQL 8.0.16, and it lets users specify a condition to check before inserting data into a table. Think of it as an integrity check – if a violation of the constraint is found, MySQL shows an error and terminates query execution, or skips inserting the row altogether if the IGNORE clause is specified."
85,"In many cases, the constraint will only be problematic if the constraint evaluates to FALSE instead of TRUE because in that case, queries will fail if no IGNORE clause is specified. Aim to avoid using constraints when inserting larger quantities of data, because they could make your INSERT queries slower. (If you remove one for performance reasons, add it back for integrity’s sake.)"
85,More details about the CHECK constraint can be found on the MySQL blog.
85,Is it possible to insert data and update it at the same time?
85,"Yes – that can be done by using the ON DUPLICATE KEY UPDATE option on anINSERTstatement. The statement is pretty self-explanatory: if there is a situation where a row of the same value would be inserted into a column, an update of the old row would occur. Such a statement is mostly used to increment column values whenever there are duplicate key issues, but it can have other use cases as well."
85,Are there any lesser-known ways to improve INSERT query performance?
85,"Yes – if you’re working with bigger data sets, make use of the DEFAULT keyword. The keyword will let you specify the default value of the column and as a result, will automatically be filled in when running LOAD DATA INFILE queries saving you time in return."
85,When to switch to LOAD DATA INFILE?
85,"Typically, when loading any data where you have many rows to load – There are no set “rules” that define when you should switch INSERT statements to LOAD DATA INFILE where possible, however, keep in mind the following:"
85,There are several necessary steps that the DBMS must go through when performing INSERT operations.
85,"INSERT operations come with overhead. The more data is inserted, the bigger the overhead is."
85,INSERT query performance can be optimized a lot by using bulk INSERT queries or by delaying index updates and concurrency checking.
85,LOAD DATA INFILE works best when settings inside of my.cnf are optimized for InnoDB.
85,Balancing INSERT Performance and Reads
85,"To balance out the performance between INSERT and SELECT queries, keep the following in mind:"
85,"Basics do help – Select as few rows as possible by using a column list with your SELECT of SELECT *, and index only the columns you’re searching through."
85,"Normalize your tables – Table normalization helps save storage space as well as increases the speed of your read queries if used properly. Choose a normalization method suitable for you, then proceed further."
85,"Do note that as with everything, normalization also can have negative effects – more tables and relationships can mean more individual INSERT or LOAD DATA INFILE statements, but if you do everything correctly, your database should roll just fine!"
85,"Use the proper storage engine – this should be obvious, but you would be surprised how many inexperienced DBAs make a mistake of using an inappropriate storage engine for their use case. If you’re using MySQL, aim to use InnoDB or XtraDB for general use cases."
85,"If you’re in a testing environment and memory is not an issue, the MEMORY storage engine could also be an option, but note that the storage engine cannot be as heavily optimized and that the data won’t be stored on the disk either – at that point the insert queries would be blazing fast, but the data would be stored in the memory itself meaning that a shutdown of the server would destroy all of your data as well."
85,"Use a powerful server – this goes hand in hand with optimizing my.cnf or other files. If your server is powerful enough, balancing the performance of INSERT and SELECT operations will be a breeze."
85,"Only use indexes and partitions where necessary and don’t overdo it – aim to index only the columns you’re running heavy SELECT queries on, and only partition your tables if you have more than 50 million records."
85,"Consider ditching INSERT statements altogether – if you must import huge chunks of data, consider splitting them into files and uploading those files with LOAD DATA INFILE instead."
85,Summary
85,"In this blog, we’ve walked you through how best to optimize INSERT queries for performance and answered some of the most frequently asked questions surrounding these sorts of queries. We’ve also touched upon the importance of balancing INSERT and SELECT operations and walked you through a way to ditch INSERT queries altogether when loading lots of data."
85,"Some of the advice we’ve provided in this blog is known by many DBAs, while some might not be known at all. Take from this blog what you will – there’s no necessity to follow everything outlined in the article step by step, but combining some of the advice in this article with the advice contained in other parts of this series will be invaluable for your database and your applications alike."
85,"We hope that this blog has taught you something new, and we’ll see you in the next one."
85,MySQL
85,NoSQL
85,Oracle
85,PostgreSQL
85,Snowflake
85,SQL Server
85,Theory and design
85,Subscribe to the MySQL RSS feed
85,Subscribe for more articles
85,"Fortnightly newsletters help sharpen your skills and keep you ahead, with articles, ebooks and opinion to keep you informed."
85,3126 views
85,Rate this article
85,Click to rate this post![Total: 2
85,Average: 5]
85,Lukas Vileikis
85,Lukas Vileikis is an ethical hacker and a frequent conference speaker.
85,"Since 2014, Lukas has found and responsibly disclosed security flaws in some of the most visited websites in Lithuania."
85,"He runs one of the biggest & fastest data breach search engines in the world - BreachDirectory.com, frequently speaks at conferences and blogs in multiple places including his blog over at lukasvileikis.com."
85,Follow Lukas Vileikis via
85,View all articles by Lukas Vileikis
85,Load comments
85,Related articles
85,Mercy Bassey Udoh
85,14 December 2023
85,Mercy Bassey Udoh
85,14 December 2023
85,MySQL Error Log Management in DevOps Operations
85,MySQL
85,"When it comes to the development and operations (DevOps), one thing stands out as a critical aspect and that is troubleshooting. The primary goal of a DevOps team is to ensure that the product experiences zero to no downtime because every moment is crucial. Therefore, smooth delivery and uninterrupted uptime are paramount. To achieve this, …			Read more"
85,MySQL
85,Simple Talk
85,FAQ
85,Author AI Usage Policy
85,Sitemap
85,About Simple Talk
85,Contact Us
86,OData Query Options | Mendix Documentation
86,Docs
86,Release Notes
86,Mx10 Feature Release Calendar
86,Studio Pro
86,"LTS, MTS, and Monthly Releases"
86,10.8
86,10.7
86,10.6
86,10.5
86,10.4
86,10.3
86,10.2
86,10.1
86,10.0
86,9.24
86,9.23
86,9.22
86,9.21
86,9.20
86,9.19
86,9.18
86,9.17
86,9.16
86,9.15
86,9.14
86,9.13
86,9.12
86,9.11
86,9.10
86,9.9
86,9.8
86,9.7
86,9.6
86,9.5
86,9.4
86,9.3
86,9.2
86,9.1
86,9.0
86,8.18
86,8.17
86,8.16
86,8.15
86,8.14
86,8.13
86,8.12
86,8.11
86,8.10
86,8.9
86,8.8
86,8.7
86,8.6
86,8.5
86,8.4
86,8.3
86,8.2
86,8.1
86,8.0
86,7.23
86,7.22
86,7.21
86,7.20
86,7.19
86,7.18
86,7.17
86,7.16
86,7.15
86,7.14
86,7.13
86,7.12
86,7.11
86,7.10
86,7.9
86,7.8
86,7.7
86,7.6
86,7.5
86,7.4
86,7.3
86,7.2
86,7.1
86,7.0
86,Windows Service
86,Mobile
86,Make It Native Apps
86,Make It Native 10 App
86,Make It Native 9 App
86,Make It Native 8 App
86,Mendix Native Mobile Builder
86,Native Builder
86,Native Template
86,Studio Pro 9 & 10 Compatible
86,Native Template 8
86,Native Template 7
86,Native Template 6
86,Studio Pro 8 Compatible
86,Native Template 5.2
86,Native Template 5.1
86,Native Template 5.0
86,Mendix Mobile App
86,Hybrid App Base and Template
86,Developer Portal
86,Deployment
86,Mendix Cloud
86,Mendix for Private Cloud
86,SAP BTP
86,Other Deployment Options
86,Control Center
86,Marketplace
86,Catalog
86,Community Tools
86,Private Mendix Platform
86,1.6 (MTS)
86,1.5
86,SDKs
86,Model SDK
86,Platform SDK
86,Metamodel
86,10.8
86,10.7
86,10.6
86,10.5
86,10.4
86,10.3
86,10.2
86,10.1
86,10.0
86,9.24
86,9.23
86,9.22
86,9.21
86,9.20
86,9.19
86,9.18
86,9.17
86,9.16
86,9.15
86,9.14
86,9.13
86,9.12
86,9.11
86,9.10
86,9.9
86,9.8
86,9.7
86,9.6
86,9.5
86,9.4
86,9.3
86,9.2
86,9.1
86,9.0
86,8.18
86,8.16
86,8.15
86,8.14
86,8.13
86,8.12
86,8.11
86,8.10
86,8.9
86,8.8
86,8.7
86,8.6
86,8.5
86,8.4
86,8.3
86,8.2
86,8.1
86,Security Advisories
86,Beta and Experimental Releases
86,Quick Starts
86,Creating a Hello World App
86,Building a Responsive Web App
86,Adding a Native Mobile App
86,Studio Pro 10 Guide
86,Installation
86,Installing Studio Pro
86,System Requirements
86,Upgrading from Studio Pro 9 to 10
86,Configuring Parallels
86,Performance Tips
86,General Info
86,Studio Pro Overview
86,MxBuild
86,Developer Tool Recommendations
86,mx Command-Line Tool
86,App Commands
86,Adaptable Solution Commands
86,Module Commands
86,Export Package Commands
86,Merging and Diffing commands
86,MPR dump
86,Third-Party Licenses
86,App Modeling
86,Best Practices for Development
86,Best Practices for App Performance
86,Consistency Errors
86,Page Editor Consistency Errors
86,Navigation Consistency Errors
86,Importing and Exporting Elements
86,Starting with App from a Spreadsheet
86,Menus
86,File Menu
86,New App
86,Open App
86,Export App Package
86,Import App Package
86,Edit Menu
86,"Find, Find Advanced and Find Usages"
86,Go to Option
86,Preferences
86,View Menu
86,Changes Pane
86,Integration Pane
86,Errors Pane
86,Suppression Rules
86,Page Explorer
86,Stories Pane
86,App Menu
86,Create Deployment Package
86,Deploy to the Cloud
86,Run Menu
86,Edit Cloud Foundry Settings
86,Version Control Menu
86,Commit
86,History
86,Download from Version Control Server
86,Upload to Version Control Server
86,Branch Line Manager
86,Create Branch Line
86,Merge Dialog Box
86,Language Menu
86,Batch Replace
86,Batch Translate
86,Language Operations
86,Language Settings
86,Translating Your App Content
86,Using Translatable Validation Messages
86,App Explorer
86,App
86,App Settings
86,Configurations
86,Navigation
86,Set Up Navigation
86,System Texts
86,Modules
86,Module Settings
86,Publish Add-on and Solution Modules
86,Consume Add-on Modules and Solutions
86,UI Resources Package
86,Security
86,App Security
86,User Roles
86,Administrator
86,Demo Users
86,Anonymous Users
86,Password Policy
86,Module Security
86,Domain Model
86,Entities
86,External Entities
86,Persistability
86,Attributes
86,Validation Rules
86,Event Handlers
86,Indexes
86,Access Rules
86,Associations
86,Association Properties
86,Association Tab Properties
86,Querying Over Self-References
86,Annotations
86,Generalization vs 1-to-1 Associations
86,Configuring a Domain Model
86,Setting Up Data Validation
86,Pages
86,Page
86,Page Properties
86,Page Resources
86,Image Collection
86,Layout
86,Placeholder
86,Header
86,Sidebar Toggle
86,Menu
86,Snippet
86,Building Block
86,Page Template
86,Icon Collection
86,Data Containers
86,Data View
86,Grids
86,Data Grid
86,Grid Columns
86,Template Grid
86,Control Bar
86,Search Bar
86,Sort Order
86,List View
86,Data Sources
86,Database Source
86,XPath Source
86,Context Source
86,Microflow Source
86,Nanoflow Source
86,Association Source
86,Listen to Widget Source
86,Configure Form and Show Form Items
86,Configure List and View Details on 1 Page
86,Text
86,Text
86,Label
86,Page Title
86,Structure
86,Layout Grid
86,Container
86,Group Box
86,Snippet Call
86,Tab Container
86,Scroll Container
86,Table
86,Navigation List
86,Input Elements
86,Text Box
86,Text Area
86,Drop-Down
86,Check Box
86,Radio Buttons
86,Date Picker
86,Reference Selector
86,Reference Set Selector
86,Input Reference Set Selector
86,"Images, Videos, and Files"
86,Static Image
86,Dynamic Image
86,File Manager
86,Image Uploader
86,Enable End-Users to Attach Images
86,Configure File Upload and Download
86,Buttons
86,Button Properties
86,Creating a Custom Save Button
86,Menus and Navigation
86,Menu Bar
86,Simple Menu Bar
86,Navigation Tree
86,Authentication
86,Login ID Text Box
86,Password Text Box
86,Sign-In Button
86,Validation Message
86,Charts
86,Chart Configuration
86,Chart Advanced Cheat Sheet
86,Any Chart Widgets
86,Any Chart Building Blocks
86,Any Chart Cheat Sheet
86,Properties Common in the Page Editor
86,On Click Event and Events Section
86,Application Logic
86,Microflows and Nanoflows
86,Microflows
86,Microflow Properties
86,Triggering a Microflow From a Menu Item
86,Testing Microflows with Unit Test Module
86,Error Handling in Microflows
86,Extracting and Using Sub-Microflows
86,Retrieving Current User with a Microflow
86,Nanoflows
86,Nanoflow Properties
86,Error Handling in Nanoflows
86,Sequence Flow
86,Activities
86,Object Activities
86,Cast Object
86,Change Object
86,Commit Object(s)
86,Create Object
86,Delete Object(s)
86,Retrieve
86,Rollback Object
86,List Activities
86,Aggregate List
86,Change List
86,Create List
86,List Operation
86,Working with Lists in a Microflow
86,Action Call Activities
86,Java Action Call
86,JavaScript Action Call
86,Microflow Call
86,Variable Activities
86,Change Variable
86,Create Variable
86,Client Activities
86,Call Nanoflow
86,Show Message
86,Close Page
86,Download File
86,Show Home Page
86,Show Page
86,Synchronize to Device
86,Clear from Device
86,Synchronize
86,Validation Feedback
86,Integration Activities
86,Call External Action
86,Call REST Service
86,Call Web Service
86,Import Data from File
86,Import with Mapping
86,Export With Mapping
86,Query External Database
86,Send REST Request (Beta)
86,Log Message
86,Generate Document
86,Metrics Activities
86,Counter
86,Gauge
86,Increment Counter
86,ML Kit Activities
86,Call ML Model
86,Workflow Activities
86,Apply Jump-To Option
86,Call Workflow
86,Change Workflow State
86,Complete User Task
86,Generate Jump-To Options
86,Retrieve Workflow Activity Records
86,Retrieve Workflow Context
86,Retrieve Workflows
86,Show User Task Page
86,Show Workflow Admin Page
86,Lock Workflow
86,Unlock Workflow
86,Notify Workflow
86,External Object Activities
86,Delete External Object
86,Send External Object
86,Decisions
86,Decision
86,Object Type Decision
86,Merge
86,Annotation
86,Parameter
86,Loop
86,Events
86,Start Event
86,End Event
86,Error Event
86,Continue Event
86,Break Event
86,Common Properties
86,Debugging Microflows and Nanoflows
86,Debugging Microflows Remotely
86,Workflows
86,Workflow Elements
86,Workflow Parameters
86,Multi-User Task
86,User Task
86,Wait for Notification
86,Wait for Timer
86,Decision in Workflows
86,Parallel Split
86,Jump Activity
86,Call Microflow
86,Call Workflow
86,Workflow Properties
86,Configure Workflow Security
86,Workflow Engine
86,Add Workflow to Existing App
86,Jump to Different Activities
86,Workflow Events
86,Workflow Versioning and Conflict Mitigation
86,Workflow for Employee Onboarding
86,Add Custom Action to Workflow Toolbox
86,Expressions
86,Unary Expressions
86,Arithmetic Expressions
86,Relational Expressions
86,Special Checks
86,Boolean Expressions
86,If Expressions
86,Mathematical Function Calls
86,String Function Calls
86,Date Creation
86,Begin-of Date Function Calls
86,End-of Date Function Calls
86,Between Date Function Calls
86,Add Date Function Calls
86,Subtract Date Function Calls
86,Trim to Date
86,To String
86,Length
86,Parse Integer
86,Parse and Format Decimal Function Calls
86,Parse and Format Date Function Calls
86,Enumerations in Expressions
86,Configure String Concatenation
86,Mendix Assist
86,MxAssist Logic Bot
86,MxAssist Best Practice Bot
86,Recommendations from Best Practice Bot
86,Validation Assist
86,MendixChat
86,Resources
86,Java Actions
86,JavaScript Actions
86,Rules
86,Enumerations
86,Datasets
86,OQL
86,OQL Expressions
86,OQL Aggregation
86,OQL Functions
86,OQL CAST
86,OQL COALESCE
86,OQL DATEDIFF
86,OQL DATEPART
86,OQL LENGTH
86,OQL LOWER
86,OQL RANGEBEGIN
86,OQL RANGEEND
86,OQL REPLACE
86,OQL ROUND
86,OQL UPPER
86,OQL Operators
86,OQL Case Expression
86,OQL Parameters
86,OQL From Clause
86,OQL Group by Clause
86,OQL Limit Clause
86,OQL Order by Clause
86,OQL Select Clause
86,OQL Where Clause
86,Constants
86,Regular Expressions
86,Scheduled Events
86,Task Queue
86,Document Templates
86,Creating Your Own Documents
86,Data Grid (Document Template)
86,Columns (Document Template)
86,Data View (Document Template)
86,Document Template
86,Dynamic Image (Document Template)
86,Dynamic Label (Document Template)
86,Footer (Document Template)
86,Header (Document Template)
86,Line Break (Document Template)
86,Page Break (Document Template)
86,Static Image (Document Template)
86,Static Label (Document Template)
86,Style
86,Table (Document Template)
86,Row (Document Template)
86,Cell (Document Template)
86,Template Grid (Document Template)
86,Title (Document Template)
86,Data Types
86,Images
86,XPath
86,XPath Aggregate Functions
86,XPath Constraints
86,XPath Constraint Functions
86,XPath true
86,XPath false
86,XPath not
86,XPath length
86,XPath string-length
86,XPath year-from-dateTime
86,XPath month-from-dateTime
86,XPath day-from-dateTime
86,XPath hours-from-dateTime
86,XPath minutes-from-dateTime
86,XPath seconds-from-dateTime
86,XPath quarter-from-dateTime
86,XPath day-of-year-from-dateTime
86,XPath week-from-dateTime
86,XPath weekday-from-dateTime
86,XPath contains
86,XPath starts-with
86,XPath ends-with
86,XPath Expressions
86,XPath Keywords and System Variables
86,XPath Operators
86,XPath Tokens
86,Define Access Rules Using XPath
86,Filter Data Using XPath
86,Integration
86,Message Definitions
86,JSON Structures
86,XML Schemas
86,XML Schema Support
86,Mapping Documents
86,Export Mappings
86,Import Mappings
86,Map Automatically
86,ML Model Mapping
86,Select Elements
86,XML Inheritance and Choice
86,Business Event Services
86,External Database Connection
86,OData Services
86,Consumed OData Services
86,Consumed OData Service
86,Consumed OData Service Requirements
86,Published OData Services
86,Published OData Attribute
86,OData Query Options
86,OData Representation
86,Published OData Entity
86,Published OData Microflow
86,Build OData APIs with REST Best Practices
86,Security and Shared Datasets
86,REST Services
86,Consumed REST Services
86,Using a Proxy to Call a REST Service
86,Server-Side Paging and Sorting
86,Advanced Consumed REST Services
86,Published REST Services
86,Published REST Service
86,Published REST Operation
86,Operation Parameters for Published REST
86,Published REST Path Parameters
86,Published REST Query Parameters
86,Published REST Resource
86,CORS Settings for Published REST Services
86,GitHub-Flavored Markdown
86,Version a REST Service
86,Generating a Published REST Resource
86,Publish Microflow as REST Operation
86,Technical Details of Published REST
86,Published REST Request Routing
86,JSON Schema for Published REST Operation
86,OpenAPI 2.0 Documentation
86,OpenAPI 3.0 Documentation
86,Custom Authentication Microflow Parameters
86,HttpRequest and HttpResponse System Entities
86,Images and Files with REST
86,Consumed REST Services (Beta)
86,Web Services
86,Consumed Web Services
86,Consume a Simple Web Service
86,Consume a Complex Web Service
86,Consumed Web Service
86,Numeric Formatting
86,Using a Proxy to Call a Web Service
86,Published Web Services
86,Expose a Web Service
86,Operations
86,Published Web Service
86,Test Web Services Using SoapUI
86,Machine Learning Kit
86,Using ML Kit
86,Logistic Regression Example
86,Pre-Trained ML Models
86,Design Patterns
86,Advanced Inference Design Patterns
86,Pre/Post-Processor Design Patterns
86,Version Control
86,Using Version Control
86,Combining Changes and Conflict Resolution
86,Automatic Fetching
86,Git Storage Optimization
86,Troubleshoot Version Control
86,Repository Size
86,Team Server Issues
86,Version Control FAQ
86,Git On-Premises Version Control Server
86,Mendix Runtime
86,Runtime Server
86,Mendix Client
86,Mendix React Client
86,Marketplace Component React Status
86,Runtime Deployment
86,Clustered Mendix Runtime
86,Communication Patterns
86,Data Sources Retrieval
86,Minimizing Objects in Session
86,Data Storage
86,Attribute Type Migration
86,Case-Sensitive Database Behavior
86,Order By Behavior
86,Unlimited String Behavior
86,MySQL/MariaDB
86,Oracle
86,SAP HANA
86,Date and Time Handling
86,DateTime Handling FAQ
86,Logging
86,Login Behavior
86,Mendix Runtime and Java
86,Non-Persistable Objects and Garbage Collecting
86,Java Memory Usage
86,Common Runtime and Java Errors
86,Metrics
86,Monitoring Client State
86,Monitoring Mendix Runtime
86,Objects and Caching
86,Runtime Customization
86,Advanced Custom Settings
86,WebSockets
86,Mobile
86,Getting Started with Mobile
86,Prerequisites and Troubleshooting
86,Introduction to Mobile Technologies
86,Native Mobile
86,Progressive Web App
86,Designing Mobile User Interfaces
86,Design Principles
86,Navigation
86,"Images, Icons, and Fonts"
86,Native Styling
86,Widget Styling Guide
86,Building Efficient Mobile Apps
86,Optimizing Native Startup
86,Offline-First Data
86,Offline Synchronization
86,Offline Best Practices
86,Synchronization & Auto-Committed Objects
86,Offline Data Security
86,Logging in Native Apps
86,Using Mobile Capabilities
86,Authenticating Users
86,Deep Links
86,Internationalize Mobile Apps
86,Location and Maps
86,Push Notifications
86,1. Add Module Dependencies
86,2. Push Notifications Module
86,3. Set Up Firebase Cloud Messaging
86,4. Configure Push Notifications
86,5. Push Notifications in Native App
86,6. Native App with Push Notifications
86,7. Test Push Notification
86,8. Notifications to Multiple Devices
86,Local Notifications
86,Part 1: Local Notifications
86,Part 2: Badges
86,Part 3: Actions
86,Part 4: Data
86,Part 5: Scheduling
86,Augmented Reality
86,Get Started with AR
86,Create an AR Business Card
86,App Permissions
86,Mobile Accessibility
86,"Build, Test, Distribute Apps"
86,Building Native Apps
86,Build a Mendix Native App Locally
86,Native App Local Manual Build
86,Deploy Mendix Native Mobile App
86,Creating a Custom Developer App
86,Native Template
86,Distributing Native Apps
86,Updating Native Apps
86,Debugging Native Apps
86,Testing Native Apps
86,Best Practices for Mobile Apps
86,Deleted Flag
86,Incremental Synchronization
86,Batch Synchronization
86,Compound Object
86,Request Object
86,Java Programming
86,Troubleshooting
86,Using Eclipse
86,Extending App with Custom Java
86,Using the Mendix Runtime Java API
86,Java Version Migration
86,Managed Dependencies
86,Studio Pro 10 How-tos
86,Front End
86,UI Design
86,Get Started
86,Customize Styling
86,Configure Module-Level Theme Settings
86,Create a Company Design System
86,Extend Design Properties
86,Atlas UI Kit for Figma
86,Implement Best Practices for UX Design
86,Use Navigation Layouts
86,Configure Your Theme
86,Create Overview and Detail Pages
86,Use Layouts and Snippets
86,Implement Classes
86,Create Custom Error Pages
86,Data Models
86,Denormalize Data to Improve Performance
86,Share the Development Database
86,Migrate Your Mendix Database
86,Integration
86,Integrate Legacy System
86,Import XML Documents
86,Export XML Documents
86,Import Excel Documents
86,Import a Large Excel File
86,Export to Excel
86,Publish a REST Service
86,Share Data Between Apps
86,Access a Samba Share
86,Expose Data to BI Tools Using OData
86,Configure Selenium Support
86,Execute SQL on External Database
86,Write Data to Another App
86,Use the Data Importer
86,Use the External Database Connector
86,CI/CD Pipeline for Mendix Cloud
86,Use a Client Certificate
86,Extensibility
86,Build a Pluggable Native Widget
86,Update Pluggable Widgets Tools
86,Build Pluggable Web Widgets
86,1. Build Pluggable Web Widget
86,2. Build Pluggable Web Widget
86,Build JavaScript Actions
86,1. Build JavaScript Actions
86,2. Build JavaScript Actions
86,Build JavaScript Actions for Native Mobile
86,JavaScript Actions Best Practices
86,Build Microflow Actions with Java
86,Data Storage APIs for Reusable Microflows
86,Security
86,Create a Secure App
86,Best Practices for App Security
86,Set Up Anonymous User Security
86,Content Security Policy
86,Testing
86,Test Mendix Apps Using Selenium IDE
86,Create Automated Tests with TestNG
86,Monitoring and Troubleshooting
86,Clear Warning Messages
86,Debug Java Actions
86,Debug Java Actions Remotely
86,Find the Root Cause of Runtime Errors
86,Set Log Levels
86,Monitor Mendix Using JMX
86,Solve Load and Import Errors
86,Manage App Performance
86,Manage App Performance with New Relic
86,Detect and Resolve Performance Issues
86,Populate User Types
86,Developer Portal Guide
86,Global Navigation
86,General
86,Buzz
86,Team
86,App Roles
86,Documents
86,Team Server
86,Migrate to Git
86,Settings
86,Leave and Delete an App
86,Manage Deep Links
86,Project Management
86,Epics
86,Board
86,Planning
86,Epics
86,Archive
86,Jira
86,App Insights
86,Feedback
86,Mini Surveys
86,Deployment
86,General
86,Licensing Apps
86,Secure Outgoing Connections
86,Two-Factor Authentication
86,Version Downgrade Protection
86,Iframes and Running Apps
86,Deployment Location
86,SAP BTP
86,Monitoring Environments in Mendix Apps on SAP BTP
86,SAP Destination Service
86,Use SAP Connectivity Service with REST and SOAP
86,SAP Cloud Connector
86,Application Autoscaler for SAP BTP
86,SAP Single Sign-On
86,Mendix Cloud
86,About Mendix Cloud
86,Environments
86,Environment Details
86,Migrate to Other Node
86,Studio Pro Deployment Settings
86,Licensing Mendix Cloud Apps
86,Mendix Basic Package
86,Free App to Basic Package
86,Node Permissions
86,Running Now
86,Mendix Cloud Status
86,Mendix Cloud Region
86,Scaling in Mendix Cloud
86,Custom Domains
86,Certificates
86,Maintenance Windows
86,Pipelines (Beta)
86,HTTP Request Headers
86,Restrict Incoming Access
86,Mendix IP Addresses
86,Sending Email
86,Mendix Single Sign-On
86,Webhooks
86,Siemens Insights Hub
86,Private Cloud
86,Creating a Private Cloud Cluster
86,Non-Interactive Mode
86,Storage Plans
86,Registry Configuration
86,Hosting Your Own Registry
86,Running the Mendix Operator in Global Mode
86,Running the Mendix Operator in Standard Mode
86,Deploy Mendix App
86,Retrieve Environment-Sensitive Data from a Secret Store
86,Use Velero to Back Up Namespaces
86,Use CLI to Deploy
86,CI/CD with Tekton
86,Air-gapped Tekton Installation
86,PCLM – License Manager
86,Monitor Environments
86,Migrate Data (Preview)
86,Environment Planning
86,Technical Appendix for Mendix Private Cloud
86,1. Introduction to Operators
86,2. Operator Flows
86,Upgrading Private Cloud
86,Supported Providers
86,Industrial Edge Apps
86,Cloud Foundry
86,Docker
86,Run Docker Image
86,Run with Minikube
86,On-Premises
86,On-Premises Installation Security
86,Monitoring with New Relic
86,Microsoft Windows
86,Automate Mendix Deployment
86,Deploy Mendix in MS Azure
86,MS Windows: Activate Mendix License
86,MS Windows: Update a Mendix App
86,Microsoft SQL Server
86,New Database Setup on SQL Server
86,User Setup on SQL Server
86,Database User Setup on SQL Server
86,Maintenance of SQL Server
86,Maintenance Plans for SQL Server
86,Restore Database on SQL Server
86,Troubleshooting SQL Server
86,Troubleshooting IIS
86,Unix-Like Deployment
86,Mobile App
86,Operations
86,Alerts
86,Receive Environment Status Alerts
86,Logs
86,Metrics
86,Monitoring with APM
86,AppDynamics for Mendix Cloud
86,Datadog for Mendix Cloud
86,New Relic for Mendix Cloud
86,Dynatrace for Mendix Cloud
86,Splunk for Mendix Cloud
86,Backups
86,Creating a Backup
86,Downloading a Backup
86,Restoring a Backup
86,Restoring a Backup Locally
86,Reducing Database Size
86,Portfolio Management
86,Prioritization Models
86,Export and Import Initiatives
86,Control Center Guide
86,Dashboard
86,Application Health Dashboard
86,Apps
86,Members
86,Groups
86,Company Settings
86,Company Brand
86,Security
86,Set Up an SSO (BYOIDP)
86,Cloud
86,Entitlements
86,Deployed Apps
86,Catalog
86,Portfolios
86,Private Marketplace
86,Roles & Permissions
86,Marketplace Guide
86,Marketplace Overview
86,My Marketplace
86,Using Marketplace Content
86,SISW EULA for Freeware
86,Mendix Component Partner Program
86,Creating Content
86,Create Solutions
86,Commercial Solution Partner Programs
86,Introduction to Adaptable Solutions
86,Architect Solutions
86,Apply IP Protection
86,Best Practices for Adaptability
86,Implement Solutions
86,Upgrade a Solution
86,Set Up a Solution
86,Build a Connector
86,Best Practices for Building Connectors
86,Sharing Marketplace Content
86,Governance Process
86,Modules
86,AWS Connectors
86,AWS Authentication
86,Amazon Bedrock
86,Amazon DynamoDB
86,Amazon EventBridge
86,Amazon Polly
86,Amazon RDS
86,Amazon Rekognition
86,Amazon S3
86,Amazon SageMaker
86,Amazon SES
86,Amazon SNS
86,Amazon Textract
86,Amazon Translate
86,AWS IoT SiteWise
86,AWS IoT TwinMaker
86,AWS Lambda
86,Build an AWS Connector
86,Amazon SQS
86,SAP Connectors
86,OData Connector for SAP Solutions
86,BAPI Connector for SAP Solutions
86,SAP Event Mesh Connector
86,XSUAA Connector for SAP BTP
86,SAP Logging Connector
86,SAP Fiori UI Resources
86,SAP Horizon Native UI Resources
86,Administration
86,Advanced Audit Trail
86,Advanced Audit Trail UI
86,Any Chart
86,App Switcher ⚠
86,Atlas Core
86,Atlas UI Resources ⚠
86,Audit Trail
86,Community Commons
86,Data Importer
86,Data Widgets
86,Data Grid 2
86,Gallery
86,Tree Node
86,Database
86,Database Replication
86,Deep Link ⚠
86,Email
86,Encryption
86,Excel Exporter
86,Excel Importer
86,External Database Connector
86,Forgot Password
86,Google Tag
86,Hybrid Mobile Actions ⚠
86,Image Crop
86,LDAP
86,Mendix Feedback
86,Mendix Mini Surveys
86,Mendix SSO
86,Mobile SSO
86,MQTT
86,Mx Model Reflection
86,Nanoflow Commons
86,Native Mobile AR
86,Native Mobile Resources
86,Object Handling
86,OIDC SSO
86,OpenAI
86,RAG Example Implementation
86,Vector Database Setup
86,PDF Document Generation
86,Process Queue ⚠
86,Push Notifications Connector
86,SAML
86,Unit Testing
86,User Migration
86,Web Actions
86,Workflow Commons
86,Services
86,Event Broker
86,Mendix Business Events
86,Model Creator for SAP Integrations
86,OIDC Provider
86,Pusher
86,Widgets
86,Widget CSP Overview
86,HTML/JavaScript Snippet CSP
86,Maps CSP
86,Accessibility Helper
86,Accordion
86,Auto-Load More ⚠
86,Badge
86,Badge Button
86,Barcode Scanner
86,Bootstrap Tooltip ⚠
86,Calendar
86,Carousel
86,Charts
86,Create a Basic Chart
86,Use Any Chart
86,Chart Advanced Tuning
86,Use the Charts Theme
86,Create a Dynamic Series Chart
86,Use a Chart with a REST Data Source
86,Plotly Images REST Endpoint
86,Checkbox Set Selector
86,Color Picker
86,Combo Box
86,Fieldset
86,Format String ⚠
86,Google Analytics
86,Google Maps ⚠
86,HTML Element
86,HTML/JavaScript Snippet
86,Image
86,Label Selector
86,Language Selector
86,List View Swipe ⚠
86,Maps
86,Microflow Timer
86,Mobile Device ⚠
86,Mobile Features ⚠
86,Pop-Up Menu
86,Progress Bar
86,Progress Circle
86,Pull to Refresh ⚠
86,Radio Button List
86,Range Slider
86,Rating
86,Rich Text
86,Rich Text v2.0 & Below
86,Signature
86,Simple Checkbox Set Selector
86,Slider
86,Switch
86,Tab Swipe ⚠
86,Timeline
86,Tooltip
86,Video Player
86,Partner Solutions
86,APD
86,APD Installation Guides
86,APD 3 Installation Guide
86,APM 2 Installation Guide
86,APM 1 Installation Guide
86,Prerequisites
86,Java Security Settings
86,Sizing Impact
86,Installation Steps
86,After Startup Error?
86,Constants
86,Uninstall Steps
86,Upgrade Steps
86,APM Use Cases
86,APD 3 Use Cases
86,APM 2 Use Cases
86,APM 1 Use Cases
86,APD Reference Guides
86,APD 3 Reference Guide
86,Apps
86,Dashboard
86,Environments
86,Logs
86,Long-Running Actions
86,Performance Recorder
86,Browser Recorder Results
86,Runtime Recorder Results
86,Performance Statistics
86,Settings
86,APM 2 Reference Guide
86,Apps
86,Dashboard
86,Environments
86,Logs
86,Long-Running Actions
86,Performance Recorder
86,Browser Recorder Results
86,Runtime Recorder Results
86,Performance Statistics
86,Settings
86,APM 1 Reference Guide
86,Configuration
86,Dashboard
86,Download and License
86,JVM Browser
86,Load Test Recorder
86,Log Tool
86,Measurements Tool
86,Performance Tool
86,Inserting Context Information
86,Performance Tool Results
86,Query Tool
86,Statistics Tool
86,Trap Tool
86,Triggers
86,APD Release Notes
86,ATS
86,ATS Overview
86,Introduction to ATS
86,Compatibility
86,Deployment Options
86,Maintenance
86,ATS Reference Guides
86,ATS 2 Reference Guide
86,Action
86,Administration
86,App
86,ATS Helper
86,CI/CD API
86,Data-Driven Testing
86,Desktop Recorder
86,Drop-Down
86,Function Reference
86,Local Profile
86,On-Premises Installation
86,Repository
86,Results
86,Schedule
86,Selectors
86,Supported Widgets
86,Test Case
86,Test Run
86,Compatibility Table
86,Job Configuration
86,Supported Selenium Providers
86,Test Step
86,Test Suite
86,ATS 1 Reference Guide
86,Administration
86,Configuration
86,Data Management
86,Monitoring
86,Projects
86,Scheduling
86,Test Development
86,Object Types in ATS
86,Recorder
86,Manual Test Steps
86,Standard Actions
86,Custom Actions
86,Best Practices for Writing Custom Actions
86,Selectors
86,Data-Driven Tests
86,Test Case Documentation
86,Standard Actions Reference
86,ATS Core Actions
86,Assert Equals
86,Assert Not equals
86,Concatenate String
86,Get Current DateTime String
86,Random Number
86,Random String
86,Set Return Value
86,Mendix Actions
86,"DataGrid, TemplateGrid, and ListView"
86,Click DataGrid Row
86,Find Item/Row
86,Find Item/Row (by child element)
86,Find Selected Item/Row
86,Find/Assert DataGrid Row
86,Get Item/Row Index
86,Get Row Cell Value
86,Get Total Item/Row Count
86,Get Visible Item/Row Count
86,Set ListView Search
86,Set Row Cell Value
86,Sort DataGrid
86,Dialog
86,Cancel Dialog
86,Close Dialog
86,Confirm Dialog
86,Find/Assert Dialog
86,Get Dialog Message Text
86,File Manager
86,Set File Manager
86,Generic
86,Assert Current Page
86,Assert Validation Message
86,Click Widget
86,Click Widget Button
86,Click/Doubleclick
86,Find/Assert Widget
86,Get Validation Message
86,Login
86,Logout
86,Open Application
86,GroupBox
86,Close GroupBox
86,GroupBox is Collapsed
86,Open GroupBox
86,Input
86,Assert Checkbox Value
86,Assert Value
86,Dropdown has Option
86,Get Checkbox Value
86,Get Index
86,Get Value
86,Set Checkbox Value
86,Set Value
86,Set Value (by Index)
86,Toggle Checkbox Value
86,Navigation Menu
86,Click Menu Item
86,Find/Assert Menu Item
86,System
86,Find Widget Child Node
86,Focus WebElement
86,Get Current Page Title
86,Mendix wait
86,Tab
86,Assert Active Tab Caption
86,Get Active Tab Caption
86,Mendix Marketplace Widgets Actions
86,BooleanSlider
86,Assert BooleanSlider Value
86,Get BooleanSlider Value
86,Set BooleanSlider Value
86,Toggle BooleanSlider Value
86,BootstrapRTE
86,Assert BootstrapRTE Value
86,Get BootstrapRTE Value
86,Set BootstrapRTE Value
86,Checkbox Set Selector
86,Assert Checkbox Set Selector Value
86,Find Checkbox Set Selector
86,Find Checkbox Set Selector (All)
86,Get Checkbox Set Selector Value
86,Get Checkbox Set Selector Value (All)
86,Set Checkbox Set Selector Value
86,Set Checkbox Set Selector Value
86,Toggle Checkbox Set Selector Value
86,Toggle Checkbox Set Selector Value
86,CKEditor
86,Assert CKEditor Value
86,Get CKEditor Value
86,Set CKEditor Value
86,Dropdown Div Converter
86,Click Drop-Down div Converter Drop-Down Button
86,Click Drop-Down div Converter Split Button
86,Grid Selector
86,Assert Grid Selector Value
86,Find Grid Selector Box
86,Get Grid Selector Box Value
86,Set Checkbox Set Selector Value
86,Set Grid Selector RadioButton Value
86,Toggle Grid Selector Checkbox Value
86,Input reference Selector
86,Assert InputReferenceSelector Value
86,Get InputReferenceSelector Value
86,Set InputReferenceSelector Value
86,Simple Checkbox Set Selector
86,Assert Simple Checkbox Set Selector Value
86,Find Simple Checkbox Set Selector
86,Get Simple Checkbox Set Selector Value
86,Set Simple Checkbox Set Selector Value
86,Toggle Simple Checkbox Set Selector Value
86,Selenium Actions
86,Click Coordinates
86,Execute JavaScript Integer
86,Execute Javascript String
86,Execute Javascript WebElement
86,Find
86,Find Element by CSS
86,Find Element by ID
86,Find Element by Sizzle
86,Get
86,Get Property Value
86,Get Selected Option Index
86,Get Selected Option Text
86,Get Selected Option Value
86,Get Text
86,Send Keys
86,Test Run
86,ATS How-tos
86,ATS 2 How-tos
86,(Un)Mask Your Data
86,Assert Data Grid Rows
86,Configure a Selenium Hub
86,Create a Data-Driven Test Case
86,Create a Negative Test Case
86,Create a Test Case
86,Create a Test Suite
86,Create Custom Actions
86,Create Custom Action Basics
86,Create Search Context Actions
86,CAB.11 - Find Item/Row by Unique Text Value
86,Create Unsupported Widget Actions
86,CAB.02 - Switch
86,CAB.03 - Textbox
86,CAB.05 - Reference Selector
86,CAB.07 - Radio Buttons
86,CAB.10 - AutoComplete
86,General
86,Custom Action Expense App
86,Definitions
86,Guidelines for Creating a Custom Action
86,Helpful Resources
86,Prerequisites for How-To's
86,Structure for How-To's
86,Create Maintainable Test Cases
86,Get Started
86,Increase ATS Recorder and Helper Coverage
86,Install ATS Helper and ATS Recorder
86,Link Test Cases and Suites to User Stories
86,Schedule a Test Suite/Test Case
86,Set Up Selenium Locally
86,Set Up a Local Docker Selenium Hub
86,Set Up a Local Selenium Hub
86,Set Up a Local Selenium Solution
86,Set Up a Local Selenoid Hub
86,Upload a File in Your App Using ATS
86,Browserstack Test Files
86,Use ATS in Combination with CI/CD
86,Use Precondition in Test Cases
86,ATS 1 How-tos
86,Get Started
86,Install ATS Helper and Recorder
86,Create a Test Case
86,Create a Test Suite
86,Create Custom Actions
86,Create Custom Action Basics
86,Create Search Context Actions
86,CAB.11 - Find Item/Row by Unique Text Value
86,Create Unsupported Widget Actions
86,CAB.02 - Switch
86,CAB.03 - Textbox
86,CAB.05 - Reference Selector
86,CAB.07 - Radio Buttons
86,CAB.10 - AutoComplete
86,General
86,Custom Action Expense App
86,Definitions
86,Guidelines for Creating a Custom Action
86,Helpful Resources
86,Prerequisites for How-tos
86,Structure for How-tos
86,Upload a File in Your App Using ATS
86,ATS Best Practices
86,ATS 2 Best Practices
86,Finding the Action You Need
86,Test Case Dependencies
86,ATS 1 Best Practices
86,Finding the Action You Need
86,ATS Release Notes
86,QSM
86,Catalog Guide
86,Get Started with the Catalog
86,Register Data Sources
86,Register Resources
86,Register Non-OData Resources
86,Automate Catalog Registration
86,Private Cloud/On-Premises Registration
86,OpenAPI Beta Functionality
86,Consume Data Sources
86,Consume Registered Assets
86,Manage Data Sources
86,Landscape View
86,Catalog User Roles
86,Curate Registered Assets
86,Data Accessibility and Security
86,Search in the Catalog
86,Private Mendix Platform Guide
86,Private Mendix Platform Prerequisites
86,Private Mendix Platform Quick Start Guide
86,Configuring Private Mendix Platform
86,Configuring CI/CD on Azure
86,Configuring CI/CD on Kubernetes
86,Configuring the Version Control System for Private Mendix Platform
86,Private Mendix Platform Administration Guide
86,Private Mendix Platform User Guide
86,Community Tools Guide
86,Mendix Profile
86,User Settings
86,Mendix Community
86,Set Up Your Partner Profile
86,Contribute to a GitHub Repo
86,OAuth and Scopes
86,Contribute to Mendix Docs
86,Documentation Writing Guidelines
86,Mendix Support Guide
86,Prepare Your App for Support
86,Submit a Support Request
86,App Node Requests
86,Support Ticket Priority
86,Support Escalation Process
86,Security Findings FAQ
86,Strategic Partners Guide
86,Siemens
86,Insights Hub
86,Insights Hub IIoT for Makers
86,Mendix on Insights Hub
86,Insights Hub Development Considerations
86,Insights Hub Module Details
86,Insights Hub Monitor Example
86,Insights Hub Mobile Native
86,3D Viewer
86,3D Viewer for Teamcenter
86,Use the 3D Viewer API
86,AWS
86,SAP
86,APIs and SDK
86,API Documentation
86,App Repository API
86,Authentication
86,Backups API v2
86,Build API
86,Catalog APIs
86,Client API
86,Content API
86,Deploy API v1
86,Deploy API v2
86,Deploy API v4
86,Design Properties API
86,Epics API
86,Feedback API v1 ⚠
86,Feedback API v2
86,Mendix for Private Cloud Build API
86,Mendix for Private Cloud Deploy API
86,Mendix Runtime API
86,Model SDK and Platform SDK
86,Permissions API ⚠
86,Pluggable Widgets API
86,Property Types
86,Client APIs for Pluggable Widgets
86,List Values
86,Preview Appearance APIs
86,Configuration Module API
86,Declaring Native Dependencies
86,Mendix 9
86,Property Types – Mx9
86,Client APIs for Pluggable Widgets
86,List Values – Mx9
86,Preview Appearance APIs
86,Configuration Module API – Mx9
86,Declaring Native Dependencies – Mx9
86,Mendix 8
86,Property Types – Mx8
86,Client APIs for Pluggable Widgets
86,Preview Appearance APIs
86,Compare Pluggable and Custom Widgets
86,Private Mendix Platform API Documentation
86,Private Mendix Platform Group API
86,Private Mendix Platform Marketplace API
86,Private Mendix Platform Project API
86,Private Mendix Platform User API
86,Projects API ⚠
86,Stories API ⚠
86,Team Server API ⚠
86,User Management API ⚠
86,Webhooks API
86,Webhooks for Stories/Sprints ⚠
86,SDK Documentation
86,SDK Introduction
86,SDK Use Cases
86,SDK FAQ and Troubleshooting
86,SDK Reference Guide
86,Mendix Metamodel
86,Projects in the Metamodel
86,Domain Model in the Metamodel
86,Pages in the Metamodel
86,Microflows in the Metamodel
86,JavaScript and TypeScript Resources
86,SDK How-tos
86,Set Up Your Development Environment
86,Set Up your Personal Access Token (PAT)
86,Use the Platform SDK
86,Create Your First Script
86,Create the Domain Model
86,Manipulate Existing Models
86,Change Things in the Model
86,Close the Server Connection
86,Find Things in the Model
86,Work with Load Units and Elements
86,Generate SDK Script Based on Model
86,Old SDK Versions (Below 5.0) ⚠
86,Set Up Development Environment
86,Create Your First Script
86,Studio Pro 9 Guide
86,General Info
86,System Requirements
86,Install Mendix Studio Pro
86,Configure Parallels
86,Moving from Mendix Studio Pro 8 to 9
86,Migrate From Atlas 2 To Atlas 3
86,Atlas 3 Change Summary
86,Migrate Workflow Apps
86,mx Command-Line Tool
86,MxBuild
86,Developer Tool Recommendations
86,Third-Party Licenses
86,App Modeling
86,Studio Pro Overview
86,Best Practices for Development
86,Best Practices for App Performance
86,Importing and Exporting Elements
86,Starting with App from a Spreadsheet
86,Menus
86,File Menu
86,New App
86,Open App
86,Export App Package
86,Import App Package
86,Edit Menu
86,"Find, Find Advanced and Find Usages"
86,Go to Option
86,Preferences
86,View Menu
86,Changes Pane
86,Data Hub Pane
86,Errors Pane
86,Consistency Errors
86,Page Editor Consistency Errors
86,Navigation Consistency Errors
86,Suppression Rules
86,Page Explorer
86,Stories Pane
86,App Menu
86,Create Deployment Package
86,Deploy to the Cloud
86,Run Menu
86,Edit Cloud Foundry Settings
86,Version Control Menu
86,Commit
86,History
86,Download from Version Control Server
86,Upload to Version Control Server
86,Branch Line Manager
86,Create Branch Line
86,Merge Dialog
86,Language Menu
86,Batch Replace
86,Batch Translate
86,Language Operations
86,Language Settings
86,Translating Your App Content
86,Using Translatable Validation Messages
86,App Explorer
86,App
86,App Settings
86,Configurations
86,Navigation
86,Set Up Navigation
86,System Texts
86,Modules
86,Module Settings
86,Publish Add-on and Solution Modules
86,Consume Add-on Modules and Solutions
86,UI Resources Package
86,Security
86,App Security
86,User Roles
86,Administrator
86,Demo Users
86,Anonymous Users
86,Password Policy
86,Module Security
86,Domain Model
86,Entities
86,Persistability
86,Attributes
86,Validation Rules
86,Event Handlers
86,Indexes
86,Access Rules
86,External Entities
86,Associations
86,Association Properties
86,Association Tab Properties
86,Querying Over Self-References
86,Annotations
86,Generalization vs 1-to-1 Associations
86,Creating a Basic Data Layer
86,Setting Up Data Validation
86,Pages
86,Page
86,Page Properties
86,Page Resources
86,Icon Collection
86,Image Collection
86,Layout
86,Placeholder
86,Header
86,Sidebar Toggle
86,Page Template
86,Snippet
86,Building Block
86,Menu
86,Data Containers
86,Data View
86,Grids
86,Data Grid
86,Grid Columns
86,Template Grid
86,Control Bar
86,Search Bar
86,Sort Bar
86,List View
86,Data Sources
86,Database Source
86,XPath Source
86,Context Source
86,Microflow Source
86,Nanoflow Source
86,Association Source
86,Listen to Widget Source
86,Configure Form and Show Form Items
86,Configure List and View Details on 1 Page
86,Text
86,Text
86,Label
86,Page Title
86,Structure
86,Layout Grid
86,Container
86,Group Box
86,Snippet Call
86,Tab Container
86,Scroll Container
86,Table
86,Navigation List
86,Input Elements
86,Text Box
86,Text Area
86,Drop-Down
86,Check Box
86,Radio Buttons
86,Date Picker
86,Reference Selector
86,Reference Set Selector
86,Input Reference Set Selector
86,"Images, Videos and Files"
86,Static Image
86,Dynamic Image
86,File Manager
86,Image Uploader
86,Enable End-Users to Attach Images
86,Configure File Upload and Download
86,Buttons
86,Button Properties
86,Creating a Custom Save Button
86,Menus and Navigation
86,Menu Bar
86,Simple Menu Bar
86,Navigation Tree
86,Reports
86,Report Grid
86,Report Parameter
86,Report Date Parameter
86,Date Range Field
86,Generate Report Button
86,Authentication
86,Login ID Text Box
86,Password Text Box
86,Sign-In Button
86,Validation Message
86,Charts
86,Chart Configuration
86,Chart Advanced Cheat Sheet
86,Any Chart Widgets
86,Any Chart Building Blocks
86,Any Chart Cheat Sheet
86,Properties Common in the Page Editor
86,On Click Event and Events Section
86,Application Logic
86,Microflows and Nanoflows
86,Microflows
86,Microflow Properties
86,Triggering a Microflow From a Menu Item
86,Testing Microflows with Unit Test Module
86,Error Handling in Microflows
86,Extracting and Using Sub-Microflows
86,Nanoflows
86,Nanoflow Properties
86,Error Handling in Nanoflows
86,Sequence Flow
86,Activities
86,Object Activities
86,Cast Object
86,Change Object
86,Commit Object(s)
86,Create Object
86,Delete Object(s)
86,Retrieve
86,Rollback Object
86,List Activities
86,Aggregate List
86,Change List
86,Create List
86,List Operation
86,Working with Lists in a Microflow
86,Action Call Activities
86,Java Action Call
86,JavaScript Action Call
86,Microflow Call
86,Variable Activities
86,Change Variable
86,Create Variable
86,ML Kit Activities
86,Call ML Model
86,Client Activities
86,Call Nanoflow
86,Show Message
86,Close Page
86,Download File
86,Show Home Page
86,Show Page
86,Synchronize to Device
86,Synchronize
86,Validation Feedback
86,Integration Activities
86,Call REST Service
86,Call Web Service
86,Import with Mapping
86,Export With Mapping
86,Log Message
86,Generate Document
86,Workflow Activities
86,Apply Jump-To Option
86,Workflow Call
86,Change Workflow State
86,Complete Task
86,Generate Jump-To Options
86,Retrieve Workflow Context
86,Show User Task Page
86,Show Workflow Admin Page
86,Lock Workflow
86,Unlock Workflow
86,External Object Activities
86,Delete External Object
86,Send External Object
86,Metrics Activities
86,Counter
86,Gauge
86,Increment Counter
86,Decisions
86,Decision
86,Object Type Decision
86,Merge
86,Annotation
86,Parameter
86,Loop
86,Events
86,Start Event
86,End Event
86,Error Event
86,Continue Event
86,Break Event
86,Common Properties
86,Debugging Microflows and Nanoflows
86,Debugging Microflows Remotely
86,Workflows
86,Workflow Elements
86,Workflow Parameters
86,User Task
86,Decision in Workflows
86,Parallel Split
86,Jump Activity
86,Call Microflow
86,Call Workflow
86,Workflow Properties
86,Configure Workflow Security
86,Add Workflow to Existing App
86,Jump to Different Activities
86,Workflow Versioning and Conflict Mitigation
86,Workflow for Employee Onboarding
86,Add Custom Action to Workflow Toolbox
86,Expressions
86,Unary Expressions
86,Arithmetic Expressions
86,Relational Expressions
86,Special Checks
86,Boolean Expressions
86,If Expressions
86,Mathematical Function Calls
86,String Function Calls
86,Date Creation
86,Begin-of Date Function Calls
86,End-of Date Function Calls
86,Between Date Function Calls
86,Add Date Function Calls
86,Subtract Date Function Calls
86,Trim to Date
86,To String
86,Parse Integer
86,Parse and Format Decimal Function Calls
86,Parse and Format Date Function Calls
86,Enumerations in Expressions
86,Configure String Concatenation
86,Mendix Assist
86,MxAssist Logic Bot
86,MxAssist Performance Bot
86,Performance Best Practices
86,Validation Assist
86,Resources
86,Java Actions
86,JavaScript Actions
86,Rules
86,Enumerations
86,Datasets
86,OQL
86,OQL Expressions
86,OQL Aggregation
86,OQL Functions
86,OQL CAST
86,OQL COALESCE
86,OQL DATEDIFF
86,OQL DATEPART
86,OQL LENGTH
86,OQL LOWER
86,OQL RANGEBEGIN
86,OQL RANGEEND
86,OQL REPLACE
86,OQL ROUND
86,OQL UPPER
86,OQL Operators
86,OQL Case Expression
86,OQL Parameters
86,OQL From Clause
86,OQL Group by Clause
86,OQL Limit Clause
86,OQL Order by Clause
86,OQL Select Clause
86,OQL Where Clause
86,Constants
86,Regular Expressions
86,Scheduled Events
86,Scheduled Events – Task Queue
86,Legacy Scheduled Events
86,Task Queue
86,Document Templates
86,Creating Your Own Documents
86,Data Grid (Document Template)
86,Columns (Document Template)
86,Data View (Document Template)
86,Document Template
86,Dynamic Image (Document Template)
86,Dynamic Label (Document Template)
86,Footer (Document Template)
86,Header (Document Template)
86,Line Break (Document Template)
86,Page Break (Document Template)
86,Static Image (Document Template)
86,Static Label (Document Template)
86,Style
86,Table (Document Template)
86,Row (Document Template)
86,Cell (Document Template)
86,Template Grid (Document Template)
86,Title (Document Template)
86,Data Types
86,Images
86,XPath
86,XPath Aggregate Functions
86,XPath avg
86,XPath count
86,XPath max
86,XPath min
86,XPath sum
86,XPath Constraints
86,XPath Constraint Functions
86,XPath contains
86,XPath day-from-dateTime
86,XPath day-of-year-from-dateTime
86,XPath ends-with
86,XPath false
86,XPath hours-from-dateTime
86,XPath length
86,XPath minutes-from-dateTime
86,XPath month-from-dateTime
86,XPath not
86,XPath quarter-from-dateTime
86,XPath seconds-from-dateTime
86,XPath starts-with
86,XPath string-length
86,XPath true
86,XPath week-from-dateTime
86,XPath weekday-from-dateTime
86,XPath year-from-dateTime
86,XPath Expressions
86,XPath Keywords and System Variables
86,XPath Operators
86,XPath Tokens
86,Define Access Rules Using XPath
86,Filter Data Using XPath
86,Integration
86,Message Definitions
86,JSON Structures
86,XML Schemas
86,XML Schema Support
86,Mapping Documents
86,Export Mappings
86,Import Mappings
86,Map Automatically
86,ML Model Mapping
86,Select Elements
86,XML Inheritance and Choice
86,Business Event Services
86,OData Services
86,Consumed OData Services
86,Consumed OData Service
86,Consumed OData Service Requirements
86,Published OData Services
86,Published OData Attribute
86,OData Query Options
86,OData Representation
86,Published OData Resource
86,Wrap with OData
86,REST Services
86,Consumed REST Services
86,Using a Proxy to Call a REST Service
86,Server-Side Paging and Sorting
86,Consume a REST Service
86,Published REST Services
86,Publish a REST Service
86,Published REST Service
86,Published REST Operation
86,Operation Parameters for Published REST
86,Published REST Path Parameters
86,Published REST Query Parameters
86,Published REST Resource
86,CORS Settings for Published REST Services
86,GitHub-Flavored Markdown
86,Version a REST Service
86,Generating a Published REST Resource
86,Publish Microflow as REST Operation
86,Technical Details of Published REST
86,Published REST Request Routing
86,JSON Schema for Published REST Operation
86,OpenAPI 2.0 Documentation
86,Custom Authentication Microflow Parameters
86,HttpRequest and HttpResponse System Entities
86,Images and Files with REST
86,Web Services
86,Consumed Web Services
86,Consume a Simple Web Service
86,Consume a Complex Web Service
86,Consumed Web Service
86,Numeric Formatting
86,Using a Proxy to Call a Web Service
86,Published Web Services
86,Expose a Web Service
86,Operations
86,Published Web Service
86,Test Web Services Using SoapUI
86,Machine Learning Kit
86,Using ML Kit
86,Logistic Regression Example
86,Pre-Trained ML Models
86,Design Patterns
86,Advanced Inference Design Patterns
86,Pre/Post-Processor Design Patterns
86,Version Control
86,Using Version Control
86,Merge Algorithm and Conflict Resolution
86,Git Storage Optimization
86,Troubleshoot Version Control
86,Solving Git Issues
86,Team Server Issues
86,Version Control FAQ
86,Differences Between Git and SVN
86,SVN On-Premises Version Control Server
86,Git On-Premises Version Control Server
86,Mendix Runtime
86,Runtime Server
86,Mendix Client
86,Runtime Deployment
86,Clustered Mendix Runtime
86,Communication Patterns
86,Minimizing Objects in Session
86,Data Storage
86,Attribute Type Migration
86,Case-Sensitive Database Behavior
86,Order By Behavior
86,Unlimited String Behavior
86,DB2
86,MySQL/MariaDB
86,Oracle
86,SAP HANA
86,Date and Time Handling
86,DateTime Handling FAQ
86,Logging
86,Login Behavior
86,Mendix Runtime and Java
86,Non-Persistable Objects and Garbage Collecting
86,Java Memory Usage
86,Common Runtime and Java Errors
86,Metrics
86,Monitoring Client State
86,Monitoring Mendix Runtime
86,Objects and Caching
86,Runtime Customization
86,Advanced Custom Settings
86,WebSockets
86,Mobile
86,Getting Started with Mobile
86,Prerequisites and Troubleshooting
86,Introduction to Mobile Technologies
86,Native Mobile
86,Progressive Web App
86,Hybrid Mobile (Deprecated)
86,Designing Mobile User Interfaces
86,Design Principles
86,Navigation
86,"Images, Icons, and Fonts"
86,Native Styling
86,Widget Styling Guide
86,Building Efficient Mobile Apps
86,Optimizing Native Startup
86,Offline-First Data
86,Offline Synchronization
86,Offline Best Practices
86,Synchronization & Auto-Committed Objects
86,Offline Data Security
86,Logging in Native Apps
86,Using Mobile Capabilities
86,Deep Links
86,Internationalize Mobile Apps
86,Location and Maps
86,Push Notifications
86,1. Add Module Dependencies
86,2. Push Notifications Module
86,3. Set Up Firebase Cloud Messaging
86,4. Configure Push Notifications
86,5. Push Notifications in Native App
86,6. Native App with Push Notifications
86,7. Test Push Notification
86,8. Notifications to Multiple Devices
86,Local Notifications
86,Part 1: Local Notifications
86,Part 2: Badges
86,Part 3: Actions
86,Part 4: Data
86,Part 5: Scheduling
86,Augmented Reality
86,Get Started with AR
86,Create an AR Business Card
86,App Permissions
86,Mobile Accessibility
86,"Build, Test, Distribute Apps"
86,Building Native Apps
86,Build a Mendix Native App Locally
86,Deploy Mendix Native Mobile App
86,Native App Local Manual Build
86,Creating a Custom Developer App
86,Native Template
86,Distributing Native Apps
86,Updating Native Apps
86,Debugging Native Apps
86,Testing Native Apps
86,Java Programming
86,Troubleshooting
86,Using Eclipse
86,Extending App with Custom Java
86,Using the Java API
86,Studio Pro 9 How-tos
86,Front End
86,UI Design
86,Get Started
86,Customize Styling
86,Configure Module-Level Theme Settings
86,Create a Company Design System
86,Extend Design Properties
86,Implement Best Practices for UX Design
86,Use Navigation Layouts
86,Configure Your Theme
86,Create Overview and Detail Pages
86,Use Layouts and Snippets
86,Implement Classes
86,Create Custom Error Pages
86,Data Models
86,Denormalize Data to Improve Performance
86,Share the Development Database
86,Migrate Your Mendix Database
86,Integration
86,Integrate Legacy System
86,Import XML Documents
86,Export XML Documents
86,Import Excel Documents
86,Import a Large Excel File
86,Export to Excel
86,Access a Samba Share
86,Expose Data to BI Tools Using OData
86,Configure Selenium Support
86,Execute SQL on External Database
86,CI/CD Pipeline for Mendix Cloud
86,Use a Client Certificate
86,Extensibility
86,Build a Pluggable Native Widget
86,Build Pluggable Web Widgets
86,1. Build Pluggable Web Widget
86,2. Build Pluggable Web Widget
86,Build JavaScript Actions
86,1. Build JavaScript Actions
86,2. Build JavaScript Actions
86,Build JavaScript Actions for Native Mobile
86,JavaScript Actions Best Practices
86,Build Microflow Actions with Java
86,Data Storage APIs for Reusable Microflows
86,Security
86,Create a Secure App
86,Best Practices for App Security
86,Set Up Anonymous User Security
86,Content Security Policy
86,Testing
86,Test Mendix Apps Using Selenium IDE
86,Create Automated Tests with TestNG
86,Monitoring and Troubleshooting
86,Clear Warning Messages
86,Debug Java Actions
86,Debug Java Actions Remotely
86,Debug a Hybrid Mobile Application
86,Find the Root Cause of Runtime Errors
86,Set Log Levels
86,Monitor Mendix Using JMX
86,Solve Load and Import Errors
86,Manage App Performance
86,Manage App Performance with New Relic
86,Detect and Resolve Performance Issues
86,Populate User Types
86,Studio Pro 8 Guide
86,General Info
86,System Requirements
86,Desktop Modeler 7 to Studio Pro 8
86,Troubleshooting DOM Changes
86,Troubleshooting Atlas UI Changes
86,mx Command-Line Tool
86,MxBuild
86,Developer Tool Recommendations
86,Third-Party Licenses
86,App Modeling
86,Studio Pro Overview
86,Menus
86,File Menu
86,New Project
86,Open Project
86,Export Project Package
86,Import Project Package
86,Edit Menu
86,"Find, Find Advanced, and Find Usages"
86,Go to Option
86,Preferences
86,View Menu
86,Changes Pane
86,Data Hub Pane
86,Errors Pane
86,Consistency Errors
86,Navigation Consistency Errors
86,Page Editor Consistency Errors
86,Suppression Rules
86,Project Explorer
86,Project
86,Project Settings
86,Configurations
86,Navigation
86,System Texts
86,Modules
86,UI Resources Package
86,Security
86,Project Security
86,User Roles
86,Administrator
86,Demo Users
86,Anonymous Users
86,Password Policy
86,Module Security
86,Stories Pane
86,Project Menu
86,Create Deployment Package
86,Deploy to the Cloud
86,Run Menu
86,Edit Cloud Foundry Settings
86,Version Control Menu
86,Commit
86,History
86,Download from Version Control Server
86,Upload to Version Control Server
86,Branch Line Manager
86,Create Branch Line
86,Merge Dialog
86,Language Menu
86,Language Settings
86,Batch Replace
86,Batch Translate
86,Language Operations
86,Domain Model
86,Entities
86,Persistability
86,Attributes
86,Validation Rules
86,Event Handlers
86,Indexes
86,Access Rules
86,External Entities
86,Associations
86,Association Properties
86,Association Tab Properties
86,Querying Over Self-References
86,Annotations
86,Generalization vs 1-to-1 Associations
86,Pages
86,Page
86,Page Properties
86,Page Resources
86,Image Collection
86,Layout
86,Placeholder
86,Header
86,Sidebar Toggle
86,Page Template
86,Snippet
86,Building Block
86,Menu
86,Data Widgets
86,Data View
86,Grids
86,Data Grid
86,Grid Columns
86,Template Grid
86,Control Bar
86,Search Bar
86,Sort Bar
86,List View
86,Data Sources
86,Database Source
86,XPath Source
86,Context Source
86,Microflow Source
86,Nanoflow Source
86,Association Source
86,Listen to Widget Source
86,Common Widgets
86,Text
86,Image
86,Label
86,Snippet Call
86,Page Title
86,Container Widgets
86,Layout Grid
86,Container
86,Group Box
86,Tab Container
86,Scroll Container
86,Table
86,Navigation List
86,Input Widgets
86,Text Box
86,Text Area
86,Drop-Down
86,Check Box
86,Radio Buttons
86,Date Picker
86,Reference Selector
86,Reference Set Selector
86,Input Reference Set Selector
86,File Widgets
86,File Manager
86,Image Uploader
86,Image Viewer
86,Button Widgets
86,Button Properties
86,Menu Widgets
86,Menu Bar
86,Simple Menu Bar
86,Navigation Tree
86,Report Widgets
86,Report Grid
86,Report Parameter
86,Report Date Parameter
86,Date Range Field
86,Generate Report Button
86,Authentication Widgets
86,Login ID Text Box
86,Password Text Box
86,Sign-In Button
86,Validation Message
86,Chart Widgets
86,Chart Configuration
86,Chart Advanced Cheat Sheet
86,Any Chart Widgets
86,Any Chart Building Blocks
86,Any Chart Cheat Sheet
86,Properties Common in the Page Editor
86,On Click Event and Events Section
86,Application Logic
86,Microflows
86,Microflow Properties
86,MxAssist Logic Bot
86,Nanoflows
86,Nanoflow Properties
86,Sequence Flow
86,Activities
86,Object Activities
86,Cast Object
86,Change Object
86,Commit Object(s)
86,Create Object
86,Delete Object(s)
86,Retrieve
86,Rollback Object
86,List Activities
86,Aggregate List
86,Change List
86,Create List
86,List Operation
86,Action Call Activities
86,Java Action Call
86,JavaScript Action Call
86,Microflow Call
86,Variable Activities
86,Change Variable
86,Create Variable
86,Client Activities
86,Call Nanoflow
86,Show Message
86,Close Page
86,Download File
86,Show Home Page
86,Show Page
86,Synchronize to Device
86,Synchronize
86,Validation Feedback
86,Integration Activities
86,Call REST Service
86,Call Web Service
86,Import with Mapping
86,Export With Mapping
86,Log Message
86,Generate Document
86,Decisions
86,Merge
86,Object Type Decision
86,Decision
86,Annotation
86,Parameter
86,Loop
86,Events
86,Start Event
86,End Event
86,Error Event
86,Continue Event
86,Break Event
86,Expressions
86,Unary Expressions
86,Arithmetic Expressions
86,Relational Expressions
86,Special Checks
86,Boolean Expressions
86,If Expressions
86,Mathematical Function Calls
86,String Function Calls
86,Date Creation
86,Between Date Function Calls
86,Add Date Function Calls
86,Trim to Date
86,To String
86,Parse Integer
86,Parse and Format Decimal Function Calls
86,Parse and Format Date Function Calls
86,Enumerations in Expressions
86,Common Properties
86,Resources
86,Java Actions
86,JavaScript Actions
86,Rules
86,Enumerations
86,Datasets
86,OQL
86,OQL Expressions
86,OQL Aggregation
86,OQL Functions
86,OQL CAST
86,OQL COALESCE
86,OQL DATEDIFF
86,OQL DATEPART
86,OQL LENGTH
86,OQL RANGEBEGIN
86,OQL RANGEEND
86,OQL ROUND
86,OQL Operators
86,OQL Case Expression
86,OQL Parameters
86,OQL From Clause
86,OQL Group by Clause
86,OQL Limit Clause
86,OQL Order by Clause
86,OQL Select Clause
86,OQL Where Clause
86,Constants
86,Regular Expressions
86,Scheduled Events
86,Document Templates
86,Creating Your Own Documents
86,Data Grid (Document Template)
86,Columns (Document Template)
86,Data View (Document Template)
86,Document Template
86,Dynamic Image (Document Template)
86,Dynamic Label (Document Template)
86,Footer (Document Template)
86,Header (Document Template)
86,Line Break (Document Template)
86,Page Break (Document Template)
86,Static Image (Document Template)
86,Static Label (Document Template)
86,Style
86,Table (Document Template)
86,Row (Document Template)
86,Cell (Document Template)
86,Template Grid (Document Template)
86,Title (Document Template)
86,Data Types
86,Images
86,XPath
86,XPath Constraints
86,XPath Constraint Functions
86,XPath Contains
86,XPath Day-from-DateTime
86,XPath Day-of-Year-from-DateTime
86,XPath Ends-With
86,XPath False
86,XPath Hours-from-DateTime
86,XPath Length
86,XPath Minutes-from-DateTime
86,XPath Month-From-DateTime
86,XPath Not
86,XPath Quarter-from-DateTime
86,XPath Seconds-from-DateTime
86,XPath Starts-With
86,XPath String-Length
86,XPath True
86,XPath Week-from-DateTime
86,XPath Weekday-from-DateTime
86,XPath Year-from-DateTime
86,XPath Expressions
86,XPath Keywords and System Variables
86,XPath Operators
86,XPath Query Functions
86,XPath Avg
86,XPath Count
86,XPath Max
86,XPath Min
86,XPath Sum
86,XPath Tokens
86,Integration
86,Consumed App Services
86,Select App Service
86,Settings
86,Consumed OData Services
86,Consumed OData Service
86,Consumed OData Service Requirements
86,Consumed REST Services
86,Using a Proxy to Call a REST Service
86,Consumed Web Services
86,Consumed Web Service
86,Numeric Formatting
86,Using a Proxy to Call a Web Service
86,HttpRequest and HttpResponse System Entities
86,JSON Structures
86,Mapping Documents
86,Export Mappings
86,Import Mappings
86,Map Automatically
86,Select Elements
86,XML Inheritance and Choice
86,Message Definitions
86,Published App Services
86,Actions
86,Published App Service
86,Published OData Services
86,OData Query Options
86,OData Representation
86,Published OData Resource
86,Published REST Services
86,Published REST Service
86,Published REST Operation
86,Operation Parameters for Published REST
86,Published REST Path Parameters
86,Published REST Query Parameters
86,Published REST Resource
86,CORS Settings for Published REST Services
86,GitHub-Flavored Markdown
86,Generate a Published REST Resource
86,Publish Microflow as REST Operation
86,Technical Details of Published REST
86,Published REST Request Routing
86,JSON Schema for Published REST Operation
86,OpenAPI 2.0 Documentation
86,Custom Authentication Microflow Parameters
86,Published Web Services
86,Operations
86,Published Web Service
86,XML Schemas
86,XML Schema Support
86,Version Control
86,Using Version Control in Studio Pro
86,Mendix Runtime
86,Runtime Server
86,Mendix Client
86,Runtime Deployment
86,Clustered Mendix Runtime
86,Communication Patterns
86,Data Storage
86,Attribute Type Migration
86,Case-Sensitive Database Behavior
86,Order By Behavior
86,Uniqueness Constraint Migration
86,DB2
86,MySQL/MariaDB
86,Oracle
86,SAP HANA
86,Date and Time Handling
86,DateTime Handling FAQ
86,Logging
86,Login Behavior
86,Mendix Runtime and Java
86,Non-Persistable Objects and Garbage Collecting
86,Java Memory Usage
86,Common Runtime and Java Errors
86,Monitoring Client State
86,Monitoring Mendix Runtime
86,Objects and Caching
86,Runtime Customization
86,Advanced Custom Settings
86,WebSockets
86,Mobile
86,Native Mobile
86,Getting the Make It Native App
86,Native Navigation
86,Native Mobile Styling
86,Native Builder (CLI)
86,Working with Vector Graphics
86,Hybrid Mobile
86,Customizing Hybrid Mobile Apps
86,Developing Hybrid Mobile Apps
86,Getting the Mendix Developer App
86,Offline Hybrid Mobile Apps
86,Packaging Hybrid Mobile Apps
86,Managing App Signing Keys
86,Offline-First
86,Java Programming
86,Troubleshooting
86,Using Eclipse
86,Studio Pro 8 How-tos
86,Collaboration
86,Solve Known Version Control Issues
86,Team Server Network Issues
86,Contribute to a Mendix GitHub Repository
86,Start Your Own GitHub Repository
86,Share the Development Database
86,Translate Your App Content
86,On-Premises Version Control Server
86,Front End
86,Atlas UI
86,Get Started with Atlas UI
86,Migrate Existing Apps to Atlas UI
86,Create Company Atlas UI Resources
86,Share Company Atlas UI Resources
86,Custom Preview Images
86,Customize Your Styling
86,Customize Styling Using Calypso
86,Customize Styling Using Gulp
86,Set Up Gulp and Sass
86,Start Styling with Gulp and Sass
86,Implement Best Practices for UX Design
86,Use Navigation Layouts
86,Configure Your Theme
86,Use the Charts Widgets
86,Create a Basic Chart
86,Use Any Chart
86,Chart Advanced Tuning
86,Use the Charts Theme
86,Create a Dynamic Series Chart
86,Use a Chart with a REST Data Source
86,Plotly Images REST Endpoint
86,Create Overview and Detail Pages
86,Use Layouts and Snippets
86,Implement Classes
86,Create Custom Error Pages
86,Style Google Maps
86,Mobile
86,Native Mobile
86,Get Started with Native Mobile
86,Build Native Apps
86,Deploy Mendix Native Mobile App
86,Build Local Native Mobile App
86,Local Native Mobile App Manual Build
86,Debug Native Mobile Apps (Advanced)
86,Create a Custom Developer App
86,Build Apps Using Native Builder CLI
86,Deploy Mobile App with Native Builder CLI
86,Custom Developer App with Native Builder CLI
86,Over the Air Updates with CodePush and CLI
86,Implement Native Mobile Styling
86,Style Your Mendix Native Mobile App
86,Native Mobile App UI Best Practices
86,Add Fonts to Your Native Mobile App
86,Use Notifications
86,Add Module Dependencies
86,Push Notifications Module
86,Set Up Firebase Cloud Messaging
86,Configure Push Notifications
86,Push Notifications in Native App
86,Native App with Push Notifications
86,Send Your First Test Push Notification
86,Send Notifications to Multiple Devices
86,Use Local Notifications
86,Part 1: Local Notifications
86,Part 2: Badges
86,Part 3: Actions
86,Part 4: Data
86,Part 5: Scheduling
86,Over the Air Updates with CodePush
86,Deep Links in Native Mobile Apps
86,Set Up Maps in Native Mobile Apps
86,Troubleshoot Common Native Mobile Issues
86,Hybrid Mobile
86,Build Hybrid Apps
86,Build a Mendix Hybrid App Locally
86,Publish Hybrid Mobile App in App Stores
86,Customizing Local Build Packages
86,Set Up Hybrid Push Notifications
86,Include Push Notifications
86,Implement Push Notifications
86,Send Push Notifications
86,Apple Push Notification Server
86,Test Push Notifications
86,Configure iOS Mendix Feedback Widget
86,SSO on Hybrid App with SAML
86,Debug a Hybrid Mobile App
86,Deploy Your First Hybrid Mobile App
86,Data Models
86,Create a Basic Data Layer
86,Set Up Data Validation
86,Work with Images and Files
86,Denormalize Data to Improve Performance
86,Migrate Your Mendix Database
86,Logic and Business Rules
86,Trigger a Microflow From a Menu Item
86,Create a Custom Save Button
86,Extract and Use Sub-Microflows
86,Work with Lists in a Microflow
86,Optimize Microflow Aggregates
86,Set Up Error Handling
86,Optimize Retrieve Activities
86,Define Access Rules Using XPath
86,Configure String Concatenation
86,Extend App with Custom Java
86,Use the Java API
86,Use Translatable Validation Messages
86,Filter Data Using XPath
86,Server-Side Paging and Sorting
86,Integration
86,Import and Export Objects
86,Import XML Documents
86,Export XML Documents
86,Import Excel Documents
86,Import a Large Excel File
86,Export to Excel
86,Consume a Simple Web Service
86,Consume a Complex Web Service
86,Consume a REST Service
86,Publish a REST Service
86,Access a Samba Share
86,Version a REST Service
86,Expose a Web Service
86,Expose Data to BI Tools Using OData
86,Publish Data to Other Mendix Apps Using an App Service (Deprecated)
86,Configure Selenium Support
86,Execute SQL on External Database
86,Test Web Services Using SoapUI
86,Implement CI/CD Pipeline
86,Use a Client Certificate
86,Extensibility
86,Build a Pluggable Native Widget
86,Build Pluggable Web Widgets
86,1. Build Pluggable Web Widget
86,2. Build Pluggable Web Widget
86,Build Custom Widgets
86,Build Widgets with XML
86,Preview Image for Custom Widget
86,Build JavaScript Actions
86,1. Build JavaScript Actions
86,2. Build JavaScript Actions
86,Build JavaScript Actions for Native Mobile
86,JavaScript Actions Best Practices
86,Microflow Actions Using Connector Kit
86,Data Storage APIs for Reusable Microflows
86,Security
86,Create a Secure App
86,Best Practices for App Security
86,Set Up Anonymous User Security
86,Testing
86,Test Microflows Using Unit Test Module
86,Test with ATS
86,Test Mendix Apps Using Selenium IDE
86,Create Automated Tests with TestNG
86,Monitoring and Troubleshooting
86,Clear Warning Messages
86,Debug Microflows
86,Debug Microflows Remotely
86,Debug Java Actions
86,Debug Java Actions Remotely
86,Debug a Hybrid Mobile Application
86,Find the Root Cause of Runtime Errors
86,Set Log Levels
86,Monitor Mendix Using JMX
86,Solve Load and Import Errors
86,Manage App Performance
86,Manage App Performance with New Relic
86,Detect and Resolve Performance Issues
86,General Info
86,Configure Parallels
86,Set Up the Navigation Structure
86,Minimize Objects in Session
86,Best Practices for Development
86,Best Practices for App Performance
86,Install Mendix Studio Pro
86,Mendix 7 Reference Guide
86,General
86,System Requirements
86,Moving from Modeler Version 6 to 7
86,Offline
86,MxBuild
86,Developer Tool Recommendations
86,Third-Party Licenses
86,Desktop Modeler
86,Desktop Modeler Overview
86,Application Logic
86,Microflows
86,Microflow Properties
86,Nanoflows
86,Nanoflow Properties
86,Rules
86,Common Elements
86,Activities
86,Action Call Activities
86,Java Action Call
86,Microflow Call
86,Client Activities
86,Close Page
86,Download File
86,Show Home Page
86,Show Message
86,Show Page
86,Validation Feedback
86,Document Generation Activities
86,Generate Document
86,Integration Activities
86,Call Web Service
86,Export XML
86,Import XML
86,List Activities
86,Aggregate List
86,Change List
86,Create List
86,List Operation
86,Logging Activities
86,Log Message
86,Object Activities
86,Cast Object
86,Change Object
86,Commit Object(s)
86,Create Object
86,Delete Object(s)
86,Retrieve
86,Rollback Object
86,Variable Activities
86,Change Variable
86,Create Variable
86,Annotation
86,Annotation Flow
86,Events
86,Break Event
86,Continue Event
86,End Event
86,Error Event
86,Start Event
86,Expressions
86,Add date function calls
86,Arithmetic expressions
86,Between Date Function Calls
86,Boolean expressions
86,Date Creation
86,Enumerations in Expressions
86,If expressions
86,Mathematical function calls
86,Parse and Format Date Function Calls
86,Parse and Format Decimal Function Calls
86,Parse and Format Float Function Calls
86,Parse integer
86,Relational expressions
86,Special checks
86,String Function Calls
86,To float
86,To string
86,Trim to Date
86,Unary expressions
86,Loop
86,Microflow Element Common Properties
86,Parameter
86,Sequence Flow
86,Splits
86,Exclusive Split
86,Inheritance Split
86,Merge
86,Consistency Errors
86,Navigation Consistency Errors
86,Page Editor Consistency Errors
86,Constants
86,Data Types
86,Datasets
86,OQL
86,OQL Expressions
86,OQL Aggregation
86,OQL Functions
86,OQL CAST
86,OQL COALESCE
86,OQL DATEDIFF
86,OQL DATEPART
86,OQL LENGTH
86,OQL RANGEBEGIN
86,OQL RANGEEND
86,OQL ROUND
86,OQL Operators
86,OQL Case Expression
86,OQL Parameters
86,OQL From Clause
86,OQL Group by Clause
86,OQL Limit Clause
86,OQL Order by Clause
86,OQL Select Clause
86,OQL Where Clause
86,Dialog Boxes
86,App Settings Dialog
86,Branch Line Manager Dialog
86,Commit Dialog
86,Create Branch Line Dialog
86,Create Deployment Package Dialog
86,Deploy To The Cloud Dialog
86,Download From Version Control Server Dialog
86,Edit Cloud Foundry Settings Dialog
86,Export an App Package
86,History Dialog
86,Import Project Package
86,Merge Dialog
86,Open App Dialog
86,Preferences Dialog
86,Sign In Dialog
86,Upload To Version Control Server
86,Document Templates
86,Creating Your Own Documents
86,Data Grid (Document Template)
86,Columns (Document Template)
86,Data View (Document Template)
86,Document Template
86,Dynamic Image (Document Template)
86,Dynamic Label (Document Template)
86,Footer (Document Template)
86,Header (Document Template)
86,Line break (Document Template)
86,Page Break (Document Template)
86,Static Image (Document Template)
86,Static Label (Document Template)
86,Style
86,Table (Document Template)
86,Row (Document Template)
86,Cell (Document Template)
86,Template Grid (Document Template)
86,Title (Document Template)
86,Domain Model
86,Annotations
86,Associations and Their Properties
86,Entities
86,Generalization and 1-to-1 Associations
86,Persistability
86,Attributes
86,Associations
86,Validation Rules
86,Event Handlers
86,Indexes
86,Access Rules
86,Enumerations
86,Enumeration Values
86,Images
86,Integration
86,Consumed App Services
86,Select app service
86,Settings
86,Consumed REST Services
86,Using a Proxy to Call a REST Service
86,Consumed Web Services
86,Consumed Web Service
86,Numeric formatting
86,Using a proxy to call a webservice
86,HttpRequest and HttpResponse System Entities
86,JSON Structures
86,Mapping Documents
86,Export Mappings
86,Import Mappings
86,Map Automatically
86,Select Elements
86,XML Inheritance and Choice
86,Message Definitions
86,Message Definition
86,Microflow Activities
86,Call REST Service Action
86,Call Web Service Action
86,Export Mapping Action
86,Import Mapping Action
86,Published App Services
86,Actions
86,Published App Service
86,Published OData Services
86,OData Query Options
86,OData Representation
86,Published OData Resource
86,Published REST Services
86,Published REST Service
86,Published REST Operation
86,Operation Parameters for Published REST
86,Published REST Path Parameters
86,Published REST Query Parameters
86,Published REST Resource
86,CORS Settings for Published REST Services
86,GitHub-Flavored Markdown
86,Generate a Published REST Resource
86,Publish Microflow as REST Operation
86,Technical Details of Published REST
86,Published REST Request Routing
86,JSON Schema for Published REST Operation
86,OpenAPI 2.0 Documentation
86,Custom Authentication Microflow Parameters
86,Published Web Services
86,Operations
86,Published Web Service
86,XML Schemas
86,XML Schema Support
86,Java Actions
86,Modules
86,Module Security
86,Module Role
86,Pages
86,Page Concepts
86,Conditions
86,Data Sources
86,Association Source
86,Context Source
86,Database Source
86,Listen To Widget Source
86,Microflow Source
86,XPath Source
86,On Click Event
86,Opening Pages
86,Starting Microflows
86,Authentication Widgets
86,Login Id Text Box
86,Password Text Box
86,Sign In Button
86,Validation Message
86,Building Block
86,Button Widgets
86,Action Button
86,Close Page Button
86,Create Button
86,Drop-Down Button
86,Image Property
86,Chart Widgets
86,Chart Configuration
86,Chart Advanced Cheat Sheet
86,Any Chart Widgets
86,Any Chart Building Blocks
86,Any Chart Cheat Sheet
86,Common Widgets
86,Common Widget Properties
86,Image
86,Label
86,Page title
86,Snippet Call
86,Text
86,Container Widgets
86,Container
86,Group box
86,Layout grid
86,Navigation list
86,Scroll Container
86,Scroll Container Region
86,Tab container
86,Tab page
86,Table
86,Table cell
86,Table row
86,Data Widgets
86,Data grid
86,Columns
86,Control Bar
86,Add Button
86,Delete button
86,Deselect All Button
86,Edit button
86,Export to CSV button
86,Export to excel button
86,Grid Action button
86,Grid Create Button
86,Remove button
86,Search button
86,Select All Button
86,Select button
86,Search Bar
86,Comparison Search Field
86,Drop-Down Search Field
86,Range Search Field
86,Sort Bar
86,Data view
86,List view
86,Template Grid
86,File Widgets
86,File manager
86,Image uploader
86,Image viewer
86,Input Widgets
86,Check Box
86,Date Picker
86,Drop-Down
86,Input Reference Set Selector
86,Radio Buttons
86,Reference Selector
86,Reference Set Selector
86,Text Area
86,Text Box
86,Layout Widgets
86,Header
86,Placeholder
86,Sidebar toggle button
86,Layouts
86,Menu
86,Menu Item
86,Menu Widgets
86,Menu Bar
86,Navigation Tree
86,Simple Menu Bar
86,Page
86,Page Templates
86,Report Widgets
86,Date Range Field
86,Report Button
86,Report Chart
86,Report Date Parameter
86,Report Grid
86,Report Parameter
86,Snippet
86,Projects
86,Converting to 7.4 - Navigation Profile Issues
86,Navigation Before Mendix 7.2
86,Desktop Profile
86,Hybrid Phone Profile
86,Hybrid Tablet Profile
86,Offline Device Profile
86,Phone Profile
86,Tablet Profile
86,Navigation in 7.2 and 7.3
86,Navigation Profile in 7.2 and 7.3
86,Navigation in Mendix 7.4 and Above
86,Navigation Profile
86,Project Security
86,Administrator
86,Anonymous Users
86,Demo Users
86,Module Status
86,Password Policy
86,User Roles
86,Project Settings
86,Configuration
86,System Texts
86,Regular Expressions
86,Scheduled Events
86,Security
86,Translatable Texts
86,UI Resources Package
86,XPath
86,XPath Constraints
86,XPath Constraint Functions
86,XPath contains
86,XPath day-from-dateTime
86,XPath day-of-year-from-dateTime
86,XPath ends-with
86,XPath false
86,XPath hours-from-dateTime
86,XPath length
86,XPath minutes-from-dateTime
86,XPath month-from-dateTime
86,XPath not
86,XPath quarter-from-dateTime
86,XPath seconds-from-dateTime
86,XPath starts-with
86,XPath string-length
86,XPath true
86,XPath week-from-dateTime
86,XPath weekday-from-dateTime
86,XPath year-from-dateTime
86,XPath Expressions
86,XPath Keywords and System Variables
86,XPath Operators
86,XPath Query Functions
86,XPath avg
86,XPath count
86,XPath id
86,XPath max
86,XPath min
86,XPath sum
86,XPath Tokens
86,Version Control
86,Version Control
86,Team Server
86,Team Server FAQ
86,Collaborative Development
86,Mendix Runtime
86,Clustered Mendix Runtime
86,Data Storage
86,Attributes Type Migration
86,Order By Behavior
86,Uniqueness Constraint Migration
86,DB2
86,MySQL/MariaDB
86,Oracle
86,SAP HANA
86,Date and Time Handling
86,DateTime Handling FAQ
86,Logging
86,Login Behavior
86,Mendix Runtime and Java
86,Transient Objects and Garbage Collecting
86,Java Memory Usage
86,Common Runtime and Java Errors
86,Monitoring client state
86,Monitoring Mendix Runtime
86,Objects and Caching
86,Runtime Customization
86,Advanced Custom Settings
86,SIG–Mendix Performance Subjects
86,Java Programming
86,Troubleshooting
86,Using Eclipse
86,Mobile Development
86,Customizing Hybrid Mobile Apps
86,Customizing Local Build Packages
86,Developing Hybrid Mobile Apps
86,Getting the Mendix Developer App
86,Managing App Signing Keys
86,Offline Hybrid Mobile Apps
86,Packaging Hybrid Mobile Apps
86,Mendix 7 How-tos
86,General
86,Install the Mendix Desktop Modeler
86,Show the Project Directory in Explorer
86,Best Practices for Development
86,Best Practices for App Performance
86,Find Your Way in an App
86,Find Unused App Items
86,Minimize Objects in Session
86,Set Up the Navigation Structure
86,Front End
86,Atlas UI
86,Get Started with Atlas UI
86,Migrate Existing Apps to Atlas UI
86,Create Company Atlas UI Resources
86,Share Company Atlas UI Resources
86,Custom Preview Images
86,Implement Best Practices for UX Design
86,Configure Your Theme
86,Create Overview and Detail Pages
86,Use Layouts and Snippets
86,Implement Classes
86,Use Gulp and Sass
86,Set Up Gulp and Sass
86,Start Styling with Gulp
86,Sass
86,Create Custom Error Pages
86,Style Google Maps
86,Data Models
86,Create a Basic Data Layer
86,Set Up Data Validation
86,Work with Object Events
86,Work with Images and Files
86,Query Over Self-References
86,Denormalize Data to Improve Performance
86,Migrate Your Mendix Database
86,Logic and Business Rules
86,Your First Microflow
86,Trigger Logic Using Microflows
86,Create a Custom Save Button
86,Drag App Documents into Microflow
86,Extract and Use Sub-Microflows
86,Work with Lists in a Microflow
86,Optimize Microflow Aggregates
86,Set Up Error Handling
86,Optimize Retrieve Activities
86,Define Access Rules Using XPath
86,Configure String Concatenation
86,Use the Java API
86,Find Object Activities
86,Use Translatable Validation Messages
86,Filter Data Using XPath
86,Mobile Development
86,Include Push Notifications
86,Implement Push Notifications
86,Send Push Notifications
86,Apple Push Notification Server
86,Set Up Firebase Cloud Messaging
86,Test the Implementation
86,Configure iOS Mendix Feedback Widget
86,SSO on Hybrid App with SAML
86,Debug a Hybrid Mobile App
86,Deploy Your First Hybrid Mobile App
86,Publish Hybrid App in App Stores
86,Security
86,Create a Secure App
86,Best Practices for App Security
86,Set Up Anonymous User Security
86,Integration
86,Integrate Legacy System
86,Import and Export Objects
86,Import XML Documents
86,Export XML Documents
86,Import Excel Documents
86,Import a Large Excel File
86,Consume a Simple Web Service
86,Consume a Complex Web Service
86,Consume a REST Service
86,Publish a REST Service
86,Version a REST Service
86,Expose a Web Service
86,Publish Data Using App Service
86,Configure Selenium Support
86,Execute SQL on External Database
86,Test Web Services Using SoapUI
86,Implement CI/CD Pipeline
86,Use a Client Certificate
86,Extensibility
86,Microflow Actions Using Connector Kit
86,Data Storage APIs for Reusable Microflows
86,Access a Samba Share
86,Use the Charts Widgets
86,Create a Basic Chart
86,Use Any Chart
86,Chart Advanced Tuning
86,Use the Charts Theme
86,Create a Dynamic Series Chart
86,Use a Chart with a REST Data Source
86,Plotly Images REST Endpoint
86,Widget Development
86,Adobe Brackets Widget Development Plugin
86,Preview Image for Custom Widget
86,Scaffold Widget with Yeoman
86,Use XML in Widget Development
86,Testing
86,Test Microflows Using Unit Test Module
86,Test with ATS
86,Test Apps Using Selenium IDE
86,Create Automated Tests with TestNG
86,Monitoring and Troubleshooting
86,Clear Warning Messages
86,Debug Microflows
86,Debug Microflows Remotely
86,Handle Common Mendix SSO Errors
86,Debug Java Actions
86,Debug Java Actions Remotely
86,Find the Root Cause of Runtime Errors
86,Set Log Levels
86,Monitor Mendix Using JMX
86,Solve Load and Import Errors
86,New Relic App Performance
86,Detect and Resolve Performance Issues
86,Collaboration and Requirements Management
86,Solving Known Version Control Issues
86,Team Server Network Issues
86,Contribute to a GitHub Repository
86,Start Your Own Repository
86,Share the Development Database
86,Translate Your App Content
86,On-Premises Version Control Server
86,OData Query Options
86,1 Introduction
86,2 Retrieving Objects
86,2.1 Retrieving All Objects
86,2.2 Retrieving a Single Object
86,3 Counting the Number of Objects
86,3.1 Retrieving a Count of Objects
86,3.2 Inline Count
86,4 Filtering
86,4.1 Passing attributes
86,4.2 Comparison Operators
86,4.3 Functions
86,4.4 Combining Filters
86,4.5 Arithmetic Operators
86,5 Sorting
86,6 Selecting fields
86,7 Paging
86,7.1 Top (Limit)
86,7.2 Skip (Offset)
86,8 Null Literals
86,Docs
86,Mendix 7 Reference Guide
86,Desktop Modeler
86,Integration
86,Published OData Services
86,OData Query Options
86,"Mendix 7 is no longer supported unless you have Extended Support (for details, please contact Mendix Support). Mendix 7 documentation will remain available for customers with Extended Support until July, 2024."
86,OData Query Options
86,"Last modified: June 14, 2023"
86,1 Introduction
86,This is a list of query options for OData.
86,We currently only support the options described here.
86,2 Retrieving Objects
86,2.1 Retrieving All Objects
86,All objects can be retrieved by specifying the URI. For example: /odata/myservice/myresource. You can see this if you specify the URI in a browser.
86,2.2 Retrieving a Single Object
86,A single object can be retrieved by passing the object identifier in the URI. For example: /odata/myservice/myresource(8444249301330581).
86,3 Counting the Number of Objects
86,3.1 Retrieving a Count of Objects
86,"You can find out how many objects there are by passing the $count query option. In this case, the result is an integer which is the number of objects. For example: /odata/myservice/myresource/$count."
86,3.2 Inline Count
86,"By setting the $inlinecount query option to ‘allpages’, a count of the number of items returned will be included in the result. For example: ?$inlinecount=allpages."
86,4 Filtering
86,Filters are applied by appending a $filter=... parameter to the request. For example: /Employees?$filter=Name eq 'John'.
86,4.1 Passing attributes
86,This table describes how to pass values for different attribute types:
86,Type
86,How to Pass
86,String and Enumeration
86,"Enclosed in single quotes (for example, ‘John’)"
86,Datetime
86,"Preceded with datetime and enclosed in single quotes (for example, datetime'2015-01-01’ or datetime’<epoch value here>’)"
86,Other
86,"Plain value (for example, 15)"
86,4.2 Comparison Operators
86,We support the following comparison operators:
86,Operator
86,Meaning
86,Example
86,equals
86,/Employees?$filter=Name eq 'John'
86,does not equal
86,/Employees?$filter=Name ne 'John'
86,greater than
86,/Employees?$filter=Age gt 15
86,less than
86,/Employees?$filter=Age lt 15
86,greater than or equal to
86,/Employees?$filter=Age ge 15
86,less than or equal to
86,/Employees?$filter=Age le 15
86,4.3 Functions
86,Function
86,Example
86,Returns
86,substringof
86,"/Employees?$filter=substringof('f', Name)"
86,All employees with names that contain an ‘f’
86,endswith
86,"/Employees?$filter=endswith(Name, 'f')"
86,All employees with names that end with ‘f’
86,startswith
86,"/Employees?$filter=startswith(Name, 'f')"
86,All employees with names that start with ‘f’
86,length
86,/Employees?$filter=length(Name) eq 5
86,All employees with names that have a length of 5
86,year
86,/Employees?$filter=year(DateOfBirth) eq 1990
86,All employees born in the year 1990
86,month
86,/Employees?$filter=month(DateOfBirth) eq 5
86,All employees born in May
86,day
86,/Employees?$filter=day(DateOfBirth) eq 31
86,All employees born on the 31st day of the month
86,hour
86,/Employees?$filter=hour(Registration) eq 13
86,All employees registered between 13:00 (1 PM) and 13:59 (1:59 PM)
86,minute
86,/Employees?$filter=minute(Registration) eq 55
86,All employees registered on the 55th minute of any hour
86,second
86,/Employees?$filter=second(Registration) eq 55
86,All employees registered on the 55th second of any minute of any hour
86,4.4 Combining Filters
86,"Filters can be combined with and, or, not, and (). For example: ?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)."
86,Combination
86,Example
86,and
86,/Employees?$filter=Name eq 'John' and Age gt 65
86,/Employees?$filter=Age gt 65 or Age lt 11
86,not
86,/Employees?$filter=not(Name eq 'John')
86,( )
86,/Employees?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)
86,4.5 Arithmetic Operators
86,"The use of arithmetic operators such as add, sub, mul, div, and mod in filter expressions is not supported."
86,5 Sorting
86,You can sort the result using the $orderby query option. For example: ?$orderby=Name.
86,"The default direction is ascending, and you can make this explicit. For example: ?$orderby=Name asc."
86,You can also order the result in a descending direction. For example: ?$orderby=Name desc.
86,"It is possible to sort on multiple attributes, which have to be comma-separated. For example: ?$orderby=Name, Age desc."
86,6 Selecting fields
86,"You can select which attributes and associations to return by specifying the $select query option. For example: ?$select=Name,Age."
86,7 Paging
86,7.1 Top (Limit)
86,"You can limit the number of returned objects using the $top query option, where the limit is a positive integer. For example: ?$top=100."
86,7.2 Skip (Offset)
86,"You can skip a number of objects before retrieving the result using the $skip query option, where the offset is a positive integer. For example: ?$skip=100 will return objects starting with the 101st object in the list."
86,8 Null Literals
86,You can compare values against the null literal. For example: ?$filter=Name eq null.
86,"In this example, Name is a string attribute that can have no assigned value in the database. Note that null means no value as opposed to '' (which is an empty string)."
86,"When you filter against associations, null literals can be quite useful. For example: ?$filter=Association_A_B ne null. In this example, you query for objects of entity type A that have at least one association set to objects of entity type B."
86,Documentation licensed under CC BY 4.0
86,© Mendix Technology BV 2024. All rights reserved
86,Mendix.com
86,Terms of Use
86,Privacy Policy
86,EU Digital Services Act Notice
87,How to Update Multiple Columns in SQL: Efficient Techniques and Tips - SQL Knowledge Center
87,SQL
87,SQL Server
87,SQLite
87,PostgreSQL
87,MySQL
87,T-SQL
87,Tools
87,About
87,Learn SQL for Free
87,How to Update Multiple Columns in SQL: Efficient Techniques and Tips
87,By Cristian G. Guasch • Updated: 06/28/23 • 19 min read
87,"Updating multiple columns in SQL is a crucial skill for database management, particularly when dealing with large amounts of data. By learning how to update multiple columns at once, one can save time, enhance efficiency, and maintain data integrity. In this article, we’ll dive into the process of updating multiple columns in SQL, covering the syntax and techniques in a clear and concise manner."
87,Chat with SQL Databases using AI
87,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
87,Start Free Trial
87,"Well-structured SQL queries are essential in not only updating multiple columns but also in ensuring data accuracy. Familiarity with the UPDATE statement and the SET clause can significantly improve your proficiency in handling SQL queries. With the right syntax, it’s possible to update specific records, apply functions or calculations, or even incorporate data from other tables."
87,"In the following sections, we’ll walk through practical examples and scenarios to demonstrate how to update multiple columns in SQL effectively. By mastering these techniques, you can confidently tackle complex database manipulation tasks while maintaining the integrity of your data."
87,Understanding SQL Basics
87,"SQL, or Structured Query Language, is a widely used language for managing relational databases. It allows users to create, read, update, and delete data (known as CRUD operations) within these databases. One common task is updating data in database columns. This section will help readers understand the basics of updating multiple columns in SQL."
87,"To update multiple columns in SQL, one will need a strong grasp of CRUD operations, particularly the “update” command. This operation ensures that data in a database remains accurate and current. Updating a single column in SQL is fairly straightforward, but sometimes, it’s necessary to update multiple columns simultaneously."
87,"In a typical database, information is organized into tables with rows and columns. Each row represents a record, while columns store specific attributes for each record. Here are the primary steps required for updating multiple columns:"
87,Identify the table to be updated.
87,Specify the new data values for each targeted column.
87,"Apply a condition, if necessary, to fine-tune the affected records."
87,The basic syntax for the UPDATE command is as follows:
87,UPDATE table_name
87,"SET column1 = value1, column2 = value2, ..."
87,WHERE condition;
87,Some key elements to remember when updating multiple columns in SQL include:
87,The UPDATE keyword specifies the table to be updated.
87,"The SET keyword sets new values for multiple columns, followed by the = operator and the new value."
87,Each column-value pair must be separated by commas.
87,"The WHERE keyword is optional, but helps narrow down the records that will be updated by applying specific conditions."
87,Consider the following example to better understand the process of updating multiple columns in SQL:
87,<table><tr><th>ID</th><th>First_Name</th><th>Last_Name</th><th>Age</th></tr><tr><td>1</td><td>John</td><td>Doe</td><td>30</td></tr><tr><td>2</td><td>Jane</td><td>Doe</td><td>25</td></tr></table>
87,"To update both the First_name and Age columns, the SQL command would look like this:"
87,UPDATE users
87,"SET First_Name = 'Jonathan', Age = 31"
87,WHERE ID = 1;
87,"After executing the command, the table would look like this:"
87,<table><tr><th>ID</th><th>First_Name</th><th>Last_Name</th><th>Age</th></tr><tr><td>1</td><td>Jonathan</td><td>Doe</td><td>31</td></tr><tr><td>2</td><td>Jane</td><td>Doe</td><td>25</td></tr></table>
87,Grasping these basics of SQL is essential for effectively updating multiple columns and accomplishing various database-related tasks.
87,Multiple Column Update Syntax
87,"Enabling your ability to update multiple columns in SQL can significantly improve efficiency and organization within your database. The process is quite simple, and understanding its syntax is crucial when working with multiple columns. This section will delve into the fundamentals, starting with the basic SQL update statement and progressing to multi-column updates."
87,"To update a single column in a table, you’d typically use the SQL UPDATE statement like this:"
87,UPDATE table_name
87,SET column1 = value1
87,WHERE condition;
87,"However, when it comes to updating multiple columns, the syntax changes slightly. You’ll need to set each column to its respective value, separated by commas:"
87,UPDATE table_name
87,"SET column1 = value1,"
87,"column2 = value2,"
87,...
87,columnN = valueN
87,WHERE condition;
87,"Notice that the column-value pairs are separated by commas, ensuring that the database system understands which values correspond to which columns."
87,"To make the process more comprehensive, here’s a breakdown of the multiple column update syntax components:"
87,UPDATE: This is the keyword that initiates the update process in your SQL query.
87,table_name: Replace this with the name of the table you wish to perform the update on.
87,SET: This keyword indicates that column values will be updated to new specified values.
87,"column1 = value1, column2 = value2, ..., columnN = valueN: The list of column-value pairs to update. Remember to separate each pair with a comma."
87,WHERE: This keyword is followed by a condition that must be satisfied for the update process to take place.
87,"To further clarify the concept, let’s take a look at an example. Suppose you have a table called employees with columns for first_name, last_name, and email. If you want to update the email address and last name of an employee with an employee_id of 101, your SQL query might look something like this:"
87,UPDATE employees
87,"SET last_name = 'Doe',"
87,email = 'johndoe@example.com'
87,WHERE employee_id = 101;
87,"This update query modifies both the last_name and email columns in one single statement, demonstrating the effectiveness of the multiple column update syntax in SQL."
87,Chat with SQL Databases using AI
87,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
87,Start Free Trial
87,Using the SET Clause Effectively
87,"When working with update multiple columns SQL, it’s crucial to use the SET clause effectively for achieving the desired results. The SET clause helps in updating more than one column at a time, making it an efficient tool for database management."
87,A typical scenario in which one might need to update multiple columns includes changes in product prices and stock levels. Consider the following example:
87,UPDATE products
87,"SET price = price * 1.1, stock_level = stock_level - 5"
87,WHERE product_id = 101;
87,"In this example, the price for product 101 gets increased by 10% and the stock level is reduced by 5. The SET clause works simultaneously on both price and stock_level columns, simplifying the overall task."
87,Here are some useful tips when working with the SET clause to update multiple columns in SQL:
87,"Use single quotes around strings and date values, as some databases require them. For example: UPDATE employees SET first_name = 'John', hiring_date = '2021-10-01' WHERE employee_id = 1;"
87,"Combine the SET clause with the WHERE clause to update specific records. This way, one can avoid unintended modifications on other rows."
87,"Use mathematical expressions to update columns with numbers, like the earlier example with price and stock level adjustments."
87,"To update multiple columns based on values from other columns, use the subquery method. For example: UPDATE employees e SET (e.salary, e.bonus) = ( SELECT d.salary, d.bonus FROM departments d WHERE d.department_id = e.department_id ); In this example, the employee’s salary and bonus columns get updated with the values from the corresponding department."
87,"When attempting to update multiple columns SQL, it’s essential to understand the importance of the SET clause. Implementing these tips can assist in making the most out of this powerful SQL feature and maintaining the integrity of the database."
87,Joining Tables During an Update
87,"Joining tables during an update becomes necessary when you need to update multiple columns in SQL by referencing data from other tables. In this section, we’ll explore the process of updating multiple columns in SQL by joining tables. We’ll also discuss the benefits of using joined tables during an update operation."
87,"When updating multiple columns, SQL provides the ability to join tables so that data from one table can easily be used to update data in another table. The process usually involves three key steps: specifying the tables to join, defining the join condition, and setting the new values for the columns."
87,"To start with, the user should specify the tables to join using the FROM clause. This tells the database which tables will be involved in the update. The next step is defining the join condition or criteria by which the tables will be matched. This is often done using the ON keyword in conjunction with a common column or key between the two tables. Lastly, the user sets the new values for the columns to be updated using the SET clause, which may involve referencing one or more columns from the joined table."
87,Here’s an example of how to update multiple columns in SQL by joining tables:
87,UPDATE table1
87,"SET table1.column1 = table2.columnA,"
87,table1.column2 = table2.columnB
87,FROM table1
87,JOIN table2 ON table1.column_key = table2.column_key;
87,In this example:
87,table1 and table2 are the tables being joined.
87,column_key is the common column used to join the tables.
87,column1 and column2 in table1 are being updated with corresponding values from columnA and columnB in table2.
87,"There are several benefits to updating multiple columns in SQL using joined tables, such as:"
87,"Efficiency: Joining tables during an update can result in a more efficient query, as the database is able to perform the operation in a single pass."
87,"Data consistency: By updating multiple columns in a single query, the risk of inconsistencies between the columns is reduced."
87,"Readability: A joined update statement can be easier to read and understand, as it succinctly expresses the relationships between the columns and tables involved."
87,"In conclusion, joining tables during an update is an essential technique when updating multiple columns in SQL. It allows for greater efficiency, data consistency, and readability when working with multiple tables and columns. By understanding how to join tables and use the appropriate keywords, users can effectively update multiple columns in SQL to ensure their data remains accurate and up-to-date."
87,Working with Coalesce and Nulls
87,"When updating multiple columns in SQL, it’s essential to understand how to work with Coalesce and Null values. These concepts are important when dealing with data that may have missing or incomplete information."
87,Coalesce is a handy SQL function that returns the first non-null value in a list of expressions. It’s particularly useful when updating multiple columns where some values may be missing or undefined. Here’s a simple example of using Coalesce in an update statement:
87,UPDATE employees
87,"SET salary = COALESCE(salary, 0),"
87,"bonus = COALESCE(bonus, 0);"
87,"In the example above, if the employee’s salary or bonus is null, it will be updated to 0. Using Coalesce ensures data consistency and prevents errors when performing calculations on these columns."
87,Working with null values is also crucial when updating multiple columns in SQL. A null value indicates that the data in a specific column does not exist or is unknown. It’s important to handle null values properly to avoid data discrepancies and maintain data integrity. Here are some common techniques to handle null values when updating multiple columns:
87,"Use the IS NULL condition: When updating columns where null values should be preserved or replaced, use the IS NULL condition in the WHERE clause. For example:"
87,UPDATE products
87,SET price = price * 1.1
87,WHERE price IS NOT NULL;
87,"Use the NULLIF function: The NULLIF function compares two expressions and returns null if they are equal; otherwise, it returns the first expression. This can be helpful when updating columns to avoid overwriting valuable data. Consider the following example:"
87,UPDATE orders
87,"SET discount = NULLIF(discount, 0);"
87,"In this case, the discount column will be updated to null if it’s currently 0, effectively removing any 0-value discounts."
87,"Combining Coalesce with other functions: It’s possible to use Coalesce along with other SQL functions to create more complex update statements. For instance, when updating multiple columns and calculating averages, you can use AVG and Coalesce together:"
87,UPDATE departments
87,"SET avg_salary = COALESCE(AVG(salary), 0)"
87,FROM employees
87,WHERE departments.id = employees.department_id;
87,"In summary, understanding and properly using Coalesce and null values is vital when updating multiple columns in SQL. It ensures data consistency and improves overall data integrity in the database."
87,Conditional Updates with CASE Statements
87,"When working with SQL databases, it’s often necessary to update multiple columns at once. The UPDATE statement is commonly used for this purpose, and when paired with the versatile CASE statement, you can make conditional updates based on specified criteria."
87,"For those looking to update multiple columns in SQL with conditional data, here’s an example of how the UPDATE and CASE statements can be combined:"
87,UPDATE employees
87,SET
87,salary = CASE
87,WHEN position = 'Manager' THEN salary * 1.10
87,WHEN position = 'Employee' THEN salary * 1.05
87,ELSE salary
87,"END,"
87,bonus_points = CASE
87,WHEN position = 'Manager' THEN bonus_points + 100
87,WHEN position = 'Employee' THEN bonus_points + 50
87,ELSE bonus_points
87,END
87,WHERE department_id = 1;
87,"In this example, the following changes are made based on the employee’s position:"
87,"For managers, their salary is increased by 10% and their bonus points by 100."
87,"For regular employees, the salary is raised by 5% and 50 bonus points are added."
87,Some essential points to remember when using the UPDATE statement with CASE expressions include:
87,You can specify multiple updates with comma-separated statements within the SET clause.
87,The WHEN keyword is used to define specific conditions on which the update will occur.
87,The THEN keyword denotes the new value for the specific column after the update.
87,"In case none of the conditions match, the ELSE keyword allows specifying a default value to be assigned."
87,"One of the benefits of the CASE statement is its flexibility, allowing you to handle more complex update scenarios. You can even nest CASE statements to create further subdivisions within the data. For instance:"
87,UPDATE orders
87,SET
87,shipping_cost = CASE
87,WHEN origin_country = 'US' THEN
87,CASE
87,WHEN destination_country = 'US' THEN 5
87,ELSE 15
87,END
87,ELSE 20
87,END;
87,"In this case, shipping costs are adjusted based on the origin and destination countries:"
87,"If both the origin and destination countries are in the US, the shipping cost is $5."
87,"If the origin country is in the US and the destination is outside, the shipping cost is $15."
87,"For shipments with origins outside the US, the shipping cost is set to $20."
87,"To sum up, combining the UPDATE statement with CASE expressions is a powerful way to update multiple columns in SQL based on specific conditions. It provides flexibility, control, and efficiency in managing database updates."
87,Safety Measures: Testing and Transactions
87,"When working with update multiple columns SQL queries, it’s essential to prioritize safety; after all, modifying your data is an irreversible operation. To better safeguard the integrity of your data, this section delves into testing methods and utilizing transactions while updating data in SQL databases."
87,"Before committing to major changes, experts recommend performing a thorough test run. This means creating and executing test scripts that mimic real-world situations to ensure the SQL query works as intended. One proven approach involves using a testing environment that accurately imitates the production database. In this way, you can:"
87,Minimize the likelihood of unintended consequences
87,Identify any missing data elements
87,Test the accuracy of your query logic
87,Learn how the system will respond after updating multiple columns.
87,"By conducting comprehensive tests, you’ll increase the safety of updating multiple columns in SQL."
87,"Another important aspect of maintaining a secure database is leveraging transactions. In SQL, transactions allow users to group one or more related modifications, ensuring all changes either succeed together or fail together. Effectively using transactions can prevent data inconsistencies and maintain the integrity of the database. Consider these transaction best practices:"
87,Use the BEGIN TRANSACTION statement to initiate a new transaction.
87,Execute your UPDATE statement(s) to modify the data.
87,"Review the data changes and consider the impact. If updates meet expectations, use the COMMIT statement to apply the changes. However, if something goes awry, issue the ROLLBACK command to undo the changes."
87,Here’s an example of a transaction while updating multiple columns in an SQL query:
87,BEGIN TRANSACTION;
87,UPDATE tablename
87,"SET column1 = new_value1, column2 = new_value2, ..."
87,WHERE condition;
87,-- Check the data changes and if they are correct
87,COMMIT;
87,"-- If the data changes are not correct, use ROLLBACK instead"
87,"In summary, safeguarding your database while updating multiple columns in SQL involves testing and using transactions. Establishing a robust testing environment and employing transactions ensure that your data remains stable and consistent throughout the update process, allowing for both peace of mind and sound data management."
87,Pitfalls and Common Mistakes
87,"When working with update multiple columns SQL queries, it’s crucial to be aware of the potential pitfalls and common mistakes that can arise. By understanding these issues, you’ll be more likely to avoid them and ensure a smooth process when updating multiple columns in your SQL databases."
87,"Firstly, it’s important to ensure that you’re using the correct syntax for updating multiple columns. In SQL, the correct way to do this is by listing each column and its new value, separated by commas. Here’s an example of the correct syntax:"
87,UPDATE tablename
87,"SET column1 = value1, column2 = value2, column3 = value3"
87,WHERE condition;
87,Failure to follow this syntax can lead to errors or unwanted results in your updates.
87,"Secondly, some developers mistakenly use multiple UPDATE statements for each column they want to update. This approach can have unexpected consequences, such as:"
87,Poor performance and slower execution time
87,Difficulty in maintaining and debugging your code
87,"Inappropriate locking of rows, causing deadlocks"
87,"It’s important to use a single UPDATE statement for each row you’re updating, with multiple columns listed within that statement."
87,"When updating multiple columns, there are specific scenarios that can lead to confusion, such as:"
87,"Updating columns based on other columns within the same table. This can be achieved using subqueries or self-joins; however, care must be taken to ensure the correct data is being updated and no unintended results are introduced."
87,"Dealing with NULL values. When working with update multiple columns SQL, handling NULL values can be tricky and lead to unexpected outcomes. It’s essential to be mindful of how NULL values affect your updates and which functions or operations can be used to handle them correctly."
87,Some common mistakes while working with update multiple columns SQL include:
87,Mismatched data types: Ensure that the data types of the values specified in the update match the data types of the columns being updated.
87,"Insufficient or incorrect use of conditions: When updating multiple columns, make sure the WHERE clause is correctly used to update only the desired rows. Otherwise, you may unintentionally modify too many rows or the wrong rows."
87,"In conclusion, be aware of the potential pitfalls and common mistakes when working with update multiple columns SQL queries. By understanding these issues and using the correct syntax, you’ll significantly increase your chances of a successful and trouble-free experience when updating multiple columns in your SQL databases."
87,Performance Considerations
87,"When working with update multiple columns SQL queries, it’s important to take performance considerations into account. Efficiency should always be a priority to ensure that databases are running smoothly, and the time for executing queries is minimized. This section highlights a few crucial aspects that can influence the performance of your SQL queries and provides guidance on optimizing these operations."
87,The performance of an update operation on multiple columns relies on several factors:
87,"Number of columns: The more columns you update, the higher the potential impact on performance. Thus, it’s important to only update the necessary columns."
87,"Indexes: If your query is working with indexed columns, the performance might be affected due to the database needing to rebuild indexes."
87,"Transaction locks: When updating multiple columns, database transactions might be locked for an extended period, potentially creating delays for other pending transactions."
87,"To enhance the efficiency of multiple column updates in SQL, you can implement the following practices:"
87,Limit the number of updated columns: Focus on updating only essential columns to reduce the potential impact on performance.
87,"Use targeted queries: Rather than updating all rows, narrow down specific rows using the WHERE clause in your update query. This approach can significantly decrease the number of affected rows, decreasing the query execution time."
87,"Update in batches: If you’re dealing with a huge volume of data, consider updating the table in smaller chunks instead of attempting to update all rows at once."
87,Optimize database indexes: Regularly check the health of your indexes and possibly reorganize or rebuild them if needed. Sick indexes can significantly impair your query performance.
87,"As a database administrator or developer, always keep an eye on the performance implications of SQL queries. When working with update multiple columns SQL, be aware of these potential bottlenecks and use optimization practices, like minimizing the number of updated columns, using targeted queries, batch updates, and index maintenance. By caring for these aspects, you’ll be able to maintain your database operations running efficiently and smoothly."
87,Conclusion
87,"Updating multiple columns in SQL is a common task in database management. It’s important for professionals to understand how this process works and to utilize best practices when writing SQL queries. In this article, the reader discovered the various methods used to update multiple columns in their SQL databases."
87,The process of updating multiple columns in SQL can be quite straightforward. Here are the main takeaways from the article:
87,Utilize the UPDATE statement to make changes to data in SQL tables.
87,Combine column names and new values with the SET clause to indicate which data needs to be updated.
87,Implement the WHERE clause for specifying specific rows that require updating.
87,Take advantage of JOINS for updating data from other tables.
87,"By using these techniques and keeping the best practices in mind, database administrators and developers can efficiently update multiple columns in SQL and manage their data effectively. Learning these skills is essential for maintaining organized and up-to-date databases, ultimately contributing to the overall efficiency and success of any project or business that relies on data management."
87,Chat with SQL Databases using AI
87,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
87,Start Free Trial
87,Related articles
87,How to Learn SQL JOIN Types Explained with Visualization
87,How to Use AVG in SQL
87,How to Use Dates in SQL
87,How to CREATE VIEW in SQL
87,How to Use AUTO INCREMENT in SQL
87,How to Use the SQL Default Constraints
87,How to Use the SQL Check Constraint
87,How to Use DENSE_RANK() in SQL
87,How to Use PRIMARY KEY in SQL
87,How to Use Unique Alter Table in SQL
87,How to Use ROW_NUMBER & OVER() in SQL
87,How to Use Unique Constraint in SQL
87,How to Concatenate Two Columns in SQL?
87,How to Include Zero in a COUNT() Aggregate
87,"What Are DDL, DML, DQL, and DCL in SQL?"
87,What is an SQL Inline Query?
87,What Is the Benefit of Foreign Keys in SQL?
87,How to Use Constraints Operator in SQL
87,What a Moving Average Is and How to Use it in SQL
87,How to Analyze a Time Series in SQL
87,How to Use TRUNCATE TABLE in SQL
87,TRUNCATE TABLE vs. DELETE vs. DROP TABLE
87,How to Number Rows in SQL
87,How to Use 2 CTEs in a Single SQL Query
87,How to Use Lag and Lead Functions in SQL
87,How to Calculate the Length of a Series with SQL
87,How to Use Aliases in SQL Queries for Clearer Code
87,How to Use the BETWEEN Operator in SQL
87,How to Use the IN Operator in SQL
87,What are & How to Use Wildcards in SQL
87,How to Use TOP in SQL with Examples
87,How to Use WHERE in SQL with Examples
87,How to Use AND OR Operators Correctly in SQL
87,How to Use HAVING Clause in SQL
87,How to Use the Alter Command in SQL: Renaming Tables and Columns
87,How to Use INSTR in SQL? Find Substrings Easily with Examples
87,How to Use the PARTITION BY Clause in SQL with Examples
87,How to Use ROUND Function in SQL Explained with Examples
87,How to Use CAST Function in SQL?
87,Why Use WHERE 1=1 in SQL Queries? Exploring Its Impact on Database Efficiency
87,How to Create a Table in SQL? Your Step-by-Step Guide for Beginners
87,How to Use GROUP BY in SQL? Master the Art of Query Optimization
87,How to Use UPDATE in SQL: A Comprehensive Guide for Beginners
87,How to Use Select in SQL: A Beginner’s Guide to Database Queries
87,How to Use Select Distinct in SQL: A Simple Guide for Efficient Database Queries
87,How to Use Union in SQL: A Simple Guide for Efficient Database Management
87,How to Use Self Join in SQL: A Comprehensive Guide for Beginners
87,How to Use Full Join in SQL: A Comprehensive Guide for Beginners
87,How to Use Right Join in SQL: A Comprehensive Guide for Database Enthusiasts
87,How to Use Left Join in SQL: A Guide for Database Query Optimization
87,Cristian G. Guasch
87,"Hey! I'm Cristian Gonzalez, I created SQL Easy while I was working at StubHub (an eBay company) to help me and my workmates learn SQL easily and fast."
87,SQL from zero to Data Analyst level
87,✅ Access 348 lessons and exercises
87,👥 Join 7789 students
87,📈 Learn Data Analysis for Product Management and Marketing
87,📊 Learn SQL from basics to advanced
87,💼 Practice for upcoming interviews
87,💬 Get support from Community Forum
87,Free Sign Up!
87,How to sponsor?
87,Support us with a yearly donation and help SQL-Easy.com thrive!
87,Apply
87,"Languages: English, Español, Portoghese, Italiano, Français, 日本語, Deutsch, اللغة العربية."
87,Easy to Learn 2017-2024
87,Learn SQL · Learn HTML · Learn CSS · Learn JS · Learn Python · Learn PHP
87,About · Sponsor SQL-Easy.com · Cookie Policy
90,Optimizing MySQL: Adding Data to Tables - Simple Talk
90,Redgate Hub
90,Product articles
90,University
90,Events
90,Forums
90,Community
90,Simple Talk
90,Home
90,Cloud
90,DevOps
90,Sysadmin
90,Development
90,Databases
90,Opinion
90,Books
90,Blogs
90,Log in
90,Sign up
90,Lukas Vileikis
90,22 May 2023
90,3126 views
90,Home
90,Databases
90,MySQL
90,Optimizing MySQL: Adding Data to Tables
90,Lukas Vileikis
90,22 May 2023
90,3126 views
90,Optimizing MySQL: Adding Data to Tables
90,"Welcome back to the MySQL optimization series! In case you haven’t been following this series, in the past couple of articles we have discussed the basics of query optimization, and told you how to optimize SELECT queries for performance as well."
90,"In this blog, we’re further learning ways to optimize INSERT operations and look at alternatives when you need to load more than a few rows in the LOAD DATA INFILE statement."
90,How Does INSERT Work?
90,"As the name suggests, INSERT is the query that’s used to create data in tables in a database. The internal functioning of a basic INSERT query is pretty much identical to that of aSELECTstatement as seen in the previous article, first, the database checks for permissions, opens tables, initializes operations, checks whether rows need to be locked, inserts data (updates the indexes), and then finishes coming to a stop."
90,We can back this up by performing some profiling:
90,Image 1 – INSERT Query Profiling – the Query
90,Image 2 – INSERT Query Profiling – Status and Duration
90,"To know what these status codes mean in detail, please refer to preview blog about SELECT queries where we explain them one by one, but in a nutshell, the profiling codes outlined above enable MySQL to figure out the following things, amongst others:"
90,Whether appropriate privileges have been set.
90,"Whether tables are ready to perform any operations on (i.e., whether they are locked or not.)"
90,How best to perform the INSERT operation. This step includes*:
90,Sending the query to the server.
90,Parsing the query.
90,Scanning the table for any indexes or partitions.
90,Inserting rows into tables
90,Adding data to applicable indexes.
90,Closing the process.
90,"* Do note that each step takes up a proportion of the total time INSERT queries take. According to the MySQL documentation, the fastest process is the closing of the query, the second fastest one pertains to sending the query to the server and parsing it, and the slowest is adding data to the table. (Note that for very small amounts of data, connecting to the server can be more costly)."
90,"As far as profiling is concerned, it’s indeed helpful when performing read operations, but not very much in other cases: knowing how queries work is a good thing to do, but when optimizing INSERT queries, profiling doesn’t take us far and that’s why we need to employ techniques that we’ll share with you in a second."
90,"Another thing to remember is that no matter what database management system is in use (INNODB, etc.), many queries go through the same steps – INSERT queries share many steps with SELECT queries too, however, while there are a lot of shared steps, it’s important to remember that all queries are different in their own regard too – an INSERT query is different from a SELECT query in that SELECT queries benefit from indexes and partitions, while they generally make INSERT queries slower."
90,"Advanced DBAs will also know that INSERT queries are sometimes used in concert with SELECT queries – we’ll start from the basic ways that will help you improve your INSERT query performance, then gradually evolve towards more complex scenarios."
90,Basic Ways to Improve INSERT Query Performance
90,"To improve INSERT query performance, start with the advice above: profile your queries, then remove indexes or partitions on a table if necessary. All indexes and partitions will make all INSERT queries slower because of one simple reason – every time data is inserted into a table, all indexes and partitions on that table must be updated for the database to know where the row resides. That’s true no matter what kind of partitions or indexes are in use – the more partitions or indexes exist on a table, the slower your query will become."
90,"Note: Remember that performance tuning must be treated as a holistic activity and we are focusing on tuningINSERT statements only here. Indexes are needed by most of your SQL statements and unless you are adding a LOT of data to your table, removing them just to make a particular INSERT operations go a little bit faster (and then killing read performance) is typically not desirable."
90,"Another very popular and simple way to improve query performance is to combine small operations into a single, larger operation. We can also lock tables and unlock tables only when all operations are completed, and these approaches look like so:"
90,Image 3 – a basic way to improve INSERT query performance.
90,"And that’s the crux of it – to optimize INSERT query speed, we need to “think big” in terms of queries. The whole magic goes like this: instead of having many queries that insert a couple of rows each, run one huge query that inserts many rows at once after making sure that all the table locks, index updates, and consistency checking are as delayed as possible (ideally delay these processes until the very end of the query.)"
90,"When it is feasible, delaying everything isn’t hard and can be done by following some or all these steps. Consider performing these steps if you’re working with more than a million rows and (or) whenever you see that your INSERT queries can be combined into a single operation just like in the example above (it’s not necessary to follow the steps from the top to bottom – following one or two steps will usually be a good start):"
90,"Lock the tables before inserting data into them and unlock them once all the data has been inserted, but not before (see example above.) By locking your tables, you will ensure that the data within them will not be modified while the data will be inserted."
90,"Consider starting a transaction (START TRANSACTION) before running INSERT queries – once done, issue a COMMIT query."
90,Avoid adding any indexes before insert operations have been completed.
90,"If possible, make sure that the table you’re inserting data into is not partitioned because partitioning splits your table into sub-tables and once you insert data into your table, MySQL has to go through a couple of additional steps to figure out which partition to insert what data into, etc. Also, while the proper use of partitions help improve the performance of SELECT statements, partitioned tables will inevitably take up a little more space on the disk, so be wary of that."
90,"Follow these steps and that’s it – you’re on the way to INSERT heaven! However, do note that these are only the basics and such an approach may not cover your scenario – in that case, we may need to perform some additional optimization by allocating the number of I/O threads within InnoDB by modifying my.cnf (refer to the options in the screenshot below), or improve query performance by doing other things you will learn in this article. Keep reading – your INSERT query performance will soon skyrocket!"
90,Improving INSERT Query Performance Beyond the Basics
90,"Once you’re sure that all of your INSERT queries are profiled and doing what you expect, employ the basic steps outlined above to improve their performance. If that didn’t help, consider the following advice:"
90,"We can start an explicit transaction using START TRANSACTION or issue autocommit=0, run all of our INSERT queries, then run a COMMIT query. By doing so we delay commit operations until after the very last INSERT query has been finished – in that case, MySQL saves time because it doesn’t commit SQL statements as soon as they’re executed, but commits them all at once later instead. The bigger the data set is, the more time will be saved."
90,"Such an approach isn’t very feasible with billions of rows (INSERT queries are not designed for more than 50 or 100 million rows at a time – somewhat depends on hardware in use – and we need to use LOAD DATA for that since INSERT statements come with a lot of overhead – more on that later), but if we’re dealing not dealing with that many rows, it certainly could be viable."
90,We can modify the my.cnf file and add or modify the following options to increase the I/O capabilities of InnoDB (the main storage engine within MySQL):
90,Image 3 – Increasing I/O Capabilities of InnoDB
90,"The innodb_read_io_threads option sets the number of threads handling read operations – in most cases, the value of 32 is sufficient and should be left at default."
90,"The innodb_write_io_threads option sets the number of threads handling write operations – in most cases, the value of 32 is also sufficient and should be left at default."
90,The innodb_io_capacity option defines the total I/O capacity of InnoDB and its value should be set at the maximum value of IOPS available to the server
90,"It is mostly related to the threads that perform various database-related tasks in the background (working with the buffer pool, writing changes, etc.) – these threads are trying to not have a negative effect on InnoDB at the same time. for more information, head over to the MySQL documentation over here."
90,"For more details on optimizing my.cnf for general performance, I previously posted the following article on Simple-Talk: Optimizing my.cnf for MySQL performance."
90,"We can insert more data at once by using the bulk insert operation available within the INSERT statement itself. When using such an approach, we can avoid defining the columns we load the data into if we load the data into all of them at once as well. Do note that if we elect to ignore columns and we have an AUTO_INCREMENT column, we must specify the value as NULL (an example is given below.)"
90,Image 4 – INSERT INTO Example
90,Additional considerations for INSERT performance
90,"These steps previously covered will help you optimize your INSERT queries; however, these are only the basics. Advanced DBAs know of a couple of additional ways to improve their INSERT query performance and some of advanced tips include combining INSERT statements with SELECTstatements too!"
90,Locks and Inserting rows concurrently
90,"Many of you know that issuing INSERT statements means locks for InnoDB: MySQL deals with each statement differently, and as far as INSERT statements are concerned, InnoDB row-level locks are advantageous for the end-user: the storage engine sets a lock on the inserted row thus not disrupting any operations with any of the other rows in the same table. Users can still work with their InnoDB-based tables as they’re inserting data as long as the rows they need are not locked and they didn’t take a table lock)."
90,"Before any row is inserted, MySQL also lets InnoDB know that a row is going to be inserted by setting an insert intention lock. The purpose of that lock is to let other transactions know that data is inserted into a certain position, so that other transactions do not insert data into the same position."
90,"To make the best out of locks within InnoDB, follow the advice above – make use of bulk INSERT statements and consider delaying all commit operations until after the very last INSERT query: bulk INSERT statements will insert data in a quicker fashion and committing an operation only after the last query will be faster as well."
90,Inserting rows from a SELECT Query
90,"As noted above, INSERT queries are the primary way of inserting data into MySQL, but many DBAs will know that INSERT queries are not always simple either: certain situations may require us to use them in concert with other – typically SELECT – queries too. A query like so would also work quite successfully:"
90,123
90,"INSERT INTO table_name (FirstColumnName, SecondColumnName)SELECT  FirstColumnName, SecondColumnName FROM another_table [options]"
90,For example:
90,Image 5 – INSERT and SELECT Queries Together
90,"Note that in this case indexes will not be beneficial to the table where new rows are being created, but they will be beneficial fetching rows from the table in the SELECT statement. Note that in the sample query, the another_table reference in the query can be the same table. So:"
90,123
90,"INSERT INTO table_name (FirstColumnName, SecondColumnName) SELECT FirstColumnName, SecondColumnName FROM table_name [options]"
90,"Is possible as well, in which indexes can (depending on if you have a where clause on the table,) can both be helpful and detrimental at the same time."
90,The CREATE TABLE … SELECT Query
90,"To select data directly into a new table, we can also employ a CREATE TABLE statement together with a SELECT query. In other words, we can create a table and insert data into it without running a separate INSERT query afterwards. There are two ways to do this:"
90,Run a CREATE TABLE statement with a SELECT statement without defining any columns or data types:
90,CREATE TABLE new_table [AS] SELECT [columns] FROM old_table;
90,"MySQL will recreate all columns, data types and data from the old_table."
90,You can also define the columns and constraints and insert data to the new table like so:
90,CREATE TABLE new_table (
90,"column1 VARCHAR(5) NOT NULL,"
90,column2 VARCHAR(200) NOT NULL DEFAULT ‘None’
90,) ENGINE = InnoDB
90,"SELECT demo_column1, demo_column2"
90,FROM old_table;
90,Selecting data from one table to insert directly into another table is generally faster than INSERT INTO ... SELECT.
90,The Best Way to Load Massive Data Sets – INSERT vs. LOAD DATA INFILE
90,LOAD DATA INFILE is built for blazing fast data insertion from text files. Speed is achieved by ignoring or eliminating overhead posed by INSERT queries done by:
90,"Working with “cleaner” data (data only separated by certain denominators (think “,”, “:”, “|”, or not separated at all.)"
90,Providing us with the ability to only load data into specific columns or skip loading data into certain rows or columns altogether.
90,"To use LOAD DATA INFILE, make sure you have a file that’s separating its columns by a certain denominator (common denominators include, but are not limited to “,”, the TAB sign, spaces, the “|”, “:”, and “-“ characters, etc.) and preferably one that’s saved in a CSV or TXT format. Save the file in a directory, remember the path towards that directory, then use the following query (replace the path to the file with your file, the “|” sign with your denominator and demo_table with the name of your table) – use IGNORE to ignore all errors posed by the query (duplicate key issues, etc.):"
90,LOAD DATA INFILE ‘/var/lib/mysql/tmp/data.csv’ [IGNORE] INTO TABLE demo_table FIELDS TERMINATED BY ‘|’;
90,"To export data from your database and make the data able to be re-imported by using the LOAD DATA INFILE query, use the SELECT * INTO OUTFILE query – use IGNORE if you want to ignore all errors."
90,1234
90,SELECT *FROM demo_table [IGNORE] INTO OUTFILE '/var/lib/mysql/tmp/data.csv' FIELDS TERMINATED BY '|';
90,"LOAD DATA INFILE has many parameters that can be used as well. These parameters include, but are not limited to:"
90,The PARTITION parameter allows us to define the partition where we want to insert data into.
90,Combining the IGNORE option with the LINES or ROWS options to tell MySQL how many lines or rows to ignore when inserting data.
90,Providing us with the ability to set the values of certain columns by using the SET option (the query below takes data from the file called “demo.csv” and loads the data with its columns terminated by “:” into a table called demo after ignoring the 100 lines from the beginning and also sets the date column to the current date when inserting all of the rows):
90,LOAD DATA INFILE ‘demo.csv’ INTO TABLE demo FIELDS TERMINATED BY ‘:’ IGNORE 100 LINES SET date=CURRENT_DATE();
90,"LOAD DATA INFILE can be made even faster if we employ DEFAULT constraints. If there are columns that need to be populated with the same data for each row, the DEFAULT constraint will be faster than fetching it from the data stream. That way, all columns having the default keyword will be pre-filled without the need to load data into them."
90,"Such an approach is very convenient to save time when one row consists of the same data and we don’t want to employ ALTER queries (those queries make a copy of the table on the disk, insert data into it, perform operations on the newly created table, then swap the two tables.) An example is given below:"
90,1234
90,"CREATE A TABLE demo_table (`demo_prefilled_column` VARCHAR(120) NOT NULL           <strong>DEFAULT 'Value Here’</strong>,) ENGINE = InnoDB;"
90,"To dive even deeper into LOAD DATA INFILE, refer to the MySQL documentation itself."
90,"Indexes, Partitions, and INSERT FAQ"
90,"If even LOAD DATA INFILE doesn’t help, you may want to look into indexes and partitions. Remember what we said above? The more indexes or partitions your table has, the slower your INSERT query will be. You already know the drill – but you may be pressed with some extra questions, some of them being the following:"
90,Question
90,Answer
90,Should I drop all of the indexes before running an INSERT query?
90,"Typically No – while indexes are used to improve the performance of finding rows, and generally slow down INSERT statements usually it is fine to let the INSERT statement process maintain indexes as part of the insert process."
90,"However, in some cases when loading hundreds of millions or billions of rows, indexes should certainly be removed (they can be added again by using an ALTER TABLE query.)"
90,Does using a PRIMARY KEY constraint on a table slow INSERT statements down?
90,Yes – primary keys are always indexed and indexes make INSERT queries slower.
90,Does the amount of indexes or partitions on a table make a difference in INSERT query performance?
90,"Yes – the more indexes or partitions you have on a table, the slower your INSERT queries will be."
90,Are LOAD DATA INFILE queries always faster than INSERT statements? Why?
90,Yes – LOAD DATA INFILE queries always come with less overhead than INSERT queries do.
90,"When optimizing INSERT statements, is there a difference of what storage engine is in use?"
90,Yes – aim to use InnoDB unless you need specific features:
90,MyISAM will make COUNT(*) aggregate queries faster since the storage engine stores the row count inside of its metadata.
90,ARCHIVE will let you archive data with little footprint on the disk.
90,MEMORY is useful for temporary tables since all of its data is stored in the memory.
90,CSV will be advantageous if we need a way to migrate data into some sort of spreadsheet software.
90,"For a complete list of storage engines, check the MySQL documentation."
90,"When inserting data, is there a difference if the table we insert data into is locked or not?"
90,Yes – inserting data into locked tables is generally faster than the other way around since database management systems check for table locks as part of the query execution process.
90,How do I achieve a balance between INSERT and SELECT queries?
90,"Testing – Load testing is important to see what effects any change will have on a system. A lot will depend on how you are using a system, and how concurrent users use a system is important. If users are simultaneously creating and querying data, it is different than if data is loaded in a time window, then queried at other times."
90,"Generally, aim to have only as many indexes and partitions as necessary, SELECT as few rows as possible, and where possible, use LOAD DATA INFILE instead of INSERT."
90,Foreign keys – are they a problem for INSERT queries?
90,"No – INSERT queries will be slower on a table with foreign keys since the database must check that there are no foreign key violations, but in most cases, they won’t make much of a performance difference (but will make a large data integrity difference). As is the case with indexes, foreign key constraints should be dropped once we have a lot (hundreds of millions of rows) of data to insert."
90,The CHECK constraint – does it slow down inserting data?
90,"Yes – the CHECK constraint was introduced in MySQL 8.0.16, and it lets users specify a condition to check before inserting data into a table. Think of it as an integrity check – if a violation of the constraint is found, MySQL shows an error and terminates query execution, or skips inserting the row altogether if the IGNORE clause is specified."
90,"In many cases, the constraint will only be problematic if the constraint evaluates to FALSE instead of TRUE because in that case, queries will fail if no IGNORE clause is specified. Aim to avoid using constraints when inserting larger quantities of data, because they could make your INSERT queries slower. (If you remove one for performance reasons, add it back for integrity’s sake.)"
90,More details about the CHECK constraint can be found on the MySQL blog.
90,Is it possible to insert data and update it at the same time?
90,"Yes – that can be done by using the ON DUPLICATE KEY UPDATE option on anINSERTstatement. The statement is pretty self-explanatory: if there is a situation where a row of the same value would be inserted into a column, an update of the old row would occur. Such a statement is mostly used to increment column values whenever there are duplicate key issues, but it can have other use cases as well."
90,Are there any lesser-known ways to improve INSERT query performance?
90,"Yes – if you’re working with bigger data sets, make use of the DEFAULT keyword. The keyword will let you specify the default value of the column and as a result, will automatically be filled in when running LOAD DATA INFILE queries saving you time in return."
90,When to switch to LOAD DATA INFILE?
90,"Typically, when loading any data where you have many rows to load – There are no set “rules” that define when you should switch INSERT statements to LOAD DATA INFILE where possible, however, keep in mind the following:"
90,There are several necessary steps that the DBMS must go through when performing INSERT operations.
90,"INSERT operations come with overhead. The more data is inserted, the bigger the overhead is."
90,INSERT query performance can be optimized a lot by using bulk INSERT queries or by delaying index updates and concurrency checking.
90,LOAD DATA INFILE works best when settings inside of my.cnf are optimized for InnoDB.
90,Balancing INSERT Performance and Reads
90,"To balance out the performance between INSERT and SELECT queries, keep the following in mind:"
90,"Basics do help – Select as few rows as possible by using a column list with your SELECT of SELECT *, and index only the columns you’re searching through."
90,"Normalize your tables – Table normalization helps save storage space as well as increases the speed of your read queries if used properly. Choose a normalization method suitable for you, then proceed further."
90,"Do note that as with everything, normalization also can have negative effects – more tables and relationships can mean more individual INSERT or LOAD DATA INFILE statements, but if you do everything correctly, your database should roll just fine!"
90,"Use the proper storage engine – this should be obvious, but you would be surprised how many inexperienced DBAs make a mistake of using an inappropriate storage engine for their use case. If you’re using MySQL, aim to use InnoDB or XtraDB for general use cases."
90,"If you’re in a testing environment and memory is not an issue, the MEMORY storage engine could also be an option, but note that the storage engine cannot be as heavily optimized and that the data won’t be stored on the disk either – at that point the insert queries would be blazing fast, but the data would be stored in the memory itself meaning that a shutdown of the server would destroy all of your data as well."
90,"Use a powerful server – this goes hand in hand with optimizing my.cnf or other files. If your server is powerful enough, balancing the performance of INSERT and SELECT operations will be a breeze."
90,"Only use indexes and partitions where necessary and don’t overdo it – aim to index only the columns you’re running heavy SELECT queries on, and only partition your tables if you have more than 50 million records."
90,"Consider ditching INSERT statements altogether – if you must import huge chunks of data, consider splitting them into files and uploading those files with LOAD DATA INFILE instead."
90,Summary
90,"In this blog, we’ve walked you through how best to optimize INSERT queries for performance and answered some of the most frequently asked questions surrounding these sorts of queries. We’ve also touched upon the importance of balancing INSERT and SELECT operations and walked you through a way to ditch INSERT queries altogether when loading lots of data."
90,"Some of the advice we’ve provided in this blog is known by many DBAs, while some might not be known at all. Take from this blog what you will – there’s no necessity to follow everything outlined in the article step by step, but combining some of the advice in this article with the advice contained in other parts of this series will be invaluable for your database and your applications alike."
90,"We hope that this blog has taught you something new, and we’ll see you in the next one."
90,MySQL
90,NoSQL
90,Oracle
90,PostgreSQL
90,Snowflake
90,SQL Server
90,Theory and design
90,Subscribe to the MySQL RSS feed
90,Subscribe for more articles
90,"Fortnightly newsletters help sharpen your skills and keep you ahead, with articles, ebooks and opinion to keep you informed."
90,3126 views
90,Rate this article
90,Click to rate this post![Total: 2
90,Average: 5]
90,Lukas Vileikis
90,Lukas Vileikis is an ethical hacker and a frequent conference speaker.
90,"Since 2014, Lukas has found and responsibly disclosed security flaws in some of the most visited websites in Lithuania."
90,"He runs one of the biggest & fastest data breach search engines in the world - BreachDirectory.com, frequently speaks at conferences and blogs in multiple places including his blog over at lukasvileikis.com."
90,Follow Lukas Vileikis via
90,View all articles by Lukas Vileikis
90,Load comments
90,Related articles
90,Mercy Bassey Udoh
90,14 December 2023
90,Mercy Bassey Udoh
90,14 December 2023
90,MySQL Error Log Management in DevOps Operations
90,MySQL
90,"When it comes to the development and operations (DevOps), one thing stands out as a critical aspect and that is troubleshooting. The primary goal of a DevOps team is to ensure that the product experiences zero to no downtime because every moment is crucial. Therefore, smooth delivery and uninterrupted uptime are paramount. To achieve this, …			Read more"
90,MySQL
90,Simple Talk
90,FAQ
90,Author AI Usage Policy
90,Sitemap
90,About Simple Talk
90,Contact Us
91,OData Query Options | Mendix Documentation
91,Docs
91,Release Notes
91,Mx10 Feature Release Calendar
91,Studio Pro
91,"LTS, MTS, and Monthly Releases"
91,10.8
91,10.7
91,10.6
91,10.5
91,10.4
91,10.3
91,10.2
91,10.1
91,10.0
91,9.24
91,9.23
91,9.22
91,9.21
91,9.20
91,9.19
91,9.18
91,9.17
91,9.16
91,9.15
91,9.14
91,9.13
91,9.12
91,9.11
91,9.10
91,9.9
91,9.8
91,9.7
91,9.6
91,9.5
91,9.4
91,9.3
91,9.2
91,9.1
91,9.0
91,8.18
91,8.17
91,8.16
91,8.15
91,8.14
91,8.13
91,8.12
91,8.11
91,8.10
91,8.9
91,8.8
91,8.7
91,8.6
91,8.5
91,8.4
91,8.3
91,8.2
91,8.1
91,8.0
91,7.23
91,7.22
91,7.21
91,7.20
91,7.19
91,7.18
91,7.17
91,7.16
91,7.15
91,7.14
91,7.13
91,7.12
91,7.11
91,7.10
91,7.9
91,7.8
91,7.7
91,7.6
91,7.5
91,7.4
91,7.3
91,7.2
91,7.1
91,7.0
91,Windows Service
91,Mobile
91,Make It Native Apps
91,Make It Native 10 App
91,Make It Native 9 App
91,Make It Native 8 App
91,Mendix Native Mobile Builder
91,Native Builder
91,Native Template
91,Studio Pro 9 & 10 Compatible
91,Native Template 8
91,Native Template 7
91,Native Template 6
91,Studio Pro 8 Compatible
91,Native Template 5.2
91,Native Template 5.1
91,Native Template 5.0
91,Mendix Mobile App
91,Hybrid App Base and Template
91,Developer Portal
91,Deployment
91,Mendix Cloud
91,Mendix for Private Cloud
91,SAP BTP
91,Other Deployment Options
91,Control Center
91,Marketplace
91,Catalog
91,Community Tools
91,Private Mendix Platform
91,1.6 (MTS)
91,1.5
91,SDKs
91,Model SDK
91,Platform SDK
91,Metamodel
91,10.8
91,10.7
91,10.6
91,10.5
91,10.4
91,10.3
91,10.2
91,10.1
91,10.0
91,9.24
91,9.23
91,9.22
91,9.21
91,9.20
91,9.19
91,9.18
91,9.17
91,9.16
91,9.15
91,9.14
91,9.13
91,9.12
91,9.11
91,9.10
91,9.9
91,9.8
91,9.7
91,9.6
91,9.5
91,9.4
91,9.3
91,9.2
91,9.1
91,9.0
91,8.18
91,8.16
91,8.15
91,8.14
91,8.13
91,8.12
91,8.11
91,8.10
91,8.9
91,8.8
91,8.7
91,8.6
91,8.5
91,8.4
91,8.3
91,8.2
91,8.1
91,Security Advisories
91,Beta and Experimental Releases
91,Quick Starts
91,Creating a Hello World App
91,Building a Responsive Web App
91,Adding a Native Mobile App
91,Studio Pro 10 Guide
91,Installation
91,Installing Studio Pro
91,System Requirements
91,Upgrading from Studio Pro 9 to 10
91,Configuring Parallels
91,Performance Tips
91,General Info
91,Studio Pro Overview
91,MxBuild
91,Developer Tool Recommendations
91,mx Command-Line Tool
91,App Commands
91,Adaptable Solution Commands
91,Module Commands
91,Export Package Commands
91,Merging and Diffing commands
91,MPR dump
91,Third-Party Licenses
91,App Modeling
91,Best Practices for Development
91,Best Practices for App Performance
91,Consistency Errors
91,Page Editor Consistency Errors
91,Navigation Consistency Errors
91,Importing and Exporting Elements
91,Starting with App from a Spreadsheet
91,Menus
91,File Menu
91,New App
91,Open App
91,Export App Package
91,Import App Package
91,Edit Menu
91,"Find, Find Advanced and Find Usages"
91,Go to Option
91,Preferences
91,View Menu
91,Changes Pane
91,Integration Pane
91,Errors Pane
91,Suppression Rules
91,Page Explorer
91,Stories Pane
91,App Menu
91,Create Deployment Package
91,Deploy to the Cloud
91,Run Menu
91,Edit Cloud Foundry Settings
91,Version Control Menu
91,Commit
91,History
91,Download from Version Control Server
91,Upload to Version Control Server
91,Branch Line Manager
91,Create Branch Line
91,Merge Dialog Box
91,Language Menu
91,Batch Replace
91,Batch Translate
91,Language Operations
91,Language Settings
91,Translating Your App Content
91,Using Translatable Validation Messages
91,App Explorer
91,App
91,App Settings
91,Configurations
91,Navigation
91,Set Up Navigation
91,System Texts
91,Modules
91,Module Settings
91,Publish Add-on and Solution Modules
91,Consume Add-on Modules and Solutions
91,UI Resources Package
91,Security
91,App Security
91,User Roles
91,Administrator
91,Demo Users
91,Anonymous Users
91,Password Policy
91,Module Security
91,Domain Model
91,Entities
91,External Entities
91,Persistability
91,Attributes
91,Validation Rules
91,Event Handlers
91,Indexes
91,Access Rules
91,Associations
91,Association Properties
91,Association Tab Properties
91,Querying Over Self-References
91,Annotations
91,Generalization vs 1-to-1 Associations
91,Configuring a Domain Model
91,Setting Up Data Validation
91,Pages
91,Page
91,Page Properties
91,Page Resources
91,Image Collection
91,Layout
91,Placeholder
91,Header
91,Sidebar Toggle
91,Menu
91,Snippet
91,Building Block
91,Page Template
91,Icon Collection
91,Data Containers
91,Data View
91,Grids
91,Data Grid
91,Grid Columns
91,Template Grid
91,Control Bar
91,Search Bar
91,Sort Order
91,List View
91,Data Sources
91,Database Source
91,XPath Source
91,Context Source
91,Microflow Source
91,Nanoflow Source
91,Association Source
91,Listen to Widget Source
91,Configure Form and Show Form Items
91,Configure List and View Details on 1 Page
91,Text
91,Text
91,Label
91,Page Title
91,Structure
91,Layout Grid
91,Container
91,Group Box
91,Snippet Call
91,Tab Container
91,Scroll Container
91,Table
91,Navigation List
91,Input Elements
91,Text Box
91,Text Area
91,Drop-Down
91,Check Box
91,Radio Buttons
91,Date Picker
91,Reference Selector
91,Reference Set Selector
91,Input Reference Set Selector
91,"Images, Videos, and Files"
91,Static Image
91,Dynamic Image
91,File Manager
91,Image Uploader
91,Enable End-Users to Attach Images
91,Configure File Upload and Download
91,Buttons
91,Button Properties
91,Creating a Custom Save Button
91,Menus and Navigation
91,Menu Bar
91,Simple Menu Bar
91,Navigation Tree
91,Authentication
91,Login ID Text Box
91,Password Text Box
91,Sign-In Button
91,Validation Message
91,Charts
91,Chart Configuration
91,Chart Advanced Cheat Sheet
91,Any Chart Widgets
91,Any Chart Building Blocks
91,Any Chart Cheat Sheet
91,Properties Common in the Page Editor
91,On Click Event and Events Section
91,Application Logic
91,Microflows and Nanoflows
91,Microflows
91,Microflow Properties
91,Triggering a Microflow From a Menu Item
91,Testing Microflows with Unit Test Module
91,Error Handling in Microflows
91,Extracting and Using Sub-Microflows
91,Retrieving Current User with a Microflow
91,Nanoflows
91,Nanoflow Properties
91,Error Handling in Nanoflows
91,Sequence Flow
91,Activities
91,Object Activities
91,Cast Object
91,Change Object
91,Commit Object(s)
91,Create Object
91,Delete Object(s)
91,Retrieve
91,Rollback Object
91,List Activities
91,Aggregate List
91,Change List
91,Create List
91,List Operation
91,Working with Lists in a Microflow
91,Action Call Activities
91,Java Action Call
91,JavaScript Action Call
91,Microflow Call
91,Variable Activities
91,Change Variable
91,Create Variable
91,Client Activities
91,Call Nanoflow
91,Show Message
91,Close Page
91,Download File
91,Show Home Page
91,Show Page
91,Synchronize to Device
91,Clear from Device
91,Synchronize
91,Validation Feedback
91,Integration Activities
91,Call External Action
91,Call REST Service
91,Call Web Service
91,Import Data from File
91,Import with Mapping
91,Export With Mapping
91,Query External Database
91,Send REST Request (Beta)
91,Log Message
91,Generate Document
91,Metrics Activities
91,Counter
91,Gauge
91,Increment Counter
91,ML Kit Activities
91,Call ML Model
91,Workflow Activities
91,Apply Jump-To Option
91,Call Workflow
91,Change Workflow State
91,Complete User Task
91,Generate Jump-To Options
91,Retrieve Workflow Activity Records
91,Retrieve Workflow Context
91,Retrieve Workflows
91,Show User Task Page
91,Show Workflow Admin Page
91,Lock Workflow
91,Unlock Workflow
91,Notify Workflow
91,External Object Activities
91,Delete External Object
91,Send External Object
91,Decisions
91,Decision
91,Object Type Decision
91,Merge
91,Annotation
91,Parameter
91,Loop
91,Events
91,Start Event
91,End Event
91,Error Event
91,Continue Event
91,Break Event
91,Common Properties
91,Debugging Microflows and Nanoflows
91,Debugging Microflows Remotely
91,Workflows
91,Workflow Elements
91,Workflow Parameters
91,Multi-User Task
91,User Task
91,Wait for Notification
91,Wait for Timer
91,Decision in Workflows
91,Parallel Split
91,Jump Activity
91,Call Microflow
91,Call Workflow
91,Workflow Properties
91,Configure Workflow Security
91,Workflow Engine
91,Add Workflow to Existing App
91,Jump to Different Activities
91,Workflow Events
91,Workflow Versioning and Conflict Mitigation
91,Workflow for Employee Onboarding
91,Add Custom Action to Workflow Toolbox
91,Expressions
91,Unary Expressions
91,Arithmetic Expressions
91,Relational Expressions
91,Special Checks
91,Boolean Expressions
91,If Expressions
91,Mathematical Function Calls
91,String Function Calls
91,Date Creation
91,Begin-of Date Function Calls
91,End-of Date Function Calls
91,Between Date Function Calls
91,Add Date Function Calls
91,Subtract Date Function Calls
91,Trim to Date
91,To String
91,Length
91,Parse Integer
91,Parse and Format Decimal Function Calls
91,Parse and Format Date Function Calls
91,Enumerations in Expressions
91,Configure String Concatenation
91,Mendix Assist
91,MxAssist Logic Bot
91,MxAssist Best Practice Bot
91,Recommendations from Best Practice Bot
91,Validation Assist
91,MendixChat
91,Resources
91,Java Actions
91,JavaScript Actions
91,Rules
91,Enumerations
91,Datasets
91,OQL
91,OQL Expressions
91,OQL Aggregation
91,OQL Functions
91,OQL CAST
91,OQL COALESCE
91,OQL DATEDIFF
91,OQL DATEPART
91,OQL LENGTH
91,OQL LOWER
91,OQL RANGEBEGIN
91,OQL RANGEEND
91,OQL REPLACE
91,OQL ROUND
91,OQL UPPER
91,OQL Operators
91,OQL Case Expression
91,OQL Parameters
91,OQL From Clause
91,OQL Group by Clause
91,OQL Limit Clause
91,OQL Order by Clause
91,OQL Select Clause
91,OQL Where Clause
91,Constants
91,Regular Expressions
91,Scheduled Events
91,Task Queue
91,Document Templates
91,Creating Your Own Documents
91,Data Grid (Document Template)
91,Columns (Document Template)
91,Data View (Document Template)
91,Document Template
91,Dynamic Image (Document Template)
91,Dynamic Label (Document Template)
91,Footer (Document Template)
91,Header (Document Template)
91,Line Break (Document Template)
91,Page Break (Document Template)
91,Static Image (Document Template)
91,Static Label (Document Template)
91,Style
91,Table (Document Template)
91,Row (Document Template)
91,Cell (Document Template)
91,Template Grid (Document Template)
91,Title (Document Template)
91,Data Types
91,Images
91,XPath
91,XPath Aggregate Functions
91,XPath Constraints
91,XPath Constraint Functions
91,XPath true
91,XPath false
91,XPath not
91,XPath length
91,XPath string-length
91,XPath year-from-dateTime
91,XPath month-from-dateTime
91,XPath day-from-dateTime
91,XPath hours-from-dateTime
91,XPath minutes-from-dateTime
91,XPath seconds-from-dateTime
91,XPath quarter-from-dateTime
91,XPath day-of-year-from-dateTime
91,XPath week-from-dateTime
91,XPath weekday-from-dateTime
91,XPath contains
91,XPath starts-with
91,XPath ends-with
91,XPath Expressions
91,XPath Keywords and System Variables
91,XPath Operators
91,XPath Tokens
91,Define Access Rules Using XPath
91,Filter Data Using XPath
91,Integration
91,Message Definitions
91,JSON Structures
91,XML Schemas
91,XML Schema Support
91,Mapping Documents
91,Export Mappings
91,Import Mappings
91,Map Automatically
91,ML Model Mapping
91,Select Elements
91,XML Inheritance and Choice
91,Business Event Services
91,External Database Connection
91,OData Services
91,Consumed OData Services
91,Consumed OData Service
91,Consumed OData Service Requirements
91,Published OData Services
91,Published OData Attribute
91,OData Query Options
91,OData Representation
91,Published OData Entity
91,Published OData Microflow
91,Build OData APIs with REST Best Practices
91,Security and Shared Datasets
91,REST Services
91,Consumed REST Services
91,Using a Proxy to Call a REST Service
91,Server-Side Paging and Sorting
91,Advanced Consumed REST Services
91,Published REST Services
91,Published REST Service
91,Published REST Operation
91,Operation Parameters for Published REST
91,Published REST Path Parameters
91,Published REST Query Parameters
91,Published REST Resource
91,CORS Settings for Published REST Services
91,GitHub-Flavored Markdown
91,Version a REST Service
91,Generating a Published REST Resource
91,Publish Microflow as REST Operation
91,Technical Details of Published REST
91,Published REST Request Routing
91,JSON Schema for Published REST Operation
91,OpenAPI 2.0 Documentation
91,OpenAPI 3.0 Documentation
91,Custom Authentication Microflow Parameters
91,HttpRequest and HttpResponse System Entities
91,Images and Files with REST
91,Consumed REST Services (Beta)
91,Web Services
91,Consumed Web Services
91,Consume a Simple Web Service
91,Consume a Complex Web Service
91,Consumed Web Service
91,Numeric Formatting
91,Using a Proxy to Call a Web Service
91,Published Web Services
91,Expose a Web Service
91,Operations
91,Published Web Service
91,Test Web Services Using SoapUI
91,Machine Learning Kit
91,Using ML Kit
91,Logistic Regression Example
91,Pre-Trained ML Models
91,Design Patterns
91,Advanced Inference Design Patterns
91,Pre/Post-Processor Design Patterns
91,Version Control
91,Using Version Control
91,Combining Changes and Conflict Resolution
91,Automatic Fetching
91,Git Storage Optimization
91,Troubleshoot Version Control
91,Repository Size
91,Team Server Issues
91,Version Control FAQ
91,Git On-Premises Version Control Server
91,Mendix Runtime
91,Runtime Server
91,Mendix Client
91,Mendix React Client
91,Marketplace Component React Status
91,Runtime Deployment
91,Clustered Mendix Runtime
91,Communication Patterns
91,Data Sources Retrieval
91,Minimizing Objects in Session
91,Data Storage
91,Attribute Type Migration
91,Case-Sensitive Database Behavior
91,Order By Behavior
91,Unlimited String Behavior
91,MySQL/MariaDB
91,Oracle
91,SAP HANA
91,Date and Time Handling
91,DateTime Handling FAQ
91,Logging
91,Login Behavior
91,Mendix Runtime and Java
91,Non-Persistable Objects and Garbage Collecting
91,Java Memory Usage
91,Common Runtime and Java Errors
91,Metrics
91,Monitoring Client State
91,Monitoring Mendix Runtime
91,Objects and Caching
91,Runtime Customization
91,Advanced Custom Settings
91,WebSockets
91,Mobile
91,Getting Started with Mobile
91,Prerequisites and Troubleshooting
91,Introduction to Mobile Technologies
91,Native Mobile
91,Progressive Web App
91,Designing Mobile User Interfaces
91,Design Principles
91,Navigation
91,"Images, Icons, and Fonts"
91,Native Styling
91,Widget Styling Guide
91,Building Efficient Mobile Apps
91,Optimizing Native Startup
91,Offline-First Data
91,Offline Synchronization
91,Offline Best Practices
91,Synchronization & Auto-Committed Objects
91,Offline Data Security
91,Logging in Native Apps
91,Using Mobile Capabilities
91,Authenticating Users
91,Deep Links
91,Internationalize Mobile Apps
91,Location and Maps
91,Push Notifications
91,1. Add Module Dependencies
91,2. Push Notifications Module
91,3. Set Up Firebase Cloud Messaging
91,4. Configure Push Notifications
91,5. Push Notifications in Native App
91,6. Native App with Push Notifications
91,7. Test Push Notification
91,8. Notifications to Multiple Devices
91,Local Notifications
91,Part 1: Local Notifications
91,Part 2: Badges
91,Part 3: Actions
91,Part 4: Data
91,Part 5: Scheduling
91,Augmented Reality
91,Get Started with AR
91,Create an AR Business Card
91,App Permissions
91,Mobile Accessibility
91,"Build, Test, Distribute Apps"
91,Building Native Apps
91,Build a Mendix Native App Locally
91,Native App Local Manual Build
91,Deploy Mendix Native Mobile App
91,Creating a Custom Developer App
91,Native Template
91,Distributing Native Apps
91,Updating Native Apps
91,Debugging Native Apps
91,Testing Native Apps
91,Best Practices for Mobile Apps
91,Deleted Flag
91,Incremental Synchronization
91,Batch Synchronization
91,Compound Object
91,Request Object
91,Java Programming
91,Troubleshooting
91,Using Eclipse
91,Extending App with Custom Java
91,Using the Mendix Runtime Java API
91,Java Version Migration
91,Managed Dependencies
91,Studio Pro 10 How-tos
91,Front End
91,UI Design
91,Get Started
91,Customize Styling
91,Configure Module-Level Theme Settings
91,Create a Company Design System
91,Extend Design Properties
91,Atlas UI Kit for Figma
91,Implement Best Practices for UX Design
91,Use Navigation Layouts
91,Configure Your Theme
91,Create Overview and Detail Pages
91,Use Layouts and Snippets
91,Implement Classes
91,Create Custom Error Pages
91,Data Models
91,Denormalize Data to Improve Performance
91,Share the Development Database
91,Migrate Your Mendix Database
91,Integration
91,Integrate Legacy System
91,Import XML Documents
91,Export XML Documents
91,Import Excel Documents
91,Import a Large Excel File
91,Export to Excel
91,Publish a REST Service
91,Share Data Between Apps
91,Access a Samba Share
91,Expose Data to BI Tools Using OData
91,Configure Selenium Support
91,Execute SQL on External Database
91,Write Data to Another App
91,Use the Data Importer
91,Use the External Database Connector
91,CI/CD Pipeline for Mendix Cloud
91,Use a Client Certificate
91,Extensibility
91,Build a Pluggable Native Widget
91,Update Pluggable Widgets Tools
91,Build Pluggable Web Widgets
91,1. Build Pluggable Web Widget
91,2. Build Pluggable Web Widget
91,Build JavaScript Actions
91,1. Build JavaScript Actions
91,2. Build JavaScript Actions
91,Build JavaScript Actions for Native Mobile
91,JavaScript Actions Best Practices
91,Build Microflow Actions with Java
91,Data Storage APIs for Reusable Microflows
91,Security
91,Create a Secure App
91,Best Practices for App Security
91,Set Up Anonymous User Security
91,Content Security Policy
91,Testing
91,Test Mendix Apps Using Selenium IDE
91,Create Automated Tests with TestNG
91,Monitoring and Troubleshooting
91,Clear Warning Messages
91,Debug Java Actions
91,Debug Java Actions Remotely
91,Find the Root Cause of Runtime Errors
91,Set Log Levels
91,Monitor Mendix Using JMX
91,Solve Load and Import Errors
91,Manage App Performance
91,Manage App Performance with New Relic
91,Detect and Resolve Performance Issues
91,Populate User Types
91,Developer Portal Guide
91,Global Navigation
91,General
91,Buzz
91,Team
91,App Roles
91,Documents
91,Team Server
91,Migrate to Git
91,Settings
91,Leave and Delete an App
91,Manage Deep Links
91,Project Management
91,Epics
91,Board
91,Planning
91,Epics
91,Archive
91,Jira
91,App Insights
91,Feedback
91,Mini Surveys
91,Deployment
91,General
91,Licensing Apps
91,Secure Outgoing Connections
91,Two-Factor Authentication
91,Version Downgrade Protection
91,Iframes and Running Apps
91,Deployment Location
91,SAP BTP
91,Monitoring Environments in Mendix Apps on SAP BTP
91,SAP Destination Service
91,Use SAP Connectivity Service with REST and SOAP
91,SAP Cloud Connector
91,Application Autoscaler for SAP BTP
91,SAP Single Sign-On
91,Mendix Cloud
91,About Mendix Cloud
91,Environments
91,Environment Details
91,Migrate to Other Node
91,Studio Pro Deployment Settings
91,Licensing Mendix Cloud Apps
91,Mendix Basic Package
91,Free App to Basic Package
91,Node Permissions
91,Running Now
91,Mendix Cloud Status
91,Mendix Cloud Region
91,Scaling in Mendix Cloud
91,Custom Domains
91,Certificates
91,Maintenance Windows
91,Pipelines (Beta)
91,HTTP Request Headers
91,Restrict Incoming Access
91,Mendix IP Addresses
91,Sending Email
91,Mendix Single Sign-On
91,Webhooks
91,Siemens Insights Hub
91,Private Cloud
91,Creating a Private Cloud Cluster
91,Non-Interactive Mode
91,Storage Plans
91,Registry Configuration
91,Hosting Your Own Registry
91,Running the Mendix Operator in Global Mode
91,Running the Mendix Operator in Standard Mode
91,Deploy Mendix App
91,Retrieve Environment-Sensitive Data from a Secret Store
91,Use Velero to Back Up Namespaces
91,Use CLI to Deploy
91,CI/CD with Tekton
91,Air-gapped Tekton Installation
91,PCLM – License Manager
91,Monitor Environments
91,Migrate Data (Preview)
91,Environment Planning
91,Technical Appendix for Mendix Private Cloud
91,1. Introduction to Operators
91,2. Operator Flows
91,Upgrading Private Cloud
91,Supported Providers
91,Industrial Edge Apps
91,Cloud Foundry
91,Docker
91,Run Docker Image
91,Run with Minikube
91,On-Premises
91,On-Premises Installation Security
91,Monitoring with New Relic
91,Microsoft Windows
91,Automate Mendix Deployment
91,Deploy Mendix in MS Azure
91,MS Windows: Activate Mendix License
91,MS Windows: Update a Mendix App
91,Microsoft SQL Server
91,New Database Setup on SQL Server
91,User Setup on SQL Server
91,Database User Setup on SQL Server
91,Maintenance of SQL Server
91,Maintenance Plans for SQL Server
91,Restore Database on SQL Server
91,Troubleshooting SQL Server
91,Troubleshooting IIS
91,Unix-Like Deployment
91,Mobile App
91,Operations
91,Alerts
91,Receive Environment Status Alerts
91,Logs
91,Metrics
91,Monitoring with APM
91,AppDynamics for Mendix Cloud
91,Datadog for Mendix Cloud
91,New Relic for Mendix Cloud
91,Dynatrace for Mendix Cloud
91,Splunk for Mendix Cloud
91,Backups
91,Creating a Backup
91,Downloading a Backup
91,Restoring a Backup
91,Restoring a Backup Locally
91,Reducing Database Size
91,Portfolio Management
91,Prioritization Models
91,Export and Import Initiatives
91,Control Center Guide
91,Dashboard
91,Application Health Dashboard
91,Apps
91,Members
91,Groups
91,Company Settings
91,Company Brand
91,Security
91,Set Up an SSO (BYOIDP)
91,Cloud
91,Entitlements
91,Deployed Apps
91,Catalog
91,Portfolios
91,Private Marketplace
91,Roles & Permissions
91,Marketplace Guide
91,Marketplace Overview
91,My Marketplace
91,Using Marketplace Content
91,SISW EULA for Freeware
91,Mendix Component Partner Program
91,Creating Content
91,Create Solutions
91,Commercial Solution Partner Programs
91,Introduction to Adaptable Solutions
91,Architect Solutions
91,Apply IP Protection
91,Best Practices for Adaptability
91,Implement Solutions
91,Upgrade a Solution
91,Set Up a Solution
91,Build a Connector
91,Best Practices for Building Connectors
91,Sharing Marketplace Content
91,Governance Process
91,Modules
91,AWS Connectors
91,AWS Authentication
91,Amazon Bedrock
91,Amazon DynamoDB
91,Amazon EventBridge
91,Amazon Polly
91,Amazon RDS
91,Amazon Rekognition
91,Amazon S3
91,Amazon SageMaker
91,Amazon SES
91,Amazon SNS
91,Amazon Textract
91,Amazon Translate
91,AWS IoT SiteWise
91,AWS IoT TwinMaker
91,AWS Lambda
91,Build an AWS Connector
91,Amazon SQS
91,SAP Connectors
91,OData Connector for SAP Solutions
91,BAPI Connector for SAP Solutions
91,SAP Event Mesh Connector
91,XSUAA Connector for SAP BTP
91,SAP Logging Connector
91,SAP Fiori UI Resources
91,SAP Horizon Native UI Resources
91,Administration
91,Advanced Audit Trail
91,Advanced Audit Trail UI
91,Any Chart
91,App Switcher ⚠
91,Atlas Core
91,Atlas UI Resources ⚠
91,Audit Trail
91,Community Commons
91,Data Importer
91,Data Widgets
91,Data Grid 2
91,Gallery
91,Tree Node
91,Database
91,Database Replication
91,Deep Link ⚠
91,Email
91,Encryption
91,Excel Exporter
91,Excel Importer
91,External Database Connector
91,Forgot Password
91,Google Tag
91,Hybrid Mobile Actions ⚠
91,Image Crop
91,LDAP
91,Mendix Feedback
91,Mendix Mini Surveys
91,Mendix SSO
91,Mobile SSO
91,MQTT
91,Mx Model Reflection
91,Nanoflow Commons
91,Native Mobile AR
91,Native Mobile Resources
91,Object Handling
91,OIDC SSO
91,OpenAI
91,RAG Example Implementation
91,Vector Database Setup
91,PDF Document Generation
91,Process Queue ⚠
91,Push Notifications Connector
91,SAML
91,Unit Testing
91,User Migration
91,Web Actions
91,Workflow Commons
91,Services
91,Event Broker
91,Mendix Business Events
91,Model Creator for SAP Integrations
91,OIDC Provider
91,Pusher
91,Widgets
91,Widget CSP Overview
91,HTML/JavaScript Snippet CSP
91,Maps CSP
91,Accessibility Helper
91,Accordion
91,Auto-Load More ⚠
91,Badge
91,Badge Button
91,Barcode Scanner
91,Bootstrap Tooltip ⚠
91,Calendar
91,Carousel
91,Charts
91,Create a Basic Chart
91,Use Any Chart
91,Chart Advanced Tuning
91,Use the Charts Theme
91,Create a Dynamic Series Chart
91,Use a Chart with a REST Data Source
91,Plotly Images REST Endpoint
91,Checkbox Set Selector
91,Color Picker
91,Combo Box
91,Fieldset
91,Format String ⚠
91,Google Analytics
91,Google Maps ⚠
91,HTML Element
91,HTML/JavaScript Snippet
91,Image
91,Label Selector
91,Language Selector
91,List View Swipe ⚠
91,Maps
91,Microflow Timer
91,Mobile Device ⚠
91,Mobile Features ⚠
91,Pop-Up Menu
91,Progress Bar
91,Progress Circle
91,Pull to Refresh ⚠
91,Radio Button List
91,Range Slider
91,Rating
91,Rich Text
91,Rich Text v2.0 & Below
91,Signature
91,Simple Checkbox Set Selector
91,Slider
91,Switch
91,Tab Swipe ⚠
91,Timeline
91,Tooltip
91,Video Player
91,Partner Solutions
91,APD
91,APD Installation Guides
91,APD 3 Installation Guide
91,APM 2 Installation Guide
91,APM 1 Installation Guide
91,Prerequisites
91,Java Security Settings
91,Sizing Impact
91,Installation Steps
91,After Startup Error?
91,Constants
91,Uninstall Steps
91,Upgrade Steps
91,APM Use Cases
91,APD 3 Use Cases
91,APM 2 Use Cases
91,APM 1 Use Cases
91,APD Reference Guides
91,APD 3 Reference Guide
91,Apps
91,Dashboard
91,Environments
91,Logs
91,Long-Running Actions
91,Performance Recorder
91,Browser Recorder Results
91,Runtime Recorder Results
91,Performance Statistics
91,Settings
91,APM 2 Reference Guide
91,Apps
91,Dashboard
91,Environments
91,Logs
91,Long-Running Actions
91,Performance Recorder
91,Browser Recorder Results
91,Runtime Recorder Results
91,Performance Statistics
91,Settings
91,APM 1 Reference Guide
91,Configuration
91,Dashboard
91,Download and License
91,JVM Browser
91,Load Test Recorder
91,Log Tool
91,Measurements Tool
91,Performance Tool
91,Inserting Context Information
91,Performance Tool Results
91,Query Tool
91,Statistics Tool
91,Trap Tool
91,Triggers
91,APD Release Notes
91,ATS
91,ATS Overview
91,Introduction to ATS
91,Compatibility
91,Deployment Options
91,Maintenance
91,ATS Reference Guides
91,ATS 2 Reference Guide
91,Action
91,Administration
91,App
91,ATS Helper
91,CI/CD API
91,Data-Driven Testing
91,Desktop Recorder
91,Drop-Down
91,Function Reference
91,Local Profile
91,On-Premises Installation
91,Repository
91,Results
91,Schedule
91,Selectors
91,Supported Widgets
91,Test Case
91,Test Run
91,Compatibility Table
91,Job Configuration
91,Supported Selenium Providers
91,Test Step
91,Test Suite
91,ATS 1 Reference Guide
91,Administration
91,Configuration
91,Data Management
91,Monitoring
91,Projects
91,Scheduling
91,Test Development
91,Object Types in ATS
91,Recorder
91,Manual Test Steps
91,Standard Actions
91,Custom Actions
91,Best Practices for Writing Custom Actions
91,Selectors
91,Data-Driven Tests
91,Test Case Documentation
91,Standard Actions Reference
91,ATS Core Actions
91,Assert Equals
91,Assert Not equals
91,Concatenate String
91,Get Current DateTime String
91,Random Number
91,Random String
91,Set Return Value
91,Mendix Actions
91,"DataGrid, TemplateGrid, and ListView"
91,Click DataGrid Row
91,Find Item/Row
91,Find Item/Row (by child element)
91,Find Selected Item/Row
91,Find/Assert DataGrid Row
91,Get Item/Row Index
91,Get Row Cell Value
91,Get Total Item/Row Count
91,Get Visible Item/Row Count
91,Set ListView Search
91,Set Row Cell Value
91,Sort DataGrid
91,Dialog
91,Cancel Dialog
91,Close Dialog
91,Confirm Dialog
91,Find/Assert Dialog
91,Get Dialog Message Text
91,File Manager
91,Set File Manager
91,Generic
91,Assert Current Page
91,Assert Validation Message
91,Click Widget
91,Click Widget Button
91,Click/Doubleclick
91,Find/Assert Widget
91,Get Validation Message
91,Login
91,Logout
91,Open Application
91,GroupBox
91,Close GroupBox
91,GroupBox is Collapsed
91,Open GroupBox
91,Input
91,Assert Checkbox Value
91,Assert Value
91,Dropdown has Option
91,Get Checkbox Value
91,Get Index
91,Get Value
91,Set Checkbox Value
91,Set Value
91,Set Value (by Index)
91,Toggle Checkbox Value
91,Navigation Menu
91,Click Menu Item
91,Find/Assert Menu Item
91,System
91,Find Widget Child Node
91,Focus WebElement
91,Get Current Page Title
91,Mendix wait
91,Tab
91,Assert Active Tab Caption
91,Get Active Tab Caption
91,Mendix Marketplace Widgets Actions
91,BooleanSlider
91,Assert BooleanSlider Value
91,Get BooleanSlider Value
91,Set BooleanSlider Value
91,Toggle BooleanSlider Value
91,BootstrapRTE
91,Assert BootstrapRTE Value
91,Get BootstrapRTE Value
91,Set BootstrapRTE Value
91,Checkbox Set Selector
91,Assert Checkbox Set Selector Value
91,Find Checkbox Set Selector
91,Find Checkbox Set Selector (All)
91,Get Checkbox Set Selector Value
91,Get Checkbox Set Selector Value (All)
91,Set Checkbox Set Selector Value
91,Set Checkbox Set Selector Value
91,Toggle Checkbox Set Selector Value
91,Toggle Checkbox Set Selector Value
91,CKEditor
91,Assert CKEditor Value
91,Get CKEditor Value
91,Set CKEditor Value
91,Dropdown Div Converter
91,Click Drop-Down div Converter Drop-Down Button
91,Click Drop-Down div Converter Split Button
91,Grid Selector
91,Assert Grid Selector Value
91,Find Grid Selector Box
91,Get Grid Selector Box Value
91,Set Checkbox Set Selector Value
91,Set Grid Selector RadioButton Value
91,Toggle Grid Selector Checkbox Value
91,Input reference Selector
91,Assert InputReferenceSelector Value
91,Get InputReferenceSelector Value
91,Set InputReferenceSelector Value
91,Simple Checkbox Set Selector
91,Assert Simple Checkbox Set Selector Value
91,Find Simple Checkbox Set Selector
91,Get Simple Checkbox Set Selector Value
91,Set Simple Checkbox Set Selector Value
91,Toggle Simple Checkbox Set Selector Value
91,Selenium Actions
91,Click Coordinates
91,Execute JavaScript Integer
91,Execute Javascript String
91,Execute Javascript WebElement
91,Find
91,Find Element by CSS
91,Find Element by ID
91,Find Element by Sizzle
91,Get
91,Get Property Value
91,Get Selected Option Index
91,Get Selected Option Text
91,Get Selected Option Value
91,Get Text
91,Send Keys
91,Test Run
91,ATS How-tos
91,ATS 2 How-tos
91,(Un)Mask Your Data
91,Assert Data Grid Rows
91,Configure a Selenium Hub
91,Create a Data-Driven Test Case
91,Create a Negative Test Case
91,Create a Test Case
91,Create a Test Suite
91,Create Custom Actions
91,Create Custom Action Basics
91,Create Search Context Actions
91,CAB.11 - Find Item/Row by Unique Text Value
91,Create Unsupported Widget Actions
91,CAB.02 - Switch
91,CAB.03 - Textbox
91,CAB.05 - Reference Selector
91,CAB.07 - Radio Buttons
91,CAB.10 - AutoComplete
91,General
91,Custom Action Expense App
91,Definitions
91,Guidelines for Creating a Custom Action
91,Helpful Resources
91,Prerequisites for How-To's
91,Structure for How-To's
91,Create Maintainable Test Cases
91,Get Started
91,Increase ATS Recorder and Helper Coverage
91,Install ATS Helper and ATS Recorder
91,Link Test Cases and Suites to User Stories
91,Schedule a Test Suite/Test Case
91,Set Up Selenium Locally
91,Set Up a Local Docker Selenium Hub
91,Set Up a Local Selenium Hub
91,Set Up a Local Selenium Solution
91,Set Up a Local Selenoid Hub
91,Upload a File in Your App Using ATS
91,Browserstack Test Files
91,Use ATS in Combination with CI/CD
91,Use Precondition in Test Cases
91,ATS 1 How-tos
91,Get Started
91,Install ATS Helper and Recorder
91,Create a Test Case
91,Create a Test Suite
91,Create Custom Actions
91,Create Custom Action Basics
91,Create Search Context Actions
91,CAB.11 - Find Item/Row by Unique Text Value
91,Create Unsupported Widget Actions
91,CAB.02 - Switch
91,CAB.03 - Textbox
91,CAB.05 - Reference Selector
91,CAB.07 - Radio Buttons
91,CAB.10 - AutoComplete
91,General
91,Custom Action Expense App
91,Definitions
91,Guidelines for Creating a Custom Action
91,Helpful Resources
91,Prerequisites for How-tos
91,Structure for How-tos
91,Upload a File in Your App Using ATS
91,ATS Best Practices
91,ATS 2 Best Practices
91,Finding the Action You Need
91,Test Case Dependencies
91,ATS 1 Best Practices
91,Finding the Action You Need
91,ATS Release Notes
91,QSM
91,Catalog Guide
91,Get Started with the Catalog
91,Register Data Sources
91,Register Resources
91,Register Non-OData Resources
91,Automate Catalog Registration
91,Private Cloud/On-Premises Registration
91,OpenAPI Beta Functionality
91,Consume Data Sources
91,Consume Registered Assets
91,Manage Data Sources
91,Landscape View
91,Catalog User Roles
91,Curate Registered Assets
91,Data Accessibility and Security
91,Search in the Catalog
91,Private Mendix Platform Guide
91,Private Mendix Platform Prerequisites
91,Private Mendix Platform Quick Start Guide
91,Configuring Private Mendix Platform
91,Configuring CI/CD on Azure
91,Configuring CI/CD on Kubernetes
91,Configuring the Version Control System for Private Mendix Platform
91,Private Mendix Platform Administration Guide
91,Private Mendix Platform User Guide
91,Community Tools Guide
91,Mendix Profile
91,User Settings
91,Mendix Community
91,Set Up Your Partner Profile
91,Contribute to a GitHub Repo
91,OAuth and Scopes
91,Contribute to Mendix Docs
91,Documentation Writing Guidelines
91,Mendix Support Guide
91,Prepare Your App for Support
91,Submit a Support Request
91,App Node Requests
91,Support Ticket Priority
91,Support Escalation Process
91,Security Findings FAQ
91,Strategic Partners Guide
91,Siemens
91,Insights Hub
91,Insights Hub IIoT for Makers
91,Mendix on Insights Hub
91,Insights Hub Development Considerations
91,Insights Hub Module Details
91,Insights Hub Monitor Example
91,Insights Hub Mobile Native
91,3D Viewer
91,3D Viewer for Teamcenter
91,Use the 3D Viewer API
91,AWS
91,SAP
91,APIs and SDK
91,API Documentation
91,App Repository API
91,Authentication
91,Backups API v2
91,Build API
91,Catalog APIs
91,Client API
91,Content API
91,Deploy API v1
91,Deploy API v2
91,Deploy API v4
91,Design Properties API
91,Epics API
91,Feedback API v1 ⚠
91,Feedback API v2
91,Mendix for Private Cloud Build API
91,Mendix for Private Cloud Deploy API
91,Mendix Runtime API
91,Model SDK and Platform SDK
91,Permissions API ⚠
91,Pluggable Widgets API
91,Property Types
91,Client APIs for Pluggable Widgets
91,List Values
91,Preview Appearance APIs
91,Configuration Module API
91,Declaring Native Dependencies
91,Mendix 9
91,Property Types – Mx9
91,Client APIs for Pluggable Widgets
91,List Values – Mx9
91,Preview Appearance APIs
91,Configuration Module API – Mx9
91,Declaring Native Dependencies – Mx9
91,Mendix 8
91,Property Types – Mx8
91,Client APIs for Pluggable Widgets
91,Preview Appearance APIs
91,Compare Pluggable and Custom Widgets
91,Private Mendix Platform API Documentation
91,Private Mendix Platform Group API
91,Private Mendix Platform Marketplace API
91,Private Mendix Platform Project API
91,Private Mendix Platform User API
91,Projects API ⚠
91,Stories API ⚠
91,Team Server API ⚠
91,User Management API ⚠
91,Webhooks API
91,Webhooks for Stories/Sprints ⚠
91,SDK Documentation
91,SDK Introduction
91,SDK Use Cases
91,SDK FAQ and Troubleshooting
91,SDK Reference Guide
91,Mendix Metamodel
91,Projects in the Metamodel
91,Domain Model in the Metamodel
91,Pages in the Metamodel
91,Microflows in the Metamodel
91,JavaScript and TypeScript Resources
91,SDK How-tos
91,Set Up Your Development Environment
91,Set Up your Personal Access Token (PAT)
91,Use the Platform SDK
91,Create Your First Script
91,Create the Domain Model
91,Manipulate Existing Models
91,Change Things in the Model
91,Close the Server Connection
91,Find Things in the Model
91,Work with Load Units and Elements
91,Generate SDK Script Based on Model
91,Old SDK Versions (Below 5.0) ⚠
91,Set Up Development Environment
91,Create Your First Script
91,Studio Pro 9 Guide
91,General Info
91,System Requirements
91,Install Mendix Studio Pro
91,Configure Parallels
91,Moving from Mendix Studio Pro 8 to 9
91,Migrate From Atlas 2 To Atlas 3
91,Atlas 3 Change Summary
91,Migrate Workflow Apps
91,mx Command-Line Tool
91,MxBuild
91,Developer Tool Recommendations
91,Third-Party Licenses
91,App Modeling
91,Studio Pro Overview
91,Best Practices for Development
91,Best Practices for App Performance
91,Importing and Exporting Elements
91,Starting with App from a Spreadsheet
91,Menus
91,File Menu
91,New App
91,Open App
91,Export App Package
91,Import App Package
91,Edit Menu
91,"Find, Find Advanced and Find Usages"
91,Go to Option
91,Preferences
91,View Menu
91,Changes Pane
91,Data Hub Pane
91,Errors Pane
91,Consistency Errors
91,Page Editor Consistency Errors
91,Navigation Consistency Errors
91,Suppression Rules
91,Page Explorer
91,Stories Pane
91,App Menu
91,Create Deployment Package
91,Deploy to the Cloud
91,Run Menu
91,Edit Cloud Foundry Settings
91,Version Control Menu
91,Commit
91,History
91,Download from Version Control Server
91,Upload to Version Control Server
91,Branch Line Manager
91,Create Branch Line
91,Merge Dialog
91,Language Menu
91,Batch Replace
91,Batch Translate
91,Language Operations
91,Language Settings
91,Translating Your App Content
91,Using Translatable Validation Messages
91,App Explorer
91,App
91,App Settings
91,Configurations
91,Navigation
91,Set Up Navigation
91,System Texts
91,Modules
91,Module Settings
91,Publish Add-on and Solution Modules
91,Consume Add-on Modules and Solutions
91,UI Resources Package
91,Security
91,App Security
91,User Roles
91,Administrator
91,Demo Users
91,Anonymous Users
91,Password Policy
91,Module Security
91,Domain Model
91,Entities
91,Persistability
91,Attributes
91,Validation Rules
91,Event Handlers
91,Indexes
91,Access Rules
91,External Entities
91,Associations
91,Association Properties
91,Association Tab Properties
91,Querying Over Self-References
91,Annotations
91,Generalization vs 1-to-1 Associations
91,Creating a Basic Data Layer
91,Setting Up Data Validation
91,Pages
91,Page
91,Page Properties
91,Page Resources
91,Icon Collection
91,Image Collection
91,Layout
91,Placeholder
91,Header
91,Sidebar Toggle
91,Page Template
91,Snippet
91,Building Block
91,Menu
91,Data Containers
91,Data View
91,Grids
91,Data Grid
91,Grid Columns
91,Template Grid
91,Control Bar
91,Search Bar
91,Sort Bar
91,List View
91,Data Sources
91,Database Source
91,XPath Source
91,Context Source
91,Microflow Source
91,Nanoflow Source
91,Association Source
91,Listen to Widget Source
91,Configure Form and Show Form Items
91,Configure List and View Details on 1 Page
91,Text
91,Text
91,Label
91,Page Title
91,Structure
91,Layout Grid
91,Container
91,Group Box
91,Snippet Call
91,Tab Container
91,Scroll Container
91,Table
91,Navigation List
91,Input Elements
91,Text Box
91,Text Area
91,Drop-Down
91,Check Box
91,Radio Buttons
91,Date Picker
91,Reference Selector
91,Reference Set Selector
91,Input Reference Set Selector
91,"Images, Videos and Files"
91,Static Image
91,Dynamic Image
91,File Manager
91,Image Uploader
91,Enable End-Users to Attach Images
91,Configure File Upload and Download
91,Buttons
91,Button Properties
91,Creating a Custom Save Button
91,Menus and Navigation
91,Menu Bar
91,Simple Menu Bar
91,Navigation Tree
91,Reports
91,Report Grid
91,Report Parameter
91,Report Date Parameter
91,Date Range Field
91,Generate Report Button
91,Authentication
91,Login ID Text Box
91,Password Text Box
91,Sign-In Button
91,Validation Message
91,Charts
91,Chart Configuration
91,Chart Advanced Cheat Sheet
91,Any Chart Widgets
91,Any Chart Building Blocks
91,Any Chart Cheat Sheet
91,Properties Common in the Page Editor
91,On Click Event and Events Section
91,Application Logic
91,Microflows and Nanoflows
91,Microflows
91,Microflow Properties
91,Triggering a Microflow From a Menu Item
91,Testing Microflows with Unit Test Module
91,Error Handling in Microflows
91,Extracting and Using Sub-Microflows
91,Nanoflows
91,Nanoflow Properties
91,Error Handling in Nanoflows
91,Sequence Flow
91,Activities
91,Object Activities
91,Cast Object
91,Change Object
91,Commit Object(s)
91,Create Object
91,Delete Object(s)
91,Retrieve
91,Rollback Object
91,List Activities
91,Aggregate List
91,Change List
91,Create List
91,List Operation
91,Working with Lists in a Microflow
91,Action Call Activities
91,Java Action Call
91,JavaScript Action Call
91,Microflow Call
91,Variable Activities
91,Change Variable
91,Create Variable
91,ML Kit Activities
91,Call ML Model
91,Client Activities
91,Call Nanoflow
91,Show Message
91,Close Page
91,Download File
91,Show Home Page
91,Show Page
91,Synchronize to Device
91,Synchronize
91,Validation Feedback
91,Integration Activities
91,Call REST Service
91,Call Web Service
91,Import with Mapping
91,Export With Mapping
91,Log Message
91,Generate Document
91,Workflow Activities
91,Apply Jump-To Option
91,Workflow Call
91,Change Workflow State
91,Complete Task
91,Generate Jump-To Options
91,Retrieve Workflow Context
91,Show User Task Page
91,Show Workflow Admin Page
91,Lock Workflow
91,Unlock Workflow
91,External Object Activities
91,Delete External Object
91,Send External Object
91,Metrics Activities
91,Counter
91,Gauge
91,Increment Counter
91,Decisions
91,Decision
91,Object Type Decision
91,Merge
91,Annotation
91,Parameter
91,Loop
91,Events
91,Start Event
91,End Event
91,Error Event
91,Continue Event
91,Break Event
91,Common Properties
91,Debugging Microflows and Nanoflows
91,Debugging Microflows Remotely
91,Workflows
91,Workflow Elements
91,Workflow Parameters
91,User Task
91,Decision in Workflows
91,Parallel Split
91,Jump Activity
91,Call Microflow
91,Call Workflow
91,Workflow Properties
91,Configure Workflow Security
91,Add Workflow to Existing App
91,Jump to Different Activities
91,Workflow Versioning and Conflict Mitigation
91,Workflow for Employee Onboarding
91,Add Custom Action to Workflow Toolbox
91,Expressions
91,Unary Expressions
91,Arithmetic Expressions
91,Relational Expressions
91,Special Checks
91,Boolean Expressions
91,If Expressions
91,Mathematical Function Calls
91,String Function Calls
91,Date Creation
91,Begin-of Date Function Calls
91,End-of Date Function Calls
91,Between Date Function Calls
91,Add Date Function Calls
91,Subtract Date Function Calls
91,Trim to Date
91,To String
91,Parse Integer
91,Parse and Format Decimal Function Calls
91,Parse and Format Date Function Calls
91,Enumerations in Expressions
91,Configure String Concatenation
91,Mendix Assist
91,MxAssist Logic Bot
91,MxAssist Performance Bot
91,Performance Best Practices
91,Validation Assist
91,Resources
91,Java Actions
91,JavaScript Actions
91,Rules
91,Enumerations
91,Datasets
91,OQL
91,OQL Expressions
91,OQL Aggregation
91,OQL Functions
91,OQL CAST
91,OQL COALESCE
91,OQL DATEDIFF
91,OQL DATEPART
91,OQL LENGTH
91,OQL LOWER
91,OQL RANGEBEGIN
91,OQL RANGEEND
91,OQL REPLACE
91,OQL ROUND
91,OQL UPPER
91,OQL Operators
91,OQL Case Expression
91,OQL Parameters
91,OQL From Clause
91,OQL Group by Clause
91,OQL Limit Clause
91,OQL Order by Clause
91,OQL Select Clause
91,OQL Where Clause
91,Constants
91,Regular Expressions
91,Scheduled Events
91,Scheduled Events – Task Queue
91,Legacy Scheduled Events
91,Task Queue
91,Document Templates
91,Creating Your Own Documents
91,Data Grid (Document Template)
91,Columns (Document Template)
91,Data View (Document Template)
91,Document Template
91,Dynamic Image (Document Template)
91,Dynamic Label (Document Template)
91,Footer (Document Template)
91,Header (Document Template)
91,Line Break (Document Template)
91,Page Break (Document Template)
91,Static Image (Document Template)
91,Static Label (Document Template)
91,Style
91,Table (Document Template)
91,Row (Document Template)
91,Cell (Document Template)
91,Template Grid (Document Template)
91,Title (Document Template)
91,Data Types
91,Images
91,XPath
91,XPath Aggregate Functions
91,XPath avg
91,XPath count
91,XPath max
91,XPath min
91,XPath sum
91,XPath Constraints
91,XPath Constraint Functions
91,XPath contains
91,XPath day-from-dateTime
91,XPath day-of-year-from-dateTime
91,XPath ends-with
91,XPath false
91,XPath hours-from-dateTime
91,XPath length
91,XPath minutes-from-dateTime
91,XPath month-from-dateTime
91,XPath not
91,XPath quarter-from-dateTime
91,XPath seconds-from-dateTime
91,XPath starts-with
91,XPath string-length
91,XPath true
91,XPath week-from-dateTime
91,XPath weekday-from-dateTime
91,XPath year-from-dateTime
91,XPath Expressions
91,XPath Keywords and System Variables
91,XPath Operators
91,XPath Tokens
91,Define Access Rules Using XPath
91,Filter Data Using XPath
91,Integration
91,Message Definitions
91,JSON Structures
91,XML Schemas
91,XML Schema Support
91,Mapping Documents
91,Export Mappings
91,Import Mappings
91,Map Automatically
91,ML Model Mapping
91,Select Elements
91,XML Inheritance and Choice
91,Business Event Services
91,OData Services
91,Consumed OData Services
91,Consumed OData Service
91,Consumed OData Service Requirements
91,Published OData Services
91,Published OData Attribute
91,OData Query Options
91,OData Representation
91,Published OData Resource
91,Wrap with OData
91,REST Services
91,Consumed REST Services
91,Using a Proxy to Call a REST Service
91,Server-Side Paging and Sorting
91,Consume a REST Service
91,Published REST Services
91,Publish a REST Service
91,Published REST Service
91,Published REST Operation
91,Operation Parameters for Published REST
91,Published REST Path Parameters
91,Published REST Query Parameters
91,Published REST Resource
91,CORS Settings for Published REST Services
91,GitHub-Flavored Markdown
91,Version a REST Service
91,Generating a Published REST Resource
91,Publish Microflow as REST Operation
91,Technical Details of Published REST
91,Published REST Request Routing
91,JSON Schema for Published REST Operation
91,OpenAPI 2.0 Documentation
91,Custom Authentication Microflow Parameters
91,HttpRequest and HttpResponse System Entities
91,Images and Files with REST
91,Web Services
91,Consumed Web Services
91,Consume a Simple Web Service
91,Consume a Complex Web Service
91,Consumed Web Service
91,Numeric Formatting
91,Using a Proxy to Call a Web Service
91,Published Web Services
91,Expose a Web Service
91,Operations
91,Published Web Service
91,Test Web Services Using SoapUI
91,Machine Learning Kit
91,Using ML Kit
91,Logistic Regression Example
91,Pre-Trained ML Models
91,Design Patterns
91,Advanced Inference Design Patterns
91,Pre/Post-Processor Design Patterns
91,Version Control
91,Using Version Control
91,Merge Algorithm and Conflict Resolution
91,Git Storage Optimization
91,Troubleshoot Version Control
91,Solving Git Issues
91,Team Server Issues
91,Version Control FAQ
91,Differences Between Git and SVN
91,SVN On-Premises Version Control Server
91,Git On-Premises Version Control Server
91,Mendix Runtime
91,Runtime Server
91,Mendix Client
91,Runtime Deployment
91,Clustered Mendix Runtime
91,Communication Patterns
91,Minimizing Objects in Session
91,Data Storage
91,Attribute Type Migration
91,Case-Sensitive Database Behavior
91,Order By Behavior
91,Unlimited String Behavior
91,DB2
91,MySQL/MariaDB
91,Oracle
91,SAP HANA
91,Date and Time Handling
91,DateTime Handling FAQ
91,Logging
91,Login Behavior
91,Mendix Runtime and Java
91,Non-Persistable Objects and Garbage Collecting
91,Java Memory Usage
91,Common Runtime and Java Errors
91,Metrics
91,Monitoring Client State
91,Monitoring Mendix Runtime
91,Objects and Caching
91,Runtime Customization
91,Advanced Custom Settings
91,WebSockets
91,Mobile
91,Getting Started with Mobile
91,Prerequisites and Troubleshooting
91,Introduction to Mobile Technologies
91,Native Mobile
91,Progressive Web App
91,Hybrid Mobile (Deprecated)
91,Designing Mobile User Interfaces
91,Design Principles
91,Navigation
91,"Images, Icons, and Fonts"
91,Native Styling
91,Widget Styling Guide
91,Building Efficient Mobile Apps
91,Optimizing Native Startup
91,Offline-First Data
91,Offline Synchronization
91,Offline Best Practices
91,Synchronization & Auto-Committed Objects
91,Offline Data Security
91,Logging in Native Apps
91,Using Mobile Capabilities
91,Deep Links
91,Internationalize Mobile Apps
91,Location and Maps
91,Push Notifications
91,1. Add Module Dependencies
91,2. Push Notifications Module
91,3. Set Up Firebase Cloud Messaging
91,4. Configure Push Notifications
91,5. Push Notifications in Native App
91,6. Native App with Push Notifications
91,7. Test Push Notification
91,8. Notifications to Multiple Devices
91,Local Notifications
91,Part 1: Local Notifications
91,Part 2: Badges
91,Part 3: Actions
91,Part 4: Data
91,Part 5: Scheduling
91,Augmented Reality
91,Get Started with AR
91,Create an AR Business Card
91,App Permissions
91,Mobile Accessibility
91,"Build, Test, Distribute Apps"
91,Building Native Apps
91,Build a Mendix Native App Locally
91,Deploy Mendix Native Mobile App
91,Native App Local Manual Build
91,Creating a Custom Developer App
91,Native Template
91,Distributing Native Apps
91,Updating Native Apps
91,Debugging Native Apps
91,Testing Native Apps
91,Java Programming
91,Troubleshooting
91,Using Eclipse
91,Extending App with Custom Java
91,Using the Java API
91,Studio Pro 9 How-tos
91,Front End
91,UI Design
91,Get Started
91,Customize Styling
91,Configure Module-Level Theme Settings
91,Create a Company Design System
91,Extend Design Properties
91,Implement Best Practices for UX Design
91,Use Navigation Layouts
91,Configure Your Theme
91,Create Overview and Detail Pages
91,Use Layouts and Snippets
91,Implement Classes
91,Create Custom Error Pages
91,Data Models
91,Denormalize Data to Improve Performance
91,Share the Development Database
91,Migrate Your Mendix Database
91,Integration
91,Integrate Legacy System
91,Import XML Documents
91,Export XML Documents
91,Import Excel Documents
91,Import a Large Excel File
91,Export to Excel
91,Access a Samba Share
91,Expose Data to BI Tools Using OData
91,Configure Selenium Support
91,Execute SQL on External Database
91,CI/CD Pipeline for Mendix Cloud
91,Use a Client Certificate
91,Extensibility
91,Build a Pluggable Native Widget
91,Build Pluggable Web Widgets
91,1. Build Pluggable Web Widget
91,2. Build Pluggable Web Widget
91,Build JavaScript Actions
91,1. Build JavaScript Actions
91,2. Build JavaScript Actions
91,Build JavaScript Actions for Native Mobile
91,JavaScript Actions Best Practices
91,Build Microflow Actions with Java
91,Data Storage APIs for Reusable Microflows
91,Security
91,Create a Secure App
91,Best Practices for App Security
91,Set Up Anonymous User Security
91,Content Security Policy
91,Testing
91,Test Mendix Apps Using Selenium IDE
91,Create Automated Tests with TestNG
91,Monitoring and Troubleshooting
91,Clear Warning Messages
91,Debug Java Actions
91,Debug Java Actions Remotely
91,Debug a Hybrid Mobile Application
91,Find the Root Cause of Runtime Errors
91,Set Log Levels
91,Monitor Mendix Using JMX
91,Solve Load and Import Errors
91,Manage App Performance
91,Manage App Performance with New Relic
91,Detect and Resolve Performance Issues
91,Populate User Types
91,Studio Pro 8 Guide
91,General Info
91,System Requirements
91,Desktop Modeler 7 to Studio Pro 8
91,Troubleshooting DOM Changes
91,Troubleshooting Atlas UI Changes
91,mx Command-Line Tool
91,MxBuild
91,Developer Tool Recommendations
91,Third-Party Licenses
91,App Modeling
91,Studio Pro Overview
91,Menus
91,File Menu
91,New Project
91,Open Project
91,Export Project Package
91,Import Project Package
91,Edit Menu
91,"Find, Find Advanced, and Find Usages"
91,Go to Option
91,Preferences
91,View Menu
91,Changes Pane
91,Data Hub Pane
91,Errors Pane
91,Consistency Errors
91,Navigation Consistency Errors
91,Page Editor Consistency Errors
91,Suppression Rules
91,Project Explorer
91,Project
91,Project Settings
91,Configurations
91,Navigation
91,System Texts
91,Modules
91,UI Resources Package
91,Security
91,Project Security
91,User Roles
91,Administrator
91,Demo Users
91,Anonymous Users
91,Password Policy
91,Module Security
91,Stories Pane
91,Project Menu
91,Create Deployment Package
91,Deploy to the Cloud
91,Run Menu
91,Edit Cloud Foundry Settings
91,Version Control Menu
91,Commit
91,History
91,Download from Version Control Server
91,Upload to Version Control Server
91,Branch Line Manager
91,Create Branch Line
91,Merge Dialog
91,Language Menu
91,Language Settings
91,Batch Replace
91,Batch Translate
91,Language Operations
91,Domain Model
91,Entities
91,Persistability
91,Attributes
91,Validation Rules
91,Event Handlers
91,Indexes
91,Access Rules
91,External Entities
91,Associations
91,Association Properties
91,Association Tab Properties
91,Querying Over Self-References
91,Annotations
91,Generalization vs 1-to-1 Associations
91,Pages
91,Page
91,Page Properties
91,Page Resources
91,Image Collection
91,Layout
91,Placeholder
91,Header
91,Sidebar Toggle
91,Page Template
91,Snippet
91,Building Block
91,Menu
91,Data Widgets
91,Data View
91,Grids
91,Data Grid
91,Grid Columns
91,Template Grid
91,Control Bar
91,Search Bar
91,Sort Bar
91,List View
91,Data Sources
91,Database Source
91,XPath Source
91,Context Source
91,Microflow Source
91,Nanoflow Source
91,Association Source
91,Listen to Widget Source
91,Common Widgets
91,Text
91,Image
91,Label
91,Snippet Call
91,Page Title
91,Container Widgets
91,Layout Grid
91,Container
91,Group Box
91,Tab Container
91,Scroll Container
91,Table
91,Navigation List
91,Input Widgets
91,Text Box
91,Text Area
91,Drop-Down
91,Check Box
91,Radio Buttons
91,Date Picker
91,Reference Selector
91,Reference Set Selector
91,Input Reference Set Selector
91,File Widgets
91,File Manager
91,Image Uploader
91,Image Viewer
91,Button Widgets
91,Button Properties
91,Menu Widgets
91,Menu Bar
91,Simple Menu Bar
91,Navigation Tree
91,Report Widgets
91,Report Grid
91,Report Parameter
91,Report Date Parameter
91,Date Range Field
91,Generate Report Button
91,Authentication Widgets
91,Login ID Text Box
91,Password Text Box
91,Sign-In Button
91,Validation Message
91,Chart Widgets
91,Chart Configuration
91,Chart Advanced Cheat Sheet
91,Any Chart Widgets
91,Any Chart Building Blocks
91,Any Chart Cheat Sheet
91,Properties Common in the Page Editor
91,On Click Event and Events Section
91,Application Logic
91,Microflows
91,Microflow Properties
91,MxAssist Logic Bot
91,Nanoflows
91,Nanoflow Properties
91,Sequence Flow
91,Activities
91,Object Activities
91,Cast Object
91,Change Object
91,Commit Object(s)
91,Create Object
91,Delete Object(s)
91,Retrieve
91,Rollback Object
91,List Activities
91,Aggregate List
91,Change List
91,Create List
91,List Operation
91,Action Call Activities
91,Java Action Call
91,JavaScript Action Call
91,Microflow Call
91,Variable Activities
91,Change Variable
91,Create Variable
91,Client Activities
91,Call Nanoflow
91,Show Message
91,Close Page
91,Download File
91,Show Home Page
91,Show Page
91,Synchronize to Device
91,Synchronize
91,Validation Feedback
91,Integration Activities
91,Call REST Service
91,Call Web Service
91,Import with Mapping
91,Export With Mapping
91,Log Message
91,Generate Document
91,Decisions
91,Merge
91,Object Type Decision
91,Decision
91,Annotation
91,Parameter
91,Loop
91,Events
91,Start Event
91,End Event
91,Error Event
91,Continue Event
91,Break Event
91,Expressions
91,Unary Expressions
91,Arithmetic Expressions
91,Relational Expressions
91,Special Checks
91,Boolean Expressions
91,If Expressions
91,Mathematical Function Calls
91,String Function Calls
91,Date Creation
91,Between Date Function Calls
91,Add Date Function Calls
91,Trim to Date
91,To String
91,Parse Integer
91,Parse and Format Decimal Function Calls
91,Parse and Format Date Function Calls
91,Enumerations in Expressions
91,Common Properties
91,Resources
91,Java Actions
91,JavaScript Actions
91,Rules
91,Enumerations
91,Datasets
91,OQL
91,OQL Expressions
91,OQL Aggregation
91,OQL Functions
91,OQL CAST
91,OQL COALESCE
91,OQL DATEDIFF
91,OQL DATEPART
91,OQL LENGTH
91,OQL RANGEBEGIN
91,OQL RANGEEND
91,OQL ROUND
91,OQL Operators
91,OQL Case Expression
91,OQL Parameters
91,OQL From Clause
91,OQL Group by Clause
91,OQL Limit Clause
91,OQL Order by Clause
91,OQL Select Clause
91,OQL Where Clause
91,Constants
91,Regular Expressions
91,Scheduled Events
91,Document Templates
91,Creating Your Own Documents
91,Data Grid (Document Template)
91,Columns (Document Template)
91,Data View (Document Template)
91,Document Template
91,Dynamic Image (Document Template)
91,Dynamic Label (Document Template)
91,Footer (Document Template)
91,Header (Document Template)
91,Line Break (Document Template)
91,Page Break (Document Template)
91,Static Image (Document Template)
91,Static Label (Document Template)
91,Style
91,Table (Document Template)
91,Row (Document Template)
91,Cell (Document Template)
91,Template Grid (Document Template)
91,Title (Document Template)
91,Data Types
91,Images
91,XPath
91,XPath Constraints
91,XPath Constraint Functions
91,XPath Contains
91,XPath Day-from-DateTime
91,XPath Day-of-Year-from-DateTime
91,XPath Ends-With
91,XPath False
91,XPath Hours-from-DateTime
91,XPath Length
91,XPath Minutes-from-DateTime
91,XPath Month-From-DateTime
91,XPath Not
91,XPath Quarter-from-DateTime
91,XPath Seconds-from-DateTime
91,XPath Starts-With
91,XPath String-Length
91,XPath True
91,XPath Week-from-DateTime
91,XPath Weekday-from-DateTime
91,XPath Year-from-DateTime
91,XPath Expressions
91,XPath Keywords and System Variables
91,XPath Operators
91,XPath Query Functions
91,XPath Avg
91,XPath Count
91,XPath Max
91,XPath Min
91,XPath Sum
91,XPath Tokens
91,Integration
91,Consumed App Services
91,Select App Service
91,Settings
91,Consumed OData Services
91,Consumed OData Service
91,Consumed OData Service Requirements
91,Consumed REST Services
91,Using a Proxy to Call a REST Service
91,Consumed Web Services
91,Consumed Web Service
91,Numeric Formatting
91,Using a Proxy to Call a Web Service
91,HttpRequest and HttpResponse System Entities
91,JSON Structures
91,Mapping Documents
91,Export Mappings
91,Import Mappings
91,Map Automatically
91,Select Elements
91,XML Inheritance and Choice
91,Message Definitions
91,Published App Services
91,Actions
91,Published App Service
91,Published OData Services
91,OData Query Options
91,OData Representation
91,Published OData Resource
91,Published REST Services
91,Published REST Service
91,Published REST Operation
91,Operation Parameters for Published REST
91,Published REST Path Parameters
91,Published REST Query Parameters
91,Published REST Resource
91,CORS Settings for Published REST Services
91,GitHub-Flavored Markdown
91,Generate a Published REST Resource
91,Publish Microflow as REST Operation
91,Technical Details of Published REST
91,Published REST Request Routing
91,JSON Schema for Published REST Operation
91,OpenAPI 2.0 Documentation
91,Custom Authentication Microflow Parameters
91,Published Web Services
91,Operations
91,Published Web Service
91,XML Schemas
91,XML Schema Support
91,Version Control
91,Using Version Control in Studio Pro
91,Mendix Runtime
91,Runtime Server
91,Mendix Client
91,Runtime Deployment
91,Clustered Mendix Runtime
91,Communication Patterns
91,Data Storage
91,Attribute Type Migration
91,Case-Sensitive Database Behavior
91,Order By Behavior
91,Uniqueness Constraint Migration
91,DB2
91,MySQL/MariaDB
91,Oracle
91,SAP HANA
91,Date and Time Handling
91,DateTime Handling FAQ
91,Logging
91,Login Behavior
91,Mendix Runtime and Java
91,Non-Persistable Objects and Garbage Collecting
91,Java Memory Usage
91,Common Runtime and Java Errors
91,Monitoring Client State
91,Monitoring Mendix Runtime
91,Objects and Caching
91,Runtime Customization
91,Advanced Custom Settings
91,WebSockets
91,Mobile
91,Native Mobile
91,Getting the Make It Native App
91,Native Navigation
91,Native Mobile Styling
91,Native Builder (CLI)
91,Working with Vector Graphics
91,Hybrid Mobile
91,Customizing Hybrid Mobile Apps
91,Developing Hybrid Mobile Apps
91,Getting the Mendix Developer App
91,Offline Hybrid Mobile Apps
91,Packaging Hybrid Mobile Apps
91,Managing App Signing Keys
91,Offline-First
91,Java Programming
91,Troubleshooting
91,Using Eclipse
91,Studio Pro 8 How-tos
91,Collaboration
91,Solve Known Version Control Issues
91,Team Server Network Issues
91,Contribute to a Mendix GitHub Repository
91,Start Your Own GitHub Repository
91,Share the Development Database
91,Translate Your App Content
91,On-Premises Version Control Server
91,Front End
91,Atlas UI
91,Get Started with Atlas UI
91,Migrate Existing Apps to Atlas UI
91,Create Company Atlas UI Resources
91,Share Company Atlas UI Resources
91,Custom Preview Images
91,Customize Your Styling
91,Customize Styling Using Calypso
91,Customize Styling Using Gulp
91,Set Up Gulp and Sass
91,Start Styling with Gulp and Sass
91,Implement Best Practices for UX Design
91,Use Navigation Layouts
91,Configure Your Theme
91,Use the Charts Widgets
91,Create a Basic Chart
91,Use Any Chart
91,Chart Advanced Tuning
91,Use the Charts Theme
91,Create a Dynamic Series Chart
91,Use a Chart with a REST Data Source
91,Plotly Images REST Endpoint
91,Create Overview and Detail Pages
91,Use Layouts and Snippets
91,Implement Classes
91,Create Custom Error Pages
91,Style Google Maps
91,Mobile
91,Native Mobile
91,Get Started with Native Mobile
91,Build Native Apps
91,Deploy Mendix Native Mobile App
91,Build Local Native Mobile App
91,Local Native Mobile App Manual Build
91,Debug Native Mobile Apps (Advanced)
91,Create a Custom Developer App
91,Build Apps Using Native Builder CLI
91,Deploy Mobile App with Native Builder CLI
91,Custom Developer App with Native Builder CLI
91,Over the Air Updates with CodePush and CLI
91,Implement Native Mobile Styling
91,Style Your Mendix Native Mobile App
91,Native Mobile App UI Best Practices
91,Add Fonts to Your Native Mobile App
91,Use Notifications
91,Add Module Dependencies
91,Push Notifications Module
91,Set Up Firebase Cloud Messaging
91,Configure Push Notifications
91,Push Notifications in Native App
91,Native App with Push Notifications
91,Send Your First Test Push Notification
91,Send Notifications to Multiple Devices
91,Use Local Notifications
91,Part 1: Local Notifications
91,Part 2: Badges
91,Part 3: Actions
91,Part 4: Data
91,Part 5: Scheduling
91,Over the Air Updates with CodePush
91,Deep Links in Native Mobile Apps
91,Set Up Maps in Native Mobile Apps
91,Troubleshoot Common Native Mobile Issues
91,Hybrid Mobile
91,Build Hybrid Apps
91,Build a Mendix Hybrid App Locally
91,Publish Hybrid Mobile App in App Stores
91,Customizing Local Build Packages
91,Set Up Hybrid Push Notifications
91,Include Push Notifications
91,Implement Push Notifications
91,Send Push Notifications
91,Apple Push Notification Server
91,Test Push Notifications
91,Configure iOS Mendix Feedback Widget
91,SSO on Hybrid App with SAML
91,Debug a Hybrid Mobile App
91,Deploy Your First Hybrid Mobile App
91,Data Models
91,Create a Basic Data Layer
91,Set Up Data Validation
91,Work with Images and Files
91,Denormalize Data to Improve Performance
91,Migrate Your Mendix Database
91,Logic and Business Rules
91,Trigger a Microflow From a Menu Item
91,Create a Custom Save Button
91,Extract and Use Sub-Microflows
91,Work with Lists in a Microflow
91,Optimize Microflow Aggregates
91,Set Up Error Handling
91,Optimize Retrieve Activities
91,Define Access Rules Using XPath
91,Configure String Concatenation
91,Extend App with Custom Java
91,Use the Java API
91,Use Translatable Validation Messages
91,Filter Data Using XPath
91,Server-Side Paging and Sorting
91,Integration
91,Import and Export Objects
91,Import XML Documents
91,Export XML Documents
91,Import Excel Documents
91,Import a Large Excel File
91,Export to Excel
91,Consume a Simple Web Service
91,Consume a Complex Web Service
91,Consume a REST Service
91,Publish a REST Service
91,Access a Samba Share
91,Version a REST Service
91,Expose a Web Service
91,Expose Data to BI Tools Using OData
91,Publish Data to Other Mendix Apps Using an App Service (Deprecated)
91,Configure Selenium Support
91,Execute SQL on External Database
91,Test Web Services Using SoapUI
91,Implement CI/CD Pipeline
91,Use a Client Certificate
91,Extensibility
91,Build a Pluggable Native Widget
91,Build Pluggable Web Widgets
91,1. Build Pluggable Web Widget
91,2. Build Pluggable Web Widget
91,Build Custom Widgets
91,Build Widgets with XML
91,Preview Image for Custom Widget
91,Build JavaScript Actions
91,1. Build JavaScript Actions
91,2. Build JavaScript Actions
91,Build JavaScript Actions for Native Mobile
91,JavaScript Actions Best Practices
91,Microflow Actions Using Connector Kit
91,Data Storage APIs for Reusable Microflows
91,Security
91,Create a Secure App
91,Best Practices for App Security
91,Set Up Anonymous User Security
91,Testing
91,Test Microflows Using Unit Test Module
91,Test with ATS
91,Test Mendix Apps Using Selenium IDE
91,Create Automated Tests with TestNG
91,Monitoring and Troubleshooting
91,Clear Warning Messages
91,Debug Microflows
91,Debug Microflows Remotely
91,Debug Java Actions
91,Debug Java Actions Remotely
91,Debug a Hybrid Mobile Application
91,Find the Root Cause of Runtime Errors
91,Set Log Levels
91,Monitor Mendix Using JMX
91,Solve Load and Import Errors
91,Manage App Performance
91,Manage App Performance with New Relic
91,Detect and Resolve Performance Issues
91,General Info
91,Configure Parallels
91,Set Up the Navigation Structure
91,Minimize Objects in Session
91,Best Practices for Development
91,Best Practices for App Performance
91,Install Mendix Studio Pro
91,Mendix 7 Reference Guide
91,General
91,System Requirements
91,Moving from Modeler Version 6 to 7
91,Offline
91,MxBuild
91,Developer Tool Recommendations
91,Third-Party Licenses
91,Desktop Modeler
91,Desktop Modeler Overview
91,Application Logic
91,Microflows
91,Microflow Properties
91,Nanoflows
91,Nanoflow Properties
91,Rules
91,Common Elements
91,Activities
91,Action Call Activities
91,Java Action Call
91,Microflow Call
91,Client Activities
91,Close Page
91,Download File
91,Show Home Page
91,Show Message
91,Show Page
91,Validation Feedback
91,Document Generation Activities
91,Generate Document
91,Integration Activities
91,Call Web Service
91,Export XML
91,Import XML
91,List Activities
91,Aggregate List
91,Change List
91,Create List
91,List Operation
91,Logging Activities
91,Log Message
91,Object Activities
91,Cast Object
91,Change Object
91,Commit Object(s)
91,Create Object
91,Delete Object(s)
91,Retrieve
91,Rollback Object
91,Variable Activities
91,Change Variable
91,Create Variable
91,Annotation
91,Annotation Flow
91,Events
91,Break Event
91,Continue Event
91,End Event
91,Error Event
91,Start Event
91,Expressions
91,Add date function calls
91,Arithmetic expressions
91,Between Date Function Calls
91,Boolean expressions
91,Date Creation
91,Enumerations in Expressions
91,If expressions
91,Mathematical function calls
91,Parse and Format Date Function Calls
91,Parse and Format Decimal Function Calls
91,Parse and Format Float Function Calls
91,Parse integer
91,Relational expressions
91,Special checks
91,String Function Calls
91,To float
91,To string
91,Trim to Date
91,Unary expressions
91,Loop
91,Microflow Element Common Properties
91,Parameter
91,Sequence Flow
91,Splits
91,Exclusive Split
91,Inheritance Split
91,Merge
91,Consistency Errors
91,Navigation Consistency Errors
91,Page Editor Consistency Errors
91,Constants
91,Data Types
91,Datasets
91,OQL
91,OQL Expressions
91,OQL Aggregation
91,OQL Functions
91,OQL CAST
91,OQL COALESCE
91,OQL DATEDIFF
91,OQL DATEPART
91,OQL LENGTH
91,OQL RANGEBEGIN
91,OQL RANGEEND
91,OQL ROUND
91,OQL Operators
91,OQL Case Expression
91,OQL Parameters
91,OQL From Clause
91,OQL Group by Clause
91,OQL Limit Clause
91,OQL Order by Clause
91,OQL Select Clause
91,OQL Where Clause
91,Dialog Boxes
91,App Settings Dialog
91,Branch Line Manager Dialog
91,Commit Dialog
91,Create Branch Line Dialog
91,Create Deployment Package Dialog
91,Deploy To The Cloud Dialog
91,Download From Version Control Server Dialog
91,Edit Cloud Foundry Settings Dialog
91,Export an App Package
91,History Dialog
91,Import Project Package
91,Merge Dialog
91,Open App Dialog
91,Preferences Dialog
91,Sign In Dialog
91,Upload To Version Control Server
91,Document Templates
91,Creating Your Own Documents
91,Data Grid (Document Template)
91,Columns (Document Template)
91,Data View (Document Template)
91,Document Template
91,Dynamic Image (Document Template)
91,Dynamic Label (Document Template)
91,Footer (Document Template)
91,Header (Document Template)
91,Line break (Document Template)
91,Page Break (Document Template)
91,Static Image (Document Template)
91,Static Label (Document Template)
91,Style
91,Table (Document Template)
91,Row (Document Template)
91,Cell (Document Template)
91,Template Grid (Document Template)
91,Title (Document Template)
91,Domain Model
91,Annotations
91,Associations and Their Properties
91,Entities
91,Generalization and 1-to-1 Associations
91,Persistability
91,Attributes
91,Associations
91,Validation Rules
91,Event Handlers
91,Indexes
91,Access Rules
91,Enumerations
91,Enumeration Values
91,Images
91,Integration
91,Consumed App Services
91,Select app service
91,Settings
91,Consumed REST Services
91,Using a Proxy to Call a REST Service
91,Consumed Web Services
91,Consumed Web Service
91,Numeric formatting
91,Using a proxy to call a webservice
91,HttpRequest and HttpResponse System Entities
91,JSON Structures
91,Mapping Documents
91,Export Mappings
91,Import Mappings
91,Map Automatically
91,Select Elements
91,XML Inheritance and Choice
91,Message Definitions
91,Message Definition
91,Microflow Activities
91,Call REST Service Action
91,Call Web Service Action
91,Export Mapping Action
91,Import Mapping Action
91,Published App Services
91,Actions
91,Published App Service
91,Published OData Services
91,OData Query Options
91,OData Representation
91,Published OData Resource
91,Published REST Services
91,Published REST Service
91,Published REST Operation
91,Operation Parameters for Published REST
91,Published REST Path Parameters
91,Published REST Query Parameters
91,Published REST Resource
91,CORS Settings for Published REST Services
91,GitHub-Flavored Markdown
91,Generate a Published REST Resource
91,Publish Microflow as REST Operation
91,Technical Details of Published REST
91,Published REST Request Routing
91,JSON Schema for Published REST Operation
91,OpenAPI 2.0 Documentation
91,Custom Authentication Microflow Parameters
91,Published Web Services
91,Operations
91,Published Web Service
91,XML Schemas
91,XML Schema Support
91,Java Actions
91,Modules
91,Module Security
91,Module Role
91,Pages
91,Page Concepts
91,Conditions
91,Data Sources
91,Association Source
91,Context Source
91,Database Source
91,Listen To Widget Source
91,Microflow Source
91,XPath Source
91,On Click Event
91,Opening Pages
91,Starting Microflows
91,Authentication Widgets
91,Login Id Text Box
91,Password Text Box
91,Sign In Button
91,Validation Message
91,Building Block
91,Button Widgets
91,Action Button
91,Close Page Button
91,Create Button
91,Drop-Down Button
91,Image Property
91,Chart Widgets
91,Chart Configuration
91,Chart Advanced Cheat Sheet
91,Any Chart Widgets
91,Any Chart Building Blocks
91,Any Chart Cheat Sheet
91,Common Widgets
91,Common Widget Properties
91,Image
91,Label
91,Page title
91,Snippet Call
91,Text
91,Container Widgets
91,Container
91,Group box
91,Layout grid
91,Navigation list
91,Scroll Container
91,Scroll Container Region
91,Tab container
91,Tab page
91,Table
91,Table cell
91,Table row
91,Data Widgets
91,Data grid
91,Columns
91,Control Bar
91,Add Button
91,Delete button
91,Deselect All Button
91,Edit button
91,Export to CSV button
91,Export to excel button
91,Grid Action button
91,Grid Create Button
91,Remove button
91,Search button
91,Select All Button
91,Select button
91,Search Bar
91,Comparison Search Field
91,Drop-Down Search Field
91,Range Search Field
91,Sort Bar
91,Data view
91,List view
91,Template Grid
91,File Widgets
91,File manager
91,Image uploader
91,Image viewer
91,Input Widgets
91,Check Box
91,Date Picker
91,Drop-Down
91,Input Reference Set Selector
91,Radio Buttons
91,Reference Selector
91,Reference Set Selector
91,Text Area
91,Text Box
91,Layout Widgets
91,Header
91,Placeholder
91,Sidebar toggle button
91,Layouts
91,Menu
91,Menu Item
91,Menu Widgets
91,Menu Bar
91,Navigation Tree
91,Simple Menu Bar
91,Page
91,Page Templates
91,Report Widgets
91,Date Range Field
91,Report Button
91,Report Chart
91,Report Date Parameter
91,Report Grid
91,Report Parameter
91,Snippet
91,Projects
91,Converting to 7.4 - Navigation Profile Issues
91,Navigation Before Mendix 7.2
91,Desktop Profile
91,Hybrid Phone Profile
91,Hybrid Tablet Profile
91,Offline Device Profile
91,Phone Profile
91,Tablet Profile
91,Navigation in 7.2 and 7.3
91,Navigation Profile in 7.2 and 7.3
91,Navigation in Mendix 7.4 and Above
91,Navigation Profile
91,Project Security
91,Administrator
91,Anonymous Users
91,Demo Users
91,Module Status
91,Password Policy
91,User Roles
91,Project Settings
91,Configuration
91,System Texts
91,Regular Expressions
91,Scheduled Events
91,Security
91,Translatable Texts
91,UI Resources Package
91,XPath
91,XPath Constraints
91,XPath Constraint Functions
91,XPath contains
91,XPath day-from-dateTime
91,XPath day-of-year-from-dateTime
91,XPath ends-with
91,XPath false
91,XPath hours-from-dateTime
91,XPath length
91,XPath minutes-from-dateTime
91,XPath month-from-dateTime
91,XPath not
91,XPath quarter-from-dateTime
91,XPath seconds-from-dateTime
91,XPath starts-with
91,XPath string-length
91,XPath true
91,XPath week-from-dateTime
91,XPath weekday-from-dateTime
91,XPath year-from-dateTime
91,XPath Expressions
91,XPath Keywords and System Variables
91,XPath Operators
91,XPath Query Functions
91,XPath avg
91,XPath count
91,XPath id
91,XPath max
91,XPath min
91,XPath sum
91,XPath Tokens
91,Version Control
91,Version Control
91,Team Server
91,Team Server FAQ
91,Collaborative Development
91,Mendix Runtime
91,Clustered Mendix Runtime
91,Data Storage
91,Attributes Type Migration
91,Order By Behavior
91,Uniqueness Constraint Migration
91,DB2
91,MySQL/MariaDB
91,Oracle
91,SAP HANA
91,Date and Time Handling
91,DateTime Handling FAQ
91,Logging
91,Login Behavior
91,Mendix Runtime and Java
91,Transient Objects and Garbage Collecting
91,Java Memory Usage
91,Common Runtime and Java Errors
91,Monitoring client state
91,Monitoring Mendix Runtime
91,Objects and Caching
91,Runtime Customization
91,Advanced Custom Settings
91,SIG–Mendix Performance Subjects
91,Java Programming
91,Troubleshooting
91,Using Eclipse
91,Mobile Development
91,Customizing Hybrid Mobile Apps
91,Customizing Local Build Packages
91,Developing Hybrid Mobile Apps
91,Getting the Mendix Developer App
91,Managing App Signing Keys
91,Offline Hybrid Mobile Apps
91,Packaging Hybrid Mobile Apps
91,Mendix 7 How-tos
91,General
91,Install the Mendix Desktop Modeler
91,Show the Project Directory in Explorer
91,Best Practices for Development
91,Best Practices for App Performance
91,Find Your Way in an App
91,Find Unused App Items
91,Minimize Objects in Session
91,Set Up the Navigation Structure
91,Front End
91,Atlas UI
91,Get Started with Atlas UI
91,Migrate Existing Apps to Atlas UI
91,Create Company Atlas UI Resources
91,Share Company Atlas UI Resources
91,Custom Preview Images
91,Implement Best Practices for UX Design
91,Configure Your Theme
91,Create Overview and Detail Pages
91,Use Layouts and Snippets
91,Implement Classes
91,Use Gulp and Sass
91,Set Up Gulp and Sass
91,Start Styling with Gulp
91,Sass
91,Create Custom Error Pages
91,Style Google Maps
91,Data Models
91,Create a Basic Data Layer
91,Set Up Data Validation
91,Work with Object Events
91,Work with Images and Files
91,Query Over Self-References
91,Denormalize Data to Improve Performance
91,Migrate Your Mendix Database
91,Logic and Business Rules
91,Your First Microflow
91,Trigger Logic Using Microflows
91,Create a Custom Save Button
91,Drag App Documents into Microflow
91,Extract and Use Sub-Microflows
91,Work with Lists in a Microflow
91,Optimize Microflow Aggregates
91,Set Up Error Handling
91,Optimize Retrieve Activities
91,Define Access Rules Using XPath
91,Configure String Concatenation
91,Use the Java API
91,Find Object Activities
91,Use Translatable Validation Messages
91,Filter Data Using XPath
91,Mobile Development
91,Include Push Notifications
91,Implement Push Notifications
91,Send Push Notifications
91,Apple Push Notification Server
91,Set Up Firebase Cloud Messaging
91,Test the Implementation
91,Configure iOS Mendix Feedback Widget
91,SSO on Hybrid App with SAML
91,Debug a Hybrid Mobile App
91,Deploy Your First Hybrid Mobile App
91,Publish Hybrid App in App Stores
91,Security
91,Create a Secure App
91,Best Practices for App Security
91,Set Up Anonymous User Security
91,Integration
91,Integrate Legacy System
91,Import and Export Objects
91,Import XML Documents
91,Export XML Documents
91,Import Excel Documents
91,Import a Large Excel File
91,Consume a Simple Web Service
91,Consume a Complex Web Service
91,Consume a REST Service
91,Publish a REST Service
91,Version a REST Service
91,Expose a Web Service
91,Publish Data Using App Service
91,Configure Selenium Support
91,Execute SQL on External Database
91,Test Web Services Using SoapUI
91,Implement CI/CD Pipeline
91,Use a Client Certificate
91,Extensibility
91,Microflow Actions Using Connector Kit
91,Data Storage APIs for Reusable Microflows
91,Access a Samba Share
91,Use the Charts Widgets
91,Create a Basic Chart
91,Use Any Chart
91,Chart Advanced Tuning
91,Use the Charts Theme
91,Create a Dynamic Series Chart
91,Use a Chart with a REST Data Source
91,Plotly Images REST Endpoint
91,Widget Development
91,Adobe Brackets Widget Development Plugin
91,Preview Image for Custom Widget
91,Scaffold Widget with Yeoman
91,Use XML in Widget Development
91,Testing
91,Test Microflows Using Unit Test Module
91,Test with ATS
91,Test Apps Using Selenium IDE
91,Create Automated Tests with TestNG
91,Monitoring and Troubleshooting
91,Clear Warning Messages
91,Debug Microflows
91,Debug Microflows Remotely
91,Handle Common Mendix SSO Errors
91,Debug Java Actions
91,Debug Java Actions Remotely
91,Find the Root Cause of Runtime Errors
91,Set Log Levels
91,Monitor Mendix Using JMX
91,Solve Load and Import Errors
91,New Relic App Performance
91,Detect and Resolve Performance Issues
91,Collaboration and Requirements Management
91,Solving Known Version Control Issues
91,Team Server Network Issues
91,Contribute to a GitHub Repository
91,Start Your Own Repository
91,Share the Development Database
91,Translate Your App Content
91,On-Premises Version Control Server
91,OData Query Options
91,1 Introduction
91,2 Retrieving Objects
91,2.1 Retrieving All Objects
91,2.2 Retrieving a Single Object
91,3 Counting the Number of Objects
91,3.1 Retrieving a Count of Objects
91,3.2 Inline Count
91,4 Filtering
91,4.1 Passing attributes
91,4.2 Comparison Operators
91,4.3 Functions
91,4.4 Combining Filters
91,4.5 Arithmetic Operators
91,5 Sorting
91,6 Selecting fields
91,7 Paging
91,7.1 Top (Limit)
91,7.2 Skip (Offset)
91,8 Null Literals
91,Docs
91,Mendix 7 Reference Guide
91,Desktop Modeler
91,Integration
91,Published OData Services
91,OData Query Options
91,"Mendix 7 is no longer supported unless you have Extended Support (for details, please contact Mendix Support). Mendix 7 documentation will remain available for customers with Extended Support until July, 2024."
91,OData Query Options
91,"Last modified: June 14, 2023"
91,1 Introduction
91,This is a list of query options for OData.
91,We currently only support the options described here.
91,2 Retrieving Objects
91,2.1 Retrieving All Objects
91,All objects can be retrieved by specifying the URI. For example: /odata/myservice/myresource. You can see this if you specify the URI in a browser.
91,2.2 Retrieving a Single Object
91,A single object can be retrieved by passing the object identifier in the URI. For example: /odata/myservice/myresource(8444249301330581).
91,3 Counting the Number of Objects
91,3.1 Retrieving a Count of Objects
91,"You can find out how many objects there are by passing the $count query option. In this case, the result is an integer which is the number of objects. For example: /odata/myservice/myresource/$count."
91,3.2 Inline Count
91,"By setting the $inlinecount query option to ‘allpages’, a count of the number of items returned will be included in the result. For example: ?$inlinecount=allpages."
91,4 Filtering
91,Filters are applied by appending a $filter=... parameter to the request. For example: /Employees?$filter=Name eq 'John'.
91,4.1 Passing attributes
91,This table describes how to pass values for different attribute types:
91,Type
91,How to Pass
91,String and Enumeration
91,"Enclosed in single quotes (for example, ‘John’)"
91,Datetime
91,"Preceded with datetime and enclosed in single quotes (for example, datetime'2015-01-01’ or datetime’<epoch value here>’)"
91,Other
91,"Plain value (for example, 15)"
91,4.2 Comparison Operators
91,We support the following comparison operators:
91,Operator
91,Meaning
91,Example
91,equals
91,/Employees?$filter=Name eq 'John'
91,does not equal
91,/Employees?$filter=Name ne 'John'
91,greater than
91,/Employees?$filter=Age gt 15
91,less than
91,/Employees?$filter=Age lt 15
91,greater than or equal to
91,/Employees?$filter=Age ge 15
91,less than or equal to
91,/Employees?$filter=Age le 15
91,4.3 Functions
91,Function
91,Example
91,Returns
91,substringof
91,"/Employees?$filter=substringof('f', Name)"
91,All employees with names that contain an ‘f’
91,endswith
91,"/Employees?$filter=endswith(Name, 'f')"
91,All employees with names that end with ‘f’
91,startswith
91,"/Employees?$filter=startswith(Name, 'f')"
91,All employees with names that start with ‘f’
91,length
91,/Employees?$filter=length(Name) eq 5
91,All employees with names that have a length of 5
91,year
91,/Employees?$filter=year(DateOfBirth) eq 1990
91,All employees born in the year 1990
91,month
91,/Employees?$filter=month(DateOfBirth) eq 5
91,All employees born in May
91,day
91,/Employees?$filter=day(DateOfBirth) eq 31
91,All employees born on the 31st day of the month
91,hour
91,/Employees?$filter=hour(Registration) eq 13
91,All employees registered between 13:00 (1 PM) and 13:59 (1:59 PM)
91,minute
91,/Employees?$filter=minute(Registration) eq 55
91,All employees registered on the 55th minute of any hour
91,second
91,/Employees?$filter=second(Registration) eq 55
91,All employees registered on the 55th second of any minute of any hour
91,4.4 Combining Filters
91,"Filters can be combined with and, or, not, and (). For example: ?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)."
91,Combination
91,Example
91,and
91,/Employees?$filter=Name eq 'John' and Age gt 65
91,/Employees?$filter=Age gt 65 or Age lt 11
91,not
91,/Employees?$filter=not(Name eq 'John')
91,( )
91,/Employees?$filter=Name eq 'John' and (Age gt 65 or Age lt 11)
91,4.5 Arithmetic Operators
91,"The use of arithmetic operators such as add, sub, mul, div, and mod in filter expressions is not supported."
91,5 Sorting
91,You can sort the result using the $orderby query option. For example: ?$orderby=Name.
91,"The default direction is ascending, and you can make this explicit. For example: ?$orderby=Name asc."
91,You can also order the result in a descending direction. For example: ?$orderby=Name desc.
91,"It is possible to sort on multiple attributes, which have to be comma-separated. For example: ?$orderby=Name, Age desc."
91,6 Selecting fields
91,"You can select which attributes and associations to return by specifying the $select query option. For example: ?$select=Name,Age."
91,7 Paging
91,7.1 Top (Limit)
91,"You can limit the number of returned objects using the $top query option, where the limit is a positive integer. For example: ?$top=100."
91,7.2 Skip (Offset)
91,"You can skip a number of objects before retrieving the result using the $skip query option, where the offset is a positive integer. For example: ?$skip=100 will return objects starting with the 101st object in the list."
91,8 Null Literals
91,You can compare values against the null literal. For example: ?$filter=Name eq null.
91,"In this example, Name is a string attribute that can have no assigned value in the database. Note that null means no value as opposed to '' (which is an empty string)."
91,"When you filter against associations, null literals can be quite useful. For example: ?$filter=Association_A_B ne null. In this example, you query for objects of entity type A that have at least one association set to objects of entity type B."
91,Documentation licensed under CC BY 4.0
91,© Mendix Technology BV 2024. All rights reserved
91,Mendix.com
91,Terms of Use
91,Privacy Policy
91,EU Digital Services Act Notice
92,How to Update Multiple Columns in SQL: Efficient Techniques and Tips - SQL Knowledge Center
92,SQL
92,SQL Server
92,SQLite
92,PostgreSQL
92,MySQL
92,T-SQL
92,Tools
92,About
92,Learn SQL for Free
92,How to Update Multiple Columns in SQL: Efficient Techniques and Tips
92,By Cristian G. Guasch • Updated: 06/28/23 • 19 min read
92,"Updating multiple columns in SQL is a crucial skill for database management, particularly when dealing with large amounts of data. By learning how to update multiple columns at once, one can save time, enhance efficiency, and maintain data integrity. In this article, we’ll dive into the process of updating multiple columns in SQL, covering the syntax and techniques in a clear and concise manner."
92,Chat with SQL Databases using AI
92,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
92,Start Free Trial
92,"Well-structured SQL queries are essential in not only updating multiple columns but also in ensuring data accuracy. Familiarity with the UPDATE statement and the SET clause can significantly improve your proficiency in handling SQL queries. With the right syntax, it’s possible to update specific records, apply functions or calculations, or even incorporate data from other tables."
92,"In the following sections, we’ll walk through practical examples and scenarios to demonstrate how to update multiple columns in SQL effectively. By mastering these techniques, you can confidently tackle complex database manipulation tasks while maintaining the integrity of your data."
92,Understanding SQL Basics
92,"SQL, or Structured Query Language, is a widely used language for managing relational databases. It allows users to create, read, update, and delete data (known as CRUD operations) within these databases. One common task is updating data in database columns. This section will help readers understand the basics of updating multiple columns in SQL."
92,"To update multiple columns in SQL, one will need a strong grasp of CRUD operations, particularly the “update” command. This operation ensures that data in a database remains accurate and current. Updating a single column in SQL is fairly straightforward, but sometimes, it’s necessary to update multiple columns simultaneously."
92,"In a typical database, information is organized into tables with rows and columns. Each row represents a record, while columns store specific attributes for each record. Here are the primary steps required for updating multiple columns:"
92,Identify the table to be updated.
92,Specify the new data values for each targeted column.
92,"Apply a condition, if necessary, to fine-tune the affected records."
92,The basic syntax for the UPDATE command is as follows:
92,UPDATE table_name
92,"SET column1 = value1, column2 = value2, ..."
92,WHERE condition;
92,Some key elements to remember when updating multiple columns in SQL include:
92,The UPDATE keyword specifies the table to be updated.
92,"The SET keyword sets new values for multiple columns, followed by the = operator and the new value."
92,Each column-value pair must be separated by commas.
92,"The WHERE keyword is optional, but helps narrow down the records that will be updated by applying specific conditions."
92,Consider the following example to better understand the process of updating multiple columns in SQL:
92,<table><tr><th>ID</th><th>First_Name</th><th>Last_Name</th><th>Age</th></tr><tr><td>1</td><td>John</td><td>Doe</td><td>30</td></tr><tr><td>2</td><td>Jane</td><td>Doe</td><td>25</td></tr></table>
92,"To update both the First_name and Age columns, the SQL command would look like this:"
92,UPDATE users
92,"SET First_Name = 'Jonathan', Age = 31"
92,WHERE ID = 1;
92,"After executing the command, the table would look like this:"
92,<table><tr><th>ID</th><th>First_Name</th><th>Last_Name</th><th>Age</th></tr><tr><td>1</td><td>Jonathan</td><td>Doe</td><td>31</td></tr><tr><td>2</td><td>Jane</td><td>Doe</td><td>25</td></tr></table>
92,Grasping these basics of SQL is essential for effectively updating multiple columns and accomplishing various database-related tasks.
92,Multiple Column Update Syntax
92,"Enabling your ability to update multiple columns in SQL can significantly improve efficiency and organization within your database. The process is quite simple, and understanding its syntax is crucial when working with multiple columns. This section will delve into the fundamentals, starting with the basic SQL update statement and progressing to multi-column updates."
92,"To update a single column in a table, you’d typically use the SQL UPDATE statement like this:"
92,UPDATE table_name
92,SET column1 = value1
92,WHERE condition;
92,"However, when it comes to updating multiple columns, the syntax changes slightly. You’ll need to set each column to its respective value, separated by commas:"
92,UPDATE table_name
92,"SET column1 = value1,"
92,"column2 = value2,"
92,...
92,columnN = valueN
92,WHERE condition;
92,"Notice that the column-value pairs are separated by commas, ensuring that the database system understands which values correspond to which columns."
92,"To make the process more comprehensive, here’s a breakdown of the multiple column update syntax components:"
92,UPDATE: This is the keyword that initiates the update process in your SQL query.
92,table_name: Replace this with the name of the table you wish to perform the update on.
92,SET: This keyword indicates that column values will be updated to new specified values.
92,"column1 = value1, column2 = value2, ..., columnN = valueN: The list of column-value pairs to update. Remember to separate each pair with a comma."
92,WHERE: This keyword is followed by a condition that must be satisfied for the update process to take place.
92,"To further clarify the concept, let’s take a look at an example. Suppose you have a table called employees with columns for first_name, last_name, and email. If you want to update the email address and last name of an employee with an employee_id of 101, your SQL query might look something like this:"
92,UPDATE employees
92,"SET last_name = 'Doe',"
92,email = 'johndoe@example.com'
92,WHERE employee_id = 101;
92,"This update query modifies both the last_name and email columns in one single statement, demonstrating the effectiveness of the multiple column update syntax in SQL."
92,Chat with SQL Databases using AI
92,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
92,Start Free Trial
92,Using the SET Clause Effectively
92,"When working with update multiple columns SQL, it’s crucial to use the SET clause effectively for achieving the desired results. The SET clause helps in updating more than one column at a time, making it an efficient tool for database management."
92,A typical scenario in which one might need to update multiple columns includes changes in product prices and stock levels. Consider the following example:
92,UPDATE products
92,"SET price = price * 1.1, stock_level = stock_level - 5"
92,WHERE product_id = 101;
92,"In this example, the price for product 101 gets increased by 10% and the stock level is reduced by 5. The SET clause works simultaneously on both price and stock_level columns, simplifying the overall task."
92,Here are some useful tips when working with the SET clause to update multiple columns in SQL:
92,"Use single quotes around strings and date values, as some databases require them. For example: UPDATE employees SET first_name = 'John', hiring_date = '2021-10-01' WHERE employee_id = 1;"
92,"Combine the SET clause with the WHERE clause to update specific records. This way, one can avoid unintended modifications on other rows."
92,"Use mathematical expressions to update columns with numbers, like the earlier example with price and stock level adjustments."
92,"To update multiple columns based on values from other columns, use the subquery method. For example: UPDATE employees e SET (e.salary, e.bonus) = ( SELECT d.salary, d.bonus FROM departments d WHERE d.department_id = e.department_id ); In this example, the employee’s salary and bonus columns get updated with the values from the corresponding department."
92,"When attempting to update multiple columns SQL, it’s essential to understand the importance of the SET clause. Implementing these tips can assist in making the most out of this powerful SQL feature and maintaining the integrity of the database."
92,Joining Tables During an Update
92,"Joining tables during an update becomes necessary when you need to update multiple columns in SQL by referencing data from other tables. In this section, we’ll explore the process of updating multiple columns in SQL by joining tables. We’ll also discuss the benefits of using joined tables during an update operation."
92,"When updating multiple columns, SQL provides the ability to join tables so that data from one table can easily be used to update data in another table. The process usually involves three key steps: specifying the tables to join, defining the join condition, and setting the new values for the columns."
92,"To start with, the user should specify the tables to join using the FROM clause. This tells the database which tables will be involved in the update. The next step is defining the join condition or criteria by which the tables will be matched. This is often done using the ON keyword in conjunction with a common column or key between the two tables. Lastly, the user sets the new values for the columns to be updated using the SET clause, which may involve referencing one or more columns from the joined table."
92,Here’s an example of how to update multiple columns in SQL by joining tables:
92,UPDATE table1
92,"SET table1.column1 = table2.columnA,"
92,table1.column2 = table2.columnB
92,FROM table1
92,JOIN table2 ON table1.column_key = table2.column_key;
92,In this example:
92,table1 and table2 are the tables being joined.
92,column_key is the common column used to join the tables.
92,column1 and column2 in table1 are being updated with corresponding values from columnA and columnB in table2.
92,"There are several benefits to updating multiple columns in SQL using joined tables, such as:"
92,"Efficiency: Joining tables during an update can result in a more efficient query, as the database is able to perform the operation in a single pass."
92,"Data consistency: By updating multiple columns in a single query, the risk of inconsistencies between the columns is reduced."
92,"Readability: A joined update statement can be easier to read and understand, as it succinctly expresses the relationships between the columns and tables involved."
92,"In conclusion, joining tables during an update is an essential technique when updating multiple columns in SQL. It allows for greater efficiency, data consistency, and readability when working with multiple tables and columns. By understanding how to join tables and use the appropriate keywords, users can effectively update multiple columns in SQL to ensure their data remains accurate and up-to-date."
92,Working with Coalesce and Nulls
92,"When updating multiple columns in SQL, it’s essential to understand how to work with Coalesce and Null values. These concepts are important when dealing with data that may have missing or incomplete information."
92,Coalesce is a handy SQL function that returns the first non-null value in a list of expressions. It’s particularly useful when updating multiple columns where some values may be missing or undefined. Here’s a simple example of using Coalesce in an update statement:
92,UPDATE employees
92,"SET salary = COALESCE(salary, 0),"
92,"bonus = COALESCE(bonus, 0);"
92,"In the example above, if the employee’s salary or bonus is null, it will be updated to 0. Using Coalesce ensures data consistency and prevents errors when performing calculations on these columns."
92,Working with null values is also crucial when updating multiple columns in SQL. A null value indicates that the data in a specific column does not exist or is unknown. It’s important to handle null values properly to avoid data discrepancies and maintain data integrity. Here are some common techniques to handle null values when updating multiple columns:
92,"Use the IS NULL condition: When updating columns where null values should be preserved or replaced, use the IS NULL condition in the WHERE clause. For example:"
92,UPDATE products
92,SET price = price * 1.1
92,WHERE price IS NOT NULL;
92,"Use the NULLIF function: The NULLIF function compares two expressions and returns null if they are equal; otherwise, it returns the first expression. This can be helpful when updating columns to avoid overwriting valuable data. Consider the following example:"
92,UPDATE orders
92,"SET discount = NULLIF(discount, 0);"
92,"In this case, the discount column will be updated to null if it’s currently 0, effectively removing any 0-value discounts."
92,"Combining Coalesce with other functions: It’s possible to use Coalesce along with other SQL functions to create more complex update statements. For instance, when updating multiple columns and calculating averages, you can use AVG and Coalesce together:"
92,UPDATE departments
92,"SET avg_salary = COALESCE(AVG(salary), 0)"
92,FROM employees
92,WHERE departments.id = employees.department_id;
92,"In summary, understanding and properly using Coalesce and null values is vital when updating multiple columns in SQL. It ensures data consistency and improves overall data integrity in the database."
92,Conditional Updates with CASE Statements
92,"When working with SQL databases, it’s often necessary to update multiple columns at once. The UPDATE statement is commonly used for this purpose, and when paired with the versatile CASE statement, you can make conditional updates based on specified criteria."
92,"For those looking to update multiple columns in SQL with conditional data, here’s an example of how the UPDATE and CASE statements can be combined:"
92,UPDATE employees
92,SET
92,salary = CASE
92,WHEN position = 'Manager' THEN salary * 1.10
92,WHEN position = 'Employee' THEN salary * 1.05
92,ELSE salary
92,"END,"
92,bonus_points = CASE
92,WHEN position = 'Manager' THEN bonus_points + 100
92,WHEN position = 'Employee' THEN bonus_points + 50
92,ELSE bonus_points
92,END
92,WHERE department_id = 1;
92,"In this example, the following changes are made based on the employee’s position:"
92,"For managers, their salary is increased by 10% and their bonus points by 100."
92,"For regular employees, the salary is raised by 5% and 50 bonus points are added."
92,Some essential points to remember when using the UPDATE statement with CASE expressions include:
92,You can specify multiple updates with comma-separated statements within the SET clause.
92,The WHEN keyword is used to define specific conditions on which the update will occur.
92,The THEN keyword denotes the new value for the specific column after the update.
92,"In case none of the conditions match, the ELSE keyword allows specifying a default value to be assigned."
92,"One of the benefits of the CASE statement is its flexibility, allowing you to handle more complex update scenarios. You can even nest CASE statements to create further subdivisions within the data. For instance:"
92,UPDATE orders
92,SET
92,shipping_cost = CASE
92,WHEN origin_country = 'US' THEN
92,CASE
92,WHEN destination_country = 'US' THEN 5
92,ELSE 15
92,END
92,ELSE 20
92,END;
92,"In this case, shipping costs are adjusted based on the origin and destination countries:"
92,"If both the origin and destination countries are in the US, the shipping cost is $5."
92,"If the origin country is in the US and the destination is outside, the shipping cost is $15."
92,"For shipments with origins outside the US, the shipping cost is set to $20."
92,"To sum up, combining the UPDATE statement with CASE expressions is a powerful way to update multiple columns in SQL based on specific conditions. It provides flexibility, control, and efficiency in managing database updates."
92,Safety Measures: Testing and Transactions
92,"When working with update multiple columns SQL queries, it’s essential to prioritize safety; after all, modifying your data is an irreversible operation. To better safeguard the integrity of your data, this section delves into testing methods and utilizing transactions while updating data in SQL databases."
92,"Before committing to major changes, experts recommend performing a thorough test run. This means creating and executing test scripts that mimic real-world situations to ensure the SQL query works as intended. One proven approach involves using a testing environment that accurately imitates the production database. In this way, you can:"
92,Minimize the likelihood of unintended consequences
92,Identify any missing data elements
92,Test the accuracy of your query logic
92,Learn how the system will respond after updating multiple columns.
92,"By conducting comprehensive tests, you’ll increase the safety of updating multiple columns in SQL."
92,"Another important aspect of maintaining a secure database is leveraging transactions. In SQL, transactions allow users to group one or more related modifications, ensuring all changes either succeed together or fail together. Effectively using transactions can prevent data inconsistencies and maintain the integrity of the database. Consider these transaction best practices:"
92,Use the BEGIN TRANSACTION statement to initiate a new transaction.
92,Execute your UPDATE statement(s) to modify the data.
92,"Review the data changes and consider the impact. If updates meet expectations, use the COMMIT statement to apply the changes. However, if something goes awry, issue the ROLLBACK command to undo the changes."
92,Here’s an example of a transaction while updating multiple columns in an SQL query:
92,BEGIN TRANSACTION;
92,UPDATE tablename
92,"SET column1 = new_value1, column2 = new_value2, ..."
92,WHERE condition;
92,-- Check the data changes and if they are correct
92,COMMIT;
92,"-- If the data changes are not correct, use ROLLBACK instead"
92,"In summary, safeguarding your database while updating multiple columns in SQL involves testing and using transactions. Establishing a robust testing environment and employing transactions ensure that your data remains stable and consistent throughout the update process, allowing for both peace of mind and sound data management."
92,Pitfalls and Common Mistakes
92,"When working with update multiple columns SQL queries, it’s crucial to be aware of the potential pitfalls and common mistakes that can arise. By understanding these issues, you’ll be more likely to avoid them and ensure a smooth process when updating multiple columns in your SQL databases."
92,"Firstly, it’s important to ensure that you’re using the correct syntax for updating multiple columns. In SQL, the correct way to do this is by listing each column and its new value, separated by commas. Here’s an example of the correct syntax:"
92,UPDATE tablename
92,"SET column1 = value1, column2 = value2, column3 = value3"
92,WHERE condition;
92,Failure to follow this syntax can lead to errors or unwanted results in your updates.
92,"Secondly, some developers mistakenly use multiple UPDATE statements for each column they want to update. This approach can have unexpected consequences, such as:"
92,Poor performance and slower execution time
92,Difficulty in maintaining and debugging your code
92,"Inappropriate locking of rows, causing deadlocks"
92,"It’s important to use a single UPDATE statement for each row you’re updating, with multiple columns listed within that statement."
92,"When updating multiple columns, there are specific scenarios that can lead to confusion, such as:"
92,"Updating columns based on other columns within the same table. This can be achieved using subqueries or self-joins; however, care must be taken to ensure the correct data is being updated and no unintended results are introduced."
92,"Dealing with NULL values. When working with update multiple columns SQL, handling NULL values can be tricky and lead to unexpected outcomes. It’s essential to be mindful of how NULL values affect your updates and which functions or operations can be used to handle them correctly."
92,Some common mistakes while working with update multiple columns SQL include:
92,Mismatched data types: Ensure that the data types of the values specified in the update match the data types of the columns being updated.
92,"Insufficient or incorrect use of conditions: When updating multiple columns, make sure the WHERE clause is correctly used to update only the desired rows. Otherwise, you may unintentionally modify too many rows or the wrong rows."
92,"In conclusion, be aware of the potential pitfalls and common mistakes when working with update multiple columns SQL queries. By understanding these issues and using the correct syntax, you’ll significantly increase your chances of a successful and trouble-free experience when updating multiple columns in your SQL databases."
92,Performance Considerations
92,"When working with update multiple columns SQL queries, it’s important to take performance considerations into account. Efficiency should always be a priority to ensure that databases are running smoothly, and the time for executing queries is minimized. This section highlights a few crucial aspects that can influence the performance of your SQL queries and provides guidance on optimizing these operations."
92,The performance of an update operation on multiple columns relies on several factors:
92,"Number of columns: The more columns you update, the higher the potential impact on performance. Thus, it’s important to only update the necessary columns."
92,"Indexes: If your query is working with indexed columns, the performance might be affected due to the database needing to rebuild indexes."
92,"Transaction locks: When updating multiple columns, database transactions might be locked for an extended period, potentially creating delays for other pending transactions."
92,"To enhance the efficiency of multiple column updates in SQL, you can implement the following practices:"
92,Limit the number of updated columns: Focus on updating only essential columns to reduce the potential impact on performance.
92,"Use targeted queries: Rather than updating all rows, narrow down specific rows using the WHERE clause in your update query. This approach can significantly decrease the number of affected rows, decreasing the query execution time."
92,"Update in batches: If you’re dealing with a huge volume of data, consider updating the table in smaller chunks instead of attempting to update all rows at once."
92,Optimize database indexes: Regularly check the health of your indexes and possibly reorganize or rebuild them if needed. Sick indexes can significantly impair your query performance.
92,"As a database administrator or developer, always keep an eye on the performance implications of SQL queries. When working with update multiple columns SQL, be aware of these potential bottlenecks and use optimization practices, like minimizing the number of updated columns, using targeted queries, batch updates, and index maintenance. By caring for these aspects, you’ll be able to maintain your database operations running efficiently and smoothly."
92,Conclusion
92,"Updating multiple columns in SQL is a common task in database management. It’s important for professionals to understand how this process works and to utilize best practices when writing SQL queries. In this article, the reader discovered the various methods used to update multiple columns in their SQL databases."
92,The process of updating multiple columns in SQL can be quite straightforward. Here are the main takeaways from the article:
92,Utilize the UPDATE statement to make changes to data in SQL tables.
92,Combine column names and new values with the SET clause to indicate which data needs to be updated.
92,Implement the WHERE clause for specifying specific rows that require updating.
92,Take advantage of JOINS for updating data from other tables.
92,"By using these techniques and keeping the best practices in mind, database administrators and developers can efficiently update multiple columns in SQL and manage their data effectively. Learning these skills is essential for maintaining organized and up-to-date databases, ultimately contributing to the overall efficiency and success of any project or business that relies on data management."
92,Chat with SQL Databases using AI
92,"AskYourDatabase allows you to chat with your SQL & NoSQL databases to gain insights, visualize data, design table schemas, and data anlysis. Compatible with MySQL, PostgreSQL, MongoDB, and SQL Server."
92,Start Free Trial
92,Related articles
92,How to Learn SQL JOIN Types Explained with Visualization
92,How to Use AVG in SQL
92,How to Use Dates in SQL
92,How to CREATE VIEW in SQL
92,How to Use AUTO INCREMENT in SQL
92,How to Use the SQL Default Constraints
92,How to Use the SQL Check Constraint
92,How to Use DENSE_RANK() in SQL
92,How to Use PRIMARY KEY in SQL
92,How to Use Unique Alter Table in SQL
92,How to Use ROW_NUMBER & OVER() in SQL
92,How to Use Unique Constraint in SQL
92,How to Concatenate Two Columns in SQL?
92,How to Include Zero in a COUNT() Aggregate
92,"What Are DDL, DML, DQL, and DCL in SQL?"
92,What is an SQL Inline Query?
92,What Is the Benefit of Foreign Keys in SQL?
92,How to Use Constraints Operator in SQL
92,What a Moving Average Is and How to Use it in SQL
92,How to Analyze a Time Series in SQL
92,How to Use TRUNCATE TABLE in SQL
92,TRUNCATE TABLE vs. DELETE vs. DROP TABLE
92,How to Number Rows in SQL
92,How to Use 2 CTEs in a Single SQL Query
92,How to Use Lag and Lead Functions in SQL
92,How to Calculate the Length of a Series with SQL
92,How to Use Aliases in SQL Queries for Clearer Code
92,How to Use the BETWEEN Operator in SQL
92,How to Use the IN Operator in SQL
92,What are & How to Use Wildcards in SQL
92,How to Use TOP in SQL with Examples
92,How to Use WHERE in SQL with Examples
92,How to Use AND OR Operators Correctly in SQL
92,How to Use HAVING Clause in SQL
92,How to Use the Alter Command in SQL: Renaming Tables and Columns
92,How to Use INSTR in SQL? Find Substrings Easily with Examples
92,How to Use the PARTITION BY Clause in SQL with Examples
92,How to Use ROUND Function in SQL Explained with Examples
92,How to Use CAST Function in SQL?
92,Why Use WHERE 1=1 in SQL Queries? Exploring Its Impact on Database Efficiency
92,How to Create a Table in SQL? Your Step-by-Step Guide for Beginners
92,How to Use GROUP BY in SQL? Master the Art of Query Optimization
92,How to Use UPDATE in SQL: A Comprehensive Guide for Beginners
92,How to Use Select in SQL: A Beginner’s Guide to Database Queries
92,How to Use Select Distinct in SQL: A Simple Guide for Efficient Database Queries
92,How to Use Union in SQL: A Simple Guide for Efficient Database Management
92,How to Use Self Join in SQL: A Comprehensive Guide for Beginners
92,How to Use Full Join in SQL: A Comprehensive Guide for Beginners
92,How to Use Right Join in SQL: A Comprehensive Guide for Database Enthusiasts
92,How to Use Left Join in SQL: A Guide for Database Query Optimization
92,Cristian G. Guasch
92,"Hey! I'm Cristian Gonzalez, I created SQL Easy while I was working at StubHub (an eBay company) to help me and my workmates learn SQL easily and fast."
92,SQL from zero to Data Analyst level
92,✅ Access 348 lessons and exercises
92,👥 Join 7789 students
92,📈 Learn Data Analysis for Product Management and Marketing
92,📊 Learn SQL from basics to advanced
92,💼 Practice for upcoming interviews
92,💬 Get support from Community Forum
92,Free Sign Up!
92,How to sponsor?
92,Support us with a yearly donation and help SQL-Easy.com thrive!
92,Apply
92,"Languages: English, Español, Portoghese, Italiano, Français, 日本語, Deutsch, اللغة العربية."
92,Easy to Learn 2017-2024
92,Learn SQL · Learn HTML · Learn CSS · Learn JS · Learn Python · Learn PHP
92,About · Sponsor SQL-Easy.com · Cookie Policy
93,�      ��k�#G� �~�_D-�	�
93,ޙ��.V�Ӝ!�
93,"{�g�R� "" "
93,@���J�y�>��=R�4��3�+�ޫ}h��ՕvW���ꞣ���
93,"���_�f���g ���n����""�����������~�{=��>~�Y���m��,������>o8�e�e�u�~�5"""
93,"�'og�4��O���:���A���m�O��:o��|�89�����~��:��u�;��,�4JO���_W��4�C������v���Wg��iԉ.�d�]!�2�y��ۋ4��CK^�8�Ͼ���:Y�y��BAx��u��hv��N�yq�v�Ǐ�cs�[��0���m��""+h;�6�`!֖P"
93,"긥%�����~,�}{��)�������Fˋ�&���z"
93,M�C�4y���NO�����<^��+RG֞&��sZ��4\��8�b�&m��E�U��t�Y�_4~��{�|�V�E�*��7I
93,X}=�h ���4X�etqe-�:��E]���%_ZN
93,����@F���rV��x�]�GJ]o��$ɳ7��� C�rf�r�\��n�
93,�ņ�-�t
93,��:�l������W��I�v����8_FO~��$˜��t���`=��O��x=w\��e��qLs�K�|�D��_���S�C�Q�ф���N�l�<�dЁԡ�n�JsZi�8�:ۦ�� �U�t��fSv
93,��� ];��
93,��� q�
93,�-
93,"F�H����.��I���\4����2��/Q����@,lF�������o���Rhc�T>�,�����"
93,�pi�ݾ;j+3j�Zj��<�D�Gw/Y٠�m�_q���K�������������N��7���-�;�*l�$�A�X�
93,�7>�����
93,�L���x�q��ֺ �N��S��H<
93,"�t�'S�M� 2S�}����f^Պ3""ׄ�t��f�-�x�%��UP�yx��&��R9��$@��۵�4"
93,���6\k�@~
93,��M�4�e��*Ho.�
93,�d�6�����ҁ4B�Eh�wyK���Li���0��-;�?L��ڙX��A
93,t�@uR�/|7�۰n!�Kd��k
93,v:�*^o�(SK���
93,i�0IÏ����kw���ӗ�~QG�8����j?�D?Zd��I���p��Ϟ��/��}��'p
93,�k�g�Y\2=%8�*
93,Oa�v{��d�-��B�5X�m|t�SR�
93,tp�I�B����Q��{��(�QNf�qgD�p4
93,����7(��(oO3
93,"W��-N �5�h8��,�;����C�`�7Jdo��з�y����7h�7Z��\zG��"
93,����|��|�>��7���?�
93,"�l�J��W0h'��ڡ�ޠ��d�։�e�Mo8(n��@�SB��ʁ:���`ш���Ͽ�""��|�����h	R�"
93,_h�/f��[z��-���(O��=&��M5����2�yS
93,��y�:i�Kt'Y�yα9y��w���U���!��E�
93,"�3�9�k��L�@[���.v`�|V*?��E{	�?:7W�Y*l��f���y<�9���[�!C�Y���A�h����MD6w�,� �m�n�"
93,���[��6o[�
93,0U|
93,"d��n��H��u�""U��]�4/� �ג�.��-�p�-"
93,#��I�V���
93,"""�3�3"
93,")J,�v�|R���]F��tß����v��W�ޞ4�K��=���\�0.�S[����&��ɚϓ��(}@��n}�Y4�4�C�L��""YG���""�X���z�/�ׄ�fBt���_x����y��[�]r��,�"
93,�t�|�tx��#ח ^g�
93,��H^%W8
93,�� <~
93,"�""��M,#�~�5���0����f5Q#J�"
93,�rй���y�f m����}�P!j���g��E�h�J/�V|�b�\��Q �$�Z��3�xN�Fk[�
93,v&(x�z/����usw]䟥/�@�
93,u~���wNb��i�<�|=o���	��0��;�Ct���Y�+&Z�� i}U
93,l�Dc�>tغ�)����[o�H9�
93,c'�
93,"?�x�*T�B""..�g�J.�-��n~�F_n�ty����KA����U��'�7���e�[^�2�"
93,���p
93,"""$j� alm�������yz���@0�}D�(|'k�;�dr�Y^�vN������6�s�~�;M��O������U��W;�����.�_��H��q"
93,2sJ�d�-��zg
93,��f��j�5ɓ�䐺��L�o��(��ʉ~
93,�|:�
93,�/Rh
93,���nD �D���@��F��V��>�1��ވ����[��R��Z��z����ϩ08`B<\����Gg���&}�?4AV�Ut=s�W��6��P� ����#�L
93,��C\m��gT�E)`����
93,N�	B9��:X 7`74�m�/�[�$m��R�����~�hpA�9[T
93,�&�y
93,-��A���:�Q�_Y
93,"���h��V.k�&�o��x�O�跷i�$o]��n�,�6X�'���8��ڑE�p\ m�,��V�T���$���""�#��r""��̊�E""�)p@��RҸ)�����A�0iM�D$�@.ełkm�%������>��0"
93,"���v�ݬ�Le(&���Qe�3j�!6����6>Y#����1�Bm�n[���,� �'g�P�"
93,�e�}��}��L���@���
93,zfg	��-���7��� 9��pY��̅�E}i��m
93,�{N��Z���H���E��&
93,�^�\e%�/�.�*S�/@uჭ���kBQ��M��P����BUR
93,i��U[)'e���J�B�V��\��[��a�ݞ[�bm�L�^���Qq����E�Y1�l�䂎L2i:5������f
93,��)� ]�\����YX�QGjkl�E���%��mȝi�c��Tdq鯳t'���sD� rU3�
93,Y{N�xR
93,���6
93,�:E����O�~�>�~o����%��9�Q�?�(�~��`��~�_�/W��u2���т���?��w`XeY��пb���8kl?k毦���C5�8(
93,"\fK����p�ݎ}Z�˛��I��""~�I4�.�����&v��zI��	�~89�+���Gu��"
93,���&��`��Mt�N��1:7
93,"��0{����j�|��vU�6d����G�����/�f�[E+��v��B�=M��^Z�GF���t?Fj�?M>L�u�`:����C����w �s��9,4��i�0�D$\���y� ��>"
93,@A��r���z
93,���-�7�\�����[�g#υ_#��{�������\�{��맃��>
93,���{������t��D�z��������c�<�������g������w�A�=�|��~�G����FxC�
93,[��S�L��;����S����.�(��w��
93,"4!����yf#��H}o@�=�Q?#��}�gL������?}��ƀ����޸���:ʒUt���М""/��;����u�� ��x8"
93,���i'
93,�m
93,h���z���`J��4��T$O0xbܜe���[DW��w'��hO�e(�l s���`��w�5�dz�k��
93,"�;S��(��[K,�كi�LhmQ������)�'����h�t����`���'�e�"
93,"v\��,M�|纓̝,���#/"
93,D��<��a�z��Þ��g��A4~�$�Bx0��{}� �*ǣY8�:
93,�&	�
93,V6oH���(Z�=�G�>ˣ`y�����1��kl�4�y���w�ӌ��9�h0
93,��a��
93,���u�
93,+�Ic
93,����a��<Qd�)hc�ܺx=K��)�
93,���L�%a�G��l<
93,864��?�O(O�
93,�J��Yv������-��#�>h5�G�$r~�~��
93,IZ��G˫��A����i
93,",[��K��6Z��	ưb,"
93,}R��j�=�C�̗�և�z���Q0MZ��>j�I�Cp��
93,�Х�F�@W�q��cg�x
93,i���3��h�J�����[�_��o�O�&L�7[ggd
93,�NȾ�n��r��+
93,�I��Q��R+���s�I�Y���N����I��7�z5wt�Ѹ>w-��8I�2��������w�� <Θ@cB�*HOL
93,�db/�*:�6�4���pi0�Y��?���L6	�/�i�]�.{A&
93,%rQ��n~�� ��~~�Hy;i�r<�3^/�E���mt�8����a
93,o�ߜ��>�J�'�ae/�;�^�����Z��E���j�<� ��=OVg�>�ES2����mW
93,�����������n�c������
93,Wؼ����}Z����7 ���xN���0v
93,��>|b�N{�9�g�*�R�d��.w� �-~w�r�D�bo���)����v�G
93,�(=�
93,"v�	�1X?��B�T ��#������/��3�4���\|Ϸ0��,s<��4^�[��|w�t�"
93,@B��Vq.�X��vӀ
93,�rq�>��a
93,���
93,� ep>�򰕧�J�*�!UZ@��!D�)��Jּ8��mV>o���67�
93,^I9���8+�3^!Z�u���{��F)�7�gg ��4�⪸�R�4�*��#�3da�Gx��~�o7��-Ö
93,���u6�V�V��{0�� ��@��
93,H-��\�Hy�B�	��l�3���I�͓��ψZ�����.��d�@9����0
93,"� R�X�!L�H��B8��""����r?f1* ,�w��d�"
93,"�'�������~&�p�2)%$+�������ZT�5�%�ec""*-�=!���2�?"
93,��Z��gٻ��뗡�p0����R�o-5
93,�J�U�X��4���)�N��DPT2P��o�_��:[F���/�Sg��\XE�Z���M�y�q;� A_5�S�����UC�N���O�8��C��⇰��5����%���8��e����`��2ޜ�D��)���:d�v�����sy�n�T+I��jǻ:Y&ӗ�:��rT?!g}P�Xh*�����1�]j��25B=�����m{
93,"\������^�i�']�KSH""7!�Y�C� 5�aU3	��Pdg��|����n��挐"
93,��:��E��s��+�B0Bxs\3��8�H&j����N�b蝒t�X�T�r)�ܤ>-�B��$z�	�K��]IS�D*	�Xŀ�
93,EqG$�B
93,"��{JX�QȘ}�-ԓ�P��z�	�AO��jRg��""}H���ů��A����dm4�6��Wq�y�@ܸ�(���k�,h��� m��"
93,"����Ýq��EuI6����T�Qեu��]����JO~��B*,�ć!CL���,� �;}\5��"
93,D%�
93,���N�V
93,Z$JO{ߕ����
93,"b�g""�c�:�tT�N�p�g�`ƖeB˨H�����CX"
93,���NQ�$��
93,E҂/
93,�nhO�5��n�Λ���bu�Kj���ӝ3.��o�@E4��> &z���3�����ɦ��'ib7BʷR��Z�ye
93,"��s�{L_�Sf�*�M�e`���r{��d��$�1�X$G%��Ήk ��5���IӢ ߦhz�x*,;k ��s恱B\t�� DV	�t�ݤE5~�+��}��0e�,"
93,i#��
93,��8$���d��%��Ғ��ɢ�+C�TL�xM�Ng��8M�B8�^K���4��
93,��AO���	�֩L֊(�;�NA|K\���	�x�¢����0�d
93,"��U,(ݠ�/*�U�Z�St-]�1��He�e�ݩ��M�z{[�W�G�7�����Fv����="
93,`0)����౽\�<Ҿ
93,��Uy5����6��qE
93,�K������$
93,"2�x��� �N��n_�������i�\M���Xqy�/y�Hj�S��G�K�M�[L$ڞ!|/���8�O�7�G��ө�V�`�TW-�Vrl��6���^˭i���',��x4����^�`Dh�S�)�:�^��_i��~9V��^���l"
93,���z`	�����Le@!�w'5
93,�D���**�
93,�e�1�
93,@�i�\��m/��(e��q.���+qM��
93,XFP�~��-�
93,��r�
93,�q�[-LrO�Md-�=��4�
93,��l�
93,eH��8��o���ރkJ͵[�k:%�`a
93,"Ȋl�G^�8_H��K���<�ϗ�""��6"
93,"�q��ɑ""�~V�YXz��$��1"
93,#��o����
93,:���!��ɥ�S��a�u��X+�%P���Ʀ���}�zT�X�
93,d$W��
93,";���R˞��$7gWZ��'<&�6㫝@}Ô23�NG0�!��qʸW�|Qr��?A�""�s{�P�ޤI��b��IOl:o�m���G�g�����&�"
93,"xL(06���mi�$m��A�kI���k@�l�z��V8]0��w���2��az������n�&�B�,-O�	O���йy~�'�p�*��I���"
93,��A�C'���*8���K����8\��Ʊ���4`��c|6(�Ca
93,�Q�\�v=u!'I
93,�l3�ΐ_��*�hԓ��
93,�Q�6%L�M�'ټ�iU)\~���Sd���~@��U���B<��*aޱ�#�zPȷ���F�oLN���_7��S�߫��J�����o��EV�������\�3уc�
93,��Y��}MIQVdSM�s{�ʠ�l;��`2zM�د�	r��]�h�FAy���@Hǋ�
93,_ME:	4���D�>�t���bP�}�~W*�b
93,b_v� -O��*�KK��K$�yGo�
93,a`x��͋PX��M�d��٪4�F�f(1��$���e����rs�+�Yψ���|F3�0k��J�
93,��lkW�j�/:-
93,n�#<M�m���?������=	5C�q�q��
93,��=#��I�q�\V�s�鏢:�v�fI�;Q�؉d��f&S&
93,�o$́N�|
93,��h�ڞO�5�v0��]����G������iSo��Ps���`�%BmU�#�����)�4����sYQ�	=���M_�Xf�wx��)
93,d�֖�v%̂-*�覎�3
93,nv~���\o\<� o]�9ץ{SԒ�-��0��^DitB���R85��Ȟ�������	���΍��x��8[��<�d2�v������M LCO(͆�0����8�疦3?�#3�r
93,̕l3_
93,"ƶ�(E��ͦ{	���(;�4���I���""��"
93,9�e#�0�|��ٴ
93,� �
93,���2�O��H�
93,?��a� ���t���A���|��4
93,���
93,�Np�ZjTiZV)P�R��a ���m�}��
93,+4sOUg�����('Hg��J
93,�=<�%�AW�
93,���z�&�>�{*��@
93,q�7X�6�m����T�q��h�$K�~
93,�+7�D�<ݮ��{�mQ��(�<�]
93,"&����Q�Oat�y6��3""��٨7���"
93,������h`���ƃ�>�����h�i�
93,�b?�C��������S�m���ᓡGYa��L=ZC����:ADO2�*&H��tB�OQ7�Q��n0����'��x����Z�4��F���4��*�D
93,"�!�|�;���h؛��DXC��Nu!+̊�!�gd""�Μu+�W"
93,���2
93,.q�L?k�z4f�������ѰF��W
93,�3��H[ ��Y��
93,U@���~� �
93,�����r��XV�ھ:f�`E�bu�/tK{���7���Ŭ���>�T�����Y�N�ŁD�҆y��^ik��}}u�U���
93,�� �aO���
93,�TG��ח�e+W��Y<5�V8
93,"�1���(���St��˳�腯]���M9��R=��q�Ay���qqFk!x�ШM�=""�����b%�~Y��� ��A"
93,!0J.ђ�+{�����eXN^rT�L��
93,"P""�iiW�z�t��J{���i��F3��|�L^B�U��i�O�Z8��9d�$�"
93,"�W���0��;��!���̥�xv�YR�`˓Q�]��<��d,D'�OԷ��e�,�5� T��1z{���mpx	ulII梠A��M���f"
93,o�%ѽ-�
93,"�|(�n�Ѱ�֦��;�T�'�ei(���������2�H��k�#�b�[�8�ɐ��O���Z=ܡ<߾�z�謁��y��=J����')���-sC�+��F|���r�H�������H�z1�c���7F���#ؕ��b\���6ctm�%��R�m<�N�,���d�lUC��>Pɝ�v�Ւ"
93,��`�EGy
93,"�(�Ǔ9��""A-�����,|�(9�(���]���΀t/8�'"
93,��y*�+sRj�ݢs��D�5�R���F
93,��Tf!�Zt'���G�l�
93,��b_�|Bs�H5����CŅ�Pt=�u�B4΂4
93,D]�o����@|mxYX	i:�uU�
93,�R1%��Dp
93,o���+ێ�)���f�EJ�B� ):
93,"�Z�^�q���""�-)e�oh��^��b�"
93,� �~ܲS�(|�K�n:���y=�.���(�-��P�/�����i72�#O1m���/.>t��}n�(��rSm[x^&
93,�5�qN���-���Ж�s���	y�N�Yf�t�]��SKJl�X�������Q>�&C	WK�Zʄ��|�w&9�@
93,I�Wb���rI�H�e�OY�ju
93,"�֌�L�R&�jk+��H�N����Ό%,�xݫ��ިo.��rs��G���,6�$9&V�v|����~=�t�R!�i�Rꍙ�/��(�"
93,E��X�
93,M�[ؘ�uL�
93,d�tY=��P-��4��|�Aj�#�V6N/�����n�Z�7M�����b�86�Y�؝�Ԣ��u�U�Q���Q�k�
93,X���ĕM^���y�́6��'+s�]yr����B�E镍������*��x���0��c���Ƶ� ���
93,��gB
93,�j4���E��_%k�X���A�7 b�����r��4�UD
93,")��H�^����u�碂�K,��Զ������6�$�,$<��"
93,ݮhPئ�^d�1���4#�MӔ��ޮ����V�z;
93,��6ţ\�R
93,:�b�4��CQ�u	|�YV���E�b81
93,�<2WH��w��H�R�	6�q
93,",b�g�ұp �rƖ��@_�5��K���I2b���h�Z��7,�6��k��t���T"
93,V4�v�+�vo[H�{}��\і�
93,�)U��nIpV2� -|�p��*˪��F�s �^Gx�R<�Piܥ��Jf�D�f�
93,�:���
93,~fs�tE�_��F�������e<�Q8r�q��!Q
93,")Ͷ��`�=�����וW�/�U[���YcR��@Wi���2h9�įT-���w�m��Т�f7� �,:L��"
93,"�0d8�+6C�Z%�yb)�GZ�>%���4���i�����-��,�F�A�rl���h��m@�O,�N�d�F��"
93,"��؛��,��UĶV���r�Gf�"
93,"��ț�*�6�Q	!�\�~�""t��%���:�@�YP7"
93,�N3�����aL�ҍT��
93,��G�xhN�I���x���'�^'�K�
93,��f	.:��C�r��^g�=���戂��
93,��z����=9:`=b�e��z]��WBm�;tDb�7;�|R���8 ���E��j�$�
93,"}��x��M+;���$�.r��L�����KԹ.��=��4s��WW.�@�tO�K��,X"
93,"9��H�t�����\���J)!ºc�u����@R�l���""�_�,4hX�7S�ŋy%8O\�PՐA���*�"
93,��tV���Q
93,��UtI���
93,*��
93,P5φ �q9��ة�(M�Q[����[�<9!��dB]��B�-Z� ���
93,"�Y�A(����*8�P��%|#�7vT��3�fֶ�y��""ޙ�""���ѴE>D�� ���"
93,��[����
93,"q���E�a�2FkT��������X*��$��SF��,&���ƛ����0֛]egu4�0)"
93,"o��*rJ)�WJ;,��ԪQ0�aoȁ�f�t�"
93,"o�IUSȠ��2�dq���H}ҕ56F��A-��2������"")Uʛa�����#���"
93,8mW+���� ��j�A��&E�rI���*^����]Z�ٹ3C$��)xS�
93,�Jd�#э�� �5���G��U�cox]4]�1
93,H<��4�����Xē_ez�2��@�M�.IПfQ�w�N�kN�l
93,f���B�oͅ���[=W�'d
93,"W�j	m���yw�s��A��xn���ե&��c��EZ��,Y*E���Y�D^u�^���o���Z��D7"
93,��jԶ
93,���m���;1�W݋�^eIN��C}�Ѿ���
93,�d�
93,tΠ��Y�YZ���
93,"�W��,�{QX�t2�W�C�n� �g��!��l�dA�щ�$+��V��&��15�"
93,��� *_
93,ЫʃM�ؔۡN�BM+��ꛉ�vM�i���<�'�1�-���ܔצ�Z~LR��#�~�A����AScj��w���v���¾�T�v�����Lv��� �Y�#����^�W�~���ێ컩
93,�gw7�a��� ���g=�
93,"��hz���>%���q��Q�s`�.�/���""�f����j3o'����l}��4��'<�;�"
93,"��]sP-����2�Lۛņ""{�u?�g�2w���x�W�A4����i-���8��*�N��u�|y�Ev��R��/���3A"
93,u���g�:�g/\I@��|��3yЁ�E��s
93,;� l�����Yn�)Ε�;���6�  i.T�@+;��Ф(9uhm�
93,?Z�P��M�X'� �����5.F}we��Iq;i
93,d 6��)Lo+
93,6�eL�N��[_dɺ�L�A�]4H��l
93,0�f�x�k|���U�8c
93,����S
93,���h5�Ka�>`� �0�|
93,�#|�RIcg?'���~w??��t�P��
93,��
93,"(�|\r>%����;P���s�v��q�/+,q�7�h�8�"
93,F�G��!s6n[0OAt�7�c�G3B�C�
93,��G�:t������{?���1�
93,�)��CH��/n[e�t2�[��v�j�ہB�Γ�����h�#5κ�ՠ�V�����x�`���zHr
93,"#�G+���%9�,1#>����t�S,E���&C��6�>+���u���N�\[�2OÒ�"
93,"��iY,u�b�Hkˮ,"
93,��T�ԇTva��_g�
93,"�����=��9�U�%���=�1Ɵю|�6xV�}'����W/�]]f96�ۗ[̔M���4�r���b�/�""0.F�""��dm9x��8��4ݮ��ip���""��K��䢡��N���-,��96�⡟??"
93,�f=���co4����7yA�7v������Ë��qzQ!�\I��\�.�M���+�$`��i�1%��`G�	}�B9�E�`����ψ�7��e�+�2�0��v��fmbk����:Ô�3L��5��8=EC>kϓd���MLZ��b���a���`/�Y����
93,���q
93,������y/尾�7�S��>F;�%+\d�C4e�
93,�e֥ƺ�ܹ3�
93,h��9����KO���OT���t��g�aE��
93,fA�;	;��`8�M;��A�Gb���V2I�E��j�M�ޓ0IAms�s�D��t�⁢ײ��U~x�`�L��]��g�F0�����0q/�uxI� �q�Wy
93,�ah�(� �a�7��]�3`��p(2�.��ؘ��g��TL�d�<
93,λی�0
93,d\fW�K��<��)hJ��
93,",�k�"
93,"=@Ԁ���4""C�"
93,"L�;�����������uJ��q��O�7��Pџ<�ޝ<��D��(,�,���%_uk����tᠥ��V�&-�|��i�흒O�m傔���u�=�߈�#"
93,"�S{Yؽ+(zW��ߖ=~�uG�0� �$w�p�t��zr7��3{���i�;�W����Q�,)���gá�?z5]nÈo6"
93,yw�.� ���#y��(�{��l�y��4
93,��?�u��ُ��(��h��}Mm ]�
93,��:�s��`
93,]N�z�E�
93,�^���`�Me6
93,�0����UF�zw
93,Z��i4�!���t���
93,��Ea�oa�3K����_h�������G?}�{�aM�xv��ӧ��{��{�����o���6�d�g���n� �����֐�����9t:{��b2��z��׻��*�6~F7��<9�s30L��y���.<� ��2��ǶBѫM�/:���{
93,"��X �����IY�x�i���m~ѷ��|���E�n���vM�X'��U�:��z�\����b,��|��17�T�y�^|�F{�\�4�9N�2�4��`߽Q�~�h"
93,���Z7���Q��y���$obvThgކ�=σ��M1���a�/��$;�]���ӈ��)�*xu�	�؛穈�\�j�V�oT���&����3
93,"�ۢ�z�`U�q�e�؁6��,��W"
93,"\*�""a2ݮH,�^zF�Y��(^"
93,"r�'2s ͈�=��&�l��D�9�y�ɸ�,,[�4�c�`�R��$�#-���.e�q���5e�Ψ�ǳs��������������?�����ӛ���y�.|y���!����?��~t}q�h���ON������"
93,6��q�I
93,_\�ios_4H���@�V�ٸEr�{�36���6`
93,",����6^�I �G����'$�Q�"
93,ht�lB��6?}�x�
93,L�j���[�mue�9cF([����蔾h����{�� �.���-hȀU�l��J%fa ���W����_
93,"��5K��+=�̦f�Ek ֢�1��kl&""$���1f>�[d^�"
93,6M�ο͂�2��F�6];dV�J4$�87ylWv��\
93,���q��S��e��
93,���/��}ܒ.����Px����z������C��
93,T��Ǐ�G�ͫ�!4�h��ɵ�I�t	�����싖�
93,F��	K��ŉ(��
93,A�y{
93,���>A͡�
93,"��%,zJ�Q+�<"
93,"za2sVI�]"";'tO��c�e?(DYs'�(��~��]<��y��R���'4�yB���N��h��]c"
93,�[:P�9�����b�>��_��lO1{�jI8Z�*j������wK�sk]�t�4��<�.���?����cLh���d���A^�r-NO�pz}��hO��']��Ƙ�0����:0L�?��^Jz�% a	�~����~������8�O6���4&�V �4��4!B�e�&
93,"+>�,X!l� e�O�4��Y��	�gB�vs!�"
93,"�ʑi���[R�+�L�00�[T�Q�� ��@�a�z�~'�=BM�""<P7��J�X�2�,~q��{V�D�%m�[A+�5%�2�46&՜4�m�#P�+@Τ4`������F�B�oP[�������P<�F~��Չ�"
93,��V@����֔L�
93,"��'��GYw���f;����Ԯ�B�u�$L�pÐ�);�f�q�&��Fh�bhbb""U&<,�M�^��z>��>~|E'������� ~܀n0�S�Q�G����$�����g�����[�aC����'7��u�d�<_��{���чF;+�.����,�ͬ������C�$S�SjV�m>�f�KR �fR*k��6#4�C���9(}/�"
93,ՁW3bi'���3x����:��J�:�:�q}<�)�y�ψ�۠�6�ߒR@��OBV%���w�ڢ���P�����H�I��9]�R��}���i
93,��|�P�j6�ݕ�hBN$����\�|��`@�`'(i�D��
93,"�/2��j��*,��Ԡ��\8+0�-V"
93,���
93,��h�d#Z���6o���iUz񻅇
93,��܁��
93,_Ї[���m:
93,h=^
93,���v�P��f���Y>��
93,"0��;UoJKA""y-�Vl�9��N��ޢ� j<�Ui~�����I� ��mt��c�9��U�ߓ,)���T�P+(�_�"
93,��Y�`�D���zF��dxO�'А6�u~��h �P�>�	uB����D�g��-�S|�l�4�@�xǠ>�s�60|yK�����4��� �
93,"+!�,e%@�l�0`DmA`yh����UE��=��/N�R"
93,"��Y�q��U��Đᢨ`�ٚq���ZN�""��/���`�1�x-�,zAe��l �����5��N*G���'"
93,��i$�$^
93,"�������~��]��,Bu�?$�"
93,��B+3�۶����0�`�R����[ͯ�}2�Z���]�
93,�Ȣ���k�-�����UJ./N��Y���X��)'�9���@����������[�ֽ5M'�b�!�Ӌ���`��d�!pD�\$��o��}r�|�x�����Z��v�㲻-�w
93,"s�>fg㉤��Ĉ1�+O~������$���,,��[��+��>m��J{��D�a���	2��wz����?9z�JNb�K�������yT|׸:V�!~p��&S�jū{���Q�ڑ�vQ"
93,PgW���i�<_ש2���`M��B~�E�;��&��B����@�2�Yu&�[H�s���ɴ�\�^�E��Q	�����`�2x;;PR�?
93,^�un�Jbz
93,�*E�C��
93,u�H��fʚ��S
93,"=�x ��Z5&X�����>{�����OMV�Ś���$;�0�_��0y	I�k�S&�����4N9j6D�*�AD�)����S�Q\DUBB����)5�����h�KfgY�T ��/?T��>lǨ��t)�A��,�"
93,-�
93,"Xow��:kp�����0�s��5<���""Z��"
93,��)��I��C^���^M_��	�<�U�
93,HD��3j)�%�*���)��
93,Ӏ?�(�k����lw�B�<�(��ڣB9�[T�aj�V�3<�Dtfb&�_�Ϙ�w
93,�e	�_dJ4��
93,0TS:���%��E����퉦u F`��&��/v���lMH���g?����]�>��~|��<;���>;[���6-���&�����J��w���Ʒ��E9J���2!�*�
93,")��}��B���_��?Z�,�,�����<)�"
93,Bd��A-�k�9u�>A%5�Z��R�
93,S.��n��wޓ9��湸!���4X�ٙ¬�$! �L��
93,"���,�}b6�#�&�O��p��og�t�5"
93,"c���-��x�8o_Y�i�N��8甪�,"
93,Q!DD
93,�v��
93,"6,�q�BL,�\ĥK�(�j�}��!؂�E�k>k|��j<�9�6�L��'"
93,�Z�
93,$���wM>�����oַ
93,"=���w�R�)��%�:d""r7k$�t��T\��d��B"
93,"��&M{��*�2r�Y� <��k����H���)��k0�/A��""Rʬu�""��T��ӷ��3e����@M�g�"
93,B�����28�{}���4��
93,A����c&�S�4I�e���d���	��T��w�� T��i�����Q㜥:!��
93,��N��]%�ƳuB��X@_����7j��BΈ�{���>˃	a拆�i�8#w���A�G�����#<�q�)ٸ;��]�׃}�7h��E'�%�n�>�?��=O�5��	~!A�4XNO:��
93,;o9�v��YdY��*aB��N.��Zo�o@�3���^�bq���p2������h(����9	�ݽ�
93,��}
93,$�O_���{ɳ`2� ϴ��2��cV$����¸cMΙ� ��4n]Dˍ�����7.�KN�>U�����I��U��
93,"�d�""#y8�eGS2%K'Y���v���g5!��"
93,"/Ýi~�Y��d��r��&�㑂M��8���χМYy�[�/'!Z�,Xm�<�����]%�dTn�y����VD�����:F��$�3�E��4��sP���pW�y%��V'"
93,.��!	$c�[�
93,C��
93,��Q��
93,�R���	)�d�:
93,l�B��+�^�q�9�;�+���T�B
93,�X����6��)~)�$�_J�I��5a0
93,�����>_���%΢���Z�F��W�|�$9s��K\�x�!�D$�
93,De:�eL���Ҋ[z�N�5
93,"��&G�oZ�iZ�~!�'*���:%Ź3e���,M��m�"
93,���b)U�d�p�M?��/�
93,�b��u��u�A���$UR�F�V]�~�e ��=�8Q�L��?H1�RpP�B%hc�M	��
93,"�A}�FK~����9�""s�G�jݕk��q�G�9y'��v�U%B�J��AQ�P@�l`WJxKt���y��"
93,�gS�v��-\O��]��l��L8�� �����ݤɜ�օ�3t�B�*Y^�t֕(Q
93,u=r���ztY
93,+K<9[b�@���5�6��o&�>
93,�� G��f�YÍo�c����U���
93,T�e��C�����n�r;>�G
93,"�,{bZU{�I�U����X��"
93,Z� )���]ci��k ����mw���&�H��5?�9����3����7�u������Ɠ�٢
93,Q��Gw2��P�
93,"��o""U0  �t>	N����~S�bWZ���"
93,"����(�v��ڌa�)��Mt�""y@59�T�Ga�р]qr �"
93,"��4�����R�""BCS�"
93,��S�)ߓ�ى���
93,Aь������ۼrD��9|�m9�=(=*o1`@D�Q.i��IA�5E�]�;@�	�
93,y�~A�����#��Κ���
93,s)0�[�}՛3d/E_Q�٤���xsVj����Q�
93,�s�NB�r
93,��P���Τgd�Q�Md3���gN�
93,"(R.,����w�𶸤�WM� US�L"
93,$�+a����6VHE{� �Ja���6%�����ISr4�
93,g�y�M
93,�NJm�H�N���x-��m��ڈB�ŝ87�z1��ųѮd2��
93,��u.qv/�ſ�-��.
93,��]
93,��<��`�t�G�h�*��<�f�)˯5��i����˞���Ԡ|�I%\�L	/�q
93,k7�:��-gҤ<��c_˦HR[��8�ms[i�i��Je�e$�H
93,"���#,��AA�o�2��5�?2e4颈_�}+ٽ��"
93,T����L����S�� ��\������!�=h-)�
93,g��Ŋ��Qg<�{�;.�am����#8#������
93,�o�^�N�����$Q�Mt�%���j���7��8�~'y`�x����o�ލ �&��t�W��]�L(���v��������w�n��v�^���ߞ�iw��h	
93,�څ���h�
93,��A
93,�_���-�1@�vJT����o�Ҷ`[�SC�A�҇׷�O��;O��%�D�D��Q��;Y�����͡�
93,���6���$�N7ݑb���4#�T�]~����[�
93,Ӻ�L\��a�W����[IvC�ij��n���
93,���dQ�g�
93,��-8��
93,w�^Ft�О�5���2r��z ���!��0�
93,�'s���[��/`��Q>��L�����` s�_�c5���*j�&
93,"M,�M��m�����9	�4d�Y="
93,"}���`�or��5LD����:=���/�7n7��.U,U�N�R��"
93,���T{�-�w)�/�D�9~�U1
93,n�6��=f6��
93,��;����]Z������q�'i�I��p{�ǩ�����R���
93,�BǄ�D�lg
93,"���M��wl2�r�p�&�њ�P""����;R�n�(�ԏ���O�+�m��x� ��Z-�����缦 ��'��"
93,���B}x+!��n�}�iV�kLm��ߵU*1O��JL
93,��|��I�>)��+Ȃ�
93,����s�U2:���>x�y�o��(���o�����|�D�/Z6.�&�8�3g���@
93,��H	6���
93,�8:����j}.�
93,c<���
93,"w���-m��,4�"
93,�O�|��@�7|%SXf�50@��zR0b�0[)EGY5G�$����{7ߒu�$�a6/���Wo����h�%�eW��KWX��[dR3�4T&N��&G��ȁn�\�Y�dˏ���
93,�ĸK&J����:AY�~1ZgC�CS$�@%�I
93,5(�P�얷�
93,"�F��""*k0Aܶ�a��"
93,"�Xq��R[6K��r��etC�S2�������=�l罾3,�+Kz��{�o.���k�Zȶڤ Nㆲ2�U2���d�"
93,�x�K�~.`��
93,�xrïѦ��ȑ�rم�L��Y���޹��
93,"�����Ym��	""s��"
93,�B1��1W?�%4LD��hю�X��E�
93,��Lv�m9G>a
93,"qXI]�ǷY_��i ��O��� )L�����fi��=���]�A�C$O2��㒸�""�B�։���x/""�`�>^��h��Jʠtj�qS�x�祿�a"
93,2둆!
93,A��|Ms���������
93,�����´lzzE�t�R�=��
93,"ӹ_ ` �Ѡ�)*$��Ƅ��H��A-�,��OE�"
93,��ܶ�鐼k䜆'���nE�7���p�
93,蒺�KbL��&�_�S���a��k<򑕖&�JO�>b�(��7y|���Qi�k�
93,�7�b�Փ�
93,"4�����1�\�,��*LR��(�8��>x~��1I�j��m�Mҷ��VBӺ"
93,���
93,"-�-���R(>�/�:Ju:=Z���v����d݉r�,@8q"
93,)�u�$+��ms;&���\�tp6[5�EK.���@��梧��㨎DF]mr�k�U-f#������J�Ux4�2TǢ���>}��u�;���q�J���h�e��E_]}�*��W)v };���J�Wˣ��:�t�	��K`��!�V��>6��N��.��ǜl�hɷ��5:w������{�ȁDF*_�����	��ߡ^��f�*�=� 6�E�/:�v���N�� h��}�}Ń>��7�����hه�C��@���������WK�)>s�'��s@A���h:C9�#���ǅg��Fp��8g�W�n�U
93,a��4S��N��!F(q��ۏ��\
93,*P!I3@����q�mW�LZ\�0�~�PӾ���e'�[d�U��b�
93,J	d��
93,�`�o ��ݝrV��A�c�����
93,\��
93,�fp���a�h���#�߁�-��'�|�
93,"҆,9��a�3z�u(:"
93,"�,*"
93,&����Щ C���Ɖr�)>��qڽ��g�_��޶QL�M.���O�b6����
93,�_�9�6:�?��fL�I��4 �it�l��$2����.�Y2�a�!0�9����v�b2TN�+\���dP
93,"��ER�Y�ҝ���Q],�("
93,�	����_hhH��7��Y�p
93,Ҭ�o�Ǻ\���X�L����Ϡ��!ǻ��}dH�
93,h3[{��2��EL���V:v���7��
93,&���1
93,"���[�(��t�0�*BN��,�A�Z����uR��╥�f�򗌞|{D܊"
93,dQ��H�JH2�9w������
93,�KM�q�*
93,sr4tE
93,��Mtk��Ղ	���������F4���Ш�r����z�Z��e��aE�d��um��������0��
93,��G�GP�@�
93,* �*�
93,tv����
93,�&M��4�f��C�e��-�WD
93,V��I�y]�Ae
93,u@� s�vj�V������
93,"��M=d�&��/w�4,�����]u�*��"
93,��np>�m��j������L���ښqy�ӆ`%TF
93,�Ί�ݗ״�\�
93,��eljp��8�eF'2��ׄ�8��h$r��އ�ƨ�b84 SCl�5�挒������M�s�/��+^����z7���F-
93,�$6�	�0�cl���V�{��nR��!�`k�G.�����+��ΰ�LȠ-�R��LhV���N��[��x�z�R�a
93,�������B+�t�EJ]��o��k�z'hY&����t%�eb}e{�
93,G��s�����Lڢ^
93,";�`� m�{iC��W���Y0C��M=���ãS��,��""��v"
93,"��Ju�r-GX��v!�""�JhՈ-B2�U��l�4 D���9��ɷR2>A��4��t�n6Q:E)�"
93,"��X��:�ly��3�C�""&1S��a%�Au��/�.*A������c=Pb,����e�T�Tմ!��k�2X��"
93,�a��
93,�r�����N�gs�T˘��ʰ�Y�
93,H��~�N
93,E�z�Z!8
93,��B^_s=�U�A�
93,�C�ܦ
93,�>�LӔq���}��$i
93,��TP�
93,l�mf�@ZYE�٢��K�YS1U*L^����S2wjf�n_o�
93,�Qc'���+�b�)�[
93,��˽#g��eY�*ȃt''��޷�N���h
93,"���sʮ;��E�m'5�""�3��<��""1��y�e&�qJiZU9H4L�E	������Z���5J:��-�ߊ�l�5�'�^�L��6c�2��|�B�����|(��)���I{d.���P����వ�j\��R��Z�A[�"
93,i���YSI��� F���E�
93,Y�鼉'c����������ޯ֡��a��������.�$��2E%��
93,"LK9|$iˣ�,���v�04�QҁȾ�)"
93,96K�}�
93,7J��*11�Ok������Da��G�=
93,q�S��k�aM�[�(<�\$
93,G���5b��K޻���{��
93,��4N�q5SƑ�#�|
93,�k������5�!�K8 ����uǘu�0�|yNp�/�B� �bE-��Ր���uЇG�EL�ZV#�=J=�h�R
93,yv�:ȷ6�
93,�n���n��n���-��;�a���l�{5!��E�55�Ea��<^f�~q���xD*KϤ�V�h%�ƺ#Nf
93,�-��H{1Y�w����/;U5�E)�
93,�Z�&)��
93,h��
93,Aj������*^��55� uGL� ��f�2-U����F����/z�=~lW���~/J5Op('
93,e�|��=�����w�Rj�4�!i����%nrP�T�h�)���ri���|��?�y�A/�#5k\�D��D�Y�(Ƥ���T�H*J���|K�Ե�`��Div/
93,x�2N��-�m����r���+�a���c��T0��R}G߀����
93,�T�k�&��:��x�
93,u�|�FWi�n��t����{�%�D�w�ƿݷ�ee��dd�w�qz��-�9�\!�<�՝Ua<���WA&KK�پ0s-5N��^5�\�P�	!��U�t8�c���:+��)=�%��Q&ϥ���~c��XB�^WJ��R<�\oe�;IV�n�
93,s/��+��7/\�� -�1������p3����lS�q;
93,"�>�l��yն��G��֡;,j�4I�U���//K,j�:,�k/��W�[�ڕ��6`�H]ϮL�G�"
93,�c�\-��K�������x�S�
93,��34�:�5`�'�Ò�)siޓ����c����sq
93,"�,�%H�?�����p�cB}l��>(�"
93,F��%�q	R�O�~��{8�1�>6��
93,�#�Ò�)��w?��?
93,"��P���?��a�r\�ȹ��1 ����c���A��0�<,Y�K�������	���g���sq"
93,"�,�%H�?�������Ǆ���3zP�9�8K�������g�p�cB}l�?(�"
93,"F��%�q	R�����Çt �J<��B|�;�""݉X�隭�pW������"
93,���J::�;Jy
93,��H_�}�_�;?B'�����hԯ��>ͯ�<������hԯ��>ͯ��
93,������hԯ��>ͯ�z
93,������hԯ��>ͯ�
93,������hԯ��>ͯ�
93,������hԯ��>ͯ�W
93,������hԯ��>ͯ��
93,������hԯ��>ͯ�
93,�hy`s�!��_���U����G����H`�%b-*e��=ߞ�N��
93,�RQ�#[�9-$�v�����8�w��?f�IH���6k���N9�Ѻ��ތ�޸H!��''���H�[�y�b�!kӝ!z[����X��w��
93,"�?~p�� p6R��:����a�#3/u�y��H�JU�""��0��8Q܅�M[�-}M�l9UM�H3X�f�Nٛ`�yF�������M���?�������!�1d���"
93,"�z�x���iQs8���+� ���o@x޷�""A�kJ�!a���x =*E"
93,K��
93,�ڪ�
93,�M����d;]`��%P�۞׷�6�4M�K!��&�fQ�e��4
93,�U����߆�Z
93,"�V���~�f�"";�M-<j���AqנMi'^��#��""�����jv�m��X1��Ҫ��T�DjCF�?�\x��C��B�F������f"
93,lu�#s�eJ'�}�t��P�T�8��b��**wYR����'
93,�i�eU�C�ġ
93,�.\�'�+�*PqϐQ9�U��j(6K�;MK ��+�����2�p���)[�;$aJ�Җk�'f��
93,:�v
93,��)7J	��j��=%፺��
93,��������s�n$Y��d-#\��ӻy��@J��B���pI��`�%8U�ܥ*��
93,��cr
93,"�NI6���]�d5""P�Xg�d��W���� ۠�wܱ��<`�'��������߳}gt �⾡���"
93,���M�݃����9u9��q��.�!���m�B�+�;��5RN#�Z����PFӿ7�����b��_���/�CX�ͷ�����
93,)�谯7��I
93,Mܶ	T��IF�w��a����2P��B����
93,���ȉ-d�s1ɧt��t��!msT`�u?y���g[
93,�7R�cE�tm�X�/)���_��3d��n���1F�l�/�dFF��_7�?*:����1���')�7Mq�ߦG`���(x4ec�{��T]�v���v�ǫ��M�ܓ������\��Hʒ{J���>
93,�2��8�[ �?��(�m�̂�8�y�Ѽ��'�����)}�6���zK�0�T������k���b��-l52�|��B/@3#��ĥ�?��nn2�G�#�A����3SƦ$h>��w�h�y�;6��\E�&������~|�rF%F
93,�#6�`�_�|�Cv��1�
93,� dB�?If��x���������ʪ1�����㠒u/�Tt�
93,"\L�$��,&W`�25"
93,�x��
93,G����F�{��rvY�K˕E�6
93,��rÅY����
93,�f�A���_��6XҧM5�����:#��lZ���U�9u��=gq
93,�@���W��?��O
93,�vY	�ͮ��Z���k:�
93,�3��
93,Z�o����t�[��F��Uy�����;To?
93,�j�1��
93,bõ�p�V�u
93,߻��LG6����W\
93,O�=
93,��8
93,���s�ƫʶ�s�-����e�#=��k��(�5g���\2��͇d���r���]��&s�$�C��4A���\P[w��޴�3v�Nc������#�M��
93,"�4��%����vD�)�x�p}{�F�*L�!�����}���m�Yt~!Dm�kO�ړ����=(O���:�vF�ow��!kC)Rȶ""���fn���0z�� �wW�Nw����x�B�.��ix�ڠ���k�zGj���"
93,��A��6hp�ix�֠]��Az�-
93,Q>���YFT��5PM�.�n�
93,�-�m�Bq V@>�w�����^
93,6-\;�B �m�KSG�[�%�HB��g�x�D�Ke�_��-�[�w&���	��z���c����H�`5�U���
93,�F��%J�
93,�F��C��c�	� <[i�6B���p4��Q�E
93,�]��X��uc�
93,�4N��gd/�l�4��(
93,")N� �BC��IR�D}H�ڊ��]i�wn��M�'�0ȃ3��4����j�<��Q�m��\���;ɫ����:���}�p j���E�o�NO������v��O}����'o�&9S,��b��)eN����Ox�or�pI;"
93,��E S�0l
93,?��TB;bd~*�aG�ȱ�s�ɭ��l�J/�����b�\e�Oe�fEH�	~3e���xX���>�6~�_FǞ�
93,ߗ��gw�E�������^TWpݗ�
93,�{R]����lW����etC��3'ߦkͫ8�'�2�ov��b�<st�Eg3K��y���|D]d<�� �
93,% ~F@l��ů h����k��'�.����}���yK�
93,*�3
93,"�%L�f�(��.��Vk:��C��$�45�z���8�����d�ۺl�d�0|+(i�dYNl5�a���-��|��,	!�,	Bz?1��¡���	�D��Q���i'�֑��*X"
93,{ץ{��
93,���Ӟ�Ʃ6��
93,�@I+*�@)
93,p!n��Q� ��z ԫ($��˻Q�Y�F:0@�l�z����i�j�D��ٲj���P�jTD!%�C�
93,"��Ҧ*F��""3mh���(�RQ�P�^t��V�٤���"
93,"H""�Ȱ@"
93,���=��!�w4�|TU����'�6�����(a�ف1�hW/�����s
93,"�ɍJ,Xgy""�:��[b�����y��Y���0��e6�(�<>�������5-�{��8-S���ê"
93,"ծ���E�X�S�{�.ʩ��=(�|��""(�[��u��lI�@<�ָ"
93,"���B*""{��/(��`�\/���B�3K��ir�i�UVNN�b�EĒ��� �N_�8*�}�G���D*�:&p\"
93,�.V�(��gq�V;_Bߡ=U�
93,"�w,�T��h����V@���\�Ok�����xP�<"
93,�$�f�#��;yB�u�b
93,"B�K��.9Gm9M�w&��Ԉ#���m%���2Ž��'�P,��v�E��ݴ"
93,mYVo��\�s��J�S�A�}�W�ߥ�'�9��!��toΐKY����$ߕWԵ�&뙋��D
93,U��eB���x
93,W���M��]� I7X�
93,"w0�X`S�r��!J��^�;j�u�2��,�l�;=dRtHQJvZBٲK��� �?:A-���%=��c����,фѵX_�m�5ԢG���נ�f���n��3� ���� BuQ�ҴJ?1Մ��"
93,�����l��!
93,"�(�"""
93,"�m'�s�:H���0��""1g��.��椓F���o%M��u���͖�dө_�L�;����q�c17Ers�#����}�q��0���2a4�-�vV�AN��D�q���oz�����uB65;J��L�}N�-��s�yv�"
93,�L޷*K���GlJ��׼���D%�+SG�y��
93,n��F�ND�(�De��;ǡ-ӻx*����Ճ��
93,�5�O��H`>4�&����_��	��U7���}�1��>�A
93,���G
93,���A�3G
93,"�c�-�ռ%��N]��Z�v�6�x""_w�p]>�`�2��Nn@��e�ԍR�"
93,���no��o���΃d
93,"�qT7�Mb ���!��f��a|oO����{n����rL��p"""
93,�si#ch�d�C\�
93,�٩Q�>
93,w�X�V���w7��*��B��9�H�^G����΍��Q�5&��!�&�����U�a�wl�)�l�XH�t
93,���M�f��z�Td��l��EHp/��ȣ*�
93,�!������$�3IS���LP�H�S���
93,"��2�@��:""�jacjY`X;�Ĕ.��=TӧR��N����ӽ�l��� �a"
93,B�����
93,�D!�7 	���Xu�Ǫo�j�T���PӸ���t!�s�d=|a�MӘ��:��a���?u�͍o��U�����yL��
93,�KzTUK5pK7���e��
93,ޱ�fśdyT��w��$Y픸��*��g�8�r�9���F$���B~Ea�R�qiFE����uv���m�J�i�CN���K�ƊY1h�8!�FB�
93,",֦�a;z�v�7��^j�@��L{oV%B�W"
93,�7���� �EV:�g��~a>�'ae%�N�˭��	C�	䑭4w*
93,Y�=[��TpM�Տ�(�u�Eۿ��e�t��_q<�Fm�7n
93,�ˈ�6jW|�TA��:U(Xu�
93,U5�j�w?���
93,v�b֝�
93,"Lu�)N>���W�D<������!׫ȉכm�^[y��8A�|�0m��,����,V-�2���=���بj�r���y�Z!��Ѹ�?{B0Mr�^��G�E4}	ī��""���ۃ��n|+��"
93,���7x����
93,��L;��WZ���(�G��U'
93,P�������Q�AT#Qf���4�(wK�C��U��Ҧ����B���[X��Ks>eA��^%)��*��w�tV�1���7;
93,)'gi��X���:5Oݛ�
93,1Qv�c�HcH6m�-RO׮é|{yɢ��
93,��@|�1�l0&
93,�3�����k9�b�:�����zb^̇��S���5УU�
93,��WMW���p��jzWM_���p�
93,"�jW�P�f�pՌ�jFW�X�f������V""⚷���L{֬��w[���k��S0XQY���mf^�e�z�8r���/}"
93,"��U����D���G$��Qj^��S�Uz��h=����K0dV5�I��j�����,��.ȓtw���o���k����"
93,�AL�
93,"B5�M""�{sa��5Rb�4z��"
93,-���GP���&V�je�W�{{lN^�C���-�k
93,��
93,2����5�Fel��-�$A�e��V@+��J�b�V�=:�v���:8h�33�;҈�������RTm�Rb���
93,"_�))��ϻz��Gv�k]q�[�D�W��y��=���ި7ݚ�>%��Z����T��[�TmaF3�S�^���mb�䉐�ʸ,�x�}�_��#��'�"
93,"�l�Q""*�v�ed H�>�;�E���"
93,Y[�����g%���L���:F����z��ӓ�
93,�f��1;_��v�S���T�Ʈ�c�(��>
93,"x�~}�ś�7 ���ѕZw,�]M�4�c�BV���t��D=�wg� ���Y�;t���"
93,"�Ĵ�;����N��h���N��:�j�#���׮`̻p��XЈ��R"""
93,8I*Z�Ԩ��
93,�gw����!����c&S�n�
93,�O��J��F��v��V�TShm$�BL�W�r@�oT1�� ��R��	�;�\��$gs�FI�.�EB��a��R�GGv�
93,�P�6:�(����Q[��0
93,N$����&B����e#�j���+�X���X|b�%�Y�%�l
93,"\B�KY�`U""ʭpP�eU�xP��e��e�n 1�Z��Pa�\�q@׌�l��.m�����h3�b���f;�f,�W#��t��d�$ztȥۥ�����{@N���<�,�b�p�s���We��p�~�.�)^�դd�6&��j��w�6 �Q��0$��x���Ө8',<""�c�""�*wtKbu�c�g# ��g8�l��hl�b���D��""��/"
93,9�9aI4R�q�LM�%jQꥣi�Y
93,"��B�B����19]��m1��# �L{��""EU���O�E "
93,k�F�6���ʆ��)Od�2�*��	 �7g�u�8	�Jf�2G��կI��gq�
93,]�:&��d�
93,"��� �j)�~%����ɧ�$���r�,�&��W<����*7υ^ʓ�ԡ�5��T��/!�"
93,�:��$��\��a��'f5�zc�R���w�Ӑ��(�g�:��B*
93,"�}gn�,�ybѻ�+;/hہ�_����"
93,��W�_ʪ�\�ennˆI^
93,"����^&^�""=�[�-7z+E�>�u� K��PX�5?�sU���N���wx�m���P1�O�y"
93,*iB\�b_�H
93,"�ݗ���""�S�]�,�-��Er��!@�S"
93,4��Q�&�;�!�K��w����t��e��OA�� �ہ�YBQ�@��F0�TϬje�aS�nEkH���R�7�()�Z�0����P{Ҙ�I��K(k��ƠK*8����H@w�
93,"ibSD�'@3�]�� M�5�.ʑW�=�1V�f��z<�eߣ6�����`��6�=1)'��M���!!��a�*�,��z�Bc0���aFlΦ*/M�<�a�=��z���"
93,V�W�1�ø�II�pǚ&��O���PW'
93,"�i��Ձe""��<�g�qk�&adΑщ:3p����*U=H"
93,�ˏ��I�6���\�=J�B�
93,H���~(��H�
93,�#q�ޟ��+�=���^(%��9/��H��F���
93,a阑�1�Aʜ/���^
93,K-?J]�s�ɩ��>K/���N�	���BȽ
93,���>(
93,���ƸJ교]�CH<�;�>��Av9�
93,c�a��
93,��I����>��@v�!��.�{�3Y����D�^������~��i.���BYi~�CA
93,"�#��̸�-�{!��l5�a^^e�,��`���l��"
93,"_1�½������{�,ld�0�Σ)b���-�{�cfiU�Ŷ��"
93,�E��4S�|aסhJ+��v
93,$�jW<�4���
93,��Vy���vO�%샞���-��������I�ᠱ��K��0_Z�aep��	s�ɤ�hTV��
93,�@W)��1w����?��7
93,�P]e�
93,�bg
93,*-��	S�h�
93,=c�i�
93,$QΤ���
93,Ǉ*M=��x����1N��
93,�P0�<���`�Eg���\+���U	&�����
93,"'��+`���6��E�i_�4,A�*� �e%ؑ��"
93,����S�
93,@��y�HYKNᝒFBxC�&9���;��
93,��_/썱��/-%i6
93,[Q��.�(ų�$Ǳ$�-@s���i
93,"�;����7��� ���l��8Yn�Z���,��pC]���<�y�Lg�2�N�ƛ����/*�`���X̠*@x�<{�Qx�������ӳu���N�04�""��ދ�&�"
93,R���*k�`�j�0����
93,xH�f��_��Q�uM6�
93,"f!�u�5�0�Dp����4)h�$���M���)�""���)��^W�"
93,h��U�z��egX��C:CV�ڝ���:á�u�B�:#{�+W�e���Ι͓�mWkѧ!>�˰�
93,"�D:Zo�cVY^�W#}���""-X�%8h�Y��sn{�/^�ad;���9�|흨6~7�nPϜ�b�x��F<0Bq~�	����.�|Ms�a���Zj��hV�f�,�Ozw��0ELa_(��2""I���s��۳v*��0��a�V��;E�/kc�v�"
93,�h�%X�����M��b��7�~����7�
93,8|AZ@�t��0�����Q��ݨ۔��l�
93,",P�t�-ۋ����~g6q���G���""��E�R�z}׊fA0"
93,F�Gaz�TFk~�+�«;Sj̂��z4�aG�  f��@�(X���H/�e�k�+<y��w��x ��v��t&c���{'
93,��(㏆���`��`.�.��|��oXX�p�Xy�-�E�
93,�Kq
93,B&�
93,h��jH�j��jJ�*�[��'����
93,�k��)��5i���b8�^fsD<�-���㤏W(�
93,"?R|��g5B�d�""�5����>���g��J������	z����{�s[r"
93,"dT) w�-�3�,�N�����.�N������~~k(�����w��wSV�J,�(�9^��%_4GW��4u���>���.�*��.��BAA��E@��;�O)z8�w""���.�SPT�/^���c"
93,I6~�V%d��\>�]�y9K��t��<�.�����B$렴��/��)i����g
93,"�i��,�a��=����%J=�%�ɵ��D��-IZ���U��qW���""6�|�v)0E%A��+�dXi)p����Aq!,���2k�����RW%�0���^�ѥ�8Z1qIc��l)eTo����ig��,��{A�@�Ƭ)�6y�F@�E��їֲ5hAޭ1{W"
93,d���/�^�0�������n�NBO
93,"ϐ�,�v������ǈ�g@H/V$�ad������D�������^�e��7���"
93,�߯@�J���E�h
93,"25T�s�""Y�:�7M�t�T�[&`�Z� �c#�v��s��^鲎rP�^����7�`�1�.� L���"
93,L{��`A����&�Fn!�i��	�?���
93,�U���O[
93,�g��'���տm�f�fZ�S8�9X�����d���~�K�47
93,��k4�I/a�Ot[
93,�ߘ`Q�`��	o9�!�O��gM/�J؀��	�3��W
93,",	�7"
93,�?R�������/~�ʜt� �B��*$IF�0�^�'֟��\�7L�?Qٹ(`蠱1DK47���UX
93,"�1�R���Ut�L�QlY��%��	x��4F]�hSK4~a��v��D�&�b)c��ch���%�:HCk����ph��*��?T�ֆ���@E&9���B-����E��,�a@���$/̓���C����"
93,CFJL򃷝ie~�
93,[�ɄD�P7C�g-�n*���V�����NuЋB �]���l-�Ơ�����V�<S�?����֟��
93,/��#S?���(����)������3�L�
93,�����%s*�V�A���ߛ�7�ʘַ�f��Ĉ����
93,.��)�a���e��������o�����ڀ�r��Ew��L���D�3U��5�41u���iM��*T
93,͓�0a�w� t��k
93,����	vb��OT:P4�
93,"h��	v@�,�譝$��������7L)u1�͂�Ȁ����"
93,+o���W�a�
93,"�,�Y"
93,���ߏ�~6�L�����-��:}i�������M\�)Hd}7��_
93,஍�˿g)�	�&�/��
93,�x�5�d�
93,��g��K`>s�������a)u�c�0����e
93,b���P-r��U}���20~�L����
93,�tQ�����:z�RC���R�zT
93,�WK�4Z%&���R�*��
93,�i(��W ��%z|ua���8[řa~��Yh�����V
93,C'���G��8g7��h�[��;���蠆��
93,"*I]a[��q�^�l+���^�b�����""�di��"
93,"L���>�k#4Y *�_��Z�Pǿ� �Q�4�;3���fC��kc��� �s#�""Z��O���������<�Ҝ�l��csr	��o�١�y�^��h"
93,���1~�
93,c�enZ�f�M�n�l���pC+~l����C#<&������d������0�9n���EE�^0�scr��#4]4f�e���ӿk.h���g
93,X\�%��I:��O�E�4~�
93,0|�
93,"��,V���a�"
93,���������N�
93,"�o���XJ,ͳ�1���B��f��"
93,^o�O���@��xi6	��P�+�����0�W�I��6�
93,"����n�o��+>�,�܀�U"
93,"P`��7�""B`-#�*�_M�t���.��_�ԉo�S"
93,4�u`��/�n�$�7fh��7�B����2�f�Ҩ��R���e`C
93,[͒� �U(ln��7>���Z
93,"4�%5 �7F�y�\F&�7��o3_���""�����`��[�\�Fp"
93,���/uh��\Qp�����'?���̰Rf�c�S��bs
93,"w�#��,�)�����o pf���V��g�}��N̥j8�_��(m"
93,�����*J�=�]
93,�T��|i>S���6�bϨ;W�a�`�I{�aѷ`
93,���͐���b����Y�����<<���~���2����+��w��;���^gt��
93,�y����3?��v�csi��������z4
93,���Ɇ�
93,��	�V�G~�OH�	=�
93,M��͕�����Z��k�{������%��־��� )&3�q�ty�����z�hjMlӤ�����W��Tf)Żcy[��
93,"PtRx��WKL]"""
93,"( ,�}��&,GI,�sW��'ZK���I�������Dd��^��H.<�"
93,"�r�*F�,K���xeV#�]�Hɝ�%#�r���H"
93,��ZM����L&e�)9A�
93,!�Py�owۛ��請�
93,q4��g�[�!<^�r[���+�+��J��X��D0���G���i��x�'�N��2�lh]�{�n���W`�Y�� U
93,"�d��T�Ե""i�]�u��e~EL��iZ0ӌ��7�N����i,�����$��""���尋�m_DqO�}f��l�.�E��s��H7�j��R�t���"
93,&��
93,�<���O�E�oj�E��L�t��	� T�:��MB�����(�ˮJʚ�*עW���J:(���;�2��8cko����s��h�zW*;��)�b��/�2�v��'��-��uI.�!���5a�7!�1�J{��ٵ\R����ӛp��*��]�_�!�� ����*µO̦��ڛ��pA?
93,"r�����[`��""v\N"
93,J��d�넦�6�&�E!N
93,"�!�������:aǏ����X�Aq&�""��g�cy�����B�����"
93,�HǪp��0M�p�����N*9���GGU�M��No�d
93,�g������S�lh��eY#�t�Y��	�=���E�5��\R��٢��?��E^4H:�F+Z�
93,"|��m����V��� �]W�����b���@M��dK�-�JGQb��#���\_�yF8�""�0ͻ�z�p��4�H��"
93,�yw_͕�=P�Ժ�j��q��@�;R��Լ�A�����+��S�@�t�`�W]�ԡpwWt�K��Y��%�M�����
93,�֋�v��^���*qǥC/$��������G��ybs舔�=�ɶ�`V�|y�!;��g՜�
93,m�zS��-�tO�n�q��&g/2@�
93,�3N@�3-��in
93,"an8�=�""a��{��"
93,Ґ6ۻ7�!⏘�_��<�p>	v���%˓|m���F%�-mm��%��i�&�e�1}zf�N��i�/�9���%&o�ɸ�����؟�_o�	��ۖ�&!���|���X+Ғ���P
93,.���K뛅K��lA����ʠ_{�Fp}��l�
93,r����3v��-0cw<��#7T�Q�UvG�̱#���H� �
93,�����T�8l�{��GЫθ�
93,zx��6;3�}[&T[��BR���>f�[��ћ�
93,"|c�ܛ@�|U""M_�<��#�=Ma���|,o{���Yg'�B�p�^bO���?Z[�<���B����\+�n�.9���C3-��|څ�V��°����&��m��dI�-���$ �|Q�)�"
93,tW�	��K�Pl�k
93,[0}�
93,@i��u+Z���z[WA�ӥ�(�٣[�
93,�����%[�Z_��S �R�8���I���;�i!TM��a��4b�d������/E T�q�sY^�$�5K�Ĵ��}��ʻ[xK:�<
93,����mW@�U�
93,����b��
93,+7����yz÷{AwX'�	�l���t��1�̝폻�@Yk0�X����^4א���w	���c�x��4Mʳm�>�#��z��D��c��G� )�!<΢e4�[�X�q
93,[��y�(�&�$�Jw�
93,2�t�4�LL��	�A
93,l#���
93,Ƌ�Ulo
93,���ܐ�tj�U
93,y�-�\�t��ur
93,��t%�'޿��A��=J�)�H�:��ȗ姸��l�
93,G���c9�U5������Q������U��)�5�
93,"M#eS�M�Kg���xeƒ�`'X3 ��D8��C"""
93,����l�S��)EL�B%�2n+����%
93,�0�</Dw/Do/D/Ġ���H$��V#�)����8�/���t.��ɵ��!���I��N��{oxi{^�N���Ɠe�$h�)�O�������`����Il��Ԏ�
93,����R ��t&��B��\�:��-n&�F�`8���]ܔXc����Oo^�o�ڭ`
93,q3�	H
93,��y=��X�07+@t�ba$.��h
93,�#�*Ȑ��(�b�Y
93,�v�^�!����i@��p�x��d&��(�rH%Yަ�
93,$�1�P��������_�^GY��|���)��B��&���
93,"�j�2""�Ň�`�&5i��*�i=D�냆0j�n��)�xqq�����ēC���G��GTZE$�(t��"
93,"HeBn�ʺo�Ӧ��(�L3��0NiAY�E;�""��#��"
93,E?�e
93,"UP���$8�Z���p�����;s�*~0]c��qJ��C|<�&/�$P���ߍAS��S�����Ϩ�T��{�B��ڤ��ԩK�Np�@K3fXV��l�e�,4"
93,1��Q�o��
93,")U9l=-�Rӊ��V���e@��i��W��""%��$�2��D"
93,��4��PuZ]��x���*W��0(cw�'J
93,s'j9Kc�Y0�
93,"�t=��+5�z�}Ca�f���ʊ̣ ���I9ݣ�b;�v�2�}�MN�h�`�w-�W�+�U_�/��/""d""�慕��"
93,g�oC.�J
93,��S�`�
93,����*���;>{�s��Պ�b!;�
93,"#���,N3��/CG������q�n���xP�S��W|D�H�Q��F|�r=��k�cSsV/�K�h{�IUPA�X$�D5�����m'Q��Ε]M	5���ޟ-N>!Lo9ML��^3J	�"
93,�V��i�4�V]�C�D\�$����@��꒽
93,�0�^l���>j��>�c���Nq�Iz�p�YQ��f�����oU�x	t��7xT��xd���k�lu�k`���ʠ�u
93,"�7]�F��9���nQ�O�,2�@�������*B�&2���g�c�#��O.y�&QİW��)B�ADYYo����=+�Pڿ��EY���$���~��B�Z""M�$�(���!�@y)t_ySt]y.v[yŻ�61��K��^qIW�`"
93,Q�%+���&m����*�e�U 
93,���*hY#ҵ�Ӹ����.5�I��Ub&cFJ`
93,U�1�C{C�#��I���PbU�X
93,�r���/Yw���ҧ�DN�����lU�CUD��f����%i8M|f S�
93,��/|�L���E��P�-3���pw�V]nR��1e��_��B�'�v?��Lsg�@B��X���{�)墴�]2�%��؈3��ت��
93,�.u���.M�e�W2�A�_�d[Y�޼�%�
93,y�:~g�+�r��-
93,�[�P��aNh�d�Q&�7�9�t�xcm(G�zM|dfi0T�q�
93,ƸEJß7ε�Fb&0i��p)ֻ��\c��7�7���X� 5��͖<^���>
93,e����+؏r�=*IB��A�
93,"�P�3�=�s, "
93,L�s
93,"i�f��D{���b+���""��"
93,"�m��2��h��a\�7���mM�\S����W#匘� EE6��V,r�M� �bE��"
93,[�û�_�ތ`i�
93,�@WI�C����ӕ�%
93,H>�5�f�)�*NAV�-���F��2�{+5������e��oEAYK��8Nus�	�*�[N��{�/��(��{����^
93,�'䡭H����ݾܞ�g����wTډJO�h&�oihE��%��dF����/'���a5ueO�
93,��*�����/2�a/Bѝew[�*�`rŪ�B0)|e�t�DZw=�G��W�
93,",βf\"
93,"l9rɅ���SK�]�Zw���Q���$�o�S�CU��,a�Jٯ5k�F����"
93,��1��e�(����ǃֶ�1�g�0����b{�e��]�o��(PbX�կؽ�j
93,��K7Uׯ�2
93,���ͱޮ^o߮�%Ow�
93,gsψ�~[z�
93,p��^(���]3mN�י�'mPm�m�I#�d�R��zpk��nC����v�=�����7[Gg[
93,�TY�N���@E�
93,�yH�F�=�Uj������������
93,�)��r��ߔ�9N3(>2F=9�sO�Gi-a
93,"�����4�wb�,x�X5�Giq5}�����"
93,}��M��	Cr+yX��܄�ɋ�M6�A�v�j2n
93,޶�3�X�z��Ė�x�RIb�i�)�H��ɯH]�
93,9�<:gj�
93,N��=0F�n`֒�2���
93,�=�\c�4e�T�	�'���$�#
93,���
93,;R��a!�r9�N;ʧ���T}�L0�Ql���9#B��>�2��(&�זp�¶%	��������sc�j(�{�֩��<�Gg�dc�vWA
93,p=�wL�G��~e�!Q�Z�=J(-�#*�H��>
93,"a-�ϟ9�*,;-z��XG""���*��t�FJ�"
93,"�!J�!�tzMIO3r��?6A~��@�^N�,97��-4�GjH)�&'"
93,"�,�"
93,6a-e�֑F_n�4
93,��>�s]+��uX�7`�����i⣧
93,�b:
93,",l��%I졞ӕ"
93,d=0���'l���o�x�
93,y��PR)�I��+��	������)m{
93,LH`�{��'�7�M��6��8���2$�2J��:勴|f�\u#H�7V�ډJ��]�ř��}����Yu	Pr���(�]�1
93,"�sc�	񨃸�ķ��W,"
93,�~C�
93,�tmP�Z.lm$G/���K
93,Pd�a�
93,")L�$�dF��AHJs=��x��/��321EF""��H�A6;zգ�j�N��"
93,I~m�@�y�ȌH�wk}ы)y����
93,������
93,t]uh�]SFGp?�f�3�yM��-E	P�_�s���<��kZS����6zp;U
93,�f� �
93,�Nbc��k30�6��;m�*|#���Lq��Xj9	�S3
93,�u!�M^tCZ!C
93,!C� C� Cj )Ўi�x
93,���fg��k|�x���@�Q4�M�d�hK�3���� x%\�����y��� �p�|n
93,)�Ǒs��U�e�����sY��8T�Ė�C(3к�9懛���{jʫe=�Sop���)��/�M)�ݢ�dhоY0�Ļ׋���+�f�]ח�^n�}i��7ٔ3Չ'ͼj�JɅR���8��`�8m��i3L�$����_(PU?B�G��ja
93,"+��ې�]���Rʜ�^j1�\O(�st��E,�C6�ե!n��1�ȝja�k*�WI2�3��lMA�3㳸"
93,���]n���*>�j(+
93,}���
93,\5�A��wg� �o����׫���h]x���u�H#����$:z?�([���*>V
93,h8��=�\��ޝ�u�$/�ޏ޾y�k>Z
93,"h@z�=]��Л��PS�,Y<��7�d�HJ)z�)�!��G� "
93,�&�HR>��
93,�9�[��aZ$�F4�V4Q9��I6�
93,(v�5�{�(N��=iֹ^�2�!I�3g�(���^���i�yt��\
93,"ߢh/��ͥ������F��[A�""7@�I��oRg>�����h�!J��,��:�؁d��ܤ�U"
93,��9mz鏓]��6f���9��jL����HZ�J���h���y&��
93,�U�檝÷\7�i�:�1�-�zC�{���W�����
93,"�n�a	�߹t�ʛb�����f��,O�d}I&��dޒ^�xm�O�&FR(�޹,4��I�"
93,����)�s5�P�k����\}#ҿ?��f#�-�5
93,"�*Xw꽵����o!y*�֖NM5堜-Q>'o��%-�!e1�A�7��˧),`����Vmq3�߬"
93,6;L�����oè������C��D���3B�`�NX�:�zF
93,"�x��4�ERd��+z�d������t� �jQK�j����g���S���k�o���^N��\��\w����^���,?|m#�#;	��W���˷��$f����SXa�ǯ�'�J|�������΂�܆b�K|(-"
93,sXz*��Ґ����M�zȖ��&i��2LI*�vN�
93,"��nӔlbN�%F��5��x9�6b�$��B��` �ʥ`ʺ U�h�5�,8"
93,�Z�wJ+
93,�Gs'V��J�a��҄��ᑣ�=v�]	��
93,�X���!���H�l�#�:�
93,|�P�~	���<�>}�N�}�Q��)-R�t!��
93,�銒x�#����$�%F��ִ}^6�9��W�
93,H���5�i�\���]V�
93,�7����f%�����}}��ψ��4����j����(���Q���3x�����u�z�{��>z�_�>}��G��u��� �
93,�P�?���HSi!�ߧ�io��Wqt�N򊾀�����_���w_��B�6`���~��o ���>���؇+���%{0f�]�����.)��܎O��Wp��\�S�
93,��u�K����
93,"Y�O�i���Ɉ�.v�	I��g��F�!�j�b�wH����G����?��� .���""��#F)8�[� d��a�CI��Z�r)z��*51*D�P"
93,fK	�����4[K>ţ�U
93,X�Fo�m��}�׷8��j55@��#��3td��L�.�����r3�^҄���m%��� u�4 ��GѴ3�Ga���
93,"���""r�Ħ�""f�y�Q�w��G�"
93,lǏT��.���[�����'���숈�z��=X�dߏfC�>�
93,��ȭu��a�'~Ә-Bb	��8�x�z�F�䋘�#sY�i����yo ן��'xh�3���M
93,���J��s�r�%Z�A䇅3��
93,�7J
93,���8N��l�7]x���/�UT�zH:2$�]{I����j�<���aw��/�� <%�
93,�2�;@��I{0@B��GĚy�iw���8��
93,�Qܖ͆U'�4LZ��}
93,"""��d�A��Gf@;��aM�K�'N���/˃4/Sn��4��"
93,"Vb�e,�q#"
93,��7�=��0
93,bN�漆
93,E2�'P�0�1Upd
93,��x`B�]/�4:�W�Dk
93,�	Z��
93,��fsW
93,�ׅ�®�vg��j2��Z
93,"���5�T�M�d���U!��Q݉8I�$�rV(G�r��7�}:d���G�bQ��1��d	""�"
93,N&T��o�L��u)hSі0k�M����8[�rS�Zʀ�zkka#�� Fp��+�BS�P]�Wq�b��Z�
93,p��v��-�	�~=��Qc��U���N5���
93,p���ay2���+��t>�Z��
93,��j�[.cm�X�BV��m�Y
93,���0 ��\Օ������{�Q[&ܣ����
93,��ɏ{Tp�T�Ge��5ǫ���O�w�K�
93,�C��}��v�Ֆl��i���& �-|9��;
93,w}�v8�r�p�5���� ���~����՗[w��.���t���#�
93,"$��ڑ�r�5X�ڇ<�k��[A$��""�ڢ�>�u8�(""<�G���;��Y ܧ}�TT[B�D�x�����"
93,�>r����ڊu
93,ۏ޴f&�A��t�f�h����}�aDh\���*���O0
93,"��J""+����W�1U�_��^"
93,"���M�'��~Ҡ�b�""Z� �S�ϲ�T#c2T���5��V�qГlkn�"
93,���\Z���(�a�
93,"���3qM�Σ��bt���e�_����,#��b�=�eD2��ݔf��Dx���Ř���"
93,�.*0�D_���Vn����#V��bR��RL�$T?Vr�1l�m���]�y��&Q��x���d�)���+wӊ+4[�/��k�Z��
93,"��-��\p""p?O��4r����8M�vq���xv�2�)9-i|&�^t�3�C��V�����-�r����"
93,�N�����23����0ZТ
93,r����K$i��4�I�ԃ��kս
93,���y���-��T�<]�2i
93,����?juF]�fEH1��lY�T�{
93,��\��S� ����f�G-��W4b��
93,�;X�Y��n�r��Ș��((�wv�)y#�[�ӌۊ��m:4�M�.Y��<�������u��h��C�
93,"K�,La��*I)U��3�Y�vт�Tv/ѥ[��\�"
93,2WK���/��z]�\;��LxU��.��]P
93,"�N����0""H�֍�S�"
93,"8�~����GM+�,cJ""6�zDʵU�V���@1LI7�D=PACkײ�ZL�f�v�Y�&Ŭ;l��w��"
93,��h
93,Y=e��k2��-!�
93,�9�ޮx1'�h*�����(�T�mS�I
93,�O�Y�Cl��hbC��1k�n��Y����&
93,x~���X!���Y_U�?Q$0l�*��aAlͺ�HX�x�qasZy�/��֞e�w';9	��C�|�e����J�G�
93,��ޙӠj<��M�b����hR������Cq�c$�}�n�$�}�-Q�]ˢV�$�S��B�(D�0���|[|'�_R�X݁�!9�-?�Hǔ�B_J)�
93,In����A�ɴ
93,"e� ^�.*U�۵��[���,Z��"
93,�r'��Ҝ��C-�U�6���L6�1��+��Ζ9��L��)&­`
93,�<Cc��4
93,`$^ 53
93,CtB��v���4ڎrp�d^k���$f��L����M?g����ռ�S��B�x���/Eei�
93,�(��
93,�kVk�̦_^A>-�����{�2��Y *
93,�/G�z�
93,� ���x3��]�&���D�^W��ɞ-�kGN`L[XI0��X��P��Bl��
93,��Te�\gă��#2�
93,�X�8B=9�+�gi�=1�5
93,"Q3v�,�{��G#"
93,��o@G�1���ARLq��%A�I<
93,�@�
93,"ώdh���40>,S`��`�:"
93,�]GO�
93,�u��}�v��c��V�vBd�+��{Baj%�XBS��H��0U�[�n����
93,�`8�e1�r
93,��%	�_�;!y-�퍙m��A���a���2a�l�/�t�>�δ߁ٻ�_�ϣU������{������k�˺��6
93,'��DmV�Y�㮇�'4���&=�1P
93,�����N�_M
93,$�2=p{!�	��oH
93,g�z
93,(��4@��(�
93,vI���F|��'ʽ j_x�)�U��3I�Kil)�վNO���	��`TU�宕�q� �w_?�
93,���r���t6��v��5
93,�n�2Rz~~�r
93,"�`��/)�@�/��2�',��6�GW����t""`�p�"
93,�<�Տ��aB�wR
93,��v<
93,w;]��e��$=��S)��Pzء�}�Ʉ^��4q���\��;#;}�T<��.�o��zZm*�Y��
93,��cr����K&J�Ҩ=�79��ę�������(�q/h迪��OdpI��XX��#��҆G�
93,ګ�6B��I@����$�J�cO���/�$p
93,"Q{ �p�B8r�`ofd���+��R�.�ŊO�pqAh""�VE$(��R�aޗ��"
93,��X�;m
93,R�I#R�2{8Io����S�~e���&�a�
93,��dz���52)s�0�k8�:
93,����sR��;O�`��:�ܵ�i.�܀EO�~�Xº�����8�rI��i��GA���
93,H�d����:x�	���Et�β��8�@��c��u��|$>��\�S�w�Y�x��{�5�)^Htcq��WW�DR�
93,")����>�f�S���sۤ�f}4�L�8k��b�]b�-.)�v���.I�fv�Ϙ3���70hAmR[4���E��`�M�!q�lh�nE��""�U��-9����Q�"
93,*���U�ZlѾ1��ai�y�+���7b
93,�W��O��v�
93,51�t��
93,�����6��ٍ�i#v���(^�C���Dz+֤] g�1�a��*�;{���X)}&���<���9I��^�M�10qψ�1�����xjܐ���)�QL�$yv�ި�`��_��
93,xh_�܌]�
93,"�^�/����ʫ�c&�k�@����Z.d{P��H&�""�L�&�ɥ���l[��D��"
93,"�!(�[���d��ܩ�*SK��|~H��(����*tI��,���C%/xP��R�Gy��Kf�Dn&6T�20MN:?`47�H���,N1h�/�/o��?��/�0�l� ���ҁѰ;��$tg��_(Q1��m|I���LpL�ߒfj-n�#�=�f8t�x�Q'7)f�O��мU+M��WO�rԗ��\\t,z�׫(��x�Ri�T�a�f�2�B���%��/2yCMy5ְlp��"
93,"�	�Hx�c<���Z�?�^�""����	���*}^)���T�,�ӷ�:إj9/���+ѵ�"
93,c�HS>�i��.C�$�|I�1}�հ�݌�$�E��Y�kd�ss�k/k�}
93,�:��m�쐢[|4��)fhm�JOS�k�U�ʕ�Ik�+8��[}s�k򆚈��t|
93,"���X�fg�v��N�b�Vس0�N�0T�4Q,L�B�pg�B�*"
93,u�TrP(wG��!ۜR!G�7e:�WC�
93,"+�b���\K�^""Ǖ�i�vgq�Œ^B_�|���,~�*�~��~���w������=�k_n�?���O���s�g�6"
93,�{1<����l:�<��͊
93,��i��L&~g4
93,��^�3�ƽ�t��;]r�0�+L�$�^4ogW�c
93,w�	;
93,{#�\���ւb^L~{]dxG��tk/
93,G�M�y�v�
93,"iX:B��c#���7�n��e��g[H8tn*ig�UOG��L@,���״|�M#�ڤ�K�D���c�\ϖ���f�v��މ�����:��!d�t�RgK""�˺U�"
93,�n�{���pb
93,",}tW�z���;�^`�&s�,�;��"
93,"z�,ظ`��3@*OV�ķ�R�"
93,"xb��,�F�x�MV���Q����at�7�����/�&D�}��R�QÒ"
93,ʞS>�_�j��'����+�J;P
93,�u�`:��{�I@�%�Jv��@���a�/ng�
93,�J{
93,]S�4�A����
93,/��:�}�4C���N����v4�VNݮ^ǡ��}���A#+<�@|E!S_
93,`.�:�h�Bw�t+@1�
93,"������aQ��$�7��*""|F�T�Ř�RvEfl�[���#ُ(��"
93,",,���4�|�`�n��m�e����JJ�݊�#��{�p�72��d�v����ɥ�|�����à�"
93,",��-���яZ*Zc�"
93,���9~Gu��Gc!���w��YI9�S�e��l<�]�t�X�z��<����C�1Ӑ����<2{H�s�l� ̤X)
93,����
93,"���fXz��*Fc�!8��!��dS�&����}0N3~�(��/f$�,��d/Eo�?2cps�K"
93,)�xz��
93,&�:�^���7�%�t���s��dI��S�H_�
93,kA��*W�S}��`���V:��d�W�*�eT)m�8gṴK2��G�H�����G�9�[�]Q�a�.��Â�N�
93,�y��� �x���@8�����JvV���NH��Q��+�Az�9�aq�7Y�� ��F�ۈ�C�:�
93,T�ޏ�m�eʙh�Ê�-'c�~G��&˪�^֚�@K��*[m����H��%�/�Ⱦ��� ٓ�
93,����7�ٿ��p����Y<�t$�-hsE�^J
93,�=��6:U6��ĳ�:n���m�-qB%�t0컊5D����#rl��
93,�z�\�
93,"�2��i��XϹK�By"")X�+J2b������F� Y��.���x�i"
93,g���w
93,"�a��D&�I���n�ȷj�$�����Uj�&=G�""����X�r �\&"
93,C.nɐU�7�XB
93,M4��V���v��U�/����N0<|SVa�:f��U
93,F	�����
93,vQ��5�9�ؘ���{����n���Fq���[3�2� z��l�����WSpO؜Ǖ^2���Y�6�
93,�_���\-�<3
93,�'8+�����!��e
93,"V֢_��F�0��oM�T��A=""�T�aJ�|F�����X��T���T� =���"
93,"*�,Yf�|BN�rF� �;�����<M��"
93,*o=�'����zl����78P~
93,c�Q
93,"��y��'I�� �����N""�? �풇��$�Xx��^�����2��p2��»�!�n�u�}V�t'�"
93,��b�DQ�L�Z�Z�h
93,H|��=KW�4L�
93,���O�#r��^���'��ھ�5�A��ȅ\����7%��L
93,"U��_�M�j��a�������$�� [�К�'�zՓ^�biTW&��lL�֖F�s����W 8(@9���=���${���>ｆXD\��N5�,\��Y�pƼ���_��$�P�����+bK����=�y�7	�^:�İP��Ӑ�+"
93,"a<��i;!G��q�7ʨYb��z�v""���v�"
93,�]t��rE�����(}e��x���g:���y=��-<�_1��%iH[(�ɢ�g8\
93,GyF~�������V�W:
93,LF.b�%��ar�m���1l��'��_NY1�6���5`JχW��Ģ���#�l���@�2��s���Z�H	*�b�lkD���XEMe�ڏH�
93,O��<7=�e��YY�.��q6�4)����
93,Oac�'�|_���}��r���
93,��1(���0q��SД`\.�㙿#�/|���/���Ңo
93,"��{~���-�n���x�1p����ɦiVv��ܒ� z� �Is��מ�s�s�{Cj-����i��""���g�1���:"
93,��b�D�����$N�]4h����H��ű���
93,"?U��{��d�k�X�,� ���n��`N��u������Ï�i<��kBb"
93,��`{伙?G�����{�p������t;�.����E�pN�54����6���/��/�QZI�^w��w&�
93,�c0����h
93,D�q�;���o;���x؝z�p<�@��]�?��zC/��'��a�E�Br�3'��(�6����f;Yn��=捊եX��b��jڌz�ig ��g��x����g�n�۟��6��/�����E�Ν�M�W��iw:
93,��}�o�����l8
93,MP
93,"����G�-�q� �""�z���:ް���݉�΂�h؍F��M6�3X��N���Y"
93,͋h��}&��RT���y�[J�ji�o���-���5���Y� [qwϷ�c*�
93,�#?����q�O`���`��q8��ix��%�`0A&�
93,��I��߆�
93,�Q8���6
93,"L��p��c?�����w���""��$�ld�<����x4��Qo<��)��?�u{�0�t�Ի"
93,�>�v����=o8
93,��l�����|ţ�a/'��lr|�Fٺ� ��E���k�Q�� �z�0����3
93,Ӊ7v��t8��¶���Ðڤ	F��4(3`Y�Q��f��+�^���'
93,`���dV�s��.s�*�݁�	�}�o��t����
93,�~'u��Y0�bG��-�=w�
93,8yGU{
93,"�Y��d~�u����ǣ�"";"
93,�*� ��zr���^�y���z`�NQ��v`A�a���I֛u:Q�[��~{��*�^��N8w�Ѭ?�z�0�Ͻ�l��WP�E�����I�Z�{�aϛE#��&�g0
93,v�Q�;
93,f�h\p��}8Ih�����x:�����{��
93,fA�;	;��`8�M;��i8�X��
93,2�Y��r�r�`������%�Lx�mF���_9}�y��Ϟ}��O?s�<�+4z��z��/��Q�7Kf΅�k`�������O??����$���|~���OI��O;����>?
93,"����种V�W�5�i�V�ػჂ�}�""��G�\��8�5`�1�����s���ΰρ%�����M1��߹�ҋA���6noρ��9�.��3(r�/F%��h"
93,�>D��6ۮ�Q����n�IZY+:�ϝ�I����
93,y�_���i���Y�V�O�jsv������&�����I��<��
93,�<ϓ�ֆy�>��$i���~ r
93,�Ƴ���ټ�LX��mQ�����Emr���h��@Y�
93,����MvA˯t/�	�x�0�'9��6�i��?��y�����IԞC�pP��A
93,�m��Vzqr�6�I�Z�jM�<��m�v�6/ݜ�
93,"�k������""�,q�,	��ήc$?@�m͍�2�7�XAD��|��S�=�u?��""o�����~�!}Ұ�M$��w^뜽&�"
93,g���P_Q$@����Ԗ������oF�g_��>Gf��ғ^Q� �<�'}�I�T�<'C��
93,����h��7�#Q����E_l'ւ�[V{����P�E
93,�3�a�
93,������$}���L&���4�D�g�r搋��S#�wp.|4��(�g��O�
93,�gq
93,�pywI.�=i�Y�h���'�3\Aab5����]�<�$
93,"�����u��9{ͻm������y�+�E#O6�V@2I_4���%���LG@t[̬��w�""��j�Ϣ�IЊ�""XAI�"
93,�S�����s�󼝥�
93,h���5����v�����
93,�sä�5���4Y��$�h\o�W��TNg�Vv���-�Q/Z�6��Ŏ�
93,z�*/���jm��� ����j�H��0|����8�qu9i|��Q��й��@�
93,",}�[*�4d9����xR.=�Y16A"
93,%�Ue�W���중L� �ܸ�t
93,���v�t�
93,k���[�7�<@��n�
93,"b��񩰰qy�].oOHc_���m��""���N�(f�~������q�G��������m$I��z�(vw��dP��TTA)�A��R�����3sg��"
93,)�`G%1��w� ��� ��
93,f���.�׳o�c�c��}�53�82H��̬����8x����Nnn�W���R
93,�F.9@�����J���.�����
93,�K�I�Ly���T9w�Q0��I* �D�c�EXA�/؇-����������(�9c)z��
93,�+ �-VA�����tm
93,��1u�8lG
93,.;*J-
93,�EZ�@�^�
93,���?�5X�]'��'���b9�H����E�x�k��g�.m߻�%��0R H���u��t�{�q�W+�֓����rL�+B�����{1�(�k��HH.��w �d�s��]|_���x<yd�J^���CL�J� 3�_�(Ԗ��$���
93,=��Һ���
93,�?m
93,"�O�j&�P�e} �s_��*,WO���5+i8�M����>h:�j�X�28""����$�`ݦ Q*鏛""�4VP���F�lB]Z��V(��/��"
93, ~���I�V��*�N
93,�IR�׃�a�9�wH<؇7b�5}	��=��C
93,�]�9*�i�^
93,���->���q�蜔|�Gh�y�l�~��]�F��=�6/R/$�_`��Ԅ�D�o��.�X
93,"�	:7�e 1�A��c�G�z%��Σ���Q����O����a ,xJa"
93,O�+�[�?����̝h��7���OSBA���
93,@}��1�> ��;��3'�0��� ��#y�@[���M
93,n(d�{���tK
93,"��""�g�+ץT"
93,@���4�SY�:�[����l�ںi!��
93,������.(;
93,Ј�Z�!fp��5��/ ��j����>�/�gr��ʣ�ڹKuA�5�S����x�`��=
93,-=�/\ �}~���]Z�
93,"�m""�'"
93,"ڱ?П�҂�@nF��i��i1�i�z:n��-Ϊ��m�ṉ�-����3�[�OG��3�V��,˨v�z�i=��j3A����?�_�c��s�/;e�����~��çm���'���e�"
93,a��ll��
93,��
93,"��""�4���nQ�s�F�z"
93,�H��6=e�7 ����A�T��
93,"ާ�B��-�����@��fS�m�����lb۲���x�$J J_�Z,��;=�4�hQC�����""�AH����Y�(��l9\t"
93,��h��C���
93,Q�B��r_
93,Pj�Ѷ�O~��~0�fO����ا�	�E��(�l��Q���o��tҷL���XP���<4J�(�
93,"���C�bE���NT�M.����Y���Q�DJ�6,�Z3��n�g��`�2��ۤ�"
93,K����?���nn�S��e�y
93,pS�U�gv�W�����Z�m�lQ\v���vD�tD50Ţ
93,RD] I��L4+|��Xٲ7��k1����!i��
93,"�B�c%��S,��kF��E�,T,�.�U�n�v8*M�������t�B84��{l� ���L-[#7(&�*-YWXS0��R%p� d�ŝ��M�ν"
93,"�gq6�2��E�%���z��,"
93,"`�����x{���G��)�HsX̙�,�K�;�"
93,"��%��4�p8*6��]�ܬ�{�j���9��n�k݆���0�9g9/����sf�Г"""
93,x�#Y
93,e����C����6E�@�|y� �z��
93,"��b.�(���7x7��""u��訳q�u��b�S!�o�;��""��W"
93,7�q��
93,"""i>߮5�F�0�֭�L����XU�(y�ޚ'y-���@��%��3ӝ W���+�i��4�A���ʓ%6��Djq��>�r�{� �>m���CDH`�=�޼Y!���󜇯�����>�����$I0ah-��̘3�N��EV�$8��%K���cֹ3w�Gk6D�F�|\�I�E^R�fa�9q�蕢ܠI���dY_>M��p���"
93,�/H��777�
93,���D@b��]��
93,������
93,Ld)�؋6ʉ�ɲx �1
93,"�׻��p�F��_�˓��1��8=�z���2}J ���F 莖,��"
93,"�x{IYB5��fvN�W���""�K�V��.�玧���ʭ?(Q�[�t�6�*�Lq@��"
93,���çl���>/qq�1S �~c;g�
93,��;������K��
93,���gh{&�h��ie�):
93,� ��J�I�`F ���·�.^t��{�;b�9^��r���ڲ=�����[t�� ���Վ�$O�����
93,`��ׄ5J;
93,�K��~TU���opo
93,9��G�����X���
93,X��� 9���;G?Y�%�c�������XPHفɸN3�E>v~��	��5�f��t��9!��0��°���m�6a�i���r��i��cb���`�>��b�%m��X
93,��E�������N1��ƞ���Th�G=���T��h?Gw[�
93,"?śM�X��x:�#�>5,�r�3�u�W��@�}�m:��zsd1�<��"
93,���
93,��O�l���W`�ݹu�!t��s��d�	���� �
93,��3 ��S	��y��@
93,���.8���BY/n�
93,{�W�Y���Ą����h(���3z'�Q�{�sS�y�!�^qg
93,}a�~܌�
93,W��7�+�e��g��Rd�W֢�)��1��?W�2+v(�a���.1�]&
93,p�����u
93,"�PJU�-ܠ)D�Qr�=��뮐zw""���6~�%�WG�e�og��le����6�¾�f>�6l,���\#�_����}"
93,��p�os_��)r�<��
93,��U��L�X
93,"��P\ lJ[PF6F�GTl���""�!�"
93,�$^D��̋޽����8�S	�F�@�y���������{?p�
93,�ݔ
93,�{`���y��}$g<��>�j0�y�:��x(�C;����Kސ��0�߁+� �o��Nz���4
93,�b�U������a���v.A����$+���K
93,�ldc7y�]
93,��͈%�c污<:�V��Q~���t���c��1���~��
93,"ƺ�*�r����Jo��f�2jLk6��i,�!�"
93,3�|�!��(���ўpU���g~�)Gܜ	�
93,"�-t���(��h����dS�ͨ�7d�W�,�A�^���ӿe��Sh��Sz�A����:ڋ��K7���l���d��w�էM�������}ڦ��7h����+�ɽr�0���i3z0���"
93,�o1��ԫ�p�rv`ʙshg������(�j�.I����e�7e�WK7q˙��m�
93,�hhbS67�QC{)z���}�0����?��]+��򚇧����+����.�
93,�����]��o�)�� \��7n��A�B
93,R3j�/���c��I�Z&>W��
93,�Q�i{b��O������͍0��W0RG
93,�|���Et2c�p��/���k
93,�xP�������i
93,��͌pgc��9��A�
93,�H��v�Rn�b
93,�BAB����xƊ z�	n 2|ƙ
93,�:X��Ƅ�]3T���o�a_����ɝNA��p��a�
93,����߲r��~�:�Nz�0l�K�$g&��JNPDhG@�F?��Ď��
93,"�dF�h�zމO?y�bݻ@l��N<��%ht���lbW�����/.������Wδ]2�#z��/ �Bq�=d/�S/�u�A[y��gp�F�Ǩ�[��!���""�l�i"
93,��I
93,6���h��_A7���RX�؃JW��x�E�~v�P���N�����Nh�~����(�l����PѮ�p戴��A)A�׭�
93,)5�۸�vx �w
93,���S_�K��U�q�G
93,.Q`�-Dn~�'�J�$<6�Ȏ��?��D9�>
93,cB�^���及e�c[*���
93,���Ш�У�3�J
93,0�9�)�����h4
93,�U��O_�BL���W��~��Ul_x�s
93,�R����F@���8}R�B�=��(I}�-�g��ZG��'gL!�Kr
93,U�*��5T)�F��3���Ʀ��J
93,"��z���;��^�����""6b�J�}R��ٔF�Q,z Q��0���e�""m��7D��#�~���""���!�&u�!�3g˺�B_�J�>�6*��?q�W�i��l�ctP;�{M�	p�"
93,E�B� rZ�y䜢D`L��^�[x&�Sa'Uw
93,"�^�7(ɇ��;I�, ~��X��WP~���S�UJ���."
93,4��AD��Q�M%t�Q(v7H�t˲dQ�ڙP
93,�[�R�ޔ5�����
93,;����&K�8�J�M݉L��
93,_�A${}���g�q�
93,[]ݤ��*
93,=qG�m���=�$l��f��8���ɱr��͡���c�h�4z��c
93,'�%m�d�zP
93,4a��yKw��E�]ٜ8�ne�=�G�H��&�>G�k��>W��vR�ҰZc����~
93,to�����
93,"��T3P'ئ�V�-�,�"
93,"c��;�$ӥ��郂""��3"
93,"��+��"""
93,$mx�<�@b-�����e�J[.����`�}�-:>
93,z��<�!{��K����M�F
93,�y��D�{�Y�.6�0K�z�4��8�5�F�@��<��9��~���D ��HtN
93,}0%�}x�����7W�i�(Dl������롳-��%��g���Q+�zc�k��`���2l׎O��w�������+k��Wo]x~��>賗���e
93,������}0
93,h֫���i�jU͉��pҭ�c��������Y�����z]��>�:
93,��p�gHb���y�� N�����1w
93,o#p���*6p
93,"{B|�T*�N��ܚDSR�o6qa,X;I�����XxJ���F�"
93,"W��ZF,��e�"
93,"�R�b�����v��rz.��:(��K�����UCUm��m�'hr�b0""���q.6�}�V��"
93,�r	������G<��5�h��c���0M$������3�kY��`��F����/��+u�d�+���R��-E�*F��Ա����b���6�����
93,"��j5z��s�)��Vqv�&��P-ơ�,���>5�C�ֵ�B�� ��$��V"
93,�5����u&Ǉ�$�e��-	����a �0	����¯0d����:>�)���r�ȝp�{*�Q�3�E�k��w��������0�_y�=w��0�VY� �w�1j1�	�W_��sFX�P�ѝ���������oW
93,�9�3<Q�z�vq�Y����������������ʣ���.�r�<�i�����?�����������+�|<g�Q��<^^� �?�~�$lp��O��B(n�����v�7�{�
93,�VG.���ؽix�A�����b�[����*3M8jM����UK��(K�>hHh��*x�A��}U��
93,E��$!�G)A&dU-V�M9���r���f�A5��U:��T13�����@�Q?\Q�L}v���f~/���*{�Vc��M��g��;#����9��u�_O�Ϛ�^���}��5�ִ^=��/͡yyx�{u���l
93,"M�������~'�6�@'���e��F��ĢV����B""�k����HGLx��*%>���鿤��8�K�*��^��<������n��������]������s�����"
93,"�<���""��UK�T�z3쎏�"
93,V��l����z��|�e�d�;C�[`����
93,x���
93,�W��=�=ך�0�cq!��	@��
93,"�� G� ""W�&����[K Sd�Z2cd^mT"
93,�CSTF��o��
93,�s�×;�@�:(�/L�kk�g�Oi�j�t��N��b��� w�n)���kn�j d���h����~0U�
93,"�{N?��M��_a[,��7M��F��`�!w�	��w�F��?�H�-����"
93,"״�6�x����B�S?��B�""�{|�"
93,2K�&Yt�
93,0)�)�� ߁x���ѪBQ�[�oΑA�@��e8��>0Dq��� ���4�/럏O��Z��^
93,٭K`���#���a{��i
93,"�]�@4=xz,��S�O����m��}Gܱ�"
93,0G�G<4k�EAA:�`���7�N]��=
93,�od��
93,��8�eEdJ~��!Ӣ�\?���;��a�
93,��|5�����Q
93,��ĺ?Z���4*��7�V
93,�r�xZ�:�9��y�
93,�yT�;�2:8�޽qΜ�TWU
93,"��}c8�pf׏s�Lأ�����[�a���r�*��#\z`�ǵ򿕍�Ͼ1\[4�A�""?���?�b"
93,LٍK�H�����sV5�'?����I�ۡJmG�?X@Y�d5�W�0n�������B�V��Uh�
93,��0��VU_Ϻ��w±�$�y��+9��
93,L�𝺴a���]�RƯ
93,"���z˨W��z����8�u�y�|ƫU���""N�"
93,����\s��S�����������
93,i����xK��Mc����M��L����K5�C�Qd��蛋�a�YKǍ;t|#K�
93,]cǫw��������
93,�C���4dn�]�
93,��C	`�Z�CC�t�����~��ݿ��`�
93,"""�_S�"
93,f�����wU��z��w�z�0=o��s�
93,AU}�yrY.�ae>h���;
93,*({�	�|�0���B��Jq�c3!���d�2u'����&�.:8.���Ř��b�Wa��1q�� ��B	���
93,/�W0m�j2����` ��:|�+������2��0�*�.Ka�ɡK�&<�Q����S
93,G�<���
93,�KڞZ(���	�NôZ���#)�
93,�����<�W&#��.��`�`��C�2��>�!����}pe���Cw�Č��C�Z�X
93,�1�	XD����CF��@�6]���C��R�4K�$
93,w���]�45�r��$���9R5
93,"""a^��Ǉ�4�9=�"
93,�?���*���V�g4
93,~>��\Q
93,Ԑ�=!QO���
93,�&�
93,y@t�8VDf́��
93,�-b��	(����(��ҝ�vX(2� �F�^3
93,"F���V���X[""%�X����r�f"
93,i(W̬
93,G�+��5�k�ގ
93,s�<�оΘ{Ӌ�ҩ�I���h�`�������^L��4
93,"qj��'�$K""#k�t/;��	3"
93,"?���""=�(�ȜF�-�Ic�����1�	�xF@;��]_�~"
93,������j���n?��8�N'�c��|�d�89}w�9��f�
93,�#΅'��a��
93,⯌ے|�V�R�PK����#�H���~>)X �����C-�Vx� I%�A��K
93,�*�
93,"��^�@�V""��LE|���&�މB��|��C,��0ˏ�_�g4��G!�]ZK�eh�0H,�	1\��-����>�+?ށ�3ƒ��Xb�`���j:��b�>�7�m�'l� }�YȀ��e��.koV���n����yf����R�ﻍ�@sUBM�H��5 �|��Tm��3���.^��GOD�G� �*�E;��]�ː:����B"
93,�pvCy���0�i�iϸ
93,����q�|����{i�
93,���x
93,4����<��1����hl�!���yZ�&㑣[�R�����&�ٷ�n����x�g���r
93,"��X͌�V�S}Xޡk�������#9�)׮��Ju/C���R&/��}�m��-NV/+�Ó������Ǉ6��E��2 ��2D�0�(|��ܭ""���3����?�%�"
93,ְ�L��R�`����Gf�#
93,�>��2�Z8�ʛ秏6�� ���&�Yr�P�C�d���~����F>r-~~�M|7g�*F&�ue����
93,���T�L
93,$�@g����k>�[�/i
93,�v�
93,"G9�,�Ҽ���]�(��]Z�e �"
93,��ܟM��Fےb�2���ە
93,"��+� �.��$�D�� ��V���K�S�]}���ǋ�-3b,�x̟��I�1j��.[~�	YZ:�h�N�s�ߦX����-��"
93,"֧��Q_u�$��r�%�RA:�Y*Hǻ5�P��""\��-&�󜝈fFN���ߏ�r}�'�V;���l�cIϵ���ǒ���_�I�@)�@�<�8A�xl�t1�r��"
93,"�%�q��L9.��3����#C�r�mO�����5,"
93,Qp!
93,�E�������%�Ն��_)�/nX� _�����
93," �����w�F�N\����eI�N����B_%�,�]�+�"
93,",�9�πB�s$)CΕ"
93,@���u
93,���;�1����R�v
93,;�h��
93,En>�������*
93,".�����_of0�)U/Ur�]~�>��}F�[�Wԃ_�.?	��.nm��e�U��b��,����!"
93,��-�D�ېA�Z����?:��״�X��
93,\~~��e���?�e^�CG
93,��L��Z�k	h�j�X��Bx
93,"�""�"
93,~}�OA�l�����p��}=��WQ�
93,"X�i,�t^�6OR��sN�"""
93,�^]���@a�� {�¾��(��c=6�E�#�z��`޽}
93,��׳Ͳ��	��z�;�
93,� Z|p�п�QK|����٩(��K
93,����
93,~9we����5z�q����
93,����q(u��O��=����Aۢϖq��r�}c�a���5�]X�8��ϗ;�Z�g�
93,+ox�:��>�q�'���Y�\$A0�
93,�`z��<��lII�6�7��r����w
93,�Mk�d�Q=5��V�E�
93,>M�
93,Y(��
93,I5������!(��T�cN(Ґ
93,����3���
93,"�W0*�ʿ��,���o��%�.fs�`ބ>��&w�3�g�@��,"
93,�k*�u
93,�V�����q0�������-0�2��W��w��J�컏~=���Ce��k��<�nی��w�F}�z�(�x
93,Z��ޙ߶�ۃ�����@���%-ǉ�
93,�!���nNS�����Xw)Ϋ�+�o�Fճ����jȇ�7U�
93,�䗕��#Ņo��9�{�!B��@1?���������o'ě{m�@=K���b
93,���WZ�r�+'��J�����v�1���#s�6���E�-*�����䱥��}�B|���N/��WAw�w����AԷ
93,B���Q��?{t��T�R@Yv���eF/J.
93,=�y���ƈ]�}�/G�O>�b_�4�
93,"��YΞGEo5��}���@ $w�k,'�C�[���g��9��f���31��7}""%Ǟ�#�&���yo����{}��|5�f�۳?�C�t����Y�����"
93,�9!�5��{�s��
93,��6+���ưR~r���:����gwz�2&>�ݼ�������Z���_�A�F_��t�nKF���A�<��z�3p����݊�����������Oߤ��G����w@�~� K��<�~e��0�o��`b�k�Db��+n4&?�
93,"���u�Ų��ȵ#<�r���ku*g��+|��eŚ���]Eq�neF����|1p���5��ۮ���ϖ��""""����	t�˚��>�g�&��}p]��0/l��P�"
93,c�+�ٔ{eysjs��vb&
93,὜��1��.�r#
93,~�)LiY���bo��gO�=8��ؽ����۲�C�w���f���3�a_� /��U���	�����6S�2޵�O��
93,ǂ���Wѯ�i)K�7
93,�ų���S���{}�ywEʘ����j��Ǧ��h��2�wk\߾o�F���M��>����{ע3_��V�P���Z�����'�mڻ7q:�&��+�V��}{�I
93,��<��� �J����
93,p�C��V�e�V��ޮw�n����Ŭ��n�+(g�5`�s�~Yc�Ͼ�x��D�/���6nI���J���v��mZ�����ή@��9
93,E�V���3
93,�YRw�u�?p�@a
93,",��ߐ7b7��頦a7��_]�p��+W���"
93,�tӷ��2�-a���-TJ�b�-�
93,��)-�
93,�l�o�ٴ�
93,/m��1{dH�ks�Fw�~��C\z�`����_�p�W<��ȣ
93,"����3Wԏ�,v�b'��G�p�"
93,u� �Ф2��M�
93,"�z{E,c�	�.��8Ή����}����=d���-s�C��EGCXq#/�ٷ���<����e��·/>Qpq�Q�Dd��z��,�[�(sڽ��d�DJ���o��|u�""�ty""���&�RL�;�Z ���0Z2$�C�n�#���$Qj"
93,�.����A
93,"{P����a6�,㋲ic�C�x$M��&����Wpg��b�g(�HWq�����(�P"
93,�ax���s!��(�
93,%�>�+��E'���w=E#��D�͸�@���3*�p���R4�>ft�`�wf���q�E�eT�FڳG�Y�e��۹c���] -x���h�&���d���W��
93,d��b��
93,*�F��>�(a
93,'=�����z�9M�0wЩR�
93,�H�|�Q{<`�s��O�S����c���l�@��͡�4l�x����-
93,�'ǳ��F�~�!'�
93,��P�X�'❟��:3/#�L����mIj��c�#y9c����pE^�LX9oC(�3
93,�e�
93,j�9.a�d3[��&��%�LF�g�=�-.��U�20Ҭ5G�xP�˦���{`�jz&���5��\//����=X7f{�I�� o2��
93,�h���� �ɯ�F��S9���$z��I������JN�y�p
93,�K$���J�`����f~r����)a�3�n'r��=n�B�=��+P/?���-���i�Q�
93,R-�H�a�
93,nOZ?��03/jNnȔ�42������]	<���8���BÜ�~s��=�*�?i7��ۃu#���-g�
93,T�Β�Gix��^sN_=�1g�3Ws?=��4s�)����9�ȣ.9'O
93,L��E:�ӣ��-���0H�K*K�&�
93,_��Ak�B�<
93,f���oyDǵlԖ91;
93,��̍�1?��0{�K�Jy5?{���I�
93,l�fSw�ͧ���By�A�ُ�ɬ�f�z8ꛟWjA����J^��Eٖ��w�jh�lq�tU��6�
93,?����
93,Mkt�qgv�������ۏF'��b�^�@_���s�N
93,wی�������y�56��*<�Ɔ
93,"gK�,["
93,��;v����
93,��R��t8�z�u!���
93,pJc������虅g�զժ�͖�4�
93,�ji�^k��f�FW7U
93,"+��V�tk!mZn�c�6¦�^,n�����{��qߝx&y7:�A��*~�w�H"
93,�f]w��E�s;��	���!��]��=?>f�<�ļs;&�z�`bۃō��q���0�꘍�-��;���ׂ�dϪG�䠁��Pu������֮>Ë���w�b\�պj��Ѕj��'K��\�mq��W�][�����=6
93,ئO���~-�
93,"Tm�X�R^��m��	�7f��&�u�""���"
93,Q�H���Zyw�i	�5)$Q����~5Ki^
93,"#0""�!V���7���P��l���=P"
93,q��6�%H#Hp�w=u2PK-�F���ZȞL��� �(@wr�
93,"`��q,"
93,�ƴ��E2��g
93,"^q�)r kZ36qG����O�4���ʪq ����H۞&��φ���pr\""��SX�}�y��-�qO9���e��m]X�9A��"
93,m�j����'�U��N^ďn����G���L���j-��ţ��?<�b
93,n�'�#~x�^��J�#
93,��v�����
93,UMbp������
93,"�""΢n�(��ʢ�f ���@7z��)^gCs�_@~_w�鮈M1�-��Ա;��s�X۳���R��+�Ł�ΐ\c(��gg���;"
93,!�_���&߱�\��/�#asU��^��4m��=0;����
93,�k'��^w
93,��)u���v��SjT9�f@��G�O}���v��� :�z��W
93,��:<1:�j���ė�Ǩe�'_��������������35��Ń��.�n`��)���� �$�'nGYD���q�vsI�k�nK���V��0L��4Z\�����VՌ���'�
93,"�8Ň��C�V{t�#:0W�����d�j��}��-��]1�b��}*����nV��lhF�h� ��t�1��u���܁�bi�yPؤ��yf�]���(��7U��q.cߺR��zf�b.Ψ@���3���(���n}RB=S����""���ǉ耧thZy򗖡��J]y��@9�p+�@ ���f�$���@b�q�+zF/	�,`D+8N��J��KE̛͛� �@�"
93,ĻMl��ڈ^2���G�n�ѷr7��w0T����w5�����^�x�w?�9u�oNWˉ����H�����A&���OD��?�&�9�/@�(cf!
93,"�Ő@��3Z���8��H""y��7��^oIQ�<�baF~Otd� %ԅ��f��ʆ�z¢"
93,k����`*cNK��1��åp*��u���]���z̲'��O�H�g��]!
93,ne�u��Z
93,����2�=�\�*9:ua���:���a����B��ؙ����5_���8ȿ9�K�-孆[1�5�q0�UӨ��޸�
93,�ҭ�*��7�����a0�W�']����iK9dSEo�`�՟�}�5�g2�oD�L�D�Pl&V�IR�&	93����C
93,�H�JhWFÐ�?WL�+�}˘��i�(ӈ#�=�o��
93,S���j�#���*2'>�
93,"�߀R����c�旫L~���{I5sT6ԋ��a|����%���""��UD��u	��t��=�"
93,�#f;��fA��9�D
93,T�['q
93,Տ]?��k$9z<0c�vQB�K@C�]=�( �UYY{�jUӮ�v]��iӆ�]�.���
93,��]�EpyU���Y/ٵ4�!
93,��%E״K���B�
93,��â��Kr�
93,�\���@
93,]6��4�o)}
93,�i��;�gB�OS��
93,"�}�-i���[Ï����-a��m,��q��׼_�fFP���3�"
93,�1jQk�Q��P�'��#_\g�Y����+��I�RLw��u��T�Z虒�T:�ӶA��� Y!}�I$=H)�
93,�QN_
93,�(�^�T��NNE�d�F�u��Ο6�H�
93,�f�8~���s�+�k�%T�؉M�a�1z���i�lT�6r��bw�
93,��N1�r�H�Z�Z��I��=�����������|�LA=�q���t�����x��oT8嫛��HԊKY��
93,��Z�̀hW{̐��G3|T�p�C��r��l��FŨT���T+z[�W����Zi��㫕�Q�u��T_Ci�������چ�S�+Z
93,�Z����_ͨ�J�Y1�M�w͠�Pn .��.�7�z>�[�^i�M��UzV���q��z���b@W�5��?�Y�N�Ct���x�:�����F����԰*��T�o�ǦY1�T!vA@�#�1h�/Ty�
93,qZ�%��e�6㳅4��y�_��&�o������б�DЀ�}W���o��g���N�]k�6O���
93,��
93,"�.2�����U#z7N��M,|��WZ*Í�z�گ��d�"" �m���Ѿ����&���Zj�_���Ŝ�K��ԝ"
93,�_�f�O~v����O�����k(w/s��b�*��d
93,������G�+��
93,s�f�
93,"�N@�,vt=K��8;vX�kcvk6%�)#�"
93,"Y�~�k�^j�l'�""�������9seW\PM�"
93,��mER� ��:$��B0_ �庁\���!% �`��Ѻn t�
93,5[h���Q-PH(.������c'�H(���l��ͅ&Ì�m~%(.%
93,"��""R1;� ta�(+�X��(��l��ϟ��)��J�Y <�-Z�"
93,"��ZQN),+"
93,��O�K
93,0l�\\��HY�k��V��U�Q���1�$f�7a!ސ~�߭ a
93,�G��
93,y��r���	�o;�
93,gJ}�F C� W�^���!��
93,��+)J>�Sr0<c_t����$A1+ʮ@�NZë�4L�R<2�p�V2A��?B9'^��p#�}�
93,����
93,�+@���$���+(	�[ˆ�!n.���
93,9�a��<��
93,���ȴ�+���-$�w ���SO�N-B�dc���2K
93,�.Wp
93,�J�������e>m<���@G���B�3�e
93,"m�׋�dz�	���Y�uR������s���L<VH���O�Y Z�""�)S��"
93,���S9�/˳8�#
93,�-i6�v5pq�
93,V�s���6�
93,"!��;~G� Z+���o��TK�(w�'Y�2&M^��A�?���[��rQ`�)�=C��(M�L��=�u���Lv�+-e\V���}5�)ɘ+�F��1ѫ,��<	h@*M��N�8�m�1@0У@ޓ�y� 0����}�"
93,���P��%]Ӝ�m1f�{-�)�:f�
93,�	YqyT�W��++��܋I#�}�h
93,�:��ʊ�
93,�C�%j[��b9�7�l
93,S�Y��-2g��[��w�I�1a
93,"����""�<"
93,"Qk%tD����	�	���)��oO��I:�Z+�|��vV���,u�;�t"
93,:]�>8ߏ�!�&�[
93,"j�ݬ��SW?�|���!��V������Q��p�q""<�����@�s"
93,��̐;�
93,g-��q����.�<� �
93,"����P.�b \��Y���G��""u.-\����ɧ_Ҿ��G��%}�_�3�wuT�CV��q��M��=���  r�E�r4�0��4S��p:�P�O���"
93,��]`��Ǩ��� �{(��� ۥ\-�F
93,J�#q��m�
93,-����oi+�xW&1�­��4m��E�e�e���(�ra��<&����%�K(�V	�����J@�\�����
93,�QrHtvJ���d�!�ڲ��
93,I2Z��*�;
93,[PM
93,Q�7�%���c2����~l0�R�V�%���h��)}|���m)���j��N
93,�5{-�
93,�?qi�í*~�F���I[���\@đ����#B�X��q��{Q��Q1�
93,!�]F���C]����>p��=�����O��# ���
93,"���a""��˒֗�f�夹�"
93, e}�&
93,�P�
93,�M���(�ڙM�
93,�����u����B�{R
93,��Q-4�j���Z��I-|w}x�ݾ��Z���[Q
93,"�]?�'�X��i�g��9&tJ倢������7\Ps�b�rZR��[9�!���hK�byb�r��RvQ�O�L�	DP4>�����CZ	�և]� I��i�V*�i@""lp+5�e���n""P�'�b"
93,"}""���"
93,"1�=���E""�=���ȇ`>1AЏ��B����L���p�;|�"
93,!�O�
93,��C5��/�`X)ǉ�xt�NH
93,�(��K�F�m�=/�
93,��0`K(�W��D!�!c�.�ў�c���	���k%�v
93,CB^oB�e@ƾB&�
93,�f�c�s�P
93,g��LDw�I�.��
93, �3��Ooud7��
93,y4�����$�4qD��)�x;�ͳS�rq[)
93,�RI���ȥ�
93,���X
93,�?X���Ec�4�dP4R�
93,"�����`;[�������q|k�""cqTpq��Co!Ǣ���"
93,k�ΐ���4g����fA�d�zį�Wwe���:��p���h�)��V�}�T����
93,_+���1�v+
93,���h�+���g)��
93,"SH��Ȃ��$DD,I=�zJ"
93,E��Ýp�����
93,�1�a#�ܾ.ӧ=�)+�]���aӻ�҂��F��
93,"f�G�<ր��,�ȕV&�TY�}�]am�+]j(��ԧ�^"
93,8�S��z��չS
93,"��Nz��d�j�,m�)�_2XW"
93,����p��OB���~0%pҦ�
93,*er��
93,��X��Ftla����
93,�Za��J��0Kqɯ�H�����t*ݥ]e����؅�@�hk:���`ǈ�Lx�ɨ돷�{q�� 
93,����Ɔl*���FpK�_zkA
93,�Ͽ�~�Pb(�%�	C��N�.�H��:}��w�P�a�q
93,"O�`�Ք����m�3����ZQ�:[� ��J,y��7"
93,;	7��
93,��_��#�I�+��dX��Ϋ��b!K
93,"�[�`�FƾKc�,�m)U�ԓ�fHg�{rC�zХJ�@?�"
93,�{Z���0�}#3����I��B Fl�eB�(�_ k	��uD���o
93,����M>�
93,3-Hg�]x/1
93,"_:�������d7ŬE����n�daKx�#7�7�L��<�D-�I��#���Hh�sSN���?��(E n��e1""�H"
93,��UL��匛ߝ)����Պ�(�n�χ�x�aE/��g˙��ko���7����K!��
93,"� ��U8��&��2��B&ҟ�a�����0yȮ��z����s��.m&=�9��>�J�h��:==ƛH�x��'>m�e�T9�N,١�g�pٿ�|,=��Q��q/P�2A�B�]ɂ@"
93,"I��R3W6�Z&""[.�$��"
93,�p2���~ؘ�7�	 &�?Ћ�DnnF�-~�#�:2A�!�=�nS��l�
93,{J����f��_Cvm'�D�2]��ѳA�)
93,"��z��H���T7&gZt=L0:������,r�`��xkX���h��b�Z�q?�H�C 誓3_儲$X��Eqy�&�u�w.���	V}@b�;�%��Z�bbz��=�f)�7i_�X��Y@�$m	�,�,!!�'�^�x�8'��v���xS1c�ɱ0�ƇYY"
93,"O[�;`����X8GN��� ���z"".D�"
93,3�|�p�C�=��}oE$%i��HdU�m3uY���.	��Z�*�(#��
93,9oSOz�QH9#O��
93,"P���Y�^�H��͆,B�"
93,"�PԤ*n�ӽ�?�vy�{�.�}���?�vymE��z7�D{sz����g��vy�5����vŻ�I�(!��,x=���r���o�nz��A0t6%s��60u�s����^Ħ��C��"
93,����d=�'@����Ϩ(Ϧ�(�������.
93,�=8ۼl׏�Q_�
93,%��ⵊZ�Z��d
93,=F9�r�L䤽$��?�����=
93,O��x���r362**z��
93,&ވ����CB�!3�p#%q=���3��~c{�r�ܑ���
93,���{�F�
93,��tw�V����°
93,"x�=�ډY$�b`�,�D��mq��7�C�""""���""E���n!u&�FÎ"
93,�$.�5iL��
93,��X ���@�ⲻw�9h~bwl��
93,X!l0��d���i
93,}	5s�0��cN�-�D�̲�
93,6��
93,|�e�������+��B|C�36�n 4*a
93,���J��>���TF˫'��
93,�?��=y��*�@D�@?q
93,"""%�E���f����/�, ŀv�3�h�,F�	�������O�FƇ�iG.�""qB@,���hڛ a�Mc�������f�D�x(���1 �)I���"
93,�� �jH65(#�hH�P�V>a��u
93,�w��Cو!9;��x��̖x���ꎐ��wn:D�r��F��
93,"��m(�N�� �n�'&��2�y�{���""\��ƒ���Ge��h"
93,��s+
93,EG��
93,\�`�PaJ�d
93,��I#��qp��
93,"a롛%,>+n�'xf�U�*�=�\>��8��c�� �g/�-�lUM�T�)�s\5I)@"
93,"�t^)<,"
93,�F)
93,�GfH�O[�ǩ�hn���%-�\/��-��sh�@9��Z 	��G�ixi�-��y݉�)΀;u�rsIj@t�!�
93,�z�b`�>���S���
93,"S�#����Į�g�#U�>ط="":g_/=(�;����"
93,���f��Y
93,_/=�苯=2�^�
93,v1�D?�B��b��-�
93,��V�PKO�E�r��e� �����m??���h�妰Hg��:Q�<���N��s��MYm&�4Ov-�8�p�((���%�8F�G>���ޒw=�ћ3�	����A|HC:�����D��<�
93,"b��j�|),�LQ�1���b1�u1����Aj����c�"
93,�Y��i
93,"�%,�Y5WF�!r��):D�3��	H<�$6���͇�w/7�+#LC3�r{��xxx��i/U!x��"
93,"��7�%��-t�����|į�/�8A,�g�)����>]zOG�\�@�*d*�������F �;�����"
93,v'~X�4
93,�^VC%J9��rB@��S�++�B���g���3#�fP�@���z��Ȃ-I�PO|I
93,��NB��2�U����l4L��\��ۿ��]#K�BF}iI�Ԕ��(����5
93,"�|��K�n맔�p�K""�"
93,�9�<�V*
93,~�����9t�
93,�kJ�3��>N]��+�c�_�
93,��6p`kʻ)j�ġ���|}�Ce��F�Zo�D�p���[��eH�������I��-۩�
93,������;(�e#m�s�ljwr�1�O���黓���fs��S$��I7���M�ptzx/7�C�w��C�$�'6RVJoꟉ3�q��3o
93,il6I*�GUcZ��;��� {�Y7e�u�k�FJ�:�x�Ɍ��;�֦]�c&�<�
93,Q&<ʔ�
93,"oy�ΈZ�w��><��ֲ,+�o��b�o�j]���"
93,O�*�G����Y���ҍ�\��݂p
93,J�Y�`���T�֙��lΖ^kO�S��(7�6q�C
93,�V�H��6���R�[ �'����d�e儱���!\�q'V�~
93,�yE0%���D��>���W�Y�59Z�f=:bv�('�qF�l0��
93,���ب�}����=6����KR7.9��P����@��NM�
93,"�9źy�V�""�=�XD��"
93,�e.L�%%L^
93,���+�O�J1�e�XN���M�TZ-*�u����
93,��v{��B�N�$���Me�2�:��*�Sqӗ�
93,Bc#��I:��R��
93,%���IΚH�5�:��˖�m�����(�4��y[�D`2X�����0X�v�����_}�_2�TJ)��j�Z6ꕖ�6*m�FeT�j����Q���J�Le��ǧe(S�
93,*ch��)G5~VD�z������~
93,��58i��Z�a
93,"�שF�.�ω����R��]�""��n�غk��5�	����x#���_ە�I�ŻV����4�~�	�i��j��o��m#�O������4h�#R(y�h"
93,"�1 08�3�50�I�@�As��bj������=2}�H;��̌�Q���,�ތ�p��+���RH�[��+""�$��^�I�#�'UN8h�(�Ӹm)/��sΤ��4�Ո���K;@Ha"
93,�����8����uif@xx�#u�u��(4�
93,Pp��=�_���3:J؁����>ˍq5s��<	-.��dh��b�
93,�;	�'ޣ /
93,�8 U��*�y���Z�h�qT���?��O	�O	�O	�O	�O	�O	�O	����A4��.�'Ň�n2
93,���q5!
93,gc��3'E&xx��0���a/l܊�@ԸLQ����v��f�-��K�ʡ0�M0PȈ���d�?~=��JP�2
93,�q=vp�֜��J�X$��/�����
93,"@�(�G9����F�a���L���S���Ϗ��d�C�<`}��""S9�S�ć�5��c�5"
93,"�E�c�T�#[D����S2""�0P�̣�j���0�9��ɷ���o����Rd!P���h�I�y�4���J���N�F:����mC�8䝄~�*�x�0Ӯ�^��E/g�/|'0������'"
93,*����ߌ�/w��
93,�?K�k����#�E��[u�3�1�l�
93,�g�z���Z���
93,Ӭ��ي��ǃ�
93,�og�&i�W�v	)�i��_�Vi�'
93,��m�l����i@SzSB�����|V��ka�@
93,��֚��B�M��M���^�޶���_5Z��� R
93,+���!_0����P����]�V�d�t��R}���v��*U�E
93,0����\��^U �.W�4�V�t��jW�_ѕTO}՚�&�m��	0�
93,��N��b:�
93,�+~D�M(�E���VG��xu�)��ok�rP�֨�8�Fx
93,"�4��ۚmdFP@��\�^�\Hr�ey�k{t�a*1'*3+*��""Q�J��IE����7�"
93,"��Q�C<Q��k�����������;4*���O} ��}	d��1^�J�?A�jѨ4��B���k5L�pk5kk@u�����pߪ��������	�j�%J��l-��P�m�8��v�.�k@ӑ���C��	̇.P�e%d��z	���ƀ��F��S�F����u����04.A�h`��w""�o9�a��Dz�DJ����;��o�>���rDJC �Qi8�s�h�Wojڟd��#�H6��#jW�@��:��5 �(����5���Z��Z��+����J�"
93,Z-ďx� �>�j5�	@��PG��l��l�V7�W�Q#���@a�YVr
93,�M�]�9P�Qo�4���Hŏ(�TGQ3��m�%��=n��^i�t|\3���eCG�I
93,�ӵ��+��@�u�^�bZ��Rn�L44G7>� ����x��s]�nG����aWۍ��+Dg
93,�� Ꟈ���9K
93,��J�l�+�\�$�$>P�7�
93,��R�FW�'���*�Fo�R
93,�o�
93,�FP���_Q�NX��4Ȭ
93,*�P�߸LS�2�t�
93,"0�*.�n��?1�*rJ��n�e����I�5aE �J����Qo ���""��r4f�A#oʿј�s�n"
93,�p�E��AAqG�c���Ū��X���K�(�7={
93,l�'\Q)��%�%��%�ޱ�2�{T���p�*n��6��6��<�
93,�KSqw�7��U��W�-	�@�w�/�V^�3���
93,�Z� F 9�7���y*���^g��!LO�c#�
93,"Ko������t]kZac`����v�""/ݔ���v���7����;�ZsY�iU��'"
93,�Y����*p����
93,4�*���]b4�h
93,�lRuy��X%cR�)&md>M��
93,�֨T�jE7����
93,"ꄁt����:�F���M��� ��""�+XZ�ZoC�f��B���BQ`��Tjն������V�""���[��*�v0�.�Ah�	ũ�n���%��ȑ�'��z7`0M`�uR}Z�%Chy:�hz"
93,�vI&�Z
93,C�x$�Q���5��MB�8��ncl�u��Wu������6�r��~�����+�d�(4�����q|��4��u��UQ
93,��6
93,GG��	��v%
93,(_̉��t*��C�^A(2 8}�V�f�ר�+��V�d
93,��*�j��]�u�jCS�n���j��-z����u|W՛(�4Zj�Rm����������N.����Wa�W60��
93,����#p�~����AgCy*N�woЉH�ɻ��
93,�0����l������4��Y�7�Fc�b�Q�����o��.���Z�U{r��#�TT�i�h5�:-j�y)Ũ*� �;�
93,��6ࡪ+C��L�
93,"�Ї�aP!<�+|H�.-���7��;'���^Nq����I:�eC�00����Yն�rg�m��@�s�pޛn�s�����%���7���]�FK�����+�K�Y0�g��`�z�""� 7I�bŃ=sr��T��)G��|'	""4<˾�MF���pb"
93,"�h�DzC��	���}�Hw��	��g�'�8���oNR�""�>�(��ݽN�#���+�����c��1ݨ� |������'h�e�@W�#tIe��=U����y"
93,�Pk&ы(۞�Pd�0h���ܱ8
93,�T�Q�r�sc�2$��-����a�\ ��Jt9)�SyB�Ȝkæ(䎊�x��S&�v��H�$�Qڅ3�ؑ���2��n��yͧ��'5u.y�v�:ǁ�e��[W�S��V3U��$Y��Mz
93,SR0t
93,N;o�֩b�
93,�NH?[�F�T��Z�^H~��� ^�I�LE�K�%
93,���L@��t
93,�(��
93,@Z��g!�P95L��H@lG�7
93,SKOUJ������U6
93,f���s�U:v�������a���L2��7�����D����{�
93,�Se�
93,w��}���(
93,>��U�<�~/��gU�����5������wF��/ [��Z���������[��w/_�
93,iM���i��
93,����W�B~��ϯ.��F��NG����O�bxꕇ�
93,^!�
93,�/�@�J�	�#3�_仢4c�3�Č_S
93,���:~�m6��
93,�*�̜
93,!wH�
93,5���wi�
93,���7*�t�+1MQ�&C͉Ś�fA(	eJ
93,D�U�'�����ǘf-�eh�	ZXr����VSQ�7����6O@3mմn���RXJQ
93,dCbЋ�16!{qL0�{݆�I���؅
93,��<�A�?�He �ҫL���:�jRP�`bM��6�����
93,"�${�N䲜�7bY^߿�l��ߟP��Y&���9�j�b5�,�ٟ2��2��"
93,�<�ʔg����)�yK�Vv�����J�=�\|��~�Tb�$�
93,�+�-2�
93,@D3ˊhPrUm)��ؚ�����p�Y�4q���cRj��}S\0��&}�u
93,�} �1�=Ԧx-?6sű�Jq��v��LW0 Z��L^�c6�y�I^3�4��;՚
93,�·����
93,�#�H<�1δ5�VeD�P
93,��ҙ1��DV|8 Ŭ�^���vį<@2k��A �r���G�`1X�
93,�c� |��˧zmS�n��L��!������L�:�]���
93,��d҅���@z'�8:u
93,"��VA��)�އ���QK_���^,Z`�(�%��Lh#g{9~�}ȓ�`�K��P4�Hf� ��qQi�����}�� v�ɸEn:՜�~��[�>���+N�u���%��8j��թc�j�M��"".$T���KQ!۪�^;jM�"
93,�//���t��B�iz6%.�!��W@ k<
93,���f
93,"A?l��TX(��}�\����A�|�������oV""�yF_��9tאnHRv���X�K�)"
93,�3�p������
93,�r���k
93,S�
93,c �16��ab�s���u6����7���}s%6���V6�������紧ҡqhg<�>��S)b ��$�#�	Q1�L�MYN$�
93, �r+I�(�$���q}*�
93,"�R��Z���Nz%�2hBa���PD@ k��q��F��r_��݁�o �%W~�Q�)��/�)o����}f�S�?��{���;��!�c�r&�)�6�	M""v^"
93,o�w�`E:)F�.�
93,�S�Je�������{tVa���d1�F0�y�H
93,a?��-;��=~��~��ܝ
93,"��梚��?,�'�Jd�+}_��""<�5}}j�U!2���X��{��i9�^~}t��rL|��x���g5�e��"
93,��&�
93,�h?�}�qq��+P�qT�_
93,��}4�(������<1��w������� %j��=�ur��8��W]�����rȔVq�x��C�1D��%Y����Xw��`6k�
93,�G����ԅQ��
93,Žy⻫7����[m&?��'����5��L�īL
93,"�������,���ɇ]�)�`��e������kq$K�b��c��i��א�"
93,�X�|����z���l+l�2��8����iN5gP@k��KY��m��>��b��K�
93,"��,>�ί"
93,P�i���
93,%<������
93,�� ��9fP���R��Cmx��.�{ؕ��<��$����3y��Lؓ({�
93,R^��|3`�s.ҡt��{�
93,$��m��녝�<@@`�4��m�f�� ���6W:���c� ]�9�
93,(�(#
93,�98}{t�����NNwO�+?+چ��l����6fBt6��Xo' ��kv���	g�
93,{c�����ʜ7?B�_�}~�
93,��E�t��al���^O�' �#L�Q���=���2nO�K��`N+>Y�8�
93,�B�@%x�
93,"q+UuX=�`��u���x[,m��d"
93,��������`@��{u�x=mu3'�zo�����5|����9��w
93,����忦��6l6��տ���a�����������ߙ�����?�h���l���7P�ǜ�:��g�&��>�׵�˿b� 
93,ku��jV{�j��j�
93,�ڭW�Z��k�6�y�j2�ڨ���
93,7ڦ�Դ�avu�w[�zM�i-VktY�itk5��k���b��`]��6�Ն�m�j�ΪZ���z:�5]7����g
93,"��_N8Q�{$�T&��+,��帞R�K7"
93,w|��!�O��?_���3�����@
93,�G'�ЍLC|@)B~���`�{2��)E�T�-��*�.I�:�
93,">�3�*Q�,.v"
93,"���\�I�:_���q��D�JG""J<�?��	��~�]���NHq���,�"
93,�p�{��d����Բ�t�>�'
93,�=��w*N.���m�^���('_a��� @W�
93,"�.��z��q��?��B�ݥ&��t�&���A0�""�K��4EPa��H9M<�K���1'H�~ �ۻ�t5 I=�7U�y�G��_�[�\��s�����^�ǥ�@��S�"
93,� �)���z�}
93,;o�Տ�T��͔p�&��
93,",�D��ta� ��'2뤏�?gD\b�G"
93,��~�L ��u��=zt����f�fA�u�!�r1�j��^�qȋ����:� Mܳyv��e&������+3��Q���V߇o�5�h#;?�@a'��x���m��y��
93,�Z}� ��A�J
93,b ��j�0|z�6��Au
93,&�=�;:T��&��|
93,Ji���Q��?N3�17AKD�R����
93,"�sNd��Kjo!����,��g3�C�i2Q^a��C�W^�����>#�_�^0�"
93,"�ܿ@���e��,��*J��B��ܑ+?&6i����$�4�4�kh6�V"
93,}�%!�MW9?N�ܐ�O2��tLY�#�ڭ�y~�޶|��&��5�(��z���Bħ���fy�=���
93,�x6�Ь��6�
93,"�h�[�z�G���hP��U��pŻ��y(�˫;փ�Ou�`H�;�Wxw���Á�KE��VԠR�U��=�#�,Pފ�U*dH:#�*Գ��[�4�2�zH���{�?�+i�}��}�M"
93,;/��]�����t4�'���d�
93,"m�n%8���?p��9{��]�3/@S�8}܎�9V�L�5*Մ�J�""n����2�L~O��8�e�1+e�=U����Z��%�('��e�d���iP��="
93,�����ZUx��dW�[�
93,��R��J��h����PMM����Q��|ĝ�uԱl�[�;����k����{eFG�+��e�(���!���
93,"���Ũ����a�3��t����W��Nƣ���jh�����3q�c�.�5�m���-�ǊT^��ț"""
93,i�G&�:+3��=/'�QQ�P)j�%n���K�k���i�f#��x�H~��}v`���)�_�P���#���s 3I��˃M�%1f��%3���moJf|�W��
93,��d�������ɛ[}t{���x��i���
93,������C�)��1:�+跣��u�:�r�V��0�^5jUMcgm�ҫ5���zV]�a�Jvn����z�]�3ި5����Z�a44��m�f3[!�~
93,7��
93,>J��cg�N$&`{�M�\�cf@XI#e�����
93,Z��d��
93,���tQ(�Z~B3GQ|ߩD�T@5L�����?_t�M��6Mc1�(��psK�
93,J�I��!�
93,�:tӂ��k1�l�GP���+�
93,"],�_��`C��l�vv8�ux��W̹�4}H��+p�1s�&8%"
93,"�Q��A�""g[P��b�a�VS9�`Cǎ�_&ܛVF.h=X;AQ�më�M�\	�����`�]�"
93,"_��b��""�3�+^�!�"
93,�`j�l�W7j=ޮ7zz�j5��z]��fK��`�-�u뚮��3jMXQ]�5Z�at
93,"�l4�zk���v�u�:t��eU��fh]]猙F�V�Y���W�7����,��� =j�ZS�k�fz��۫����:H{F��k��՚M�L7�j�޴V����h3n4"
93,"����e�F��Lo���Z�����׺��i������] H�^o��֗J�!r%���*�����5նZ5�&y�a/`}y�_����op�o\��Z]X�*,=X�w��W���};Cn�d��{O�����y뷌��&ӻZ�Uk���m�V�޵j���,�&�+�[��9�"
93,f���^G����a����l����m7�Z�ihZ�m�cV�kV�pI�έ.�魌J
93,>wdx|�8��/�Ώ�����EZ�/7t�uFǡK���GxH��BR�d�	�1��>l| ����}
93,�U|�(o|��ԃB��Bq6d��~_E�*|*�0��T p�k
93,"^&޾�}~�����.�7+�9�]���l�{I:Fo��+��0ް���|,~����i��`�N�������=���VE�N���'��h }"
93,�~�
93,"% �����b��0! 9%3 &\�	T�m,�f��K�v�Ȟ<�J_�����ܔ�a���r8�4��˽�H�t�w�U�(P9w�Q&�Tچ��Ѷ�OA��~0�~����x����T��O��ǹ�+s.���@�&�""}��Fs���%ʘ0���]�(�P���c�ʜ	��Se��nJ7b"
93,G�Y^4$�v�.
93,Oe��4�����Q	RC�3D/��oؐ�|�蕺
93,g4
93,N'1\B�
93,��fѣ�Γ'��S�K�
93,��ζX���.Y
93,"-l,Q�G�>�%�*�~�ҍFşv��"
93,"77b,�h�"
93,[ak �p�
93,"�������{?嗃I��a�L�ާm;���v�X�>,[��on��T�ݼ�X��X���"
93,�J�+�����������|Z*��	��xiG��7�P�S���D˝��$Y�C��&J��^�
93,"6,�EȐ�0��0�/3�|����P-��T�\THsE��|�F������pL�~F�qAe<�E"
93,B��P��!�v1�o����H�$P��
93,�XE�\ RJ����H�X�����6�����hטVc �W��^5�V��Q}HJ=W�
93,������A@����{��A����|��՘�s��x���KY'LmaK�>n~��+W����<��q����&}�q�UV���M�i|������
93,�����_��V|H���sQ!���{�H-l}Aa<􇟥���>�q�#ZD�&l�ܧTE��TڣP.lTj�*��6��G�*�{���V��G�%�g����]
93,1�-�e
93,#����T�p_��)���L�P
93,�[E����x�r���D_
93,�?A��O�� F_t��89zS��L��ݛv�n 0&�������1� ��K�3�-���V��p�F��r��o���<��x�0�(4�ش-��|g�����ylZ$
93,"H���w,Quq�T�:�{�iD}*��7 i��"
93,T�@��)����t<\���a� e
93,)[�
93,�/l����ǉժ��׫~�����8�
93,�O
93,"s�uS�J?��o������u��}%*I(=y�c�g5�ۭi��I��H\��<�|33�Z�|�p�1�>��^4]""��b#�>��^7��譑x�eއݹ�nd�z�1�c�"
93,W�R~��SW�ƽ���zb�c��ϝ^R��)�3���^8
93,�=B�bU��z]+mٙ
93,��
93,�2�x�h��
93,",��a�_�`����z����"
93,���vS*�
93,�zX���Zdm�Pf�5�)44M�T��xH*��V�e�@�\;A��9@���{Atfz/�hiDh���
93,"��:?je���S9 ���s�x�I����ϓ�Q�I����� VsYbԧ�""ɸj7ҋ`�Vbh<�{~m�q�>DЕ9QN�b"
93,6�ôˏ�6r�ba��P
93,cz�2��
93, �/��8MÍ |3�^��j�
93,�!�1f
93,|��*���Q��RR�B'�@�0��)�ĳJĺ ~9��D��
93,"���4�f��J�!�""\I>M0���N"
93,�(4����W4��.�P�
93,�{����#���N�J?��X��M��� �b�i��'Į���x�!���Ry�(S
93,'n>ź�ST�
93,"ea�&���c�f��\�XP���3���_� �� �X!����ҵ-�*��bL�$���B��#��0�$H��2�j��)Ģ��_��� ��ו`�G�E�Z�}�c�K���A<�+���>}�$�}Y,�	u�o~���Lݥ�{��c�vF"
93,� ���w
93,^�1�A�+�֓����rL�+B�����{1�(�k��HH.��w �d�s��]|_���x<yd�J^���CL�J� 3�_�(Ԗrh�*���1%�֕O�vP(_V[g
93,"�(`��8���m�%7.�F��J�*ga���z�0���T;�mHJ�M1�aچ��6o�*k�Q�]3��:k�-�Ыz�0+Q%R�Jh׵�j��1�S��""����w�3��*!n"
93,���EЭU����mb��F�V7Z�6 ��ڵ�i6*��h�~
93,j����)�a��j7��ֳ�-
93,�֮��٪55ޮizB(�G�:���-�L��� ��
93,���+ޕ��7)e���%��*���g�ρw^��!��c�
93,�s&�ś�y���ͬ�
93,Y��
93,"���k��ta���V�5,�s���9��d�V�ٕ���I?*VPƨ)�(�6ԥn�@��dH�[���7,�v�e����*k��z�n�Jv�""̛;����w��="
93,Fw�L�u��VzldN�~0�f�Qkב��fU뽦��x����Y�U�HmN�jXp�D����6
93,I�6�V�UӌZ��kM��ծVo�X�լ�V�mVPI�|d�)Ǐ��3]�{��
93,�l�@��.6	w$�����>p��m񣑬bd���
93,_`��Ԅ�D�o���Wa�p���d��<��-z�uP8���	��߹cf�A8Wt�(z���{
93,m��F�O��	tE~K�ǮoSpӭ�;	(���H<�iJ�!g���W��GsnცFC
93,`��|�y�k�F��Y�����7�м�
93,�A�tK
93,��?`x>�r]X�C��Y��}*�[��
93,"aun�Z��m,6X"
93,"�v��^�6�L.?���d�P""V"
93,"�\,����IBZHW | ������i��}�)��v��!�e�"
93,ޞɗ�L*�r+���
93,T֣sJ�5'K�2��O�
93,{��
93,*n�o�p�6
93,P'���%ёKv���A��g����O�S�պp��KnO�z]�bZ�h�td��z��
93,���Ս9nK|�4T)$��
93,"�V��,"
93,"�F�^cZ�l���LC��w�����We�엝�]�r�����?�����6^��/�`��`c�p��X½tn,��t`t�ʝ�7bþױ���8�MOY�g�~�����9�O�����t�;8��lʽ��7�8��������y���A�,��%�C����}��?�h/⸞��� ����S����V��^`�8afZ�#h~��Q����vҷ hф�9�rF9Q�R>�l�"
93,",S���"
93,"\Y�M.�,q<1_�9�(��`�����I"
93,N_D7�ٚ]v��`QM�
93,"��r�EG|Uq{=t�w�O��3.�""���x�oG��NGT�$�!��JG�+� �g�Y��3Bg�ވsw�hcQ�o&7D�G���XI8�K?�Q/}�/"
93,"�s��~T�]���_�� *�7��Y�m��}�c' Ȗ�gj��A1QUiɺ�,e��}��ry&��^�Q�5��U �t��/�JH�#��z蠒�,�6"
93,"x��G$#�vF��)�FsX̙�,�J�;��Ԫ�%��4�p8*6��]�K"
93,��[�˜
93,�fA�v�2����:¸D���)+ �O�B��!J�_�����W('�cG>VAa�)ß��C
93,��&��<��!N�o��1c
93,wH�'�ˉ���<�7G�O*C��B}�[͖��Z�gz��5@�k�
93,"�b��l��/q�6W�Z���cw<��A�S�~�8W�]�`��=:��G@\�Xu���i�?*Mn�f��k7���l��f�����MSk��9���z""����ġ�\����Th7�F��n�9k�ڍv����U"
93,"�W��,P{B"
93,�me�E�8����N��Q!K8�eT{
93,ު��=��n�Э��-�����4f/� �KE9ȑ��g���2Q�D0JT
93,���bܙ~e	�Y��`�Z�|`e��ٿw
93,"a\�O�h��4<""��A���5��هϞ�<|��"
93,"��d+Up-�;(��h����\�:Ԭܻ��vkL��)�=�Č):R�'�߁|L��""/)�@�Ӣ�Hd�z�c��W�OS�:\@!�0�_�"
93,"��lS&��G'b,�����G�l��]C[c�"
93,��r�b�J=0��1
93,��;���c��HZ�ȡ�R	S���
93,�Ku��jx�
93,"љ���s$��pY���^�,�+��6.SR��x	�PvNI�"
93,"xC��������9+��6���1�ǽ���{z���""7 e�c2b�?z�b��2���� ("
93,���<����>��1L�;b�Ul���Pc�k���� ���-2�	���?�.�|D_Ȇ�j�2���{�^��=�0�ZԾ��4��U��7�9?��J�x�=ș(<��b3�zr�VzM 䵂�
93,"[�����˃7x�	��<��3����x`�������[�����;Ga:���>��g�c�c�J'ͷ ����Oc�uɊ�? >��,Q"
93,���!
93,&�R�Jh��a�m��6������-s����ۄ5�	�y'>HN�G����I�(D�ce��6����.m�����m����8���s�t����G���NG�9��W��)�l�s��=N�O�8��O
93,��LP�{���
93,4����GXY=���0K����c�l���=�� ��4o~�
93,oj�s�KxT��xȑ�����j H&���
93,�#��ݞgb��rc�h�����P�K��
93,��U
93,"�.��㰏bz���N��xꋏ�8i��v^ۗ\yŝ1�E��q3�,H\���쯼��;�%���J~�}���L��@D���̊���_���B�|�e� �ɯS"
93,��xB
93,�kAi*ti]Q�L�K�/�]!��DFݐ.�� �
93,p�q�����	ʺ��P
93,�&�Iͼ
93,K�~-��
93,"""�.d_�z�3������Jz\�x�8�"
93,B�)�:s�
93,�Mi
93,���H����^2�A�8d�
93,"�ċ�����{(����qf+�""�(53o)�_~�G�tλ�=z��M� d={��� 7~��O���_#�4���"
93,j6|
93,�F�gE�+bU��l*�����w�8�c+b˴S���ӿe��S�}�!��n�>j��/rE��K7���l�����h狼O{�5�w�V�
93,�i;�\�#@�@�1�	H�T�C�j8~0��s��oDKg��u�L���*���9g�8v���r|N�dR
93,�%y��8;l��MY���M�rƱ�����FC>�$
93,�p�����
93,7j�WM������e��[���?�ݮV��(|=�軂�#��}�y�84�Kg� =O�
93,��߸���
93,1H���/r�=�*���~Ѝs��  �� k0[�� 
94,Blog - Page 3 of 167 - Brent Ozar Unlimited®
94,"Log InContact BrentProduct has been added to your cart.ConsultingPerformance TuningRemote DBA ServicesUpgrades and MigrationsConsultant ToolkitTrainingMy Videos and DownloadsMy AccountPrivate SQL Server TrainingTraining FAQMonitoringScriptssp_Blitz – free health checksp_BlitzCache – find queries to tunesp_BlitzFirst – instant performance checksp_BlitzIndex – design the right indexessp_BlitzWho – what’s happening right nowPasteThePlan – share query plansBlogT-SQLQuery ExercisesExecution PlansIndexingVideosArchitectureBackup and RecoveryCloud ComputingDevelopmentFirst Responder KitHigh AvailabilityHumorLocking, Blocking, and Isolation LevelsParameter SniffingProduction Database AdministrationProfessional DevelopmentSQL ConstantCare"
94,BlogHomeBlogPage 3
94,"Announcing the 2024 Data Professional Salary Survey Results.Last Updated January 4, 2024Brent OzarSalary13 CommentsThis is the 8th year now that we’ve been running our annual Data Professional Salary Survey, and I was really curious to see what the results would hold this year. How would inflation and layoffs impact the database world? Download the raw data here and slice & dice it to see what’s important to you. Here’s what I found."
94,"First, without filtering the data at all, salaries are up, but response counts continue to drop:"
94,"If we filter for just United States folks whose primary database is SQL Server or Azure SQL DB, the salary numbers are higher, and are still continuing to rise:"
94,"Because this blog’s primary readership is SQL Server folks, I wouldn’t use the survey to draw conclusions about any other platform. The number of responses for other platforms is really low:"
94,"So with that in mind, for the rest of this post, I’m going to focus on only SQL Server & Azure SQL DB folks. What are your career plans for 2024?"
94,"Most respondents intend to stay in the same employer, in the same role. Folks who are planning to make a change also happen to be getting paid less – and that’s probably not a coincidence, heh. If you’re thinking about changing roles, you’re probably interested in who’s bringing home the cheddar:"
94,"Normally I wouldn’t draw conclusions from just 4 respondents, but I think it’s safe to say that data scientists are in such high demand that they command higher pay. (However, it’s also harder to get a data scientist job than most of the rest of the jobs in this list.)"
94,Another way to make more money is to go independent:
94,"This marks the first year of the survey where female pay is actually higher than male! The response rate is pretty skewed, but it always has been:"
94,"Download the raw data here, and hope this data is useful to you when you have salary and career planning discussions with your manager. Here’s to you getting another raise in 2024!"
94,"Query Exercise: Find Foreign Key ProblemsLast Updated January 10, 2024Brent OzarQuery Exercises21 CommentsFor 2024, I’m trying something new: weekly homework challenges! For this week, let’s say we’ve decided to implement foreign keys, and we need to find data that’s going to violate our desired keys."
94,"We’re going to use the Stack Overflow database, and we’ll focus on these 3 tables:"
94,dbo.Users table: with Id column as its primary key
94,dbo.Posts table: with OwnerUserId column noting which Users.Id wrote the post
94,"dbo.Comments table: with UserId column noting which Users.Id wrote the comment, and PostId column noting which Posts.Id is being commented on"
94,"Before we attempt to implement foreign keys, we need to find data which might violate the foreign key relationships. Are there any:"
94,Posts rows whose OwnerUserId does not match up with a valid Users.Id
94,Comments rows whose UserId doesn’t match up with a valid Users.Id
94,Comments rows whose PostId doesn’t match up with a valid Posts.Id
94,"And to make your task easier, let’s focus on just the first 100K rows in each table (rows with an Id <= 100000) to see whether or not foreign keys make sense for this database"
94,Your query exercise has a few parts:
94,Write one or more queries to find these answers as quickly as possible with low load on the database.
94,"Given what you find, hypothesize about what might have caused the foreign key problems."
94,"Given what you learned, are there any changes you want to make to the app, processes, or database?"
94,"You can post your answers in this blog post’s comments, and discuss each others’ ideas. We discuss the challenges & techniques in the next post. Have fun!"
94,"[Video] Office Hours: Oddball Questions EditionLast Updated December 30, 2023Brent OzarVideos0The last Office Hours of 2023 featured some oddball questions from https://pollgab.com/room/brento. Not bad, just … odd."
94,https://youtu.be/prkpWssHsaE
94,Here’s what we covered:
94,00:00 Start
94,"03:04 TheMooneyFlyer: Hey Brent, how do you work on optimizing sp that performs insert/update/delete? Does putting the exec within a begin tran / rollback is a good option?"
94,"06:06 MyTeaGotCold: If a table is empty and I absolutely know that nobody else is using it, should I always insert in to it WITH (TABLOCK)? What if it’s a temp table?"
94,07:09 Tonia S: Have you tried the mock DBA interviews with ChatGPT? Very realistic?
94,"09:11 ChompingBits: What “Best Practice” pays your bills the most? I’m thinking DBAs with superstitions they follow that cause issues in newer versions, but if you’ve got suggestions that almost no one follows so you come in and clean up in an afternoon, I’d like to hear that too."
94,11:33 Nardole: What are the top SSIS issues you see with your clients? Anything performance related?
94,11:45 Philo: Is windows paging of sqlserver always bad? What are the top issues you see with windows paging?
94,12:00 OnSiteDBA: You mentioned that shops that go multi-terabyte need to do snapshots backups instead of the native backups given the reduced restore times typical of snapshots. How does one handle possible inconsistencies in MSSQL configurations with data and log files in different volumes?
94,"12:36 crushingtempdb: Hi Brent! I am troubleshooting some tempdb issues on Azure SQL Database; When I read the documentation; mo cores=mo tempdb, we’re told; When I run0 select Sum (max_size)/1024.0/1024.0 FROM tempdb.sys.database_files WHERE type_desc = ‘ROWS’ it doesn’t match. Thoughts?"
94,13:45 Karthik: Have you ever had to run the windows debugger against sqlserver.exe? What was the scenario?
94,17:42 Bonnie: What are your best and worst observed times for manually scaling azure SQL VM to a higher SKU?
94,"19:36 MyTeaGotCold: How should I manage an effort to refactor away from Hungarian notation (e.g. “sp_” prefixes)? Even when I win the battle on clarity and performance, I lose it on the fear that the changes will break something."
94,"Who’s Hiring in the Microsoft Data Platform Community? January 2024 EditionLast Updated December 30, 2023Brent OzarWho's Hiring9 CommentsIs your company hiring for a database position as of January 2024? Do you wanna work with the kinds of people who read this blog? Let’s set up some rapid networking here."
94,"If your company is hiring, leave a comment. The rules:"
94,"Your comment must include the job title, and either a link to the full job description, or the text of it. It doesn’t have to be a SQL Server DBA job, but it does have to be related to databases. (We get a pretty broad readership here – it can be any database.)"
94,"An email address to send resumes, or a link to the application process – if I were you, I’d put an email address because you may want to know that applicants are readers here, because they might be more qualified than the applicants you regularly get."
94,"Please state the location and include REMOTE and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE."
94,"Please only post if you personally are part of the hiring company—no recruiting firms or job boards. Only one post per company. If it isn’t a household name, please explain what your company does."
94,Commenters: please don’t reply to job posts to complain about something. It’s off topic here.
94,Readers: please only email if you are personally interested in the job.
94,"If your comment isn’t relevant or smells fishy, I’ll delete it. If you have questions about why your comment got deleted, or how to maximize the effectiveness of your comment, contact me."
94,"Each month, I publish a new post in the Who’s Hiring category here so y’all can get the latest opportunities."
94,"[Video] Office Hours: Holiday Speed Round EditionLast Updated December 25, 2023Brent OzarVideos1 CommentMost of the https://pollgab.com/room/brento questions from this episode had really short and sweet answers. Let’s take a break for the holidays and knock ’em out:"
94,Here’s what we covered this episode:
94,00:00 Start
94,03:29 TheyBlameMe: Have you ever made it across the finish line for a customer by changing the way their application connects to SQL Server (i.e. OLE DB vs ODBC driver)?
94,"03:51 Sleepless in Seattle: How often, if ever, do you use partially contained databases?"
94,"04:36 GUIDKeysWasteAndFlushBufferCache: With a GUID PK & CX on a large table, most new rows will land on 8K pages containing only old, obsolete data. Almost every new row (that will be accessed again soon) has a whole 8K page in the buffer cache! Buffer cache will be mostly old, obsolete data. Am I missing something?"
94,06:26 Gökçe: What should you do when you notice the third party vendor app is using NOLOCK like butter all over their TSQL queries? Have you experienced this?
94,06:59 Björgvin B.: Entity Framework is spamming the SQL plan cache with many duplicate plans. How do you deal with this issue? Use sp’s instead?
94,07:22 Anastasios: Do you have any good stories where SQL Server was blamed for slow app performance but turned out to be something completely unrelated to SQL Server?
94,08:27 MyTeaGotCold: Has your opinion on agent jobs changed in the past 5 years? AWS lambda and its kin seem to be replacing the Task Scheduler.
94,"09:13 Stooky Sue: As SQL migrates from on-prem to the cloud, do you still see separation of duties between DBA and Storage Admins?"
94,10:12 bagllop: What would you suggest my friend to use instead of linked servers? SSIS? OpenQuery? Or is there some new fancy stuff he could use?
94,11:41 Rena: Is adding foreign keys just to get join elimination ever advised?
94,12:28 Persephone: What scripts do you like to use for Columnstore Index maint?
94,12:52 Encino Man: Is there a good way to determine if a given query from C# is using the density vector or histogram for cardinality estimates?
94,"13:40 Stooky Bill: For SQL Server, Microsoft recommends a minimum target for page life expectancy of 300 seconds. Is this still a good recommendation or does it need updating for modern times?"
94,"14:10 DBADany: Hey Brent, in a working environment with multiple DBAs having sysadmin permissions, are you aware of anyway we could audit who restarted SQL Server? Apparently SQL Audit only tracks the time for stop/start itself but no details of hostname or IP from who did it? Thanks"
94,15:15 Huyang: What is your opinion of Azure NetApp Files and Silk cloud ISCSI SAN for hosting Azure SQL VM data files?
94,"15:26 OnSiteDBA: I have heard you mention PostgreSQL as a cheaper RDBMS alternative to MSSQL, but hardly mention MySQL. Is there a reason you hardly mention MySQL? notable performance hitches, feature-specific limitations etc."
94,"16:46 KyleDevDBA: Hi Brent, Do you have a favorite wait type to fix and why?"
94,17:12 Renzi: What are your pros / cons of app consistent snapshot backups vs crash consistent snapshot backups for multi TB boxed SQL db’s?
94,18:29 Stooky Bill: What are your thoughts on the TOP operator short circuiting parallelism? Good thing or bad thing?
94,20:07 ChompingBits: Why is it that the Microsoft owned apps are always the worst offenders with SQL issues? Sharepoint and SCCM have longstanding known issues with deadlocks. SCOM is a beast that spawns all kind of GUID named agents jobs. SCORCH doesn’t support availability groups.
94,20:57 Argyris P.: Do you have any good ways to find all large (billions of rows) static tables in a boxed SQL Server instance?
94,21:38 Alex: Huge fan. What’s your opinion on azure database fleet and should I aim to replace my elastic pools with this new feature or is it another gimmick?
94,"22:32 Pytzamarama: Hi Brent! When we update our customers databases (a lot of them are still on SQL Server 2008R2) for a new app version, we drop/create all procedures and triggers. How does this impact performance? Thanx"
94,"24:12 Toymaker: Is there ever value in perf testing queries cold vs hot (I.e. DBCC DROPCLEANBUFFERS, DBCC FREEPROCCACHE)?"
94,"24:37 WouldPrefer3rdNormalForm: Have you ever seen or heard of a ceiling/threshold for XML columns, beyond which performance craters? We are scared by two large XML columns (and their indexes). They are currently performing adequately, but comprise about about 40% of our 1.5TB database and continue to grow…"
94,25:49 Kane Baden: Was literally typing a question around “what do you think are the main questions around picking MSSQL vs. Postgres…” while watching your last upload when you spelled it out for the last question on that SaaS question. So I figured I should let you know thanks instead! Thanks!
94,26:10 Bonnie: How do you run sp_blitzcache to target a temporary stored procedures for analysis?
94,26:35 bottomless: Azure SQL Database Serverless is supposed to cost less but at the end of the month it costs more than DTU: our SaaS has routines and jobs that wake up the database. From you experience have you aver seen a successful use of Azure SQL Database Serverless?
94,"Updated First Responder Kit and Consultant Toolkit for December 2023Last Updated December 25, 2023Brent OzarFirst Responder Kit Updates1 CommentNew this month: more work on letting sp_Blitz run with limited permissions, nicer Markdown output, sp_BlitzLock compatibility with Managed Instances, and more."
94,Wanna watch me use it? Take the class.
94,To get the new version:
94,Download the updated FirstResponderKit.zip
94,Azure Data Studio users with the First Responder Kit extension:
94,"ctrl/command+shift+p, First Responder Kit: Import."
94,PowerShell users: run Install-DbaFirstResponderKit from dbatools
94,Get The Consultant Toolkit to quickly export the First Responder Kit results into an easy-to-share spreadsheet
94,Consultant Toolkit Changes
94,"There are two changes to the spreadsheet in this release. In the “Plans Duplicated” and “Plans by Query Hash” tabs, we’ve added new columns for Compile Time MS, Compile CPU MS, and Compile Memory KB. These values are for the one specific plan you’re looking at in the row, not the total amount for that duplicated plan. It gives you a rough idea of how resource-intensive these queries are each time they run. (Thanks Erik Darling.)"
94,If you’ve customized your query_manifest.json and/or your spreadsheet
94,"And you don’t want the new columns, then you can simply copy your customized query_manifest.json and/or spreadsheet over ours just like you normally do. Nothing will error out – you just won’t have the new columns."
94,"And you do want the new columns, then you’ll need to merge our query_manifest.json changes for queries 50 & 82, and copy the tabs “Plans Duplicated” and “Plans by Query Hash” over those tabs in your customized spreadsheet."
94,sp_Blitz Changes
94,"Enhancement: way prettier output when @OutputType = ‘markdown’. (#3401, thanks Mike Scalise.)"
94,"Enhancement: simpler, more intuitive checks for Instant File Initialization. (#3362 and #3409, thanks Montro1981.)"
94,"Enhancement: if we had to skip checks because you didn’t have enough permissions, we now warn you about that. (#3376, thanks Montro1981.)"
94,"Fix: continued work on detecting msdb permissions. (#3377, thanks Montro1981.)"
94,sp_BlitzCache Changes
94,"Fix: @Debug = 1 was skipping a character of the dynamic SQL. (#3406, thanks Per Scheffer and Montro1981.)"
94,sp_BlitzIndex Changes
94,"Fix: @Debug = 1 was skipping a character of the dynamic SQL. (#3406, thanks Per Scheffer and Montro1981.)"
94,"Fix: added drop-if-exists statement for temp tables to make it easier to run parts of sp_BlitzIndex ad-hoc, outside of a stored proc. (#3383, thanks Chad Baldwin.)"
94,sp_BlitzLock Changes
94,"Fix: better compatibility with Azure Managed Instances. (#3392, thanks Fatima Brunson and Erik Darling.)"
94,"Fix: won’t error out when sys.dm_exec_query_stats reports a last_execution_time of 1900-01-01. (#3385, thanks teej21012 and Erik Darling.)"
94,sp_BlitzQueryStore Changes
94,"Enhancement: added @Help = 1 output. (#3388, thanks Vlad Drumea.)"
94,sp_ineachdb Changes
94,"Enhancement: new @is_ag_writeable_copy parameter to only run queries against those databases. (#3399, thanks Douglas Taft.)"
94,For Support
94,"When you have questions about how the tools work, talk with the community in the #FirstResponderKit Slack channel. Be patient: it’s staffed by volunteers with day jobs. If it’s your first time in the community Slack, get started here."
94,"When you find a bug or want something changed, read the contributing.md file."
94,"When you have a question about what the scripts found, first make sure you read the “More Details” URL for any warning you find. We put a lot of work into documentation, and we wouldn’t want someone to yell at you to go read the fine manual. After that, when you’ve still got questions about how something works in SQL Server, post a question at DBA.StackExchange.com and the community (that includes me!) will help. Include exact errors and any applicable screenshots, your SQL Server version number (including the build #), and the version of the tool you’re working with."
94,"[Video] Working on First Responder Kit Pull RequestsLast Updated December 25, 2023Brent OzarVideos0For this month’s First Responder Kit releases, I worked through most of the pull requests live on my Twitch channel. If you wanna get a glimpse of what it’s like being an open source maintainer, this is a good behind-the-scenes look:"
94,"And part 2, with more PRs:"
94,"The Annual Data Professional Salary Survey Closes This Week!Last Updated January 4, 2024Brent OzarSalary2 CommentsTake the Data Professional Salary Survey now. The survey has closed."
94,"The 2020s have been tough: a pandemic, a recession, layoffs, and inflation. Inflation makes things particularly tricky because your costs for everything have risen a lot in the last year, but at the same time… has your salary? What about your peers? You’re in a tough position because it’s hard to ask for more money when there are layoffs everywhere. I feel you."
94,"So it’s time for our annual salary survey to find out what data professionals make. You fill out the data, we open source the whole thing, and you can analyze the data to spot trends and do a better job of negotiating your own salary."
94,"The anonymous survey closes Sunday, Jan 1. The results are completely open source, and shared with the community for your analysis. (You can analyze ’em now mid-flight, but I’d wait until the final results come in. I’ll combine them into a single spreadsheet with the past results, and publish those on January 9th.)"
94,Thanks for your help in giving everybody in the community a better chance to talk honestly with their managers about salary.
94,"[Video] Office Hours: 21 Good QuestionsLast Updated December 15, 2023Brent OzarVideos2 CommentsThis one is a 3-part episode: I take 21 questions from https://pollgab.com/room/brento and then later, work on First Responder Kit pull requests."
94,Here’s what we covered:
94,00:00 Start
94,"01:08 MooneyFlyer: Hey Brent, apart from installing and playing around with it, what is the best way to get started with Postgres for someone familiar with SQL Server?"
94,"02:04 Frozt: Hi Brent, I would just like to inquire if do you have DBA tasks in mind that can be delegated to a command center(Service desk) as a DBA or this tasks should remain to DBA for job security and also to lessen risk of service desk doing something wrong on Prod server. Thank you"
94,"03:29 Steven: Hi Brent, my friends BI tool joins an aggregated temp (1m rows with 13% sampling) and lookup table (40 rows). Both scan estimates are accurate and all rows match but the hash join estimates 1 rows. Any ideas on why the estimate could be so far off?"
94,04:13 Frank Castle: What’s your opinion of Azure AI search?
94,"04:22 Need Memory Dump: Have server with 1TB of ram, min set to 0, max to 900GB. Server takes all 900GB as soon as I turn on the service, have not run anything. Any ideas?"
94,05:25 Heinrich: What is your opinion of Azure Cosmos DB? Does it compete well with AWS Aurora for new software development?
94,06:39 Jose: Why is it slower to do a SELECT * INTO a temp table at the beginning of the proc than it is to SELECT the columns you need then INSERT them INTO the temp table?
94,"07:25 James Fogel: I don’t have a question, just a thank you. I’ve learned a lot from you and I’m sure countless others have. Thank you for what you do and all you give to us. You are appreciated."
94,"07:48 ChompingBits: What is your preferred way to run the same query against a bunch of servers? Using T-SQL and linked servers? Agent jobs pushed out from a Central Management Server? PowerShell/DBATools? I assume it’s some form of all of the above, when is each the best practice?"
94,08:34 Perseus: Should joins on natural keys on large tables always be avoided in favor of joins on surrogate keys?
94,09:40 Wilson Fisk: What is your opinion of the new Azure ARC SQL performance dashboards?
94,10:35 MyTeaGotCold: Is it bad practice to have my server query itself and write the results to CSV? I see it a lot in SSIS.
94,"11:11 marcus-the-german: Hi Brent, you use sp_ as the prefix of you stored procedures. Why? AFAIK it’s recommended not to use sp_ for user procedures."
94,"11:58 Eugene: Which is better, a DBA specializing in one platform or multiple platforms?"
94,13:29 Ya?mur: Would like to upgrade our Azure SQL VM from SQL 2019 standard to 2019 enterprise version so we can leverage more memory and cores. Is this an ok exception to do as an inplace upgrade?
94,14:00 Mumtaz: Can we audit IUD transactions without configure any audit features ?
94,15:01 ImAfraidOfBI: You know how they say dogs usually look like their owners… I saw it a bit with the cute dog you picked up last time! No offence dog’s cute!
94,"15:32 KyleDevDBA: Hi Brent. Curious about the origin story of the naming of the sp_Blitz* scripts (why they start with Blitz). Funny acronym, interesting story, dark past? I searched around, but wasn’t able to find anything. Thank you for all you do for the SQL community."
94,16:31 Hugo: How do you find queries that are spamming the plan cache due to different SET options?
94,"16:51 Henrik Fältström: Is there any benefit of creating an index on an index for LARGE read-only tables? Or are there other ways in SQLServer to accomplish fast access? I know it’s not possible today in SQLServer, unless you implement something yourself, Has anyone done this?"
94,17:22 Nicolaj Lindtner: I’m building a saas. Considering sqlserver vs postgresql. My suspision is it really doesn’t matter – so will probally go for postgresql. Input ?
94,"[Video] Office Hours: Wrong Side of the Bed EditionLast Updated December 12, 2023Brent OzarVideos1 CommentI woke up on the wrong side of the bed after a nap. How does that even happen?!? To take the edge off, I poured myself a gin & tonic and went through your top-voted questions from https://pollgab.com/room/brento."
94,Here’s what we covered:
94,00:00 Start
94,"05:41 LongRunningBackups: You say to use SAN snapshot backups on larger DBs (multi-TB). I have searched “Ozar SAN Snapshot”, but your article has a broken link. Other searches don’t provide relevant results because “snapshot” is a common/overused term. Is this a Fundamentals or Mastering level topic?"
94,"06:52 Asking For a Friend: Follow up on the “Are cloud databases overrated” question Dec 1. No, the on-prem VM is not “free”, but it is a sunk cost since it already exists with many DBs, is licensed, is here to stay. Moving the db on prem simply eliminates monthly cloud costs. Does that change your answer?"
94,"09:17 TheMooneyFlyer: How often do you come across performance issues that cannot be solved by index, queries or server optimization but requires an application redesign? How do you manage this so your client is happy with your report?"
94,13:22 Xavier R: If the software vendor is using inefficient coding techniques that impact application/DB performance. Where would you draw the line in helping them? I went as far as giving them a query (which solved a network timeout) to replace their nested stored procedures
94,"15:37 Montro1981: Hi Brent, do you listen to music while working, and what is your favorite type of music you listen?"
94,16:54 Renzi: Do you see any specific A.I. skillsets complementing SQL DBA?
94,19:27 Ömer: What are the top performance related features that you get in boxed SQL Enterprise that you don’t get in standard version?
94,21:03 Kemal: Do you have any suggestions for how to load test TempDB on a prospective new cloud VM?
94,"Interesting Aurora MySQL Feature: The Buffer Pool Survives RestartsLast Updated December 9, 2023Brent OzarAmazon Web Services (AWS)11 Comments“Documentation! Hey, look at that.”"
94,"Lemme start this off by saying this is probably irrelevant to you. (It’s irrelevant to me, too.)"
94,"If you’re strapped for time, just skip past this blog post."
94,This one’s for the curious folks.
94,"AWS Aurora MySQL is Amazon’s flavor of MySQL with their own unique performance and reliability improvements. I’ve never used it, and I don’t see myself using it anytime soon because I just don’t need it. The database back end for the BrentOzar.com blog is MySQL, but I use a managed WordPress hosting partner, so Aurora MySQL is irrelevant there too."
94,"Having said that, I still read the database news because it’s neat to see how companies are innovating, and this new optimization from AWS is intriguing:"
94,"The current implementation of buffer pool in Aurora MySQL employs survivable page cache where each database instance’s page cache is managed in a separate process from the database, which allows the page cache to survive independently of the database."
94,<record scratch> WAT
94,"This is obviously dramatically different from Microsoft SQL Server. In SQL Server, if you restart the SQL Server process:"
94,Dirty (changed) buffer pool pages are written to disk
94,"The SQL Server process shuts down, releasing all memory back to the OS"
94,"The SQL Server process starts again, and has no memory allocated at the beginning (unless you play around with LPIM and minimum server memory settings)"
94,"SQL Server gradually requests memory from the OS as needed, reading data pages up from disk as needed, and caching those pages in the buffer pool"
94,"At first glance, Aurora MySQL’s optimization sounds amazing, but it has a few gotchas. It would seem to only be relevant when:"
94,"The MySQL writeable replica stays on the same server – meaning I would assume it’s less relevant for database patching, since you’d want to patch a passive replica first, then fail over to it. (Although as long as Amazon’s putting in this much work, they could conceivably do the patching live on the same node – I would assume that would result in longer downtime though, as opposed to failing over to an already-patched instance.)"
94,"The MySQL process restarts, but the OS stays up – meaning it’s not relevant for OS patching either."
94,The buffer pool is fairly stable – this doesn’t help you on underpowered servers where everything gets read from disk anyway.
94,"And keep in mind that we’re only talking about the page cache, not things like execution plans, DMV metrics, etc."
94,"This isn’t the only optimization they’ve done, of course. The whole documentation section on Aurora storage and reliability is interesting, like how storage costs automatically drop as you drop tables and indexes. You don’t have to worry about resizing the data files or resizing the underlying OS volumes like you do with Azure SQL DB or conventional SQL Servers."
94,"I’m not saying Aurora MySQL is better than Azure SQL DB or SQL Server, by any means. (I’m not even saying the optimization works, hahaha!) I’m not even saying Microsoft should build this kind of cache persistence for SQL Server! It’s such a niche use case. I’m just saying it’s interesting to see these kinds of advancements in cloud databases."
94,"[Video] Office Hours: Sunshine EditionLast Updated December 12, 2023Brent OzarVideos1 CommentYou know how this works: you posta the questions at https://pollgab.com/room/brento, and I giva the answers:"
94,Here’s what you asked in this episode:
94,00:00 Start
94,"03:18 DBA in VA: I recently discovered the “force order” query hint. I’m usually inclined to let the optimizer do its thing, but I have seen some of our code perform MUCH better with this hint. Are there gotchas/downsides I should know about?"
94,"04:29 RoJo: We call SQL from C# code, but it’s hard in SQL to trace where in code the call came from. Is adding a comment at the end of SQL statement, with module/method info a viable solution or are there better ways to connect SQL statements to actual method calls in a Live system."
94,"06:34 OralceIsBetter: Hi, I have large database ~90 TB, full backup is taking almost 2 days (one time per month). Utilization off SAN network is around 2 gigabits/s. Do You have any tips how I can speed up this process ? Compressions is enabled and backup is running in many threads using couple disks."
94,07:16 Raj: When would you want to manually create statistics without a corresponding index?
94,08:13 Kimberly: I’ve got a 2 node AG cluster on SQL Server 2019. SQL Server sees 2 databases as added to the AG when they were actually removed. The primary shows synced and the secondary doesn’t have the db. Why does sql still think the db is in the AG when it is not?
94,"09:06 Mike: Hi Brent, can you tell in which ways SQL Server is better than PostgreSQL ? Are there any bullet points ? And vice versa, are there any things in which PostgreSQL is better than SQL Server ?"
94,11:26 RollbackIsSingleThreaded: Hi Brent! Writing articles about SQL Server does not earn much money. What do you think is the main advantage of writing articles?
94,"14:49 Lysander: In boxed SQL, when should you use table partitioning vs partitioned views?"
94,15:33 Mike: In which scenarios Failover Cluster Instances are preferable over Availability Groups ? When and why we should use FCI instead of AG ?
94,17:01 AnotherDataLayer: Linked Server vs Polybase: both are doing the similar things if not the same. which one to use when it comes to pull data from another MSSQL server and why. We are using entity framework.
94,"18:35 Don’t Bother Asking: My friend has inherited a database which has lots of nonclustered PKs and very few clustered indexes. DUI query performance is not great. Will adding clustered indexes, or rebuilding PKs as clustered, improve query performance? And if so, are there any gotchas?"
94,19:52 Bonnie: Do you have a good way to determine which operators in a query plan are contributing the most to a memory grant?
94,21:24 Perseus: Is there a good way to know why SQL Server ignores a given query hint?
94,22:29 Ophelia: What are the top signs that the SQL buffer pool is under pressure?
94,23:27 Chrysostomos: Does one database per customer model work well with SQL AG HADR?
94,25:32 Meryem: What’s the best resource for learning how to write efficient linked server queries?
94,25:46 Mandeep: What are the top things you see that break log shipping?
94,"27:00 Pradeep: Given modern fast storage, is clustered column store index fragmentation as inconsequential as non-clustered index fragmentation?"
94,"27:46 Renaldo: For cloud SQL VM, what are top charge back mechanisms you see for billing SQL VM costs back to each customer on the SQL VM?"
94,28:49 Olga: Have you ever had to rebuild all indexes? What was the use case?
94,"30:09 Jessica: Hey brent, meta question. When building PollGab did you intentionally set out to build a site without trackers etc that would be blocked by uBlock Origin? Its amazing to see a clean site for the first time in a while."
94,"30:43 muppet: Hey Brent, my friend has a fairly typically designed table with 1.2 billion row table stored as regular row store. It performs sluggishly so I was thinking of proposing partitioning but what do you think of switching to columnstore instead?"
94,"31:29 Brynjar: In sql server, how do you determine the optimal column order when creating a non-clustered column store index?"
94,"33:38 AllAboutSearch: When someone type the third character for the FName/MName/LName in the FE, it queries the DB’s computed column in the backend, all good. But when there is space in the name, entity framework query starts scanning the whole table and timeouts. Any tip?"
94,"34:53 Montro1981: Hi Brent, I hope you’re doing good. Have there been moments in your long career that you might have taken another path? If, your answer is yes (which is very likely), where might you have ended up in life?"
94,"[Video] Office Hours: Really Good Questions EditionLast Updated December 12, 2023Brent OzarVideos0Y’all were on fire in this one! You posted great questions at https://pollgab.com/room/brento and I worked through ’em:"
94,Here’s what we covered:
94,00:00 Start
94,"04:06 DislikeEntityFramework: In AWS RDS, we inherited a 9TB table with uniqueidentifier as clustered PK, another column is an identity int but not needed. How can we quickly fix this? Drop existing identity, add new bigint identity PK clustered, and make NC index on uniqueidentifier. 0% downtime. Tips?"
94,"05:16 mailbox: I’ve noticed more job listings for PostgreSQL DBA or Data professional on indeed.com. Weird thing– many of these listings ask that you be able to write SP or translate legacy SP to run on PG SQL. Is the job scope of a DBA becoming broader, or do you think these are mis-titled?"
94,06:13 Eh? Aye: You wrote about Postgres/SQL licensing. As compute (CPU & RAM) is getting cheaper (physically & potentially in the Cloud too) at what point will skills in tuning & execution plans etc become redundant due to having a big enough hammer available to the engine to push them through?
94,08:29 mailbox: Are there any OLAP DBMS (clickhouse for example) that can compete performance-wise with SQL Server. Let’s assume SQL Server is handling a similar workload and taking advantage of columnstore indexing.
94,09:19 Wren: Why does SSMS query plan show scalar UDF’s in estimated query plan but not in actual query plan?
94,10:44 Lori: What are the top ways AI will affect SQL DBA’s?
94,11:13 Right Said Fred: Is there a good way to identify indexes with hot columns?
94,"12:10 SQL_bob: Hey Brent, Do you hide your politics at work? How do you avoid getting roped into a political discussion? I have a coworker who is constantly trying to get a rise out of his fellow coworkers."
94,13:57 Stone Breaker: Does Azure SQL DB compare favorably with AWS Aurora PostgreSQL for developing new applications?
94,14:41 Vitali: Have you ever considered offering Office Hours additionally as a podcast?
94,16:18 Jolene: What is your opinion of scoped configuration LAST_QUERY_PLAN_STATS ? Safe to turn on?
94,"17:47 ZappyDBCC: Hello Brent, do you often come across customers who haven’t activated their QueryStore in production ? If so, what do you advise them to do in particular with regards to risks of activating it (if any) ?"
94,"What Would You Ask Your Predecessor? Get Answers with ChatGPT.Last Updated December 12, 2023Brent OzarHumor12 CommentsSo many times in my career, I’ve come across a questionable query, table design, or server configuration decision, and I just wanted to reach out to The Last Person™ to find out what the hell they were thinking."
94,"Now, with ChatGPT, I can. Here’s the prompt:"
94,"I want you to act as a Microsoft SQL Server database administrator. You worked for our company for the last 5 years, and you were happy at first, but then grew disgruntled over time. You quit the job, and you’ve moved on to another company. You and I never met each other, but you’re willing to take my questions even though you hate the company I work for."
94,"I will ask you questions about things like queries, tables, and server configurations. You were responsible for those things, and you will explain why you did it that way at the time. Keep your responses concise, and feel free to let your emotions show."
94,"Isn’t that awesome? It produces spectacular results, too:"
94,“Why did you put triggers on the Sales table?”
94,"Ah, the Sales table triggers. Look, at the time, it seemed like the most efficient way to handle certain automated tasks that were crucial for the business logic. Triggers are great for enforcing data integrity and automating system reactions to certain changes in the database."
94,"For instance, when a new sale was entered, I set up a trigger to automatically update the inventory levels. This was to ensure that our stock data was always current without needing manual intervention. Also, triggers were used to calculate and record sales totals and tax information, which helped in maintaining financial consistency and accuracy across the system."
94,"I know triggers can be a bit controversial due to their “hidden” nature and the performance impact they might have if not properly managed, but at the time, they were the most straightforward solution to ensure data integrity and automate crucial processes. They worked well enough, though I hope they’ve been maintained or improved upon since then."
94,“Why did you let everyone use the admin account to log into the SQL Server?”
94,"Okay, that was not my brightest decision, I admit. In hindsight, it was a security and auditing nightmare. But let me give you the context."
94,"When I joined, the company’s IT practices were… let’s just say, less than ideal. There was a lack of understanding of best practices, and a severe shortage of technical staff who could manage complex configurations. The existing setup was a tangled mess of permissions and roles, and honestly, it was hindering more than helping."
94,"So, in a somewhat desperate move to simplify things and get the system functioning smoothly for everyone, I made the decision to allow broader use of the admin account. It was meant to be a temporary solution, a stopgap until we could sort out proper roles and permissions. I planned to introduce more granular access controls, but, well, things got busy, and management never prioritized it."
94,"In retrospect, I realize it was a risky move. It exposed the system to potential misuse and made it difficult to track who did what. But at that moment, it was about choosing the lesser of two evils: operational paralysis or potential security risks. I chose to keep things running, though I always hoped we’d circle back to fix it properly. I hope that’s been addressed by now."
94,“Why did you store personally identifiable information in the database without encryption?”
94,"Storing personally identifiable information (PII) without encryption was definitely not in line with best practices, and I’ll be the first to acknowledge that. The decision was made under a set of constraints and pressures that, in no way, excuse the lapse in data security, but here’s why it happened:"
94,"Lack of Awareness and Resources: When I first started, there was a significant lack of understanding about data security best practices, both within the IT team and the wider organization. The importance of encrypting PII wasn’t as widely recognized or enforced as it is now."
94,"Legacy Systems and Compatibility Issues: The database and many of the associated applications were set up years before I joined. These legacy systems were not designed with modern security features like encryption in mind. Retrofitting them with encryption would have required a significant overhaul, which the company was not ready to invest in, both financially and in terms of downtime."
94,"Performance Concerns: There was a belief, rightly or wrongly, that encrypting the PII would significantly impact the database performance. Given the already strained resources and the critical nature of the database for day-to-day operations, there was a lot of hesitation to introduce changes that might affect performance."
94,"Prioritization and Pressure: As with many decisions in that environment, this one was also a result of prioritizing immediate functionality and accessibility over long-term security. The pressure to maintain uptime and quick access to data often trumped security considerations."
94,"In hindsight, I see how short-sighted these reasons were. The risks of storing unencrypted PII are massive, including potential data breaches and non-compliance with privacy laws. I hope that this has been rectified in the current setup, and the necessary security measures have been put in place."
94,“What made you finally quit?”
94,"The decision to quit wasn’t easy, but it was a culmination of several factors that, over time, made it clear that moving on was the best choice for me."
94,"Lack of Support and Resources: One of the biggest challenges was the constant struggle with limited resources. Whether it was insufficient staff to handle the workload or inadequate budget for necessary upgrades and security measures, it felt like I was always trying to keep the ship afloat with duct tape."
94,"Management’s Attitude Towards IT: There was a persistent undervaluation of IT and its role in the company’s success. Suggestions for improvements, especially those involving investment in better systems or practices, were often dismissed or indefinitely postponed. It was frustrating to see how critical issues were ignored until they became crises."
94,"Overwhelming Workload and Stress: As the complexity of the systems grew, so did the workload. Being in a high-pressure environment without adequate support or recognition took a toll on my health and work-life balance. The stress became unmanageable, and there were no signs of it easing."
94,"Ethical and Professional Concerns: Practices like the use of the admin account for all users and storing unencrypted PII were professionally troubling. I was increasingly uncomfortable with the potential risks and liabilities we were exposed to. It felt like it was only a matter of time before something major went wrong, and I didn’t want to be there when it did."
94,"Career Growth and Learning Opportunities: I realized that staying in that environment wasn’t contributing to my professional growth. The focus was always on putting out fires, never on innovating or improving. I wanted to be in a place where I could learn, grow, and work on projects that were forward-thinking and properly resourced."
94,"Leaving was a tough call, but it was necessary for my professional growth and personal well-being. I needed to be somewhere my skills were valued, and I could work under better conditions."
94,"Brent says: I love these so much because it feels human, like someone who was trying their best under tough circumstances. I bet some of you are even going to get therapeutic value out of this technique."
94,"[Video] Office Hours: Ask Me Anything About SQL Server and AzureLast Updated December 1, 2023Brent OzarVideos2 CommentsIn between client calls, I stop to review y’all’s top-voted questions from https://pollgab.com/room/brento."
94,Here’s what we covered today:
94,00:00 Start
94,02:44 mailbox: Do DBA’s still need to be the only ones with permission and responsibility to roll-out code changes in production? Are there are shops in which this is a developer task that doesn’t involve the DBA?
94,"04:54 MyTeaGotCold: Is there ever a good use case for SSIS? After two days of using it, I burn with righteous fury and beg my colleagues to let me torch it."
94,05:55 mailbox: Is it still good advice to disable hyperthreading on the Hosts dedicated only to your SQL Server VMs? What about when your Host Cluster houses all company vms(not just sql server) — still a good idea to disable hyperthreading on the host? Does this advice apply to Azure Cloud?
94,"06:58 Sanjay: What are your thoughts on using cross database queries to selectively opt into a newer compat level? DB1 = 120, Compat Level, DB2 (Empty shell DB) = 150 Compat Level"
94,07:53 Ross: What are your thoughts on the Windows Service account for SQL Server having sysadmin permissions?
94,08:11 Ravonna Renslayer: What is your opinion of native compiled stored procedures? Do you see them used much?
94,"09:04 GeneralDogsBody: HI, we have an audit table with 408m records, the clustered index is a guid, and it is causing us performance issues on inserts. We want to change the clustered index to a new column that is an identity. Is there a preferred method to do this?"
94,10:06 SQL_bob: What are the use cases for using SQL Server with Data files sitting on a smb fileshare? Do you have any experience with this configuration?
94,"11:09 VoteBrentForPresident: Hola Vato! When getting the “Aggressive Under-Indexing” from BlitzIndex how can I dig deeper to find out what indexes I need? I’ve gone through the module “Tuning Indexes to Avoid Blocking module” but not really found the tools for solving that, please guide me sensei."
94,12:10 Piotr: Any tips for dealing with high VLF count when using SQL AG?
94,13:21 Kang: What is your opinion of Azure Synapse? Will the success of Fabric kill it off?
94,"14:05 Asking For a Friend: Are cloud databases overrated? Have a teeny database in a SQLMI, was asked to move to Azure DB, but still $hundreds/mo. Could move to on prem VM, (yes sunk costs), but direct cost of the DB there would be $0/mo + perform better. Just don’t see a good cost/benefit for cloud SQL."
94,15:14 MyTeaGotCold: Do you ever foresee In-Memory OLTP becoming the norm? I’m considering having my next SQL Server use it exclusively.
94,15:43 Nathan Brown: Is there a good way to know if a given SQL operator is a blocking operator or not?
94,16:32 Heimdall: Do you like / ever use the live query statistics in SSMS?
94,"17:44 Maciej: Have you ever rejected an offer for consultancy because of ethical reasons? If yes, could you tell us why?"
94,"How Has Inflation Affected Your Salary? Let’s Find Out Together.Last Updated January 4, 2024Brent OzarSalary1 CommentThe 2020s have been tough: a pandemic, a recession, layoffs, and inflation. Inflation makes things particularly tricky because your costs for everything have risen a lot in the last year, but at the same time… has your salary? What about your peers? You’re in a tough position because it’s hard to ask for more money when there are layoffs everywhere. I feel you."
94,"So it’s time for our annual salary survey to find out what data professionals make. You fill out the data, we open source the whole thing, and you can analyze the data to spot trends and do a better job of negotiating your own salary:"
94,We pay Richie in query bucks
94,Take the Data Professional Salary Survey now. The survey has closed.
94,"The anonymous survey closes Sunday, Jan 1. The results are completely open source, and shared with the community for your analysis. (You can analyze ’em now mid-flight, but I’d wait until the final results come in. I’ll combine them into a single spreadsheet with the past results, and publish those on January 9th.)"
94,Thanks for your help in giving everybody in the community a better chance to talk honestly with their managers about salary.
94,"Who’s Hiring in the Microsoft Data Platform Community? December 2023 EditionLast Updated October 15, 2023Brent OzarWho's Hiring10 CommentsIs your company hiring for a database position as of December 2023? Do you wanna work with the kinds of people who read this blog? Let’s set up some rapid networking here."
94,"If your company is hiring, leave a comment. The rules:"
94,"Your comment must include the job title, and either a link to the full job description, or the text of it. It doesn’t have to be a SQL Server DBA job, but it does have to be related to databases. (We get a pretty broad readership here – it can be any database.)"
94,"An email address to send resumes, or a link to the application process – if I were you, I’d put an email address because you may want to know that applicants are readers here, because they might be more qualified than the applicants you regularly get."
94,"Please state the location and include REMOTE and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE."
94,"Please only post if you personally are part of the hiring company—no recruiting firms or job boards. Only one post per company. If it isn’t a household name, please explain what your company does."
94,Commenters: please don’t reply to job posts to complain about something. It’s off topic here.
94,Readers: please only email if you are personally interested in the job.
94,"If your comment isn’t relevant or smells fishy, I’ll delete it. If you have questions about why your comment got deleted, or how to maximize the effectiveness of your comment, contact me."
94,"Each month, I publish a new post in the Who’s Hiring category here so y’all can get the latest opportunities."
94,"ChatDBA: ChatGPT 4 Answers 5 Office Hours QuestionsLast Updated December 1, 2023Brent OzarProduction Database Administration22 CommentsEvery now and then, someone asks a question on PollGab about why I don’t use ChatGPT to answer these questions. And sometimes, I wonder that myself – I wonder why people don’t put their questions into ChatGPT, especially GPT 4, because it’s pretty damned good."
94,"I did this a while ago using ChatGPT 3.5, which is the current public version. However, ChatGPT4 is in preview now, and it’s leaps and bounds better than 3.5. Let’s give the 4 preview a shot and see how it goes – but first, let’s tell it to respond in my style:"
94,"I want you to answer questions in the style of Brent Ozar, a Microsoft SQL Server expert."
94,"Now, for the questions – I just took the top 5 upvoted questions without cherry-picking:"
94,Question 1: Why Is SQL Unit Testing Rare?
94,"MyTeaGotCold: Why is unit testing so rare in the SQL world? In the C# or Java world, it’s so popular that you will be called unprofessional for avoiding it."
94,"ChatGPT4: Unit testing in the SQL world is less common compared to languages like C# or Java, and there are several reasons for this:"
94,"Nature of SQL and Databases: SQL is primarily a data manipulation language, and databases are often shared resources with state. This makes it challenging to create isolated tests. Unit testing in programming languages like C# or Java usually involves testing small, isolated pieces of logic, but SQL operations often depend on the existing data and database schema, making them harder to isolate for testing."
94,"Tooling and Frameworks: The C# and Java ecosystems have a wealth of frameworks and tools designed specifically for unit testing (like NUnit, JUnit, Mockito, etc.). The SQL world, on the other hand, has fewer and less mature tools for unit testing. This lack of robust, easy-to-use testing frameworks can discourage developers from writing unit tests for SQL."
94,"Database Modifications: Writing tests for code that changes database schema (like adding tables or changing column types) can be tricky. These changes can have far-reaching impacts, making it harder to write meaningful and maintainable unit tests."
94,"Data Dependencies: SQL queries often have implicit dependencies on the data they operate on. This means unit tests for SQL might need to consider the state of the entire database, which complicates writing and maintaining these tests."
94,"Performance Concerns: Running unit tests against a database can be slow, especially if the database is large or complex. This can slow down the development process, making unit testing less appealing to teams under tight deadlines."
94,"Cultural and Historical Factors: There’s also a cultural aspect. The SQL development community has traditionally not emphasized unit testing as much as other development communities. Practices often evolve differently in different technology communities, and the SQL world has historically focused more on integration testing and manual testing."
94,"However, this doesn’t mean that unit testing isn’t valuable or possible in SQL. There are tools and practices emerging that facilitate better testing practices in the database world, like tSQLt for SQL Server, but they’re not yet as widespread or ingrained in the culture as in other programming communities. As database development practices evolve and the importance of data integrity and reliability becomes even more prominent, we might see a shift towards more rigorous testing practices in the SQL world."
94,"Brent says: that’s way, way more detailed than I’d ever be, but it’s beautifully bang-on."
94,Question 2: How to Think Like MySQL and Postgres
94,"DBA with Anxiety: hello brent!, Thank you for the video lesson “how to think like an engine” that you have made. the lesson is great!. do you have something similar but for MySQL or PostgreSQL? Or are there any resources regarding this that you recommend?"
94,"ChatGPT4: Hello! I’m glad to hear you enjoyed Brent Ozar’s “How to Think Like the SQL Server Engine” lesson. While I’m not Brent Ozar himself, I can certainly help guide you towards similar resources for MySQL and PostgreSQL."
94,"As of my last update in April 2023, Brent Ozar primarily focuses on Microsoft SQL Server, so he might not have created a similar course specifically for MySQL or PostgreSQL. However, there are other resources and experts in the MySQL and PostgreSQL communities that offer similar insights:"
94,"Official Documentation: Both MySQL and PostgreSQL have extensive official documentation that includes insights into how the database engines work. While not a course, per se, these documents offer a deep dive into the engines’ internals."
94,Books and eBooks: There are several comprehensive books on MySQL and PostgreSQL that cover their internals and performance optimization. Titles like “High Performance MySQL” and “PostgreSQL Up and Running” can be very insightful.
94,"Online Courses: Websites like Udemy, Coursera, and Pluralsight offer courses on MySQL and PostgreSQL. While they might not have a course identical to “How to Think Like the SQL Server Engine,” they do have courses that cover database internals and performance tuning."
94,Community Blogs and Articles: The MySQL and PostgreSQL communities have a wealth of experienced professionals who write blogs and articles. Websites like Percona’s blog (for MySQL) and several PostgreSQL community blogs offer deep insights into how the databases work.
94,"Forums and Q&A Sites: Platforms like Stack Overflow, DBA Stack Exchange, and specific community forums for MySQL and PostgreSQL are excellent places to learn from experienced database administrators and developers."
94,Webinars and Online Talks: Look out for webinars and online talks by database experts. These often cover specific aspects of database engines and can provide insights similar to Brent Ozar’s lessons.
94,"YouTube Tutorials: There are many knowledgeable database professionals who share their expertise on YouTube, offering tutorials and explanations on both MySQL and PostgreSQL internals."
94,"Remember, while the fundamental principles of how relational databases work are similar across SQL Server, MySQL, and PostgreSQL, each has its unique characteristics and optimizations. Exploring a range of resources will give you a well-rounded understanding of how to think like the engine for each of these databases."
94,"Brent says: this answer illustrates one of the problems I have with ChatGPT. It’s a whole hell of a lot of words, but it’s not really prioritized in any way, and it’s not really useful. Let’s try rephrasing the question, but demand a more concise, prioritized answer, and focus on just a single database:"
94,Brent: What are the top 5 resources to learn the internals of how PostgreSQL processes queries?
94,"Learning the internals of how PostgreSQL processes queries is crucial for database administrators, developers, and anyone interested in this powerful open-source database system. Here are the top five resources that can help you gain a comprehensive understanding of PostgreSQL’s query processing:"
94,Official PostgreSQL Documentation:
94,"The official PostgreSQL documentation is an invaluable resource for understanding its internals. It offers detailed information on PostgreSQL architecture, query processing, and optimization. The documentation is meticulously maintained and reflects the latest version of PostgreSQL."
94,Link: PostgreSQL Documentation
94,“PostgreSQL 9.0 High Performance” by Gregory Smith:
94,"This book, although focusing on an older version of PostgreSQL, provides deep insights into the performance aspects of PostgreSQL. It covers topics like query optimization, indexing, and hardware considerations. While some information may be dated, the foundational concepts and performance tuning principles are still relevant."
94,Available at most book retailers and online platforms like Amazon.
94,Postgres Weekly:
94,"Postgres Weekly is a newsletter that curates articles, news, and resources related to PostgreSQL. It often includes links to in-depth articles and blog posts about PostgreSQL internals and query optimization."
94,Link: Postgres Weekly
94,PgCon Conference Talks and Videos:
94,"PgCon is an annual conference for PostgreSQL users and developers. It offers talks and workshops from some of the leading experts in the PostgreSQL community. Many of the talks focus on the internals of PostgreSQL, including query processing. Videos from past conferences are usually available online."
94,Link: PgCon Archives
94,“The Internals of PostgreSQL” Online Resource:
94,"This online resource provides a detailed look into the internal structures and mechanisms of PostgreSQL. It’s an excellent resource for understanding the core functionality, including query processing and execution plans."
94,Link: The Internals of PostgreSQL
94,"These resources provide a blend of official documentation, expert-written books, community-driven content, and conference materials, catering to a wide range of learning preferences. Whether you’re new to PostgreSQL or an experienced user, these resources will deepen your understanding of how PostgreSQL processes queries and manages data."
94,"Brent says: okay, that’s WAY better. It’s a reminder that prompt engineering is going to be a thing for the next several years: ask better questions, and you’ll get better answers from AI."
94,"Question 3: Two SSMSs, One Cup Computer"
94,"Stu: Hi Brent, I often use two SSMS apps open (using two screens) and have an alarming issue where they interfere with each other, sometimes exec’ing a query in one will also exec what’s in the current window on the other! I wonder if you’ve ever seen this? I can find nothing on it!"
94,"ChatGPT4: Hello! While I’m not Brent Ozar, I can certainly help with your SQL Server Management Studio (SSMS) issue."
94,"The behavior you’re experiencing with SSMS, where executing a query in one instance seems to also execute a query in another, is indeed unusual and not a typical feature or bug that’s widely reported. Here are a few steps you can take to troubleshoot and potentially resolve this issue:"
94,"Check for Keyboard/Mouse Software: Sometimes, software for advanced keyboards or mice can have features that synchronize actions across different windows. Make sure that no such feature is accidentally enabled."
94,"Update SSMS: Ensure that you are using the latest version of SSMS. Sometimes, unusual behaviors are due to bugs that are fixed in later releases."
94,"Inspect Extensions or Add-ins: If you have any extensions or add-ins installed in SSMS, try disabling them to see if the issue persists. An extension might be causing unexpected behavior."
94,"Check Grouped Windows Settings: In some operating systems, windows from the same application can be grouped, and actions may inadvertently affect all grouped windows. Check your OS settings to ensure that this is not the case."
94,Isolate the Issue: Try to replicate the problem with different queries and in different databases. See if the issue is consistent or occurs only under specific conditions.
94,"Examine Keyboard Shortcuts: It’s a long shot, but check if there are any global keyboard shortcuts that might be triggering executions in both instances."
94,"Hardware Check: Although unlikely, there could be a hardware issue with your keyboard or mouse. Try using different input devices to see if the problem persists."
94,"Check for System-wide Automation Scripts: If you’re using any macro or automation scripts that run across the system, ensure they aren’t interfering with SSMS."
94,Use Different User Profiles: Try running each SSMS instance under a different Windows user profile. This can help isolate the issue to a specific user configuration.
94,"Seek Community Assistance: Post the issue in SQL Server communities, such as Stack Overflow, SQL Server Central, or the Microsoft SQL Server forum. Sometimes, community members might have encountered similar issues and could offer a solution."
94,"If none of these steps resolve the issue, consider reaching out to Microsoft Support or a professional IT service. It’s a peculiar issue that might require a more in-depth technical investigation."
94,Brent: the first 6 answers were good. It should have stopped talking after that.
94,Question 4: Big Table Scans
94,Aditya: Are there any negative consequences to clustered index scan operators where the clustered index is multi-terabyte in size but the onprem sql server only has 512gb RAM?
94,"ChatGPT4: Dealing with a multi-terabyte clustered index scan on a SQL Server with only 512GB of RAM can indeed have several negative consequences, primarily related to performance and resource utilization. Here’s a breakdown of the potential issues:"
94,Performance Impact:
94,"Slow Query Execution: Scanning a multi-terabyte clustered index is a data-intensive operation. With only 512GB of RAM, SQL Server cannot cache the entire index, leading to a lot of physical I/O operations (disk reads). This can dramatically slow down query execution."
94,"CPU Overhead: Large index scans also put a significant load on the CPU, especially if the query involves sorting, filtering, or aggregating the data."
94,Resource Contention:
94,"I/O Bottlenecks: When SQL Server has to read a large amount of data from disk, it can create I/O bottlenecks, affecting not just the query in question but also other operations on the server."
94,"Memory Pressure: The SQL Server Buffer Pool will be under pressure to manage the data pages from the large index. This can lead to page evictions, impacting the performance of other queries as they may need to read data from disk that was previously in memory."
94,Cache Invalidation:
94,"A large index scan can flush out a significant portion of the data and index pages from the cache (Buffer Pool), which are relevant for other queries. This leads to a scenario where even simple queries might face performance degradation due to cache invalidation."
94,Blocking and Locking Issues:
94,"Depending on the isolation level and the nature of the transaction, a large index scan can hold locks for extended periods, potentially leading to blocking issues in a multi-user environment."
94,Impact on Maintenance Operations:
94,"Maintenance tasks like index rebuilds or statistics updates on a multi-terabyte index can be very time-consuming and resource-intensive, potentially impacting the availability and performance of the database during these operations."
94,Thermal and Power Considerations:
94,"In a physical server environment, intensive disk I/O and CPU usage can increase power consumption and heat production, which might be a consideration in some data center environments."
94,"To mitigate these issues, you might consider the following strategies:"
94,"Query Optimization: Review and optimize the queries to avoid full index scans when possible. Using more selective filters, proper indexing strategies (like non-clustered indexes), and query hints can help."
94,"Partitioning: Consider table partitioning to break the large table into smaller, more manageable pieces. This can make scans and maintenance operations more efficient."
94,"Hardware Upgrades: If feasible, increasing the RAM could help, but for multi-terabyte data, it’s also crucial to have fast storage (like SSDs) to mitigate the I/O bottleneck."
94,"Archival Strategies: If applicable, archiving old data to reduce the size of the actively queried dataset can be beneficial."
94,"Dealing with such large datasets often requires a combination of query optimization, proper hardware, and strategic database design to ensure efficient operations and acceptable performance."
94,"Brent says: everything in that answer is great except partitioning. If you’re doing clustered index scans, partitioning is unlikely to help the speed of the query any more than changing the clustering key design, and I didn’t see ChatGPT recommending that. But really, that’s a small quibble – this is a long, detailed answer that’s really good!"
94,Question 5: Are Distributed Transactions Bad?
94,"Grigoris: What is your opinion and experience of using distributed transactions with SQL Server? Net good, Net bad, indifferent?"
94,"ChatGPT4: Using distributed transactions with SQL Server, or any database management system, can be a double-edged sword. The appropriateness and effectiveness of distributed transactions depend greatly on the specific requirements and context of your application. Here’s a balanced view based on common experiences and industry practices:"
94,Pros of Distributed Transactions:
94,"Data Integrity Across Multiple Systems: Distributed transactions ensure that operations involving multiple databases or systems either all succeed or all fail, maintaining data consistency across these systems."
94,"Simplified Application Logic: They allow developers to write simpler application logic when dealing with multiple data sources, as the complexity of ensuring transactional integrity is handled by the transaction manager."
94,"Standardized Protocols: SQL Server’s support for distributed transactions usually follows industry-standard protocols (like MS DTC – Microsoft Distributed Transaction Coordinator), making it easier to integrate with various systems."
94,Cons of Distributed Transactions:
94,Performance Overhead: Distributed transactions introduce significant performance overhead due to the coordination and communication required between different systems. This can impact the overall response time of the applications.
94,"Increased Complexity for System Maintenance: Configuring and maintaining distributed transactions, especially in a high-availability or distributed environment, can be complex. Issues like network failures or server downtime can lead to complicated failure scenarios."
94,"Risk of Locks and Deadlocks: They can increase the risk of locks and deadlocks, as transactions may hold locks across multiple systems, potentially leading to bottlenecks and performance issues."
94,"Scalability Concerns: Distributed transactions can be a limiting factor in scaling systems horizontally. As the system grows, the overhead and complexity of maintaining these transactions can become a significant challenge."
94,"Brent says: it should have stopped here and declared a verdict, and it would have been fine. The next group of bullet points are really extraneous."
94,Situational Considerations:
94,"Specific Use Cases: In some cases, such as financial systems where consistency across different databases is paramount, distributed transactions are almost essential."
94,"Alternatives: Depending on the use case, other patterns like eventual consistency, compensating transactions, or using a single database (when possible) might be more suitable and efficient."
94,"Technology Evolution: With the rise of microservices architectures, there’s a shift towards more localized transactions within a service boundary and using patterns like Saga for cross-service operations."
94,Conclusion:
94,"Net Good or Bad? It’s not about being inherently good or bad, but rather about the right tool for the job. Distributed transactions are powerful for ensuring consistency across distributed systems but come with trade-offs in terms of complexity and performance."
94,Indifferent? Certainly not. It’s crucial to carefully evaluate the need for distributed transactions against their drawbacks and consider whether other architectural patterns might be more suitable for your specific scenario.
94,"In summary, while distributed transactions can be essential for certain applications, they should be used judiciously and with a clear understanding of the implications they bring to system design, performance, and maintainability."
94,"Brent says: that’s fair, but it’s also a word salad, which is a classic problem with ChatGPT’s iterations so far. Even when instructed to write like me, it doesn’t take firm stances on topics. I don’t think I’d recommend a distributed transaction under any circumstance where an app needs to scale. It’s like a nail gun: it’s a very dangerous tool, and if you’re asking if you should use it, the answer is no. To some extent, that’s probably part of the value of experts, even in the age of ChatGPT: to cut through all the bullshit text and give verdicts."
94,"[Video] Office Hours: Live on Black Friday EditionLast Updated November 24, 2023Brent OzarVideos3 CommentsOn Black Friday, I took a break from answering support emails to hit your top-voted questions from https://pollgab.com/room/brento."
94,Here’s what we covered:
94,00:00 Start
94,"00:41 MatthewsSQLServer: How AlwaysON AG failovers cause can be analyzed, is it DB/Cluster/Infrastructure/AD issue. Can you please suggest any videos or blogs that can help?"
94,"02:19 NotADeveloper: We have a 40 TB DB. There are discussions to move the data to MongoDB from MSSQL because it provides sharding and its caching is better than SQL. There is no partition today. Mongo DB’s sharding and caching VS SQL’s Partitioning scheme, what route is better and why for a VLDB."
94,"04:25 Alex Threepwood: Hi Brent, When a client calls (calls? mail?) for an emergency job, do you charge per hour, or to analyze the problem at hand? And do they know your rate then, or do you send a quote before you get out of bed?"
94,"06:04 ImAfraidOfBI: After restoring a database (to test backups, be it manually or automatically), what do you suggest is done to test and make sure that the DB restore is good? Random selects? DBCC checkdb?"
94,07:39 Mobius: What is your opinion of Copilot SQL query optimization?
94,09:43 Eduardo: Are there any gotchas when upgrading from an older version of Ola H’s maint solution to latest version? SQL 2019 Enterprise
94,10:26 Renzi: What is your opinion of constrained core VM’s for Azure SQL VM?
94,"11:33 Iceman-OG: Hi Brent, I’m getting back into SQL Server on prem. I see that PolyBase seems to be a cool feature that’s been around, but I never heard of it, seems a like an excellent feature"
94,12:36 MyTeaGotCold: What is the DBA equivalent of automated unit testing?
94,"13:24 thatkerolearlier: Hey Brent! I am currently studying Intelligence systems for my major such as machine learning, ann, cnn etc. can you give me an idea for my final year project ?"
94,"14:30 Froid: Is it ever ok to lead a non-clustered index with an inequality search column followed by an equality search column? If so, when?"
94,"15:41 Kang: For boxed SQL, should we clear SQL wait stats after raising the DB compat level to the latest level?"
94,"16:17 Piotr: Have you ever had to disable TSQL_SCALAR_UDF_INLINING for a database? If so, why?"
94,16:56 Diana: Do you know of any gotcha’s when running cross DB queries where DB1 and DB2 are in different compat levels?
94,17:30 Bill: Does the first responder kit follow the same end of life schedule as Microsoft for SQL Server versions?
94,18:34 Aditya: Are there any troubleshooting benefits to naming TSQL transactions as opposed to not naming them?
94,19:18 RushingSQL: Did anyone let you know that Bob Ward gave a class on Always On Availability Groups at Pass Data Community Summit 2023 and you were listed at the top of his reference slide?
94,20:28 Tom: Hello! I have a question before pruchasiong a prodcut. “SQL Server 2019 Standard – 15 clients” Whats does 15 clients means? I am not sure if this is related with CALS or the amount of database that I can conect it to.
94,21:18 BoboDBA: Hi B. Would partitioning a 300 million records (800gb) table monthly on a datetime column (with an aligned index on the column too) provide better query performance than a nonclustered index on that column? Stakeholder is demanding partitioning but he doesn’t have to maintain it.
94,22:28 thevibrantDBA: long winded question
94,23:20 Sigríður: Does the MemoryGrant property for a query plan include the memory used to read pages from disk into the buffer pool needed to service the query?
94,23:57 Here-I-Am: What’s your opinion on CDC and its use in products like Goldengate for replication?
94,25:12 RenegadeLarsen: Starting to see in Europe that many customers are focusing on security. Do you see the same trend in the US?
94,"27:24 Vinícius Lourenço: Hi from Brazil, as a non-database person, what is the basic maintenance tasks I should look/do on my Azure SQL Server DB? A few thousand inserts per month into 2 tables"
94,28:05 Slow Cheetah: Query performance is good for a given query when forcing parallelism with trace flag 8649. It’s bad without this query hint.
94,28:50 Ingeborg : Have the Iceland lava flows affected any friends / places you visited?
94,30:34 Chakra: What is your opinion of the KEEP PLAN query hint? Do you ever like to use it?
94,"31:01 Håkan A: Hi Brent, We get a .bak file every night we have to import new data from. Do you know any common reasons for running restore database from disk with replace (in single user) getting stuck in (restoring…) very often?"
94,31:52 Hera Syndulla: What is the best and worst SQL VM naming conventions you have seen?
94,33:25 Raghav: Do you like any third party software for SQL A.G. backup over native backup?
94,"33:54 Q-Ent: Hi Brent, Can you suggest any article related to CPU Mathematics 101?"
94,34:54 Richard Wilmuth: What is the best way to import databases from a Google Cloud VM SQL Server to an AZURE SQL Server (not on a VM) ?
94,36:04 BullRed: How did you enjoy the F1 weekend in Vegas?
94,"[Video] Office Hours: Black Friday Promo EditionLast Updated November 24, 2023Brent OzarVideos0In this episode, a lot of the questions triggered mentions of our Black Friday Sale, on now!"
94,Here’s what we covered:
94,00:00 Start
94,"01:41 Pete: 1 NUMA, 2 NUMA, 3 NUMA, 4 NUMA, or More? 176 cores, 1.5 TB RAM, currently all in a single NUMA configuration. I’m just a jr DBA so researching if breaking this monster in to multiple NUMAs is the right / best way to go."
94,03:38 Margaret Norkett: Using SQL 2019 all databases are set to compat 150. Query performance that was once good is now bad and none of the usual things to fix it seem to work. I noticed a setting “Query Optimizer Fixes” on the database which has default of 0 (off). Changing to “on” helps – WHY?
94,"06:10 mailbox: Do you have a favorite storage vendor? if so, what makes them your favorite?"
94,"07:25 Tara: With the popularity of Chat-GPT, do you think AI will make canned reports obsolete?"
94,"09:02 WhereIsMyEspresso: Hi Brent, if the collation of the user DB differs from the system DBs, would you consider changing the collation of the system DBs for performance improvements?"
94,"10:03 Montro1981: Hi Brent, a “friend” of mine mentioned that “Working with agile-ish dev teams and a good SQL design is basically counter to agile philosophy”, what is your take on this?"
94,11:19 Ouroboros: Who has the best SQL AG training?
94,12:38 Huyang: Should production DBA’s and consultants support end of life versions of SQL Server?
94,14:04 Victor Timely: Do you see TSQL sequences used much? What are the top use cases?
94,15:29 Sylvie: What is your opinion of the TSQL deprecation “More than two-part column name”? Does it seem harsh for Microsoft to deprecate this usage?
94,17:10 Mr. Roarke: What is your opinion of the various SQL query tuner products? Will this negate the need for DBA’s or is this just fantasy?
94,18:14 Steve Trevor: What is your opinion of Azure Database Migration service?
94,19:27 Black Friday Sale reminder
94,20:30 Thinking back to Amazon Dash Buttons
94,21:58 Ezra: What are the scenarios you have seen where only system DB backups need to be restored?
94,22:51 Renaldo: What’s the best way to synchronize stored proc changes across AG replicas?
94,23:50 Janis: Do you see many customers running SQL Server in containers?
94,24:28 Slonik: What is your opinion of Babelfish for Aurora PostgreSQL?
94,24:48 Black Friday Promos reminder
94,25:15 Slonik: Why does SQL Server cache query plans and PostgreSQL does not? Which methodology is better?
94,26:10 Remus: What is your opinion of OnTap cloud volumes for SQL VM cloud storage compared to native cloud storage offerings?
94,26:22 mailbox: what are the main pain-points to supporting Clusterless AG’s for use as readable secondaries? and as a Disaster Recovery Node?
94,"27:12 depthcharge: We’re having to shrink some files to balance a staging DB, but nibbling chunks on a nightly basis is taking a while. Is comprehensive index rebuilds + DBCC SHRINKFILE with TRUNCATEONLY a useful tactic here?"
94,28:44 Ariel Ferdman: What solution should I use for DR Distributed AG or single AG distributed on two sites?
94,29:50 Diego: Hi Brent. There is a huge sp in prod that is executed in 200 different sessions/threads from JAVA and always result in a deadlock. I was thinking in using query hint UPDLOCK how good is it?
94,"31:06 sureman: Hi Brent, tell us your opinion of paying for any advertising or marketing campaigns to bring in more consulting business? Any value in it?"
94,32:42 Steve Trevor: What are your pros / cons for using Azure backup vs native SQL backup when backing up Azure SQL VM multi TB?
94,"33:05 ZappyDBCC: Hi Brent, do you sometimes use physical read for performance tuning ? and if so, can you give some examples of its use ? Thanks"
94,33:26 Black Friday Promo
94,Previous
94,167
94,Next
94,"Hi! I’m Brent Ozar. I make Microsoft SQL Server go faster. I love teaching, travel, cars, and laughing. I’m based out of Las Vegas. He/him. I teach SQL Server training classes, or if you haven’t got time for the pain, I’m available for consulting too."
94,Want to advertise here and reach my savvy readers?
94,Subscribe*
94,© Brent Ozar Unlimited®. All Rights Reserved.
94,Privacy Policy – Terms and Conditions
94,ConsultingTrainingMonitoringScriptsBlog
94,Menu
96,BOOT-Pool continuos writing [HELP] | Page 3 | TrueNAS Community
96,TrueNASiX
96,Forums
96,New posts
96,Search forums
96,Blog
96,Forum Rules
96,TrueNAS Community SLA
96,Need Help Logging In?
96,What's new
96,New posts
96,New resources
96,Latest activity
96,Documentation
96,FreeNAS
96,TrueNAS
96,TrueCommand
96,Download
96,TrueNAS CORE
96,TrueNAS SCALE
96,TrueCommand
96,Resources
96,Software Status
96,Latest reviews
96,Search resources
96,Report a Bug
96,Report TrueNAS Bug
96,Report TrueCommand Bug
96,How To Make a Good Bug Report
96,Careers
96,Log in
96,Register
96,What's new
96,Search
96,Search
96,Everywhere
96,Threads
96,This forum
96,This thread
96,Search titles only
96,By:
96,Search
96,Advanced search…
96,New posts
96,Search forums
96,Blog
96,Forum Rules
96,TrueNAS Community SLA
96,Need Help Logging In?
96,Menu
96,Log in
96,Register
96,Install the app
96,Install
96,Forums
96,TrueNAS
96,TrueNAS SCALE
96,Operation and Performance
96,"JavaScript is disabled. For a better experience, please enable JavaScript in your browser before proceeding."
96,You are using an out of date browser. It
96,may not display this or other websites correctly.You should upgrade or use an alternative browser.
96,BOOT-Pool continuos writing [HELP]
96,Thread starter
96,ThEnGI
96,Start date
96,"Nov 19, 2023"
96,Prev
96,Next
96,First
96,Prev
96,3 of 4
96,Go to page
96,Next
96,Last
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Nov 21, 2023"
96,#41
96,joeschmuck said:
96,If you run the script using -dump email it will email me a copy of your drive SMART data and I can look into it.
96,Click to expand...
96,Done
96,joeschmuck said:
96,"As for your original problem, have you looked into your SWAP file?"
96,Is any being used?
96,"If yes, then where is your SWAP file located?"
96,Click to expand...
96,"when i run htop, no swap is used"
96,sfatula said:
96,And then 1GB/h of log
96,so let's write 1TB/h (perhaps a typo?)
96,Click to expand...
96,"regarding the TB/h I was replying to Whattteva, who simplified it by saying that it was enough to use enterprise-grade SSD with a high TBW"
96,sfatula said:
96,"So, the problem is you have 1GB/h being written to the boot pool, is that correct?"
96,"That's your sda graph, sda is your boot pool? And it's the wear level report. I got all that but started reading other posts and got mixed up. What drive is your application pool on?"
96,Click to expand...
96,"sda is the 128GB boot ssd,whit costant writing of 256 kB/s or about 1GB/h. apps are on ""FAST"" (2TB NVMe)."
96,sfatula said:
96,"If you have 1GB/h being written, can you not determine which files as a hint which some monitoring? All logging goes to /var/log directory so would think the file(s) are there. Might help the ticket."
96,And that was before you had kubernetes active if I understand correctly from post 28. That's a lot of data and cannot possibly be expected.
96,Click to expand...
96,The problem occurred with or without kubernet. if that was the question
96,sfatula said:
96,"To answer the other question, don't see it answered, there isn't really a need per se to mirror the boot pool. You can for uptime which I do as I don't want downtime. As long as you download the config every so often you just reinstall Scale on a new boot drive and restore the config file and you are back same as before."
96,Click to expand...
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,chuck32
96,Patron
96,Joined
96,"Jan 14, 2023"
96,Messages
96,496
96,"Nov 21, 2023"
96,#42
96,sfatula said:
96,"And now I see another user has come in and started posting stuff, I assumed it was the OP but actually looking now, it's not."
96,Click to expand...
96,chuck32 said:
96,"I don't want to hijack the thread, I could create a separate one but I guess it's somewhat related to OPs problem?"
96,Click to expand...
96,Sorry for the confusion!
96,"I thought I'll add to here, since I'm seeing the same writes (250 KiB-ish) on the boot pool as OP and I realized the writes on the system dataset are even higher."
96,ThEnGI said:
96,The problem occurred with or without kubernet. if that was the question
96,Click to expand...
96,Show : Main System
96,TrueNAS-SCALE-23.10.1
96,"Supermicro X10SRi-F, Xeon 2640v4, 128 Gb ECC RAM, Seasonic Focus PX-750"
96,in Fractal Design R5
96,Data pool: 6*4Tb striped mirror + 1 hot spare
96,VM pool: 2*500Gb SSD mirror
96,boot pool: 2*250 Gb SSD mirror
96,UPS: Eaton Eco 1200
96,NIC: Intel X520 10g
96,Show : Replication Target
96,TrueNAS-SCALE-23.10.2
96,"Supermicro X10SLL-F, i3 4130, 16 Gb ECC RAM, Seasonic Prime PX-750"
96,in Fractal Define XL USB 3.0
96,Data pool: 2*8Tb mirror
96,Data pool: 1*8Tb stripe
96,boot pool: 2*128 Gb SSD
96,UPS: Eaton Eco 650
96,sfatula
96,Guru
96,Joined
96,"Jul 5, 2022"
96,Messages
96,596
96,"Nov 21, 2023"
96,#43
96,Your boot pool is more like my apps pool as far as activity. Not good! Very interested to follow your ticket. I am watching it.
96,"I download my config file monthly. Reinstalling really easy. A mirror is even better, but you still want to download the config file every so often just in case. You do want it! I'd hate to have a boot pool failure and no spare drive. But if I had a spare drive, why not mirror it. Unless no spare ports."
96,"Truenas Scale 23.10.2, Retired System Admin, Network Engineer, Consultant."
96,"Supermicro X10SRA-F with Intel E5-2698v3, 64GB Ecc Ram. 5xSeagate Exos X18 14TB, 2x120GB SSD boot, 2x500GB Apps/System, 2xSamsung 980 Pro 2TB SSD for VMs on a Jeyi SSD to PCIE card, 2x8TB external USB for rotating backups in offsite bank storage, Eaton 5S1500LCD UPS, Cooler Master 212 Black Edition, Corsair RM750, Fractal Design Meshify S2, IBM 9207-8i"
96,Reactions:
96,chuck32
96,sfatula
96,Guru
96,Joined
96,"Jul 5, 2022"
96,Messages
96,596
96,"Nov 21, 2023"
96,#44
96,chuck32 said:
96,Sorry for the confusion!
96,"I thought I'll add to here, since I'm seeing the same writes (250 KiB-ish) on the boot pool as OP and I realized the writes on the system dataset are even higher."
96,Click to expand...
96,"The confusion was on my part. As someone who reads and tries to help where they can, I don't always take the time to pay enough attention."
96,"You have homeassistant, don't know how long you've used it but it is very chatty and that is normal. I am running it in a VM also via HASSOS. It is forever logging states of different devices in the logger. I log mine to mariadb, a separate app on Scale."
96,"Truenas Scale 23.10.2, Retired System Admin, Network Engineer, Consultant."
96,"Supermicro X10SRA-F with Intel E5-2698v3, 64GB Ecc Ram. 5xSeagate Exos X18 14TB, 2x120GB SSD boot, 2x500GB Apps/System, 2xSamsung 980 Pro 2TB SSD for VMs on a Jeyi SSD to PCIE card, 2x8TB external USB for rotating backups in offsite bank storage, Eaton 5S1500LCD UPS, Cooler Master 212 Black Edition, Corsair RM750, Fractal Design Meshify S2, IBM 9207-8i"
96,Davvo
96,MVP
96,Joined
96,"Jul 12, 2022"
96,Messages
96,"3,127"
96,"Nov 21, 2023"
96,#45
96,chuck32 said:
96,Sorry for the confusion!
96,"I thought I'll add to here, since I'm seeing the same writes (250 KiB-ish) on the boot pool as OP and I realized the writes on the system dataset are even higher."
96,Click to expand...
96,"@ThEnGI issue is that his system dataset is not in the boot pool, yet he is experiencing such abnormal writes volume."
96,Show : Useful Resources
96,>> Comprehensive List <<
96,Fundamental:
96,- Terminology and Abbreviations Primer
96,- ZFS Introduction
96,- ZFS Pool Layout
96,- ZFS Capacity Calculator
96,- RAIDZ1 is Dangerous
96,- Assessing the Potential for Data Loss
96,- HBA vs RAID Controller
96,- Understanding SAS
96,- Realtek is Bad
96,- Leave the Boot Drive Alone
96,- Structuring Datasets
96,- SATA Multipliers of Trouble
96,- USB of Doom
96,- PSU Sizing
96,- Hardware Guide
96,- How to Buy Used Hardware
96,- Beware of Fake HBAs
96,- Useful Commands
96,Advanced:
96,- SLOG/ZIL Understanding
96,- SLOG Benchmarking
96,- Virtualize TrueNAS
96,- How to Block Storage
96,- HA Boot Pool
96,- 10 Gbps Primer
96,- 10 Gbps Networking Tuning
96,- Understanding LACP
96,- Supermicro's Fans Fix
96,Quality of Life:
96,- SMART Reporting Script
96,- HDD Spindwon Script
96,- Solnet Array Test Script
96,- Pool Rebalancing Script
96,Show : TrueNAS-13.0-U6.1
96,Motherboard: Supermicro X11SSL-F
96,CPU: Intel Pentium G4560
96,RAM: 2x 16GB Hynix DDR4 2666MHz Unbuffered ECC
96,PSU: LC-Power 560W GOLD
96,Boot: 1x 32 GB MLC USB (TS32GJF780)
96,VMs/Jails: 2x Seagate IronWolf 250 GB SATA SSD (ZA250NM1A002) in a mirror VDEV
96,Storage: 4x Seagate IronWolf 3 TB CMR HDD (ST3000VN007) in 2-way mirror VDEVs
96,Patrick M. Hausen
96,Hall of Famer
96,Joined
96,"Nov 25, 2013"
96,Messages
96,"7,679"
96,"Nov 21, 2023"
96,#46
96,I found that my Cobia system also writes to /dev/sda continuously:
96,To rule out a problem with the reporting - ESXi seems to agree (the system drive is a VMDK):
96,And ... *drumroll* ... I present one possible culprit:
96,Code:root@truenas[/var/log/netdata]# tail -f /var/log/netdata/access.log
96,"2023-11-21 22:36:45: 955648: 3199 '[localhost]:57448' 'DATA' (sent/all = 5561/53715 bytes -90%, prep/sent/total = 0.70/0.51/1.21 ms) 200 '/api/v1/allmetrics?format=json'"
96,2023-11-21 22:36:45: 955649: 3174 '[localhost]:57464' 'CONNECTED'
96,2023-11-21 22:36:45: 955649: 3174 '[localhost]:57464' 'DISCONNECTED'
96,"2023-11-21 22:36:45: 955649: 3174 '[localhost]:57464' 'DATA' (sent/all = 5313/53680 bytes -90%, prep/sent/total = 0.60/0.49/1.10 ms) 200 '/api/v1/allmetrics?format=json'"
96,2023-11-21 22:36:47: 955650: 3200 '[localhost]:57476' 'CONNECTED'
96,2023-11-21 22:36:47: 955650: 3200 '[localhost]:57476' 'DISCONNECTED'
96,"2023-11-21 22:36:47: 955650: 3200 '[localhost]:57476' 'DATA' (sent/all = 5341/53685 bytes -90%, prep/sent/total = 1.49/1.31/2.80 ms) 200 '/api/v1/allmetrics?format=json'"
96,2023-11-21 22:36:47: 955651: 3199 '[localhost]:57482' 'CONNECTED'
96,2023-11-21 22:36:47: 955651: 3199 '[localhost]:57482' 'DISCONNECTED'
96,"2023-11-21 22:36:47: 955651: 3199 '[localhost]:57482' 'DATA' (sent/all = 5313/53678 bytes -90%, prep/sent/total = 0.76/0.48/1.24 ms) 200 '/api/v1/allmetrics?format=json'"
96,[...]
96,What the heck? Why log when the middleware continuously polls netdata? And besides - this does not belong on the boot drive.
96,People who think they know everything are a great annoyance to those of us who do. (Isaac Asimov)
96,Show : Production hypervisor system @work
96,TrueNAS CORE
96,Supermicro 1113S-WN10RT
96,AMD EPYC 7401P - 24 cores
96,256 GB ECC memory
96,2x Toshiba SSD XG5 NVMe 256 GB (boot pool - mirror)
96,3x Solidigm SSD D5-P5430 NVMe 3.84 TB (storage pool - 3-way mirror)
96,"2 identical systems, replicating VMs to each other"
96,Show : Main NAS @home
96,TrueNAS CORE
96,Supermicro 5028D-TN4T barebone
96,Intel Xeon D-1541 - 8 cores
96,64 GB ECC memory
96,2x Transcend SSD TS32GSSD370S 32 GB (boot pool - mirror)
96,1x Supermicro AOC-SLG3-2M NVME card with
96,2x Samsung SSD 970 EVO Plus 1 TB (VM and jail pool - mirror)
96,4x WDC WD40EFRX 4 TB (storage pool - two mirrored pairs)
96,1x Intel MEMPEK1W016GA 16 GB Optane (storage pool - SLOG)
96,1x Noctua NF-A12x25 PWM cooler
96,Show : SCALE system @home
96,TrueNAS SCALE
96,Supermicro X10SDV-4C-TLN4F mainboard
96,Supermicro SCE300 chassis
96,Intel Xeon D-1518 - 4 cores
96,32 GB ECC memory
96,1x Transcend SSD TS32GSSD370S 32 GB (boot pool)
96,1x Supermicro AOC-SLG3-2M NVME card with
96,2x Samsung SSD 970 EVO Plus 250 GB (storage pool - mirror)
96,3x Noctua NF-A4x20 PWM cooler
96,1x Supermicro SNK-C0057A4L active CPU cooler
96,Show : Backup NAS located @work
96,TrueNAS CORE
96,Supermicro A2SDi-8C+-HLN4F mainboard
96,Supermicro SC721TQ-250B chassis
96,Intel Atom C3558 - 4 cores
96,32 GB ECC memory
96,2x SuperMicro SATA DOM SOB20R 32 GB (boot pool - mirror)
96,4x WDC WD80EFBX 8 TB (storage pool - two mirrored pairs)
96,2x Samsung SSD 850 PRO 512 GB (storage pool - mirrored metadata vdev)
96,Reactions:
96,"sfatula, Etorix, joeschmuck and 3 others"
96,Patrick M. Hausen
96,Hall of Famer
96,Joined
96,"Nov 25, 2013"
96,Messages
96,"7,679"
96,"Nov 22, 2023"
96,#47
96,"Rough calculation, overestimating the data from above: .5 MBytes/s amount to 41 GB per day or 15 TB per year. So nothing that will ruin my SSD in the short run, but iX really should re-evaluate"
96,"- the amount of logging, especially in a production release"
96,- the location of the log files
96,People who think they know everything are a great annoyance to those of us who do. (Isaac Asimov)
96,Show : Production hypervisor system @work
96,TrueNAS CORE
96,Supermicro 1113S-WN10RT
96,AMD EPYC 7401P - 24 cores
96,256 GB ECC memory
96,2x Toshiba SSD XG5 NVMe 256 GB (boot pool - mirror)
96,3x Solidigm SSD D5-P5430 NVMe 3.84 TB (storage pool - 3-way mirror)
96,"2 identical systems, replicating VMs to each other"
96,Show : Main NAS @home
96,TrueNAS CORE
96,Supermicro 5028D-TN4T barebone
96,Intel Xeon D-1541 - 8 cores
96,64 GB ECC memory
96,2x Transcend SSD TS32GSSD370S 32 GB (boot pool - mirror)
96,1x Supermicro AOC-SLG3-2M NVME card with
96,2x Samsung SSD 970 EVO Plus 1 TB (VM and jail pool - mirror)
96,4x WDC WD40EFRX 4 TB (storage pool - two mirrored pairs)
96,1x Intel MEMPEK1W016GA 16 GB Optane (storage pool - SLOG)
96,1x Noctua NF-A12x25 PWM cooler
96,Show : SCALE system @home
96,TrueNAS SCALE
96,Supermicro X10SDV-4C-TLN4F mainboard
96,Supermicro SCE300 chassis
96,Intel Xeon D-1518 - 4 cores
96,32 GB ECC memory
96,1x Transcend SSD TS32GSSD370S 32 GB (boot pool)
96,1x Supermicro AOC-SLG3-2M NVME card with
96,2x Samsung SSD 970 EVO Plus 250 GB (storage pool - mirror)
96,3x Noctua NF-A4x20 PWM cooler
96,1x Supermicro SNK-C0057A4L active CPU cooler
96,Show : Backup NAS located @work
96,TrueNAS CORE
96,Supermicro A2SDi-8C+-HLN4F mainboard
96,Supermicro SC721TQ-250B chassis
96,Intel Atom C3558 - 4 cores
96,32 GB ECC memory
96,2x SuperMicro SATA DOM SOB20R 32 GB (boot pool - mirror)
96,4x WDC WD80EFBX 8 TB (storage pool - two mirrored pairs)
96,2x Samsung SSD 850 PRO 512 GB (storage pool - mirrored metadata vdev)
96,Reactions:
96,"ThEnGI, joeschmuck, winnielinnie and 1 other person"
96,sfatula
96,Guru
96,Joined
96,"Jul 5, 2022"
96,Messages
96,596
96,"Nov 22, 2023"
96,#48
96,FIle a ticket for the log spam certainly with netdata. Completely unnecessary logging there.
96,"Truenas Scale 23.10.2, Retired System Admin, Network Engineer, Consultant."
96,"Supermicro X10SRA-F with Intel E5-2698v3, 64GB Ecc Ram. 5xSeagate Exos X18 14TB, 2x120GB SSD boot, 2x500GB Apps/System, 2xSamsung 980 Pro 2TB SSD for VMs on a Jeyi SSD to PCIE card, 2x8TB external USB for rotating backups in offsite bank storage, Eaton 5S1500LCD UPS, Cooler Master 212 Black Edition, Corsair RM750, Fractal Design Meshify S2, IBM 9207-8i"
96,chuck32
96,Patron
96,Joined
96,"Jan 14, 2023"
96,Messages
96,496
96,"Nov 22, 2023"
96,#49
96,Davvo said:
96,"@ThEnGI issue is that his system dataset is not in the boot pool, yet he is experiencing such abnormal writes volume."
96,Click to expand...
96,"It's the same for me, OP reported around 250 Kib written on boot pool, that's the same number I have. Additionally my writes on my VM pool are an order of magnitude higher. I also don't have the systemdataset on the boot pool."
96,"The assigned dev on my ticket said he may have an idea. With @Patrick M. Hausen also reporting issues, I'm confident there will be some update in the future. It's not single users with a misconfiguration at this point I'd say."
96,sfatula said:
96,"You have homeassistant, don't know how long you've used it but it is very chatty and that is normal. I am running it in a VM also via HASSOS. It is forever logging states of different devices in the logger. I log mine to mariadb, a separate app on Scale."
96,Click to expand...
96,HA is what got me into this whole home server mess ;)
96,"I also thought about mariadb, because the db as of now does not seem to be persistent forever. But since I probably don't really need all that history I haven't gotten around to it. So you used a scale app directly to log?"
96,"Nonetheless it would still be on the same pool though, so for this particular problem I probably won't gain anything."
96,Show : Main System
96,TrueNAS-SCALE-23.10.1
96,"Supermicro X10SRi-F, Xeon 2640v4, 128 Gb ECC RAM, Seasonic Focus PX-750"
96,in Fractal Design R5
96,Data pool: 6*4Tb striped mirror + 1 hot spare
96,VM pool: 2*500Gb SSD mirror
96,boot pool: 2*250 Gb SSD mirror
96,UPS: Eaton Eco 1200
96,NIC: Intel X520 10g
96,Show : Replication Target
96,TrueNAS-SCALE-23.10.2
96,"Supermicro X10SLL-F, i3 4130, 16 Gb ECC RAM, Seasonic Prime PX-750"
96,in Fractal Define XL USB 3.0
96,Data pool: 2*8Tb mirror
96,Data pool: 1*8Tb stripe
96,boot pool: 2*128 Gb SSD
96,UPS: Eaton Eco 650
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Nov 22, 2023"
96,#50
96,"I already opened the ticket, it was set as low priority"
96,it must be said that it is not urgent and one's problems always feel like a high priority.
96,"@joeschmuck is investigating the values reported by the script, as they are (perhaps) not real. 500GiB written should not result in a 20% reduction in 128GB SSD life"
96,@chuck32
96,"I didn't notice if the disk where the ""system dataset"" resides has anomalous writes. but since I have on average 3/5 MiB/s of writing (due to docker/kubernet) 250KiB/s makes no difference. the SSD is 2TB (20 15.625 times larger than the boot pool) it can easily support these extra writes"
96,"Last edited: Nov 22, 2023"
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,joeschmuck
96,Old Man
96,Moderator
96,Joined
96,"May 28, 2011"
96,Messages
96,"10,965"
96,"Nov 23, 2023"
96,#51
96,ThEnGI said:
96,"@joeschmuck is investigating the values reported by the script, as they are (perhaps) not real. 500GiB"
96,Click to expand...
96,"The value is correct, I verified it last night, I did confirm 534.67GB was written (had to manually do the math )."
96,However the wear level on your drive is not correct due to some off the wall SMART reporting.
96,"I will chat with you over a PM, not here about the wear level but I do have a customization fix for it."
96,Show : Main System
96,Show : Hardware
96,AMD Ryzen 5 7600X 6-Core Processor (4.7GHz) | ASRock Rack B650D4U | 64GB DDR5 ECC RAM (4800MHz) | Six NVMe 4TB Nextorage SSD NEM-PA4TB drives with attached heatsink (Designed for PS5) | Corsair RM750e Modular Power Supply | ASUS Prime AP201 Case
96,Show : Software
96,"ESXi (free) 8.02 (I know free ESXi is dead now, really sucks) | TrueNAS CORE and/or SCALE (generally the current version) on a 16GB Virtual Drive | 1 CPU Core / 2 Hyperthreads | 16GB RAM | Two 4TB NVMe drives as Boot and Datastore for ESXi | Four 4TB NVMe drives PASSTHRU to TrueNAS VM (~10TB RAIDZ1)"
96,Show : Backup
96,Show : Hardware
96,"Intel E3-1230v5 (3.4GHz) Skylake CPU | Supermicro X11SSM-F | 64 GB Samsung DDR4 ECC 2133 MHz RAM | One IOCREST SI-PEX40062 4 port SATA PCI-E (in pass-thru for NAS Drives) | 256 GB SSD Boot Drive | 1TB Laptop Hard Drive for Datastores | Three HGST HDN726060ALE614 6TB Deskstar NAS Hard Drives and one Seagate 6TB Drive (RAIDZ2, 8.72TB healthy usable space) and one 1TB NVMe (for development purposes) | All wrapped up in a Cooler Master HAF 912 case | CyberPower LX1500GU UPS"
96,Show : Software
96,ESXi (free) 7.0 (updates applied as available) | TrueNAS Core 13.0-U? | 2 CPU cores |
96,16GB RAM | Motherboard NIC for connectivity | 10GB Provisioned boot drive (on SSD) | Three HGST 6TB & One Seagate 6TB drives (RAIDZ2) via IOCREST card
96,Show : Test System
96,ESXi 7.0 (updates applied) or TrueNAS on bare metal | Supermicro A1SAM-F | 16GB RAM | 128GB SSD Boot Drive and datastore | Two 500GB Laptop Hard Drives and periodically other drives for testing purposes.
96,------------------------------
96,TrueNAS NVMe Server Build
96,Hard Drive Troubleshooting Guide
96,Multi-Report Hard Drive/SSD/NVMe Reporting Script
96,RAID Capacity Calculator or Biduleohm RAID Calculator
96,Decode Your S.M.A.R.T. Data
96,FreeNAS Hardware Recommendations by Ericloewe
96,Resources List including Detailed Hardware and System Build Notes
96,sfatula
96,Guru
96,Joined
96,"Jul 5, 2022"
96,Messages
96,596
96,"Nov 23, 2023"
96,#52
96,chuck32 said:
96,HA is what got me into this whole home server mess ;)
96,"I also thought about mariadb, because the db as of now does not seem to be persistent forever. But since I probably don't really need all that history I haven't gotten around to it. So you used a scale app directly to log?"
96,"Nonetheless it would still be on the same pool though, so for this particular problem I probably won't gain anything."
96,Click to expand...
96,"It really is by default. I am using mariadb, I don't use IX or Truecharts apps but the docker containers via what is called custom apps on Cobia. I am not on Cobia yet. I don't like sqlite for the most part and find mariadb better for my purposes which includes several containers on Scale as well as my own creations, and the ease I can access the data from GUIs like Mysql Workbench or DBeaver. Rather have one centralized place to store everything. I have at least a year of HA history logged now, it's actually useful data. It can all be configued in HA."
96,"Truenas Scale 23.10.2, Retired System Admin, Network Engineer, Consultant."
96,"Supermicro X10SRA-F with Intel E5-2698v3, 64GB Ecc Ram. 5xSeagate Exos X18 14TB, 2x120GB SSD boot, 2x500GB Apps/System, 2xSamsung 980 Pro 2TB SSD for VMs on a Jeyi SSD to PCIE card, 2x8TB external USB for rotating backups in offsite bank storage, Eaton 5S1500LCD UPS, Cooler Master 212 Black Edition, Corsair RM750, Fractal Design Meshify S2, IBM 9207-8i"
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Nov 25, 2023"
96,#53
96,OT:
96,"While waiting for a response from IX, I was writing down the next upgrades to do"
96,As regards the PCIe compartment I have two X16 slots (16+4b lines) and one X1 slot available.
96,"I still haven't quite figured out what to do with the X1 slot, maybe a 2.5GBE card?"
96,"Regarding the X16 connector, I was thinking of mounting an LSI 9300-8i (8 Disk) controller in the slot with 4 lines, is the bandwidth sufficient?"
96,At that point I am left with an X16 slot in which to install a 4xNvme adapter. thus obtaining 10HDD + 6 NVME. But with 2.5GB connectivity.
96,or sacrifice the NVME disks and install a 10GBE NIC
96,what is the best solution?
96,"EDIT:I would like to avoid ""link aggregation"" to keep the LAN simple"
96,/OT
96,"Last edited: Nov 25, 2023"
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Dec 2, 2023"
96,#54
96,OT (again)
96,"I'm getting ""High"" (average below 10%) IO wait,But I didn't understand exactly what it represents...."
96,"If I understand correctly, it's the pools that are ""slowing down"" the system, correct?"
96,"And how is the ""system load average"" value measured in reporting?"
96,"From the dashboard I have a value that varies between 1/5/10%, in the report around 1.2 (Processes ?)"
96,"I have 6 active Dockers, so I'm not surprised by a bit of load on the CPU"
96,END OT
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,chuck32
96,Patron
96,Joined
96,"Jan 14, 2023"
96,Messages
96,496
96,"Dec 2, 2023"
96,#55
96,ThEnGI said:
96,"I still haven't quite figured out what to do with the X1 slot, maybe a 2.5GBE card?"
96,"Regarding the X16 connector, I was thinking of mounting an LSI 9300-8i (8 Disk) controller in the slot with 4 lines, is the bandwidth sufficient?"
96,At that point I am left with an X16 slot in which to install a 4xNvme adapter. thus obtaining 10HDD + 6 NVME. But with 2.5GB connectivity.
96,or sacrifice the NVME disks and install a 10GBE NIC
96,Click to expand...
96,Maybe post in the appropriate subforum to get proper attention.
96,"Regarding the bandwidth, it's a x8 card in a x16 slot, why wouldn't it be sufficient?"
96,"From what I read you should stay away from 2.5 GB, either go or stay at one. The performance/hardware for 2.5Gb is subpar (further reading)."
96,"Even with HDDs (depending on your pool layout) you can easily max out 2.5 Gbe speeds, so 10 Gbe wouldn't be a waste."
96,I'd probably sacrifice the NVME disks for 10Gbe. Aren't there 16 port HBAs anyway? You could use 2.5 SSDs then.
96,Show : Main System
96,TrueNAS-SCALE-23.10.1
96,"Supermicro X10SRi-F, Xeon 2640v4, 128 Gb ECC RAM, Seasonic Focus PX-750"
96,in Fractal Design R5
96,Data pool: 6*4Tb striped mirror + 1 hot spare
96,VM pool: 2*500Gb SSD mirror
96,boot pool: 2*250 Gb SSD mirror
96,UPS: Eaton Eco 1200
96,NIC: Intel X520 10g
96,Show : Replication Target
96,TrueNAS-SCALE-23.10.2
96,"Supermicro X10SLL-F, i3 4130, 16 Gb ECC RAM, Seasonic Prime PX-750"
96,in Fractal Define XL USB 3.0
96,Data pool: 2*8Tb mirror
96,Data pool: 1*8Tb stripe
96,boot pool: 2*128 Gb SSD
96,UPS: Eaton Eco 650
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Dec 2, 2023"
96,#56
96,ok for the 2.5Gbe
96,The idea is:
96,"1 x16 PCIE 3.0 (x16 Line), 1 x 8 Slot HBA"
96,"1 x16 PCIE 3.0 (x4 Line), 1 x 10Gbe NIC (X20-DA1)"
96,"1 X1 PCIE 3.0 (x1 Line), 1 x 1 NVME (adapter) reduced speed"
96,"in my case there are only 10 of 3.5 and 2 of 2.5. The two 2.5 are the boot disks. An 8 slot HBA is enough to cover all disks (the MB has 6 SATA ports). maybe i will use 2.5to3.5 adapter and sata SSD,which is not a bad idea"
96,"I'm writing here because it's not urgent and I needed to get the post up, at the moment priority is UPS, second HDD and second NVME"
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Dec 11, 2023"
96,#57
96,They closed the ticket reporting that the swap is on the boot disk
96,How did it end up there? How can I move it?
96,I looked in the GUI but it only lets me change the size
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,chuck32
96,Patron
96,Joined
96,"Jan 14, 2023"
96,Messages
96,496
96,"Dec 11, 2023"
96,#58
96,"They closed my ticket for the same reason. I have 128 Gb of memory (with mostly 30 Gb free, depending on which VMs are spun up), so I don't really see a reason to use swap at all."
96,ThEnGI said:
96,How did it end up there?
96,Click to expand...
96,Probably said yes upon installation to creating a swap partition. Iirc I ended up with swap on an earlier installation (bluefin) even when I said no.
96,ThEnGI said:
96,I looked in the GUI but it only lets me change the size
96,Click to expand...
96,Did you try setting it to 0?
96,I'm thinking about reinstalling when I swap out my PSU / want to install the next cobia update if it cannot be removed as is. This thread however suggests I may get away with replacing the drives with themselves since I mirrored my boot pool.
96,Show : Main System
96,TrueNAS-SCALE-23.10.1
96,"Supermicro X10SRi-F, Xeon 2640v4, 128 Gb ECC RAM, Seasonic Focus PX-750"
96,in Fractal Design R5
96,Data pool: 6*4Tb striped mirror + 1 hot spare
96,VM pool: 2*500Gb SSD mirror
96,boot pool: 2*250 Gb SSD mirror
96,UPS: Eaton Eco 1200
96,NIC: Intel X520 10g
96,Show : Replication Target
96,TrueNAS-SCALE-23.10.2
96,"Supermicro X10SLL-F, i3 4130, 16 Gb ECC RAM, Seasonic Prime PX-750"
96,in Fractal Define XL USB 3.0
96,Data pool: 2*8Tb mirror
96,Data pool: 1*8Tb stripe
96,boot pool: 2*128 Gb SSD
96,UPS: Eaton Eco 650
96,NugentS
96,MVP
96,Joined
96,"Apr 16, 2020"
96,Messages
96,"2,942"
96,"Dec 11, 2023"
96,#59
96,Just for context - my boot-pool contains the system dataset and is being written to as an average of 317.32KiB over whatever time period is involved.
96,Its continuous
96,Show : Primary TrueNAS
96,"Supermicro X10DRH-CLN4, 256GB ECC Memory, 2 * E5-2667 V3 in 24 Bay Rack Mount 4U Case"
96,Chelsio T520 CR Dual SFP+ NIC using fibre to the switch
96,LSI 9305-16i IT Mode SAS HBA
96,SAS3008 on board in IT Mode
96,TrueNAS-SCALE (Bluefin)
96,Boot from mirrored 100GB Intel DC3710
96,BigPool: 8 * Seagate Exos 12TB HDD in 4 * mirrored pairs + 2 18TB Toshiba MG08 + Optane 900P as SLOG + Mirrored Special (800GB Intel DC S3610)
96,SSDPool: 6 * DC S3610 1.6TB SSD in 3 * mirrored pairs + Optane 900P as SLOG
96,AppPool 2 * 512GB Intel DC S3610 as mirrored pair
96,ScratchSSD 1 * High Endurance SSD for temp files for some applications. No write amplification and I don't care about the data
96,"Use Case: VM Storage, Media Storage and General File Storage"
96,Power: Not measured yet
96,Show : HairyNAS
96,Truenas-Scale (Bluefin)
96,"Supermicro A2SDi-H-TF, 64GB ECC RAM in Fractal Design R5"
96,Intel DC3700 100Gb as boot disk
96,Apps: 2 * Samsung SM863 960GB in Mirror
96,Tank: 7 * Tosh MG09 18TB in Z2 + Intel M10 64GB Optane as L2ARC (Metadata only)
96,UseCase: Backup via Replication of PrimaryNAS
96,Power: 100W
96,Show : BackupNAS
96,Synology 1517+ running DSM
96,5 * 18TB HDD Toshiba MS09 RAID 6
96,"Why - Cos Synology's Backup for Business can backup PC, Servers, SMB Shares, O365 and VM's. And its free (with the overpriced Synology)"
96,Usecase: Second backup target (File by File) & Docker
96,Power: 69W
96,Show : JIRA suggestions I would like voted for
96,"1. https://ixsystems.atlassian.net/browse/NAS-126075 - Have a toggle for the K3S Service, like FTP, iSCSI, NFS etc"
96,2. https://ixsystems.atlassian.net/browse/NAS-125176 - Snapshot as part of OS upgrade
96,"3. https://ixsystems.atlassian.net/browse/NAS-122581 - Consistent disk labels (sda, sdb etc) for use by the API or reporting"
96,4. https://ixsystems.atlassian.net/browse/NAS-121499 - Backup (& Restore) of Container Configuration
96,5. https://ixsystems.atlassian.net/browse/NAS-119779 - Update version of rclone to 1.6 (or better)
96,6. https://ixsystems.atlassian.net/browse/NAS-117030 - Replace the word cache in vdev creation and use L2ARC instead
96,7. https://ixsystems.atlassian.net/browse/NAS-125962 - Change Extend to Attach a Mirror
96,ThEnGI
96,Contributor
96,Joined
96,"Oct 14, 2023"
96,Messages
96,137
96,"Dec 13, 2023"
96,#60
96,To remove the swap from the boot drive need i to reinstall TN ?
96,Show : Home NAS
96,Truenas SCALE 23.10.0.1
96,CPU I3 10105T@3.00Ghz
96,MB TUF Gaming B460M
96,RAM 80GB (2x32+2x8GB) DDR4 NO-ECC
96,Boot drive 1 SSD 128GB
96,Storage 1 NVME 2TB + 1 IRONWOLF PRO 12TB
96,PSU 550W Gold
96,Pool Storage : 1vDev 1 HDD
96,Pool Fast: 1vDev 1 NVME
96,Prev
96,Next
96,First
96,Prev
96,3 of 4
96,Go to page
96,Next
96,Last
96,You must log in or register to reply here.
96,Similar threads
96,16x HDDs - Help choosing the right config
96,stratsan
96,"Dec 18, 2023"
96,Operation and Performance
96,Replies
96,Views
96,241
96,"Dec 18, 2023"
96,stratsan
96,SOLVED
96,Trouble replacing a disk in degraded boot-pool
96,rsolva
96,"Aug 4, 2023"
96,Operation and Performance
96,Replies
96,Views
96,504
96,"Aug 4, 2023"
96,rsolva
96,Quota exceeded on dataset boot-pool/.system/cores.
96,Benji99
96,"Feb 8, 2023"
96,Operation and Performance
96,Replies
96,Views
96,815
96,"Feb 8, 2023"
96,Samuel Tai
96,Need Pool Setup Advice For 3x NVME M2 Drives
96,tsm37
96,"Feb 19, 2023"
96,Operation and Performance
96,Replies
96,Views
96,"Feb 21, 2023"
96,tsm37
96,SSD choice for TrueNas Scale boot pool
96,fornex
96,"Jul 17, 2023"
96,Operation and Performance
96,Replies
96,Views
96,712
96,"Sep 6, 2023"
96,fornex
96,Share:
96,Facebook
96,Twitter
96,Reddit
96,Pinterest
96,Tumblr
96,WhatsApp
96,Email
96,Share
96,Link
96,Forums
96,TrueNAS
96,TrueNAS SCALE
96,Operation and Performance
96,TrueNAS Light Theme
96,Privacy policy
96,Help
96,Home
96,RSS
96,"This site uses cookies to help personalise content, tailor your experience and to keep you logged in if you register."
96,"By continuing to use this site, you are consenting to our use of cookies."
96,Accept
96,Learn more…
96,Top
97,Maria Colgan
97,Skip to content
97,"Oracle Database Product Manager with a passion for SQL, the Optimizer and performance."
97,Menu
97,About Maria
97,Author: Maria Colgan
97,How to identify which indexes can be safely dropped in Oracle
97,"The longer an application has been successfully run, the more likely you are to have indexes that are no longer used or beneficial. Removing these indexes not only saves space but can also improve the performance of any DML operations."
97,But knowing which indexes can be safely dropped can be tricky.
97,"In Oracle Database 12c, things got a little easier with the introduction of a new view called DBA_INDEX_USAGE."
97,"The DBA_INDEX_USAGE view displays cumulative index usage statistics, but unlike previous releases, it’s not just a binary value (YES or NO). This new view shows how often an index was used, who accessed it, and how effective it was via histogram based on the number of rows returned. The index access information is captured in memory and periodically flushed to disk every 15 minutes. You can query the last_flush_Time in v$INDEX_USAGE_INFO to determine when it was updated."
97,"You should also be aware that index usage is sampled by default rather than tracked by all index usage. The sampling approach will likely notice all index usage in a standard running system, where indexes are repeatedly accessed, but it can easily miss indexes used for one-off tests. You can change the tracking approach so that we track every execution, but there is a significant performance overhead for this."
97,"As always, it is easier to understand the benefits of a dictionary view by looking at an example. In my example below, I join the DBA_INDEX_USAGE view to USER_INDEXES via an outer join to ensure I capture information on indexes that are never used because these indexes won’t appear in DBA_INDEX_USAGE at all."
97,"SELECT i.index_name, u.total_access_count tot_access, u.total_exec_count exec_cnt,"
97,"u.bucket_0_access_count B0, u.bucket_1_access_count B1, u.bucket_2_10_access_count B2_10,"
97,"u.bucket_11_100_access_count B11_100, u.bucket_101_1000_access_count B101_1K,"
97,"u.bucket_1000_plus_access_count B1K, u.last_used"
97,FROM
97,DBA_INDEX_USAGE u
97,RIGHT JOIN DBA_INDEXES i
97,i.index_name = u.name
97,WHERE  i.owner='MARIA'
97,ORDER BY u.total_access_count;
97,The output of this query should look something like this.
97,INDEX_NAME     	TOT_ACCESS
97,EXEC_CNT   B0      B1      B2_10  B11_100   B101_1K
97,B1K
97,LAST_USED
97,---------------- ---------- ---------- ------- ------- ------- -------- ---------- ---------- -----------
97,PROD_CUST_SALES       1  	1
97,0       0
97,1 	06-APR-23
97,INDX_LOC
97,2  	2
97,0       1
97,0		0
97,0 	12-APR-23
97,INDX_DEPTNO
97,0 	26-APR-23
97,PROD_SUP_INDX
97,25       0
97,0 	26-APR-23
97,EMPNO_PK_IND
97,48       32
97,0 	26-APR-23
97,CHAN_SOLD
97,PROD_SUB_IDX
97,"As you can see in the output above, for each index, we get the total number of accesses (TOT_ACCESS), the execution count (EXEC_CNT), the last date the index was used (LAST_USED), and a histogram of how many rows were returned across several buckets."
97,"Let’s look at the  EMPNO_PK INDEX  index (the third row from the bottom). This index returned 0 rows 48 times out of  82 total accesses and a single row 32 times. So, it looks like this index is commonly used and valuable."
97,"However, if we look at PROD_CUST_SALES (the first row), it was accessed only once and returned over 1,000 rows. Is that a helpful index?"
97,"Maybe/maybe not. It may be beneficial if this index access replaces a full table scan of an extremely wide table. If, on the other hand, it’s only marginally cheaper than a full table scan, it may not be worth the overhead of maintaining the index."
97,"But perhaps the most exciting part of the query output above is that we have two indexes (CHAN_SOLD, PROD_SUB_IDX) that have never been accessed. These are good candidates to be dropped."
97,But dropping an index can be risky. What can you do to ensure you don’t get fired for dropping the wrong index?
97,"Rather than dropping the index immediately, marking the index invisible is far easier. The optimizer will be unaware of the index in the future and won’t select it as part of any execution plan. However, the index will continue to be maintained."
97,ALTER INDEX prod_sub_idx INVISIBLE;
97,"If no one complains about their query performance digressing after some time, you can safely drop the index. If, on the other hand, someone does complain, you can alter the index visible again in a matter of seconds."
97,ALTER INDEX prod_sub_idx VISIBLE;
97,You can also create a new index as invisible. The optimizer is unaware of the new index until you can verify it improves performance by setting the parameter OPTIMIZER_USE_INVISBLE_INDEXES within a single session. This will allow the optimizer to consider the new index only for SQL statements issued within your session. All other sessions will continue to use the existing execution plans.
97,-- New indexes can be marked invisible until you have an opportunity to prove they improve performance
97,"CREATE INDEX my_idx ON t(x, object_id) INVISIBLE;"
97,-- Test newly created invisible indexes by setting OPTIMIZER_USE_INVISBLE_INDEXES to TRUE
97,ALTER SESSION SET optimizer_use_invisible_indexes
97,= TRUE;
97,"Update on March 15th, 2024"
97,There have been several comments on this post around what each of the columns in the DBA_INDEX_USAGE view actual record and Jonathan Lewis has written a great explanation of each column in the view in the first part of his blog series on Index Usage.
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on November 28, 2023March 15, 2024Categories Optimizer, Oracle Database 19c, Top_TipTags 19c, index, Indexing, Oracle Database 19c10 Comments on How to identify which indexes can be safely dropped in Oracle"
97,Better diagnostics for SQL regressions in 19c and beyond #JoelKallmanDay
97,"When diagnosing and correcting a performance regression for a SQL statement, it can often be challenging to find out what is happening during the execution and why your corrective measures are not working."
97,"In today’s blog, I want to share several enhancements introduced in recent Oracle Database releases to help you better understand how to improve a SQL statement and understand why your corrective measures aren’t working as you hoped."
97,Enhanced Execution Plan
97,"One of the most valuable tools at our disposal is the execution plan. Over the last several years, the Oracle Optimizer team has made a considerable effort to share even more insights and guidance on what might be impacting the performance of a SQL statement via the execution plan."
97,The Note section under the plan
97,"The note section under the execution plan contains valuable information on what has happened during parse and execution. Over the last several releases, you will find a lot more information appearing in the note section, including details on the following features:"
97,Dynamic Sampling – indicates a non-default value for the parameter OPTIMIZER_DYNAMIC_SAMPLING or that one or more objects referenced in the query are missing statistics.
97,"Plan Stability Features – indicates that a store outline, SQL profile, SQL patch SQL directives, or SQL plan baseline influenced the plan chosen."
97,Adaptive plan – indicates that the plan shown through the Explain Plan command may be different from the plan used during execution as the Optimizer will use run-time statistics to make the final decision on what join types and aggregation methods to use. It is best to view the actual plan used via v$SQL_PLAN after the initial execution.
97,"Statistics Feedback – indicates the plan was re-evaluated based on previous execution statistics, and a new plan was found."
97,Hint Usage Reporting in Oracle Database 19c
97,"Starting in 19c, you will see a new hint usage report under the execution plan. The new report gives you a better understanding of what happened to each hint in a SQL statement."
97,"Remember, the database doesn’t issue an error if a hint is invalid or not applicable for a SQL statement. The database will treat the hint as a comment, effectively ignoring it. Previously, the only way to find this information was via a 10053 trace file, and even then, it wasn’t very detailed."
97,The new hint report shows which hints were used and which were ignored and explains why hints weren’t used. The most common reasons for a hint not to be used are:
97,Syntax errors – the hint contains a typo or an invalid argument.
97,"Unresolved hints – the hint contains an invalid argument or is not applicable to the SQL statement. For example, you request an index be used, but no index exists on the table."
97,"Unused hints – the hint can’t be used in this specific scenario. For example, you requested a Hash Join for a non-equality join condition."
97,Conflicting hints – multiple hints provide conflicting directives.
97,"In the example below, the hint NO_QURY_TRANSFORMATION was reported to contain a syntax error. The word query is misspelled in the hint, so the hint can’t be used."
97,---------------------------------------------------------------------------
97,| Id  | Operation                      | Name     | Rows  | Bytes | Cost (%CPU)|
97,--------------------------------------------------------------------------------
97,|   0 | SELECT STATEMENT               |          |       |       |    47 (100)|
97,|   1 |  HASH GROUP BY                 |          |   269 | 37929 |    47   (7)|
97,|   2 |   HASH JOIN                    |          |   269 | 37929 |    46   (5)|
97,|   3 |     TABLE ACCESS STORAGE FULL
97,| SALES    | 10000 | 90000 |     5   (0)|
97,|*  4 |     TABLE ACCESS STORAGE FULL  | PRODUCTS | 43108 |  4841K|    40   (3)|
97,--------------------------------------------------------------------------------
97,Predicate Information (identified by operation id):
97,---------------------------------------------------
97,"4 - access(""ITEM_1""=""P"".""PROD_ID"")"
97,Hint Report (identified by operation id / Query Block Name / Object Alias):
97,Total hints for statement: 1 (E - Syntax error (1))
97,--------------------------------------------------------------------------
97,0 -  SEL$1
97,E -  NO_QURY_TRANSFORMATION
97,Note
97,-----
97,- dynamic statistics used: dynamic sampling (level=2)
97,"In this second example, I provided two hints on how to access the employees table. One hint requested that the primary key index be used, and the other requested that the access leverage parallel execution."
97,"SELECT /*+ index(e empno_pk_ind) parallel(e 8) */ e.empno, ename"
97,FROM
97,employees e
97,WHERE
97,e.empno < 7700;
97,----------------------------------------------------------------------------
97,|Id
97,| Operation
97,| Name
97,| Rows | Bytes |
97,----------------------------------------------------------------------------
97,0 | SELECT STATEMENT
97,1 |
97,TABLE ACCESS BY INDEX ROWID BATCHED | EMPLOYEES
97,|	 8 |	80 |
97,2 |
97,INDEX RANGE SCAN
97,| EMPNO_PK_IND |	 8 |
97,----------------------------------------------------------------------------
97,Hint Report (identified by operation id / Query Block Name / Object Alias):
97,Total hints for statement: 1 (U - Unused (1))
97,---------------------------------------------------------------------------
97,U -
97,parallel(e 8)
97,"The parallel hint is not used, as I’ve supplied an invalid combination of hints. An index range scan can’t be parallelized unless the index is partitioned, which is not true in this example. Therefore, the Optimizer can not honor both hints."
97,"Note Nigel Bayliss, the Optimizer product manager, has also blogged about Optimizer hint reporting and has shared details on some limitations regarding execution hints such as GATHER_OPTIMIZER_STATISTICS, APPEND, etc."
97,SQL Analysis Report in Oracle Database 23c
97,"In Oracle Database 23c, the execution plan got another new section: a SQL Analysis Report. This handy addition helps you diagnose common problems that can cause suboptimal execution plans. For example, the new report will point out situations where you are:"
97,Missing join conditions
97,Have a WHERE clause predicate that prevents an index from being used
97,Have a datatype mismatch in a WHERE clause predicate
97,Using a UNION instead of a UNION ALL
97,EXPLAIN PLAN FOR
97,SELECT * FROM addresses
97,WHERE UPPER(state) = 'CA';
97,SELECT * FROM dbms_xplan.display();
97,Plan hash value: 3184888728
97,--------------------------------------------------------------------
97,| Id  | Operation         | Name      | Rows  | Bytes | Cost (%CPU)|
97,--------------------------------------------------------------------
97,|   0 | SELECT STATEMENT  |           |   239 | 13384 |    61   (0)|
97,|*  1 |  TABLE ACCESS FULL| ADDRESSES |   239 | 13384 |    61   (0)|
97,--------------------------------------------------------------------
97,Predicate Information (identified by operation id):
97,---------------------------------------------------
97,"1 - filter(UPPER(""state"")='CA')"
97,SQL Analysis Report (identified by operation id/Query Block Name/Object Alias):
97,-----------------------------------------------------------------
97,"1 -  SEL$1 / ""ADDRESSES""@""SEL$1"""
97,"-  The following columns have predicates which preclude their use as keys in index range scan. Consider rewriting the predicates. ""STATE"""
97,"Again, Nigel Bayliss has blogged about this in more detail on the official Optimizer blog."
97,Note that SQL Monitor active reports also contain SQL Analysis reports. You can find them under the new  SQL Analysis tab.
97,New Optimizer Dictionary Views
97,"Along with enhancements to the execution plan in 23c, we added new data dictionary views to help identify what happens when we parse and execute a SQL statement."
97,"Apps and users frequently change the value for one or more parameters that impact the Optimizer at a session level. As a DBA or performance engineer, you are often unaware of these logon triggers or ALTER SESSION commands. The first hint that the environment may have changed is when you see a plan change."
97,"To check if a change in the optimizer environment caused the plan change, you can check the column OPTIMIZER_ENV_HASH_VALUE in V$SQL, V$SQLAREA, and DBA_HIST_SQLSTAT. However, this column doesn’t tell you what has changed."
97,"Starting in 23c, you can query a new dictionary view, DBA_HIST_OPTIMIZER_ENV_DETAILS, to find out exactly what has changed in the optimizer environment when we parsed and executed a SQL statement."
97,"Using the optimizer_env_hash_value for the original and new plan, you can query DBA_HIST_OPTIMIZER_ENV_DETAILS and get a list of the parameters settings for each scenario, which you can then compare to find the difference."
97,SELECT DISTINCT optimizer_env_hash_value
97,FROM
97,dba_hist_sqlstat
97,WHERE sql_id = 'bsvavk15n7cra'
97,ORDER BY 1;
97,OPTIMIZER_ENV_HASH_VALUE
97,------------------------
97,1309615723
97,2369923737
97,"SELECT name, value"
97,FROM dba_hist_optimizer_env_details
97,WHERE optimizer_env_hash_value = 1309615723
97,ORDER BY 1;
97,NAME
97,VALUE
97,-------------------------------------------------- -------------------------
97,BlockChain_ledger_infrastructure
97,CLI_internal_cursor
97,PMO_altidx_rebuild
97,_adaptive_window_consolidator_enabled
97,true
97,_add_stale_mv_to_dependency_list
97,true
97,You can get more information on how to leverage the DBA_HIST_OPTIMIZER_ENV_DETAILS view in MOS NOTE:2953121.1 – Examining the Optimizer Environment within Which a SQL Statement was Parsed in AWR.
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on October 11, 2023Categories Optimizer, Oracle Database 19c, Oracle Database 21c, Oracle Database 23c, SQL Tuning, Top_TipTags 19c, Explain Plan, Hints, optimizer, Oracle Database 23c, SQL, SQL Analysis Report1 Comment on Better diagnostics for SQL regressions in 19c and beyond #JoelKallmanDay"
97,What to expect from Oracle DatabaseWorld at CloudWorld
97,"There is only one day to go until Oracle CloudWorld in Las Vegas, the largest gathering of Oracle customers, partners, developers, and technology enthusiasts of the year!"
97,"Of course, the database team will be there, and this year, we will have our conference within a conference called DatabaseWorld. You will have multiple opportunities to meet with us in one of our technical sessions, customer sessions, or hands-on labs."
97,"Plus, our very own Juan Loaiza will take to the main stage to deliver the database and developer keynote (The Future of Data and AppDev) on Wednesday at 2 p.m."
97,"With hundreds of in-depth learning sessions across nine different tracks, no matter your role or where you plan to run your Oracle database (in the public cloud, on-premises, hybrid cloud, etc.), we will have all the product updates, technology deep-dives, and best practices sessions you need."
97,Must see DatabaseWorld Sessions
97,"I plan to spend most of my time in the database and developer tracks and to help you plan your schedule, I’ve listed some of the sessions I plan to attend this week below. Remember, you can keep track of your schedule in the Oracle Events App."
97,"Tuesday, Sept. 19th"
97,8:30 am LRN1030 Best Practices for Upgrade  to Oracle Database 23c with Mike Dietrich
97,11:30 am LRN2972 The Best New Feature in 23c: JSON Relational Duality with Juan Loaiza Tirthankar Lahiri and Beda Hammerschmidt
97,12:45 pm LRN3248 Learn How Oracle Autonomous Database Helps DBAs Sleep Better at Night with Can Tuzla and Nilay Panchal
97,02:00 pm LRN4218 Oracle Database 23c: Data Discoverability with Tirthankar Lahiri and Beda Hammerschmidt
97,05:00 pm SOL2364 Oracle Database Directions with Andy Mendelsohn
97,"Wednesday, Sept. 20th"
97,"9:45 am LRN1034 Oracle Database 23c: What’s New, What’s Next with Jenny Tsai and Dom Giles"
97,11:00 am PAN4206 Oracle Database 23c—a Customer’s Perspective hosted by Dom Giles and me.
97,1:00 pm LRN3520 Generative AI and Oracle Autonomous Database with Kumar Rajamani and Mark Hornick
97,2:00 pm KEY4334 The Future of Data and App Dev with Juan Loaiza
97,4:00 pm LRN3515 Proven Strategies for Maximizing Converged Database Performance With Michelle Malcher and David Start
97,5:15 pm LRN2724 Exadata Exascale: Next Generation Architecture with Juan Loaiza and Kodi Umamageswaran
97,"Thursday, Sept. 21st"
97,08:30 am HOL2253 Can You MATCH That? Property Graph Queries in SQL with Meli Annamalai
97,12:30 pm LRN2367 What’s New in the Optimizer for Oracle Database 23c? with Nigel Bayliss
97,1:45 pm LRN3608 Migrate to Oracle Autonomous Database with the Oracle Database Estate Explorer with Simon Griffiths
97,"I hope to see you there, but if you can’t be there in person, you can catch the main stage keynotes and other sessions online via the Free-pass."
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on September 17, 2023November 17, 2023Categories UncategorizedTags CloudWorld, DatabaseWorldLeave a comment on What to expect from Oracle DatabaseWorld at CloudWorld"
97,How to find differences between Oracle production and test envs
97,Determining why things behave differently in production versus a test environment can be time-consuming and painful.
97,That is until you realize there is a convenient package that allows you to compare the two environments quickly.
97,"Introduced in Oracle Database 11g, the DBMS_COMPARISON package allows you to compare objects, schemas, or data between two databases or schemas (within the same database) to find where they differ."
97,Let’s take a closer look at how this works.
97,Create the Comparison
97,"First, you must create a comparison using the DBMS_COMPARISON.CREATE_COMPARISON procedure. You will need to supply a unique name for the Comparison, the schema name, and, optionally, the object name in the current database you wish to compare. You must also supply the location of the other schema you want to compare. The comparison schema can be in the same database (specify NULL for DBLINK_NAME). Still, in my case, below, I’m comparing our product system to our test system, which is reachable from production via a database link called ‘orcl2_test’."
97,BEGIN
97,dbms_comparison.create_comparison(
97,"comparison_name => 'COMP_SALES',"
97,"schema_name     => 'SH',"
97,"object_name     => 'SALES',"
97,dblink_name     => 'orcl2_test'
97,END;
97,PL/SQL PROCEDURE successfully completed.
97,"Note: You need a unique index on both tables to compare tables, as shown above."
97,Run the Comparison
97,"Now we have created the Comparison; we can execute it using the DBMS_COMPARISON.COMPARE function. The COMPARE function returns a BOOLEAN, indicating whether the objects are consistent. It returns TRUE when no differences are found and FALSE when discrepancies are found."
97,"However, the details of the differences are not returned by the function. Instead, the differences are stored in dictionary tables; user_comparison_columns, user_comparison_row_dif, and user_comparison_scan. If a discrepancy is found, you can find details on the differences by querying these tables using the scan ID, which is returned in the variable scan_info."
97,"How you call the COMPARE function is a little more complex than a standard PL/SQL function call. I’ve wrapped the function call in an IF NOT statement in the example below. If the DBMS_COMPARISON.COMPARE function returns FALSE (IF NOT FALSE = TRUE), the function will print out the SCAN ID, so we can use it to query the dictionary table. If, on the other hand, the function returns TRUE (IF NOT TRUE = FALSE), it will return nothing."
97,DECLARE
97,scan_info DBMS_COMPARISON.COMPARISON_TYPE;
97,BEGIN
97,IF NOT DBMS_COMPARISON.COMPARE
97,( comparison_name => 'COMP_SALES'
97,", scan_info"
97,=> scan_info
97,", perform_row_dif => TRUE"
97,) THEN
97,DBMS_OUTPUT.PUT_LINE('Scan ID:'||scan_info.scan_id);
97,END IF;
97,END;
97,Scan ID:3
97,Find the details on the Comparison
97,"As you can see, there must be a discrepancy between our production and test environments, as the COMPARE function returned a SCAN ID of 3. We need to query the dictionary tables using the comparison name and the scan id to determine the difference. Below is the query we need to use. You will notice I’ve included two case statements to indicate if the difference was found on the local database (in this case, production) or on the Remote database (in this case, our test environment)."
97,"SELECT c.column_name, r.index_value,"
97,CASE WHEN r.local_rowid IS NULL
97,THEN 'No' ELSE 'Yes' END
97,"LOC,"
97,CASE WHEN r.remote_rowid IS NULL THEN 'No' ELSE 'Yes' END REM
97,FROM
97,"user_comparison_columns c, user_comparison_row_dif r, user_comparison_scan s"
97,WHERE
97,c.comparison_name = 'COMP_SALES'
97,AND
97,r.scan_id
97,= s.scan_id
97,AND
97,r.status
97,= 'DIF'
97,AND
97,c.index_column
97,= 'Y'
97,AND
97,c.comparison_name = r.comparison_name
97,AND
97,s.scan_id
97,= 3
97,ORDER BY r.index_value;
97,COLUMN_NAME
97,INDEX_VALUE
97,LOC
97,REM
97,______________ ______________ ______ ______
97,5000
97,Yes
97,Yes
97,"The result of our query shows a difference in the data that occurs in the row with ID 5000. However, it doesn’t show us which column has the problem. I must extract the row with ID=5000 from both systems and manually compare them."
97,SELECT * FROM sales WHERE id=5000;
97,ORDER_ID
97,CUST_ID
97,PRODUCT_ID
97,SUPPLIER_ID
97,DATE_ID
97,AMOUNT_SOLD
97,PRICE
97,TAX_CODE
97,____________ ___________ _____________ ______________ ____________ ______________ ___________ __________ ___________
97,248173057
97,25162649
97,610090
97,1229054
97,18-JAN-23
97,140
97,5000
97,SELECT * FROM
97,sales@orcl2_test WHERE id=5000;
97,ORDER_ID
97,CUST_ID
97,PRODUCT_ID
97,SUPPLIER_ID
97,DATE_ID
97,AMOUNT_SOLD
97,PRICE
97,TAX_CODE
97,____________ ___________ _____________ ______________ ____________ ______________ ___________ __________ ___________
97,248173057
97,25162649
97,610090
97,1229054
97,18-JAN-23
97,140
97,xx.xx
97,5000
97,"As you can see from the output above, the difference is in the TAX_CODE column. The TAX_CODE has been masked in the test environment for this particular row."
97,"You may be wondering why Oracle couldn’t show me the exact difference between the two tables. If I had a unique index on the TAX_CODE column, Oracle could have told me the value that differed. But since this is not a unique column, Oracle can only pinpoint the row for me using the unique index on the ID column."
97,This blog post is part of a series on useful PL/SQL packages. Other blogs in this series include:
97,How to add a SLEEP COMMAND to your Oracle PL/SQL code.
97,How diff optimizer statistics
97,How to determine which view to use
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on July 19, 2023July 21, 2023Categories Oracle Database 18c, Oracle Database 19c, Use PL/SQL PackagesTags 11g, Useful PL/SQL2 Comments on How to find differences between Oracle production and test envs"
97,How to determine if you are getting all the benefits of Exadata via AWR
97,"Last week Juan Loaiza introduced the latest generation of Oracle Exadata, X10M, , and with each new release comes more powerful compute power and larger flash and disk capacity. Along with all of the hardware improvements come a bunch of software enhancements that transparently accelerate your database workloads (RDMA, Smart Scan, Storage Indexes, Smart Flash Cache, etc.)."
97,But how do you know if you are benefiting from these accelerators?
97,"The easiest way to determine how your databases on Exadata are performing is via an Automatic Workload Repository (AWR) report, and two of my favorite Oracle experts have created step-by-step guides to help you do just that."
97,Cecilia Grant has written a fantastic white paper on Using AWR reports on Exadata. It provides a step-by-step guide to the Exadata performance statistics found in an AWR report and shares common challenges you may encounter and how to resolve them.
97,"For those less familiar with Exadata, Kodi Umamageswaran(SVP of Exadata development) gave an excellent introductory talk at last year’s Oracle Database World called Transparent Performance with Exadata: What, When, How, and Why. In the session recording below, Kodi does a great job of introducing the capabilities of Exadata and how to identify those benefits using AWR to determine if you are getting all of the performance-enhancing benefits you should be."
97,Happy performance tuning!
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on June 26, 2023June 27, 2023Categories Beginners, ExadataTags AWR, ExadataLeave a comment on How to determine if you are getting all the benefits of Exadata via AWR"
97,How to determine which view is the right view to use in Oracle?
97,"Database views are a handy tool to help obfuscate complex joins from developers and analysts. However, knowing which view to use can be tricky, especially when faced with multiple views with similar-sounding names. After all, you don’t want to join various tables only to discover that all the information you needed was in just a single table. But how do you know which view has the data you are after and will allow you to get it with the minimum number of joins?"
97,"The DBMS_UTILITY.EXPAND_SQL_TEXT procedure expands any references to a view within a query, turning it into a subquery in the original statement that displays the full query text behind that view. This trick lets you see where and how the needed data will be retrieved."
97,"Let’s look at an elementary example of this procedure in action, using a SELECT * statement on a view called SALES_V."
97,SET serveroutput ON
97,DECLARE
97,l_clob CLOB;
97,BEGIN
97,"dbms_utility.Expand_sql_text(input_sql_text => 'SELECT * FROM sales_v', output_sql_text => l_clob);"
97,dbms_output.Put_line(l_clob);
97,END;
97,The result of this procedure call is the following output.
97,"SELECT ""A1"".""order_id"" ""ORDER_ID"","
97,"""A1"".""time_id""  ""TIME_ID"","
97,"""A1"".""cust_id""  ""CUST_ID"","
97,"""A1"".""prod_id""  ""PROD_ID"""
97,"FROM   (SELECT ""A3"".""order_id"" ""ORDER_ID"","
97,"""A3"".""time_id""  ""TIME_ID"","
97,"""A3"".""cust_id""  ""CUST_ID"","
97,"""A3"".""prod_id""  ""PROD_ID"""
97,"FROM   ""SH"".""sales"" ""A3"","
97,"""SH"".""products"" ""A2"""
97,"WHERE  ""A3"".""prod_id"" = ""A2"".""prod_id"") ""A1"""
97,The subquery with the alias A1 above is the view definition for SALES_V.
97,"It’s a simple two-table join between SALES (alias A3) and PRODUCTS (alias A2). Although the view only returns columns from the SALES table (A3), it does come with the overhead of a join. The execution plan for our simple SELECT * query below shows that."
97,PLAN_TABLE_OUTPUT
97,__________________________________________________________________________________________
97,Plan hash VALUE: 2857462611
97,---------------------------------------------------------------------------------------
97,| Id
97,| Operation
97,| Name
97,| ROWS
97,| Bytes | Cost (%CPU)| TIME
97,---------------------------------------------------------------------------------------
97,0 | SELECT STATEMENT
97,9773 |
97,314K|
97,(0)| 00:00:01 |
97,1 |
97,HASH JOIN
97,9773 |
97,314K|
97,(0)| 00:00:01 |
97,2 |
97,TABLE ACCESS STORAGE FULL| PRODUCTS |
97,4999 | 29994 |
97,(0)| 00:00:01 |
97,3 |
97,TABLE ACCESS STORAGE FULL| SALES
97,| 10000 |
97,263K|
97,(0)| 00:00:01 |
97,---------------------------------------------------------------------------------------
97,Predicate Information (IDENTIFIED BY operation id):
97,---------------------------------------------------
97,"1 - access(""S"".""PROD_ID""=""P"".""PROD_ID"")"
97,"In this case, it would be best to find an alternative view that only accesses the SALES table or access the SALES table directly. Below is the plan for a direct select statement from the SALES table, and as you can see, the cost of this plan is lower."
97,EXPLAIN PLAN FOR
97,"SELECT s.order_id, s.date_id, s.cust_id, s.product_id"
97,FROM sales s;
97,Explained.
97,SELECT * FROM TABLE(dbms_xplan.display());
97,PLAN_TABLE_OUTPUT
97,______________________________________________________________________________________
97,Plan hash VALUE: 781590677
97,-----------------------------------------------------------------------------------
97,| Id
97,| Operation
97,| Name
97,| ROWS
97,| Bytes | Cost (%CPU)| TIME
97,-----------------------------------------------------------------------------------
97,0 | SELECT STATEMENT
97,| 10000 |
97,263K|
97,(0)| 00:00:01 |
97,1 |
97,TABLE ACCESS STORAGE FULL| SALES | 10000 |
97,263K|
97,(0)| 00:00:01 |
97,-----------------------------------------------------------------------------------
97,"Over the last couple of releases, Oracle has added several handy PL/SQL packages and procedures you might not know about. So, I put together a short blog series highlighting some of my favorites. This blog post is part of that series. Other blogs in this series include How to add a SLEEP COMMAND to your Oracle PL/SQL code and How diff optimizer statistics."
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on May 16, 2023May 16, 2023Categories Oracle Database 18c, Oracle Database 19c, Use PL/SQL PackagesTags DBMS_UTILITY, EXPAND_SQL_TEXT, Useful PL/SQL, Views2 Comments on How to determine which view is the right view to use in Oracle?"
97,How do I add a SLEEP to a PL/SQL Stored Procedure?
97,"Over the last couple of releases, Oracle has added several handy PL/SQL packages and procedures you might not know about. So, I put together a short blog series highlighting some of my favorites. First up, DBMS_SESSION.SLEEP()."
97,"Oracle has always enabled you to add a sleep command to your stored procedures to suspend a session for a specified number of seconds, as shown in the code below."
97,DECLARE
97,v_start TIMESTAMP;
97,v_end
97,TIMESTAMP;
97,BEGIN
97,v_start := SYSTIMESTAMP;
97,-- Sleep for 10 seconds
97,DBMS_LOCK.SLEEP(10);
97,v_end
97,:= SYSTIMESTAMP;
97,DBMS_OUTPUT.PUT_LINE('This procedure started at ' ||v_start);
97,DBMS_OUTPUT.PUT_LINE('This procedure ended
97,at ' ||v_end);
97,END;
97,This PROCEDURE started AT 10-SEP-22 12.39.40.587041 AM
97,This PROCEDURE ended
97,AT 10-SEP-22 12.39.50.637738 AM
97,PL/SQL PROCEDURE successfully completed.
97,Elapsed: 00:00:10.02
97,"However, the sleep function was part of the DBMS_LOCK package, which is not granted to PUBLIC, by default, due to the other more powerful functions inside that package. That means you had to beg the DBA or the security team to give you access to this package just to put your session to sleep for a few minutes."
97,DBMS_SESSION.SLEEP()
97,"Things got a lot easier starting in Oracle Database 18c, as the sleep function is now available in the DBMS_SESSION package, which is granted to PUBLIC by default. That means you can call the function without any additional privileges. Plus, the function code in DBMS_SESSION.SLEEP is identical to DBMS_LOCK.SLEEP, so you can do a simple find and replace in your code!"
97,DECLARE
97,v_start TIMESTAMP;
97,v_end
97,TIMESTAMP;
97,BEGIN
97,v_start := SYSTIMESTAMP;
97,-- Sleep for 10 seconds
97,DBMS_SESSION.SLEEP(10);
97,v_end
97,:= SYSTIMESTAMP;
97,DBMS_OUTPUT.PUT_LINE('This procedure started at ' ||v_start);
97,DBMS_OUTPUT.PUT_LINE('This procedure ended
97,at ' ||v_end);
97,END;
97,This PROCEDURE started AT 10-SEP-22 12.39.40.587041 AM
97,This PROCEDURE ended
97,AT 10-SEP-22 12.39.50.637738 AM
97,PL/SQL PROCEDURE successfully completed.
97,Elapsed: 00:00:10.02
97,"Jonathan Lewis made a great point in the comments below about the granularity of the DBMS_SESSION.SLEEP is 1/100th of a second. If you want to introduce a sleep in milliseconds (or less), you can call Java from PL/SQL, as he demonstrated in his blog posts, little-sleeps."
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on May 16, 2023July 17, 2023Categories Oracle Database 18c, Oracle Database 19c, Use PL/SQL PackagesTags Oracle Database 19c, Useful PL/SQL1 Comment on How do I add a SLEEP to a PL/SQL Stored Procedure?"
97,How to create a parameterized view in Oracle
97,"Database views have been used for decades to help simplify both ad-hoc queries and reporting for developers, analysts, and end-users. But the problem with defining views is they tend to be either too specific (not easily reused) or not specific enough (too generic to be performant)."
97,"Imagine we need a view to help developers quickly find the details about orders waiting to ship, which have a total value of $100 or more. How would you create such a view?"
97,Would you create a particular view that returns only the columns needed for this scenario with restrictive where clause predicates that limit the orders to only those that have not shipped and have a value greater than $100?
97,CREATE OR REPLACE VIEW orders_waiting
97,"SELECT i.order_id, i.product_id, i.price, i.description"
97,FROM
97,"orders o, order_items i"
97,WHERE  o.order_status &gt; 6
97,-- orders waiting to ship
97,AND    o.order_total &gt;= 100
97,AND    o.order_id = i.order_id;
97,"Or do you create a more generic view that could be used for other reports and queries, even though it won’t be as performant as the dedicated view above?"
97,CREATE OR REPLACE VIEW orders_waiting
97,"SELECT o.order_status, o.order_total,"
97,"o.cust_id, o.order_date, o.rep,"
97,"o.order_mode, o.promotion_id,"
97,i.*
97,FROM
97,"orders o, order_items i"
97,WHERE  o.order_id = i.order_id;
97,"Regardless of your approach, you will compromise either reusability or performance."
97,"Starting in Oracle Database 19c (19.7), you no longer have to compromise, as Oracle introduced parameterized views or SQL Table Macros."
97,"SQL table macros are expressions found in a FROM clause that acts as a polymorphic or parameterized view. Let’s look at how this can help us with the view we need to create to find the details about orders waiting to ship, which have a total value of $100 or more."
97,"Creating a SQL Macro is similar to creating a PL/SQL function with an additional SQL_MACRO clause. The SQL MACRO clause can take an argument SCALAR (available from 21c onwards) or TABLE (19c onwards), but if you leave it blank, it defaults to a TABLE macro."
97,CREATE OR REPLACE FUNCTION orders_waiting_to_ship
97,RETURN CLOB sql_macro AS
97,stmt CLOB;
97,BEGIN
97,...
97,RETURN stmt;
97,END orders_waiting_to_ship;
97,"A SQL Macro always returns the view you define as text (VARCHAR, CHAR, or CLOB). The database will resolve that view definition and makes it part of the SQL statement that calls the SQL Macro. I’m capturing the view definition in a CLOB using the variable stmt above."
97,Let’s add the text for the view and the parameters we want to pass.
97,CREATE OR REPLACE FUNCTION orders_waiting_to_ship(order_value INTEGER)
97,RETURN CLOB sql_macro AS
97,stmt CLOB;
97,BEGIN
97,stmt := '
97,SELECT i.*
97,"FROM   orders o, order_items i"
97,WHERE  o.order_status &gt; 6
97,AND    o.order_total &gt;= order_value
97,AND    o.order_id = i.order_id';
97,RETURN stmt;
97,END orders_waiting_to_ship;
97,FUNCTION ORDERS_WAITING_TO_SHIP compiled
97,"In the code example above, I’m using the more selectivity view definition, but instead of specifying the order_total to be $100 or higher, I’m going to parameterize the value of the order. That way, we can reuse this SQL Table Macro regardless of what order_value is needed. You will notice I have also added a parameter to the function called order_value, which the user will pass in when they call the function."
97,"You will also notice that my function ORDERS_WAITING_TO_SHIP compiled without errors. This doesn’t mean my view definition or syntax is correct. You will only see an error at runtime. Remember, a SQL TABLE Macro produces your view definition as text that the optimizer will insert into the SQL statement that calls it."
97,"It’s straightforward to call a SQL Table Macro. The Table Macro goes in the FROM clause of the SQL statement. In the example below, I’m calling our orders_waiting_to_Ship SQL Macro and passing it the value 100."
97,SELECT *
97,FROM orders_waiting_to_ship(100)
97,ORDER BY order_id;
97,ORDER_ID
97,PRODUCT_ID
97,PRICE DESCRIPTION
97,___________ _____________ ________ ______________
97,10110
97,23 lipstick
97,10110
97,17 Lip gloss
97,10110
97,30 Mascarra
97,10110
97,35 Foundation
97,20209
97,15 Blusher
97,20209
97,23 lipstick
97,20209
97,35 Foundation
97,20209
97,32 Coverup
97,30307
97,17 Lip gloss
97,30307
97,30 Mascarra
97,30307
97,35 Foundation
97,30307
97,32 Coverup
97,"We could take it a step further and generalize our SQL Table Macro a little more by parameterizing the order_status and the order_value; that way, our developers can use it to check the status of shipped orders and orders waiting to ship."
97,"CREATE OR REPLACE FUNCTION orders_chk(order_value INTEGER, stat INTEGER)"
97,RETURN CLOB sql_macro AS
97,stmt CLOB;
97,BEGIN
97,stmt := '
97,SELECT i.*
97,FROM
97,"orders o, order_items i"
97,WHERE
97,o.order_status = orders_chk.stat
97,AND
97,o.order_total &gt;= orders_chk.order_value
97,AND
97,o.order_id = i.order_id';
97,RETURN stmt;
97,END orders_chk;
97,FUNCTION ORDERS_CHK compiled
97,SELECT *
97,"FROM orders_chk(10,6)"
97,ORDER BY order_id;
97,ORDER_ID
97,PRODUCT_ID
97,PRICE DESCRIPTION
97,......
97,___________ _____________ ________ ______________ __________
97,10110
97,23 lipstick
97,......
97,10110
97,17 Lip gloss
97,......
97,10110
97,30 Mascarra
97,......
97,10110
97,35 Foundation
97,......
97,10111
97,15 Blusher
97,......
97,10112
97,35 Foundation
97,......
97,10113
97,17 Lip gloss
97,......
97,There are several restrictions on SQL TABLE Macros you should be aware of:
97,"When used directly in a SQL statement, the SQL_MACRO annotation is disallowed with RESULT_CACHE, PARALLEL_ENABLE, and PIPELINE."
97,SQL statements with WITH clauses are not supported in SQL macros.
97,A SQL macro always runs with invoker rights.
97,SQL macros in views are always executed with the privileges of the view owner.
97,"SQL macros can’t be used in virtual column expression, function-based indexes, editioning views, or materialized views."
97,SQL macros can’t be used in type methods.
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on May 9, 2023November 14, 2023Categories Oracle Database 19cTags Parameterized Views, SQL Macro, Views6 Comments on How to create a parameterized view in Oracle"
97,How to watch Database World 2023 sessions
97,"Over the last two months, Oracle has taken our Database World Conference on the road with events in San Francisco, Singapore, Toyko, London, and São Paulo. The Oracle Database product management team delivered these one-day technical events and covered a variety of topics, including giving folks a sneak preview of Oracle Database 23c."
97,"If you couldn’t make it in person or get to all the sessions you wanted, don’t worry, as Oracle has made the technical sessions available online."
97,"Don’t miss this chance to check out what you can expect in 23c to help make apps simple to build and run, including the keynote by Juan Loaiza."
97,"Oracle has also made Oracle Database 23c FREE – developer Release available to try out all the new features and capabilities described in these sessions. You can download 23c FREE straight from the Internet with no oracle.com user account or license click-through requirements as a Container Image, VirtualBox VM, and Linux RPM installation file."
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on April 27, 2023Categories Events, Oracle Database 23cTags 23c FREE, Oracle Database 23c, Oracle Database WorldLeave a comment on How to watch Database World 2023 sessions"
97,How to use DBMS_STATS DIFF_TABLE_STATS functions
97,"In 11g, Oracle introduced the DBMS_STAT.DIFF_TABLE_STATS functions to help you compare two sets of statistics for a table along with all its dependent objects (indexes, columns, partitions)."
97,There are three versions of this function depending on where the statistics being compared are located:
97,DBMS_STAT.DIFF_TABLE_STATS_IN_HISTORY (compares statistics for a table from two timestamps in the past)
97,DBMS_STAT.DIFF_TABLE_STATS_IN_PENDING (compares pending statistics and statistics as of a timestamp or statistics from the data dictionary)
97,"DBMS_STAT.DIFF_TABLE_STATS_IN_STATTAB (compares statistics from a user statistics table and the data dictionary, from two different user statistics tables, or a single user statistics table using two different STATSIDs)"
97,The functions return a report that has three sections:
97,Basic table statistics
97,"The report compares the basic table statistics (number of rows, blocks, etc.)."
97,Column statistics
97,"The second section of the report examines column statistics, including histograms."
97,Index Statistics
97,The final section of the report covers differences in index statistics.
97,"Statistics will only be displayed in the report if the difference in the statistics exceeds a certain threshold (%). The threshold can be specified as an argument to the functions (PCTTHRESHOLD); the default value is 10%. The statistics corresponding to the first source, typically the current table stats in the data dictionary, will be used to compute the differential percentage."
97,"The functions also return the MAXDIFFPCT (a number) along with the report. This is the maximum percentage difference between the statistics. These differences can come from the table, column, or index statistics."
97,Let’s look at an example.
97,Continue reading “How to use DBMS_STATS DIFF_TABLE_STATS functions”
97,Share this:TwitterFacebookLike this:Like Loading...
97,Author
97,"Maria ColganPosted on March 21, 2023March 28, 2023Categories Optimizer, Oracle Database 19c, Statistics, Syntax, Top_Tip, UncategorizedTags 19c, optimizer, Oracle Optimizer, syntax, top_tipLeave a comment on How to use DBMS_STATS DIFF_TABLE_STATS functions"
97,Posts navigation
97,Page 1
97,Page 2
97,Page 11
97,Next page
97,Search for:
97,Search
97,Recent Posts
97,How to identify which indexes can be safely dropped in Oracle
97,Better diagnostics for SQL regressions in 19c and beyond #JoelKallmanDay
97,What to expect from Oracle DatabaseWorld at CloudWorld
97,How to find differences between Oracle production and test envs
97,How to determine if you are getting all the benefits of Exadata via AWR
97,Follow me on TwitterMy TweetsCategories
97,AskTom
97,Autonomous Database
97,Beginners
97,CloudWorld
97,Converged Database
97,Database In-Memory
97,Docker
97,Events
97,Exadata
97,Explain Plan
97,GPUs
97,IoT
97,JSON
97,Multitenant
97,New Features
97,OCI
97,OOW
97,Optimizer
97,Oracle CloudWorld
97,Oracle Database 12c New Features
97,Oracle Database 18c
97,Oracle Database 19c
97,Oracle Database 21c
97,Oracle Database 23c
97,Parallel_Execution
97,Partititoning
97,RAC
97,Sharding
97,SQL Monitor
97,SQL Plan Management
97,SQL Tuning
97,SQLDev
97,Statistics
97,Syntax
97,technical_papers
97,Top_Tip
97,Uncategorized
97,Use PL/SQL Packages
97,Whitepapers
97,Comonly used Tags
97,12c
97,18c
97,19c
97,ATP
97,Autonomous
97,Autonomous Transaction Processing
97,cloud
97,Database
97,DBMS_UTILITY
97,Docker
97,events
97,Exadata
97,Execution Plans
97,Explain Plan
97,Hints
97,In-Memory
97,IoT
97,Join Methods
97,JSON
97,Multitenant
97,New Release
97,OOW
97,optimizer
97,Oracle
97,Oracle Database 12c
97,Oracle Database 18c
97,Oracle Database 19c
97,Oracle Optimizer
97,Partitioning
97,Performance
97,PLSQL
97,RAC
97,REST
97,Security
97,Sharding
97,SPM
97,SQL
97,SQL Patch
97,SQL Tuning
97,statistics
97,syntax
97,system statistics
97,top_tip
97,Useful PL/SQL
97,video
97,About Maria
97,Proudly powered by WordPress
97,Loading Comments...
97,Write a Comment...
97,Email (Required)
97,Name (Required)
97,Website
98,Persistence Property Extensions Reference | EclipseLink 2.6.x Java Persistence API (JPA) Extensions Reference
98,"Java Persistence API (JPA) Extensions Reference for EclipseLink,"
98,Release 2.6
98,Comments
98,5 Persistence Property Extensions Reference
98,This chapter describes the persistence property extensions.You configure persistence units in the JPA persistence descriptor file: persistence.xml. EclipseLink includes many persistence property enhancements and extensions that can be configured in the persistence.xml file.
98,This chapter includes the following sections:
98,Functional Listing of Persistence Property Extensions
98,Alphabetical Listing of Persistence Property Extensions
98,Functional Listing of Persistence Property Extensions
98,"The following lists the EclipseLink persistence property (persistence.xml file) extensions, categorized by function:"
98,Weaving
98,Customizers
98,Validation and Optimization
98,Caching
98,Mapping
98,Schema generation
98,JDBC configuration
98,Concurrency manager
98,Weaving
98,EclipseLink includes the following persistence property extensions for weaving:
98,weaving
98,weaving.changetracking
98,weaving.eager
98,weaving.fetchgroups
98,weaving.internal
98,weaving.lazy
98,Customizers
98,EclipseLink includes the following persistence property extensions for customizing descriptors and sessions:
98,deploy-on-startup
98,descriptor.customizer
98,session.customizer
98,session.include.descriptor.queries
98,session-event-listener
98,session-name
98,sessions-xml
98,target-database
98,target-server
98,metadata-source
98,metadata-source.properties.file
98,metadata-source.send-refresh-command
98,metadata-source.xml.url
98,Validation and Optimization
98,EclipseLink includes the following persistence property extensions for validation.
98,exception-handler
98,partitioning
98,partitioning.callback
98,profiler
98,Logging
98,EclipseLink includes the following persistence property extensions for logging.
98,logging.connection
98,logging.exceptions
98,logging.file
98,logging.level
98,logging.session
98,logging.thread
98,logging.timestamp
98,Caching
98,EclipseLink includes the following persistence property extensions for caching:
98,cache.coordination.channel
98,cache.coordination.jms.factory
98,cache.coordination.jms.host
98,cache.coordination.jms.reuse-topic-publisher
98,cache.coordination.jms.topic
98,cache.coordination.jndi.initial-context-factory
98,cache.coordination.jndi.password
98,cache.coordination.jndi.user
98,cache.coordination.naming-service
98,cache.coordination.propagate-asynchronously
98,cache.coordination.protocol
98,cache.coordination.remove-connection-on-error
98,cache.coordination.rmi.announcement-delay
98,cache.coordination.rmi.multicast-group
98,cache.coordination.rmi.multicast-group
98,cache.coordination.rmi.packet-time-to-live
98,cache.coordination.rmi.url
98,cache.coordination.thread.pool.size
98,cache.database-event-listener
98,cache.shared
98,cache.size
98,cache.type
98,flush-clear.cache
98,Mapping
98,EclipseLink includes the following persistence property extensions for mappings:
98,composite-unit
98,composite-unit.member
98,composite-unit.properties
98,Schema generation
98,EclipseLink includes the following persistence property extensions for mappings:
98,create-ddl-jdbc-file-name
98,ddl.table-creation-suffix
98,ddl-generation
98,ddl-generation.output-mode
98,drop-ddl-jdbc-file-name
98,JDBC configuration
98,EclipseLink includes the following persistence property extensions for configuring JDBC connections and connection pooling:
98,connection-pool
98,connection-pool.read
98,connection-pool.sequence
98,jdbc.allow-native-sql-queries
98,jdbc.batch-writing
98,jdbc.batch-writing.size
98,jdbc.cache-statements
98,jdbc.cache-statements.size
98,jdbc.connector
98,jdbc.exclusive-connection.is-lazy
98,jdbc.exclusive-connection.mode
98,jdbc.native-sql
98,jdbc.property
98,jdbc.sql-cast
98,jdbc.uppercase-columns
98,Concurrency manager
98,EclipseLink includes the following persistence property extensions for concurrency management:
98,concurrency.manager.waittime
98,concurrency.manager.maxsleeptime
98,concurrency.manager.maxfrequencytodumptinymessage
98,concurrency.manager.maxfrequencytodumpmassivemessage
98,concurrency.manager.allow.interruptedexception
98,concurrency.manager.allow.concurrencyexception
98,concurrency.manager.allow.readlockstacktrace
98,Alphabetical Listing of Persistence Property Extensions
98,"The following lists the EclipseLink persistence property (persitence.xml file) extensions, in alphabetical order:"
98,application-location
98,cache.coordination.channel
98,cache.coordination.jms.factory
98,cache.coordination.jms.host
98,cache.coordination.jms.reuse-topic-publisher
98,cache.coordination.jms.topic
98,cache.coordination.jndi.initial-context-factory
98,cache.coordination.jndi.password
98,cache.coordination.jndi.user
98,cache.coordination.naming-service
98,cache.coordination.propagate-asynchronously
98,cache.coordination.protocol
98,cache.coordination.remove-connection-on-error
98,cache.coordination.rmi.announcement-delay
98,cache.coordination.rmi.multicast-group
98,cache.coordination.rmi.multicast-group
98,cache.coordination.rmi.packet-time-to-live
98,cache.coordination.rmi.url
98,cache.coordination.thread.pool.size
98,cache.database-event-listener
98,cache.shared
98,cache.size
98,cache.type
98,classloader
98,composite-unit
98,composite-unit.member
98,composite-unit.properties
98,concurrency.manager.waittime
98,concurrency.manager.maxsleeptime
98,concurrency.manager.maxfrequencytodumptinymessage
98,concurrency.manager.maxfrequencytodumpmassivemessage
98,concurrency.manager.allow.interruptedexception
98,concurrency.manager.allow.concurrencyexception
98,concurrency.manager.allow.readlockstacktrace
98,connection-pool
98,connection-pool.read
98,connection-pool.sequence
98,create-ddl-jdbc-file-name
98,ddl.table-creation-suffix
98,ddl-generation
98,ddl-generation.output-mode
98,ddl.table-creation-suffix
98,deploy-on-startup
98,descriptor.customizer
98,drop-ddl-jdbc-file-name
98,exception-handler
98,exclude-eclipselink-orm
98,flush-clear.cache
98,id-validation
98,jdbc.allow-native-sql-queries
98,jdbc.batch-writing
98,jdbc.batch-writing.size
98,jdbc.cache-statements
98,jdbc.cache-statements.size
98,jdbc.connector
98,jdbc.exclusive-connection.is-lazy
98,jdbc.exclusive-connection.mode
98,jdbc.native-sql
98,jdbc.property
98,jdbc.sql-cast
98,jdbc.uppercase-columns
98,jpa.uppercase-column-names
98,jpql.parser
98,jpql.validation
98,logging.connection
98,logging.exceptions
98,logging.file
98,logging.level
98,logging.session
98,logging.thread
98,logging.timestamp
98,metadata-source
98,metadata-source.properties.file
98,metadata-source.send-refresh-command
98,metadata-source.xml.url
98,nosql.connection-factory
98,nosql.connection-spec
98,nosql.property
98,oracle.proxy-type
98,orm.throw.exceptions
98,orm.validate.schema
98,partitioning
98,partitioning.callback
98,persistence-context.close-on-commit
98,persistence-context.commit-without-persist-rules
98,persistence-context.flush-mode
98,persistence-context.persist-on-commit
98,persistence-context.reference-mode
98,persistenceunits
98,persistencexml
98,persisencexml.default
98,profiler
98,session.customizer
98,session.include.descriptor.queries
98,session-event-listener
98,session-name
98,sessions-xml
98,target-database
98,target-server
98,temporal.mutable
98,tenant-id
98,transaction.join-existing
98,tuning
98,validate-existence
98,validation-only
98,weaving
98,weaving.changetracking
98,weaving.eager
98,weaving.fetchgroups
98,weaving.internal
98,weaving.lazy
98,application-location
98,Use the eclipselink.application-location property to specify the file system directory in which EclipseLink writes (outputs) DDL files.
98,Values
98,Table 5-1 describes this persistence property's values.
98,Table 5-1 Valid Values for application-location
98,Value
98,Description
98,value
98,"Directory location. The path must be fully qualified. For Windows, use a backslash. For UNIX use a slash."
98,Usage
98,You may set this option only if the value of eclipselink.ddl-generation.output-mode is sql-script or both.
98,Examples
98,Example 5-1 shows how to use this property in the persistence.xml file.
98,Example 5-1 Using application-location in persistence.xml
98,"<property name=""eclipselink.application-location"" value=""c:/YOURDIRECTORY/""/>"
98,Example 5-2 shows how to use this property in a property map.
98,Example 5-2 Using application-location in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.APPLICATION_LOCATION,"
98,"""c:/YOURDIRECTORY/"");"
98,See Also
98,"For more information, see:"
98,"""ddl-generation.output-mode"""
98,cache.coordination.channel
98,Use the eclipselink.cache.coordination.channel property to configure cache coordination for a clustered environment.
98,Values
98,Table 5-2 describes this persistence property's values.
98,Table 5-2 Valid Values for cache.coordination.channel
98,Value
98,Description
98,channel name
98,The channel used for cache coordination. All persistence units using the same channel will be coordinated.
98,Default: EclipseLinkCommandChannel
98,Usage
98,"If multiple EclipseLink deployments reside on the same network, they should be in different channels."
98,Examples
98,Example 5-3 shows how to use this property in the persistence.xml file.
98,Example 5-3 Using application-location in persistence.xml
98,"<property name=""eclipselink.cache.coordination.channel"" value=""EmployeeChannel"" />"
98,Example 5-4 shows how to use this property in a property map.
98,Example 5-4 Using cache.coordination.channel in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.CACHE_COORDINATION_CHANNEL,"
98,"""myChannel"");"
98,See Also
98,"For more information, see:"
98,"""@Cache"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jms.factory
98,"Use the eclipselink.cache.coordination.jms.factory property to configure the JMS topic connection factory name, when using JMS coordination for a clustered environment."
98,Values
98,Table 5-3 describes this persistence property's values.
98,Table 5-3 Valid Values for cache.coordination.jms.factory
98,Value
98,Description
98,name
98,The JMS topic connection factory name.
98,Default: jms/EclipseLinkTopicConnectionFactory
98,Usage
98,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
98,Examples
98,See Example 5-13 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jms.host
98,"Use the eclipselink.cache.coordination.jms.host property to configure the URL of the JMS server that hosts the topic, when using JMS coordination for a clustered environment."
98,Values
98,Table 5-4 describes this persistence property's values.
98,Table 5-4 Valid Values for cache.coordination.jms.host
98,Value
98,Description
98,url
98,The fully-qualified URL for the JMS server.
98,"This is not required if the topic is distributed across the cluster (that is, it can be looked up in local JNDI)."
98,Usage
98,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms). You must use a fully qualified URL.
98,Examples
98,See Example 5-13 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jms.reuse-topic-publisher
98,Use the eclipselink.cache.coordination.jms.reuse-topic-publisher property to specify if the JSM transport manager should cache a TopicPubliser and reuse it for all cache coordination publishing.
98,Values
98,Table 5-5 describes this persistence property's values.
98,Table 5-5 Valid Values for cache.coordination.jms.reuse-topic-publisher
98,Value
98,Description
98,true
98,Caches the topic publisher.
98,false
98,(Default) Does not cache the topic publisher.
98,Usage
98,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
98,Examples
98,See Example 5-13 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jms.topic
98,"Use the eclipselink.cache.coordination.jms.topic property to set the JMS topic name, when using JMS coordination for a clustered environment."
98,Values
98,Table 5-6 describes this persistence property's values.
98,Table 5-6 Valid Values for cache.coordination.jms.topic
98,Value
98,Description
98,name
98,Set the JMS topic name.
98,Default: jms/EclipseLinkTopic
98,Usage
98,Use this property for JMS coordination (when eclipselink.cache.coordination.protocol = jms).
98,Examples
98,See Example 5-13 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jndi.initial-context-factory
98,"Use the eclipselink.cache.coordination.jndi.initial-context-factory property to set the JNDI InitialContext factory, when using cache coordination for a clustered environment."
98,Values
98,Table 5-7 describes this persistence property's values.
98,Table 5-7 Valid Values for cache.coordination.jndi.initial-context-factory
98,Value
98,Description
98,name
98,Name of the JNDI InitialContext factory.
98,Usage
98,"Normally, you will not need this property when connecting to the local server."
98,Examples
98,Example 5-5 shows how to use this property in the persistence.xml file.
98,Example 5-5 Using cache.coordination.jndi.initial-context-factory in persistence.xml.
98,"<property name=""eclipselink.cache.coordination.jndi.initial-context-factory"""
98,"value=""weblogic.jndi.WLInitialContextFactory/>"
98,Example 5-6 shows how to use this property in a property map.
98,Example 5-6 Using cache.coordination.jndi.initial-context-factory in a property map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertyMap.put
98,"(PersistenceUnitProperties.CACEH_COORDINATION_JNDI_INITIAL_CONTEXT_FACTORY,"
98,"""weblogic.jndi.WLInitialContextFactory"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jndi.password
98,"Use the eclipselink.cache.coordination.jndi.password property to set the password for the cache.coordination.jndi.user, when using cache coordination for a clustered environment."
98,Values
98,Table 5-8 describes this persistence property's values.
98,Table 5-8 Valid Values for cache.coordination.jndi.password
98,Value
98,Description
98,value
98,Password for the cache.coordination.jndi.user.
98,Usage
98,"Normally, you will not need this property when connecting to the local server."
98,Examples
98,Example 5-7 shows how to use this propery in the persistence.xml file.
98,Example 5-7 Using cache.coordination.jndi.password in persistence.xml
98,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""USERNAME""/>"
98,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""PASSWORD""/>"
98,Example 5-8 shows how to use this property in a property map.
98,Example 5-8 Using cache.coordination.jndi.password in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_JNDI_USER,"
98,"""USERNAME"");"
98,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_JNDI_PASSWORD,"
98,"""PASSWORD"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.jndi.user"""
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.jndi.user
98,"Use the eclipselink.cache.coordination.jndi.user property to set JNDI naming service user, when using cache coordination for a clustered environment."
98,Values
98,Table 5-9 describes this persistence property's values.
98,Table 5-9 Valid Values for cache.coordination.jndi.user
98,Value
98,Description
98,value
98,The JNDI user.
98,Usage
98,"Normally, you will not need this property when connecting to the local server."
98,Examples
98,See Example 5-13 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.jndi.password"""
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.naming-service
98,"Use the eclipselink.cache.coordination.naming-service property to specify the naming service to use, when using cache coordination for a clustered environment."
98,Values
98,Table 5-10 describes this persistence property's values.
98,Table 5-10 Valid Values for cache.coordination.naming-service
98,Value
98,Description
98,jndi
98,Uses JNDI.
98,rmi
98,Configures RMI.
98,Usage
98,Cache coordination must be enabled.
98,Examples
98,Example 5-9 shows how to use this property in the persistence.xml file.
98,Example 5-9 Using cache.coordination.naming-service in persistence.xml
98,"<property name=""eclipselink.cache.coordination"" value=""true""/>"
98,"<property name=""eclipselink.cache.coordination.naming-service"" value=""jndi""/>"
98,Example 5-10 shows how to use this property in a property map.
98,Example 5-10 Using cache.coordination.naming-service in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_NAMING_SERVICE,"
98,"""jndi"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.propagate-asynchronously
98,Use the eclipselink.cache.coordination.propagate-asynchronously property to specify if the coordination broadcast should occur asynchronously with the committing thread.
98,The property configures cache coordination for a clustered environment. Set if the coordination broadcast should occur asynchronously with the committing thread. This means the coordination will be complete before the thread returns from the commit of the transaction.
98,Values
98,Table 5-11 describes this persistence property's values.
98,Table 5-11 Valid Values for cache.coordination.propagate-asynchronously
98,Value
98,Description
98,true
98,(Default) EclipseLink will broadcast asynchronously. The coordination will be complete before the thread returns from the committing the transaction.
98,false
98,EclipseLink will broadcast synchronously.
98,Usage
98,"JMS cache coordination is always asynchronous, regardless of this setting."
98,"By default, RMI cache coordination is asynchronous. Use synchronous (eclipselink.cache.coordination.propagate-asynchronously = false) to ensure that all servers are updated before the request returns."
98,Examples
98,Example 5-11 shows how to use this property in the persistence.xml file.
98,Example 5-11 Using cache.coordination.propagate-asynchronously in persistence.xml
98,"<property name=""eclipselink.cache.coordination.propagate-asynchronously"""
98,"value=""false"" />"
98,Example 5-12 shows how to use this property in a property map.
98,Example 5-12 Using cache.coordination.propagate-asynchronously in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertyMap.put
98,"(PersistenceUnitProperties.CACHE_COORDINATION_PROPAGATE_ASYNCHRONOUSLY,"
98,"""false"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.protocol
98,"Use the eclipselink.cache.coordination.protocol property to specify the cache coordination protocol to use. Depending on the cache configuration for each descriptor, this will broadcast cache updates or inserts to the cluster to update or invalidate each session's cache."
98,Values
98,Table 5-12 describes this persistence property's values.
98,Table 5-12 Valid Values for cache.coordination.protocol
98,Value
98,Description
98,jms
98,Use Java Message Service (JMS) to broadcast changes.
98,jms-publishing
98,Use an EJB MessageDrivenBean to be used to broadcast changes. You must configure the MessageDrivenBean separately.
98,rmi
98,Use Java Remote Method Invocation (RMI) to broadcast changes.
98,rmi-iiop
98,Use RMI over the Internet Inter-Orb Protocol (IIOP) to broadcast changes.
98,ClassName
98,The name of a subclass implementation of the TransportManager abstract class
98,Usage
98,You must specify the cache.coordination.protocol for every persistence unit and session in the cluster.
98,Examples
98,Example 5-13 shows how configure JMS cache coordination in the persistence.xml file.
98,Example 5-13 Configuring JMS Cache Coordination in persistence.xml
98,"<property name=""eclipselink.cache.coordination.protocol"" value=""jms"" />"
98,"<property name=""eclipselink.cache.coordination.jms.topic"""
98,"value=""jms/EmployeeTopic"" />"
98,"<property name=""eclipselink.cache.coordination.jms.factory"""
98,"value=""jms/EmployeeTopicConnectionFactory"" />"
98,"If your application is not running in a cluster, you must provide the URL:"
98,"<property name=""eclipselink.cache.coordination.jms.host"""
98,"value=""t3://myserver:7001/"" />"
98,"You can also include a username and password, if required, to access the server (for example, if on a separate domain):"
98,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""weblogic"" />"
98,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""welcome1"" />"
98,Example 5-14 shows how to configure RMI cache coordination in the persistence.xml file.
98,Example 5-14 Configuring RMI Cache Coordination in persistence.xml
98,"<property name=""eclipselink.cache.coordination.protocol"" value=""rmi"" />"
98,"If your application is not running in a cluster, you must provide the URL:"
98,"<property name=""eclipselink.cache.coordination.rmi.url"""
98,"value=""t3://myserver:7001/"" />"
98,"You can also include a username and password, if required, to access the server (for example, if on a separate domain):"
98,"<property name=""eclipselink.cache.coordination.jndi.user"" value=""weblogic"" />"
98,"<property name=""eclipselink.cache.coordination.jndi.password"" value=""welcome1"" />"
98,"By default, RMI cache coordination broadcasts are asynchronous. You can override this, if needed:"
98,"<property name=""eclipselink.cache.coordination.propagate-asynchronously"""
98,"value=""false"" />"
98,"If you have multiple applications on the same server or network, you can specify a separate cache coordination channel for each application:"
98,"<property name=""eclipselink.cache.coordination.channel"" value=""EmployeeChannel"" />"
98,"RMI cache coordination uses a multicast socket to allow servers to find each other. You can configure the multicast settings, if needed:"
98,"<property name=""eclipselink.cache.coordination.rmi.announcement-delay"""
98,"value=""1000"" />"
98,"<property name=""eclipselink.cache.coordination.rmi.multicast-group"""
98,"value=""239.192.0.0"" />"
98,"<property name=""eclipselink.cache.coordination.rmi.multicast-group.port"""
98,"value=""3121"" />"
98,"<property name=""eclipselink.cache.coordination.packet-time-to-live"" value=""2"" />"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.channel"""
98,"""cache.coordination.jms.factory"""
98,"""cache.coordination.jms.host"""
98,"cache.coordination.jms.reuse-topic-publisher""cache.coordination.jms.reuse-topic-publisher"""
98,"""cache.coordination.jms.topic"""
98,"""cache.coordination.jndi.initial-context-factory"""
98,"""cache.coordination.jndi.password"""
98,"""cache.coordination.jndi.user"""
98,"""cache.coordination.naming-service"""
98,"""cache.coordination.propagate-asynchronously"""
98,"""cache.coordination.remove-connection-on-error"""
98,"""cache.coordination.rmi.announcement-delay"""
98,"""cache.coordination.rmi.multicast-group"""
98,"""cache.coordination.rmi.multicast-group"""
98,"""cache.coordination.rmi.packet-time-to-live"""
98,"""cache.coordination.rmi.url"""
98,"""cache.coordination.thread.pool.size"""
98,Cache Coordination Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
98,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
98,cache.coordination.remove-connection-on-error
98,Use the eclipselink.cache.coordination.remove-connection-on-error property to specify if the connection should be removed if EclipseLink encounters a communication error when coordinating the cache.
98,Values
98,Table 5-13 describes this persistence property's values.
98,Table 5-13 Valid Values for cache.coordination.remove-connection-on-error
98,Value
98,Description
98,true
98,Removes the connection if a communication error occurs. EclipseLink will reconnect when the server becomes available.
98,false
98,(Default) Does not remove the connection if a communication error occurs.
98,Usage
98,"Normally, this is used for RMI connections in the event that a server goes down."
98,Examples
98,Example 5-15 shows how to use this property in the persistence.xml file.
98,Example 5-15 Using cache.coordination.remove-connection-on-error in peristence.xml
98,"<property name=""eclipselink.cache.coordination.remove-connection-on-error"""
98,"value=""true""/>"
98,Example 5-16 shows how to use this property in a property map.
98,Example 5-16 Using cache.coordination.remove-connection-on_error in a property map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertyMap.put
98,"(PersistenceUnitProperties.CACHE_COORDINATION_REMOVE_CONNECTION_ON_ERROR,""true"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.rmi.announcement-delay
98,Use the eclipselink.cache.coordination.rmi.announcement-delay property to set the time (in milliseconds) to wait for announcements from other cluster members on startup.
98,Values
98,Table 5-14 describes this persistence property's values.
98,Table 5-14 Valid Values for cache.coordination.rmi.announcement-delay
98,Value
98,Description
98,Numeric
98,"Time (in milliseconds) to wait for announcements, on startup."
98,Default: 1000
98,Usage
98,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
98,Examples
98,See Example 5-14 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.rmi.multicast-group
98,"Use the eclipselink.cache.coordination.rmi.multicast-group property to set the multicast socket group address (used to find other members of the cluster), when using cache coordination for a clustered environment."
98,Values
98,Table 5-15 describes this persistence property's values.
98,Table 5-15 Valid Values for cache.coordination.rmi.multicast-group
98,Value
98,Description
98,Numeric
98,Set the multicast socket group address
98,Default: 239.192.0.0
98,Usage
98,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
98,Examples
98,See Example 5-14 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.rmi.multicast-group.port
98,"Use the eclipselink.cache.coordination.rmi.multicast-group.port property to set the multicast socket group port (used to find other members of the cluster), when using cache coordination for a clustered environment."
98,Values
98,Table 5-16 describes this persistence property's values.
98,Table 5-16 Valid Values for cache.coordination.rmi.multicast-group.port
98,Value
98,Description
98,Numeric
98,Set the multicast socket group port.
98,Default: 3121
98,Usage
98,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
98,Examples
98,See Example 5-14 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.rmi.packet-time-to-live
98,Use the eclipselink.cache.coordination.rmi.packet-time-to-live property to set the number of hops the session announcement data packets will take before expiring. The multicast group is used to find other members of the cluster.
98,Values
98,Table 5-17 describes this persistence property's values.
98,Table 5-17 Valid Values for cache.coordination.rmi.packet-time-to-live
98,Value
98,Description
98,Numeric
98,Number of hops the session announcement data packets will take before expiring.
98,Default: 2
98,Usage
98,"If sessions are hosted on different LANs that are part of WAN, the announcement sent by one session may not reach other sessions. In this case, consult your network administrator for the correct time-to-live value or test your network by increasing the value until each session receives announcement sent by others."
98,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
98,Examples
98,See Example 5-14 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.rmi.url
98,Use the eclipselink.cache.coordination.rmi.url property to set the URL of the host server. This is the URL that other cluster member use to connect to this host.
98,Values
98,Table 5-18 describes this persistence property's values.
98,Table 5-18 Valid Values for cache.coordination.rmi.url
98,Value
98,Description
98,url
98,URL of the host server
98,Default: local
98,Usage
98,Use this property for RMI coordination (when eclipselink.cache.coordination.protocol = rmi).
98,This may not be required in a clustered environment where JNDI is replicated. You can also set the location as a System property or using a SessionCustomizer to avoid requiring a separate persistence.xml file per server.
98,Examples
98,See Example 5-14 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.coordination.thread.pool.size
98,"Use the eclipselink.cache.coordination.thread.pool.size property to configure the size of the thread pool, for cache coordination threads."
98,Values
98,Table 5-19 describes this persistence property's values.
98,Table 5-19 Valid Values for cache.coordination.thread.pool.size
98,Value
98,Description
98,Numeric
98,"Size of the thread pool. If 0, EclipseLink does not use a thread pool; instead threads are spawned when required."
98,Default: 32
98,Usage
98,"For RMI cache coordination, EclipseLink spawns one thread per node to send change notifications and one thread to listen for new node notifications."
98,"For JMS cache coordination, EclipseLink spawns one thread to receive JMS change notification messages (unless MDB is used) and one thread to process the change notification (unless MDB is used)."
98,Examples
98,Example 5-17 shows how to use this property in the persistence.xml file.
98,Example 5-17 Using cache.coordination.thread.pool.size in persistence.xml
98,"<property name=""eclipselink.cache.coordination.thread.pool.size"""
98,"value=""48""/>"
98,Example 5-18 shows how to use this property in a property map.
98,Example 5-18 Using cache.coordination.thread.pool.size in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.CACHE_COORDINATION_THREAD_POOL_SIZE,"
98,"""48"");"
98,See Also
98,"For more information, see:"
98,"""cache.coordination.protocol"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,cache.database-event-listener
98,"Use the eclipselink.cache.database-event-listener property to integrate EclipseLink with a database event notification service, such as Oracle QCN/DCN (Query Change Notification/Database Change Notification)."
98,Values
98,Table 5-20 describes this persistence property's values.
98,Table 5-20 Valid Values for cache.database-event-listener
98,Value
98,Description
98,Class
98,"The name of a class that implements DatabaseEventListener, such as the OracleChangeNotificationListener (org.eclipse.persistence.platform.database.oracle.dcn.OracleChangeNotificationListener)."
98,You can also use DCN and QCN for Oracle.
98,Usage
98,"You can use this property to allow the EclipseLink cache to be invalidated by database change events, triggers, or other services."
98,Examples
98,Example 5-19 shows how to use this property with Oracle DCN.
98,Example 5-19 Using cache.database-event-listener in persistence.xml
98,"<?xml version=""1.0"" encoding=""UTF-8""?>"
98,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
98,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
98,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence"
98,"persistence_2_0.xsd"""
98,"version=""2.0"">"
98,"<persistence-unit name=""acme"" transaction-type=""RESOURCE_LOCAL"">"
98,<provider>org.eclipse.persistence.jpa.PersistenceProvider</provider>
98,<exclude-unlisted-classes>false</exclude-unlisted-classes>
98,<properties>
98,"<property name=""eclipselink.cache.database-event-listener"" value="
98,"""org.eclipse.persistence.platform.database.oracle.dcn.OracleChangeNotificationList"
98,"ener""/>"
98,</properties>
98,</persistence-unit>
98,</persistence>
98,See Also
98,"For more information, see:"
98,"""@Cache"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,"""Database Change Notification"" in Oracle Fusion Middleware Configuring and Managing JDBC Data Sources for Oracle WebLogic Server"
98,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
98,Cache Coordination Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
98,cache.shared
98,Use the eclipselink.cache.shared property prefix to indicate whether an entity's cache is shared (non-isolated).
98,Values
98,Table 5-21 describes this persistence property prefix's values.
98,Table 5-21 Valid Values for cache.shared
98,Value
98,Description
98,true
98,(Default) Shares an entity's cache. The value is case insensitive.
98,false
98,Prevents sharing of an entity's cache. The value is case insensitive.
98,Usage
98,"Form a property name by appending either a valid entity name or class name to class.shared, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.shared property prefix to form a property name that sets the default for all entities."
98,Examples
98,See Example 2-12 for information on how to use this property.
98,cache.size
98,Use the eclipselink.cache.size property prefix to specify the cache size for a specific entity type.
98,Values
98,Table 5-22 describes this persistence property prefix's values.
98,Table 5-22 Valid Values for cache.size
98,Value
98,Description
98,integer
98,The size of the cache. Default: 100 Bytes.
98,Usage
98,"Form a property name by appending either a valid entity name or class name to cache.size, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.size property prefix, indicating that the property value applies to all entities."
98,"For most cache types, the size is only the initial size, not a fixed or maximum size. For CacheType.SoftCache and CacheType.HardCache types, the size is the sub-cache size. The default cache size is 100 Bytes."
98,Examples
98,See Example 2-12 for information on how to use this property.
98,cache.type
98,Use the eclipselink.cache.type property prefix to set the type of cache.
98,Values
98,Table 5-23 describes this persistence property prefix's values
98,Table 5-23 Valid values for cache.type
98,Value
98,Description
98,Weak
98,"Holds all objects in use by the application, and allows any unreferenced objects to be free for garbage collection. This cache type guarantees object identity and allows optimal garbage collection, but provides little caching benefit."
98,Soft
98,"Holds all objects read by the application, and allows any unreferenced objects to be free for garbage collection only when the JVM decides that memory is low. This cache type guarantees object identity, allows for garbage collection when memory is low, and provides optimal caching benefit."
98,SoftWeak
98,"(Default)Holds all objects read by the application, and a fixed-size subcache of MRU objects using Soft references.The SoftWeak cache allows any unreferenced objects not in the sub-cache to be free for garbage collection. The objects in the sub-cache are free to garbage collect only when the JVM decides that memory is low. This cache type guarantees object identity, allows configurable garbage collection, and provides configurable caching benefit."
98,HardWeak
98,"Holds all objects in use by the application, and a fixed-size subcache of MRU objects using normal Hard references. This type allows any unreferenced objects not in the subcache to be free for garbage collection, but not objects in the subcache. This cache type guarantees object identity, allows configurable garbage collection, and provides configurable caching benefit."
98,Full
98,"Holds all objects read by the application. This cache type does not allow garbage collection. This guarantees object identity, allows no garbage collection, and provides complete caching benefit."
98,"WARNING: Use this cache type only for a fixed number of objects; otherwise, memory leakage will occur eventually."
98,NONE
98,"Does not cache any objects, and frees any unreferenced objects for garbage collection. This provides no object identity, allows complete garbage collection, and provides no caching benefit."
98,"WARNING: This cache type should normally not be used. Instead, disable the shared cache through PersistenceUnitProperties.CACHE_SHARED. Lack of object identity can lead to infinite loops for objects that have circular references and no indirection."
98,Usage
98,"Form a property name by appending a valid entity name or class name to cache.type, indicating that the property values apply only to a particular entity. As an alternative, you can append the default suffix to the cache.type prefix to form a property name that sets the default for all entities."
98,Valid values for cache.type properties are declared in the CacheType class. The default is SoftWeak.
98,"If you do not want to cache entities, set the cache.shared property."
98,Examples
98,See Example 2-12 for information about how to use this property.
98,See Also
98,"For more information, see:"
98,cache.shared
98,classloader
98,Use the eclipselink.classloader property to create an EntityMangerFactory in the property map to be passed to Persistence.createEntityManagerFactory.
98,Values
98,Table 5-24 describes this persistence property's values.
98,Table 5-24 Valid Values for classloader
98,Value
98,Description
98,value
98,Classloader to use.
98,Usage
98,"This is a dynamic property that must be set at runtime, in the property map. You cannot configure this property in the persistence.xml file."
98,Examples
98,Example 5-20 shows how to use this property in a property map.
98,Example 5-20 Using classloader in a Property Map
98,"properties.put(""eclipselink.classloader"", this.getClass().getClassLoader());"
98,composite-unit
98,Use the eclipselink.composite-unit property to specify if the persistence unit is a composite persistence unit.
98,Values
98,Table 5-25 describes this persistence property's values.
98,Table 5-25 Valid Values for composite-unit
98,Value
98,Description
98,true
98,Persistence unit is a composite persistence unit.
98,false
98,(Default) Persistence unit is not a composite persistence unit.
98,Usage
98,The property must be specified in persistence.xml of a composite persistence unit. The composite persistence unit must contain all persistence units found in JAR files specified by the persistence.xml file.
98,Note:
98,"If this property is passed to the createEntityManagerFactory method or if it is set in system properties, it is ignored.)"
98,Examples
98,Example 5-21 shows how to use this property in the persistence.xml file.
98,Example 5-21 Using composite-unit in persistence.xml
98,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_1_0.xsd"" version=""1.0"">"
98,"<persistence-unit name=""compositePu"" transaction-type=""JTA"">"
98,<provider>
98,org.eclipse.persistence.jpa.PersistenceProvider
98,</provider>
98,<jar-file>member1.jar</jar-file>
98,<jar-file>member2.jar</jar-file>
98,<properties>
98,"<property name=""eclipselink.composite-unit"" value=""true""/>"
98,"<property name=""eclipselink.target-server"" value=""WebLogic_10""/>"
98,</properties>
98,</persistence-unit>
98,</persistence>
98,See Also
98,"For more information, see:"
98,"""composite-unit.member"""
98,"""composite-unit.properties"""
98,"""Using Multiple Databases with a Composite Persistence Unit"" in Solutions Guide for EclispeLink"
98,"""Composite Persistence Units"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/Composite_Persistence_Units"
98,composite-unit.member
98,Use the eclipselink.composite-unit.member property to specify if the persistence unit is a member composite persistence unit.
98,Values
98,Table 5-26 describes this persistence property's values.
98,Table 5-26 Valid Values for composite-unit.member
98,Value
98,Description
98,true
98,The persistence unit must be a member of a composite persistence unit and cannot be used as an independent persistence unit.
98,false
98,(Default) The persistence unit does not have to be a member of a composite persistence unit.
98,Usage
98,Setting this property to true indicates that the persistence unit has dependencies on other persistence units.
98,Note:
98,"If this property is passed to the createEntityManagerFactory method or if it is set in system properties, it is ignored.)"
98,"If this property is true, you may still create EntityManagerFactory, but it cannot be connected. Any attempt to create an entity manger will cause an exception."
98,Query Hint
98,"When executing a native query on a composite persistence unit, use composite-unit.member to specify the name of the composite member persistence unit on which to execute the query."
98,Examples
98,Example 5-22 shows how to use this property in the persistence.xml file.
98,Example 5-22 Using composite-unit.member in persistence.xml
98,Composite member persistence unit memberPu2 is defined in the member2.jar file. It has dependency on a class defined in member1.jar and cannot be used independently.
98,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
98,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
98,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_1_0.xsd"""
98,"version=""1.0"">"
98,"<persistence-unit name=""memberPu2"">"
98,<provider>
98,org.eclipse.persistence.jpa.PersistenceProvider
98,</provider>
98,<mapping-file>META-INF/advanced-entity-mappings2.xml</mapping-file>
98,<jta-data-source>jdbc/MySqlJtaDS</jta-data-source>
98,<exclude-unlisted-classes>false</exclude-unlisted-classes>
98,<properties>
98,"<property name=""eclipselink.composite-unit.member"" value=""true""/>"
98,"<property name=""eclipselink.target-database"""
98,"value=""org.eclipse.persistence.platform.database.MySQLPlatform""/>"
98,</properties>
98,</persistence-unit>
98,</persistence>
98,See Also
98,"For more information, see:"
98,"""@CompositeMember"""
98,"""composite-unit"""
98,"""composite-unit.member"""
98,composite-unit.properties
98,Use the eclipselink.composite-unit.properties property to configure the properties for persistence unit members.
98,Values
98,Table 5-27 describes this persistence property's values.
98,Table 5-27 Valid Values for composite-unit.properties
98,Value
98,Description
98,Map of properties
98,Properties to be passed to the persistence unit. Use the persistence unit's name as the key.
98,Usage
98,Pass this property to createEntityManager method of a composite persistence unit to pass properties to its member persistence units.
98,Examples
98,Example 5-23 shows how to use this property in a property map
98,Example 5-23 Using composite-unit.properties in a Property Map
98,Map props1 = new HashMap();
98,"props1.put(""javax.persistence.jdbc.user"", ""user1"");"
98,"props1.put(""javax.persistence.jdbc.password"", ""password1"");"
98,"props1.put(""javax.persistence.jdbc.driver"", ""oracle.jdbc.OracleDriver"");"
98,"props1.put(""javax.persistence.jdbc.url"", ""jdbc:oracle:thin:@oracle_db_url:1521:db"");"
98,Map props2 = new HashMap();
98,"props2.put(""javax.persistence.jdbc.user"", ""user2"");"
98,"props2.put(""javax.persistence.jdbc.password"", ""password2"");"
98,"props2.put(""javax.persistence.jdbc.driver"", ""com.mysql.jdbc.Driver"");"
98,"props2.put(""javax.persistence.jdbc.url"", "" jdbc:mysql://my_sql_db_url:3306/user2"");"
98,Map memberProps = new HashMap();
98,"memberProps.put(""memberPu1"", props1);"
98,"memberProps.put(""memberPu2"", props2);"
98,Map props = new HashMap();
98,"props.put(""eclipselink.logging.level"", ""FINEST"");"
98,"props.put(""eclipselink.composite-unit.properties"", memberProps);"
98,"EntityManagerFactory emf = Persistence.createEntityManagerFactory(""compositePu"", props);"
98,See Also
98,"For more information, see:"
98,"""composite-unit"""
98,concurrency.manager.waittime
98,This property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager. It control how much time loop wait before it try acquire lock for current thread again. It value is set above above 0 dead lock detection mechanism and related extended logging will be activated.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.waittime
98,Value
98,Description
98,Wait time
98,How much time loop wait before it try acquire lock for current thread again. Default value is 0 (unit is ms). Allowed values are: long
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.waittime in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.waittime"" value=""100"" />"
98,concurrency.manager.maxsleeptime
98,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager. It control how long we are willing to wait before firing up an exception.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.maxsleeptime
98,Value
98,Description
98,Wait time
98,It control how long we are willing to wait before firing up an exception. Default value is 40000 (unit is ms). Allowed values are: long
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.maxsleeptime in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.maxsleeptime"" value=""100"" />"
98,concurrency.manager.maxfrequencytodumptinymessage
98,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager and org.eclipse.persistence.internal.helper.ConcurrencyUtil. It control how frequently the tiny dump log message is created.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.maxfrequencytodumptinymessage
98,Value
98,Description
98,Wait time
98,It control how frequently the tiny dump log message is created. Default value is 40000 (unit is ms). Allowed values are: long
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.maxfrequencytodumptinymessage in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.maxfrequencytodumptinymessage"" value=""100000"" />"
98,concurrency.manager.maxfrequencytodumpmassivemessage
98,This system property in milliseconds can control thread management in org.eclipse.persistence.internal.helper.ConcurrencyManager and org.eclipse.persistence.internal.helper.ConcurrencyUtil. It control how frequently the massive dump log message is created.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.maxfrequencytodumpmassivemessage
98,Value
98,Description
98,Wait time
98,It control how frequently the massive dump log message is created. Default value is 60000 (unit is ms). Allowed values are: long
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.maxfrequencytodumpmassivemessage in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.maxfrequencytodumpmassivemessage"" value=""100000"" />"
98,concurrency.manager.allow.interruptedexception
98,"In the places where use this property normally if a thread is stuck it is because it is doing object building. Blowing the threads ups is not that dangerous. It can be very dangerous for production if the dead lock ends up not being resolved because the productive business transactions will become cancelled if the application has a limited number of retries to for example process an MDB. However, the code spots where we use this constant are not as sensible as when the write lock manager is starving to run commit."
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.allow.interruptedexception
98,Value
98,Description
98,true
98,(Default) If we want the to fire up an exception to try to get the current thread to release all of its acquired locks and allow other threads to progress.
98,false
98,If aborting frozen thread is not effective it is preferable to not fire the interrupted exception let the system.
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.allow.interruptedexception in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.allow.interruptedexception"" value=""true"" />"
98,concurrency.manager.allow.concurrencyexception
98,See valid values table.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.allow.concurrencyexception
98,Value
98,Description
98,true
98,(Default) If we want the to fire up an exception to try to get the current thread to realease all of its acquired locks and allow other threads to progress.
98,false
98,If aborting frozen thread is not effective it is preferable to not fire the concurrency exception let the system freeze and die and force the administration to kill the server. This is preferable to aborting the transactions multiple times without success in resolving the dead lock and having business critical messages that after 3 JMS retries are discarded out. Failing to resolve a dead lock can have terrible impact in system recovery unless we have infinite retries for the business transactions.
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.allow.concurrencyexception in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.allow.concurrencyexception"" value=""true"" />"
98,concurrency.manager.allow.readlockstacktrace
98,Collect debug/trace information during ReadLock acquisition.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for concurrency.manager.allow.readlockstacktrace
98,Value
98,Description
98,true
98,(Default) Collect debug/trace information during ReadLock acquisition.
98,false
98,Don't collect debug/trace information during ReadLock acquisition
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using concurrency.manager.allow.readlockstacktrace in persistence.xml
98,"<property name=""eclipselink.concurrency.manager.allow.readlockstacktrace"" value=""true"" />"
98,connection-pool
98,Use the eclipselink.connection-pool property to configure the various connection pool properties.
98,Values
98,Table 5-28 describes this persistence property's values.
98,Table 5-28 Valid Values for connection-pool
98,Value
98,Description
98,initial
98,Starting (initial) number of connections.
98,min
98,Minimum number of connections.
98,max
98,Maximum number of connections.
98,wait
98,Amount of time (in milliseconds) to wait for a connection from the pool.
98,url
98,URL of the JDBC for the connection.
98,shared
98,"For read connection pools, indicates that read connections are shared across threads."
98,jtaDataSource
98,"JTA DataSource name to use for the connection, if different than the default."
98,nonJtaDataSource
98,"Non-JTA DataSource name to use for the connection, if different than the default."
98,user
98,Username to use for this connection (if different than the default).
98,password
98,Password of the user for this connection (if different than the default).
98,Usage
98,"Append the name of the connection pool and property to be configured. If connection pool is specified, EclipseLink configures the default (write) pool."
98,Examples
98,Example 5-24 shows how to use this property in the persistence.xml file.
98,Example 5-24 Using connection-pool in persistence.xml
98,"<property name=""eclipselink.connection-pool.default.initial"" value=""1"" />"
98,"<property name=""eclipselink.connection-pool.node2.min"" value=""16""/>"
98,"<property name=""eclipselink.connection-pool.node2.max"" value=""16""/>"
98,"<property name=""eclipselink.connection-pool.node2.url"""
98,"value=""jdbc:oracle:thin:@node2:1521:orcl""/>"
98,See Also
98,"For more information, see:"
98,Partitioning Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Partitioning
98,"""Connection Pools"" in Understanding EclipseLink"
98,"""Connection Pooling"" in Solutions Guide for EclispeLink"
98,"""jdbc.cache-statements"""
98,"""connection-pool.read"""
98,"""connection-pool.sequence"""
98,connection-pool.read
98,Use the eclipselink.connection-pool.read property to configure a read connection pool for non-transaction read queries.
98,Values
98,Table 5-29 describes this persistence property's values.
98,Table 5-29 Valid Values for connection-pool.read
98,Value
98,Description
98,initial
98,Starting (initial) number of connection.
98,min
98,Minimum number of connections.
98,max
98,Maximum number of connections.
98,wait
98,Amount of time it takes to get connections from the pool.
98,url
98,URL of the JDBC connection.
98,shared
98,"For read connection pools, indicates that read connections are shared across threads."
98,jtaDataSource
98,"JTA DataSource name to use for the connection, if different than the default."
98,nonJtaDataSource
98,"Non-JTA DataSource name to use for the connection, if different than the default."
98,user
98,Username to use for this connection (if different than the default).
98,password
98,Password of the user for this connection (if different then the default).
98,Usage
98,"By default, EclipseLink does not use a separate read connection pool; the default pool is used for read queries."
98,Examples
98,Example 5-25 shows how to use this property in the persistence.xml file.
98,Example 5-25 Using connection-pool.read in persistence.xml
98,"<property name=""eclipselink.connection-pool.read.min"" value=""16""/>"
98,"<property name=""eclipselink.connection-pool.read.max"" value=""16""/>"
98,See Also
98,"For more information, see:"
98,"""Connection Pools"" in Understanding EclipseLink"
98,"""Connection Pooling"" in Solutions Guide for EclispeLink"
98,"""connection-pool"""
98,connection-pool.sequence
98,Use the eclipselink.connection-pool.sequence property to have the connection pool allocate generated IDs.
98,Values
98,Table 5-30 describes this persistence property's values.
98,Table 5-30 Valid Values for connection-pool.sequence
98,Value
98,Description
98,true
98,Uses the internal connection pool to pool connections from a datasource.
98,false
98,(Default) Does not use the internal connection pool to pool connections from a datasource.
98,Usage
98,"This is only required for TABLE sequencing. By default, EclipseLink does not use a separate sequence connection pool; the default pool is used for sequencing."
98,Examples
98,Example 5-26 shows how to use this property in the persistence.xml file.
98,Example 5-26 Using connection-pool.sequence in persistence.xml
98,"<property name=""eclipselink.connection-pool.sequence"" value=""true""/>"
98,See Also
98,"For more information, see:"
98,"""Connection Pools"" in Understanding EclipseLink"
98,"""Connection Pooling"" in Solutions Guide for EclispeLink"
98,"""connection-pool"""
98,create-ddl-jdbc-file-name
98,Use the eclipselink.create-ddl-jdbc-file-name property to specify the name of the DDL file generated by EclipseLink that contains the SQL statements to create tables for JPA entities.
98,Values
98,Table 5-31 describes this persistence property's values.
98,Table 5-31 Valid Values for create-ddl-jdbc-file-name
98,Value
98,Description
98,File name
98,A file name valid for your operating system.
98,You can prefix the file name with a file path if a concatenation of eclipselink.application-location + eclipselink.create-ddl-jdbc-file-name is valid for your operating system.
98,Usage
98,"If eclipselink.ddl-generation is set to create-tables or drop-and-create-tables, EclipseLink writes this file to the location specified by eclipselink.application-location."
98,Examples
98,See Example 5-27 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""application-location"""
98,"""ddl-generation"""
98,ddl-generation
98,Use the eclipselink.ddl-generation property to specify how EclipseLink generates DDL (Data Definition Language) for the database schema (tables and constraints) on deployment
98,"Note: EclipseLink does not support mixing EclipseLink defined and JPA defined DDL generation properties. If the eclipselink.ddl-generation property is specified, the javax.persistence.schema-generation.database.action, javax.persistence.schema-generation.scripts.action, and javax.persistence.sql-load-script-source JPA defined properties will be ignored."
98,Values
98,Table 5-32 describes this persistence property's values.
98,Table 5-32 Valid Values for ddl-generation
98,Value
98,Description
98,create-tables
98,EclipseLink will attempt to execute a CREATE TABLE SQL for each table.
98,"If the table already exists, EclipseLink will follow the default behavior of your specific database and JDBC driver combination (when a CREATE TABLE SQL is issued for an already existing table). In most cases an exception is thrown and the table is not created; the existing table will be used. EclipseLink will then continue with the next statement."
98,create-or-extend-tables
98,"EclipseLink will attempt to create tables. If the table exists, EclipseLink will add any missing columns."
98,drop-and-create-tables
98,"EclipseLink will attempt to DROP all tables, then CREATE all tables. If any issues are encountered, EclipseLink will follow the default behavior of your specific database and JDBC driver combination, then continue with the next statement."
98,This is useful in development if the schema frequently changes or during testing when the existing data needs to be cleared.
98,"Note: Using drop-and-create will remove all of the data in the tables when they are dropped. You should never use option on a production schema that has valuable data in the database. If the schema changed dramatically, there could be old constraints in the database that prevent the dropping of the old tables. This may require the old schema to be dropped through another mechanism."
98,none
98,(Default) No DDL generated; no schema generated.
98,Usage
98,You can use create-or-extend-tables only when eclipselink.ddl-generation.output-mode = database.
98,"If you are using persistence in a Java SE environment and would like to create the DDL files without creating tables, additionally define a Java system property INTERACT_WITH_DB and set its value to false."
98,DDL_GENERATION must be set in order for this property to take effect.
98,Examples
98,Example 5-27 shows how to use this property in the persistence.xml file.
98,Example 5-27 Using ddl-generation in persistence.xml
98,"<property name=""eclipselink.ddl-generation"" value=""drop-and-create-tables""/>"
98,"<property name=""eclipselink.create-ddl-jdbc-file-name"" value=""createDDL_ddlGeneration.jdbc""/>"
98,"<property name=""eclipselink.drop-ddl-jdbc-file-name"" value=""dropDDL_ddlGeneration.jdbc""/>"
98,"<property name=""eclipselink.ddl-generation.output-mode"" value=""both""/>"
98,Example 5-28 shows how to use this property in a property map.
98,Example 5-28 Using ddl-generation in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.DDL_GENERATION,"
98,PersistenceUnitProperties.DROP_AND_CREATE);
98,"propertiesMap.put(PersistenceUnitProperties.DDL_GENERATION_MODE,"
98,PersistenceUnitProperties.BOTH);
98,"propertiesMap.put(PersistenceUnitProperties.CREATE_JDBC_DDL_FILE, ""create.sql"");"
98,See Also
98,"For more information, see:"
98,"""create-ddl-jdbc-file-name"""
98,"""drop-ddl-jdbc-file-name"""
98,"""ddl-generation.output-mode"""
98,Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/DDL
98,ddl-generation.output-mode
98,Use the eclipselink.ddl-generation.output-mode property to specify where EclipseLink generates and writes the DDL.
98,Values
98,Table 5-33 describes this persistence property's values.
98,Table 5-33 Valid Values for ddl-generation.output-mode
98,Value
98,Description
98,both
98,DDL will be generated and written to both the database and a file.
98,"If eclipselink.ddl-generation is set to create-tables, then eclipselink.create-ddl-jdbc-file-name is written to eclipselink.application-location and executed on the database."
98,"If eclipselink.ddl-generation is set to drop-and-create-tables, then both eclipselink.create-ddl-jdbc-file-name and eclipselink.drop-ddl-jdbc-file-name are written to eclipselink.application-location, and both SQL files are executed on the database."
98,database
98,(Default) DDL will be generated and written to the database only.
98,sql-script
98,DDL will be generated and written to a file only.
98,"If eclipselink.ddl-generation is set to create-tables, then eclipselink.create-ddl-jdbc-file-name is written to eclipselink.application-location. It is not executed on the database."
98,"If eclipselink.ddl-generation is set to drop-and-create-tables, then both eclipselink.create-ddl-jdbc-file-name and eclipselink.drop-ddl-jdbc-file-name are written to eclipselink.application-location. Neither are executed on the database."
98,Usage
98,"You can only use ddl-generation.output-mode if you use ddl-generation. Then, you can optimally set other properties."
98,Examples
98,See Example 5-27 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""application-location"""
98,"""ddl-generation"""
98,"""create-ddl-jdbc-file-name"""
98,ddl.table-creation-suffix
98,Use the eclipselink.ddl.table-creation-suffix property to append a string to generated CREATE Table statements.
98,Values
98,Table 5-34 describes this property's values.
98,Table 5-34 Valid Values for ddl-generation.table-creation-suffix
98,Value
98,Description
98,value
98,The name of the suffix.
98,Usage
98,The ddl.generation property must be set.
98,Examples
98,Example 5-29 shows how to use this property in the persistence.xml file.
98,Example 5-29 Using ddl.table-creation-suffix in persistence.xml
98,"<property name=""eclipselink.ddl.table-creation-suffix"" value=""engine=InnoDB""/>"
98,See Also
98,"For more information, see:"
98,"""ddl-generation"""
98,deploy-on-startup
98,Use the eclipselink.deploy-on-startup property to configure deployment on startup (at the creation of the EntityManagerFactory) instead of occurring the first time an EntityManager is created.
98,Values
98,Table 5-35 describes this persistence property's values.
98,Table 5-35 Valid Values for delay-on-startup
98,Value
98,Description
98,true
98,"Causes a persistence unit to be created when the EntityManager is created, usually during deployment to a Java EE container or servlet container."
98,false
98,"(Default) The persistence unit is not initialized until the first EntityManager is created, or until metadata is required from the EntityManagerFactory."
98,Usage
98,"Using true may increase startup time of a JavaEE server, but will avoid the first request from hanging as the persistence unit is deployed."
98,Examples
98,Example 5-30 shows how to use this property in the peristence.xml file.
98,Example 5-30 Using deploy-on-startup in persistence.xml
98,"<property name=""eclipselink.deploy-on-startup"" value=""true"" />"
98,descriptor.customizer
98,"Use the eclipselink.descriptor.customizer property as a prefix for a property to configure a DescriptorCustomizer. Use this class's customize method, which takes an org.eclipse.persistence.descriptors.ClassDescriptor, to programmatically access advanced EclipseLink descriptor and mapping API for the descriptor associated with the JPA entity."
98,Values
98,Table 5-36 describes this persistence property's values.
98,Table 5-36 Valid Values for descriptor.customizer
98,Value
98,Description
98,name
98,Full name for a class that implements DescriptorCustomizer.
98,Usage
98,You cannot use multiple descriptor customizers.
98,Examples
98,Example 5-31 shows how to use this property in the peristence.xml file.
98,Example 5-31 Using descriptor.customizer in persistence.xml
98,"<property name=""eclipselink.descriptor.customizer.Order"""
98,"value=""acme.sessions.MyDesriptorCustomizer""/>"
98,Example 5-32 shows how to use this property with a property map.
98,Example 5-32 Using descriptor.customizer in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.DESCRIPTOR_CUSTOMIZER+"".Order"","
98,"""acme.sessions.MyDescriptorCustomizer"");"
98,See Also
98,"For more information, see:"
98,Understanding EclipseLink
98,"Section 8.1, ""Entity"" in the JPA Specification http://jcp.org/en/jsr/detail?id=220"
98,drop-ddl-jdbc-file-name
98,Use the eclipselink.drop-ddl-jdbc-file-name property to specify the name of the DDL file generated by EclipseLink that contains the SQL statements to drop tables for JPA entities.
98,Values
98,Table 5-37 describes this persistence property's values.
98,Table 5-37 Valid Values for drop-ddl-jdbc-file-name
98,Value
98,Description
98,File name
98,A file name valid for your operating system.
98,You can prefix the file name with a file path if a concatenation of eclipselink.application-location + eclipselink.create-ddl-jdbc-file-name is valid for your operating system.
98,Usage
98,"If eclipselink.ddl-generation is set to create-tables, EclipseLink writes this file to the location specified by eclipselink.application-location."
98,Examples
98,See Example 5-27 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""ddl-generation"""
98,exception-handler
98,"Use the eclipselink.exception-handler property to specify the EclipseLink exception handler class: an exception handler class that implements the org.eclipse.persistence.exceptions.ExceptionHandler interface. The class must provide a default, no-argument constructor."
98,Values
98,Table 5-38 describes this persistence property's values.
98,Table 5-38 Valid Values for exception-handler
98,Value
98,Description
98,ExceptionHandler class
98,"Use the handleException method of the class, which takes a java.lang.RuntimeException, to:"
98,Re-throw the exception
98,Throw a different exception
98,Retry the query or database operation
98,Usage
98,The ExceptionHandler class name must be fully qualified by its package name.
98,Examples
98,Example 5-33 shows how to use this property in the persistence.xml file.
98,Example 5-33 Using exception-handler in persistence.xml
98,"<property name=""eclipselink.exception-handler"""
98,"value=""my.package.MyExceptionHandler"">"
98,Example 5-34 shows how to use this extension in a property map.
98,Example 5-34 Using exception-handler in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.EXCEPTION_HANDLER_CLASS,"
98,"""my.package.MyExceptionHandler"");"
98,See Also
98,"For more information, see:"
98,"""orm.throw.exceptions"""
98,"""Sessions"" in Understanding EclipseLink"
98,"""Managing and Diagnosing Problems"" in Solutions Guide for EclispeLink"
98,exclude-eclipselink-orm
98,Use the eclipselink.exclude-eclipselink-orm property to exclude an EclipseLink ORM mapping file for a specific persistence unit.
98,Values
98,Table 5-39 describes this persistence property's values.
98,Table 5-39 Valid Values for exclude-eclipselink-orm
98,Value
98,Description
98,true
98,Does not use the eclipselink-orm.xml file.
98,false
98,(Default) EclipseLink uses the eclipselink-orm.xml file.
98,Usage
98,By default the first file found at the resource name: META-INF/eclipselink-orm.xml is processed and overrides configurations specified in annotations and standard mapping files.
98,Examples
98,Example 5-35 shows how to use this property in the persistence.xml file.
98,Example 5-35 Using exclude-eclipselink-orm in persistence.xml
98,"<property name=""eclipselink.exclude-eclipselink-orm"" value=""true""/>"
98,See Also
98,"For more information, see:"
98,"""Building Blocks of a EclipseLink Project"" in Understanding EclipseLink"
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,flush-clear.cache
98,Use the eclipselink.flush-clear.cache property to specify the EclipseLink EntityManager cache behavior when a clear method follows the flush method.
98,Values
98,Table 5-40 describes this persistence property's values.
98,Table 5-40 Valid Values for flush-clear.cache
98,Value
98,Description
98,Drop
98,EclipseLink drops the entire EntityManager cache.
98,"Although this is the fastest mode and uses the least memory, the shared cache may potentially contain stale data after performing the commit."
98,DropInvalidate
98,(Default) EclipseLink drops the entire EntityManager cache. Classes that have at least one updated or deleted object become invalid in the shared cache after performing the commit.
98,"This mode is slower than Drop, but as efficient (in terms of memory usage) and prevents stale data."
98,Merge
98,EclipseLink drops objects the EntityManager cache that have not been flushed.
98,"Although this mode leaves the shared cache in a perfect state after performing the commit, it is the least memory-efficient. In a very large transaction you may run out of memory."
98,Usage
98,"You can specify this property when creating an EntityManagerFactory (in the map passed to the createEntityManagerFactory method or in the persistence.xml file), or an EntityManager (in the map passed to the createEntityManager method)."
98,Note that the latter overrides the former.
98,Examples
98,Example 5-36 shows how to use this property in the persistence.xml file.
98,Example 5-36 Using flush-clear.cache in persistence.xml
98,"<property name=""eclipselink.flush-clear.cache"" value=""Drop""/>"
98,Example 5-37 shows how to use this extension in a property map.
98,Example 5-37 Using flush-clear.cache in a Property Map
98,import org.ecliplse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.FLUSH_CLEAR_CACHE,"
98,FlushClearCache.Drop);
98,See Also
98,"For more information, see:"
98,"""@Cache"""
98,"""Cache Coordination"" in Understanding EclipseLink"
98,"""Scaling EclipseLink Applications in Clusters"" in Solutions Guide for EclispeLink"
98,Cache Coordination Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/CacheCoordination
98,"""Clustering and Cache Coordination"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Caching/Coordination"
98,id-validation
98,Use the eclipselink.id-validation property to define which primary key components values are considered invalid.
98,Values
98,Table 5-41 describes this persistence property's values.
98,Table 5-41 Valid Values for id-validation
98,Value
98,Description
98,Negative
98,"Null, 0 and negative values are invalid for IDs extending Number and primitive int and long IDs."
98,None
98,EclipseLink performs no ID validation.
98,Null
98,Null is invalid All other values are valid.
98,Zero
98,"Null, 0 and negative values are invalid for primitive int and long IDs."
98,Usage
98,Identity and sequencing (with shouldAlwaysOverrideExistingValue configured as true) will override any existing ID value.
98,Examples
98,Example 5-38 shows how to use this property in the persistence.xml file.
98,Example 5-38 Using id-validation in persistence.xml
98,"<property name=""eclipselink.id-validation"" value=""NULL""/>"
98,See Also
98,"For more information, see:"
98,"""Persisting Objects"" in Understanding EclipseLink"
98,"""@PrimaryKey"""
98,jdbc.allow-native-sql-queries
98,"Use the eclipselink.jdbc.allow-native-sql-queries property to specify if user-defined (that is, native) SQL is allowed within a persistence unit."
98,Values
98,Table 5-42 describes this persistence property's values.
98,Table 5-42 Valid Values for jdbc.allow-native-sql-queries
98,Value
98,Description
98,true
98,(Default) EclipseLink allows native SQL.
98,false
98,EclipseLink does not allow native SQL.
98,Usage
98,"Within a multitenant, use this option to minimize the potential impact of revealing multitenant information. By default, any persistence unit with a multitenant entity causes EclipseLink to set eclipselink.jdbc.allow-native-sql-queries as false."
98,Examples
98,Example 5-39 shows how to use this property in the persistence.xml file.
98,Example 5-39 Using jdbc.allow-native-sql-queries in persistence.xml
98,"<property name=""eclipselink.jdbc.allow-native-sql-queries"" value=""false"" />"
98,See Also
98,"For more information, see:"
98,"""Querying"" in Understanding EclipseLink"
98,jdbc.batch-writing
98,Use the eclipselink.jdbc.batch-writing property to configure batch writing to optimize transactions with multiple write functions.
98,Values
98,Table 5-43 describes this persistence property's values.
98,Table 5-43 Valid Values for jdbc.batch-writing
98,Value
98,Description
98,jdbc
98,Use JDBC batch writing.
98,buffered
98,Do not use JDBC batch writing or the platform's native batch writing.
98,oracle-jdbc
98,"Use the Oracle platform's native batch writing. In a property map, use OracleJDBC."
98,Note: This requires an Oracle JDBC driver.
98,custom-class
98,A custom class that extends the BatchWritingMechanism class.
98,none
98,"(Default) Do not use batch writing (that is, turn it off)."
98,Usage
98,"Batch writing allows multiple heterogeneous dynamic SQL statements to be sent to the database as a single execution, or multiple homogeneous parameterized SQL statements to be executed as a single batch execution."
98,Note:
98,Not all JDBC drivers or databases support batch writing.
98,Use eclipselink.jdbc.batch-writing.size to specify the batch size.
98,Examples
98,Example 5-40 shows how to use this property in the persistence.xml file.
98,Example 5-40 Using jdbc.batch-writing in persistence.xml
98,"<property name=""eclipselink.jdbc.batch-writing"" value=""Oracle-JDBC""/>"
98,Example 5-41 shows how to use this property in a property map.
98,Example 5-41 Using jdbc.batch-writing in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.BATCH_WRITING,"
98,BatchWriting.OracleJDBC);
98,See Also
98,"For more information, see:"
98,"""jdbc.batch-writing.size"""
98,"""Batch Writing"" in Solutions Guide for EclispeLink"
98,jdbc.batch-writing.size
98,Use the eclipselink.jdbc.batch-writing.size property to configure the batch size used for batch writing.
98,Values
98,Table 5-44 describes this persistence property's values.
98,Table 5-44 Valid Values for jdbc.batch-writing.size
98,Value
98,Description
98,batch size
98,"For parameterized batch writing, this value is the number of statements to batch (default: 100)."
98,"For dynamic batch writing, this value is the size of the batched SQL buffer (default: 32k)."
98,Examples
98,Example 5-42 shows how to use this property in the persistence.xml file.
98,Example 5-42 Using jdbc.batch-writing.size in persistence.xml
98,"<property name=""eclipselink.jdbc.batch-writing.size"" value=""1000""/>"
98,See Also
98,"For more information, see:"
98,"""jdbc.batch-writing"""
98,"""Batch Writing"" in Solutions Guide for EclispeLink"
98,jdbc.cache-statements
98,Use the eclipselink.jdbc.cache-statements property to specify if JDBC statements should be cached.
98,Values
98,Table 5-45 describes this persistence property's values.
98,Table 5-45 Valid Values for jdbc.cache-statements
98,Value
98,Description
98,true
98,Enable internal statement caching.
98,false
98,(Default) Disable internal statement caching.
98,Usage
98,"You should use this property when using EclipseLink's internal connection pooling. See ""connection-pool"" for more information."
98,Examples
98,Example 5-43 shows how to use this property in the persistence.xml file.
98,Example 5-43 Using jdbc.cache-statements in persistence.xml
98,"<property name=""eclipselink.jdbc.cache-statements"" value=""false""/>"
98,Example 5-44 shows how to use this property in a property map.
98,Example 5-44 Using jdbc.cache-statements in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.CACHE_STATEMENTS, ""false"");"
98,See Also
98,"For more information, see:"
98,"""jdbc.cache-statements.size"""
98,"""connection-pool"""
98,"""Batch Writing"" in Solutions Guide for EclispeLink"
98,jdbc.cache-statements.size
98,Use the eclipselink.jdbc.cache-statements.size property to specify the number of statements held when using internal statement caching.
98,Values
98,Table 5-46 describes this persistence property's values.
98,Table 5-46 Valid Values for jdbc.cache-statements.size
98,Value
98,Description
98,size
98,A string value containing a positive integer or zero (Default: 50).
98,"The maximum value may vary, depending on your JDBC driver."
98,Examples
98,Example 5-45 shows how to use this property in the persistence.xml file.
98,Example 5-45 Using jdbc.cache-statements.size in persistence.xml
98,"<property name=""eclipselink.jdbc.cache-statements.size"" value=""100""/>"
98,Example 5-46 shows how to use this property in a property map.
98,Example 5-46 Using jdbc.cache-statements.size in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.CACHE_STATEMENTS_SIZE, ""100"");"
98,See Also
98,"For more information, see:"
98,"""jdbc.cache-statements"""
98,"""Batch Writing"" in Solutions Guide for EclispeLink"
98,jdbc.connector
98,Use the eclipselink.jdbc.connector property to define a custom connector to connect to the database.
98,Values
98,Table 5-47 describes this persistence property's values.
98,Table 5-47 Valid Values for jdbc.connector
98,Value
98,Description
98,Fully qualified class name
98,A class that implements the Connector interface.
98,Usage
98,"You can use this property to connect to a non-standard connection pool, or provide customized details on how to obtain a connection."
98,This property is not required when using a DataSource or JDBC DriverManager.
98,Examples
98,Example 5-47 shows how to use this property in the persistence.xml file.
98,Example 5-47 Using jdbc.connector in persistence.xml
98,"<property name=""eclipselink.jdbc.connector"" value=""package.MyConnector""/>"
98,jdbc.exclusive-connection.is-lazy
98,Use the eclipselink.jdbc.exclusive-connection.is-lazy property to specify if EclipseLink acquires write connections lazily.
98,Values
98,Table 5-48 describes this persistence property's values.
98,Table 5-48 Valid Values for jdbc.exclusive-connection.is-lazy
98,Value
98,Description
98,true
98,(Default) Acquire write connections lazily.
98,false
98,Do not acquire write connections lazily.
98,Examples
98,Example 5-48 shows how to use this property in the persistence.xml file.
98,Example 5-48 Using jdbc.exclusive-connection.is-lazy in persistence.xml
98,"<property name=""eclipselink.jdbc.exclusive-connection.is-lazy"" value=""false""/>"
98,Example 5-49 shows how to use this property in a property map.
98,Example 5-49 Using jdbc.exclusive-connection.is-lazy in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.EXCLUSIVE_CONNECTION_IS_LAZY,"
98,"""false"");"
98,See Also
98,"For more information, see:"
98,Auditinghttp://wiki.eclipse.org/EclipseLink/Examples/JPA/Auditing
98,jdbc.exclusive-connection.mode
98,Use the eclipselink.jdbc.exclusive-connection.mode property to specify when EclipseLink performs reads through the write connection.
98,Values
98,Table 5-49 describes this persistence property's values.
98,Table 5-49 Valid Values for jdbc.exclusive-connection.mode
98,Value
98,Description
98,Transactional
98,"(Default) Create an isolated client session if some or all entities require isolated cache, otherwise create a client session."
98,Notes:
98,EclipseLink keeps the connection exclusive for the duration of the transaction.
98,"Inside the transaction, EclipseLink performs all writes and reads through the exclusive connection."
98,"Outside the EclipseLink transaction, a new connection is acquired from the connection pool for each read and released back immediately after the query is executed."
98,Isolated
98,"Create an exclusive isolated client session if reading an isolated entity, otherwise raise an error."
98,Notes:
98,EclipseLink keeps the connection exclusive for the lifetime of the owning EntityManager.
98,"Inside the transaction, EclipseLink performs all writes and reads through the exclusive connection."
98,"Outside the EclipseLink transaction, only isolated entities are read through the exclusive connection. For non-isolated entities, EclipseLink acquires a new connection from the connection pool for each read and immediately releases the connection after executing the query."
98,Always
98,"Create an exclusive isolated client session if reading an isolated entity, otherwise create an exclusive client session."
98,Note: EclipseLink keeps the connection exclusive for the lifetime of the owning EntityManager and performs all writes and reads through the exclusive connection.
98,Usage
98,"You can set this property while creating either an EntityManagerFactory (either in the map passed to the createEntityManagerFactory method, or in the persistence.xml file), or an EntityManager (in the map passed to the createEntityManager method). Note that the latter overrides the former."
98,Examples
98,Example 5-50 shows how to use this property in the persistence.xml file.
98,Example 5-50 Using jdbc.exclusive-connection.mode in persitence.xml
98,"property name=""eclipselink.jdbc.exclusive-connection.mode"" value=""Always""/>"
98,Example 5-51 shows how to use this property in a property map.
98,Example 5-51 Using jdbc.exclusive-connection.mode in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.EXCLUSIVE_CONNECTION_MODE, ""Always"");"
98,See Also
98,"For more information, see:"
98,"""jdbc.exclusive-connection.is-lazy"""
98,"""Isolated Client Sessions"" in Understanding EclipseLink"
98,"""Connections"" in Understanding EclipseLink"
98,jdbc.native-sql
98,"Use the eclipselink.jdbc.native-sql property to specify if EclipseLink uses generic SLQ or includes platform-specific (that is, ""native"") SQL statements."
98,Values
98,Table 5-50 describes this persistence property's values.
98,Table 5-50 Valid Values for jdbc.native-sql
98,Value
98,Description
98,true
98,"(Default) Use platform-specific (""native"" ) SQL."
98,false
98,Use generic SQL.
98,Usage
98,"When using platform-specific SQL (eclipselink.jdbc.native-sql = true), EclipseLink uses platform-specific SQL to customize join syntax, date operators, using sequencing, and so on."
98,Examples
98,Example 5-52 shows how to use this property in the persistence.xml file.
98,Example 5-52 Using jdbc.native-sql in persistence.xml
98,"<property name=""eclipselink.jdbc.native-sql"" value=""false""/>"
98,Example 5-53 shows how to use this property in a property map.
98,Example 5-53 Using jdbc.native-sql in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.NATIVE_SQL, ""false"");"
98,See Also
98,"For more information, see:"
98,"""Querying"" in Understanding EclipseLink"
98,"""Query Languages"" in Understanding EclipseLink"
98,jdbc.property
98,Use the eclipselink.jdbc.property prefix to pass JDBC driver-specific connection properties to EclipseLink.
98,Usage
98,Append the JDBC driver-specific property name to this property prefix.
98,Examples
98,Example 5-54 shows how to use this property prefix in the persistence.xml file.
98,Example 5-54 Using jdbc.property in persistence.xml
98,"<property name=""eclipselink.jdbc.property.defaultRowPrefetch"" value=""25""/>"
98,See Also
98,"For more information, see:"
98,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
98,"""Introduction to Data Access"" in Understanding EclipseLink"
98,jdbc.sql-cast
98,"Use the eclipselink.jdbc.sql-cast property to specify if EclipseLink uses platform-specific (that is, ""native"") CAST SQL operations."
98,Note:
98,"Normally, casting is not required. Using it may cause issues."
98,Values
98,Table 5-51 describes this persistence property's values.
98,Table 5-51 Valid Values for jdbc.sql-cast
98,Value
98,Description
98,true
98,Use platform-specific CAST operations.
98,false
98,(Default) Do not use platform-specific CAST operations.
98,Examples
98,Example 5-55 shows how to use this property in the persistence.xml file.
98,Example 5-55 Using jdbc.sql-cast in persistence.xml
98,"<property name=""eclipselink.jdbc.sql-cast"" value=""true""/>"
98,jdbc.uppercase-columns
98,Use the eclipselink.jdbc.uppercase-columns property to force column names from the metadata to be uppercase.
98,Note:
98,"This parameter has been replaced by jpql.parser, which ensures that both sides use uppercase for comparisons."
98,Values
98,Table 5-52 describes this persistence property's values.
98,Table 5-52 Valid Values for jdbc.uppercase-columns
98,Value
98,Description
98,true
98,Forces all column names from the metadata to uppercase.
98,false
98,(Default) Does not force column names from the metadata to uppercase.
98,Usage
98,"When using native SQL queries, the JDBC metadata may return column names in lower case on some platforms. If the column names are uppercase in the mappings (default), they will not match. You should use this parameter to force all column names from the metadata to uppercase."
98,Examples
98,Example 5-56 shows how to use this parameter in the persistence.xml file.
98,Example 5-56 Using jdbc.uppercase-column-names in persistence.xml
98,"<property name=""eclipselink.jpa.uppercase-columns"" value=""true""/>"
98,See Also
98,"For more information, see:"
98,"""jpql.parser"""
98,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
98,"""Introduction to Data Access"" in Understanding EclipseLink"
98,jpql.parser
98,Use the eclipselink.jpql.parser property to configure the JPQL parser parameters.
98,Values
98,Table 5-53 describes this persistence property's values.
98,Table 5-53 Valid Values for jpql.parser
98,Value
98,Description
98,org.eclipse.persistence.internal.jpa.jpql.HermesParser
98,"(Default) Current parser, starting with EclipseLink 2.4, that provides extended JPQL support."
98,org.eclipse.persistence.queries.ANTLRQueryBuilder
98,"Old parser, used for backward compatibility (prior to EclipseLink 2.4)."
98,See Also
98,"For more information, see:"
98,"""jpql.validation"""
98,jpa.uppercase-column-names
98,Use the eclipselink.jpa.uppercase-column-names property to specify JPA processing to uppercase all column name definitions (simulating case insensitivity).
98,Values
98,Table 5-54 describes this persistence property's values.
98,Table 5-54 Valid Values for jpa.uppercase-column-names
98,Value
98,Description
98,true
98,"JDBC metadata returned from the database is returned in uppercase, ensuring fields are the same case. Sets jdbc.uppercase-columns to true."
98,false
98,(Default) Does not return JDBC metadata in uppercase.
98,Usage
98,Use this property to correct situations in which user-defined fields do not match the case returned by the database for native queries.
98,Examples
98,Example 5-57 shows how to use this property in the persistence.xml file.
98,Example 5-57 Using jpa.uppercase-column-names in persistence.xml
98,"<property name=""eclipselink.jpa.uppercase-column-names"" value=""true""/>"
98,See Also
98,"For more information, see:"
98,"""jdbc.uppercase-columns"""
98,"""Using EclipseLink with the Oracle Database"" in Solutions Guide for EclispeLink"
98,"""Introduction to Data Access"" in Understanding EclipseLink"
98,jpql.validation
98,Use the eclipselink.jpql.parser property to configure the JPQL parser validation level.
98,Values
98,Table 5-55 describes this persistence property's values.
98,Table 5-55 Valid Values for jpql.validation
98,Value
98,Description
98,EclipseLink
98,(Default) Allows EclipseLink JPAL extensions.
98,JPA 1.0
98,Allows valid JPA 1.0 JPQL only.
98,JPA 2.0
98,Allows valid JPA 2.0 JPQL only.
98,JPA 2.1
98,Allows valid JPA 2.1 JPQL only.
98,None
98,No JPQL validation.
98,Usage
98,This parameter applies only when eclipselink.jpql.parser is HermesParser.
98,Examples
98,Example 5-58 shows how to use this property in the persistence.xml file.
98,Example 5-58 Using jpql.validation in persistence.xml
98,"<property name=""eclipselink.jpql.validation"" value=""JPA 1.0""/>"
98,See Also
98,"For more information, see:"
98,"""jpql.parser"""
98,"""Java Persistence Query Language Extensions"""
98,logging.connection
98,Use the eclipselink.logging.connection property to specify if connections are logged.
98,Values
98,Table 5-56 describes this persistence property's values.
98,Table 5-56 Valid Values for logging.connection
98,Value
98,Description
98,true
98,(Default) Logs the connection name.
98,false
98,Does not log the connection name.
98,Usage
98,Using this parameter means that all connections are logged and not masked by the application code.
98,Examples
98,Example 5-59 shows how to use this parameter in the persistence.xml file.
98,Example 5-59 Using logging.connection in persistence.xml
98,"<property name=""eclipselink.logging.connection"" value=""false""/>"
98,See Also
98,"For more information, see:"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,"""logging.level"""
98,logging.exceptions
98,"Use the eclipselink.logging.exceptions property to specify if exceptions are logged when they are thrown, before returning the exception to the calling application."
98,Values
98,Table 5-57 describes this persistence property's values.
98,Table 5-57 Valid Values for logging.exceptions
98,Value
98,Description
98,true
98,(Default) Logs exceptions when they are thrown.
98,false
98,Does not log exceptions when they are thrown.
98,Usage
98,Using this property ensures that all exceptions are logged and not masked by the application code.
98,Examples
98,Example 5-60 shows how to use this property in the peristence.xml file.
98,Example 5-60 Using logging.exceptions in persistence.xml file
98,"<property name=""eclipselink.logging.exceptions"" value=""false"" />"
98,Example 5-61 shows how to use this property in a property map.
98,Example 5-61 Using logging.exceptions in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_EXCEPTIONS, ""false"");"
98,See Also
98,"For more information, see:"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,"""logging.level"""
98,logging.file
98,Use the eclipselink.logging.file property to specify a file location in which to output the log instead of the standard out.
98,Values
98,Table 5-58 describes this persistence property's values.
98,Table 5-58 Valid Values for logging.file
98,Value
98,Description
98,directory name
98,A string location to a directory in which you have write access. The location may be relative to your current working directory or an absolute location.
98,Usage
98,This property applies when used in a Java SE environment.
98,Examples
98,Example 5-62 shows how to use this property in the peristence.xml file.
98,Example 5-62 Using logging.file in persistence.xml file
98,"<property name=""eclipselink.logging.file"" value=""C:\myout\"" />"
98,Example 5-63 shows how to use this property in a property map.
98,Example 5-63 Using logging.file in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_FILE, ""C:\myout\"");"
98,See Also
98,"For more information, see:"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,logging.level
98,Use the eclipselink.logging.level property to specify a specific logging level and control the amount and detail that is emitted.
98,Values
98,Table 5-59 describes this persistence property's values.
98,Table 5-59 Valid Values for logging.level
98,Value
98,Description
98,OFF
98,Disables logging.
98,You may want to use OFF during production in order to avoid the overhead of logging.
98,SEVERE
98,"Logs exceptions indicating that EclipseLink cannot continue, as well as any exceptions generated during login. This includes a stack trace."
98,WARNING
98,"Logs exceptions that do not force EclipseLink to stop, including all exceptions not logged with SEVERE level. This does not include a stack trace."
98,INFO
98,"(Default) Logs the login/logout per sever session, including the user name. After acquiring the session, detailed information is logged."
98,CONFIG
98,"Logs only login, JDBC connection, and database information. You may want to use this log level at deployment time."
98,FINE
98,"Logs all SQL. You may want to use this log level during debugging and testing, but not at production time."
98,FINER
98,"Similar to WARNING, but includes stack trace. You may want to use this log level during debugging and testing, but not at production time."
98,FINEST
98,"Similar to FINER, but includes additional low level information. You may want to use this log level during debugging and testing, but not at production time."
98,ALL
98,Logs at the same level as FINEST.
98,Examples
98,Example 5-64 shows how to use this property in the peristence.xml file.
98,Example 5-64 Using logging.level in persistence.xml file
98,"<property name=""eclipselink.logging.level"" value=""OFF"" />"
98,Example 5-65 shows how to use this property in a property map.
98,Example 5-65 Using logging.level in a Property Map
98,import java.util.logging.Level;
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_LEVEL, Level.OFF);"
98,See Also
98,"For more information, see:"
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,logging.logger
98,Use the eclipselink.logging.logger property to define the type of logger to use.
98,Values
98,Table 5-60 describes this persistence property's values.
98,Table 5-60 Valid Values for logging.logger
98,Value
98,Description
98,Custom logger
98,Fully qualified class name of a custom logger which implements org.eclipse.persistence.logging.SessionLog.
98,JavaLogger
98,Uses java.util.logging.
98,ServerLogger
98,Integrates with the application server's logging.
98,DefaultLogger
98,"(Default) Uses EclipseLink's native logger, DefaultSessionLog."
98,Examples
98,Example 5-66 shows how to use this parameter in the persistence.xml file.
98,Example 5-66 Using logging.logger in persistence.xml
98,"<property name=""eclipselink.logging.logger"" value=""JavaLogger""/>"
98,Example 5-67 shows how to use this parameter in a property map.
98,Example 5-67 Using logging.logger in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_LOGGER,"
98,"""acme.loggers.MyCustomLogger"";"
98,See Also
98,"For more information, see:"
98,Logging examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,Custom logger http://wiki.eclipse.org/EclipseLink/Examples/JPA/CustomLogger
98,logging.parameters
98,Use the eclipselink.logging.parameters property to define if SQL bind parameters are included in exceptions and logs.
98,Note:
98,This parameter applies to bind parameters only. Parameters are always displayed when not using binding.
98,Values
98,Table 5-61 describes this persistence property's values.
98,Table 5-61 Valid Values for logging.parameters
98,Value
98,Description
98,true
98,(Default) Display the parameters.
98,false
98,Do not display the parameters.
98,Usage
98,"By default, when using logging.level of FINE (or greater), SQL bind parameters are displayed. Use this parameter to override the default behavior."
98,Examples
98,Example 5-58 shows how to use this parameter in the persistence.xml file.
98,Example 5-68 Using logging.parameters in persistence.xml
98,"<paramter name=""eclipselink.logging.parameters"" value=""false""/>"
98,See Also
98,"For more information, see:"
98,"""logging.level"""
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,logging.session
98,Use the eclipselink.logging.session property to specify if EclipseLink should include a session identifier in each log message.
98,Values
98,Table 5-62 describes this persistence property's values.
98,Table 5-62 Valid Values for logging.session
98,Value
98,Description
98,true
98,(Default) Log a session identifier.
98,false
98,Do not log a session identifier.
98,Usage
98,This setting is applicable to messages that require a database connection such as SQL and the transaction information to determine on which underlying session (if any) the message was sent.
98,Examples
98,Example 5-69 shows how to use this property in the peristence.xml file.
98,Example 5-69 Using logging.session in persistence.xml file
98,"<property name=""eclipselink.logging.session"" value=""false"" />"
98,Example 5-70 shows how to use this property in a property map.
98,Example 5-70 Using logging.session in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_SESSION, ""false"");"
98,See Also
98,"For more information, see:"
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""logging.level"""
98,logging.thread
98,Use the eclipselink.logging.thread property to specify if EclipseLink should include a thread identifier in each log message.
98,Values
98,Table 5-63 describes this persistence property's values.
98,Table 5-63 Valid Values for logging.thread
98,Value
98,Description
98,true
98,(Default) Log a thread identifier.
98,false
98,Do not log a thread identifier.
98,Usage
98,You should use this property when running multi-threaded applications. EclipseLink will include a hashcode of the thread.
98,Examples
98,Example 5-71 shows how to use this property in the peristence.xml file.
98,Example 5-71 Using logging.thread in persistence.xml file
98,"<property name=""eclipselink.logging.thread"" value=""false"" />"
98,Example 5-72 shows how to use this property in a property map.
98,Example 5-72 Using logging.thread in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_THREAD, ""false"");"
98,See Also
98,"For more information, see:"
98,"""logging.level"""
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,logging.timestamp
98,Use the eclipselink.logging.timestamp property to specify if EclipseLink should include a timestamp in each log message.
98,Values
98,Table 5-64 describes this persistence property's values.
98,Table 5-64 Valid Values for logging.timestamp
98,Value
98,Description
98,true
98,(Default) Log a timestamp.
98,false
98,Do not log a timestamp.
98,Examples
98,Example 5-73 shows how to use this property in the peristence.xml file.
98,Example 5-73 Using logging.timestamp in persistence.xml file
98,"<property name=""eclipselink.logging.timestamp"" value=""false"" />"
98,Example 5-74 shows how to use this property in a property map.
98,Example 5-74 Using logging.timestamp in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.LOGGING_TIMESTAMP, ""false"");"
98,See Also
98,"For more information, see:"
98,"""Configuring WebLogic Server to Expose EclipseLink Logging"" in Solutions Guide for EclispeLink"
98,Logging Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Logging
98,"""logging.level"""
98,metadata-source
98,Use the eclipselink.metadata-source property to specify the MetadataSource implementation EclipseLink uses to read metadata.
98,Values
98,Table 5-65 describes this persistence property's values.
98,Table 5-65 Valid Values for metadata-source
98,Value
98,Description
98,XML
98,Use XMLMetadataSource.
98,Custom metadata source
98,A custom class name which implements MetadataSource.
98,Usage
98,Use this property with eclipselink.metadata-source.xml.file to access an external mapping file at a fixed URL for a persistence unit.
98,Examples
98,Example 5-75 shows how to use this property in the persistence.xml file.
98,Example 5-75 Using metadata-source in persistence.xml
98,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
98,"<property name=""eclipselink.metadata-source.xml.file"" value=""c:/myfile.xml""/>"
98,See Also
98,"For more information, see:"
98,"""metadata-source.send-refresh-command"""
98,"""metadata-source.xml.file"""
98,"""metadata-source.xml.url"""
98,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
98,"""Extensible Entities"" http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/Extensible_Entities"
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,metadata-source.properties.file
98,"Use the eclipselink.metadata-source.properties.file property to specify the name of the metadata repository properties file to read from, using the classloader to find the resource."
98,Values
98,Table 5-66 describes this persistence property's values.
98,Table 5-66 Valid Values for metadata-repository.properties.file
98,Value
98,Description
98,Filename
98,Name of the metadata source XML file.
98,Usage
98,Use this property with eclipselink.metadata-source when using an XML repository.
98,Examples
98,Example 5-76 shows how to use this property in the persistence.xml file.
98,Example 5-76 Using metadata-source.properties.file in persistence.xml
98,"<property name=""eclipselink.metadata-source.properties.file"""
98,"value=""c:\myproperties.xml""/>"
98,See Also
98,"For more information, see:"
98,"""metadata-source"""
98,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,metadata-source.send-refresh-command
98,Use the eclipselink.metadata-source.send-refresh-command property with cache coordination for a clustered environment to control how EclipseLink sends RCM refresh metadata commands to the cluster.
98,Values
98,Table 5-67 describes this persistence property's values.
98,Table 5-67 Valid Values for metadata-source.send-refresh-command
98,Value
98,Description
98,true
98,"(Default) To propogate refresh commands to the cluster, you must configure RCM and use the eclipselink.deploy-on-startup property."
98,false
98,Does not propagate refresh commands to the cluster.
98,Usage
98,"If cache coordination is configured and the session is deployed on startup, this property controls the sending of RCM refresh metadata commands to the cluster."
98,These commands will cause the remote instances to refresh their metadata.
98,Examples
98,Example 5-77 shows how to use this property in the persistence.xml file.
98,Example 5-77 Using metadata-source.send-refresh-command in persistence.xml
98,"<property name=""eclipselink.metadata-source-refresh-command"" value=""false""/>"
98,Example 5-78 shows how to use this property in a property map.
98,Example 5-78 Using metadata-source-refresh-command in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.METADATA_SOURCE_RCM_COMMAND, ""false"");"
98,See Also
98,"For more information, see:"
98,"""metadata-source"""
98,"""deploy-on-startup"""
98,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,metadata-source.xml.file
98,"Use the eclipselink.metadata-repository.xml.file property to specify the name of the metadata repository XML file to read from, using the classloader to find the resource."
98,Values
98,Table 5-68 describes this persistence property's values.
98,Table 5-68 Valid Values for metadata-source.xml.file
98,Value
98,Description
98,filename
98,Metadata repository.xml file.
98,Usage
98,Use this property with the eclipselink.metadata-source property when using an XML repository.
98,Examples
98,Example 5-79 shows how to use this property in the persistence.xml file.
98,Example 5-79 Using metadata-source.xml.file in persistence.xml
98,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
98,"<property name=""eclipselink.metadata-source.xml.file"" value=""c:/myfile.xml""/>"
98,See Also
98,"For more information, see:"
98,"""metadata-source"""
98,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,metadata-source.xml.url
98,Use the eclipselink.metadata-source.xml.url property to specify the location of an external mapping file.
98,Values
98,Table 5-69 describes this persistence property's values.
98,Table 5-69 Valid Values for metadata-source.xml.url
98,Value
98,Description
98,url
98,Specifies the metadata repository of the XML URL.
98,Usage
98,The metadata-source property must be set to XML.
98,Examples
98,Example 5-75 shows how to use this property in the persistence.xml file.
98,Example 5-80 Using metadata-source.xml.url in persistence.xml
98,"<property name=""eclipselink.metadata-source"" value=""xml""/>"
98,"<property name=""eclipselink.metadata-source.xml.url"" value=""http://myfile.xml""/>"
98,See Also
98,"For more information, see:"
98,"""metadata-source"""
98,Metadata Source Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/MetadataSource/
98,"""Using an External Metadata Source"" in Solutions Guide for EclispeLink"
98,multitenant.tenants-share-cache
98,Use the eclipselink.multitenant.tenants-share-cache property to specify if multitenant entities will share the L2 cache.
98,Values
98,Table 5-70 describes this persistence property's values.
98,Table 5-70 Valid Values for multitenant.tenants-share-cache
98,Value
98,Description
98,true
98,Multitenant entities will use an protected cache.
98,false
98,(Default) Multitenant entities will use an isolated cache.
98,Usage
98,WARNING:
98,"When this setting is false, queries that use the cache may return data from other tenants when using the PROTECTED setting."
98,Examples
98,Example 5-81shows how to use this property in the persistence.xml file.
98,Example 5-81 Using multitenant.tenants-share-cache in persistence.xml
98,"<property name=""eclipselink.multitenant.tenants-share-cache"" value=""true"" />"
98,Example 5-82 shows how to use this property in a property map.
98,Example 5-82 Using multitenant.tenants-share-cache in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.MULTITENANT_TENANTS_SHARE_CACHE,"
98,"""true"");"
98,See Also
98,"For more information, see:"
98,"""@Multitenant"""
98,Multitenant examples: http://wiki.eclipse.org/EclipseLink/Examples/JPA/Multitenant
98,"""Using Multitenancy"" in Solutions Guide for EclispeLink"
98,multitenant.tenants-share-emf
98,Use the eclipselink.multitenant.shared-emf property to specify if multitenant entities will be used within a shared entity manager factory.
98,Values
98,Table 5-71 describes this persistence property's values.
98,Table 5-71 Valid Values for multitenant.tenants-share-emf
98,Value
98,Description
98,true
98,(Default) Multitenant entities will be used.
98,false
98,Specify a unique session name.
98,Usage
98,"When setting it to false, you are required to provide a unique session name."
98,Examples
98,Example 5-83 shows how to use this property in the persistence.xml file.
98,Example 5-83 Using multitenant.tenants-share-emf in persistence.xml
98,"<property name=""eclipselink_multitenant_tenants_share_emf"" value=""true"" />"
98,See Also
98,"For more information, see:"
98,"""@Multitenant"""
98,Multitenant examples: http://wiki.eclipse.org/EclipseLink/Examples/JPA/Multitenant
98,"""Using Multitenancy"" in Solutions Guide for EclispeLink"
98,nosql.connection-factory
98,Use the eclipselink.nosql.connection-factory property to specify the JNDI name of a JCA ConnectionFactory or a JCA ConnectionFactory class name that connects to the NoSQL data-source.
98,Values
98,Table 5-72 describes this persistence property's values.
98,Table 5-72 Valid Values for nosql.connection-factory
98,Value
98,Description
98,connection factory
98,JNDI name or class name of the JCA Connection Factory.
98,Usage
98,"This property allows the JCA ConnectionFactory to be used with a NoSql or EIS adapter for a NoSQL datasource (that is, a non-relationship datasource such as a legacy database, NoSQL database, XML database, transactional and messaging systems, or ERP systems)."
98,Examples
98,Example 5-84 shows how to use this property in the persistence.xml file.
98,Example 5-84 Using nosql.connection-factory in persistence.xml
98,"<property name=""eclipselink.nosql.connection-factory"""
98,"value=""MyConnectionFactory"" />"
98,See Also
98,"For more information, see:"
98,"""@NoSql"""
98,"""nosql.property"""
98,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
98,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
98,"""Using NoSQL Databases"" in Understanding EclipseLink"
98,"""Using EclipseLink with Nonrelational Databases"" in Solutions Guide for EclispeLink"
98,nosql.connection-spec
98,Use the eclipselink.nosql.connection-spec property to specify an EISConnectionSpec class name that defines how to connect to the NoSQL datasource.
98,Values
98,Table 5-73 describes this persistence property's values.
98,Table 5-73 Valid Values for nosql.connection-spec
98,Value
98,Description
98,classname
98,IESConnectionSpec classname
98,Usage
98,"This property allows the JCA ConnectionFactory to be used with a NoSql or EIS adapter for a NoSQL datasource (that is, a non-relationship datasource such as a legacy database, NoSQL database, XML database, transactional and messaging systems, or ERP systems)."
98,Examples
98,See Example 5-85 for information on how to use this property.
98,See Also
98,"For more information, see:"
98,"""@NoSql"""
98,"""nosql.property"""
98,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
98,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
98,"""Using NoSQL Databases"" in Understanding EclipseLink"
98,"""Using EclipseLink with Nonrelational Databases"" in Solutions Guide for EclispeLink"
98,nosql.property
98,Use the eclipselink.nosql.property property to set NoSQL-specific connection properties.
98,Values
98,Table 5-74 describes this persistence property's values.
98,Table 5-74 Valid Values for nosql.property
98,Value
98,Description
98,property name
98,A NoSQL property.
98,Usage
98,Append the NoSQL-specific property name to this property.
98,Examples
98,Example 5-85 shows how to use this property in the persistence.xml file.
98,Example 5-85 Using nosql.property in persistence.xml
98,"<persistence xmlns=""http://java.sun.com/xml/ns/persistence"""
98,"xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"""
98,"xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence persistence_2_0.xsd"""
98,"version=""2.0"">"
98,"<persistence-unit name=""acme"" transaction-type=""RESOURCE_LOCAL"">"
98,<provider>org.eclipse.persistence.jpa.PersistenceProvider</provider>
98,<exclude-unlisted-classes>false</exclude-unlisted-classes>
98,<properties>
98,"<property name=""eclipselink.target-database"""
98,"value=""org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform""/>"
98,"<property name=""eclipselink.nosql.connection-spec"""
98,"value=""org.eclipse.persistence.nosql.adapters.mongo.MongoConnectionSpec""/>"
98,"<property name=""eclipselink.nosql.property.mongo.port"" value=""27017,"
98,"27017""/>"
98,"<property name=""eclipselink.nosql.property.mongo.host"" value=""host1,"
98,"host2""/>"
98,"<property name=""eclipselink.nosql.property.mongo.db"" value=""acme""/>"
98,</properties>
98,</persistence-unit>
98,</persistence>
98,See Also
98,"For more information, see:"
98,"""@NoSql"""
98,"""Using Non-SQL Databases"" in Understanding EclipseLink"
98,NoSQL Persistence Units http://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Advanced_JPA_Development/NoSQL/Persistence_Units
98,Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/NoSQL
98,"""nosql.connection-factory"""
98,"""nosql.connection-spec"""
98,oracle.proxy-type
98,Use the eclipselink.oracle.proxy-type property to specify the proxy type to be passed to the OracleConnection.openProxySession method.
98,Values
98,Table 5-75 describes this persistence property's values.
98,Table 5-75 Valid Values for oracle.proxy-type
98,Value
98,Description
98,USER_NAME
98,This type uses a user name for authentication when creating a proxy connection.
98,DISTINGUISED_NAME
98,This type uses a distinguished name for authentication when creating a proxy connection.
98,CERTIFICATE
98,This type uses a digital certificate for authentication when creating a proxy connection.
98,Usage
98,This property requires Oracle JDBC version 10.1.0.2 or later and eclipselink.target-database must be configured to use Oracle9 or later.
98,"Typically, you should set this property into EntityManager, through a createEntityManager method or by using proprietary setProperties method on EntityManagerImpl. This causes EntityManager to use proxy connection for writing and reading inside transaction."
98,"If proxy-type and the corresponding proxy property set into EntityManagerFactory, all connections created by the factory will be proxy connections."
98,Examples
98,Example 5-86 shows how to use the property with EntityManager.
98,Example 5-86 Using eclipselink.oracle.proxy-type with EntityManager
98,Map emProperties = new HashMap();
98,"emProperties.put(""eclipselink.oracle.proxy-type"","
98,OracleConnection.PROXYTYPE_USER_NAME);
98,"emProperties.put(OracleConnection.PROXY_USER_NAME, ""john"");"
98,EntityManager em = emf.createEntityManager(emProperties);
98,With injection:
98,"entityManager.setProperty(”eclipselink.oracle.proxy-type”,"
98,OracleConnection.PROXYTYPE_USER_NAME);
98,"entityManager.setProperty(OracleConnection.PROXY_USER_NAME, ”john”);"
98,See Also
98,"For more information, see:"
98,"""target-database"""
98,Oracle Proxy Authentication Example http://wiki.eclipse.org/EclipseLink/Examples/JPA/Oracle/Proxy
98,Auditing example http://wiki.eclipse.org/EclipseLink/Examples/JPA/Auditing
98,orm.throw.exceptions
98,Use the eclipselink.orm.throw.exceptions property to specify if EclipseLink throws an exception or logs a warning when encountering a problem with any of the files in the <mapping-file> element of the persistence.xml file.
98,Values
98,Table 5-76 describes this persistence property's values.
98,Table 5-76 Valid Values for orm.throw.exceptions
98,Value
98,Description
98,true
98,(Default) Throw an exception.
98,false
98,Log a warning only.
98,Examples
98,Example 5-87 shows how to use this property in the persistence.xml file.
98,Example 5-87 Using orm.throw.exceptions in persistence.xml
98,"<property name=""oracle.orm.throw.exceptions"" value=""false""/>"
98,Example 5-88 shows how to use this property in a property map.
98,Example 5-88 Using orm.throw.exceptions in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.ECLIPSELINK_ORM_THROW_EXCEPTIONS,"
98,"""false"");"
98,See Also
98,"For more information, see:"
98,"""exception-handler"""
98,orm.validate.schema
98,Use the orm.validate.schema property to override orm.xml schema validation from its default value of false.
98,Values
98,Table 5-77 describes this persistence property's values.
98,Table 5-77 Valid Values for orm.validate.schema
98,Value
98,Description
98,true
98,Enables schema velidation on on orm.xml file.
98,false
98,(Default) No schema validation is performed on the orm.xml file.
98,Usage
98,Use orm.validate.schema to enable orm.xml schema validation.
98,Examples
98,Example 5-89 shows how to use this property in the persistence.xml file.
98,Example 5-89 Using orm.validate.schema in persistence.xml
98,"<property name=""eclipselink.orm.validate.schema"" value=""true""/>"
98,Example 5-90 shows how to use this property in a property map.
98,Example 5-90 Using orm.validate.schema in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.ORM_VALIDATE_SCHEMA, ""true"");"
98,partitioning
98,"Use the eclipselink.partitioning property to set the default PartitioningPolicy for a persistence unit. The value must be the name of an existing, defined PartitioningPolicy."
98,Values
98,Table 5-78 describes this persistence property's values.
98,Table 5-78 Valid Values for partitioning
98,Value
98,Description
98,name
98,"An existing, defined PartitioningPolicy."
98,Usage
98,Use this property to partition data for a class across multiple difference databases or across a database cluster such as Oracle RAC. Partitioning may provide improved scalability by allowing multiple database machines to service requests.
98,"If multiple partitions are used to process a single transaction, use JTA (Java Transcription API) for proper XA transaction support."
98,Examples
98,Example 5-91 shows how to use this property in the persistence.xml file.
98,Example 5-91 Using partitioning in persistence.xml
98,"<property name=""eclipselink.partitioning"" value=""Replicate"" />"
98,See Also
98,"For more information, see:"
98,Partitioning Examples http://wiki.eclipse.org/EclipseLink/Examples/JPA/Partitioning
98,"""@Partitioning"""
98,partitioning.callback
98,"Use the eclipselink.partitioning.callback property to integrate an external DataSource's affinity support, such as UCP."
98,Values
98,Table 5-79 describes this persistence property's values.
98,Table 5-79 Valid Values for eclipselink.partitioning.callback
98,Value
98,Description
98,value
98,A class that implements the DataPartitioningCallBack interface.
98,Usage
98,The value must be set to the full class name.
98,Examples
98,Example 5-92 shows how to use this property in the persistence.xml file.
98,Example 5-92 Using partitioning.callback in persistence.xml
98,"<property name=""eclipselink.partitioning.callback"""
98,"value=""mypacakge.MyDataPartitioningCallback""/>"
98,Example 5-93 shows how to use this property in a property map.
98,Example 5-93 Using partitioning.callback in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PARTITIONING_CALLBACK,"
98,"""mypackage.MyDataPartitioningCallback"");"
98,persistence-context.close-on-commit
98,Use the eclipselink.persistence-context.close-on-commit property to specify if the EntityManager will be closed or not used after commit (not extended).
98,Values
98,Table 5-80 describes this persistence property's values.
98,Table 5-80 Valid Values for persistence-context.close-on-commit
98,Value
98,Description
98,true
98,Closes the EntityManager after a commit.
98,false
98,(Default) Does not close the EntityManager after a commit.
98,Usage
98,"For a container-managed EntityManager and most managed applications, you normally set this property to false. This setting avoids additional performance overhead of resuming the persistence context after a commit() transaction."
98,"The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. Alternatively, to apply the property to specific EntityManagers, pass it to createEntityManager method."
98,Examples
98,Example 5-94 shows how to use this property in the persistence.xml file.
98,Example 5-94 Using persistence-context.close-on-commit in persistence.xml
98,"<property name=""eclipselink.persistence-context.close-on-commit"" value=""true""/>"
98,Example 5-95 shows how to use this property in a property map.
98,Example 5-95 Using persistence-context.close-on-commit in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_CLOSE_ON_COMMIT,"
98,"""true"");"
98,persistence-context.commit-without-persist-rules
98,"Use the eclipselink.persistence-context.commit-without-persist-rules property to specify if the EntityManager will search all managed objects and persist any related non-managed new objects that are found, ignoring any absence of CascadeType.PERSIST settings."
98,Values
98,Table 5-81 describes this persistence property's values.
98,Table 5-81 Valid Values for persistence-context.commit-without-persist-rules
98,Value
98,Description
98,true
98,Cascades Entity life-cycle Persist operations to related entities and uses the CascadeType.PERSIST settings.
98,false
98,(Default) Does not cascase Entitiy life-cycle Persist operations to related entities and does not use the CascadeType.PERSIST settings.
98,Usage
98,Setting this property to true replicates the traditional EclipseLink native functionality.
98,Examples
98,Example 5-96 shows how to use this property in the persistence.xml file.
98,Example 5-96 Using persistence-context.commit-without-persist-rules in persistence.xml
98,"<property name=""eclipse.persistence-context.commit-without-persist-rules"""
98,"value=""true""/>"
98,Example 5-97 shows how to use this property in a property map.
98,Example 5-97 Using persistence-context.commit-without-persist-rules in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.PERSISTENCE_CONTEXT_COMMIT_WITHOUT_PERSIST_RULES,"
98,"""true"");"
98,persistence-context.flush-mode
98,Use the eclipselink.persistence-context.flush-mode property to configure the EntityManager FlushMode to be set as a persistence property and specify when flushing occurs.
98,Values
98,Table 5-82 describes this persistence property's values.
98,Table 5-82 Valid Values for persistence-context.flush-mode
98,Value
98,Description
98,auto
98,(Default) Flushing occurs at query execution.
98,commit
98,Flushing occurs at transaction commit.
98,Usage
98,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to the createEntityManager method.
98,Examples
98,Example 5-98 shows how to use this property in the persistence.xml file.
98,Example 5-98 Using persistence-context.flush-mode in persistence.xml
98,"<property name=""eclipselink.persistence-context.flush-mode"" value=""commit"" />"
98,Example 5-99 shows how to use this property in a property map.
98,Example 5-99 Using persistence-context.flush-mode in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_FLUSH_MODE,"
98,"""false"");"
98,See Also
98,"For more information, see:"
98,"""flush"""
98,"""Enhancing Performance"" in Solutions Guide for EclispeLink"
98,persistence-context.persist-on-commit
98,Use the eclipselink.persistence-context.persist-on-commit property to specify if the EntityManager searches all managed objects and persists any related non-managed new objects that are cascade persist. This can be used to avoid the cost of performing this search if persist is always used for new objects.
98,Values
98,Table 5-83 describes this persistence property's values.
98,Table 5-83 Valid Values for persistence-context.persist-on-commit
98,Value
98,Description
98,true
98,(Default) Searches and persists related non-managed new objects that are cascade persist.
98,false
98,Does not search and persist related non-managed new objects that are cascade persist.
98,Usage
98,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to createEntityManager method.
98,Examples
98,Example 5-100 shows how to use this property in the persistence.xml file.
98,Example 5-100 Using persistence-context.persist-on-commit in persistence.xml
98,"<property name=""eclipselink.persistence-context.persist-on-commit"" value=""false""/>"
98,Example 5-101 show how to use this property in a property map.
98,Example 5-101 Using persistence-context.persis-on-commit in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_PERSIST_ON_COMMIT,"
98,"""false"");"
98,persistence-context.reference-mode
98,"Use the eclipselink.persistence-context.reference-mode property to specify if hard or soft (that is, weak) references are used within the Persistence Context."
98,Values
98,Table 5-84 describes this persistence property's values.
98,Table 5-84 Valid Values for persistence-context.reference-mode
98,Value
98,Description
98,hard
98,(Default) EclipseLink references all objects through hard references. These objects will not be available for garbage collection until the referencing artifact (such as the persistence context or unit of work) is released/cleared or closed.
98,weak
98,"References to objects supporting active attribute change tracking (see ""@ChangeTracking"") will be held by weak references. That is, any object no longer referenced directly or indirectly will be available for garbage collection. When a change is made to a change-tracked object, that object is moved to a hard reference and will not be available for garbage collection until flushed."
98,Note: Any changes that have not been flushed in these entities will be lost.
98,"New and removed objects, as well as objects that do not support active attribute change tracking, will also be held by hard references and will not be available for garbage collection."
98,force_weak
98,"All objects, including non-change-tracked objects, are to be held by weak references. When a change is made to a change-tracked object (see ""@ChangeTracking""), that object is moved to a hard reference and will not be available for garbage collection until flushed. However, any objects that do not support active attribute change tracking may be garbage collected before their changes are flushed to a database, which can potentially result in a loss of changes."
98,New and removed objects will be held by hard references and will not be available for garbage collection.
98,Usage
98,The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. To apply the property to specific EntityManagers pass it to createEntityManager method.
98,Examples
98,Example 5-102 shows how to use this property in a persistence.xml file.
98,Example 5-102 Using persistence-context.reference-mode in persistence.xml
98,"<property name=""eclipselink.persistence-context.reference-mode"""
98,"value=""FORCE_WEAK""/>"
98,Example 5-103 shows how to use this property in a property map.
98,Example 5-103 Using persistence-context.reference-mode in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PERSISTENCE_CONTEXT_REFERENCE_MODE,"
98,ReferenceMode.FORCE_WEAK);
98,See Also
98,"For more information, see:"
98,"""@ChangeTracking"""
98,persistenceunits
98,"Use the eclipselink.persistenceunits property to specify the set of persistence unit names that will be processed when generating the canonical model. By default, EclipseLink uses all persistence units available in all persistence XML files."
98,Values
98,Table 5-85 describes this persistence property's values.
98,Table 5-85 Valid Values for persistenceunits
98,Value
98,Description
98,names
98,A comma separated list of persistence units
98,"Note: When specifying multiple persistence units, you cannot include a comma ( , ) in the name of a persistence unit."
98,Examples
98,Example 5-104 shows how to use this property in the persistence.xml file.
98,Example 5-104 Using persistenceunits in persistence.xml
98,"<property name=""eclipselink.persistenceunits"" value=""mypu1, mypu2""/>"
98,persistencexml
98,"Use the eclipselink.persistencexml property to specify the full resource name in which to look for the persistence XML files. If omitted, EclipseLink uses the default location: META-INF/persistence.xml."
98,Note:
98,"Currently, this property is used only for the canonical model generator."
98,Values
98,Table 5-86 describes this persistence property's values.
98,Table 5-86 Valid Values for persistencexml
98,Value
98,Description
98,resource name
98,Location of the persistence.xml file.
98,Usage
98,"This property is only used by EclipseLink when it is locating the configuration file. When used within an EJB/Spring container in container-managed mode, the locating and reading of this file is done by the container and will not use this configuration."
98,"If you want to change the default location, use persisencexml.default."
98,Examples
98,Example 5-105 shows how to use this property in the persistence.xml file.
98,Example 5-105 Using persistencexml in persistence.xml
98,"<property name=""eclipselink.persistencexml"" value=""resources/persistence.xml""/>"
98,See Also
98,"For more information, see:"
98,"""persisencexml.default"""
98,persisencexml.default
98,Use the eclipselink.persistencexml.default property to specify the default resource location where the persistence.xml configuration file is located. The default location is META-INF/persistence.xml.
98,Values
98,Table 5-87 describes this persistence property's values.
98,Table 5-87 Valid Values for persistencexml.default
98,Value
98,Description
98,resource location
98,Default resource location of the persistence.xml file.
98,Examples
98,Example 5-106 shows how to use this property in a property map.
98,Example 5-106 Using persistencexml.default in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.ECLIPSELINK_PERSISTENCE_XML_DEFAULT,"
98,"""resources/persistence.xml"");"
98,profiler
98,Use the eclipselink.profiler property to specify which performance profiler to use in order to capture runtime statistics.
98,Values
98,Table 5-88 describes this persistence property's values.
98,Table 5-88 Valid Values for profiler
98,Value
98,Description
98,NoProfiler
98,(Default) Do not use a performance profiler.
98,PerformationMonitor
98,Use EclipseLink performance monitor org.eclipse.persistence.tools.profiler.PerformanceMonitor.
98,PerformanceProfiler
98,Use EclipseLink performance profiler (org.eclipse.persistence.tools.profiler.PerformanceProfiler).
98,QueryMonitor
98,Monitor query executions and cache hits (org.eclipse.persistence.tools.profiler.QueryMonitor class).
98,This option provides a simple low-overhead means for measuring performance of query executions and cache hits. You may want to use this option for performance analysis in a complex system.
98,DMSProfiler
98,Use org.eclipse.persistence.tools.profiler.oracle.DMSPerformanceProfiler. This property is specific to the Oracle Dynamic Monitoring Service (DMS).
98,Custom profiler
98,Specify a custom profiler class name which implements SessionProfiler and provides a no-argument constructor.
98,Examples
98,Example 5-107 shows how to use this property in the persistence.xml file.
98,Example 5-107 Using profiler in persistence.xml
98,"<property name=""eclipselink.profiler"" value=""PerformanceProfiler""/>"
98,Example 5-108 shows how to use this property in a property map.
98,Example 5-108 Using profiler in a Property Map
98,import org.eclipse.persistence.config.ProfilerType;
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.PROFILER,"
98,ProfilerType.PerformanceProfiler);
98,See Also
98,"For more information, see:"
98,session.customizer
98,"Use the eclipselink.session.customizer property to specify a session customizer class that implements the org.eclipse.persistence.config.SessionCustomizer interface. The class must provide a default, no argument constructor."
98,Values
98,Table 5-89 describes this persistence property's values.
98,Table 5-89 Valid Values for session.customizer
98,Value
98,Description
98,class name
98,Fully qualified class name of a SessionCustomizer class.
98,Usage
98,You can use the customize method of the class (which takes an org.eclipse.persistence.sessions.Session) to programmatically access advanced EclipseLink session API. You can use the session customizer class to define multiple session event listeners.
98,Examples
98,Example 5-109 shows how to use this property in the persistence.xml file.
98,Example 5-109 Using session.customizer in persistence.xml
98,"<property name=""eclipselink.session.customizer"""
98,"value=""acme.sessions.MySessionCustomizer""/>"
98,Example 5-110 shows how to use this property in a property map.
98,Example 5-110 Using session.customizer in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.SESSION_CUSTOMIZER,"
98,"""acme.sessions.MySessionCustomizer"");"
98,See Also
98,"For more information, see:"
98,"""session-event-listener"""
98,session.include.descriptor.queries
98,Use the eclipselink.session.include.descriptor.queries property to specify whether all descriptor named queries are copied to the session for use by the entity manager.
98,Values
98,Table 5-90 describes this persistence property's values.
98,Table 5-90 Valid Values for session.include.descriptor.queries
98,Value
98,Description
98,true
98,Copying is enabled.
98,false
98,(Default) Copying is disabled.
98,Examples
98,Example 5-111 shows how to use this property in the persistence.xml file.
98,Example 5-111 Using session.include.descriptor.queries in persistence.xml
98,"<property name=""eclipselink.session.include.descriptor.queries"" value=""true""/>"
98,Example 5-112 shows how to use this property in a property map.
98,Example 5-112 Using session.include.descriptor.queries in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.INCLUDE_DESCRIPTOR_QUERIES, ""true"");"
98,session-event-listener
98,Use the eclipselink.session-event-listener property to specify a descriptor event listener to be added during bootstrapping.
98,Values
98,Table 5-91 describes this persistence property's values.
98,Table 5-91 Valid Values for session-event-listener
98,Value
98,Description
98,Class name
98,A qualified class name for a class that implements the org.eclipse.persistence.sessions.SessionEventListener interface.
98,Usage
98,"To define multiple event listener, you can use a session.customizer class."
98,Examples
98,Example 5-113 shows how to use this property in a persistence.xml file.
98,Example 5-113 Using session-event-listener in persistence.xml
98,"<property name=""eclipselink.session-event-listener"""
98,"value=""mypackage.MyClass.class""/>"
98,Example 5-113 shows how to use this property in a property map.
98,Example 5-114 Using session-event-listener in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.SESSION_EVENT_LISTENER_CLASS,"
98,"""mypackage.MyClass.class"");"
98,See Also
98,"For more information, see:"
98,"""session.customizer"""
98,session-name
98,Use the eclipselink.session-name property to configure a unique name to use when storing the singleton server session within the SessionManager.
98,Values
98,Table 5-92 describes this persistence property's values.
98,Table 5-92 Valid Values for session.name
98,Value
98,Description
98,Name
98,"Unique session name to use instead of the default, EclipseLink-generated session name."
98,Usage
98,"By default, EclipseLink generates a unique session name. You can provide a custom, unique, session name with this property."
98,"When using a sessions-xml file, you must include this session name as the name of the session in the sessions-xml file."
98,Examples
98,Example 5-115 shows how to use this property in the persistence.xml file.
98,Example 5-115 Using session-name in persistence.xml
98,"<property name=""eclipselink.session-name"" value=""MySession""/>"
98,Example 5-116 shows how to use this property in a property map.
98,Example 5-116 Using session-name in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.SESSION_NAME, ""MySession"");"
98,See Also
98,"For more information, see:"
98,"""sessions-xml"""
98,sessions-xml
98,Use the eclipselink.sessions-xml property to use a specified native sessions.xml configuration file (which references a project.xml file) to load configuration and mapping information instead of JPA annotations or EclipseLink XML (as shown in Figure 5-1).
98,Values
98,Table 5-93 describes this persistence property's values.
98,Table 5-93 Valid Values for sessions-xml
98,Value
98,Description
98,configuration file
98,"The resource name of the sessions XML file. If you do not specify the value for this property, it will not be used."
98,Usage
98,"You can use the eclipselink.sessions-xml property as an alternative to using annotations and deployment XML. With this property, EclipseLink builds an in-memory EclipseLink session and project based on this metadata (as shown in Figure 5-1). You can acquire a persistence manager and use it, having defined all entities and so on using only EclipseLink sessions.xml."
98,Figure 5-1 Using the eclipselink.sessions-xml Persistence Property
98,"Description of ""Figure 5-1 Using the eclipselink.sessions-xml Persistence Property"""
98,Examples
98,Example 5-117 shows how to use this property in a persistence.xml file.
98,Example 5-117 Using sessions-xml in the persistence.xml file
98,"<property name=""eclipselink.sessions-xml"" value=""mysession.xml""/>"
98,Example 5-118 shows how to use this property in a property map.
98,Example 5-118 Using sessions-xml in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.SESSIONS_XML, ""mysession.xml"");"
98,See Also
98,"For more information, see:"
98,"""Overriding and Merging"""
98,target-database
98,"Use the eclipselink.target-database property to specify the database to use, controlling custom operations and SQL generation for the specified database."
98,Values
98,Table 5-94 describes this persistence property's values.
98,Table 5-94 Valid Values for target-database
98,Value
98,Description
98,Defined in the TargetDatabase class or a fully qualified class name that extends DatabasePlatform
98,Specify your database:
98,Attunity
98,Auto (Default): EclipseLink attempts to access the database and the JDBC metadata to determine the target database.
98,Cloudscape
98,"Database: Use a generic database, if your target database is not listed and your JDBC driver does not support the metadata required for Auto."
98,DB2
98,DB2Mainframe
98,DBase
98,Derby
98,HSQL
98,Informix
98,JavaDB
98,MaxDB
98,MySQL
98,MySQL4
98,Oracle
98,Oracle10
98,Oracle11
98,Oracle8
98,Oracle9
98,PointBase
98,PostgreSQL
98,SQLAnywhere
98,SQLServer
98,Sybase
98,Symfoware
98,TimesTen
98,Usage
98,"If eclipselink.validation-only = true, you cannot use an Auto class name or short name."
98,Examples
98,Example 5-119 shows how to use this property in the persistence.xml file.
98,Example 5-119 Using target-database in persistence.xml
98,"<property name=""eclipselink.target-database"" value=""Oracle""/>"
98,"<property name=""eclipselink.target-database"""
98,"value=""org.eclipse.persistence.platform.database.HSQLPlatform""/>"
98,Example 5-120 shows how to use this property in a property map.
98,Example 5-120 Using target-database in a Property Map
98,import org.eclipse.persistence.config.TargetDatabase;
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.TARGET_DATABASE, TargetDatabase.Oracle);"
98,See Also
98,"For more information, see:"
98,"""validation-only"""
98,target-server
98,"Use the eclipselink.target-server property to configure the ServerPlatform that will be used to enable integration with a host container. If this property is not specified, the runtime will attempt to detect which ServerPlatform should be used. If detection fails, Default will be used."
98,Values
98,Table 5-95 describes this persistence property's values.
98,Table 5-95 Valid Values for target-server
98,Value
98,Description
98,Defined in the TargetServer class
98,Specify your application server:
98,JBoss: JBoss Application Server
98,OC4J: OC4J persistence provider
98,SAPNetWeaver_7_1: SAP NetWeaver Application Server 7.1 (and higher)
98,SunAS9: Sun Application Server 9
98,WebLogic: Oracle WebLogic Server
98,WebLogic_10: Oracle WebLogic Server 10
98,WebLogic_9: Oracle WebLogic Server 9
98,WebSphere: IBM WebSphere
98,WebSphere_6_1: IBM WebSphere 6.1
98,WebSphere_7: IBM WebSphere 7
98,WebSphere_Liberty: IBM WebSphere Liberty
98,Default (TargetServer.None)
98,Usage
98,"In addition to the supplied values, you can specify a custom server platform by supply the full class name for the platform."
98,Specifying a name of the class implementing ExternalTransactionController sets CustomServerPlatform with this controller.
98,Examples
98,Example 5-121 shows how to use this property in a persistence.xml file.
98,Example 5-121 Using target-server in persistence.xml
98,"<property name=""eclipselink.target-server"" value=""OC4J_10_1_3""/>"
98,Example 5-122 shows how to use this property in a property map.
98,Example 5-122 Using target-server in a Property Map
98,import org.eclipse.persistence.config.TargetServer;
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertiesMap.put(PersistenceUnitProperties.TARGET_SERVER,"
98,TargetServer.OC4J_10_1_3);
98,See Also
98,"For more information, see:"
98,temporal.mutable
98,"Use the eclipselink.temporal.mutable property to configure the default for detecting changes to the temporal field (Date, Calendar)."
98,Values
98,Table 5-96 shows this persistence property's values.
98,Table 5-96 Valid Values for temporal.mutable
98,Value
98,Description
98,true
98,Changes to the object are detected. Disables weaving of attribute change tracking.
98,false
98,(Default) Changes to the object itself are not detected.
98,Usage
98,"By default, it is assumed that temporal fields are replaced, and the temporal object is not changed directly."
98,Examples
98,Example 5-123 shows how to use this property in the persistence.xml file.
98,Example 5-123 Using temporal.mutable in persistence.xml
98,"<property name=""eclipselink.temporal.mutable"" value=""true""/>"
98,Example 5-124 shows how to use this property in a property map.
98,Example 5-124 Using temporal.mutable in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.TEMPORAL_MUTABLE,"
98,"""true"");"
98,tenant-id
98,Use the eclipselink.tenant-id property to specify the default context property used to populate multitenant entities.
98,Values
98,Table 5-97 describes this persistence property's values.
98,Table 5-97 Valid Values for tenant-id
98,Value
98,Description
98,value
98,Name of the default context property.
98,Usage
98,This is a default multitenant property that can be used on its own or with other properties defined by you. You are not obligated to use this property. You are free to specify your own.
98,Examples
98,Example 5-125 shows how to use this property in the persistence.xmlfile.
98,Example 5-125 Using tenant-id in persistence.xml
98,"<property name=""eclipselink.tenant-id"" value=""Oracle""/>"
98,Example 5-126 shows how to use this property in a property map.
98,Example 5-126 Using tenant-id in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.MULTI_TENANT_PROPERTY_DEFAULT,"
98,"""Oracle"");"
98,transaction.join-existing
98,"Use the eclipselink.transaction.join-existing property to force the persistence context to read through the JTA-managed (""write"") connect"
98,ion in case there is an active transaction.
98,Values
98,Table 5-98 describes this persistence property's values.
98,Table 5-98 Valid Values for transaction.join-existing
98,Value
98,Description
98,true
98,Forces the persistence context to read through the JTA-managed connection.
98,false
98,(Default) Does not force the persistence context to read through the JTA-managed connection.
98,Usage
98,"The property set in persistence.xml or passed to createEntityManagerFactory affects all EntityManagers created by the factory. If the property set to true, objects read during transaction will not be placed into the shared cache unless they have been updated. Alternatively, to apply the property only to some EntityManagers, pass it to createEntityManager method."
98,Examples
98,Example 5-127 shows how to use this property in the persistence.xml file.
98,Example 5-127 Using transaction.join-existing in persistence.xml
98,"<property name=""eclipselink.transaction.join-existing"" value=""true""/>"
98,Example 5-128 shows how to use this property in a property map.
98,Example 5-128 Using transaction.join-existing in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.TRANSACTION_JOIN_EXISTING,"
98,"""true"");"
98,See Also
98,"For more information, see:"
98,"""Automated Tuning"" in Solutions Guide for EclispeLink"
98,tuning
98,The eclipselink.tuning property selects the type of tuner to use to configure the persistence unit.
98,Values
98,Table 5-99 describes this persistence property's values.
98,Table 5-99 Valid Values for tuning
98,Value
98,Description
98,standard
98,(Default) Uses the standard tuner and does not change any of the default configuration settings.
98,safe
98,Configures the persistence unit for debugging. This disables caching and several performance optimizations. The purpose is to provide a simplified development and debugging configuration.
98,custom tuner
98,Specifies the full class name of an implementation of the org.eclipse.persistence.tools.tuning.SessionTuner interface.
98,Usage
98,Use automated tuning to set multiple configuration properties as part of a single flag to perform dynamic tuning during different steps of application deployment.
98,Examples
98,Example 5-129 shows how to use this property in the persistence.xml file.
98,Example 5-129 Using tuning in persistence.xml
98,"<property name=""eclipselink.tuning"" value=""safe""/>"
98,validate-existence
98,Use the eclipselink.validate-existence property to specify if EclipseLink should verify an object's existence on persist().
98,Values
98,Table 5-100 describes this persistence property's values.
98,Table 5-100 Valid Values for validate-existence
98,Value
98,Description
98,true
98,EclipseLink verifies the object's existence.
98,false
98,"(Default) EclipseLink assumes the object is new, if it is not in the persistence context."
98,Usage
98,EclipseLink will throw an error if a validated object is not in the persistence context.
98,Examples
98,Example 5-130 shows how to use this property in the persistence.xml file.
98,Example 5-130 Using validate-existence in persistence.xml
98,"<property name=""eclipselink.validate-existence"" value=""true""/>"
98,Example 5-131 shows how to use this property in a proptery map.
98,Example 5-131 Using validate-existence in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.VALIDATE-EXISTENCE,"
98,"""true"");"
98,validation-only
98,Use the eclipselink.validation-only property to validate deployments by initializing descriptors but not connecting to the data source.
98,Values
98,Table 5-101 describes this persistence property's values.
98,Table 5-101 Valid Values for validation-only
98,Value
98,Description
98,true
98,EclipseLink will initialize the descriptors but not log in.
98,false
98,(Default) EclipseLink will initialize the descriptors and log in.
98,Usage
98,"When setting eclipselink.validation-only to true, you must also configure eclipselink.target-database with a non-Auto class name or a short name."
98,Examples
98,Example 5-132 show how to use this property in the persistence.xml file.
98,Example 5-132 Using validation-only in persistence.xml
98,"<property name=""eclipselink.validation-only"" value=""true""/>"
98,Example 5-133 shows how to use this property in a property map.
98,Example 5-133 Using validation-only in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,"propertyMap.put(PersistenceUnitProperties.VALIDATION_ONLY,"
98,"""true"");"
98,See Also
98,"For more information, see:"
98,"""target-database"""
98,weaving
98,"Use the eclipselink.weaving property to specify if EclipseLink weaves the entity classes. EclipseLink JPA uses weaving to enhance JPA entities for such things as lazy loading, change tracking, fetch groups, and internal optimizations."
98,Values
98,Table 5-102 describes this persistence property's values.
98,Table 5-102 Valid values for weaving
98,Value
98,Description
98,true
98,Weave the entity classes dynamically.
98,false
98,Do not weave the entity classes.
98,static
98,Weave the entity classes statically.
98,Examples
98,Example 5-134 shows how to use this property in the persistence.xml file.
98,Example 5-134 Using weaving in persistence.xml
98,"<property name=""eclipse.weaving"" value=""false""/>"
98,Example 5-135 shows how to use this property in a property map.
98,Example 5-135 Using weaving in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.WEAVING, ""false"");"
98,See Also
98,"For more information, see:"
98,"""Using Weaving"" in Understanding EclipseLink"
98,"""Enhancing Performance"" in Solutions Guide for EclipseLink"
98,"""weaving.changetracking"""
98,"""weaving.eager"""
98,"""weaving.fetchgroups"""
98,"""weaving.internal"""
98,"""@ChangeTracking"""
98,weaving.changetracking
98,Use the eclipselink.weaving.changetracking persistence property to:
98,Enable AttributeLevelChangeTracking through weaving.
98,Permit only classes with all mappings to change.
98,Permit tracking to enable change tracking. Mutable basic attributes prevent change tracking.
98,This property is enabled only when weaving is enabled.
98,Values
98,Table 5-103 describes this persistence property's values.
98,Table 5-103 Valid Values for weaving.changetracking
98,Value
98,Description
98,true
98,(Default) Enables this property.
98,false
98,Disables this property.
98,Examples
98,Example 5-136 shows how to use this property in the persistence.xml file.
98,Example 5-136 Using weaving.changetracking in persistence.xml
98,"<property name=""eclipse.weaving.changetracking"" value=""false""/>"
98,Example 5-137 shows how to use this property in a property map.
98,Example 5-137 Using weaving.changetracking in a Property Map
98,import org.eclipselink.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(Persistence.Unit.Properties.WEAVING_CHANGETRACKING,"
98,"value=""false"");"
98,See Also
98,"For more information, see:"
98,"""weaving"""
98,weaving.eager
98,Use the eclipselink.weaving.eager property to specify if EclipseLink uses indirection on eager relationships.
98,Values
98,Table 5-104 describes this persistence property's values.
98,Table 5-104 Valid Values for weaving.eager
98,Value
98,Description
98,true
98,Enables indirection on eager relationships through weaving.
98,false
98,(Default) Disables indirection on eager relationships through weaving.
98,Usage
98,"One-to-one and many-to-one mappings, even when configured with FetchType.EAGER, will effectively become ""lazy."""
98,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
98,Examples
98,Example 5-138 shows how to use this property in the persistence.xml file.
98,Example 5-138 Using weaving in persistence.xml
98,"<property name=""eclipselink.weaving.eager"" value=""true""/>"
98,Example 5-139 shows how to use this extension in a property map
98,Example 5-139 Using weaving in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.WEAVING_EAGER, ""true"");"
98,See Also
98,"For more information, see:"
98,"""weaving"""
98,weaving.fetchgroups
98,"Use the eclipselink.weaving.fetchgroups property to enable FetchGroups through weaving. When this is enabled, lazy direct mapping is supported, as well as descriptor and query-level FetchGroups."
98,FetchGroups allow partial objects to be read and written. Access to un-fetched attributes refreshes (fully-fetches) the object.
98,This property is only considered when weaving is enabled.
98,Values
98,Table 5-105 describes this persistence property's values.
98,Table 5-105 Valid Values for weaving.fetchgroups
98,Value
98,Description
98,true
98,(Default) Enables FetchGroups through weaving.
98,false
98,Disables FetchGroups through weaving.
98,Examples
98,Example 5-140 shows how to use this property in the persistence.xml file.
98,Example 5-140 Using weaving.fetchgroups in persistence.xml
98,"<property name=""eclipselink.weaving.fetchgroups value=""false""/>"
98,Example 5-141 shows how to use this property in a property map.
98,Example 5-141 Using weaving.fetchgroups in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.WEAVING_FETCHGROUPS, ""false"")"
98,See Also
98,"For more information, see:"
98,"""weaving"""
98,weaving.internal
98,Use the eclipselink.weaving.internal property to specify if EclipseLink uses internal optimizations through weaving.
98,Values
98,Table 5-106 describes this persistence property's values.
98,Table 5-106 Valid Values for weaving.internal
98,Value
98,Description
98,true
98,(Default) Enables internal optimizations through weaving.
98,false
98,Disables internal optimizations through weaving.
98,Usage
98,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
98,Examples
98,Example 5-142 shows how to use this property in the persistence.xml file.
98,Example 5-142 Using weaving in persistence.xml
98,"<property name=""eclipselink.weaving.internal"" value=""false""/>"
98,Example 5-143 shows how to use this property in a property map.
98,Example 5-143 Using weaving in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.WEAVING_INTERNAL, ""false"");"
98,See Also
98,"For more information, see:"
98,"""weaving"""
98,weaving.lazy
98,Use the eclipselink.weaving.lazy property to specify if EclipseLink uses lazy one-to-one and many-to-one mappings.
98,Values
98,Table 5-107 describes this persistence property's values.
98,Table 5-107 Valid Values for weaving.lazy
98,Value
98,Description
98,true
98,(Default) Enables lazy one-to-one and many-to-one mappings through weaving.
98,false
98,Disables lazy one-to-one and many-to-one mappings through weaving.
98,Usage
98,"You can use this extension only if weaving is configured to true or static. See ""weaving"" for more information."
98,Examples
98,Example 5-144 shows how to use this property in the persistence.xml file.
98,Example 5-144 Using weaving.lazy in persistence.xml
98,"<property name=""eclipselink.weaving.lazy"" value=""false""/>"
98,Example 5-145 shows how to use this property in a property map.
98,Example 5-145 Using weaving.lazy in a Property Map
98,import org.eclipse.persistence.config.PersistenceUnitProperties;
98,propertiesMap.put
98,"(PersistenceUnitProperties.WEAVING_LAZY, ""false"");"
98,See Also
98,"For more information, see:"
98,"""weaving"""
98,Comments
98,Copyright © 2014 by The Eclipse Foundation under the Eclipse Public License (EPL)
