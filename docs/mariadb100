filenr,sentence
2,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks - MariaDB.org
2,Skip to content
2,Download
2,Documentation
2,Contribute
2,Server Fest
2,Events
2,Sponsor
2,Blog
2,Planet MariaDB Server
2,About
2,Back
2,"Latest releases 11.3.1 (RC), 11.2.2, 11.1.3, 11.0.4, 10.11.6, 10.10.7, 10.6.16,  10.5.23, 10.4.32. Vote in our poll!"
2,Facebook
2,Twitter
2,LinkedIn
2,Reddit
2,Instagram
2,Search for:
2,Download
2,Documentation
2,Contribute
2,Server Fest
2,Events
2,Sponsor
2,Blog
2,Planet MariaDB Server
2,About
2,Back
2,Search for:
2,Facebook
2,Twitter
2,LinkedIn
2,Reddit
2,Instagram
2,Post navigation
2,Older Posts Older post:Newer Posts Newer posts:
2,"Tags: chatgpt, openai"
2,Post navigation
2,Older Posts Older post:Newer Posts Newer posts:
2,Ask ChatGPT: MariaDB Performance Tuning: Tips and Tricks
2,Written by Written by Ian Gilfillan 2023-04-212023-04-21
2,"This is the next in the series of posts exploring ChatGPT and its insights about MariaDB. The series began with ChatGPT, OpenAI and MariaDB Foundation, followed by Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for Beginners."
2,"This week’s topic, suggested by ChatGPT itself, was “MariaDB Performance Tuning”. So how did it do?"
2,"MariaDB is undoubtedly one of the most popular open-source database management systems available today, with a strong focus on performance and ease of use. Whether you are a developer, database administrator, or an IT professional, ensuring that your MariaDB installation performs efficiently is crucial. In this blog, we will discuss some key tips and tricks for tuning your MariaDB server, improving its performance in various environments."
2,Identifying Bottlenecks
2,"Before diving into any optimization techniques, it’s crucial to identify the areas where performance might need improvement. Some common tools that will help you to pinpoint performance bottlenecks are:"
2,"MariaDB’s slow query log: This log records queries that take longer-than-expected to execute, helping you identify problematic SQL code."
2,"Performance Schema: A feature in MariaDB that allows you to monitor various metrics, such as query performance and resource usage."
2,mtop and mytop: These are real-time monitoring tools that show the MySQL server’s status and queries being executed at any given time.
2,"I thought we were talking about MariaDB here? And as far as I know, mtop was last updated in 2004, so I’m not sure this would be very useful. A pet bugbear about ChatGPT is its reluctance to provide links. It sometimes behaves a bit like an ignorant loudmouth, confidently spouting forth on something or other, but when you challenge them to provide evidence, nothing is forthcoming. At least it doesn’t launch into ad hominems. Its advice to use the slow query log and the performance schema can be helpful, but links would be useful. So, here are links to find out more about the slow query log, Performance Schema, as well as mtop and mytop."
2,Using these tools will give you an idea of the specific areas in which your MariaDB server needs tuning.
2,General Performance Tips
2,Optimize SQL queries
2,"Poorly written queries can lead to significant performance hits, and as such, properly optimizing SQL code is a critical step in improving the performance of your MariaDB server."
2,"Use EXPLAIN: The EXPLAIN command helps you understand how the MariaDB optimizer executes a query, allowing you to identify potentially slow operations and improve the query strategy."
2,"Proper indexing: Design and use indexes effectively to optimize search performance. Keep in mind that while more indexes can speed up SELECT queries, they may also slow down INSERT, UPDATE, and DELETE operations."
2,"Avoid using SELECT *: Instead of using SELECT * (which retrieves all columns), specify only the required columns in your queries."
2,"True, but a rather lacking in specifics, and again, links to actually find out anything useful. Try EXPLAIN and Optimizing and Indexes. To save repetition, I’ll add links directly into GPT’s text for the rest of this post."
2,Update MariaDB to the latest version
2,"MariaDB receives regular updates and improvements, so keeping your installation up-to-date is an essential step in getting the best performance possible. New releases might include feature enhancements, bug fixes, and security patches, all of which contribute to better overall performance."
2,Server-specific Performance Tips
2,Configuring InnoDB Variables
2,"InnoDB is the default storage engine for MariaDB, and adjusting its settings can significantly improve the performance of your server. Some critical InnoDB settings to consider are:"
2,"innodb_buffer_pool_size: Allocate more memory to the InnoDB buffer pool to reduce disk I/O. Typically, this value should be set to at least 70-80% of the available memory."
2,"innodb_log_file_size: A larger log file size reduces the need for frequent log rotations, thus improving write performance. However, a log size that is too large might slow down the recovery process after a crash."
2,Tweak Query Cache Variables
2,"MariaDB’s query cache is a feature that stores the results of executed queries in memory, allowing the server to serve identical queries without re-executing them."
2,"query_cache_size: Increase the size of the query cache if you have spare memory, which can improve performance for repeat queries."
2,"query_cache_limit: This value sets the maximum size of an individual query’s result that can be cached. A larger limit might cache more queries, but it might also cause more fragmentation."
2,"This isn’t very good advice. There’s a reason the query cache is disabled by default, as in many environments it actually slows things down. It may help in certain contexts, but you’d want to test this, and not blindly enable it."
2,Hardware Optimizations
2,"When it comes to improving database performance, hardware optimization is often overlooked. Some quick hardware-related adjustments you can make include:"
2,Use fast SSDs for your storage: SSDs can significantly improve read/write performance and offer better reliability than traditional hard disk drives.
2,"Increase available memory: More RAM will allow you to allocate larger InnoDB buffer pools and query caches, reducing I/O operations and improving performance."
2,"In conclusion, performance tuning for MariaDB is a continuous process. Regularly monitoring and optimizing your queries, staying up-to-date with new releases, and fine-tuning server settings as needed will contribute to the overall performance of your MariaDB installation. By following the tips and tricks outlined in this article, you are well on your way to maintaining a high-performance MariaDB server."
2,"Overall, very light on details, but a reasonably helpful high-level overview with just enough wrong to lead someone following it down some dead ends. My guess is that it overly prioritises some well-respected, but now dated, sources. It would help to train GPT to prioritise the official MariaDB Server documentation."
2,Post navigation
2,Older Posts Older posts: Ask ChatGPT – Getting Started with MariaDB: A Comprehensive Guide for BeginnersNewer Posts Newer posts: Code of Conduct Update
2,Contact
2,Facebook
2,Twitter
2,LinkedIn
2,Reddit
2,Instagram
2,Governance
2,Maintenance policy
2,Security policy
2,Privacy policy
2,Logos and badges
2,Usage statistics
2,Service providers
2,Copyright @ 2009 - 2023 MariaDB Foundation.
8,hash_join_cardinality optimizer_switch Flag - MariaDB Knowledge Base
8,Search
8,Products
8,Services
8,Resources
8,About
8,Contact
8,Login
8,Copyright © 2023 MariaDB. All rights reserved.
8,Knowledge Base
8,Contact
8,Login
8,Search
8,Products
8,Services
8,Pricing
8,Resources
8,About Us
8,Download
8,Knowledge Base
8,» MariaDB Server Documentation
8,» High Availability & Performance Tuning
8,» Optimization and Tuning
8,» Query Optimizations
8,» hash_join_cardinality optimizer_switch Flag
8,Home
8,Open Questions
8,MariaDB Server
8,MariaDB MaxScale
8,MariaDB ColumnStore
8,Connectors
8,History
8,Source
8,Flag as Spam / Inappropriate
8,Translate
8,Created
8,"5 months, 2 weeks ago"
8,Modified
8,"3 months, 3 weeks ago"
8,Type
8,article
8,Status
8,active
8,License
8,CC BY-SA / Gnu FDL
8,History
8,Comments
8,EditAttachments
8,No attachments exist
8,Product Versions
8,MariaDB starting with 10.6.13
8,hash_join_cardinality optimizer_switch Flag
8,MariaDB starting with 10.6.13The hash_join_cardinality optimizer_switch flag was added in
8,"MariaDB 11.0.2, MariaDB 10.11.3,"
8,"MariaDB 10.10.4, MariaDB 10.9.6, MariaDB 10.8.8 and MariaDB 10.6.13."
8,"In MySQL and MariaDB, the output cardinality of a part of query has historically been tied to the used access method(s). This is different from the approach used in database textbooks. There, the cardinality ""x JOIN y"" is the same regardless of which access methods are used to compute it."
8,Example
8,Consider a query joining customers with their orders:
8,select *
8,from
8,"customer, orders, ..."
8,where
8,customer.id = orders.customer_id and ...
8,"Suppose, table orders has an index IDX on orders.customer_id."
8,"If the query plan is using this index to fetch orders for each customer, the optimizer will use index statistics from IDX to estimate the number of rows in the customer-joined-with-orders."
8,"On the other hand, if the optimizer considers a query plan that"
8,"joins customer with orders without use of indexes, it will ignore the customer.id = orders.customer_id equality completely and will compute the"
8,output cardinality as if customer was cross-joined with orders.
8,Hash Join
8,"MariaDB supports Block Hash Join. It is not enabled by default, one needs to set it join_cache_level to 3 or a bigger value to enable it."
8,"Before MDEV-30812, Query optimization for Block Hash Join would work as described in the above example: It would assume that the join operation is a cross join."
8,"MDEV-30812 introduces a new optimizer_switch flag, hash_join_cardinality. In MariaDB versions before 11.0, it is off by default."
8,"If one sets it to ON, the optimizer will make use of column histograms when computing the cardinality of hash join operation output."
8,"One can see the computation in the Optimizer Trace,"
8,search for hash_join_cardinality.
8,← GUID/UUID Performance
8,↑ Query Optimizations ↑
8,IGNORE INDEX →
8,Comments
8,Comments loading...
8,"Content reproduced on this site is the property of its respective owners,"
8,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
8,expressed by this content do not necessarily represent those of MariaDB or any other party.
8,↑ Query Optimizations ↑
8,Index Hints: How to Force Query Plans
8,Subquery Optimizations
8,Optimization Strategies
8,Optimizations for Derived Tables
8,Table Elimination
8,Statistics for Optimizing Queries
8,Filesort with Small LIMIT Optimization
8,LIMIT ROWS EXAMINED
8,index_merge sort_intersection
8,MariaDB 5.3 Optimizer Debugging
8,optimizer_switch
8,How to Quickly Insert Data Into MariaDB
8,Index Condition Pushdown
8,Query Limits and Timeouts
8,Aborting Statements that Exceed a Certain Time to Execute
8,Partition Pruning and Selection
8,Big DELETEs
8,Data Sampling: Techniques for Efficiently Finding a Random Row
8,Data Warehousing High Speed Ingestion
8,Data Warehousing Summary Tables
8,Data Warehousing Techniques
8,Equality propagation optimization
8,FORCE INDEX
8,Groupwise Max in MariaDB
8,GUID/UUID Performance
8,hash_join_cardinality optimizer_switch Flag
8,IGNORE INDEX
8,not_null_range_scan Optimization
8,"Optimizing for ""Latest News""-style Queries"
8,Pagination Optimization
8,Pivoting in MariaDB
8,Rollup Unique User Counts
8,Rowid Filtering Optimization
8,Sargable UPPER
8,USE INDEX
8,Products
8,MariaDB Platform
8,MariaDB SkySQL
8,Pricing
8,Download MariaDB
8,Services
8,Remote DBA
8,SkyDBA
8,Enterprise Architect
8,Technical Support
8,Migration Practice
8,Consulting
8,Training
8,Resources
8,Documentation
8,Developers
8,Blog
8,Support
8,OpenWorks
8,Customer Stories
8,Events
8,MariaDB Roadshow
8,About
8,Contact
8,Leadership
8,Partners
8,Newsroom
8,Investors
8,Careers
8,Trust Center
8,Vulnerability Reporting
8,Contact
8,Subscribe to our newsletter!
8,Legal
8,Privacy Policy
8,Cookie Policy
8,Copyright © 2023 MariaDB. All rights reserved.
10,Connection Redirection Mechanism in the MariaDB Client/Server Protocol - MariaDB Knowledge Base
10,Search
10,Products
10,Services
10,Resources
10,About
10,Contact
10,Login
10,Copyright © 2023 MariaDB. All rights reserved.
10,Knowledge Base
10,Contact
10,Login
10,Search
10,Products
10,Services
10,Pricing
10,Resources
10,About Us
10,Download
10,Knowledge Base
10,» MariaDB Server Documentation
10,» High Availability & Performance Tuning
10,» Connection Redirection Mechanism in the MariaDB Client/Server Protocol
10,Home
10,Open Questions
10,MariaDB Server
10,MariaDB MaxScale
10,MariaDB ColumnStore
10,Connectors
10,History
10,Source
10,Flag as Spam / Inappropriate
10,Translate
10,Created
10,2 months ago
10,Modified
10,2 months ago
10,Type
10,article
10,Status
10,active
10,License
10,CC BY-SA / Gnu FDL
10,History
10,Comments
10,EditAttachments
10,No attachments exist
10,Product Versions
10,MariaDB starting with 11.3
10,Connection Redirection Mechanism in the MariaDB Client/Server Protocol
10,MariaDB starting with 11.3A connection redirection mechanism was added in MariaDB 11.3.0 (MDEV-15935)
10,Redirection mechanisms are widely used in proxy-based scenarios.
10,"Previously, when multiple servers shared one proxy, the proxy forwarded all packets between servers and clients. Thus, the proxy added latency, consuming computing resources and impacting overall performance. For scenarios with many short connections, such as WordPress, latency can be a critical issue."
10,"With a redirection mechanism, much like HTTP redirects or Oracle redirected connections, clients get the servers’ address from proxies and connect to servers transparently, without latency and without wasting resources."
10,Usage
10,"Redirection is handled through a new system variable, redirect_url. The value defaults to an empty string, but can also contain a connection string in the conventional format (in the style of a Connector/C etc. connection url)."
10,"This variable is appended to the default value of the session_track_system_variables system variable. If not empty, clients will be redirected to the specified server."
10,Possible Use Cases
10,Always redirect all clients to a new location:
10,"set @@global.redirect_url, start the server with --redurect-url=, or put it in my.cnf"
10,redirect to a group of servers randomly
10,"create a table with connection urls, one per row."
10,use an sql script that selects a random row from it and sets @@redirect_url to this value
10,specify this script in the --init-connect server parameter
10,dynamically redirect from the primary to one of the replicas
10,"same as above, but use INFORMATION_SCHEMA.PROCESSLIST to get the list of active replicas."
10,Example
10,"set global redirect_url=""mysql://mariadb.org:12345"";"
10,Invalid formats are not permitted:
10,"set global redirect_url=""mysql://mariadb.org:"";"
10,ERROR 1231 (42000): Variable 'redirect_url' can't be set to the value of 'mysql://mariadb.org:'
10,← Optimization and Tuning
10,↑ High Availability & Performance Tuning ↑
10,Comments
10,Comments loading...
10,"Content reproduced on this site is the property of its respective owners,"
10,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
10,expressed by this content do not necessarily represent those of MariaDB or any other party.
10,↑ High Availability & Performance Tuning ↑
10,MariaDB Replication
10,MariaDB Galera Cluster
10,Optimization and Tuning
10,Connection Redirection Mechanism in the MariaDB Client/Server Protocol
10,Products
10,MariaDB Platform
10,MariaDB SkySQL
10,Pricing
10,Download MariaDB
10,Services
10,Remote DBA
10,SkyDBA
10,Enterprise Architect
10,Technical Support
10,Migration Practice
10,Consulting
10,Training
10,Resources
10,Documentation
10,Developers
10,Blog
10,Support
10,OpenWorks
10,Customer Stories
10,Events
10,MariaDB Roadshow
10,About
10,Contact
10,Leadership
10,Partners
10,Newsroom
10,Investors
10,Careers
10,Trust Center
10,Vulnerability Reporting
10,Contact
10,Subscribe to our newsletter!
10,Legal
10,Privacy Policy
10,Cookie Policy
10,Copyright © 2023 MariaDB. All rights reserved.
12,Performance recommendations - MoodleDocs
12,Forums
12,Documentation
12,Downloads
12,Demo
12,Tracker
12,Development
12,Translation
12,Search
12,Search
12,Moodle Sites
12,What are you looking for?
12,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
12,Moodle.com
12,Our social network to share and curate open educational resources.
12,MoodleNet
12,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
12,Moodle Academy
12,Moodle.com
12,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
12,MoodleNet
12,Our social network to share and curate open educational resources.
12,Moodle Academy
12,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
12,Documentation
12,Menu
12,Main pageTable of contentsDocs overviewRecent changes
12,Log in
12,4.3 docs4.2 docs
12,4.1 docs
12,4.0 docs
12,3.11 docs
12,3.9 docs
12,Article
12,Page Comments
12,View source
12,History
12,Performance recommendations
12,"From MoodleDocsJump to:navigation, search"
12,Main page ► Managing a Moodle site ► Performance ► Performance recommendations
12,Performance
12,Performance recommendations
12,Performance settings
12,Performance overview
12,Caching
12,Performance FAQ
12,MUC FAQ
12,"Moodle can be made to perform very well, at small usage levels or scaling up to many thousands of users. The factors involved in performance are basically the same as for any PHP-based database-driven system. When trying to optimize your server, try to focus on the factor which will make the most difference to the user. For example, if you have relatively more users browsing than accessing the database, look to improve the webserver performance."
12,Contents
12,1 Obtain a baseline benchmark
12,2 Scalability
12,2.1 Server cluster
12,3 Hardware configuration
12,4 Operating System
12,5 Caching Performance
12,6 Web Server Performance
12,6.1 PHP Performance
12,6.1.1 APC
12,6.2 Apache Performance
12,6.3 IIS Performance
12,6.4 OpenLiteSpeed
12,"6.5 Lighttpd, NginX and Cherokee Performance"
12,6.6 X-Sendfile
12,7 Cron Performance
12,8 Database Performance
12,8.1 MariaDB Performance
12,8.2 MySQL Performance
12,8.3 PostgreSQL Performance
12,8.4 Read replicas
12,8.5 Other database performance links
12,9 Performance of different Moodle modules
12,10 See also
12,Obtain a baseline benchmark
12,"Before attempting any optimization, you should obtain a baseline benchmark of the component of the system you are trying to improve. For Linux try LBS (Note: Last updated May 2002) and for Windows use the Performance Monitor. Once you have quantitative data about how your system is performing currently, you'll be able to determine if the change you have made has had any real impact."
12,"The overall aim of adjustments to improve performance is to use RAM (cacheing) and to reduce disk-based activity. It is especially important to try to eliminate swap file usage as much as you can. If your system starts swapping, this is a sign that you need more RAM."
12,"The optimization order preference is usually: primary storage (more RAM), secondary storage (faster hard disks/improved hard disk configuration), processor (more and faster)."
12,It can be interesting to install and use the Benchmark plugin in order to find the bottlenecks of your system that specifically affect Moodle or do a load test / stress test with tool like JMeter. See moodledev JMeter documentation
12,Scalability
12,Moodle's design (with clear separation of application layers) allows for strongly scalable setups. (Please check the list of large Moodle installations.)
12,"Large sites usually separate the web server and database onto separate servers, although for smaller installations this is typically not necessary."
12,"It is possible to load-balance a Moodle installation, for example by using more than one webserver. The separate webservers should query the same database and refer to the same filestore and cache areas (see Caching), but otherwise the separation of the application layers is complete enough to make this kind of clustering feasible. Similarly, the database could be a cluster of servers (e.g. a MySQL cluster), but this is not an easy task and you should seek expert support, e.g. from a Moodle Partner."
12,"On very large, load-balanced, systems the performance of the shared components become critical. It's important that your shared file areas are properly tuned and that you use an effective cache (Redis is highly recommended). A good understanding of these areas of system administration should be considered a minimum requirement."
12,Server cluster
12,Using Moodle forum discussions:
12,Moodle clustering
12,Software load balancing
12,TCP load balancing
12,Installation for 3000 simultaneous users
12,Hardware configuration
12,Note: The fastest and most effective change that you can make to improve performance is to increase the amount of RAM on your web server - get as much as possible (e.g. 4GB or more). Increasing primary memory will reduce the need for processes to swap to disk and will enable your server to handle more users.
12,"Better performance is gained by obtaining the best processor capability you can, i.e. dual or dual core processors. A modern BIOS should allow you to enable hyperthreading, but check if this makes a difference to the overall performance of the processors by using a CPU benchmarking tool."
12,"If you can afford them, use SCSI hard disks instead of SATA drives. SATA drives will increase your system's CPU utilization, whereas SCSI drives have their own integrated processors and come into their own when you have multiple drives. If you must have SATA drives, check that your motherboard and the drives themselves support NCQ (Native Command Queuing)."
12,"Purchase hard disks with a low seek time. This will improve the overall speed of your system, especially when accessing Moodle's reports. Naturally these days Solid State Drives outperform rotating media immensely, especially Enterprise-Grade SSD's."
12,Size your swap file correctly. The general advice is to set it to 4 x physical RAM.
12,"Use a RAID disk system. Although there are many different RAID configurations you can create, the following generally works best:"
12,install a hardware RAID controller (if you can)
12,the operating system and swap drive on one set of disks configured as RAID-1.
12,"Moodle, Web server and Database server on another set of disks configured as RAID-5."
12,"If your 'moodleData' area is going to be on relatively slow storage (e.g. NFS mount on to a NAS device) you will have performance issues with the default cache configuration (which writes to this storage). See the page on Caching and choose an alternative. Redis is recommended. Using GlusterFS / OCFS2 / GFS2 on a SAN device and Fiber Channel could improve performance (See more info on the Moodle forum thread, NFS performance tuing )"
12,Use gigabit ethernet for improved latency and throughput. This is especially important when you have your webserver and database server separated out on different hosts.
12,Check the settings on your network card. You may get an improvement in performance by increasing the use of buffers and transmit/receive descriptors (balance this with processor and memory overheads) and off-loading TCP checksum calculation onto the card instead of the OS.
12,Read this Case Study on a server stress test with 300 users.
12,See this accompanying report on network traffic and server loads.
12,Also see this SFSU presentation at Educause (using VMWare): [1]
12,Operating System
12,"You can use Linux(recommended), Unix-based, Windows or Mac OS X for the server operating system. *nix operating systems generally require less memory than Mac OS X or Windows servers for doing the same task as the server is configured with just a shell interface. Additionally Linux does not have licensing fees attached, but can have a big learning curve if you're used to another operating system. If you have a large number of processors running SMP, you may also want to consider using a highly tuned OS such as Solaris."
12,Check your own OS and vendor specific instructions for optimization steps.
12,For Linux look at the Linux Performance Team site.
12,"For Linux investigate the hdparm command, e.g. hdparm -m16 -d1 can be used to enable read/write on multiple sectors and DMA. Mount disks with the ""async"" and ""noatime"" options."
12,"For Windows set the sever to be optimized for network applications (Control Panel, Network Connections, LAN connection, Properties, File & Printer Sharing for Microsoft Networks, Properties, Optimization). You can also search the Microsoft TechNet site for optimization documents."
12,Caching Performance
12,"Caching in Moodle can default to disk for a lot of the different caches which is rather slow overall, and so pretty solid gains can be made by moving this to RAM, by use of a Memory Caching Application such as Redis or Memcached. In fact I'd go as far to say the single biggest improvement we made to our (relatively small) Moodle site was installing Redis, and this is amplified when you're using classic Hard Drives rather than SSD's, and especially when they slowly but surely begin to fail (the classic slow to write, but no SMART errors or write errors - just reeeeaaallly slow)."
12,"These will also cache some database queries, meaning that they don't have to be re-run, again improving performance there. Personally, I would recommend Redis over Memcached due to better security features and being more up to date/developed. For more information/how to install Redis in particular, visit the Redis cache store page."
12,Web Server Performance
12,"Most web browsers these days feature Inspector elements which will allow you to watch the time it takes for each page component to load, typically found under the ""Network"" tab. Also, the Yslow extension will evaluate your page against Yahoo's 14 rules, full text Best Practices for Speeding Up Your Web Site, (video) for fast loading websites."
12,PHP Performance
12,"PHP contains a built-in accelerator (for more recent versions of PHP, this is OpCache). Make sure it is enabled."
12,Improvements in read/write performance can be improved by putting the cached PHP pages on a TMPFS filesystem - but remember that you'll lose the cache contents when there is a power failure or the server is rebooted.
12,Performance of PHP is better when installed as an Apache/IIS6 ISAPI module (rather than a CGI). IIS 7.0/7.5 (Windows Server 2008/R2) users should choose a FastCGI installation for best performance.
12,"Also check the memory_limit in php.ini. The default value for the memory_limit directive is 128M. On some sites, it may need to be larger - especially for some backup operations."
12,Also see PHP settings by Moodle version
12,Use PHP-FPM (with apache).
12,APC
12,APC on CentOS 5.x (linux)
12,APC on Debian (linux)
12,Apache Performance
12,"If you are using Apache on a Windows server, use the build from Apache Lounge which is reported to have performance and stability improvements compared to the official Apache download. Note that this is an unofficial build, so may not keep up with official releases."
12,Set the MaxRequestWorkers directive correctly (MaxClients before Apache 2.4). Use this formula to help (which uses 80% of available memory to leave room for spare):
12,MaxRequestWorkers = Total available memory * 80% / Max memory usage of apache process
12,"Memory usage of apache process is usually 10MB but Moodle can easily use up to 100MB per process, so a general rule of thumb is to divide your available memory in megabytes by 100 to get a conservative setting for MaxClients. You are quite likely to find yourself lowering the MaxRequestWorkers from its default of 150 on a Moodle server. To get a more accurate estimate read the value from the shell command:"
12,#ps -ylC httpd --sort:rss
12,"If you need to increase the value of MaxRequestWorkers beyond 256, you will also need to set the ServerLimit directive."
12,Warning: Do not be tempted to set the value of MaxRequestWorkers higher than your available memory as your server will consume more RAM than available and start to swap to disk.
12,Consider reducing the number of modules that Apache loads in the httpd.conf file to the minimum necessary to reduce the memory needed.
12,Use the latest version of Apache - Apache 2 has an improved memory model which reduces memory usage further.
12,"For Unix/Linux systems, consider lowering MaxConnectionsPerChild (MaxRequestsPerChild before Apache 2.4) in httpd.conf to as low as 20-30 (if you set it any lower the overhead of forking begins to outweigh the benefits)."
12,"For a heavily loaded server, consider setting KeepAlive Off (do this only if your Moodle pages do not contain links to resources or uploaded images) or lowering the KeepAliveTimeout to between 2 and 5. The default is 15 (seconds) - the higher the value the more server processes will be kept waiting for possibly idle connections. A more accurate value for KeepAliveTimeout is obtained by observing how long it takes your users to download a page. After altering any of the KeepAlive variables, monitor your CPU utilization as there may be an additional overhead in initiating more worker processes/threads."
12,"As an alternative to using KeepAlive Off, consider setting-up a Reverse Proxy server in front of the Moodle server to cache HTML files with images. You can then return Apache to using keep-alives on the Moodle server."
12,"If you do not use a .htaccess file, set the AllowOverride variable to AllowOverride None to prevent .htaccess lookups."
12,Set DirectoryIndex correctly so as to avoid content-negotiation. Here's an example from a production server:
12,DirectoryIndex index.php index.html index.htm
12,"Unless you are doing development work on the server, set ExtendedStatus Off and disable mod_info as well as mod_status."
12,Leave HostnameLookups Off (as default) to reduce DNS latency.
12,Consider reducing the value of TimeOut to between 30 and 60 (seconds).
12,"For the Options directive, avoid Options Multiviews as this performs a directory scan. To reduce disk I/O further use"
12,Options -Indexes FollowSymLinks
12,Compression reduces response times by reducing the size of the HTTP response
12,Install and enable mod_deflate - refer to documentation or man pages
12,Add this code to the virtual server config file within the <directory> section for the root directory (or within the .htaccess file if AllowOverrides is On):
12,<ifModule mod_deflate.c>
12,AddOutputFilterByType DEFLATE text/html text/plain text/xml text/x-js text/javascript text/css application/javascript
12,</ifmodule>
12,Use Apache event MPM (and not the default Prefork or Worker)
12,IIS Performance
12,All alter this location in the registry:
12,HKLM\SYSTEM\CurrentControlSet\Services\Inetinfo\Parameters\
12,The equivalent to KeepAliveTimeout is ListenBackLog (IIS - registry location is HKLM\ SYSTEM\ CurrentControlSet\ Services\ Inetinfo\ Parameters). Set this to between 2 and 5.
12,Change the MemCacheSize value to adjust the amount of memory (Mb) that IIS will use for its file cache (50% of available memory by default).
12,"Change the MaxCachedFileSize to adjust the maximum size of a file cached in the file cache in bytes. Default is 262,144 (256K)."
12,"Create a new DWORD called ObjectCacheTTL to change the length of time (in milliseconds) that objects in the cache are held in memory. Default is 30,000 milliseconds (30 seconds)."
12,OpenLiteSpeed
12,"OpenLiteSpeed has it's own built in cache called LSCache, which is controlled through the Web GUI, and also is compatible with PHP OpCache. More info on optimizing OpenLiteSpeed can be found on the OpenLiteSpeed page."
12,"Lighttpd, NginX and Cherokee Performance"
12,"You can increase server performance by using a light-weight webserver like lighttpd, nginx or cherokee in combination with PHP in FastCGI-mode. Lighttpd was originally created as a proof-of-concept [2] to address the C10k problem and while primarily recommended for memory-limited servers, its design origins and asynchronous-IO model make it a suitable and proven [3] alternative HTTP server for high-load websites and web apps, including Moodle. See the MoodleDocs Lighttpd page for additional information, configuration example and links."
12,"Alternatively, both lighttpd and nginx are capable of performing as a load-balancer and/or reverse-proxy to alleviate load on back-end servers [4], providing benefit without requiring an actual software change on existing servers."
12,Do note that these are likely to be the least tested server environments of all particularly if you are using advanced features such as web services and/or Moodle Networking. They are probably best considered for heavily used Moodle sites with relatively simple configurations.
12,X-Sendfile
12,X-Sendfile modules improve performance when sending large files from Moodle. It is recommended to configure your web server and Moodle to use this feature if available.
12,Configure web server:
12,Apache - https://tn123.org/mod_xsendfile/
12,Lighttpd - http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file
12,Nginx - http://wiki.nginx.org/XSendfile
12,OpenLiteSpeed - https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:config:internal-redirect
12,Enable support in config.php (see config-dist.php):
12,$CFG->xsendfile = 'X-Sendfile';
12,// Apache {@see https://tn123.org/mod_xsendfile/}
12,$CFG->xsendfile = 'X-LIGHTTPD-send-file'; // Lighttpd {@see http://redmine.lighttpd.net/projects/lighttpd/wiki/X-LIGHTTPD-send-file}
12,$CFG->xsendfile = 'X-Accel-Redirect';
12,// Nginx {@see http://wiki.nginx.org/XSendfile}
12,Configure file location prefixes if your server implementation requires it:
12,$CFG->xsendfilealiases = array(
12,"'/dataroot/' => $CFG->dataroot,"
12,"'/cachedir/' => '/var/www/moodle/cache',"
12,// for custom $CFG->cachedir locations
12,"'/localcachedir/' => '/var/local/cache',"
12,// for custom $CFG->localcachedir locations
12,'/tempdir/'
12,"=> '/var/www/moodle/temp',"
12,// for custom $CFG->tempdir locations
12,'/filedir'
12,"=> '/var/www/moodle/filedir',"
12,// for custom $CFG->filedir locations
12,Cron Performance
12,"Cron is a very important part of the overall performance of Moodle as many asynchronous processes are offloaded to Cron, so it needs to be running and have enough through put to handle the work being given to it by the front ends."
12,See Cron with Unix or Linux#High performance cron tasks
12,Database Performance
12,MariaDB Performance
12,"MariaDB Optimizations are similar to MySQL, but at the same time different due to the way MariaDB operates. Performance as a whole is typically better than MySQL using MariaDB, so if you're looking for Database Optimization, potentially switching from MySQL to MariaDB may help with performance, otherwise if you're already using MariaDB and looking to Optimize it, Performance Recommendations can be found on the MariaDB Page."
12,MySQL Performance
12,"The number one thing you can do to improve MySQL performance is to read, understand and implement the recommendations in the Innodb Buffer Pool article."
12,"The buffer pool size can safely be changed while your server is running, as long as your server has enough memory (RAM) to accommodate the value you set. On a machine that is dedicated to MySQL, you can safely set this value to 80% of available memory."
12,"Consider setting innodb_buffer_pool_instances to the number of cores, vCPUs, or chips you have available. Adjust this value in accordance with the recommendations in the MySQL documentation."
12,The following are MySQL specific settings which can be adjusted for better performance in your my.cnf (my.ini in Windows). The file contains a list of settings and their values. To see the current values use these commands
12,SHOW STATUS;
12,SHOW VARIABLES;
12,"Important: You must make backups of your database before attempting to change any MySQL server configuration. After any change to the my.cnf, restart mysqld."
12,"If you are able, the MySQLTuner tool can be run against your MySQL server and will calculate appropriate configuration values for most of the following settings based on your current load, status and variables automatically."
12,Enable the query cache with
12,query_cache_type = 1.
12,"For most Moodle installs, set the following:"
12,query_cache_size = 36M
12,query_cache_min_res_unit = 2K.
12,The query cache will improve performance if you are doing few updates on the database.
12,Set the table cache correctly. For Moodle 1.6 set
12,table_cache = 256 #(table_open_cache in MySQL > 5.1.2)
12,"(min), and for Moodle 1.7 set"
12,table_cache = 512 #(table_open_cache in MySQL > 5.1.2)
12,"(min). The table cache is used by all threads (connections), so monitor the value of opened_tables to further adjust - if opened_tables > 3 * table_cache(table_open_cache in MySQL > 5.1.2) then increase table_cache up to your OS limit. Note also that the figure for table_cache will also change depending on the number of modules and plugins you have installed. Find the number for your server by executing the mysql statement below. Look at the number returned and set table_cache to this value."
12,mysql>SELECT COUNT(table_name) FROM information_schema.tables WHERE table_schema='yourmoodledbname';
12,Set the thread cache correctly. Adjust the value so that your thread cache utilization is as close to 100% as possible by this formula:
12,thread cache utilization (%) = (threads_created / connections) * 100
12,"The key buffer can improve the access speed to Moodle's SELECT queries. The correct size depends on the size of the index files (.myi) and in Moodle 1.6 or later (without any additional modules and plugins), the recommendation for this value is key_buffer_size = 32M. Ideally you want the database to be reading once from the disk for every 100 requests so monitor that the value is suitable for your install by adjusting the value of key_buffer_size so that the following formulas are true:"
12,key_read / key_read_requests < 0.01
12,key_write / key_write_requests <= 1.0
12,"Set the maximum number of connections so that your users will not see a ""Too many connections"" message. Be careful that this may have an impact on the total memory used. MySQL connections usually last for milliseconds, so it is unusual even for a heavily loaded server for this value to be over 200."
12,Manage high burst activity. If your Moodle install uses a lot of quizzes and you are experiencing performance problems (check by monitoring the value of threads_connected - it should not be rising) consider increasing the value of back_log.
12,"Optimize your tables weekly and after upgrading Moodle. It is good practice to also optimize your tables after performing a large data deletion exercise, e.g. at the end of your semester or academic year. This will ensure that index files are up to date. Backup your database first and then use:"
12,mysql>CHECK TABLE mdl_tablename;
12,mysql>OPTIMIZE TABLE mdl_tablename;
12,"The common tables in Moodle to check are mdl_course_sections, mdl_forum_posts, mdl_log and mdl_sessions (if using dbsessions). Any errors need to be corrected using REPAIR TABLE (see the MySQL manual and this forum script)."
12,Maintain the key distribution. Every month or so it is a good idea to stop the mysql server and run these myisamchk commands.
12,#myisamchk -a -S /pathtomysql/data/moodledir/*.MYI
12,"Warning: You must stop the mysql database process (mysqld) before running any myisamchk command. If you do not, you risk data loss."
12,Reduce the number of temporary tables saved to disk. Check this with the created_tmp_disk_tables value. If this is relatively large (>5%) increase tmp_table_size until you see a reduction. Note that this will have an impact on RAM usage.
12,PostgreSQL Performance
12,"There are some good papers around on tuning PostgreSQL (like this one), and Moodle's case does not seem to be different to the general case."
12,The first thing to recognise is that if you really need to worry about tuning you should be using a separate machine for the database server. If you are not using a separate machine then the answers to many performance questions are substantially muddied by the memory requirements of the rest of the application.
12,"You should probably enable autovacuum, unless you know what you are doing. Many e-learning sites have predictable periods of low use, so disabling autovacuum and running a specific vacuum at those times can be a good option. Or perhaps leave autovacuum running but do a full vacuum weekly in a quiet period."
12,"Set shared_buffers to something reasonable. For versions up to 8.1 my testing has shown that peak performance is almost always obtained with buffers < 10000, so if you are using such a version, and have more than 512M of RAM just set shared_buffers to 10,000 (8MB)."
12,"The buffer management had a big overhaul in 8.2 and ""reasonable"" is now a much larger number. I have not conducted performance tests with 8.2, but the recommendations from others are generally that you should now scale shared_buffers much more with memory and may continue to reap benefits even up to values like 100,000 (80MB). Consider using 1-2% of system RAM."
12,"PostgreSQL will also assume that the operating system is caching its files, so setting effective_cache_size to a reasonable value is also a good idea. A reasonable value will usually be (total RAM - RAM in use by programs). If you are running Linux and leave the system running for a day or two you can look at 'free' and under the 'cached' column you will see what it currently is. Consider taking that number (which is kB) and dividing it by 10 (i.e. allow 20% for other programs cache needs and then divide by 8 to get pages). If you are not using a dedicated database server you will need to decrease that value to account for usage by other programs."
12,"Some other useful parameters that can have positive effects, and the values I would typically set them to on a machine with 4G RAM, are:"
12,work_mem = 10240
12,"That's 10M of RAM to use instead of on-disk sorting and so forth. That can give a big speed increase, but it is per connection and 200 connections * 10M is 2G, so it can theoretically chew up a lot of RAM."
12,maintenance_work_mem = 163840
12,"That's 160M of RAM which will be used by (e.g.) VACUUM, index rebuild, cluster and so forth. This should only be used periodically and should be freed when those processes exit, so I believe it is well worth while."
12,wal_buffers = 64
12,"These buffers are used for the write-ahead log, and there have been a number of reports on the PostgreSQL mailing lists of improvement from this level of increase."
12,This is a little out of date now (version 8.0) but still worth a read: http://www.powerpostgresql.com/Docs
12,And there is lots of good stuff here as well: http://www.varlena.com/GeneralBits/Tidbits/index.php
12,Based on Andrew McMillan's post at Tuning PostgreSQL forum thread.
12,Splitting mdl_log to several tables and using a VIEW with UNION to read them as one. (See Tim Hunt explanation on the Moodle forums)
12,Read replicas
12,Since Moodle 3.9 you can configure read replica's to be used where possible. For very large systems as much as 80-90% of the DB load can be moved away from the primary. For configuration see config-dist:
12,https://github.com/moodle/moodle/blob/master/config-dist.php#L84-L117
12,Other database performance links
12,Consider using a distributed caching system like memcached but note that memcached does not have any security features so it should be used behind a firewall.
12,Consider using PostgreSQL. See how to migrate from MySQL to PostgreSQL (forum discussion).
12,General advice on tuning MySQL parameters (advice from the MySQL manual)
12,InnoDB performance optimization taken from the MySQL performance blog site.
12,Performance of different Moodle modules
12,"Moodle's activity modules, filters, and other plugins can be activated/deactivated. If necessary, you may wish to deactivate some features (such as chat) if not required - but this isn't necessary. Some notes on the performance of certain modules:"
12,"The Chat module is said to be a hog in terms of frequent HTTP requests to the main server. This can be reduced by setting the module to use Streamed updates, or, if you're using a Unix-based webserver, by running the chat in daemon mode. When using the Chat module use the configuration settings to tune for your expected load. Pay particular attention to the chat_old_ping and chat_refresh parameters as these can have greatest impact on server load."
12,The Moodle Cron task is triggered by calling the script cron.php. If this is called over HTTP (e.g. using wget or curl) it can take a large amount of memory on large installations. If it is called by directly invoking the php command (e.g. php -f /path/to/moodle/directory/admin/cli/cron.php) efficiency can be much improved.
12,The Recent activities block is consuming too many resources if you have huge number of records mdl_log. This is being tested to optimize the SQL query.
12,"The Quiz module is known to stretch database performance. However, it has been getting better in recent versions, and we don't know of any good, up-to-date performance measurements. (Here is a case study from 2007 with 300 quiz users.). The following suggestions were described by Al Rachels in this forum thread:"
12,"make sure both Moodle, and the operating system, are installed on a solid state drive"
12,upgrade to and use PHP 7
12,run MySQLTuner and implement its recommendations
12,See Performance settings for more information on performance-related Moodle settings.
12,See also
12,Using Moodle: Hardware and Performance forum
12,Why Your Moodle Site is Slow: Five Simple Settings blog post from Jonathan Moore
12,I teach with Moodle performance testing: http://www.iteachwithmoodle.com/2012/11/17/moodle-2-4-beta-performance-test-comparison-with-moodle-2-3/
12,Moodle 2.4.5 vs 2.5.2 performance and MUC APC cahe store
12,Moodle performance testing 2.4.6 vs 2.5.2 vs 2.6dev
12,Moodle performance analysis revisited (now with MariaDB)
12,"Tim Hunt's blog (May 2, 2013) on performance testing Moodle"
12,"New Relic, Application Performance Monitoring"
12,Performance enhancements for Apache and PHP (Apache Event MPM and PHP-FPM)
12,Performance recommendations
12,Moodle performance investigation – using performance info
12,Moodle Caching at Scale
12,"There have been a lot of discussions on moodle.org about performance, here are some of the more interesting and (potentially) useful ones:"
12,Performance woes!
12,Performance perspectives - a little script
12,Comments on planned server hardware
12,Moodle performance in a pil by Martin Langhoff
12,Advice on optimising php/db code in moodle2+
12,Moodle 2.5 performance testing at the OU
12,100 active users limit with 4vCPU
12,Performance Tip ... shared...
12,"Retrieved from ""https://docs.moodle.org/403/en/index.php?title=Performance_recommendations&oldid=146449"""
12,Category: Performance
12,Tools
12,What links here
12,Related changes
12,Special pages
12,Printable version
12,Permanent link
12,Page information
12,In other languages
12,Español
12,Français
12,日本語
12,Deutsch
12,"This page was last edited on 17 July 2023, at 08:22."
12,Content is available under GNU General Public License unless otherwise noted.
12,Privacy
12,About Moodle Docs
12,Disclaimers
13,MySQLTuner: A Comprehensive Guide to MySQL/MariaDB Performance Tuning
13,Skip to content
13,Menu
13,Menu WindowsWindows 11Windows 10Windows 8Windows 7Windows VistaWindows XPDevelopmentJavaPythonPHPRuby/RailsDatabasesPostgreSQLMySQL (MariaDB)CassandraTipsLinux TipsAndroid TipsiPhone / iPad TipsMacOS TipsWeb TipsSoftware TipsOtherGadgetsCool Websites
13,MySQL (MariaDB)MySQLTuner: A Comprehensive Guide to MySQL/MariaDB Performance Tuning
13,"soodJuly 5, 2023"
13,"MySQL (MariaDB) is an open-source relational database management system that powers a significant portion of the web. However, to get the most out of it, you need to keep it well-optimized. One of the best ways to achieve that is by using MySQLTuner, a Perl script that quickly reviews your MySQL settings and provides recommendations for improved performance and stability.Table of Contents"
13,"Downloading and Installing MySQLTunerAnalyzing MySQL (MariaDB) Performance with MySQLTunerInterpreting MySQLTuner’s RecommendationsImproving MySQL (MariaDB) PerformanceConclusionDownloading and Installing MySQLTunerMySQLTuner is an open-source script that you can download directly from its Github repository. Follow these steps to download and install it:Log into your Linux server via SSH.Before you download MySQLTuner, you need to ensure you have Perl installed. If it’s not, you can install it using the package manager of your distribution. For Debian-based systems, use: sudo apt-get install perl For Red Hat-based systems, use: sudo yum install perlDownload MySQLTuner from its Github repository: wget http://mysqltuner.pl/ -O mysqltuner.pl wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/basic_passwords.txt -O basic_passwords.txt wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/vulnerabilities.csv -O vulnerabilities.csvOnce downloaded, make the script executable: chmod +x mysqltuner.plNow that MySQLTuner is installed, let’s see how to use it to analyze your MySQL performance.Analyzing MySQL (MariaDB) Performance with MySQLTunerTo run the MySQLTuner script, use the following command:perl mysqltuner.plThis command will prompt you for your MySQL root password. After you enter the password, MySQLTuner will start analyzing your system. It will review your MySQL configuration, the database statistics, and the hardware system in general.The output will be divided into several sections:General Statistics: This section provides an overview of your server’s uptime, the number of databases, and the total number of tables and indexes.Storage Engine Statistics: Here, you’ll see the number of tables using the InnoDB and MyISAM storage engines.Security Recommendations: This part alerts you of any security vulnerabilities like anonymous accounts or accounts without a password.Performance Metrics: This section is the heart of the report. It contains detailed information about the query cache, sorting, joins, and more.Recommendations: Finally, MySQLTuner provides specific advice based on its analysis.Interpreting MySQLTuner’s RecommendationsThe recommendations from MySQLTuner are guidelines and not a definitive playbook. They should be reviewed carefully and implemented with discretion. Let’s discuss some common recommendations:Adjust the query cache size: MySQL uses the query cache to store the result of SELECT queries. If MySQLTuner recommends increasing the query_cache_size, it means that MySQL could benefit from storing more queries in the cache.Increase table_open_cache: This cache is used to store file handles for all threads. Increasing it can improve performance if you have many tables.Optimize join statements: If MySQLTuner suggests optimizing your joins, it means that some of your queries are not using indexes. This can slow down query execution time.Upgrade MySQL version: MySQLTuner will suggest this if it finds any known vulnerabilities in your current MySQL version.Improving MySQL (MariaDB) PerformanceAfter reviewing the recommendations, you can adjust your MySQL settings accordingly. The settings are located in theSee also  MySQL 5.6 - How to log slow queries (Linux)my.cnf file, usually found in the /etc/mysql/ directory. Open this file with a text editor like nano or vim, and make the necessary changes. For instance:sudo nano /etc/mysql/my.cnfOnce you’ve made your changes, save the file and exit the editor. You’ll then need to restart the MySQL service for the changes to take effect. For systems using systemd, this would be:sudo systemctl restart mysqlRemember, changes to the MySQL configuration should be made incrementally, with careful monitoring of the effect. In some cases, increasing values too much can lead to worse performance, or even instability. Also, be aware that your server’s resources are finite. You cannot, for instance, assign more RAM to MySQL than your system physically has available.Bonus Tip: Performance benefits of using a SSD Disk for MySQL/MariaDBMySQL (MariaDB), like any other database management system, can greatly benefit from the underlying storage technology, whether it’s Solid State Drive (SSD) or traditional Hard Disk Drive (HDD). This is because I/O operations are a significant part of any database system.SSDs have no moving parts, using flash memory to store data, which leads to faster data access times. In the case of MySQL, SSDs can handle a high number of IOPS (Input/Output Operations Per Second), which directly affects database performance. The low latency and high-speed data transfer can significantly enhance MySQL operations, especially those that are I/O intensive like large scale read-write operations, complex joins, or full-table scans. Consequently, SSDs can be particularly beneficial for OLTP (Online Transaction Processing) systems or databases with heavy write operations.On the other hand, traditional HDDs use mechanical parts to read and write data. They’re generally slower than SSDs due to higher latency and lower IOPS. This could result in slower SQL query execution times and overall reduced database performance. However, HDDs offer a higher storage capacity at a lower price, making them a cost-effective choice for data archiving or for databases where performance is not the primary concern.In a nutshell, if MySQL performance is critical, an SSD will likely provide superior performance due to its higher speed and lower latency. However, the choice between SSD and HDD should consider other factors such as cost, storage capacity, and the specific workload characteristics.ConclusionMySQLTuner is a powerful tool for diagnosing performance issues and tuning your MySQL database. However, it’s essential to remember that its suggestions are starting points for tuning, not definitive solutions. Each MySQL deployment is unique, and what works best for one system may not work for another. Always test changes in a controlled environment before deploying them to production.With careful analysis and thoughtful implementation of MySQLTuner’s recommendations, you can significantly improve the performance and efficiency of your MySQL databases, ensuring your applications run smoothly and your data is readily available when you need it.Remember, database tuning is not a one-time task, but a continuous process of monitoring, adjusting, and refining. Regular use of MySQLTuner as part of this process can help keep your MySQL databases running at their best.Leave a CommentCommentName"
13,Email
13,"Website Save my name, email, and website in this browser for the next time I comment."
13,ΔTagscPanel / WHM
13,Django Tips
13,Facebook Tips
13,Google Tips
13,iPhone Tips
13,Laravel Tips
13,Samsung Tips
13,Software Testing (QA)
13,Video Games
13,"VirtualizationFind us on socialFacebookTwitterAmazonRSS FeedAbout UsWelcome to HeatWare.net - A technology blog for geeks and non-geeks alike! We programming tips, code examples, Cloud technology help, Windows & application troubleshooting, and more! The owner of this blog also run a feedback and rating database for online transactions."
13,© 2023 HeatWare.net
13,Search for:
15,Reddit - Dive into anything
15,Reddit and its partners use cookies and similar technologies to provide you with a better experience.
15,"By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising."
15,"By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform."
15,"For more information, please see our"
15,Cookie Notice
15,and our
15,Privacy Policy.
15,Open menu
15,Open navigation
15,Go to Reddit Home
15,r/laravel
15,A chip
15,A close button
15,Get app
15,Get the Reddit app
15,Log In
15,Log in to Reddit
15,Expand user menu
15,Open settings menu
15,Log In / Sign Up
15,Advertise on Reddit
15,Shop Collectible Avatars
15,Get the Reddit app
15,Scan this QR code to download the app now
15,Or check it out in the app stores
15,Go to laravel
15,r/laravel
15,r/laravel
15,Laravel is a free and open-source PHP web framework created by Taylor Otwell.
15,"Laravel features expressive, elegant syntax - freeing you to create without sweating the small things."
15,Members
15,Online
15,ragabekov
15,How MySQL Tuning Can Dramatically Improve Laravel Performance
15,Discussion
15,"Hey everyone,"
15,I've recently published an interesting research article on how MySQL tuning can significantly impact the performance of Laravel applications.
15,"The study compared the performance of two identical Laravel applications (we used Aimeos) running on identical servers, with the only difference being the MySQL configuration. One was set up with default MySQL settings, while the other was tuned based on best practices for Laravel."
15,The results were striking. Laravel website Response Time decreased by 42% and CPU utilization by 86% after tuning MySQL configuration.
15,"So, what were the specific tuning changes made to MySQL? Here are some of the highlights:"
15,"Increasing the innodb_buffer_pool_sizeto allocate more memory for caching data, which reduced the number of disk reads required and improved query response times."
15,Adjusting the innodb_log_file_sizeto improve write performance and reduce contention.
15,"There were several other tuning adjustments made as well. Of course, it's important to note that the exact tuning settings may vary depending on the specifics of your application and server setup."
15,If you're interested to checking out the original research article the link is here. It provides a detailed explanation of research and MySQL cofiguration.
15,"Thanks for reading, and happy improving performance of your apps!"
15,Update
15,I decided to make this research to show the real value of MySQL tuning.
15,"I know many developers who spend a lot of time optimizing the codebase, but it reaches a point where it no longer brings a valuable result for the time and energy invested."
15,Because even greatly optimized queries with right indexes might be slow if MySQL doesn’t use enough memory.
15,Read more
15,Archived post. New comments cannot be posted and votes cannot be cast.
15,Locked post. New comments cannot be posted.
15,Popular
15,TOPICS
15,Gaming
15,Valheim
15,Genshin Impact
15,Minecraft
15,Pokimane
15,Halo Infinite
15,Call of Duty: Warzone
15,Path of Exile
15,Hollow Knight: Silksong
15,Escape from Tarkov
15,Watch Dogs: Legion
15,Sports
15,NFL
15,NBA
15,Megan Anderson
15,Atlanta Hawks
15,Los Angeles Lakers
15,Boston Celtics
15,Arsenal F.C.
15,Philadelphia 76ers
15,Premier League
15,UFC
15,Business
15,GameStop
15,Moderna
15,Pfizer
15,Johnson & Johnson
15,AstraZeneca
15,Walgreens
15,Best Buy
15,Novavax
15,SpaceX
15,Tesla
15,Crypto
15,Cardano
15,Dogecoin
15,Algorand
15,Bitcoin
15,Litecoin
15,Basic Attention Token
15,Bitcoin Cash
15,Television
15,The Real Housewives of Atlanta
15,The Bachelor
15,Sister Wives
15,90 Day Fiance
15,Wife Swap
15,The Amazing Race Australia
15,Married at First Sight
15,The Real Housewives of Dallas
15,My 600-lb Life
15,Last Week Tonight with John Oliver
15,Celebrity
15,Kim Kardashian
15,Doja Cat
15,Iggy Azalea
15,Anya Taylor-Joy
15,Jamie Lee Curtis
15,Natalie Portman
15,Henry Cavill
15,Millie Bobby Brown
15,Tom Hiddleston
15,Keanu Reeves
15,RESOURCES
15,About Reddit
15,Advertise
15,Help
15,Blog
15,Careers
15,Press
15,"Reddit, Inc. © 2023. All rights reserved."
15,Top 2%
15,Rank by size
15,Top Posts
15,Reddit
15,"reReddit: Top posts of February 23, 2023"
15,Reddit
15,reReddit: Top posts of February 2023
15,Reddit
15,reReddit: Top posts of 2023
16,MariaDB - MoodleDocs
16,Forums
16,Documentation
16,Downloads
16,Demo
16,Tracker
16,Development
16,Translation
16,Search
16,Search
16,Moodle Sites
16,What are you looking for?
16,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
16,Moodle.com
16,Our social network to share and curate open educational resources.
16,MoodleNet
16,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
16,Moodle Academy
16,Moodle.com
16,"Learn about Moodle's products, like Moodle LMS or Moodle Worplace, or find a Moodle Certified Service Provider."
16,MoodleNet
16,Our social network to share and curate open educational resources.
16,Moodle Academy
16,"Courses and programs to develop your skills as a Moodle educator, administrator, designer or developer."
16,Documentation
16,Menu
16,Main pageTable of contentsDocs overviewRecent changes
16,Log in
16,4.3 docs4.2 docs
16,4.1 docs
16,4.0 docs
16,3.11 docs
16,3.9 docs
16,Article
16,Page Comments
16,View source
16,History
16,MariaDB
16,"From MoodleDocsJump to:navigation, search"
16,"MariaDB is a MySQL fork that is developed and maintained by original MySQL developers organised under MariaDB Foundation. It is considered to be more open and is being distributed as default MySQL compatible database by majority of modern Linux distributions. More modern versions of MariaDB also feature better performance than MySQL with lower latency and higher throughput - MariaDB themselves actually ran a benchmark against MySQL, the results of which can be seen here: Benchmark: MariaDB vs MySQL on Commodity Cloud Hardware."
16,"MariaDB is a drop-in replacement for Oracle MySQL, you can use it with any stable supported Moodle version using the standard mysqli drivers that ship with PHP by default."
16,Contents
16,1 MariaDB driver in Moodle
16,2 Installation
16,3 Optimizing MariaDB Post-Installation & Tuning
16,3.1 Memory Allocation
16,4 FAQ
16,5 See also
16,MariaDB driver in Moodle
16,There is a dedicated driver in Moodle for MariaDB. It is recommended to explicitly specify mariadb Moodle driver in config.php:
16,$CFG->dbtype = 'mariadb';
16,$CFG->dblibrary = 'native';
16,At the moment the driver is similar to MySQL but in the future the implementations may diverge significantly. Moodle MariaDB driver is not compatible with MyISAM database engine.
16,"See MySQL for more information, the setup procedure is nearly identical."
16,Installation
16,"Installing MariaDB is relatively straightforward, and is similar to installing any package on Linux or Windows - you can head to the site and download what you need directly (mainly for Windows), otherwise on Linux I'd grab it through APT like most packages."
16,"A good guide for installing/setting up MariaDB post-installation has been written by DigitalOcean: How To Install MariaDB on Ubuntu 22.04, but if you're going to install a LOMP Stack anyway, this guide might be more relevant: How To Install Linux, OpenLiteSpeed, MariaDB, PHP (LOMP stack) on Ubuntu 22.04. Note that the first guide may be more relevant as it also covers how to change a user to password authentication if required (the default on Linux is now Unix-Socket Authentication), but it could be worth having a quick look at both - they're very similar (and easy to flick through) otherwise."
16,"Setting up the database is give or take exactly the same as MySQL (although you don't need to worry about setting the server character set to utf8mb4 or enabling InnoDB, as this is the default in MariaDB - just make sure to specify it when creating the database), so the instructions can be followed as per Installing Moodle or the Step-by-step Installation Guide for Ubuntu."
16,Optimizing MariaDB Post-Installation & Tuning
16,"MariaDB out of the box comes pretty well tuned, and according to the developers, unnecessary tuning may actually reduce performance, so proceed with caution here. There's a whole section on Optimization and Tuning in the MariaDB Docs: Optimization and Tuning, but most of it is largely out of scope for users here as it talks about designing database queries and the like etc, so I'll skip past it for the moment and continue to the couple of bits the internet seemed to have consensus on about optimizing performance. First - Memory usage."
16,Memory Allocation
16,"There's a dedicated page in the MariaDB Docs to Memory Allocation, and I'd say this is potentially the most useful page I found within those docs. The short of it is similar to MySQL, the majority of the Performance Gains to be had come from correctly configuring the innoDB_buffer_pool_size, depending on the memory you have available. Unfortunately due to how wide-ranging this can be, I can't really offer any sort of guidance for the most part here - it all depends on your setup, and how much RAM you have, and what else is running on the server. As a minimum though, if you're building something lightweight, I wouldn't really allocate any less than 1GB to MariaDB total, considering how cheap RAM can be with hosting these days, so it's a piece of mind thing more than anything else. This is however based off a relatively small Moodle Server, so naturally this will change as you scale. This GitHub page (dronezzzko/mysql-mariadb-tuning-and-optimization-for-best-performance.md) does offer some suggestions about optimizing MariaDB depending on your memory, though I'd again recommend having a look at what the options actually do (definitely don't just blindly set them) before making a decision as to whether they're right for you - the MariaDB Docs are pretty good in this respect. Here's the page you'll be looking for if that's the case: Server System Variables."
16,"Continuing back along our original Memory Allocation page, I would read this page and have a look at what it says - it's actually nowhere near as long as it looks, but does offer some other useful suggestions:"
16,Adjusting parameters depending on your configuration: 64-bit OS and MariaDB¶ (Generally you should be 64-bit these days).
16,"Adjusting key_buffer_size, and reducing this if you're not using MyISAM Tables, (It's only relevant to have a large key_buffer_size if you're using MyISAM Tables, which by default MariaDB uses InnoDB. General consensus is InnoDB offers better performance as well, so really only use MyISAM if you have a reason to)."
16,"Adjusting table_open_cache (previously called table_cache) - in Performance recommendations under MySQL, it recommends having this as 512 for most Moodle Installations, but I left it at the default MariaDB setting of 2000."
16,"Adjusting the query_cache - this differs from MySQL where by default it is disabled in MariaDB. Enabling it may even reduce performance if it isn't needed, as per the docs. I unfortunately don't know enough about how Moodle uses the database as to whether it is better to have this on or off these days, so went with the MySQL recommendation of On, but with a query_cache_size of 64M (seems to be the recommendation of many other sites, as memory is plentiful these days). I did however leave the query_cache_min_res_unit with the default of 4096 (4KB)."
16,"Adjusting the thread_cache_size - as mentioned on the page, normally this doesn't have a huge effect on things, so leaving it as default (256) doesn't seem to do too much damage. Other sites however recommend setting this at a value of 16 or so - the best thing would be to test it in your setup as recommended, and see what works best. I also tried a setting of 16 to no real noticeable difference on a small Moodle site."
16,"Adjusting Swappiness - Swappiness is how aggressively the OS will swap to and from RAM to disk - useful for applications you're using and then quickly switching, not so much for databasing where the buffer_pool could then be swapped out to disk, defeating the purpose of the buffer_pool. I set this to the recommended value of 5."
16,Other Adjustments from the Inter-webs:
16,"I adjusted tmp_table_size to 64MB, seems to be recommended a lot over the internet, and I figured it couldn't hurt due to the amount of RAM we have available (default is 16MB for this)."
16,"I also adjusted max_heap_table_size to 64MB again, on the recommendation of the internet. Looking at the information as to what it does, I figured it couldn't hurt - default is again 16MB."
16,"Following the GitHub suggestion linked above, I also set innodb_flush_log_at_trx_commit to 0 - this means the database will no longer flush the to disk every time a transaction is performed, meaning it no longer conforms with ACID Principles, but however giving you a noticeable performance boost. This does mean if you lose power, you will lose the last second of transactions, but I'd prefer to have a UPS installed with Auto-Shutdown configured instead, or if in a datacenter some sort of failover and redundancy there. If you are turning this off though, make sure you have some sort of alternative to have those transactions written to disk in place - the last thing you want is to corrupt your Moodle Installation! (Hence why you have a Site backup of course)."
16,"I also changed the log_slow_query_time to 1, meaning that anything taking longer than 1 second to process is considered a slow query. Seems quick to the naked eye, but bear in mind usually MariaDB/MySQL connections last milliseconds."
16,"Finally, I also enabled skip_name_resolve by setting it to 1 - this means that only IP addresses are used for connections, and host names are not resolved. Something to consider if your webserver isn't setup correctly with reverse DNS (where DNS Lookups will continuously fail and slow down the database), but also on our installation the database and webserver are on the same box, meaning the only address it needs to resolve is localhost/itself anyway, so this doesn't really hurt anything (I think)."
16,"That should do most of the heavy lifting and get your database server running reasonably well hopefully - in most cases, if you're getting beyond the point of these settings not working and needing to be (re)configured for more connections/transactions/memory usage, you'd also be familiar with what your needs are and therefore what needs to be configured and how, so that's a problem that almost solves itself documentation wise. In any case, we arrive back at the start point - unfortunately every installation is different, and so every one will need you to look at what you have available and what your needs are, and to configure appropriately."
16,"But if you've made it this far, congratulations! You should hopefully have a working MariaDB Server (and Moodle instance by extension), so time to take a break and celebrate - buy yourself something nice (a coffee perhaps?) :)"
16,FAQ
16,It fails during installation due to binlog_format configuration:
16,In my.cnf file set binlog_format = ROW under [mysqld] and restart mysql service
16,See also
16,Performance recommendations
16,MariaDB.org
16,Configuring MariaDB for Optimal Performance
16,dronezzzko/mysql-mariadb-tuning-and-optimization-for-best-performance.md
16,13 Tips for Tuning and Optimizing MySQL and MariaDB
16,15 Useful MySQL/MariaDB Performance Tuning and Optimization Tips
16,"Retrieved from ""https://docs.moodle.org/403/en/index.php?title=MariaDB&oldid=146447"""
16,Tools
16,What links here
16,Related changes
16,Special pages
16,Printable version
16,Permanent link
16,Page information
16,In other languages
16,日本語
16,"This page was last edited on 17 July 2023, at 07:49."
16,Content is available under GNU General Public License unless otherwise noted.
16,Privacy
16,About Moodle Docs
16,Disclaimers
17,Comments - Descending Indexes
17,- MariaDB Knowledge Base
17,Search
17,Products
17,Services
17,Resources
17,About
17,Contact
17,Login
17,Copyright © 2023 MariaDB. All rights reserved.
17,Knowledge Base
17,Contact
17,Login
17,Search
17,Products
17,Services
17,Pricing
17,Resources
17,About Us
17,Download
17,Knowledge Base
17,» MariaDB Server Documentation
17,» High Availability & Performance Tuning
17,» Optimization and Tuning
17,» Optimization and Indexes
17,» Descending Indexes
17,Comments - Descending Indexes
17,"5 months, 3 weeks ago"
17,Marko Mäkelä
17,Re: Descending Indexes
17,[Answer]
17,MDEV-13756 implemented descending index. The earliest long-term support (LTS) release that includes the feature is MariaDB Server 10.11.
17,"Content reproduced on this site is the property of its respective owners,"
17,"and this content is not reviewed in advance by MariaDB. The views, information and opinions"
17,expressed by this content do not necessarily represent those of MariaDB or any other party.
17,Products
17,MariaDB Platform
17,MariaDB SkySQL
17,Pricing
17,Download MariaDB
17,Services
17,Remote DBA
17,SkyDBA
17,Enterprise Architect
17,Technical Support
17,Migration Practice
17,Consulting
17,Training
17,Resources
17,Documentation
17,Developers
17,Blog
17,Support
17,OpenWorks
17,Customer Stories
17,Events
17,MariaDB Roadshow
17,About
17,Contact
17,Leadership
17,Partners
17,Newsroom
17,Investors
17,Careers
17,Trust Center
17,Vulnerability Reporting
17,Contact
17,Subscribe to our newsletter!
17,Legal
17,Privacy Policy
17,Cookie Policy
17,Copyright © 2023 MariaDB. All rights reserved.
18,MySQL Performance Tuning Tool - Releem
18,SQL Query OptimizationProductWhy ReleemCompatibilityDocumentationFAQFor DevelopersCompareMySQLTunerPricingBlogAboutLog inSign up free ProductSQL Query OptimizationWhy ReleemCompatibilityHow it works?DocumentationFAQFor DevelopersCompareMySQLTunerPricingBlogAboutLog inMySQL Performance Tuning and Monitoring tool
18,"Releem automatically tunes MySQL configuration to improve performance and reduce server costs. Tune MySQL NowMySQL Performance Tuning Tool Save time, Boost DB, Reduce costs! Free. No credit card required."
18,"How it works Releem is a MySQL Tuning Tool consisting of 3 components that work together to assess and configure MySQL servers 1Releem Agent collects metrics Releem Agent automatically collects MySQL metrics, system information and transfers them to the Releem Cloud Platform 2Releem Cloud Platform analyzes metrics Releem Cloud Platform analyzes collected metrics, calculates Releem Score and automatically detects performance issues 3Releem Cloud Platform makes recommendations Releem Cloud Platform recommends new MySQL server configuration to improve performance 4Releem Agent applies recommended configurations You can apply the new configuraton file manually or automatically Try Releem today for FREE! No credit card required.Sign Up FreeWhy Releem? Using Releem developers can focus on improving their products without worrying about gaining a depth knowledge of the database backend domain Clutter FreeReleem provides simple dashboard and it cuts through the noise. No layers of menus, no need for custom reports. Get all the important metrics on one single page. No training necessary.Hassle freeSimple one-step Installation on most popular Linux platforms and Support of all MySQL/MariaDB/Percona versions, and cloud services like AWS RDS and Aurora.Performance BoosterRecommended configuration delivers a 30% boost to MySQL 8 / MySQL 5.7 / MariaDB and Web Applications performance compared to the default configuration.Health ChecksReleem performs health checks twice a day, providing users with up-to-date insights describing the efficiency and ""best practices"" of using Memory, Connections, Logs, Cache, Disk, Indexes, and Threads. Releem calculates the Releem Score metric by summarizing Health Checks results.SecuritySecurity is our top priority. Releem does not use your database data. It uses only MySQL metrics and system information, and HTTPS to transfer them. Releem Agent is open-source and can be reviewed to ensure it meets your security requirements.Email reportsKeep an eye on your servers with weekly email reports.Compatibility A word from some of our happy users ❤️"
18,We work hard every day to make your database servers faster
18,"Releem has proven its effectiveness for thousands of databases. Increase the performance of your MySQLquickly and easily. No credit card required. Sign Up FreeAbout us We are an international team. Our current MySQL server tuning and maintenance experience spans over 10 years, through which we have been cutting costs of our customers by boosting their MySQL servers. Roman AgabekovFounder & CEONews and Articles How MySQL Configuration Impacts the Performance of Web ApplicationsMySQL Configuration tuning is an important component of database management implemented by database professionals and administrators. It aims to configure the database to suit its hardware and workload.read moreKeeping an Eye on MySQL with Health Checks Health checks play a crucial role in ensuring the smooth operation of databases. These checks provide insights into a database's health, allowing administrators to monitor performance, diagnose issues, and take proactive measures to resolve them.read moreMySQL 5.7 Performance BenchmarkIn this article, we benchmark the performance of MySQL 5.7 default configuration vs. configuration recommended by Releem.read moreMySQL 8 Performance BenchmarkIn this article, we benchmark the performance of MySQL 8 default configuration vs. configuration recommended by Releem.read morehello@releem.com Contact us Still have questions? Reach us below! +1 984 368-5788Releem, Inc.500 Westover Dr #11329, Sanford, NC 27330 US"
18,"Any questions?Have a question about Releem, performance tuning, pricing, partnerships or anything else you're curious about, let's talk. Our team is more than happy to help. Your nameYour e-mailCompanyHow can we help? (optional) Send This website uses cookies to ensure you get the best experienceOK © 2023 Releem, Inc.+1 984 368-5788500 Westover Drive #11329 Sanford, NC 27330 US Product SQL Query OptimizationFeaturesCompetitorsPricingDocumentationFor DevelopersSocialTwitterFacebookLinkedInGitHubCompany FAQ Blog Privacy"
18,About us Sitemap
19,Limitations - Azure Database for MariaDB | Microsoft Learn
19,Skip to main content
19,This browser is no longer supported.
19,"Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
19,Download Microsoft Edge
19,More info about Internet Explorer and Microsoft Edge
19,Table of contents
19,Exit focus mode
19,Read in English
19,Save
19,Table of contents
19,Read in English
19,Save
19,Edit
19,Print
19,Twitter
19,LinkedIn
19,Facebook
19,Email
19,Table of contents
19,Limitations in Azure Database for MariaDB
19,Article
19,09/19/2023
19,7 contributors
19,Feedback
19,In this article
19,Important
19,"Azure Database for MariaDB is on the retirement path. We strongly recommend for you to migrate to Azure Database for MySQL. For more information about migrating to Azure Database for MySQL, see What's happening to Azure Database for MariaDB?"
19,"The following sections describe capacity, storage engine support, privilege support, data manipulation statement support, and functional limits in the database service."
19,Server parameters
19,Note
19,"If you are looking for min/max values for server parameters like max_connections and innodb_buffer_pool_size, this information has moved to the server parameters article."
19,"Azure Database for MariaDB supports tuning the values of server parameters. The min and max value of some parameters (ex. max_connections, join_buffer_size, query_cache_size) is determined by the pricing tier and vCores of the server. Refer to server parameters for more information about these limits."
19,"Upon initial deployment, an Azure for MariaDB server includes systems tables for time zone information, but these tables aren't populated. The time zone tables can be populated by calling the mysql.az_load_timezone stored procedure from a tool like the MySQL command line or MySQL Workbench. Refer to the Azure portal or Azure CLI articles for how to call the stored procedure and set the global or session-level time zones."
19,"Password plugins such as ""validate_password"" and ""caching_sha2_password"" aren't supported by the service."
19,Storage engine support
19,Supported
19,InnoDB
19,MEMORY
19,Unsupported
19,MyISAM
19,BLACKHOLE
19,ARCHIVE
19,Privileges & data manipulation support
19,"Many server parameters and settings can inadvertently degrade server performance or negate ACID properties of the MariaDB server. To maintain the service integrity and SLA at a product level, this service doesn't expose multiple roles."
19,The MariaDB service doesn't allow direct access to the underlying file system. Some data manipulation commands aren't supported.
19,Privilege support
19,Unsupported
19,The following are unsupported:
19,"DBA role: Restricted. Alternatively, you can use the administrator user (created during new server creation), allows you to perform most of DDL and DML statements."
19,"SUPER privilege: Similarly, SUPER privilege is also restricted."
19,"DEFINER: Requires super privileges to create and is restricted. If importing data using a backup, remove the CREATE DEFINER commands manually or by using the --skip-definer command when performing a mysqldump."
19,System databases: The mysql system database is read-only and used to support various PaaS functionalities. You can't make changes to the mysql system database.
19,SELECT ... INTO OUTFILE: Not supported in the service.
19,"Azure Database for MariaDB supports at largest, 1 TB, in a single data file. If your database size is larger than 1 TB, you should create the table in innodb_file_per_table tablespace. If you have a single table size larger than 1 TB, you should use the partition table."
19,Supported
19,"LOAD DATA INFILE is supported, but the [LOCAL] parameter must be specified and directed to a UNC path (Azure storage mounted through SMB)."
19,Functional limitations
19,Scale operations
19,Dynamic scaling to and from the Basic pricing tiers is currently not supported.
19,Decreasing server storage size isn't supported.
19,Server version upgrades
19,Automated migration between major database engine versions is currently not supported.
19,Point-in-time-restore
19,"When using the PITR feature, the new server is created with the same configurations as the server it's based on."
19,Restoring a deleted server isn't supported.
19,Subscription management
19,Dynamically moving pre-created servers across subscription and resource group is currently not supported.
19,VNet service endpoints
19,Support for VNet service endpoints is only for General Purpose and Memory Optimized servers.
19,Storage size
19,Please refer to pricing tiers for the storage size limits per pricing tier.
19,Current known issues
19,"MariaDB server instance displays the incorrect server version after connection is established. To get the correct server instance engine version, use the select version(); command."
19,Next steps
19,What's available in each service tier
19,Supported MariaDB database versions
19,Feedback
19,Submit and view feedback for
19,This product
19,This page
19,View all page feedback
19,Additional resources
19,Theme
19,Light
19,Dark
19,High contrast
19,Previous Versions
19,Blog
19,Contribute
19,Privacy
19,Terms of Use
19,Trademarks
19,© Microsoft 2023
19,Additional resources
19,In this article
19,Theme
19,Light
19,Dark
19,High contrast
19,Previous Versions
19,Blog
19,Contribute
19,Privacy
19,Terms of Use
19,Trademarks
19,© Microsoft 2023
21,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
21,Home Page
21,Cloud IaaS Solutions
21,Azure VM Solutions
21,AWS VM Solutions
21,GCP VM Solutions
21,WordPress SSO
21,About Us
21,Contact Us
21,Home Page
21,Cloud IaaS Solutions
21,Azure VM Solutions
21,AWS VM Solutions
21,GCP VM Solutions
21,WordPress SSO
21,About Us
21,Contact Us
21,Home Page
21,Cloud IaaS Solutions
21,Azure VM Solutions
21,AWS VM Solutions
21,GCP VM Solutions
21,WordPress SSO
21,About Us
21,Contact Us
21,Aug
21,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
21,by Dennis Muvaa
21,in MariaDBComments
21,MariaDB Performance Tuning: Optimize MariaDB for High Traffic. MariaDB is an open source relational database management system (RDBMS) designed as a suitable alternative to MySQL. Supports multiple storage engines and enables users to easily scale out their database.
21,"To achieve optimal database performance, it’s essential to optimize MariaDB server sufficiently. It reduces query time and improves user experience. Essential when running critical applications that rely on real time data processing. Optimizing MariaDB server leads to efficient resource usage, lower CPU and memory usage, and lower disk I/O."
21,This article discusses MariaDB Performance Tuning: Optimize MariaDB for High Traffic. Read on!
21,Also ReadHow to MariaDB/MySQL Show Users and GRANTS Privileges
21,How to Optimize Your MariaDB Server for High Traffic
21,1. Optimize Your Database Schema
21,"Well, it is essential to normalize your database schema. This step involves reducing data redundancy and bolstering data integrity by breaking down large tables into smaller ones and creating relationships between them. Also, denormalize tables at times i.e merge small tables into larger tables to minimize the need for complex querying."
21,Also ReadHow to setup MariaDB on Windows in Azure/AWS/GCPHow to Setup MariaDB Ubuntu Cloud in Azure/AWS/GCP
21,2. Choose Appropriate Data Types
21,"Choose the right data types for your table columns. It saves space, improves query speed, and strengthens data integrity. In addition, consider using indexes. They greatly improve the speed of data retrieval operations. However, it’s best to note that while indexes improve read performance, they also slow down write operations."
21,3. Use Connection Pooling
21,"Connection pooling helps optimize MariaDB server performance by addressing connection overhead issues. Each new database connection comes with a significant amount of overhead due to handshaking protocols and authentication procedures. These procedures consume considerable processing power and time, especially if the application needs to frequently open and close connections. Therefore, by allowing a pool of reusable connections, the system does to have to repeat this process multiple times, hence improving MariaDB performance."
21,"Use persistent connections, that remain open over multiple requests. This significantly reduces the latency. Overall, it reduces the time it takes to execute a large number of database queries. Especially helpful in high-volume transaction."
21,"Also, leverage connection pooling solutions like ProxySQL. ProxySQL acts as a gateway between MariaDB and the application layer. It manages the connections to the database server and effectively reduces the burden on the database. The pooling solution dynamically adjusts the number of connections in the pool based on the load, allowing for better resource management. Most pooling solutions come with intelligent routing capabilities that help direct queries to different servers based on their type and load."
21,"Also ReadWhat are MariaDB Data Types (Numeric, Date, String) Explained"
21,4. Use Appropriate Storage Engines
21,Following with MariaDB Performance Tuning: Optimize MariaDB for High Traffic. is to choose the right storage engine significantly affects performance. MariaDB has 6 main storage engines:
21,InnoDBMyISAMMariaDB ColumnStoreAriaSpiderMyRocks
21,"InnoDB, the default storage engine for MariaDB and is ideal for transaction heavy workloads. It supports row-level locking, allowing for higher concurrency and faster recovery. Alternatively, use MyISAM for workloads with heady read and light write scenarios. This storage engine has a small footprint that InnoDB anand consumes less memory and disk space than InnoDB. Aria is a modern improvement of MyISAM and allows easy copying between systems."
21,ColumnStore has a parallel data architecture and is suitable for processing petabytes of data. MyRocks provides greater compression than InnoDB and is ideal for high throughput operations. Spider supports partitioning and allows handling of multiple instances as if they were on the same instance.
21,"There are lots of other MariaDB storage engines, each designed for particular operations."
21,5. Allocate Memory Sufficiently
21,Tune memory allocation to improve MariaDB performance. There are various buffers and caches to configure when allocating memory. These include:
21,join_buffer_sizenet_buffer_lengthread_buffer_sizesort_buffer_sizemax_heap_table_sizemrr_buffer_size
21,"The join_buffer_size controls the size of the buffer used for full join operations. The sort_buffer_size dictates the size of the buffer used for sorting. The read_buffer_size is used for sequential table scans, while read_rnd_buffer_size is used when the server reads rows in a sorted order following a sort operation."
21,Also ReadMariadb Create Table – How to Guide (Step by Step)
21,6. Fine-Tune Your Server Configuration
21,"Tweak server settings. Adjust InnoDB settings such as the innodb_buffer_pool_size, where InnoDB engine caches table and index data. A well proportioned buffer pool significantly boosts the database performance by lowering the amount of disk I/O."
21,"If you are using the MyISAM storage engine, consider optimizing parameters like key_buffer_size, used to cache index blocks for MyISAM tables, and thread_cache_size, which sets the number of threads that the server should cache for reuse."
21,"Remember to adjust MariaDB’s query cache feature for optimal performance. For instance, when a SELECT statement is executed, the result set is stored in the query cache. If a similar statement is executed again, the server retrieves the results from the cache instead of executing the query again."
21,7. Use the Right Table Partitioning
21,"Table partitioning improves performance of large databases. It involves dividing a single table into smaller, more manageable pieces. There are different types of table partitioning in MariaDB:"
21,List PartitioningRange PartitioningHash PartitioningKey Partitioning
21,"Range partitioning assigns a range of values to each partitioning. This means each partition contains rows for which the partitioning expression value lies within a given range. On the other hand, list partitioning assigns a list of values to each partition."
21,"In Hash partitioning, the server decides the partition for each data to achieve even partitions.  Well, it applies a consistent hash function to a key and then assigns rows based on the hash function’s result. Key partitioning is a variant of hash partitioning whereby the server assigns the distribution of rows across partitions."
21,Also ReadHow to Install MariaDB on Ubuntu 22.04
21,8. Optimize Your Query
21,"Optimizing your SQL queries helps boost MariaDB server performance. Well-written SQL query reduces the amount of time taken to fetch data. To understand how your queries perform, use the built-in EXPLAIN feature. It helps you understand how MariaDB executes your queries and enables you to see the different types of indexes are in place. With EXPLAIN, you also understand the sequence of table joins."
21,Optimize SQL query by reducing the result set size. Fetching more data than necessary not only slows your query but also adds more burden to your server. Restrict the number of rows your query returns through LIMIT clauses.
21,"Indexing all columns used in ‘where’, ‘group by’, ‘order by’ and ‘join’ clauses from the start. This ensures that the database does not do complete table scans in order to retrieve records. However, you should avoid using functions on indices as MariaDB tends to ignore the index."
21,9. Optimize Hardware Components
21,"To run an efficient MariaDB database, you need robust hardware with the right configurations. First, use solid-state drives (SSDs) for data storage rather than hard disks as they can significantly enhance disk I/O speed. SSDs provide faster read/write speeds compared to traditional hard drives."
21,"Memory optimization is also crucial, as a large memory means larger key and table caches. MariaDB stores data and indexes in memory to quicken read and write operations. To achieve optimal performance, it’s imperative to first set the server variables to utilize the available memory. In addition, use the highest RAM size per slot as it reduces latency compared to having more RAM slots on the motherboard."
21,"Secondly, you need extremely fast processors especially if you run critical workloads. A CPU with fast speeds allows for faster calculations enabling the clients to receive results quickly. CPU resources should also be allocated properly. A server with more CPUs handles more simultaneous connections and perform more operations in parallel."
21,Also ReadHow to Install MariaDB on Windows Server 2019 (Tutorial)
21,10. Use Concurrency and Threading
21,Understanding thread concurrency and how to apply it to your MariaDB server helps improve performance issues. Optimize how InnoDB multitasks between transactions and requests.
21,"InnoDB manages requests via threads, whereby each thread represents a single unit of processing. Therefore, you should configure system variables so as to have many threads executed at the same time."
21,11. Adjust Binary Log Settings
21,"The binary log records changes made to the database and is used for replication and recovery purposes. Choosing the right format for replication is crucial for performance. The row-based format logs changes made to each row, while the mixed format logs changes on a statement basis."
21,Adjust the binary log cache size to control the amount of memory allocated for caching changes before they are written to the binary log. Managing log retention with the expire_logs_days parameter avoids consuming too much disk space.
21,Also ReadHow to Install MariaDB on CentOS 8 (Step by Step Guide)
21,12. Disable DNS Reverse Lookups
21,"Disabling DNS reverse lookups improve the performance of a MariaDB database. Whenever a client connects to the database, MariaDB performs a DNS reverse lookup to convert the client’s IP address to a hostname. This process increases latency hence reducing speed although it provides additional security by verifying the identity of the client. The delay can be significant depending on the network configuration and the DNS server speed. This latency increases if there is a large number of clients connecting to the MariaDB server and is detrimental to the overall performance."
21,"By disabling DNS reverse lookups, MariaDB avoids potential delays and speeds up the process of establishing connections. This saves time especially in a system with high transaction rates or where numerous connections are being established and closed. Besides, it allows MariaDB to accept and handle more client connections in a shorter amount of time."
21,"Disabling DNS reverse lookups comes with potential security risks. DNS lookups provide an additional layer of protection by authenticating the source of incoming connections. Therefore, disabling DNS reverse lookups applies to systems with other robust security measures."
21,13. Check for Idle Connections
21,"Checking for idle connections is a crucial strategy in faster performance. Idle connections represent open database connections that are currently not in use. These connections, although inactive, consume resources. They occupy slots in the connection pool that could be used by other active connections, thereby potentially limiting the database’s ability to handle new incoming connections."
21,"Over time, a build-up of idle connections lead to a degradation in the performance of MariaDB. Identifying and properly handling these idle connections, such as by setting a timeout or closing them after a certain period of inactivity, frees up system resources and improve the overall performance."
21,14. Setting Your Disk I/O Scheduler
21,"Last point of the article MariaDB Performance Tuning: Optimize MariaDB for High Traffic. The Disk I/O Scheduler is a component of the operating system that decides in what order the block I/O operations is submitted to storage devices. Depending on the type of workload, different scheduling algorithms are more efficient. For example, some schedulers prioritize minimizing seek time for read-heavy applications, while others optimize for write-heavy or mixed workloads."
21,Thank you for reading MariaDB Performance Tuning: Optimize MariaDB Server for High Traffic. We shall conclude the rticle now.
21,Also ReadMariaDB vs MySQL Performance Differences (Pros and Cons)
21,MariaDB Performance Tuning: Optimize MariaDB for High Traffic
21,Conclusion
21,"Optimizing MariaDB server is a continuous process, as performance is not pegged on a single factor. Use database monitoring tools to gain visibility into the state of their server and the database. By following the above practices, you are able to avoid lots of performance bottles and have your applications running seamlessly."
21,Related Posts:Optimize Memory & CPU Usage in Node.js: Performance Tuning TechniquesHyper-V Performance: Optimize VM Performance (Memory & CPU)MySQL Performance Tuning: For Optimal Database PerformanceWhat is HPC? High Performance Computing and How it WorksCloud HPC Architecture for Hybrid High-Performance ComputingUsing Redis for Caching: Best Practices and Performance Tuning
21,Tags:
21,MariaDB
21,Dennis Muvaa
21,"Dennis is an expert content writer and SEO strategist in cloud technologies such as AWS, Azure, and GCP."
21,"He's also experienced in cybersecurity, big data, and AI."
21,votes
21,Article Rating
21,Subscribe
21,Login and comment with
21,I allow to create an account
21,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
21,DisagreeAgree
21,Notify of
21,new follow-up comments
21,new replies to my comments
21,Login and comment with
21,I allow to create an account
21,"When you login first time using a Social Login button, we collect your account public profile information shared by Social Login provider, based on your privacy settings. We also get your email address to automatically create an account for you in our website. Once your account is created, you'll be logged-in to this account."
21,DisagreeAgree
21,Please login to comment
21,0 Comments
21,Inline Feedbacks
21,View all comments
21,Cloud Infrastructure Services Ltd
21,2A Cedar Drive
21,Hatch End
21,Pinner
21,"HA5 4DE, UK"
21,Terms
21,Privacy Policy
21,Recent Posts
21,Real Time Communication with Node.js: Building WebSocket Apps
21,Magento Server Migration: Migrate Magento Store to a New Server
21,Docker Swarm Tutorial: Orchestrate Containers for Scalable Apps
21,How to Implement FTP in Your Organization: A Step-by-Step Guide
21,How to Set Up a Public Key Authentication for SFTP
21,Build MySQL HA/Replication Fault-Tolerant Architecture
21,"How to Create a RESTful API Using Node, Express, and MongoDB"
21,Node.js Caching Strategies: Improve Performance and Scalability
21,Docker and Kubernetes: How to Manage Containers at Scale
21,How to Setup Jenkins Build Agent on Windows (Tutorial)
21,PagesContact Us
21,About Us
21,Azure Marketplace Solutions
21,AWS Marketplace Solutions
21,GCP Marketplace Solutions
21,Azure Management
21,Cloud IaaS Setup & Management Services
21,Active Directory Reporting Tool
21,WordPress SSO
21,Blog
21,Follow Us
21,"wpDiscuz00Would love your thoughts, please comment.x()x| ReplyInsert"
22,MariaDB Skills Assessment Test | iMocha
22,"'Skills-First' Companies have achieved 2x higher revenue growth than their peers. Don't miss out.Download the report now!ProductsSkills Intelligence Cloud ™Skills IntelligenceTalent AcquisitionTalent ManagementCampus HiringServicesWhy iMochaWorld’s Largest Skill LibraryAssessment InnovationBusiness English AssessmentResourcesBlogGuidesHR HandbookSkill MappingWebinarCompanyAbout UsPartnersContact UsCareersNewsroomCustomersLoginStart your free trialHomeDatabase TestsMariaDB Skills TestTest duration: 30 minNo. of questions:20Level of experience:Entry levl/Mid/SeniorMariaDB Skills Assessment TestThe assessment will help you assess various skills of the candidates and employees, such as Database Design and Schema Creation, Performance Optimization, Data Manipulation,  Administration etc. It also enables you to improve your interview-selection ratio by 70% and measure the learning programs' ROI.Start your free trialWhat is MariaDB?It is an open-source relational database management system (RDBMS) that is designed to be a drop-in replacement for MySQL. It also aims to provide a reliable, scalable, and high-performance database solution while maintaining compatibility with MySQL.Why use iMocha’s MariaDB skills test?This skill test assesses the candidates' and employees' ability to work on Database Design and Schema Creation, Performance Optimization, etc. Thus, recruiters and L&D managers can leverage this test to identify potential and existing employees' weaknesses and strengths to make intelligent talent decisions.Wondering what other skills we have in our World’s Largest Skills Assessment library?Visit hereHow it worksTest SummaryThe skill assessment evaluates the following skills of individuals:Understanding of DB Query Language (SQL) and ability to write efficient and accurate SQL queries.Understanding of concepts such as tables, columns, indexes, and relationships, and being able to create well-structured and optimized database structures.Knowledge of performance optimization techniques.Proficiency in performing common data manipulation tasks such as inserting, updating, and deleting records.Configuring database settings, optimizing memory usage, tuning buffer sizes, and other techniques to improve overall database performance.Knowledge of maintaining data integrity in MariaDB, including the use of constraints, transactions, and error handling.The test analytics feature also allows you to compare candidate scores and make informed decisions about who to hire and where to put extra training efforts.Useful for hiringMariaDB Database DeveloperMariaDB Database EngineerMariaDB Database ArchitectMariaDB Database AnalystMariaDB Database ConsultantMariaDB Database SpecialistMariaDB SQL DeveloperMariaDB Performance Tuning SpecialistMariaDB Database Migration SpecialistTest Duration30minNo. of Questions20Level of ExpertiseEntry levl/Mid/SeniorTopics CoveredSample QuestionChoose from our 100,000+ questions library or add your own questions to make powerful custom tests.Question typeTopics coveredDifficultyQuestion:A helicopter view of the employee's progressView Full ReportYou can customize this test bySetting the difficulty level of the testChoose easy, medium, or tricky questions from our skill libraries to assess candidates of different experience levels.Combining multiple skills into one testAdd multiple skills in a single test to create an effective assessment and assess multiple skills together.Adding your ownquestions to the testAdd, edit, or bulk upload your coding, MCQ, and whiteboard questions.Requesting a tailor-made testReceive a tailored assessment created by our subject matter experts to ensure adequate screening.Start your free trialRelated SkillsDatabase SQL fundamentals testMySQL DBA assessment testOracle DBA Online testFAQHow is this skill test customized?This skill test can be customized with the help of iMocha's SMEs (Subject Matter Experts). They can create a custom set of questions on areas like Database Design and Schema Creation, Performance Optimization, Data Manipulation, Administration, etc. Furthermore, you can also set the difficulty level of the question to assess individuals' abilities better.What are the most common interview questions related to MariaDB?Here are some common interview questions related to MariaDB:‍Explain the process of installing and setting up this tool.What are the different storage engines available? Can you explain their differences and use cases?How can you import and export data in this tool? Explain the available methods.What is the purpose of the ""EXPLAIN"" statement in MariaDB? How can it be used to optimize query performance? If you are looking for a more custom set of questions, iMocha can help!What are the required skillsets to work on MariaDB?Here is a list of technical and non-technical skills required for the role:Technical Skills:Database ManagementSQL DevelopmentData Import/ExportPerformance OptimizationReplication and High Availability Non-Technical Skills:Problem-SolvingAttention to DetailCommunicationTime ManagementContinuous LearningPopular FeaturesSkills Intelligence CloudTalent AcqusitionTalent ManagementWorld Largest Skill LibraryRemote HiringCampus RecruitmentDiversity & InclusionAI-LogicBoxCoding SimulatorsLive Coding InterviewAutomated Video InterviewsSmart Video ProctoringBusiness English AssessmentTrending AssessmentsAll SkillsCoding SkillsCognitive SkillsDesign SkillsDomain SkillsEntry-level SkillsLanguage SkillsNext Gen SkillsSoft SkillsTech SkillsResourcesBlogsJD TemplatesHR GlossarySkills Intelligence: Hiring MetricTech Skills Transformation ReportCampus Hiring PlaybookiMocha Hiring Trends 2023CompanyAbout UsPartnersContact UsCareersNewsroompopular featuresSkills Intelligence CloudTalent AcqusitionTalent ManagementWorld Largest Skill LibraryRemote HiringCampus RecruitmentDiversity & InclusionAI-LogicBoxCoding SimulatorsLive Coding InterviewAutomated Video InterviewsSmart Video ProctoringBusiness English AssessmentTrending AssessmentsAll SkillsCoding SkillsCognitive SkillsDesign SkillsDomain SkillsEntry-level SkillsLanguage SkillsNext Gen SkillsSoft SkillsTech SkillsContact uscontact@imocha.io+1 408 915 5185Work with usResourcesBlogsJD TemplatesHR GlossarySkill MappingSkills Intelligence: Hiring MetricTech Skills Transformation ReportCampus Hiring PlaybookiMocha Hiring Trends 2023CompanyAbout UsPartnersContact UsCareersNewsroomContact uscontact@imocha.io+1 408 915 5185Work with us"
22,Copyright © 2023 iMochaContact uscontact@imocha.io1-408-759-4457SecurityPrivacy Policy Terms of ServiceGDPR
22,We use cookies to ensure you have the best browsing experience on our website. Please read our privacy policy for more information about how we use cookies.
24,"Magento 2 MySQL Optimization Guide 2023 | Onilab BlogServicesAboutCareersPortfolioContactsBlogGet in touchHomePortfolioServicesContactsBlogMoreTable of ContentTable of Content1. The Role of Overall Magento Store Optimization2. Remove Useless Entries From Magento Database3. Switch to a Magento MySQL Database Alternative4. DO NOT Switch to Flat Catalogs5. Turn on Logging to Find Poorly Performing Queries6. Update Your Database Version7. Add Indexes to Large Tables8. Set Elasticsearch/OpenSearch as Your Search Engine9. Discover Hidden Issues Using Advanced ToolsMaking Magento Database as Fast as PossibleView moreEcommerceMagentoPerformance Optimization11 minJul 11, 202311 minJul 11, 20238 Levels of Magento 2 MySQL Optimization (Updated 2023)Alex Husar, Onilab CTOAlex Husar, Onilab CTOMySQL can become a real issue with database queries taking too long when you lack indexes for the most popular entries or have an improperly configured DB server. Furthermore, it also affects the time to first byte metric and Magento performance in general.In this guide, we assume you already know your database is the real bottleneck of the system, so there's no need to investigate this issue further. Here we'll discuss actionable steps to optimize the Magento MySQL database.Table of ContentTable of Content1. The Role of Overall Magento Store Optimization2. Remove Useless Entries From Magento Database3. Switch to a Magento MySQL Database Alternative4. DO NOT Switch to Flat Catalogs5. Turn on Logging to Find Poorly Performing Queries6. Update Your Database Version7. Add Indexes to Large Tables8. Set Elasticsearch/OpenSearch as Your Search Engine9. Discover Hidden Issues Using Advanced ToolsMaking Magento Database as Fast as PossibleView moreThe Role of Overall Magento Store OptimizationTo see a significantly better store loading speed, Magento 2 database optimization should be a part of a broader optimization strategy. We recommend you read our Magento optimization guide, with a comprehensive list of improvements you can make for Magento cache, servers, media, and code.And if you're ready to delve deeper into the matter, our other guides on Magento optimization for Google Page Speed Insights and Magento Core Web Vitals optimization definitely come in handy. And without further ado, let's see how to diagnose and troubleshoot Magento database performance issues.1. Remove Useless Entries From Magento DatabaseWe've already mentioned this tip in our Magento Add to Cart slow guide. This is a great solution for more than just Shopping Cart optimization. Truncating works well for bloated log entries, various system events, comparisons inside the catalog, and some other files you might not really care about.Create a backup and then use the TRUNCATE command on the following tables:dataflow_batch_export,dataflow_batch_import,log_customer,log_quote,log_summary,log_summary_type,log_url,log_url_info,log_visitor,log_visitor_info,log_visitor_online,report_viewed_product_index,report_compared_product_index,report_event,index_event,catalog_compare_item,catalogindex_aggregation,catalogindex_aggregation_tag,catalogindex_aggregation_to_tag,adminnotification_inbox,aw_core_logger.In case you like to automate ALL THE THINGS, take a look at our speed optimization guide, where you can find step-by-step instructions on how to install and use a dedicated module that cleans Magento store logs via cron once a week.2. Switch to a Magento MySQL Database AlternativeTraditional MySQL implementations might be the safest choice for your Magento install, but if you want to squeeze every last bit of juice out of your Magento database setup, look into switching to either MariaDB or Percona. But what's the difference between these 3 systems?MySQLIn general, the biggest advantage of MySQL is its long-term endurance. MySQL entered the market in the 90s and has been in the leading positions ever since.Magneto MySQL 8.0 boasts two times higher speed in comparison to its previous version supported by Magento. It also has improved compatibility, robust architecture, and huge help documentation. To list both pros and cons, MySQL, in some cases, requires a paid license for commercial use, offers limited capabilities in scaling and security, and generally rests on its laurels too much.MySQL advantages for Magento:With MySQL, one thing you need to understand well is expectations. MySQL is a great baseline solution for database management if you don't expect the store to handle extreme loads. Sure, it's a bit basic, but it gets the job done. You will always find tons of help online and knowledgeable teams ready to manage your database in a conservative but solid way.MariaDBMariaDB is more community-focused and works better for users who care more about features than pure performance. MariaDB supports a large array of database engines, disk encryption, complex horizontal interconnectivity, and scaling features which could be interesting for a large Magento store.One thing to consider, though. MariaDB supports switching from MySQL to MariaDB, but not the other way around if you plan on using a complex replication schema with global transactional IDs. Migration is a one-way ticket for those who would like to make a move, so think twice. Backward compatibility could be important if you are just experimenting and not ready to commit to MariaDB in the long term.Among prominent MariaDB adherents are Ansell, America Movil, BlaBlaCar, Nokia, Red Hat, and Samsung.MariaDB advantages for Magento:Advanced Magento sharding support (Spider storage engine or Galera cluster);In-depth horizontal replication between multiple sources, intricate scaling, and connectivity;Performance boost compared to the default MySQL configuration (see this review for an in-depth comparison of different workloads).PerconaPercona is a fork of MySQL that centers around performance and peak load handling. Compared to MariaDB, Percona is a less popular, more specialized database engine. Even though the two alternatives were released roughly the same time (2008-2009), they have very different adoption rates and community sizes.Among high-profile Percona fans are Facebook, Netflix, SoundCloud, and Adobe.Percona advantages for Magento:Better performance for large datasets and heavy loads especially using expensive enterprise-level hardware (thread pool scaling, 48-CPU scaling limit);Noticeable advantages for high-load eCommerce applications;Good security (encryption, advanced user isolation through sharding, audit logging, PAM authentication);Proper database performance optimization and diagnostics tools (query logging, I/O count, access counters, etc.).MariaDB or Percona?Choose MariaDB if you need more quality-of-life and DevOps features. Go for Percona if you aim to gain high-load performance in large-scale datasets.NOTE: Comparing MySQL, Percona, and MariaDB, you would inevitably come to comparisons of XtraDB and InnoDB. Here's our take on the situation. XtraDB is only slightly better than InnoDB, which means you shouldn't worry too much about which engine you actually use. The gain in Magento performance is not worth the upgrade. You are better off investing in other optimization efforts.3. DO NOT Switch to Flat CatalogsFlat catalogs used to help reduce database load. It worked like this: flattening cuts down on DB queries since the system can fetch the same amount of data with fewer calls. In large-scale stores with vast product counts, these gains from flat catalogs and flat product categories were especially noticeable, but only up to Magento 2.1.x versions.But if you run the site on newer editions, flat catalog usage becomes more of a problem. With flat tables and indexers enabled you can encounter ""performance degradation and other indexing issues"". So, according to official Magento guidelines, the flat catalog feature must be turned off on Magento 2.1.x versions and higher (the type of Adobe Commerce edition doesn't matter).In Magento 2 Admin Panel, go to: Stores > Configuration > Catalog. Make sure to switch to ""NO"" on both ""Use Flat Catalog Category"" and ""Use Flat Catalog Product"".4. Turn on Logging to Find Poorly Performing QueriesMySQL offers built-in tools to facilitate performance optimization, one of which is a slow query log. Turn it on, define which kind of queries you consider too slow, and watch the log grow. All queries longer than a certain amount will end up here.For example, let's start with populating the log with queries longer than 1 second. Once you deal with the slower ones, you can move on to queries longer than 500 ms, etc. To define which queries the system should consider slow, use the long_query_time value.Use MySQL EXPLAIN statement to find out the structure and contents of each slow query and properly analyze it.5. Update Your Database VersionSounds obvious, but newer is always better. Update your Magento database to the latest version to improve performance, security and fix bugs. New versions often boast better read and write I/O speeds, concurrency performance, improved algorithms, and other upgrades. Keeping your database up to date costs literally nothing.As of July 2023, the latest supported version of MySQL is 8.0, and MariaDB is 10.6.6. Add Indexes to Large TablesProcessing queries from larger MySQL tables can be extremely slow, up to 30-60 seconds in the worst cases. Add indexes to the tables to reduce the amount of time needed to process data. The bigger the table, the more visible the website performance gain. The only downside of total indexing is data storage.Why don't extension developers add indexes to their tables? Most of the time, they either don't bother to add or forget to do that before release. So it's up to you to fix this issue. All tables will benefit from having an index.7. Set Elasticsearch/OpenSearch as Your Search EngineIf your Magento is older than 2.4.x version, then it's worth switching from MySQL as a default search option to Elasticsearch or another engine. Elastic-powered search and layered navigation work faster than the default MySQL setup. Elasticsearch will lift some workload from your database and, at the same time, offer better performance, especially if you move it to a separate machine.If your store is on Magento 2.4.x or a later version, then Elasticsearch/OpenSearch will be your built-in search engine. Learn more about how to improve Magento search in our comprehensive guide.Another MySQL tuning tip is to turn off product count from layered navigation. This setting slows down page loading and doesn't bring a lot of value in return. Switching it off is standard practice in most optimization scenarios.Magento versionGo to ""Stores > Configuration > Catalog > Catalog. Locate ""Layered Navigation"" then untick ""Use System Value"" and set ""Display Product Count"" to ""No"".8. Discover Hidden Issues Using Advanced ToolsMySQLTuner is a well-known diagnostics script that helps developers find issues in their MySQL setups and fix them following the script's recommendations. It can uncover hidden issues that are otherwise hard to see.If you have already implemented all our tips and looking for more action, run MySQLTuner and see what it can find. Alternatively, use the ProfileSQL tool to analyze each database request and optimize them one by one. Amongst other advanced tools that could help you with database optimization, we can name the Tuning Primer script which focuses on the following tables:Slow Query Log,Max Connections,Worker Threads,Memory Usage,Key Buffer,Query Cache,Sort Buffer,Joins,Temp Tables,Table (Open & Definition) Cache,Table Scans (read_buffer),Table Locking,InnoDB Status.If you want to know more about how to speed up your Magento backend further, we've created an in-depth guide to reveal all the secrets.Making Magento Database as Fast as PossibleAlways keep in mind your return on investment. Sometimes it doesn't make a lot of business sense to invest hundreds of person-hours into honing and tweaking your Magento MySQL config. If you can gain the same website performance improvement from expanding your hardware setup, do it. Take into account both the time and cost of implementation when you make this business decision.Throwing more money at the problem might not be the most elegant solution, but if it's the most cost-effective, it's a great alternative. Want to know how we deal with slow Magento database performance? With our Magento speed optimization services, your store will be as swift as an arrow! Let's look at your website together and discuss how our team can speed it up.Magento 2 Database Optimization FAQWhat database does Magento use?By default, Magento uses the MySQL database. The newest version of it, 8.0, shows up to two times better speed. However, a Magento-based online store can also utilize other solutions like MariaDB, Percona, or MySQL Aurora. Generally, they're recommended for enterprise-level websites with extensive catalogs and high traffic volume.How to optimize MySQL database in Magento for best performance?There are lots of optimization tips for dealing with Magento 2 slow database. The most efficient recommendations include data cleansing in the tables, refining indexing, updating the database, switching to an alternative one, and finding queries with poor performance using specialized tools.Some steps depend on the version your Magento website operates on. Starting from Magento 2.1 (Adobe Commerce cloud, Adobe Commerce on-premises, Magento Open Source), it's highly recommended to turn off flat catalogs (on the admin panel, set ""NO"" to both the ""flat catalog category"" and ""flat catalog product"" fields). For Magento sites older than v.2.4, we advise substituting the default MySQL search with Elasticsearch/OpenSearch (the latest versions of Magento have it as a search engine by default).How to optimize Magento site performance in general?Any Magento website (like any other eCommerce website) needs periodic performance checks to optimize it and reduce loading times for crucial pages. Various factors are making Magento slow, so there are plenty of measures to tackle poor website performance.Combating slow performance often involves upgrading the Magento version, finding the proper hosting configuration, setting up an effective Magento cache, and, surely, Magento 2 database optimization. Turn to our seasoned Magento 2 developers to drastically improve your Magento database and optimize other aspects of your eCommerce system.Our ServicesPerfomance OptimizationMagento DevelopmentHire Magento DevelopersMagento SupportMagento AuditsLet’s stay in touchSubscribe to our newsletter to receive the latest news and updates.Your EmailSubscribeServicese-CommerceSalesforceUX/UI DesignAnalitycs & ReportingTechnologyMagentoShopifycompanyAbout UsCareersBlogContact UsUSA18 Bartol Street, San Francisco, USAUS: + 1(650) 488-7907Poland58 Ogrodowa Street, Warsaw, PolandPL: +48 (56) 9625-940© Copyright 2023 Onilab LLC. All right reserved."
26,Comparison between equivalent Intel & Graviton instances for MariaDB & PostgreSQL on Amazon RDS
26,Search
26,Browse
26,Community
26,About Community
26,Private Forums
26,Private Forums
26,Intel oneAPI Toolkits Private Forums
26,All other private forums and groups
26,Intel AI Software - Private Forums
26,GEH Pilot Community Sandbox
26,Intel® Connectivity Research Program (Private)
26,Intel-Habana Gaudi Technology Forum
26,Developer Software Forums
26,Developer Software Forums
26,Toolkits & SDKs
26,Software Development Tools
26,Software Development Topics
26,Software Development Technologies
26,Intel® DevCloud
26,Intel® Developer Cloud
26,"oneAPI Registration, Download, Licensing and Installation"
26,Software Archive
26,GPU Compute Software
26,Product Support Forums
26,Product Support Forums
26,Intel® NUCs
26,Memory & Storage
26,Embedded Products
26,Visual Computing
26,FPGA
26,Graphics
26,Processors
26,Wireless
26,Ethernet Products
26,Server Products
26,Intel® Enpirion® Power Solutions
26,Intel Unite® App
26,Intel vPro® Platform
26,Intel® Trusted Execution Technology (Intel® TXT)
26,Intel® Unison™ App
26,Intel® QuickAssist Technology (Intel® QAT)
26,Gaming Forums
26,Gaming Forums
26,Intel® ARC™ Graphics
26,Gaming on Intel® Processors with Intel® Graphics
26,Developing Games on Intel Graphics
26,Blogs
26,Blogs
26,@Intel
26,Products and Solutions
26,Tech Innovation
26,Thought Leadership
26,Cloud
26,Examine critical components of Cloud computing with Intel® software experts
26,Success!
26,Subscription added.
26,Success!
26,Subscription removed.
26,"Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your"
26,profile.
26,Intel Community
26,Blogs
26,Tech Innovation
26,Cloud
26,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
26,94 Discussions
26,Comparing Amazon RDS performance between Intel & Graviton instances for MariaDB and PostgreSQL
26,Subscribe
26,Article Options
26,Subscribe to RSS Feed
26,Mark as New
26,Mark as Read
26,Bookmark
26,Subscribe
26,Printer Friendly Page
26,Report Inappropriate Content
26,Mohan_Potheri
26,Employee
26,‎03-31-2023
26,08:00 AM
26,"7,625"
26,Introduction:
26,"Databases are typically the crown jewel of enterprise applications. All workloads including web-based e-commerce, social media, cloud services are typically backed by a database. Open-source databases[i] have become completely mainstream over the past decade and are the primary leaders in innovation in the database space. Open-source software has many attributes that make them successful in this cloud era. One of the major benefits is that developers can use open-source software and databases, without any licensing fees. Open-source software as developers code in features that they need quickly and contribute it back to the community. Open-source projects are therefore more agile and have out-evolved closed source alternatives since the early 2000s."
26,"AWS is positioning Graviton (an ARM based processor) aggressively from a price perspective compared to Intel 3rd generation Xeon Scalable Processors based instances.[ii] They are using cost savings as the primary mechanism to lure customers away from Intel based instances. Re-platforming is needed for customers moving to Graviton, which requires enterprise re-certification of the software with associated porting cost.  Intel Xeon leads across most popular database, web, and throughput related workloads. The cloud ecosystem for Intel has developed over the past 15 years, whereas ARM is relatively new and untested. Customers can potentially experience cloud vendor lock-in as Graviton is unique to AWS."
26,"The critical nature of open-source databases in the cloud makes them a good workload to compare Amazon EC2 Intel 3rd generation Xeon Scalable and Graviton instances. MariaDB[iii]  is an open-source variant of MySQL that offers a consistent set of advanced features and functionality across all major cloud platforms. PostgreSQL is one of the most powerful open-source databases known for its proven architecture, reliability, data integrity, robust feature set and extensibility. We will use MariaDB and PostgreSQL as the two open-source relational databases used in comparison testing on AWS EC2 between Intel 3rd generation Xeon Scalable processors and Amazon Graviton."
26,Amazon RDS:
26,"The Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.  Amazon RDS is used in modern applications for data storage in web and mobile applications. Customers move to managed databases from RDS to avoid having to manage their own databases. Many customers want to leverage open-source databases in the public cloud and break free from legacy databases"
26,MariaDB and PostgreSQL are popular open-source relational databases used for cloud-based applications. We will be deploying identically sized RDS instances for these two databases on Intel 3rd generation Xeon Scalable and Graviton based instances and running the commonly used Sysbench workload to compare their relative performance.
26,Instance Configuration:
26,The details about the Amazon EC2 instance choices that were made with Intel and Graviton instances are shown in Table 1.
26,Category
26,Attribute
26,Config1
26,Config2
26,Run Info
26,Testing Date
26,"Nov 3-11, 2022"
26,"Nov 3-11, 2022"
26,Cloud
26,AWS
26,AWS
26,Instance Type and CPU
26,Instance Type
26,db.r6g.4xlarge or
26,db.r6i.4xlarge
26,db.r6g.8xlarge or db.r6i.8xlarge
26,CPU(s)
26,Memory
26,128GB
26,256GB
26,Network BW / Instance
26,12.5 Gbps
26,25 Gbps
26,Storage: Direct attached
26,SSD GP2
26,SSD GP2
26,Drive Summary
26,1 volume 75GB
26,1 volume 75GB
26,Table 1: Instance configuration details for the testing
26,Workload Configuration:
26,Details about the workload and its attributes are shown in Table 2. Sysbench 1.0.18 was run 4 times per configuration and the results were then averaged for both MariaDB and PostgreSQL.
26,Category
26,Attribute
26,Config1
26,Config2
26,Run Info
26,Benchmark
26,sysbench 1.0.18
26,sysbench 1.0.18
26,Dates
26,"Nov 3-11, 2022"
26,"Nov 3-11, 2022"
26,CPUs
26,Thread(s) per Core
26,"1,2,4"
26,"0.5,1,2"
26,Core(s)
26,CPU Models
26,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
26,"Intel 3rd generation Xeon Scalable AWS SKU (r6i), AWS EC2 Graviton2 (r6g)"
26,BIOS
26,Workload Specific Details
26,Workload
26,MariaDB 10.6.10
26,PostgreSQL 14.4-R1
26,Command Line
26,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --mysql-user=sbtest --mysql-password=password --mysql-host=mariadb-r6g-4xl-v1.couqinukves2.us-east-1.rds.amazonaws.com  --db-driver=mysql --mysql-db=sbtest run
26,sysbench oltp_read_only --time=300 --threads=16 --table-size=100000 --pgsql-user=sbtest --pgsql-password=password --pgsql-host=pg-r6i-16xl-v1.couqinukves2.us-east-1.rds.amazonaws.com --pgsql-port=5432 --db-driver=pgsql --pgsql-db=sbtest run
26,Table 2: Workload configuration details for the testing
26,MariaDB Results:
26,The results from the sysbench testing for MariaDB are shown in Table 3. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
26,Queries per second
26,Threads
26,r6g.4xlarge (Graviton)
26,r6i.4xlarge (Intel)
26,Abs Diff
26,Percentage
26,Difference (qps)
26,45420.95
26,55450.305
26,10029.355
26,22%
26,82310.385
26,103531.905
26,21221.52
26,26%
26,138068.393
26,161312.673
26,23244.28
26,17%
26,Threads
26,r6g.8xlarge (Graviton)
26,r6i.8xlarge (Intel)
26,Abs Diff
26,Percentage
26,Difference (qps)
26,102649.813
26,127593.153
26,24943.34
26,24%
26,169209.105
26,216709.785
26,47500.68
26,28%
26,250122.328
26,302356.915
26,52234.5875
26,21%
26,Table 3: MariaDB QPS comparison between Intel and Graviton Instances.
26,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 20-30% for MariaDB.
26,Figure 1: Graphical comparison of sysbench performance for MariaDB between Intel and Graviton based instances (Higher is better)
26,PostgreSQL Results:
26,The results from the sysbench testing for PostgreSQL are shown in Table 4. The queries per second (QPS) were calculated and the performance compared between Intel and Graviton based instances. The percentage increase in performance for Intel over Graviton was then calculated as shown.
26,Queries per second
26,Threads
26,r6g.4xlarge (Graviton)
26,r6i.4xlarge (Intel)
26,Abs Diff
26,Percentage
26,Difference (qps)
26,58117.2825
26,65466.75
26,7349.4675
26,13%
26,99423.5025
26,114673.365
26,15249.8625
26,15%
26,140116.51
26,152913.408
26,12796.8975
26,Queries per second - 32 vCPU (8xlarge)
26,Threads
26,r6g.8xlarge (Graviton)
26,r6i.8xlarge
26,(Intel)
26,Abs Diff
26,Percentage
26,Difference (qps)
26,81133.195
26,125247.73
26,44114.535
26,54%
26,141914.253
26,208751.038
26,66836.785
26,47%
26,219109.908
26,288519.455
26,69409.5475
26,32%
26,Table 4: PostgreSQL QPS comparison between Intel and Graviton Instances.
26,The results were then compared in graphical format as shown below. Intel instances showed a performance improvement over Graviton between 30-50% for PostgreSQL.
26,Figure 2: Graphical comparison of sysbench performance for PostgreSQL between Intel and Graviton based instances (Higher is better)
26,Conclusion:
26,"Customers need to be careful with their choice of instances for their workloads in the cloud. Our results show that not all instances are created equal. Intel 3rd generation Xeon Scalable processors-based instances outperform Amazon similar Graviton based instances for open-source relational databases by 20-50% as the results have shown. Intel’s active participation in the open-source community and its innovative HW and SW optimizations work to boost performance of Database workloads as we have shown. By choosing Intel instances and right sizing them based on their performance characteristics in Amazon RDS, lower TCO with optimal performance can be attained."
26,Disclosure text:
26,"Tests were performed in October-November 2022 on AWS in region us-east-1. All configurations used general Purpose SSD gp2 storage. Baseline I/O performance for gp2 storage is 3 IOPS for each GiB, with a minimum of 100 IOPS. This relationship means that larger volumes have better performance. For our experiments we used. 550GiB storage with a baseline performance of 1500 IOPS. We ran the following Database Engines: - MariaDB 10.6.10, - PostgreSQL 14.4-R1. These were run on each of 4 DB server Instances described below. Database server used AWS RDS servers with 4 DB Instance types."
26,db.r6g – memory-optimized instance classes powered by AWS Graviton2 processors
26,"db.r6g.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
26,"db.r6g.4xlarge, 16 vCPU, 128             GB Memory  & Up to 10 Gbps Network interface"
26,db.r6i – memory-optimized instance classes powered by 3rd Generation Intel Xeon Scalable processors
26,"db.r6i.8xlarge, 32 vCPU, 256 GB Memory & 12 Gbps Network interface"
26,"db.r6i.4xlarge, 16 vCPU, 128               GB Memory & Up to 10 Gbps Network interface"
26,Pricing URL for MariaDB: https://aws.amazon.com/rds/mariadb/pricing/
26,For PostgreSQL:   https://aws.amazon.com/rds/postgresql/pricing/
26,"DB Client machine details:  For Database client machine, we used the EC2 instance type: c6i.4xlarge with 16vCPU (8 core), with 32 GB Memory, 75 GB GP2 Storage volume  with 12.5GB Network bandwidth powered by 3rd Generation Intel Xeon Scalable processors. The client machines use the following Software Image (AMI) with Canonical, Ubuntu, 20.04 LTS, amd64 focal image build on 2022-09-14 & ami-0149b2da6ceec4bb0. All DB Instances, as well as the client Instances were run in US-EAST-1 region. Benchmarking Software: We used sysbench tool to load data and to run oltp_read tests on all these configurations. We used sysbench version"
26,1.0.18 (using system LuaJIT 2.1.0-beta3) for all the DB testing.
26,Disclaimer text:
26,"Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation."
26,Bibliography
26,[i] https://www.dbta.com/BigDataQuarterly/Articles/The-Past-Present-and-Future-of-Open-Source-Databases-150954.aspx discusses the present and future of open source databases
26,"[ii] https://www.percona.com/blog/comparing-graviton-arm-performance-to-intel-and-amd-for-mysql-part-3/ compares DB Engines (and clients on the same instances) on M6i.* (Intel) , M6a.* (AMD),  M6g.*(Graviton) EC2 instances."
26,[iii] https://mariadb.com/database-topics/mariadb-vs-mysql/ provides a good comparison of MySQL and MariaDB.
26,Appendix A: DB Configuration Tuning:
26,------------------------------------------------------------------------------------------------------
26,Tuning for MariaDB:
26,We followed this article for performance tuning mariadb:
26,https://mariadb.com/resources/blog/10-database-tuning-tips-for-peak-workloads/
26,The following parameters were tuned.
26,1. InnoDB Buffer Pool Size
26,Making the InnoDB buffer pool size as large as possible ensures you use memory rather than disks for most read operations (because the buffer pool is where data and indexes are cached).
26,LEFT IT UNCHANGED from the RDS default which is {DBInstanceClassMemory*3/4}
26,where
26,DBInstanceClassMemory is a Formula variable with this description:
26,(from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ParamValuesRef.html
26,2. InnoDB Log File Size
26,"The redo logs make sure writes are fast and durable, and  the InnoDB redo space size is important for write-intensive workloads. The logs’ size is determined by innodb_log-file-size. For best results, generally you’ll want to set a combined total size to be at least 1/4 (or even 1/2) of the InnoDB buffer pool size, or equal to one hour’s worth of log entries during peak load. For MariaDB, we set innodb_log_file_size as {DBInstanceClassMemory*(3/4)*(1/4)}:"
26,- We computed and entered the number in a custom parameter group.
26,innodb_log_file_size
26,For 4xl instance this is   25769803776 (24GB of log file size for 128GB of RAM in that instance)
26,For 8xl instance this is   51539607552 (48GB of log file size for 256GB of RAM in that instance)
26,Tuning for PostgreSQL:
26,+-----------------------------------------------------------------------------+
26,CHANGED THIS FOR EVERY DB Instance class before DB Instance creation:
26,+-----------------------------------------------------------------------------+
26,max_wal_size = '96GB'
26,-->Default is 2048 (specified in MB)
26,16xl - 393216
26,8xl - 196608
26,4xl = 98304
26,2xl - 49152
26,+-----------------------------------------------------------------------------+
26,Refer to the blog:
26,https://www.percona.com/blog/2021/01/22/postgresql-on-arm-based-aws-ec2-instances-is-it-any-good/
26,Tags (4)
26,Tags:MariaDBopen sourcePostgreSQLRelational Databases
26,Kudo
26,"You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in."
26,Comment
26,About the Author
26,"Mohan Potheri is a Cloud Solutions Architect with more than 20 years in IT infrastructure, with in depth experience on Cloud architecture. He currently focuses on educating customers and partners on Intel capabilities and optimizations available on Amazon AWS. He is actively engaged with the Intel and AWS Partner communities to develop compelling solutions with Intel and AWS. He is a VMware vExpert (VCDX#98) with extensive knowledge on premises and hybrid cloud. He also has extensive experience with business critical applications such as SAP, Oracle, SQL and Java across UNIX, Linux and Windows environments. Mohan Potheri is an expert on AI/ML, HPC and has been a speaker in multiple conferences such as VMWorld, GTC, ISC and"
26,other Partner events.
26,Community support is provided during standard business hours (Monday to Friday 7AM - 5PM PST). Other contact methods are available here.
26,"Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade."
26,"For more complete information about compiler optimizations, see our Optimization Notice."
26,©Intel Corporation
26,Terms of Use
26,*Trademarks
26,Cookies
26,Privacy
26,Supply Chain Transparency
26,Site Map
28,performance-tuning · GitHub Topics · GitHub
28,Skip to content
28,Toggle navigation
28,Sign up
28,Product
28,Actions
28,Automate any workflow
28,Packages
28,Host and manage packages
28,Security
28,Find and fix vulnerabilities
28,Codespaces
28,Instant dev environments
28,Copilot
28,Write better code with AI
28,Code review
28,Manage code changes
28,Issues
28,Plan and track work
28,Discussions
28,Collaborate outside of code
28,Explore
28,All features
28,Documentation
28,GitHub Skills
28,Blog
28,Solutions
28,For
28,Enterprise
28,Teams
28,Startups
28,Education
28,By Solution
28,CI/CD & Automation
28,DevOps
28,DevSecOps
28,Resources
28,Learning Pathways
28,"White papers, Ebooks, Webinars"
28,Customer Stories
28,Partners
28,Open Source
28,GitHub Sponsors
28,Fund open source developers
28,The ReadME Project
28,GitHub community articles
28,Repositories
28,Topics
28,Trending
28,Collections
28,Pricing
28,Search or jump to...
28,"Search code, repositories, users, issues, pull requests..."
28,Search
28,Clear
28,Search syntax tips
28,Provide feedback
28,"We read every piece of feedback, and take your input very seriously."
28,Include my email address so I can be contacted
28,Cancel
28,Submit feedback
28,Saved searches
28,Use saved searches to filter your results more quickly
28,Name
28,Query
28,"To see all available qualifiers, see our documentation."
28,Cancel
28,Create saved search
28,Sign in
28,Sign up
28,You signed in with another tab or window. Reload to refresh your session.
28,You signed out in another tab or window. Reload to refresh your session.
28,You switched accounts on another tab or window. Reload to refresh your session.
28,Dismiss alert
28,Explore
28,Topics
28,Trending
28,Collections
28,Events
28,GitHub Sponsors
28,performance-tuning
28,Star
28,Here are
28,151 public repositories
28,matching this topic...
28,Language:
28,All
28,Filter by language
28,All
28,151
28,Python
28,Java
28,JavaScript
28,C++
28,Shell
28,TypeScript
28,HTML
28,Sort:
28,Recently updated
28,Sort options
28,Most stars
28,Fewest stars
28,Most forks
28,Fewest forks
28,Recently updated
28,Least recently updated
28,Manav-Khandurie
28,Cloud-Performance-Tuning
28,Star
28,Code
28,Issues
28,Pull requests
28,Discussions
28,"Cloud performance tuning Project , deployed in Azure"
28,cloud
28,azure
28,performance-metrics
28,performance-tuning
28,performance-analysis
28,Updated
28,"Nov 25, 2023"
28,CSS
28,winsiderss
28,systeminformer
28,Star
28,9.6k
28,Code
28,Issues
28,Pull requests
28,Discussions
28,"A free, powerful, multi-purpose tool that helps you monitor system resources, debug software and detect malware. Brought to you by Winsider Seminars & Solutions, Inc. @ http://www.windows-internals.com"
28,windows
28,debugger
28,security
28,benchmarking
28,process-manager
28,performance
28,monitor
28,monitoring
28,realtime
28,administrator
28,process-monitor
28,performance-tuning
28,profiling
28,monitor-performance
28,performance-monitoring
28,system-monitor
28,processhacker
28,systeminformer
28,system-informer
28,Updated
28,"Nov 25, 2023"
28,cherusk
28,godon
28,Star
28,Code
28,Issues
28,Pull requests
28,"(ancient german = improving, rearranging, rendering benign)"
28,infrastructure
28,performance
28,cloud
28,service
28,optimization
28,daemon
28,evolutionary-algorithms
28,performance-tuning
28,control-systems
28,feedback-loop
28,configurator
28,genetic-algorithms
28,self-improvement
28,evolutionary-computation
28,optimization-algorithms
28,metaheuristics
28,continuous-optimization
28,aiops
28,genetic-optimization-algorithm
28,metaheuristic-algorithms
28,Updated
28,"Nov 24, 2023"
28,Python
28,Releem
28,mysqlconfigurer
28,Star
28,212
28,Code
28,Issues
28,Pull requests
28,Discussions
28,Releem is a simple MySQL tuning tool to improve database performance and reduce servers costs.
28,mysql
28,linux
28,bash
28,mariadb
28,mysql-server
28,performance-tuning
28,percona
28,percona-server
28,mysql-tuner
28,mariadb-server
28,performance-optimization
28,mysql-performance
28,mariadb-performance
28,mysqltuner
28,aiops
28,mysql-configuration
28,mysqltuner-report
28,mysql-status
28,Updated
28,"Nov 24, 2023"
28,sjinks
28,wp-performance-tweaks
28,Star
28,Code
28,Issues
28,Pull requests
28,Performance tweaks for WordPress
28,wordpress
28,wordpress-plugin
28,performance
28,tweaks
28,performance-tuning
28,Updated
28,"Nov 24, 2023"
28,PHP
28,rammpeter
28,panorama
28,Star
28,Code
28,Issues
28,Pull requests
28,Discussions
28,Tool for monitoring performance issues of Oracle databases
28,performance
28,oracle
28,performance-tuning
28,oracle-db
28,performance-analysis
28,performance-monitoring
28,oracle-database
28,Updated
28,"Nov 23, 2023"
28,Ruby
28,kruize
28,autotune
28,Star
28,143
28,Code
28,Issues
28,Pull requests
28,Discussions
28,Autonomous Performance Tuning for Kubernetes!
28,kubernetes
28,performance
28,hyperparameter-optimization
28,performance-tuning
28,sla
28,autotune
28,tunables
28,Updated
28,"Nov 24, 2023"
28,Java
28,CachyOS
28,linux-cachyos
28,Star
28,421
28,Code
28,Issues
28,Pull requests
28,Discussions
28,Archlinux Kernel based on different schedulers and some other performance improvements.
28,performance
28,kernel
28,archlinux
28,linux-kernel
28,performance-tuning
28,cachy-scheduler
28,cachy
28,cachyos
28,cacule-sched
28,baby-sched
28,Updated
28,"Nov 22, 2023"
28,Shell
28,SystemXFiles
28,process-governor
28,Star
28,Code
28,Issues
28,Pull requests
28,Discussions
28,"Process Governor is a Python utility designed to manage Windows processes and services by adjusting their priorities, I/O priorities, and core affinity based on user-defined rules in a JSON configuration."
28,windows
28,process-manager
28,automation
28,performance
28,performance-tuning
28,windows-services
28,process-orchestrator
28,cpu-affinity
28,cpu-priority
28,process-tuner
28,Updated
28,"Nov 18, 2023"
28,Python
28,phatdangx
28,random-joke
28,Star
28,Code
28,Issues
28,Pull requests
28,"A demonstration of clean architecture in Golang, including performance tuning using load testing and caching mechanisms."
28,golang
28,clean-architecture
28,performance-tuning
28,Updated
28,"Nov 17, 2023"
28,HTML
28,le0pard
28,pgtune
28,Star
28,1.8k
28,Code
28,Issues
28,Pull requests
28,Discussions
28,Pgtune - tuning PostgreSQL config by your hardware
28,javascript
28,performance
28,database
28,pwa
28,hardware
28,postgresql
28,rdbms
28,performance-tuning
28,tunning
28,pgtune
28,Updated
28,"Nov 16, 2023"
28,JavaScript
28,ev2900
28,OpenSearch_Refresh_Interval
28,Star
28,Code
28,Issues
28,Pull requests
28,Example covering how to adjust the refresh interval on an OpenSearch index
28,aws
28,performance-tuning
28,opensearch
28,refresh-interval
28,Updated
28,"Nov 14, 2023"
28,oslabs-beta
28,infernode
28,Star
28,Code
28,Issues
28,Pull requests
28,Process tracing and flame graph visualization tool for Node.js development
28,nodejs
28,javascript
28,open-source
28,typescript
28,developer-tools
28,flamegraph
28,performance-tuning
28,performance-analysis
28,dtrace
28,Updated
28,"Nov 11, 2023"
28,TypeScript
28,YurinDoctrine
28,ubuntu-base-setup
28,Sponsor
28,Star
28,Code
28,Issues
28,Pull requests
28,A tweaker script works on Ubuntu or Debian
28,linux
28,debian
28,ubuntu
28,rice
28,systemd
28,post-installation
28,performance-tuning
28,lz4
28,hardening
28,linux-desktop
28,tweaking
28,linux-tuning
28,zswap
28,fresh-install
28,Updated
28,"Nov 8, 2023"
28,Shell
28,YurinDoctrine
28,arch-linux-base-setup
28,Sponsor
28,Star
28,Code
28,Issues
28,Pull requests
28,A tweaker script works on Arch Linux
28,linux
28,rice
28,archlinux
28,arch
28,systemd
28,arch-linux
28,post-installation
28,performance-tuning
28,lz4
28,hardening
28,linux-desktop
28,tweaking
28,linux-tuning
28,zswap
28,fresh-install
28,tweaker-script
28,Updated
28,"Nov 8, 2023"
28,Shell
28,goranschwarz
28,DbxTune
28,Star
28,Code
28,Issues
28,Pull requests
28,mysql
28,java
28,postgres
28,database
28,replication
28,sap
28,oracle
28,performance-tuning
28,performance-monitoring
28,db2
28,sqlserver
28,sybase
28,hana
28,performance-counters
28,ase
28,asetune
28,asemon
28,Updated
28,"Nov 4, 2023"
28,Java
28,ehmicky
28,precise-now
28,Star
28,Code
28,Issues
28,Pull requests
28,Like `performance.now()` but in nanoseconds
28,nodejs
28,javascript
28,resolution
28,library
28,performance
28,typescript
28,date
28,milliseconds
28,performance-metrics
28,performance-tuning
28,performance-analysis
28,performance-monitoring
28,browsers
28,seconds
28,performance-testing
28,nanoseconds
28,nanosecond-resolution
28,hrtime
28,microseconds
28,date-now
28,Updated
28,"Oct 28, 2023"
28,TypeScript
28,ehmicky
28,time-resolution
28,Star
28,Code
28,Issues
28,Pull requests
28,Find the process's time resolution
28,nodejs
28,javascript
28,resolution
28,library
28,performance
28,typescript
28,date
28,milliseconds
28,performance-metrics
28,performance-tuning
28,performance-analysis
28,performance-monitoring
28,browsers
28,seconds
28,performance-testing
28,nanoseconds
28,nanosecond-resolution
28,hrtime
28,microseconds
28,date-now
28,Updated
28,"Oct 28, 2023"
28,TypeScript
28,kremersit
28,SQL-Server-Scripts
28,Star
28,Code
28,Issues
28,Pull requests
28,tuning
28,performance-tuning
28,sqlserver
28,Updated
28,"Oct 25, 2023"
28,TSQL
28,LinShunKang
28,MyPerf4J
28,Star
28,3.2k
28,Code
28,Issues
28,Pull requests
28,"High performance Java APM. Powered by ASM. Try it. Test it. If you feel its better, use it."
28,agent
28,performance
28,microservices
28,monitor
28,monitoring
28,bytecode
28,profiler
28,metrics
28,apm
28,performance-metrics
28,java-agent
28,performance-tuning
28,performance-visualization
28,performance-analysis
28,profiling
28,performance-monitoring
28,observability
28,diagnosis
28,monitoring-tool
28,jvm-monitor
28,Updated
28,"Oct 15, 2023"
28,Java
28,Load more…
28,Improve this page
28,"Add a description, image, and links to the"
28,performance-tuning
28,topic page so that developers can more easily learn about it.
28,Curate this topic
28,Add this topic to your repo
28,To associate your repository with the
28,performance-tuning
28,"topic, visit your repo's landing page and select ""manage topics."""
28,Learn more
28,Footer
28,"© 2023 GitHub, Inc."
28,Footer navigation
28,Terms
28,Privacy
28,Security
28,Status
28,Docs
28,Contact GitHub
28,Pricing
28,API
28,Training
28,Blog
28,About
28,You can’t perform that action at this time.
29,Improving query performance for RDS for MySQL with Amazon RDS Optimized Reads - Amazon Relational Database ServiceImproving query performance for RDS for MySQL with Amazon RDS Optimized Reads - Amazon Relational Database ServiceAWSDocumentationAmazon RDSUser GuideOverviewUse casesBest practicesUsingMonitoringLimitationsImproving query performance for RDS for MySQL with Amazon RDS Optimized ReadsYou can achieve faster query processing for RDS for MySQL with Amazon RDS Optimized Reads. An
29,RDS for MySQL DB instance or Multi-AZ DB cluster that uses RDS Optimized Reads can achieve up to 2x faster
29,query processing compared to a DB instance or cluster that doesn't use it.TopicsOverview of RDS Optimized ReadsUse cases for RDS Optimized ReadsBest practices for RDS Optimized ReadsUsing RDS Optimized ReadsMonitoring DB instances that use RDS Optimized ReadsLimitations for RDS Optimized Reads
29,Overview of RDS Optimized Reads
29,When you use an RDS for MySQL DB instance or Multi-AZ DB cluster that has RDS Optimized Reads turned
29,"on, it achieves faster query performance through the use of an instance store. An"
29,instance store provides temporary block-level
29,storage for your DB instance or Multi-AZ DB cluster. The storage is located on Non-Volatile Memory
29,Express (NVMe) solid state drives (SSDs) that are physically attached to the host
29,"server. This storage is optimized for low latency, high random I/O performance, and high"
29,sequential read throughput.
29,RDS Optimized Reads is turned on by default when a DB instance or Multi-AZ DB cluster uses a DB
29,"instance class with an instance store, such as db.m5d or db.m6gd. With RDS Optimized"
29,"Reads, some temporary objects are stored on the instance store. These temporary objects"
29,"include internal temporary files, internal on-disk temp tables, memory map files, and"
29,"binary log (binlog) cache files. For more information about the instance store, see"
29,Amazon EC2
29,instance store in the Amazon Elastic Compute Cloud User Guide for
29,Linux Instances.
29,The workloads that generate temporary objects in MySQL for query processing can take
29,advantage of the instance store for faster query processing. This type of workload
29,"includes queries involving sorts, hash aggregations, high-load joins, Common Table"
29,"Expressions (CTEs), and queries on unindexed columns. These instance store volumes"
29,"provide higher IOPS and performance, regardless of the storage configurations used for"
29,persistent Amazon EBS storage. Because RDS Optimized Reads offloads operations on temporary
29,"objects to the instance store, the input/output operations per second (IOPS) or"
29,throughput of the persistent storage (Amazon EBS) can now be used for operations on
29,"persistent objects. These operations include regular data file reads and writes,"
29,"and background engine operations, such as flushing and insert buffer merges."
29,NoteBoth manual and automated RDS snapshots only contain engine files for persistent
29,objects. The temporary objects created in the instance store aren't included in RDS
29,snapshots.
29,Use cases for RDS Optimized Reads
29,"If you have workloads that rely heavily on temporary objects, such as internal tables"
29,"or files, for their query execution, then you can benefit from turning on RDS Optimized"
29,Reads. The following use cases are candidates for RDS Optimized Reads:
29,Applications that run analytical queries with complex common table expressions
29,"(CTEs), derived tables, and grouping operations"
29,Read replicas that serve heavy read traffic with unoptimized queries
29,Applications that run on-demand or dynamic reporting queries that involve
29,"complex operations, such as queries with GROUP BY and ORDER"
29,BY clauses
29,Workloads that use internal temporary tables for query processing
29,You can monitor the engine status variable created_tmp_disk_tables to determine the number of
29,disk-based temporary tables created on your DB instance.
29,"Applications that create large temporary tables, either directly or in procedures, to"
29,store intermediate results
29,Database queries that perform grouping or ordering on non-indexed columns
29,Best practices for RDS Optimized Reads
29,Use the following best practices for RDS Optimized Reads:
29,Add retry logic for read-only queries in case they fail because the instance
29,store is full during the execution.
29,Monitor the storage space available on the instance store with the CloudWatch metric FreeLocalStorage.
29,"If the instance store is reaching its limit because of workload on the DB instance, modify the DB instance to"
29,use a larger DB instance class.
29,When your DB instance or Multi-AZ DB cluster has sufficient memory but is still reaching the
29,"storage limit on the instance store, increase the binlog_cache_size"
29,value to maintain the session-specific binlog entries in memory. This
29,configuration prevents writing the binlog entries to temporary binlog cache
29,files on disk.
29,The binlog_cache_size parameter is session-specific. You can
29,change the value for each new session. The setting for this parameter can
29,increase the memory utilization on the DB instance during peak workload.
29,"Therefore, consider increasing the parameter value based on the workload pattern"
29,of your application and available memory on the DB instance.
29,Use the default value of MIXED for the
29,"binlog_format. Depending on the size of the transactions, setting"
29,binlog_format to ROW can result in large binlog
29,cache files on the instance store.
29,Set the internal_tmp_mem_storage_engine parameter to
29,"TempTable, and set the temptable_max_mmap parameter to match the size of the available"
29,storage on the instance store.
29,Avoid performing bulk changes in a single transaction. These types of
29,transactions can generate large binlog cache files on the instance store and can
29,cause issues when the instance store is full. Consider splitting writes into
29,multiple small transactions to minimize storage use for binlog cache
29,files.
29,Use the default value of ABORT_SERVER for the binlog_error_action parameter. Doing so avoids issues
29,with the binary logging on DB instances with backups enabled.
29,Using RDS Optimized Reads
29,When you provision an RDS for MySQL DB instance with one of the following DB instance
29,"classes in a Single-AZ DB instance deployment, Multi-AZ DB instance deployment, or Multi-AZ DB cluster"
29,"deployment, the DB instance automatically uses RDS Optimized Reads."
29,"To turn on RDS Optimized Reads, do one of the following:"
29,Create an RDS for MySQL DB instance or Multi-AZ DB cluster using one of these DB instance
29,"classes. For more information, see Creating an Amazon RDS DB instance."
29,Modify an existing RDS for MySQL DB instance or Multi-AZ DB cluster to use one of these DB
29,"instance classes. For more information, see Modifying an Amazon RDS DB instance."
29,RDS Optimized Reads is available in all AWS Regions RDS where one or more of the DB
29,instance classes with local NVMe SSD storage are supported.
29,For information about DB
29,"instance classes, see DB instance classes."
29,DB instance class availability differs for AWS Regions. To determine whether a DB instance class is supported
29,"in a specific AWS Region, see Determining DB instance class"
29,support in AWS Regions.
29,"If you don't want to use RDS Optimized Reads, modify your DB instance or Multi-AZ DB cluster so that"
29,it doesn't use a DB instance class that supports the feature.
29,Monitoring DB instances that use RDS Optimized Reads
29,You can monitor DB instances that use RDS Optimized Reads with the following CloudWatch metrics:
29,FreeLocalStorage
29,ReadIOPSLocalStorage
29,ReadLatencyLocalStorage
29,ReadThroughputLocalStorage
29,WriteIOPSLocalStorage
29,WriteLatencyLocalStorage
29,WriteThroughputLocalStorage
29,"These metrics provide data about available instance store storage, IOPS, and throughput. For more information"
29,"about these metrics, see Amazon CloudWatch instance-level metrics for Amazon RDS."
29,Limitations for RDS Optimized Reads
29,The following limitations apply to RDS Optimized Reads:
29,RDS Optimized Reads is supported for RDS for MySQL version 8.0.28 and higher. For information about
29,"RDS for MySQL versions, see MySQL on Amazon RDS versions."
29,You can't change the location of temporary objects to persistent storage (Amazon EBS) on the DB
29,instance classes that support RDS Optimized Reads.
29,"When binary logging is enabled on a DB instance, the maximum transaction size is limited by the"
29,"size of the instance store. In MySQL, any session that requires more storage"
29,than the value of binlog_cache_size writes transaction changes to temporary binlog
29,"cache files, which are created on the instance store."
29,Transactions can fail when the instance store is full.
29,"Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsUsing Kerberos authentication for MySQLImproving write performance with RDS Optimized Writes for MySQLDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better."
30,Magento 2 Database Optimization Guide [2023] for Peak Performance
30,"CloudPanel for AWS Graviton is now available, up to 40% better price-performance!"
30,Blog
30,Tutorials
30,Contact Us
30,Get Started
30,Blog
30,Tutorials
30,Contact Us
30,Magento 2 Database Optimization for Peak Performance
30,Nikita S.
30,Technical Writer
30,"Apr 10, 2023 · 5 min read"
30,Boosting your Magento 2 website's performance is critical to providing a great user experience. One way to enhance performance is by optimizing the database.
30,This guide will discuss the importance of having the right server resources. We will cover the benefits of different database management software. Get helpful tips for optimizing your Magento 2 database.
30,Key Takeaways
30,Helpful Tips for Magento 2 Database Optimization
30,FAQ Section
30,Key Takeaways
30,Why having enough server resources matters
30,"Pros of various database software (MySQL, MariaDB, Percona)"
30,Helpful tips to optimize Magento 2 database
30,How hardware affects website performance
30,Advanced tools and performance optimization tips
30,Updating database software for better results
30,FAQ section answering the common Magento 2 questions
30,The Importance of Proper Server Resources
30,"Memory, Processing Power, and Storage"
30,"Ensuring your Magento 2 hosting has enough memory, processing power, or storage can cause slow page loading and a bad user experience."
30,"If there is high traffic, ensure you have enough resources to handle your Magento store's needs."
30,Database Management Software
30,"Several options exist for managing your Magento 2 website's database. It includes MySQL, MariaDB, and Percona. Each has its benefits and can impact your website's performance differently."
30,1. MySQL
30,MySQL is a popular database system that works well with Magento 2.
30,But it might not always be the best choice for huge websites with lots of traffic.
30,2. MariaDB
30,"MariaDB is much like MySQL but has extra features, like better scaling and performance. It makes it a good choice for bigger stores."
30,3. Percona
30,"Percona is another option that's great for large databases and heavy loads. It has extra features like XtraDB, improved security, and more performance based tools."
30,Helpful Tips for Magento 2 Database Optimization
30,Enabling Flat Catalogs
30,Flat catalogs can make your website faster by reducing the number of database queries. This is especially helpful for websites with many products.
30,"To enable flat catalogs, go to your Magento admin panel."
30,Navigate to Stores > Settings > Configuration > Catalog > Catalog
30,"Update the values for ""Use Flat Catalog Category"" and ""Use Flat Catalog Product""."
30,Using Elasticsearch for Layered Navigation
30,Elasticsearch can make your website faster by handling search-related queries. It also has advanced features like layered navigation filters.
30,It lightens your database workload and improves performance. It is beneficial on a separate machine.
30,"To optimize your Magento MySQL database, disable product count in layered navigation. It slows down page loading and offers little value."
30,"To turn it off, go to ""Stores > Configuration > Catalog > Catalog,"" and find ""Layered Navigation”."
30,"Deselect ""Use System Value,"" and set ""Display Product Count"" to ""No."""
30,Adding Indexes to Large Tables
30,Adding indexes to large tables can make your database queries faster. This is especially helpful for websites with lots of products.
30,The Role of Hardware in Website Performance
30,The hardware you choose can significantly affect your Magento 2 website's performance. Factors that affect speed include:
30,Server location
30,CPU speed
30,Bandwidth
30,Available RAM
30,Make sure your hardware can handle your website's needs.
30,Advanced Tools for Further Optimization
30,MySQLTuner and Tuning Primer Scripts
30,Tools like MySQLTuner and Tuning Primer scripts can help analyze your database and suggest solutions. These tools can find hidden issues and help you optimize your database even more.
30,Updating Database Versions
30,"Keeping your database software up to date enhances performance, security, and fixing bugs. The latest versions often have better features that can make your website faster."
30,FAQ Section
30,1. What are some tips for improving Magento 2 performance?
30,"Some tips include using the latest software versions, enabling flat catalogs, setting up a Varnish cache. Integrate a CDN and use tools like Elasticsearch."
30,Optimize your database by adding indexes to large tables. Use advanced tools like MySQLTuner and Tuning Primer scripts.
30,2. Why use the latest software versions for Magento 2 optimization?
30,"Latest software versions have better performance and security. This can lead to faster load times, even for large-scale stores."
30,3. How does full-page caching help my Magento website?
30,"Full-page caching makes your website faster by storing fully rendered pages. This lets the server quickly show cached pages to visitors, making it faster and using fewer resources."
30,4. What is a CDN's role in Magento 2 optimization?
30,A content delivery network (CDN) helps make your website faster. It helios in delivering static content from multiple servers worldwide. This makes content load faster for visitors.
30,5. How do Varnish cache and Magento cache improve website performance?
30,Varnish cache stores fully rendered pages for faster delivery. Magento cache stores data like configuration and layout. Both help make your website faster by reducing the time and resources needed to process and serve content.
30,6. What is the purpose of Dataflow Batch Export and Cron jobs in Magento 2?
30,"Dataflow Batch Export lets you export large amounts of data more efficiently. Cron jobs are scheduled tasks that run automatically at certain times, like reindexing and cache cleaning. Both help maintain and optimize your Magento 2 store's performance."
30,7. How can I optimize the ‘my.cnf’ file for better Magento 2 database performance?
30,"To optimize my.cnf file, adjust settings such as innodb_buffer_pool_size, and max_connections. It is based on your server's resources and traffic."
30,Fine-tuning these values can improve database performance for your Magento 2 store.
30,What are the benefits of using a flat catalog category in Magento 2?
30,"Flat catalog categories can reduce the number of database queries. It results in faster page loading times, especially for websites with many products and categories."
30,8. How can I monitor the performance of my Magento 2 store and identify areas for optimization?
30,"Use performance monitoring tools like New Relic, Google PageSpeed Insights, and GTmetrix. These tools help to analyze your website's performance."
30,You can identify areas for optimization and improving your Magento store's speed.
30,9. Can I use third-party extensions to optimize my Magento 2 database performance?
30,"Yes, several third-party extensions can help optimize database performance. However, ensure the extensions are from reputable developers. It should be compatible with your Magento version and have good reviews to avoid potential issues."
30,Summary
30,Optimizing your Magento 2 database is very important for making your website faster. It improves user experience and helps you increase conversions.
30,"This guide helps ensure your Magento store runs smoothly and efficiently. With the tips, you can improve customer experience and help your search engine rankings."
30,Get the latest Magento news and enhance your ecommerce store performance.
30,Nikita S.
30,Technical Writer
30,"As a professional content writer, Nikita S. is experienced in crafting well-researched articles that simplify complex information and promote technical communication. She is enthusiastic about cloud computing and holds a specialization in digital marketing."
30,Get the fastest Magento Hosting!
30,Get Started
30,Company
30,About Us
30,Contact Us
30,Imprint
30,Copyright © 2011 - 2023 mgt-commerce.com. All rights reserved.
32,MariaDB - ArchWiki
32,Home Packages Forums Wiki GitLab Security AUR Download
32,Jump to content
32,Toggle sidebar
32,ArchWiki
32,Search
32,Create accountLog in
32,Personal tools
32,Create account Log in Dark mode
32,Navigation
32,Main pageTable of contentsGetting involvedWiki newsRandom page
32,Interaction
32,HelpContributingRecent changesRecent talksNew pagesStatisticsRequests
32,Tools
32,What links hereRelated changesSpecial pagesPrintable versionPermanent linkPage information
32,Contents
32,move to sidebar
32,hide
32,Beginning
32,1Installation
32,2Configuration
32,Toggle Configuration subsection
32,2.1Add user
32,2.2Configuration files
32,2.3Enable auto-completion
32,2.4Using UTF8MB4
32,2.5Using a tmpfs for tmpdir
32,2.6Time zone tables
32,3Security
32,Toggle Security subsection
32,3.1Improve initial security
32,3.2Listen only on the loopback address
32,3.3Enable access locally only via Unix sockets
32,3.4Grant remote access
32,3.5Configure access to home directories
32,4Maintenance
32,Toggle Maintenance subsection
32,4.1Upgrade databases on major releases
32,"4.2Checking, optimizing and repairing databases"
32,5Backup
32,Toggle Backup subsection
32,5.1Compression
32,5.2Non-interactive
32,5.2.1Example script
32,5.3Holland Backup
32,6Troubleshooting
32,Toggle Troubleshooting subsection
32,6.1Unable to run mariadb-upgrade because MariaDB cannot start
32,6.2Reset the root password
32,6.3Check and repair all tables
32,6.4Optimize all tables
32,6.5OS error 22 when running on ZFS
32,"6.6Cannot login through CLI, but phpmyadmin works well"
32,6.7MariaDB binary logs are taking up huge disk space
32,6.8OpenRC fails to start MariaDB
32,6.9Changed limits warning on max_open_files/table_open_cache
32,"6.1010.4 to 10.5 upgrade crash: ""InnoDB: Upgrade after a crash is not supported. The redo log was created with MariaDB 10.4.x"""
32,6.11Table 'mysql.xxx' does not exist in engine
32,7See also
32,Toggle the table of contents
32,Toggle the table of contents
32,MariaDB
32,4 languages
32,DeutschFrançais日本語中文（简体）
32,Page
32,Discussion
32,English
32,Read
32,View source
32,View history
32,More
32,ReadView sourceView history
32,From ArchWiki
32,Related articles
32,phpMyAdmin
32,Adminer
32,JDBC and MySQL
32,Open Database Connectivity
32,"MariaDB is a reliable, high performance and full-featured database server which aims to be an 'always Free, backward compatible, drop-in' replacement of MySQL. Since 2013 MariaDB is Arch Linux's default implementation of MySQL.[1]"
32,Installation
32,"MariaDB is the default implementation of MySQL in Arch Linux, provided with the mariadb package."
32,Tip:
32,"If the database (in /var/lib/mysql) resides on a Btrfs file system, you should consider disabling Copy-on-Write for the directory before creating any database."
32,"If the database resides on a ZFS file system, you should consult ZFS#Databases before creating any database."
32,"Install mariadb, and run the following command before starting the mariadb.service:"
32,# mariadb-install-db --user=mysql --basedir=/usr --datadir=/var/lib/mysql
32,"Tip: If you use something different from /var/lib/mysql for your data dir, you need to set datadir=YOUR_DATADIR under section [mariadb] of your /etc/my.cnf.d/server.cnf."
32,Now mariadb.service can be started and/or enabled.
32,"Note: Before continuing, it is recommended to improve the initial security of MariaDB installation."
32,"To simplify administration, you might want to install a front-end."
32,Configuration
32,"Once you have started the MariaDB server and added a root account, you may want to change the default configuration."
32,"To log in as root on the MariaDB server, use the following command:"
32,# mariadb
32,Add user
32,"Creating a new user takes two steps: create the user; grant privileges. In the below example, the user monty with some_pass as password is being created, then granted full permissions to the database mydb:"
32,# mariadb -u root -p
32,MariaDB> CREATE USER 'monty'@'localhost' IDENTIFIED BY 'some_pass';
32,MariaDB> GRANT ALL PRIVILEGES ON mydb.* TO 'monty'@'localhost';
32,MariaDB> quit
32,Configuration files
32,MariaDB configuration options are read from the following files in the given order (according to mysqld --help --verbose | tail -20 output):
32,/etc/my.cnf /etc/my.cnf.d/ ~/.my.cnf
32,Create a configuration file in /etc/my.cnf.d/ with a .cnf extension to ensure that upgrades preserve your configuration.
32,"Depending on the scope of the changes you want to make (system-wide, user-only...), use the corresponding file. See this entry of the Knowledge Base for more information."
32,Enable auto-completion
32,Note: Enabling this feature can make the client initialization longer.
32,"The MariaDB client completion feature is disabled by default. To enable it system-wide edit /etc/my.cnf.d/client.cnf, and add auto-rehash under client-mariadb. Note that this must not be placed under mysqld. Completion will be enabled next time you run the MariaDB client."
32,Using UTF8MB4
32,Warning: Before changing the character set be sure to create a backup first.
32,Note:
32,The mariadb package already uses utf8mb4 as charset and utf8mb4_unicode_ci as collation. Users using the default (character) settings may want to skip this section.
32,UTF8MB4 is recommended over UTF-8 since it allows full Unicode support [2] [3].
32,Append the following values to the main configuration file located at /etc/my.cnf.d/my.cnf:
32,[client]
32,default-character-set = utf8mb4
32,[mariadb]
32,collation_server = utf8mb4_unicode_ci
32,character_set_server = utf8mb4
32,[mariadb-client]
32,default-character-set = utf8mb4
32,"Restart mariadb.service to apply the changes. Changing the character set does not change existing table formats, only newly created tables, and the protocol interaction that fetches data."
32,See #Maintenance to optimize and check the database health.
32,Using a tmpfs for tmpdir
32,"The directory used by MariaDB for storing temporary files is named tmpdir. For example, it is used to perform disk based large sorts, as well as for internal and explicit temporary tables."
32,Create the directory with appropriate permissions:
32,# mkdir -pv /var/lib/mysqltmp
32,# chown mysql:mysql /var/lib/mysqltmp
32,Add the following tmpfs mount to your /etc/fstab file:
32,tmpfs
32,/var/lib/mysqltmp
32,tmpfs
32,"rw,gid=mysql,uid=mysql,size=100M,mode=0750,noatime"
32,0 0
32,Add to your /etc/my.cnf.d/server.cnf file under the mysqld group:
32,tmpdir
32,= /var/lib/mysqltmp
32,"Stop mariadb.service, mount /var/lib/mysqltmp/ and start mariadb.service."
32,Time zone tables
32,"Although time zone tables are created during the installation, they are not automatically populated. They need to be populated if you are planning on using CONVERT_TZ() in SQL queries."
32,To populate the time zone tables with all the time zones:
32,$ mariadb-tzinfo-to-sql /usr/share/zoneinfo | mariadb -u root -p mysql
32,"Optionally, you may populate the table with specific time zone files:"
32,$ mariadb-tzinfo-to-sql timezone_file timezone_name | mariadb -u root -p mysql
32,Security
32,Improve initial security
32,"The mariadb-secure-installation command will interactively guide you through a number of recommended security measures, such as removing anonymous accounts and removing the test database:"
32,# mariadb-secure-installation
32,"Warning: After running this, please note that TCP port 3306 will still be open, but refusing connections with an error message. To prevent MySQL from listening on an external interface, see the #Listen only on the loopback address and #Enable access locally only via Unix sockets sections."
32,Listen only on the loopback address
32,"By default, MariaDB will listen on the 0.0.0.0 address, which includes all network interfaces. In order to restrict MariaDB to listen only to the loopback address, add the following line in /etc/my.cnf.d/server.cnf:"
32,[mariadb]
32,bind-address = localhost
32,"This will bind to both 127.0.0.1 and ::1, and enable MariaDB to receive connections both in IPv4 and IPv6."
32,Enable access locally only via Unix sockets
32,"By default, MariaDB is accessible via both Unix sockets and the network. If MariaDB is only needed for the localhost, you can improve security by not listening on TCP port 3306, and only listening on Unix sockets instead. To do this, add the following line in /etc/my.cnf.d/server.cnf:"
32,[mariadb]
32,skip-networking
32,"You will still be able to log in locally as before, but only using Unix sockets."
32,Grant remote access
32,"Warning: This is not considered as best practice and may cause security issues. Consider using Secure Shell, VNC or VPN, if you want to maintain the MariaDB server from another host inside/outside your network."
32,"To allow remote access to the MariaDB server, ensure that MariaDB has networking enabled and is listening on the appropriate interface."
32,Grant any MariaDB user remote access (example for root):
32,# mariadb -u root -p
32,Check current users with remote access privileged:
32,"SELECT User, Host FROM mysql.user WHERE Host <> 'localhost';"
32,Now grant remote access for your user (here root):
32,GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.1.%' IDENTIFIED BY 'my_optional_remote_password' WITH GRANT OPTION;
32,You can change the '%' wildcard to a specific host if you like. The password can be different from user's main password.
32,Configure access to home directories
32,"For security reasons, the systemd service file contains ProtectHome=true, which prevents MariaDB from accessing files under the /home, /root and /run/user hierarchies. The datadir has to be in an accessible location and owned by the mysql user and group."
32,You can modify this behavior by creating a supplementary service file as described here.
32,Maintenance
32,Upgrade databases on major releases
32,"Upon a major version release of mariadb (for example mariadb-10.3.10-1 to mariadb-10.9.4-1), it is wise to upgrade the system database to make new server features available:"
32,# mariadb-upgrade -u root -p
32,To upgrade from 10.3.x to 10.9.x:
32,perform a clean shutdown of the 10.3.x server
32,upgrade the package
32,run mariadb_upgrade (from the new package version) against the new running daemon
32,"If the (new) daemon is not starting, see #Unable to run mariadb-upgrade because MariaDB cannot start."
32,"Checking, optimizing and repairing databases"
32,"mariadb-clients ships with mariadb-check which can be used to check, repair, and optimize tables within databases from the shell. See mariadb-check(1)[dead link 2023-08-10] for more."
32,Several command tasks are shown:
32,To check all tables in all databases:
32,$ mariadb-check --all-databases -u root -p -c
32,To analyze all tables in all databases:
32,$ mariadb-check --all-databases -u root -p -a
32,To repair all tables in all databases:
32,$ mariadb-check --all-databases -u root -p -r
32,To optimize all tables in all databases:
32,$ mariadb-check --all-databases -u root -p -o
32,Backup
32,There are various tools and strategies to back up your databases.
32,"If you are using the default InnoDB storage engine, a suggested way of backing up all your bases online while provisioning for point-in-time recovery (also known as ""roll-forward"", when you need to restore an old backup and replay the changes that happened since that backup) is to execute the following command:"
32,$ mariadb-dump --single-transaction --flush-logs --events --routines --master-data=2 --all-databases -u root -p > all_databases.sql
32,"This will prompt for MariaDB's root user's password, which was defined during database #Configuration."
32,"Specifying the password on the command line is strongly discouraged, as it exposes it to discovery by other users through the use of ps aux or other techniques. Instead, the aforementioned command will prompt for the specified user's password, concealing it away."
32,Compression
32,"As SQL tables can get pretty large, it is recommended to pipe the output of the aforementioned command in a compression utility like gzip:"
32,$ mariadb-dump --single-transaction --flush-logs --events --routines --master-data=2 --all-databases -u root -p | gzip > all_databases.sql.gz
32,Decompressing the backup thus created and reloading it in the server is achieved by doing:
32,$ zcat all_databases.sql.gz | mariadb -u root -p
32,This will recreate and repopulate all the databases previously backed up (see this or this).
32,Non-interactive
32,"If you want to setup non-interactive backup script for use in cron jobs or systemd timers, see option files and this illustration for mariadb-dump."
32,Basically you should add the following section to the relevant configuration file:
32,[mariadb-dump]
32,user=mysqluser
32,password=secret
32,"Mentioning a user here is optional, but doing so will free you from having to mention it on the command line. If you want to set this for all tools, including mariadb-client, use the [client] group."
32,Example script
32,"The database can be dumped to a file for easy backup. The following shell script will do this for you, creating a db_backup.gz file in the same directory as the script, containing your database dump:"
32,#!/bin/sh
32,"THISDIR=$(dirname $(readlink -f ""$0""))"
32,mariadb-dump --single-transaction --flush-logs --events --routines --master-data=2 --all-databases \
32,| gzip > $THISDIR/db_backup.gz
32,"echo 'purge master logs before date_sub(now(), interval 7 day);' | mariadb"
32,See also the official mariadb-dump page in the MariaDB manuals.
32,Holland Backup
32,"A python-based software package named Holland Backup allows to automate all of the backup work. It supports direct mysqldump, LVM snapshots to tar files (mysqllvm), LVM snapshots with mysqldump (mysqldump-lvm), and xtrabackup methods to extract the data. The Holland framework supports a multitude of options and is highly configurable to address almost any backup situation."
32,"The main hollandAUR and holland-commonAUR packages provide the core framework; one of the sub-packages (holland-mysqldumpAUR, holland-mysqllvmAUR and/or holland-xtrabackupAUR must be installed for full operation. Example configurations for each method are in the /usr/share/doc/holland/examples/ directory and can be copied to /etc/holland/backupsets/, as well as using the holland mk-config command to generate a base configuration for a named provider."
32,Troubleshooting
32,Unable to run mariadb-upgrade because MariaDB cannot start
32,Try run MariaDB in a standalone:
32,# mariadbd-safe --datadir=/var/lib/mysql/
32,And then run:
32,# mariadb-upgrade -u root -p
32,Reset the root password
32,Stop mariadb.service.
32,Start the MariaDB server with safety features: # mariadbd-safe --skip-grant-tables --skip-networking &
32,Connect to it: # mariadb -u root
32,Change root password: MariaDB [mysql]> FLUSH PRIVILEGES;
32,MariaDB [mysql]> ALTER USER 'root'@'localhost' IDENTIFIED BY 'new_password';
32,MariaDB [mysql]> exit
32,Kill running mariadbd* processes: # kill $(cat /var/lib/mysql/$HOSTNAME.pid)
32,Start mariadb.service.
32,Check and repair all tables
32,"Check and auto repair all tables in all databases, see more:"
32,# mariadb-check -A --auto-repair -u root -p
32,Optimize all tables
32,"Forcefully optimize all tables, automatically fixing table errors that may come up."
32,# mariadb-check -A --auto-repair -f -o -u root -p
32,OS error 22 when running on ZFS
32,"If using MySQL databases on ZFS, the error InnoDB: Operating system error number 22 in a file operation may occur."
32,A workaround is to disable aio_writes in /etc/my.cnf.d/my.cnf:
32,/etc/my.cnf.d/my.cnf
32,[mariadb]
32,innodb_use_native_aio = 0
32,"Cannot login through CLI, but phpmyadmin works well"
32,"This may happen if you are using a long (>80) password. mariadb CLI cannot handle that many characters in readline mode. So, if you are planning to use the recommended password input mode:"
32,$ mariadb -u user -p
32,Password:
32,Consider changing the password to smaller one.
32,Note: You still can log in by specifying the password as an argument to mariadb command.
32,"Warning: This behavior is considered dangerous, because your password might leak, for example, to the logs. Use it only in case of emergency and do not forget to change password right afterwards."
32,"$ mariadb -u user -p""some-very-strong-password"""
32,MariaDB binary logs are taking up huge disk space
32,This article or section is out of date.
32,"Reason: /etc/my.cnf.d/my.cnf no longer exists (Discuss in Talk:MariaDB#Mistakes in ""MariaDB binary logs are taking up huge disk space"")"
32,"By default, mariadbd creates binary log files at /var/lib/mysql/mysql-bin.XXXXXX with the numbers ascending. These logs are useful for replication master server or data recovery, but these binary logs can easily eat up large amounts of disk space. If you do not plan to use replication or data recovery features, you may disable binary logging by commenting out these lines in /etc/my.cnf.d/my.cnf then restart:"
32,#log-bin=mysql-bin
32,#binlog_format=mixed
32,"Or, if you want to keep these logs but keep their size in check and old logs deleted, you can set these limits then restart:"
32,log-bin=mysql-bin
32,expire_logs_days = 10
32,max_binlog_size
32,= 100M
32,"Alternatively, there exists a MariaDB command to manually purge logs older than a specific one. For example, you may see a file named mysql-bin.000023 and want to delete every log older than it. As long as the log-bin=mysql-bin setting is in effect, you would run:"
32,"# mariadb -u root -p""PASSWORD"" -e ""PURGE BINARY LOGS TO 'mysql-bin.000023;"""
32,Warning: Using any of these methods may decrease the chances of successful data recovery when trying to repair database tables (i.e. on database corruption).
32,OpenRC fails to start MariaDB
32,"To use MariaDB with OpenRC you need to add the following lines to the [mariadb] section in the MySQL configuration file, located at /etc/my.cnf.d/my.cnf."
32,user = mysql
32,basedir = /usr
32,datadir = /var/lib/mysql
32,pid-file = /run/mysqld/mysql.pid
32,You should now be able to start MariaDB using:
32,# rc-service mysql start
32,Changed limits warning on max_open_files/table_open_cache
32,"Increase the number of file descriptors by creating a systemd drop-in, e.g.:"
32,/etc/systemd/system/mariadb.service.d/limit_nofile.conf
32,[Service]
32,LimitNOFILE=8192
32,"10.4 to 10.5 upgrade crash: ""InnoDB: Upgrade after a crash is not supported. The redo log was created with MariaDB 10.4.x"""
32,"Before MariaDB 10.5, redo log was unnecessarily split into multiple files.[4]"
32,Do NOT ever remove the old binary logs /var/lib/mysql/ib_logfile* out of the way.
32,"To resolve this, install MariaDB 10.4. Start it and let it undergo a clean shutdown. After that happens you can upgrade to 10.5 again. Same applies if another version of MariaDB was specified."
32,Table 'mysql.xxx' does not exist in engine
32,"Symptom: When running mariadb-upgrade or mariadb-check, it return one or more error like these:"
32,Table 'mysql.xxx' does not exist in engine
32,"Where ""xxx"" usually is the system table inside the mysql database."
32,"Steps to fix this,"
32,"Create backup directory outside of MariaDB ${DATADIR}</nowiki>, for example in $HOME/mariadb_backup."
32,"Copy the offending files from ${DATADIR}/mysql/xxx.{frm,ibd}</nowiki> to backup directory. The xxx.ibd may not exist."
32,Drop the tables with DROP TABLE mysql.xxx on the mariadb prompt.
32,"Run the mariadb-check. On success, the file xxx.frm and xxx.ibd should be created again."
32,Re-run mariadb-upgrade if necessary. You may need the --force option.
32,See also
32,MariaDB Official Website
32,MariaDB knowledge Base
32,MySQL Performance Tuning Scripts and Know-How
32,"Retrieved from ""https://wiki.archlinux.org/index.php?title=MariaDB&oldid=785048"""
32,Category: Relational DBMSsHidden categories: Pages with dead linksPages or sections flagged with Template:Out of date
32,"This page was last edited on 10 August 2023, at 21:16."
32,Content is available under GNU Free Documentation License 1.3 or later unless otherwise noted.
32,Privacy policy
32,About ArchWiki
32,Disclaimers
37,鲲鹏社区-官网丨凝心聚力 共创行业新价值鲲鹏社区鲲鹏开发者鲲鹏开发者社区鲲鹏众智鲲鹏MVP鲲鹏生态创新中心Powered by Kunpeng请启用JavaScript
39,MySQL Database Performance: Avoid this common mistake
39,"hayden_james@linux aboutarticlesservicesfeedbacksponsorscontactforumsMySQL Database Performance: Avoid this common mistake March 21, 2023 by Hayden James, in Blog LinuxOne frequent topic of support request emails I receive is MySQL database performance. Clients complain about MySQL using too much server memory, too many MySQL slow queries, the famous Mysql server has gone away errors, and many other MySQL performance-related issues. As such, I wanted to share solutions to some common MySQL configuration mistakes.If you are a DBA, feel free to share your experiences or suggestions in the comments section. I’m referring to four MySQL config variables. These four my.cnf config lines are often responsible for poor MySQL database performance and scaling due to insufficient server memory for incoming connections. This article also applies to MariaDB and Percona drop-in replacements for MySQL.Note: This is an update and follow-up to this article. I’ve added some relevant and more up-to-date information and links, as well as a new section that covers some additional my.cnf performance config tips. Avoid arbitrarily increasing MySQL per-connection buffers Excerpt of my.cnf edits for a client. (some lines were removed and some blurred to avoid misuse)Who knows where or when this bad practice started. I frequently encounter instances where I log into production servers for the first time and find that the values of virtually every my.cnf variable have been arbitrarily increased without sound reasoning behind the changes.This is never a good practice, especially for four my.cnf variables discussed below. Although increasing the value of some variables can improve performance, these four will almost always degrade MySQL server performance and capacity when increased.The four buffers in question are the join_buffer_size, sort_buffer_size, read_buffer_size and read_rnd_buffer_size.These four buffers are allocated per connection. For example, a setting of join_buffer_size=1M with max_connections=200 will configure MySQL to allocate an additional 1M per connection (1M x 200). The same goes for the other three buffers. Again, all are per connection.A good rule of thumb is that if you can’t provide a valid reason to increase any of these buffers, keep them set to the default values.In nearly all cases, it’s best to keep the defaults by removing or commenting out these four config lines. As connections increase with traffic, queries that need more space than what’s available due to larger buffer settings may trigger paging those buffers to disk. This dramatically slows down your DB server and creates a bottleneck.I’ve often seen significantly improved MySQL performance by simply reverting these buffers to their defaults. Also, read Analyzing Linux server performance with atop.The above screenshot shows MySQL status output two weeks after I made the first my.cnf optimization pass. Now, let’s take a closer look at each of these buffers. MySQL join_buffer_sizeThe join_buffer_size is allocated for each full join between two tables. From MySQL’s documentation the join_buffer_size is described as: “The minimum size of the buffer that is used for plain index scans, range index scans, and joins that do not use indexes and thus perform full table scans.”MySQL’s documentation goes on to say: “Memory allocation time can cause substantial performance drops if the global size is larger than needed by most queries that use it.” The join buffer is allocated to cache table rows when the join can’t use an index.If your database suffers from many joins performed without indexes, it cannot be solved by increasing join_buffer_size. The problem is “joins performed without indexes.” Thus, the solution for faster joins is to add indexes. MySQL sort_buffer_sizeUnless you have data indicating otherwise, you should avoid arbitrarily increasing the sort_buffer_size. Memory here is also assigned per connection!MySQL’s documentation warns: “On Linux, there are thresholds of 256KB and 2MB where larger values may significantly slow down memory allocation, so you should consider staying below one of those values.”Avoid increasing sort_buffer_size above 2M since there is a performance penalty that will eliminate benefits. MySQL read_buffer_size & read_rnd_buffer_sizeUpdate: Some of you still use MyISAM over InnoDB. MyISAM is based on the old ISAM storage engine. It has many useful extensions, as discussed here. You should consider converting your MySQL tables to the InnoDB storage engine.InnoDB is the default storage engine of MySQL 5.7 and MySQL 8.0. InnoDB features improved performance, rollback and crash-recovery capabilities to protect data. read_buffer_size Applies generally to MyISAM.Each request that performs a sequential table scan allocates a read buffer. The read_buffer_size system variable determines the buffer size.Starting from MySQL 8.0.22, the value of select_into_buffer_size overrides the value of read_buffer_size when performing SELECT INTO DUMPFILE and SELECT INTO OUTFILE statements. read_buffer_size is used for the I/O cache buffer size in all other cases.The read_rnd_buffer_size variable is also used mainly for MyISAM reads from tables. Again, consider InnoDB or MariaDB’s Aria storage engines. For the past decade, the default values of these buffers have remained the same. Additional my.cnf Performance config tipsMonitor the appropriate MySQL status variable_name(s) to assist you in tuning the below config lines. An example of viewing the status of MySQL runtime variables. (So I increased max_connections)max_connectionsA MySQL configuration parameter that sets the maximum number of concurrent connections the server can handle. A value that is too large can allocate too much server memory and lead to performance issues, while a low value can result in connection rejections.To tune this parameter, monitor the max_used_connections status variable; try to set this to ~ 2x to 3x your max used connections after a few days of uptime. However, if your MySQL max connections are more than 200 then make sure to have at least 100 extra connections available. So if max_used_connections is 300 then set max_connections to at least 400. Watch memory usage!!thread_cache_sizeDetermines the number of threads that can be cached and reused by the server. Properly tuning this parameter can help improve the performance of your MySQL server by reducing the overhead of creating new threads and freeing up system resources.To tune this parameter, you can monitor the threads_created and threads_cached status variables and set the value to a reasonable number based on the number of expected concurrent connections. As a general rule, a value of 16 is a good starting point. Then increase gradually until it’s at least 50% of max_connections. For example, if the threads_cached value is 16 and the max_used_connections value is 40 then increase this from 16 to 32. At the same time, as per above advice, if max_used_connections is 40 then your max_connections should be at least 80.table_definition_cacheA MySQL configuration parameter that sets the number of table definitions (metadata) that can be stored in the cache. A high value of table_definition_cache can improve performance by reducing the time required to open tables, while a low value can result in increased overhead of reading the table definition from disk.To tune this parameter, monitor the opened_table_definitions and open_table_definitionsstatus variables, set table_definition_cache to ~ 2x the number of open table definitions and adjust as needed. Consider other factors, such as system memory and query complexity when tuning. .table_open_cacheA MySQL configuration parameter that sets the number of open table objects that can be stored in the cache. A high value of table_open_cache can improve performance by reducing the time required to open tables, while a low value can result in increased overhead of opening tables from disk.To tune this parameter, monitor the opened_tables and open_tables status variables, set table_open_cache to ~ 2x the number of open tables, and adjust as needed. Consider other factors such as memory and query complexity when tuning. Generally table_open_cache and table_definition_cache are set to the same value. The number of open table objects and table definitions are directly related, so setting these two parameters to the same value can ensure that the metadata and open table objects are stored in the cache in a consistent and efficient manner.wait_timeoutA MySQL configuration parameter that sets the number of seconds the server waits for activity on a non-interactive connection before closing it. A high value of wait_timeout can result in increased memory usage and increased # of connections, while a low value can result in frequent disconnections for long-running queries.To tune this parameter, set wait_timeout to the lowest resonable expected duration of non-interactive connections, and adjust as needed. The default value is 28800 seconds. This is way to high in most cases and will consume a lot of system memory by keeping many connections open. I set most of my configs to 30 seconds or less.connect_timeoutA MySQL configuration parameter that sets the number of seconds the server waits for a successful connection to be established before timing out. A high value of connect_timeout can result in increased wait time for clients and increase in max_connections. A low value can result in connection failures for slow or heavily loaded servers.To tune this parameter, set connect_timeout to a reasonable value based on the expected response time of the server. Consider other factors such as network latency, mobile users, and server load when tuning. I set most of my configs to 15 seconds or less. For example, this blog’s mysql connection timeout is set to 5 seconds.interactive_timeoutA MySQL configuration parameter that sets the number of seconds the server waits for activity on an interactive connection before closing it. A high value of interactive_timeout can result in increased memory usage and increased concurrent connetions. A low value can result in frequent disconnections for long-running queries.To tune this parameter, set interactive_timeout to a reasonable value based on the expected duration of your longest-running requests.tmp_table_sizeA MySQL configuration parameter that sets the maximum size of in-memory temporary tables used by the server. When a query creates a temporary table that exceeds the value of tmp_table_size, the table will be automatically converted to an on-disk table, which can result in decreased performance.To tune this parameter, set tmp_table_size to a reasonable value based on the available memory and the value of the created_tmp_disk_table variable.max_heap_table_sizeA MySQL configuration parameter that sets the maximum size of in-memory temporary tables created with the MEMORY storage engine. When a query creates a temporary table that exceeds the value of max_heap_table_size, the table will be automatically converted to an on-disk table, which can result in decreased performance.To tune this parameter, set max_heap_table_size to the same value as tmp_table_size.Want more tips?For example, Innodb tuning tips, setting open_files_limit, etc. Let me know your interests in the comments section below, or hire me for MySQL optimization. Additional MySQL performance related articlesMySQL Performance Tuning: Tips, Scripts and Tools.MySQL Performance: Stop hoarding. Drop unused MySQL databases.“MySQL server has gone away” error – Solution(s).Could not increase number of max_open_files to more than… (Solution).Linux server performance: Is disk I/O slowing your application?Tuning MySQL my.cnf? Avoid this common pitfall! ConclusionIn conclusion, we discussed some common issues with MySQL performance often raised in support requests. The focus was on four specific my.cnf variables frequently responsible for poor database performance and scaling due to a shortage of server memory.It was emphasized that arbitrarily increasing these variables can lead to a decline in MySQL server performance and capacity and that it is important to keep these buffers set to their default values unless there is a specific reason to increase them. Then we covered tuning some additional MySQL performance config lines.A good rule of thumb: if you can’t provide a valid reason for increasing these buffers, keep them set to the default values. This also applies less strictly to all of MySQL’s variables. Be careful when making changes, don’t overwrite your entire my.cnf all at once. Back up everything first, make one or two changes per restart and test for 24 to 48 hours before making another pass."
39,"Tags: mariadb, mysql, performance, sysadminsComments ← Redis Monitoring: Tools and SolutionsMySQL 8 sample config (my.cnf example) and tuning. →Join over 10,000 Linux and tech industry experts: subscribe to my newsletter!+ Receive a free PDF of 101 useful Linux commands. - Subscribe now!Sponsored by:haydenjames.io: 1+ million readers yearly. Weekly Linux/tech articles & free discussion forums.Popular Topicssysadmins, linux, performance, server, security, commands, apm, desktop, observability, debianPopular Articles90 frequently used Linux Commands60 Linux Networking commands and scripts100 Top Server Monitoring & APM SolutionsUnderstanding PHP memory_limitBest Linux Distros for DesktopHome Lab Beginners guide – Hardwareiowait – How does it affect Linux performance?“MySQL server has gone away” – Solution(s)Almost Always Add Swap SpaceHow to Securely Copy Files Using SCP examplesRecent Articles PHP 8.3 is Out! – 60% Still Using End-of-Life PHP 7 PHP 8 Compatibility Check and Performance Tips Passwordless Authentication Services Building a Neural Network with PyTorch Join us on linuxcommunity.io – Your New Linux Hub! 50 Mind-blowing ways AI will Reshape Our World in 2024 Must-Have Devices to boost your Linux skills Best Linux Compatible Laptops ThinkPad T14s Gen 3 AMD Linux User Review + Tweaks 25 Best CDN Providers 2023 Don’t miss out on Ubuntu Pro (Free for Personal use) 5 Network Devices for work-from-home and Small Business 2023 AI Regulation: AI to Regulate Itself! bottom (btm) – A Comprehensive System Resource Monitor Cloud monitoring 101: Benefits and best practices© 2023 Hayden James. Privacy Policy, Terms. Follow:~# Twitter | Linkedin | Newsletter _"
40,7 Best MariaDB Monitoring Tools for 2023 (Paid & Free)
40,Menu
40,Close
40,Search
40,Search
40,VPN
40,By Use
40,Best VPNs of 2023
40,Business VPN
40,Netflix
40,Kodi
40,Torrenting
40,Hulu
40,Sky Go
40,Gaming
40,BBC iPlayer
40,Tor
40,By OS/Device
40,Mac
40,Windows
40,Linux
40,Windows 10
40,Firestick
40,iPhone and iPad
40,Android
40,Windows Phone
40,DD-WRT Routers
40,By Country
40,China
40,Japan
40,Canada
40,Australia
40,Germany
40,France
40,UAE & Dubai
40,Guides
40,Fastest VPNs
40,Cheapest VPNs
40,Free VPNs
40,How to access the deep web
40,Is torrenting safe and legal?
40,Build your own VPN
40,Facebook privacy and security
40,How to encrypt email
40,How to stay anonymous online
40,How we test VPNs
40,See all
40,Reviews
40,NordVPN
40,Surfshark
40,ExpressVPN
40,IPVanish
40,PrivateVPN
40,StrongVPN
40,CyberGhost
40,PureVPN
40,See all
40,Antivirus
40,Reviews
40,Norton Antivirus
40,TotalAV
40,Intego VirusBarrier X9
40,McAfee
40,VIPRE
40,Panda Security
40,Eset
40,See all
40,By OS/Device
40,Mac
40,Windows
40,Guides
40,Best Antivirus in 2023
40,Best Free Firewalls
40,Free Antivirus Software
40,Malware Statistics & Facts
40,See all
40,Compare providers
40,McAfee vs Kaspersky
40,Norton vs Kaspersky
40,McAfee vs Norton
40,Online backup
40,Streaming
40,Kodi
40,Plex
40,Sports Streaming
40,TV Streaming
40,IPTV
40,Blog
40,VPN & Privacy
40,Cloud and Online Backup
40,Information Security
40,More Comparisons
40,Password Managers
40,Identity Theft Protection
40,Usenet
40,Privacy & Security Tools
40,Internet Providers
40,Parental Control Software
40,Net Admin Tools
40,Data Privacy Management
40,Data Recovery Software
40,Crypto
40,Utilities
40,About Us
40,About Our Company
40,Press
40,Software Testing Methodology
40,Editorial Process
40,Join us
40,Contact
40,Net AdminThe Best MariaDB Monitoring Tools
40,We are funded by our readers and may receive a commission when you buy using links on our site.
40,The Best MariaDB Monitoring Tools
40,"MariaDB is growing in popularity. If you use this DBMS or are thinking of switching to it, you need to know about MariaDB monitoring tools."
40,Stephen Cooper
40,@VPN_News
40,"UPDATED: June 13, 2023"
40,"For a free, open-source, community-supported system, MariaDB has some very strong development talent backing it. MariaDB is a fork of MySQL and it was created in 2009 as a response to Oracle Corporation taking over Sun Microsystems and therefore, acquiring the free-to-use MySQL. The original creator of MySQL was worried that Oracle would make MySQL commercial-only and so copied the entire system and gave it the name MariaDB."
40,"Oracle Corporation wasn’t as greedy as its opponents feared it would be. Therefore, MySQL is still very widely used, especially for website databases. This continued free availability of MySQL has possibly reduced the growth of MariaDB. MySQL outranks MariaDB in a number of implementations to a great degree."
40,Here is our list of the seven best MariaDB monitoring tools:
40,"AppOptics EDITOR’S CHOICE This is a cloud-based infrastructure and applications monitor that includes monitoring procedures to track the performance of relational databases, including MariaDB. Start a 30-day free trial."
40,"ManageEngine Applications Manager (FREE TRIAL) This package monitors a long list of DBMSs, including MariaDB and MySQL, and covers servers and cloud platforms as well. Runs on Windows Server, Linux, AWS, and Azure. Start 30-day free trial."
40,Site24x7 Server Monitoring (FREE TRIAL) This cloud-based package of monitoring services allows you to observe the full stack and put its MariaDB performance tracking into a system-wide context. Start a 30-day free trial.
40,"Sematext MariaDB Monitoring (FREE TRIAL) This service is part of an Infrastructure Monitoring package that will also monitor networks, servers, and applications. Offered as a SaaS package or a virtual appliance. Access the 14-day free trial."
40,"Datadog A cloud platform that offers a range of monitoring tools and the Infrastructure module covers services and the servers they run on. This service will identify all of your services and, with an integration, keep a close eye on MariaDB"
40,Opsview A system monitor with a plugin for specialized MariaDB monitoring capabilities. It is available for installation on Linux or as a cloud-based service.
40,LogicMonitor A flexible cloud-based monitoring tool that includes AI processes and has MariaDB monitoring capabilities.
40,"The similarity between MySQL and MariaDB makes it very easy to port applications from one system to the other. Another name to look out for is SkySQL, which is a hosted version of MariaDB. Amazon RDS offers MariaDB support. So, when you are looking for a MariaDB monitoring tool, the probability is that the system will also monitor these related systems."
40,Database monitoring considerations
40,"The current structure of the IT industry is very diverse. The creation of cloud services means businesses take on subscriptions to bundles of applications and services, choosing a specific headline system, not paying attention to the backend systems that support it. As a result, it is very likely that a typical business has many different services hosting in geographically dispersed locations to monitor and they could very well all have different DBMSs supporting them."
40,"It isn’t unexpected that you have a mix of DBMSs even if you made a corporate decision to use MariaDB for your in-house development. Although this guide is focused on monitoring tools for MariaDB, getting a system that is able to monitor other DBMSs as well is a big help."
40,The best MariaDB monitoring tools
40,"When looking for MariaDB monitoring tools, we focused on systems that had a wide range of capabilities, not those that only monitor MariaDB. This will save you a lot of money because you won’t need to buy other monitoring systems whenever you discover that some online service you subscribe to actually has a different DBMS behind it."
40,"Luck favors the prepared, so having the capability to monitor many different DBMSs means that your business is ready for anything. You don’t want to wade through very long menus of service that you don’t need and you don’t want to have to install a very large software package that has all of the code to monitor databases that you might not use in the future."
40,"The ideal database monitoring system is one that allows you to add on the capability of monitoring a specific DBMS, so you end up with just those screens you need to monitor the systems that you know you have. Of course, the database monitors that made it onto our list all have MariaDB monitoring capabilities."
40,Our methodology for selecting a MariaDB monitoring tool
40,We reviewed the market for MariaDB monitoring systems and analyzed tools based on the following criteria:
40,Instance query activity
40,SQL performance tuning
40,Recommendations to optimize database objects
40,Live tracking of supporting server resource availability
40,Historical analysis tools
40,A free trial or a demo system that enables the monitor to be tested before buying
40,Value for money from a monitoring tool that efficiently tracks and tunes MariaDB performance for a fair price
40,"With these selection criteria in mind, we identified database monitoring tools that can save money by monitoring a range of DBMSs, including MariaDB."
40,1. AppOptics (FREE TRIAL)
40,"AppOptics by SolarWinds is a cloud-based application performance monitor. This monitoring service can watch the performance of on-premises and SaaS systems and it can unify the monitoring of all systems no matter where they are hosted. This service has database monitoring capabilities, which include performance tracking for MariaDB."
40,Key Features:
40,Cloud-based
40,Monitor databases
40,Monitor cloud-based MariaDB
40,Track 60 metrics
40,Alerts for problems
40,Why do we recommend it?
40,AppOptics is a cloud SaaS package with two plans and you can get MariaDB monitoring in the cheaper Infrastructure plan. All of the features of that plan are bundled into the higher APM edition as well. The top plan uses AI to create a service map and predict resource demand.
40,"The base package of AppOptics covers standard services and applications and is also able to check on server resources. You need to add on the capability to monitor MariaDB. However, the procedure to activate the MariaDB plugin is very straightforward and the extra modules are free.  The module for MariaDB monitoring is actually a MySQL add-on. However, it works just as well with MariaDB instances."
40,The MariaDB service monitors about 60 factors. These are measured from within the database and cover a range of factors including actual query activity and administration actions. The monitor also checks on the resources of the host for your database and looks out for resource shortages.
40,Who is it recommended for?
40,"The Infrastructure Plan is surprisingly affordable for even for small businesses. Both editions are priced per host, so the system is scaleable, catering to all sizes of businesses. The cloud location means that the service has no problems combining hosts and planforms in different locations into the same plan."
40,Pros:
40,Consolidate monitoring for multiple instances on many sites
40,Unify the monitoring of on-site and cloud-based databases
40,Identifies application dependencies and creates a map
40,"Monitors PostgreSQL, Cassandra, MariaDB, Aurora, and MySQL"
40,Applies distributed tracing to record module interactions
40,Cons:
40,No self-hosted option
40,"AppOptics is a subscription service and it is offered in two editions. The lower of these is called Infrastructure Monitoring and this includes the MariaDB capabilities. So, it is all you need to monitor your databases. The higher plan is called Infrastructure and Application Monitoring. SolarWinds offers AppOptics on a 30-day free trial."
40,EDITOR’S CHOICE
40,AppOptics is our first choice for monitoring MariaDB! We particularly like how it automatically builds an application dependency map that supports root cause analysis for performance problem-solving. SolarWinds AppOptics includes an alerting mechanism that triggers email and SMS notifications to technicians if problems arise.
40,Start 14-day FREE Trial: https://my.appoptics.com/sign_up
40,OS: Cloud-based
40,2. ManageEngine Applications Manager (FREE TRIAL)
40,"ManageEngine Applications Manager provides monitoring services for custom code and off-the-shelf applications with an extensive list of database systems that it can track. Maria DB is on that list, as are MySQL, SQL Server, PostgreSQL, and Oracle. The tool is able to extract activity metrics from within running databases to enable the measurement of responses as well as capacity issues."
40,Key Features:
40,MariaDB performance analysis
40,Resource demand examination
40,Server resource availability monitoring
40,Application dependency mapping
40,Why do we recommend it?
40,"ManageEngine Applications Manager provides monitoring for all of the systems that lie between user-facing software and the resources of servers that underpin everything. This service is able to monitor databases, including MySQL and MariaDB and it can watch them whether they are hosted on your servers, on cloud platforms, or in a mix of locations."
40,The Applications Manager includes a discovery service that scours your servers and cloud accounts and lists all of the software that it encounters. It then follows process threads to identify all of the supporting modules and microservices that the application triggers. The system then looks at interconnections between applications such as a frontend interaction with a database. This creates an application dependency map that enables Application Manager to lay down plans for root cause analysis in case problems arise.
40,"The system focuses on response time and calls for services. This serves as a chain of reliance that can pinpoint the exact cause of a user-facing app running badly. For example, if the synthetic monitoring tool in the package records a slow response in a Web page, it can see straightaway if a call to a MariaDB instance is the cause. Then the issue is going to be whether the connection, the DBMS, or the specific query was slow. Identifying this issue solves the problem automatically."
40,"Carrying into solutions for slow-running queries in MariaDB, the monitor shows the objects – tables and indexes – involved in delivering a result. It also shows you whether locks were to blame for slow responses or whether file I/O was involved."
40,"Looking beneath the MariaDB instance, Applications Manager links the running events inside the database with access to server resources. If the CPU is overloaded or memory is sparse, the monitor has found the cause of the response time problem."
40,"Alongside the automated scrutiny of database activity, the Applications Manager offers distribute tracing and code profiling that simultaneously identifies each line of code as it is being executed. Maybe the system problem wasn’t caused by the database at all, but you will immediately know what other issue was in play at the time that an issue arose."
40,"The Applications Manager includes a series of thresholds for performance and you can also formulate your own. When one of these thresholds gets crossed, Applications Manager generates an alert and you can get those notifications sent to your phone, email account, or Service Desk ticketing system."
40,Who is it recommended for?
40,"The Applications Manager is free to use in order to monitor up to five assets. The full, paid version is accessible even to small businesses and it gives you lots of features, chief among which, are asset discovery and application dependency mapping. This provides root cause analysis if anything goes wrong."
40,Pros:
40,Query analysis and live execution tracking
40,Database resource availability monitoring
40,Root cause analysis
40,Tracks issues on servers and cloud platforms
40,Cons:
40,Not a SaaS package
40,"ManageEngine Applications Manager is available as a free edition but that version is limited to tracking five applications. Two paid editions offer monitoring for a single network or a multi-site conglomerate. The software package installs on Windows Server or Linux or and it is also available as a service on AWS and Azure. You can get a 30-day free trial of either of the paid editions. If you decide not to buy, the package switches to the free edition at the end of the trial period."
40,ManageEngine Applications Manager
40,Access 30-day FREE Trial
40,3. Site24x7 Server Monitoring (FREE TRIAL)
40,"Site24x7 is a cloud-based monitoring service. This is another monitoring system that doubles up its MySQL sensors to monitor MariaDB as well. The MySQL/MariaDB service needs to be added to the base package of the monitoring system. This extends the dashboard of the service, adding extra screens for database monitoring."
40,Key Features:
40,Database resource monitoring
40,Alert notifications by email and SMS
40,Tracks replication and backup
40,Why do we recommend it?
40,"Site24x7 Server Monitoring is part of a cloud platform of system monitoring tools the base package can be extended by activating integrations, which are listed in a library on the platform. There is an integration for MySQL, which also monitors MariaDB. This operates in concert with all other monitoring services on the platform."
40,"The MariaDB monitor looks inside each database instance and assesses its structure. It makes notes on the types and numbers of objects and records this information on a details screen in the dashboard. It will then continuously monitor actions such as memory and cache activity, disk interactions, and network interface activity. It counts all queries as they occur and identifies the user account and source for each request."
40,"The monitor also checks on DBMS administrative activities, such as replication. It will give a rundown of top query targets and sources for each instance and also aggregate all of the statistics that it gathers for an overview of the performance of all instances."
40,Site24x7 sets performance thresholds that trigger alerts when crossed. Those alerts can be sent out to technicians by email or SMS. This notification allows data center staff to assume that the MariaDB database instances are all running smoothly because they will be drawn back to the console by a notification if problems are building.
40,Who is it recommended for?
40,The Site24x7 utilities are all bundled into plans that give you full-stack observability. The capacity of the core package is sized to be suitable for small businesses and then larger companies pay for more capacity. There is also an edition for managed service providers.
40,Pros:
40,"Monitors Oracle, SQL Server, MySQL, PostgreSQL, and MariaDB"
40,Records access to databases
40,"Watches server memory, CPU, and disk I/O capacity"
40,Cons:
40,No self-hosting option
40,"Site24x7 is a subscription package that provides an allowance for a range of services, which include network, server, and application monitors. You are only allowed one add-in per server. However, if you need more, you can raise that limit for a fee. The standard subscription account is able to monitor up to 10 servers. You can try Site24x7 Infrastructure on a 30-day free trial."
40,Site24x7 Server Monitoring
40,Access the 30-day FREE Trial
40,4. Sematext MariaDB Monitoring (FREE TRIAL)
40,"Sematext Infrastructure Monitoring is a package of monitoring services that will watch over networks, servers, and applications. This system is very strong at monitoring databases and MariaDB is one of the DBMSs that it will track. The service is also able to monitor SQL Server, PostgreSQL, and other DBMSs."
40,Key Features:
40,Database performance metrics
40,MariaDB log collection
40,Server status tracking
40,Why do we recommend it?
40,"Sematext MariaDB Monitoring is part of an Infrastructure Monitoring package that will monitor server resources and system services, such as Web servers, email systems, and databases. This tool can track the performance of MariaDB databases and it doesn’t matter whether they are hosted on your servers or on cloud platforms."
40,"The MariaDB Monitoring service records important metrics from your MariaDB instances and it can watch over databases on multiple sites and cloud platforms, consolidating the reporting for all of them and offering drill-down paths through to data on the performance of each individual instance."
40,"The package provides full-stack observability, so if your applications that access the MariaDB instances experience problems, the monitoring service can identify which of the two systems is the cause of the issue. The monitor goes beneath the database to watch over the supporting server and it also measures network activity and performance."
40,"If a performance problem arises anywhere on your system, the Sematext service will raise an alert. This notification will be sent to you by email, Slack message, or Webhooks and it will identify which component of the stack is the real cause of the problem. So, you will know immediately whether your MariaDB instances are in trouble or just badly served by supporting infrastructure."
40,Who is it recommended for?
40,"This package has a free tier that monitors five hosts. Even if you pay for higher plans, you still get those first five hosts monitored for free. Sematext is built around Elastic Stack and that system’s maker also produces its own Observability package. Sematext is a SaaS plan or you can run it as a virtual appliance over Docker."
40,Pros:
40,Full stack observability with root cause analysis
40,Deployment options of SaaS or a virtual appliance
40,Database statistics and resource usage
40,Cons:
40,Long data retention periods rack up the bill
40,You can choose to use the Sematext system as a SaaS package or run the package on your own server as a virtual appliance over Docker. The starter plan is called Basic and it is free to use. This edition is limited to monitoring five hosts and has a data retention period of 30 minutes. Longer retention periods or the ability to monitor more servers costs money and the price increases as you lengthen your required data retention period. You can get a 14-day free trial of an unlimited plan.
40,Sematext MariaDB Monitoring
40,Start 14-day FREE Trial
40,5. Datadog Infrastructure
40,"Since the Datadog platform is based in the cloud it is able to monitor systems anywhere. So, whether you are looking to monitor your local site, remote sites, or cloud resources, this tool should be of interest. It can even combine the monitoring of resources located in different locations – including the cloud – to create a system-wide overview."
40,Key Features:
40,SaaS package
40,Monitor on-premises and cloud-based instances
40,Gathers more than 80 metrics
40,Machine learning to create thresholds
40,Why do we recommend it?
40,"Datadog Infrastructure is a strong rival to the Site24x7 system. This SaaS system monitors server resources, services, and background applications, including databases, such as MariaDB. You can link this unit with the Network Device Monitoring, Network Traffic Monitoring, and APM modules to get full-stack observability."
40,"The Datadog Infrastructure module can be expanded by a library of 400 free plugins, called “integrations.” There is an integration available that adds on MariaDB monitoring capabilities. As well as the integration for monitoring your on-premises MariaDB instances, the integrations library offers a special plugin to monitor Azure DB for MariaDB and another for AWS RDS."
40,"The core of the Datadog Infrastructure package monitors those applications and services that support user-facing software and it also keeps track of the availability of server resources. The abilities of this monitoring system extend to virtual systems and cloud resources. This service includes databases, such as MariaDB. The monitor can also track the performance of many other database management systems and provide you with a single console to supervise all of them."
40,The MariaDB monitor collects statistics on more than 80 database statuses. These include query performance data taken from within operating instances. The monitor assesses issues such as cache performance and disk interactions when it examines how the database interfaces to server resources. It can also track the availability of capacity to account for typical requirements of the running database.
40,The Datadog Infrastructure system is able to use machine learning to apply thresholds that predict typical performance levels and identify when a potential resource shortage could impair performance.
40,"By tracking all of the applications that access the database, Datadog creates a dependency map – this feature is called Correlations and it is based on AI processes. The Correlations service is a preparatory system that sets up paths of investigations when applications start to perform badly. This speeds up root cause analysis and buys you time to take evasive action."
40,"The alerts that Datadog Infrastructure generates can all be sent out to technicians as notifications by email or messaging system. So, everyone can leave the monitoring of MariaDB and other services to the automated processes of the Datadog system."
40,Datadog Infrastructure is also great for MariaDB monitoring because it covers the performance of queries within the database as well as factors related to resources needed by the database management system. This tool is able to monitor MariaDB instances running on cloud platforms as well as those instances that you have running on your own servers. It can even consolidate the reporting on a mix of on-site and SaaS databases.
40,Who is it recommended for?
40,"Datadog doesn’t bundle all of its modules together into subscription plans like Site24x7. Instead, each unit is offered individually and you can choose to take one or many of them. Multiple units complement each other and provide extra facilities in combination. The Datadog route is a little more expensive than Site24x7."
40,Pros:
40,Application dependency mapping with AI
40,Integrates with reporting layer of AWS and Azure
40,Identifies application dependencies and creates a map
40,Performs SQL optimization and database tuning
40,Cons:
40,Improves with the addition of other modules
40,"Datadog Infrastructure is packaged in three editions: Free, Pro, and Enterprise. The Free service will only monitor up to five hosts. The Pro has all the monitoring services that you need to support MariaDB databases. The higher-level features of Datadog Infrastructure, such as Correlations and machine learning for performance thresholds are reserved for the Enterprise plan. You can get a 14-day free trial of either of the paid plans."
40,6. Opsview
40,"Opsview is an adaptable resource monitoring package that covers networks, servers, and applications. Its capabilities can be augmented by a library of add-ons, called “opspacks.” There is an opspacks available to monitor MariaDB. It is also possible to install Nagios plugins into Opsview."
40,Key Features:
40,Extension for MariaDB
40,Takes Nagios plug-ins
40,Query analyzer
40,Why do we recommend it?
40,"Opsview provides monitoring for servers, services, and applications with an extra module for network monitoring. This platform provides extensions, called Opspacks and there is one of these that will monitor MySQL and MariaDB databases. The service monitors both on-premises servers and cloud platforms and your databases can be deployed in either of these systems."
40,The MariaDB opspack adds screens to the system dashboard and makes a data collecting agent available. This agent will collect information on replication and interactions with server resources. It also examines the performance of queries. It allows you to select a particular query and watch the variations in its execution performance over time. This is particularly useful for focusing on the performance of a critical database action.
40,"Opsview is able to monitor other database management systems as well as MariaDB. It gives you a single console to monitor all IT resources. As well as monitoring your on-premises services, this tool can check on the performance of cloud resources. It is able to blend together services in overviews that cover resources located on your premises and in the cloud."
40,"The Opsview system includes an autodiscovery service that explores your network and identifies all resources. This extends up to the logging of software and applications. The system scan is continuous, so it spots any changes you make to your infrastructure and updates its records automatically. It will also map VMWare structures. This means that the system sets itself up and keeps itself updated."
40,"Opsview includes a performance threshold system for each of the metrics that it tracks. These trip alerts if crossed and those alerts can be sent out as notifications by email, SMS, or Slack message. The Opsview system creates a stack map and provides support for anomaly detection and root cause analysis."
40,Who is it recommended for?
40,"Opsview is affordable for all sizes of enterprises and there is a plan for SMBs. The Enterprise edition downloads onto Linux, and the Cloud edition is a SaaS package. Opsview Cloud is considerably more expensive but it includes the Network Analyzer, which is a paid extra on the Enterprise plan."
40,Pros:
40,"Provides autodiscovery for all applications, including MariaDB database instances"
40,Creates an application dependency map
40,Query optimization and database tracking
40,Cons:
40,MariaDB has to be added on as a plug-in
40,"Opsview plans are available in three editions: SMB, Enterprise, and Opsview Cloud. SMB is for small businesses and Enterprise is designed for larger organizations. These are both on-premises packages for installation on Linux servers. Opsview Cloud is a hosted SaaS system. All versions have the same functions and all will take the MariaDB opspack. Get a free trial of the cloud service."
40,7. LogicMonitor
40,LogicMonitor applies its monitoring system for MySQL to MariaDB monitoring. This service is part of a cloud-based IT resource monitoring package that can be extended by integration. The MySQL service is one of a library of 2000 integrations.
40,Key Features:
40,Query performance monitoring
40,Replication tracking
40,Server resource planning
40,Why do we recommend it?
40,"LogicMonitor is similar to Opsview but it is only available as a SaaS platform. This is a full-stack monitoring package and it also offers some system management tools, such as configuration management and log management. This system can also be used for security monitoring and it is able to watch over MariaDB databases."
40,"The database monitoring service examines query performance, database management system activity, interactions with server resources and the network, and access by applications and users. All of the metrics that the service collects are displayed as graphs and charts in the dashboard."
40,"The LogicMonitor service deploys AI to set performance expectations and it uses these to generate alerts if anomalies are detected. The system creates a full-stack view, crossing networks, servers, and applications and linking their dependencies. This speeds up root cause analysis for problems, which is also AI-supported."
40,Who is it recommended for?
40,LogicMonitor doesn’t publish its prices and that makes assessing its suitability for small businesses difficult. The SaaS platform is cable to monitor both on-premises systems and cloud platforms equally. The bundle has two plan levels with both including Maria DB monitoring capabilities. The top plan adds on AI-based demand forecasting and application sandboxing.
40,Pros:
40,Implements database monitoring in relation to the systems that use it
40,Correlates database activity to server resource usage
40,Uses AI in root cause analysis
40,Cons:
40,No self-hosting option
40,LogicMonitor has a Core system and also a Website Monitor service – these are two separate packages and you only need the Core system to monitor MariaDB. This plan is available in two editions. These are called Pro and Enterprise – these have the same basic facilities but Enterprise will monitor more than 200 devices and includes the AI features of the service. You can assess LogicMonitor for a 14-day free trial.
40,MariaDB monitoring FAQs
40,Is MariaDB secure?MariaDB databases are secured by encryption and the requirement for user accounts for access means that all activity within a database instance is traceable and logged.
40,"Does MariaDB replace MySQL?MariaDB is a very close copy of MySQL. The two systems are mutually compatible. However, there are no plans to retire MySQL so MariaDB can’t be considered as its replacement."
40,Is MariaDB better than MySQL?Analysts claim that MariaDB is faster at searching data than MySQL. A high number of concurrent connections can slow down the response times of a MySQL instance but that doesn’t happen with MariaDB.
40,What's in this article?Database monitoring considerationsThe best MariaDB monitoring toolsMariaDB monitoring FAQs
40,Comments
40,Leave a Reply Cancel replyCommentName
40,Email
40,This site uses Akismet to reduce spam. Learn how your comment data is processed.
40,Search
40,Search
40,Twitter icon
40,Home
40,Blog
40,Our Authors
40,Privacy policy
40,Cookies Policy
40,Terms of use
40,Disclosure
40,About Comparitech
40,Contact Us
40,Accessibility
40,© 2023 Comparitech Limited. All rights reserved.
40,"Comparitech.com is owned and operated by Comparitech Limited, a registered company in England and Wales (Company No. 09962280), Suite 3 Falcon Court Business Centre, College Road, Maidstone, Kent, ME15 6TF, United Kingdom. Telephone +44(0)333 577 0163"
40,SolarWinds
40,Everything To Manage & Monitor Your Network In One Simple Bundle
40,SolarWinds Top 5 Essential IT Tools
40,Web Heldesk simplifies support ticketing
40,Remote Support - desktop remote control
40,FTP Server for internal and external file sharing
40,Patch Manager - network wide Windows updates
40,"Engineers Toolset has 60+ tools for network monitoring, management and security"
40,DOWNLOAD FREE TRIAL
40,Fully functional for 30 days
40,Comparitech uses cookies. More info.
40,Close
41,MariaDB 10.11 key features overview for DBAs | PPTSlideShare a Scribd company logoSubmit SearchUploadMariaDB 10.11 key features overview for DBAsReportFederico RazzoliVettabase Founder at VettabaseFollow•0 likes•141 views1 of 38MariaDB 10.11 key features overview for DBAs•0 likes•141 viewsDownload NowDownload to read offlineReportSoftwareWebinar: MariaDB 10.11 key features overview for DBAs
41,Orgnised by Vettabase
41,27 April 2023
41,Amongst other topics:
41,- Long ALTER TABLES now don’t cause replicas to lag
41,"- InnoDB configuration is now more dynamic, and certain important variables can be modified without a restart"
41,- Populating an empty table is now much faster
41,"- New data types: UUID, INET4, INET6"
41,"- SFORMAT() function, NATURAL_KEY_SORT() functionRead moreFederico RazzoliVettabase Founder at VettabaseFollowRecommendedWhat's new in MariaDB TX 3.0MariaDB plc142 views•56 slidesThe Full MySQL and MariaDB Parallel Replication TutorialJean-François Gagné3.3K views•113 slidesMariaDB AX: Analytics with MariaDB ColumnStoreMariaDB plc135 views•31 slidesMariaDB: in-depth (hands on training in Seoul)Colin Charles9.2K views•148 slidesNew optimizer features in MariaDB releases before 10.12Sergey Petrunya112 views•27 slidesHistogram-in-Parallel-universe-of-MySQL-and-MariaDBMydbops190 views•41 slidesMore Related ContentWhat's hotOptimizing MariaDB for maximum performanceMariaDB plc2.1K views•35 slidesMariaDB MaxScaleMariaDB plc1.9K views•41 slidesHow to Manage Scale-Out Environments with MariaDB MaxScaleMariaDB plc1.2K views•42 slidesDemystifying MySQL Replication Crash SafetyJean-François Gagné457 views•39 slidesMySQL GTID 시작하기I Goo Lee6.3K views•17 slidesMonitoring MySQL with Prometheus and GrafanaJulien Pivotto6.3K views•107 slidesWhat's hot(20)Optimizing MariaDB for maximum performanceMariaDB plc•2.1K viewsMariaDB MaxScaleMariaDB plc•1.9K viewsHow to Manage Scale-Out Environments with MariaDB MaxScaleMariaDB plc•1.2K viewsDemystifying MySQL Replication Crash SafetyJean-François Gagné•457 viewsMySQL GTID 시작하기I Goo Lee•6.3K viewsMonitoring MySQL with Prometheus and GrafanaJulien Pivotto•6.3K viewsM|18 Architectural Overview: MariaDB MaxScaleMariaDB plc•1.7K viewsMySQL Parallel Replication: inventory, use-case and limitationsJean-François Gagné•3K viewsMariaDB Server Performance Tuning & OptimizationMariaDB plc•8.1K viewsWars of MySQL Cluster ( InnoDB Cluster VS Galera ) Mydbops•2.5K viewsMySQL Performance schema missing_manual_flossukValeriy Kravchuk•4.2K viewsProxySQL & PXC(Query routing and Failover Test)YoungHeon (Roy) Kim•1.4K viewsMySQL Parallel Replication: All the 5.7 and 8.0 Details (LOGICAL_CLOCK)Jean-François Gagné•1.1K viewsQuery logging with proxysqlYoungHeon (Roy) Kim•2.5K viewsMariaDB MaxScale: an Intelligent Database ProxyMarkus Mäkelä•417 viewsAchieving compliance With MongoDB Security Mydbops•171 viewsErrant GTIDs breaking replication @ Percona Live 2019Dieter Adriaenssens•604 viewsWhy MySQL Replication Fails, and How to Get it BackSveta Smirnova•2.8K viewsProxySQL High Avalability and Configuration Management OverviewRené Cannaò•367 viewsMySQL Advanced Administrator 2021 - 네오클로바NeoClova•575 viewsSimilar to MariaDB 10.11 key features overview for DBAsRecent MariaDB features to learn for a happy lifeFederico Razzoli31 views•38 slides[db tech showcase Tokyo 2017] C23: Lessons from SQLite4 by SQLite.org - Richa...Insight Technology, Inc.2.4K views•39 slidesHow to leave the ORM at home and write SQLMariaDB plc167 views•39 slidesDBCC - Dubi Lebelsqlserver.co.il1.3K views•49 slidesCassandra 2012beobal756 views•34 slidesPgconfSV compressionAnastasia Lubennikova577 views•28 slidesSimilar to MariaDB 10.11 key features overview for DBAs(20)Recent MariaDB features to learn for a happy lifeFederico Razzoli•31 views[db tech showcase Tokyo 2017] C23: Lessons from SQLite4 by SQLite.org - Richa...Insight Technology, Inc.•2.4K viewsHow to leave the ORM at home and write SQLMariaDB plc•167 viewsDBCC - Dubi Lebelsqlserver.co.il•1.3K viewsCassandra 2012beobal•756 viewsPgconfSV compressionAnastasia Lubennikova•577 viewsM|18 Understanding the Architecture of MariaDB ColumnStoreMariaDB plc•1.7K viewsMariaDB 10.4 New FeaturesFromDual GmbH•406 viewsIT Tage 2019 MariaDB 10.4 New FeaturesFromDual GmbH•76 viewsM|18 Battle of the Online Schema Change MethodsMariaDB plc•3.7K viewsDevelopers’ mDay 2019. - Bogdan Kecman, Oracle – MySQL 8.0 – why upgrademCloud•132 viewsRocksDB Performance and Reliability PracticesYoshinori Matsunobu•678 viewsUnderstanding the architecture of MariaDB ColumnStoreMariaDB plc•2.8K viewsPostgreSQL WAL for DBAs PGConf APAC•4.6K viewsPerformance improvements in PostgreSQL 9.5 and beyondTomas Vondra•10.3K viewsMySQL Parallel Replication by Booking.comJean-François Gagné•3.1K viewsHow to build TiDBPingCAP•762 viewsPostgreSQL 8.4 TriLUG 2009-11-12Andrew Dunstan•1.5K viewsOOW16 - Oracle Database 12c - The Best Oracle Database 12c New Features for D...Alex Zaballa•93 viewsOOW16 - Oracle Database 12c - The Best Oracle Database 12c New Features for D...Alex Zaballa•569 viewsMore from Federico RazzoliA first look at MariaDB 11.x features and ideas on how to use themFederico Razzoli0 views•36 slidesMariaDB stored procedures and why they should be improvedFederico Razzoli0 views•32 slidesWebinar - MariaDB Temporal Tables: a demonstrationFederico Razzoli29 views•32 slidesWebinar - Key Reasons to Upgrade to MySQL 8.0 or MariaDB 10.11Federico Razzoli49 views•36 slidesAdvanced MariaDB features that developers love.pdfFederico Razzoli98 views•38 slidesAutomate MariaDB Galera clusters deployments with AnsibleFederico Razzoli487 views•21 slidesMore from Federico Razzoli(19)A first look at MariaDB 11.x features and ideas on how to use themFederico Razzoli•0 viewsMariaDB stored procedures and why they should be improvedFederico Razzoli•0 viewsWebinar - MariaDB Temporal Tables: a demonstrationFederico Razzoli•29 viewsWebinar - Key Reasons to Upgrade to MySQL 8.0 or MariaDB 10.11Federico Razzoli•49 viewsAdvanced MariaDB features that developers love.pdfFederico Razzoli•98 viewsAutomate MariaDB Galera clusters deployments with AnsibleFederico Razzoli•487 viewsCreating Vagrant development machines with MariaDBFederico Razzoli•62 viewsMariaDB, MySQL and Ansible: automating database infrastructuresFederico Razzoli•172 viewsPlaying with the CONNECT storage engineFederico Razzoli•126 viewsMariaDB Temporal TablesFederico Razzoli•167 viewsDatabase Design most common pitfallsFederico Razzoli•553 viewsMySQL and MariaDB BackupsFederico Razzoli•774 viewsJSON in MySQL and MariaDB DatabasesFederico Razzoli•900 viewsHow MySQL can boost (or kill) your application v2Federico Razzoli•124 viewsMySQL Transaction Isolation Levels (lightning talk)Federico Razzoli•118 viewsCassandra sharding and consistencyFederico Razzoli•2.3K viewsMariaDB Temporal TablesFederico Razzoli•405 viewsMySQL Query Optimisation 101Federico Razzoli•852 viewsHow MySQL can boost (or kill) your applicationFederico Razzoli•340 viewsRecently uploadedCiti TechTalk Session 2: Kafka Deep Diveconfluent7 views•60 slidesWHMCS CUSTOM SERVICESWHMCS Smarters17 views•7 slidesScala Left Fold Parallelisation"
41,"- Three ApproachesPhilip Schwarz26 views•44 slidesPrompt Engineering - an Art, a Science, or your next Job Title?Maxim Salnikov44 views•33 slidesApplying Platform Engineering Thining to Observability.pdfNatan Yellin8 views•16 slidesHow to Make the Most of Regression and Unit Testing.pdfAbhay Kumar6 views•9 slidesRecently uploaded(20)Citi TechTalk Session 2: Kafka Deep Diveconfluent•7 viewsWHMCS CUSTOM SERVICESWHMCS Smarters•17 viewsScala Left Fold Parallelisation"
41,"- Three ApproachesPhilip Schwarz•26 viewsPrompt Engineering - an Art, a Science, or your next Job Title?Maxim Salnikov•44 viewsApplying Platform Engineering Thining to Observability.pdfNatan Yellin•8 viewsHow to Make the Most of Regression and Unit Testing.pdfAbhay Kumar•6 viewsCycleops - Automate deployments on top of bare metal.pptxThanassis Parathyras•18 viewsHotel API IntegrationSharmiMehta•6 viewsBuild real-time streaming data pipelines to AWS with Confluentconfluent•46 viewsManufacturing route card system with Xero integration.pdfBlackCatComputers•11 views[PHPCon 2023] Blaski i ciebie BDDMateusz Zalewski•27 viewsBSides Lisbon 2023 - AI in Cybersecurity.pdfTiago Henriques•61 viewsMark Simpson - UKOUG23 - Refactoring Monolithic Oracle Database Applications ...marksimpsongw•66 viewsTopic 1 What is Evolutionary Prototyping.pptxAHMADAIMAN77•7 viewsAstraZeneca at Neo4j GraphSummit London 14Nov23.pptxNeo4j•31 viewsesqlfunctions.pdfVotarikari Shravan•305 views[KubeConNA2023] Lima pavilionAkihiro Suda•6 viewsWhat Can Employee Monitoring Software Do?​wAnywhere•7 viewsNeo4j : Graphes de Connaissance, IA et LLMsNeo4j•36 views[Podman Special Event] Kubernetes in Rootless PodmanAkihiro Suda•9 viewsMariaDB 10.11 key features overview for DBAs1. MariaDB 10.11"
41,key features
41,overview for DBAs
41,2. Why this talk?
41,3. ● Features are
41,only good if you use them
41,● You only use features you know
41,● MariaDB is very good at developing features
41,● But not as good at documenting and advertising them
41,The importance of documentation
41,4. mariadb.org/kb
41,● MariaDB KnowledgeBase
41,is a public wiki
41,○ You can contribute under the terms of
41,CC BY-SA / GNU FDL
41,○ The KB has pages on how to contribute
41,Or you can write your own blog posts
41,The importance of documentation
41,5. MariaDB
41,versions
41,6. MariaDB Versions
41,MariaDB 10.6
41,Latest LTS (Long Term Support)
41,Supported until July 2026
41,MariaDB 10.7 Stable STS (Short Term Support)
41,MariaDB 10.8 Stable STS
41,MariaDB 10.9 Stable STS
41,MariaDB 10.10 Alpha STS
41,MariaDB 10.11 Stable LTS
41,EOL: February 2028
41,MariaDB 11.0 RC
41,MariaDB 11.1 Alpha
41,7. ● Maturity levels:
41,Alpha
41,/ Beta / Gamma / RC / Stable
41,● STS versions shouldn’t be used in production
41,○ (sort of)
41,○ Because they’re only supported for 1 year
41,● LTS support is 4 years
41,8. ● We’re going
41,to cover the most important features included in
41,"STSs, that you can expect to see in the next LTS"
41,"● “Most important” according to my opinion, our internal"
41,"discussions, and our customers needs."
41,Others may disagree
41,"● In a logical order, not in a chronological order"
41,● I will not assume that you are an expert
41,This talk
41,9. Replication / HA
41,10. Replication / HA
41,“Lag-free”
41,ALTER TABLE MDEV-11675 10.8
41,Galera IP Allowlist for SST MDEV-27246 10.10
41,JSON SST progress logging MDEV-26971 10.9
41,"11. ● Previously, ALTER"
41,TABLE was executed on the master first
41,"● Only after it succeeded, it started on the replicas"
41,● This usually blocked replication until completion
41,● And ALTER TABLE can take a lot of time on big tables
41,“Lag-free” ALTER TABLE
41,12. You run:
41,SET SESSION
41,binlog_alter_two_phase = 1;
41,ALTER TABLE atest CHANGE COLUMN b b BIGINT SIGNED NOT NULL;
41,Binary log says:
41,ALTER TABLE atest CHANGE COLUMN b b BIGINT SIGNED NOT NULL
41,DEFAULT 1
41,#230425 21:29:41 server id 1 end_log_pos 1087 CRC32
41,0xb1f3cec6 GTID 0-1-5 ddl START ALTER
41,#230425 21:29:41 server id 1 end_log_pos 1271 CRC32
41,0x1b978324 GTID 0-1-6 ddl COMMIT ALTER id= 5
41,“Lag-free” ALTER TABLE
41,13. ● ALTER TABLE
41,"starts on master, and then it can start"
41,immediately on replicas
41,"● Even if it’s faster on replicas, it will only be finalised when it"
41,completes on master
41,"● If it fails on master, replicas will stop the operation"
41,“Lag-free” ALTER TABLE
41,14. SST = State
41,Snapshot Transfer
41,IST = Incremental Snapshot Transfer
41,● When a new Galera node joins a cluster it receives an SST
41,● When a node restarts:
41,○ ideally it receives an IST
41,○ if the Galera Cache doesn’t contain all the changes that
41,"happened since it disconnected, it receives an SST"
41,"● It’s a good practice to keep Galera in a private network, so"
41,unknown nodes can’t request an SST/IST
41,Galera allowlist for SST and SSI
41,15. ● Now we
41,can use wsrep_allowlist:
41,"wsrep_allowlist = '11.11.11.11,22.22.22.22,33.33.33.33'"
41,● It accepts IPv4 and 6
41,"○ No IP ranges, no hostnames, no wildcards"
41,"● Requires a node restart, should be done on all nodes one by"
41,one
41,● The mysql.wsrep_allowlist table shows the allowlist
41,Galera allowlist for SST and SSI
41,16. ● It was
41,a bit hard to debug SST failures based on the error log
41,● Now SST is logged into wsrep_status.json
41,● The format is both human-readable and machine-readable
41,SST logging in JSON
41,17. {
41,"""date"": ""2021-09-04 15:35:02.000"","
41,"""timestamp"":"
41,"1630758902.00000000,"
41,"""errors"": ["
41,"""timestamp"": 1630758901.00000000,"
41,"""msg"": ""mysqld: Can't open shared library"
41,"'/tmp/galera/0/mysql/lib64/mysql/plugin/audit_log.so' (errno: 0, cannot open shared object file: No such file or"
41,"directory)"""
41,"""warnings"": [ …"
41,"""status"": {"
41,"""state"": ""DISCONNECTED"","
41,"""comment"": ""Disconnected"","
41,"""progress"": -1.00000"
41,SST logging in JSON
41,18. Performance
41,19. Performance
41,More dynamic configuration
41,MDEV-27812
41,MDEV-19229
41,and others
41,10.9
41,10.11
41,Faster INSERTs into empty tables MDEV-24621 10.7
41,ASC / DESC indexes 10.8
41,JSON histograms MDEV-26971 10.8
41,Faster collations Multi
41,20. ● These variables
41,are now dynamic:
41,● innodb_log_file_size
41,● innodb_undo_tablespaces
41,● innodb_write_io_threads and
41,innodb_read_io_threads
41,● innodb_buffer_pool_chunk_size is now allocated
41,dynamically
41,● replicate_rewrite_db is now a variable
41,More dynamic configuration
41,21. ● INSERTs into
41,"empty tables already improved in 10.6, more"
41,optimisation was made in 10.7
41,● Especially when the table has indexes (not just the primary
41,key)
41,Faster INSERTs
41,22. ● The ASC
41,and DESC syntaxes were accepted but had no
41,meaning with older versions:
41,"ALTER TABLE post ADD INDEX (date DESC, author ASC);"
41,● Now they work. Each column in an index can be ascending
41,(default) or descending
41,● It’s important to optimise queries that need to order columns
41,in different directions:
41,SELECT *
41,FROM post
41,"ORDER BY date DESC, author ASC;"
41,ASC / DESC indexes
41,23. ● Statistics are
41,"estimations about data distribution in tables,"
41,indexes and columns
41,● They are used by the Optimiser to determine the fastest
41,query plan
41,● In previous versions MariaDB implemented:
41,○ Engine-independent statistics
41,○ Histograms
41,JSON Histograms
41,24. ● Regular statistics
41,"are just number of values, number of unique"
41,values (cardinality)
41,● Histograms are actual histograms about the distribution of ranges
41,of values in a column (not index)
41,● JSON histograms allow more granular and precise statistics
41,● Useful for JOINs with a WHERE that doesn’t use indexes
41,"● JSON histograms are used if they exist, but they need to be"
41,created explicitly:
41,ANALYZE TABLE tbl PERSISTENT FOR ALL;
41,ANALYZE TABLE tbl PERSISTENT FOR COLUMNS (column_list);
41,JSON Histograms
41,25. ● A character
41,set defines which characters may exist in a
41,column
41,● A collation defines how to order these characters
41,"● And, for example, if some characters are the same (A and a,"
41,"a and à, etc…)"
41,● This affects all comparisons and sorting:
41,">, <, =, ORDER BY, GROUP BY…"
41,● Recent versions implemented several optimisations
41,● Like the “group comparison” for characters that are included
41,in the ASCII character set
41,Faster collations
41,26. New Data Types
41,27. INET6 was already
41,supported
41,UUID() function was already supported
41,Data Types
41,UUID 4B 10.7
41,INET4 16B 10.9
41,28. ● Problems with
41,AUTO__INCREMENT
41,○ InnoDB has an auto_increment lock
41,■ (will be removed in a future version)
41,"○ Once you reach the end, you can't insert new values"
41,○ It's guessable (potential security problem)
41,UUID Primary Key
41,29. CREATE TABLE user
41,username
41,"id UUID NOT NULL DEFAULT UUID(),"
41,PRIMARY KEY (id)
41,INSERT INTO user (username)
41,VALUES ('tom.baker');
41,UUID Primary Key
41,30. New Functions
41,31. Functions
41,SFORMAT() MDEV-25015 10.7
41,NATURAL_SORT_KEY()
41,MDEV-4742 10.7
41,RANDOM_BYTES() MDEV-25704 10.10
41,JSON_EQUALS() MDEV-23143 10.7
41,JSONPath enhancements MDEV-27911
41,MDEV-22224
41,10.9
41,32. ● Concatenating strings
41,with CONCAT() can lead to
41,unreadable queries
41,○ Especially if you need to handle NULLs
41,"● SFORMAT() enables more modern interpolation, based on"
41,the fmt library
41,"> SELECT SFORMAT('{} + {} = {}', 3, 5, (3 + 5)) AS str;"
41,+-----------+
41,| str |
41,+-----------+
41,| 3 + 5 = 8 |
41,+-----------+
41,SFORMAT()
41,33. > SELECT SFORMAT('{2}
41,"- {1} = {0}', 3, 5, (3 + 5)) AS str;"
41,+-----------+
41,| str |
41,+-----------+
41,| 8 - 5 = 3 |
41,+-----------+
41,"> SELECT SFORMAT('{:d}th son of a {:d}th son', 7, 7) AS"
41,iron_maiden;
41,+----------------------+
41,| iron_maiden |
41,+----------------------+
41,| 7th son of a 7th son |
41,+----------------------+
41,SFORMAT()
41,34. ● Strings are
41,"ordered alphabetically, according to a collation"
41,● But some strings contain numbers or separators. In that
41,"case, the order decided by a collation alone is different than"
41,what humans expect:
41,> SELECT ip FROM impression ORDER BY 1;
41,+-----------------+
41,| ip |
41,+-----------------+
41,| 100.120.122.200 |
41,| 110.120.122.200 |
41,| 80.222.120.200 |
41,| 80.1.120.200 |
41,+-----------------+
41,NATURAL_SORT_KEY()
41,35. ● NATURAL_SORT_KEY() solves
41,the problem by producing a
41,modified version of the string that will be sorted correctly:
41,"> SELECT ip, NATURAL_SORT_KEY(ip) FROM impression ORDER BY 2;"
41,+-----------------+----------------------+
41,| ip | NATURAL_SORT_KEY(ip) |
41,+-----------------+----------------------+
41,| 80.120.2.200 | 180.2120.02.2200 |
41,| 99.120.122.200 | 199.2120.2122.2200 |
41,| 100.120.122.200 | 2100.2120.2122.2200 |
41,| 110.120.122.200 | 2110.2120.2122.2200 |
41,+-----------------+----------------------+
41,NATURAL_SORT_KEY()
41,36. ● In practice
41,you don’t need to see that modified string:
41,> SELECT ip FROM impression ORDER BY NATURAL_SORT_KEY(ip);
41,+-----------------+
41,| ip |
41,+-----------------+
41,| 80.120.2.200 |
41,| 99.120.122.200 |
41,| 100.120.122.200 |
41,| 110.120.122.200 |
41,+-----------------+
41,NATURAL_SORT_KEY()
41,37. ● JSON_EQUALS() compares
41,two JSON documents ignoring
41,"spaces, the keys order, etc"
41,● Some functions accept JSONPath expressions to find a part
41,of a JSON document. Now MariaDB JSONPath supports:
41,○ Array ranges
41,"SELECT JSON_EXTRACT(json_arr, '$[2 to 5]');"
41,○ Negative indexes
41,"SELECT JSON_EXTRACT(json_arr, '$[-1]');"
41,JSON
41,38. https://vettabase.com/services
41,● Migrations to/from
41,MariaDB
41,"● Upgrades (tests, no downtime)"
41,● Database health checks
41,"● Training for DBAs, devops, developers, data analysts"
41,● And more
41,"MariaDB, MySQL, PostgreSQL, Cassandra"
41,All major cloud providers
41,AboutSupportTermsPrivacyCopyrightCookie PreferencesDo not sell or share my personal informationEnglishCurrent LanguageEnglishEspañolPortuguesFrançaisDeutsche© 2023 SlideShare from Scribd
42,MySQL query cache stats – WordPress plugin | WordPress.org
42,Skip to content
42,Log InRegister
42,WordPress.org
42,NewsShowcaseHostingExtendThemesPluginsPatternsBlocksOpenverse ↗︎LearnLearn WordPressDocumentationForumsDevelopersWordPress.tv ↗︎CommunityMake WordPressPhoto DirectoryFive for the FutureWordCamp ↗︎Meetups ↗︎Job Board ↗︎AboutAbout WordPressEnterpriseGutenberg ↗︎Swag Store ↗︎Get WordPress
42,Search in WordPress.org
42,Get WordPress
42,Plugins
42,My Favorites
42,Beta Testing
42,Developers
42,Search for:
42,Search plugins
42,Download
42,MySQL query cache stats
42,By Moris Dov
42,Details
42,Reviews
42,Development
42,Support
42,Description
42,Admin dashboard widget measuring and monitoring MySQL / MariaDB query cache performance & statistics – to optimize sizing configuration and highlight bottlenecks.
42,Why
42,Database Query for Options Autoload is repeatedly executed for each WP page load and its poor performance has been identified as a good indicator for a poorly performing database resulting in slow site.
42,How
42,Use dashboard widget to measure and monitor Options Autoload Query Time
42,Use dashboard widget to measure and monitor Options Autoload Query Size
42,"This query result size increases constantly over time, as WP site usage grows – but its oversized sudden growth or constant balooning must not be overlooked."
42,Heavy plugins might rely heavily on adding data to Options Autoload Query Size.
42,Poorly written plugins causing WP performance degradation have been found to constantly increase this query result size.
42,Time Metrics
42,‘Options Autoload Query Time’ is measured in milliseconds (ms)
42,0 ms – great performance below one millisecond
42,1-2 ms – good performance
42,3-5 ms – OK performance
42,50+ ms – poor performance should be investigated
42,Size Metrics
42,‘Options Autoload Size’ is measured in Bytes
42,If you have the control to enable the database server wide query cache – configure query_cache_limit to be larger than Options Autoload Size
42,"20,000 Bytes – freshly installed clean wordpress site"
42,"30,000 Bytes – small wordpress site"
42,"50,000 Bytes – medium wordpress site"
42,"250,000 Bytes – large wordpress site"
42,"2,000,000+ Bytes – too large should be investigated"
42,( all above are rough metrics )
42,Screenshots
42,Active query cache statsDisabled query cache statsExecute database command RESET QUERY CACHE
42,FAQ
42,This plugin improves page load performance ?
42,"No, This plugin in itself does not improve performance by being installed, this plugin only measures time and size of database query."
42,This plugin improves database performance ?
42,"No, This plugin in itself does not improve performance by being installed, an administrator should use the dashboard widget to measure, monitor and identify problems."
42,Must database query cache be enabled ?
42,"No, This plugin measures time and size of a very important database query and these measurements should be valuable to an administrator whether database query cache is enabled or not."
42,Reviews
42,There are no reviews for this plugin.
42,Contributors & Developers
42,“MySQL query cache stats” is open source software. The following people have contributed to this plugin.Contributors
42,Moris Papasmadov
42,"Translate “MySQL query cache stats” into your language.Interested in development?Browse the code, check out the SVN repository, or subscribe to the development log by RSS."
42,Changelog
42,1.0.4
42,Added wp_options Autoload Query Time
42,1.0.3
42,Added wp_options Autoload Size
42,1.0.2
42,Added Database Size
42,Added Refresh button
42,1.0.1
42,"Added Questions, removed Queries"
42,Query cache does not cache statements executed within stored programs.
42,Queries differs from Questions in that it also counts statements executed within stored programs.
42,Questions differs from Queries in that it doesn’t count statements executed within stored programs.
42,Added widget button RESET QUERY CACHE
42,Execute database command RESET QUERY CACHE.
42,1.0
42,First release
42,Meta
42,Version: 1.0.4
42,Last updated: 1 month ago
42,Active installations: 40+
42,WordPress Version:
42,3.6 or higher
42,Tested up to: 6.3.2
42,PHP Version:
42,5.6 or higher
42,Tags: databasemariadbperformancetuningwp_options
42,Advanced View
42,Ratings
42,This plugin has not been rated yet.
42,Log in to submit a review.
42,Contributors
42,Moris Papasmadov
42,Support Got something to say? Need help?
42,View support forum
42,Donate
42,Would you like to support the advancement of this plugin?
42,Donate to this plugin
42,About
42,News
42,Hosting
42,Privacy
42,Showcase
42,Themes
42,Plugins
42,Patterns
42,Learn
42,Documentation
42,Developers
42,WordPress.tv ↗
42,Get Involved
42,Donate ↗
42,Swag Store ↗
42,WordCamp ↗
42,WordPress.com ↗
42,Matt ↗
42,bbPress ↗
42,BuddyPress ↗
42,WordPress.org
42,WordPress.org
42,Visit our Facebook page
42,Visit our X (formerly Twitter) account
42,Visit our Instagram account
42,Visit our LinkedIn account
42,Visit our YouTube channel
43,MySQL / MariaDB Configuration Tips: Boost Zabbix Database!
43,Skip to content
43,Menu
43,"MenuConsultingBest ToolsMIB BrowserContactHomeCategoryZabbixSNMPGeneralDisclaimerPrivacy policyHome » Zabbix » MySQL / MariaDB Configuration Tips: Boost Zabbix Database!MySQL / MariaDB Configuration Tips: Boost Zabbix Database!2023-09-032023-09-02 by Aldin OsmanagicDatabases are the achilles’ heel of Zabbix! If not properly configured, they can lead to major problems! But, don’t worry; it’s your lucky day! In this tutorial, I will share with you the my MySQL / MariaDB configuration that I’ve fine-tuned over the years. You’re welcome!There is good guide on Zabbix blog about MySQL / MariaDB optimization. However, in this tutorial I will provide a ready-to-go configuration with minimal changes needed on your end.Table of ContentsZabbix MySQL / MariaDB Configuration ExampleExplaining Database Configuration Parametersa) Performance and Optimizationb) Connections and Threadsc) InnoDB Configurationd) Timeoutse) Caches and LimitsZabbix MySQL / MariaDB Configuration ExampleYou can find MySQL/MariaDB configuration (.cnf) in default directory path “/etc/my.cnf” or in directory “/etc/my.cnf.d/“.The MySQL configuration below is tweaked for a dedicated database server with 4 CPUs and 32GB RAM. Please make sure to adjust certain configuration parameters to match your specific server specifications![mysqld]"
43,# MySQL Server Configuration
43,# Directories and File Paths
43,datadir=/data/mysql
43,socket=/data/mysql/mysql.sock
43,log-error=/var/log/mysqld.log
43,pid-file=/var/run/mysqld/mysqld.pid
43,# Logging and Replication
43,skip-log-bin=true
43,# Performance and Optimization
43,skip_name_resolve
43,skip-performance-schema
43,optimizer_switch = 'index_condition_pushdown=off'
43,tmp-table-size = 96M
43,max-heap-table-size = 96M
43,key_buffer_size = 64M
43,# Connections and Threads
43,max_connections = 1250
43,thread_cache_size = 8
43,max_allowed_packet = 64M
43,# InnoDB Configuration
43,innodb-log-file-size = 128M
43,innodb-log-buffer-size = 128M
43,innodb-file-per-table = 1
43,innodb_buffer_pool_instances = 8
43,innodb_old_blocks_time = 1000
43,innodb_stats_on_metadata = off
43,innodb-flush-method = O_DIRECT
43,innodb-flush-log-at-trx-commit = 2
43,innodb_buffer_pool_size = 23G
43,innodb_read_io_threads = 32
43,innodb_write_io_threads = 32
43,innodb_io_capacity = 4000
43,innodb_io_capacity_max = 6000
43,# Timeouts
43,connect_timeout = 300
43,wait_timeout = 30000
43,# Caches and Limits
43,table_open_cache_instances = 16
43,table_open_cache
43,= 32012
43,open_files_limit = 65535
43,max_connect_errors = 1000000
43,[client]
43,# MySQL Client Configuration
43,# Client Socket
43,socket=/data/mysql/mysql.sock
43,"Later, we will go through every configuration parameter with our ChatGPT friend, but first, there are two things to remember!Configuration parameter max_connections must be larger than the total number of all Zabbix server processes plus 150. You can use the command below to automatically check the number of Zabbix processes and add 150 to that number:egrep ""^Start.+=[0-9]"" /etc/zabbix/zabbix_server.conf | awk -F ""="" '{s+=$2} END {print s+150}'"
43,"295The second most important parameter is innodb_buffer_pool_size, which determines how much memory can MySQL get for caching InnoDB tables and index data. This is the most important parameter in the configuration for a performance boost! You should set that parameter to 70% of system memory if you have dedicated server for the database.I didn’t have any problems with memory, but if your Zabbix server crashes because of lack of memory, reduce “innodb_buffer_pool_size” and restart MySQL server.Explaining Database Configuration ParametersI asked ChatGPT to describe every parameter, defining its purpose, and providing the min/default/max possible values, and I must say it looks very good. If you notice any errors, please let me know in the comments section. Thanks!a) Performance and Optimizationskip_name_resolve:Purpose: Disables DNS name resolution during client connections, improving performance when IP addresses are used.Min/Default/Max Value: Not applicable; it’s a boolean parameter (true/false).skip-performance-schema:Purpose: Disables the Performance Schema, which provides performance-related monitoring and statistics.Min/Default/Max Value: Not applicable; it’s a boolean parameter (true/false).optimizer_switch:Purpose: Enables or disables specific query optimization switches. In this case, it sets ‘index_condition_pushdown’ to ‘off’.Min/Default/Max Value: Not applicable; it depends on the optimization switches available.tmp-table-size:Purpose: Sets the maximum size of temporary tables in memory for each session.Min/Default/Max Value: 0/16M/No limit (memory available to the server).max-heap-table-size:Purpose: Sets the maximum size of MEMORY (HEAP) storage engine tables for each session.Min/Default/Max Value: 0/16M/No limit (memory available to the server).key_buffer_size:Purpose: Sets the size of the key buffer used for index blocks for MyISAM tables.Min/Default/Max Value: 8M/128M/No limit (memory available to the server).b) Connections and Threadsmax_connections:Purpose: Limits the maximum number of simultaneous client connections.Min/Default/Max Value: 1/151/100000thread_cache_size:Purpose: Sets the number of threads MySQL can keep waiting in a cache to reuse for client connections.Min/Default/Max Value: 0/9/100000max_allowed_packet:Purpose: Sets the maximum size of a packet or a BLOB/TEXT field sent to or received from the server.Min/Default/Max Value: 1024/16M/1073741824 (1GB)c) InnoDB Configurationinnodb-log-file-size:Purpose: Sets the size of each InnoDB log file.Min/Default/Max Value: 5M/48M/Unlimited (limited by the file system)innodb-log-buffer-size:Purpose: Sets the size of the buffer used for InnoDB redo log entries.Min/Default/Max Value: 1M/16M/Unlimited (limited by the memory available to the server)innodb-file-per-table:Purpose: Enables or disables the use of a separate tablespace file for each InnoDB table.Min/Default/Max Value: Not applicable; it’s a boolean parameter (true/false).innodb_buffer_pool_instances:Purpose: Sets the number of InnoDB buffer pool instances to divide the buffer pool into for multi-threaded performance.Min/Default/Max Value: 1/8/64innodb_old_blocks_time:Purpose: Specifies how long in milliseconds an InnoDB block remains in the “old” sublist before being moved to the “new” sublist.Min/Default/Max Value: 1000/1000/Unlimitedinnodb_stats_on_metadata:Purpose: Enables or disables automatic InnoDB statistics gathering during metadata statements.Min/Default/Max Value: Not applicable; it’s a boolean parameter (on/off).innodb-flush-method:Purpose: Specifies the method used to flush data to the InnoDB data files.Min/Default/Max Value: Not specified; depends on the available flush methods.innodb-flush-log-at-trx-commit:Purpose: Controls how InnoDB flushes the log buffer to the log file on each transaction commit.Min/Default/Max Value: 0/1/2innodb_buffer_pool_size:Purpose: Sets the size of the InnoDB buffer pool, which holds data and indexes for InnoDB tables.Min/Default/Max Value: 128M/128M/Unlimited (limited by the memory available to the server)innodb_read_io_threads:Purpose: Sets the number of background InnoDB I/O threads for read operations.Min/Default/Max Value: 1/4/64innodb_write_io_threads:Purpose: Sets the number of background InnoDB I/O threads for write operations.Min/Default/Max Value: 1/4/64innodb_io_capacity:Purpose: Sets the number of I/O operations per second the InnoDB background I/O thread can perform.Min/Default/Max Value: 100/200/2**64-1innodb_io_capacity_max:Purpose: Sets the maximum number of I/O operations per second the InnoDB background I/O thread can perform.Min/Default/Max Value: 2000/2000/2**64-1d) Timeoutsconnect_timeout:Purpose: Sets the number of seconds the server waits for a client to establish a connection before terminating it.Min/Default/Max Value: 2/10/31536000 (1 year)wait_timeout:Purpose: Sets the number of seconds the server waits for activity on an interactive connection before closing it.Min/Default/Max Value: 2/28800/31536000 (1 year)e) Caches and Limitstable_open_cache_instances:Purpose: Sets the number of table cache instances to divide the table cache into for multi-threaded performance.Min/Default/Max Value: 1/1/64table_open_cachePurpose: The number of open tables for all threads.Min/Default/Max Value: 1/4000/524288open_files_limit:Purpose: Limits the maximum number of files that MySQL can keep open.Min/Default/Max Value: 1024/65535/Unlimitedmax_connect_errors:Purpose: Limits the number of interrupted connection attempts a host can make to the server.Min/Default/Max Value: 1/100/18446744073709551615 (unlimited)Categories ZabbixInstall Zabbix-Proxy on Debian 11 in 10 minutes!Raspberry Pi SNMP monitoring: Install & enable Agent (server)4 thoughts on “MySQL / MariaDB Configuration Tips: Boost Zabbix Database!”Mark2023-08-22 at 12:35 PMAccording to docs, “index_condition_pushdown” optimizer switch “can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine”. Why did you turn it off?ReplyAldin Osmanagic2023-09-04 at 11:09 PMHi, Mark,This was a workaround for some Zabbix slow query problems. Please check this Zabbix trouble ticket at https://support.zabbix.com/browse/ZBX-10652 and read the last comment. Maybe this is no longer needed, but I’m not sure, so I left it.RegardsAldinReplyfikrat2023-08-09 at 11:52 AMhi sirI want to deploy zabbix server for 7000 host.can I do that.what is the configration I have to changeReplyAldin Osmanagic2023-08-14 at 2:24 PMIt all depends on how many items (metrics) you will have, the intervals for those items, and whether you have a fast disk (SSD)ReplyLeave a Comment Cancel replyCommentName"
43,Email
43,Website
43,"ΔZABBIX TUTORIALSInstallationInstall the latest Zabbix on CentOS, RHEL, Ubuntu, Debian, and Raspberry PiHow to Setup Zabbix SNMP traps?Network MonitoringHow to monitor Cisco Switch or Router?Datacenter MonitoringThe Ultimate VMware Monitoring GuideZabbix Website Monitoring TutorialHow to install Zabbix-Agent and Setup Windows and Linux Server Monitoring?VisualizationHow to create Interactive Zabbix Maps?AdministrationSetup Zabbix Email Alerts & EscalationsOptimizationInstall Zabbix-Proxy on CentOS, RHEL, Ubuntu, Debian, and Raspberry PiHow to optimize Zabbix database?Partition Zabbix database tables in 5 min!How to change Zabbix timeout value?MaintenanceHow to Upgrade Your old Zabbix to 6.x?Import SNMP MIB to Zabbix serverSNMP TUTORIALSWhat is SNMP? Learn Step-by-Step!Online MIB Browser (15 000+ MIB's)Configure SNMP v3 on Cisco devicesFree MIB Browser: SnmpB TutorialSNMP Walk/Get/Set/Trap ExamplesTop SNMP AlternativesHow to download any MIB (zip)Raspberry Pi SNMP monitoringIdentify devices with sysObjectID DBGENERAL POSTSZabbix vs PrometheusZabbix vs CactiZabbix vs PRTGZabbix vs CheckmkOpen-Source Evolution© 2023 Best Monitoring Tools • Built with GeneratePressx"
44,"databases - Page 1 of 4 - NexcessChat with usProductsProductsProductsProductsEnterpriseEnterpriseResourcesResourcesWhy NexcessWhy NexcessPartner ProgramsPartner ProgramsPricingPricingContact UsContact UsSign inSign inNexcess Knowledge BaseKnowledge Base Home|Category : databasesSearch June 01, 2023By NexcessHow to truncate MySQL tables from the CLILearn how to truncate MySQL tables from the CLI using this article from Nexcess. Truncation of MySQL tables is simple with these eight simple steps.Read More Posted in:File Management,Databases,Applications,WordPress,Websites,Performance,Magento 2,SSH,Website Management,Web designTags:mysql database,mysql query optimization,database,data,mysql database management ,database tables,database tuning,wordpress database,database table names & sizes,database field names & details ,mysql servers and databases,tuning your mysql database for performance ,woocommerce setup,Magento,web applications,website design,website performanceMay 25, 2023By Kiki SheldonChanging passwords for MariaDB users in the Nexcess CloudNexcess Cloud web hosting plans provide a great way to manage your website databases and database MariaDB users from the Nexcess Client Portal. Learn more in this informative article.Read More Posted in:Control Panel Tools,File Management,Client Portal,Databases,Applications,WordPress,Drupal,Magento 2,Magento,File Management,SecurityTags:database,password,Password Protection,Security,WordPress,drupal at nexcess,Magento,Magento 2,magento security,WordPress security,WooCommerce security,web,web applicationsApril 27, 2023By Zachary ArmstrongMySQL vs. MariaDB: Nexcess application stack with MariaDBLet's examine the history of MySQL and MariaDB and see how they compare — MySQL vs. MariaDB — and the performance advantages behind why the Nexcess application stack is built using MariaDB.Read More Posted in:Databases,File Management,WordPress,WooCommerce,Applications,Other Applications,Nexcess,Performance,Websites,Website Management,Web development,Ecommerce,Magento,Magento 2Tags:mysql database,database,mysql database management ,mysql servers and databases,nexcess web hosting technology stack,web applications,web,Open source,nexcess cloud,website,Web development,WordPress,woocommerce setup,Magento 2March 03, 2023By Kiki SheldonTruncating MariaDB/MySQL tables in the Nexcess CloudTruncating MariaDB/MySQL tables allows you to easily remove all data from the chosen MySQL database tables without having to manually delete and recreate them.Read More Posted in:Control Panel Tools,File Management,Client Portal,Databases,Applications,WordPress,File Management,WooCommerce,Magento,Magento 2Tags:repairing database tables,database tuning,data,mysql database management ,mysql servers and databases,mysql database,tuning your mysql database for performance ,database,database tables,mysql database,phpmyadmin,what are the best features of phpmyadmin?March 03, 2023By Kiki SheldonImporting and exporting a MySQL databaseExporting a MySQL database helps create a database backup for a website restoration process, cloning, or migration to a new server. You can export/import MySQL databases using different tools.Read More Posted in:Control Panel Tools,File Management,Client Portal,Databases,Applications,WordPress,WooCommerce,Magento 2,Magento,Other ApplicationsTags:data,mysql database,wordpress database,mysql database management ,database using wp-cli,mysql servers and databases,database tables,database table names & sizes,database field names & details ,wp-cli commands for your wordpress database,data migration,how do you export a mysql database using phpmyadmin?February 23, 2023By Mohammed NoufalTop 6 free MariaDB and MySQL GUI tools in 2023This post will introduce you to the top six free MariaDB and MySQL GUI tools available in 2023. Whether you're a novice or an expert, chances are you'll find the tool suitable for your needs.Read More Posted in:Databases,File Management,Nexcess,Website Management,Web development,Web design,Website Management,Other Applications,Applications,WordPress,WooCommerce,Magento 2,MagentoTags:mysql database,mysql servers and databases,wordpress database,database,mysql database management ,tuning your mysql database for performance ,database tuning,database table names & sizes,database field names & details ,modifying existing mysql database permissionsFebruary 21, 2023By Kiki SheldonConvert MyISAM to InnoDB tables for database performanceWith regard to database tables and storage engines, after learning why we would want to convert MyISAM to InnoDB for better performance, let’s review four methods for doing so.Read More Posted in:Databases,Websites,Web development,Website Management,Web design,WordPress,ApplicationsTags:wordpress database,data,mysql servers and databases,mysql database,database tables,database table names & sizes,web applications,woocommerce setup,store setup,mysql database,mysql database management February 21, 2023By Kiki SheldonMoving a WordPress site to a new serverYou know your website the best, so knowing how to perform a WordPress migration manually can save you time and money by moving a WordPress site to a new server yourself.Read More Posted in:WordPress,WooCommerce,Web development,Website Management,Websites,Backups,Domain Management,Databases,IP ManagementTags:migrate,wordpress migration,moving a wordpress site,WordPress,woocommerce setup,wordpress configuration,wordpress database,WordPress hosting,Web development,Web HostingJanuary 31, 2023By Kiki SheldonHow to access a WordPress database with connection errorsIn this article, we will learn how to access a WordPress database by troubleshooting and resolving the condition behind the ""Error Establishing a Database Connection"" message.Read More Posted in:WordPress,Databases,WooCommerce,Applications,Performance,Website Management,Web developmentTags:troubleshooting wordpress,database,wordpress database,mysql database,mysql database management ,database tuning,wordpress configuration,errors,php,woocommerce setup,configureJanuary 23, 2023By Christy JoyWordPress database setup — changing the database prefixWith regard to WordPress database setup and security, establishing a custom prefix for your database tables will harden your site against database-based hacker attacks.Read More Posted in:Databases,Website Management,WordPress,Applications,WooCommerce,Web development,WebsitesTags:database,mysql database,database tables,database tuning,mysql database management ,wordpress database,database table names & sizes,mysql servers and databases1234Next →Subscribe For Monthly TipsGrow your online business faster with news, tips, strategies, and inspiration.Featured ArticlesTransfer from Pantheon Hosting to Nexcess HostingRefer a friend and get $100Transfer from SiteGround Hosting to Nexcess HostingTransfer from Bluehost hosting to Nexcess hostingMigration guide: transfer my Wix website to Nexcess Transferring Webflow websites to Nexcess hostingTransfer from HostGator hosting to Nexcess hostingMigration Guide: Transfer a Shopify Store to NexcessTransfer from InMotion Hosting to Nexcess hostingTransfer from Hostinger hosting to Nexcess hostingTransfer from Flywheel hosting to Nexcess hostingTransfer from Kinsta hosting to Nexcess hosting Transferring a domain from GoDaddyCategories.htaccessAffiliatesApplicationsBackupsBilling BusinessCDNCDN SSLClient PortalContent Delivery Networks (CDNs)Control Panel ToolsCraft CMSCron JobsDatabasesDev SitesDomain ManagementDrupalEcommerceEmail Enterprise HostingExpressionEngineFTPFile ManagementGetting StartedHostingIP ManagementMagentoMagento 1Magento 2Membership sitesMiscellaneous NexcessNexcess Email ServicesNodeWorxOther ApplicationsOther Best PracticesPCI DSSPWAPerformanceReports and MonitoringSSHSSL ScriptsSecuritySiteWorxStoreBuilderThird Party ClientsWPQuickStartWeb designWeb developmentWebsite ManagementWebsitesWooCommerceWordPressProducts+MagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingProductsMagentoWordPressWooCommerceEnterprise HostingEnterprise Cloud InfrastructureCloud Hosting PlansHosting for AgenciesHosting for NonprofitsStoreBuilderExpressionEngineCraft CMSShopware HostingAdd Ons+AutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceAdd OnsAutoscalingDevelopment SitesCDNContainersSSL CertificatesDomain RegistrationWeb Application SecurityMalware Removal ServiceFeatures+Application StackPCI ComplianceDNSFree MigrationFeaturesApplication StackPCI ComplianceDNSFree MigrationPlugins+EventsFundraisingWordPress LMSPluginsEventsFundraisingWordPress LMSResources+Customer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQResourcesCustomer StoriesKnowledge BaseBlogCompare NexcessSystems StatusWebinarsWeb ToolsEbooksSite SearchSupportContact UsFAQCompany+AboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersStudent DiscountWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutCompanyAboutReviewsData CentersColocation ServicesNewsroomCareersFamily of BrandsPartnersStudent DiscountWhy WP Users Trust UsWhy Choose Nexcess?$1,000 Contract BuyoutLEGAL  |  © 2023 Nexcess.Net, LLC All Rights Reserved.We use cookies to understand how you interact with our site, to personalize and streamline your experience, and to tailor advertising. By continuing to use our site, you accept our use of cookies and accept our Privacy Policy.Accept"
45,MariaDB Services - Sourcetoad
45,ServicesDevelopmentSoftware DevelopmentMobile App DevelopmentAI DevelopmentSecurity and AuditsSecurity and Pen TestsAWS AuditsCode AuditsSupport and MaintenanceAWS MaintenanceApplication MaintenanceApplication Help DeskFractional CTO ServicesWorkCompanyAbout UsOur TeamCareersIn the NewsOpen SourceBlog
45,Services
45,Development
45,Software Development
45,Mobile App Development
45,AI Development
45,Security and Audits
45,Security and Pen Tests
45,AWS Audits
45,Code Audits
45,Support and Maintenance
45,AWS Maintenance
45,Application Maintenance
45,Application Help Desk
45,Fractional CTO Services
45,Work
45,Company
45,About Us
45,Our Team
45,Careers
45,In the News
45,Open Source
45,Blog
45,Contact Us
45,TECHNOLOGY
45,MariaDB Services
45,Unlock the power of MariaDB with Sourcetoad.
45,Let’s Talk
45,Powering Your Success with MariaDB
45,"At Sourcetoad, we harness the power of MariaDB to deliver robust, scalable, and efficient database solutions tailored to our clients’ unique needs. As seasoned experts in MariaDB development, we empower businesses to achieve data-driven excellence, reliability, and performance in their applications. Dive into the advantages of collaborating with us and discover how MariaDB can transform your data management strategy."
45,Why choose Sourcetoad for MariaDB Services?
45,"MariaDB, an open-source relational database management system (RDBMS), stands as a versatile choice with numerous benefits:"
45,Data Reliability
45,"MariaDB is renowned for its commitment to data integrity and security. It adheres to ACID (Atomicity, Consistency, Isolation, Durability) principles, ensuring the consistency and security of your data even during peak loads."
45,Scalability
45,"MariaDB is inherently scalable, making it suitable for businesses of all sizes. Whether you’re a startup or an established enterprise, MariaDB adapts seamlessly to your evolving data and traffic demands."
45,Exceptional Performance
45,"Designed for speed, MariaDB employs advanced query optimization techniques, multiple indexing options, and a range of storage engines. This translates to outstanding performance, enabling your applications to run smoothly with large datasets."
45,Open-Source Freedom
45,"As open-source software, MariaDB enjoys continuous contributions from a dedicated community. This ensures access to an array of features, updates, and support without the cost constraints typically associated with proprietary databases."
45,Compatibility
45,"MariaDB maintains full compatibility with MySQL, making it an effortless transition for businesses considering migration from MySQL or integration with their existing technology stack."
45,Leveraging MariaDB’s Capabilities for Client Success
45,"At Sourcetoad, our team of MariaDB experts is committed to delivering exceptional database solutions tailored to your specific requirements. Here’s how we leverage MariaDB to empower our clients:"
45,Custom Database Development
45,"We specialize in crafting personalized database solutions using MariaDB. Collaborating closely with you, we gain a deep understanding of your data management needs and design a database schema that perfectly aligns with your business objectives."
45,Database Optimization
45,"Our expertise ensures that your MariaDB database operates at peak efficiency. Through meticulous query optimization, strategic indexing, and fine-tuning, we unlock maximum speed and performance, resulting in enhanced application responsiveness."
45,Migration and Integration
45,"If you’re contemplating a migration to MariaDB from another database system or require seamless integration into your existing tech stack, our team excels in making the transition smooth. We manage data migration, handle schema conversions, and seamlessly integrate MariaDB to minimize disruption and downtime."
45,Data Security
45,"Data security is paramount to us. Our team implements robust security measures, including encryption, access control, and auditing, to safeguard your data. Rest assured that your sensitive information remains secure and complies with industry standards."
45,Explore how Sourcetoad can revolutionize your data management strategy through MariaDB.
45,Partner with Sourcetoad to harness the full potential of MariaDB for your data management requirements.
45,"Our seasoned expertise in MariaDB development, combined with our unwavering commitment to delivering exceptional results, makes us the ideal choice for optimizing your data into a strategic asset."
45,"Embrace data-driven excellence, reliability, and performance in your applications with MariaDB."
45,Get in Touch
45,Privacy Policy
45,Back to Top
45,IndustriesHealthcare and Pharmacy
45,Financial Services
45,"Architecture, Engineering & Construction"
45,Telecom
45,Education & Training
45,Retail
45,Artificial Intelligence and Machine Learning
45,Cruise and Ferry
45,More SourcetoadCareers
45,Decoder Podcast
45,"LilypadsTampa, Florida"
45,"Perth, Australia"
45,"Bacalod, Philippines"
45,More SourcetoadCareers
45,Decoder Podcast
45,"LilypadsTampa, Florida"
45,"Perth, Australia"
45,"Bacalod, Philippines"
45,© 2023 by Sourcetoad.
47,AWS IQ
48,"Database Management Systems (DBMS) Comparison: MySQL, PostgrAltexsoftCompanyTravel Technology PracticeDigital TransformationEngineering ServicesUX/UI ConsultingData Science СonsultingTechnology ConsultingBusiness VerticalsCase StudiesContactsNews & EventsBlogTechTalksGlossaryCareersAltexSoft LabPolicies© Copyright AltexSoft 2023Email: solutions@altexsoft.comPhone: Request for ProposalComparing Database Management Systems: MySQL, PostgreSQL, MSSQL Server, MongoDB, Elasticsearch, and others26 min readBusiness,  EngineeringLast updated: 16 May, 20232 CommentsShareIn the world of software development, choosing the right database is a crucial decision that can significantly impact your application's performance, scalability, and ease of use. With many options available, it can be challenging to determine the best database management system (DBMS) that will perfectly suit your needs."
48,"In this article, we’ll compare the 12 most commonly used DBMSs: MySQL, MariaDB, Oracle, PostgreSQL, MSSQL, SQLite, MongoDB, Redis, Cassandra, Elasticsearch, Firebase, and DynamoDB. We will focus on their business-related benefits and challenges while highlighting the ideal use cases for each. With this database comparison at hand, you will be able to make an informed decision for your project."
48,What is a database management system?
48,"A Database Management System (DBMS) is a specialized software designed to store, retrieve, and manipulate data. It acts as a mediator between the database, applications, and user interfaces to manage and organize data effectively. The system provides a comprehensive suite of tools to govern databases, ensuring data security, consistency, and integrity."
48,"A DBMS supports various applications, from simple storage and retrieval tasks to complex data-driven systems, by implementing efficient data access and management practices. Additionally, the system can handle concurrent users, maintain transactional consistency, and provide robust backup and recovery options, making it an essential component in any data-centric environment."
48,"Since databases are just a part of the whole data management strategy, learn about this comprehensive approach in our dedicated article."
48,Types of databases: Relational vs non-relational
48,"There are two types of DBMSs: relational and non-relational, also referred to as SQL and NoSQL respectively. Before discussing the most popular database options, let’s take a closer look at how relational and non-relational database systems differ, considering commonly used data structures, performance, scalability, and security."
48,Relational vs non-relational databases in a nutshell.
48,Relational or SQL databases
48,"A relational database management system (RDBMS) is an information repository that organizes data into tables consisting of rows (records) and columns (attributes that contain the properties of these records). Each table represents a relation, and the rows (also called tuples) hold individual records within that relation. RDBMSs have a predefined schema with a strict structure and clear dependencies between different data points."
48,"So tables in relational databases are connected to other tables through primary key or foreign key relationships. A primary key is a unique identifier for each record in a table, ensuring that no two records have the same value for that specific column or set of columns. On the other hand, a foreign key is a column or a set of columns in one table that refers to the primary key in another table, establishing a link between them."
48,"Despite these connections between tables, the term relational in relational database systems comes from the mathematical concept of relations. Dr. Edgar F. Codd proposed this idea as a new way to organize and manage data using principles from mathematics in his seminal paper ""A Relational Model of Data for Large Shared Data Banks,"" published in 1970."
48,The second name of such systems is SQL databases. This is because Structured Query Language (SQL) is used to communicate with and manage these databases.
48,"Scalability. Relational databases usually scale vertically, meaning data lives on a single server, and scaling is done by adding more computer (CPU, GPU, and RAM) power to that one server. However, switching from smaller to bigger machines often involves downtime. Scaling an SQL database between multiple servers (horizontal scaling) can be challenging as it requires data structure changes and additional engineering efforts."
48,"Performance. Relational databases perform well with intensive read/write operations on small to medium datasets. They also offer improved speed of data retrieval by adding indexes to data fields to query and join tables. However, the performance may suffer when the amount of data and user requests grows."
48,"Security. Due to the integrated structure and data storage system, SQL databases don’t require much engineering effort to render them well-protected. They are a good choice for building and supporting complex software solutions, where any interaction has a range of consequences. One of the SQL fundamentals is ACID compliance (Atomicity, Consistency, Isolation, Durability). ACID-compliance is a preferred option if you build, for instance, eCommerce or financial applications, where database integrity is critical."
48,Non-relational or NoSQL databases
48,"A non-relational or non-tabular database uses different data models for storing, managing, and accessing data. The most common data models are"
48,"document-oriented — to store, retrieve, and manage data such as JSON documents;"
48,"key-value — to represent data as a collection of key-value pairs, where keys are unique strings having corresponding data values;"
48,graph — to store data in the node-edge-node structure where nodes are data points and edges are their relationships; and
48,"wide-column — to store data in the tabular format with flexible columns, meaning they can vary from row to row in the same table."
48,"As these databases aren’t limited to a table structure, they are called NoSQL. They allow for storing unstructured data such as texts, photos, videos, PDF files, and a bunch of other formats. Data is simple to query but isn’t always classified into rows and columns as in a relational database."
48,"Scalability. When the number of data and requests increases, non-relational or NoSQL databases are usually scaled horizontally by adding more servers to the pool. They share data between various servers where each contains only a part of the data, decreasing the request-per-second rate in each server."
48,"Performance. Non-relational databases are known for their high performance: They have a distributed design, which lowers the performance load on the system and provides a large number of users with simultaneous access. Such databases can store unlimited data sets that come in all types and shapes. They are also quite flexible when it comes to changing data types."
48,"Security. Unlike relational systems, NoSQL databases have weak security, making them a major concern for many infrastructures. While they may provide ACID guarantees, they are typically available within the scope of one database partition. However, some DBMSs offer advanced security features that meet strict security and compliance standards."
48,"Since NoSQL databases allow for reserving various data types together and scaling across multiple servers, their never-decreasing popularity is understandable Also, NoSQL databases can be highly advantageous when it comes to building an MVP. They don’t require pre-deployment preparations, making quick, time-lag-free updates to the data structure easier."
48,"So what are the most commonly used database systems in SQL and NoSQL? What are their main advantages and disadvantages, and how should businesses use them? Let’s take a deeper look."
48,"Below, we’ll discuss the following list of SQL databases:"
48,MySQL
48,MariaDB
48,Oracle
48,PostgreSQL
48,MSSQL
48,SQLite
48,and will complement it with such NoSQL databases as:
48,MongoDB
48,Redis
48,Cassandra
48,Elasticsearch
48,Firebase
48,Amazon DynamoDB
48,The screenshot below reflects the popularity of these and a few other databases.
48,Most popular database systems. Source: 2022 Developer Survey by StackOverflow
48,"While more detailed descriptions of the aforementioned databases await you further in the post, the table here provides a quick-look comparison against key criteria."
48,Database management systems comparison.
48,"Now that you have a general understanding of the differences between relational and non-relational databases, we’re moving on to describe the main modern database management systems along with the pros, cons, and use cases of each."
48,MySQL
48,"MySQL is one of the most popular relational database systems. Originally an open-source solution, MySQL is now owned by Oracle Corporation. Today, MySQL is a pillar of LAMP application software. That means it’s a part of Linux, Apache, MySQL, and Perl/PHP/Python stack. Having C and C++ under the hood, MySQL works well with such system platforms as Windows, Linux, MacOS, IRIX, and others."
48,Pros of MySQL
48,"Free installation. The community edition of MySQL is free to download. With a basic set of tools for individual use, MySQL community edition is a good option, to begin with. Of course, there are other, prepaid versions for Enterprise or Cluster purposes with richer functionality. Nevertheless, if your company is too small to pay for one of them, the free-to-download model is the most suitable for a fresh start."
48,"Simple syntax and mild complexity. MySQL’s structure and style are very plain. Developers even consider MySQL a database with a human-like language. MySQL is often used in tandem with the PHP programming language. Because they share a gentle learning curve, it’s much easier to form a team to manage your database. Also, MySQL is easy to use. For instance, most of the tasks can be executed right in the command line, reducing development steps."
48,"Cloud compatibility. Business-oriented by nature and originally developed for the web, MySQL is supported by the most popular cloud providers. It’s available on leading platforms like Amazon, Microsoft, and others. This makes MySQL even more attractive and gives businesses room for growth."
48,Cons of MySQL
48,"Scalability challenges. MySQL was not built with scalability in mind, which is inherent in its code. Theoretically, you can scale MySQL, but it will need more engineering effort than any of the NoSQL databases. So, if you expect one day your database will increase substantially, keep this limitation in mind or choose another DBMS option."
48,"Partial open-source support. Although MySQL has an open-source part, it’s mostly under Oracle’s license. This limits the MySQL community in terms of improving the DBMS. Why do you care? Because when you have completely open-source support, you expect many problem-specific implementations and community assistance. This is not the case when the software belongs to corporate owners, and you have to pay for support."
48,"Limited compliance with SQL standards. Structured Query Language has specific standards. MySQL doesn’t completely follow them, i.e., MySQL provides no support for some standard SQL features. On the other hand, MySQL has some extensions and distinct features that don’t match the Structured Query Language standards. It’s not a big deal for small web applications. The issues may appear when you have to shift to other databases, which will likely happen when your business starts growing."
48,Use cases
48,"Small web-based solutions. MySQL database system is the best option when you’re designing a small, web-based solution with a small volume of data. For example, when building a local eCommerce store, MySQL may come in handy."
48,"OLAP/OLTP systems. This is one of the best use cases for a MySQL database, as OLAP/OLTP don’t require complex queries and large volumes of data. Also, consider applying MySQL for the same reason if you’re building a business intelligence tool."
48,"IoT applications. MySQL can be used for small to medium-sized Internet of Things (IoT) applications to store and manage sensor data, device information, and user interactions."
48,"Keep in mind that while MySQL supports these use cases, its performance and suitability might vary depending on the specific requirements and size of the project."
48,MariaDB
48,"MariaDB, an open-source fork from MySQL, is a great SQL database example with commercial support. It works under a GNU General Public License and has similar commands, APIs, and libraries to MySQL."
48,Pros of MariaDB
48,"Encryption. For MariaDB, open source doesn’t mean insecure. In addition to internal security and password check, MariaDB provides such features as PAM and LDAP authentication, Kerberos, and user roles. Combined with encrypted tablespaces, tables, and logs, it creates a robust protective layer for data. Beyond that, MariaDB publishes related releases on each security update, keeping the security patches totally transparent."
48,"Broad functionality. MariaDB has introduced a lot of new features in the last few years. For instance, GIS support suggests smooth coordinate storage and location data queries. Dynamic columns allow a single DBMS to provide both SQL and NoSQL data handling for different needs. You can also extend its functionality with plugins that are available at MySQL via 3rd parties only. MariaDB is shipped with storage engines for NoSQL backend, legacy database migration tools, sharding options, and much more."
48,"High performance. Although MariaDB originates from the MySQL engine, it's gotten very far in terms of performance. Extensive optimization features improve thread pool management and data processing. Thus, when rows from the table are deleted, the operating system immediately accesses the free space, eliminating gaps in the tablespace. On top of that, the database management system suggests engine-independent table statistics. This feature enhances the optimizer’s performance, accelerates query processing, and helps customize data analysis."
48,Cons of MariaDB
48,"Still a growing community. Although MariaDB has substantial open-source contribution, its community has yet to grow much. Since this database management system was established not so long ago, the number of professionals involved is relatively small."
48,"Gaps between MySQL and MariaDB update versions. Though the MariaDB team is constantly merging its code with MySQL's, it’s already not that simple to keep them in line. Given the currently existing differences between MariaDB 10.6 and MySQL 8.0.32, further deviations are yet to come. Additionally, MySQL engineers introduced some native features to the code that are only available to commercial MySQL users. This can create compatibility issues or data migration problems from MariaDB back to MySQL."
48,Use cases
48,"Since MariaDB is close to MySQL, it can be used to work with the same types of web-based applications. Additionally, you get extended location data storage, higher performance, and improved scalability."
48,Oracle
48,"Oracle is a relational database management system created and run by the Oracle Corporation. Among all the types of SQL databases, Oracle stands out. Currently, it supports multiple data models like document, graph, relational, and key-value within a single database. In its latest releases, it refocused on cloud computing. Oracle database engine licensing is fully proprietary, with both free and paid options available."
48,Pros of Oracle
48,"Innovations for daily workflow. Starting with the Oracle 12c release, when the software entered the hybrid cloud era, new cloud computing technologies appeared regularly. With every new release, Oracle tries to keep up with the innovation pace while focusing on information security, including active data guard, partitioning, improved backup, and recovery."
48,"Strong tech support and documentation. Oracle ensures decent customer support and provides comprehensive tech documentation across multiple resources. So, you’ll likely find solutions to any issues that appear. You may also expect some community support."
48,"Large capacity. Oracle’s multi-model solution allows for accommodating and processing a vast amount of data. Thanks to the recently released multi-tenancy feature, the database architecture now simplifies packing many databases and manages them smoothly. In combination with in-memory data processing capabilities, it creates a strong engine for synchronous data processing."
48,Cons of Oracle
48,"High cost. Though the Oracle database has free editions, they are very limited in terms of functionality. Standard Edition, which doesn’t include all available features, costs $17,500 per unit. The Enterprise Edition is over $47,000 per unit."
48,"Resource-consuming technology. The Oracle database needs powerful infrastructure. Not only does installation require a lot of disk space, but you’ll also have to consider constant hardware updates if you deploy it on-premises."
48,"Hard learning curve. Oracle database is not a system to start using right away. It’s better to have certified Oracle DB engineers to run it. Oracle’s documentation, while covering many issues, can sometimes be overwhelming and even confusing. So, to install and run an Oracle database, you’ll have to consider hiring dedicated experts."
48,Use cases
48,"Large-scale enterprise applications. Given all those perks and pitfalls, you can consider Oracle RDBMS as a reasonable solution for online OLTP, data warehousing, and even mixed (OLTP and DW) database applications. If you have a billion records to hold and manage – and a sufficient budget to support it – Oracle hybrid cloud software is a good option."
48,"Financial institutions. Oracle is widely used in the financial sector, where data integrity and security are paramount. Banks, insurance companies, and investment firms often rely on Oracle to manage sensitive financial data and transactions."
48,"Government and public sector. Oracle is often chosen for its robust features and security in government and public sector applications, including national security, healthcare, and transportation systems."
48,PostgreSQL
48,"The PostgreSQL database management system shares its popularity with MySQL. This is an object-relational DBMS where user-defined objects and table approaches are combined to build more complex data structures. Besides that, PostgreSQL has a lot of similarities with MySQL. It’s aimed at strengthening the standards of compliance and extensibility. Consequently, it can process any workload, for both single-machine products and complex applications. Owned and developed by PostgreSQL Global Development Group, it still remains completely open-source. This DBMS is available for use with platforms like Microsoft, iOS, Android, and many more."
48,Pros of Postgre
48,"Great scalability. Vertical scalability is a hallmark of PostgreSQL. Considering that almost any custom software solution tends to grow, resulting in database extension, this particular option certainly supports business growth and development."
48,"Support for custom data types. PostgreSQL natively supports many data types by default, such as JSON, XML, H-Store, and others. PostgreSQL takes advantage of it, being one of the few relational databases with strong support for NoSQL features. Additionally, it allows users to define their own data types. As your software business model may need different types of databases throughout its existence for better performance or application comprehensiveness, this option brings improved flexibility to the table."
48,"Easily-integrated third-party tools. The PostgreSQL database management system has the strong support of additional tools, both free and commercial. The scope of these includes extensions to improve many aspects. For example, ClusterControl provides impressive assistance in managing, monitoring, and scaling SQL and NoSQL open-source databases. To make data comparison and synchronization more effective, consider using DB Data Directive. In case you’re going to scale up your data to heavy workloads, the pgBackRest backup and restore system will be a nice option to choose from."
48,"Open-source and community-driven support. Postgres is completely open-source and supported by its community, strengthening it as a complete ecosystem. Additionally, developers can always expect free and prompt community assistance."
48,Cons of Postgre
48,"Inconsistent documentation. While PostgreSQL has a large community and strongly supports its participants, the documentation still lacks consistency and completeness. As the PostgreSQL community is rather distributed, the documentation doesn’t follow uniform standards for all Postgre features."
48,Lack of reporting and auditing instruments. A significant shortcoming of PostgreSQL is the absence of revising tools that would show the current condition of a database. You have to continuously check if something goes wrong. There’s always a risk that DB engineers will notice a failure too late.
48,Use cases
48,"Due to complicated queries and a wide choice of custom interfaces accomplished with predefined functions, PostgreSQL is a perfect match for data analysis and warehousing. If you are building a database automation tool, PostgreSQL is the best fit for it due to its strong analytical capabilities, ACID compliance, and powerful SQL engine. All in all, it significantly accelerates the processing of vast amounts of data. This DBMS is popular with financial institutions and telecommunication systems."
48,MSSQL
48,"As a completely commercial tool, Microsoft SQL Server is one of the most popular relational DBMSs, in addition to MySQL, PostgreSQL, and Oracle. It copes well with effective storing, changing, and managing relational data. To interact with SQL Server databases, DB engineers usually utilize the Transact-SQL (T-SQL) language, which is an extension of the SQL standard."
48,Pros of MSSQL
48,"Variety of versions. Microsoft SQL Server provides a wide choice of different options with diverse functionalities. For instance, the Express edition with a free database offers entry-level tooling, the perfect match for learning and building desktop or small server data-driven applications. The Developers option allows for building and testing applications, including some enterprise functionalities, but without a production server license. For bigger projects, there are also Web, Standard, and Enterprise editions, with a varying extent of administrative capabilities and service levels."
48,"End-to-end business data solution. With a focus on mostly commercial solutions, MSSQL provides a lot of business value-added features. The optional selection of components allows building ETL solutions, forming a knowledge base, and implementing data clearance. Also, it provides tools for overall data administration, online analytical processing, and data mining, additionally offering solutions for report and visualization generation."
48,"Rich documentation and community assistance. With Microsoft SQL Server aimed at comprehensive database maintenance, the full online documentation also reflects this concept. The correspondingly structured guidelines, numerous whitepapers, and demos give a full picture of the MSSQL data system. Also, Microsoft Premier provides access to dedicated Microsoft community support, which is an advantage when a DB engineer needs assistance."
48,"Cloud database support. A part of the consistent Microsoft ecosystem, MSSQL can be integrated with Microsoft Cloud, Azure SQL Database, or SQL Server on Azure Virtual Machines. The solutions allow shifting database administration to the cloud if your business software database becomes really overwhelming and hard to administer."
48,Cons of MSSQL
48,"High cost. Being mostly used at the enterprise scale, MSSQL Server remains one of the most expensive solutions. Speaking of numbers, the Enterprise edition currently costs over $15, 123 per core, sold as 2 core packs."
48,"Unclear and floating license conditions. Another issue is the ever-changing licensing process. The pricing strategy itself is hard to understand, and the elements included in a particular edition are floating, tending to shift from one to another."
48,"Complicated tuning process. For those beginners who have to operate heavy data sets, working with query optimization and performance tuning may be problematic. As the process is not so obvious, it can create substantial bottlenecks early on."
48,Use cases
48,"MSSQL Server is a reasonable option for companies with other Microsoft product subscriptions. As Microsoft builds a robust ecosystem with seamless integration of services, MSSQL emerges as a powerful database solution. With its cloud accessibility and advanced data retrieval tools, MSSQL proves to be a valuable asset for businesses, ensuring a sustainable and efficient system that aligns with evolving needs."
48,SQLite
48,"SQLite is a self-sufficient, serverless, and no-configuration-required database management system. Frequently utilized as an embedded database, it is popular for small-scale mobile and desktop applications."
48,Pros of SQLite
48,"Small in size and easily portable. SQLite is a streamlined database engine that operates without a separate server process. The entire database is contained within a single cross-platform disk file, enhancing its portability and simplifying its integration into applications."
48,"Minimal resource consumption. SQLite is engineered for optimal memory and disk space efficiency, making it an ideal choice for applications with constrained resources, such as those found in mobile and IoT devices."
48,"Reliable and user-friendly. SQLite is an ACID-compliant database, ensuring the integrity and consistency of data. Additionally, it is simple to set up and demands minimal configuration."
48,Cons of SQLite
48,"Restricted concurrency. SQLite employs file-based locking, limiting its capacity to manage multiple concurrent write operations. This makes it less appropriate for applications with high write concurrency or multiple users accessing the database simultaneously."
48,"Absence of advanced features. SQLite lacks some of the sophisticated features found in other database management systems, such as stored procedures, triggers, or user-defined functions."
48,"Restricted scalability. Owing to its serverless structure, SQLite is not tailored for extensive applications or distributed settings. Its performance may diminish when handling substantial datasets or elevated levels of concurrent access."
48,Use cases
48,"SQLite is well-suited for modest-sized applications and mobile and desktop applications that demand a lightweight, easily portable, and user-friendly database. It is also fitting for embedded systems and IoT devices with limited resources where implementing a server-based DBMS would be unfeasible."
48,MongoDB
48,"A free, open-source, non-relational DBMS, MongoDB also includes a commercial version. Although MongoDB wasn’t initially intended for structured data processing, it can be employed for applications that use both structured and unstructured data. In MongoDB, databases are connected to applications via database drivers. They are widely available within the database management system. Multiple data types are processed simultaneously and use the internal cache for this purpose."
48,Pros of MongoDB
48,"Simple data access, storage, input, and retrieval. One of the benefits of MongoDB derived from its NoSQL nature is the fast and easy data operation. That is to say, data can be entered, stored, and withdrawn from the database quickly and without any additional confirmation. As with any other non-relational database, it places emphasis on RAM usage, so the records can be manipulated really fast and without any consequences to data integrity."
48,"Easy compatibility with other data models. MongoDB is easily combined with different database management systems, both SQL and NoSQL types. Besides that, it has pluggable storage engine APIs. To make a long story short, this option allows third parties to build their own data storage engines for MongoDB. From a commercial point of view, it creates extra value for business software."
48,"Horizontally scalable solution. Scalability – where data is spread out across a distributed network of manageable servers – is a facet of MongoDB’s fundamental nature. It becomes even more important for enterprises operating big data applications. Additionally, the database can allocate data across a cluster of machines. How can that help you? The data is distributed faster and equally, free of bulkiness. As it leads to faster data processing, the application performance is accelerated too."
48,Cons of MongoDB
48,"Extensive memory consumption. The denormalization process, when previously normalized data in a database is grouped to increase performance, usually results in high memory consumption. Also, this DBMS keeps in memory all key names for each value pair. Beyond that, because there is no support for joins, Mongo databases have data oversupply, resulting in big memory waste and lower application performance."
48,"Data insecurity. With a focus on fast data operation, MongoDB, like any other NoSQL DBMS, lacks data security. As user authentication isn’t a default Mongo option, and higher protection is available with a commercial edition only, you can’t consider it totally secure. Additionally, there are constant MongoDB update releases, with no guarantee that all amendments or data changes will work as they did before. Keep in mind that all manipulations should be formed around these updates, being covered with additional tests."
48,"Complicated process to interpret into other query languages. As MongoDB wasn’t initially developed to deal with relational data models, the performance may slow down in these cases. Besides, the translation of SQL to MongoDB queries takes additional action to use the engine, which may delay the development and deployment."
48,Use cases
48,"MongoDB works best in real-time data integration and database scalability. For instance, it’s the right option for product catalogs due to its capacity to stock a multiplicity of objects with various attribute collections. Also, consider here analytic platforms, as MongoDB’s speed provides dynamic performance that can help track the user’s behavior in real time."
48,Redis
48,"An open-source, NoSQL, in-memory data structure store, Redis can also be used as a cache. Instead of documents, it uses key-value pairs. Its distinct feature is that there are several options for data structuring, such as lists, sets, and hashes."
48,"Allowing for data replication and supporting transactions, Redis executes commands in a queue instead of setting it one at a time."
48,Pros of Redis
48,"Rapid solution. Due to its replication and transaction features, Redis processes the data really fast. The absence of dependencies and in-memory data store type makes Redis a worthy competitor even among simple SQL alternatives."
48,"Massive data processing. From the data perception and refining perspective, Redis can be considered a colossus. It can easily upload up to 1GB of data for one entry. Add built-in data caching and you get a powerhouse data machine."
48,Cons of Redis
48,"Dependency on the application memory. Total reliance and dependency on the application memory is a real drawback. That is to say, your database will crash if its size exceeds the size of available memory."
48,"No support for query language or joins. Regarding compatibility with other dataset types, Redis lags behind. Given that at some time your business may need scaling and using other data formats, having rapid entries as a single option leaves this issue open."
48,Use cases
48,"Redis basically has a few different directions to work with. And the first of them is IoT applications. Here, heavy data from IoT devices can be transferred to Redis to process these records before keeping them in any steady data storage. Also, Redis is a perfect option for microservice architectures with scalable cloud hosting. As data here doesn’t have to be long-term persistent, Redis seems a reasonable decision."
48,Cassandra
48,"Cassandra is a decentralized system developed by Apache. It’s a free Java-based DBMS with multi-replication and multi-deployment features as its strengths. These peculiarities allow for numerous query copying and deploying all of them at the same time. Being rapidly scalable, Cassandra allows for managing large data volumes by replicating them into multiple nodes. It eliminates the problem of database crash – if some of the nodes fail at any time, it’s replaced immediately, and the system keeps working as long as at least one single node is safe."
48,"Cassandra uses its own query language, CQL. In its syntax, it’s very similar to SQL but doesn’t apply joins, replacing them with so-called column families. And the second difference is that not all columns in a table are stored for subqueries. Some of them are used as clustering columns, where adjacent data is put next to each other for fast retrieval. Why does that matter? It provides faster querying from massive datasets, accelerating data processing."
48,Pros of Cassandra
48,"Data security. Due to its master node replication feature, Cassandra stays failure tolerant. It means that DB engineers can feel confident about data safety unless master nodes fail all at the same time. As long as it’s extremely unlikely, the database and the application built on it will stay sound and secure."
48,"Flexibility and on-hand amendments. Cassandra’s simple syntax has the best of SQL and NoSQL. In addition to scalability, it largely contributes to dataset flexibility. Cassandra collects data on the go, and data retrieval shares the same simplicity, despite dataset size. This allows for enlarging the database to the fullest extent."
48,Cons of Cassandra
48,"Slow reading. As Cassandra was initially designed for fast writing, its weakness lies in its incapacity for fast reading. One of the reasons for it is that the system doesn't have bottlenecks for incoming information. So while data can be written to the database quickly, the system may take longer to process and retrieve that data. This can be further explained by the fact that Cassandra spreads the data across multiple nodes in a cluster. When you query the data, it may have to read from various nodes, which can slow down the read performance."
48,"Need for additional resources. As Cassandra processes multiple layers of data simultaneously, it demands enough power to do it. This means additional investment in both software and hardware. If this is the first time a company faces such a necessity and is not sure about the resources, then maybe it should consider other database systems."
48,Use cases
48,"Thanks to even data distribution, Cassandra is relevant in applications where large volumes of information are processed. For instance, it’s a great choice for data centers. Also, Cassandra fits well with real-time analytics, as it allows linear scaling and data increase in real time. You may also consider it for applications with constant data streaming, like weather apps. Another option is using it as a DBMS for an eCommerce store, as it allows for storing purchase history and other transactions. Add here the feasibility of tracking such data types as order status and packages, and you’ll get the full solution with eCommerce delivery integration."
48,Elasticsearch
48,"Elasticsearch is a NoSQL, document-oriented database management system having a full-text search engine at its heart. Built on the Apache Lucene library, it stores data as a JSON file, supports RESTful APIs, and uses a powerful analytical engine for faster data retrieval. Being open-source software, it includes both free and paid editions."
48,Pros of Elasticsearch
48,"Scalable architecture. One of Elasticsearch’s peculiarities is its robust distributed architecture. Its key structure options, such as clustering, indexing, sharding, and many more, provide extensive horizontal scaling, which allows for accommodating terabytes of records with further automation. The architecture’s abstraction levels streamline system management on both individual and aggregate levels."
48,"Fast data processing. Due to the distributed data structure and built-in parallelization, the Elasticsearch DB shows excellent performance results. Even when executing a complex data query, it generates lightning search result responses. This is partly available due to documents being maintained close to relevant metadata in the index, which makes them fast to find."
48,Cons of Elasticsearch
48,"Lack of multi-language support. When handling request or response data, Elasticsearch DBMS lags behind. Though it’s perfectly combined with Cassandra DB to complement database performance, other languages and formats are not available for it. In these terms, it only supports JSON document format."
48,"Limited consistent health check tools. When something goes wrong, as it may at any stage, Elasticsearch can only show the status as “yellow” or “red.” Simply put, it has no reporting tools. Though issues are usually like memory threshold or disk capacity, DBA engineers complain about the situation."
48,Use cases
48,"Due to its NoSQL distributed nature and flexible data models, Elasticsearch is a great tool for eCommerce products with huge databases that tend to use search engines. It’s very helpful when creating or updating a customer’s profile regarding the workload that real-time engagement usually demands."
48,Firebase databases
48,"Owned by Google, Firebase is a real-time Backend-as-a-Service used to develop web and mobile software. As far as NoSQL databases, there are two options: Firebase Realtime Database (providing real-time access to data residing in different platforms) and Cloud Firestore (offering more scalability and more complex data models). As such, both solutions fit nicely into the scenario when you need to deal with lots of data in real time: Changes for the databases are fetched as they happen. Google’s child stores data in JSON format and provides various data management offerings, including a convenient data browsing tool."
48,Pros of Firebase
48,"Beginner friendliness. Firebase can be a great option when there’s little software development expertise available, as it presents an easy-to-use environment to kick off the project."
48,"Convenient data access. Both Realtime and Firestore are great options for storing and managing different types of data. There is a Firebase console for easy data access. Being cloud-based and NoSQL, they offer decent flexibility and scalability when the amount of data grows. Moreover, Firebase tools allow for working with responsive applications and keeping data updated even when there’s no Internet connection."
48,"Top-notch documentation. The solution comes with well-written documentation that facilitates the work with provided services for all users. It includes guidelines, technical documentation, SDK references, information about integration, and much more. If we get back to the StackOverflow survey, Firebase is the 12th most popular database choice of developers. The size of the product community is significant, which makes it easy to find answers to problems that pop up."
48,Cons of Firebase
48,"Limited querying capabilities. While this is valid only for Realtime Database, it’s still an issue if you are mainly planning to use this storage. The problem here is that you are restricted to making simple queries as there are no filter capabilities for more complex ones. This is because the entire database is a big JSON file with no options for data modeling."
48,"Limited data migration. If you use Firebase to host all your data, migrating it to another platform can become an issue. The service lacks migration tools to transfer data or set the default database of a project."
48,Use cases
48,"Firebase databases can be a good option to consider when your software deals with real-time data that needs to be synchronized between different browsers and devices. They are often chosen for such projects as messaging apps, social media apps, and gaming apps."
48,Amazon DynamoDB
48,"Amazon DynamoDB is a NoSQL database service managed by Amazon Web Services (AWS), designed for applications necessitating high scalability, low latency, and consistent performance."
48,Pros of Amazon DynamoDB
48,"Exceptional scalability. DynamoDB can effortlessly scale up or down to accommodate any level of traffic and data, making it ideal for applications that experience rapid growth or fluctuating demand."
48,"Low latency. DynamoDB delivers single-digit millisecond latency for read and write operations, ensuring quick and consistent data access, vital for real-time applications."
48,"Fully managed service. DynamoDB handles operational tasks such as hardware provisioning, patching, and backups as a managed service, allowing you to concentrate on your application's development and features."
48,"Adaptable data model. DynamoDB supports various data models, including key-value and document-oriented models, offering flexibility in structuring and querying your data."
48,Cons of Amazon DynamoDB
48,"High cost. DynamoDB's pricing structure can be intricate and may result in higher costs compared to self-managed NoSQL databases, particularly for applications with variable or unpredictable workloads."
48,"Restricted querying capabilities. Although DynamoDB supports basic queries and filters, it does not provide support for complex querying and aggregation operations needed in some use cases."
48,"Vendor lock-in. As a proprietary AWS service, transitioning from DynamoDB to another database system might necessitate significant effort and planning."
48,Use cases
48,"Amazon DynamoDB is well-suited for applications requiring high scalability, low latency, and consistent performance, such as serverless apps, eCommerce platforms, gaming platforms, and IoT solutions. It is also ideal for serverless architectures and applications that utilize other AWS services, as it integrates seamlessly with the AWS ecosystem."
48,How to choose a database management system
48,"Apart from the options described in the post, there are a lot of other database management systems out there. Each of them is good in its own way, having some drawbacks as well. Though we haven’t covered even a third of all databases, we tried to compare those commonly used for both small web applications and big data warehousing systems."
48,"So, how do you choose the right one for your own software application?"
48,"If you are just starting a local eCommerce business, databases like MySQL can be a sensible jumping-off point that will also work well for web-based BI tools and OLTP systems."
48,"In case you are striving to build an eCommerce giant with a complete buyer journey for your customer, you may go with Cassandra. To complement it with a powerful search engine, you may also attach the Elasticsearch database solution."
48,"Speaking of Cassandra, it’s also a pretty respectable option for data centers and real-time analytics with oceanic volumes of data."
48,"When speaking of analytic tools without multiple data layers, it may be reasonable to opt for NoSQL databases like MongoDB. It also performs well for product catalogs."
48,"Following up the scope of data warehousing applications, MSSQL is also worth a mention, especially for companies with a number of other Microsoft subscriptions."
48,"In terms of building an OLTP solution and data warehousing applications, Oracle is a good choice as well."
48,IoT applications and microservice architecture that tend to scale its data hosting will summarize our list of best use cases with Redis.
48,"Sure, there are more database systems to consider. It all depends on your business model and your business needs."
48,"Which one do you use? Please share your ideas with us.Comments2Sort by newestLoad More CommentsAdd CommentContentsWhat is a database management system?Types of databases: Relational vs non-relationalRelational or SQL databasesNon-relational or NoSQL databasesMySQLPros of MySQLCons of MySQLUse casesMariaDBPros of MariaDBCons of MariaDBUse casesOraclePros of OracleCons of OracleUse casesPostgreSQLPros of PostgreCons of PostgreUse casesMSSQLPros of MSSQLCons of MSSQLUse casesSQLitePros of SQLiteCons of SQLiteUse casesMongoDBPros of MongoDBCons of MongoDBUse casesRedisPros of RedisCons of RedisUse casesCassandraPros of CassandraCons of CassandraUse casesElasticsearchPros of ElasticsearchCons of ElasticsearchUse casesFirebase databasesPros of FirebaseCons of FirebaseUse casesAmazon DynamoDBPros of Amazon DynamoDBCons of Amazon DynamoDBUse casesHow to choose a database management system2 CommentsShareStay tuned to the latest industry updates.By clicking subscribe you confirm, that you understand and agree to the Privacy PolicySubscribe to our newsletterStay tuned to the latest industry updates.SubscribeBy clicking subscribe you confirm, that you understand and agree to the Privacy PolicyLatest Business ArticlesNov 17, 2023Nov 17, 2023Scaled Agile Framework: Overview, Pros and Cons, Alternatives15 min readBusinessNov 10, 2023Nov 10, 2023Analytics and Machine Learning Redefining Travel Expense Management11 min readBusiness,  TravelNov 01, 2023Nov 01, 2023Named Entity Recognition: The Mechanism, Methods, Use Cases, and Implementation Tips13 min readBusiness,  Data ScienceOct 31, 2023Oct 31, 2023Omnichannel Strategy: How to Create an Integrated Customer Experience13 min readBusinessBrowse Articles by TopicsMobileBusinessData ScienceFinanceEngineeringUX DesignTravelCloudTransportationHealthcareJoin us on the TechTalksDiscover new opportunities for your travel business, ask about the integration of certain technology, and of course - help others by sharing your experience.Visit TechTalksWrite an article for our blogAlmost 50 guest articles published from such contributors as Amadeus, DataQuest, MobileMonkey, and CloudFactory.Read how to become a contributor.Any Questions? Let's Discuss!Discuss your project needs with our architects.Attach file (jpg, pdf, doc up to 2mb)Contact UsBy clicking contact us you confirm, that you understand and agree to the Privacy PolicyOur website uses cookies to ensure you get the best experience. By browsing the website you agree to our use of cookies. Please note, we do not collect sensitive data and child data. See Privacy PolicyOk! Don't show it again"
49,Performance tuning | Platform.sh Documentation
49,Log in
49,Free trial
49,Site navigation
49,Get started
49,Introduction
49,Deploy
49,Git init
49,Git commit
49,Add data
49,Git branch
49,Git merge
49,Monitor and troubleshoot
49,Git log
49,Git status
49,Learn
49,What is Platform.sh?
49,Philosophy
49,YAML
49,What YAML is
49,Platform.sh YAML tags
49,Structure
49,Build and deploy
49,Get support
49,Tutorials
49,Automate your code updates
49,Restrict service access
49,Exporting data
49,Migrating to Platform.sh
49,Best practices
49,HTTP caching
49,"Monolith, headless or microservices?"
49,Keep your Git repository clean
49,More resources
49,Frameworks
49,Python
49,Django
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Local development
49,Integrated environments
49,DDEV
49,Supported environments
49,Tethered local
49,PHP
49,Drupal
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Elasticsearch
49,Memcached
49,Multi-site
49,Redis
49,SimpleSAML
49,FAQ
49,Ibexa DXP
49,Get started
49,Fastly
49,Symfony
49,Get Started
49,Symfony Integration
49,Environment Variables
49,Workers
49,Cron Jobs
49,Blackfire
49,Local development
49,FAQ
49,Laravel
49,Get started
49,Configure
49,Laravel Bridge
49,Scheduling tasks
49,Blackfire
49,Octane
49,Deploy
49,Local development
49,DDEV
49,Lando
49,TYPO3
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,WordPress
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Why use Composer?
49,Upgrade to use Composer
49,Deploy without Composer
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Redis
49,Javascript/Node.js
49,Gatsby
49,Get started
49,Configure
49,Deploy
49,Next steps
49,Headless CMS
49,Drupal
49,Strapi
49,WordPress
49,Next.js
49,Get started
49,Strapi
49,Get started
49,Configure
49,Deploy
49,Database Configuration
49,SQLite
49,PostgreSQL
49,MySQL
49,MongoDB
49,Local development
49,Strapi v3
49,Strapi v4
49,Adding frontends
49,Gatsby
49,Java
49,Hibernate
49,Get started
49,Jakarta
49,Get started
49,Micronaut
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Elasticsearch
49,Micronaut Data
49,MongoDB
49,Redis
49,JPA
49,Quarkus
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Elasticsearch
49,JPA
49,MongoDB
49,Panache
49,Redis
49,Spring
49,MariaDB/MySQL
49,PostgreSQL
49,RabbitMQ
49,Solr
49,Get started
49,Configure
49,Customize
49,Deploy
49,Next steps
49,Elasticsearch
49,JPA
49,MongoDB
49,Redis
49,Reference
49,Configure apps
49,App reference
49,Source operations
49,Runtime operations
49,Configure what’s served
49,PHP with front controller
49,Rewrite requests
49,Serve different paths
49,Static sites
49,Custom headers
49,Multiple apps
49,Choose a project structure
49,Define routes
49,Define relationships
49,Timezones
49,Troubleshoot disks
49,Troubleshoot mounts
49,Upgrading
49,Use build and deploy hooks
49,Change hooks in different environments
49,Comparison of hooks
49,Use hooks with dependencies
49,Work with workers
49,Add services
49,Elasticsearch
49,Premium
49,Headless Chrome
49,InfluxDB
49,Kafka
49,MariaDB/MySQL
49,MariaDB/MySQL Replication
49,Troubleshoot
49,Memcached
49,MongoDB
49,Premium
49,Network Storage
49,OpenSearch
49,PostgreSQL
49,RabbitMQ
49,Redis
49,Solr
49,Varnish
49,Vault KMS
49,Define routes
49,Server Side Includes (SSI)
49,HTTP cache
49,HTTPS
49,Proxy routes
49,Redirects
49,Languages
49,C#/.NET Core
49,Elixir
49,Java
49,Frameworks
49,Moving to Platform.sh
49,Tuning
49,JavaScript/Node.js
49,Debugging
49,Manage Node.js versions
49,Lisp
49,PHP
49,Extensions
49,Performance tuning
49,PHP-FPM sizing
49,Xdebug
49,Custom Redis
49,Swoole
49,Authenticated Composer
49,Troubleshoot
49,Python
49,Manage dependencies
49,Web servers
49,Python in non-Python containers
49,Ruby
49,Rust
49,Development
49,Project templates
49,Local development
49,Integrated environments
49,DDEV
49,Supported environments
49,Tethered
49,Untethered
49,Docksal
49,Lando
49,Variables overview
49,Set variables
49,Use variables
49,Access your site
49,Transfer files
49,Headers
49,Email
49,Private repositories
49,Git submodules
49,Connect with SSH
49,SSH keys
49,Troubleshoot SSH
49,Sync to Dedicated Gen 2
49,Regions
49,Troubleshoot
49,Deploy on Platform.sh
49,Sanitize databases
49,MariaDB and Drupal
49,PostgreSQL and Django
49,PostgreSQL and Symfony
49,Integrations
49,Overview
49,Activity scripts
49,Activity reference
49,Utility routines
49,Example: Discord
49,Example: Slack
49,Webhooks
49,Source integrations
49,Bitbucket
49,GitHub
49,GitLab
49,Resolve access
49,Health notifications
49,Increase observability
49,Infrastructure metrics
49,Dedicated Gen 2 metrics
49,Dedicated Gen 3 metrics
49,Grid metrics
49,Integrate observability
49,In-house observability tool
49,Blackfire
49,Third-party observability tools
49,New Relic
49,Tideways
49,Consume logs
49,Access logs
49,Forward Platform.sh and Blackfire logs
49,Forward Fastly CDN logs
49,Manage environments
49,Back up an environment
49,Restore an environment
49,Cancel an activity
49,Change parent
49,Configure HTTP access control
49,Deactivate an environment
49,Rename the default environment
49,Scalability
49,Set search engine visibility
49,Manage projects
49,Change regions
49,Change the project timezone
49,Delete a project
49,Custom domains
49,Set up a custom domain
49,DNS and apex domains
49,Custom TLS certificates
49,Handle subdomains
49,Preview environments
49,Content delivery networks
49,Fastly setup
49,Managed Fastly CDN
49,Cloudflare setup
49,Troubleshooting
49,Administration
49,Command line interface (CLI)
49,API tokens
49,Console
49,Configure a project
49,Configure environments
49,MFA
49,Organizations
49,Users
49,Pricing
49,Single sign-on (SSO)
49,Server upgrades
49,Security and compliance
49,Backup and restore
49,Data retention
49,Web Application Firewall (WAF)
49,Platform.sh WAF
49,Fastly Next-Gen WAF
49,Dedicated Gen 3
49,Incident monitoring
49,Security and data privacy
49,Multiple availability zones
49,Dedicated Gen 2
49,Overview
49,Incident Monitoring
49,Security and privacy
49,Updates and upgrades
49,Backups
49,Differences in development
49,Features
49,Deploying
49,Dev environments
49,Optional features
49,Glossary
49,Changelog
49,API Documentation
49,Edit page
49,Performance tuning
49,Back to home
49,On this page
49,Upgrade to PHP 8
49,Optimize the FPM worker count
49,OPcache preloading
49,Enable OPcache preloading
49,Configure OPcache
49,Disable OPcache timestamp validation
49,Restart PHP-FPM
49,Optimize your code
49,Is this page helpful?
49,❤️ Thanks for letting us know!
49,😞 Thanks for the feedback.
49,Suggest a change
49,Submit an issue
49,🙁 Couldn't process your feedback. Thanks for trying!
49,Upsun Beta access
49,Platform.sh will soon open up early access to test and provide feedback for our newest offering - Upsun.You can register for the beta by clicking here and completing the form.
49,Sign up for early access
49,Once your app is up and running it still needs to be kept fast.
49,"Platform.sh offers a wide degree of flexibility in how PHP behaves,"
49,but that does mean you may need to take a few steps to ensure your site is running optimally.
49,The following recommendations are guidelines only.
49,They’re also listed in about the order to investigate them.
49,Upgrade to PHP 8
49,"To make a PHP-based site run faster, the first step is to upgrade the PHP version."
49,Upgrading the PHP version might require changes to your app.
49,"For more details and recommendations, see the PHP migration guides."
49,"To change your PHP version, change the type in your app configuration."
49,"Before merging to production, test the change on a branch and make sure that your app is working as expected."
49,Optimize the FPM worker count
49,PHP-FPM uses a fixed number of simultaneous worker processes to handle incoming requests.
49,"If more simultaneous requests are received than the number of workers,"
49,then some requests wait until worker processes are available.
49,The default worker count is set to a conservative default value.
49,"To determine and set the optimal value for your app, see PHP-FPM sizing."
49,OPcache preloading
49,"OPcache preloading loads selected files into shared memory,"
49,"making their content (functions, classes) globally available for requests."
49,It also removes the need to include these files later.
49,"When OPcache is correctly configured, it can result in significant improvements to both CPU and memory usage."
49,Consult your framework’s documentation to see
49,if there are recommendations for optimal preload configuration or ready-to-use preload scripts.
49,OPcache is only available on PHP 7.4+ and uses PHP-CGI.
49,"If your PHP version doesn’t support OPcache, this is a good reason to upgrade."
49,Note that the only way to clear the preload cache is by restarting PHP-FPM.
49,"If you have disabled OPcache timestamp validation,"
49,you need to clear the OPcache explicitly on deployment (which can be done by restarting PHP-FPM).
49,Enable OPcache preloading
49,"To enable preloading, add a variable that specifies a preload script:"
49,.platform.app.yaml
49,variables:
49,php:
49,opcache.preload: 'PRELOAD_SCRIPT'
49,PRELOAD_SCRIPT is a file path relative to the app root.
49,It may be any PHP script that calls opcache_compile_file().
49,The following example uses a preload.php file as the preload script.
49,This script loads all .php files in the vendor directory (and subdirectories):
49,preload.php
49,<?php
49,$directory = new RecursiveDirectoryIterator(getenv('PLATFORM_APP_DIR') . '/vendor');
49,$iterator = new RecursiveIteratorIterator($directory);
49,"$regex = new RegexIterator($iterator, '/^.+\.php$/i', RecursiveRegexIterator::GET_MATCH);"
49,foreach ($regex as $key => $file) {
49,// This is the important part!
49,opcache_compile_file($file[0]);
49,Configure OPcache
49,OPcache needs to be tuned before production usage and can be configured the same way as PHP.
49,Let the app run for a while before tuning OPcache
49,since the preload script may change some of the configuration.
49,Set the maximum number of cached files
49,opcache.max_accelerated_files is the maximum number of files that OPcache can cache at once.
49,"If this value is lower than the number of files in the app,"
49,the cache becomes less effective because it starts thrashing.
49,"To determine the maximum number of files to cache, follow these steps:"
49,Connect to the container via SSH using the CLI
49,by running platform ssh.
49,Determine roughly how many .php files your app has by running this command from your app root:
49,find . -type f -name '*.php' | wc -l
49,Note that the returned valued is an approximation.
49,Some apps have PHP code in files that don’t end in .php or files that are generated at runtime.
49,Set opcache.max_accelerated_files to a value slightly higher than the returned number.
49,PHP automatically rounds the value up to the next highest prime number.
49,An example configuration:
49,.platform.app.yaml
49,variables:
49,php:
49,'opcache.max_accelerated_files': 22000
49,Set memory consumption
49,opcache.memory_consumption is the total memory (in megabytes) that OPcache can use with FastCGI.
49,"If the app uses more than this, the cache starts thrashing and becomes less effective."
49,Determining the optimal limit to memory consumption requires executing code via a web request to get adequate statistics.
49,CacheTool is an open-source tool to help you get the statistics.
49,"To determine the total amount of memory to use, follow these steps:"
49,Connect to the container via SSH using the CLI
49,by running platform ssh.
49,Change to the /tmp directory (or any other non-web-accessible writable directory) with cd /tmp.
49,Download CacheTool with curl -sLO https://github.com/gordalina/cachetool/releases/latest/download/cachetool.phar.
49,Make CacheTool executable with chmod +x cachetool.phar.
49,Check the OPcache status for FastCGI commands by running the following command:
49,php cachetool.phar opcache:status --fcgi=$SOCKET
49,The --fcgi=$SOCKET option ensures the PHP-FPM process on the server connects through the right socket.
49,Analyze the output to determine the optimal value for opcache.memory_consumption.
49,The most important values from CacheTool’s output are the following:
49,Memory used
49,Memory free
49,Oom restarts (out of memory restarts)
49,"If the value is different than 0, you don’t have enough memory allocated to OPcache."
49,"If Memory free is too low or Oom Restarts too high,"
49,set a higher value for memory consumption.
49,Set opcache.memory_consumption.
49,Note: The unit for opcache.memory_consumption is megabytes.
49,An example configuration:
49,.platform.app.yaml
49,variables:
49,php:
49,'opcache.memory_consumption': 96
49,Restart PHP-FPM and make sure that OPcache works as expected by rerunning CacheTool
49,with the following command:
49,php cachetool.phar opcache:status --fcgi=$SOCKET
49,Remove CacheTool by deleting the cachetools.phar file with rm -rf cachetools.phar.
49,Disable OPcache timestamp validation
49,"By default, OPcache checks that the cached version of a file is always up-to-date."
49,"This means that every time a cached file is used, OPcache compares it to the file on disk."
49,"If that file has changed, it gets reloaded and re-cached."
49,This allows to support apps that generate compiled PHP code from user configuration.
49,"If you know your code isn’t going to change outside a new deployment,"
49,you can disable that check and get a small performance improvement.
49,Timestamp validation can be disabled by adding the following variable to your app configuration:
49,.platform.app.yaml
49,variables:
49,php:
49,'opcache.validate_timestamps': 0
49,"When you have disabled OPcache timestamp validation,"
49,you need to explicitly clear OPcache on deployment by restarting PHP-FPM.
49,"Note: If your app generates PHP code at runtime based on user configuration, don’t disable timestamp validation."
49,Doing so would prevent updates to the generated code from being loaded.
49,Restart PHP-FPM
49,To force a restart of PHP-FPM:
49,Connect to your app container via SSH using the CLI by running platform ssh.
49,"Run pkill -f -u ""$(whoami)"" php-fpm."
49,Optimize your code
49,"To optimize your app, consider using a profiler."
49,A profiler helps determine what slow spots can be found and addressed and helps improve performance.
49,The web agency Pixelant has released a log analyzer tool for Platform.sh
49,that offers visualization of access logs to determine how much memory requests are using on average.
49,It also offers additional insights into the operation of your site and can suggest places to further optimize your configuration or when it’s time to increase your plan size.
49,"Note that this tool is maintained by a third party, not by Platform.sh."
49,Is this page helpful?
49,❤️ Thanks for letting us know!
49,😞 Thanks for the feedback.
49,Suggest a change
49,Submit an issue
49,🙁 Couldn't process your feedback. Thanks for trying!
50,Oracle MySQL Live and On-Demand Webinars
50,home nav
50,Oracle
50,Back
50,Search
50,Search by voice
50,View Accounts
50,Sign In
50,Back ORACLE ACCOUNT
50,Sign-In
50,Create an Account
50,Help
50,Sign Out
50,CLOUD ACCOUNT Sign in to Cloud
50,Try Oracle Cloud Free Tier
50,No results found
50,Your search did not match any results.
50,We suggest you try the following to help find what you're looking for:
50,Check the spelling of your keyword search.
50,"Use synonyms for the keyword you typed, for example, try ""application"" instead of ""software."""
50,Try one of the popular searches shown below.
50,Start a new search.
50,Trending Questions
50,Back
50,Oracle MySQL Live and On-Demand Webinars
50,"Hi Colleague , Thank You for Registering"
50,Select your Webinars and Register
50,You have registered successfully for Selected Webinars
50,Register now
50,Live Webinars
50,On-Demand Webinars
50,Register now
50,Live Webinar
50,On-demand Webinar
50,Upcoming Live Webinars
50,Turbocharge Business Insights with MySQL HeatWave – Learn HeatWave New Capabilities
50,"Date: July 21, 2022"
50,10:00 a.m. to 11:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Turbocharge Business Insights with MySQL HeatWave – Learn HeatWave New Capabilities
50,Webinar Details
50,"Date: July 21, 2022"
50,Start Time: 10:00 a.m. to 11:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"Oracle MySQL HeatWave, is the only service that enables database administrators and app developers to run OLTP and OLAP workloads directly from their MySQL database. This eliminates the need for complex, time-consuming, and expensive data movement and integration with a separate analytics database. The service is optimized for and exclusively available in Oracle Cloud Infrastructure (OCI). HeatWave accelerates MySQL performance by orders of magnitude for analytics and mixed workloads. Optimized for Oracle Cloud Infrastructure, it is the only database service that runs on MySQL Enterprise Edition. HeatWave now offers real-time elasticity and brings machine learning to data, eliminating the need for ETL processes"
50,In this hands-on workshop you will get an in-depth overview of functional capabilities and recommendations.
50,All attendees to this hands-on lab will get a 500 USD credit to be used toward a 30-day Oracle Cloud FREE trial.
50,Do not miss this opportunity!
50,Event Speaker
50,Bill Papp MySQL Principal Solution Engineer
50,Oracle MySQL Security from Data Protection to Regulation Compliance
50,"Date: Nov 3, 2022"
50,10:00 a.m. to 11:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Oracle MySQL Security from Data Protection to Regulation Compliance
50,Webinar Details
50,"Date: Nov 3, 2022"
50,Start Time: 10:00 a.m. to 11:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"In the era of data breaches, security issues can bring lots of headaches to any organization. Besides fines from authorities, bad reputation can put many customers away. Being able to protect important data from external threats, as well as control the increasing number of people who have internal access to the database systems means minimizing risks and ensuring that monitoring and prevention is in place."
50,"Furthermore, to protect sensitive data and privacy of personal information, governments and industry organizations have developed multiple privacy regulations and data protection laws (GDPR, PCI DSS, HIPAA)."
50,"Discover how the latest MySQL Enterprise Edition includes the most comprehensive set of advanced features, management tools, and technical support to achieve the highest levels of MySQL security."
50,Event Speaker
50,Mike Frank
50,MySQL Product Management Director
50,Migrate from On-Premises MySQL database to MySQL HeatWave on OCI or AWS
50,"Date: April 6, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Migrate from On-Premises MySQL database to MySQL HeatWave on OCI or AWS
50,Webinar Details
50,"Date: April 6, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave delivers the simplicity of transactions, real-time analytics, and machine learning (ML) in one database service, eliminating the cost and complexity of separate analytics databases, such as Amazon Redshift and Snowflake; ML services; and extract, transform, and load (ETL) duplication."
50,"In this webinar, we will show you how easily you can migrate from your on-premises MySQL database to MySQL HeatWave on Oracle Cloud Infrastructure (OCI) with confidence in 5 easy steps."
50,•	Discover and access
50,•	Plan migration
50,•	Provision and configure
50,•	Export and load data
50,•	Test and launch
50,Event Speaker
50,Ravish Patel
50,MySQL Solution Engineer
50,What’s New and Next with MySQL
50,"Date: April 27, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,What’s New and Next with MySQL
50,Webinar Details
50,"Date: April 27, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"In this session, we are going to discover the latest capabilities and innovations that have become available for MySQL in the recent quarters. We will highlight some major 8.0.x features and provide updates on new or improved features in 8.0.32. We will also cover some of the significant updates including InnoDB ClusterSet for High Availability, operator for Kubernetes, MySQL HeatWave Machine Learning, and more!"
50,Event Speaker
50,Harsh Nayak
50,MySQL Principal Solution Engineer
50,MySQL Security from Data Protection to Regulation Compliance
50,"Date: May 24, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL Security from Data Protection to Regulation Compliance
50,Webinar Details
50,"Date: May 24, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"In the era of data breaches, security issues can bring lots of headaches to any organization. Besides fines from authorities, bad reputation can put many customers away. Being able to protect important data from external threats, as well as control the increasing number of people who have internal access to the database systems means minimizing risks and ensuring that monitoring and prevention is in place."
50,"Furthermore, to protect sensitive data and privacy of personal information, governments and industry organizations have developed multiple privacy regulations and data protection laws (GDPR, PCI DSS, HIPAA)."
50,"Discover how the latest MySQL Enterprise Edition includes the most comprehensive set of advanced features, management tools, and technical support to achieve the highest levels of MySQL security."
50,Event Speaker
50,Mike Frank
50,MySQL Product Management Director
50,Unlock the Power of MySQL – Top Reasons to Choose MySQL Enterprise Edition
50,"Date: June 22, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Unlock the Power of MySQL – Top Reasons to Choose MySQL Enterprise Edition
50,Webinar Details
50,"Date: June 22, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"If you are currently using MySQL, you probably started with MySQL Community Edition."
50,"But how will you ensure you are using the most reliable, secure, scalable, up-to-date version?"
50,Do you know how to configure MySQL variables to ensure your applications run at their peak performance?
50,How will you monitor and tune poorly performing user/application SQL code?
50,"To help you answer these questions with confidence, MySQL provides MySQL Enterprise Edition."
50,"MySQL Enterprise Edition includes the most comprehensive set of advanced features and management tools to achieve the highest levels of scalability, security, reliability, and uptime. It reduces the risk, cost, and complexity in developing, deploying, and managing business-critical MySQL applications."
50,Join this webinar to learn the top reasons for using MySQL Enterprise Edition.
50,Event Speaker
50,Ravish Patel
50,MySQL Solution Engineer
50,Empowering Healthcare Innovations with MySQL HeatWave
50,"Date: June 29, 2023"
50,9:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Empowering Healthcare Innovations with MySQL HeatWave
50,Webinar Details
50,"Date: June 29, 2023"
50,Start Time: 9:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,Healthcare is one of the largest industries in the world.
50,"According to RBC Capital Markets, approximately 30% of all the world’s data volume is being generated by the healthcare industry. Healthcare providers and a new breed of digital healthcare suppliers are combining modern technologies with a data driven approach to lower healthcare costs, increase access to healthcare, and improve healthcare outcomes."
50,"Learn why MySQL HeatWave is the ideal solution to innovate faster, protect patient data, and comply with healthcare regulations. Avoiding common pitfalls for a seamless experience"
50,Event Speaker
50,Karthik Gnanakumar
50,MySQL Solution Engineer
50,MySQL Security from Data Protection to Regulation Compliance
50,"Date: July 6, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL Security from Data Protection to Regulation Compliance
50,Webinar Details
50,"Date: July 6, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"In the era of data breaches, security issues can bring lots of headaches to any organization. Besides fines from authorities, bad reputation can put many customers away. Being able to protect important data from external threats, as well as control the increasing number of people who have internal access to the database systems means minimizing risks and ensuring that monitoring and prevention is in place."
50,"Furthermore, to protect sensitive data and privacy of personal information, governments and industry organizations have developed multiple privacy regulations and data protection laws (GDPR, PCI DSS, HIPAA)."
50,"Discover how the latest MySQL Enterprise Edition includes the most comprehensive set of advanced features, management tools, and technical support to achieve the highest levels of MySQL security."
50,Event Speaker
50,Mike Frank
50,MySQL Product Management Director
50,What's New with MySQL HeatWave
50,"Date: August 10, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,What's New with MySQL HeatWave
50,Webinar Details
50,"Date: August 10, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave is the only MySQL cloud service with a built-in, high performance, in-memory query accelerator—HeatWave. It increases MySQL performance by orders of magnitude for analytics and mixed workloads without any changes to current applications."
50,"Join this webinar to learn the latest innovations implemented on MySQL HeatWave, including MySQL HeatWave AutoML, VS code plug-in, MySQL HeatWave for the distributed cloud, MySQL HeatWave Lakehouse, and more!"
50,Event Speaker
50,Harsh Nayak
50,MySQL Principal Solution Engineer
50,Migrating from MySQL On-premises to MySQL HeatWave
50,"Date: September 28, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Migrating from MySQL On-premises to MySQL HeatWave
50,Webinar Details
50,"Date: September 28, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave delivers the simplicity of transactions, real-time analytics, and machine learning (ML) in one database service, eliminating the cost and complexity of separate analytics databases, such as Amazon Redshift and Snowflake; ML services; and extract, transform, and load (ETL) duplication."
50,"In this webinar, we will show you how easily you can migrate from your on-premises MySQL database to MySQL HeatWave on Oracle Cloud Infrastructure (OCI) with confidence in 5 easy steps."
50,•	Discover and Assess
50,•	Plan Migration
50,•	Provision and Configure
50,•	Export and Load Data
50,•	Test and Launch
50,Event Speaker
50,Ravish Patel
50,MySQL Solution Engineer
50,Virtual Conference: Machine Learning for Beginners - From Data to Insights
50,"Date: October 5, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 2 and ½ hours
50,Selected Select
50,Learn more
50,Virtual Conference: Machine Learning for Beginners - From Data to Insights
50,Webinar Details
50,"Date: October 5, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 2 and ½ hours
50,Webinar Description
50,"Machine learning (ML) can be used to address various use cases such as fraud detection, product recommendations, predictive maintenance, and more. However, many organizations struggle to effectively implement ML based projects due the complexity involved in setting up the ML infrastructure, making the relevant data available to build models, preprocessing the data, training the models and the lack of transparency of the generated output."
50,"In the virtual conference, we will show you how to overcome these challenges with the automated machine learning capabilities offered by MySQL HeatWave AutoML, which enable database users as well as business analysts to incorporate ML into their applications with ease and get fast, accurate results while lowering costs."
50,You will also see demos of using MySQL HeatWave on Oracle Cloud and on AWS.
50,Agenda:
50,10:00 a.m. PT: Opening
50,"10:05 a.m. PT: Introduction to Machine Learning: Salil Pradhan, Product Manager, MySQL HeatWave Team"
50,"10:35 a.m. PT: A MySQL HeatWave Overview for Every Developer: Mandy Pang, Senior Principal Product Manager, MySQL HeatWave Team"
50,"11:05 a.m. PT: Query Processing on an Object Store with MySQL HeatWave: Abhinav Agarwal, Senior Principal Product Manager, MySQL HeatWave Team"
50,"11:35 a.m. PT: Deep Dive for Machine Learning Development with MySQL HeatWave: Salil Pradhan, Product Manager, MySQL HeatWave Team"
50,12:05 p.m. PT: Q&A
50,12:20 p.m. PT: Closing
50,Event Speaker
50,Salil Pradhan
50,"Product Manager, MySQL HeatWave, Oracle"
50,Mandy Pang
50,"Senior Principal Product Manager, MySQL HeatWave, Oracle"
50,Abhinav Agarwal
50,"Senior Principal Product Manager, MySQL HeatWave, Oracle"
50,MySQL Security from Data Protection to Regulation Compliance
50,"Date: October 19, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL Security from Data Protection to Regulation Compliance
50,Webinar Details
50,"Date: October 19, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"In the era of data breaches, security issues can bring lots of headaches to any organization. Besides fines from authorities, bad reputation can put many customers away. Being able to protect important data from external threats, as well as control the increasing number of people who have internal access to the database systems means minimizing risks and ensuring that monitoring and prevention is in place."
50,"Furthermore, to protect sensitive data and privacy of personal information, governments and industry organizations have developed multiple privacy regulations and data protection laws (GDPR, PCI DSS, HIPAA)."
50,"Discover how the latest MySQL Enterprise Edition includes the most comprehensive set of advanced features, management tools, and technical support to achieve the highest levels of MySQL security."
50,Event Speaker
50,Mike Frank
50,MySQL Product Management Director
50,Hands-On Lab: Analyze Data at Scale in Object Storage with MySQL HeatWave Lakehouse
50,"Date: November 9, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Hands-On Lab: Analyze Data at Scale in Object Storage with MySQL HeatWave Lakehouse
50,Webinar Details
50,"Date: November 9, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave has been enhanced to allow users to process hundreds of terabytes of data in the object store in a variety of file formats like CSV, Parquet, export files from other databases like Aurora and Redshift. Customers can query data in the object store and combine it with data in the database in a single query using standard MySQL syntax."
50,"With this capability, MySQL HeatWave provides one service for transaction processing, analytics across data warehouses and data lakes, and machine learning — without ETL across cloud services. And with no additional cost for this capability except the cost of storing the data in object store."
50,"In this hands-on lab, you will learn how to use MySQL HeatWave Lakehouse to process and query data in the object store. Understand how to load various file formats and run query processing with standard SQL syntax without the data having to leave the object store."
50,Event Speaker
50,Perside Foster
50,MySQL Principal Solution Engineer
50,Start on Your MySQL HeatWave Journey - Essential Steps for Beginners
50,"Date: December 7, 2023"
50,10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Start on Your MySQL HeatWave Journey - Essential Steps for Beginners
50,Webinar Details
50,"Date: December 7, 2023"
50,Start Time: 10:00 a.m. Pacific Time
50,Duration: 60 mins
50,Webinar Description
50,"Discover the world's most popular open-source database, MySQL, renowned for fueling the most-visited websites like Facebook, Twitter, and YouTube."
50,"If you're new to MySQL HeatWave, this session is a must-attend. MySQL HeatWave is a fully managed database service for transactions, real-time analytics across data warehouses and data lakes, and machine learning services, without the complexity, latency, and cost of ETL duplication. It is available on OCI, AWS, and Azure."
50,Event Speaker
50,Karthik Gnanakumar
50,MySQL Solution Engineer
50,Webinar: What’s New and Next with MySQL
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Webinar: What’s New and Next with MySQL
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"In this session, we are going to discover the latest capabilities and innovations that have become available for MySQL in the recent months. We will highlight some major 8.0.x features and provide updates on new or improved features since 8.0.32."
50,"We will also cover some of the significant enhancements to MySQL HeatWave, including support for vector store, generative AI, new in-database ML features, and more."
50,Event Speaker
50,Harsh Nayak
50,MySQL Principal Solution Engineer
50,Deep Dive into MySQL InnoDB Cluster Read Scale-out Capabilities
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Deep Dive into MySQL InnoDB Cluster Read Scale-out Capabilities
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL's first Innovation Release is out, 8.1.0, and with it, we're introducing MySQL InnoDB Cluster Read Replicas."
50,"The main purpose of secondaries on MySQL InnoDB Cluster is to be ready to take over when a primary member has failed (High Availability). This is done using MySQL Group Replication. Another commonly used purpose for the secondaries is to use them to offload read workloads away from the primary. With MySQL InnoDB Cluster Read Replicas, it's now possible to add asynchronous replicas to the database topology, to be used to offload read traffic away from primary or secondaries, to have dedicated read replicas, special purpose read replicas (e.g. for reporting), or to scale beyond what the secondaries can handle by adding multiple read replicas."
50,"This talk will cover the read replicas functionality, showcase its usage in different database architectures, and include a demonstration on its setup and management."
50,Event Speakers
50,Miguel Araújo
50,MySQL Senior Principal Software Engineer
50,Start on Your MySQL Journey - Essential Steps for Beginners
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Start on Your MySQL Journey - Essential Steps for Beginners
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Discover the world's most popular open-source database, MySQL, renowned for fueling the most-visited websites like Facebook, Twitter, and YouTube."
50,"If you're new to MySQL, this session is a must-attend. We will cover the top things you need to know about MySQL as a beginner, also show you the latest innovations on MySQL and MySQL HeatWave, including machine learning and lakehouse."
50,Event Speaker
50,Karthik Gnanakumar
50,MySQL Solution Engineer
50,"MySQL HeatWave Hands-on Lab: One Database for OLTP, OLAP, ML & Lakehouse"
50,Duration: 90 mins
50,Selected Select
50,Learn more
50,"MySQL HeatWave Hands-on Lab: One Database for OLTP, OLAP, ML & Lakehouse"
50,Webinar Details
50,Duration: 90 mins
50,Webinar Description
50,"Oracle MySQL HeatWave is the only MySQL cloud service with a built-in, high performance, in-memory query accelerator—HeatWave. It increases MySQL performance by orders of magnitude for analytics and mixed workloads, without any changes to current applications."
50,"With MySQL HeatWave ML, developers and data analysts can build, train, deploy, and explain machine learning models in MySQL HeatWave, without moving data to a separate machine learning service."
50,"MySQL HeatWave also has been enhanced to allow users to process hundreds of terabytes of data in the object store in a variety of file formats like CSV, Parquet, export files from other databases like Aurora and Redshift. Customers can query data in the object store and combine it with data in the database in a single query using standard MySQL syntax."
50,"HeatWave Lakehouse offers the best performance and price performance in the industry compared to Snowflake, Databricks, Redshift and Google Big Query both for loading data and running queries on several hundred terabytes of data."
50,"In this hands-on lab, we will walk you through,"
50,•	HeatWave OLTP/ Analytics
50,•	HeatWave AutoML
50,•	Heatwave Lakehouse
50,Event Speaker
50,Perside Foster
50,MySQL Principal Solution Engineer
50,Introducing MySQL HeatWave Lakehouse - processing data in object store with record performance
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Introducing MySQL HeatWave Lakehouse - processing data in object store with record performance
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave has been enhanced to allow users to process hundreds of terabytes of data in the object store in a variety of file formats like CSV, Parquet, export files from other databases like Aurora and Redshift. Customers can query data in the object store and combine it with data in the database in a single query using standard MySQL syntax."
50,"HeatWave Lakehouse offers the best performance and price performance in the industry compared to Snowflake, Databricks, Redshift and Google Big Query both for loading data and running queries on several hundred terabytes of data."
50,"MySQL Autopilot has been enhanced to provide differentiated capabilities for files in object store including automatic schema mapping, predicting the time to load data, adaptive data flow and automatic query plan improvement. The data in the object store remains in the object store, yet the performance of querying data in object store is identical to querying data in the database."
50,Join this webinar to gain first-hand insights and discover the latest advancements of MySQL HeatWave Lakehouse.
50,Event Speaker
50,Nipun Agarwal
50,"Senior VP, MySQL HeatWave Development"
50,Beginner's Guide to Implementing InnoDB ClusterSet with MySQL Shell
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Beginner's Guide to Implementing InnoDB ClusterSet with MySQL Shell
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL InnoDB ClusterSet provides an integrated solution for disaster recovery of InnoDB Cluster deployments in alternate locations. By using the AdminAPI, which is included with MySQL Shell, an advanced client and code editor for MySQL, you can easily configure and manage a Database Architecture that brings together High Availability and Disaster Recovery. Join this session to learn how to implement InnoDB ClusterSet with MySQL Shell."
50,Event Speaker
50,Miguel Araújo
50,MySQL Senior Principal Software Engineer
50,Optimizing MySQL Performance: Fine-Tuning MySQL for Your Business Needs and Growth
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Optimizing MySQL Performance: Fine-Tuning MySQL for Your Business Needs and Growth
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"To fully leverage the capabilities of MySQL, it is essential to optimize and fine-tune your database environment according to your specific requirements and needs. In this session, we will explore various methods to analyze your MySQL instances' performance and share with you the suggestions and best practices for achieving optimal performance."
50,Event Speaker
50,Michael Marx
50,MySQL Master Principal Solution Engineer
50,In-Database Machine Learning with MySQL HeatWave
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,In-Database Machine Learning with MySQL HeatWave
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave automates machine learning application development, making it easier to build, train, deploy, and explain ML models -- all without the data leaving the database."
50,"Learn how MySQL HeatWave automates the ML lifecycle, including algorithm selection, intelligent data sampling for model training, feature selection, hyperparameter optimization, and more —saving you significant time and effort at no additional cost."
50,Event Speaker
50,Eric Yanta
50,MySQL Principal Solution Engineer
50,Start on Your MySQL Journey - Essential Steps for Beginners
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Start on Your MySQL Journey - Essential Steps for Beginners
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Discover the world's most popular open-source database, MySQL, renowned for fueling the most-visited websites like Facebook, Twitter, and YouTube."
50,"If you're new to MySQL, this session is a must-attend."
50,•	Join us as we guide you through the fundamental aspects:
50,•	Essential tools and where to download them
50,•	Step-by-step installation and configuration of MySQL Server
50,•	Loading a sample database and executing basic queries
50,•	Avoiding common pitfalls for a seamless experience
50,Event Speaker
50,Karthik Gnanakumar
50,MySQL Solution Engineer
50,MySQL HeatWave for SaaS ISVs
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL HeatWave for SaaS ISVs
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL is the most popular opensource database in the world. Though best known for its use in Web properties such as Facebook, Twitter and Booking.com, MySQL is also an extremely popular embedded database. Eight of the top ten largest software companies in the world, along with numerous startups and mid-tier Independent Software Vendors (ISVs) all rely on MySQL to power their applications, appliances, and devices."
50,"Join this webinar to learn how using MySQL HeatWave can improve the three most fundamental measures of business success by tapping into new revenue streams, reducing costs, and improving margins."
50,Event Speaker
50,John Kehoe
50,MySQL Principal Solution Engineer
50,Migrating from MariaDB to MySQL
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Migrating from MariaDB to MySQL
50,Webinar Details
50,"Date: May 17, 2023"
50,Webinar Description
50,"Many of the world's largest and fastest-growing organizations including Facebook, Twitter, Booking.com, and Verizon rely on MySQL to save time and money powering their high-volume Web sites, business-critical systems, and packaged software."
50,"Recently, we have been seeing increasing requests for migrations from MariaDB to MySQL. MariaDB has significantly diverged and is no longer drop-in compatible with MySQL."
50,"Join this webinar to learn why you need to migrate from MariaDB to MySQL 8.0, and the operations that are involved in the migration process."
50,Event Speaker
50,Frederic Descamps
50,MySQL Community Manager
50,Hands on Lab: Intro to MySQL HeatWave on AWS
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Hands on Lab: Intro to MySQL HeatWave on AWS
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave on AWS is a fully managed service, developed and supported by the MySQL team at Oracle. Oracle automates tasks such as database and operating system patching. You are responsible for managing your data, schema designs, and access."
50,"With MySQL HeatWave on AWS, you can create and manage a MySQL DB System with a HeatWave Cluster for your AWS applications. This workshop will provide step-by-step instructions on creating and managing MySQL HeatWave on AWS."
50,Lab 1: Create MySQL HeatWave and Cluster
50,Lab 2: Connect to and Load Your MySQL DB System
50,Lab 3: Provision HeatWave Cluster
50,Lab 4: Run Queries in HeatWave Cluster
50,Lab 5: Monitor HeatWave
50,"We are also going to cover MySQL HeatWave ML – in memory query accelerator with built-in ML, that developers and data analysts can build, train, deploy, and explain machine learning models in MySQL HeatWave without moving data to a separate machine learning service."
50,Event Speaker
50,Perside Foster
50,MySQL Principal Solution Engineer
50,Introduction to MySQL and Healthcare
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Introduction to MySQL and Healthcare
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Healthcare is one of the largest industries in the world. According to RBC Capital Markets, approximately 30% of all the world’s data volume is being generated by the healthcare industry."
50,"Healthcare providers and a new breed of digital healthcare suppliers are combining modern technologies with a data driven approach to lower healthcare costs, increase access to healthcare, and improve healthcare outcomes."
50,"Join this webinar and learn why MySQL is the ideal solution to innovate faster, protect patient data, and comply with healthcare regulations."
50,Event Speaker
50,Tony Darnell
50,MySQL Principal Solution Engineer
50,Lower Interruption and Reduce Downtime with MySQL HeatWave High Availability
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Lower Interruption and Reduce Downtime with MySQL HeatWave High Availability
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave - one MySQL Database service for transactions, analytics, and machine learning (ML). Real-time, secure analytics without the complexity, latency, and cost of extract, transform, and load (ETL) duplication."
50,"MySQL HeatWave is now easier to use and has flexible deployment options, including High Availability. The High Availability option enables applications to meet higher uptime requirements and zero data loss tolerance."
50,Join this session to learn how you can leverage MySQL HeatWave High Availability for your business needs.
50,Event Speaker
50,Satish Senapathy
50,Senior MySQL Solution Engineer
50,Introduce MySQL Enterprise Thread Pool for Optimal Performance and Throughput
50,Selected Select
50,Learn more
50,Introduce MySQL Enterprise Thread Pool for Optimal Performance and Throughput
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Many of the world's most trafficked web properties like Facebook, Twitter, and Booking.com rely on MySQL performance and scalability to serve millions of users and handle their exponential growth."
50,"To meet the sustained performance and scalability of ever-increasing user, query and data loads MySQL Enterprise Edition provides the MySQL Thread Pool. The Thread Pool provides a highly scalable thread-handling model designed to reduce overhead in managing client connections and statement execution threads."
50,Event Speaker
50,Andrew Grimo
50,MySQL Principal Solution Engineer
50,Beginner’s Guide on Using MySQL HeatWave
50,Selected Select
50,Learn more
50,Beginner’s Guide on Using MySQL HeatWave
50,Webinar Details
50,Webinar Description
50,"MySQL is the world's most popular open-source database. It powers the most-trafficked websites in the world such as Facebook, Twitter, and YouTube. It's also one of the easiest databases to use and it's perfect for beginners."
50,"In this session, you'll learn:"
50,• Which tools are essential for getting started.
50,• Basic steps to install and configure MySQL in Oracle Cloud.
50,• How to load a sample database and execute basic queries.
50,• To migrate data to the cloud using MySQL Shell.
50,"Finally, we’ll demonstrate how you can improve query performance with MySQL HeatWave."
50,Event Speaker
50,Karthik Gnanakumar
50,MySQL Solution Engineer
50,Tips for MySQL Performance Tuning
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Tips for MySQL Performance Tuning
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,Do you want to deliver blazing fast MySQL applications? Are you developing new mission-critical applications for MySQL? Are you experiencing performance bottlenecks in your production MySQL applications? Can you identify and tune slow queries? Do you want to eliminate common mistakes?
50,"Led by MySQL experts, this session will address these questions with practical tips and best practices for troubleshooting and performance tuning MySQL."
50,Event Speaker
50,Michael Marx
50,MySQL Master Principal Solution Engineer
50,MySQL Enterprise in Financial Services
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL Enterprise in Financial Services
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"A new breed of fast growing FinTech is combining modern, agile technologies with a data driven approach to meet the evolving needs of consumers."
50,"MySQL is enabling this industry transformation by providing organizations the agility to cost-effectively develop modern, scalable financial services quickly."
50,The companies that survive will be those that adopt a data-first first approach to give customers better insights into their financial picture and the tools to meet their financial goals.
50,The webinar will cover:
50,•	Market trends
50,•	Open Source for Fintech (FINOS)
50,•	The FinTech ecosystem
50,•	How FinTech is enhancing financial services
50,•	Technology requirements for Fintech
50,"•	Customer testimonials, and more"
50,Event Speakers
50,Pedro Andrade
50,MySQL Key Account Director
50,Tom Disheroon MySQL Principal Solution Engineer
50,"MySQL HeatWave Database Service – One Database Service for OLTP, OLAP, and Machine Learning"
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,"MySQL HeatWave Database Service – One Database Service for OLTP, OLAP, and Machine Learning"
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave is the only MySQL cloud service with a built-in, high performance, in-memory query accelerator—HeatWave. It increases MySQL performance by orders of magnitude for analytics and mixed workloads without any changes to current applications."
50,"MySQL HeatWave is 6.5X faster than Amazon Redshift at half the cost, 7X faster than Snowflake at one-fifth the cost, and 1,400X faster than Amazon Aurora at half the cost. Customers run analytics on data that’s stored in MySQL databases without a separate analytics database and ETL duplication."
50,"MySQL HeatWave is available on Oracle Cloud Infrastructure (OCI), Amazon Web Services (AWS), and Microsoft Azure."
50,"With MySQL HeatWave AutoML, developers and data analysts can build, train, deploy, and explain machine learning models in MySQL HeatWave without moving data to a separate machine learning service. Benchmarks demonstrate that, on average, HeatWave AutoML produces more accurate results than Amazon Redshift ML, trains models 25X faster at 1% of the cost, and scales as more nodes are added."
50,Event Speaker
50,Satish Senapathy
50,Senior MySQL Solution Engineer
50,Connecting MySQL HeatWave with External Data Sources Using Oracle Data Integration
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Connecting MySQL HeatWave with External Data Sources Using Oracle Data Integration
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Whether you need to migrate data from legacy applications to MySQL HeatWave or combine different data sources for analytical purposes, this webinar is for you. Learn how to integrate virtually all datatypes from any data source into MySQL HeatWave. When you need to collect data in a timely manner, Oracle Cloud Infrastructure (OCI) Data Integration, a cloud native, serverless ETL (extract, transform, load) service on the Oracle Cloud, provides a unified solution for building, deploying, and managing complex data warehouses."
50,"In this webinar you will discover how Oracle Data Integration allows you to easily integrate data from disparate sources using a graphical no-code designer with interactive data preparation all powered by Spark ETL or E-LT push-down execution which together ensures that information is timely, accurate, and consistent across complex systems."
50,Event Speakers
50,Eric Yanta
50,MySQL Principal Solution Engineer
50,Gurudixit Chepuri
50,Senior Cloud Engineer
50,Using Oracle Cloud Infrastructure GoldenGate with MySQL HeatWave
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Using Oracle Cloud Infrastructure GoldenGate with MySQL HeatWave
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Join this session to learn how Oracle Cloud Infrastructure (OCI) GoldenGate integrates with MySQL databases on-premises and in the cloud, including OCI MySQL Database Service. OCI GoldenGate is a managed service providing a real-time data mesh platform, which uses replication to keep data highly available and enable real-time analysis. You will hear how OCI GoldenGate complements MySQL databases for high availability, data synchronization, and data mesh use cases."
50,Event Speaker
50,Julien Testut
50,GoldenGate Senior Principal Product Manager
50,Hands on Lab: Intro to MySQL HeatWave on AWS
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Hands on Lab: Intro to MySQL HeatWave on AWS
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL HeatWave on AWS is a fully managed service, developed and supported by the MySQL team at Oracle. Oracle automates tasks such as database and operating system patching. You are responsible for managing your data, schema designs, and access."
50,"With MySQL HeatWave on AWS, you can create and manage a MySQL DB System with a HeatWave Cluster for your AWS applications. This workshop will provide step-by-step instructions on creating and managing MySQL HeatWave on AWS."
50,Lab 1: Create MySQL HeatWave and Cluster
50,Lab 2: Connect to and Load Your MySQL DB System
50,Lab 3: Provision HeatWave Cluster
50,Lab 4: Run Queries in HeatWave Cluster
50,Lab 5: Monitor HeatWave
50,"We are also going to cover MySQL HeatWave ML – in memory query accelerator with built-in ML, that developers and data analysts can build, train, deploy, and explain machine learning models in MySQL HeatWave without moving data to a separate machine learning service."
50,Event Speaker
50,Perside Foster
50,MySQL Principal Solution Engineer
50,Using MySQL Document Store with Node.js
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Using MySQL Document Store with Node.js
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL Document Store offers developers the best features of NoSQL and relational databases. In this session, we will discuss how we can use Node.js to access a MySQL Document Store, create/remove collections, create/update/remove documents, retrieve, filter, & sort document lists, and leverage the power of native SQL to retrieve aggregate data from our JSON documents for reporting purposes."
50,Event Speaker
50,Scott Stroz
50,MySQL Developer Advocate
50,How MySQL Security Helps Public Sector Improve and Expand Services While Cutting Costs
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,How MySQL Security Helps Public Sector Improve and Expand Services While Cutting Costs
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL is used extensively in the public sector. The MySQL database is a highly secure, effective, available, resilient, and cost-efficient database solution that meets the complex needs of"
50,•	Public health systems and their patients
50,•	Finance
50,•	Legal and Judicial
50,•	Educational institutions
50,"•	Local, State and National government"
50,•	Public safety
50,•	Defense
50,"In this webinar, we will cover MySQL"
50,•	Security Architecture
50,•	Identity Management
50,•	Advance Authentication
50,•	Compliance Auditing
50,•	Key Management
50,•	Regulatory Compliance
50,•	Data Privacy
50,•	Security Hardening Guidelines
50,Event Speaker
50,Mike Frank
50,MySQL Product Management Director
50,Beginner's Guide to Implementing InnoDB ClusterSet with MySQL Shell
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Beginner's Guide to Implementing InnoDB ClusterSet with MySQL Shell
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL InnoDB ClusterSet provides an integrated solution for disaster recovery of InnoDB Cluster deployments in alternate locations. By using the AdminAPI, which is included with MySQL Shell, an advanced client and code editor for MySQL, you can easily configure and manage a Database Architecture that brings together High Availability and Disaster Recovery. Join this session to learn how to implement InnoDB ClusterSet with MySQL Shell."
50,Event Speaker
50,Miguel Araújo
50,MySQL Senior Principal Software Engineer
50,MySQL for FinTech and Financial Services
50,Selected Select
50,Learn more
50,MySQL for FinTech and Financial Services
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"How Fintechs are enhancing financial services: Digital payment, Neobanks, Loans & financing, money transfers, peer to peer payments, alternative lending, robo investment advisors, Blockchain & cryptocurrency."
50,How Sam.ai Doubled its Conversational AI Output with MySQL High Availability Architecture on OCI
50,Selected Select
50,Learn more
50,How Sam.ai Doubled its Conversational AI Output with MySQL High Availability Architecture on OCI
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,Sam.ai needed to deliver nuanced reporting and conversational intelligence that captures specificity and insights at many levels deeper and faster than traditional methods.
50,"MySQL High Availability, a fault-tolerant system with automatic failover and zero data loss, it helps businesses minimize interruption and reduce downtown."
50,"Hear insights from John Kehoe, Principal Solution Engineer at Oracle and Raz Choudhury, Founder & CEO of Sam.ai on how they have achieved 2x speed performance."
50,Join this session to learn how they leverage MySQL High Availability on OCI to turbocharge their Conversation AI engine.
50,Event Speakers
50,Raz Choudhury
50,"Founder & CEO, Sam.ai"
50,John Kehoe
50,MySQL Principal Solution Engineer
50,"20x Faster Analytics, 25x Faster Machine Learning with MySQL HeatWave on AWS"
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,"20x Faster Analytics, 25x Faster Machine Learning with MySQL HeatWave on AWS"
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Learn how to build MySQL applications on Amazon Web Services (AWS) encompassing OLTP, analytics, and machine learning using Oracle MySQL HeatWave—at a price performance that is better than Amazon Aurora and Amazon Redshift. Find out how to create well-tuned, in-database machine learning models in a fully automated, highly parallel, and secure way. Plus, explore the interactive console within MySQL HeatWave on AWS—an element that further improves its usability."
50,Event Speaker
50,Nipun Agarwal
50,"Senior VP, MySQL HeatWave Development"
50,Mandy Pang
50,MySQL HeatWave Senior Principal Product Manager
50,"MySQL, JSON, & YOU: Perfect together"
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,"MySQL, JSON, & YOU: Perfect together"
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Long before 'NoSQL' databases became all the rage, developers stored JSON in relational database tables. With the advent of the JSON data type, developers now have a powerful set of functions to update, retrieve, and filter data based on values stored in a JSON blob. In this session, we discuss storing JSON in our database and how the tools in MySQL can make that task a bit easier. We will start by discussing why you might want to store data as JSON instead of other storage methods. We will then talk about the different ways in which MySQL can help us achieve that goal and how we might go about deciding what method is best. We will see examples of using the JSON data type to store data in a 'normal' table and use SQL commands to retrieve, filter, and sort that data. We will also show how you can validate your JSON schema."
50,Event Speaker
50,Scott Stroz
50,MySQL Developer Advocate
50,MySQL as an Embedded Database for ISVs
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,MySQL as an Embedded Database for ISVs
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,Integrated relational database management systems (RDBMS) are databases that ISVs (Independent Software Vendors) and OEMs (Original Equipment Manufacturers) ship with their products. These embedded databases are typically “invisible” to the end user. The standard functions for database management are handled either by the database itself or by the ISV’s own software via an application interface.
50,"MySQL, the world’s most popular open-source database, has been named by Forrester and other analyst groups as a leader in the market for integrated and global database management. The reasons include the advantages that the open-source model offers OEMs and ISVs, numerous technical advantages of the MySQL database, very high usability, full technical support, a dual license model and very low total cost of ownership."
50,Join this webinar to take a detailed look at all the benefits MySQL offers OEMs and ISVs.
50,Event Speaker
50,Andrew Grimo
50,MySQL
50,Principal Solution Engineer
50,In-Database Machine Learning with MySQL HeatWave
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,In-Database Machine Learning with MySQL HeatWave
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Learn about the in-database machine learning (ML) capabilities of Oracle MySQL HeatWave. Hear our experts discuss how to fully automate this process; eliminate extract, transform, and load (ETL); and achieve the maximum benefit from a highly performant, scalable, and secure way of applying ML. We also cover how to explain predictions based on MySQL HeatWave ML models."
50,Event Speaker
50,Satish Senapathy
50,MySQL Senior Solution Engineer
50,Top Things You Need to Learn about MySQL as a Beginner
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Top Things You Need to Learn about MySQL as a Beginner
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"MySQL is the world’s most popular open-source database. It powers the most-trafficked websites in the world such as Facebook, Twitter, and YouTube. It’s also one of the easiest databases to use and it’s perfect for beginners. In this webinar, we are going to cover the top things you need to know about MySQL as a beginner:"
50,• Which tools are essential and where to download them
50,• Basic steps to install and configure MySQL Server
50,• How to load a sample database and execute basic queries.
50,• How to avoid common pitfalls.
50,"In Addition, we’ll demonstrate essential tips for using MySQL Shell to manage your MySQL instance."
50,Event Speaker
50,Lee Stigile MySQL Solution Engineering Director
50,What’s New and Next with MySQL
50,– Latest Updates on MySQL 8.0.29 Onwards and More!
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,What’s New and Next with MySQL
50,– Latest Updates on MySQL 8.0.29 Onwards and More!
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"In this session, we are going to discover the latest capabilities and innovations that have become available for MySQL in the recent quarters. We will highlight some major 8.0.x features and provide updates on new or improved features since 8.0.29. We will also cover some of the significant updates including InnoDB ClusterSet for high availability, operator for Kubernetes, HeatWave Machine Learning, and more!"
50,Event Speaker
50,Harsh Nayak
50,MySQL Principal Solution Engineer
50,Oracle MySQL Performance Tuning
50,Duration: 60 mins
50,Selected Select
50,Learn more
50,Oracle MySQL Performance Tuning
50,Webinar Details
50,Duration: 60 mins
50,Webinar Description
50,"Of all the features that are available in MySQL, we will still need to optimize and fine tune our environment to our specific needs and the immediate or not-too-distant future growth."
50,In this session we will have a look into ways to analyze the instances' performance and take into consideration some suggestions and best practices.
50,Event Speaker
50,Michael Marx Master Principal Solution Engineer
50,On-Demand Webinars
50,Live Webinars
50,On-Demand Webinars
52,"Solutions for MySQL | SolarWindsEventsPartnersGovernmentCustomer PortalContact UsContact SalesEnglishInput Search boxSubmit Search boxHide Search boxShow Search boxSolarWinds Logo, return to the Home page.ProductsSolutionsResourcesQuoteInput Search boxSubmit Search boxProductsObservabilityExplore the PlatformSolarWinds ObservabilityHybrid Cloud ObservabilityView All Observability ProductsNetwork ManagementHybrid Cloud ObservabilityNetwork Configuration ManagerKiwi Syslog Server NGNetwork Performance MonitorNetwork Traffic AnalyzerNetwork Topology ManagerView All Network Management ProductsSystems ManagementHybrid Cloud ObservabilityVirtualization ManagerWeb Performance MonitorServer & Application MonitorStorage Resource MonitorServer Configuration MonitorView All Systems Management ProductsDatabase ManagementDatabase Performance AnalyzerSQL SentryView All Database Management ProductsIT Service ManagementService DeskWeb Help DeskExplore DamewareDameware Remote EverywhereDameware Remote SupportView All IT Service Management ProductsApplication ManagementSolarWinds ObservabilityAppOpticsServer & Application MonitorPingdomLogglyWeb Performance MonitorView All Application Management ProductsIT SecurityAccess Rights ManagerPatch ManagerServ-U Secured FTPSecurity Event ManagerIdentity MonitorServ-U Managed File TransferView All IT Security Productsor View All Products & Free TrialsSolutionsby NeedHybrid Cloud ObservabilityDatabase ManagementObservability for APMIT Service Managementby IndustrySmall BusinessEnterpriseEducationPublic Sectorby TechnologyAzureMySQLSQL DiagnosticKubernetesor View All SolutionsResourcesResource CenterResourcesOrange Matter BlogLogicalRead BlogSecure by DesignGDPRService & SupportCustomer SuccessTechnical DocsRenew MaintenanceTech SupportTraining & CertificationCommunityTHWACK IT CommunityEventsPartnersEventsPartnersGovernmentCustomer PortalContact UsContact SalesPerformance tuning for MySQLNo matter where MySQL runs—from the cloud to a VM—SolarWinds has you covered.Request DemoIs your database performance monitoring tool up for the job?MySQL has become one of the most popular databases for a new generation of enterprise applications.We know a lot about database performance managementWe’ve provided solutions for MySQL, MariaDB, and Percona for years and helped monitor thousands of database instances.Request DemoFull MySQL database coverageWe’ve been in database performance management for over a decade, and we’ve taken a consistent approach by supporting a broad range of MySQL instances.A holistic approachOur database solutions include performance monitoring, MySQL query tuning, I/O tuning, and more.SolarWinds Database Performance AnalyzerSolarWinds® Database Performance Analyzer (DPA) supports a broad set of database platforms, including Oracle, SQL Server, PostgreSQL, and more. SolarWinds Database Performance MonitorSolarWinds Database Performance Monitor (DPM) is a monitoring solution for MySQL designed to deliver a software as a service (SaaS)-based interface capable of being used by developers, site reliability engineers (SREs), and DBAs.Request DemoCase StudyPatientPing brings teams together with real-time data and performance analysisBuilding out a patient-centric platform to serve multi-layered healthcare networks across the U.S. is a huge job. PatientPing’s engineers know the continued success of their groundbreaking application depends in large part on how effectively it interacts with their MySQL database.Read Case StudyRead Case StudyExperience yields insights. Here's some of what we've learned.Communications service provider saves millionsThis customer saved more than $2 million in recurring annual costs after replacing several disparate monitoring tools with Hybrid Cloud Observability.Read Case StudyRead Case StudyEnterprise cloud operations team gains 5x ROI over three yearsThe popular retailer achieved these savings by retiring an array of open-source tools and problematic SaaS-based IT monitoring tools.Read Case StudyRead Case StudySolarWinds is a trusted leader, year after yearSolarWinds Recognized in GigaOm Radar Reports as a Leader in Network and Cloud ObservabilityLearn MoreLearn MoreYou may still have questions. We have answers.How Can We Help You?We’re Geekbuilt.® Developed by network and systems engineers who know what it takes to manage today’s dynamic IT environments, SolarWinds has a deep connection to the IT community. The result? IT management products that are effective, accessible, and easy to use.CompanyCareer CenterIT GlossaryPreference CenterFor GovernmentDocumentationTrust CenterInvestorsSecure by DesignResource CenterFor CustomersGDPR Resource CenterSecurity InformationFacebookTwitterYoutubeLinkedInQuoteLegal Documents Privacy California Privacy Rights©2023 SolarWinds Worldwide, LLC. All rights reserved."
53,Tutorial: Example configurations for better performance - aaPanel - Hosting control panel. One-click LAMP/LEMP.
53,Loading...
53,This site is best viewed in a modern browser with JavaScript enabled.
53,Something went wrong while trying to load the full version of this site. Try hard-refreshing this page to fix the error.
53,Tutorial: Example configurations for better performance
53,aaP_loonysnow
53,"After much agony, when it would seem that everything is set up perfectly, and the site is still loading for a long time, I was able to find the best config for a bundle of MariaDB 10.5 + PHP 8.0 + Apache 2.4 (yes, not NGINX) + OPcache extension + APCU extension."
53,"P.S. What the built-in ""Optimization"" functionality offers is complete nonsense, only complementing the friezes, no matter how beautiful ""aaPanel"" is."
53,"P.P.S. I have a whole fleet of VPS for various clients where ""aaPanel"" is installed. Believe my experience."
53,"And so, config for VPS 4 CPU, 8GB RAM, SSD (not HDD)."
53,MariaDB:
53,key_buffer_size = 1200
53,query_cache_size = 50
53,tmp_table_size = 128
53,innodb_buffer_pool_size = 4096
53,innodb_log_buffer_size = 32
53,sort_buffer_size = 2048
53,read_buffer_size = 256
53,read_rnd_buffer_size = 512
53,join_buffer_size = 2048
53,thread_stack = 512
53,binlog_cache_size = 64
53,thread_cache_size = 16
53,table_open_cache = 2048
53,max_connections = 200
53,PHP:
53,max_children = 4
53,start_servers = 2
53,min_spare_servers = 1
53,max_spare_servers = 2
53,Apache:
53,Timeout 30
53,KeepAliveTimeout 5
53,MaxKeepAliveRequests 200
53,StartServers 4
53,MaxSpareThreads 50
53,MinSpareThreads 20
53,ThreadsPerChild 50
53,MaxRequestWorkers 200
53,MaxConnectionsPerChild 10000
53,"After setting the parameters, BE SURE to save and restart the server. It's fast."
53,Result:
53,"An example on the ""heaviest"" project that I have to support."
53,"The server response on a WordPress multisite with more than 50 sites inside loads not 1.5 seconds, but 0.543 seconds."
53,"With ""WP Super Cache"" caching enabled in 0.3 seconds."
53,aaP_loonysnow
53,"A little later I will show the configuration for a VPS with 2 CPUs, 6GB RAM."
53,UPD:
53,I forgot to add: the WordPress site from the example has an attendance of 70+ thousand unique users per month.
53,Information for understanding about the load on the server and that it copes with it perfectly.
53,klaus
53,aaP_loonysnow 70k users per months means roughly 1 user at 5 seconds is not such a big load. If half of them are visitors than doesn't even coun't because of WP Super Cache. So practically you get a user at every 6-7 seconds that request something so you have like 20 to 40 requests per second.
53,Nginx is not magically improve the performance it just works differently and use less resources.
53,"You have KeepAlive settings but you don't have KeepAlive on, that would enable KeepAlive in the first place,"
53,"MaxConnectionsPerChild 0 This is again just pointless, you can lower from default of 1000 to 500-300 if you have KeepAlive on but you don't keep that ON and you set this to 0, this will cause mem­ory leaks and your server will fail under ddos in seconds."
53,"KeepAliveTimeout 5 this should be 2 or 3 for your loading time of 0.543, but would not matter in this case because KeepAlive is not enabled."
53,aaP_loonysnow
53,klaus
53,"The fact is that there is not one such site on the server. A lot of WordPress projects, Laravel, MODX. And they all eat up resources."
53,KeepAlive is enabled by default. It's not even up for discussion.
53,"About DDoS: Projects are connected via CloudFlare and fail2ban is hard-configured. Of course, this will not protect against a serious attack, but first try to find a server. If the issue with MaxConnectionsPerChild is important to you, then set the value to 10000."
53,Dhanamerdeka
53,I really glad someone sharing this.... and it's apache...
53,"on past i already sharing for ""OLS"" but i didn't know much about how to optimize it for apache...."
53,thanks alot bro...
53,"maybe if i can suggest you.... you can make ""1CPU, 1GB/2GB ram"" for baseline optimization"
53,"i really sure, if we know how roughly the baseline is.... the higher spec can be easily calculated too...."
53,thanks.... thanks alot for the sharing!
53,aaP_loonysnow
53,Dhanamerdeka
53,"With this configuration, it is better to install HestiaCP."
53,"I will try to conduct tests, but I doubt very much about good results."
53,UPD:
53,I made a mistake in the message above.
53,"""The fact is that there are many other sites on the server."" instead of ""The fact is that there is not one such site on the server."""
53,"English is not my native language, I use a translator."
53,klaus
53,"aaP_loonysnow I don't use apache so i know basics about it, in the past KeepAlive was off by default if is on now okay."
53,"The MaxConnectionsPerChild directive sets the limit on the number of connections that an individual child server process will handle. After MaxConnectionsPerChild connections, the child process will die. If MaxConnectionsPerChild is 0, then the process will never expire."
53,Setting MaxConnectionsPerChild to a non-zero value limits the amount of memory that a process can consume by (accidental) memory leakage.
53,This is pretty simple why this shouldn't be on 0 on a live server but you do you.
53,Any protection outside physical firewall are useless on a large scale ddos.
53,Dhanamerdeka
53,aaP_loonysnow I hope i can find the best optimization configuration soon....
53,BTW Keep Alive on Apache Aapanel isn't on by default...
53,2 server that i install aapanel to... they two had all keep alive on by default BUT not really turned on
53,how to fix this is just to turn off keep alive and then save
53,after it you can turn it on again.....
53,i check it from https://technumero.com/keep-alive-test/
53,aaP_loonysnow
53,klaus
53,I know what each parameter is responsible for.
53,"The problem with aaPanel was the inactivity of the usual configurations, such as in the classic LAMP or HestiaCP, and I was tired of selecting the optimal parameters."
53,"But thanks anyway! I'll leave it as it is for myself, but for other users it would be worth changing the parameter."
53,@aaPanel_Kern
53,"I would like to ask one of the moderators to replace Apache's ""MaxConnectionsPerChild"" from 0 to 10000 in the first message, since I don't have the ability to edit :<"
53,Dhanamerdeka
53,aaP_loonysnow Btw if we start from 1CPU 1GB ram Server
53,"does ""MaxConnectionsPerChild"" => 10000 should be placed"
53,or we should make it smaller? like.... 5000 or others?
53,i confuse about this setting....
53,aaP_loonysnow
53,Dhanamerdeka
53,Conducted tests with VPS 1 CPU core and 1GB RAM.
53,Apache 2.4.56 + PHP 8.0 (OPcache + APCU + imagemagick) + MySQL 5.7.40.
53,"MariaDB for some reason stopped being installed even on better hardware, and MySQL 8 requires at least 6GB of RAM."
53,"A real WordPress-based website was taken as a basis (full of plugins, heavy theme, WP Super Cache, Autoptimize)."
53,Results:
53,"Without caching, the server response averages 800-900ms."
53,With caching of about 100-150ms.
53,The whole problem is the number of cores. I advise you to install at least 2 CPU cores.
53,"Given the limitations of iron, the result is very good. For personal projects or a blog, that's the thing."
53,MySQL:
53,key_buffer_size = 16
53,query_cache_size = 0
53,tmp_table_size = 32
53,innodb_buffer_pool_size = 512
53,innodb_log_buffer_size = 8
53,sort_buffer_size = 1024
53,read_buffer_size = 1024
53,read_rnd_buffer_size = 4096
53,join_buffer_size = 1024
53,thread_stack = 192
53,binlog_cache_size = 32
53,thread_cache_size = 8
53,table_open_cache = 64
53,max_connections = 20
53,PHP:
53,max_children = 10
53,start_servers = 2
53,min_spare_servers = 1
53,max_spare_servers = 3
53,Apache:
53,Timeout = 30
53,KeepAlive = On
53,KeepAliveTimeout = 5
53,MaxKeepAliveRequests = 100
53,StartServers = 2
53,MaxSpareThreads = 25
53,MinSpareThreads = 10
53,ThreadsPerChild = 25
53,MaxRequestWorkers = 50
53,MaxConnectionsPerChild = 5000
53,klaus
53,"Dhanamerdeka That is basically a setting for memory leaks, depend on how much visitors you expect you can lower or increase that one on the run, start with 500 on linux, mostly that problem persist on windows servers."
53,If you don't need a feature from apache better use nginx.
53,Dhanamerdeka
53,klaus do you had better configuration? share it on here bro
53,well i need apache that's why i need to optimize them
53,"for nginx i didn't want to change all my sites to nginx yet, since i need to recoding something"
53,klaus
53,Dhanamerdeka
53,KeepAlive = On
53,KeepAliveTimeout = 2
53,StartServers 2
53,MinSpareServers 2
53,MaxSpareServers 10
53,MaxClients 5
53,MaxRequestsPerChild 300
53,AllowOverride None
53,"You should gradually move toward nginx the problem with apache is the memory you need atleast 2 GB of RAM to use it, after all the minimum requirements are 4GB and thats only for apache."
53,Mysql need's by default 512MB of RAM.
53,"innodb_buffer_pool_size = 180M with this settings you can limit the RAM ussage, a high value is pointless"
53,Mysql is runing on a local server so if you have a ssd on it the access is pretty fast there is no point to use your ram for nothing.
53,sort_buffer_size = 10M
53,"read_buffer_size = 10M ( always on mysql configuration use M (MB) because if you use 1024 is translated to bytes and the sort_buffer_size have a minimum value of 32768 bytes so that's being said, 1024 is a useless number used two times in the above configuration, read_buffer_size have a 8192 minimum value)"
53,"The best configuration is your own, just modify the things to suit your server, allways ping your website when is not under a CDN ( like Cloudflare) that is useless because you don't ping your server so you don't know the real numbers, using cache to serve static files that again improve the load on server and you can see above the performance is not great for dynamic content so you should modify the settings and improve the performance of the most used service on the VPS that would be apache."
53,"And run PHP in fastcgi mode, https://httpd.apache.org/mod_fcgid/ ."
53,https://www.php.net/manual/en/faq.installation.php#faq.installation.apache2
53,"Doing some research on your own is better than any ""better performance"" configuration you can find on internet."
53,"Without caching, the server response averages 800-900ms."
53,With caching of about 100-150ms.
53,Even default settings can do better than this.
53,Add atleast 1GB of RAM and a second CPU the second one is important when you get more visitors the first one will give apache enought memory to don't care about them atleast for a time.
54,Rev Up Your Database Management with AI2sql for MariaDB - Write Queries in Seconds and Get More Done Than Ever Before!
54,How It WorksFeaturesConnectorsChatGPT PluginChrome ExtensionER Diagram Schema AssistanceFine Tuned LLM SolutionsLearnDocsBlogHow-to VideosPodcastsEnterprisePricingContact UsGet Started How It WorksFeatures Connectors ChatGPT Plugin Chrome Extension ER Diagram Schema Assistance Fine Tuned LLM SolutionsLearn Docs Blog How-to Videos PodcastsEnterprisePricingContact UsGet Started
54,7 Days Free Trial
54,Learn more about how AI2sql can help you generate your SQL queries and save time!
54,Try it now
54,Powered by OpenAI GPT-4
54,Copyright AI2sql 2023
54,"13553 Atlantic Blvd, Suite 201"
54,FL 32225
54,support@ai2sql.io
54,CompanyHomePricingReviewsAffiliate ProgramBlogContactHelpFree 7 Day TrialFree ToolsSQL FormatterSQL Code Error FinderPluginsChrome ExtensionChatGPT PluginMoreChangelogRoadmapFAQTerms and ConditionsPrivacy Policy
54,Made with
54,Softr
55,Tendril - Wikitech
55,Tendril
55,From Wikitech
55,Jump to navigation
55,Jump to search
55,Tendril has been deprecated as of January 2022
55,This page contains historical information. Tendril has been archivedSee task T343707 2023
55,"Tendril is a tool for analytics and performance tuning of the MariaDB servers, developed by Springle (with features added in an ad-hoc fashion as required) and used by the DBA team. For security reasons it is only accessible to people with NDA."
55,"Prior to October 2015, the Ishmael tool provided a similar service."
55,"As of late 2021, it is planned to deprecate Tendril in favour of Orchestrator."
55,Contents
55,1 Service
55,2 Features
55,2.1 Reports
55,2.2 Replication tree
55,2.3 Host viewer
55,3 Service dependencies
55,4 See also
55,5 External link
55,Service
55,"Front end (vanilla PHP) runs on dbmonitor1001,2001.wikimedia.org; scripts in /srv/tendril/"
55,Backend database presently on db1115.eqiad.wmnet (former DB was db1011.eqiad.wmnet) using MariaDB event scheduler and FEDERATEDX engine.
55,Uses a pull method for monitoring with one watchdog database client connection to each host.
55,Entirely and intentionally separate from ganglia and mediawiki-config to allow sanity checks both ways.
55,Uses Labs LDAP for authentication (restricted to people with NDA).
55,Features
55,"It houses a lot of functionality, but here are a few highlighted features:"
55,Reports
55,https://tendril.wikimedia.org/report
55,"There are many different report views, such as:"
55,"Slow queries. E.g. Slow queries from MediaWiki requests, slow analytics queries (from user 'research')"
55,An ad-hoc cluster-wide INFORMATION SCHEMA view (mostly standard schema plus server_id field).
55,Cluster-wide query process list close to real time (10s intervals).
55,Replication tree
55,Based on actual reported configuration (no dependence on coredb or wmf-config/db-eqiad.php)
55,https://tendril.wikimedia.org/tree
55,Host viewer
55,https://tendril.wikimedia.org/host
55,"Information about each database host (e.g. IP, RAM, uptime) and interactive graphs and charts indicating their recent query activity and various InnoDB statistics."
55,Also linked to from most other reports.
55,Service dependencies
55,"Tendril, or more accurately, its host and mariadb database server instance (as of Oct 2019, db1115 is the active host for it), provides the following services, documented here so that on maintenance, we know they will be affected:"
55,"Tendril database monitoring, as documented on this page and serving queries from url https://tendril.wikimedia.org"
55,"Dbtree, public version of tendril tree view at https://dbtree.wikimedia.org"
55,"Both dbtree and tendril are served by dbmonitor frontend, and use the tendril schema"
55,"Zarcillo database inventory, used manually by DBAs (dashboard url to be developed, some cli tools on cumin hosts)"
55,"Database backups metadata, started on cumin + dbprov hosts, and using zarcillo and alerting on backup status on icinga (db1115)"
55,"Systemd unit on 2+2 Grafana servers on eqiad and codfw, automatically adding production databases to monitoring from zarcillo (alerting on icinga)"
55,These last 3 use the zarcillo schema
55,See also
55,MariaDB/monitoring
55,External link
55,https://tendril.wikimedia.org
55,"Retrieved from ""https://wikitech.wikimedia.org/w/index.php?title=Tendril&oldid=2116373"""
55,Categories: ArchiveServicesBot and monitoringMySQL
55,Navigation menu
55,Personal tools
55,EnglishLog in
55,Namespaces
55,PageDiscussion
55,English
55,Views
55,ReadView sourceView history
55,More
55,Search
55,Navigation
55,Main pageRecent changesServer admin log: ProdAdmin log: RelEngIncident statusDeploymentsSRE Team Help
55,Cloud VPS & Toolforge
55,Cloud VPS portalToolforge portalRequest VPS projectAdmin log: Cloud VPS
55,Tools
55,What links hereRelated changesSpecial pagesPermanent linkPage informationCite this page
55,Print/export
55,Create a bookDownload as PDFPrintable version
55,"This page was last edited on 29 September 2023, at 19:08."
55,Text is available under the Creative Commons Attribution-ShareAlike License;
55,additional terms may apply.
55,See Terms of Use for details.
55,Privacy policy
55,About Wikitech
55,Disclaimers
55,Code of Conduct
55,Developers
55,Statistics
55,Cookie statement
55,Mobile view
57,Master MariaDB Database Administration Online Course | Learn SQL
57,Germany
57,About us
57,About us
57,EEDS (Placements)
57,Corporate Training
57,Leadership
57,Awards and Recognitions
57,Hire Koenig Trainers
57,What's New
57,Career@Koenig
57,Flexi
57,Payment Methods
57,Course Catalogue
57,All Courses
57,Microsoft
57,PL-300T00: Microsoft Power BI Data Analyst
57,AZ-104T00-A: Microsoft Azure Administrator
57,AZ-900T01: Microsoft Azure Fundamentals (1 day)
57,AZ-305T00: Designing Microsoft Azure Infrastructure Solutions
57,DP-203T00: Data Engineering on Microsoft Azure
57,AZ-400T00-A: Designing and Implementing Microsoft DevOps solutions
57,AI-102T00: Designing and Implementing a Microsoft Azure AI Solution
57,AZ-500: Microsoft Azure Security Technologies
57,MS-900T01-A: Microsoft 365 Fundamentals
57,AZ-204T00: Developing Solutions for Microsoft Azure
57,Microsoft Power Platform Fundamentals
57,AI-900T00: Microsoft Azure AI Fundamentals
57,SC-200T00: Microsoft Security Operations Analyst
57,MB-300T00: Microsoft Dynamics 365: Core Finance and Operations
57,MB-800T00: Microsoft Dynamics 365 Business Central Functional Consultant
57,VMware
57,"VMware vSphere: Install, Configure, Manage [V8]"
57,"VMware NSX: Install, Configure, Manage [V4.0]"
57,VMware vSphere with Tanzu: Deploy and Manage [V7]
57,VMware NSX: Troubleshooting and Operations [V4.x]
57,VMware HCX: Management and Operation
57,VMware Horizon 8: Deploy and Manage plus App Volumes Fast Track
57,"VMware NSX-T Data Center: Install, Configure, Manage [V3.2]"
57,VMware vRealize Automation SaltStack SecOps: Deploy and Manage [V8.6]
57,"VMware Aria Automation: Install, Configure, Manage [V8.10]"
57,"VMware NSX Advanced Load Balancer: Install, Configure, Manage [V21.x]"
57,VMware NSX: Design [V4.x]
57,VMware Spring: Core
57,VMware NSX-T Data Center: Troubleshooting and Operations [V3.2]
57,VMware vRealize Automation: Orchestration and Extensibility [V8.6]
57,"VMware vRealize Operations: Install, Configure, Manage [V8.6]"
57,AWS
57,AWS Certified Solutions Architect - Associate (Architecting on AWS)
57,AWS Certified Cloud Practitioner ( AWS Cloud Practitioner )
57,AWS Certified Developer – Associate (Developing on AWS)
57,AWS Certified Solutions Architect – Professional ( Advanced Architecting on AWS )
57,AWS Certified Security – Specialty ( Security Engineering on AWS )
57,AWS Certified DevOps Engineer – Professional ( DevOps Engineering on AWS )
57,AWS Certified Sysops Administrator - Associate (Cloud Operations on AWS)
57,AWS Technical Essentials
57,AWS Security Essentials
57,Developing Serverless Solutions on AWS
57,AWS Certified Database - Specialty (Planning and Designing Databases on AWS)
57,The Machine Learning Pipeline on AWS
57,MLOps Engineering on AWS
57,Data Warehousing on AWS
57,Authoring Visual Analytics Using Amazon QuickSight
57,Oracle
57,Oracle Database 19c: Administration Workshop
57,Java Spring Boot
57,Oracle Database 19c: RAC Administration Workshop
57,Oracle GoldenGate 19c: Fundamentals for Oracle
57,Oracle WebLogic Server 14c: Administration I
57,Oracle WebLogic Server 14c: Administration II
57,Oracle Exadata Database Machine: Implementation and Administration
57,Oracle Cloud Infrastructure Architect Associate
57,Java SE 8 Fundamentals Ed 1
57,Oracle Database 19c: Backup and Recovery
57,Oracle Database 19c: Multitenant Architecture
57,Oracle Database 19c: PL/SQL Workshop
57,Oracle Database 12c: Implement Partitioning
57,Oracle WebLogic Server 12c: Administration I Ed 3
57,Java SE 8 Programming Ed 1
57,EC-Council
57,Certified Ethical Hacker v12 - CEHv12
57,Certified Penetration Testing Professional - CPENT
57,Certified Chief Information Security Officer( CCISO )
57,Certified Application Security Engineer .NET
57,Certified SOC Analyst-CSA
57,EC-Council Disaster Recovery Professional v3
57,CHFI V10
57,EC-Council Certified Incident Handler (ECIH V2)
57,Certified Threat Intelligence Analyst (CTIA)
57,Certified Network Defender (CNDv2)
57,Advanced Network Defense
57,EC-Council Certified Security Specialist (ECSS)
57,Certified Blockchain Professional
57,Certified Application Security Engineer JAVA
57,Certified advanced Penetration Tester
57,AXELOS
57,ITIL® 4 Foundation
57,"ITIL® 4 Specialist Create, Deliver and Support"
57,ITIL® 4 Digital and IT Strategy
57,"ITIL® 4 Strategist: Direct, Plan, and Improve"
57,PRINCE2® 6th Edition Foundation & Practitioner
57,PRINCE2® Agile Foundation
57,PRINCE2® 6th Edition
57,Foundation
57,PRINCE2® 6th Edition Practitioner
57,PRINCE2® Agile Foundation and Practitioner
57,ITIL® 4 Specialist Drive Stakeholder Value
57,ITIL® 4 Specialist High-velocity IT
57,"ITIL® 4 Specialist: Monitor, Support and Fulfil"
57,P3O Foundation
57,PRINCE2® agile practitioner
57,ISACA
57,Certified Information Security Manager (CISM)
57,Cobit 2019 Foundation
57,Certified Information Systems Auditor-CISA
57,CRISC
57,Certified in the Governance of Enterprise IT (CGEIT)
57,COBIT 2019 Design and Implementation
57,Certified Data Privacy Solutions Engineer-CDPSE
57,Certificate of Cloud Auditing Knowledge (CCAK)
57,IT Risk Fundamentals
57,Cloud Fundamentals Certificate
57,COBIT-5-Assessor
57,Implementing NIST Cyber Security Framework using COBIT 2019
57,Cyber Security Audit
57,CSX Fundamentals
57,COBIT5 Foundation
57,Red Hat
57,Red Hat System Administration I (RH124) – RHEL 9
57,Red Hat OpenShift Administration I: Operating a Production Cluster
57,Red Hat System Administration II (RH134) – RHEL 9
57,RHCSA Rapid Track (RH199) – RHEL 9
57,Red Hat OpenShift Administration II: Configuring a Production Cluster (DO280)
57,Red Hat System Administration III: Linux Automation (RH294) – RHEL 9
57,Red Hat OpenShift Developer II: Building Kubernetes Applications (DO288)
57,Apache and Secure Web Server Administration
57,Red Hat High Availability Clustering
57,Red Hat JBoss Application Administration I (AD248)
57,Red Hat Security: Identity Management and Active Directory Integration (RH362)
57,Red Hat OpenShift Administration III: Scaling Kubernetes Deployments in the Enterprise
57,Red Hat Ceph Storage for OpenStack - CL260
57,Developing Advanced Automation with Red Hat Ansible Automation Platform
57,Red Hat OpenShift Development I: Introduction to Containers with Podman
57,PECB
57,ISO 27001 (ISMS) Lead Implementer
57,ISO 27001 (ISMS) Lead Auditor
57,ISO/IEC 20000 (ITSM) Lead Implementer
57,Certified Data Protection Officer : CDPO (includes GDPR)
57,ISO 31000 Risk Manager
57,ISO 9001 (QMS) Lead Auditor
57,ISO 31000 Lead Risk Manager
57,ISO 22301 (BCMS) Lead Auditor
57,ISO 20000 (ITSM) Lead Auditor
57,ISO/IEC 27001 Foundation
57,ISO 37301 Lead Implementer
57,Certified ISO 22000:2018 FSMS Lead Auditor
57,ISO 20400 Lead Manager
57,ISO 22301 (BCMS) Lead Implementer
57,ISO 45001 Lead Auditor
57,CompTIA
57,CompTIA-SY0-601-Security+
57,CompTIA Network+
57,(N10-008)
57,CompTIA A+ 1101-1102
57,CompTIA Cybersecurity Analyst (CySA+)
57,CompTIA Data+ DA0-001
57,CompTIA Pentest+ ( PT0-002)
57,CompTIA Project+ PK0-005
57,CompTIA Advanced Security Practitioner (CASP+)(CAS-004)
57,IT Fundamentals+
57,CompTIA Cloud+ CV0-003
57,CompTIA Server+ (SK0-005)
57,CompTIA Pentest+
57,CompTIA Cloud Essentials+
57,CompTIA A+ 1001-1002
57,PROJECT+
57,(ISC)2
57,Certified Information Systems Security Professional (CISSP)
57,Certified Cloud Security Professional
57,(CCSP)
57,"Certified in Governance, Risk and Compliance (CGRC)"
57,Certified Secure Software Lifecycle Professional (CSSLP)
57,Systems Security Certified Practitioner (SSCP )
57,CISSP-ISSAP
57,Certified in Cybersecurity
57,CISSP-ISSEP
57,HCISPP
57,CISSP-ISSMP
57,Certified Authorization Professional (CAP)
57,PMI
57,Project Management Professional (PMP)® Certification Prep
57,PMI Agile Certified Practitioner (PMI-ACP)®
57,PMI Professional in Business Analysis (PMI-PBA)®
57,Certified Associate in Project Management (CAPM)® Certification Prep
57,Program Management Professional (PgMP)®
57,Portfolio Management Professional (PfMP)
57,PMI-RMP Exam Prep
57,Project Management for Software Development
57,Disciplined Agile® Senior Scrum Master (DASSM)
57,Disciplined Agile® Value Stream Consultant (DAVSC)
57,Disciplined Agile® Coach (DAC)
57,Disciplined Agile® Scrum Master (DASM)
57,Understanding Project Budget and Accounting
57,Project Portfolio Management
57,PMI Scheduling Professional (PMI-SP)
57,SAP
57,ADM100 - System Administration I of SAP S/4HANA and SAP Business Suite
57,ADMCLD - Introduction to SAP Business Technology Platform (BTP) Administration
57,TS410
57,Integrated Business Processes in SAP S/4HANA
57,Introduction to SAP Analytics Cloud
57,SAP Ariba Procurement: Administration - AR720
57,Management Accounting in SAP S/4HANA - Academy Part I - TS4F03
57,GRC300 - SAP Access Control Implementation and Configuration
57,SAP Global Trade Services Overview - GTS100
57,Configuring SAP Global Trade Services - GTS200
57,TS450 - Sourcing and Procurement in SAP S/4HANA - Academy Part I
57,C4H340 - SAP Commerce Cloud Developer Part 1
57,C4H341 - SAP Commerce Cloud Developer Part 2
57,TSCM50 SAP ERP Procurement Academy Part I
57,TSCM52 SAP ERP Procurement Academy Part II
57,SAPTEC Technology Fundamentals of SAP S/4HANA and SAP Business Suite
57,F5-ASM/WAF
57,F5 BIG IP LTM
57,F5 BIG-IP ASM: Application Security Manager
57,F5 BIG IP GTM/DNS
57,F5 Administering BIG-IP
57,F5 BIG-IP Local Traffic Manager Configuration
57,F5 BIG-IP Advanced Web Application Firewall Configuration
57,F5 Networks Configuring BIG-IP LTM - Local Traffic Manager
57,BIG-IP APM
57,Configuring BIG-IP ASM: Application Security Manager
57,Securing Applications with NGINX+
57,Securing Apps with F5 Solutions
57,Developing iRules for BIG-IP v15
57,Configuring BIG-IQ
57,Automating BIG-IP with Ansible
57,Koenig Originals Courses
57,Rare Courses
57,Our Partners
57,Contact Us
57,Login
57,About us
57,About us
57,EEDS (Placements)
57,Corporate Training
57,Leadership
57,Awards and Recognitions
57,Hire Koenig Trainers
57,What's New
57,Career@Koenig
57,Flexi
57,Payment Methods
57,Course Catalogue
57,Our Partners
57,Contact Us
57,Login
57,Search for a Course
57,Search for a Technical Answer
57,Start Search
57,All
57,Fundamental Series
57,Microsoft
57,Security
57,VMware
57,Oracle
57,RedHat
57,EC-Council
57,AWS
57,Open Source
57,MariaDB Database Administration
57,Master MariaDB Database Administration: Comprehensive Online Course
57,Download Course Contents
57,Overview
57,Schedule & Fees
57,Course Benefits
57,Student Feedback
57,FAQ
57,Course Modules
57,MariaDB Database Administration Course Overview
57,"The MariaDB Database Administration certification denotes proficiency in managing, optimizing, and securing the MariaDB database - an open-source relational database management system. It ensures professionals possess a deep knowledge to efficiently operate complex MariaDB environments. Industries use MariaDB administrators to ensure the database's smooth running, implementing backup strategies, ensuring database security and optimization. The certification tests knowledge of MariaDB server and related technologies like replication, performance tuning, and user management. It plays a significant role in setting up, securing, and maintaining MariaDB platforms, increasing organizational efficiency and reliability. It is a globally accepted standard of MariaDB expertise."
57,This is a Rare Course and it can be take up to 3 weeks to arrange the training.
57,Certificate Insurance 2nd Shot Free
57,Koenig will pay your 2nd attempt if you fail 1st attempt
57,Cost of 1st attempt is not included
57,Related Qubits questions must be attempted with >80% score
57,Can be availed within three months of end of training
57,Fee will be 50% of the associated exam
57,The 1-on-1 Advantage
57,Get 1-on-1 session with our expert trainers at a date & time of your convenience.
57,Flexible Dates
57,"Start your session at a date of your choice-weekend & evening slots included, and reschedule if necessary."
57,4-Hour Sessions
57,Training never been so convenient- attend training sessions 4-hour long for easy learning.
57,Destination Training
57,"Attend trainings at some of the most loved cities such as Dubai, London, Delhi(India), Goa, Singapore, New York and Sydney."
57,USP Video
57,Live Online Training
57,(Duration : 40 Hours)
57,3300
57,"2,640"
57,If you accept merging of other students.
57,Per Participant & excluding VAT/GST
57,3300
57,1-on-1 Public
57,1-on-1 Private
57,We Offer :
57,1-on-1 Public - Select your own start date. Other students can be merged.
57,1-on-1 Private - Select your own start date. You will be the only student in the class.
57,4 Hours
57,8 Hours
57,Week Days
57,Weekend
57,Start Time : At any time
57,12 AM
57,12 PM
57,Select Time Zone
57,Training Schedule:
57,1-On-1 Training is Guaranteed to Run (GTR)
57,Group Training
57,3300
57,Date On Request
57,Request More Information
57,Enroll Now
57,Book Now
57,+ If you accept merging of other students.
57,Suggested Courses
57,Become an Expert in Business Communication with our Course
57,Hone Your Skills with our Certified Secure Coding for Software Developers (CSCSD) Course
57,Master CI/CD with Ansible and Terraform: Comprehensive Online Course
57,Comprehensive Document Controller Training Course Online
57,Learn the Basics of Good Email Etiquette With Our Email Etiquette Course!
57,Learn OpenAI & GitHub: Comprehensive Guide for End-Users Course Introduction
57,Master Python for Machine Learning: A Comprehensive Course
57,React JS Certification and Training Course
57,Course Prerequisites
57,"• Basic knowledge of SQL and Oracle DB• Understanding of relational database concepts • Familiarity with MariaDB fundamentals, commands, and operations• Experience in setting up and managing MariaDB databases • Familiarity with XtraBackup and MaxScale• Knowledge of MariaDB High Availability and Security strategies.MariaDB Database Administration Certification Training OverviewMariaDB Database Administration certification training is a comprehensive course designed for database professionals to gain mastery in managing MariaDB, an open-source relational database system. The training covers a wide array of topics including installation and configuration of MariaDB, security management, database backup and recovery, performance tuning, and query optimization. It also involves understanding MariaDB architecture, data types and indexes, table maintenance and server monitoring. Upon completion, participants will be fully equipped with the skills necessary to efficiently administer MariaDB databases.Why Should You Learn MariaDB Database Administration?Learning MariaDB Database Administration entails benefits such as expanding employment opportunities, increasing earning potential, and improving data management skills. The course equips individuals with knowledge on server optimization, security enhancement, and troubleshooting, making them valuable assets in data-intensive industries. Furthermore, mastering MariaDB, an open-source RDBMS, can facilitate efficient handling of large datasets."
57,"Target Audience for MariaDB Database Administration Certification Training- Database administrators looking to enhance their skills - IT professionals wanting to learn database management - Computer science students seeking practical database knowledge - Developers looking to understand MariaDB administration - SQL experts aiming to broaden their database proficiency - Tech enthusiasts interested in database administration using MariaDB.Why Choose Koenig for MariaDB Database Administration Certification Training?- Training under a certified instructor with industry experience in MariaDB Database Administration- Opportunity to boost your career through knowledge enhancement and skill acquisition- Customized training programs to suit individual's specific learning requirements- Access to destination training, facilitating an immersive learning experience- Affordable pricing structures allowing people from diverse financial backgrounds to participate- Being trained by a top-rated global training institute- Flexible dates for training sessions, offering convenience to participants- Option of instructor-led online training, allowing remote learning- Offering a wide range of courses to choose from- Assured accredited training that is globally recognized and valued.MariaDB Database Administration Skills MeasuredAfter completing MariaDB Database Administration certification training, individuals gain skills such as installing and configuring MariaDB, understanding MariaDB architecture, managing data and transactions using MariaDB, performing backup and recovery operations, and writing stored programs and queries. They also learn to secure MariaDB instances, use advanced features like clustering and replication, monitor performance and troubleshoot common problems, and maintain database performance by optimizing queries. They would be equipped with the complete knowledge and skills required to administer MariaDB databases efficiently.Top Companies Hiring MariaDB Database Administration Certified ProfessionalsCompanies like IBM, Microsoft, and Oracle are at the forefront of hiring MariaDB Database Administration certified professionals. These companies demand extensive knowledge in MariaDB for maintaining their substantial database systems. Other prominent companies, including Deloitte and Accenture, also highly seek these certified experts enhancing their IT infrastructure and data management.Learning Objectives - What you will Learn in this"
57,"MariaDB Database Administration Course?The main objectives of a MariaDB Database Administration course would include providing students with a comprehensive understanding of the MariaDB database system - it's architecture, usage, and administration. Students would be learning how to install and configure MariaDB, manage users and their permissions, use SQL effectively, and ensure the database's security and performance. The course would also impart skills needed to effectively backup and restore databases, troubleshoot errors, and implement replication and clustering for high availability and performance. The goal is to equip students with all the essential skills needed for efficient and effective database administration using MariaDB."
57,"I am interested but not right now, Keep me updated about offers/webinars. Subscribe me to your Newsletter."
57,Subscribe
57,Trending Technologies
57,Microsoft Azure
57,Security
57,Management
57,Technology
57,Computer Security
57,Security Engineer
57,Security Analyst
57,Azure Infrastructure
57,Problem Solving
57,Cyber Security
57,FAQ's
57,Is there Hands-on training?
57,"Yes, course requiring practical include hands-on labs."
57,How can we pay? Do you have a payment link?
57,"You can buy online from the page by clicking on ""Buy Now"". You can view alternate payment method on payment options page."
57,How will I receive the certificate after attending the course?
57,You will receive the letter of course attendance post training completion via learning enhancement tool after registration.
57,What is the difference between 1-on-1 Public and 1-on-1 Private?
57,1-on-1 Public - Select your start date. Other students can be merged.
57,1-on-1 Private - Select your start date. You will be the only student in the class.
57,Can I see trainers profile before the training?
57,Yes you can.
57,Do you provide self-paced videos?
57,"Yes, we do. For details go to flexi"
57,What payment options are available?
57,You can pay through debit/credit card or bank wire transfer.
57,Can I request for a demo class before Registering?
57,Yes you can request your customer experience manager for the same.
57,Can I pay from website?
57,"Yes, you can pay from the course page and flexi page."
57,Is this website Secure?
57,"Yes, the site is secure by utilizing Secure Sockets Layer (SSL) Technology. SSL technology enables the encryption of sensitive information during online transactions. We use the highest assurance SSL/TLS certificate, which ensures that no unauthorized person can get to your sensitive payment data over the web."
57,Is my information secure?
57,We use the best standards in Internet security. Any data retained is not shared with third parties.
57,"Once I made my payment online, can I cancel it?"
57,You can request a refund if you do not wish to enroll in the course.
57,How do I get a copy of my payment?
57,"To receive an acknowledgment of your online payment, you should have a valid email address. At the point when you enter your name, Visa, and other data, you have the option of entering your email address. Would it be a good idea for you to decide to enter your email address, confirmation of your payment will be emailed to you."
57,How will I know that my payment has been accepted?
57,"After you submit your payment, you will land on the payment confirmation screen.It contains your payment confirmation message. You will likewise get a confirmation email after your transaction is submitted."
57,What types of credit cards are accepted?
57,"We do accept all major credit cards from Visa, Mastercard, American Express, and Discover."
57,How long does it take for a credit card transaction to process if I pay online?
57,"Credit card transactions normally take 48 hours to settle. Approval is given right away; however,it takes 48 hours for the money to be moved."
57,Can I use more than one payment method per transaction?
57,"Yes, we do accept partial payments, you may use one payment method for part of the transaction and another payment method for other parts of the transaction."
57,Can I still send in a paper check?
57,"Yes, if we have an office in your city."
57,Do you offer corporate training?
57,"Yes, we do offer corporate training More details"
57,Do you accept purchase orders?
57,"Yes, we do."
57,Are weekend classes available?
57,"Yes, we also offer weekend classes."
57,Do I have to bring my laptop?
57,"Yes, Koenig follows a BYOL(Bring Your Own Laptop) policy."
57,Do I have to go through the course material before I come to class?
57,It is recommended but not mandatory. Being acquainted with the basic course material will enable you and the trainer to move at a desired pace during classes.You can access courseware for most vendors.
57,"I received an email from ""koenigindia@gmail.com"". Is this Koenig's official email id?"
57,"Yes, this is our official email address which we use if a recipient is not able to receive emails from our @koenig-solutions.com email address."
57,Is there any Installments/EMI option available regarding the course payment?
57,Buy-Now. Pay-Later option is available using credit card in USA and India only.
57,Prices & Payments
57,I am an entitled to claim tax rebate for training expenses. Can I get an invoice for Associated expenses including travel related costs?
57,Yes of course.
57,"Are you open during Christmas, New Year, Ramadan, Other holidays?"
57,"Yes, We are"
57,Travel and Visa
57,Do you provide visa assistance?
57,Yes we do after your registration for course.
57,Food and Beverages
57,Is Halal food available in India?
57,Yes.
57,Others
57,I am worried about the communication skills of the trainers. Do they speak good English?
57,All our trainers are fluent in English . Majority of our customers are from outside India and our trainers speak in
57,a neutral accent which is easily understandable by students from all nationalities.
57,Our money back guarantee also stands for accent of the trainer.
57,We want to train 4 people and would like them in a class to themselves. Is it possible?
57,"Yes, if you send 4 participants, we can offer an exclusive training for them which can be started from Any Date™ suitable for you."
57,What other benefits can I avail when visiting Koenig?
57,Medical services in India are at par with the world and are a fraction of costs in Europe and USA. A number of our
57,"students have scheduled cosmetic, dental and ocular procedures during their stay in India. We can provide advice about this, on request."
57,What is the genesis of the name of your company - Koenig?
57,Says our CEO-
57,“It is an interesting story and dates back half a century.
57,My father started a manufacturing business in India in the 1960's
57,for import substitute electromechanical components such as microswitches.
57,German and Japanese goods were held in high esteem so he named his company
57,Essen Deinki (Essen is a well known industrial town in Germany and Deinki
57,is Japanese for electric company). His products were very good quality and
57,the fact that they sounded German and Japanese also helped. He did quite well.
57,In 1970s he branched out into electronic products and again looked for a German name.
57,"This time he chose Koenig, and Koenig Electronics was born. In 1990s after graduating from college I"
57,was looking for a name for my company and Koenig Solutions sounded just right. Initially
57,we had marketed under the brand of Digital Equipment Corporation but DEC went out of business and we switched
57,to the Koenig name. Koenig is difficult to pronounce and marketeers said it is not a good choice for a B2C brand.
57,But it has proven lucky for us.” – Says Rohit Aggarwal (Founder and CEO - Koenig Solutions)
57,Free Webinar Registration
57,Flexi Enquery for MariaDB Database Administration
57,Fee On Request for MariaDB Database Administration
57,Request Classroom Fee for MariaDB Database Administration
57,Date On Request for MariaDB Database Administration
57,Book a Demo Class
57,WhatsApp Us!
57,Most Popular
57,About us
57,Qubits
57,Join Koenig Alumni New
57,Leadership
57,Career@Koenig
57,Contact Us
57,Upcoming Webinars
57,Payment
57,Methods
57,After Course Support Desk
57,What's New
57,Koenig in Media
57,Employment Exchange for Digital Skills
57,Course Advice Desk
57,Intoxicating Feedbacks
57,Learning Methods
57,Live Online
57,Flexi
57,1-on-1TMTraining
57,Class Room Training
57,Destination Training
57,Fly-me-a-Trainer
57,Hire Koenig Trainers
57,Certification-Oriented Training
57,Technical Questions and Answers
57,EduPass (Subscription Model)
57,Techlabs
57,My Training Mate (2-Hour per Week Training)
57,Sell-IT (for Sales Professionals)
57,Others
57,FAQ's
57,The Story of Koenig Solutions
57,Koenig Ethos
57,Partnerships Solicited
57,Environment Policy
57,Blog
57,Sitemap
57,Seeking Great Trainers
57,"All rights reserved. Â©1997 - 2023, Koenig Solutions Pvt. Ltd."
57,PMP ® is a registered trademark of the Project Management Institute.
57,ITIL ®
57,"and PRINCE2 ® are registered Trademarks of AXELOS Limited,"
57,used under the permission of AXELOS Limited. All rights reserved.
57,Koenig Solutions Pvt. Ltd. is rated 4.4 stars by www.facebook.com/KoenigSolutions based on 107
57,reviews
57,TOGAF ® is a registered trademark of The Open Group.
57,"The APMG International and swirl device logo is a trademark of the APM Group Limited, used under permission of The APM Group Limited. All rights reserved."
57,AgilePM® is a registered trademark of Agile Business Consortium Limited. All rights reserved.
57,"We believe in the philosophy To Err is Human, to Admit Divine! We are not perfect but we are trying. Keep visiting our website, you will see improvements and occasional blunders, Feel free to tell us how we can improve by writing to"
57,webmaster@koenig-solutions.com
57,Sitemap
57,Terms Of Service
57,Happiness Guarantee
57,Privacy Policy
58,Nextсloud Performance Hacks with EverSQL and DataGrip | The DataGrip Blog
58,Blog
58,Skip to content
58,Blogs by Topic
58,Search
58,Burger menu icon
58,IDEs
58,AppCode
58,CLion
58,DataGrip
58,DataSpell
58,Fleet
58,GoLand
58,IntelliJ IDEA
58,PhpStorm
58,PyCharm
58,RustRover
58,Rider
58,RubyMine
58,WebStorm
58,Plugins & Services
58,Big Data Tools
58,Code With Me
58,Quality Assurance
58,JetBrains Platform
58,Scala
58,Toolbox App
58,Writerside
58,Team Tools
58,Datalore
58,Space
58,TeamCity
58,Upsource
58,YouTrack
58,Hub
58,Qodana
58,.NET & Visual Studio
58,.NET Tools
58,ReSharper C++
58,Languages & Frameworks
58,Kotlin
58,Ktor
58,MPS
58,Education & Research
58,JetBrains Academy
58,Research
58,Company
58,Company Blog
58,Security
58,The DataGrip Blog
58,The Cross-Platform IDE for Databases & SQL
58,Follow
58,Follow:
58,Twitter Twitter
58,RSS RSS
58,Download
58,All
58,Releases
58,Tutorials
58,Early Access Program
58,Minor updates
58,News
58,News
58,Tutorials
58,Nextсloud Performance Hacks with EverSQL and DataGrip
58,Pasha Finkelshteyn
58,"Hi, I’m Pasha. In my everyday life, I’m a Developer Advocate for data engineering, but by night, I’m a geek. Like, a really geeky geek! I have used Linux since 2009 as my primary OS (BTW I use Arch). If possible, I always prefer to host everything I use. So if you decide to drop me an email at me@asm0dey.site, rest assured, I’ll receive it on my self-hosted mail server. While self-hosting certainly has its benefits, it does have some downsides, like poorer performance, that should be addressed.This is the story of how I “fought” with self-hosted MySQL to get better database performance."
58,Nextcloud
58,"Frankly, I don’t like Dropbox. For myself and a couple of friends, I self-host Nextcloud, an alternative to Dropbox. Nextcloud is much more powerful, featuring a vast assortment of supported plugins, which the Nextcloud people refer to as “applications”. It has an application for to-do lists, an application for creating polls, and an application for having Google-like Memories and whatnot. It’s written in PHP and (more importantly) uses MySQL as a database."
58,"How many tables could there be in such an application? If I didn’t know any better, I’d say something like “two: users and files”. However, there are also some maintenance and versioning routines, so you probably also have to account for tasks, versions, preferences, and so on. Here’s what it looks like in reality:"
58,"No joke – there are a whopping 199 tables! They mostly don’t have references between them. I think this is a legacy of MyISAM, which didn’t support references at all. By the way, if you know the actual references, you can create them right inside DataGrip with the Create virtual reference action."
58,"What are all those tables? For example, here is everything related to files:"
58,"After careful consideration, we understand why it works this way. File locks are needed should one file be changed from different locations."
58,"Trash, for example, is also just pure metadata when you think about it – it has a different physical location from anything else in the store, and it should be possible to clean the trash entirely, partially, or maybe even recover it!"
58,Here’s what the trash table looks like:
58,"Hopefully, this image alone gives you an idea of how enormous the whole thing is. And since I’m hosting it myself, I’m solely responsible for its performance. This is usually a good thing because I can learn a lot and help a lot of people (after all, knowledge is what makes me a good Developer Advocate). But on the other hand, it means there’s a lot of pressure on me! What if the performance becomes so poor that the server starts failing the people who rely on it?"
58,"If you’re still unsure about the quality of Nextcloud itself, I just want to say that it’s an excellent piece of software! Many people and organizations use it successfully, myself included!"
58,"But three weeks ago, I had some bad luck when my instance of Nextcloud became terribly slow. I didn’t know what happened. I didn’t change anything. “Too much data” would be one possible explanation, but I only have 150 GB of data, so what could have gone wrong?"
58,Debugging
58,"The first step I took was connecting to the database of my Nextcloud. Of course, it’s not exposed to the internet, so I needed to use an SSH tunnel to do this, as shown here:"
58,"It’s so simple, and the configuration even supports parsing of the `~/.ssh/config` file! Then, I entered my login and password:"
58,"I realized the IDE is actually smarter than me: I constantly forget what arguments I should use with SSH to establish an SSH tunnel. I just had to click the “Test connection” link, and:"
58,But you know what’s even more impressive? This small notification:
58,"To be honest, I sometimes forget that I use MariaDB, not MySQL. And I didn’t even know that it has its own JDBC driver. You bet I want it!"
58,"As a second step, I tried to understand what was causing my instance of Nextcloud to run so slowly. It’s simple in MariaDB – even more straightforward than in Postgres (I’m much more familiar with it than with DBs that resemble MySQL):"
58,SHOW PROCESSLIST;
58,"Since I would rather not have to memorize vendor-specific clauses, I created a virtual view in DataGrip:"
58,What did I see in the output? Something like:
58,UPDATE
58,`oc_authtoken`
58,SET
58,`last_activity` = 1680386487
58,WHERE
58,(`id` = 16684)
58,AND (`last_activity` < 1680386472);
58,"I didn’t have any idea what it meant, and to be honest, I wasn’t sure how to optimize it. I wanted to start by checking indexes."
58,"In my fevered imagination, having an index on the ID should make this query instantaneous. And yet, I was looking at this query for almost a minute!"
58,"When I realized that this was happening with other queries, too, I started googling. But beyond the obvious “check indexes,” there was almost no information. I then started googling things like “MySQL query optimizer”. Of course, the first few results were about EXPLAIN and ANALYZE, but frankly, they’re alien to me in MySQL. Even the nice visual representation I could see in EXPLAIN didn’t help me much:"
58,"But then, a miracle was found. And its name was EverSQL."
58,EverSQL
58,"EverSQL is a SaaS SQL optimizer for several databases, including MySQL and its flavors, as well as Postgres and its flavors. When I signed up, I got one credit for free. Credits are a virtual currency for “buying” single optimizations. What is required for optimization? Two things:"
58,"The query you want to improve (in my case, it was that UPDATE statement)."
58,"A database description in a special format: EverSQL generates a crazy SQL query that I won’t even paste here. The query produces a huge JSON with the full description of the database: tables, columns, constraints, and indexes. No actual data goes to EverSQL, so you can rest assured that your data is not being shared."
58,"The story wouldn’t be any fun without a few failures, right? Well, I managed to screw up the second step 🤦🏽. I issued that huge SQL query on the wrong console and got a schema for a completely different database. Be careful, do not repeat my mistakes!"
58,"But I was brave and lucky, and when the team from EverSQL (the co-founders, no less) dropped me a line and asked if I had been able to solve my optimization question, I didn’t blindly delete the message (as sometimes I do with marketing emails). Instead, I told them my sad story, and they were kind enough to give me several credits in order to experiment with EverSQL. On the second try, I was careful and issued the correct query in the correct DB schema. They even gave me some advice on how to improve my query. I just needed to add a new index. I no longer have the original recommendation, but it was something like this:"
58,CREATE OR REPLACE INDEX oc_authtoken_idx_id_last_activity
58,"ON oc_authtoken (id, last_activity);"
58,"While it may seem obvious to those who know what’s going on, it’s still a mystery to me why this recommendation improves anything. However, the following chart tells you everything you need to know:"
58,"This is the average execution time. We can see that it hasn’t spiked all that much recently, but at one point it was around nine seconds. No amount of indexes could save me if the disk was very busy. It’s also worth noting that, if a lot of slow queries run simultaneously, they can negatively affect each other. So I needed a more systematic approach to improve my database performance."
58,"For the usual code, we should run static analysis tools as frequently as possible to control certain code health metrics. The same goes for databases: We should monitor performance. I usually have some performance monitoring tools installed on my self-hosted applications, but not for MySQL. Well, I could check whether there was sufficient space on my spinning drive and enough memory. But that was definitely not enough."
58,This is where the second exciting feature of EverSQL came into play: sensors!
58,EverSQL sensors
58,"A sensor is a small, non-intrusive daemon that monitors slow log, CPU, and additional external signals. Basically, the installation consisted of only two steps:"
58,"Changing the MySQL settings to publish a slow log to a particular destination, as described here."
58,Installing the sensor (that is just a Python file) to a specific location.
58,"After that, the sensor sent all the slow queries to EverSQL’s servers so that they could analyze patterns and find bottlenecks. After a couple of weeks of reporting and some optimizations, the resulting dashboard looked like this:"
58,"Do you see the improvement basically everywhere? Hopefully, it’s the result of adding approximately 8 indexes recommended by EverSQL. Some optimizations appeared later than others, so don’t waste all your credits on the first day!"
58,"Then, after about a week of work, something even more interesting appeared on the dashboard!"
58,"Every “Show me” button turned on a filter on the dashboard. Yes, it recommended not only adding indexes but also deleting some, like these:"
58,I also had to click this checkbox in the settings:
58,"That’s because I don’t actually manage the code of Nextcloud. Otherwise, I would have probably gotten even more recommendations on how to tune my queries to make the database perform even better."
58,But what if you’re writing the code and want it optimized? We’ve got a solution for you. There is an EverSQL plugin for our IDEs!
58,EverSQL plugin for JetBrains IDEs
58,"The actual name of the plugin is “SQL Optimizer,​ Indexing Advisor,​ MySQL,​ PostgreSQL,​ by EverSQL”. It is compatible with the whole spectrum of our IDEs, which is a smart move, since people work with supported databases from almost every language."
58,Here is what it looks like in Marketplace:
58,"Its working principle is straightforward: Just type your query, select it, and in the context menu, choose “Optimize”."
58,"After that, you will be taken to the same interface of EverSQL in your browser with a dialog like this:"
58,"There, you’ll be able to see all the recommendations for optimization immediately."
58,In conclusion
58,"One’s choice of tools is essential. For me, the choice is always apparent – if I can control something, I will. But sometimes having this control exposes gaps in my knowledge, and I have to use other tools to close those gaps."
58,DataGrip helps me be as effective as possible when working with databases.
58,"Nextcloud allows me to take control over my data (and sync my photos to my server, not Google’s),"
58,And EverSQL helps me where I’m just not good enough – in the optimization of my queries in my production. The tool is definitely worth trying.
58,EverSQL
58,mariaDB
58,newsletter
58,NextCloud
58,Share
58,Facebook
58,Twitter
58,Linkedin
58,Prev post DataGrip 2023.1.1DataGrip 2023.1.2 Next post
58,Subscribe to Blog updates
58,Subscribe form
58,"By submitting this form, I agree to the JetBrains Privacy Policy Notification icon"
58,"By submitting this form, I agree that JetBrains s.r.o. (""JetBrains"") may use my name, email address, and location data to send me newsletters, including commercial communications, and to process my personal data for this purpose. I agree that JetBrains may process said data using third-party services for this purpose in accordance with the JetBrains Privacy Policy. I understand that I can revoke this consent at any time in my profile. In addition, an unsubscribe link is included in each email."
58,Submit
58,"Thanks, we've got you!"
58,Discover more
58,DataGrip 2023.3 EAP 2 is Out!
58,"Hello, DataGrip community! The second Early Access Program version for DataGrip 2023.3 is here. If you want to try the new features before the official release, this is for you!"
58,Introspection scheduler
58,It's now possible to set an introspection interval for each data source.
58,Materialized…
58,Alisa Lukonina
58,DataGrip 2023.3 EAP is Open
58,"Hello! We’re starting our 2023.3 Early Access Program (EAP) and, as usual, we invite you to try the latest features and improvements we’re adding to DataGrip ahead of the official release."
58,Let’s take a look at what’s inside the first EAP build.
58,DataGrip takes a long time to introspec…
58,Alisa Lukonina
58,"DataGrip 2023.2: New UI with toolbar icons in the header, AI Assistant, time zones in the data editor, support for Redis Cluster"
58,"DataGrip 2023.2 is out!Here’s a sneak peek at what it has to offer. For a detailed description of this update, please visit our What’s New page."
58,User Interface
58,New UI: The toolbar icons have been moved to the header
58,Improved main toolbar customization
58,Light theme with light …
58,Alisa Lukonina
58,DataGrip 2023.1.1
58,The first update for DataGrip 2023.1 introduces some important fixes:
58,Alisa Lukonina
58,Privacy & Security
58,Terms of Use
58,Legal
58,Genuine tools
58,Twitter
58,Facebook
58,Linkedin
58,Instagram
58,Youtube
58,RSS
58,Tiktok
58,Copyright © 2000 JetBrains s.r.o.
62,Guest VM configuration and tuning | Deploying MariaDB server on Dell PowerFlex | Dell Technologies Info Hub
62,Your Browser is Out of Date
62,Nytro.ai uses technology that works best in other browsers.
62,For a full experience use one of the browsers below
62,Deploying MariaDB server on Dell PowerFlex
62,Executive summary
62,Products overview
62,Solution architecture
62,Solution validation tests
62,Best practices
62,Overview
62,PowerFlex Rack
62,PowerFlex
62,PowerFlex networking and server nodes
62,Guest VM configuration and tuning
62,MariaDB
62,Snapshots
62,Conclusion
62,Appendix
62,References
62,Guest VM configuration and tuning
62,Guest VM configuration and tuning
62,Thank you for your feedback!
62,"Perform the following best practices for the guest VM: Set VM power plan to High Performance. Reserve the memory used by the VMs. Use dedicated para virtual Small Computer System Interface (SCSI) controllers for each attached disk. Use the XFS file system for all database volumes as it performs well on multithreaded environments with a high number of CPUs. Create Virtual Machine Disks (VMDKs) as Eager Zero thick provisioned to improve the performance and security. Enable block multi queue in the VMs. Enable transparent huge pages. Expand resource limits for mysql user. For more information, see MariaDB documentation. Disable swap."
63,RDS for MariaDB - Fully Managed Database | Huawei Cloud
63,检测到您已登录华为云国际站账号，为了您更好的体验，建议您访问国际站服务网站 https://www.huaweicloud.com/intl/zh-cn
63,不再显示此消息
63,Intl-English
63,International
63,English Bahasa Indonesia Español Português Türkçe عربي ไทย 简体中文 日本語
63,中国站
63,简体中文
63,Europe
63,English Deutsch Español
63,Activities
63,Products
63,Solutions
63,Pricing
63,KooGallery
63,Partners
63,Developers
63,Support
63,About Us
63,Hot
63,Free Packages Elastic Cloud Server (ECS) Cloud Backup and Recovery (CBR) Host Security Service (HSS) Cloud Container Engine (CCE)
63,Show more results for “”
63,Contact Us Documentation Console
63,My Account Billing & Costs Service Tickets Unread Messages Console Partner Center
63,Sign In Sign Up
63,Sign In
63,Sign Up
63,admin
63,My Account Billing & Costs Service Tickets Unread Messages Console Partner Center
63,admin Log Out
63,Cancel
63,Hot
63,Free Packages Elastic Cloud Server (ECS) Cloud Backup and Recovery (CBR) Host Security Service (HSS) Cloud Container Engine (CCE)
63,RDS for MariaDB
63,RDS for MariaDB
63,"Effortlessly deploy and scale RDS for MariaDB on the cloud - a fully managed, high-performance relational database service."
63,"Effortlessly deploy and scale RDS for MariaDB on the cloud - a fully managed, high-performance relational database service."
63,Buy Now
63,Access Console
63,Documentation
63,Product Advantages
63,15+ Security Certifications
63,First in China to earn ISO/IEC 27034 and CSA STAR V4 certifications
63,First in China to earn ISO/IEC 27034 and CSA STAR V4 certifications
63,Up to 732-Day Backup Retention
63,"Primary/standby switchover within seconds, lower RTO, automated backup and restoration, backups saved for 732 days, no data lost"
63,"Primary/standby switchover within seconds, lower RTO, automated backup and restoration, backups saved for 732 days, no data lost"
63,Fast Scaling
63,Scale instance specifications and storage in just minutes and view monitoring metrics in real time.
63,Scale instance specifications and storage in just minutes and view monitoring metrics in real time.
63,Why RDS for MariaDB?
63,"Huawei Cloud RDS presents a significant edge over traditional databases. With RDS, you can deploy enterprise-grade databases without any worries about setup, configuration, maintenance, backups, or uptime."
63,More Services at Lower Costs
63,More Services at Lower Costs
63,RDS Instance
63,You only pay for RDS for MariaDB DB instances. There is no hardware or software investment needed.
63,Self-Managed Database
63,"You invest heavily in servers, systems, equipment hosting, and complex O&M."
63,undefined
63,End-to-End Server Security and Data Encryption
63,End-to-End Server Security and Data Encryption
63,Access Control
63,"Configure inbound and outbound rules for security groups hosting RDS DB instances, controlling the network range that can connect to your DB instances."
63,Security Protection
63,Control access with multiple firewalls to keep your data safe.
63,Data Encryption
63,Use Secure Sockets Layer (SSL) to encrypt data during transmission. Use static encryption to encrypt the data to be stored.
63,Operation Audit
63,Cloud Trace Service (CTS) records every operation you perform on your DB instances.
63,undefined
63,Custom Deployment and DR Solutions
63,Custom Deployment and DR Solutions
63,Hot Standby
63,"Leverage the hot standby architecture, in which failover upon fault occurrence takes only a few seconds."
63,Intra-city DR
63,Deploy a DB instance across AZs. AZs are physically isolated but interconnected through an intranet.
63,undefined
63,Scalable Databases that Give You the Flexibility You Need
63,Scalable Databases that Give You the Flexibility You Need
63,A Range of Product Specifications
63,You can create single or primary/standby instances and add read replicas.
63,Elastic Scaling
63,Add or delete read replicas to adapt to sudden surges or sudden drops in traffic.
63,Multiple Instance Classes
63,General-purpose instances offer excellent performance at a low price point by sharing CPU resources on the same physical machine.Dedicated instances offer premium performance by using CPU and memory resources dedicated to only your services.
63,undefined
63,"Save Time, Stay Productive"
63,"Save Time, Stay Productive"
63,Backup and Restoration
63,"Restore data from backups to any point in time, and save data for up to 732 days, with no data loss."
63,Automatic Reminders
63,Define monitoring objects and alarm rules to help you keep track of the statuses of your instances.
63,undefined
63,Application Scenarios
63,Scalability: IoT Apps
63,Reliability: E-Commerce
63,Security: E-Government
63,Performance: Mobile Apps
63,Scalability: IoT Apps
63,IoT
63,"Dedicated instances support high concurrency and throughput, making them an excellent choice for IoT applications."
63,Advantages
63,Compatibility
63,RDS for MariaDB is compatible with MariaDB and MySQL. No changes need to be made on your applications.
63,Excellent Performance
63,Database performance is maintained in high concurrency scenarios.
63,Process Huge Numbers of Read Requests
63,Create additional read replicas in a region to increase application throughput.
63,Related Services
63,OBS
63,ECS
63,Reliability: E-Commerce
63,Online Stores and Mobile E-Commerce
63,"RDS for MariaDB is a reliable, cost-effective database for e-commerce and mobile-commerce applications, enabling your applications to handle spikes in traffic and sales without downtime or data loss."
63,Advantages
63,No Data Lost
63,"Replication to the standby instance is performed with enhanced semi-synchronous replication, ensuring zero data loss under heavy loads."
63,Excellent Performance
63,Database performance is maintained in high concurrency scenarios.
63,Cost-effective
63,"RDS for MariaDB is open source, making it an inexpensive choice for enterprises."
63,Related Services
63,ECS
63,Security: E-Government
63,Digital Public Services
63,RDS for MariaDB can be securely scaled for e-government platforms to handle requests from millions of users.
63,Advantages
63,Feature-rich
63,"A rich suite of business continuity management functions are available, including backup and restoration, metric monitoring, alarm reporting, and primary/standby high availability configurations."
63,Secure
63,"VPCs, security groups, and SSL connections are used to harden security and protect data."
63,Cost-effective
63,"RDS for MariaDB is open source, making it an inexpensive choice for enterprises."
63,Related Services
63,OBS
63,ECS
63,WAF
63,Performance: Mobile Apps
63,Mobile Apps and Mobile Gaming
63,Dedicated instances with high availability provide the high performance and reliability needed to run online mobile apps and games with millions of users.
63,Advantages
63,Excellent Performance
63,Database performance is maintained in high concurrency scenarios.
63,Process Huge Numbers of Read Requests
63,Create additional read replicas in a region to increase application throughput.
63,Related Services
63,OBS
63,ECS
63,New Features
63,"May 1, 2023"
63,RDS for MariaDB Open beta testing (OBT)
63,Documentation
63,Getting Started
63,What Is RDS?
63,Advantages
63,Getting Started with RDS for MariaDB
63,Technical Documents
63,Instance Usage Suggestions
63,Instance Connection
63,Performance Tuning
63,FAQs﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿
63,Product Consulting FAQs
63,Resource and Disk Management FAQs
63,Database Connection FAQs
63,Contact Sales After-Sales Self Service
63,Site Terms Privacy Statement
63,Explore Huawei Cloud Why Us Customer Stories Trust Center Legal Press Releases
63,Featured Services Elastic Cloud Server (ECS) Elastic IP (EIP) RDS for MySQL Elastic Volume Service (EVS) MapReduce Service (MRS)
63,Service and Support Documentation Contact Us Public Notices Support Plans Service Health Dashboard
63,Account and Payment Top Up Invoices Billing Center My Account Payment Method
63,Quick Links Huawei Corporate Huawei Enterprise Huawei Consumer Business Huawei Developers
63,"© 2023, Huawei Cloud Computing Technologies Co., Ltd. and/or its affiliates. All rights reserved."
63,Site Terms Privacy Statement
64,Koha Tuning Guide - Koha Wiki
64,Koha Tuning Guide
64,From Koha Wiki
64,Jump to navigation
64,Jump to search
64,See also the article about performance.
64,Contents
64,1 MySQL tuning
64,2 Apache configuration
64,3 Reduce CPU usage of webcrawlers: robots.txt
64,4 See also
64,MySQL tuning
64,# The MySQL database server configuration file.
64,# You can copy this to one of:
64,"# - ""/etc/mysql/my.cnf"" to set global options,"
64,"# - ""~/.my.cnf"" to set user-specific options."
64,# One can use all long options that the program supports.
64,# Run program with --help to get a list of available options and with
64,# --print-defaults to see which it would actually understand and use.
64,# For explanations see
64,# http://dev.mysql.com/doc/mysql/en/server-system-variables.html
64,# This will be passed to all mysql clients
64,# It has been reported that passwords should be enclosed with ticks/quotes
64,"# escpecially if they contain ""#"" chars..."
64,# Remember to edit /etc/mysql/debian.cnf when changing the socket location.
64,# Here is entries for some specific programs
64,# The following values assume you have at least 32M ram
64,# This was formally known as [safe_mysqld]. Both versions are currently parsed.
64,[mysqld]
64,# * Basic Settings
64,# * Fine Tuning
64,key_buffer		= 2000M
64,max_allowed_packet	= 16M
64,thread_stack		= 128K
64,thread_cache_size	= 8
64,# This replaces the startup script and checks MyISAM tables if needed
64,# the first time they are touched
64,myisam-recover		= BACKUP
64,max_connections
64,= 500
64,max_user_connections
64,= 500
64,#table_cache
64,= 64
64,thread_concurrency
64,= 300
64,table_cache
64,= 30000
64,open_files_limit
64,= 65000
64,#innodb_force_recovery =6
64,# * Query Cache Configuration
64,query_cache_limit
64,= 50M
64,query_cache_size
64,= 300M
64,# * Logging and Replication
64,# * InnoDB
64,# InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.
64,# Read the manual for more InnoDB related options. There are many!
64,# You might want to disable InnoDB to shrink the mysqld process by circa 100MB.
64,#skip-innodb
64,innodb_buffer_pool_size=16777216
64,innodb_file_per_table
64,Apache configuration
64,Add that to your configuration file.
64,<IfModule mod_gzip.c>
64,mod_gzip_on yes
64,mod_gzip_dechunk yes
64,mod_gzip_keep_workfiles No
64,mod_gzip_can_negotiate yes
64,mod_gzip_update_static No
64,mod_gzip_temp_dir /tmp
64,mod_gzip_minimum_file_size 512
64,mod_gzip_maximum_file_size 1000000
64,mod_gzip_maximum_inmem_size 1000000
64,mod_gzip_handle_methods GET POST
64,"mod_gzip_item_exclude reqheader ""User-Agent: .*Mozilla/4\..*\["""
64,mod_gzip_item_exclude mime ^image/.*
64,mod_gzip_item_exclude rspheader Content-Type:image/*
64,mod_gzip_item_include file \.js$
64,mod_gzip_item_include mime ^application/x-javascript$
64,mod_gzip_item_include file \.php$
64,mod_gzip_item_include mime ^text/html$
64,mod_gzip_item_include file \.css$
64,mod_gzip_item_include mime ^text/css$
64,</IfModule>
64,<IfModule mod_deflate.c>
64,"# Compress content with type html, text, and css, ..."
64,AddOutputFilterByType DEFLATE text/plain text/html text/xml text/css
64,AddOutputFilterByType DEFLATE application/xml application/xhtml+xml application/rss+xml application/javascript application/x-javascript
64,DeflateCompressionLevel 9
64,# Properly handle old browsers that do not support compression
64,BrowserMatch ^Mozilla/4 gzip-only-text/html
64,BrowserMatch ^Mozilla/4\.0[678] no-gzip
64,BrowserMatch \bMSIE !no-gzip !gzip-only-text/html
64,DeflateFilterNote Input instream
64,DeflateFilterNote Output outstream
64,DeflateFilterNote Ratio ratio
64,"LogFormat '""%r"" %{outstream}n/%{instream}n (%{ratio}n%%)' deflate"
64,<IfModule mod_headers.c>
64,#properly handle requests coming from behind proxies
64,Header append Vary User-Agent
64,</IfModule>
64,</IfModule>
64,SetOutputFilter DEFLATE
64,SetEnvIfNoCase Request_URI \.(?:gif|jpe?g|png)$ no-gzip
64,"<FilesMatch ""\.(jpg|gif|png|css|js$)$"">"
64,"ExpiresDefault ""access plus 7 days"""
64,"Header set Cache-Control ""public"""
64,</FilesMatch>
64,"<FilesMatch ""\.(pl|html?)$"">"
64,"Header set Pragma ""no-cache"""
64,"Header set Cache-Control ""no-cache, must-revalidate, post-check=0, pre-check=0"""
64,Header set Expires 1
64,</FilesMatch>
64,"To enable Apache modules gzip, deflate, expires and headers in Ubuntu:"
64,sudo a2enmod gzip deflate expires headers
64,Reduce CPU usage of webcrawlers: robots.txt
64,Ona typical installation just:
64,cd /usr/share/koha/opac/htdocs/ # Or on /var/lib/koha/INSTANCE/
64,sudo nano robots.txt
64,And add (source):
64,Crawl-delay: 60
64,User-agent: *
64,Disallow: /
64,User-agent: Googlebot
64,Disallow: /cgi-bin/koha/opac-search.pl
64,Disallow: /cgi-bin/koha/opac-showmarc.pl
64,Disallow: /cgi-bin/koha/opac-detailprint.pl
64,Disallow: /cgi-bin/koha/opac-ISBDdetail.pl
64,Disallow: /cgi-bin/koha/opac-MARCdetail.pl
64,Disallow: /cgi-bin/koha/opac-reserve.pl
64,Disallow: /cgi-bin/koha/opac-export.pl
64,Disallow: /cgi-bin/koha/opac-detail.pl
64,Disallow: /cgi-bin/koha/opac-authoritiesdetail.pl
64,Check at <yourserver>/robots.txt
64,Related bug
64,Related thread
64,"Documentation: robotstxt.org, robots.txt on Google"
64,See also
64,Performance
64,"Retrieved from ""https://wiki.koha-community.org/w/index.php?title=Koha_Tuning_Guide&oldid=31750"""
64,Categories: Documentation examplesTips & TricksPerformanceHome > Documentation
64,Home > Documentation
64,Home > Documentation > Documentation examples
64,Koha > Technical > Administration > Performance
64,Tips & Tricks > Tips & Tricks/Customising Notices and Slips > Tips & Tricks
64,Navigation menu
64,Personal tools
64,EnglishLog inRequest account
64,Namespaces
64,PageDiscussion
64,Variants
64,Views
64,ReadView sourceView history
64,More
64,Search
64,koha links
64,Official Koha SiteKoha ManualsKoha Newsletter
64,Navigation
64,Wiki HomeCategoriesAdvanced search currentAdvanced search obsolete archiveRecent changesRandom pageHelp
64,Categories
64,About KohaParticipationDevelopmentDocumentationKoha VersionsKoha & GitKoha ConferencesLegal MattersTips & Tricks
64,Tools
64,What links hereRelated changesSpecial pagesPrintable versionPermanent linkPage informationBrowse properties
64,"This page was last edited on 24 February 2023, at 03:51."
64,Content is available under GNU General Public License 2.0 or later unless otherwise noted.
64,Privacy policy
64,About Koha Wiki
64,Disclaimers
66,MariaDB | Mysmartservice
66,Home
66,Services
66,Databases
66,Amazon Aurora PostgreSQL
66,Microsoft SQL Server
66,PostgreSQL
66,Oracle
66,AWS RDS
66,Azure SQL Database
66,Cassandra
66,IBM Db2
66,DynamoDB
66,Firebird
66,AlloyDB
66,Google Cloud SQL
66,IBM Informix
66,MariaDB
66,MongoDB
66,MySQL
66,PostgreSQL on Docker
66,Redis
66,Redshift
66,SQLite
66,Data Analytics
66,DataOps
66,DevOps
66,Apache Spark
66,Scrum Master
66,Career
66,About Us
66,Contact Us
66,Services
66,MariaDB
66,"MariaDB as a service is a cloud-based database solution that provides organizations with a managed platform to host MariaDB databases. It offers scalability, security, and simplified database administration, allowing businesses to focus on their applications while outsourcing database management tasks to the service provider."
66,"MariaDB as a service is a cloud-based database solution that provides organizations with a managed platform to host MariaDB databases. It offers scalability, security, and simplified database administration, allowing businesses to focus on their applications while outsourcing database management tasks to the service provider."
66,Database Migration
66,Category: Databases
66,Big Bang Database Migration. A big bang migration transfers all data from one source system to a target database in a single operation at a single point in time.Trickle Database Migration.Zero-Downtime Database Migration.
66,Add to Cart
66,$69.99
66,Database security management
66,Category: Databases
66,"We provide professional support in safeguarding their databases against unauthorized access, data breaches, and other security threats. This service includes implementing access controls, encryption, vulnerability assessments, and ongoing monitoring to ensure the confidentiality, integrity, and availability of sensitive data within the database."
66,Add to Cart
66,$69.99
66,"Database installation, configuration, and maintenance"
66,Category: Databases
66,"Company expert assistance in setting up, optimizing, and managing database systems. This comprehensive service covers the entire lifecycle, from the initial installation and configuration to ongoing maintenance tasks, ensuring a well-tuned and secure database environment."
66,Add to Cart
66,$69.99
66,Backup and recovery of database
66,Category: Databases
66,"Effective backup and recovery strategies are integral parts of database management, providing data reliability, business continuity, and peace of mind in the event of unexpected incidents"
66,Add to Cart
66,$69.99
66,Performance tuning and optimization
66,Category: Databases
66,"Performance tuning and optimization as a service provides expert assistance to organizations in enhancing the speed, efficiency, and reliability of their database systems."
66,Add to Cart
66,$69.99
66,"Data modeling, design, and implementation"
66,Category: Databases
66,"Data modeling, design, and implementation as a service offers organizations expert guidance and assistance in structuring their databases for optimal efficiency and usability."
66,Add to Cart
66,$69.99
66,Data migration and integration with other systems
66,Category: Databases
66,"Data migration and integration as a service helps organizations seamlessly transfer and integrate data between various systems and databases. Expert consultants and tools are used to ensure a smooth transition, whether moving data to a new database, consolidating data from multiple sources, or synchronizing data between different systems, enabling efficient data management and access across the organization."
66,Add to Cart
66,$69.99
66,Database capacity planning and scalability management
66,Category: Databases
66,"Database capacity planning and scalability management as a service assist organizations in determining and optimizing their database resource needs to accommodate current and future data demands. Experts analyze usage patterns, performance metrics, and growth projections to ensure the database infrastructure scales effectively, maintaining optimal performance and cost-efficiency as data volumes and user loads increase."
66,Add to Cart
66,$69.99
66,User account management and access control
66,Category: Databases
66,"User account management and access control as a service provide organizations with expert support in managing user privileges and securing database access. This service includes creating, modifying, and revoking user accounts, defining role-based access controls, and implementing robust security measures to safeguard sensitive data and ensure compliance with data privacy regulations."
66,Add to Cart
66,$69.99
66,"Database monitoring, troubleshooting, and issue resolution"
66,Category: Databases
66,"Database monitoring, troubleshooting, and issue resolution as a service offers continuous oversight of database performance, proactive problem identification, and rapid resolution of database-related issues. Experienced professionals use advanced monitoring tools to detect anomalies, diagnose problems, and implement solutions, ensuring the database operates smoothly and minimizing downtime."
66,Add to Cart
66,$69.99
66,Disaster recovery planning and implementation
66,Category: Databases
66,"Disaster recovery planning and implementation a service delivered by expert support, involves designing and implementing comprehensive strategies to safeguard their databases against unexpected disruptions. This service includes risk assessments, backup strategies, replication setup, and the establishment of recovery procedures, ensuring data resilience and rapid restoration during disasters or unforeseen incidents."
66,Add to Cart
66,$69.99
66,Database version upgrades and patch management
66,Category: Databases
66,"Database version upgrades and patch management a service provide with expert assistance in seamlessly transitioning to new database versions and applying critical patches. This service ensures that databases remain secure, stable, and up-to-date, with professionals handling the intricacies of version upgrades, data migration, and the systematic application of patches to minimize downtime and mitigate potential risks."
66,Add to Cart
66,$69.99
66,Documentation of database architecture and design
66,Category: Databases
66,"Documentation of database architecture and design as a service provides organizations with professional assistance in creating comprehensive records outlining the structure, relationships, and configurations of their databases. This service ensures clarity and accessibility of critical information, facilitating efficient database management, troubleshooting, and knowledge transfer within the organization."
66,Add to Cart
66,$69.99
66,Write your requirement
66,Close
66,Add to Cart
66,"At MySmartServices, we understand the critical role that databases play in the success of your"
66,business/Career.
66,Quick Links
66,About Us
66,Contact Us
66,Career
66,Contact Us
66,info@mysmartservice.com
66,SmartServices
66,Northwest Registered Agent LLC
66,"418 Broadway, STE N"
66,"Albany,NY"
66,12207
66,Follow Us:
66,Privacy Policy
66,Terms of Use
66,Refund Policy
67,"Database Administrator in Bala Cynwyd (Philadelphia Area), Pennsylvania United States | Technology - Infrastructure, Support + Engineering at SIG"
67,About
67,Trading
67,Quant Research
67,Technology
67,Campus Programs
67,gamer blog
67,Careers
67,US Capital Markets
67,Institutional Brokerage
67,ETF Group
67,Global Private Equity
67,Structured Capital
67,We're sorry… the job you are trying to apply for has been filled.
67,Maybe you would like to consider the Categories below :
67,Career Site Cookie Settings
67,Personal Information
67,about
67,about
67,approach
67,culture
67,meet our team
67,news + events
67,speaker series
67,beyond work + benefits
67,locations
67,Quant Trading
67,Quantitative Trading
67,Products
67,Game Theory
67,Decision Making in Trading
67,Quant Research
67,Quantitative Research
67,Day of a quantitative researcher
67,Technology
67,Technology
67,Day of a Developer
67,Campus Programs
67,Campus Recruiting
67,Trading
67,Technology
67,Quant Research
67,Trading Operations
67,Research
67,Co-op
67,Search Campus Openings
67,Gamer Blog
67,Careers
67,accounting
67,equity research
67,"growth equity, private equity + venture capital"
67,"human resources, recruiting + marketing"
67,institutional sales
67,legal + compliance
67,Machine Learning
67,operations
67,quantitative research
67,quantitative trading + strategy
67,Sports Analytics
67,technology - hardware engineering
67,"technology - infrastructure, support + engineering"
67,technology - software engineering
67,additional opportunities
67,US Capital Markets
67,Institutional Brokerage
67,ETF Group
67,Global Private Equity
67,Structured Capital
67,Disclosures
67,Privacy Statement
67,"SIG is committed to providing equal opportunity to qualified individuals regardless of sex, age, race, color, religion, creed, marital status, national origin, ancestry, citizenship, military status, veteran status, handicap, disability, gender, sexual preference or orientation, gender identity, political affiliation, genetic information or any other characteristic protected by law."
67,"Copyright © 2020 Susquehanna International Group, LLP. All rights reserved. Susquehanna Financial Group, LLP (SFG), an affiliate of SIG, is a member of"
67,FINRA.
67,a0cb77ba7a5648df98368508e96ed797
71,How to Optimize MySQL Performance using MySQLTuner
71,Skip to content
71,Integrations
71,Data Pipeline
71,Pricing
71,Customers
71,Resources
71,Blog
71,Blog
71,Read about our transformative ideas on all things data
71,Learning Hub
71,Learning Hub
71,Read about our transformative ideas on all things data
71,Success Stories
71,Success Stories
71,Know how our customers use Hevo
71,Guides
71,Guides
71,Study latest technologies with Hevo exclusives
71,Videos
71,Videos
71,Learn how to get started with Hevo
71,Events
71,Events
71,Explore our Webinars and Masterclasses
71,Documentation
71,Documentation
71,Check out Hevo’s extensive documentation
71,API Docs
71,API Docs
71,Get programmatic access to Hevo
71,Documentation
71,Login
71,Start for free
71,Schedule a Demo
71,Home
71,Learn
71,How to Optimize MySQL Performance using MySQLTuner?
71,No-code Data Pipeline for MySQL
71,Easily load data from MySQL and all your sources into a Data Warehouse or a destination of your choice in real-time without writing any code using Hevo!
71,Try Hevo for free
71,How to Optimize MySQL Performance using MySQLTuner?
71,Sanchit Agarwal
71,"Last Modified: December 29th, 2022"
71,"To handle the exponentially increasing data, a growing business often requires a Database Management System that is scalable, effective, reliable, and secure."
71,Table of Contents
71,What is MySQL?Key Features of MySQLSimplify MySQL ETL Using Hevo’s No-code Data PipelineHow to use MySQLTuner for MySQL Performance Optimization?1) Installing MySQLTuner2) Running MySQLTuner3) MySQLTuner RecommendationsFrequently Asked Questions (FAQs)What is MySQL Tuning?What is Table_Open_Cache?What is Key_Buffer_Size?What is Wait_timeout in MySQL?Conclusion
71,"MySQL is one of the popular choices as an Open Source Relational Database Management System. With Standard SQL support, you can quickly query, manipulate & add data to your MySQL Tables."
71,"In order to get the best performance out of your MySQL Database according to your use case, you can use the MySQLTuner command. This is a PERL script that provides you with suggestions to enhance your MySQL performance, security, etc."
71,"In this article, you will learn how to effectively use the MySQLTuner command to optimize your MySQL performance."
71,Table of Contents
71,What is MySQL?Key Features of MySQLHow to use MySQLTuner for MySQL Performance Optimization?Installing MySQLTunerRunning MySQLTunerMySQLTuner RecommendationsFrequently Asked Questions (FAQ)Conclusion
71,What is MySQL?
71,Image Source
71,"MySQL is a popular Open-Source Relational Database Management System. Data in MySQL is stored in tables consisting of rows and columns. It is fully developed, distributed, and maintained by the Oracle Corporation."
71,"From a technical point of view, MySQL is written in C and C++ language. It is compatible with several platforms such as Microsoft Windows, Oracle Solaris, AIX, Symbian, Linux, MAC OS, etc."
71,"MySQL is also an integral part of the Modern LAMP Stack consisting of a Linux-based Operating System, the Apache Web Server, a MySQL Database, and PHP for processing."
71,"Using the Standard SQL(Standard Query Language) commands you can easily define, manipulate, control, and query your data in MySQL tables."
71,"Client-Side GUIs(Graphical User Interface) such as MySQL WorkBench, SequelPro, or DBVisualizer can be used to type in the SQL commands and the server will respond with the requested information."
71,MySQL also offers a paid Enterprise version that comes with premium support services as well as a series of extensions that can be installed as Server Plugins.
71,Key Features of MySQL
71,"MySQL is used to store and retrieve data in a wide variety of popular applications, websites, and services. It offers a collection of eye-catching features that makes it one of the top-performing RDMS:"
71,"Ease of Use: MySQL supports a wide variety of programming languages ​​such as PHP, PERL, C, C ++, and JAVA. The MySQL environment also provides a set of tools to facilitate tasks such as Server Management, Reporting, and Data Analysis. To enable you to work with a wider range of datasets, MySQL fully supports multiple Data structures, JSON and Geospatial data, as well as logical, Numeric, Alphanumeric, Date, and Time data types. Best-in-class Performance: MySQL assures a top-notch query performance with a wide range of clustered servers. It provides fast-loading utilities with distinct memory caches as well as Table Index Partitioning. MySQL can also effectively handle fluctuating workloads and process large amounts of data at optimal speeds. It allows you to store data in 50 million rows or more in a table with a default file size limit for a table of 4GB. However, depending on your Operating System configuration, you can increase it to the theoretical limit of 8 million terabytes (TB). You can also improve your performance by viewing the list of recommendations given by the MySQLTuner command. Open Source: MySQL is licensed under the GNU General Public License (GPL). In other words, MySQL is always free to use. Based on Oracle’s Open-Source MySQL Codebase, you can customize it as per your needs. Because it is Open-Source software, a large public community has been developed that regularly enriches the MySQL documentation and online support culture. Localization: MySQL supports several different character sets including latin1 (cp1252), german, big5, ujis, and more. You can also set the language of error messages that the server provides to clients. The MySQL Server Time can also be changed dynamically, so you can set a specific time zone for each client. Data Security & Protection: SQL allows you to configure data access control settings. You can use powerful mechanisms such as the Access Privilege System and User Account Management to determine who can view or use MySQL data. MySQL sets the bar with Host-based Validation and Password Encryption."
71,Simplify MySQL ETL Using Hevo’s No-code Data Pipeline
71,Hevo Data is a No-code Data Pipeline that offers a fully managed solution to set up Data Integration for 100+ Data Sources (Including 40+ Free sources) and will let you directly load data from sources like MySQL to a Data Warehouse or the Destination of your choice. Hevo also supports MySQL as a Destination for loading Data into it. It will automate your data flow in minutes without writing any line of code. Its fault-tolerant architecture makes sure that your data is secure and consistent. Hevo provides you with a truly efficient and fully automated solution to manage data in real-time and always have analysis-ready data.
71,Get Started with Hevo for Free
71,Let’s look at some of the salient features of Hevo:
71,"Fully Managed: It requires no management and maintenance as Hevo is a fully automated platform.Data Transformation: It provides a simple interface to perfect, modify, and enrich the data you want to transfer. Real-Time: Hevo offers real-time data migration. So, your data is always ready for analysis.Schema Management: Hevo can automatically detect the schema of the incoming data and map it to the destination schema.Connectors: Hevo supports 100+ Integrations to SaaS platforms such as WordPress, FTP/SFTP, Files, Databases, BI tools, and Native REST API & Webhooks Connectors. It supports various destinations including Google BigQuery, Amazon Redshift, Snowflake, Firebolt, Data Warehouses; Amazon S3 Data Lakes; Databricks; and MySQL, SQL Server, TokuDB, DynamoDB, PostgreSQL Databases to name a few.  Secure: Hevo has a fault-tolerant architecture that ensures that the data is handled in a secure, consistent manner with zero data loss.Hevo Is Built To Scale: As the number of sources and the volume of your data grows, Hevo scales horizontally, handling millions of records per minute with very little latency.Live Monitoring: Advanced monitoring gives you a one-stop view to watch all the activities that occur within Data Pipelines.Live Support: Hevo team is available round the clock to extend exceptional support to its customers through chat, email, and support calls."
71,Sign up here for a 14-Day Free Trial!
71,How to use MySQLTuner for MySQL Performance Optimization?
71,MySQLTuner is a read-only script written in PERL. It analyses your MySQL Installation and quickly provides you with a list of recommendations and adjustments for improving performance and stability.
71,"It retrieves the current Configuration Variables and Status Data and displays a complete list of statistics & suggestions associated with performance, security, etc."
71,"It is continuously maintained and supports 300+ Indicator Configurations such as Galera Cluster, TokuDB, Performance schema, Linux OS Metrics, InnoDB, MyISAM, Aria, etc."
71,"To start using MySQLTuner, you can go through the following aspects given below:"
71,Installing MySQLTunerRunning MySQLTunerMySQLTuner Recommendations
71,1) Installing MySQLTuner
71,You can follow the following steps to download and install MySQLTuner:
71,Step 1: You can directly download by executing any of the following commands given below:
71,wget http://mysqltuner.pl/ -O mysqltuner.pl
71,wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/basic_passwords.txt -O basic_passwords.txt
71,wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/vulnerabilities.csv -O vulnerabilities.csv
71,You can also download the complete repository by using git clone or git clone –depth 1 -b master followed by the cloning URL above.
71,Step 2: To run MySQLTuner without calling Perl you can run the following command for changing the script’s permissions to be executable.
71,chmod +x mysqltuner.pl
71,2) Running MySQLTuner
71,"Now that you have installed MySQLTuner, you can run it by following the simple steps given below:"
71,"Step 1: Execute the following command to run MySQLTuner. After this, MySQL will ask you for the administrative login and password."
71,./mysqltuner.pl
71,You can also run MySQLTuner using any existing Perl executable on your system
71,perl mysqltuner.pl
71,Image Source
71,Step 2: You can also apply MySQLTuner according to the following Use Cases:
71,Operating Remotely for Minimal Usage.
71,perl mysqltuner.pl --host targetDNS_IP --user admin_user --pass admin_password
71,Operating Locally for Minimal Usage.
71,perl mysqltuner.pl --host 127.0.0.1
71,Getting the Maximum Output Information around MySQL without Debugging.
71,perl mysqltuner.pl --verbose
71,perl mysqltuner.pl --buffers --dbstat --idxstat --sysstat --pfstat --tbstat
71,Enable CVE(Common Vulnerabilities and Exposures) check for your MySQL Version.
71,perl mysqltuner.pl --cvefile=vulnerabilities.csv
71,Save your MySQLTuner Output in a file with the Information displayed.
71,perl mysqltuner.pl --outputfile /tmp/result_mysqltuner.txt
71,Save your MySQLTuner Result in a file without displaying Information.
71,perl mysqltuner.pl --silent --outputfile /tmp/result_mysqltuner.txt
71,"Based on the Text::Template syntax, Customize your Reporting File using the Template Model."
71,perl mysqltuner.pl --silent --reportfile /tmp/result_mysqltuner.txt --template=/tmp/mymodel.tmpl
71,Toggle On Debugging Information.
71,perl mysqltuner.pl --debug
71,Updating MySQLTuner and the Data Files (password and cve) when necessary.
71,perl mysqltuner.pl --checkversion --updateversion
71,3) MySQLTuner Recommendations
71,The configuration file of MySQL is generally located at /etc/mysql/my.cnf. It is always a good practice to create a backup for your configuration file my.cnf by executing the following command:
71,cp /etc/mysql/my.cnf ~/my.cnf.backup
71,You can start making small changes one at a time based on the suggestions and check the server after each change. Restarting the server after every modification can help you detect anomalies.
71,systemctl restart mysqld
71,"If your system has different init systems, then use the following command:"
71,service mysql restart
71,MySQLTuner can provide you with suggestions associated with the following parameters to increase performance & stability:
71,"Key_buffer: By modifying this, you can allot more memory to MySQL, thereby speeding up your databases. The preferred size should not exceed 25% of the total system memory when using the MyISAM Table Engine, and up to 70 percent for InnoDB. MySQL Documentation suggests setting 64M for servers with 256MB of RAM with many tables. Whereas, the default size of 16M is recommended for Servers with fewer tables and 128MB of RAM.Max_allowed_packet: This allows you to set the maximum size of a sendable packet. This packet can be a single SQL state, a single row being sent to a client, or even a log being sent from a source database to a replica. For situations when your MySQL Server is going to process large packets, you can increase this to the size of your largest packet.Thread_stack: Though MySQL sets the default stack size for each thread, you can increase the size if an error related to the thread_stack is logged.Thread_cache_size: For instance, when you are receiving hundreds of connections per minute, you can increase it so that the majority of connections can be made on cached threads.Max_connections: Using this, you can set the maximum number of concurrent connections. This number can be estimated based on past usage so that there is a buffer between that past upper number and the max_connections parameter.tmp_table_size or max_heap_table_size: While changing this, ensure that you modify both of them and set them equal. Based on the Memory Availability, you can increase them by large chunks since these are global values.Join_buffer_size: It is a good practice to increase it in small increments as it gets multiplied by the max_connections.Innodb_buffer_pool_size: If MySQLTuner suggests increasing this, you can make it large enough to accommodate all your InnoDB databases as this parameter significantly affects your performance."
71,"Always run MySQLTuner after 24 hrs of starting MySQL for accurate results. For cases when the recommendations are not clear, it is advised to consult your DBA or a System Administrator that you trust."
71,Frequently Asked Questions (FAQs)
71,What is MySQL Tuning?
71,"MySQL tuning refers to the process of improving the performance of SQL statements in the database for better, more accurate, and faster results. MySQLTuner can be used to improve the query performances as well as verify the installation."
71,What is Table_Open_Cache?
71,"Table_open_cache defines the maximum number of tables that can be kept open in the cache. This parameter tunes the performance by allowing to re-open tables frequently with ease. Also, the newer tables created will receive the cache memory, hence saving the efforts of memory allocation."
71,What is Key_Buffer_Size?
71,"The key_buffer_size variable determines the amount of memory that can be readily available for the MySQL index. The more the memory, the faster the performance, and this tunes the performance of MySQL queries."
71,What is Wait_timeout in MySQL?
71,"It is the time for which the server waits for inactivity, before closing the connection. It can be defined as the number of seconds the server waits for before receiving a bad handshake."
71,Conclusion
71,"In this article, you have learned how to install, run & effectively use MySQLTuner to optimize your MySQL performance. MySQLTuner reviews your MySQL Installation and provides a list of suggestions to increase your performance & stability."
71,"Before making any changes to your MySQL settings, you can always create a backup of your configuration file to easily revert back to the initial setup in case of an error."
71,"After you have enhanced your MySQL Performance, you can start querying your data at a faster rate. To get a complete picture of your business health & performance, you need to consolidate data from MySQL and all the other applications used across your business for Marketing, Customer Relationship Management, Accounting, Sales, etc."
71,"To achieve this you need to assign a portion of your Engineering Bandwidth to Integrate Data from all sources, Clean & Transform it, and finally, Load it to a Cloud Data Warehouse or a destination of your choice for further Business Analytics."
71,All of these challenges can be comfortably solved by a Cloud-Based ETL tool such as Hevo Data.
71,"If you are using MySQL as your Database Management System and searching for a no-fuss alternative to Manual Data Integration, then Hevo can effortlessly automate this for you."
71,Visit our Website to Explore Hevo
71,"Hevo Data, a No-code Data Pipeline can seamlessly transfer data from a vast sea of 100+ sources such as MySQL to a Data Warehouse or a Destination of your choice to be visualized in a BI Tool. It is a reliable, completely automated, and secure service that doesn’t require you to write any code!"
71,Want to take Hevo for a ride? Sign Up for a 14-day free trial and simplify your Data Integration process.
71,Now that you know how to use MySQLTuner to optimize the performance of your MySQL Database you can also check out this article on Optimizing MySQL Tables and manage the arrangement of data within the database.
71,Tell us about your experience of Optimising MySQL using MySQLTuner! Share your thoughts with us in the comments below.
71,No-code Data Pipeline for MySQL
71,Try for free
71,Database Management SystemsMySQLSQL Commands
71,Related Articles
71,How to Optimize MySQL Tables? | 2023’s Critical Step Guide
71,mysqlcheck: Check and Repair Tables & Databases
71,Continue Reading
71,Suchitra Shenoy
71,Connect PostgreSQL on Amazon RDS to MySQL: 2 Easy Ways to Integrate Data
71,Tejaswini Kasture
71,MySQL on Amazon RDS to Databricks: 2 Easy Methods to Load Data
71,Tejaswini Kasture
71,Connect MySQL on Amazon RDS to Azure Synapse: 2 Easy Ways to Integrate Data
71,Bring Real-Time Data from Any Source into your Warehouse
71,Get Started for Free
71,Talk to a Product Expert
71,Platform
71,Hevo Pipeline
71,Integrations
71,Hevo Pricing
71,Security
71,Request Demo
71,Free Trial
71,Upcoming Features
71,Changelog
71,Hevo Service Status
71,Resources
71,Videos
71,Resources Guide
71,Success Stories
71,Events
71,Blog
71,Learn
71,Documentation
71,API Docs
71,Engineering Blog
71,Sitemap
71,Solutions
71,ETL
71,ELT
71,Amazon Redshift ETL
71,Google BigQuery ETL
71,Snowflake ETL
71,Change Data Capture
71,Data Ingestion
71,Data Replication
71,Data Integration
71,Data Integration Solution
71,Convert SQL Server to MySQL
71,Import Excel File to MySQL
71,Open Source ETL Tools
71,Integrating APIs with SQL Server
71,Migrate MySQL to PostgreSQL
71,NetSuite to Snowflake
71,MySQL Change Data Capture (CDC)
71,MS SQL Server to Snowflake
71,MySQL to BigQuery
71,HubSpot to BigQuery
71,Industry
71,E-commerce
71,Finance
71,Marketing
71,Healthcare
71,Sales
71,Retail
71,Company
71,Contact Us
71,Careers
71,Partners
71,Hevo Design
71,Team
71,Top Connectors
71,MongoDB
71,Google Analytics
71,Facebook Ads
71,PostgreSQL
71,MySQL
71,Top Destinations
71,BigQuery
71,Snowflake
71,Redshift
71,PostgreSQL
71,SQL
71,Popular Integrations
71,Postgres Export to CSV
71,Java Connect to Microsoft SQL
71,Oracle SQL
71,Developer to Excel & CSV
71,Connect Excel to PostgreSQL
71,SQLite to MySQL
71,Our Investors
71,Hevo in News
71,Try Hevo today
71,Book a Demo
71,© Hevo Data Inc. 2023. All Rights Reserved.
71,Privacy policy
71,Terms of Service
71,Free Trial
71,I want to read this e-book
71,Name*
71,Company*
71,Designation* (Select the one that most closely resembles your work.)
71,Please Select
71,Data Engineer/Data Engineer Lead
71,Data Analyst/Analytics Lead
71,Data Scientist/Data Science Lead
71,Analytics Engineer
71,Founder/CXO
71,Developer/Programmer/Lead
71,Marketer/Marketing Lead
71,Other
71,Your Designation*
71,Business Email*
71,Phone Number
71,Download Now
72,MySQL Benchmarks – NFS vs Local Storage – ERRANT MINDS
72,Toggle navigation
72,HOME
72,PROJECTS
72,SERVICES
72,Flint & Steel Sound System
72,BLOG
72,ABOUT
72,MORE ABOUT MAX
72,CONTACT
72,Skip to content
72,ERRANT MINDS
72,HOME
72,PROJECTS
72,SERVICES
72,Flint & Steel Sound System
72,BLOG
72,ABOUT
72,MORE ABOUT MAX
72,CONTACT
72,Search for:
72,Toggle sidebar & navigation
72,"MySQL Benchmarks – NFS vs Local Storageby Max Jacobyin Private Cloudon Posted on February 13, 2023"
72,"Recently, I migrated my containers from a standalone Docker node to Docker Swarm. On the standalone node I used a local VHD for persistent storage, but now I’m using NFS Docker volumes defined in the compose file. I knew I’d take a slight performance hit serving the files over NFS, but I was happy to make that sacrifice to be able to schedule containers on any node in the swarm. It also means I can run the nodes on a separate physical host in my XCP-NG pool and not have to copy several terabytes of VHD."
72,"Apparently, you shouldn’t run databases with NFS as the storage backend. I have a few database containers, so I got wondering just how bad this could be. NFS suffers under high IOPS, and the HDDs that the NFS server is running on suffer under random read/write – which is basically the worst combination for a database. As a quick litmus test, I ran some synthetic benchmarks on a MariaDB container to see what I might be losing."
72,"N.B: This is a poor testing methodology on old crappy hardware. Take these sparse and incomplete results with a fistful of salt. Like I said, this is just a litmus test."
72,MariaDB Containers:
72,version: '3.8'
72,services:
72,db-local:
72,image: mariadb:10.5.13
72,volumes:
72,- /home/max/dbtests:/var/lib/mysql
72,environment:
72,- MYSQL_ROOT_PASSWORD=peepeepoopoo
72,- MARIADB_DATABASE=test
72,ports:
72,- 3310:3306
72,deploy:
72,placement:
72,constraints:
72,"- ""node.labels.number==1"""
72,db-nfs:
72,image: mariadb:10.5.13
72,volumes:
72,- db-nfs:/var/lib/mysql
72,environment:
72,- MYSQL_ROOT_PASSWORD=peepeepoopoo
72,- MARIADB_DATABASE=test
72,ports:
72,- 3311:3306
72,deploy:
72,placement:
72,constraints:
72,"- ""node.labels.number==1"""
72,volumes:
72,db-nfs:
72,driver_opts:
72,"type: ""nfs4"""
72,"o: ""addr=10.11.11.108,nolock,soft,rw,nfsvers=4.2"""
72,"device: "":/mnt/xvdb/dbtests"""
72,"The local database is using the VHD of the swarm node as storage, and the NFS database is using a generously resourced NFS server on a separate physical machine. The local VHD is running on 4 10K SAS drives in RAID0 (so pretty much a best case scenario), and the NFS server is running on 6 10K SAS drives in RAID6. There is a 10 gigabit link between the NFS server and the swarm node running the containers."
72,Synthetic Test Method: Sysbench with
72,Percona Lab’s Benchmark
72,Install:
72,curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash
72,sudo apt -y install sysbench
72,git clone https://github.com/Percona-Lab/sysbench-tpcc /usr/share/sysbench/percona
72,cd /usr/share/sysbench/percona
72,Prepare test:
72,sysbench /usr/share/sysbench/percona/tpcc.lua --mysql-host=10.11.11.100 --mysql-port=3310 --mysql-user=root --mysql-password='xxxxxx' --mysql-db=test --time=300 --threads=10 --report-interval=1 --tables=1 --scale=20 --db-driver=mysql prepare
72,Run test:
72,sysbench /usr/share/sysbench/percona/tpcc.lua --mysql-host=10.11.11.100 --mysql-port=3310 --mysql-user=root --mysql-password='xxxxxx' --mysql-db=test --time=300 --threads=10 --report-interval=1 --tables=1 --scale=20 --db-driver=mysql run
72,"This should run a test with 10 concurrent connection with a database size of around 2GB for 300s. Not too worried about tuning the parameters here, since I’m just looking for the performance delta."
72,Results:
72,Local Storage:
72,SQL statistics:
72,queries performed:
72,read:
72,708780
72,write:
72,735778
72,other:
72,108912
72,total:
72,1553470
72,transactions:
72,54441
72,(181.45 per sec.)
72,queries:
72,1553470 (5177.54 per sec.)
72,ignored errors:
72,250
72,(0.83 per sec.)
72,reconnects:
72,(0.00 per sec.)
72,General statistics:
72,total time:
72,300.0387s
72,total number of events:
72,54441
72,Latency (ms):
72,min:
72,2.67
72,avg:
72,55.11
72,max:
72,2669.28
72,95th percentile:
72,147.61
72,sum:
72,2999980.83
72,Threads fairness:
72,events (avg/stddev):
72,5444.1000/65.92
72,execution time (avg/stddev):
72,299.9981/0.01
72,NFS Storage:
72,SQL statistics:
72,queries performed:
72,read:
72,450370
72,write:
72,467478
72,other:
72,69218
72,total:
72,987066
72,transactions:
72,34594
72,(115.29 per sec.)
72,queries:
72,987066 (3289.66 per sec.)
72,ignored errors:
72,155
72,(0.52 per sec.)
72,reconnects:
72,(0.00 per sec.)
72,General statistics:
72,total time:
72,300.0493s
72,total number of events:
72,34594
72,Latency (ms):
72,min:
72,3.51
72,avg:
72,86.72
72,max:
72,2303.80
72,95th percentile:
72,262.64
72,sum:
72,3000095.22
72,Threads fairness:
72,events (avg/stddev):
72,3459.4000/37.66
72,execution time (avg/stddev):
72,300.0095/0.01
72,"Wow! NFS extracts a pretty heavy toll on overall performance – 35% performance hit across the board, including latency, which is already worryingly high. Hmmm. NFS isn’t looking good, but local isn’t exactly exemplary!! There’s definitely some more digging to be done on this subject. A job for another day though."
72,How about tuning MariaDB a bit?
72,"Adding –innodb-buffer-pool-size=2048M allows MariaDB to cache up to 2GB in memory for much better performance. Not enough to fit the whole dataset in, but I was running out of memory and it should at least give an indication."
72,NFS Results:
72,SQL statistics:
72,queries performed:
72,read:
72,975367
72,write:
72,1011328
72,other:
72,151412
72,total:
72,2138107
72,transactions:
72,75691
72,(252.27 per sec.)
72,queries:
72,2138107 (7126.11 per sec.)
72,ignored errors:
72,337
72,(1.12 per sec.)
72,reconnects:
72,(0.00 per sec.)
72,General statistics:
72,total time:
72,300.0369s
72,total number of events:
72,75691
72,Latency (ms):
72,min:
72,2.53
72,avg:
72,39.63
72,max:
72,608.02
72,95th percentile:
72,132.49
72,sum:
72,2999799.67
72,Threads fairness:
72,events (avg/stddev):
72,7569.1000/50.96
72,execution time (avg/stddev):
72,299.9800/0.01
72,"Ok! That’s more like it. Roughly a 50% improvement over performance straight from storage. This would likely improve if you ran a second test since the data would already be in memory. The results also scaled similarly with the local storage database. This makes me not so worried about my NFS storage. I’m not exactly running high traffic websites, so I really don’t need super performance."
72,Docker Overlay Network Performance Impact
72,"The results above were taken by connecting straight to the docker swarm node that the databases were running on. However, it’s rare that a database container and the web server container will be scheduled on the same node in a swarm, for instance. I’ve casually observed that inter-node routing through the docker overlay network has a BIG performance hit, so I was curious to measure this too. For this test I connected to the second node in the cluster, which re routes the traffic to the first node where the databases are running."
72,NFS storage with –innodb-buffer-pool-size=2048M:
72,SQL statistics:
72,queries performed:
72,read:
72,767978
72,write:
72,796842
72,other:
72,118478
72,total:
72,1683298
72,transactions:
72,59224
72,(197.36 per sec.)
72,queries:
72,1683298 (5609.44 per sec.)
72,ignored errors:
72,272
72,(0.91 per sec.)
72,reconnects:
72,(0.00 per sec.)
72,General statistics:
72,total time:
72,300.0815s
72,total number of events:
72,59224
72,Latency (ms):
72,min:
72,4.02
72,avg:
72,50.66
72,max:
72,551.36
72,95th percentile:
72,158.63
72,sum:
72,3000078.59
72,Threads fairness:
72,events (avg/stddev):
72,5922.4000/39.31
72,execution time (avg/stddev):
72,300.0079/0.02
72,"All over the shop! Big reduction in queries per second and 10ms increase in average latency. That’s a bit rubbish. My lateral move is clearly not as lateral as I thought – it’s a sizable downgrade! Not only that, but it’s massively inconsistent. As it was running I could see individual values fluctuating all over. Really not very impressive."
72,Look at the fluctuations in those QPS!
72,"I’ve noticed high utilisation on a single core on a swarm node when it’s routing through the overlay network, so I’m inclined to believe that the docker network stack is single threaded. Since my single core performance is pretty poor, it makes sense that this is the limiting factor. For reference, HTTP transfers on my local network happen at full 1GbE speeds when the traffic is internal to a swarm node, but drops to <40MB/s when the traffic is inter-node."
72,"It seems I’ve opened a can of worms! This definitely requires some further investigation if I want to tune performance on my swarm cluster. I think the easiest option is to move back to a single node Docker – I really don’t need / have any use for the added functionally of swarm and I can’t afford the performance overhead with the hardware I have. Better to keep it lean and mean. Some day I’d like to try this with Kubernetes for comparison, but for now I just want my gigabit speed back!"
72,TagsdockerPost authorWritten byMax Jacoby
72,Leave a Comment CancelYour email address will not be published. Required fields are marked *Message: Name:
72,Email Address:
72,Website:
72,"Save my name, email, and website in this browser for the next time I comment."
72,About Errant Minds
72,"Errant Minds is the umbrella for my all projects, procrastinations and creations. These are the things that make me tick - whether it's blacksmithing or private clouds, I'm usually running before I can walk so I can quench my thirst for a challenge."
72,Latest Posts
72,Weird Network Problems in XCP-NG
72,MySQL Benchmarks – NFS vs Local Storage
72,Optimising VM Performance on XCP-NG – domU Clock Source
72,The ULTIMATE HAProxy Configuration
72,GPT-J-6B Chatbot Experiments
72,Powered by WordPress
72,Inspiro WordPress Theme by WPZOOM
73,openSUSE Leap 15.4 | System Analysis and Tuning Guide | Kernel control groups
73,Jump to contentJump to page navigation: previous page [access key p]/next page [access key n]System Analysis and Tuning Guide / Resource management / Kernel control groupsSystem Analysis and Tuning Guide PrefaceI Basics1 General notes on system tuningII System monitoring2 System monitoring utilities3 System log filesIII Kernel monitoring4 SystemTap—filtering and analyzing system data5 Kernel probes6 Hardware-based performance monitoring with Perf7 OProfile—system-wide profilerIV Resource management8 General system resource management9 Kernel control groups10 Automatic Non-Uniform Memory Access (NUMA) balancing11 Power managementV Kernel tuning12 Tuning I/O performance13 Tuning the task scheduler14 Tuning the memory management subsystem15 Tuning the network16 Tuning SUSE Linux Enterprise for SAPVI Handling system dumps17 Tracing tools18 Kexec and Kdump19 Using systemd-coredump to debug application crashesVII Synchronized clocks with Precision Time Protocol20 Precision Time ProtocolA GNU licenses
73,On this pageApplies to
73,openSUSE Leap 15.49 Kernel control groups #
73,Kernel Control Groups (“cgroups”) are a kernel feature for
73,assigning and limiting hardware and system resources for processes.
73,Processes can also be organized in a hierarchical tree structure.
73,9.1 Overview #
73,Every process is assigned exactly one administrative cgroup. cgroups are
73,ordered in a hierarchical tree structure. You can set resource
73,"limitations, such as CPU, memory, disk I/O, or network bandwidth usage,"
73,for single processes or for whole branches of the hierarchy tree.
73,"On openSUSE Leap, systemd uses cgroups to organize all processes in"
73,"groups, which systemd calls slices. systemd also provides an"
73,interface for setting cgroup properties.
73,The command systemd-cgls displays the hierarchy tree.
73,"The kernel cgroup API comes in two variants, v1 and v2. Additionally,"
73,there can be multiple cgroup hierarchies exposing different APIs. From
73,"the numerous possible combinations, there are two practical choices:"
73,"hybrid: v2 hierarchy without controllers, and the controllers are on v1"
73,hierarchies
73,unified: v2 hierarchy with controllers
73,The current default mode is hybrid (as of SLE SP3). This
73,provides backwards compatibility for applications that need it.
73,The following features are available only with the unified v2 hierarchy:
73,"memory controller: reclaim protection (aka memory.low), memory.high,"
73,PSI (pressure stall information)
73,"io controller: writeback control, new control policies"
73,controller delegation to non-privileged users (rootless containers)
73,freezer support in systemd
73,simpler handling of the single hierararchy
73,You may set only one mode.
73,"To enable the unified control group hierarchy, append"
73,systemd.unified_cgroup_hierarchy=1 as a kernel command
73,line parameter to the GRUB 2 boot loader. (Refer to
73,"Book “Reference”, Chapter 12 “The boot loader GRUB 2” for more details about configuring GRUB 2.)"
73,9.2 Resource accounting #
73,Organizing processes into different cgroups can be used to obtain per-cgroup
73,resource consumption data.
73,"The accounting has relatively small but non-zero overhead, whose impact"
73,depends on the workload. Activating accounting for one unit will also
73,"implicitly activate it for all units in the same slice, and for all its"
73,"parent slices, and the units contained in them."
73,The accounting can be set on a per-unit basis with directives such as
73,MemoryAccounting= or globally for all units in
73,/etc/systemd/system.conf with the directive
73,DefaultMemoryAccounting=. Refer to man
73,systemd.resource-control for the exhaustive list of possible
73,directives.
73,9.3 Setting resource limits #  Note: Implicit resource consumption
73,Be aware that resource consumption implicitly depends on the environment
73,"where your workload executes (for example, size of data structures in"
73,"libraries/kernel, forking behavior of utilities, computational efficiency)."
73,Hence it is recommended to (re)calibrate your limits should the environment
73,change.
73,Limitations to cgroups can be set with the systemctl
73,set-property command. The syntax is:
73,# systemctl set-property [--runtime] NAME PROPERTY1=VALUE [PROPERTY2=VALUE]
73,"The configured value is applied immediately. Optionally, use the"
73,"--runtime option, so that the new values do not persist"
73,after reboot.
73,"Replace NAME with a systemd service, scope, or"
73,slice name.
73,"For a complete list of properties and more details, see man"
73,systemd.resource-control.
73,9.4 Preventing fork bombs with TasksMax #
73,systemd supports configuring task count limits both for each individual
73,"leaf unit, or aggregated on slices. Upstream systemd ships with defaults"
73,"that limit the number of tasks in each unit (15% of the kernel global limit,"
73,run /usr/sbin/sysctl kernel.pid_max to see the total
73,"limit). Each user's slice is limited to 33% of the kernel limit. However,"
73,this is different for SLE.
73,9.4.1 Finding the current default TasksMax values #
73,"It became apparent, in practice, that there is not a single default that"
73,applies to all use cases. openSUSE Leap ships with two custom
73,configurations that override the upstream defaults for system units and for
73,"user slices, and sets them both to infinity."
73,/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf
73,contains these lines:
73,[Manager]
73,DefaultTasksMax=infinity
73,/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf
73,contains these lines:
73,[Slice]
73,TasksMax=infinity
73,Use systemctl to verify the DefaultTasksMax value:
73,> systemctl show --property DefaultTasksMax
73,DefaultTasksMax=infinity
73,infinity means having no limit. It is not a requirement
73,"to change the default, but setting some limits may help to prevent system"
73,crashes from runaway processes.
73,9.4.2 Overriding the DefaultTasksMax value #
73,Change the global DefaultTasksMax value by creating a
73,"new override file,"
73,"/etc/systemd/system.conf.d/90-system-tasksmax.conf,"
73,and write the following lines to set a new default limit of 256 tasks per
73,system unit:
73,[Manager]
73,DefaultTasksMax=256
73,"Load the new setting, then verify that it changed:"
73,> sudo systemctl daemon-reload
73,> systemctl show --property DefaultTasksMax
73,DefaultTasksMax=256
73,Adjust this default value to suit your needs. You can set different limits
73,on individual services as needed. This example is for MariaDB. First check
73,the current active value:
73,> systemctl status mariadb.service
73,● mariadb.service - MariaDB database server
73,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
73,Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
73,Docs: man:mysqld(8)
73,https://mariadb.com/kb/en/library/systemd/
73,Main PID: 11845 (mysqld)
73,"Status: ""Taking your SQL requests now..."""
73,Tasks: 30 (limit: 256)
73,CGroup: /system.slice/mariadb.service
73,└─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
73,"The Tasks line shows that MariaDB currently has 30 tasks running, and has"
73,"an upper limit of the default 256, which is inadequate for a database. The"
73,following example demonstrates how to raise MariaDB's limit to 8192.
73,> sudo systemctl set-property mariadb.service TasksMax=8192
73,> systemctl status mariadb.service
73,● mariadb.service - MariaDB database server
73,Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
73,Drop-In: /etc/systemd/system/mariadb.service.d
73,└─50-TasksMax.conf
73,Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
73,Docs: man:mysqld(8)
73,https://mariadb.com/kb/en/library/systemd/
73,"Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>"
73,"Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>"
73,Main PID: 3452 (mysqld)
73,"Status: ""Taking your SQL requests now..."""
73,Tasks: 30 (limit: 8192)
73,CGroup: /system.slice/mariadb.service
73,└─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
73,systemctl set-property applies the new limit and creates
73,"a drop-in file for persistence,"
73,"/etc/systemd/system/mariadb.service.d/50-TasksMax.conf,"
73,that contains only the changes you want to apply to the existing unit file.
73,"The value does not have to be 8192, but should be whatever limit is"
73,appropriate for your workloads.
73,9.4.3 Default TasksMax limit on users #
73,"The default limit on users should be fairly high, because user sessions"
73,need more resources. Set your own default for any user by creating a new
73,"file, for example"
73,/etc/systemd/system/user-.slice.d/40-user-taskmask.conf.
73,The following example sets a default of 16284:
73,[Slice]
73,TasksMax=16284Note: Numeric prefixes reference
73,See
73,https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in
73,to learn what numeric prefixes are expected for drop-in files.
73,"Then reload systemd to load the new value, and verify the change:"
73,> sudo systemctl daemon-reload
73,> systemctl show --property TasksMax user-1000.slice
73,TasksMax=16284
73,How do you know what values to use? This varies according to your
73,"workloads, system resources, and other resource configurations. When your"
73,"TasksMax value is too low, you will see error messages"
73,such as Failed to fork (Resources temporarily
73,"unavailable), Can't create thread to handle new"
73,"connection, and Error: Function call 'fork' failed"
73,"with error code 11, 'Resource temporarily unavailable'."
73,"For more information on configuring system resources in systemd, see"
73,systemd.resource-control (5).
73,9.5 I/O control with cgroups #
73,This section introduces using the Linux kernel's block I/O controller to
73,prioritize or throttle I/O operations. This leverages the means provided by
73,"systemd to configure cgroups, and discusses probable pitfalls when dealing"
73,with proportional I/O control.
73,9.5.1 Prerequisites #
73,The following subsections describe steps that you must take in advance when
73,"you design and configure your system, since those aspects cannot be changed"
73,during runtime.
73,9.5.1.1 File system #
73,You should use a cgroup-writeback-aware file system (otherwise writeback charging is not possible).
73,The recommended SLES file systems added support in the following upstream
73,releases:
73,Btrfs (v4.3)
73,Ext4 (v4.3)
73,XFS (v5.3)
73,"As of openSUSE Leap 15 SP3, any of the named file systems can be used."
73,9.5.1.2 Unified cgroup hierarchy #
73,"To properly account writeback I/O, it is necessary to have equal I/O"
73,"and memory controller cgroup hierarchies, and to use the cgroup v2 I/O"
73,"controller. Together, this means that one has to use the"
73,unified cgroup hierarchy. This has to be requested
73,"explicitly in SLES by passing a kernel command line option,"
73,systemd.unified_cgroup_hierarchy=1.
73,9.5.1.3 Block I/O scheduler #
73,"The throttling policy is implemented higher in the stack, therefore it"
73,does not require any additional adjustments.
73,The proportional I/O control policies have two different implementations:
73,"the BFQ controller, and the cost-based model."
73,We describe the BFQ controller here. In order to exert its proportional
73,"implementation for a particular device, we must make sure that BFQ is the"
73,chosen scheduler. Check the current scheduler:
73,> cat /sys/class/block/sda/queue/scheduler
73,mq-deadline kyber bfq [none]
73,Switch the scheduler to BFQ:
73,# echo bfq > /sys/class/block/sda/queue/scheduler
73,You must specify the disk device (not a partition). The
73,optimal way to set this attribute is a udev rule specific to the device
73,(note that SLES ships udev rules that already enable BFQ for rotational
73,disk drives).
73,9.5.1.4 Cgroup hierarchy layout #
73,"Normally, all tasks reside in the root cgroup and they compete against"
73,each other. When the tasks are distributed into the cgroup tree the
73,competition occurs between sibling cgroups only.
73,This applies to the proportional I/O control; the throttling
73,hierarchically aggregates throughput of all descendants (see the following
73,diagram).
73,IOWeight=100
73,`- [c] IOWeight=300
73,IOWeight=100
73,`- [b]
73,IOWeight=200
73,I/O is originating only from cgroups c and b. Even though c has a higher
73,"weight, it will be treated with lower priority because it is level-competing"
73,with b.
73,9.5.2 Configuring control quantities #
73,You can apply the values to (long running) services permanently.
73,> sudo systemctl set-property fast.service IOWeight=400
73,> sudo systemctl set-property slow.service IOWeight=50
73,"> sudo systemctl set-property throttled.service IOReadBandwidthMax=""/dev/sda 1M"""
73,"Alternatively, you can apply I/O control to individual commands, for"
73,example:
73,> sudo systemd-run --scope -p IOWeight=400 high_prioritized_command
73,> sudo systemd-run --scope -p IOWeight=50 low_prioritized_command
73,"> sudo systemd-run --scope -p IOReadBandwidthMax=""/dev/sda 1M"" dd if=/dev/sda of=/dev/null bs=1M count=109.5.3 I/O control behavior and setting expectations #"
73,"The following list items describe I/O control behavior, and what you"
73,should expect under various conditions.
73,"I/O control works best for direct I/O operations (bypassing page cache),"
73,the situations where the actual I/O is decoupled from the caller (typically
73,"writeback via page cache) may manifest variously. For example, delayed I/O"
73,control or even no observed I/O control (consider little bursts or competing
73,"workloads that happen to never ""meet"", submitting I/O at the same time, and"
73,"saturating the bandwidth). For these reasons, the resulting ratio of"
73,I/O throughputs does not strictly follow the ratio of configured weights.
73,systemd performs scaling of configured weights (to adjust for narrower BFQ
73,"weight range), hence the resulting throughput ratios also differ."
73,"The writeback activity depends on the amount of dirty pages, besides the"
73,global sysctl knobs (vm.dirty_background_ratio and
73,vm.dirty_ratio)). Memory limits of individual cgroups
73,"come into play when the dirty limits are distributed among cgroups, and"
73,this in turn may affect I/O intensity of affected cgroups.
73,Not all storages are equal. The I/O control happens at the I/O scheduler
73,"layer, which has ramifications for setups with devices stacked on these"
73,that do no actual scheduling. Consider device mapper logical volumes
73,"spanning multiple physical devices, MD RAID, or even Btrfs RAID."
73,I/O control over such setups may be challenging.
73,There is no separate setting for proportional I/O control of reads and
73,writes.
73,Proportional I/O control is only one of the policies that can interact with
73,each other (but responsible resource design perhaps avoids that).
73,The I/O device bandwidth is not the only shared resource on the I/O path.
73,"Global file system structures are involved, which is relevant"
73,"when I/O control is meant to guarantee certain bandwidth; it will not, and"
73,it may even lead to priority inversion (prioritized cgroup waiting for a
73,transaction of slower cgroup).
73,"So far, we have been discussing only explicit I/O of file system data, but"
73,swap-in and swap-out can also be controlled. Although if such a need
73,"arises, it usually points out to improperly provisioned memory (or memory limits)."
73,9.6 More information #
73,Kernel documentation (package kernel-source):
73,files in
73,/usr/src/linux/Documentation/admin-guide/cgroup-v1
73,and file
73,/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst.
73,man systemd.resource-control
73,"https://lwn.net/Articles/604609/—Brown, Neil:"
73,"Control Groups Series (2014, 7 parts)."
73,"https://lwn.net/Articles/243795/—Corbet,"
73,Jonathan: Controlling memory use in containers (2007).
73,"https://lwn.net/Articles/236038/—Corbet,"
73,Jonathan: Process containers (2007).
73,PreviousChapter 8 General system resource management NextChapter 10 Automatic Non-Uniform Memory Access (NUMA) balancing On this page9.1 Overview9.2 Resource accounting9.3 Setting resource limits9.4 Preventing fork bombs with TasksMax9.5 I/O control with cgroups9.6 More informationShare this page
73,© SUSE
73,2023
74,"PawSQL Advisor,SQL Audit/Rewrite/Index Advice,Tune SQL by Clicks - IntelliJ IDEs Plugin | Marketplace"
77,5 ways to rapidly improve MySQL database performance - LogRocket Blog
77,Blog
77,Dev
77,Product Management
77,UX Design
77,Podcast
77,Features
77,Solutions
77,Solve User-Reported Issues
77,Find Issues Faster
77,Optimize Conversion and Adoption
77,Start Monitoring for Free
77,Sign In
77,2940
77,104
77,"Dec 29, 2022 ⋅ 10 min read"
77,5 ways to rapidly improve MySQL database performance
77,Lukas Vileikis
77,"I'm an ethical hacker and frequent conference speaker who is known for running one of the biggest and fastest data breach search engines in the world, BreachDirectory.com."
77,Table of contents
77,MySQL and your applications
77,Improving database performance in general
77,#1: Managing the my.cnf file
77,#2 and #3: Check up on MySQL storage engines and schema design
77,#4 and #5: Indexes and partitions
77,Advanced operations: Tips and tricks
77,The downsides of each approach
77,Conclusion
77,"Every developer that works with MySQL understands how crucial the RDBMS is for their projects. The database management system can support all kinds of projects, from gaming forums to healthcare solutions. According to research performed by DatabaseJournal, it takes up almost half, 44 percent, of the database market share."
77,"Improving MySQL performance is also something that every DBA struggles with at some point in their career. Worry not though, we’re here to help. In this article, we’ll walk through five ways to quickly improve your MySQL database performance. Let’s get started!"
77,MySQL and your applications
77,Improving database performance in general
77,#1: Managing the my.cnf file
77,#2 and #3: Check up on MySQL storage engines and schema design
77,#4 and #5: Indexes and partitions
77,Advanced operations: Tips and tricks
77,MySQL and your applications
77,"Before attempting to improve your MySQL app performance, one of the first things to consider is the infrastructure backing your application. No system has ever been improved upon without first understanding what it’s built on. For this reason, we need to take a step back and check up on the server backing MySQL."
77,"We’ll first want to check up on two basic things, starting with the amount of memory installed on the server. You can observe this by issuing a free command. Secondly, you can issue the df"" command to observe the amount of hard drive space on the server."
77,"Keeping these things in mind, you can connect to MySQL. First, you’ll want to check whether any unnecessary queries are running by using the SHOW PROCESSLIST command. You should receive an output like the one below:"
77,"If you see any long-running queries that you don’t recognize, it’s a good idea to terminate them; a long-running query may be an obstacle to other queries. However, chances are that you won’t see any suspicious queries. Even if you do, terminating one or two slow queries won’t rapidly improve your database performance."
77,"To speed up your queries, you need to gain some understanding of how queries and MySQL work in general."
77,Improving database performance in general
77,"Before walking you through the specific measures that help improve MySQL database performance, you need to understand the basics of database performance."
77,"Where database performance improvement is concerned, people are usually talking about improving the performance of CRUD, Create, Read, Update, and Delete queries. In MySQL, these queries span the INSERT, SELECT, UPDATE, and DELETE queries."
77,"All queries within MySQL lean on the settings defined in one core file related to MySQL, my.cnf. All of the settings defined in my.cnf have a direct impact on query performance."
77,"You can usually improve the INSERT query performance by removing indexes from the table that data is inserted to. The more indexes are on a specific table, the harder it is for INSERT to proceed."
77,"To improve SELECT query performance, we typically use indexes. To improve UPDATE query performance, we perform updates in batches, meaning we perform many smaller updates instead of one big update."
77,"To improve DELETE query performance, we switch the DELETE query to TRUNCATE. TRUNCATE deletes all rows in a table. Such a query is generally much faster than deleting rows using DELETE because TRUNCATE provides MySQL with less overhead."
77,Over 200k developers use LogRocket to create better digital experiences
77,Learn more →
77,The advice given above will certainly be a good starting point when trying to understand why a MySQL-based database is misbehaving in the performance realm.
77,"To understand the reasons behind the assumptions given above, though, we’ll need to dive deeper. I recommend taking a backup of your database, then coming back to this blog. Now, we’ll review five ways that will help you rapidly improve your database performance."
77,#1: Managing the my.cnf file
77,"When attempting to improve MySQL query performance, one of the first things to take a closer look at would be the my.cnf file, which holds all the necessary parameters for MySQL to function. If you’re using Linux, you can find the my.cnf file in one of the following directories:"
77,/var/lib/mysql/my.cnf
77,/etc/my.cnf
77,/etc/mysql/my.cnf
77,/usr/etc/my.cnf
77,"If you‘re using Windows, you can find the file in the /bin/mysql/mysql *.*.* directory."
77,mysql *.``*.*``* refers to your MySQL version. Open up the file and look for the parameters surrounding InnoDB:
77,"All of these parameters are related to one of the main storage engines within MySQL, InnoDB. You can use other storage engines, but since InnoDB is the default storage engine offered by MySQL, we suggest you go with it."
77,my.cnf parameters
77,"Let’s review the parameters. The innodb-buffer-pool-size parameter defines the size of the buffer pool, which is used to cache data related to InnoDB tables. The innodb-data-file-path parameter specifies the path where the ibdata1 file is stored. ibdata1 is the main file related to InnoDB, storing all of the necessary data."
77,innodb-default-row-format specifies the row format within InnoDB tables. These can be either fixed or dynamic. innodb-doublewrite specifies whether or not the doublewrite mechanism within InnoDB is enabled.
77,innodb-flush-log-at-trx-commit specifies how data is flushed to log files when transactions commit and finish. The innodb-flush-method parameter defines the method used to flush data to log files.
77,More great articles from LogRocket:
77,"Don't miss a moment with The Replay, a curated newsletter from LogRocket"
77,Learn how LogRocket's Galileo cuts through the noise to proactively resolve issues in your app
77,Use React's useEffect to optimize your application's performance
77,Switch between multiple versions of Node
77,Discover
77,how to use the React children prop with TypeScript
77,Explore creating a custom mouse cursor with CSS
77,"Advisory boards aren’t just for executives. Join LogRocket’s Content Advisory Board. You’ll help inform the type of content we create and get access to exclusive meetups, social accreditation, and swag."
77,Setting my.cnf parameters
77,Remember how you figured out the amount of RAM and hard drive space available within your infrastructure? Now is the time to use those details for the best possible performance. We’ll set the parameters as follows.
77,"The innodb-buffer-pool-size parameter should be set to 50 to 60 percent of the available RAM. The bigger it is, the more data will be cached, and therefore, inserting data will be faster."
77,Increase the size of the innodb-data-file-path variable so that it is able to accommodate all of the data within MySQL. We recommend setting the parameter to 5-10 GB.
77,"If the parameter is not present, include an innodb-file-per-table parameter and set it to one. The innodb-file-per-table parameter will help MySQL understand that it needs to store all tables as separate files, thereby making the size of the buffer pool significantly smaller. The buffer pool will only hold metadata."
77,"We advise leaving the innodb-flush-log-at-trx-commit parameter at its default value. The default value guarantees ACID compliance, but, if you want faster write performance, you can also consider changing the value to 0 or 2. Bear in mind that ACID, the properties guaranteeing data integrity, will be traded off as a result."
77,Leave the flush method as is. The O_DIRECT flush method guarantees faster performance when importing data due to the Linux kernel avoiding the OS cache.
77,Performing the steps specified above will guarantee faster performance even if your server has a limited amount of RAM and storage space.
77,#2 and #3: Check up on MySQL storage engines and schema design
77,"In addition to fiddling around with the my.cnf file, we should also examine the storage engines we use and the way they are designed.If you’re using MySQL, use InnoDB. If you’re using Percona Server, use Percona XtraDB."
77,InnoDB parameters
77,"At the time of writing, InnoDB is the only storage engine that supports ACID properties. These properties guarantee data integrity even in the event of power outages or any similar disruptions. As mentioned previously, ACID can be exchanged for speed by setting the innodb-flush-log-at-trx-commit parameter to 0 or 2."
77,"InnoDB offers multiple parameters that you can use to rapidly improve query performance and other operations, including innodb-buffer-pool-size and innodb-log-file-size."
77,"Set the buffer pool size to 60 percent of RAM available within your infrastructure and the log file size to approximately a quarter of the value allocated to the buffer pool. The log files are scanned when MySQL is restoring the data within InnoDB. The bigger the size, the faster the speed of the restore process."
77,"Both InnoDB and XtraDB support row-level locking. In simple terms, row-level locking refers to only locking access to rows that are directly impacted by a transaction. Compared to table-level locking, it has one significant advantage; developers can continue working with rows when updating data."
77,"If your use case doesn’t require such an approach, you should avoid using any other storage engine than InnoDB. MyISAM isn’t reliable, and other storage engines are to be used only in specific corner cases. For more information, refer to the MySQL documentation."
77,"If the data you’re working with exceeds 10 million rows, all of your tables are normalized."
77,Indexing queries
77,"At least some of the columns within the tables that you run SELECT queries on are indexed. For optimal results, index either all of the columns that go after the WHERE clause or the first one to save space. This type of approach will improve the performance of queries that read data because indexes will let MySQL know how to find columns with specific values quickly."
77,Data types and integers
77,"It’s important that you know your way around data types and character sets. To occupy less space on the disk, you should use CHAR (character) or VARCHAR (variable character) data types instead of TEXT– CHAR and VARCHAR data types. It’s the same with integers; consider using SMALLINT instead of INT if necessary to save hard drive space."
77,"Specify the length of the data types properly. Consider specifying a size of, say, 50, instead of 255, the maximum value, when dealing with big data. Such an approach will save massive amounts of space on the disk."
77,"Ensure that your tables do not store any data that’s not necessary. The less data that is stored, the less data you have to read and update."
77,#4 and #5: Indexes and partitions
77,"In addition to the factors described above, indexes and partitions are also immensely important. Indexes help us to find rows with specific values quickly, whereas partitions act as tables within tables to further improve performance."
77,"Both of those approaches come with a cost on the INSERT, UPDATE, and DELETE queries since the data that is inserted or updated needs to be updated inside of the index or partition itself."
77,"However, both of these approaches have an upside as well; they both speed up read operations. Partitions make SELECT queries faster because they can split tables into smaller tables beginning with a certain character, only running queries through them. On the other hand, the job of an index is to make SELECT queries with a WHERE clause faster."
77,Best practices for indexes
77,"Both indexes and partitions have multiple different types. For the purposes of this article, we won’t discuss them all, but for indexes, keep the following in mind."
77,"The most common type of indexes, B-tree indexes, are useful when used in conjunction with queries with operators containing an equality sign =."
77,"Covering indexes cover all of the columns that are used by a specific query. For example, a covering index on the columns a1, a2, and a3 would satisfy the following query:"
77,SELECT * FROM demo WHERE a1 = 'Demo' AND a2 = 'Demo 2' AND a3 = 'Demo 3';
77,"Hash indexes only work in specific storage engines and specific search operators within MySQL, including = and <=>."
77,Partitions in MySQL
77,"For partitions, keep in mind that they come in multiple flavors as well."
77,Partitioning by RANGE lets us partition values falling within a given range. This type of partitioning is particularly useful when splitting large tables by character or number.
77,"Partitioning by HASH splits the table into multiple tables according to a number of columns. For example, PARTITION BY HASH(id) PARTITIONS 8; would split the table into multiple different tables at the database level with eight partitions in total."
77,"All types of partitioning can be found in the MySQL documentation. Partitioning is usually defined upon creating a table, and in many cases, it looks like the following:"
77,CREATE TABLE table_name (
77,[column_details]
77,) [partitioning_details];
77,"Partitioning by range, for example, would look like this:"
77,CREATE TABLE table_name (
77,`demo_column` VARCHAR(255) NOT NULL DEFAULT ''
77,) PARTITION BY RANGE (column) (
77,"PARTITION p1 VALUES LESS THAN (100),"
77,PARTITION p2 VALUES LESS THAN (200)
77,"Other types of partitioning look very similar to the partitioning defined above. However, partitioning by RANGE is replaced by LIST, HASH, or other types."
77,Partitioning has another very important upside as well. It allows users to delete all of the data stored in a single partition; ALTER TABLE demo TRUNCATE PARTITION partition_name will do the trick.
77,Advanced operations: Tips and tricks
77,"Both indexing and partitioning will help immensely in improving read operations, but there are a couple of additional things that we need to keep in mind."
77,COUNT(*) queries are only fast when the MyISAM storage engine is used. Faster COUNT(*) queries are the only upside of the MyISAM storage engine since it stores the row count within its metadata. No other storage engines do that.
77,"For faster SELECT queries with wildcards, employ wildcards only at the end of the search query. Queries should look like the code below; keep in mind that there’s no wildcard sign at the start of the string:"
77,SELECT * FROM demo_table WHERE column LIKE 'string%';
77,"Wildcards at the start of the string tell MySQL that it should search for anything % at the beginning, which can slow the query down."
77,"UNIQUE indexes help us to ensure that each entry inside a column is unique. If that’s not the case, MySQL will error out."
77,"The IGNORE keyword is useful if we want to ignore errors when inserting data or performing other operations. Simply specify IGNORE within the statement, then proceed as usual:"
77,INSERT IGNORE INTO demo_table (c1) VALUES ('Demo');
77,LOAD DATA INFILE and SELECT … INTO OUTFILE is significantly faster than issuing INSERT queries and backing up data in a regular fashion. Such queries avoid a lot of the overhead that exists when INSERT queries are being run. Refer to the MySQL documentation for more information.
77,"Older versions of MySQL cannot deal with FULLTEXT indexes on bigger data sets when we’re searching for anything with an @ sign. That’s a bug within MySQL, BUG#104263. This approach causes the query to timeout."
77,"Avoid issuing ALTER queries on tables running big data sets. Due to how ALTER works internally, it forces MySQL to create a new table, then inserts the data into it, makes the necessary changes, and swaps the original table with the copy. As far as big data sets are concerned, this approach usually takes a long time, so just keep that in mind."
77,"Sometimes, it’s helpful to use the DEFAULT keyword to set default values to many rows at once. Imagine creating a table, then inserting a billion rows into it. When the DEFAULT keyword is used, rows will be pre-filled with a specific keyword, thereby preventing the need for potentially problematic ALTER queries as described above. Define a column as follows:"
77,`column_name` VARCHAR(255) NOT NULL DEFAULT 'value';
77,"Hopefully, the advice in this article will help you improve the performance of your MySQL databases. However, as with everything, note that there are downsides."
77,The downsides of each approach
77,Improving MySQL performance using the methods described above may come with the following drawbacks.
77,"Checking up on my.cnf requires some knowledge of Linux internals, and in many cases, a rather strong server. You can’t improve performance much if your RAM is limited to 256MB or if you have only 2GB of disk space in total."
77,"Knowing your way around my.cnf and storage engines and modifying their settings usually requires deep knowledge of the MySQL space. One needs to know exactly what each parameter that is modified does, what their appropriate values are, and more."
77,"Windows users have it easy since my.ini, a my.cnf equivalent, provides them with a lot of comments within itself, but Linux users usually have to define many settings themselves."
77,"The main downside of data types and character sets is the fact that each character requires space on the disk, and some character sets have different requirements in the storage space. Four bytes per character or eight bytes per character certainly makes a difference if we’re dealing with large data sets, so that’s something to think about too. Refer to the MySQL documentation for more information."
77,"Indexes and partitions usually speed up SELECT operations at the expense of slowing down everything else, including INSERT, UPDATE, and DELETE, since all of those queries have to insert, update, or delete data in indexes and partitions as well."
77,Conclusion
77,"In this article, we’ve discussed five ways that you can rapidly improve your MySQL database performance. Each approach has its own unique upsides and downsides and is applicable in different scenarios. However, whether or not the pros outweigh the cons is for you to decide."
77,"Familiarize yourself with the documentation surrounding your storage engine of choice. Use InnoDB or XtraDB as your storage engine, try your best to normalize the tables you’re working with, avoid using unnecessary sizes for your data types, and index your columns to speed up SELECT queries."
77,"Indexes and partitions are used to speed up SELECT queries at the expense of slowing down INSERT, UPDATE, and DELETE. Both of these approaches have multiple types and can be incredibly useful if used wisely."
77,"As always, before attempting to improve your MySQL app performance using one or more of the ways described in this article, be sure to evaluate all of the options available to you, take backups before you test anything, and try all modifications on a local environment first. Make modifications wisely, and always keep in mind that a performance increase in one place most likely means a performance decrease in another place."
77,"I hope you enjoyed this article. Be sure to leave a comment if you have any questions, and happy coding."
77,Get set up with LogRocket's modern error tracking in minutes:
77,Visit https://logrocket.com/signup/ to get
77,an app ID
77,"Install LogRocket via npm or script tag. LogRocket.init() must be called client-side, not"
77,server-side
77,npm
77,Script tag
77,$ npm i --save logrocket
77,// Code:
77,import LogRocket from 'logrocket';
77,LogRocket.init('app/id');
77,// Add to your HTML:
77,"<script src=""https://cdn.lr-ingest.com/LogRocket.min.js""></script>"
77,<script>window.LogRocket && window.LogRocket.init('app/id');</script>
77,(Optional) Install plugins for deeper integrations with your stack:
77,Redux middleware
77,NgRx middleware
77,Vuex plugin
77,Get started now
77,Share this:Click to share on Twitter (Opens in new window)Click to share on Reddit (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on Facebook (Opens in new window)
77,Recent posts:
77,Guide to mobile emulator testing with MiniSim
77,MiniSim makes virtual emulator testing easy — learn how to use it to test both Android and iOS apps on macOS in this detailed post.
77,Emmanuel John
77,"Nov 22, 2023 ⋅ 5 min read"
77,"Implementing safe, dynamic localization in TypeScript apps"
77,"After internationalization, you can further localize your apps to make user experiences more friendly. Learn how to do this in TypeScript."
77,Yan Sun
77,"Nov 21, 2023 ⋅ 6 min read"
77,Using dev containers with VS Code for an easier dev setup
77,You can leverage containers to streamline the process of setting up a dev environment. Let’s see how using VS Code and Docker.
77,Yashodhan Joshi
77,"Nov 21, 2023 ⋅ 11 min read"
77,Building a progressive web app in Remix with Remix PWA
77,"Progressive web apps (PWAs) are applications that look and behave like mobile apps, but are built using web technologies. While […]"
77,Chimezie Innocent
77,"Nov 20, 2023 ⋅ 11 min read"
77,View all posts
77,Leave a ReplyCancel reply
77,Loading Comments...
77,Write a Comment...
77,Email (Required)
77,Name (Required)
77,Website
81,"Remote Support Manager Job at MariaDB | HimalayasHimalayas logo - JobsJobsCompaniesTalentSalariesPricingAdviceUpdatesLog inPost a jobSign upMASupport ManagerShare jobSign up to save this jobMariaDB frees companies from the costs, constraints, and complexity of proprietary databases, enabling them to reinvest in what matters most – rapidly developing innovative, customer-facing applications.MariaDBEmployee count: 11-50 Philippines onlyApply nowShare jobMariaDB is making a big impact on the world. Whether you’re checking your bank account, buying a coffee, shopping online, making a phone call, listening to music, taking out a loan or ordering takeout – MariaDB is the backbone of applications used everyday. Companies small and large, including 75% of the Fortune 500, run MariaDB, touching the lives of billions of people. With massive reach through Linux distributions, enterprise deployments and public clouds, MariaDB is uniquely positioned as the leading database for modern application development.The OpportunityAs a MariaDB Support Manager, you directly interface with our customers each and every day to help their business, users and customers succeed and grow.Our customers use MariaDB Server, MaxScale, ColumnStore, Galera Cluster, Xpand (previously ClustrixDB), client connectors for C, Java, ODBC and other programming languages and environments, and new and emerging technologies made available through MariaDB Server. They also use other open source technologies, operating systems, Linux and Windows. Their servers are deployed on premises, in the cloud, in virtual servers and containers and on physical servers.We believe technical support is not just a job, but a calling. Our model is unconventional: we do not view support as merely a “cost center,” the unfortunate byproduct of creating products. We do not frustrate our customers or our engineers with scripts that must be parroted and “tiers” that must be escalated through to find someone who can solve an issue. We spend most of our time solving issues that go far beyond simple “break/fix” support or documentation lookup. Our goal is to provide the very best support experience across any industry, and we achieve that by hiring and empowering the very best in the world.ResponsibilitiesAs a MariaDB Support Manager you are vital to a customer’s success in this challenging, complex and ever-changing technology landscape. You’re expected to deliver expert-level technical support services to worldwide customers, across our entire portfolio of supported products. Support services range from software usage and best practices, to service restoration and performance tuning. You will utilize advanced troubleshooting skills and creative problem-solving capabilities and you must be self-motivated and capable of working autonomously.Because we must provide coverage to our customers 24x7x365, this position does occasionally involve work on nights, weekends and holidays, however, we leverage our global team to minimize this impact as much as possible.If you have a true passion for helping customers succeed and feel rewarded by solving complex problems, and you enjoy challenges, then join the MariaDB Support team today.QualificationsPrior MariaDB, Percona or MySQL 24x7 support engineer experience8+ years of expertise with MariaDB Server or MySQL Enterprise, and related toolsWorking knowledge of other relational and non-relational database systemsPassion for serving customers and solving technical problemsStrong knowledge of SQL and query optimizationStrong knowledge of database design, administration and architectureIn-depth knowledge of various highly available environments and technologiesProficient with multiple backup solutionsExperience with cloud offerings and environmentsExperience with automation tool setsExperience with containersDatabase monitoring software experienceProficient in one or more scripting languages (Bash, Python, Go, etc.)Sysadmin experience (Linux, Windows, etc.)Very strong troubleshooting and problem-solving skillsExperience working remotelyExcellent written and spoken English communication skillsBonus Experience and SkillsContributions to open source software in the MariaDB/MySQL ecosystemExperience with other MariaDB products such as MaxScale, ColumnStore, Xpand, SkySQL, etc.Proficient with virtualization software (Xen, VirtualBox, VMware, etc.)MariaDB and/or other related certificationsLocationRemote – South Korea, Philippines, Malaysia, Singapore or Australia (citizens or PRs only)What’s in It for You?Impact the world of technology by pushing the boundaries of technology and business models, working at MariaDB. Be part of a game-changing organization that encourages outside-the-box thinking, values empowerment, and is truly shaping the future of the software industry. You’ll be collaborating with high-caliber colleagues around the world, offering unparalleled learning and growth opportunities. We provide a very competitive compensation package, 25 days paid annual leave (plus holidays), stock options, a massive degree of flexibility and freedom, and more.Salaries for candidates outside the U.S. will vary based on local compensation structures.How to ApplyIf you are interested in this position, please submit your application along with your resume/CV.MariaDB does not sponsor work visas or relocation.MariaDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter.MariaDB will not accept agency resumes without a prior contractual agreement with HR. Please do not forward resumes to any recruiting alias or employee directly. MariaDB is not responsible for any unsolicited resumes.MariaDB is an equal opportunities employer.Apply nowJob expired?Please let MariaDB know you found this job on Himalayas. This helps us grow!Apply nowApply nowJob expired?Please let MariaDB know you found this job on Himalayas. This helps us grow!Apply nowAbout the jobApply beforeJan 14, 2024Posted onNov 15, 2023Job typeFull TimeExperience levelManagerLocation requirementsSuggest an editPhilippinesHiring timezonesShow UTC offsetsPhilippines +/- 0 hoursJob categoriesSupport ManagerCustomer Support ManagerSales Support SpecialistCommunity Support SpecialistSkillsMySQL MariaDBSQLDatabase DesignDatabase AdministrationHigh AvailabilityCloud ServicesAutomation ToolsContainersScripting LanguagesJavaLinuxMariaDBMySQLPerconaVirtual BoxWindowsXenBASHPythonDatabase ManagementVirtualizationVirtualBoxAbout MariaDBLearn more about MariaDB and their company culture.View company profileMariaDB frees companies from the costs, constraints, and complexity of proprietary databases, enabling them to reinvest in what matters most – rapidly developing innovative, customer-facing applications. MariaDB uses pluggable, purpose-built storage engines to support workloads that previously required a variety of specialized databases. With complexity and constraints eliminated, enterprises can now depend on a single complete database for all their needs, whether on commodity hardware or their cloud of choice. Deployed in minutes for transactional or analytical use cases, MariaDB delivers unmatched operational agility without sacrificing key enterprise features including real ACID compliance and full SQL. Trusted by organizations such as Deutsche Bank, DBS Bank, Nasdaq, Red Hat, ServiceNow, Verizon, and Walgreens – MariaDB meets the same core requirements as proprietary databases at a fraction of the cost. No wonder it’s the fastest-growing open-source database. Real business relies on MariaDB™.Tech stackLearn about the tools and technologies that MariaDB uses to build, market, and sell its products.View tech stackBootstrapJavaScriptPythonHTML5JavaES6CSS 3PHPC#GoC++70 moreMariaDB employees can create an account to update this tech stack.Employee benefitsLearn about the employee benefits and perks provided at MariaDB.View benefitsLife insuranceLife insurance so you don't have to worry.Disability insuranceDisability insurance so you don't have to worry.Paid parental leavePaid family leave for all parents to support you and your family.Employee assistance program (EAP)We offer an employee assistance program focused on mental health.View MariaDB's employee benefitsHimalayasRemote companiesMariaDBSupport ManagerApply nowJob expired?Please let MariaDB know you found this job on Himalayas. This helps us grow!Apply nowAbout the jobApply beforeJan 14, 2024Posted onNov 15, 2023Job typeFull TimeExperience levelManagerLocation requirementsSuggest an editPhilippinesHiring timezonesShow UTC offsetsPhilippines +/- 0 hoursJob categoriesSupport ManagerCustomer Support ManagerSales Support SpecialistCommunity Support SpecialistSkillsMySQL MariaDBSQLDatabase DesignDatabase AdministrationHigh AvailabilityCloud ServicesAutomation ToolsContainersScripting LanguagesJavaLinuxMariaDBMySQLPerconaVirtual BoxWindowsXenBASHPythonDatabase ManagementVirtualizationVirtualBoxClaim this profileMAMariaDBCompany size11-50Founded in2014Chief executive officerMichael HowardMarketsRelational DatabaseOpen SourceDatabaseAnalytical DatabaseEmployees live inUnited StatesFinlandDenmarkFranceItalyNorwaySwedenBulgariaSingaporeSocial mediaMariaDB's TwitterMariaDB's FacebookMariaDB's LinkedInView company profileVisit mariadb.comSimilar remote jobsHere are other jobs you might want to apply for.View all remote jobs4 remote jobs at MariaDBExplore the variety of open remote roles at MariaDB, offering flexible work options across multiple disciplines and skill levels.View all jobs at MariaDBTop remote companiesRemote companies like MariaDBFind your next opportunity by exploring profiles of companies that are similar to MariaDB. Compare culture, benefits, and job openings on Himalayas."
81,"View all companiesFind your dream jobSign up now and join thousands of other remote workers who receive personalized job alerts, curated job matches, and more for free! Sign up with GoogleSign upWe contribute 10% of every payment to remove CO₂ from the atmosphere with Stripe Climate.Our pledgeHimalayas logohi@himalayas.appFacebookLinkedInHimalayasAbout usCommunityTech stackEmployee benefitsTerms and conditionsPrivacy policyContact usFor job seekersCreate your profileDiscover your job matchesBrowse remote jobsRemote jobs RSSRemote jobs APIDiscover remote companiesRemote work adviceCommunity rewardsFor companiesPost a remote jobHire remote talentCreate a company profileAdd your tech stackShare employee benefitsPricingRemote work statisticsRemote salariesTop remote job categoriesTop remote job skillsTop countries for remote jobsTop industries for remote jobsTop countries for remote companiesTop industries for remote companiesJoin the remote work revolutionJoin thousands who get tailored alerts and access to top recruiters. Sign up with GoogleSign up© 2023 Himalayas. All rights reserved. Built with Untitled UI.MA South Korea onlySupport Manager (MariaDB 지원 관리자)MariaDBEmployee count: 11-50Full TimeIT Support ManagerMA Philippines onlySupport ManagerMariaDBEmployee count: 11-50Full TimeSupport ManagerMA United States onlyManager, Global Marketing Programs and OperationsMariaDBEmployee count: 11-50Salary: 115k-125k USDFull TimeDirector Of Global Social Media MarketingMA United States onlyBusiness Development Representative (BDR)MariaDBEmployee count: 11-50Salary: 53k-70k USDFull TimeBusiness Development Representative (BDR)GISNDATop remote companiesRemote companies like MariaDBFind your next opportunity by exploring profiles of companies that are similar to MariaDB. Compare culture, benefits, and job openings on Himalayas."
81,"View all companiesGI51 jobsGitLabGitLab's Tech stackGitLab's Employee benefitsGitLab is a complete DevOps platform, delivered as a single application.Web DevelopmentDeveloper ToolsSN5 jobsSnowflakeSnowflake's Tech stackSnowflake's Employee benefitsSnowflake’s cloud data platform shatters the barriers that have prevented organizations of all sizes from unleashing the true value from their data.Data WarehouseCloud InfrastructureDA1 jobDataStaxDataStax's Tech stackDataStax's Employee benefitsDataStax is the company behind the massively scalable, highly available, cloud-native NoSQL data platform built on Apache Cassandra™.NoSQLOpen SourcePE63 jobsPerconaPercona's Tech stackPercona's Employee benefitsPercona is an industry leader in providing elite services, training and software for MySQL®, MariaDB®, MongoDB®, PostgreSQL® and other open-source databases in on-premises and cloud environments.Open SourceDatabaseBRBriteCoreBriteCore's Tech stackBriteCore's Employee benefitsBriteCore was built from the ground up using the latest modern technology.InsurtechEnterprise SoftwareAC5 jobsAcquiaAcquia's Tech stackAcquia's Employee benefitsAcquia is the open source digital experience company that empowers the world’s most ambitious brands to embrace innovation and create customer moments that matter.SaaSOpen SourcePE AS, AU + 51 moreDatabase Performance Consultant - MySQLPerconaEmployee count: 201-500Full TimeDatabase Performance ConsultantQU United States onlyCoordinator - Influencer MarketingQuinceEmployee count: 201-500Full TimeMarketing Activation ManagerWS United States onlyCustomer Success Manager (Commercial Retail)Wiser SolutionsSalary: 80k-80k USDFull TimeCustomer Success ManagerEQCryptographic EngineerEquilibriumEmployee count: 11-50Full TimeCryptographic EngineerFA United States onlySoftware Engineering ManagerFathomEmployee count: 11-50Salary: 180k-250k USDFull TimeSoftware Engineering ManagerAC United States onlySenior Software DeveloperActianEmployee count: 201-500Full TimeSenior Software Developer"
83,What filesystem to use for lxd? - LXD - Linux Containers Forum
83,Linux Containers Forum
83,What filesystem to use for lxd?
83,LXD
83,denisrud
83,(Denis)
83,"January 6, 2023,"
83,4:41pm
83,"I’m trying to host websites inside lxd. The thing common to all websites is static content, php and database. The ideal scenario wud be a different container for every website. Some containers might just include file backups, some I’ll mount as drives on my laptop."
83,"I’ve wanted to use lxd’s snapshot feature but I don’t want it at the cost of severely reduced io. Ideally if the server went down or I had to move to a bigger server, cud I restore a snapshot quickly on a different host server?"
83,I’m worried that any wrong decision now will hurt me later. What is the ideal filesystem for a situation like this?
83,k4my4b
83,"January 6, 2023,"
83,5:52pm
83,Btrfs!
83,Why not ZFS?
83,biggest issue is that it’s not mainlined and will never be (as far as we can tell)
83,secondly it has a lot of features and abstraction layers for larger installations that will get in your way.
83,Does not work with docker which I’m guessing you will be using with container nesting
83,NOTE
83,you should consider disabling Copy-on-Write for the directory that the database resides in for better I/O.
83,Don’t forget to enable scrubbing service for Btrfs
83,2 Likes
83,denisrud
83,(Denis)
83,"January 6, 2023,"
83,7:18pm
83,When I’ve run yabs.sh on btrfs I’ve observed a 15-20% degradation in read and write compared to the host. I don’t know why that is. I chose lvm and the io rate at times was even better than host. But I don’t know if in the long run any of the filesystems will suit my needs and if the degradation is actually relevant in the first place.
83,k4my4b
83,"January 6, 2023, 10:16pm"
83,denisrud:
83,When I’ve run yabs.sh on btrfs I’ve observed a 15-20% degradation in read and write compared to the host
83,"Btrfs is being actively developed, you might wanna take a look at recent changes merged in linux 6.1 and 6.2"
83,denisrud:
83,I chose lvm and the io rate at times was even better than host. But I don’t know if in the long run any of the filesystems will suit my needs and if the degradation is actually relevant in the first place
83,"If future proofing is a priority then Btrfs is pretty much the way forward, Btrfs has become the default file system for most major distros and companies like synology have already started shipping many of their NAS products with btrfs out of the box."
83,TomvB
83,(Tom)
83,"January 7, 2023,"
83,2:16pm
83,The default option for LXD is ZFS at the moment. I’ve been using it for a few years now without any problem. The performance is excellent (especially if you have a lot of memory).
83,You can also enable ZFS in regular Ubuntu desktop installations. ZFS vs BTRFS depends on your workload. I have docker in LXD VMs with ZFS storage. Btrfs is the best option for native docker support in LXD containers. ZFS is also the default in Proxmox. It is part of the kernel. I’ve never run into any limitations with ZFS. I usually adjust ashift to 12 for 4k blocks and sometimes the ARC memory cache limit.
83,Check the documentation for more information:
83,LXD documentation
83,Storage drivers
83,"LXD supports the following storage drivers for storing images, instances and custom volumes: Directory - dir, Btrfs - btrfs, LVM - lvm, ZFS - zfs, Ceph RBD - ceph, CephFS - cephfs, Ceph Object - ce..."
83,ckruijntjens
83,(Chris Kruijntjens)
83,"January 9, 2023,"
83,2:14pm
83,"I agree,"
83,I used btrfs before. now i use zfs and the performance increase on zfs is great!
83,votsalo
83,"January 23, 2023, 11:48am"
83,I just found this interesting article which compares btrfs and zfs.
83,"RedHat does not seem to like either and is rooting for Stratis, a user-space daemon that coordinates various other disk management technologies (seems similar to what LXD is doing for containers)."
83,"The choice of filesystem impacts your procedures about backups, snapshots, etc."
83,"I am very happy with ZFS, and I use it more and more."
83,"Important ZFS features are mirroring (RAID), snapshots, incremental send/receive, compression, and encryption."
83,Make sure that your choice of database works well with the filesystem.
83,I’ve adopted some performance tuning recommendations for the recordsize of zfs filesystems used by mariadb.
83,"Admittedly this increases the complexity of setting up the database as I need to attach two additional filesystems to the mariadb container (data, log), and configure mariadb to use them."
83,"If you do not do such special tuning, the performance may still be acceptable."
83,"Run a benchmark to compare running the database in a container or outside of LXD, with different filesystems."
83,"The “benchmark” that I use is timing the loading of a large .sql file, previously created with mysqldump."
83,I generally use a single debian-based mariadb container and have the website containers use that.
83,"I made an exception for a larger database, which I run on the same container as the website."
83,"My standard two-container configuration was causing this particular website/database to hang on certain database operations, for reasons that I have not fully understood."
83,"Because of this, I may remove the special mariadb tuning that I mentioned."
83,Beware of too much optimization.
83,Home
83,Categories
83,FAQ/Guidelines
83,Terms of Service
83,Privacy Policy
83,"Powered by Discourse, best viewed with JavaScript enabled"
87,MariaDB | Data Workspace Services
87,HomeServicesDatabasesDatabase ServicesMySQLOraclePostgreSQLMariaDBMongoDBMS SQL ServerContact UsMoreHomeServicesDatabasesDatabase ServicesMySQLOraclePostgreSQLMariaDBMongoDBMS SQL ServerContact UsHomeServicesDatabasesDatabase ServicesMySQLOraclePostgreSQLMariaDBMongoDBMS SQL ServerContact Us
87,"MariaDBAdditional Information At Dataworkspace, we understand the importance of a well-optimized and efficiently managed MariaDB database in driving your organization's success. Whether you need expert administration or robust development services, our team of skilled professionals is here to help you unleash the full potential of MariaDB. In this guide, we will outline the comprehensive services we offer to ensure seamless database management and development.MariaDB Database Administration: Our team of experienced MariaDB DBAs excels in managing and optimizing MariaDB databases. Key aspects of our DBA services include:Installation and Configuration: We ensure a seamless and error-free installation and configuration of your MariaDB database, customized to meet your specific requirements and environment.Performance Tuning and Optimization: We analyze query execution, optimize SQL queries, fine-tune database parameters, and implement indexing strategies to maximize performance and enhance the overall efficiency of your MariaDB database.Backup and Recovery: We implement robust backup and recovery solutions to safeguard your data. Our experts design and execute backup strategies, perform regular backups, and develop comprehensive disaster recovery plans to minimize downtime and ensure data integrity.High Availability and Scalability: We configure and maintain high availability solutions for MariaDB, such as MariaDB Cluster and replication setups, to ensure fault tolerance, scalability, and data redundancy. We design and implement efficient failover and load balancing mechanisms to ensure continuous availability of your MariaDB database.Security and Access Control: We implement stringent security measures, including user management, role-based access control, data encryption, and audit trail implementation, to protect your sensitive data and ensure compliance with industry regulations.MariaDB Database Development: Our team of skilled developers is proficient in MariaDB database development, offering a wide range of services, including:Database Design and Modeling: We design efficient and scalable database schemas using MariaDB's data modeling tools. Our experts ensure data integrity, optimize performance, and align the database structure with your application requirements.SQL Development: We develop robust and efficient SQL queries to meet your specific application needs. Our expertise extends to stored procedures, functions, triggers, and views, allowing for seamless integration of application logic with the MariaDB database.Data Integration and ETL: We assist in designing and implementing Extract, Transform, Load (ETL) processes to efficiently integrate data from various sources into your MariaDB database. Our experts leverage ETL tools to ensure seamless data migration, synchronization, and transformation.Performance Optimization: We analyze and optimize SQL queries, indexes, and database structures to improve overall performance. Our team identifies and resolves bottlenecks, fine-tunes MariaDB parameters, and optimizes indexing strategies to ensure optimal query execution and response times.Database Upgrades and Migrations: We assist in upgrading your MariaDB database to the latest versions, ensuring compatibility, data integrity, and minimal downtime. Additionally, we handle database migrations from other platforms to MariaDB, facilitating a smooth transition with minimal disruption.MariaDB Application Development: Our team has extensive experience in developing applications using MariaDB technologies, including:MariaDB Connector/ODBC and Connector/J: We utilize the official MariaDB connectors for ODBC and Java to develop applications that interact seamlessly with your MariaDB database.Web Application Development: We leverage MariaDB with popular web application frameworks like PHP and Node.js to develop scalable and secure web applications tailored to your business needs.API Development: We build robust APIs using MariaDB, enabling seamless integration of your applications with external systems and services.Conclusion: At Dataworkspace we offer comprehensive MariaDB database administration and development services, enabling you to harness the full potential of your MariaDB database. With our expert DBAs and skilled developers, we ensure seamless database management, optimal performance, and efficient development practices. Trust us to handle your MariaDB environment while you focus on leveraging your data to drive business success. Contact us today to take your MariaDB database to new heights"
87,"Data Workspace ServicesCopyright © 2023 Data Workspace Services - All Rights Reserved. This website uses cookies.We use cookies to analyze website traffic and optimize your website experience. By accepting our use of cookies, your data will be aggregated with all other user data.Accept"
88,"Low Latency Storage Optimizations for Proxmox, KVM, & QEMU | Blockbridge Knowledgebase"
88,OVERVIEW
88,SUMMARY
88,Tuning Reduces Latency
88,HARDWARE CONCEPTS
88,NUMA Topology
88,Processor Topology
88,BASELINE PERFORMANCE
88,Optimized Bare-Metal Latency
88,Non-Optimized Guest QD1 Latency
88,TUNING PROCEDURE
88,QEMU IOThreads
88,NIC Interrupt Modulation
88,NIC Interrupt Affinity
88,QEMU VCPU Affinity
88,QEMU IOThread Affinity
88,Guest Halt Polling
88,Processor C-States
88,Processor Vulnerability Mitigtion
88,ENVIRONMENT
88,Network Diagram
88,Description
88,Software Versions
88,Hardware and Networking
88,ADDITIONAL RESOURCES
88,"Low Latency Storage Optimizations for Proxmox, KVM, & QEMU"
88,Inline storage latency limits the performance of applications that
88,"rely on databases such as MySQL, PostgreSQL, and MariaDB. This"
88,technote describes how to optimize I/O latency in a
88,"performance-critical virtual environment consisting of KVM, QEMU, and"
88,"Proxmox. Using a step-by-step approach, we explore essential tuning"
88,concepts and quantify the effects of configuration changes across a
88,"range of block sizes using a QD1 workload. Through tuning, we"
88,demonstrate how to reduce latency by up to 40% and increase QD1 IOPS
88,by 65%.
88,Tests were conducted using Proxmox
88,7.3 on a 16-core
88,AMD RYZEN 5950X processor with Mellanox 25-gigabit networking in a
88,production customer hosting environment. The storage system under test
88,is a DELL-NVME48-ZEN3
88,running Blockbridge 6. The network storage protocol is NVMe/TCP. For
88,"each configuration change, we used"
88,fio to measure
88,QD1 latency. Each data point collected represents the average
88,performance over a 10-minute interval following a 5-minute warm-up.
88,"Optimizations are limited to tunable software and hardware parameters,"
88,do not involve third-party drivers or software modification and are
88,fit for a production environment.
88,SUMMARY
88,Tuning Reduces Latency
88,You can run applications that require high availability and low
88,latency on commodity hardware using Proxmox. A guest can achieve QD1
88,I/O latency within roughly 10 microseconds of bare metal by optimizing
88,both the host and guest.
88,The chart below compares non-optimized guest latency with optimized
88,guest latency and includes optimized bare-metal latency as a
88,reference. The data shows that a 40% reduction in QD1 latency is
88,achievable through system tuning.
88,HARDWARE CONCEPTS
88,Performance optimization requires an understanding of your system’s
88,"processor and memory layout. High-performance packet processing,"
88,"message passing, and inter-thread synchronization depend on"
88,cache-to-cache and memory latencies. The following sections cover the
88,essential concepts needed to understand your hardware.
88,NUMA Topology
88,Start by evaluating the system’s
88,NUMA
88,topology. It is important to constrain performance critical workloads
88,to a single NUMA node to minimize memory latency. Since we’re dealing
88,"with network-attached storage, it makes sense to identify which NUMA"
88,node the NIC connects to and the associated set of NUMA-local
88,CPUs. This information is conveniently available in
88,sysfs.
88,# The NUMA node that the NIC is connected to:
88,root@host:~# cat /sys/class/net/enp45s0f0np0/device/numa_node
88,# The CPUs that are local to the NIC's NUMA node:
88,root@host:~# cat /sys/class/net/enp45s0f0np0/device/local_cpulist
88,0-31
88,The information above indicates that our CPU has uniform memory access
88,(numa_node is -1) and that all logical CPUs are an equal distance
88,from RAM (every CPU is a local CPU).
88,Tip: Some systems can hide the details of
88,"NUMA. If you know that you have multiple NUMA domains, but don’t see"
88,"them, check your BIOS NPS (NUMA Per Socket) settings."
88,Processor Topology
88,"With most modern CPUs, optimization requires an understanding of the"
88,processor’s internal architecture. The logical block diagram below
88,shows an AMD Ryzen 5950x processor. Notice that it has cores
88,distributed across two
88,chiplets.
88,┌────────────────────────────┐
88,CORE CHIPLET DIE (CCD-0) │
88,│ ┌────────────────────────┐ │
88,│ │
88,│ │
88,┌──────────────────────────────┐
88,│ │
88,CORE COMPLEX (CCX)
88,│ │
88,I/O CONTROLLER DIE (cIOD)
88,│ │
88,8 CORE / 16 THREAD
88,│ │
88,│ ┌──────────┐
88,┌────────────┐ │
88,┌────────┐
88,│ │
88,32MB SHARED L3
88,├───────────────┤
88,│ │
88,│ │
88,├───────────────┤
88,│──│
88,MEMORY
88,├──────┤
88,DDR4
88,│ └────────────────────────┘ │
88,│ │
88,│──│ CONTROLLER ├──────┤
88,DRAM
88,└────────────────────────────┘
88,│ │
88,│ │
88,│ │ INFINITY │
88,└────────────┘ │
88,└────────┘
88,┌────────────────────────────┐
88,│ │
88,FABRIC
88,┌────────────┐ │
88,┌────────┐
88,CORE CHIPLET DIE (CCD-1) │
88,│ │
88,│ │
88,│ ┌────────────────────────┐ │
88,│ │
88,│──│
88,I/O
88,├──────┤
88,25Gb
88,│ │
88,├───────────────┤
88,│──│ CONTROLLER ├──────┤
88,NIC
88,│ │
88,CORE COMPLEX (CCX)
88,├───────────────┤
88,│ │
88,│ │
88,8 CORE / 16 THREAD
88,│ │
88,│ └──────────┘
88,└────────────┘ │
88,└────────┘
88,│ │
88,32MB SHARED L3
88,│ │
88,└──────────────────────────────┘
88,│ │
88,│ │
88,│ └────────────────────────┘ │
88,└────────────────────────────┘
88,Each core within a chiplet has uniform access to devices and main
88,"memory. However, core-to-core communication between chiplets is more"
88,expensive than within a chiplet. The benchmark results below
88,demonstrate the penalty using a test that measures synchronization
88,latency between threads pinned to different cores.
88,Results are given in
88,nanoseconds.
88,CPU: AMD Ryzen 9 5950X 16-Core Processor
88,Num cores: 16 (hyperthreads disabled)
88,Num iterations per samples: 10000
88,Num samples: 300
88,Single-writer single-reader latency on two shared cache lines
88,186
88,187
88,186
88,187
88,186
88,188
88,186
88,193
88,187
88,192
88,187
88,195
88,187
88,195
88,188
88,195
88,186
88,187
88,186
88,187
88,186
88,188
88,187
88,193
88,187
88,194
88,187
88,195
88,188
88,195
88,188
88,195
88,186
88,187
88,187
88,187
88,186
88,188
88,186
88,192
88,187
88,195
88,187
88,195
88,189
88,196
88,191
88,196
88,187
88,187
88,187
88,187
88,187
88,187
88,187
88,189
88,187
88,192
88,186
88,195
88,187
88,195
88,187
88,195
88,Min
88,"latency: 41.5ns ±0.0 cores: (6,0)"
88,Max
88,"latency: 195.5ns ±0.0 cores: (13,7)"
88,Mean latency: 122.6ns
88,The data above shows sub-50ns latencies when communicating between
88,cores on the same chiplet. Latencies rise to over 190ns when the cores
88,are on different chiplets. That’s a 4.7x latency penalty for
88,"core-to-core synchronization latency across chiplets. Therefore,"
88,peak performance for our I/O benchmarks will be achieved by
88,"constraining our workload to either one of the chiplets (i.e.,"
88,"physical cores 0-7 or 8-15). For the sake of simplicity, we’ll make"
88,"use of the first chiplet (i.e., cores 0-7)."
88,Note: A careful reader will note the
88,potential tradeoffs in cores per CCX. A processor with two CCDs and
88,"eight cores per CCX (i.e., 2x8) is radically different than a"
88,"processor with eight CCDs and two cores per CCX (i.e., 8x2)."
88,Tip: There is an excellent article on
88,Anandtech that describes the Ryzen
88,architecture
88,and quantifies the latency impact of cross-core
88,synchronization. You can measure it for yourself with a
88,core-to-core latency
88,benchmark. Please
88,note that the tests used to generate the Anandtech results are
88,different than what is provided above: the results are not directly
88,comparable as they test fundamentally different operations.
88,BASELINE PERFORMANCE
88,Optimized Bare-Metal Latency
88,Measuring optimized bare-metal latency establishes the best-case
88,performance achievable on the host without virtualization. Use it to establish
88,a lower bound on VM latency since the guest can’t outperform the host.
88,Tip: Complete optimization of a bare metal
88,"system is worthy of a separate technote. However, if you apply the"
88,"host tunings found in this technote, you will achieve comparable"
88,performance.
88,Non-optimized Guest Latency
88,Non-Optimized Guest latency establishes the performance of a
88,non-optimized guest operating on a non-optimized host. This metric
88,represents the default performance of the system.
88,Note: Since the storage we are testing is
88,"fast, the overhead of virtualization is relatively high. Our baseline"
88,"measurement show an increase in latency of over 2.5x, translating to a"
88,60% reduction in QD1 IOPS!
88,TUNING PROCEDURE
88,Virtual I/O processing involves synchronized communication between the
88,"guest’s virtual storage controller, QEMU’s I/O processing logic, and"
88,"the storage device. In the case of network-attached storage, the"
88,"“storage device” is a NIC. To achieve best-case I/O latency, our"
88,optimization efforts will focus on:
88,"The physical CPU handling the NVMe/TCP device (i.e., NIC) interrupts"
88,The physical CPU running QEMU’s I/O logic
88,The physical CPU running the guest’s VCPU
88,Tip: You know you have it right if the NIC
88,"interrupts, VCPU, and IOThread share an L3 cache."
88,QEMU IOThreads
88,Minimum latency requires that your guest VM uses an IOThread to
88,"offload I/O processing. With Proxmox, you must use the"
88,scsi-virtio-single storage controller. Our testing shows that
88,aio=native and aio=io_uring offer comparable overall performance.
88,Our recommendation is to use aio=native where possible based on
88,code maturity.
88,"Tip: For a technical comparison of AIO models,"
88,"the quantified impact of IOThreads, and benchmark analysis, see"
88,"Optimizing Proxmox: iothreads, aio, & io_uring"
88,You can verify that your virtual machine is configured correctly by
88,reviewing the configuration using the Proxmox shell. In the example
88,"below, the storage pool name is bb-nvme and the VMID is 101. There"
88,are two disks: disk-0 and disk-1.
88,root@host# qm config 101 | grep scsi
88,boot: order=scsi0
88,"scsi0: bb-nvme:vm-101-disk-0,aio=native,iothread=1,size=80G"
88,"scsi1: bb-nvme:vm-101-disk-1,aio=native,iothread=1,size=16G"
88,scsihw: virtio-scsi-single
88,"If your virtual machine is not configured for IOThreads, use the qm"
88,set command to update the guest configuration. You will need to
88,stop and start the guest for the changes to fully take effect.
88,"# example: configuring virtio-scsi-single, aio=native, and iothreads"
88,"root@host# qm set 101 --scsihw virtio-scsi-single --scsi1 bb-nvme:vm-101-disk-1,aio=native,iothread=1"
88,root@host# qm stop 101
88,root@host# qm start 101
88,Performance Impact Of QEMU IOThreads
88,The graph below shows a comparison of performance with and without IOThreads
88,enabled. The performance with IOThreads enabled is shown in blue. Latency
88,improvements range from 12% to 20%.
88,NIC Interrupt Modulation
88,Interrupt Modulation (aka Interrupt
88,Coalescing) is a
88,mechanism to reduce the number of interrupts issued to a CPU. When
88,"configured, your NIC will delay sending an interrupt in an attempt to"
88,batch multiple notifications with a single interrupt. This can reduce
88,"CPU utilization and increase throughput, at the expense of latency."
88,There are two major types of interrupts: receive and transmit. Receive
88,interrupts allow the NIC to notify the operating system that a packet
88,has arrived. Transmit interrupts signal the operating system that
88,packets were transmitted and resources can be reclaimed.
88,"Optimal values for NIC interrupt coalescing are NIC, CPU, and use-case"
88,"dependent. By default, Mellanox NICs are optimized for balanced"
88,"performance. Our goal is minimum latency. Therefore, we must ensure"
88,that the NIC does not hold on to packets in an attempt to optimize
88,resources.
88,"On our Mellanox ConnectX-4 NIC, we’ll:"
88,disable adaptive receive coalescing
88,set the receive coalescing delay to 1
88,Our system has a dual-port 25Gb NIC configured in an active-active
88,LACP LAG. We’ll need
88,to modify the coalescing settings for both ports.
88,The example below shows the default settings for one of our
88,ethernet ports.
88,root@host:~# ethtool -c enp45s0f1np1
88,Coalesce parameters for enp45s0f1np1:
88,Adaptive RX: on
88,TX: on
88,stats-block-usecs: n/a
88,sample-interval: n/a
88,pkt-rate-low: n/a
88,pkt-rate-high: n/a
88,rx-usecs: 8
88,rx-frames: 32
88,rx-usecs-irq: n/a
88,rx-frames-irq: n/a
88,tx-usecs: 8
88,tx-frames: 128
88,tx-usecs-irq: n/a
88,tx-frames-irq: n/a
88,The example below shows how to disable adaptive receive modulation and
88,set the receive modulation interval for both ethernet ports.
88,root@host:~# ethtool -C enp45s0f0np0 adaptive-rx off
88,root@host:~# ethtool -C enp45s0f0np0 rx-usecs 1
88,root@host:~# ethtool -C enp45s0f1np1 adaptive-rx off
88,root@host:~# ethtool -C enp45s0f1np1 rx-usecs 1
88,Performance Impact Of Interrupt Modulation
88,The graph below shows incremental improvements achieved by adjusting
88,interrupt modulation. Notice that substantial gains occur only when
88,the I/O size exceeds 8KiB: this correlates with our networking
88,MTU of
88,9000.
88,Our findings suggest that our NIC employs heuristics to optimize
88,"network latency, even when the adaptive algorithms are disabled. When"
88,"our I/O reply data fits within a single ethernet frame, the NIC sees"
88,no benefit in coalescing interrupts (as there’s a long time between successive
88,"packets). However, when our I/O reply data takes"
88,"multiple ethernet frames to transfer, the NIC invokes the timer-based coalescing"
88,"logic, likely while holding the second frame in hopes of receiving a"
88,third.
88,"Tip: MTU matters! If your MTU is 1500, expect"
88,to see gains for I/O sizes exceeding 1KiB
88,NIC Interrupt Affinity
88,Modern NICs implement multiple packet queues to
88,facilitate Receive Side
88,Scaling
88,"(i.e., RSS), Flow"
88,"Steering,"
88,"QoS, and more. Typically, each packet queue has an associated"
88,Message Signaled
88,Interrupt
88,"to notify the operating system of packet-related events. By default,"
88,a NIC’s packet queues and interrupts are evenly distributed across CPU
88,cores.
88,"Internally, a NIC uses a hash function to associate a packet flow"
88,with a packet queue. Linux assigns responsibility for a packet
88,queue to a physical CPU using an interrupt mask. The seemingly
88,random association of flows to queues (caused by the hash function)
88,and the fair distribution of interrupts over available CPUs leads to
88,unpredictable behavior and performance anomalies in latency-sensitive
88,workloads.
88,"To optimize performance, we’ll need to ensure that our NVMe/TCP flow"
88,gets associated with a packet queue that routes to the first
88,"chiplet, where our guest will be running. A straightforward approach"
88,is to modify the interrupt masks of each packet queue.
88,Our system has a dual-port 25Gb NIC configured in an active-active
88,LACP LAG. For
88,"consistency, we’ll need to specify interrupt affinity for both"
88,"ports. Using the list of NIC interrupts available in sysfs, we can"
88,"set interrupt affinity dynamically via the /proc filesystem, as"
88,shown below.
88,# Direct all interrupts for Port 0 (interface enp1s0f0np0) to CPU 1
88,for irq in /sys/class/net/enp1s0f0np0/device/msi_irqs/* ; \
88,do echo 2 > /proc/irq/$(basename $irq)/smp_affinity ; \
88,done
88,# Direct all interrupts for Port 1 (interface enp1s0f1np1) to CPU 1
88,for irq in /sys/class/net/enp1s0f1np1/device/msi_irqs/* ; \
88,do echo 2 > /proc/irq/$(basename $irq)/smp_affinity ; \
88,done
88,Performance Impact of Interrupt Affinity
88,"Interrupt affinity on its own, with this CPU configuration, is not"
88,expected to affect performance significantly. The benefits of
88,interrupt affinity will be recognized only when the guest’s QEMU
88,threads are pinned to the same chiplet that handles the NIC interrupts
88,"(i.e., the next section)"
88,The graph below shows the latency effect of our changes to interrupt
88,affinity.
88,QEMU VCPU Affinity
88,A virtual machine is a process comprised of several threads spawned by
88,"QEMU. As previously established, we need these threads to execute on"
88,the same chiplet that handles the NIC interrupts. Proxmox does not
88,have a built-in facility to manage CPU affinity that’s flexible enough
88,"to pin specific QEMU threads to specific CPUs. However, you can"
88,manually administer affinity using basic tools available in the
88,"shell. To get and set the CPU affinity, use"
88,taskset.
88,Determine The PID of QEMU Process
88,You can find the main PID for your VM using qm list.
88,root@host:~# qm list
88,VMID NAME
88,STATUS
88,MEM(MB)
88,BOOTDISK(GB) PID
88,101
88,TestUbuntuVM002
88,running
88,8192
88,100.00
88,200906
88,"You can view a list of all threads associated with your VM, use"
88,ps. The example below shows the threads for our test VM
88,configuration which has four VCPUs and IOThread.
88,root@host:~# ps -T -p 200906
88,PID
88,SPID TTY
88,TIME CMD
88,200906
88,200906 ?
88,00:00:01 kvm
88,200906
88,200907 ?
88,00:00:00 call_rcu
88,200906
88,200908 ?
88,00:00:22 kvm
88,200906
88,200909 ?
88,00:03:22 kvm
88,200906
88,200929 ?
88,00:09:17 CPU 0/KVM
88,200906
88,200930 ?
88,00:00:05 CPU 1/KVM
88,200906
88,200931 ?
88,00:00:02 CPU 2/KVM
88,200906
88,200932 ?
88,00:00:03 CPU 3/KVM
88,Setting VCPU Affinity
88,Set affinity for the VCPU threads of the guest VM using taskset as
88,shown below. We must confine execution to the physical cores of the
88,"first chiplet (i.e., CPUs 0-7). You can pin each VCPU thread to a"
88,dedicated core for more predictable results (as shown in the example
88,below).
88,root@host:~# taskset -p --cpu-list 4 200929
88,pid 200929's current affinity list: 0-31
88,pid 200929's new affinity list: 4
88,root@host:~# taskset -p --cpu-list 5 200930
88,pid 200930's current affinity list: 0-31
88,pid 200930's new affinity list: 5
88,root@host:~# taskset -p --cpu-list 6 200931
88,pid 200931's current affinity list: 0-31
88,pid 200931's new affinity list: 6
88,root@host:~# taskset -p --cpu-list 7 200932
88,pid 200932's current affinity list: 0-31
88,pid 200932's new affinity list: 7
88,QEMU IOThread Affinity
88,"When a guest VM executes a disk I/O operation, the guest OS submits a"
88,request to the hypervisor and waits for a completion event. By
88,"default, the QEMU main"
88,loop
88,handles requests and completions. An IOThread provides a dedicated
88,event loop operating in a separate thread that handles I/O. IOThreads
88,offload work from the “main loop” into a separate thread that executes
88,"concurrently, which reduces latency."
88,"Previously, we established CPU 1 for NIC interrupt handling. In"
88,"theory, you can permit the IOThread to float across all the cores"
88,on the first chiplet and remain local to the NIC
88,"interrupts. However, to reduce scheduler latency, maximize cache"
88,"efficiency, and further enable micro-optimizations, we’ll bind the"
88,IOThread execution to CPU 2.
88,Determine The PID Of The IOThread
88,"To find the PID, we can use the qm"
88,monitor
88,command
88,root@host:~# qm monitor 101
88,Entering Qemu Monitor for VM 101 - type 'help' for help
88,qm> info iothreads
88,iothread-virtioscsi1:
88,thread_id=200909
88,poll-max-ns=32768
88,poll-grow=0
88,poll-shrink=0
88,aio-max-batch=0
88,iothread-virtioscsi0:
88,thread_id=200908
88,poll-max-ns=32768
88,poll-grow=0
88,poll-shrink=0
88,aio-max-batch=0
88,Set CPU Affinity Of the IOThread
88,"To get and set the CPU affinity, use"
88,taskset.
88,root@host:~# taskset -p --cpu-list 2 200909
88,pid 200909's current affinity list: 0-31
88,pid 200909's new affinity list: 2
88,Performance Impact Of VCPU and IOThread Affinity
88,The graph below shows the combined latency effect of pinning the
88,guest’s VCPUs and IOThread to the same chiplet that handles the NIC
88,interrupts. A consistent improvement across all I/Os sizes correlates
88,with reduced inter-core synchronization latency and caching.
88,Guest Halt Polling
88,A significant source of I/O latency in virtual machines can be
88,"attributed to delays in detecting completion events. Typically, when a"
88,"guest VCPU becomes idle or is otherwise blocked, the guest OS hands"
88,control over to the hypervisor allowing it to perform other
88,tasks. This context
88,switch results in
88,significant latency for several reasons:
88,"After an I/O completes, our VCPU might not be immediately scheduled"
88,if other runnable processes are present; I/O latency becomes
88,correlated with the system load.
88,"If our VCPU is not actively polling for events, QEMU must send"
88,a notification to schedule the VCPU for execution.
88,We lose the benefit of the processor cache if other workloads
88,pollute it or our VCPU gets scheduled on a different physical CPU.
88,One solution to minimize the “wakeup latency” is to use the
88,cpuidle_haltpoll
88,driver to avoid yielding the CPU altogether. Instead of returning
88,"control to the hypervisor when the guest is idle, the driver polls for"
88,"events for a short period, reducing completion latency at the expense"
88,of CPU cycles.
88,Warning: Use this technique sparingly and only
88,with low-latency storage. You will likely only see a noticeable
88,benefit if your storage is capable of delivering sub hundred
88,microsecond latency).
88,Install The cpuidle-haltpoll Kernel Module
88,"To load the cpuidle-haltpoll, use"
88,modprobe:
88,root@guest:~# modprobe cpuidle-haltpoll force=1
88,Tip: Some distributions do not package the
88,cpuidle_haltpoll driver with the core kernel modules to reduce
88,"footprint. For example, you need to install the"
88,linux-modules-extra-*-generic package on Ubuntu.
88,Confirm The cpuidle-haltpoll Module Is Loaded
88,Find it in the output of lsmod:
88,root@guest:~# lsmod | grep cpuidle
88,cpuidle_haltpoll
88,16384
88,Enable the Haltpoll CPU Governor
88,"To enable the haltpoll governor, update the CPU’s current_governor in sysfs."
88,root@guest:~# echo haltpoll > /sys/devices/system/cpu/cpuidle/current_governor
88,Additional Tuning Parameters
88,Guest Haltpoll Tuning Parameters are
88,available in sysfs. The default parameters are more than suitable for
88,Blockbridge storage.
88,root@guest:~# ls -l /sys/module/haltpoll/parameters/
88,total 0
88,-rw-r--r-- 1 root root 4096 Jan 12 22:28 guest_halt_poll_allow_shrink
88,-rw-r--r-- 1 root root 4096 Jan 12 22:28 guest_halt_poll_grow
88,-rw-r--r-- 1 root root 4096 Jan 12 22:28 guest_halt_poll_grow_start
88,-rw-r--r-- 1 root root 4096 Jan 12 22:28 guest_halt_poll_ns
88,-rw-r--r-- 1 root root 4096 Jan 12 22:28 guest_halt_poll_shrink
88,Performance Impact Of Guest Haltpolling
88,The total impact of the haltpoll governor optimizations is shown in the graph
88,below.
88,Processor C-States
88,C-states are a
88,power-saving mechanism for CPUs that are idle. The basic C-states
88,(defined by ACPI) are:
88,C0: Active - executing instructions
88,"C1: Halt - not executing instructions, but can return to C0"
88,“instantly”
88,"C2: StopClock - similar to C1, with a delayed transition to C0"
88,"While a processor waits for I/O completion (i.e., an interrupt), it is"
88,"often idle. When a processor is idle, it can optionally stop"
88,processing instructions and shut down internal subsystems to save
88,power. This allows a processor to use the power for other
88,"purposes. For example, it may decide to boost the frequency of"
88,another core that’s busy. The transition from an idle state back to
88,an active state has a measurable penalty known as exit latency.
88,The exit latency from C1 to C0 is measured in low single-digit
88,microseconds. The exit latency from C2 to C0 is measured in the
88,low tens of microseconds. A table of exit latencies reported by our
88,5950X is shown below.
88,C-STATE
88,DESCRIPTION
88,EXIT LATENCY
88,ACTIVE
88,0 us
88,HALT
88,1 us
88,STOP-CLOCK
88,18 us
88,SLEEP
88,350 us
88,Tip: Exit latencies for your processor may be available in /sys/devices/system/cpu/cpu0/cpuidle/state*
88,Disable C-States For Selected CPUs
88,We can reduce storage latency by several microseconds by disabling
88,C-States on the IOThread and NIC interrupt CPUs.
88,# Disable Processor Idle States for CPUs 1 and 2
88,"root@host:~# cpupower --cpu-list 1,2 idleset -d 2"
88,"root@host:~# cpupower --cpu-list 1,2 idleset -d 1"
88,Warning: Disabling idle states can lead to
88,higher power utilization and thermal-based frequency throttling: apply
88,sparingly.
88,Performance Impact Of Processor C-States
88,The graph below shows the latency effect of our changes to the
88,processor idle states. Incremental latency improvements are limited to
88,about 1 microsecond since these cores are usually busy enough to
88,operate in C1 and C0.
88,Tip: More focused testing suggests that the
88,IOThread contributes most of the performance benefit. To save
88,"additional power, consider disabling C1 only on the IOThread core."
88,Processor Vulnerability Mitigation
88,Attacks on Transient execution CPU
88,vulnerabilities
88,can be used to extract sensitive data in multi-user systems. The
88,vulnerabilities are a byproduct of how modern CPUs achieve high
88,performance. Operating systems implement software-based techniques to
88,mitigate vulnerabilities. The overhead of mitigations is significant
88,relative to the performance of high-speed storage.
88,"By default, mitigations are enabled. You can check what mitigations"
88,are in place for your CPU as shown below.
88,# Show Vulnerabilities with Mitigation
88,root@host:~# lscpu | grep Mitigation
88,Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
88,Vulnerability Spectre v1:
88,Mitigation; usercopy/swapgs barriers and __user pointer sanitization
88,Vulnerability Spectre v2:
88,"Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, ..."
88,"If you are operating in a trusted environment, it may be safe to"
88,"disable mitigations. To do so, add mitigations=off to the linux"
88,kernel command line parameters of the host. Add or modify the following
88,line in the grub configuration file (/etc/default/grub) and update
88,your grub configuration (update-grub on Ubuntu). A reboot is
88,"required for the changes to take effect. If successful, your CPU"
88,vulnerabilities will show as vulnerable.
88,# Show Vulnerabilities without Mitigations
88,root@host:~# lscpu | grep Vulnerable
88,Vulnerability Spec store bypass: Vulnerable
88,Vulnerability Spectre v1:
88,Vulnerable: __user pointer sanitization and usercopy barriers only; ...
88,Vulnerability Spectre v2:
88,"Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected"
88,Performance Impact Of Vulnerability Mitigation
88,QEMU uses syscalls to interact with the Linux kernel to perform guest
88,I/O and send event notifications. Mitigations have a measurable
88,negative impact on syscall performance. The data below shows latency
88,improvements of up to 2.8 microseconds are possible.
88,Warning: Attacks on CPU vulnerabilities are a
88,real world security concern. We recommend carefully assessing risks
88,before deploying systems with mitigations=off into production.
88,ENVIRONMENT
88,Network Diagram
88,┌──────────────────────────┐
88,┌─────────────────────┐
88,┌──────────────────┐
88,┌────┐
88,PROXMOX 7.3
88,│── NVME/TCP ─┤ 25G SN3700C 100G ├───────┤
88,BLOCKBRIDGE 6.X
88,25G DUAL PORT
88,└──────────────────┘
88,QUAD ENGINE
88,│ VM │
88,X8 GEN3
88,┌──────────────────┐
88,2X 100G DUAL PORT
88,└────┘
88,16 CORE RYZEN
88,|── NVME/TCP ─┤ 25G SN3700C 100G ├───────┤
88,4M IOPS / 25 GB/s
88,└──────────────────┘
88,└──────────────────────────┘
88,└─────────────────────┘
88,Description
88,Proxmox 7.3 (kernel version 5.15.83-1-pve) is installed on an
88,"ASRockRack 1U4LW-X570 with an AMD Ryzen 5950X 16-Core Processor, 128GB"
88,"of RAM, and a single Mellanox dual-port 25Gbit network adapter. The"
88,Mellanox adapter is an x8 Gen3 device with a maximum throughput of
88,63Gbit/s. The server is running with default settings and hyperthreads
88,enabled.
88,The Proxmox host connects to a redundant pair of Mellanox 100G SN3700C
88,switches using an Active-Active LACP LAG. While the Blockbridge
88,"storage is 100G connected, the port speed of the host limits"
88,performance to 25Gbit. The network MTU is 9000.
88,A single virtual machine is provisioned on the host. The VM is
88,"installed with Ubuntu 23.04, running Linux kernel version"
88,5.19.0-21-generic. The VM has four virtual CPUs and 8GB of RAM. The VM
88,has a boot block device containing the root filesystem
88,separate from the storage under test.
88,A read-only workload is executed that fits within the encrypted data
88,cache of the storage system to ensure consistency and repeatability of
88,the results. QD1 tests are executed for seven block sizes. Each test
88,consists of a 5-minute warmup followed by a 10-minute measurement
88,period. A sample workload description appears below:
88,$ cat read-bs4096-qd1.fio
88,[global]
88,rw=read
88,direct=1
88,ioengine=libaio
88,time_based=1
88,runtime=600
88,ramp_time=300
88,numjobs=1
88,cpus_allowed=0
88,[device]
88,filename=/dev/sdb
88,size=1G
88,Software
88,Proxmox Version
88,# pveversion
88,pve-manager/7.3-4/d69b70d4 (running kernel: 5.15.83-1-pve)
88,Linux Kernel Options
88,BOOT_IMAGE=/boot/vmlinuz-5.15.83-1-pve root=/dev/mapper/pve-root ro quiet
88,Blockbridge Version
88,version:
88,6.0.0
88,release:
88,6712.2
88,build:
88,4102
88,Hardware And Networking
88,Server Platform
88,System Information
88,Manufacturer: ASRockRack
88,Product Name: 1U4LW-X570 RPSU
88,Processor
88,Architecture:
88,x86_64
88,CPU op-mode(s):
88,"32-bit, 64-bit"
88,Byte Order:
88,Little Endian
88,Address sizes:
88,"48 bits physical, 48 bits virtual"
88,CPU(s):
88,On-line CPU(s) list:
88,0-31
88,Thread(s) per core:
88,Core(s) per socket:
88,Socket(s):
88,NUMA node(s):
88,Vendor ID:
88,AuthenticAMD
88,CPU family:
88,Model:
88,Model name:
88,AMD Ryzen 9 5950X 16-Core Processor
88,Stepping:
88,Frequency boost:
88,enabled
88,CPU MHz:
88,3400.000
88,CPU max MHz:
88,5083.3979
88,CPU min MHz:
88,2200.0000
88,BogoMIPS:
88,6787.10
88,Virtualization:
88,AMD-V
88,L1d cache:
88,512 KiB
88,L1i cache:
88,512 KiB
88,L2 cache:
88,8 MiB
88,L3 cache:
88,64 MiB
88,NUMA node0 CPU(s):
88,0-31
88,Network Adapter
88,Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
88,"Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACAT"
88,"Flags: bus master, fast devsel, latency 0, IRQ 121, IOMMU group 25"
88,"Memory at c0000000 (64-bit, prefetchable) [size=32M]"
88,Expansion ROM at fcd00000 [disabled] [size=1M]
88,"Capabilities: [60] Express Endpoint, MSI 00"
88,Capabilities: [48] Vital Product Data
88,Capabilities: [9c] MSI-X: Enable+ Count=64 Masked-
88,Capabilities: [c0] Vendor Specific Information: Len=18 <?>
88,Capabilities: [40] Power Management version 3
88,Capabilities: [100] Advanced Error Reporting
88,Capabilities: [150] Alternative Routing-ID Interpretation (ARI)
88,Capabilities: [180] Single Root I/O Virtualization (SR-IOV)
88,Capabilities: [1c0] Secondary PCI Express
88,Capabilities: [230] Access Control Services
88,Kernel driver in use: mlx5_core
88,Kernel modules: mlx5_core
88,Network Adapter PCI Connectivity
88,2.453955] mlx5_core 0000:2d:00.0: firmware version: 14.31.1014
88,2.453985] mlx5_core 0000:2d:00.0: 63.008 Gb/s available PCIe bandwidth (8.0 GT/s PCIe x8 link)
88,"2.740344] mlx5_core 0000:2d:00.0: E-Switch: Total vports 10, per vport: max uc(128) max mc(2048)"
88,"2.743839] mlx5_core 0000:2d:00.0: Port module event: module 0, Cable plugged"
88,3.079562] mlx5_core 0000:2d:00.0: MLX5E: StrdRq(0) RqSz(1024) StrdSz(256) RxCqeCmprss(0)
88,"3.325403] mlx5_core 0000:2d:00.0: Supported tc offload range - chains: 4294967294, prios: 429496729"
88,Network Adapter Link
88,Settings for enp45s0f0np0:
88,Supported ports: [ Backplane ]
88,Supported pause frame use: Symmetric
88,Supports auto-negotiation: Yes
88,Advertised pause frame use: Symmetric
88,Advertised auto-negotiation: Yes
88,Advertised FEC modes: None	 RS	 BASER
88,Link partner advertised link modes:
88,Not reported
88,Link partner advertised pause frame use: No
88,Link partner advertised auto-negotiation: Yes
88,Link partner advertised FEC modes: Not reported
88,Speed: 25000Mb/s
88,Duplex: Full
88,Auto-negotiation: on
88,Port: Direct Attach Copper
88,PHYAD: 0
88,Transceiver: internal
88,Supports Wake-on: d
88,Wake-on: d
88,Current message level: 0x00000004 (4)
88,link
88,Link detected: yes
88,Network Adapter Interrupt Coalesce Settings
88,Adaptive RX: on
88,TX: on
88,stats-block-usecs: n/a
88,sample-interval: n/a
88,pkt-rate-low: n/a
88,pkt-rate-high: n/a
88,rx-usecs: 8
88,rx-frames: 128
88,rx-usecs-irq: n/a
88,rx-frames-irq: n/a
88,tx-usecs: 8
88,tx-frames: 128
88,tx-usecs-irq: n/a
88,tx-frames-irq: n/a
88,ADDITIONAL RESOURCES
88,Blockbridge // Proxmox Overview
88,Blockbridge // Proxmox Storage Guide
88,"Blockbridge // Optimizing Proxmox: iothreads, aio, & io_uring"
88,Blockbridge // Proxmox & ESXi Performance Comparison
88,Oracle Blog // Introduction To Virtio
88,Intel // KVM Tuning Guide
88,"©2023 Blockbridge Networks, LLC. All rights reserved."
88,"Site last generated: Nov 7, 2023"
89,"MariaDB: The Database for All, Built by the Community – Open Source Software"
89,Open Source Software
89,Open Source SoftwareWordPressLinuxLibreOfficeMySQLGIMPLinux KernelApache HTTP ServerGitGNU Compiler Collection (GCC)DockerMozilla FirefoxOpenSSLVLC Media PlayerPostgreSQLBlenderNode.jsTensorFlowJupyter NotebookKubernetesReactElasticsearchVisual Studio CodeMariaDBAudacityInkscapeNextcloudNginxJenkinsQtWiresharkD3.jsBootstrap
89,"MariaDB: The Database for All, Built by the Community"
89,Delving into MariaDB’s Open Source Pedigree and Its Importance in Modern Data Management
89,"MariaDB is more than just another database management system; it’s a demonstration of how the spirit of open source can transform an industry. This page will explore MariaDB, its open source principles, and why it has become an essential part of many enterprises’ data architecture."
89,What is MariaDB?
89,"MariaDB is an open-source relational database management system (RDBMS) that serves as a drop-in replacement for MySQL. It was created by one of MySQL’s original developers, Michael “Monty” Widenius, after concerns arose about MySQL’s acquisition by Oracle. MariaDB aims to maintain high compatibility with MySQL while adding new features and optimizations."
89,MariaDB and Open Source Principles
89,MariaDB embodies the core attributes of open source software:
89,"Freedom to Use: Available for free, MariaDB is easily accessible to startups, established enterprises, and individual developers alike."
89,"Freedom to Study: The source code of MariaDB is open for anyone to inspect, allowing the community and researchers to understand its inner workings."
89,"Freedom to Modify: Companies can customize MariaDB to suit their specific needs, from performance tuning to added functionalities."
89,"Freedom to Share: MariaDB’s welcoming community encourages the sharing of improvements, bug fixes, and even entirely new features."
89,The MariaDB Community
89,"With a vibrant community of developers, database administrators, and users, MariaDB has a thriving ecosystem. Frequent updates, extensions, and user-contributed improvements are commonplace, reinforcing its position as a community-driven project."
89,Why is MariaDB Important?
89,Compatibility and Flexibility
89,"MariaDB offers high compatibility with MySQL, allowing for straightforward migrations. Additionally, it supports a wider range of storage engines and database functionalities."
89,Security
89,"MariaDB places a strong emphasis on database security, offering advanced features like data masking and encryption-at-rest."
89,Scalability and Performance
89,"MariaDB provides robust performance optimization features, including a variety of storage engines, advanced indexing, and the Aria storage engine for high-performance read and write operations."
89,Key Features of MariaDB
89,ColumnStore: A columnar storage engine optimized for analytics and big data solutions.
89,Galera Cluster: Provides synchronous multi-master replication for high availability.
89,JSON Support: Advanced support for JSON data types for better unstructured data handling.
89,"Subqueries: Enhanced support for subqueries, providing more complex and flexible data retrieval options."
89,MariaDB’s Impact on Open Source and Data Management
89,"MariaDB has set a new standard for what an open source RDBMS can achieve. Its emphasis on community-driven development, performance, and security has made it a preferred choice for organizations looking to escape vendor lock-in and foster innovation."
89,Conclusion
89,"MariaDB stands as a testament to the power of open source, offering a feature-rich, secure, and highly compatible database solution. Its strong community backing and forward-looking development strategy make it a compelling choice for modern data needs."
89,Open Source Software
89,Proudly powered by WordPress
90,Bucketed tables in HiveCloudera Docs
90,Bucketed tables in Hive
90,"If you migrated data from earlier Apache Hive versions to Hive 3, you might need to"
90,handle bucketed tables that impact performance.
90,Review how CDP simplifies handling buckets. You learn about best practices for handling dynamic capabilities.
90,"You can divide tables or partitions into buckets, which are stored in the following ways:"
90,As files in the directory for the table.
90,As directories of partitions if the table is partitioned.
90,"Specifying buckets in Hive 3 tables is not necessary. In CDP, Hive 3 buckets data"
90,"implicitly, and does not require a user key or user-provided bucket number as earlier versions"
90,(ACID V1) did. For example:
90,V1:
90,"CREATE TABLE hello_acid (load_date date, key int, value int)"
90,CLUSTERED BY(key) INTO 3 BUCKETS
90,STORED AS ORC TBLPROPERTIES ('transactional'='true');
90,"V2:CREATE TABLE hello_acid_v2 (load_date date, key int, value int);"
90,Performance of ACID V2 tables is on a par with non-ACID tables using buckets. ACID V2 tables
90,are compatible with native cloud storage.
90,A common challenge related to using buckets in tables migrated from earlier versions is
90,"maintaining query performance while the workload or data scales up or down. For example, you"
90,"could have an environment that operates smoothly using 16 buckets to support 1000 users, but a"
90,"spike in the number of users to 100,000 for a day or two creates problems if you do not promptly"
90,tune the buckets and partitions. Tuning the buckets is complicated by the fact that after you
90,"have constructed a table with buckets, the entire table containing the bucketed data must be"
90,"reloaded to reduce, add, or eliminate buckets."
90,"In CDP, you only need to deal with the buckets of the biggest table. If workload demands change"
90,"rapidly, the buckets of the smaller tables dynamically change to complete table JOINs."
90,Bucket configurations
90,You can enable buckets as follows:
90,SET hive.tez.bucket.pruning=true
90,"When you load data into tables that are both partitioned and bucketed, set the hive.optimize.sort.dynamic.partition property to optimize the process:"
90,SET hive.optimize.sort.dynamic.partition=true
90,"If you have 20 buckets on user_id data, the following query returns only the data associated with user_id = 1: SELECT * FROM tab WHERE user_id = 1;"
90,"To best leverage the dynamic capability of table buckets, adopt the following practices:"
90,Use a single key for the buckets of the largest table.
90,"Usually, you need to bucket the main table by the biggest dimension table. For example, the sales table might be bucketed by customer and not by merchandise item or store. However, in this scenario, the sales table is sorted by item and store."
90,"Normally, do not bucket and sort on the same column."
90,A table that has more bucket files than the number of rows is an indication that you should reconsider how the table is bucketed.
91,"First steps with pgBackRest, a backup solution for PostgreSQL - Vettabase"
91,Email
91,sales@vettabase.com
91,Schedule Meeting
91,Calendly Booking
91,Phone
91,+44 203 962 5762
91,Home
91,About
91,Careers
91,Resources
91,Database Services
91,Database Automation
91,Automation for MariaDB
91,Automation for MySQL
91,Automation for PostgreSQL
91,Automation for Cassandra
91,Database Training
91,MariaDB Training
91,MySQL Training
91,Database Upgrade
91,Upgrade for MariaDB
91,Upgrade for MySQL
91,Upgrade for PostgreSQL
91,Upgrade for Cassandra
91,Database Health Check
91,MariaDB Health Check
91,MySQL Health Check
91,PostgreSQL Health Check
91,Cassandra Health Check
91,Monthly DBA Time
91,Monthly DBA Time for MariaDB
91,Monthly DBA Time for MySQL
91,Monthly DBA Time for PostgreSQL
91,Monthly DBA Time for Cassandra
91,Blog
91,Recent Posts
91,MariaDB
91,MySQL
91,Cassandra
91,PostgreSQL
91,Testimonials
91,Vettabase Featured Customers
91,Vettabase Partners
91,Case Study : Treedom
91,Contact
91,Free Consultation
91,Home
91,About
91,Careers
91,Resources
91,Database Services
91,Database Automation
91,Database Health Check
91,Database Training
91,Database Upgrade
91,Monthly DBA Time
91,Blog
91,Vettabase Testimonials
91,Contact
91,Free Consultation
91,"First steps with pgBackRest, a backup solution for PostgreSQL by Pramod Gupta | Aug 21, 2023 | PostgreSQL"
91,"pgBackRest is an open source backup tool that creates physical backups with some improvements compared to the classic pg_basebackup tool. pg_basebackup is included in the PostgreSQL binaries, and it offers a great set of features for hot binary backups, remote backups, and standby building, etc."
91,"pgBackRest is a solution that addresses the shortcomings of pg_basebackup. pgBackRest implements all backup features internally using a custom protocol for communicating with remote systems. Powerful features of pgBackRest include parallel backup and restore, local or remote operation, full, incremental, and differential backup types, backup rotation, archive expiration, backup integrity, page checksums, backup resume, streaming compression and checksums, delta restore, and much more.pgBackRest doesn’t rely on traditional backup tools like tar or rsync. This is a custom solution, and the protocol used by pgBackRest is a perfect fit for PostgreSQL-specific backup challenges. It allows for more flexibility and limits the types of connections that are required to perform a backup, which increases security. pgBackRest is a simple, but feature-rich, reliable backup and restore system that can seamlessly scale up to the largest databases and workloads."
91,Important features of pgBackRest
91,"Full, incremental, and differential backups: pgBackRest supports full, incremental, and differential backups. It uses a differential backup model, which allows for efficient backup storage while maintaining the ability to restore to any point in time (PITR)."
91,"Parallel backup and restore. pgBackRest can perform backups and restores in parallel, taking advantage of multiple CPU cores and significantly reducing the time required for these operations."
91,"Point-in-time recovery (PITR). pgBackRest enables Point-in-Time Recovery, allowing you to restore a PostgreSQL database to a specific point in time, rather than just the time of the last backup."
91,"Compression, encryption and integrity checks. It provides options for compressing backup data on the fly to reduce storage requirements and supports encryption of backup files for enhanced security. pgBackRest also performs integrity checks on backup files, ensuring that they are consistent and can be trusted for recovery."
91,"Backup rotation, retention and throttling. The tool includes built-in retention policies to manage backup rotation automatically. You can control how many backups are kept and when they are removed. pgBackRest also provides the ability to throttle the backup and restore operations, preventing them from overloading the system and affecting production performance."
91,"Delta restore and parallel backup verification. Thanks to the delta restore feature, pgBackRest can quickly apply incremental changes to an existing database to reduce the time required for restoration. The tool also supports parallel verification of backups, ensuring that the backups are valid and reliable for recovery."
91,How to use pgBackRest
91,Installation
91,"A prerequisite for installation is the PGDG repository. If we don’t have it already installed, we should be doing that from yum.postgresql.org."
91,sudo yum install pgbackrest
91,"pgBackRest is developed in Perl, so when you install pgBackRest, all dependent Perl libraries will also get installed if they are not already present."
91,Configuring backups
91,"The first step is to create a stanza definition in /etc/pgbackrest.conf. Here is a simple example. We can see that all the pg options are specified with a 1, which serves as the index of the configuration. These indexes are intended for configuring multiple PostgreSQL hosts. For example, a single master is configured with the pg1-path, pg1-host, and similar options. If a standby is configured, then index the pg- options as pg2-host, pg2-path, etc."
91,sudo bash -c 'cat << EOF  > /etc/pgbackrest.conf
91,[global]
91,repo1-path=/var/lib/pgbackrest
91,repo1-retention-full=2
91,[pg0app]
91,pg1-path=/var/lib/pgsql/11/data
91,pg1-port=5432
91,EOF'
91,"The next step is the creation of a stanza. This needs to be done on the server where the repository is located. It is highly recommended to use the same non-root user under which PostgreSQL processes are configured and running. As mentioned before, a stanza holds the backup configuration related to one PostgreSQL instance. In this step the actual internal structure and the definition of stanza will be created out of the stanza definition."
91,pgbackrest stanza-create --stanza=pg0app --log-level-console=info
91,"Do the necessary parameter changes in PostgreSQL. At a minimal level, WAL archiving should be enabled, and archive_command should be using pgbackrest as shown below."
91,ALTER SYSTEM SET wal_level = 'replica';
91,ALTER SYSTEM SET archive_mode = 'on';
91,ALTER SYSTEM SET archive_command = 'pgbackrest --stanza=pg0app archive-push %p';
91,ALTER SYSTEM SET max_wal_senders = '10';
91,ALTER SYSTEM SET hot_standby = 'on';
91,The above-mentioned parameter changes require the restart of the PostgreSQL instance:
91,sudo systemctl restart postgresql-11
91,Now we can check whether we have configured everything correctly using the following command:
91,pgbackrest check --stanza=pg0app --log-level-console=info
91,Running backups
91,Database backup using pgbackrest can be performed using the following command:
91,pgbackrest backup --stanza=pg0app --log-level-console=info
91,Conclusion
91,"In this blog post, we have covered basic understanding and configuration details of the pgBackRest backup tool. In our future articles, we will tackle its advanced features. This backup tool has a number of them, so we expect the post to be helpful for PostgreSQL users. If you are looking for expert PostgreSQL professional services, please contact the Vettabase team."
91,Reference
91,You can find the pgBackRest documentation here.
91,"All content in this blog is distributed under the CreativeCommons Attribution-ShareAlike 4.0 International license. You can use it for your needs and even modify it, but please refer to Vettabase and the author of the original post. Read more about the terms and conditions: https://creativecommons.org/licenses/by-sa/4.0/"
91,About Pramod Gupta
91,"Pramod Gupta is a database professional, who has been working with SQL and NoSQL databases since year 2011. Before joining Vettabase, he served in a number of companies Like Datavail, Lazada Group, Alibaba Group, Paytm, Byju's and Ola as a DBA, Lead DBA, Principle DBA, Database Architect and Database Manager. Pramod has vast experience with MySQL/MariaDB/Percona Server, MongoDB, PostgreSQL, Cassandra, ScyllaDB, Couchbase and Aerospike databases. His professional interests are database Performance tuning, HA, scalabilty and DB architecture design and planning."
91,Recent Posts
91,Hints to optimise queries with a LIKE comparison
91,"Sep 15, 2022In SQL, using the LIKE operator is a powerful way to find strings that match a certain pattern. It's suitable for most use cases, thanks to its two jolly characters: _ means any one character. % means any sequence of zero or more characters. However, many queries out..."
91,AlloyDB versus PostgreSQL: a performance review
91,"Jul 19, 2022What is faster: PostgreSQL or AlloyDB? Some benchmarks and a performance analysis."
91,Tuning PostgreSQL Auto-vacuum
91,"Apr 28, 2022Why autovacuum is important in PostgreSQL and how to configure it properly."
91,Services
91,Database Automation
91,Database Training
91,Database Health Check
91,Monthly DBA Time
91,Database Upgrade
91,← Previous Post
91,Next Post →
91,"backup , PostgreSQL"
91,0 Comments
91,Submit a Comment Cancel replyYour email address will not be published. Required fields are marked *Comment * Name *
91,Email *
91,Website
91,"Save my name, email, and website in this browser for the next time I comment."
91,Submit Comment
91,Subscribe Now
91,"Read our expert blog with articles on MariaDB, MySQL, PostgreSQL, Cassandra and related tools & technologies."
91,Full Name *Email Address *Choose your databaseAll Database PostsMariaDB PostsMySQL PostsPostgreSQL PostsCassandra PostsPolicy consent *I agree with Vettabase Privacy Policy.Marketing consent *I agree to receive marketing communications and discounts info from Vettabase.Sign Up
91,Email
91,sales@vettabase.com
91,Schedule Meeting
91,Calendly Booking
91,Phone
91,+44 203 962 5762
91,"Vettabase specialises in expert consulting and automation for your database infrastructure. We offer professional services and training for MySQL, MariaDB, PostgreSQL and Cassandra. Our specialties include a wide range of services from performance optimisation to complex database upgrades and migrations."
91,Vettabase LtdRegistered in England and WalesCompany number: 12769372
91,Quick Links
91,Home
91,About
91,Careers
91,Database Automation
91,Database Training
91,Database Health Check
91,Monthly DBA Time
91,Database Upgrade
91,Vettabase Testimonials
91,Blog
91,Contact
91,Recent Posts
91,MariaDB Catalogs: some use cases
91,"Oct 11, 2023This month I attended Monty’s talk about Catalogs at MariaDB Server Fest 2023 in Helsinki. He described this nice feature, which is still under development, and how it will work. But when he talked about the use cases, I was under the impression that catalogs have..."
91,MariaDB/MySQL: working with storage engines
91,"Aug 8, 2023MariaDB and MySQL support several storage engines. See MariaDB and MySQL storage engines: an overview for a discussion about existing MariaDB and MySQL storage engines. Here we'll see how to work with them. We'll see how to obtain information about storage engines,..."
91,MariaDB and MySQL storage engines: an overview
91,"Jul 26, 2023MySQL was the first DBMS to introduce the concept of storage engines in the early 2000s. This was one of its main features. Later, MariaDB extended the storage engine API and included some storage engine maintained by third parties as part of its official..."
91,Policies & Licenses
91,Consultancy Policy
91,Privacy Policy
91,Creative Commons Attribution License (for Vettabase blog only)
91,Follow Us on Social Media
91,FollowFollowFollowFollow
91,"© 2020-2023, Vettabase Ltd., all rights reserved"
91,Website maintained by Butteredhost.com
92,SQLite Database Speed Comparison
92,Small. Fast. Reliable.Choose any three.
92,Home
92,Menu
92,About
92,Documentation
92,Download
92,License
92,Support
92,Purchase
92,Search
92,About
92,Documentation
92,Download
92,Support
92,Purchase
92,Search Documentation
92,Search Changelog
92,Database Speed Comparison
92,Note:
92,This document is very very old.
92,It describes a speed comparison between
92,"archaic versions of SQLite, MySQL and PostgreSQL."
92,The numbers here have become meaningless.
92,This page has been retained only
92,as an historical artifact.
92,Executive Summary
92,A series of tests were run to measure the relative performance of
92,"SQLite 2.7.6, PostgreSQL 7.1.3, and MySQL 3.23.41."
92,The following are general
92,conclusions drawn from these experiments:
92,SQLite 2.7.6 is significantly faster (sometimes as much as 10 or
92,20 times faster) than the default PostgreSQL 7.1.3 installation
92,on RedHat 7.2 for most common operations.
92,SQLite 2.7.6 is often faster (sometimes
92,more than twice as fast) than MySQL 3.23.41
92,for most common operations.
92,SQLite does not execute CREATE INDEX or DROP TABLE as fast as
92,the other databases.
92,But this is not seen as a problem because
92,those are infrequent operations.
92,SQLite works best if you group multiple operations together into
92,a single transaction.
92,The results presented here come with the following caveats:
92,These tests did not attempt to measure multi-user performance or
92,optimization of complex queries involving multiple joins and subqueries.
92,These tests are on a relatively small (approximately 14 megabyte) database.
92,They do not measure how well the database engines scale to larger problems.
92,Test Environment
92,The platform used for these tests is a 1.6GHz Athlon with 1GB or memory
92,and an IDE disk drive.
92,The operating system is RedHat Linux 7.2 with
92,a stock kernel.
92,The PostgreSQL and MySQL servers used were as delivered by default on
92,RedHat 7.2.
92,(PostgreSQL version 7.1.3 and MySQL version 3.23.41.)
92,No effort was made to tune these engines.
92,Note in particular
92,the default MySQL configuration on RedHat 7.2 does not support
92,transactions.
92,Not having to support transactions gives MySQL a
92,"big speed advantage, but SQLite is still able to hold its own on most"
92,tests.
92,I am told that the default PostgreSQL configuration in RedHat 7.3
92,is unnecessarily conservative (it is designed to
92,work on a machine with 8MB of RAM) and that PostgreSQL could
92,be made to run a lot faster with some knowledgeable configuration
92,tuning.
92,Matt Sergeant reports that he has tuned his PostgreSQL installation
92,and rerun the tests shown below.
92,His results show that
92,PostgreSQL and MySQL run at about the same speed.
92,For Matt's
92,"results, visit"
92,Obsolete URL: http://www.sergeant.org/sqlite_vs_pgsync.html
92,SQLite was tested in the same configuration that it appears
92,on the website.
92,It was compiled with -O6 optimization and with
92,"the -DNDEBUG=1 switch which disables the many ""assert()"" statements"
92,in the SQLite code.
92,The -DNDEBUG=1 compiler option roughly doubles
92,the speed of SQLite.
92,All tests are conducted on an otherwise quiescent machine.
92,A simple Tcl script was used to generate and run all the tests.
92,A copy of this Tcl script can be found in the SQLite source tree
92,in the file tools/speedtest.tcl.
92,The times reported on all tests represent wall-clock time
92,in seconds.
92,Two separate time values are reported for SQLite.
92,The first value is for SQLite in its default configuration with
92,full disk synchronization turned on.
92,With synchronization turned
92,"on, SQLite executes"
92,an fsync() system call (or the equivalent) at key points
92,to make certain that critical data has
92,actually been written to the disk drive surface.
92,Synchronization
92,is necessary to guarantee the integrity of the database if the
92,operating system crashes or the computer powers down unexpectedly
92,in the middle of a database update.
92,The second time reported for SQLite is
92,when synchronization is turned off.
92,"With synchronization off,"
92,"SQLite is sometimes much faster, but there is a risk that an"
92,operating system crash or an unexpected power failure could
92,damage the database.
92,"Generally speaking, the synchronous SQLite"
92,times are for comparison against PostgreSQL (which is also
92,synchronous) and the asynchronous SQLite times are for
92,comparison against the asynchronous MySQL engine.
92,Test 1: 1000 INSERTs
92,"CREATE TABLE t1(a INTEGER, b INTEGER, c VARCHAR(100));"
92,"INSERT INTO t1 VALUES(1,13153,'thirteen thousand one hundred fifty three');"
92,"INSERT INTO t1 VALUES(2,75560,'seventy five thousand five hundred sixty');"
92,... 995 lines omitted
92,"INSERT INTO t1 VALUES(998,66289,'sixty six thousand two hundred eighty nine');"
92,"INSERT INTO t1 VALUES(999,24322,'twenty four thousand three hundred twenty two');"
92,"INSERT INTO t1 VALUES(1000,94142,'ninety four thousand one hundred forty two');"
92,PostgreSQL:   4.373
92,MySQL:   0.114
92,SQLite 2.7.6:   13.061
92,SQLite 2.7.6 (nosync):   0.223
92,"Because it does not have a central server to coordinate access,"
92,"SQLite must close and reopen the database file, and thus invalidate"
92,"its cache, for each transaction."
92,"In this test, each SQL statement"
92,is a separate transaction so the database file must be opened and closed
92,and the cache must be flushed 1000 times.
92,"In spite of this, the asynchronous"
92,version of SQLite is still nearly as fast as MySQL.
92,Notice how much slower
92,"the synchronous version is, however."
92,SQLite calls fsync() after
92,each synchronous transaction to make sure that all data is safely on
92,the disk surface before continuing.
92,For most of the 13 seconds in the
92,"synchronous test, SQLite was sitting idle waiting on disk I/O to complete."
92,Test 2: 25000 INSERTs in a transaction
92,BEGIN;
92,"CREATE TABLE t2(a INTEGER, b INTEGER, c VARCHAR(100));"
92,"INSERT INTO t2 VALUES(1,59672,'fifty nine thousand six hundred seventy two');"
92,... 24997 lines omitted
92,"INSERT INTO t2 VALUES(24999,89569,'eighty nine thousand five hundred sixty nine');"
92,"INSERT INTO t2 VALUES(25000,94666,'ninety four thousand six hundred sixty six');"
92,COMMIT;
92,PostgreSQL:   4.900
92,MySQL:   2.184
92,SQLite 2.7.6:   0.914
92,SQLite 2.7.6 (nosync):   0.757
92,"When all the INSERTs are put in a transaction, SQLite no longer has to"
92,close and reopen the database or invalidate its cache between each statement.
92,It also does not
92,have to do any fsync()s until the very end.
92,When unshackled in
92,"this way, SQLite is much faster than either PostgreSQL and MySQL."
92,Test 3: 25000 INSERTs into an indexed table
92,BEGIN;
92,"CREATE TABLE t3(a INTEGER, b INTEGER, c VARCHAR(100));"
92,CREATE INDEX i3 ON t3(c);
92,... 24998 lines omitted
92,"INSERT INTO t3 VALUES(24999,88509,'eighty eight thousand five hundred nine');"
92,"INSERT INTO t3 VALUES(25000,84791,'eighty four thousand seven hundred ninety one');"
92,COMMIT;
92,PostgreSQL:   8.175
92,MySQL:   3.197
92,SQLite 2.7.6:   1.555
92,SQLite 2.7.6 (nosync):   1.402
92,There were reports that SQLite did not perform as well on an indexed table.
92,This test was recently added to disprove those rumors.
92,It is true that
92,SQLite is not as fast at creating new index entries as the other engines
92,(see Test 6 below) but its overall speed is still better.
92,Test 4: 100 SELECTs without an index
92,BEGIN;
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=0 AND b<1000;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=100 AND b<1100;"
92,... 96 lines omitted
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=9800 AND b<10800;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=9900 AND b<10900;"
92,COMMIT;
92,PostgreSQL:   3.629
92,MySQL:   2.760
92,SQLite 2.7.6:   2.494
92,SQLite 2.7.6 (nosync):   2.526
92,"This test does 100 queries on a 25000 entry table without an index,"
92,thus requiring a full table scan.
92,Prior versions of SQLite used to
92,"be slower than PostgreSQL and MySQL on this test, but recent performance"
92,enhancements have increased its speed so that it is now the fastest
92,of the group.
92,Test 5: 100 SELECTs on a string comparison
92,BEGIN;
92,"SELECT count(*), avg(b) FROM t2 WHERE c LIKE '%one%';"
92,"SELECT count(*), avg(b) FROM t2 WHERE c LIKE '%two%';"
92,... 96 lines omitted
92,"SELECT count(*), avg(b) FROM t2 WHERE c LIKE '%ninety nine%';"
92,"SELECT count(*), avg(b) FROM t2 WHERE c LIKE '%one hundred%';"
92,COMMIT;
92,PostgreSQL:   13.409
92,MySQL:   4.640
92,SQLite 2.7.6:   3.362
92,SQLite 2.7.6 (nosync):   3.372
92,This test still does 100 full table scans but it uses
92,uses string comparisons instead of numerical comparisons.
92,SQLite is over three times faster than PostgreSQL here and about 30%
92,faster than MySQL.
92,Test 6: Creating an index
92,CREATE INDEX i2a ON t2(a);CREATE INDEX i2b ON t2(b);
92,PostgreSQL:   0.381
92,MySQL:   0.318
92,SQLite 2.7.6:   0.777
92,SQLite 2.7.6 (nosync):   0.659
92,SQLite is slower at creating new indices.
92,This is not a huge problem
92,(since new indices are not created very often) but it is something that
92,is being worked on.
92,"Hopefully, future versions of SQLite will do better"
92,here.
92,Test 7: 5000 SELECTs with an index
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=0 AND b<100;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=100 AND b<200;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=200 AND b<300;"
92,... 4994 lines omitted
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=499700 AND b<499800;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=499800 AND b<499900;"
92,"SELECT count(*), avg(b) FROM t2 WHERE b>=499900 AND b<500000;"
92,PostgreSQL:   4.614
92,MySQL:   1.270
92,SQLite 2.7.6:   1.121
92,SQLite 2.7.6 (nosync):   1.162
92,All three database engines run faster when they have indices to work with.
92,But SQLite is still the fastest.
92,Test 8: 1000 UPDATEs without an index
92,BEGIN;
92,UPDATE t1 SET b=b*2 WHERE a>=0 AND a<10;
92,UPDATE t1 SET b=b*2 WHERE a>=10 AND a<20;
92,... 996 lines omitted
92,UPDATE t1 SET b=b*2 WHERE a>=9980 AND a<9990;
92,UPDATE t1 SET b=b*2 WHERE a>=9990 AND a<10000;
92,COMMIT;
92,PostgreSQL:   1.739
92,MySQL:   8.410
92,SQLite 2.7.6:   0.637
92,SQLite 2.7.6 (nosync):   0.638
92,"For this particular UPDATE test, MySQL is consistently"
92,five or ten times
92,slower than PostgreSQL and SQLite.
92,I do not know why.
92,MySQL is
92,normally a very fast engine.
92,Perhaps this problem has been addressed
92,in later versions of MySQL.
92,Test 9: 25000 UPDATEs with an index
92,BEGIN;
92,UPDATE t2 SET b=468026 WHERE a=1;
92,UPDATE t2 SET b=121928 WHERE a=2;
92,... 24996 lines omitted
92,UPDATE t2 SET b=35065 WHERE a=24999;
92,UPDATE t2 SET b=347393 WHERE a=25000;
92,COMMIT;
92,PostgreSQL:   18.797
92,MySQL:   8.134
92,SQLite 2.7.6:   3.520
92,SQLite 2.7.6 (nosync):   3.104
92,"As recently as version 2.7.0, SQLite ran at about the same speed as"
92,MySQL on this test.
92,But recent optimizations to SQLite have more
92,than doubled speed of UPDATEs.
92,Test 10: 25000 text UPDATEs with an index
92,BEGIN;
92,UPDATE t2 SET c='one hundred forty eight thousand three hundred eighty two' WHERE a=1;
92,UPDATE t2 SET c='three hundred sixty six thousand five hundred two' WHERE a=2;
92,... 24996 lines omitted
92,UPDATE t2 SET c='three hundred eighty three thousand ninety nine' WHERE a=24999;
92,UPDATE t2 SET c='two hundred fifty six thousand eight hundred thirty' WHERE a=25000;
92,COMMIT;
92,PostgreSQL:   48.133
92,MySQL:   6.982
92,SQLite 2.7.6:   2.408
92,SQLite 2.7.6 (nosync):   1.725
92,"Here again, version 2.7.0 of SQLite used to run at about the same speed"
92,as MySQL.
92,But now version 2.7.6 is over two times faster than MySQL and
92,over twenty times faster than PostgreSQL.
92,"In fairness to PostgreSQL, it started thrashing on this test."
92,knowledgeable administrator might be able to get PostgreSQL to run a lot
92,faster here by tweaking and tuning the server a little.
92,Test 11: INSERTs from a SELECT
92,"BEGIN;INSERT INTO t1 SELECT b,a,c FROM t2;INSERT INTO t2 SELECT b,a,c FROM t1;COMMIT;"
92,PostgreSQL:   61.364
92,MySQL:   1.537
92,SQLite 2.7.6:   2.787
92,SQLite 2.7.6 (nosync):   1.599
92,The asynchronous SQLite is just a shade slower than MySQL on this test.
92,(MySQL seems to be especially adept at INSERT...SELECT statements.)
92,The PostgreSQL engine is still thrashing - most of the 61 seconds it used
92,were spent waiting on disk I/O.
92,Test 12: DELETE without an index
92,DELETE FROM t2 WHERE c LIKE '%fifty%';
92,PostgreSQL:   1.509
92,MySQL:   0.975
92,SQLite 2.7.6:   4.004
92,SQLite 2.7.6 (nosync):   0.560
92,"The synchronous version of SQLite is the slowest of the group in this test,"
92,but the asynchronous version is the fastest.
92,The difference is the extra time needed to execute fsync().
92,Test 13: DELETE with an index
92,DELETE FROM t2 WHERE a>10 AND a<20000;
92,PostgreSQL:   1.316
92,MySQL:   2.262
92,SQLite 2.7.6:   2.068
92,SQLite 2.7.6 (nosync):   0.752
92,This test is significant because it is one of the few where
92,PostgreSQL is faster than MySQL.
92,"The asynchronous SQLite is,"
92,"however, faster then both the other two."
92,Test 14: A big INSERT after a big DELETE
92,INSERT INTO t2 SELECT * FROM t1;
92,PostgreSQL:   13.168
92,MySQL:   1.815
92,SQLite 2.7.6:   3.210
92,SQLite 2.7.6 (nosync):   1.485
92,Some older versions of SQLite (prior to version 2.4.0)
92,would show decreasing performance after a
92,sequence of DELETEs followed by new INSERTs.
92,"As this test shows, the"
92,problem has now been resolved.
92,Test 15: A big DELETE followed by many small INSERTs
92,BEGIN;
92,DELETE FROM t1;
92,"INSERT INTO t1 VALUES(1,10719,'ten thousand seven hundred nineteen');"
92,... 11997 lines omitted
92,"INSERT INTO t1 VALUES(11999,72836,'seventy two thousand eight hundred thirty six');"
92,"INSERT INTO t1 VALUES(12000,64231,'sixty four thousand two hundred thirty one');"
92,COMMIT;
92,PostgreSQL:   4.556
92,MySQL:   1.704
92,SQLite 2.7.6:   0.618
92,SQLite 2.7.6 (nosync):   0.406
92,"SQLite is very good at doing INSERTs within a transaction, which probably"
92,explains why it is so much faster than the other databases at this test.
92,Test 16: DROP TABLE
92,DROP TABLE t1;DROP TABLE t2;DROP TABLE t3;
92,PostgreSQL:   0.135
92,MySQL:   0.015
92,SQLite 2.7.6:   0.939
92,SQLite 2.7.6 (nosync):   0.254
92,SQLite is slower than the other databases when it comes to dropping tables.
92,"This probably is because when SQLite drops a table, it has to go through and"
92,erase the records in the database file that deal with that table.
92,MySQL and
92,"PostgreSQL, on the other hand, use separate files to represent each table"
92,"so they can drop a table simply by deleting a file, which is much faster."
92,"On the other hand, dropping tables is not a very common operation"
92,"so if SQLite takes a little longer, that is not seen as a big problem."
92,This page last modified on
92,2023-01-02 14:22:42 UTC
93,Job Application for Support Manager at MariaDB plc
93,Apply Now
93,Support Manager
93,at MariaDB plc
93,Remote - Philippines
93,"MariaDB is making a big impact on the world. Whether you’re checking your bank account, buying a coffee, shopping online, making a phone call, listening to music, taking out a loan or ordering takeout – MariaDB is the backbone of applications used everyday. Companies small and large, including 75% of the Fortune 500, run MariaDB, touching the lives of billions of people. With massive reach through Linux distributions, enterprise deployments and public clouds, MariaDB is uniquely positioned as the leading database for modern application development."
93,The Opportunity
93,"As a MariaDB Support Manager, you directly interface with our customers each and every day to help their business, users and customers succeed and grow."
93,"Our customers use MariaDB Server, MaxScale, ColumnStore, Galera Cluster, Xpand (previously ClustrixDB), client connectors for C, Java, ODBC and other programming languages and environments, and new and emerging technologies made available through MariaDB Server. They also use other open source technologies, operating systems, Linux and Windows. Their servers are deployed on premises, in the cloud, in virtual servers and containers and on physical servers."
93,"We believe technical support is not just a job, but a calling. Our model is unconventional: we do not view support as merely a “cost center,” the unfortunate byproduct of creating products. We do not frustrate our customers or our engineers with scripts that must be parroted and “tiers” that must be escalated through to find someone who can solve an issue. We spend most of our time solving issues that go far beyond simple “break/fix” support or documentation lookup. Our goal is to provide the very best support experience across any industry, and we achieve that by hiring and empowering the very best in the world."
93,Responsibilities
93,"As a MariaDB Support Manager you are vital to a customer’s success in this challenging, complex and ever-changing technology landscape. You’re expected to deliver expert-level technical support services to worldwide customers, across our entire portfolio of supported products. Support services range from software usage and best practices, to service restoration and performance tuning. You will utilize advanced troubleshooting skills and creative problem-solving capabilities and you must be self-motivated and capable of working autonomously."
93,"Because we must provide coverage to our customers 24x7x365, this position does occasionally involve work on nights, weekends and holidays, however, we leverage our global team to minimize this impact as much as possible."
93,"If you have a true passion for helping customers succeed and feel rewarded by solving complex problems, and you enjoy challenges, then join the MariaDB Support team today."
93,Qualifications
93,"Prior MariaDB, Percona or MySQL 24x7 support engineer experience"
93,"8+ years of expertise with MariaDB Server or MySQL Enterprise, and related tools"
93,Working knowledge of other relational and non-relational database systems
93,Passion for serving customers and solving technical problems
93,Strong knowledge of SQL and query optimization
93,"Strong knowledge of database design, administration and architecture"
93,In-depth knowledge of various highly available environments and technologies
93,Proficient with multiple backup solutions
93,Experience with cloud offerings and environments
93,Experience with automation tool sets
93,Experience with containers
93,Database monitoring software experience
93,"Proficient in one or more scripting languages (Bash, Python, Go, etc.)"
93,"Sysadmin experience (Linux, Windows, etc.)"
93,Very strong troubleshooting and problem-solving skills
93,Experience working remotely
93,Excellent written and spoken English communication skills
93,Bonus Experience and Skills
93,Contributions to open source software in the MariaDB/MySQL ecosystem
93,"Experience with other MariaDB products such as MaxScale, ColumnStore, Xpand, SkySQL, etc."
93,"Proficient with virtualization software (Xen, VirtualBox, VMware, etc.)"
93,MariaDB and/or other related certifications
93,Location
93,"Remote – South Korea, Philippines, Malaysia, Singapore or Australia (citizens or PRs only)"
93,What’s in It for You?
93,"Impact the world of technology by pushing the boundaries of technology and business models, working at MariaDB. Be part of a game-changing organization that encourages outside-the-box thinking, values empowerment, and is truly shaping the future of the software industry. You’ll be collaborating with high-caliber colleagues around the world, offering unparalleled learning and growth opportunities. We provide a very competitive compensation package, 25 days paid annual leave (plus holidays), stock options, a massive degree of flexibility and freedom, and more."
93,Salaries for candidates outside the U.S. will vary based on local compensation structures.
93,How to Apply
93,"If you are interested in this position, please submit your application along with your resume/CV."
93,MariaDB does not sponsor work visas or relocation.
93,"MariaDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter."
93,MariaDB will not accept agency resumes without a prior contractual agreement with HR. Please do not forward resumes to any recruiting alias or employee directly. MariaDB is not responsible for any unsolicited resumes.
93,MariaDB is an equal opportunities employer.
93,Apply for this Job
93,* Required
93,First Name *
93,Last Name *
93,Email *
93,Phone *
93,Resume/CV *
93,Drop files here
93,Attach
93,Dropbox
93,Google Drive
93,or enter manually
93,"(File types: pdf, doc, docx, txt, rtf)"
93,Cover Letter
93,Drop files here
93,Attach
93,Dropbox
93,Google Drive
93,or enter manually
93,"(File types: pdf, doc, docx, txt, rtf)"
93,When autocomplete results are available use up and down arrows to review
93,+ Add another education
93,Do you have expert knowledge of MariaDB server and related tools? *
93,Yes
93,Which of the following do you have direct administration or support experience with? *
93,MySQL
93,MariadB Enterprise Server
93,Xpand
93,SkySQL
93,ColumnStore
93,MaxScale
93,None of the Above
93,How many years of
93,database administration experience do you have? *
93,Please select0 years
93,1 years
93,2-4 years
93,4-6 years
93,7+ years
93,"Do you have strong knowledge of database design, administration and architecture? *"
93,Yes
93,Do you have strong knowledge of SQL and query optimization? *
93,Yes
93,Have you done 24x7 support work in the past? *
93,Yes
93,Would you describe yourself as having strong troubleshooting and problem-solving skills? *
93,Yes
93,Voluntary Self-Identification
93,"For government reporting purposes, we ask candidates to respond to the below self-identification survey."
93,"Completion of the form is entirely voluntary. Whatever your decision, it will not be considered in the hiring"
93,process or thereafter. Any information that you do provide will be recorded and maintained in a
93,confidential file.
93,"As set forth in MariaDB plc’s Equal Employment Opportunity policy,"
93,we do not discriminate on the basis of any protected group status under any applicable law.
93,Gender
93,Please selectMale
93,Female
93,Decline To Self Identify
93,Are you Hispanic/Latino?
93,Please selectYes
93,Decline To Self Identify
93,Please identify your race
93,Please selectAmerican Indian or Alaskan Native
93,Asian
93,Black or African American
93,Hispanic or Latino
93,White
93,Native Hawaiian or Other Pacific Islander
93,Two or More Races
93,Decline To Self Identify
93,Race & Ethnicity Definitions
93,"If you believe you belong to any of the categories of protected veterans listed below, please indicate by making the appropriate selection."
93,"As a government contractor subject to the Vietnam Era Veterans Readjustment Assistance Act (VEVRAA), we request this information in order to measure"
93,the effectiveness of the outreach and positive recruitment efforts we undertake pursuant to VEVRAA. Classification of protected categories
93,is as follows:
93,"A ""disabled veteran"" is one of the following: a veteran of the U.S. military, ground, naval or air service who is entitled to compensation (or who but for the receipt of military retired pay would be entitled to compensation) under laws administered by the Secretary of Veterans Affairs; or a person who was discharged or released from active duty because of a service-connected disability."
93,"A ""recently separated veteran"" means any veteran during the three-year period beginning on the date of such veteran's discharge or release from active duty in the U.S. military, ground, naval, or air service."
93,"An ""active duty wartime or campaign badge veteran"" means a veteran who served on active duty in the U.S. military, ground, naval or air service during a war, or in a campaign or expedition for which a campaign badge has been authorized under the laws administered by the Department of Defense."
93,"An ""Armed forces service medal veteran"" means a veteran who, while serving on active duty in the U.S. military, ground, naval or air service, participated in a United States military operation for which an Armed Forces service medal was awarded pursuant to Executive Order 12985."
93,Veteran Status
93,Please selectI am not a protected veteran
93,I identify as one or more of the classifications of a protected veteran
93,I don't wish to answer
93,Voluntary Self-Identification of Disability
93,Form CC-305
93,Page 1 of 1
93,OMB Control Number 1250-0005
93,Expires 04/30/2026
93,Why are you being asked to complete this form?
93,"We are a federal contractor or subcontractor. The law requires us to provide equal employment opportunity to qualified people with disabilities. We have a goal of having at least 7% of our workers as people with disabilities. The law says we must measure our progress towards this goal. To do this, we must ask applicants and employees if they have a disability or have ever had one. People can become disabled, so we need to ask this question at least every five years."
93,"Completing this form is voluntary, and we hope that you will choose to do so. Your answer is confidential. No one who makes hiring decisions will see it. Your decision to complete the form and your answer will not harm you in any way. If you want to learn more about the law or this form, visit the U.S. Department of Labor’s Office of Federal Contract Compliance Programs (OFCCP) website at www.dol.gov/ofccp."
93,How do you know if you have a disability?
93,"A disability is a condition that substantially limits one or more of your “major life activities.” If you have or have ever had such a condition, you are a person with a disability. Disabilities include, but are not limited to:"
93,Alcohol or other substance use disorder (not currently using drugs illegally)
93,"Autoimmune disorder, for example, lupus, fibromyalgia, rheumatoid arthritis, HIV/AIDS"
93,Blind or low vision
93,Cancer (past or present)
93,Cardiovascular or heart disease
93,Celiac disease
93,Cerebral palsy
93,Deaf or serious difficulty hearing
93,Diabetes
93,"Disfigurement, for example, disfigurement caused by burns, wounds, accidents, or congenital disorders"
93,Epilepsy or other seizure disorder
93,"Gastrointestinal disorders, for example, Crohn's Disease, irritable bowel syndrome"
93,Intellectual or developmental disability
93,"Mental health conditions, for example, depression, bipolar disorder, anxiety disorder, schizophrenia, PTSD"
93,Missing limbs or partially missing limbs
93,"Mobility impairment, benefiting from the use of a wheelchair, scooter, walker, leg brace(s) and/or other supports"
93,"Nervous system condition, for example, migraine headaches, Parkinson’s disease, multiple sclerosis (MS)"
93,"Neurodivergence, for example, attention-deficit/hyperactivity disorder (ADHD), autism spectrum disorder, dyslexia, dyspraxia, other learning disabilities"
93,Partial or complete paralysis (any cause)
93,"Pulmonary or respiratory conditions, for example, tuberculosis, asthma, emphysema"
93,Short stature (dwarfism)
93,Traumatic brain injury
93,Disability Status
93,"Please selectYes, I have a disability, or have had one in the past"
93,"No, I do not have a disability and have not had one in the past"
93,I do not want to answer
93,PUBLIC BURDEN STATEMENT: According to the Paperwork Reduction Act of 1995 no persons are required to respond to a collection of information unless such collection displays a valid OMB control number. This survey should take about 5 minutes to complete.
93,Please reach out to our support team via our help center.
93,Please complete the reCAPTCHA above.
93,School
93,Degree
93,High School
93,Associate's Degree
93,Bachelor's Degree
93,Master's Degree
93,Master of Business Administration (M.B.A.)
93,Juris Doctor (J.D.)
93,Doctor of Medicine (M.D.)
93,Doctor of Philosophy (Ph.D.)
93,Engineer's Degree
93,Other
93,Powered by
93,Read our Privacy Policy
94,Performance Best Practice for Efficient Queries - ... - ServiceNow Community
94,ServiceNow Community
94,servicenow
94,community
94,Help
94,Developer
94,"Build, test, and deploy applications on the Now Platform."
94,ServiceNow
94,Learn more about ServiceNow products and solutions.
94,Learning
94,Build your skills with instructor-led and online training.
94,Support
94,"Manage your instances, access self-help, and get technical support."
94,Documentation
94,"Find detailed info about ServiceNow products, apps, features, and releases."
94,Impact
94,Drive a faster ROI and amplify your expertise with ServiceNow Impact.
94,Partner
94,"Grow your business with promotions, news, and marketing tools for partners."
94,Store
94,Download certified apps and integrations that complement ServiceNow.
94,Join the Community
94,Browse
94,Product Hubs
94,Releases and Upgrades
94,App Development
94,App Engine Studio
94,Citizen Development Center
94,Asset Management
94,Cloud Cost Management
94,Enterprise Asset Management
94,Hardware Asset Management
94,Software Asset Management
94,Customer Service Management
94,Data foundations
94,CMDB
94,CSDM
94,Employee Center
94,Human Resources Service Delivery
94,ServiceNow Impact
94,Industry solutions
94,Financial Services Operations
94,Healthcare and Life Sciences
94,Manufacturing
94,Technology Provider
94,Telecommunications
94,IT Operations Management
94,IT Service Management
94,Now Platform
94,Admin Experience
94,"Agent Chat, Routing, and Sidebar"
94,AI & Intelligence
94,Knowledge Management
94,Mobile Apps & Platform
94,Next Experience
94,Platform Analytics
94,Platform Privacy & Security
94,Service Portal
94,Virtual Agent & NLU
94,Workflow Automation
94,Strategic Portfolio Management
94,Security Operations
94,More ServiceNow Products
94,Application Portfolio Management
94,Automation Engine
94,Cloud Observability
94,DevOps
94,Enterprise Asset Management
94,"Environmental, Social, and Governance"
94,Field Service Management
94,"Governance, Risk, & Compliance"
94,Internationalization & Localization
94,Legal Service Delivery
94,Process Mining
94,Service Operations Workspace
94,ServiceNow for MS Teams
94,Source-to-Pay Operations
94,Workplace Service Delivery
94,Discussions
94,Architects
94,Developers
94,Knowledge Managers
94,New Customer Onboarding
94,SysAdmins
94,ServiceNow Jobs
94,Training and Certifications
94,Blogs
94,Community Blogs
94,Developer Advocate Blog
94,Developer Blog
94,ServiceNow Blogs
94,Japan
94,Groups
94,Community Belonging Groups
94,ServiceNow User Groups (SNUGs)
94,Special Interest Groups (SIGs)
94,Energy
94,Financial Services
94,Government
94,Healthcare
94,Higher Education
94,Life Sciences
94,Manufacturing
94,User Experience
94,All Special Interest Groups
94,Events
94,Live on ServiceNow
94,The Devvies: App of the Year
94,DemoCenter
94,ServiceNow Podcasts
94,Knowledge 2023
94,Upcoming Events
94,ServiceNow Events
94,Resources
94,Community Welcome Guide
94,Community Guidelines
94,Community Resources
94,Recognition & Rewards Program
94,Leaderboards
94,Become an Expert
94,ServiceNow MVP Program
94,Rising Star Program
94,Idea Portal
94,RiseUp with ServiceNow
94,RiseUp with ServiceNow Forum
94,RiseUp with ServiceNow Events
94,RiseUp with ServiceNow Blogs
94,Join the Community
94,All communityThis categoryThese articlesUsersProducts
94,cancel
94,Turn on suggestions
94,Auto-suggest helps you quickly narrow down your search results by suggesting possible matches as you type.
94,Showing results for
94,Show  only
94,Search instead for
94,Did you mean:
94,ServiceNow Community
94,Discussions
94,Developer
94,Developer articles
94,Performance Best Practice for Efficient Queries - ...
94,Options
94,Subscribe to RSS Feed
94,Mark as New
94,Mark as Read
94,Bookmark
94,Subscribe
94,Printer Friendly Page
94,Report Inappropriate Content
94,Performance Best Practice for Efficient Queries - Top 10 Practices
94,Mwatkins
94,ServiceNow Employee
94,Options
94,Post History
94,Subscribe to RSS Feed
94,Mark as New
94,Mark as Read
94,Bookmark
94,Subscribe
94,Printer Friendly Page
94,Report Inappropriate Content
94,‎06-21-2017
94,10:58 AM
94,- edited on
94,‎11-22-2022
94,09:31 AM
94,Lisa Latour
94,< Previous Article
94,Next Article >
94,Caching data to improve performance
94,Improving Slow OR and JOIN Queries
94,Overview
94,"This guide is written by the ServiceNow Technical Support Performance team (All Articles). We are a global group of experts that help our customers with performance issues. If you have questions about the content of this article we will try to answer them here. However, if you have urgent questions or specific issues, please see the list of resources on our profile page: ServiceNowPerformanceGTS"
94,"The goal of this article is to address some common types of configuration issues that can lead to slow query performance problems on the database. Nearly every piece of data and configuration in ServiceNow exists as a row in a MySQL database (technically MariaDB as of 2020). Therefore nearly every operation in ServiceNow requires hundreds of queries to the database to return the results. In the simplest sense, ServiceNow is just a web API for a database. When you open the list of incidents for example, you might notice the URL says /incident_list.do. At the App tier there is logic that interprets this request to mean, ""get a list of rows from the incident table in the database""."
94,"In this article we attempt to address some of the common ""gotcha's"" that we see administrators hit when configuring their application layer in ways that are not optimal for Database performance."
94,A note about building efficient queries
94,"ServiceNow allows users to build their own query filters in many areas of the product. Most notably, power users can build query filters on lists and reports. With this power and flexibility comes the potential for negative impact on performance. By knowing how to build efficient filters, power users will be able to get the information they need without negatively impacting performance of the system. The following is a list of things power users should consider when creating queries. This becomes extremely important when creating a query that will be run automatically many times per day, such as a gauge on your homepage that auto-refreshes."
94,"These same tips also apply to administrator and developer users who design solutions by writing code or making configuration changes. Whether you are a power user, administrator or developer, these principles will go a long way to ensuring you and your team have a great experience with the ServiceNow platform. I would say that probably 90% of the slow transaction issues experienced by ServiceNow users can boil down to these few things. Originally the list had only 10 items but has grown over time. We didn't want to change the name because it is frequently bookmarked and referenced externally, so please excuse the fact that there are more than 10 items! We hope this helps you in your pursuit of service excellence!"
94,We have tried to order this list based on how frequently we have given each recommendation to customers.
94,#1 Limit the Rowcount User Preference
94,Issue: Best practice for rowcount settings should be limited to 20 rows per page
94,"Business Impact: When the rowcount setting is higher than 20, list rendering will be sub-optimal user experience. There is only so many records that a human wants to read through and, in addition, for every record that is loaded multiple layers of business logic including security rules must be executed - individually these may be only a few milliseconds here and there but they can add up.  In extreme cases, if enough users have the setting above 20, list rendering can cause levels of database impact that can affect other transactions."
94,"Affected: The user with high rowcount will experience slow list, homepage and form render times. If enough users execute inefficient operations at the same time this can result in a system-wide performance degradation."
94,Solution:
94,There are three things that can be done to address the issue:
94,"1) Individual users can change their ""rowcount"" user preference via the hamburger icon (three horizontal lines) on the list UI header."
94,"2) Administrators can manually set the values of the rowcount preference through the module ""User Administration > User Preferences"" or the list below that already has the filter added for rowcounts:"
94,/sys_user_preference_list.do?sysparm_query=nameLIKErowcount%5Evalue!%3D20%5EORvalue%3DNULL%5Evalue!%3D50%5EORvalue%3DNULL%5Evalue!%3DNULL%5Evalue!%3D10%5EORvalue%3DNULL%5Evalue!%3D15%5EORvalue%3DNULL
94,"3) Administrators can restrict the options that users are allowed to select by setting the ""glide.ui.per_page"" property"
94,"NOTE: The rowcount setting becomes especially impactful when using the ""group by field"" option in the list UI. If rowcount is set to 100, each group in the list UI will have up to 100 records in it. For every record displayed in the UI, the platform has to execute hundreds of security and rendering activities. This can all add up very quickly."
94,#2 Use Database indexes with the most efficient operator for the job
94,"Databases indexes are very much like phone books. A Database index is a file that stores a sorted version of a table based on a certain field or fields. So, one index may be like a phone book that sorts the entries by the last name then first name. Another index may be like a phone book that sorts the entries by their phone numbers. It is important to make sure that you have the indexes you need to support your intended usage of the system. (see Create a table index)"
94,"Whether you have a supporting index or not, you also need to consider the way your users will query the data in your tables. There are different types of operators that can be used to filter the data in your tables. It is critical that any commonly used filters (like a module or a report) are designed to use the most efficient operator for the job. ""Equals"" and ""starts with"" queries are more efficient than ""contains"" or ""ends with"". For example consider the relative difficultly of finding the following things in a phone book where the names are organized alphabetically by last name:"
94,"Find people whose last name starts with ""Bro"""
94,"Find people whose last name equal ""Brown"""
94,"Find people whose last name ends with ""own"""
94,"Find people whose last name contains ""row"""
94,"For the starts with and equals operators it is a fairly easy thing to do, right? But if you want to find someone whose name ends with ""own"" you'd have to read the whole phone book, reading the ends of each person's name. To find all the people with last names that contain ""row"",  you would have to read the whole phone book and you'd have to read the whole name of every person in the phone book - not just the end."
94,"Think about these other situations, how would they stack up against the above queries?"
94,"Find people whose last name does not contain ""row"""
94,"Find people whose last name is neither ""Green"" nor ""White"""
94,"Find people whose last name is greater than ""Brown"" (e.g., Green and White, but not Alabaster or Brown)"
94,"Find people whose last name is less than ""Brown"" (e.g., Alabaster, but not Brown, Green or White)"
94,#3 Add efficient Filter Conditions like active=true to improve slow queries
94,"Suppose you have a filter that is performing an expensive operation. For example, suppose you want to search the email table for a record whose subject contains a certain term. This is an incredibly expensive operation since it is on a huge table, on a large field, and does a contains operation (""LIKE"" in ServiceNow query string syntax) - which cannot leverage an index. To make this query more efficient, you should consider how you could add a second, more efficient condition to your query so that the total query will become more efficient."
94,Original query string: subjectLIKEChange Request
94,Improved query string using an additional condition:
94,sys_created_onONLast 7 days@javascript&colon;gs.daysAgoStart(7)@javascript&colon;gs.daysAgoEnd(0)^subjectLIKEChange Request
94,"Why does this work? The second query above limits the timeframe to be searched to only records created in the last 7 days - an efficient way of narrowing the result set. Databases build their final result sets by first creating intermediate result sets. By adding a condition that narrows the intermediate result set in an efficient way, the database can perform the less efficient operation — subjectLIKEChange Request - against the much smaller, intermediate result set. Usually it can do this in-memory, although sometimes it must perform the operation on the disk with a temporary table. This particular strategy — using a field like sys_created_on that reflects chronological order — is sometimes called time boxing. By executing an expensive operation within just a small ""box"" of time, the total query execution becomes much faster. Note that this strategy requires that the table has an index on the field that is being used in the time boxing strategy!"
94,"Far and away, the #1 most efficient filter condition that should be added whenever possible is ""active = true"". Most work that is done in ServiceNow is done on tables that extends task. Suppose you have over 3,000,000 records in the task table and you want to query all the incidents that have the word ""email"" in their short description. Further suppose the following distribution of data:"
94,Count
94,State Label
94,State
94,Active
94,763
94,New
94,"20,722"
94,Active
94,"1,952"
94,Pending Change
94,"2,178"
94,Pending Customer
94,552
94,Pending RCA
94,"5,009"
94,Paused
94,698
94,Resolved
94,"2,997,319"
94,Closed
94,Consider the following two queries:
94,Original query string: short_descriptionLIKEemail
94,Improved query string: short_descriptionLIKEemail^active=true
94,"By adding the active=1 condition, you can immediately reduce the intermediate result set from 3 million down to a few thousand! Keep the number of active tasks low, include the ""active"" column in your indexes and always add ""active=true"" to your filter conditions if possible."
94,You can customize your lists and modules to force the active=true condition to stay in the breadcrumbs by using adding &sysparm_fixed_query=active=true to the URL. https://docs.servicenow.com/bundle/orlando-platform-administration/page/administer/list-administrati...
94,"NOTE: In order for the above strategy to work, the number of active records in each table that extends task must be kept relatively small. For active=true to improve your queries, the number of active tasks must be small - like under 10%. Out-of-the-box tasks will go to a closed state and the active flag will be set to false. However, sometimes customizations made to an instance result in tasks not going to active=false. If one table in the task hierarchy has many active tasks, this can affect query execution efficiency for all classes of task tables!! You should make sure that the percentage of tasks in your tables stays low - under 10%."
94,#4 Avoid Conditions on Dot-Walked Fields
94,"A ""dot-walked"" field in ServiceNow is a field that references a different table than the one currently selected, i.e. a Reference field. For example, a condition to check the name of the assignment group of a task would be a dot-walk from the task table to the sys_user_group table via the dot-walked relationship: task.assignment_group.name. When used in a condition, dot-walked fields create an implicit JOIN operation in the database between the related tables. Whenever possible, attempt to avoid conditions on dot-walked expressions. Relational databases are inherently bad at selecting good execution plans for JOIN queries between large data sets."
94,"For example, the following filters return the same result sets but may mean the difference between a 10 second and a 10 millisecond query when dealing with millions of records."
94,Using a dot-walked field on the referenced table:
94,Using a direct reference field comparison:
94,"#5 Beware the Out-of-box ""Go to"" search option for lists"
94,In the ServiceNow list header there is a search bar that allows you to narrow the list results by using a desired term against a particular field.
94,"By default the ""Go to"" search will use the greater than or equal to comparison operator (>=). So, for example, in the screenshot above the system would do a search for any incident whose short description is greater than or equal to ""SAP"". This behavior is probably not what the customer desired or expected by default."
94,"Set the property glide.ui.goto_use_starts_with = true (true as of Kingston on z-boots, see PRB1149592). For later versions, add the property to your sys_properties table and set it to true."
94,"Use wildcard shortcuts with ""Go to"" for greater efficiencyFor example, by using SAP% instead of SAP, the search will return records whose short description starts with SAP.See the documentation page: Available list search wildcards"
94,"Avoid using the ""goto_use_contains"" propertyChanging the default behavior to do contains searches rather than >= searches is even less efficient."
94,"[Available in Kingston] Specify the default ""Go to"" behavior on a per field basisThis feature allows administrators to specify that certain fields should use a different default comparison operator. To activate:"
94,Upgrade to Kingston or later
94,"In the Dictionary record (sys_dictionary) for the field in question (e.g. task.short_description), add the attribute ""goto_starts_with_search=true"""
94,#6 Break complex reports - especially ÜBER ORs - into multiple reports on a single Home page
94,"Homepages allow multi-threaded gauge processing. By default the multi-threading is set to 2. Therefore ServiceNow will process two reports on a homepage at the same time. You will be able to render three small reports faster than one big one. It is not always possible to break a complex query into multiple simple reports in a meaningful way without compromising the business requirements, but it should be considered. For example consider the following situation:"
94,Big report takes 35 seconds:
94,(Closed today) OR (Opened today) OR (Active=true AND Assigned To=Service Desk)
94,3 little reports
94,Closed Today (1.3 seconds)
94,Opened Today (1.2 seconds)
94,Active=true AND Assigned To=Service Desk (2 seconds)
94,"On a homepage with multi-threading of 3, the 3 little reports only take about 2 seconds total - the time it takes to render the slowest of the three reports."
94,This same principle also applies to you developers and administrators who are trying to write complex GlideRecord and GlideAggregate code. See the community article Database Performance: Ways to Improve Slow OR and JOIN Queries
94,#7 Design efficient Database Views
94,A Database View in ServiceNow is just a way of doing a JOIN query. Consider how you can include limiting conditions to create smaller intermediate result sets into the design of your Database View to avoid unnecessarily querying a huge dataset.
94,"For example, suppose you build a Database View that joins Metric Definition with Metric Incident with Incident.   There is one like this that comes out of the box. It allows you to see different metrics in the same report. However, consider the case where you only want to see a certain Metric Definition. It may be much more efficient to add a condition to your Database View that restricts the results from Metric Incident to only the particular Metric Definition that you are interested in. Due to the shear size of the tables involved you may need to sacrifice the convenience of having every possible Metric Definition option available to be selected for the efficiency of just bringing back what you need."
94,#8 Limit the number of columns that you see in List view
94,"There is a list mechanic (cog wheel icon) feature on every list view in ServiceNow that will allow you to personalize the columns returned with every list query that you make. By reducing this list to only the specific columns that are of interest to you, the list rendering process can be greatly improved."
94,#9 Avoid adding Journal fields to Lists
94,"The content of journal fields (e.g. ""work notes"" or ""comments"") are actually stored in a child table. When a journal field is displayed on a list it requires one extra query per row. With 50-100 rows per page this can result in diminished performance during list rendering. This can also be true with other large or dynamic fields such as the Workflow type field. When designing your UI, test to see if removing such fields from the list view has a large impact on performance."
94,"#10 Beware of your ""Order By"" field (sort order in lists)"
94,"One of the easiest ways to make a query run slowly is to ""Order By"" a field that does not support easy sorting. If you are performing a query that is running slowly, see if you can significantly speed up the query by removing the ""Order By"" field or selecting a different field upon which to perform the ordering."
94,"If you cannot achieve your goal without performing a certain ""Order By"" operation, see if your administrator can add a Database index to the table that will support your query running more efficiently."
94,"Databases almost always choose just one index per table when designing a query plan (there are rare exceptions like an index merge). One common situation is that the field that is being ordered on is forcing the database to take a sub-optimal query plan. For example, consider the following two time boxed queries:"
94,"SELECT ... FROM incident WHERE sys_created_on > ""2017-06-20 01:00:00"" AND sys_created_on < ""2017-06-20 02:00:00"" ORDER BY sys_created_on SELECT ... FROM incident WHERE sys_created_on > ""2017-06-20 01:00:00"" AND sys_created_on < ""2017-06-20 02:00:00"" ORDER BY opened_at"
94,"Let us suppose that both sys_created_on and opened_at have database indexes on them. Query A will operate lightning fast, because it can use the index on sys_created_on and only grab 1 hour of data. However, for query B the optimizer may choose to use the Database index on opened_at and now the database cannot assume that the results are ordered by the sys_created_on field, it must scan through every record in the opened_at index, looking for ones that fall within the 1-hour sys_created_on timeframe. So, whenever possible, use this principle: align your sort field with a database index that will also efficiently restrict the results of your WHERE clause."
94,#11 Load Related Lists on-demand
94,"In the ServiceNow Fuji UI we added the ability to load Related Lists asynchronously or ""on-demand"".   To change the default behavior for all users to be ""On Demand"" ServiceNow administrators can do it with the following user preference (note that the User field is intentionally blank to make this default for all users)."
94,Name: glide.ui.related_list_timing
94,System: true
94,User: <leave this field blank>
94,"Value: deferred (or ""ondemand"")"
94,"With this in place, the user can still override the behaviour and change it to something else, but at least we start off with a clean slate. If any existing settings for glide.ui.related_list_timing exist then you can remove those to force them to honor the new default behavior."
94,Pros:
94,The top portion of all Forms will load very quickly.
94,Users who are simply trying to update a record and do not need to see the Related Lists will have a much faster user experience (Related Lists are often the cause of 75% to 90% of form load latency).
94,Users have the ability to override the preference for how their individual forms load if they do not like the behavior
94,Cons:
94,"Users who want to see the related lists may feel that the things they want to see are taking longer than before - especially if they are using the ""ondemand"" style list."
94,"Prior to Fuji Patch 5 this behavior had a user experience issue that caused the form to ""jump down"" to the related list section once the list completed rendering. Most users found this annoying. Fuji Patch 6 and later remove the ""jump down"" feature."
94,#12 Minimize the volume and frequency of running Homepage loads
94,One of the heaviest uses of ServiceNow is the automatically refreshing homepage. This is a very useful feature but can also lead to excessive system usage and performance degradation.
94,If a user's homepage is loading they will not be able to perform any other operation during that time. ServiceNow limits the number of concurrent transactions for each logged in user to 1 — this is known as session synch. To avoid running into session synch it is a good idea to reduce the frequency of your homepage auto-reload.
94,If you have 200 users who want to see a certain report daily or weekly one idea is to have this done as a scheduled report that goes out to them via email instead of putting it on their Homepage. This is especially true for slow loading reports. A homepage will only load as fast as its slowest report. Avoid adding slow reports (ones that go more than 3 or 4 seconds) to homepages.
94,If enough users have automatically loading homepages the combined impact can lead to performance degradation across the whole system during peak usage hours.
94,Select a very fast homepage as your default homepage. Your default homepage will be the last homepage you looked at. When you first log into the system your default homepage will be rendered automatically. If your colleagues all log in at the same time in the morning this can lead to a very heavy system load right when everyone logs in. A common customization is to create a homepage splash screen where users are redirected on login. See KB0712404 - Setting a Light Weight Home Page for Users for instructions about how to do this.
94,#13 Consider the data distribution and future growth
94,"This is probably one of the most overlooked but important considerations. Whether you are just trying to run a query one time or you are designing some feature that will be used thousands of times a day for the next 10 years, this type of consideration will take on different implications. However, it is always important to think about how the data you are querying is distributed."
94,"Here's an example. Suppose you have an application that manages many-to-many relationships between Tasks and Configuration Items. There is a many-to-many table named ""task_to_ci"". The data distribution in this many-to-many table is like this:"
94,task_id
94,configuration_item_id
94,"In this scenario there are 3 tasks and each has two configuration items's connected - a pretty even distribution. However, configuration item #1 has 3 tasks connected to it, while all other configuration items have only 1 task connected to them. Now imagine that 3 months later this pattern has repeated 100,000 times and configuration item #1 ends up with 100,000 tasks connected to it. Which of these filters will execute faster?"
94,task_to_ci_list.do?sysparm_query=configuration_item_id=1
94,task_to_ci_list.do?sysparm_query=configuration_item_id=2
94,"The answer is pretty obvious when stated this way. The query for all records for configuration item #1 will be much slower because it will return 100,000 matches. However, if you didn't realize the data was skewed like this, you would not anticipate that one query would be slow and the other fast. This is just a very simple example to demonstrate how important data distribution is to the execution time of a certain query. There are too many permutations of this pattern to elaborate completely, but here are some questions that might help you when you are planning your query strategy for future data distribution and table growth:"
94,Am I designing my solution based on an example use case that will be different than the real world data distribution?
94,"In places where I am aware of an uneven data distribution, how can I configure my modules, filters, homepages, reports, dashboards to optimize for that uneven distribution?"
94,Are there data cleanup strategies that I can employ to keep the size of this table small enough?
94,"What logic is in place to avoid unexpected data distributions (these last two are more of a developer question, but certainly important to consider)?"
94,"One common scenario for uneven data distribution is where most records either have NULL for a certain column or most records are not NULL for that column. In such a case, consider any queries written that rely on a NOT NULL or IS NULL condition."
94,< Previous Article
94,Next Article >
94,Caching data to improve performance
94,Improving Slow OR and JOIN Queries
94,Labels:
94,Developer
94,Preview file
94,8 KB
94,Preview file
94,11 KB
94,Preview file
94,7 KB
94,Preview file
94,24 KB
94,Preview file
94,63 KB
94,Preview file
94,144 KB
94,Helpfuls
94,"48,795 Views"
94,Comments
94,layla
94,Giga Expert
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎09-26-2017
94,09:28 AM
94,"Really interesting, thanks for these practices I would add also ""Omit nested queries"", we have encountered a serious performance issue that was caused by some nested gliderecord queries."
94,Helpfuls
94,Mwatkins
94,ServiceNow Employee
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎09-26-2017
94,09:50 AM
94,"That's a great point, Ait! I wrote a bit about avoiding nested queries here: How large can an array be?However, that thread was having to do with pressure on JVM memory, but nested loops can put pressure on many different resources and cause performance issues in many different ways (e.g. infinite loops, excessive application server CPU usage, exponential processing costs per nesting layer - think the movie Inception).I think I might write another article that focuses on scripting best practices in particular. The items in this article tend to focus on items that result in database pressure due to inefficient queries."
94,Helpfuls
94,layla
94,Giga Expert
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎09-26-2017
94,09:56 AM
94,Thanks for the clarification.Waiting for the next article
94,Helpfuls
94,JC Moller
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎02-03-2018
94,06:41 AM
94,"Hi,This is an excellent article! Bullet number 3. with the missing Active = true filter is a fairly regular issue that we run into with end user's reports and homepages/dashboards. Educating the users and doing regular follow-ups is also important.Should something still go wrong with the performance on your instance, there is an excellent KB-article on the HI-portal (publ. 2017/11). Tons of stuff to read about.""Performance Troubleshooting Guide""https://hi.service-now.com/kb_view.do?sys_kb_id=bbdde490dbbac384d7e37aa31f96198d- Jan"
94,Helpfuls
94,Robert Fedoruk
94,Giga Sage
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎05-30-2018
94,06:03 AM
94,"""UBER ORs"""
94,Love it.
94,Helpful
94,Inactive_Us2366
94,Tera Contributor
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎07-29-2019
94,02:11 AM
94,Great article!!
94,I would like to add that use setLimit() to improve the query performance
94,Normal query
94,"var gr = new GlideRecord(""table_name"");gr.addQuery(""serach_something"");gr.query();if (gr.next()) { //Do something}"
94,Better Query
94,"var gr = new GlideRecord(""table_name"");gr.addQuery(""serach_something"");"
94,gr.setLimit(1) gr.query();if (gr.next()) {//Do something}
94,Also avoid multiple encoded queries
94,Helpfuls
94,Suri2
94,ServiceNow Employee
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎11-14-2019
94,10:42 AM
94,Thank you very much for this great article !!
94,Helpfuls
94,RainDog
94,Mega Explorer
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎06-12-2020
94,03:10 PM
94,Great article! Thanks
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎06-29-2020
94,05:19 PM
94,"We've just posted a related article, Performance Best Practices for Server-side Coding in ServiceNow."
94,Helpful
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎06-30-2020
94,10:54 AM
94,We have posted a new article for server-side scripting best practices. You can find it here:
94,Performance Best Practices for Server-side Coding in ServiceNow
94,"(I know, I know, only 3 years later, right?!)"
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎08-14-2020
94,02:08 PM
94,"Added a new rule #13 to this article, ""Consider the data distribution and future growth"""
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎10-29-2020
94,08:26 AM
94,Helpfuls
94,Reginald Fawcet
94,Giga Expert
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎03-30-2021
94,05:33 AM
94,Great article. I learned a lot from it.
94,I do have some questions though.
94,"How does a table query work, does it go row by row, or column by column?"
94,Which columns does it search first?
94,"""Consider the following two queries:"
94,Original query string: short_descriptionLIKEemail^state!=6^state!=7
94,"Improved query string: short_descriptionLIKEemail^state!=6^state!=7^active=true"""
94,"This part of the article makes it seem as if it is checking column by column, and that active=true is checked first, despite being the last condition."
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎04-05-2021
94,04:40 PM
94,@Reginald Fawcett Thanks for the compliment and the questions! I'll endeavor to give a simple answer without oversimplifying too much. My apologies if this is too rudimentary of an answer.
94,"Q. How does a table query work, does it go row by row, or column by column?"
94,"A. Technically the database might go row-by-row if it can't find an index, but that should almost never happen. In almost all cases, ServiceNow will use MySQL/MariaDB B-Tree indexes to do table queries."
94,"A B-Tree index is a generalization of the Binary Tree data structure; with which you may be familiar. I won't go into the details of how that actually works here - it is well documented elsewhere. Suffice to say, B-Tree Index works similar to how a phone book works. B-Tree Indexes store indexed column data in separate storage in an Ordered List. When we search for a value, the database searches in the indexed column(s) by first checking the middle value in the ordered indexed column - like flipping to the middle of a phonebook. If the value is greater, it will continue to search on the right side of the column. If the value is lesser than the the middle value, it will search to left of the column. This method of search continues till it matches the value. This way, indexed columns need not scan all the rows of the table. It is a bit more complicated than this in real life, but it is close enough to provide a basic understanding."
94,Helpful
94,PB7
94,Tera Expert
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎05-27-2021
94,01:44 PM
94,"Hi Jan,"
94,The system informs me that I lack sufficient privileges to view the linked material 😞
94,Helpfuls
94,Mwatkins
94,ServiceNow Employee
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎05-27-2021
94,01:52 PM
94,Try this link: https://support.servicenow.com/kb?id=kb_article_view&sysparm_article=KB0516495
94,Helpfuls
94,Gabriela Cortes
94,ServiceNow Employee
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎01-18-2022
94,09:23 AM
94,"Hi,"
94,"Question. When building a Service Catalog and many of the topics have variables specific to the process, since the variables are all stored in another table, ¿is indexing enough to generate the reports quickly? ¿Is there something customers can consider to ensure we have the best efficiency in this regard?"
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎02-10-2022
94,04:00 PM
94,"Hi @Gabriela Cortes ,"
94,Sorry for missing your question!
94,"Off the top of my head I would say that having all the right indexes is not necessarily the only thing you need to do to ensure that such reports execute quickly, but I really couldn't say what other problems you might encounter without knowing a lot more about your detailed use case. I'm not familiar with the concepts of ""topic"" or ""process"" as it relates to ServiceNow's Service Catalog. Perhaps you are referring to the Topics in the Unified Taxonomy in Employee Center? In that case, I would have to know a lot more about what you are trying to accomplish with the reports."
94,"Ultimately, your question is not really within the scope of the content of this article and, unfortunately, we don't have enough information to answer it here. If you haven't already, please reach out to our Technical Support team at support.servicenow.com if you have a specific question about the performance of your Service Catalog Variable reports. They'll be happy to help!"
94,"Regards,"
94,ServiceNowPerformanceGTS
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎02-10-2022
94,04:02 PM
94,This is a great point and something that we cover in-depth in our Scripting Best Practice article here.
94,Helpfuls
94,Thomas Wright1
94,Tera Contributor
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎02-24-2023
94,06:04 AM
94,"When querying with GlideRecord for data on other tables, which scenario is more efficient?Looking up the other record first, e.g. a Manufacturer by it's name, and then doing a second query with the sys_id of that Manufacturer as one of the queries for the record you really want to look upDot walking as part of the query, e.g. addQuery(""manufacturer.name"", ""myname"");On a related note, are encoded queries any faster or slower than addQuery? Especially for OR conditions I use encoded queries since it is so simple to build them from a list view compared to multiple addQuery and addOrCondition methods."
94,Helpfuls
94,GTSPerformance
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎02-27-2023
94,11:38 AM
94,"@Thomas Wright1 Thanks for your questions. 1. When querying with GlideRecord for data on other tables, is it more efficient to dot-walk as part of the query, e.g., addQuery(""manufacturer.name"", ""myname""), or to do two GlideRecord queries, using the output of the first query (e.g. sys_id from a reference field) as input for the second query on the referenced table. Answer: There's a few factors that may influence this decision. The important thing to know is that when you dot-walk inside addQuery, it requires a database JOIN operation (usually an INNER JOIN). In terms of database efficiency, it is probably more efficient to avoid a JOIN and do two separate queries. However, in most cases, the efficiency gain is going to be at or near 0%. I would definitely say that putting the dot-walk inside the addQuery would be more efficient from a code complexity standpoint and, perhaps more importantly, it will let the database do what it does best: retrieve the data. As long as it is efficient to do so, I would follow the axiom of ""let the database do the data processing"". For your specific example, I would add that if you can remove the need for either a dot-walk or two queries, that would be ideal. Perhaps by changing it to addQuery(""manufacturer"", mfrID), where mfrID is a variable containing a reference (i.e. sys_id) of the desired manufacturer, you could reduce it to a single query without the need of a JOIN. 2. Are encoded queries any faster or slower than addQuery? Answer: No. Encoded queries are neither faster nor slower than the addQuery and addOrCondition methods. The processing time to execute those methods vs. addEncodedQuery is negligible.Regards,ServiceNowPerformanceGTS"
94,Helpful
94,Abhishek Pande1
94,Giga Guru
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎10-12-2023
94,01:24 AM
94,Hi @GTSPerformance Thanks for sharing this is very helpful and informative.
94,Regards
94,Abhishek Pandey
94,Helpfuls
94,shamil-ibrahim
94,Tera Contributor
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,‎10-24-2023
94,07:52 AM
94,With one of our client we experienced slowness due to usage of multiple addOrCondition statements.
94,Helpfuls
94,Mark Edson
94,Tera Contributor
94,Mark as Read
94,Mark as New
94,Bookmark
94,Permalink
94,Print
94,Report Inappropriate Content
94,a month ago
94,Very helpful article!
94,Helpfuls
94,Version history
94,Last update:
94,‎11-22-2022
94,09:31 AM
94,Updated by:
94,Lisa Latour
94,Contributors
94,GTSPerformance
94,Lisa Latour
94,The world works with ServiceNow™
94,Terms and conditions
94,GDPR
94,Privacy statement
94,Your privacy choices
94,Cookie policy
94,©2023 ServiceNow. All rights reserved.
95,"Mydbops | Expert Database SolutionsMySQLOur MySQL experts are here to help you harness the full potential of this robust open source database system. From installation and configuration to performance tuning and optimization, we've got you covered.Know MoreMariaDBMariaDB users can count on us for seamless database management. Our experts provide services ranging from backup solutions to security enhancements.Know MoreMongoDBLooking to leverage the flexibility and scalability of MongoDB? Mydbops is your go-to partner for MongoDB database management services, including sharding, replication, and data modeling.Know MoreProxySQLOptimize your database infrastructure with ProxySQL, a high-performance, open-source proxy. Mydbops provides expert support for installation, configuration, and fine-tuning to ensure your databases run smoothly and efficiently.Know MorePostgreSQLPostgreSQL users, your search for expert database management ends here. Mydbops offers comprehensive PostgreSQL services, from high availability setups to query optimization.Know MoreMaxscaleEnhance your database infrastructure with Maxscale solutions. We offer installation, configuration, and optimization services to ensure your systems run at peak efficiency.Know MoreCassandraFor organizations relying on Cassandra for their NoSQL needs, Mydbops offers comprehensive support, including data modeling, performance tuning, and data distribution strategies.Know MoreTiDBHarness the power of TiDB with our specialized management services. Our team can help you with deployment, monitoring, and scaling of your TiDB clusters.Know MoreWhy Mydbops?Unlock the true potential of your databases with our top-notch open source servicesExpertiseOur team comprises seasoned professionals with in-depth knowledge of various open source database systems.Performance Driven SolutionsWe understand that no two businesses are the same. That's why we tailor our services to meet your specific requirements.Seamless ScalabilityWe focus on optimizing your database systems for improved efficiency and reliability.Reliable SupportOur dedicated support team is available round the clock to address your queries and concerns.Database management made easy!Discover the easiest way to manage your databases with our expert solutions. Our team specializes in streamlining the process, making it hassle-free for you. With expertise in open source technology, trust us to handle your database needs efficiently and securely."
95,"Say goodbye to complexity and welcome seamless database management, including installations, upgrades, and migrations without data loss. Enjoy cost-effective services tailored to your requirements."
95,"Rely on our experienced team for top-notch database management, data security, and 24/7 support. Explore seamless database management with Mydbops!Expert solutions for open-source databases.Unlock seamless scalability, performance-driven solutions, and reliable support.Contact us©2023. Mydbops. All Rights Reserved"
98,[MDEV-30501] MariaDB 10.6.11 performance 65% slower than MySQL 8.0.32 - Jira
98,Log inSkip to main contentSkip to sidebarLinked ApplicationsLoading…DashboardsProjectsIssues
98,Give feedback to Atlassian
98,Help
98,Jira Core help
98,Keyboard Shortcuts
98,Issue Reminders help
98,About Jira
98,Jira Credits
98,Log In
98,MariaDB ServerMDEV-30501MariaDB 10.6.11 performance 65% slower than MySQL 8.0.32Log In ExportXMLWordPrintableDetails
98,Type:
98,Bug
98,Status:
98,Needs Feedback
98,(View Workflow)
98,Priority:
98,Major
98,Resolution:
98,Unresolved
98,Affects Version/s:
98,10.6.11
98,Fix Version/s:
98,10.6
98,Component/s:
98,None
98,Labels:
98,performance
98,Environment:
98,Hide
98,Server : DELL PowerEdge R7525
98,- Dual socket AMD EPYC 7543 32-Core Processor (64 core / 128 threads)
98,- RAM : 512GB DDR4
98,- Storage : SSD SAS (OS) & NVMe (/var/lib)
98,- OS : Ubuntu Server 22.04
98,-----------------------------------------------------------------------------
98,Tested DB version : MariaDB 10.6.11 & MySQL 8.0.32
98,DB Configuration :
98,large_pages=ON
98,skip-log-bin
98,max_connections=4000
98,table_open_cache=8000
98,table_open_cache_instances=16
98,back_log=1500
98,performance_schema=OFF
98,max_prepared_stmt_count=128000
98,transaction_isolation=REPEATABLE-READ
98,innodb_file_per_table
98,innodb_log_file_size=1024M
98,innodb_log_files_in_group=32
98,innodb_open_files=4000
98,innodb_buffer_pool_size=200G
98,innodb_buffer_pool_instances=16
98,innodb_log_buffer_size=64M
98,innodb_doublewrite=0
98,innodb_thread_concurrency=0
98,innodb_flush_log_at_trx_commit=0
98,innodb_max_dirty_pages_pct=90
98,innodb_max_dirty_pages_pct_lwm=10
98,join_buffer_size=32K
98,sort_buffer_size=32K
98,innodb_use_native_aio=1
98,innodb_stats_persistent=1
98,innodb_spin_wait_delay=6
98,innodb_max_purge_lag_delay=300000
98,innodb_max_purge_lag=0
98,innodb_flush_method=O_DIRECT_NO_FSYNC
98,innodb_checksum_algorithm=crc32
98,innodb_io_capacity=4000
98,innodb_io_capacity_max=20000
98,innodb_lru_scan_depth=9000
98,innodb_change_buffering=none
98,innodb_read_only=0
98,innodb_page_cleaners=4
98,innodb_undo_log_truncate=off
98,innodb_adaptive_flushing=1
98,innodb_flush_neighbors=0
98,innodb_read_io_threads=16
98,innodb_write_io_threads=16
98,innodb_purge_threads=4
98,innodb_adaptive_hash_index=0
98,-----------------------------------------------------------------------------
98,Benchmark software : HammerDB-4.6
98,*HammerDB is running on the same server as the DB.
98,(Generated 800 warehouse)
98,MariaDB run configuration :
98,"puts ""MariaDB 10.6 Test Started"""
98,dbset db maria
98,dbset bm TPC-C
98,diset connection maria_socket /run/mysqld/mysqld.sock
98,diset tpcc maria_driver timed
98,diset tpcc maria_rampup 2
98,diset tpcc maria_duration 5
98,vuset logtotemp 1
98,vuset unique 1
98,loadscript
98,foreach z {10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320} {
98,"puts ""$z VU test"""
98,vuset vu $z
98,vucreate
98,vurun
98,runtimer 480
98,vudestroy
98,"puts ""MariaDB 10.6 Test Complete"""
98,MySQL run configuration :
98,"puts ""Mysql 8 Test Started"""
98,dbset db mysql
98,dbset bm TPC-C
98,diset connection mysql_socket /var/run/mysqld/mysqld.sock
98,diset tpcc mysql_driver timed
98,diset tpcc mysql_rampup 2
98,diset tpcc mysql_duration 5
98,vuset logtotemp 1
98,vuset unique 1
98,loadscript
98,foreach z {10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320} {
98,"puts ""$z VU test"""
98,vuset vu $z
98,vucreate
98,vurun
98,runtimer 480
98,vudestroy
98,"puts ""Mysql 8 Test Complete"""
98,Show
98,Server : DELL PowerEdge R7525
98,- Dual socket AMD EPYC 7543 32-Core Processor (64 core / 128 threads)
98,- RAM : 512GB DDR4
98,- Storage : SSD SAS (OS) & NVMe (/var/lib)
98,- OS : Ubuntu Server 22.04
98,-----------------------------------------------------------------------------
98,Tested DB version : MariaDB 10.6.11 & MySQL 8.0.32
98,DB Configuration :
98,large_pages=ON
98,skip-log-bin
98,max_connections=4000
98,table_open_cache=8000
98,table_open_cache_instances=16
98,back_log=1500
98,performance_schema=OFF
98,max_prepared_stmt_count=128000
98,transaction_isolation=REPEATABLE-READ
98,innodb_file_per_table
98,innodb_log_file_size=1024M
98,innodb_log_files_in_group=32
98,innodb_open_files=4000
98,innodb_buffer_pool_size=200G
98,innodb_buffer_pool_instances=16
98,innodb_log_buffer_size=64M
98,innodb_doublewrite=0
98,innodb_thread_concurrency=0
98,innodb_flush_log_at_trx_commit=0
98,innodb_max_dirty_pages_pct=90
98,innodb_max_dirty_pages_pct_lwm=10
98,join_buffer_size=32K
98,sort_buffer_size=32K
98,innodb_use_native_aio=1
98,innodb_stats_persistent=1
98,innodb_spin_wait_delay=6
98,innodb_max_purge_lag_delay=300000
98,innodb_max_purge_lag=0
98,innodb_flush_method=O_DIRECT_NO_FSYNC
98,innodb_checksum_algorithm=crc32
98,innodb_io_capacity=4000
98,innodb_io_capacity_max=20000
98,innodb_lru_scan_depth=9000
98,innodb_change_buffering=none
98,innodb_read_only=0
98,innodb_page_cleaners=4
98,innodb_undo_log_truncate=off
98,innodb_adaptive_flushing=1
98,innodb_flush_neighbors=0
98,innodb_read_io_threads=16
98,innodb_write_io_threads=16
98,innodb_purge_threads=4
98,innodb_adaptive_hash_index=0
98,-----------------------------------------------------------------------------
98,Benchmark software : HammerDB-4.6
98,*HammerDB is running on the same server as the DB.
98,(Generated 800 warehouse)
98,MariaDB run configuration :
98,"puts ""MariaDB 10.6 Test Started"""
98,dbset db maria
98,dbset bm TPC-C
98,diset connection maria_socket /run/mysqld/mysqld.sock
98,diset tpcc maria_driver timed
98,diset tpcc maria_rampup 2
98,diset tpcc maria_duration 5
98,vuset logtotemp 1
98,vuset unique 1
98,loadscript
98,foreach z {10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320} {
98,"puts ""$z VU test"""
98,vuset vu $z
98,vucreate
98,vurun
98,runtimer 480
98,vudestroy
98,"puts ""MariaDB 10.6 Test Complete"""
98,MySQL run configuration :
98,"puts ""Mysql 8 Test Started"""
98,dbset db mysql
98,dbset bm TPC-C
98,diset connection mysql_socket /var/run/mysqld/mysqld.sock
98,diset tpcc mysql_driver timed
98,diset tpcc mysql_rampup 2
98,diset tpcc mysql_duration 5
98,vuset logtotemp 1
98,vuset unique 1
98,loadscript
98,foreach z {10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320} {
98,"puts ""$z VU test"""
98,vuset vu $z
98,vucreate
98,vurun
98,runtimer 480
98,vudestroy
98,"puts ""Mysql 8 Test Complete"""
98,Description
98,"Hi guys,"
98,Here is the benchmark result :
98,"When MySQL reaches the 100 virtual users (vUSER), it can easily reach 2.5 million or more transactions per minute (TPM)."
98,MariaDB cannot even reach 1 million TPM.
98,What is going on with MariaDB's performance? Why does it seem to be throttled to under 1 million TPM?
98,Please show me the right direction to improve the MariaDB configuration and maybe match the MySQL 8 performance.
98,Regards.
98,Irwandy.
98,AttachmentsAttachmentsOptionsSort By NameSort By DateAscendingDescendingThumbnailsListDownload All10.8.7-MariaDB_vs_mysql8.png29 kB2023-01-31 07:40fg_mdb11_withsps.svg720 kB2023-10-09 10:54fg_mysql57_withsps.svg575 kB2023-10-09 10:54fg_mysql8_withsps.svg647 kB2023-10-09 10:54image-2023-01-30-09-28-41-271.png22 kB2023-01-30 01:29image-2023-01-30-23-14-59-419.png31 kB2023-01-30 15:15innodb_io_data_writes.png46 kB2023-02-01 03:12innodb_io_writes.png72 kB2023-02-01 03:13mariadb_vs_mysql_NOPM.png31 kB2023-02-01 13:19mariadbmysql1.png46 kB2023-10-03 09:31mariadbmysql2.png35 kB2023-10-03 09:31mariadbmysql3.png47 kB2023-10-03 09:31mariadbmysql4.png32 kB2023-10-03 09:31mariadbmysql5.png29 kB2023-10-03 09:31tpcc2.png13 kB2023-02-14 15:14Issue Links
98,relates to
98,MDEV-30628
98,10.6 performance regression with sustained high-connection write-only OLTP workload (55-80% degradation)
98,Needs Feedback
98,Activity
98,People
98,Assignee:
98,Marko Mäkelä
98,Reporter:
98,Irwandy
98,Votes:
98,Vote for this issue
98,Watchers:
98,Start watching this issue
98,Dates
98,Created:
98,2023-01-30 01:43
98,Updated:
98,2023-10-26 06:57
98,Git IntegrationError rendering 'com.xiplink.jira.git.jira_git_plugin:git-issue-webpanel'. Please contact your Jira administrators.
98,Atlassian Jira Project Management Software
98,About Jira
98,Report a problem
98,Powered by a free Atlassian Jira open source license for MariaDB Corporation Ab. Try Jira - bug tracking software for your team.
98,Atlassian
99,"Optimization suggestions for thumbnail display and general ""smooth"" user experience - ℹ️ Support - Nextcloud community"
99,Nextcloud community
99,"Optimization suggestions for thumbnail display and general ""smooth"" user experience"
99,ℹ️ Support
99,luxzg
99,"March 30, 2023, 12:38pm"
99,Nextcloud version: 26.0.0
99,Operating system and version: Ubuntu 22.04.2 LTS
99,Apache version:
99,Apache/2.4.52 (Ubuntu)
99,Database version: MariaDB 10.6.12
99,"PHP version (eg, 7.4): 8.1.2"
99,CPU: Intel(R) Core(TM) i5-6400 CPU @ 2.70GHz (4 cores)
99,Memory: 15.06 GB
99,OS and installation: 256GB SATA SSD
99,Data drive: 8TB WD Purple HDD
99,The issue you are facing:
99,"This is not an issue as such, I am just trying to get best user experience from my new NC installation."
99,"It is a home build, for just 2 users, but I’m a deamding user"
99,Specs are above.
99,"I’m curently at about half of my 8TB drive, and 99% of data are pictures and videos."
99,I have done the following so far:
99,"installed NC26 on Ubuntu, OS, NC, and all related data is on SATA SSD"
99,"created one btrfs partition across the WD 8TB rotational HDD for the data, and configured it during NC installation"
99,"installed preview generator app, and I’m at about half the scan at the moment"
99,moved appdata folder that includes preview thumbnails to SSD via mount bind which improved thumbnail display in web interface drastically
99,installed and setup Recognize app which started to show faces
99,installed and setup Memories app and is working fine
99,"installed and serup Maps app, and photos are showing correctly on it (about 14k photos with coordinates so far)"
99,"What I have NOT done yet, but plan to explore in the following days:"
99,opcache
99,"redis, APCu, or similar"
99,HTTP/2
99,What I want:
99,"Fast user experience when browsing folders with pictures, or scrolling the Map with thumbnails shown on it. Preferably having whole database and all thumbnails in RAM."
99,"At the moment I can load Map, and thumbnails show up in 20-30 seconds. After that they work relatively smoothly, though if I move map a lot and zoom in/out I get several “black” (unloaded) thumbnails that need few seconds to start loading. Same with browsing folders."
99,"I know part of it is due to my preview-generation still running, my face recognition still working in the background slowly indexing faces, loading my server’s CPU and disk, and some thumbnails still missing, and so on. But I still think it can be even better."
99,"So since I have 16GB RAM, and I’m using around 1.1-1.2GB under load, I started thinking how to load more in RAM."
99,"My database is about 1.5GB, and I’d love it to be cached to memory completely."
99,"My thumbnail cache is about 1GB at the moment, and I’d love it to load in server’s memory as well."
99,"I’ve started reading about memcache techniques for local system (files), and I have some experience with databases, and I know they’ll both cache files and tables once they’re requested."
99,"But I’d love them to be loaded when server starts. Even if my database and thumbnail folders double up by the time everything is fully indexed, that’s still “only” 5GB. If I fill my data drive completely (eventually) that would be just about 10GB of RAM taken by DB and thumbnails. So I’d still be quite good on RAM and would keep having plenty of it spare."
99,"Nextcloud documentation is pretty good on caching, but - it still doesn’t say how to get the effect I’m expecting."
99,"Sure, is server is up for weeks, it will eventually cache everything I regularly use. But I’d still like to use all that RAM to the fullest."
99,"Same/similar with database. Though I could in theory make a small script, or just manually run through the database, run couple “select *” and force most of it into RAM that way, I’d rather have it using something more like MEMORY tables (though obviously permanently, not as temp tables)."
99,"I like to learn, and I’m open to suggestions. I’ll keep reading docs and tutorials, but if anyone has any hands-on experience with setting up NC in a similar way, it would save me days and days of reading."
99,Thanks in advance!
99,luxzg
99,"April 12, 2023,"
99,6:06pm
99,"After 2 weeks, I’m somewhat smarter."
99,"I gave up on caching thumbnails, as my preview folder kept growing, and is now stable at 64GB. Bit out of the range"
99,"I’ve set memory limit for database to 3GB, and currently it seems to be sitting at 2GB."
99,"I’ve setup OPcache, APCu, and redis, not sure I feel much of the difference, but at least I feel better about it"
99,"I’ve setup daily mysqldump, unsure if that basically does “select *” on all tables, but at least I have backups done."
99,"I’ve stated using Maps from inside Memories app, it’s much faster at displaying thumbnails."
99,"Experience in LAN is pretty good, server is mostly idle, so power consumption is fine, temperatures are low, but when I want to use it feels snappy enough."
99,"I’ve also taken my time and wrote what I did, more as a reminder to myself, but it ended up being pretty nice guide IMHO, so I’ve published it to github here:"
99,GitHub
99,"GitHub - luxzg/Nextcloud-setup: Guides, scripts and configs to help install..."
99,"Guides, scripts and configs to help install and setup Nextcloud 26 - GitHub - luxzg/Nextcloud-setup: Guides, scripts and configs to help install and setup Nextcloud 26"
99,"I’ve included sample configs for PHP, MySQL, redis, OPcache, Nextcloud config, and so on. Hopefully someone finds it useful (beside myself)."
99,Cheers!
99,pulsejet
99,"April 17, 2023,"
99,6:27pm
99,Just a few notes:
99,You need HTTP/2 for best performance in most cases.
99,You can reduce the size of the previews (see this). I think the default is too high for many use cases.
99,"Another optimization I found very useful is placing the appdata folder (that contains the previews) on the SSD, that way it’s much faster."
99,"BTW, the reason Memories is faster at previews is because it pipelines the previews into a single HTTP request as part of the application logic (just saying it’s not a coincidence)."
99,luxzg
99,"April 17, 2023,"
99,6:44pm
99,"Thanks for the suggestions, that pretty much confirmed I was on a right path"
99,"I’ve set preview limits to 1440*1440 (my PC screen is 1440 high, and phones are mostly fine with FHD anyway)"
99,"I’ve enabled HTTP/2, at least on PC I can confirm it’s working in Chrome"
99,"and whole appdata folder is on SSD, that truly made the biggest difference (huge jump in performance right away)"
99,And thanks for your work on the Nextcloud apps and all
99,It’s really appreciated
99,Home
99,Categories
99,FAQ/Guidelines
99,Terms of Service
99,Privacy Policy
99,"Powered by Discourse, best viewed with JavaScript enabled"
